final_method	final_comment
@Override public void writeTo(StreamOutput out) throws IOException { out.writeVInt(entries.size()); for (Entry entry : entries) { entry.snapshot().writeTo(out); out.writeBoolean(entry.includeGlobalState()); out.writeBoolean(entry.partial()); out.writeByte(entry.state().value()); out.writeVInt(entry.indices().size()); for (IndexId index : entry.indices()) { index.writeTo(out); } out.writeLong(entry.startTime()); out.writeVInt(entry.shards().size()); for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : entry.shards()) { shardEntry.key.writeTo(out); // TODO: Change this to an appropriate version when it's backported if (out.getVersion().onOrAfter(Version.V_6_0_0_beta1)) { shardEntry.value.writeTo(out); } else { out.writeOptionalString(shardEntry.value.nodeId()); out.writeByte(shardEntry.value.state().value()); } } if (out.getVersion().onOrAfter(REPOSITORY_ID_INTRODUCED_VERSION)) { out.writeLong(entry.repositoryStateId); } if (out.getVersion().onOrAfter(Version.V_6_7_0)) { out.writeOptionalString(entry.failure); } } }	we will have to mute bwc tests before merging this and then adjust this version to 6.7 in master afterwards, currently we have a 7.0.0 here in master.
@Override public void onFailure(Exception e) { logger.warn( () -> new ParameterizedMessage("[{}] [{}] failed to update snapshot state", snapshot, status), e); } }, (req, reqListener) -> transportService.sendRequest(transportService.getLocalNode(), UPDATE_SNAPSHOT_STATUS_ACTION_NAME, req, new TransportResponseHandler<UpdateIndexShardSnapshotStatusResponse>() { @Override public UpdateIndexShardSnapshotStatusResponse read(StreamInput in) throws IOException { final UpdateIndexShardSnapshotStatusResponse response = new UpdateIndexShardSnapshotStatusResponse(); response.readFrom(in); return response; } @Override public void handleResponse(UpdateIndexShardSnapshotStatusResponse response) { reqListener.onResponse(null); } @Override public void handleException(TransportException exp) { reqListener.onFailure(exp); } @Override public String executor() { return ThreadPool.Names.SAME; } }) ); } /** * Updates the shard status on master node * * @param request update shard status request */ private void innerUpdateSnapshotState(final UpdateIndexShardSnapshotStatusRequest request, ActionListener<UpdateIndexShardSnapshotStatusResponse> listener) { logger.trace("received updated snapshot restore state [{}]", request); clusterService.submitStateUpdateTask( "update snapshot state", request, ClusterStateTaskConfig.build(Priority.NORMAL), snapshotStateExecutor, new ClusterStateTaskListener() { @Override public void onFailure(String source, Exception e) { listener.onFailure(e); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { listener.onResponse(new UpdateIndexShardSnapshotStatusResponse()); } }); } private class SnapshotStateExecutor implements ClusterStateTaskExecutor<UpdateIndexShardSnapshotStatusRequest> { @Override public ClusterTasksResult<UpdateIndexShardSnapshotStatusRequest> execute(ClusterState currentState, List<UpdateIndexShardSnapshotStatusRequest> tasks) { final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); if (snapshots != null) { int changedCount = 0; final List<SnapshotsInProgress.Entry> entries = new ArrayList<>(); for (SnapshotsInProgress.Entry entry : snapshots.entries()) { ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder(); boolean updated = false; for (UpdateIndexShardSnapshotStatusRequest updateSnapshotState : tasks) { if (entry.snapshot().equals(updateSnapshotState.snapshot())) { logger.trace("[{}] Updating shard [{}] with status [{}]", updateSnapshotState.snapshot(), updateSnapshotState.shardId(), updateSnapshotState.status().state()); if (updated == false) { shards.putAll(entry.shards()); updated = true; } shards.put(updateSnapshotState.shardId(), updateSnapshotState.status()); changedCount++; } } if (updated) { if (completed(shards.values()) == false) { entries.add(new SnapshotsInProgress.Entry(entry, shards.build())); } else { // Snapshot is finished - mark it as done // TODO: Add PARTIAL_SUCCESS status? SnapshotsInProgress.Entry updatedEntry = new SnapshotsInProgress.Entry(entry, State.SUCCESS, shards.build()); entries.add(updatedEntry); } } else { entries.add(entry); } } if (changedCount > 0) { logger.trace("changed cluster state triggered by {} snapshot state updates", changedCount); return ClusterTasksResult.<UpdateIndexShardSnapshotStatusRequest>builder().successes(tasks) .build(ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(unmodifiableList(entries))).build()); } } return ClusterTasksResult.<UpdateIndexShardSnapshotStatusRequest>builder().successes(tasks).build(currentState); } } static class UpdateIndexShardSnapshotStatusResponse extends ActionResponse { } private class UpdateSnapshotStatusAction extends TransportMasterNodeAction<UpdateIndexShardSnapshotStatusRequest, UpdateIndexShardSnapshotStatusResponse> { UpdateSnapshotStatusAction(TransportService transportService, ClusterService clusterService, ThreadPool threadPool, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) { super( settings, SnapshotShardsService.UPDATE_SNAPSHOT_STATUS_ACTION_NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, UpdateIndexShardSnapshotStatusRequest::new ); } @Override protected String executor() { return ThreadPool.Names.SAME; } @Override protected UpdateIndexShardSnapshotStatusResponse newResponse() { return new UpdateIndexShardSnapshotStatusResponse(); } @Override protected void masterOperation(UpdateIndexShardSnapshotStatusRequest request, ClusterState state, ActionListener<UpdateIndexShardSnapshotStatusResponse> listener) { innerUpdateSnapshotState(request, listener); }	passing the settings here (we don't have that argument in master anymore) is the only difference between this pr and master in this file.
public void testSuccessfulDecodeHttpRequest() throws IOException { String uri = "localhost:9090/" + randomAlphaOfLength(8); io.netty.handler.codec.http.HttpRequest httpRequest = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.GET, uri); ByteBuf buf = requestEncoder.encode(httpRequest); int slicePoint = randomInt(buf.writerIndex() - 1); ByteBuf slicedBuf = buf.retainedSlice(0, slicePoint); ByteBuf slicedBuf2 = buf.retainedSlice(slicePoint, buf.writerIndex()); try { handler.consumeReads(toChannelBuffer(slicedBuf)); verify(transport, times(0)).incomingRequest(any(HttpRequest.class), any(NioHttpChannel.class)); handler.consumeReads(toChannelBuffer(slicedBuf2)); ArgumentCaptor<HttpRequest> requestCaptor = ArgumentCaptor.forClass(HttpRequest.class); verify(transport).incomingRequest(requestCaptor.capture(), any(NioHttpChannel.class)); HttpRequest nioHttpRequest = requestCaptor.getValue(); assertEquals(HttpRequest.HttpVersion.HTTP_1_1, nioHttpRequest.protocolVersion()); assertEquals(RestRequest.Method.GET, nioHttpRequest.method()); } finally { handler.close(); buf.release(); slicedBuf.release(); slicedBuf2.release(); } }	i don't know that we care about closing the handler. it probably does not matter too much, but there should not be any resources hanging around if we properly consume all the requests.
private FullHttpResponse executeCorsRequest(final Settings settings, final String originValue, final String host) throws IOException { HttpHandlingSettings httpHandlingSettings = HttpHandlingSettings.fromSettings(settings); NioCorsConfig nioCorsConfig = NioHttpServerTransport.buildCorsConfig(settings); HttpReadWriteHandler handler = new HttpReadWriteHandler(nioHttpChannel, transport, httpHandlingSettings, nioCorsConfig); prepareHandlerForResponse(handler); DefaultFullHttpRequest httpRequest = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.GET, "/"); if (originValue != null) { httpRequest.headers().add(HttpHeaderNames.ORIGIN, originValue); } httpRequest.headers().add(HttpHeaderNames.HOST, host); NioHttpRequest nioHttpRequest = new NioHttpRequest(httpRequest, 0); BytesArray content = new BytesArray("content"); HttpResponse response = nioHttpRequest.createResponse(RestStatus.OK, content); response.addHeader("Content-Length", Integer.toString(content.length())); SocketChannelContext context = mock(SocketChannelContext.class); List<FlushOperation> flushOperations = handler.writeToBytes(handler.createWriteOperation(context, response, (v, e) -> { ReferenceCountUtil.release(v); })); handler.close(); FlushOperation flushOperation = flushOperations.get(0); ((ChannelPromise) flushOperation.getListener()).setSuccess(); return responseDecoder.decode(Unpooled.wrappedBuffer(flushOperation.getBuffersToWrite())); }	this does not do anything. the listener takes void. v will always be null.
private FullHttpResponse executeCorsRequest(final Settings settings, final String originValue, final String host) throws IOException { HttpHandlingSettings httpHandlingSettings = HttpHandlingSettings.fromSettings(settings); NioCorsConfig nioCorsConfig = NioHttpServerTransport.buildCorsConfig(settings); HttpReadWriteHandler handler = new HttpReadWriteHandler(nioHttpChannel, transport, httpHandlingSettings, nioCorsConfig); prepareHandlerForResponse(handler); DefaultFullHttpRequest httpRequest = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.GET, "/"); if (originValue != null) { httpRequest.headers().add(HttpHeaderNames.ORIGIN, originValue); } httpRequest.headers().add(HttpHeaderNames.HOST, host); NioHttpRequest nioHttpRequest = new NioHttpRequest(httpRequest, 0); BytesArray content = new BytesArray("content"); HttpResponse response = nioHttpRequest.createResponse(RestStatus.OK, content); response.addHeader("Content-Length", Integer.toString(content.length())); SocketChannelContext context = mock(SocketChannelContext.class); List<FlushOperation> flushOperations = handler.writeToBytes(handler.createWriteOperation(context, response, (v, e) -> { ReferenceCountUtil.release(v); })); handler.close(); FlushOperation flushOperation = flushOperations.get(0); ((ChannelPromise) flushOperation.getListener()).setSuccess(); return responseDecoder.decode(Unpooled.wrappedBuffer(flushOperation.getBuffersToWrite())); }	no need for this as the listener does not release anything.
static List<DisplayHeader> buildDisplayHeaders(Table table, RestRequest request) { String pHeaders = request.param("h"); List<DisplayHeader> display = new ArrayList<>(); if (pHeaders != null) { Map<String, Table.Cell> headerMap = table.getHeaderMap(); Collection<String> headers = new LinkedHashSet<>(table.getHeaders().size()); // check headers and aliases for (String header : Strings.splitStringByCommaToArray(pHeaders)) { if (Regex.isSimpleMatchPattern(header)) { for (Map.Entry<String, Table.Cell> configuredHeaderEntry : headerMap.entrySet()) { String configuredHeader = configuredHeaderEntry.getKey(); if (Regex.simpleMatch(header, configuredHeader)) { headers.add(configuredHeader); } else if (configuredHeaderEntry.getValue().attr.containsKey("alias")) { String[] aliases = Strings.splitStringByCommaToArray(configuredHeaderEntry.getValue().attr.get("alias")); for (String alias : aliases) { if (Regex.simpleMatch(header, alias)) { headers.add(configuredHeader); break; } } } } } else { headers.add(header); } } for (String possibility : headers) { DisplayHeader dispHeader = null; if (table.getAsMap().containsKey(possibility)) { dispHeader = new DisplayHeader(possibility, possibility); } else { for (Table.Cell headerCell : table.getHeaders()) { String aliases = headerCell.attr.get("alias"); if (aliases != null) { for (String alias : Strings.splitStringByCommaToArray(aliases)) { if (possibility.equals(alias)) { dispHeader = new DisplayHeader(headerCell.value.toString(), alias); break; } } } } } if (dispHeader != null) { // We know we need the header asked for: display.add(dispHeader); // Look for accompanying sibling column Table.Cell hcell = table.getHeaderMap().get(dispHeader.name); String siblingFlag = hcell.attr.get("sibling"); if (siblingFlag != null) { // ...link the sibling and check that its flag is set String sibling = siblingFlag + "." + dispHeader.name; Table.Cell c = table.getHeaderMap().get(sibling); if (c != null && request.paramAsBoolean(siblingFlag, false)) { display.add(new DisplayHeader(c.value.toString(), siblingFlag + "." + dispHeader.display)); } } } } } else { for (Table.Cell cell : table.getHeaders()) { String d = cell.attr.get("default"); if (Booleans.parseBoolean(d, true)) { display.add(new DisplayHeader(cell.value.toString(), cell.value.toString())); } } } return display; }	why a set? are duplicates possible?
static List<DisplayHeader> buildDisplayHeaders(Table table, RestRequest request) { String pHeaders = request.param("h"); List<DisplayHeader> display = new ArrayList<>(); if (pHeaders != null) { Map<String, Table.Cell> headerMap = table.getHeaderMap(); Collection<String> headers = new LinkedHashSet<>(table.getHeaders().size()); // check headers and aliases for (String header : Strings.splitStringByCommaToArray(pHeaders)) { if (Regex.isSimpleMatchPattern(header)) { for (Map.Entry<String, Table.Cell> configuredHeaderEntry : headerMap.entrySet()) { String configuredHeader = configuredHeaderEntry.getKey(); if (Regex.simpleMatch(header, configuredHeader)) { headers.add(configuredHeader); } else if (configuredHeaderEntry.getValue().attr.containsKey("alias")) { String[] aliases = Strings.splitStringByCommaToArray(configuredHeaderEntry.getValue().attr.get("alias")); for (String alias : aliases) { if (Regex.simpleMatch(header, alias)) { headers.add(configuredHeader); break; } } } } } else { headers.add(header); } } for (String possibility : headers) { DisplayHeader dispHeader = null; if (table.getAsMap().containsKey(possibility)) { dispHeader = new DisplayHeader(possibility, possibility); } else { for (Table.Cell headerCell : table.getHeaders()) { String aliases = headerCell.attr.get("alias"); if (aliases != null) { for (String alias : Strings.splitStringByCommaToArray(aliases)) { if (possibility.equals(alias)) { dispHeader = new DisplayHeader(headerCell.value.toString(), alias); break; } } } } } if (dispHeader != null) { // We know we need the header asked for: display.add(dispHeader); // Look for accompanying sibling column Table.Cell hcell = table.getHeaderMap().get(dispHeader.name); String siblingFlag = hcell.attr.get("sibling"); if (siblingFlag != null) { // ...link the sibling and check that its flag is set String sibling = siblingFlag + "." + dispHeader.name; Table.Cell c = table.getHeaderMap().get(sibling); if (c != null && request.paramAsBoolean(siblingFlag, false)) { display.add(new DisplayHeader(c.value.toString(), siblingFlag + "." + dispHeader.display)); } } } } } else { for (Table.Cell cell : table.getHeaders()) { String d = cell.attr.get("default"); if (Booleans.parseBoolean(d, true)) { display.add(new DisplayHeader(cell.value.toString(), cell.value.toString())); } } } return display; }	wondering if extracting this block to a new method would make it a bit more readable, i have no strong opinion though
@Override public long ramBytesUsed() { return RamUsageEstimator.NUM_BYTES_OBJECT_HEADER + Long.BYTES + RamUsageEstimator.NUM_BYTES_OBJECT_REF; }	so translog.location does not need to implement accountable anymore?
static TermVectorsResponse getTermVectors(IndexShard indexShard, TermVectorsRequest request, LongSupplier nanoTimeSupplier) { final long startTime = nanoTimeSupplier.getAsLong(); final TermVectorsResponse termVectorsResponse = new TermVectorsResponse(indexShard.shardId().getIndex().getName(), request.type(), request.id()); final Term uidTerm = new Term(UidFieldMapper.NAME, Uid.createUidAsBytes(request.type(), request.id())); Engine.GetResult get = indexShard.get(new Engine.Get(request.realtime(), uidTerm).version(request.version()).versionType(request.versionType())); Fields termVectorsByField = null; AggregatedDfs dfs = null; TermVectorsFilter termVectorsFilter = null; /* handle potential wildcards in fields */ if (request.selectedFields() != null) { handleFieldWildcards(indexShard, request); } final Engine.Searcher searcher = indexShard.acquireSearcher("term_vector"); try { Fields topLevelFields = MultiFields.getFields(get.searcher() != null ? get.searcher().reader() : searcher.reader()); Versions.DocIdAndVersion docIdAndVersion = get.docIdAndVersion(); /* from an artificial document */ if (request.doc() != null) { termVectorsByField = generateTermVectorsFromDoc(indexShard, request, true); // if no document indexed in shard, take the queried document itself for stats if (topLevelFields == null) { topLevelFields = termVectorsByField; } termVectorsResponse.setArtificial(true); termVectorsResponse.setExists(true); } /* or from an existing document */ else if (docIdAndVersion != null) { // fields with stored term vectors termVectorsByField = docIdAndVersion.context.reader().getTermVectors(docIdAndVersion.docId); Set<String> selectedFields = request.selectedFields(); // generate tvs for fields where analyzer is overridden if (selectedFields == null && request.perFieldAnalyzer() != null) { selectedFields = getFieldsToGenerate(request.perFieldAnalyzer(), termVectorsByField); } // fields without term vectors if (selectedFields != null) { termVectorsByField = addGeneratedTermVectors(indexShard, get, termVectorsByField, request, selectedFields); } termVectorsResponse.setDocVersion(docIdAndVersion.version); termVectorsResponse.setExists(true); } /* no term vectors generated or found */ else { termVectorsResponse.setExists(false); } /* if there are term vectors, optional compute dfs and/or terms filtering */ if (termVectorsByField != null) { if (request.filterSettings() != null) { termVectorsFilter = new TermVectorsFilter(termVectorsByField, topLevelFields, request.selectedFields(), dfs); termVectorsFilter.setSettings(request.filterSettings()); try { termVectorsFilter.selectBestTerms(); } catch (IOException e) { throw new ElasticsearchException("failed to select best terms", e); } } // write term vectors termVectorsResponse.setFields(termVectorsByField, request.selectedFields(), request.getFlags(), topLevelFields, dfs, termVectorsFilter); } termVectorsResponse.setTookInMillis(TimeUnit.NANOSECONDS.toMillis(nanoTimeSupplier.getAsLong() - startTime)); } catch (Exception ex) { throw new ElasticsearchException("failed to execute term vector request", ex); } finally { searcher.close(); get.release(); } return termVectorsResponse; }	this seems to be the only call site of generatetermvectorsfromdoc so maybe we can simplify generatetermvectorsfromdoc by removing all branches that handle the false case
@Override public Optional<EngineFactory> getEngineFactory(IndexSettings indexSettings) { if (SearchableSnapshotsConstants.isSearchableSnapshotStore(indexSettings.getSettings()) && indexSettings.getSettings().getAsBoolean("index.frozen", false) == false) { return Optional.of(engineConfig -> new ReadOnlyEngine(engineConfig, null, new TranslogStats(), false, Function.identity()) { @Override protected void ensureMaxSeqNoEqualsToGlobalCheckpoint(SeqNoStats seqNoStats) { // searchable snapshots may not satisfy this property but that's ok since they're only subject to file-based // recoveries and the files "on disk" never change } }); } return Optional.empty(); }	instead of overriding the method every time it's needed maybe we could pass a ensuremaxseqnoequalstoglobalcheckpoint boolean flag when the readonlyengine is instanciated?
private void warnOnSpecialAttributeName(Assertion assertion, String attributeName, String fieldNameForLogMessage) { if (SPECIAL_ATTRIBUTE_NAMES.contains(attributeName)) { logger.warn( "SAML assertion [{}] has attribute with {} [{}] which clashes with a special attribute name. " + "Attributes with a name clash cannot be mapped. " + "Change attribute {} to not clash with any of [{}].", assertion.getElementQName(), fieldNameForLogMessage, attributeName, fieldNameForLogMessage, String.join(",", SPECIAL_ATTRIBUTE_NAMES) ); } }	i know you've done 1 round on this message already, but i think it's worth another go. the issue is, in the scenario we're dealing with, we can assume that the administrator doesn't really understand saml very well (because they made this mistake) and they really need an error message that helps them without assuming much knowledge. suggestion "saml assertion [{}] has attribute with {} [{}] which clashes with a special attribute name. " + "attributes with a name clash may prevent authentication or interfere will role mapping. " + "change your idp configuration to use a different attribute {}" + " that will not clash with any of [{}]"
@Override public List<XLookupResult> lookup(final XLookupOptions lookupOptions) { final CharSequence key = lookupOptions.key; final int num = lookupOptions.num; final AtomicReader reader = lookupOptions.reader; final Set<String> payloadFields = lookupOptions.payloadFields; /* DEBUG try { PrintWriter pw = new PrintWriter("/tmp/out.dot"); Util.toDot(fst, pw, true, true); pw.close(); } catch (IOException e) { e.printStackTrace(); } */ assert num > 0; if (fst == null) { return Collections.emptyList(); } final double liveDocsRatio = (reader != null) ? calculateLiveDocRatio(reader.numDocs(), reader.maxDoc()) : 1.0d; if (liveDocsRatio == -1) { return Collections.emptyList(); } final Bits liveDocs = (reader != null) ? reader.getLiveDocs() : null; if (payloadFields != null && reader == null) { throw new IllegalArgumentException("can't retrieve payloads if reader=null"); } //System.out.println("lookup key=" + key + " num=" + num); for (int i = 0; i < key.length(); i++) { if (key.charAt(i) == holeCharacter) { throw new IllegalArgumentException("lookup key cannot contain HOLE character U+001E; this character is reserved"); } if (key.charAt(i) == sepLabel) { throw new IllegalArgumentException("lookup key cannot contain unit separator character U+001F; this character is reserved"); } } try { Automaton lookupAutomaton = toLookupAutomaton(key); // Intersect automaton w/ suggest wFST and get all // prefix starting nodes & their outputs: //final PathIntersector intersector = getPathIntersector(lookupAutomaton, fst); //System.out.println(" prefixPaths: " + prefixPaths.size()); final List<XLookupResult> results = new ArrayList<>(); List<FSTUtil.Path<Pair<Long,BytesRef>>> prefixPaths = FSTUtil.intersectPrefixPaths(convertAutomaton(lookupAutomaton), fst); XUtil.TopNSearcher<Pair<Long,BytesRef>> searcher; searcher = new XUtil.TopNSearcher<Pair<Long,BytesRef>>(fst, num, getMaxTopNSearcherQueueSize(num, liveDocsRatio), weightComparator) { private final Set<BytesRef> seen = new HashSet<>(); private CharsRefBuilder spare = new CharsRefBuilder(); @Override protected boolean acceptResult(IntsRef input, Pair<Long,BytesRef> output) { XPayLoadProcessor.PayloadMetaData metaData = XPayLoadProcessor.parse(output.output2, hasPayloads, payloadSep, spare); if (liveDocs != null && metaData.hasDocID()) { if (!liveDocs.get(metaData.docID)) { return false; } } // Dedup: when the input analyzes to a graph we // can get duplicate surface forms: if (!lookupOptions.duplicateSurfaceForm && seen.contains(metaData.surfaceForm)) { return false; } seen.add(metaData.surfaceForm); try { XLookupResult result = getLookupResult(spare.get(), output.output1, metaData.payload, getPayloadFields(metaData.docID, payloadFields, reader)); results.add(result); return true; } catch (IOException e) { throw new RuntimeException(e); } } }; prefixPaths = getFullPrefixPaths(prefixPaths, lookupAutomaton, fst); for (FSTUtil.Path<Pair<Long,BytesRef>> path : prefixPaths) { searcher.addStartPaths(path.fstNode, path.output, true, path.input); } // TODO: for fuzzy case would be nice to return boolean isComplete = searcher.search(); // search admissibility is not guaranteed // see comment on getMaxTopNSearcherQueueSize //assert isComplete; return results; } catch (IOException bogus) { throw new RuntimeException(bogus); } }	i love that change :) all unneeded stuff imo
protected boolean acceptResult(IntsRef input, Pair<Long,BytesRef> output) { XPayLoadProcessor.PayloadMetaData metaData = XPayLoadProcessor.parse(output.output2, hasPayloads, payloadSep, spare); if (liveDocs != null && metaData.hasDocID()) { if (!liveDocs.get(metaData.docID)) { return false; } } // Dedup: when the input analyzes to a graph we // can get duplicate surface forms: if (!lookupOptions.duplicateSurfaceForm && seen.contains(metaData.surfaceForm)) { return false; } seen.add(metaData.surfaceForm); try { XLookupResult result = getLookupResult(spare.get(), output.output1, metaData.payload, getPayloadFields(metaData.docID, payloadFields, reader)); results.add(result); return true; } catch (IOException e) { throw new RuntimeException(e); } }	imo we should allow the interface to throw ioexception that just makes sense here
public void testGetSnapshots() { Map<String, String> expectedParams = new HashMap<>(); String repository = randomIndicesNames(1, 1)[0]; String snapshot1 = "snapshot1-" + randomAlphaOfLengthBetween(2, 5).toLowerCase(Locale.ROOT); String snapshot2 = "snapshot2-" + randomAlphaOfLengthBetween(2, 5).toLowerCase(Locale.ROOT); String endpoint = String.format(Locale.ROOT, "/_snapshot/%s/%s,%s", repository, snapshot1, snapshot2); GetSnapshotsRequest getSnapshotsRequest = new GetSnapshotsRequest(); getSnapshotsRequest.repository(repository); getSnapshotsRequest.snapshots(Arrays.asList(snapshot1, snapshot2).toArray(new String[0])); setRandomMasterTimeout(getSnapshotsRequest, expectedParams); if (randomBoolean()) { getSnapshotsRequest.ignoreUnavailable(true); expectedParams.put("ignore_unavailable", Boolean.TRUE.toString()); } if (randomBoolean()) { getSnapshotsRequest.verbose(false); expectedParams.put("verbose", Boolean.FALSE.toString()); } Request request = RequestConverters.getSnapshots(getSnapshotsRequest); assertThat(endpoint, equalTo(request.getEndpoint())); assertThat(HttpGet.METHOD_NAME, equalTo(request.getMethod())); assertThat(expectedParams, equalTo(request.getParameters())); assertNull(request.getEntity()); }	maybe just simplify these by setting it to the value of the randomboolean() and using boolean.valueof? not super necessary, just a nit.
public static GetSnapshotsResponse fromXContent(XContentParser parser) throws IOException { XContentParser.Token token = parser.nextToken(); ensureExpectedToken(XContentParser.Token.START_OBJECT, token, parser::getTokenLocation); ArrayList<SnapshotInfo> snapshots = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { String currentFieldName = parser.currentName(); if ("snapshots".equals(currentFieldName)) { token = parser.nextToken(); ensureExpectedToken(XContentParser.Token.START_ARRAY, token, parser::getTokenLocation); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { snapshots.add(SnapshotInfo.fromXContent(parser)); } } } if (parser.currentToken() != XContentParser.Token.END_OBJECT) { throw new IllegalArgumentException("unexpected token [" + parser.currentToken() + "], expected ['}']"); }	any reason that an object parser was not used here?
static List<Index> getLeaderIndicesToFollow(AutoFollowPattern autoFollowPattern, ClusterState leaderClusterState, ClusterState followerClusterState, List<String> followedIndexUUIDs) { List<Index> leaderIndicesToFollow = new ArrayList<>(); for (IndexMetaData leaderIndexMetaData : leaderClusterState.getMetaData()) { // We should not automatically follow a leader index that is also a follow index and // that both leader and follow index are in the same cluster otherwise this can // result into an index creation explosion. if (leaderIndexMetaData.getCustomData(Ccr.CCR_CUSTOM_METADATA_KEY) != null && leaderClusterState.getClusterName().equals(followerClusterState.getClusterName())) { continue; } if (autoFollowPattern.match(leaderIndexMetaData.getIndex().getName())) { if (followedIndexUUIDs.contains(leaderIndexMetaData.getIndex().getUUID()) == false) { // TODO: iterate over the indices in the followerClusterState and check whether a IndexMetaData // has a leader index uuid custom metadata entry that matches with uuid of leaderIndexMetaData variable // If so then handle it differently: not follow it, but just add an entry to // AutoFollowMetadata#followedLeaderIndexUUIDs leaderIndicesToFollow.add(leaderIndexMetaData.getIndex()); } } } return leaderIndicesToFollow; }	i am not sure if we should use the cluster name since the local and remote cluster can have the same name (i might be wrong here). should we check if the cluster alias is "_local_"?
private synchronized Collection<FullHttpResponse> sendRequests( final SocketAddress remoteAddress, final Collection<?> requests) throws InterruptedException { final CountDownLatch latch = new CountDownLatch(requests.size()); final Collection<FullHttpResponse> content = Collections.synchronizedList(new ArrayList<>(requests.size())); clientBootstrap.handler(new CountDownLatchHandler(latch, content)); ChannelFuture channelFuture = null; try { channelFuture = clientBootstrap.connect(remoteAddress); channelFuture.sync(); for (Object request : requests) { channelFuture.channel().writeAndFlush(request); } latch.await(10, TimeUnit.SECONDS); } finally { if (channelFuture != null) { channelFuture.channel().close().sync(); } } return content; }	may be final collection<? extends httprequest>?
private synchronized Collection<FullHttpResponse> sendRequests( final SocketAddress remoteAddress, final Collection<?> requests) throws InterruptedException { final CountDownLatch latch = new CountDownLatch(requests.size()); final Collection<FullHttpResponse> content = Collections.synchronizedList(new ArrayList<>(requests.size())); clientBootstrap.handler(new CountDownLatchHandler(latch, content)); ChannelFuture channelFuture = null; try { channelFuture = clientBootstrap.connect(remoteAddress); channelFuture.sync(); for (Object request : requests) { channelFuture.channel().writeAndFlush(request); } latch.await(10, TimeUnit.SECONDS); } finally { if (channelFuture != null) { channelFuture.channel().close().sync(); } } return content; }	so we can may be keep here httprequest?
static Settings additionalSettings(final Settings settings, final boolean enabled, final boolean transportClientMode) { if (enabled && transportClientMode) { return Settings.builder() .put(SecuritySettings.addTransportSettings(settings)) .put(SecuritySettings.addUserSettings(settings)) .build(); } else { return Settings.EMPTY; } }	im going to assume all of these changes are approved by someone else :)
*/ private static boolean getOrientation(Coordinate[] points, int offset, int length) { // calculate the direction of the points: find the southernmost point // and check its neighbors orientation. final int top = top(points, offset, length); final int prev = (top + length - 1) % length; final int next = (top + 1) % length; final int determinantSign = orient( points[offset + prev].x, points[offset + prev].y, points[offset + top].x, points[offset + top].y, points[offset + next].x, points[offset + next].y); if (determinantSign == 0) { // Points are collinear, but `top` is not in the middle if so, so the edges either side of `top` are intersecting. throw new InvalidShapeException("Cannot determine orientation: edges adjacent to (" + points[offset + top].x + "," + points[offset +top].y + ") coincide"); } return determinantSign < 0; }	i think there's an extra 'if so' here.
private static void verifyDefaultInstallation(Installation es) { Stream.of( "elasticsearch-certgen", "elasticsearch-certutil", "elasticsearch-croneval", "elasticsearch-saml-metadata", "elasticsearch-setup-passwords", "elasticsearch-sql-cli", "elasticsearch-syskeygen", "elasticsearch-users", "elasticsearch-service-tokens", "x-pack-env", "x-pack-security-env", "x-pack-watcher-env" ).forEach(executable -> assertPermissionsAndOwnership(es.bin(executable), p555)); // at this time we only install the current version of archive distributions, but if that changes we'll need to pass // the version through here assertPermissionsAndOwnership(es.bin("elasticsearch-sql-cli-" + getCurrentVersion() + ".jar"), p555); Stream.of("role_mapping.yml", "roles.yml", "users", "users_roles") .forEach(configFile -> assertPermissionsAndOwnership(es.config(configFile), p664)); }	if the platform difference doesn't make it too complicated you could maybe add a similar assertion for the ml programs: controller, autodetect, normalize and data_frame_analyzer. they're in $es_home/modules/x-pack-ml/platform/linux-x86_64/bin on x86_64 and $es_home/modules/x-pack-ml/platform/linux-aarch64/bin on aarch64.
public void start(Settings settings, TransportService transportService, ClusterSettings clusterSettings, MetaStateService metaStateService, MetaDataIndexUpgradeService metaDataIndexUpgradeService, MetaDataUpgrader metaDataUpgrader) { assert persistedState.get() == null : "should only start once, but already have " + persistedState.get(); final Tuple<Manifest, ClusterState> manifestClusterStateTuple; try { upgradeMetaData(settings, metaStateService, metaDataIndexUpgradeService, metaDataUpgrader); manifestClusterStateTuple = loadStateAndManifest(ClusterName.CLUSTER_NAME_SETTING.get(settings), metaStateService); } catch (IOException e) { throw new ElasticsearchException("failed to load metadata", e); } final IncrementalClusterStateWriter incrementalClusterStateWriter = new IncrementalClusterStateWriter(settings, clusterSettings, metaStateService, manifestClusterStateTuple.v1(), prepareInitialClusterState(transportService, clusterSettings, manifestClusterStateTuple.v2()), transportService.getThreadPool()::relativeTimeInMillis); if (DiscoveryNode.isMasterNode(settings) == false) { if (DiscoveryNode.isDataNode(settings)) { persistedState.set(new DataOnlyNodePersistedState(settings, incrementalClusterStateWriter, transportService.getThreadPool())); } else { // Non-master non-data nodes do not need to persist the cluster state as they do not use a persistent store persistedState.set(new InMemoryPersistedState(manifestClusterStateTuple.v1().getCurrentTerm(), manifestClusterStateTuple.v2())); } } else { // Master-ineligible nodes must persist the cluster state when accepting it because they must reload the (complete, fresh) // last-accepted cluster state when restarted. persistedState.set(new MasterEligibleNodePersistedState(incrementalClusterStateWriter)); } }	suggest inverting this if -- it fixes the line length issue here and also improves harmony. also my comment should have read master-eligible nodes not master-ineligible nodes. suggestion if (discoverynode.ismasternode(settings)) { // master-eligible nodes must persist the cluster state when accepting it because they must reload the (complete, fresh) // last-accepted cluster state when restarted. persistedstate.set(new mastereligiblenodepersistedstate(incrementalclusterstatewriter)); } else if (discoverynode.isdatanode(settings)) { // master-ineligible data nodes blah blah... persistedstate.set(new dataonlynodepersistedstate(settings, incrementalclusterstatewriter, transportservice.getthreadpool())); } else { // non-master non-data nodes do not need to persist the cluster state as they do not use a persistent store persistedstate.set(new inmemorypersistedstate(manifestclusterstatetuple.v1().getcurrentterm(), manifestclusterstatetuple.v2())); }
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(Job.ID.getPreferredName(), jobId); builder.field(CATEGORY_ID.getPreferredName(), categoryId); if (partitionFieldName != null) { builder.field(PARTITION_FIELD_NAME.getPreferredName(), partitionFieldName); } if (partitionFieldValue != null) { builder.field(PARTITION_FIELD_VALUE.getPreferredName(), partitionFieldValue); } builder.field(TERMS.getPreferredName(), terms); builder.field(REGEX.getPreferredName(), regex); builder.field(MAX_MATCHING_LENGTH.getPreferredName(), maxMatchingLength); builder.field(EXAMPLES.getPreferredName(), examples); if (grokPattern != null) { builder.field(GROK_PATTERN.getPreferredName(), grokPattern); } if (preferredToCategories.length > 0) { builder.field(PREFERRED_TO_CATEGORIES.getPreferredName(), preferredToCategories); } if (numMatches > 0) { builder.field(NUM_MATCHES.getPreferredName(), numMatches); } // Copy the patten from AnomalyRecord that by/over/partition field values are added to results // as key value pairs after all the fixed fields if they won't clash with reserved fields if (partitionFieldName != null && partitionFieldValue != null && ReservedFieldNames.isValidFieldName(partitionFieldName)) { builder.field(partitionFieldName, partitionFieldValue); } builder.field(Result.RESULT_TYPE.getPreferredName(), TYPE.getPreferredName()); builder.field(MLCATEGORY.getPreferredName(), categoryId); builder.endObject(); return builder; }	it would be good to add a comment here (or somewhere) to say that result_type cannot be relied upon to find category definitions. we still need to get them by checking for the existence of a category_id field until results generated in versions that don't have this are several years old.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(Job.ID.getPreferredName(), jobId); builder.field(CATEGORY_ID.getPreferredName(), categoryId); if (partitionFieldName != null) { builder.field(PARTITION_FIELD_NAME.getPreferredName(), partitionFieldName); } if (partitionFieldValue != null) { builder.field(PARTITION_FIELD_VALUE.getPreferredName(), partitionFieldValue); } builder.field(TERMS.getPreferredName(), terms); builder.field(REGEX.getPreferredName(), regex); builder.field(MAX_MATCHING_LENGTH.getPreferredName(), maxMatchingLength); builder.field(EXAMPLES.getPreferredName(), examples); if (grokPattern != null) { builder.field(GROK_PATTERN.getPreferredName(), grokPattern); } if (preferredToCategories.length > 0) { builder.field(PREFERRED_TO_CATEGORIES.getPreferredName(), preferredToCategories); } if (numMatches > 0) { builder.field(NUM_MATCHES.getPreferredName(), numMatches); } // Copy the patten from AnomalyRecord that by/over/partition field values are added to results // as key value pairs after all the fixed fields if they won't clash with reserved fields if (partitionFieldName != null && partitionFieldValue != null && ReservedFieldNames.isValidFieldName(partitionFieldName)) { builder.field(partitionFieldName, partitionFieldValue); } builder.field(Result.RESULT_TYPE.getPreferredName(), TYPE.getPreferredName()); builder.field(MLCATEGORY.getPreferredName(), categoryId); builder.endObject(); return builder; }	as above, actually store it as a string for complete consistency with the results documents. suggestion builder.field(mlcategory.getpreferredname(), long.tostring(categoryid));
private static ConstructingObjectParser<CategoryDefinition, Void> createParser(boolean ignoreUnknownFields) { ConstructingObjectParser<CategoryDefinition, Void> parser = new ConstructingObjectParser<>(TYPE.getPreferredName(), ignoreUnknownFields, a -> new CategoryDefinition((String) a[0])); parser.declareString(ConstructingObjectParser.constructorArg(), Job.ID); parser.declareLong(CategoryDefinition::setCategoryId, CATEGORY_ID); parser.declareString(CategoryDefinition::setPartitionFieldName, PARTITION_FIELD_NAME); parser.declareString(CategoryDefinition::setPartitionFieldValue, PARTITION_FIELD_VALUE); parser.declareString(CategoryDefinition::setTerms, TERMS); parser.declareString(CategoryDefinition::setRegex, REGEX); parser.declareLong(CategoryDefinition::setMaxMatchingLength, MAX_MATCHING_LENGTH); parser.declareStringArray(CategoryDefinition::setExamples, EXAMPLES); parser.declareString(CategoryDefinition::setGrokPattern, GROK_PATTERN); parser.declareLongArray(CategoryDefinition::setPreferredToCategories, PREFERRED_TO_CATEGORIES); parser.declareLong(CategoryDefinition::setNumMatches, NUM_MATCHES); parser.declareString((cd, rt) -> { /*Ignore as it is always category_definition*/ }, Result.RESULT_TYPE); parser.declareLong((cd, mc) -> { /*Ignore as it is always equal to category_id*/ }, MLCATEGORY); return parser; }	i think it's best if it's a string in the json. the mapping on the index will be keyword and the json docs storing the field in results will have the field as a string, so for all round consistency it will be best if this is a string too. suggestion parser.declarestring((cd, mc) -> { /*ignore as it is always equal to category_id*/ }, mlcategory);
* If `false`, stats are not persisted until the next periodic persistence action. */ public void queueStats(InferenceStats stats, boolean flush) { if (stats.hasStats() == false) { if (flush) { threadPool.executor(MachineLearning.UTILITY_THREAD_POOL_NAME).execute(this::updateStats); } return; } statsQueue.compute(InferenceStats.docId(stats.getModelId(), stats.getNodeId()), (k, previousStats) -> previousStats == null ? stats : InferenceStats.accumulator(stats).merge(previousStats).currentStats(stats.getTimeStamp())); if (flush) { threadPool.executor(MachineLearning.UTILITY_THREAD_POOL_NAME).execute(this::updateStats); } }	2 threads could be calling updatestats at the same time, one invoked here and the other from the the scheduler. is that safe? both threads will be iterating and removing entries from the map, maybe use an iterator on the entryset
public StoredContext stashContext() { final ThreadContextStruct context = threadLocal.get(); /** * X-Opaque-ID should be preserved in a threadContext in order to propagate this across threads. * This is needed so the DeprecationLogger in another thread can see the value of X-Opaque-ID provided by a user. * Otherwise when context is stash, it should be empty. */ if (context.requestHeaders.containsKey(Task.X_OPAQUE_ID)) { ThreadContextStruct threadContextStruct = DEFAULT_CONTEXT.putHeaders(Map.of(Task.X_OPAQUE_ID, context.requestHeaders.get(Task.X_OPAQUE_ID))); threadLocal.set(threadContextStruct); } else { threadLocal.set(DEFAULT_CONTEXT); } return () -> { // If the node and thus the threadLocal get closed while this task // is still executing, we don't want this runnable to fail with an // uncaught exception threadLocal.set(context); }; } /** * Removes the current context and resets a default context marked with as * originating from the supplied string. The removed context can be * restored by closing the returned {@link StoredContext}. Callers should * be careful to save the current context before calling this method and * restore it any listeners, likely with * {@link ContextPreservingActionListener}. Use {@link OriginSettingClient} * which can be used to do this automatically. * <p> * Without security the origin is ignored, but security uses it to authorize * actions that are made up of many sub-actions. These actions call * {@link #stashWithOrigin}	your change sets the default context here, while master sets it to null and then checks for null at read time to return the default context instead. these approaches look equivalent to me and i think your change makes things simpler but i wonder whether i'm missing something?
@Before public void cleanup() throws IOException { if (threadContext != null) { threadContext = null; } }	we could remove the if statement?
public void invariant() { synchronized (mutex) { final Optional<DiscoveryNode> peerFinderLeader = peerFinder.getLeader(); assert peerFinder.getCurrentTerm() == getCurrentTerm(); assert followersChecker.getFastResponseState().term == getCurrentTerm() : followersChecker.getFastResponseState(); assert followersChecker.getFastResponseState().mode == getMode() : followersChecker.getFastResponseState(); if (lastCommittedState.isPresent()) { assert applierState != null; assert lastCommittedState.get().term() == applierState.term(); assert lastCommittedState.get().version() == applierState.version(); } assert (applierState.nodes().getMasterNodeId() == null) == applierState.blocks().hasGlobalBlock(NO_MASTER_BLOCK_WRITES.id()); if (mode == Mode.LEADER) { final boolean becomingMaster = getStateForMasterService().term() != getCurrentTerm(); assert coordinationState.get().electionWon(); assert lastKnownLeader.isPresent() && lastKnownLeader.get().equals(getLocalNode()); assert joinAccumulator instanceof JoinHelper.LeaderJoinAccumulator; assert peerFinderLeader.equals(lastKnownLeader) : peerFinderLeader; assert electionScheduler == null : electionScheduler; assert prevotingRound == null : prevotingRound; assert becomingMaster || getStateForMasterService().nodes().getMasterNodeId() != null : getStateForMasterService(); assert leaderCheckScheduler == null : leaderCheckScheduler; assert applierState.nodes().getMasterNodeId() == null || getLocalNode().equals(applierState.nodes().getMasterNode()); if (becomingMaster && activePublicationInProgress() == false) { // cluster state update task to become master is submitted to MasterService, but publication has not started yet assert followersChecker.getKnownFollowers().isEmpty() : followersChecker.getKnownFollowers(); } else { final ClusterState lastPublishedState; if (activePublicationInProgress()) { // active publication in progress: followersChecker is up-to-date with nodes that we're actively publishing to lastPublishedState = currentPublication.get().publishedState(); } else { // no active publication: followersChecker is up-to-date with the nodes of the latest publication lastPublishedState = coordinationState.get().getLastAcceptedState(); } final Set<DiscoveryNode> lastPublishedNodes = new HashSet<>(); lastPublishedState.nodes().forEach(lastPublishedNodes::add); assert lastPublishedNodes.remove(getLocalNode()); // followersChecker excludes local node assert lastPublishedNodes.equals(followersChecker.getKnownFollowers()) : lastPublishedNodes + " != " + followersChecker.getKnownFollowers(); } } else if (mode == Mode.FOLLOWER) { assert coordinationState.get().electionWon() == false : getLocalNode() + " is FOLLOWER so electionWon() should be false"; assert lastKnownLeader.isPresent() && (lastKnownLeader.get().equals(getLocalNode()) == false); assert joinAccumulator instanceof JoinHelper.FollowerJoinAccumulator; assert peerFinderLeader.equals(lastKnownLeader) : peerFinderLeader; assert electionScheduler == null : electionScheduler; assert prevotingRound == null : prevotingRound; assert getStateForMasterService().nodes().getMasterNodeId() == null : getStateForMasterService(); assert leaderChecker.currentNodeIsMaster() == false; assert leaderCheckScheduler != null; assert followersChecker.getKnownFollowers().isEmpty(); assert activePublicationInProgress() == false; } else { assert mode == Mode.CANDIDATE; assert joinAccumulator instanceof JoinHelper.CandidateJoinAccumulator; assert peerFinderLeader.isPresent() == false : peerFinderLeader; assert prevotingRound == null || electionScheduler != null; assert getStateForMasterService().nodes().getMasterNodeId() == null : getStateForMasterService(); assert leaderChecker.currentNodeIsMaster() == false; assert leaderCheckScheduler == null : leaderCheckScheduler; assert followersChecker.getKnownFollowers().isEmpty(); assert applierState.nodes().getMasterNodeId() == null; assert activePublicationInProgress() == false; } } }	this assertion fails with ./gradlew :server:test -dtests.class=org.elasticsearch.cluster.coordination.coordinatortests -dtests.method=testfollowerdisconnectiondetectedquickly -dtests.seed=396e71acba99321e:9198d7e2d2fc9784 -dtests.jvm.argline=-dhppc.bitmixer=deterministic - a publication that times out before being accepted locally results in becomingmaster (the last-accepted term is wrong) and activepublicationinprogress() == false, but by this point the followerschecker is active. this is essentially why i had to do this in #34241: https://github.com/elastic/elasticsearch/pull/34241/commits/34eb71b87b3d1723308dc5540b123f20e9ca9644
public void invariant() { synchronized (mutex) { final Optional<DiscoveryNode> peerFinderLeader = peerFinder.getLeader(); assert peerFinder.getCurrentTerm() == getCurrentTerm(); assert followersChecker.getFastResponseState().term == getCurrentTerm() : followersChecker.getFastResponseState(); assert followersChecker.getFastResponseState().mode == getMode() : followersChecker.getFastResponseState(); if (lastCommittedState.isPresent()) { assert applierState != null; assert lastCommittedState.get().term() == applierState.term(); assert lastCommittedState.get().version() == applierState.version(); } assert (applierState.nodes().getMasterNodeId() == null) == applierState.blocks().hasGlobalBlock(NO_MASTER_BLOCK_WRITES.id()); if (mode == Mode.LEADER) { final boolean becomingMaster = getStateForMasterService().term() != getCurrentTerm(); assert coordinationState.get().electionWon(); assert lastKnownLeader.isPresent() && lastKnownLeader.get().equals(getLocalNode()); assert joinAccumulator instanceof JoinHelper.LeaderJoinAccumulator; assert peerFinderLeader.equals(lastKnownLeader) : peerFinderLeader; assert electionScheduler == null : electionScheduler; assert prevotingRound == null : prevotingRound; assert becomingMaster || getStateForMasterService().nodes().getMasterNodeId() != null : getStateForMasterService(); assert leaderCheckScheduler == null : leaderCheckScheduler; assert applierState.nodes().getMasterNodeId() == null || getLocalNode().equals(applierState.nodes().getMasterNode()); if (becomingMaster && activePublicationInProgress() == false) { // cluster state update task to become master is submitted to MasterService, but publication has not started yet assert followersChecker.getKnownFollowers().isEmpty() : followersChecker.getKnownFollowers(); } else { final ClusterState lastPublishedState; if (activePublicationInProgress()) { // active publication in progress: followersChecker is up-to-date with nodes that we're actively publishing to lastPublishedState = currentPublication.get().publishedState(); } else { // no active publication: followersChecker is up-to-date with the nodes of the latest publication lastPublishedState = coordinationState.get().getLastAcceptedState(); } final Set<DiscoveryNode> lastPublishedNodes = new HashSet<>(); lastPublishedState.nodes().forEach(lastPublishedNodes::add); assert lastPublishedNodes.remove(getLocalNode()); // followersChecker excludes local node assert lastPublishedNodes.equals(followersChecker.getKnownFollowers()) : lastPublishedNodes + " != " + followersChecker.getKnownFollowers(); } } else if (mode == Mode.FOLLOWER) { assert coordinationState.get().electionWon() == false : getLocalNode() + " is FOLLOWER so electionWon() should be false"; assert lastKnownLeader.isPresent() && (lastKnownLeader.get().equals(getLocalNode()) == false); assert joinAccumulator instanceof JoinHelper.FollowerJoinAccumulator; assert peerFinderLeader.equals(lastKnownLeader) : peerFinderLeader; assert electionScheduler == null : electionScheduler; assert prevotingRound == null : prevotingRound; assert getStateForMasterService().nodes().getMasterNodeId() == null : getStateForMasterService(); assert leaderChecker.currentNodeIsMaster() == false; assert leaderCheckScheduler != null; assert followersChecker.getKnownFollowers().isEmpty(); assert activePublicationInProgress() == false; } else { assert mode == Mode.CANDIDATE; assert joinAccumulator instanceof JoinHelper.CandidateJoinAccumulator; assert peerFinderLeader.isPresent() == false : peerFinderLeader; assert prevotingRound == null || electionScheduler != null; assert getStateForMasterService().nodes().getMasterNodeId() == null : getStateForMasterService(); assert leaderChecker.currentNodeIsMaster() == false; assert leaderCheckScheduler == null : leaderCheckScheduler; assert followersChecker.getKnownFollowers().isEmpty(); assert applierState.nodes().getMasterNodeId() == null; assert activePublicationInProgress() == false; } } }	this assertion fails: :server:test -dtests.class=org.elasticsearch.cluster.coordination.coordinatortests -dtests.method=testunresponsiveleaderdetectedeventually -dtests.seed=b39fa830e397231c:e88d9536d29a794 -dtests.jvm.argline=-dhppc.bitmixer=deterministic (i have some local modifications so may not reproduce exactly) the reason is the same as in https://github.com/elastic/elasticsearch/pull/34257/files#r222583570: a publication is committed but times out before being accepted locally. here it's a commit that's adding a node after the cluster formed, so the known followers are updated at the start of the publication but the last-accepted state doesn't reflect this.
@Override protected void onFoundPeersUpdated() { synchronized (mutex) { if (mode == Mode.CANDIDATE) { final CoordinationState.VoteCollection expectedVotes = new CoordinationState.VoteCollection(); getFoundPeers().forEach(expectedVotes::addVote); expectedVotes.addVote(Coordinator.this.getLocalNode()); final ClusterState lastAcceptedState = coordinationState.get().getLastAcceptedState(); final boolean foundQuorum = CoordinationState.isElectionQuorum(expectedVotes, lastAcceptedState); if (foundQuorum) { if (electionScheduler == null) { final TimeValue gracePeriod = TimeValue.ZERO; // TODO variable grace period electionScheduler = electionSchedulerFactory.startElectionScheduler(gracePeriod, new Runnable() { @Override public void run() { synchronized (mutex) { if (mode == Mode.CANDIDATE) { if (prevotingRound != null) { prevotingRound.close(); } prevotingRound = preVoteCollector.start(lastAcceptedState, getDiscoveredNodes()); } } } @Override public String toString() { return "scheduling of new prevoting round"; } }); } } else { closePrevotingAndElectionScheduler(); } } } } } class CoordinatorPublication extends Publication { private final PublishRequest publishRequest; private final ListenableFuture<Void> localNodeAckEvent; private final AckListener ackListener; private final ActionListener<Void> publishListener; private boolean completed; CoordinatorPublication(PublishRequest publishRequest, ListenableFuture<Void> localNodeAckEvent, AckListener ackListener, ActionListener<Void> publishListener) { super(Coordinator.this.settings, publishRequest, new AckListener() { @Override public void onCommit(TimeValue commitTime) { ackListener.onCommit(commitTime); } @Override public void onNodeAck(DiscoveryNode node, Exception e) { // acking and cluster state application for local node is handled specially if (node.equals(getLocalNode())) { synchronized (mutex) { if (e == null) { localNodeAckEvent.onResponse(null); } else { localNodeAckEvent.onFailure(e); } } } else { ackListener.onNodeAck(node, e); } } }, transportService.getThreadPool()::relativeTimeInMillis); this.publishRequest = publishRequest; this.localNodeAckEvent = localNodeAckEvent; this.ackListener = ackListener; this.publishListener = publishListener; } private void removePublicationAndPossiblyBecomeCandidate(String reason) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; assert currentPublication.get() == this; currentPublication = Optional.empty(); // check if node has not already switched modes (by bumping term) if (mode == Mode.LEADER && publishRequest.getAcceptedState().term() == getCurrentTerm()) { becomeCandidate(reason); } } @Override protected void onCompletion(boolean committed) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; completed = true; localNodeAckEvent.addListener(new ActionListener<Void>() { @Override public void onResponse(Void ignore) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; assert committed; clusterApplier.onNewClusterState(CoordinatorPublication.this.toString(), () -> applierState, new ClusterApplyListener() { @Override public void onFailure(String source, Exception e) { synchronized (mutex) { removePublicationAndPossiblyBecomeCandidate("clusterApplier#onNewClusterState"); } ackListener.onNodeAck(getLocalNode(), e); publishListener.onFailure(e); } @Override public void onSuccess(String source) { synchronized (mutex) { assert currentPublication.get() == CoordinatorPublication.this; currentPublication = Optional.empty(); // trigger term bump if new term was found during publication updateMaxTermSeen(getCurrentTerm()); } ackListener.onNodeAck(getLocalNode(), null); publishListener.onResponse(null); } }); } @Override public void onFailure(Exception e) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; removePublicationAndPossiblyBecomeCandidate("Publication.onCompletion(false)"); FailedToCommitClusterStateException exception = new FailedToCommitClusterStateException( "publication failed", e); ackListener.onNodeAck(getLocalNode(), exception); // other nodes have acked, but not the master. publishListener.onFailure(exception); } }, EsExecutors.newDirectExecutorService()); } boolean isActive() { return completed == false; } @Override protected boolean isPublishQuorum(CoordinationState.VoteCollection votes) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; return coordinationState.get().isPublishQuorum(votes); } @Override protected Optional<ApplyCommitRequest> handlePublishResponse(DiscoveryNode sourceNode, PublishResponse publishResponse) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; assert getCurrentTerm() >= publishResponse.getTerm(); return coordinationState.get().handlePublishResponse(sourceNode, publishResponse); } @Override protected void onJoin(Join join) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; if (join.getTerm() == getCurrentTerm()) { handleJoin(join); } // TODO: what to do on missing join? } @Override protected void sendPublishRequest(DiscoveryNode destination, PublishRequest publishRequest, ActionListener<PublishWithJoinResponse> responseActionListener) { publicationHandler.sendPublishRequest(destination, publishRequest, wrapWithMutex(responseActionListener)); } @Override protected void sendApplyCommit(DiscoveryNode destination, ApplyCommitRequest applyCommit, ActionListener<Empty> responseActionListener) { publicationHandler.sendApplyCommit(destination, applyCommit, wrapWithMutex(responseActionListener)); }	perhaps isincomplete to align with the "completed" nomenclature?
boolean blackhole() { boolean unDisconnected = disconnectedNodes.remove(localNode.getId()); boolean blackholed = blackholedNodes.add(localNode.getId()); assert blackholed || unDisconnected == false; return blackholed; }	i think this is a duplicate of connect() above (minus an assertion) although i prefer this name a bit.
private void onLagDetected(DiscoveryNode node, ActionListener<NodesHotThreadsResponse> debugListener) { if (debugListener != null) { if (client == null) { debugListener.onFailure(new NullPointerException("client")); } else { // we're removing the node from the cluster so we need to keep the connection open for the hot threads request transportService.connectToNode(node, new ActionListener<>() { @Override public void onResponse(Releasable releasable) { boolean success = false; final ThreadContext threadContext = transportService.getThreadPool().getThreadContext(); try (ThreadContext.StoredContext ignored = threadContext.stashContext()) { threadContext.markAsSystemContext(); client.execute( NodesHotThreadsAction.INSTANCE, new NodesHotThreadsRequest(node).threads(9999), ActionListener.runBefore(debugListener, () -> Releasables.close(releasable))); success = true; } finally { if (success == false) { Releasables.close(releasable); } } } @Override public void onFailure(Exception e) { debugListener.onFailure(e); } }); } } removeNode(node, "lagging"); }	i think i would find it more intuitive to locate this inside lagdetector, letting it have access to transportservice and client to allow it to handle the logging on its own.
private void onLagDetected(DiscoveryNode node, ActionListener<NodesHotThreadsResponse> debugListener) { if (debugListener != null) { if (client == null) { debugListener.onFailure(new NullPointerException("client")); } else { // we're removing the node from the cluster so we need to keep the connection open for the hot threads request transportService.connectToNode(node, new ActionListener<>() { @Override public void onResponse(Releasable releasable) { boolean success = false; final ThreadContext threadContext = transportService.getThreadPool().getThreadContext(); try (ThreadContext.StoredContext ignored = threadContext.stashContext()) { threadContext.markAsSystemContext(); client.execute( NodesHotThreadsAction.INSTANCE, new NodesHotThreadsRequest(node).threads(9999), ActionListener.runBefore(debugListener, () -> Releasables.close(releasable))); success = true; } finally { if (success == false) { Releasables.close(releasable); } } } @Override public void onFailure(Exception e) { debugListener.onFailure(e); } }); } } removeNode(node, "lagging"); }	i find 9999 a bit high, perhaps just 100?
public static ActionListener<NodesHotThreadsResponse> getDebugListener( DiscoveryNode discoveryNode, long appliedVersion, long expectedVersion ) { if (logger.isDebugEnabled()) { return new ActionListener<>() { @Override public void onResponse(NodesHotThreadsResponse nodesHotThreadsResponse) { if (nodesHotThreadsResponse.getNodes().size() == 0) { assert nodesHotThreadsResponse.failures().size() == 1; onFailure(nodesHotThreadsResponse.failures().get(0)); return; } logger.debug( "hot threads from node [{}] lagging at version [{}] despite commit of cluster state version [{}]:\\n{}", discoveryNode.descriptionWithoutAttributes(), appliedVersion, expectedVersion, nodesHotThreadsResponse.getNodes().get(0).getHotThreads() ); } @Override public void onFailure(Exception e) { logger.debug(new ParameterizedMessage( "failed to get hot threads from node [{}] lagging at version {} despite commit of cluster state version [{}]", discoveryNode.descriptionWithoutAttributes(), appliedVersion, expectedVersion ), e); } }; } else { return null; } }	nit: remove empty line?
public static void main(String[] args) throws Exception { if (args.length != 1 && args.length != 3) { throw new IllegalArgumentException("Expected: MiniHDFS <baseDirectory> [<kerberosPrincipal> <kerberosKeytab>], " + "got: " + Arrays.toString(args)); } boolean secure = args.length == 3; // configure Paths Path baseDir = Paths.get(args[0]); // hadoop-home/, so logs will not complain if (System.getenv("HADOOP_HOME") == null) { Path hadoopHome = baseDir.resolve("hadoop-home"); Files.createDirectories(hadoopHome); System.setProperty("hadoop.home.dir", hadoopHome.toAbsolutePath().toString()); } // hdfs-data/, where any data is going Path hdfsHome = baseDir.resolve("hdfs-data"); // configure cluster Configuration cfg = new Configuration(); cfg.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsHome.toAbsolutePath().toString()); // lower default permission: TODO: needed? cfg.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_KEY, "766"); // optionally configure security if (secure) { String kerberosPrincipal = args[1]; String keytabFile = args[2]; cfg.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, "kerberos"); cfg.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, "true"); cfg.set(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, kerberosPrincipal); cfg.set(DFSConfigKeys.DFS_DATANODE_KERBEROS_PRINCIPAL_KEY, kerberosPrincipal); cfg.set(DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY, kerberosPrincipal); cfg.set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY, keytabFile); cfg.set(DFSConfigKeys.DFS_DATANODE_KEYTAB_FILE_KEY, keytabFile); cfg.set(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, "true"); cfg.set(DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY, "true"); cfg.set(DFSConfigKeys.IGNORE_SECURE_PORTS_FOR_TESTING_KEY, "true"); cfg.set(HdfsClientConfigKeys.DFS_DATA_TRANSFER_PROTECTION_KEY, "authentication"); cfg.set(DFSConfigKeys.DFS_ENCRYPT_DATA_TRANSFER_KEY, "true"); } UserGroupInformation.setConfiguration(cfg); // TODO: remove hardcoded port! MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(cfg); if (secure) { builder.nameNodePort(9998); } else { builder.nameNodePort(9999); } // Configure HA mode String haNameService = System.getProperty("ha-nameservice"); boolean haEnabled = haNameService != null; if (haEnabled) { MiniDFSNNTopology.NNConf nn1 = new MiniDFSNNTopology.NNConf("nn1").setIpcPort(10001); MiniDFSNNTopology.NNConf nn2 = new MiniDFSNNTopology.NNConf("nn2").setIpcPort(10002); MiniDFSNNTopology.NSConf nameservice = new MiniDFSNNTopology.NSConf(haNameService).addNN(nn1).addNN(nn2); MiniDFSNNTopology namenodeTopology = new MiniDFSNNTopology().addNameservice(nameservice); builder.nnTopology(namenodeTopology); } MiniDFSCluster dfs = builder.build(); // Configure contents of the filesystem org.apache.hadoop.fs.Path esUserPath = new org.apache.hadoop.fs.Path("/user/elasticsearch"); FileSystem fs; if (haEnabled) { dfs.transitionToActive(0); fs = HATestUtil.configureFailoverFs(dfs, cfg); } else { fs = dfs.getFileSystem(); } try { // Set the elasticsearch user directory up fs.mkdirs(esUserPath); if (UserGroupInformation.isSecurityEnabled()) { List<AclEntry> acls = new ArrayList<>(); acls.add(new AclEntry.Builder().setType(AclEntryType.USER).setName("elasticsearch").setPermission(FsAction.ALL).build()); fs.modifyAclEntries(esUserPath, acls); } // Install a pre-existing repository into HDFS String directoryName = "readonly-repository"; String archiveName = directoryName + ".tar.gz"; URL readOnlyRepositoryArchiveURL = MiniHDFS.class.getClassLoader().getResource(archiveName); if (readOnlyRepositoryArchiveURL != null) { Path tempDirectory = Files.createTempDirectory(MiniHDFS.class.getName()); File readOnlyRepositoryArchive = tempDirectory.resolve(archiveName).toFile(); FileUtils.copyURLToFile(readOnlyRepositoryArchiveURL, readOnlyRepositoryArchive); FileUtil.unTar(readOnlyRepositoryArchive, tempDirectory.toFile()); fs.copyFromLocalFile(true, true, new org.apache.hadoop.fs.Path(tempDirectory.resolve(directoryName).toAbsolutePath().toUri()), esUserPath.suffix("/existing/" + directoryName) ); FileUtils.deleteDirectory(tempDirectory.toFile()); } } finally { fs.close(); } // write our PID file Path tmp = Files.createTempFile(baseDir, null, null); String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0]; Files.write(tmp, pid.getBytes(StandardCharsets.UTF_8)); Files.move(tmp, baseDir.resolve(PID_FILE_NAME), StandardCopyOption.ATOMIC_MOVE); // write our port file String portFileContent = Integer.toString(dfs.getNameNodePort(0)); if (haEnabled) { portFileContent = portFileContent + "\\n" + Integer.toString(dfs.getNameNodePort(1)); } tmp = Files.createTempFile(baseDir, null, null); Files.write(tmp, portFileContent.getBytes(StandardCharsets.UTF_8)); Files.move(tmp, baseDir.resolve(PORT_FILE_NAME), StandardCopyOption.ATOMIC_MOVE); }	is this still needed if we have dfs.encrypt.data.transfer set to true? according to the docs, one should supersede the other, see https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
private void addFixtureProject(String path, Map<String, String> globalMap) { Project fixtureProject = this.project.findProject(path); if (fixtureProject == null) { throw new IllegalArgumentException("Could not find test fixture " + fixtureProject); } if (fixtureProject.file(TestFixturesPlugin.DOCKER_COMPOSE_YML).exists() == false) { throw new IllegalArgumentException( "Project " + path + " is not a valid test fixture: missing " + TestFixturesPlugin.DOCKER_COMPOSE_YML ); } fixtures.add(fixtureProject); // Check for exclusive access if (globalMap.containsKey(path)) { throw new GradleException("Projects " + globalMap.get(path) + " and " + this.project.getPath() + " both " + "claim all services from " + path + ". This is not supported because it breaks running in parallel. " + "Configure specific services in docker-compose.yml for each and add the service name to `useFixture`" ); } }	the name serviceinuse implies this is some kind of runtime check. probably something like isservicerequired would make more sense.
private void configureServiceInfoForTask( Task task, Project fixtureProject, boolean enableFilter, BiConsumer<String, Integer> consumer ) { // Configure ports for the tests as system properties. // We only know these at execution time so we need to do it in doFirst TestFixtureExtension extension = task.getProject().getExtensions().getByType(TestFixtureExtension.class); task.doFirst(new Action<Task>() { @Override public void execute(Task theTask) { fixtureProject.getExtensions().getByType(ComposeExtension.class).getServicesInfos() .entrySet().stream() .filter(entry -> enableFilter == false|| extension.isServiceInUse(entry.getKey(), fixtureProject.getPath()) ) .forEach(entry -> { String service = entry.getKey(); ServiceInfo infos = entry.getValue(); infos.getTcpPorts() .forEach((container, host) -> { String name = "test.fixtures." + service + ".tcp." + container; theTask.getLogger().info("port mapping property: {}={}", name, host); consumer.accept( name, host ); }); infos.getUdpPorts() .forEach((container, host) -> { String name = "test.fixtures." + service + ".udp." + container; theTask.getLogger().info("port mapping property: {}={}", name, host); consumer.accept( name, host ); }); }); } } ); }	nit: the formatting here is a bit odd, should be a space in false||.
public void testMergeNestedMappings() throws IOException { MapperService mapperService = createIndex("index1", Settings.EMPTY, jsonBuilder().startObject() .startObject("properties") .startObject("nested1") .field("type", "nested") .endObject() .endObject().endObject()).mapperService(); Function<String, String> mapping1 = type -> { try { return Strings.toString(XContentFactory.jsonBuilder().startObject().startObject(type).startObject("properties") .startObject("nested1").field("type", "nested").field("include_in_parent", true) .endObject().endObject().endObject().endObject()); } catch (IOException e) { throw new UncheckedIOException(e); } }; // cannot update `include_in_parent` dynamically MapperException e1 = expectThrows(MapperException.class, () -> mapperService.merge("type", new CompressedXContent(mapping1.apply("type")), MergeReason.MAPPING_UPDATE)); assertEquals("The [include_in_parent] parameter can't be updated for the nested object mapping [nested1].", e1.getMessage()); Function<String, String> mapping2 = type -> { try { return Strings.toString(XContentFactory.jsonBuilder().startObject().startObject(type).startObject("properties") .startObject("nested1").field("type", "nested").field("include_in_root", true) .endObject().endObject().endObject().endObject()); } catch (IOException e) { throw new UncheckedIOException(e); } }; // cannot update `include_in_root` dynamically MapperException e2 = expectThrows(MapperException.class, () -> mapperService.merge("type", new CompressedXContent(mapping2.apply("type")), MergeReason.MAPPING_UPDATE)); assertEquals("The [include_in_root] parameter can't be updated for the nested object mapping [nested1].", e2.getMessage()); }	let's try to simplify the definition of mapping1 and mapping2: * we can remove the 'supplier' function, and just define a string mapping directly. * there's no need to catch and rewrap the ioexception, since this method includes throws ioexception in its signature. i see that you took inspiration from testlimitofnestedfieldsperindex -- apologies that the method wasn't the best example.
@Override protected RecoveryRequest readRequestFrom(StreamInput in) throws IOException { return new RecoveryRequest(in); }	we also need to revert this change after reverting the recoveryrequest class.
public static void indexDocs(Logger logger, String index, long numDocs, long start, long end, TimeValue interval) { final long intervalMillis = interval.millis(); assert intervalMillis > TimeValue.timeValueSeconds(1).millis(); final long secondMillis = TimeValue.timeValueSeconds(1).millis(); end = Intervals.alignToFloor(end, intervalMillis); start = Intervals.alignToCeil(start, intervalMillis); int maxDelta = (int) (end - start - 1); BulkRequestBuilder bulkRequestBuilder = client().prepareBulk(); for (int i = 0; i < numDocs; i++) { IndexRequest indexRequest = new IndexRequest(index); long timestamp = Intervals.alignToFloor(start + randomIntBetween(0, maxDelta), interval.millis()) + randomLongBetween(secondMillis, intervalMillis - secondMillis); assert timestamp >= start && timestamp < end; indexRequest.source("time", timestamp, "@timestamp", timestamp).opType(DocWriteRequest.OpType.CREATE); bulkRequestBuilder.add(indexRequest); } BulkResponse bulkResponse = bulkRequestBuilder .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) .get(); if (bulkResponse.hasFailures()) { int failures = 0; for (BulkItemResponse itemResponse : bulkResponse) { if (itemResponse.isFailed()) { failures++; logger.error("Item response failure [{}]", itemResponse.getFailureMessage()); } } fail("Bulk response contained " + failures + " failures"); } logger.info("Indexed [{}] documents", numDocs); }	i am a bit worried about adding a second version of indexdocs here. i think the problem with the test is that we need to ensure the first bucket is not partial. could we not change the existing indexdocs to always index the first record with start time and then change the test to do the bucket alignment?
String hostName, String hostAddress, TransportAddress address, Map<String, String> attributes, Set<DiscoveryNodeRole> roles, Version version ) { if (nodeName != null) { this.nodeName = nodeStringDeduplicator.deduplicate(nodeName); } else { this.nodeName = ""; } this.nodeId = nodeStringDeduplicator.deduplicate(nodeId); this.ephemeralId = nodeStringDeduplicator.deduplicate(ephemeralId); this.hostName = nodeStringDeduplicator.deduplicate(hostName); assert Strings.hasText(hostAddress); this.hostAddress = nodeStringDeduplicator.deduplicate(hostAddress); this.address = address; if (version == null) { this.version = Version.CURRENT; } else { this.version = version; } this.attributes = Map.copyOf(attributes); assert DiscoveryNodeRole.roleNames().stream().noneMatch(attributes::containsKey) : "Node roles must not be provided as attributes but saw attributes " + attributes; this.roles = Collections.unmodifiableSortedSet(new TreeSet<>(roles)); } /** Creates a DiscoveryNode representing the local node. */ public static DiscoveryNode createLocal(Settings settings, TransportAddress publishAddress, String nodeId) { Map<String, String> attributes = Node.NODE_ATTRIBUTES.getAsMap(settings); Set<DiscoveryNodeRole> roles = getRolesFromSettings(settings); return new DiscoveryNode(Node.NODE_NAME_SETTING.get(settings), nodeId, publishAddress, attributes, roles, Version.CURRENT); }	i am not sure i follow why we want to copy the map here and when deserializing? wrapping seems less intrusive.
@Override public HealthIndicatorResult calculate(boolean includeDetails) { HealthStatus stableMasterStatus; String summary; Map<String, Object> details = new HashMap<>(); Collection<HealthIndicatorImpact> impacts = new ArrayList<>(); MasterHistory localMasterHistory = masterHistoryService.getLocalMasterHistory(); if (hasSeenMasterInLast30Seconds()) { long masterChanges = getNumberOfMasterChanges(localMasterHistory); logger.trace("Have seen a master in the last 30 seconds: {}", localMasterHistory.getMostRecentNonNullMaster()); if (masterChanges > 3 && localMasterHistory.hasSameMasterGoneNullNTimes(4) == false) { logger.trace("Have seen {} master changes in the last 30 minutes", masterChanges); stableMasterStatus = HealthStatus.YELLOW; summary = String.format(Locale.ROOT, "The master has changed %d times in the last 30 minutes", masterChanges); impacts.add( new HealthIndicatorImpact( 3, "The cluster currently has a master node, but having multiple master node changes in a short time is an indicator " + "that the cluster is at risk of of not being able to create, delete, or rebalance indices", List.of(ImpactArea.INGEST) ) ); if (includeDetails) { List<DiscoveryNode> mastersInLast30Minutes = localMasterHistory.getImmutableView(); details.put("current_master", new DiscoveryNodeXContentObject(localMasterHistory.getCurrentMaster())); details.put("recent_masters", mastersInLast30Minutes.stream().map(DiscoveryNodeXContentObject::new).toList()); } } else if (localMasterHistory.hasSameMasterGoneNullNTimes(4)) { DiscoveryNode master = localMasterHistory.getMostRecentNonNullMaster(); logger.trace("One master has gone null 4 or more times recently: " + master); boolean localNodeIsMaster = clusterService.localNode().equals(master); final List<DiscoveryNode> remoteHistory; if (localNodeIsMaster) { remoteHistory = null; // We don't need to fetch the remote master's history if we are that remote master } else { remoteHistory = masterHistoryService.getRemoteMasterHistory(master); } if (localNodeIsMaster || remoteHistory == null || getNumberOfMasterChanges(remoteHistory) > 3) { if (localNodeIsMaster == false && remoteHistory == null) { logger.trace("Unable to get master history from {}}", master); } else { logger.trace("The master node {} thinks it is unstable", master); } stableMasterStatus = HealthStatus.YELLOW; summary = String.format( Locale.ROOT, "The cluster's master has alternated between %s and no master multiple times in the last 30 minutes", master ); impacts.add( new HealthIndicatorImpact( 3, "The cluster is at risk of not being able to create, delete, or rebalance indices", List.of(ImpactArea.INGEST) ) ); if (includeDetails) { details.put("current_master", new DiscoveryNodeXContentObject(localMasterHistory.getCurrentMaster())); } } else { logger.trace("This node thinks the master is unstable, but the master node {} thinks it is stable", master); stableMasterStatus = HealthStatus.GREEN; summary = "The cluster has a stable master node"; } } else { logger.trace("The cluster has a stable master node"); stableMasterStatus = HealthStatus.GREEN; summary = "The cluster has a stable master node"; } } else { stableMasterStatus = HealthStatus.RED; summary = "Placeholder summary"; }	this is only called once outside of tests, we could probably make getnumberofmasterchanges(list<discoverynode> masterhistory) more visible (and static) and then remove this method.
private static Map<String, RoleDescriptor> initializeReservedRoles() { return MapBuilder.<String, RoleDescriptor>newMapBuilder() .put("superuser", new RoleDescriptor("superuser", new String[] { "all" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("all").build()}, new String[] { "*" }, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("transport_client", new RoleDescriptor("transport_client", new String[] { "transport_client" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_user", new RoleDescriptor("kibana_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("manage", "read", "index", "delete") .build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("all").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("monitoring_user", new RoleDescriptor("monitoring_user", new String[] { "cluster:monitor/main" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_agent", new RoleDescriptor("remote_monitoring_agent", new String[] { "manage_index_templates", "manage_ingest_pipelines", "monitor", "cluster:monitor/xpack/watcher/watch/get", "cluster:admin/xpack/watcher/watch/put", "cluster:admin/xpack/watcher/watch/delete", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices("metricbeat-*").privileges("index", "create_index").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_collector", new RoleDescriptor( "remote_monitoring_collector", new String[] { "monitor" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("monitor").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("read").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null )) .put("ingest_admin", new RoleDescriptor("ingest_admin", new String[] { "manage_index_templates", "manage_pipeline" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) // reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role .put("reporting_user", new RoleDescriptor("reporting_user", null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_dashboard_only_user", new RoleDescriptor( "kibana_dashboard_only_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*").privileges("read", "view_index_metadata").build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put(KibanaUser.ROLE_NAME, new RoleDescriptor(KibanaUser.ROLE_NAME, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", "manage_token" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*", ".reporting-*", ".code-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build(), }, null, new ConditionalClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("logstash_system", new RoleDescriptor("logstash_system", new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("beats_admin", new RoleDescriptor("beats_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.BEATS_ROLE, new RoleDescriptor(UsernamesField.BEATS_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.APM_ROLE, new RoleDescriptor(UsernamesField.APM_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_user", new RoleDescriptor("machine_learning_user", new String[] { "monitor_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-notifications*") .privileges("view_index_metadata", "read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*") .privileges("view_index_metadata", "read", "write").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_admin", new RoleDescriptor("machine_learning_admin", new String[] { "manage_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".ml-anomalies*", ".ml-notifications*", ".ml-state*", ".ml-meta*") .privileges("view_index_metadata", "read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*") .privileges("view_index_metadata", "read", "write").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_admin", new RoleDescriptor("watcher_admin", new String[] { "manage_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX, TriggeredWatchStoreField.INDEX_NAME, HistoryStoreField.INDEX_PREFIX + "*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_user", new RoleDescriptor("watcher_user", new String[] { "monitor_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX) .privileges("read") .build(), RoleDescriptor.IndicesPrivileges.builder().indices(HistoryStoreField.INDEX_PREFIX + "*") .privileges("read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("logstash_admin", new RoleDescriptor("logstash_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".logstash*") .privileges("create", "delete", "index", "manage", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_user", new RoleDescriptor("rollup_user", new String[] { "monitor_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_admin", new RoleDescriptor("rollup_admin", new String[] { "manage_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.CODE_ROLE, new RoleDescriptor(UsernamesField.CODE_ROLE, new String[] {}, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".code-*").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .immutableMap(); }	nit: add the settings for .code-* indices separately here to make it more readable? e.g. adding .indices(".code-*").privileges("create_index", "read", "write").build(),
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); if (!settings.isEmpty()) { builder.startObject(SETTINGS.getPreferredName()); settings.toXContent(builder, params); builder.endObject(); } if (!mappings.isEmpty()) { builder.startObject(MAPPINGS.getPreferredName()); for (Map.Entry<String, String> entry : mappings.entrySet()) { builder.rawField(entry.getKey(), new BytesArray(entry.getValue()), XContentType.JSON); } builder.endObject(); } if (!aliases.isEmpty()) { builder.startObject(ALIASES.getPreferredName()); for (Alias alias : aliases) { alias.toXContent(builder, params); } builder.endObject(); } for (Map.Entry<String, IndexMetaData.Custom> entry : customs.entrySet()) { builder.field(entry.getKey(), entry.getValue(), params); } builder.endObject(); return builder; }	seems infelicitous that we store the mappings as json-serialized strings, and hence have to deserialize & reserialize them when making an http request. it would presumably be nicer if we stored mappings as typed objects, akin to aliases and settings? would that be a good idea/outside the scope of this pr?
public void executeAsInternalUser(User internalUser, Version version, Consumer<StoredContext> consumer) { assert User.isInternal(internalUser); final StoredContext original = threadContext.newStoredContext(true); try (ThreadContext.StoredContext ignore = threadContext.stashContext()) { setInternalUser(internalUser, version); consumer.accept(original); } }	i'd prefer this constructor to be an instance method of authentication. it then can have a better name for the intention, e.g. something like authentication.mayberewriteforversion(version).
public Subject getEffectiveSubject() { return effectiveSubject; }	this change is now irrelevant and i think we can revert.
public void usageStats(ActionListener<Map<String, Object>> listener) { Map<String, Object> stats = new HashMap<>(); stats.put("name", name()); stats.put("order", order()); listener.onResponse(stats); }	nit: existing getter methods in this class following the builder type naming convention, e.g. name() instead of getname(), while this method uses the javabean naming convention (getxxx). i am ok with either of them, but would prefer consistency within a single class.
public RealmRef getRealmRef() { RealmRef realmRef = this.realmRef.get(); if (realmRef == null) { throw new IllegalStateException("Realm not initialized"); } assert domainName() == null || (realmRef.getDomain() != null && domainName().equals(realmRef.getDomain().name())); return realmRef; }	seeing how our own realms, e.g. ldaprealm, did not call super.initialise(...). i worry that most custom realms out there do not call the super method either. so i think we should make these error message more elaborative to help guide user/developer to the proper fix. alternatively, we can add a new init method, say initrealmref, and let realms to call it right after current initialize method is called. it feels a bit messy though.
public void testSystemIndexAccessBlockedByDefault() throws Exception { // create index { Request putDocRequest = new Request("POST", "/_sys_index_test/add_doc/42"); Response resp = getRestClient().performRequest(putDocRequest); assertThat(resp.getStatusLine().getStatusCode(), equalTo(201)); } // make sure the system index now exists assertBusy(() -> { Request searchRequest = new Request("GET", "/" + SystemIndexTestPlugin.SYSTEM_INDEX_NAME + "/_count"); searchRequest.setOptions(expectWarnings("this request accesses system indices: [" + SystemIndexTestPlugin.SYSTEM_INDEX_NAME + "], but in a future major version, direct access to system indices will be prevented by default")); // Disallow no indices to cause an exception if the flag above doesn't work searchRequest.addParameter("allow_no_indices", "false"); searchRequest.setJsonEntity("{\\"query\\": {\\"match\\": {\\"some_field\\": \\"some_value\\"}}}"); final Response searchResponse = getRestClient().performRequest(searchRequest); assertThat(searchResponse.getStatusLine().getStatusCode(), is(200)); Map<String, Object> responseMap = entityAsMap(searchResponse); assertThat(responseMap, hasKey("count")); assertThat(responseMap.get("count"), equalTo(1)); }); // And with a partial wildcard assertDeprecationWarningOnAccess(".test-*", SystemIndexTestPlugin.SYSTEM_INDEX_NAME); // And with a total wildcard assertDeprecationWarningOnAccess(randomFrom("*", "_all"), SystemIndexTestPlugin.SYSTEM_INDEX_NAME); // If we're not expanding wildcards, we don't get anything { Request searchRequest = new Request("GET", "/" + randomFrom("*", "_all") + randomFrom("/_count", "/_search")); searchRequest.setJsonEntity("{\\"query\\": {\\"match\\": {\\"some_field\\": \\"some_value\\"}}}"); searchRequest.addParameter("allow_no_indices", "false"); try { getRestClient().performRequest(searchRequest); } catch (ResponseException e) { assertThat(e.getResponse().getStatusLine().getStatusCode(), equalTo(404)); assertThat(e.getMessage(), containsString("no such index")); } } // Try to index a doc directly { String expectedWarning = "this request accesses system indices: [" + SystemIndexTestPlugin.SYSTEM_INDEX_NAME + "], but in a " + "future major version, direct access to system indices will be prevented by default"; Request putDocDirectlyRequest = new Request("PUT", "/" + SystemIndexTestPlugin.SYSTEM_INDEX_NAME + "/_doc/43"); putDocDirectlyRequest.setJsonEntity("{\\"some_field\\": \\"some_other_value\\"}"); putDocDirectlyRequest.setOptions(expectWarnings(expectedWarning)); Response response = getRestClient().performRequest(putDocDirectlyRequest); assertThat(response.getStatusLine().getStatusCode(), equalTo(201)); } }	try/catch? that's so 90s. use expectthrows instead!
* @param request the update request * @return a list of system indexes that this request would make visible */ private List<String> checkForHidingSystemIndex(Index[] concreteIndices, UpdateSettingsRequest request) { // Requests that a cluster generates itself are permitted to have a difference in settings // so that rolling upgrade scenarios still work. We check this via the request's origin. if (request.settings().getAsBoolean(IndexMetadata.SETTING_INDEX_HIDDEN, false) == false) { return List.of(); } final List<String> systemPatterns = new ArrayList<>(); for (Index index : concreteIndices) { final SystemIndexDescriptor descriptor = systemIndices.findMatchingDescriptor(index.getName()); if (descriptor != null) { systemPatterns.add(index.getName()); } } return systemPatterns; }	shouldn't this be changed?
public IndexMetadata build() { /* * We expect that the metadata has been properly built to set the number of shards and the number of replicas, and do not rely * on the default values here. Those must have been set upstream. */ if (INDEX_NUMBER_OF_SHARDS_SETTING.exists(settings) == false) { throw new IllegalArgumentException("must specify number of shards for index [" + index + "]"); } final int numberOfShards = INDEX_NUMBER_OF_SHARDS_SETTING.get(settings); if (INDEX_NUMBER_OF_REPLICAS_SETTING.exists(settings) == false) { throw new IllegalArgumentException("must specify number of replicas for index [" + index + "]"); } final int numberOfReplicas = INDEX_NUMBER_OF_REPLICAS_SETTING.get(settings); int routingPartitionSize = INDEX_ROUTING_PARTITION_SIZE_SETTING.get(settings); if (routingPartitionSize != 1 && routingPartitionSize >= getRoutingNumShards()) { throw new IllegalArgumentException("routing partition size [" + routingPartitionSize + "] should be a positive number" + " less than the number of shards [" + getRoutingNumShards() + "] for [" + index + "]"); } // fill missing slots in inSyncAllocationIds with empty set if needed and make all entries immutable ImmutableOpenIntMap.Builder<Set<String>> filledInSyncAllocationIds = ImmutableOpenIntMap.builder(); for (int i = 0; i < numberOfShards; i++) { if (inSyncAllocationIds.containsKey(i)) { filledInSyncAllocationIds.put(i, Set.copyOf(inSyncAllocationIds.get(i))); } else { filledInSyncAllocationIds.put(i, Collections.emptySet()); } } final Map<String, String> requireMap = INDEX_ROUTING_REQUIRE_GROUP_SETTING.getAsMap(settings); final DiscoveryNodeFilters requireFilters; if (requireMap.isEmpty()) { requireFilters = null; } else { requireFilters = DiscoveryNodeFilters.buildFromKeyValue(AND, requireMap); } Map<String, String> includeMap = INDEX_ROUTING_INCLUDE_GROUP_SETTING.getAsMap(settings); final DiscoveryNodeFilters includeFilters; if (includeMap.isEmpty()) { includeFilters = null; } else { includeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, includeMap); } Map<String, String> excludeMap = INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getAsMap(settings); final DiscoveryNodeFilters excludeFilters; if (excludeMap.isEmpty()) { excludeFilters = null; } else { excludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, excludeMap); } Map<String, String> initialRecoveryMap = INDEX_ROUTING_INITIAL_RECOVERY_GROUP_SETTING.getAsMap(settings); final DiscoveryNodeFilters initialRecoveryFilters; if (initialRecoveryMap.isEmpty()) { initialRecoveryFilters = null; } else { initialRecoveryFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, initialRecoveryMap); } Version indexCreatedVersion = indexCreatedVersion(settings); if (primaryTerms == null) { initializePrimaryTerms(); } else if (primaryTerms.length != numberOfShards) { throw new IllegalStateException("primaryTerms length is [" + primaryTerms.length + "] but should be equal to number of shards [" + numberOfShards() + "]"); } final ActiveShardCount waitForActiveShards = SETTING_WAIT_FOR_ACTIVE_SHARDS.get(settings); if (waitForActiveShards.validate(numberOfReplicas) == false) { throw new IllegalArgumentException("invalid " + SETTING_WAIT_FOR_ACTIVE_SHARDS.getKey() + "[" + waitForActiveShards + "]: cannot be greater than " + "number of shard copies [" + (numberOfReplicas + 1) + "]"); } final List<String> routingPaths = INDEX_ROUTING_PATH.get(settings); final String uuid = settings.get(SETTING_INDEX_UUID, INDEX_UUID_NA_VALUE); List<String> tierPreference; try { tierPreference = DataTier.parseTierList(DataTier.TIER_PREFERENCE_SETTING.get(settings)); } catch (Exception e) { assert e instanceof IllegalArgumentException : e; // BwC hack: the setting failed validation but it will be fixed in // #IndexMetadataVerifier#convertSharedCacheTierPreference(IndexMetadata)} later so we just store a null // to be able to build a temporary instance tierPreference = null; } if (isSystem) { settings = Settings.builder().put(settings).put(INDEX_HIDDEN_SETTING.getKey(), true).build(); }	this seems dangerous -- could index metadata be different across nodes? i should move this operation elsewhere.
private ClusterState applyCreateIndexRequestForSystemDataStream(final ClusterState currentState, final CreateIndexClusterStateUpdateRequest request, final boolean silent, final BiConsumer<Metadata.Builder, IndexMetadata> metadataTransformer) throws Exception { Objects.requireNonNull(request.systemDataStreamDescriptor()); logger.debug("applying create index request for system data stream [{}]", request.systemDataStreamDescriptor()); ComposableIndexTemplate template = request.systemDataStreamDescriptor().getComposableIndexTemplate(); if (request.dataStreamName() == null && template.getDataStreamTemplate() != null) { throw new IllegalArgumentException("cannot create index with name [" + request.index() + "], because it matches with a system data stream"); } final Map<String, ComponentTemplate> componentTemplates = request.systemDataStreamDescriptor().getComponentTemplates(); final List<Map<String, Object>> mappings = collectSystemV2Mappings(template, componentTemplates, xContentRegistry, request.index()); final Settings aggregatedIndexSettings = Settings.builder().put(aggregateIndexSettings( currentState, request, resolveSettings(template, componentTemplates), null, settings, indexScopedSettings, shardLimitValidator, indexSettingProviders, this.enforceDefaultTierPreference )) .put(SETTING_INDEX_HIDDEN, true) .build(); final int routingNumShards = getIndexNumberOfRoutingShards(aggregatedIndexSettings, null); final IndexMetadata tmpImd = buildAndValidateTemporaryIndexMetadata(aggregatedIndexSettings, request, routingNumShards); return applyCreateIndexWithTemporaryService(currentState, request, silent, null, tmpImd, mappings, indexService -> resolveAndValidateAliases(request.index(), request.aliases(), MetadataIndexTemplateService.resolveAliases(template, componentTemplates), currentState.metadata(), // the context is only used for validation so it's fine to pass fake values for the // shard id and the current timestamp aliasValidator, xContentRegistry, indexService.newSearchExecutionContext(0, 0, null, () -> 0L, null, emptyMap()), indexService.dateMathExpressionResolverAt(request.getNameResolvedAt())), List.of(), metadataTransformer); }	this seems like something that should be done at a higher level of the call stack -- by the time we get here, something should have already made sure index.hidden is set.
private static IndexMetadata.Builder getIndexMetadata( SystemIndexDescriptor descriptor, String mappings, int format, IndexMetadata.State state ) { IndexMetadata.Builder indexMetadata = IndexMetadata.builder( descriptor.getPrimaryIndex() == null ? descriptor.getIndexPattern() : descriptor.getPrimaryIndex() ); final Settings.Builder settingsBuilder = Settings.builder(); settingsBuilder.put(getSettings()); if (descriptor.getSettings() != null) { settingsBuilder.put(descriptor.getSettings()); } settingsBuilder.put(IndexMetadata.INDEX_FORMAT_SETTING.getKey(), format); indexMetadata.settings(settingsBuilder.build()); if (descriptor.getAliasName() != null) { indexMetadata.putAlias(AliasMetadata.builder(descriptor.getAliasName()).build()); } indexMetadata.state(state); if (mappings != null) { indexMetadata.putMapping(mappings); } return indexMetadata; }	it looks like you messed up the logic here, william.
public void close() { if (isClosed.compareAndSet(false, true)) { Page page; RuntimeException closingException = null; while ((page = pages.pollFirst()) != null) { try { page.close(); } catch (RuntimeException e) { if (closingException == null) { closingException = e; } else { closingException.addSuppressed(e); } } } if (closingException != null) { throw closingException; } } }	honestly i think we should just copy these helper classes 1 to 1 and then work on a common-lib jar. it's time for it anyway @rjernst @jasontedor wdyt?
private static boolean assertNotCalledFromClusterStateApplier(String reason) { if (Thread.currentThread().getName().contains(UPDATE_THREAD_NAME)) { for (StackTraceElement element: Thread.currentThread().getStackTrace()) { if (element.getClassName().equals(ClusterService.class.getName()) && element.getMethodName().equals("callClusterStateAppliers")) { throw new AssertionError("should not be called by a cluster state applier. reason [" + reason + "]"); } } } return true; }	i wonder if assertnotcalledfromclusterstateapplier should also add checks for the call to clustersettings.applysettings that is done before the state is updated. zendiscovery for example adds a settings update consumer that accesses clusterservice.state().
@Override public void removeIndex(final Index index, final IndexRemovalReason reason, final String extraInfo) { final String indexName = index.getName(); try { final IndexService indexService; final IndexEventListener listener; synchronized (this) { if (hasIndex(index) == false) { return; } logger.debug("[{}] closing ... (reason [{}])", indexName, reason); Map<String, IndexService> newIndices = new HashMap<>(indices); indexService = newIndices.remove(index.getUUID()); assert indexService != null : "IndexService is null for index: " + index; indices = unmodifiableMap(newIndices); listener = indexService.getIndexEventListener(); } listener.beforeIndexRemoved(indexService, reason); logger.debug("{} closing index service (reason [{}][{}])", index, reason, extraInfo); indexService.close(extraInfo, reason == IndexRemovalReason.DELETED); logger.debug("{} closed... (reason [{}])", index, reason); final IndexSettings indexSettings = indexService.getIndexSettings(); listener.afterIndexRemoved(indexService.index(), indexSettings, reason); if (reason == IndexRemovalReason.DELETED) { // now we are done - try to wipe data on disk if possible deleteIndexStore(extraInfo, indexService.index(), indexSettings); } } catch (Exception e) { logger.warn((Supplier<?>) () -> new ParameterizedMessage("failed to remove index {} ([{}][{}])", index, reason, extraInfo), e); } }	also add extrainfo to log message here
public Query rangeQuery(String field, boolean hasDocValues, Object from, Object to, boolean includeFrom, boolean includeTo, ShapeRelation relation, @Nullable ZoneId timeZone, @Nullable DateMathParser dateMathParser, QueryShardContext context) { Object lower = from == null ? minValue() : parseValue(from, false, dateMathParser); Object upper = to == null ? maxValue() :parseValue(to, false, dateMathParser); return createRangeQuery(field, hasDocValues, lower, upper, includeFrom, includeTo, relation); }	could add a space after :? i have trouble reading it as a ternary without for some reason!
public void testConcurrentSnapshotAndRepoDelete() throws Exception { internalCluster().startMasterOnlyNodes(1); internalCluster().startDataOnlyNode(); final String repoName = "test-repo"; createRepository(repoName, "fs"); // create a few snapshots so deletes will run for a while final int snapshotCount = randomIntBetween(10, 25); final List<String> snapshotNames = createNSnapshots(repoName, snapshotCount); // concurrently trigger repository and snapshot deletes final List<ActionFuture<AcknowledgedResponse>> deleteFutures = new ArrayList<>(snapshotCount); final ActionFuture<AcknowledgedResponse> deleteRepoFuture = clusterAdmin().prepareDeleteRepository(repoName).execute(); for (String snapshotName : snapshotNames) { deleteFutures.add(clusterAdmin().prepareDeleteSnapshot(repoName, snapshotName).execute()); } try { assertAcked(deleteRepoFuture.actionGet()); } catch (Exception e) { assertThat( e.getMessage(), containsString( "trying to modify or unregister repository [test-repo] that is currently " + "used (snapshot deletion is in progress)" ) ); } for (ActionFuture<AcknowledgedResponse> deleteFuture : deleteFutures) { try { assertAcked(deleteFuture.actionGet()); } catch (RepositoryException e) { assertThat( e.getMessage(), either(containsString("[test-repo] repository is not in started state")).or(containsString("[test-repo] missing")) ); } } }	nit: evaluate this string constant?
static List<String> substitutePlaceholders(final List<String> jvmOptions, Map<String, String> substitutions) { final Map<String, String> placeholderSubstitutions = substitutions.entrySet().stream().collect(Collectors.toMap(e -> "${" + e.getKey() + "}", Map.Entry::getValue)); final List<String> substitutedJvmOptions = new ArrayList<>(jvmOptions.size()); for (final String jvmOption : jvmOptions) { if (jvmOption.matches(".*\\\\$\\\\{[^}]+\\\\}.*")) { String actualJvmOption = jvmOption; for (final Map.Entry<String, String> placeholderSubstitution : placeholderSubstitutions.entrySet()) { actualJvmOption = actualJvmOption.replace(placeholderSubstitution.getKey(), placeholderSubstitution.getValue()); } substitutedJvmOptions.add(actualJvmOption); } else { substitutedJvmOptions.add(jvmOption); } }	this method can be simplified with use of mathce.appendreplacement method: java private static final pattern substitution_pattern = pattern.compile("\\\\$\\\\{([^}]+)}"); static list<string> substituteplaceholders(final list<string> jvmoptions, map<string, string> substitutions) { return jvmoptions.stream().map(jvmoption -> { stringbuilder sb = new stringbuilder(); matcher matcher = substitution_pattern.matcher(jvmoption); while (matcher.find()) { matcher.appendreplacement(sb, substitutions.get(matcher.group(1))); } matcher.appendtail(sb); return sb.tostring(); }).collect(collectors.tolist()); } or made faster (~2x) when you replace regex with indexof: java static list<string> substituteplaceholders(final list<string> jvmoptions, map<string, string> substitutions) { final map<string, string> placeholdersubstitutions = substitutions.entryset().stream().collect(collectors.tomap(e -> "${" + e.getkey() + "}", map.entry::getvalue)); return jvmoptions.stream().map(jvmoption -> { string actualjvmoption = jvmoption; int start = jvmoption.indexof("${"); if (start >= 0 && jvmoption.indexof('}', start) > 0) { for (final map.entry<string, string> placeholdersubstitution : placeholdersubstitutions.entryset()) { actualjvmoption = actualjvmoption.replace(placeholdersubstitution.getkey(), placeholdersubstitution.getvalue()); } } return actualjvmoption; }).collect(collectors.tolist()); }
void wrapExportBulk(final ActionListener<ExportBulk> listener) { final ClusterState state = clusterService.state(); // wait until we have a usable cluster state if (state.blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK) || ClusterState.UNKNOWN_UUID.equals(state.metaData().clusterUUID()) || state.version() == ClusterState.UNKNOWN_VERSION) { logger.trace("skipping exporters because the cluster state is not loaded"); listener.onResponse(null); } final Map<String, Exporter> exporterMap = exporters.get(); final AtomicArray<ExportBulk> accumulatedBulks = new AtomicArray<>(exporterMap.size()); final CountDown countDown = new CountDown(exporterMap.size()); int i = 0; // get every exporter's ExportBulk and, when they've all responded, respond with a wrapped version for (final Exporter exporter : exporterMap.values()) { exporter.openBulk( new AccumulatingExportBulkActionListener(exporter.name(), i++, accumulatedBulks, countDown, threadContext, listener)); } }	this is missing a return statement
* @param resourceBasePath The base path/endpoint to check for the resource (e.g., "/_template"). * @param resourceName The name of the resource (e.g., "template123"). * @param resourceType The type of resource (e.g., "monitoring template"). * @param resourceOwnerName The user-recognizeable resource owner. * @param resourceOwnerType The type of resource owner being dealt with (e.g., "monitoring cluster"). * @param xContent The XContent used to parse the response. * @param minimumVersion The minimum version allowed without being replaced (expected to be the last updated version). */ protected void versionCheckForResource(final RestClient client, final ActionListener<Boolean> listener, final Logger logger, final String resourceBasePath, final String resourceName, final String resourceType, final String resourceOwnerName, final String resourceOwnerType, final XContent xContent, final int minimumVersion) { final CheckedFunction<Response, Boolean, IOException> responseChecker = (response) -> shouldReplaceResource(response, xContent, resourceName, minimumVersion); checkForResource(client, listener, logger, resourceBasePath, resourceName, resourceType, resourceOwnerName, resourceOwnerType, GET_EXISTS, GET_DOES_NOT_EXIST, responseChecker, this::alwaysReplaceResource); } /** * Determine if the current {@code resourceName} exists at the {@code resourceBasePath} endpoint. * <p> * This provides the base-level check for any resource that cares about existence and also its contents. * * @param client The REST client to make the request(s). * @param listener Returns {@code true} if the resource was successfully published. {@code false} otherwise. * @param logger The logger to use for status messages. * @param resourceBasePath The base path/endpoint to check for the resource (e.g., "/_template"), if any. * @param resourceName The name of the resource (e.g., "template123"). * @param resourceType The type of resource (e.g., "monitoring template"). * @param resourceOwnerName The user-recognizeable resource owner. * @param resourceOwnerType The type of resource owner being dealt with (e.g., "monitoring cluster"). * @param exists Response codes that represent {@code EXISTS}. * @param doesNotExist Response codes that represent {@code DOES_NOT_EXIST}. * @param responseChecker Returns {@code true} if the resource should be replaced. * @param doesNotExistResponseChecker Returns {@code true}	maybe add each parameter on a new line?
* @param resourceBasePath The base path/endpoint to check for the resource (e.g., "/_template"). * @param resourceName The name of the resource (e.g., "template123"). * @param resourceType The type of resource (e.g., "monitoring template"). * @param resourceOwnerName The user-recognizeable resource owner. * @param resourceOwnerType The type of resource owner being dealt with (e.g., "monitoring cluster"). * @param xContent The XContent used to parse the response. * @param minimumVersion The minimum version allowed without being replaced (expected to be the last updated version). */ protected void versionCheckForResource(final RestClient client, final ActionListener<Boolean> listener, final Logger logger, final String resourceBasePath, final String resourceName, final String resourceType, final String resourceOwnerName, final String resourceOwnerType, final XContent xContent, final int minimumVersion) { final CheckedFunction<Response, Boolean, IOException> responseChecker = (response) -> shouldReplaceResource(response, xContent, resourceName, minimumVersion); checkForResource(client, listener, logger, resourceBasePath, resourceName, resourceType, resourceOwnerName, resourceOwnerType, GET_EXISTS, GET_DOES_NOT_EXIST, responseChecker, this::alwaysReplaceResource); } /** * Determine if the current {@code resourceName} exists at the {@code resourceBasePath} endpoint. * <p> * This provides the base-level check for any resource that cares about existence and also its contents. * * @param client The REST client to make the request(s). * @param listener Returns {@code true} if the resource was successfully published. {@code false} otherwise. * @param logger The logger to use for status messages. * @param resourceBasePath The base path/endpoint to check for the resource (e.g., "/_template"), if any. * @param resourceName The name of the resource (e.g., "template123"). * @param resourceType The type of resource (e.g., "monitoring template"). * @param resourceOwnerName The user-recognizeable resource owner. * @param resourceOwnerType The type of resource owner being dealt with (e.g., "monitoring cluster"). * @param exists Response codes that represent {@code EXISTS}. * @param doesNotExist Response codes that represent {@code DOES_NOT_EXIST}. * @param responseChecker Returns {@code true} if the resource should be replaced. * @param doesNotExistResponseChecker Returns {@code true}	maybe add each parameter on a new line?
* @param resourceOwnerName The user-recognizeable resource owner. * @param resourceOwnerType The type of resource owner being dealt with (e.g., "monitoring cluster"). */ protected void putResource(final RestClient client, final ActionListener<Boolean> listener, final Logger logger, final String resourceBasePath, final String resourceName, final java.util.function.Supplier<HttpEntity> body, final String resourceType, final String resourceOwnerName, final String resourceOwnerType) { logger.trace("uploading {} [{}] to the [{}] {}", resourceType, resourceName, resourceOwnerName, resourceOwnerType); final Request request = new Request("PUT", resourceBasePath + "/" + resourceName); addParameters(request); request.setEntity(body.get()); client.performRequestAsync(request, new ResponseListener() { @Override public void onSuccess(final Response response) { final int statusCode = response.getStatusLine().getStatusCode(); // 200 or 201 if (statusCode == RestStatus.OK.getStatus() || statusCode == RestStatus.CREATED.getStatus()) { logger.debug("{} [{}] uploaded to the [{}] {}", resourceType, resourceName, resourceOwnerName, resourceOwnerType); listener.onResponse(true); } else { onFailure(new RuntimeException("[" + resourceBasePath + "/" + resourceName + "] responded with [" + statusCode + "]")); } } @Override public void onFailure(final Exception exception) { logger.error((Supplier<?>) () -> new ParameterizedMessage("failed to upload {} [{}] on the [{}] {}", resourceType, resourceName, resourceOwnerName, resourceOwnerType), exception); listener.onFailure(exception); } }); } /** * Delete the {@code resourceName} using the {@code resourceBasePath} endpoint. * <p> * Note to callers: this will add an "ignore" parameter to the request so that 404 is not an exception and therefore considered * successful if it's not found. You can override this behavior by specifying any valid value for "ignore", at which point 404 * responses will result in {@code false} and logged failure. * * @param client The REST client to make the request(s). * @param listener Returns {@code true} if it successfully deleted the item; otherwise {@code false}	maybe add each parameter on a new line?
* @param resourceBasePath The base path/endpoint to check for the resource (e.g., "/_template"). * @param resourceName The name of the resource (e.g., "template123"). * @param resourceType The type of resource (e.g., "monitoring template"). * @param resourceOwnerName The user-recognizeable resource owner. * @param resourceOwnerType The type of resource owner being dealt with (e.g., "monitoring cluster"). * @param xContent The XContent used to parse the response. * @param minimumVersion The minimum version allowed without being replaced (expected to be the last updated version). */ protected void versionCheckForResource(final RestClient client, final ActionListener<Boolean> listener, final Logger logger, final String resourceBasePath, final String resourceName, final String resourceType, final String resourceOwnerName, final String resourceOwnerType, final XContent xContent, final int minimumVersion) { final CheckedFunction<Response, Boolean, IOException> responseChecker = (response) -> shouldReplaceResource(response, xContent, resourceName, minimumVersion); checkForResource(client, listener, logger, resourceBasePath, resourceName, resourceType, resourceOwnerName, resourceOwnerType, GET_EXISTS, GET_DOES_NOT_EXIST, responseChecker, this::alwaysReplaceResource); } /** * Determine if the current {@code resourceName} exists at the {@code resourceBasePath} endpoint. * <p> * This provides the base-level check for any resource that cares about existence and also its contents. * * @param client The REST client to make the request(s). * @param listener Returns {@code true} if the resource was successfully published. {@code false} otherwise. * @param logger The logger to use for status messages. * @param resourceBasePath The base path/endpoint to check for the resource (e.g., "/_template"), if any. * @param resourceName The name of the resource (e.g., "template123"). * @param resourceType The type of resource (e.g., "monitoring template"). * @param resourceOwnerName The user-recognizeable resource owner. * @param resourceOwnerType The type of resource owner being dealt with (e.g., "monitoring cluster"). * @param exists Response codes that represent {@code EXISTS}. * @param doesNotExist Response codes that represent {@code DOES_NOT_EXIST}. * @param responseChecker Returns {@code true} if the resource should be replaced. * @param doesNotExistResponseChecker Returns {@code true}	maybe add each parameter on a new line?
protected void doCheckAndPublish(final RestClient client, final ActionListener<Boolean> listener) { logger.trace("checking [{}] to ensure that it supports the minimum version [{}]", resourceOwnerName, minimumVersion); final Request request = new Request("GET", "/"); request.addParameter("filter_path", "version.number"); client.performRequestAsync(request, new ResponseListener() { @Override public void onSuccess(final Response response) { try { // malformed responses can cause exceptions during validation listener.onResponse(validateVersion(response)); } catch (IOException | RuntimeException e) { onFailure(e); } } @Override public void onFailure(final Exception exception) { logger.error((Supplier<?>) () -> new ParameterizedMessage("failed to verify minimum version [{}] on the [{}] monitoring cluster", minimumVersion, resourceOwnerName), exception); listener.onFailure(exception); } }); } /** * Ensure that the {@code response} contains a {@link Version} that is {@linkplain Version#onOrAfter(Version) on or after} the * {@link #minimumVersion}. * * @param response The response to parse. * @return {@code true}	maybe just catch exception here.
@Override public void openBulk(final ActionListener<ExportBulk> listener) { if (state.get() != State.RUNNING) { listener.onResponse(null); } else { listener.onResponse(resolveBulk(clusterService.state(), false)); } }	the resolvebulk(...) method also needs to be made async, because the setupifelectedmaster(...) method makes remote calls in a sync manner. the setupifelectedmaster(...) method collects async remote calls in the asyncactions list and then later executes these calls in a sync manner.
@SuppressForbidden(reason = "Need to open socket connection") public void testRenegotiation() throws Exception { SSLService sslService = createSSLService(); final SSLConfiguration sslConfiguration = sslService.getSSLConfiguration("xpack.ssl"); SocketFactory factory = sslService.sslSocketFactory(sslConfiguration); try (SSLSocket socket = (SSLSocket) factory.createSocket()) { SocketAccess.doPrivileged(() -> socket.connect(serviceA.boundAddress().publishAddress().address())); CountDownLatch handshakeLatch = new CountDownLatch(1); HandshakeCompletedListener firstListener = event -> handshakeLatch.countDown(); socket.addHandshakeCompletedListener(firstListener); socket.startHandshake(); handshakeLatch.await(); socket.removeHandshakeCompletedListener(firstListener); OutputStreamStreamOutput stream = new OutputStreamStreamOutput(socket.getOutputStream()); stream.writeByte((byte) 'E'); stream.writeByte((byte) 'S'); stream.writeInt(-1); stream.flush(); CountDownLatch renegotiationLatch = new CountDownLatch(1); HandshakeCompletedListener secondListener = event -> renegotiationLatch.countDown(); socket.addHandshakeCompletedListener(secondListener); socket.startHandshake(); AtomicBoolean stopped = new AtomicBoolean(false); socket.setSoTimeout(50); Thread emptyReader = new Thread(() -> { while (stopped.get() == false) { try { socket.getInputStream().read(); } catch (SocketTimeoutException e) { // Ignored } catch (IOException e) { throw new AssertionError(e); } } }); emptyReader.start(); renegotiationLatch.await(); stopped.set(true); emptyReader.join(); socket.removeHandshakeCompletedListener(secondListener); stream.writeByte((byte) 'E'); stream.writeByte((byte) 'S'); stream.writeInt(-1); stream.flush(); } }	let's reduce the timeout now. maybe 5-10 ms since we are retrying. no reason to require this test to take 50 ms when it will normally take much less (now that we are retrying).
@Override public String toString() { return "Response{clusterName=" + clusterName + ", status=" + status + ", components=" + components + '}'; } } public static class Request extends ActionRequest { private final boolean includeDetails; private final String componentName; private final String indicatorName; public Request(boolean includeDetails) { this.includeDetails = includeDetails; this.componentName = null; this.indicatorName = null; } public Request(String componentName, String indicatorName) { assert componentName != null; includeDetails = true; this.componentName = componentName; this.indicatorName = indicatorName; } @Override public ActionRequestValidationException validate() { return null; } } public static class TransportAction extends org.elasticsearch.action.support.TransportAction<Request, Response> { private final ClusterService clusterService; private final HealthService healthService; @Inject public TransportAction( ActionFilters actionFilters, TransportService transportService, ClusterService clusterService, HealthService healthService ) { super(NAME, actionFilters, transportService.getTaskManager()); this.clusterService = clusterService; this.healthService = healthService; } @Override protected void doExecute(Task task, Request request, ActionListener<Response> listener) { listener.onResponse( new Response( clusterService.getClusterName(), healthService.getHealth(request.componentName, request.indicatorName, request.includeDetails), request.componentName == null && request.indicatorName == null ) ); }	is this flag needed? if component or indicator name is specified then details are requested i think (?)
public void setupRemoteClusterConfig() throws Exception { setupRemoteClusterConfig("local_cluster"); }	thanks for cleaning this up.
@Override public XContentBuilder toXContent(XContentBuilder builder, ToXContent.Params params) throws IOException { builder.startObject(); if (resourceId != null) { builder.field(getResourceField(), resourceId); } if (message.length() > MAX_AUDIT_MESSAGE_CHARS) { assert message.length() > MAX_AUDIT_MESSAGE_CHARS : "Audit message is unexpectedly large"; builder.field(MESSAGE.getPreferredName(), truncateMessage(message, MAX_AUDIT_MESSAGE_CHARS)); } else { builder.field(MESSAGE.getPreferredName(), message); } builder.field(LEVEL.getPreferredName(), level); builder.field(TIMESTAMP.getPreferredName(), timestamp.getTime()); if (nodeName != null) { builder.field(NODE_NAME.getPreferredName(), nodeName); } String jobType = getJobType(); if (jobType != null) { builder.field(JOB_TYPE.getPreferredName(), jobType); } builder.endObject(); return builder; }	could you explain what is the value of this assertion given that you check this condition in the line above?
@Override protected String getResourceField() { return TEST_ID.getPreferredName(); } } private static final String RESOURCE_ID = "foo"; private static final String MESSAGE = "some message"; private static final Date TIMESTAMP = new Date(123456789); private static final String NODE_NAME = "some_node"; public void testGetResourceField() { TestAuditMessage message = new TestAuditMessage(RESOURCE_ID, MESSAGE, Level.INFO, TIMESTAMP, NODE_NAME); assertThat(message.getResourceField(), equalTo(TestAuditMessage.TEST_ID.getPreferredName())); } public void testGetJobType() { TestAuditMessage message = createTestInstance(); assertThat(message.getJobType(), equalTo("test_type")); } public void testNewInfo() { TestAuditMessage message = new TestAuditMessage(RESOURCE_ID, MESSAGE, Level.INFO, TIMESTAMP, NODE_NAME); assertThat(message.getResourceId(), equalTo(RESOURCE_ID)); assertThat(message.getMessage(), equalTo(MESSAGE)); assertThat(message.getLevel(), equalTo(Level.INFO)); assertThat(message.getTimestamp(), equalTo(TIMESTAMP)); assertThat(message.getNodeName(), equalTo(NODE_NAME)); } public void testNewWarning() { TestAuditMessage message = new TestAuditMessage(RESOURCE_ID, MESSAGE, Level.WARNING, TIMESTAMP, NODE_NAME); assertThat(message.getResourceId(), equalTo(RESOURCE_ID)); assertThat(message.getMessage(), equalTo(MESSAGE)); assertThat(message.getLevel(), equalTo(Level.WARNING)); assertThat(message.getTimestamp(), equalTo(TIMESTAMP)); assertThat(message.getNodeName(), equalTo(NODE_NAME)); } public void testNewError() { TestAuditMessage message = new TestAuditMessage(RESOURCE_ID, MESSAGE, Level.ERROR, TIMESTAMP, NODE_NAME); assertThat(message.getResourceId(), equalTo(RESOURCE_ID)); assertThat(message.getMessage(), equalTo(MESSAGE)); assertThat(message.getLevel(), equalTo(Level.ERROR)); assertThat(message.getTimestamp(), equalTo(TIMESTAMP)); assertThat(message.getNodeName(), equalTo(NODE_NAME)); } public void testLongMessageIsTruncated() throws IOException { AbstractAuditMessage longMessage = new AbstractAuditMessage( randomBoolean() ? null : randomAlphaOfLength(10), "thisis17charslong".repeat(490), randomFrom(Level.values()), new Date(), randomBoolean() ? null : randomAlphaOfLengthBetween(1, 20) ) { @Override public String getJobType() { return "unused"; } @Override protected String getResourceField() { return "unused"; } }; assertThat(longMessage.getMessage().length(), greaterThan(AbstractAuditMessage.MAX_AUDIT_MESSAGE_CHARS)); // serialise the message and check the new message is truncated XContentType xContentType = randomFrom(XContentType.values()); BytesReference originalXContent = XContentHelper.toXContent(longMessage, xContentType, randomBoolean()); XContentParser parser = createParser(XContentFactory.xContent(xContentType), originalXContent); AbstractAuditMessage parsed = doParseInstance(parser); assertThat(parsed.getMessage().length(), equalTo(AbstractAuditMessage.MAX_AUDIT_MESSAGE_CHARS)); } public void testTruncateString() { String message = "a short message short message short message short message short message"; String truncated = AbstractAuditMessage.truncateMessage(message, 20); assertEquals("a ... (truncated)", truncated); assertThat(truncated.length(), lessThanOrEqualTo(20)); truncated = AbstractAuditMessage.truncateMessage(message, 23); assertEquals("a short ... (truncated)", truncated); assertThat(truncated.length(), lessThanOrEqualTo(23)); truncated = AbstractAuditMessage.truncateMessage(message, 31); assertEquals("a short message ... (truncated)", truncated); assertThat(truncated.length(), lessThanOrEqualTo(31)); truncated = AbstractAuditMessage.truncateMessage(message, 32); assertEquals("a short message ... (truncated)", truncated); assertThat(truncated.length(), lessThanOrEqualTo(32)); } public void testTruncateString_noSpaceChar() { String message = "ashortmessageshortmessageshortmessageshortmessageshortmessage"; String truncated = AbstractAuditMessage.truncateMessage(message, 20); assertEquals("ashor... (truncated)", truncated); assertEquals(20, truncated.length()); truncated = AbstractAuditMessage.truncateMessage(message, 25); assertEquals("ashortmess... (truncated)", truncated); assertEquals(25, truncated.length()); } public void testTruncateString_tabsInsteadOfSpaces() { String truncated = AbstractAuditMessage.truncateMessage("a\\tshort\\tmessage\\tshort\\tmessage", 25); assertEquals("a\\tshort\\tme... (truncated)", truncated); assertEquals(25, truncated.length()); } @Override protected TestAuditMessage doParseInstance(XContentParser parser) { return TestAuditMessage.PARSER.apply(parser, null); } @Override protected boolean supportsUnknownFields() { return true; }	assertthat(..., equalto(...)) could be used instead of assertequals for consistency as you're already using assertthat with lessthanorequalto below.
@Override protected String getResourceField() { return TEST_ID.getPreferredName(); } } private static final String RESOURCE_ID = "foo"; private static final String MESSAGE = "some message"; private static final Date TIMESTAMP = new Date(123456789); private static final String NODE_NAME = "some_node"; public void testGetResourceField() { TestAuditMessage message = new TestAuditMessage(RESOURCE_ID, MESSAGE, Level.INFO, TIMESTAMP, NODE_NAME); assertThat(message.getResourceField(), equalTo(TestAuditMessage.TEST_ID.getPreferredName())); } public void testGetJobType() { TestAuditMessage message = createTestInstance(); assertThat(message.getJobType(), equalTo("test_type")); } public void testNewInfo() { TestAuditMessage message = new TestAuditMessage(RESOURCE_ID, MESSAGE, Level.INFO, TIMESTAMP, NODE_NAME); assertThat(message.getResourceId(), equalTo(RESOURCE_ID)); assertThat(message.getMessage(), equalTo(MESSAGE)); assertThat(message.getLevel(), equalTo(Level.INFO)); assertThat(message.getTimestamp(), equalTo(TIMESTAMP)); assertThat(message.getNodeName(), equalTo(NODE_NAME)); } public void testNewWarning() { TestAuditMessage message = new TestAuditMessage(RESOURCE_ID, MESSAGE, Level.WARNING, TIMESTAMP, NODE_NAME); assertThat(message.getResourceId(), equalTo(RESOURCE_ID)); assertThat(message.getMessage(), equalTo(MESSAGE)); assertThat(message.getLevel(), equalTo(Level.WARNING)); assertThat(message.getTimestamp(), equalTo(TIMESTAMP)); assertThat(message.getNodeName(), equalTo(NODE_NAME)); } public void testNewError() { TestAuditMessage message = new TestAuditMessage(RESOURCE_ID, MESSAGE, Level.ERROR, TIMESTAMP, NODE_NAME); assertThat(message.getResourceId(), equalTo(RESOURCE_ID)); assertThat(message.getMessage(), equalTo(MESSAGE)); assertThat(message.getLevel(), equalTo(Level.ERROR)); assertThat(message.getTimestamp(), equalTo(TIMESTAMP)); assertThat(message.getNodeName(), equalTo(NODE_NAME)); } public void testLongMessageIsTruncated() throws IOException { AbstractAuditMessage longMessage = new AbstractAuditMessage( randomBoolean() ? null : randomAlphaOfLength(10), "thisis17charslong".repeat(490), randomFrom(Level.values()), new Date(), randomBoolean() ? null : randomAlphaOfLengthBetween(1, 20) ) { @Override public String getJobType() { return "unused"; } @Override protected String getResourceField() { return "unused"; } }; assertThat(longMessage.getMessage().length(), greaterThan(AbstractAuditMessage.MAX_AUDIT_MESSAGE_CHARS)); // serialise the message and check the new message is truncated XContentType xContentType = randomFrom(XContentType.values()); BytesReference originalXContent = XContentHelper.toXContent(longMessage, xContentType, randomBoolean()); XContentParser parser = createParser(XContentFactory.xContent(xContentType), originalXContent); AbstractAuditMessage parsed = doParseInstance(parser); assertThat(parsed.getMessage().length(), equalTo(AbstractAuditMessage.MAX_AUDIT_MESSAGE_CHARS)); } public void testTruncateString() { String message = "a short message short message short message short message short message"; String truncated = AbstractAuditMessage.truncateMessage(message, 20); assertEquals("a ... (truncated)", truncated); assertThat(truncated.length(), lessThanOrEqualTo(20)); truncated = AbstractAuditMessage.truncateMessage(message, 23); assertEquals("a short ... (truncated)", truncated); assertThat(truncated.length(), lessThanOrEqualTo(23)); truncated = AbstractAuditMessage.truncateMessage(message, 31); assertEquals("a short message ... (truncated)", truncated); assertThat(truncated.length(), lessThanOrEqualTo(31)); truncated = AbstractAuditMessage.truncateMessage(message, 32); assertEquals("a short message ... (truncated)", truncated); assertThat(truncated.length(), lessThanOrEqualTo(32)); } public void testTruncateString_noSpaceChar() { String message = "ashortmessageshortmessageshortmessageshortmessageshortmessage"; String truncated = AbstractAuditMessage.truncateMessage(message, 20); assertEquals("ashor... (truncated)", truncated); assertEquals(20, truncated.length()); truncated = AbstractAuditMessage.truncateMessage(message, 25); assertEquals("ashortmess... (truncated)", truncated); assertEquals(25, truncated.length()); } public void testTruncateString_tabsInsteadOfSpaces() { String truncated = AbstractAuditMessage.truncateMessage("a\\tshort\\tmessage\\tshort\\tmessage", 25); assertEquals("a\\tshort\\tme... (truncated)", truncated); assertEquals(25, truncated.length()); } @Override protected TestAuditMessage doParseInstance(XContentParser parser) { return TestAuditMessage.PARSER.apply(parser, null); } @Override protected boolean supportsUnknownFields() { return true; }	let's put a blank line before each example to be able to visually tell them apart.
@Override public TopDocsAndMaxScore topDocs(SearchHit hit) throws IOException { Weight innerHitQueryWeight = getInnerHitQueryWeight(); String joinName = getSortedDocValue(joiner.getJoinField(), context, hit.docId()); if (joinName == null) { return new TopDocsAndMaxScore(Lucene.EMPTY_TOP_DOCS, Float.NaN); } QueryShardContext qsc = context.getQueryShardContext(); Query q; if (fetchChildInnerHits) { Query hitQuery = new TermQuery(new Term(joiner.parentJoinField(typeName), hit.getId())); q = new BooleanQuery.Builder() // Only include child documents that have the current hit as parent: .add(hitQuery, BooleanClause.Occur.FILTER) // and only include child documents of a single relation: .add(new TermQuery(new Term(joiner.getJoinField(), typeName)), BooleanClause.Occur.FILTER) .build(); } else { String parentId = getSortedDocValue(joiner.childJoinField(typeName), context, hit.docId()); if (parentId == null) { return new TopDocsAndMaxScore(Lucene.EMPTY_TOP_DOCS, Float.NaN); } q = context.fieldType(IdFieldMapper.NAME).termQuery(parentId, qsc); } Weight weight = context.searcher().createWeight(context.searcher().rewrite(q), ScoreMode.COMPLETE_NO_SCORES, 1f); if (size() == 0) { TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector(); for (LeafReaderContext ctx : context.searcher().getIndexReader().leaves()) { intersect(weight, innerHitQueryWeight, totalHitCountCollector, ctx); } return new TopDocsAndMaxScore( new TopDocs( new TotalHits(totalHitCountCollector.getTotalHits(), TotalHits.Relation.EQUAL_TO), Lucene.EMPTY_SCORE_DOCS ), Float.NaN); } else { int topN = Math.min(from() + size(), context.searcher().getIndexReader().maxDoc()); TopDocsCollector<?> topDocsCollector; MaxScoreCollector maxScoreCollector = null; if (sort() != null) { topDocsCollector = TopFieldCollector.create(sort().sort, topN, Integer.MAX_VALUE); if (trackScores()) { maxScoreCollector = new MaxScoreCollector(); } } else { topDocsCollector = TopScoreDocCollector.create(topN, Integer.MAX_VALUE); maxScoreCollector = new MaxScoreCollector(); } for (LeafReaderContext ctx : context.searcher().getIndexReader().leaves()) { intersect(weight, innerHitQueryWeight, MultiCollector.wrap(topDocsCollector, maxScoreCollector), ctx); } TopDocs topDocs = topDocsCollector.topDocs(from(), size()); float maxScore = Float.NaN; if (maxScoreCollector != null) { maxScore = maxScoreCollector.getMaxScore(); } return new TopDocsAndMaxScore(topDocs, maxScore); } }	this has existed for a while and we just weren't using it.
public DocumentMapperForType documentMapperWithAutoCreate() { DocumentMapper mapper = documentMapper(); if (mapper != null) { return new DocumentMapperForType(mapper, null); } mapper = parse(SINGLE_MAPPING_NAME, null); return new DocumentMapperForType(mapper, mapper.mapping()); } /** * Given the full name of a field, returns its {@link MappedFieldType}. * @deprecated Use {@link #snapshot()} and {@link Snapshot#fieldType(String)}	these are still used in a bunch of places. i *think* we should avoid them if we decide that this is the right way to go.
public SearchExtBuilder getSearchExt(String name) { return searchContext.getSearchExt(name); } /** * Get the {@link MappedFieldType}	i put all of there here rather than calling getqueryshardcontext().fieldtype because there is some confusion around what is "ok" to call where. putting these are methods does two things: 1. it makes us talk about whether or not it is "ok" to use queryshardcontext here. 2. if we decide that it is, then we have a method folks can use that they know is "ok".
* @param locations - paths to directories with state folder * @return maximum id of state file or -1 if no such files are found * @throws IOException if IOException occurs */ protected long findMaxGenerationId(final String prefix, Path... locations) throws IOException { long maxId = -1; for (Path dataLocation : locations) { final Path resolve = dataLocation.resolve(STATE_DIR_NAME); if (Files.exists(resolve)) { try (DirectoryStream<Path> stream = Files.newDirectoryStream(resolve, prefix + "*")) { for (Path stateFile : stream) { final Matcher matcher = stateFilePattern.matcher(stateFile.getFileName().toString()); if (matcher.matches()) { final long id = Long.parseLong(matcher.group(1)); maxId = Math.max(maxId, id); } } } } } return maxId; }	can we make the visibility of this package private instead?
public void testInconsistentMultiPathState() throws IOException { Path paths[] = new Path[3]; for (int i = 0; i < paths.length; i++) { paths[i] = createTempDir(); } Format format = new Format("foo-"); DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 100), randomInt(), randomLong(), randomDouble(), randomBoolean()); // Call write without clean-up to simulate multi-write transaction. long genId = format.write(state, paths); assertEquals(state, format.loadLatestState(logger, NamedXContentRegistry.EMPTY, paths)); ensureOnlyOneStateFile(paths); for (Path path : paths) { assertEquals(genId, format.findMaxGenerationId("foo-", path)); } assertEquals(0, format.findStateFilesByGeneration(-1, paths).size()); assertEquals(paths.length, format.findStateFilesByGeneration(genId, paths).size()); Path badPath = paths[paths.length-1]; format.failOnPaths(badPath.resolve(MetadataStateFormat.STATE_DIR_NAME)); format.failOnMethods(Format.FAIL_RENAME_TMP_FILE); expectThrows(WriteStateException.class, () -> format.write(state, paths)); long firstPathId = format.findMaxGenerationId("foo-", paths[0]); assertEquals(firstPathId, format.findMaxGenerationId("foo-", paths[1])); assertEquals(genId, format.findMaxGenerationId("foo-", badPath)); assertEquals(genId, firstPathId-1); }	i think we can also verify that we can find max genid and read state when supplying all paths (and get the new state out)?
public void testInconsistentMultiPathState() throws IOException { Path paths[] = new Path[3]; for (int i = 0; i < paths.length; i++) { paths[i] = createTempDir(); } Format format = new Format("foo-"); DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 100), randomInt(), randomLong(), randomDouble(), randomBoolean()); // Call write without clean-up to simulate multi-write transaction. long genId = format.write(state, paths); assertEquals(state, format.loadLatestState(logger, NamedXContentRegistry.EMPTY, paths)); ensureOnlyOneStateFile(paths); for (Path path : paths) { assertEquals(genId, format.findMaxGenerationId("foo-", path)); } assertEquals(0, format.findStateFilesByGeneration(-1, paths).size()); assertEquals(paths.length, format.findStateFilesByGeneration(genId, paths).size()); Path badPath = paths[paths.length-1]; format.failOnPaths(badPath.resolve(MetadataStateFormat.STATE_DIR_NAME)); format.failOnMethods(Format.FAIL_RENAME_TMP_FILE); expectThrows(WriteStateException.class, () -> format.write(state, paths)); long firstPathId = format.findMaxGenerationId("foo-", paths[0]); assertEquals(firstPathId, format.findMaxGenerationId("foo-", paths[1])); assertEquals(genId, format.findMaxGenerationId("foo-", badPath)); assertEquals(genId, firstPathId-1); }	can we also verify that format.loadlateststage(..., paths) return null and that findmaxgenerationid return -1?
public void testInconsistentMultiPathState() throws IOException { Path paths[] = new Path[3]; for (int i = 0; i < paths.length; i++) { paths[i] = createTempDir(); } Format format = new Format("foo-"); DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 100), randomInt(), randomLong(), randomDouble(), randomBoolean()); // Call write without clean-up to simulate multi-write transaction. long genId = format.write(state, paths); assertEquals(state, format.loadLatestState(logger, NamedXContentRegistry.EMPTY, paths)); ensureOnlyOneStateFile(paths); for (Path path : paths) { assertEquals(genId, format.findMaxGenerationId("foo-", path)); } assertEquals(0, format.findStateFilesByGeneration(-1, paths).size()); assertEquals(paths.length, format.findStateFilesByGeneration(genId, paths).size()); Path badPath = paths[paths.length-1]; format.failOnPaths(badPath.resolve(MetadataStateFormat.STATE_DIR_NAME)); format.failOnMethods(Format.FAIL_RENAME_TMP_FILE); expectThrows(WriteStateException.class, () -> format.write(state, paths)); long firstPathId = format.findMaxGenerationId("foo-", paths[0]); assertEquals(firstPathId, format.findMaxGenerationId("foo-", paths[1])); assertEquals(genId, format.findMaxGenerationId("foo-", badPath)); assertEquals(genId, firstPathId-1); }	i think this tests the cleanupoldfiles method more than delete (while it fails on delete)? perhaps name it: suggestion public void testcleanupoldfileswitherrorpath() throws ioexception {
final void verifyAfterCleanup(MetadataSnapshot sourceMetaData, MetadataSnapshot targetMetaData) { final RecoveryDiff recoveryDiff = targetMetaData.recoveryDiff(sourceMetaData); if (recoveryDiff.identical.size() != recoveryDiff.size()) { if (recoveryDiff.missing.isEmpty()) { for (StoreFileMetaData meta : recoveryDiff.different) { StoreFileMetaData local = targetMetaData.get(meta.name()); StoreFileMetaData remote = sourceMetaData.get(meta.name()); // if we have different files then they must have no checksums; otherwise something went wrong during recovery. // we have that problem when we have an empty index (is only a segments_1 file) then we can't tell if it's a Lucene 4.8 file // and therefore no checksum is included. That isn't a problem since we simply copy it over anyway but those files come out as // different in the diff. That's why we have to double check here again if the rest of it matches. // all is fine this file is just part of a commit or a segment that is different if (local.isSame(remote) == false) { logger.debug("Files are different on the recovery target: {} ", recoveryDiff); throw new IllegalStateException("local version: " + local + " is different from remote version after recovery: " + remote, null); } } } else { logger.debug("Files are missing on the recovery target: {} ", recoveryDiff); throw new IllegalStateException("Files are missing on the recovery target: [different=" + recoveryDiff.different + ", missing=" + recoveryDiff.missing + ']', null); } } }	i think this should be: "we have an empty index is only a segments_1 file *so* we can't tell"
private RecoveryPlannerService getRecoveryPlannerService(ThreadPool threadPool, ClusterService clusterService, RepositoriesService repositoryService) { final List<RecoveryPlannerPlugin> recoveryPlannerPlugins = pluginsService.filterPlugins(RecoveryPlannerPlugin.class); if (recoveryPlannerPlugins.isEmpty()) { return new SourceOnlyRecoveryPlannerService(); } if (recoveryPlannerPlugins.size() > 1) { throw new IllegalStateException("A single RecoveryPlannerPlugin was expected but got: " + recoveryPlannerPlugins); } final ShardSnapshotsService shardSnapshotsService = new ShardSnapshotsService(client, repositoryService, threadPool, clusterService ); final RecoveryPlannerPlugin recoveryPlannerPlugin = recoveryPlannerPlugins.get(0); return recoveryPlannerPlugin.createRecoveryPlannerService(shardSnapshotsService); }	maybe we could also move the creation of this service and the service into the x-pack module as a follow-up? no need to do it now.
public abstract Mapper build(BuilderContext context); } public interface TypeParser { class ParserContext { private final MapperService mapperService; private final Function<String, TypeParser> typeParsers; private final Version indexVersionCreated; private final Supplier<QueryShardContext> queryShardContextSupplier; public ParserContext(MapperService mapperService, Function<String, TypeParser> typeParsers, Version indexVersionCreated, Supplier<QueryShardContext> queryShardContextSupplier) { this.mapperService = mapperService; this.typeParsers = typeParsers; this.indexVersionCreated = indexVersionCreated; this.queryShardContextSupplier = queryShardContextSupplier; } public IndexAnalyzers getIndexAnalyzers() { return mapperService.getIndexAnalyzers(); } public MapperService mapperService() { return mapperService; } public TypeParser typeParser(String type) { return typeParsers.apply(type); } public Version indexVersionCreated() { return indexVersionCreated; } public Supplier<QueryShardContext> queryShardContextSupplier() { return queryShardContextSupplier; } public boolean isWithinMultiField() { return false; } protected Function<String, TypeParser> typeParsers() { return typeParsers; } public ParserContext createMultiFieldContext(ParserContext in) { return new MultiFieldParserContext(in); } static class MultiFieldParserContext extends ParserContext { MultiFieldParserContext(ParserContext in) { super(in.mapperService(), in.typeParsers(), in.indexVersionCreated(), in.queryShardContextSupplier()); } @Override public boolean isWithinMultiField() { return true; } } } Mapper.Builder<?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException; } private final String simpleName; public Mapper(String simpleName) { Objects.requireNonNull(simpleName); this.simpleName = simpleName; } /** Returns the simple name, which identifies this mapper against other mappers at the same level in the mappers hierarchy * TODO: make this protected once Mapper and FieldMapper are merged together */ public final String simpleName() { return simpleName; } /** Returns the canonical name which uniquely identifies the mapper against other mappers in a type. */ public abstract String name(); /** * Returns a name representing the type of this mapper. */ public abstract String typeName(); /** Return the merge of {@code mergeWith} into this. * Both {@code this} and {@code mergeWith}	if i'm understanding correctly, it's still possible to set a similarity on a non-text field, but we now ignore it. although i agree that similarities aren't nearly as useful for non-text fields, this behavior seems trappy and it's technically a breaking change. my vote would be to maintain the previous behavior where similarities on types like keyword were respected. if we want to disallow setting similarities on non-text types, we could open a dedicated issue to discuss it.
@Override public void onRegistered(TracingPlugin.Traceable traceable) { final Tracer tracer = this.tracer; if (tracer != null) { spans.computeIfAbsent(traceable.getSpanId(), spanId -> { final Span span = tracer.spanBuilder(traceable.getSpanName()).startSpan(); for (Map.Entry<String, Object> entry : traceable.getAttributes().entrySet()) { final Object value = entry.getValue(); if (value instanceof String) { span.setAttribute(entry.getKey(), (String) value); } else if (value instanceof Long) { span.setAttribute(entry.getKey(), (Long) value); } else if (value instanceof Integer) { span.setAttribute(entry.getKey(), (Integer) value); } else if (value instanceof Double) { span.setAttribute(entry.getKey(), (Double) value); } else if (value instanceof Boolean) { span.setAttribute(entry.getKey(), (Boolean) value); } else { throw new IllegalArgumentException( "span attributes do not support value type of [" + value.getClass().getCanonicalName() + "]" ); } } return span; }); } }	this is not pretty. but it does the job for now and keep other parts unware of the opentelemetry dependencies. we can improve it once we know more about what other things needed by the new traceable interface.
*/ private boolean joinElectedMaster(DiscoveryNode masterNode) { try { // first, make sure we can connect to the master transportService.connectToNode(masterNode); } catch (Exception e) { logger.warn("failed to connect to master [{}], retrying...", e, masterNode); return false; } for (int joinAttempt = 1; joinAttempt <= this.joinRetryAttempts; joinAttempt++) { try { logger.trace("joining master {}", masterNode); membership.sendJoinRequestBlocking(masterNode, localNode, joinTimeout); } catch (ElasticsearchIllegalStateException e) { if (joinAttempt >= this.joinRetryAttempts) { logger.info("failed to send join request to master [{}], reason [{}]. Tried [{}] times", masterNode, e.getDetailedMessage(), joinAttempt); return false; } else { logger.trace("master {} failed with [{}]. retrying... (attempts done: [{}])", masterNode, e.getDetailedMessage(), joinAttempt); } } catch (Exception e) { if (e instanceof ElasticsearchException) { logger.info("failed to send join request to master [{}], reason [{}]", masterNode, ((ElasticsearchException) e).getDetailedMessage()); } else { logger.info("failed to send join request to master [{}], reason [{}]", masterNode, e.getMessage()); } if (logger.isTraceEnabled()) { logger.trace("detailed failed reason", e); } return false; } try { Thread.sleep(this.joinRetryDelay.millis()); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } return false; }	i wrote this logic, and i don't like it, maybe turn it upside down, and do "if logger.istrace -> log.trace the failure, else log.info("....", exceptionhelper.detailedmessage)
private void pruneDeletedTombstones() { final long timeMSec = engineConfig.getThreadPool().relativeTimeInMillis(); versionMap.pruneTombstones(timeMSec, engineConfig.getIndexSettings().getGcDeletesInMillis()); lastDeleteVersionPruneTimeMSec = timeMSec; }	wait - this feels weird - current time is -1 and interval is 0? shouldn't current time be long.max_value?
private void putUnderLock(BytesRef uid, VersionValue version, Maps maps) { assert keyedLock.isHeldByCurrentThread(uid); assert uid.bytes.length == uid.length : "Oversized _uid! UID length: " + uid.length + ", bytes length: " + uid.bytes.length; if (version.isDelete() == false) { maps.put(uid, version); removeTombstoneUnderLock(uid); } else { DeleteVersionValue versionValue = (DeleteVersionValue) version; putTombstone(uid, versionValue); maps.remove(uid, versionValue); } }	this comment is wrong now.
public void testConcurrently() throws IOException, InterruptedException { HashSet<BytesRef> keySet = new HashSet<>(); int numKeys = randomIntBetween(50, 200); for (int i = 0; i < numKeys; i++) { keySet.add(uid(TestUtil.randomSimpleString(random(), 10, 20))); } List<BytesRef> keyList = new ArrayList<>(keySet); ConcurrentHashMap<BytesRef, VersionValue> values = new ConcurrentHashMap<>(); LiveVersionMap map = new LiveVersionMap(); int numThreads = randomIntBetween(2, 5); Thread[] threads = new Thread[numThreads]; CountDownLatch startGun = new CountDownLatch(numThreads); CountDownLatch done = new CountDownLatch(numThreads); int randomValuesPerThread = randomIntBetween(5000, 20000); AtomicLong clock = new AtomicLong(0); for (int j = 0; j < threads.length; j++) { threads[j] = new Thread(() -> { startGun.countDown(); try { startGun.await(); } catch (InterruptedException e) { done.countDown(); throw new AssertionError(e); } try { for (int i = 0; i < randomValuesPerThread; ++i) { BytesRef bytesRef = randomFrom(random(), keyList); try (Releasable r = map.acquireLock(bytesRef)) { VersionValue versionValue = values.computeIfAbsent(bytesRef, v -> new VersionValue(randomLong(), randomLong(), randomLong())); boolean isDelete = versionValue instanceof DeleteVersionValue; if (isDelete) { map.removeTombstoneUnderLock(bytesRef); } if (isDelete == false && rarely()) { versionValue = new DeleteVersionValue(versionValue.version + 1, versionValue.seqNo + 1, versionValue.term, clock.incrementAndGet()); } else { versionValue = new VersionValue(versionValue.version + 1, versionValue.seqNo + 1, versionValue.term); } values.put(bytesRef, versionValue); map.putUnderLock(bytesRef, versionValue); } if (rarely()) { map.pruneTombstones(0, 0); } } } finally { done.countDown(); } }); threads[j].start(); } do { final Map<BytesRef, VersionValue> valueMap = new HashMap<>(map.getAllCurrent()); map.beforeRefresh(); valueMap.forEach((k, v) -> { try (Releasable r = map.acquireLock(k)) { VersionValue actualValue = map.getUnderLock(k); assertNotNull(actualValue); assertTrue(v.version <= actualValue.version); } }); map.afterRefresh(randomBoolean()); valueMap.forEach((k, v) -> { try (Releasable r = map.acquireLock(k)) { VersionValue actualValue = map.getUnderLock(k); if (actualValue != null) { if (actualValue instanceof DeleteVersionValue) { assertTrue(v.version <= actualValue.version); // deletes can be the same version } else { assertTrue(v.version < actualValue.version); } } } }); if (randomBoolean()) { Thread.yield(); } } while (done.getCount() != 0); for (int j = 0; j < threads.length; j++) { threads[j].join(); } map.getAllCurrent().forEach((k, v) -> { VersionValue versionValue = values.get(k); assertNotNull(versionValue); assertEquals(v, versionValue); }); map.getAllTombstones().entrySet().forEach(e -> { VersionValue versionValue = values.get(e.getKey()); assertNotNull(versionValue); assertEquals(e.getValue(), versionValue); assertTrue(versionValue instanceof DeleteVersionValue); }); map.beforeRefresh(); map.afterRefresh(false); map.pruneTombstones(clock.incrementAndGet(), 0); assertEquals(0, StreamSupport.stream(map.getAllTombstones().entrySet().spliterator(), false).count()); }	can you comment we explicitly don't do any pruning? otherwise this looks odd.
public void testConcurrently() throws IOException, InterruptedException { HashSet<BytesRef> keySet = new HashSet<>(); int numKeys = randomIntBetween(50, 200); for (int i = 0; i < numKeys; i++) { keySet.add(uid(TestUtil.randomSimpleString(random(), 10, 20))); } List<BytesRef> keyList = new ArrayList<>(keySet); ConcurrentHashMap<BytesRef, VersionValue> values = new ConcurrentHashMap<>(); LiveVersionMap map = new LiveVersionMap(); int numThreads = randomIntBetween(2, 5); Thread[] threads = new Thread[numThreads]; CountDownLatch startGun = new CountDownLatch(numThreads); CountDownLatch done = new CountDownLatch(numThreads); int randomValuesPerThread = randomIntBetween(5000, 20000); AtomicLong clock = new AtomicLong(0); for (int j = 0; j < threads.length; j++) { threads[j] = new Thread(() -> { startGun.countDown(); try { startGun.await(); } catch (InterruptedException e) { done.countDown(); throw new AssertionError(e); } try { for (int i = 0; i < randomValuesPerThread; ++i) { BytesRef bytesRef = randomFrom(random(), keyList); try (Releasable r = map.acquireLock(bytesRef)) { VersionValue versionValue = values.computeIfAbsent(bytesRef, v -> new VersionValue(randomLong(), randomLong(), randomLong())); boolean isDelete = versionValue instanceof DeleteVersionValue; if (isDelete) { map.removeTombstoneUnderLock(bytesRef); } if (isDelete == false && rarely()) { versionValue = new DeleteVersionValue(versionValue.version + 1, versionValue.seqNo + 1, versionValue.term, clock.incrementAndGet()); } else { versionValue = new VersionValue(versionValue.version + 1, versionValue.seqNo + 1, versionValue.term); } values.put(bytesRef, versionValue); map.putUnderLock(bytesRef, versionValue); } if (rarely()) { map.pruneTombstones(0, 0); } } } finally { done.countDown(); } }); threads[j].start(); } do { final Map<BytesRef, VersionValue> valueMap = new HashMap<>(map.getAllCurrent()); map.beforeRefresh(); valueMap.forEach((k, v) -> { try (Releasable r = map.acquireLock(k)) { VersionValue actualValue = map.getUnderLock(k); assertNotNull(actualValue); assertTrue(v.version <= actualValue.version); } }); map.afterRefresh(randomBoolean()); valueMap.forEach((k, v) -> { try (Releasable r = map.acquireLock(k)) { VersionValue actualValue = map.getUnderLock(k); if (actualValue != null) { if (actualValue instanceof DeleteVersionValue) { assertTrue(v.version <= actualValue.version); // deletes can be the same version } else { assertTrue(v.version < actualValue.version); } } } }); if (randomBoolean()) { Thread.yield(); } } while (done.getCount() != 0); for (int j = 0; j < threads.length; j++) { threads[j].join(); } map.getAllCurrent().forEach((k, v) -> { VersionValue versionValue = values.get(k); assertNotNull(versionValue); assertEquals(v, versionValue); }); map.getAllTombstones().entrySet().forEach(e -> { VersionValue versionValue = values.get(e.getKey()); assertNotNull(versionValue); assertEquals(e.getValue(), versionValue); assertTrue(versionValue instanceof DeleteVersionValue); }); map.beforeRefresh(); map.afterRefresh(false); map.pruneTombstones(clock.incrementAndGet(), 0); assertEquals(0, StreamSupport.stream(map.getAllTombstones().entrySet().spliterator(), false).count()); }	i think this is now less strong as we don't track deletes here any more... we should somehow show that we capture deletes correctly in the various phases of refresh and that we don't forget them until we prune. please me correct if i'm missing something. this is tricky ;)
@Override public BatchResult<PutMappingClusterStateUpdateRequest> execute(ClusterState currentState, List<PutMappingClusterStateUpdateRequest> tasks) throws Exception { Map<Index, MapperService> indexMapperServices = new HashMap<>(); BatchResult.Builder<PutMappingClusterStateUpdateRequest> builder = BatchResult.builder(); try { for (PutMappingClusterStateUpdateRequest request : tasks) { try { for (Index index : request.indices()) { final IndexMetaData indexMetaData = currentState.metaData().getIndexSafe(index); if (indexMapperServices.containsKey(indexMetaData.getIndex()) == false) { MapperService mapperService = indicesService.createIndexMapperService(indexMetaData); // add mappings for all types, we need them for cross-type validation for (ObjectCursor<MappingMetaData> mapping : indexMetaData.getMappings().values()) { mapperService.merge(mapping.value.type(), mapping.value.source(), MapperService.MergeReason.MAPPING_RECOVERY, request.updateAllTypes()); } indexMapperServices.put(index, mapperService); } } currentState = applyRequest(currentState, request, indexMapperServices); builder.success(request); } catch (Exception e) { builder.failure(request, e); } } return builder.build(currentState); } finally { IOUtils.close(indexMapperServices.values()); } }	should we put it in the map before we perform the merging, so that it is closed in the finally block if the merging fails too?
public void accept(SecurityIndexManager.State previousState, SecurityIndexManager.State currentState) { final PrintStream out = BootstrapInfo.getOriginalStandardOut(); // Check if it has been closed, try to write something so that we trigger PrintStream#ensureOpen out.println(); if (out.checkError()) { outputOnError(new IllegalStateException("Stashed standard output stream is closed.")); return; } if (previousState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) && currentState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) == false && securityIndexManager.indexExists() == false) { final SecureString elasticPassword = new SecureString(generatePassword(20)); final SecureString kibanaSystemPassword = new SecureString(generatePassword(20)); nativeUsersStore .updateReservedUser( ElasticUser.NAME, elasticPassword.getChars(), DocWriteRequest.OpType.CREATE, WriteRequest.RefreshPolicy.IMMEDIATE, ActionListener.wrap(result -> { nativeUsersStore .updateReservedUser( KibanaSystemUser.NAME, kibanaSystemPassword.getChars(), DocWriteRequest.OpType.CREATE, WriteRequest.RefreshPolicy.IMMEDIATE, ActionListener.wrap( r -> { outputOnSuccess(elasticPassword, kibanaSystemPassword, out); }, this::outputOnError ) ); }, this::outputOnError)); securityIndexManager.removeStateListener(this); } }	not sure if there is any other way to check if stdout is closed
protected void taskOperation(ForecastJobAction.Request request, JobTask task, ActionListener<ForecastJobAction.Response> listener) { jobManager.getJob(task.getJobId(), ActionListener.wrap( job -> { validate(job, request); ForecastParams.Builder paramsBuilder = ForecastParams.builder(); if (request.getDuration() != null) { paramsBuilder.duration(request.getDuration()); } if (request.getExpiresIn() != null) { paramsBuilder.expiresIn(request.getExpiresIn()); } Long adjustedLimit = getAdjustedMemoryLimit(job, request.getMaxModelMemory(), auditor); if (adjustedLimit != null) { paramsBuilder.maxModelMemory(adjustedLimit); } // tmp storage might be null, we do not log here, because it might not be // required Path tmpStorage = nativeStorageProvider.tryGetLocalTmpStorage(task.getDescription(), FORECAST_LOCAL_STORAGE_LIMIT); if (tmpStorage != null) { paramsBuilder.tmpStorage(tmpStorage.toString()); } if (cppMinAvailableDiskSpaceBytes >= 0) { paramsBuilder.minAvailableDiskSpace(cppMinAvailableDiskSpaceBytes); } ForecastParams params = paramsBuilder.build(); ActionListener<Boolean> resultsMappingUpdateHandler = ActionListener.wrap( r -> processManager.forecastJob(task, params, e -> { if (e == null) { getForecastRequestStats(request.getJobId(), params.getForecastId(), listener); } else { listener.onFailure(e); } }), ex -> { logger.warn( new ParameterizedMessage( "[{}] job failed to update results mappings before requesting forecast", request.getJobId() ), ex); listener.onFailure(ex); }); ElasticsearchMappings.addDocMappingIfMissing( AnomalyDetectorsIndex.jobResultsAliasedName(request.getJobId()), AnomalyDetectorsIndex::resultsMapping, client, clusterService.state(), resultsMappingUpdateHandler); }, listener::onFailure )); }	i think the changes in this file are unnecessary, because from the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-forecast.html): > the job must be open when you create a forecast. otherwise, an error occurs. and we already update the mappings when opening a job: https://github.com/elastic/elasticsearch/blob/034926651708cbe8e96f28d10e029e1f3bf1a3a9/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/job/process/autodetect/autodetectprocessmanager.java#l484-l495
private static boolean checkGroupByOrder(LogicalPlan p, Set<Failure> localFailures, Set<LogicalPlan> groupingFailures, AttributeMap<Expression> attributeRefs) { if (p instanceof OrderBy) { OrderBy o = (OrderBy) p; LogicalPlan child = o.child(); if (child instanceof Project) { child = ((Project) child).child(); } if (child instanceof Filter) { child = ((Filter) child).child(); } if (child instanceof Aggregate) { Aggregate a = (Aggregate) child; Map<Expression, Node<?>> missing = new LinkedHashMap<>(); o.order().forEach(oe -> { final Expression e = oe.child(); final Expression resolvedE = attributeRefs.getOrDefault(e, e); // aggregates are allowed if (Functions.isAggregate(resolvedE)) { return; } // take aliases declared inside the aggregates which point to the grouping (but are not included in there) // to correlate them to the order List<Expression> groupingAndMatchingAggregatesAliases = new ArrayList<>(a.groupings()); a.aggregates().forEach(as -> { if (as instanceof Alias) { Alias al = (Alias) as; if (Expressions.anyMatch(a.groupings(), g -> Expressions.equalsAsAttribute(al.child(), g))) { groupingAndMatchingAggregatesAliases.add(al); } } }); // Make sure you can apply functions on top of the grouped by expressions in the ORDER BY: // e.g.: if "GROUP BY f2(f1(field))" you can "ORDER BY f4(f3(f2(f1(field))))" // // Also, make sure to compare attributes directly if (resolvedE.anyMatch(expression -> Expressions.anyMatch(groupingAndMatchingAggregatesAliases, g -> { Expression resolvedG = attributeRefs.getOrDefault(g, g); resolvedG = expression instanceof Attribute ? Expressions.attribute(resolvedG) : resolvedG; return expression.semanticEquals(resolvedG); }))) { return; } // nothing matched, cannot group by it missing.put(e, oe); }); if (missing.isEmpty() == false) { String plural = missing.size() > 1 ? "s" : StringUtils.EMPTY; // get the location of the first missing expression as the order by might be on a different line localFailures.add( fail(missing.values().iterator().next(), "Cannot order by non-grouped column" + plural + " {}, expected {} or an aggregate function", Expressions.names(missing.keySet()), Expressions.names(a.groupings()))); groupingFailures.add(a); return false; } } } return true; }	see my comment above - it's misleading for a map lookup to do resolution.
public void testSubqueryWithAliasOrderByAlias() throws Exception { PhysicalPlan p = optimizeAndPlan("SELECT i FROM " + "( SELECT int AS i FROM test ) AS s " + "ORDER BY s.i > 10"); }	please break this method into multiple tests that indicate the difference between them in their name.
void performDuringNoSnapshot(IndexMetaData indexMetaData, ClusterState currentClusterState, Listener listener) { String followerIndex = indexMetaData.getIndex().getName(); Map<String, String> customIndexMetadata = indexMetaData.getCustomData(CCR_METADATA_KEY); if (customIndexMetadata == null) { listener.onResponse(true); return; } if (indexMetaData.getState() == IndexMetaData.State.OPEN) { CloseIndexRequest closeIndexRequest = new CloseIndexRequest(followerIndex); getClient().admin().indices().close(closeIndexRequest, ActionListener.wrap( r -> { assert r.isAcknowledged() : "close index response is not acknowledged"; listener.onResponse(true); }, listener::onFailure) ); } else { listener.onResponse(true); } }	can you explain this check? is this a separate issue you found?
void performDuringNoSnapshot(IndexMetaData indexMetaData, ClusterState currentClusterState, Listener listener) { String followerIndex = indexMetaData.getIndex().getName(); Map<String, String> customIndexMetadata = indexMetaData.getCustomData(CCR_METADATA_KEY); if (customIndexMetadata == null) { listener.onResponse(true); return; } if (indexMetaData.getState() == IndexMetaData.State.OPEN) { CloseIndexRequest closeIndexRequest = new CloseIndexRequest(followerIndex); getClient().admin().indices().close(closeIndexRequest, ActionListener.wrap( r -> { assert r.isAcknowledged() : "close index response is not acknowledged"; listener.onResponse(true); }, listener::onFailure) ); } else { listener.onResponse(true); } }	this change can (and should be imo) be broken out into a separate pr that can be merged before this one.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(index); out.writeBoolean(managedByILM); if (managedByILM) { out.writeString(policyName); out.writeOptionalLong(lifecycleDate); out.writeOptionalString(phase); out.writeOptionalString(action); out.writeOptionalString(step); out.writeOptionalString(failedStep); out.writeOptionalLong(phaseTime); out.writeOptionalLong(actionTime); out.writeOptionalLong(stepTime); out.writeOptionalBytesReference(stepInfo); out.writeOptionalWriteable(phaseExecutionInfo); out.writeOptionalBoolean(isTransitiveError); out.writeOptionalVInt(failedStepRetryCount); } }	same comment here about version checks when backporting
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(INDEX_FIELD.getPreferredName(), index); builder.field(MANAGED_BY_ILM_FIELD.getPreferredName(), managedByILM); if (managedByILM) { builder.field(POLICY_NAME_FIELD.getPreferredName(), policyName); if (lifecycleDate != null) { builder.timeField(LIFECYCLE_DATE_MILLIS_FIELD.getPreferredName(), LIFECYCLE_DATE_FIELD.getPreferredName(), lifecycleDate); builder.field(AGE_FIELD.getPreferredName(), getAge().toHumanReadableString(2)); } if (phase != null) { builder.field(PHASE_FIELD.getPreferredName(), phase); } if (phaseTime != null) { builder.timeField(PHASE_TIME_MILLIS_FIELD.getPreferredName(), PHASE_TIME_FIELD.getPreferredName(), phaseTime); } if (action != null) { builder.field(ACTION_FIELD.getPreferredName(), action); } if (actionTime != null) { builder.timeField(ACTION_TIME_MILLIS_FIELD.getPreferredName(), ACTION_TIME_FIELD.getPreferredName(), actionTime); } if (step != null) { builder.field(STEP_FIELD.getPreferredName(), step); } if (stepTime != null) { builder.timeField(STEP_TIME_MILLIS_FIELD.getPreferredName(), STEP_TIME_FIELD.getPreferredName(), stepTime); } if (Strings.hasLength(failedStep)) { builder.field(FAILED_STEP_FIELD.getPreferredName(), failedStep); } if (isTransitiveError != null) { builder.field(IS_TRANSITIVE_ERROR_FIELD.getPreferredName(), isTransitiveError); } if(failedStepRetryCount != null) { builder.field(FAILED_STEP_RETRY_COUNT_FIELD.getPreferredName(), failedStepRetryCount); } if (stepInfo != null && stepInfo.length() > 0) { builder.rawField(STEP_INFO_FIELD.getPreferredName(), stepInfo.streamInput(), XContentType.JSON); } if (phaseExecutionInfo != null) { builder.field(PHASE_EXECUTION_INFO.getPreferredName(), phaseExecutionInfo); } } builder.endObject(); return builder; }	super minor nit: suggestion if (failedstepretrycount != null) {
static LifecycleExecutionState fromCustomMetadata(Map<String, String> customData) { Builder builder = builder(); if (customData.containsKey(PHASE)) { builder.setPhase(customData.get(PHASE)); } if (customData.containsKey(ACTION)) { builder.setAction(customData.get(ACTION)); } if (customData.containsKey(STEP)) { builder.setStep(customData.get(STEP)); } if (customData.containsKey(FAILED_STEP)) { builder.setFailedStep(customData.get(FAILED_STEP)); } if (customData.containsKey(IS_TRANSITIVE_ERROR)) { builder.setIsTransitiveError(Boolean.parseBoolean(customData.get(IS_TRANSITIVE_ERROR))); } if (customData.containsKey(FAILED_STEP_RETRY_COUNT)) { builder.setFailedStepRetryCount(Integer.parseInt(customData.get(FAILED_STEP_RETRY_COUNT))); } if (customData.containsKey(STEP_INFO)) { builder.setStepInfo(customData.get(STEP_INFO)); } if (customData.containsKey(PHASE_DEFINITION)) { builder.setPhaseDefinition(customData.get(PHASE_DEFINITION)); } if (customData.containsKey(INDEX_CREATION_DATE)) { try { builder.setIndexCreationDate(Long.parseLong(customData.get(INDEX_CREATION_DATE))); } catch (NumberFormatException e) { throw new ElasticsearchException("Custom metadata field [{}] does not contain a valid long. Actual value: [{}]", e, INDEX_CREATION_DATE, customData.get(INDEX_CREATION_DATE)); } } if (customData.containsKey(PHASE_TIME)) { try { builder.setPhaseTime(Long.parseLong(customData.get(PHASE_TIME))); } catch (NumberFormatException e) { throw new ElasticsearchException("Custom metadata field [{}] does not contain a valid long. Actual value: [{}]", e, PHASE_TIME, customData.get(PHASE_TIME)); } } if (customData.containsKey(ACTION_TIME)) { try { builder.setActionTime(Long.parseLong(customData.get(ACTION_TIME))); } catch (NumberFormatException e) { throw new ElasticsearchException("Custom metadata field [{}] does not contain a valid long. Actual value: [{}]", e, ACTION_TIME, customData.get(ACTION_TIME)); } } if (customData.containsKey(STEP_TIME)) { try { builder.setStepTime(Long.parseLong(customData.get(STEP_TIME))); } catch (NumberFormatException e) { throw new ElasticsearchException("Custom metadata field [{}] does not contain a valid long. Actual value: [{}]", e, STEP_TIME, customData.get(STEP_TIME)); } } return builder.build(); } /** * Converts this object to an immutable map representation for use with * {@link IndexMetaData.Builder#putCustom(String, Map)}	this (and the one below it) should probably be wrapped in a try/catch to clarify the exception if the value isn't parseable, like the other fields in this method: https://github.com/elastic/elasticsearch/blob/016d1c95135eb785321baf899dcb1e800de03e4e/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/lifecycleexecutionstate.java#l152-l156
public void performAction(IndexMetaData indexMetaData, ClusterState currentClusterState, ClusterStateObserver observer, Listener listener) { if (indexMetaData.getState() == IndexMetaData.State.CLOSE) { OpenIndexRequest request = new OpenIndexRequest(indexMetaData.getIndex().getName()); getClient().admin().indices().open(request, ActionListener.wrap( r -> { assert r.isAcknowledged() : "open index response is not acknowledged"; listener.onResponse(true); }, listener::onFailure )); } else { listener.onResponse(true); } }	is this addressing a separate bug?
public void performAction(IndexMetaData indexMetaData, ClusterState currentClusterState, ClusterStateObserver observer, Listener listener) { if (indexMetaData.getState() == IndexMetaData.State.CLOSE) { OpenIndexRequest request = new OpenIndexRequest(indexMetaData.getIndex().getName()); getClient().admin().indices().open(request, ActionListener.wrap( r -> { assert r.isAcknowledged() : "open index response is not acknowledged"; listener.onResponse(true); }, listener::onFailure )); } else { listener.onResponse(true); } }	again, i think this should be broken out into another pr along with the change to closefollowerindexstep.
public void testCloseFollowerIndexIsNoopForAlreadyClosedIndex() { IndexMetaData indexMetadata = IndexMetaData.builder("follower-index") .settings(settings(Version.CURRENT).put(LifecycleSettings.LIFECYCLE_INDEXING_COMPLETE, "true")) .putCustom(CCR_METADATA_KEY, Collections.emptyMap()) .state(IndexMetaData.State.CLOSE) .numberOfShards(1) .numberOfReplicas(0) .build(); Client client = Mockito.mock(Client.class); CloseFollowerIndexStep step = new CloseFollowerIndexStep(randomStepKey(), randomStepKey(), client); step.performAction(indexMetadata, null, null, new AsyncActionStep.Listener() { @Override public void onResponse(boolean complete) { assertThat(complete, is(true)); } @Override public void onFailure(Exception e) { } }); Mockito.verifyZeroInteractions(client); }	same deal - break out into separate pr
@Override protected IndexLifecycleExplainResponse mutateInstance(IndexLifecycleExplainResponse instance) throws IOException { String index = instance.getIndex(); String policy = instance.getPolicyName(); String phase = instance.getPhase(); String action = instance.getAction(); String step = instance.getStep(); String failedStep = instance.getFailedStep(); Boolean isTransitiveError = instance.isTransitiveError(); Integer failedStepRetryCount = instance.getFailedStepRetryCount(); Long policyTime = instance.getLifecycleDate(); Long phaseTime = instance.getPhaseTime(); Long actionTime = instance.getActionTime(); Long stepTime = instance.getStepTime(); boolean managed = instance.managedByILM(); BytesReference stepInfo = instance.getStepInfo(); PhaseExecutionInfo phaseExecutionInfo = instance.getPhaseExecutionInfo(); if (managed) { switch (between(0, 11)) { case 0: index = index + randomAlphaOfLengthBetween(1, 5); break; case 1: policy = policy + randomAlphaOfLengthBetween(1, 5); break; case 2: phase = randomAlphaOfLengthBetween(1, 5); action = randomAlphaOfLengthBetween(1, 5); step = randomAlphaOfLengthBetween(1, 5); break; case 3: phaseTime = randomValueOtherThan(phaseTime, () -> randomLongBetween(0, 100000)); break; case 4: actionTime = randomValueOtherThan(actionTime, () -> randomLongBetween(0, 100000)); break; case 5: stepTime = randomValueOtherThan(stepTime, () -> randomLongBetween(0, 100000)); break; case 6: if (Strings.hasLength(failedStep) == false) { failedStep = randomAlphaOfLength(10); } else if (randomBoolean()) { failedStep = failedStep + randomAlphaOfLengthBetween(1, 5); } else { failedStep = null; } break; case 7: policyTime = randomValueOtherThan(policyTime, () -> randomLongBetween(0, 100000)); break; case 8: if (Strings.hasLength(stepInfo) == false) { stepInfo = new BytesArray(randomByteArrayOfLength(100)); } else if (randomBoolean()) { stepInfo = randomValueOtherThan(stepInfo, () -> new BytesArray(new RandomStepInfo(() -> randomAlphaOfLength(10)).toString())); } else { stepInfo = null; } break; case 9: phaseExecutionInfo = randomValueOtherThan(phaseExecutionInfo, () -> PhaseExecutionInfoTests.randomPhaseExecutionInfo("")); break; case 10: return IndexLifecycleExplainResponse.newUnmanagedIndexResponse(index); case 11: isTransitiveError = true; failedStepRetryCount = randomInt(13); break; default: throw new AssertionError("Illegal randomisation branch"); } return IndexLifecycleExplainResponse.newManagedIndexResponse(index, policy, policyTime, phase, action, step, failedStep, isTransitiveError, failedStepRetryCount, phaseTime, actionTime, stepTime, stepInfo, phaseExecutionInfo); } else { switch (between(0, 1)) { case 0: return IndexLifecycleExplainResponse.newUnmanagedIndexResponse(index + randomAlphaOfLengthBetween(1, 5)); case 1: return randomManagedIndexExplainResponse(); default: throw new AssertionError("Illegal randomisation branch"); } } }	this could randomly cause a test failure if the count were the same, it should use suggestion failedstepretrycount = randomvalueotherthan(failedstepretrycount, () -> randomint(13));
private static IndexLifecycleExplainResponse randomManagedIndexExplainResponse() { boolean stepNull = randomBoolean(); return IndexLifecycleExplainResponse.newManagedIndexResponse(randomAlphaOfLength(10), randomAlphaOfLength(10), randomBoolean() ? null : randomNonNegativeLong(), stepNull ? null : randomAlphaOfLength(10), stepNull ? null : randomAlphaOfLength(10), stepNull ? null : randomAlphaOfLength(10), randomBoolean() ? null : randomAlphaOfLength(10), stepNull ? null : randomBoolean(), stepNull ? null : randomInt(15), stepNull ? null : randomNonNegativeLong(), stepNull ? null : randomNonNegativeLong(), stepNull ? null : randomNonNegativeLong(), randomBoolean() ? null : new BytesArray(new RandomStepInfo(() -> randomAlphaOfLength(10)).toString()), randomBoolean() ? null : PhaseExecutionInfoTests.randomPhaseExecutionInfo("")); }	because this isn't arbitrary (as opposed to, for example, the 10 in randomalphaoflength(10) above) this should be made a constant instead of a magic number.
@Override protected OpenFollowerIndexStep copyInstance(OpenFollowerIndexStep instance) { return new OpenFollowerIndexStep(instance.getKey(), instance.getNextStepKey(), instance.getClient()); }	same comment above about this being in a separate pr
public void testRolloverAlreadyExists() throws Exception { String originalIndex = index + "-000001"; String secondIndex = index + "-000002"; createIndexWithSettings(originalIndex, Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put(RolloverAction.LIFECYCLE_ROLLOVER_ALIAS, "alias") .put(LifecycleSettings.LIFECYCLE_MAX_FAILED_STEP_RETRIES_COUNT, 1) ); Request updateLifecylePollSetting = new Request("PUT", "_cluster/settings"); updateLifecylePollSetting.setJsonEntity("{" + " \\"transient\\": {\\n" + " \\"indices.lifecycle.poll_interval\\" : \\"1s\\" \\n" + " }\\n" + "}"); client().performRequest(updateLifecylePollSetting); // create policy createNewSingletonPolicy("hot", new RolloverAction(null, null, 1L)); // update policy on index updatePolicy(originalIndex, policy); // Manually create the new index Request request = new Request("PUT", "/" + secondIndex); request.setJsonEntity("{\\n \\"settings\\": " + Strings.toString(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).build()) + "}"); client().performRequest(request); // wait for the shards to initialize ensureGreen(secondIndex); // index another doc to trigger the policy index(client(), originalIndex, "_id", "foo", "bar"); assertBusy(() -> { logger.info(originalIndex + ": " + getStepKeyForIndex(originalIndex)); logger.info(secondIndex + ": " + getStepKeyForIndex(secondIndex)); assertThat(getStepKeyForIndex(originalIndex), equalTo(new StepKey("hot", RolloverAction.NAME, ErrorStep.NAME))); assertThat(getFailedStepForIndex(originalIndex), equalTo(WaitForRolloverReadyStep.NAME)); assertThat(getReasonForIndex(originalIndex), containsString("already exists")); }); }	this (and the later one) shouldn't be necessary as we set this via gradle for this test suite: https://github.com/elastic/elasticsearch/blob/7e06888bae01b138604faf37c6294ab857231b1e/x-pack/plugin/ilm/qa/multi-node/build.gradle#l29
private void onErrorMaybeRetryFailedStep(String policy, IndexMetaData indexMetaData) { String index = indexMetaData.getIndex().getName(); LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(indexMetaData); Step failedStep = stepRegistry.getStep(indexMetaData, new StepKey(lifecycleState.getPhase(), lifecycleState.getAction(), lifecycleState.getFailedStep())); if (failedStep == null) { logger.warn("failed step [{}] is not part of policy [{}] anymore, or it is invalid. skipping execution", lifecycleState.getFailedStep(), policy); return; } if (failedStep.isRetryable()) { if (lifecycleState.isTransitiveError()) { int retryCount = lifecycleState.getFailedStepRetryCount() == null ? 0 : lifecycleState.getFailedStepRetryCount(); Integer maxRetriesCount = indexMetaData.getSettings().getAsInt(LifecycleSettings.LIFECYCLE_MAX_FAILED_STEP_RETRIES_COUNT, 15); if (++retryCount > maxRetriesCount) { logger.debug("maximum retries [{}] reached for step [{}] on policy [{}] for index [{}], skipping retry execution", maxRetriesCount, lifecycleState.getFailedStep(), policy, index); } else { logger.info("policy [{}] for index [{}] on an error step due to a transitive error, moving back to the failed " + "step [{}] for execution. retry attempt [{}]", policy, index, lifecycleState.getFailedStep(), retryCount); clusterService.submitStateUpdateTask("ilm-retry-failed-step", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { return moveClusterStateToRetryFailedStep(currentState, index); } @Override public void onFailure(String source, Exception e) { logger.error("retry execution of step [{}] failed due to [{}]", failedStep.getKey().getName(), e); } }); } } else { logger.debug("policy [{}] for index [{}] on an error step after a terminal error, skipping execution", policy, index); } } else { logger.debug("policy [{}] for index [{}] on an error step, skipping execution", policy, index); } }	this log message should specify the index in question.
private void onErrorMaybeRetryFailedStep(String policy, IndexMetaData indexMetaData) { String index = indexMetaData.getIndex().getName(); LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(indexMetaData); Step failedStep = stepRegistry.getStep(indexMetaData, new StepKey(lifecycleState.getPhase(), lifecycleState.getAction(), lifecycleState.getFailedStep())); if (failedStep == null) { logger.warn("failed step [{}] is not part of policy [{}] anymore, or it is invalid. skipping execution", lifecycleState.getFailedStep(), policy); return; } if (failedStep.isRetryable()) { if (lifecycleState.isTransitiveError()) { int retryCount = lifecycleState.getFailedStepRetryCount() == null ? 0 : lifecycleState.getFailedStepRetryCount(); Integer maxRetriesCount = indexMetaData.getSettings().getAsInt(LifecycleSettings.LIFECYCLE_MAX_FAILED_STEP_RETRIES_COUNT, 15); if (++retryCount > maxRetriesCount) { logger.debug("maximum retries [{}] reached for step [{}] on policy [{}] for index [{}], skipping retry execution", maxRetriesCount, lifecycleState.getFailedStep(), policy, index); } else { logger.info("policy [{}] for index [{}] on an error step due to a transitive error, moving back to the failed " + "step [{}] for execution. retry attempt [{}]", policy, index, lifecycleState.getFailedStep(), retryCount); clusterService.submitStateUpdateTask("ilm-retry-failed-step", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { return moveClusterStateToRetryFailedStep(currentState, index); } @Override public void onFailure(String source, Exception e) { logger.error("retry execution of step [{}] failed due to [{}]", failedStep.getKey().getName(), e); } }); } } else { logger.debug("policy [{}] for index [{}] on an error step after a terminal error, skipping execution", policy, index); } } else { logger.debug("policy [{}] for index [{}] on an error step, skipping execution", policy, index); } }	this hopefully would never be null, but maybe we should check for it?
private void onErrorMaybeRetryFailedStep(String policy, IndexMetaData indexMetaData) { String index = indexMetaData.getIndex().getName(); LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(indexMetaData); Step failedStep = stepRegistry.getStep(indexMetaData, new StepKey(lifecycleState.getPhase(), lifecycleState.getAction(), lifecycleState.getFailedStep())); if (failedStep == null) { logger.warn("failed step [{}] is not part of policy [{}] anymore, or it is invalid. skipping execution", lifecycleState.getFailedStep(), policy); return; } if (failedStep.isRetryable()) { if (lifecycleState.isTransitiveError()) { int retryCount = lifecycleState.getFailedStepRetryCount() == null ? 0 : lifecycleState.getFailedStepRetryCount(); Integer maxRetriesCount = indexMetaData.getSettings().getAsInt(LifecycleSettings.LIFECYCLE_MAX_FAILED_STEP_RETRIES_COUNT, 15); if (++retryCount > maxRetriesCount) { logger.debug("maximum retries [{}] reached for step [{}] on policy [{}] for index [{}], skipping retry execution", maxRetriesCount, lifecycleState.getFailedStep(), policy, index); } else { logger.info("policy [{}] for index [{}] on an error step due to a transitive error, moving back to the failed " + "step [{}] for execution. retry attempt [{}]", policy, index, lifecycleState.getFailedStep(), retryCount); clusterService.submitStateUpdateTask("ilm-retry-failed-step", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { return moveClusterStateToRetryFailedStep(currentState, index); } @Override public void onFailure(String source, Exception e) { logger.error("retry execution of step [{}] failed due to [{}]", failedStep.getKey().getName(), e); } }); } } else { logger.debug("policy [{}] for index [{}] on an error step after a terminal error, skipping execution", policy, index); } } else { logger.debug("policy [{}] for index [{}] on an error step, skipping execution", policy, index); } }	this shouldn't specify a default since a default is already specified in the setting itself (otherwise we have to change two places if it's changed in the future)
private void onErrorMaybeRetryFailedStep(String policy, IndexMetaData indexMetaData) { String index = indexMetaData.getIndex().getName(); LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(indexMetaData); Step failedStep = stepRegistry.getStep(indexMetaData, new StepKey(lifecycleState.getPhase(), lifecycleState.getAction(), lifecycleState.getFailedStep())); if (failedStep == null) { logger.warn("failed step [{}] is not part of policy [{}] anymore, or it is invalid. skipping execution", lifecycleState.getFailedStep(), policy); return; } if (failedStep.isRetryable()) { if (lifecycleState.isTransitiveError()) { int retryCount = lifecycleState.getFailedStepRetryCount() == null ? 0 : lifecycleState.getFailedStepRetryCount(); Integer maxRetriesCount = indexMetaData.getSettings().getAsInt(LifecycleSettings.LIFECYCLE_MAX_FAILED_STEP_RETRIES_COUNT, 15); if (++retryCount > maxRetriesCount) { logger.debug("maximum retries [{}] reached for step [{}] on policy [{}] for index [{}], skipping retry execution", maxRetriesCount, lifecycleState.getFailedStep(), policy, index); } else { logger.info("policy [{}] for index [{}] on an error step due to a transitive error, moving back to the failed " + "step [{}] for execution. retry attempt [{}]", policy, index, lifecycleState.getFailedStep(), retryCount); clusterService.submitStateUpdateTask("ilm-retry-failed-step", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { return moveClusterStateToRetryFailedStep(currentState, index); } @Override public void onFailure(String source, Exception e) { logger.error("retry execution of step [{}] failed due to [{}]", failedStep.getKey().getName(), e); } }); } } else { logger.debug("policy [{}] for index [{}] on an error step after a terminal error, skipping execution", policy, index); } } else { logger.debug("policy [{}] for index [{}] on an error step, skipping execution", policy, index); } }	this log message should specify the index in question.
private void onErrorMaybeRetryFailedStep(String policy, IndexMetaData indexMetaData) { String index = indexMetaData.getIndex().getName(); LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(indexMetaData); Step failedStep = stepRegistry.getStep(indexMetaData, new StepKey(lifecycleState.getPhase(), lifecycleState.getAction(), lifecycleState.getFailedStep())); if (failedStep == null) { logger.warn("failed step [{}] is not part of policy [{}] anymore, or it is invalid. skipping execution", lifecycleState.getFailedStep(), policy); return; } if (failedStep.isRetryable()) { if (lifecycleState.isTransitiveError()) { int retryCount = lifecycleState.getFailedStepRetryCount() == null ? 0 : lifecycleState.getFailedStepRetryCount(); Integer maxRetriesCount = indexMetaData.getSettings().getAsInt(LifecycleSettings.LIFECYCLE_MAX_FAILED_STEP_RETRIES_COUNT, 15); if (++retryCount > maxRetriesCount) { logger.debug("maximum retries [{}] reached for step [{}] on policy [{}] for index [{}], skipping retry execution", maxRetriesCount, lifecycleState.getFailedStep(), policy, index); } else { logger.info("policy [{}] for index [{}] on an error step due to a transitive error, moving back to the failed " + "step [{}] for execution. retry attempt [{}]", policy, index, lifecycleState.getFailedStep(), retryCount); clusterService.submitStateUpdateTask("ilm-retry-failed-step", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { return moveClusterStateToRetryFailedStep(currentState, index); } @Override public void onFailure(String source, Exception e) { logger.error("retry execution of step [{}] failed due to [{}]", failedStep.getKey().getName(), e); } }); } } else { logger.debug("policy [{}] for index [{}] on an error step after a terminal error, skipping execution", policy, index); } } else { logger.debug("policy [{}] for index [{}] on an error step, skipping execution", policy, index); } }	doesn't this update task need a clusterstateprocessed() method that calls mayberunasyncaction? like here: https://github.com/elastic/elasticsearch/blob/96b4f3df9067d579bf7e22e09613ea94f4f05462/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/transportretryaction.java#l69-l80 or am i missing where we call mayberunasyncaction?
public void runPolicyAfterStateChange(String policy, IndexMetaData indexMetaData) { String index = indexMetaData.getIndex().getName(); LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(indexMetaData); Step currentStep = getCurrentStep(stepRegistry, policy, indexMetaData, lifecycleState); if (currentStep == null) { if (stepRegistry.policyExists(policy) == false) { markPolicyDoesNotExist(policy, indexMetaData.getIndex(), lifecycleState); return; } else { logger.error("current step [{}] for index [{}] with policy [{}] is not recognized", getCurrentStepKey(lifecycleState), index, policy); return; } } if (currentStep instanceof TerminalPolicyStep) { logger.debug("policy [{}] for index [{}] complete, skipping execution", policy, index); return; } else if (currentStep instanceof ErrorStep) { onErrorMaybeRetryFailedStep(policy, indexMetaData); return; } logger.trace("[{}] maybe running step ({}) after state change: {}", index, currentStep.getClass().getSimpleName(), currentStep.getKey()); if (currentStep instanceof PhaseCompleteStep) { // Only proceed to the next step if enough time has elapsed to go into the next phase if (isReadyToTransitionToThisPhase(policy, indexMetaData, currentStep.getNextStepKey().getPhase())) { moveToStep(indexMetaData.getIndex(), policy, currentStep.getKey(), currentStep.getNextStepKey()); } } else if (currentStep instanceof ClusterStateActionStep || currentStep instanceof ClusterStateWaitStep) { logger.debug("[{}] running policy with current-step [{}]", indexMetaData.getIndex().getName(), currentStep.getKey()); clusterService.submitStateUpdateTask("ilm-execute-cluster-state-steps", new ExecuteStepsUpdateTask(policy, indexMetaData.getIndex(), currentStep, stepRegistry, this, nowSupplier)); } else { logger.trace("[{}] ignoring step execution from cluster state change event [{}]", index, currentStep.getKey()); } } /** * Retrieves the current {@link StepKey} from the index settings. Note that * it is illegal for the step to be set with the phase and/or action unset, * or for the step to be unset with the phase and/or action set. All three * settings must be either present or missing. * * @param lifecycleState the index custom data to extract the {@link StepKey}	i actually think we should go back to just skipping it here. we can leave automatic ilm retries happening *only* on the periodic ilm interval (every 10 minutes by default) rather than every time state changes. this will work for our "backoff" of automatic retries.
* @param stepRegistry The steps registry to check a step-key's existence in the index's current policy * @param forcePhaseDefinitionRefresh When true, step information will be recompiled from the latest version of the * policy. Otherwise, existing phase definition is used. * @return The updated cluster state where the index moved to <code>nextStepKey</code> */ static ClusterState moveClusterStateToStep(String indexName, ClusterState currentState, StepKey currentStepKey, StepKey nextStepKey, LongSupplier nowSupplier, PolicyStepsRegistry stepRegistry, boolean forcePhaseDefinitionRefresh) { validateTransition(indexName, currentState, currentStepKey, nextStepKey, stepRegistry); IndexMetaData idxMeta = currentState.getMetaData().index(indexName); Settings indexSettings = idxMeta.getSettings(); String policy = LifecycleSettings.LIFECYCLE_NAME_SETTING.get(indexSettings); logger.info("moving index [{}] from [{}] to [{}] in policy [{}]", indexName, currentStepKey, nextStepKey, policy); return IndexLifecycleRunner.moveClusterStateToNextStep(idxMeta.getIndex(), currentState, currentStepKey, nextStepKey, nowSupplier, forcePhaseDefinitionRefresh); }	do we have tests for this? would you mind making this package-visible and adding a unit test for its validation? (can be done in a separate pr)
static ClusterState moveClusterStateToErrorStep(Index index, ClusterState clusterState, StepKey currentStep, Exception cause, LongSupplier nowSupplier, BiFunction<IndexMetaData, StepKey, Step> stepLookupFunction) throws IOException { IndexMetaData idxMeta = clusterState.getMetaData().index(index); IndexLifecycleMetadata ilmMeta = clusterState.metaData().custom(IndexLifecycleMetadata.TYPE); LifecyclePolicyMetadata policyMetadata = ilmMeta.getPolicyMetadatas() .get(LifecycleSettings.LIFECYCLE_NAME_SETTING.get(idxMeta.getSettings())); XContentBuilder causeXContentBuilder = JsonXContent.contentBuilder(); causeXContentBuilder.startObject(); ElasticsearchException.generateThrowableXContent(causeXContentBuilder, STACKTRACE_PARAMS, cause); causeXContentBuilder.endObject(); LifecycleExecutionState currentState = LifecycleExecutionState.fromIndexMetadata(idxMeta); LifecycleExecutionState nextStepState = moveExecutionStateToNextStep(policyMetadata, currentState, currentStep, new StepKey(currentStep.getPhase(), currentStep.getAction(), ErrorStep.NAME), nowSupplier, false); LifecycleExecutionState.Builder failedState = LifecycleExecutionState.builder(nextStepState); failedState.setFailedStep(currentStep.getName()); failedState.setStepInfo(BytesReference.bytes(causeXContentBuilder).utf8ToString()); Step failedStep = stepLookupFunction.apply(idxMeta, currentStep); // as an initial step we'll mark the failed step with a transitive error without actually looking at the cause to determine // if it's a transitive (ie. recoverable from) error failedState.setIsTransitiveError(failedStep.isRetryable()); // maintain the retry count of the failed step as it will be cleared after a successful execution failedState.setFailedStepRetryCount(currentState.getFailedStepRetryCount()); ClusterState.Builder newClusterStateBuilder = newClusterStateWithLifecycleState(index, clusterState, failedState.build()); return newClusterStateBuilder.build(); }	i think it's getting confusing having moveclusterstatetoerrorstep and moveclusterstatetofailedstep. the names are very similar and it's difficult to tell at a glance how the two differ. how about we rename this to something like moveclusterstateoutoferrorstep so it makes it clearer that one is entering the error state and the other is exiting the error state?
public void testMoveClusterStateToRetryFailedStep() { String indexName = "my_index"; String policyName = "my_policy"; long now = randomNonNegativeLong(); StepKey failedStepKey = new StepKey("current_phase", MockAction.NAME, "current_step"); StepKey errorStepKey = new StepKey(failedStepKey.getPhase(), failedStepKey.getAction(), ErrorStep.NAME); Step retryableStep = new RetryableMockStep(failedStepKey, null); LifecyclePolicy policy = createPolicy(policyName, failedStepKey, null); LifecyclePolicyMetadata policyMetadata = new LifecyclePolicyMetadata(policy, Collections.emptyMap(), randomNonNegativeLong(), randomNonNegativeLong()); PolicyStepsRegistry policyRegistry = createOneStepPolicyStepRegistry(policyName, retryableStep, indexName); Settings.Builder indexSettingsBuilder = Settings.builder() .put(LifecycleSettings.LIFECYCLE_NAME, policyName); LifecycleExecutionState.Builder lifecycleState = LifecycleExecutionState.builder(); lifecycleState.setPhase(errorStepKey.getPhase()); lifecycleState.setPhaseTime(now); lifecycleState.setAction(errorStepKey.getAction()); lifecycleState.setActionTime(now); lifecycleState.setStep(errorStepKey.getName()); lifecycleState.setStepTime(now); lifecycleState.setFailedStep(failedStepKey.getName()); ClusterState clusterState = buildClusterState(indexName, indexSettingsBuilder, lifecycleState.build(), Collections.singletonList(policyMetadata)); Index index = clusterState.metaData().index(indexName).getIndex(); IndexLifecycleRunner runner = new IndexLifecycleRunner(policyRegistry, null, threadPool, () -> now); ClusterState nextClusterState = runner.moveClusterStateToRetryFailedStep(clusterState, indexName); IndexLifecycleRunnerTests.assertClusterStateOnNextStep(clusterState, index, errorStepKey, failedStepKey, nextClusterState, now); LifecycleExecutionState executionState = LifecycleExecutionState.fromIndexMetadata(nextClusterState.metaData().index(indexName)); assertThat(executionState.getFailedStepRetryCount(), is(1)); }	minor nit, but can you move this to the end of the file so it's not interspersed with the other test* tests?
@Override public String toString() { StringBuilder sb = new StringBuilder(); if (nodeName.length() > 0) { sb.append('{').append(nodeName).append('}'); } sb.append('{').append(nodeId).append('}'); sb.append('{').append(ephemeralId).append('}'); sb.append('{').append(hostName).append('}'); sb.append('{').append(address).append('}'); if (!attributes.isEmpty()) { sb.append(attributes); } return sb.toString(); }	why did you change this?
public String[] resolveNodesIds(String... nodesIds) { if (isAllNodes(nodesIds)) { int index = 0; nodesIds = new String[nodes.size()]; for (DiscoveryNode node : this) { nodesIds[index++] = node.getId(); } return nodesIds; } else { ObjectHashSet<String> resolvedNodesIds = new ObjectHashSet<>(nodesIds.length); for (String nodeId : nodesIds) { if (nodeId.equals("_local")) { String localNodeId = getLocalNodeId(); if (localNodeId != null) { resolvedNodesIds.add(localNodeId); } } else if (nodeId.equals("_master")) { String masterNodeId = getMasterNodeId(); if (masterNodeId != null) { resolvedNodesIds.add(masterNodeId); } } else if (nodeExists(nodeId)) { resolvedNodesIds.add(nodeId); } else { boolean matchedEphemeralId = false; for (DiscoveryNode node : this) { if (node.getEphemeralId().equals(nodeId)) { matchedEphemeralId = true; resolvedNodesIds.add(node.getId()); break; // persistent node id is unique } } if (matchedEphemeralId == false) { // neither ephemeral nor persistent node id, try and search by name for (DiscoveryNode node : this) { if (Regex.simpleMatch(nodeId, node.getName())) { resolvedNodesIds.add(node.getId()); } } for (DiscoveryNode node : this) { if (Regex.simpleMatch(nodeId, node.getName())) { resolvedNodesIds.add(node.getId()); } } for (DiscoveryNode node : this) { if (Regex.simpleMatch(nodeId, node.getHostAddress())) { resolvedNodesIds.add(node.getId()); } else if (Regex.simpleMatch(nodeId, node.getHostName())) { resolvedNodesIds.add(node.getId()); } } int index = nodeId.indexOf(':'); if (index != -1) { String matchAttrName = nodeId.substring(0, index); String matchAttrValue = nodeId.substring(index + 1); if (DiscoveryNode.Role.DATA.getRoleName().equals(matchAttrName)) { if (Booleans.parseBoolean(matchAttrValue, true)) { resolvedNodesIds.addAll(dataNodes.keys()); } else { resolvedNodesIds.removeAll(dataNodes.keys()); } } else if (DiscoveryNode.Role.MASTER.getRoleName().equals(matchAttrName)) { if (Booleans.parseBoolean(matchAttrValue, true)) { resolvedNodesIds.addAll(masterNodes.keys()); } else { resolvedNodesIds.removeAll(masterNodes.keys()); } } else if (DiscoveryNode.Role.INGEST.getRoleName().equals(matchAttrName)) { if (Booleans.parseBoolean(matchAttrValue, true)) { resolvedNodesIds.addAll(ingestNodes.keys()); } else { resolvedNodesIds.removeAll(ingestNodes.keys()); } } else { for (DiscoveryNode node : this) { for (Map.Entry<String, String> entry : node.getAttributes().entrySet()) { String attrName = entry.getKey(); String attrValue = entry.getValue(); if (Regex.simpleMatch(matchAttrName, attrName) && Regex.simpleMatch(matchAttrValue, attrValue)) { resolvedNodesIds.add(node.getId()); } } } } } } } } return resolvedNodesIds.toArray(String.class); } }	why did add this extra resolvenode path? isn't ephemeralid supposed to be internal and shouldn't be used in an api?
public void run() { listener.offMaster(); } } private static class DelegetingAckListener implements Discovery.AckListener { final private List<Discovery.AckListener> listeners; private DelegetingAckListener(List<Discovery.AckListener> listeners) { this.listeners = listeners; } @Override public void onNodeAck(DiscoveryNode node, @Nullable Throwable t) { for (Discovery.AckListener listener : listeners) { listener.onNodeAck(node, t); } } @Override public void onTimeout() { throw new UnsupportedOperationException("no timeout delegation"); } } private static class AckCountDownListener implements Discovery.AckListener { private static final ESLogger logger = Loggers.getLogger(AckCountDownListener.class); private final AckedClusterStateTaskListener ackedTaskListener; private final CountDown countDown; private final DiscoveryNodes nodes; private final long clusterStateVersion; private final Future<?> ackTimeoutCallback; private Throwable lastFailure; AckCountDownListener(AckedClusterStateTaskListener ackedTaskListener, long clusterStateVersion, DiscoveryNodes nodes, ThreadPool threadPool) { this.ackedTaskListener = ackedTaskListener; this.clusterStateVersion = clusterStateVersion; this.nodes = nodes; int countDown = 0; for (DiscoveryNode node : nodes) { if (ackedTaskListener.mustAck(node)) { countDown++; } } //we always wait for at least 1 node (the master) countDown = Math.max(1, countDown); logger.trace("expecting {} acknowledgements for cluster_state update (version: {})", countDown, clusterStateVersion); this.countDown = new CountDown(countDown); this.ackTimeoutCallback = threadPool.schedule(ackedTaskListener.ackTimeout(), ThreadPool.Names.GENERIC, new Runnable() { @Override public void run() { onTimeout(); } }); } @Override public void onNodeAck(DiscoveryNode node, @Nullable Throwable t) { if (!ackedTaskListener.mustAck(node)) { //we always wait for the master ack anyway DiscoveryNode masterNode = nodes.getMasterNode(); if (masterNode == null || node.getId().equals(masterNode.getId()) == false || node.getEphemeralId().equals(masterNode.getEphemeralId()) == false) { return; } } if (t == null) { logger.trace("ack received from node [{}], cluster_state update (version: {})", node, clusterStateVersion); } else { this.lastFailure = t; logger.debug("ack received from node [{}], cluster_state update (version: {})", t, node, clusterStateVersion); } if (countDown.countDown()) { logger.trace("all expected nodes acknowledged cluster_state update (version: {})", clusterStateVersion); FutureUtils.cancel(ackTimeoutCallback); ackedTaskListener.onAllNodesAcked(lastFailure); } } @Override public void onTimeout() { if (countDown.fastForward()) { logger.trace("timeout waiting for acknowledgement for cluster_state update (version: {})", clusterStateVersion); ackedTaskListener.onAckTimeout(); } } }	what was the rational behind this change? equals checks for all of these no?
public Engine.Operation convertToEngineOp(Translog.Operation operation, Engine.Operation.Origin origin) { return translogOpToEngineOpConverter.convertToEngineOp(operation, origin); }	any chance we can add a test for this to indexshardtests
public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final Set<String> indicesToDisplay = new HashSet<>(); final Set<String> returnedAliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : response.getAliases()) { for (final AliasMetaData aliasMetaData : cursor.value) { if (namesProvided) { // only display indices that have aliases indicesToDisplay.add(cursor.key); } returnedAliasNames.add(aliasMetaData.alias()); } } // compute explicitly requested aliases that have are not returned in the result final SortedSet<String> missingAliases = new TreeSet<>(); // first wildcard index, leading "-" as an alias name after this index means // that it is an exclusion int firstWildcardIndex = aliases.length; for (int i = 0; i < aliases.length; i++) { if (Regex.isSimpleMatchPattern(aliases[i])) { firstWildcardIndex = i; break; } } for (int i = 0; i < aliases.length; i++) { if (MetaData.ALL.equals(aliases[i]) || Regex.isSimpleMatchPattern(aliases[i]) || (i > firstWildcardIndex && aliases[i].charAt(0) == '-')) { // only explicitly requested aliases will be called out as missing (404) continue; } // check if aliases[i] is subsequently excluded int j = Math.max(i + 1, firstWildcardIndex); for (; j < aliases.length; j++) { if (aliases[j].charAt(0) == '-') { // this is an exclude pattern if (Regex.simpleMatch(aliases[j].substring(1), aliases[i]) || MetaData.ALL.equals(aliases[j].substring(1))) { // aliases[i] is excluded by aliases[j] break; } } } if (j == aliases.length) { // explicitly requested aliases[i] is not excluded by any subsequent "-" wildcard in expression if (false == returnedAliasNames.contains(aliases[i])) { // aliases[i] is not in the result set missingAliases.add(aliases[i]); } } } final RestStatus status; builder.startObject(); { if (missingAliases.isEmpty()) { status = RestStatus.OK; } else { status = RestStatus.NOT_FOUND; final String message; if (missingAliases.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", Strings.collectionToCommaDelimitedString(missingAliases)); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", Strings.collectionToCommaDelimitedString(missingAliases)); } builder.field("error", message); builder.field("status", status.getStatus()); } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { if (namesProvided == false || (namesProvided && indicesToDisplay.contains(entry.key))) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } } builder.endObject(); return new BytesRestResponse(status, builder); }	it would be great to have a unit test for this code. otherwise we only rely on yaml tests to test it. could be done as a followup.
private static CheckedFunction<XContentParser, String, IOException> ignoreUseFieldMappingStringParser() { return (p) -> { if (p.currentToken() == XContentParser.Token.VALUE_NULL) { return null; } else { String text = p.text(); if (text.equals("use_field_mapping")) { DEPRECATION_LOGGER.compatibleApiWarning("explicit_default_format", "[" + USE_DEFAULT_FORMAT + "] is a special format that was only used to " + "ease the transition to 7.x. It has become the default and shouldn't be set explicitly anymore."); return null; } else { return text; } } }; } /** * Parse a {@link FieldAndFormat} from some {@link XContent}	small comment, could use use_default_format constant here.
private void expandDots() throws IOException { String field = delegate().currentName(); String[] subpaths = splitAndValidatePath(field); if (subpaths.length == 0) { throw new IllegalArgumentException("field name cannot contain only dots: [" + field + "]"); } if (subpaths.length == 1) { return; } Token token = delegate().nextToken(); if (token == Token.START_OBJECT || token == Token.START_ARRAY) { parsers.push(new DotExpandingXContentParser(new XContentSubParser(delegate()), delegate(), subpaths)); } else if (token == Token.END_OBJECT || token == Token.END_ARRAY) { throw new IllegalStateException("Expecting START_OBJECT or START_ARRAY or VALUE but got [" + token + "]"); } else { parsers.push(new DotExpandingXContentParser(new SingletonValueXContentParser(delegate()), delegate(), subpaths)); } }	@romseygeek this check is redundant now?
private static String[] splitAndValidatePath(String fullFieldPath) { if (fullFieldPath.isEmpty()) { throw new IllegalArgumentException("field name cannot be an empty string"); } if (fullFieldPath.contains(".") == false) { return new String[] { fullFieldPath }; } String[] parts = fullFieldPath.split("\\\\."); if (parts.length == 0) { throw new IllegalArgumentException("field name cannot contain only dots"); } for (String part : parts) { // check if the field name contains only whitespace if (part.isEmpty()) { throw new IllegalArgumentException("object field cannot contain only whitespace: ['" + fullFieldPath + "']"); } if (part.isBlank()) { throw new IllegalArgumentException( "object field starting or ending with a [.] makes object resolution ambiguous: [" + fullFieldPath + "]" ); } } return parts; }	@romseygeek is the mention of "object" in this error and the next one accurate? don't we split any path regardless of the token we are reading from the parser?
public void apply(Project project) { project.getPluginManager().apply(TestClustersPlugin.class); project.getPluginManager().apply(ElasticsearchTestBasePlugin.class); project.getTasks().withType(RestIntegTestTask.class).configureEach(restIntegTestTask -> { @SuppressWarnings("unchecked") NamedDomainObjectContainer<ElasticsearchCluster> testClusters = (NamedDomainObjectContainer<ElasticsearchCluster>) project .getExtensions() .getByName(TestClustersPlugin.EXTENSION_NAME); ElasticsearchCluster cluster = testClusters.maybeCreate(restIntegTestTask.getName()); restIntegTestTask.useCluster(cluster); restIntegTestTask.include("**/*IT.class"); restIntegTestTask.systemProperty("tests.rest.load_packaged", Boolean.FALSE.toString()); if (System.getProperty(TESTS_REST_CLUSTER) == null) { if (System.getProperty(TESTS_CLUSTER) != null || System.getProperty(TESTS_CLUSTER_NAME) != null) { throw new IllegalArgumentException( String.format("%s, %s, and %s must all be null or non-null", TESTS_REST_CLUSTER, TESTS_CLUSTER, TESTS_CLUSTER_NAME) ); } SystemPropertyCommandLineArgumentProvider runnerNonInputProperties = (SystemPropertyCommandLineArgumentProvider) restIntegTestTask.getExtensions().getByName("nonInputProperties"); runnerNonInputProperties.systemProperty(TESTS_REST_CLUSTER, () -> String.join(",", cluster.getAllHttpSocketURI())); runnerNonInputProperties.systemProperty(TESTS_CLUSTER, () -> String.join(",", cluster.getAllTransportPortURI())); runnerNonInputProperties.systemProperty(TESTS_CLUSTER_NAME, cluster::getName); } else { if (System.getProperty(TESTS_CLUSTER) == null || System.getProperty(TESTS_CLUSTER_NAME) == null) { throw new IllegalArgumentException( String.format("%s, %s, and %s must all be null or non-null", TESTS_REST_CLUSTER, TESTS_CLUSTER, TESTS_CLUSTER_NAME) ); } } }); }	when will this have not already been created? shouldn't the java/yaml plugins have already created it?
private static IntervalDayTime truncateIntervalSmallerThanWeek(IntervalDayTime r, ChronoUnit unit) { Duration d = r.interval(); int isNegative = 1; if (d.isNegative()) { d = d.negated(); isNegative = -1; } long durationInSec = d.getSeconds(); long day = durationInSec / SECONDS_PER_DAY; durationInSec = durationInSec % SECONDS_PER_DAY; long hour = durationInSec / SECONDS_PER_HOUR; durationInSec = durationInSec % SECONDS_PER_HOUR; long min = durationInSec / SECONDS_PER_MINUTE; durationInSec = durationInSec % SECONDS_PER_MINUTE; long sec = durationInSec; long miliseccond = TimeUnit.NANOSECONDS.toMillis(d.getNano()); Duration newDuration = Duration.ZERO; if (unit.ordinal() <= ChronoUnit.DAYS.ordinal()) newDuration = newDuration.plusDays(day * isNegative); if (unit.ordinal() <= ChronoUnit.HOURS.ordinal()) newDuration = newDuration.plusHours(hour * isNegative); if (unit.ordinal() <= ChronoUnit.MINUTES.ordinal()) newDuration = newDuration.plusMinutes(min * isNegative); if (unit.ordinal() <= ChronoUnit.SECONDS.ordinal()) newDuration = newDuration.plusSeconds(sec * isNegative); if (unit.ordinal() <= ChronoUnit.MILLIS.ordinal()) newDuration = newDuration.plusMillis(miliseccond * isNegative); return new IntervalDayTime(newDuration, r.dataType()); } } public DateTrunc(Source source, Expression truncateTo, Expression timestamp, ZoneId zoneId) { super(source, truncateTo, timestamp, zoneId); } @Override public DataType dataType() { if (isInterval(right().dataType())) { return right().dataType(); } return DataTypes.DATETIME; } @Override protected TypeResolution resolveType() { TypeResolution resolution = super.resolveType(); if (resolution.unresolved()) { return resolution; } resolution = isDateOrInterval(right(), sourceText(), Expressions.ParamOrdinal.SECOND); if (resolution.unresolved()) { return resolution; } return TypeResolution.TYPE_RESOLVED; } @Override protected BinaryScalarFunction replaceChildren(Expression newTruncateTo, Expression newTimestamp) { return new DateTrunc(source(), newTruncateTo, newTimestamp, zoneId()); } @Override protected NodeInfo<? extends Expression> info() { return NodeInfo.create(this, DateTrunc::new, left(), right(), zoneId()); } @Override protected String scriptMethodName() { return "dateTrunc"; } @Override public Object fold() { return DateTruncProcessor.process(left().fold(), right().fold(), zoneId()); } @Override protected Pipe createPipe(Pipe truncateTo, Pipe timestamp, ZoneId zoneId) { return new DateTruncPipe(source(), this, truncateTo, timestamp, zoneId); } @Override protected boolean resolveDateTimeField(String dateTimeField) { return Part.resolve(dateTimeField) != null; } @Override protected List<String> findSimilarDateTimeFields(String dateTimeField) { return Part.findSimilar(dateTimeField); }	please use curly brackets, even for one line statements.
private void handleLeaderCheck(LeaderCheckRequest request) { final DiscoveryNodes discoveryNodes = this.discoveryNodes; assert discoveryNodes != null; if (discoveryNodes.isLocalNodeElectedMaster() == false) { logger.debug("rejecting leader check on non-master {}", request); throw new CoordinationStateRejectedException( "rejecting leader check from [" + request.getSender() + "] sent to a node that is not the master"); } else if (discoveryNodes.nodeExists(request.getSender()) == false) { logger.debug("rejecting leader check from removed node: {}", request); throw new CoordinationStateRejectedException( "rejecting leader check since [" + request.getSender() + "] has been removed from the cluster"); } else { logger.trace("handling {}", request); } }	following the same logic you've used for the nodeexists check below, should this say "is no longer the master"?
@Deprecated public final void onModule(DiscoveryModule module) {} /** * PluginSettings allow plugins to extend the core by decaring custom settings, adding settings in addition to the defined * node level settings and allows to filter settings based on simple expression filters. * Since the settings must be present before any plugins are instantiated the PluginService will try to obtain a {@link PluginSettings} * instance in a static context. A plugin that needs to extend elasticsearch with settings must define a static method as follows: * <pre> * public static PluginSettings getPluginSettings(Settings settings) { * return new PluginSettings() {}; * } * </pre> * * Previously Plugins were able to extends settings after the plugin was instantiated via {@link Plugin#additionalSettings()}, * {@link Plugin#getSettingsFilter()} and {@link Plugin#getSettings()}	typo: decaring -> declaring
public Builder deleteJob(String jobId, PersistentTasksCustomMetaData tasks) { checkJobHasNoDatafeed(jobId); JobState jobState = MlTasks.getJobState(jobId, tasks); if (jobState.isAnyOf(JobState.CLOSED, JobState.FAILED) == false) { throw ExceptionsHelper.conflictStatusException("Unexpected job state [" + jobState + "], expected [" + JobState.CLOSED + " or " + JobState.FAILED + "]"); } Job job = jobs.remove(jobId); if (job == null) { throw new ResourceNotFoundException("job [" + jobId + "] does not exist"); } if (job.isDeleting() == false) { throw ExceptionsHelper.conflictStatusException("Cannot delete job [" + jobId + "] because it hasn't marked as deleted"); } return this; }	so, what happens when deletejob gets called twice in a row while a job is in the deleting status? on the second call, would the error not be resourcenotfoundexception("job [" + jobid + "] does not exist"); or is the private final sortedmap<string, job> jobs; repopulated somewhere?
@Override protected void masterOperation(Task task, DeleteJobAction.Request request, ClusterState state, ActionListener<AcknowledgedResponse> listener) { logger.debug("Deleting job '{}'", request.getJobId()); TaskId taskId = new TaskId(clusterService.localNode().getId(), task.getId()); ParentTaskAssigningClient parentTaskClient = new ParentTaskAssigningClient(client, taskId); // Check if there is a deletion task for this job already and if yes wait for it to complete Optional<Task> existingStartedTask; synchronized (this) { existingStartedTask = findExistingStartedTask(task); if (existingStartedTask.isPresent() == false) { ((JobDeletionTask) task).start(); } } if (existingStartedTask.isPresent()) { logger.debug("[{}] Deletion task [{}] will wait for existing deletion task [{}]", request.getJobId(), task.getId(), existingStartedTask.get().getId()); TaskId existingTaskId = new TaskId(clusterService.localNode().getId(), existingStartedTask.get().getId()); waitForExistingTaskToComplete(parentTaskClient, request.getJobId(), existingTaskId, listener); return; } auditor.info(request.getJobId(), Messages.getMessage(Messages.JOB_AUDIT_DELETING, taskId)); ActionListener<Boolean> markAsDeletingListener = ActionListener.wrap( response -> { if (request.isForce()) { forceDeleteJob(parentTaskClient, request, listener); } else { normalDeleteJob(parentTaskClient, request, listener); } }, e -> { auditor.error(request.getJobId(), Messages.getMessage(Messages.JOB_AUDIT_DELETING_FAILED, e.getMessage())); listener.onFailure(e); }); markJobAsDeleting(request.getJobId(), markAsDeletingListener, request.isForce()); }	i think with this change we no longer have a race condition, when two tasks could end up waiting for each other as we had before, but it is still very complicated (especially the waitforexistingtasktocomplete part). what would you think about having a static map of arrays of listeners here. on start, in a critical section, you can check if there is already an element with the same job id present in the map if not, you add your job id with an empty array and start delete process, if job id exists - just add yourself to the array of listeners. on completion, again in a critical section, you remove the array with your job id from the map and execute all listeners that were registered there.
@Override protected void masterOperation(Task task, DeleteJobAction.Request request, ClusterState state, ActionListener<AcknowledgedResponse> listener) { logger.debug("Deleting job '{}'", request.getJobId()); TaskId taskId = new TaskId(clusterService.localNode().getId(), task.getId()); ParentTaskAssigningClient parentTaskClient = new ParentTaskAssigningClient(client, taskId); // Check if there is a deletion task for this job already and if yes wait for it to complete Optional<Task> existingStartedTask; synchronized (this) { existingStartedTask = findExistingStartedTask(task); if (existingStartedTask.isPresent() == false) { ((JobDeletionTask) task).start(); } } if (existingStartedTask.isPresent()) { logger.debug("[{}] Deletion task [{}] will wait for existing deletion task [{}]", request.getJobId(), task.getId(), existingStartedTask.get().getId()); TaskId existingTaskId = new TaskId(clusterService.localNode().getId(), existingStartedTask.get().getId()); waitForExistingTaskToComplete(parentTaskClient, request.getJobId(), existingTaskId, listener); return; } auditor.info(request.getJobId(), Messages.getMessage(Messages.JOB_AUDIT_DELETING, taskId)); ActionListener<Boolean> markAsDeletingListener = ActionListener.wrap( response -> { if (request.isForce()) { forceDeleteJob(parentTaskClient, request, listener); } else { normalDeleteJob(parentTaskClient, request, listener); } }, e -> { auditor.error(request.getJobId(), Messages.getMessage(Messages.JOB_AUDIT_DELETING_FAILED, e.getMessage())); listener.onFailure(e); }); markJobAsDeleting(request.getJobId(), markAsDeletingListener, request.isForce()); }	this comparison seems a bit fragile. if we want to do it with task manager (see my other comment above). i would suggest adding a filter for the action name and then if action matches jobdeletion, we can cast it to jobdeletiontask and get job id from the task instead of description.
public long minDocCount() { return minDocCount; }	shouldn't this still have the @override annotation?
@Override protected final ValuesSourceAggregatorFactory doBuild(AggregationContext context, AggregatorFactory parent, Builder subFactoriesBuilder) throws IOException { ValuesSourceConfig config = resolveConfig(context); ValuesSourceAggregatorFactory factory; factory = innerBuild(context, config, parent, subFactoriesBuilder); return factory; }	i'd like to leave some version of this comment; i find it useful to note that this is our last chance to check the values source type mapping before we attempt to register usage. maybe something like /* the inner builder implementation is responsible for validating the valuessourcetype mapping, typically by checking if an aggregation supplier has been registered for that type on this aggregation, and throw illegalargumentexception if the mapping is not valid. note that we need to throw from here because abstractaggregationbuilder#build, which called this, will attempt to register the agg usage next, and if the usage is invalid that will fail with a weird error. */
@Override public boolean rebalance(RoutingAllocation allocation) { final int numberOfInFlightFetch = gatewayAllocator.getNumberOfInFlightFetch(); if (numberOfInFlightFetch == 0) { /* * see https://github.com/elastic/elasticsearch/issues/14387 * if we allow rebalance operations while we are still fetching shard store data * we might end up with unnecessary rebalance operations which can be super confusion/frustrating * since once the fetches come back we might just move all the shards back again. * Therefore we only do a rebalance if we have fetched all information. */ return allocator.rebalance(allocation); } else { logger.debug("skip rebalance [{}] shard store fetch operations are still in-flight", numberOfInFlightFetch); } return false; }	i _think_ we have a race condition here where the gateway's requests have come back before we got here on the cluster state update thread. the in flight ops will be 0 but the results are not used yet. i wonder if we should have a flag on the allocation, something like "disablerebalance" which we can set in the gateway allocator. maybe that will be simpler?
public void testRebalanceWhileShardFetching() { final AtomicInteger numFetch = new AtomicInteger(1); AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build(), new NoopGatewayAllocator() { @Override public int getNumberOfInFlightFetch() { return numFetch.get(); } }); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(2).numberOfReplicas(0)) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsNew(metaData.index("test")) .build(); ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build(); logger.info("start two nodes"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1"))).build(); routingTable = strategy.reroute(clusterState).routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); for (int i = 0; i < routingTable.index("test").shards().size(); i++) { assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(1)); assertThat(routingTable.index("test").shard(i).primaryShard().state(), equalTo(INITIALIZING)); } logger.debug("start all the primary shards for test"); RoutingNodes routingNodes = clusterState.getRoutingNodes(); routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test", INITIALIZING)).routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); for (int i = 0; i < routingTable.index("test").shards().size(); i++) { assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(1)); assertThat(routingTable.index("test").shard(i).primaryShard().state(), equalTo(STARTED)); } logger.debug("now, start 1 more node, check that rebalancing will not happen since we have shard sync going on"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()) .put(newNode("node2"))) .build(); RoutingAllocation.Result reroute = strategy.reroute(clusterState); assertFalse(reroute.changed()); numFetch.set(0); reroute = strategy.reroute(clusterState); assertTrue(reroute.changed()); routingTable = reroute.routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); int numStarted = 0; int numRelocating = 0; for (int i = 0; i < routingTable.index("test").shards().size(); i++) { assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(1)); if (routingTable.index("test").shard(i).primaryShard().state() == STARTED) { numStarted++; } else if (routingTable.index("test").shard(i).primaryShard().state() == RELOCATING) { numRelocating++; } } assertEquals(numStarted, 1); assertEquals(numRelocating, 1); }	can we insert an extra log message here? i misses the strategy.reroute(clusterstate); at first read and it drove me nuts - why is there a relocating shard? ;)
@Override protected void append(LoggingEvent event) { String message = event.getMessage().toString(); if (event.getLoggerName().equals("indices.memory")) { if (message.contains("using index_buffer_size")) { sawUsing = true; } if (message.contains("recalculating shard indexing buffer")) { sawRecalculating = true; } if (message.contains("updating [" + IndexingMemoryController.INDEX_BUFFER_SIZE + "]") && message.contains(" to [52mb]")) { sawUpdatingIndexBuffer = true; } if (message.contains("updating [" + IndexingMemoryController.MIN_SHARD_INDEX_BUFFER_SIZE + "]") && message.contains(" to [1mb]")) { sawUpdatingIndexMinShardBuffer = true; } if (message.contains("updating [" + IndexingMemoryController.MAX_SHARD_INDEX_BUFFER_SIZE + "]") && message.contains("128mb")) { sawUpdatingIndexMaxShardBuffer = true; } } }	for the test, can't we set the settings, and then assertbusy on the stats api to see that the updated index writer buffers have been applies? this will also test the actual logic to set it, which is a nice benefit, and then won't need to custom appender and depend on logging?
* @param str the String to match * @return whether the String matches the given pattern */ public static boolean simpleMatch(String pattern, String str) { if (pattern == null || str == null) { return false; } final int firstIndex = pattern.indexOf('*'); if (firstIndex == -1) { return pattern.equals(str); } if (firstIndex == 0) { if (pattern.length() == 1) { return true; } final int nextIndex = pattern.indexOf('*', firstIndex + 1); if (nextIndex == -1) { return str.endsWith(pattern.substring(1)); } else if (nextIndex == 1) { // Double wildcard "**" - skipping the first "*" return simpleMatch(pattern.substring(1), str); } final String part = pattern.substring(1, nextIndex); int partIndex = str.indexOf(part); while (partIndex != -1) { if (simpleMatch(pattern.substring(nextIndex), str.substring(partIndex + part.length()))) { return true; } partIndex = str.indexOf(part, partIndex + 1); } return false; } final String patternBeforeFirstIndex = pattern.substring(0, firstIndex); if (firstIndex == pattern.length() - 1) { return str.startsWith(patternBeforeFirstIndex); } return (str.length() >= firstIndex && patternBeforeFirstIndex.equals(str.substring(0, firstIndex)) && simpleMatch(pattern.substring(firstIndex), str.substring(firstIndex))); }	while we are micro optimizing :), why not leave this as is and also inline the pattern.substring(0, firstindex) inside the new conditional above? that way we still get the free breakout for short strings in the first part of this boolean?
public void processExistingRecoveries(RoutingAllocation allocation) { MetaData metaData = allocation.metaData(); RoutingNodes routingNodes = allocation.routingNodes(); List<Runnable> shardCancellationActions = new ArrayList<>(); for (RoutingNode routingNode : routingNodes) { for (ShardRouting shard : routingNode) { if (shard.primary()) { continue; } if (shard.initializing() == false) { continue; } if (shard.relocatingNodeId() != null) { continue; } // if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one... if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) { continue; } AsyncShardFetch.FetchResult<NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation); if (shardStores.hasData() == false) { logger.trace("{}: fetching new stores for initializing shard", shard); continue; // still fetching } ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId()); assert primaryShard != null : "the replica shard can be allocated on at least one node, so there must be an active primary"; TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryShard, allocation, shardStores); if (primaryStore == null) { // if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed) // just let the recovery find it out, no need to do anything about it for the initializing shard logger.trace("{}: no primary shard store found or allocated, letting actual allocation figure it out", shard); continue; } MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, primaryStore, shardStores, false); if (matchingNodes.getNodeWithHighestMatch() != null) { DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId()); DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch(); BooleanSupplier currentNodeCanSkipPhase1 = () -> { TransportNodesListShardStoreMetaData.StoreFilesMetaData storeMetadata = shardStores.getData().get(currentNode).storeFilesMetaData(); // current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider if (storeMetadata == null) { return false; } IndexMetaData indexMetadata = allocation.metaData().index(shard.index()); if (canPerformOperationBasedRecovery(indexMetadata, primaryStore, currentNode, storeMetadata)) { return true; } final String currentSyncId = storeMetadata.syncId(); return currentSyncId != null && currentSyncId.equals(primaryStore.syncId()); }; if (currentNode.equals(nodeWithHighestMatch) == false && matchingNodes.canSkipPhase1(nodeWithHighestMatch) && currentNodeCanSkipPhase1.getAsBoolean() == false) { // we found a better match that can skip phase 1, cancel the existing allocation. logger.debug("cancelling allocation of replica on [{}], sync id match found on node [{}]", currentNode, nodeWithHighestMatch); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + currentNode + "] cancelled, sync id match found on node ["+ nodeWithHighestMatch + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT); // don't cancel shard in the loop as it will cause a ConcurrentModificationException shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } } } } for (Runnable action : shardCancellationActions) { action.run(); } } /** * Is the allocator responsible for allocating the given {@link ShardRouting}	i think it would be nicer to just turn this into a regular method?
public void processExistingRecoveries(RoutingAllocation allocation) { MetaData metaData = allocation.metaData(); RoutingNodes routingNodes = allocation.routingNodes(); List<Runnable> shardCancellationActions = new ArrayList<>(); for (RoutingNode routingNode : routingNodes) { for (ShardRouting shard : routingNode) { if (shard.primary()) { continue; } if (shard.initializing() == false) { continue; } if (shard.relocatingNodeId() != null) { continue; } // if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one... if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) { continue; } AsyncShardFetch.FetchResult<NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation); if (shardStores.hasData() == false) { logger.trace("{}: fetching new stores for initializing shard", shard); continue; // still fetching } ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId()); assert primaryShard != null : "the replica shard can be allocated on at least one node, so there must be an active primary"; TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryShard, allocation, shardStores); if (primaryStore == null) { // if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed) // just let the recovery find it out, no need to do anything about it for the initializing shard logger.trace("{}: no primary shard store found or allocated, letting actual allocation figure it out", shard); continue; } MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, primaryStore, shardStores, false); if (matchingNodes.getNodeWithHighestMatch() != null) { DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId()); DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch(); BooleanSupplier currentNodeCanSkipPhase1 = () -> { TransportNodesListShardStoreMetaData.StoreFilesMetaData storeMetadata = shardStores.getData().get(currentNode).storeFilesMetaData(); // current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider if (storeMetadata == null) { return false; } IndexMetaData indexMetadata = allocation.metaData().index(shard.index()); if (canPerformOperationBasedRecovery(indexMetadata, primaryStore, currentNode, storeMetadata)) { return true; } final String currentSyncId = storeMetadata.syncId(); return currentSyncId != null && currentSyncId.equals(primaryStore.syncId()); }; if (currentNode.equals(nodeWithHighestMatch) == false && matchingNodes.canSkipPhase1(nodeWithHighestMatch) && currentNodeCanSkipPhase1.getAsBoolean() == false) { // we found a better match that can skip phase 1, cancel the existing allocation. logger.debug("cancelling allocation of replica on [{}], sync id match found on node [{}]", currentNode, nodeWithHighestMatch); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + currentNode + "] cancelled, sync id match found on node ["+ nodeWithHighestMatch + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT); // don't cancel shard in the loop as it will cause a ConcurrentModificationException shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } } } } for (Runnable action : shardCancellationActions) { action.run(); } } /** * Is the allocator responsible for allocating the given {@link ShardRouting}	i think this will cancel recoveries too enthusiastically, and would prefer we only cancel an ongoing recovery if its replacement is trivial (has a sync id match or has a prrl that matches the primary).
private static boolean canPerformOperationBasedRecovery( IndexMetaData indexMetaData, TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore, DiscoveryNode replicaNode, TransportNodesListShardStoreMetaData.StoreFilesMetaData replicaStore) { if (replicaStore.isEmpty()) { // a corrupted store - it won't be able to perform operation-based recovery return false; } // If an index is closed or frozen, we can perform an operation-based recovery only if the last commit on the replica is safe and // has the same operations as the last commit on the primary. Here we must ignore the peer recovery retention lease as we don't // have the persisted global checkpoint from the replica to determine if the last commit is safe. However, the likelihood that // the last commit unsafe is very small. It's probably okay to use the retention lease if the sequence numbers of the last commit // from the primary and replica equal. if (indexMetaData.getState() == IndexMetaData.State.CLOSE || IndexMetaData.INDEX_BLOCKS_WRITE_SETTING.get(indexMetaData.getSettings())) { return false; } final String retentionLeaseId = ReplicationTracker.getPeerRecoveryRetentionLeaseId(replicaNode.getId()); return primaryStore.peerRecoveryRetentionLeases().stream().anyMatch(lease -> lease.id().equals(retentionLeaseId)); }	i think this comment mainly relates to the unresolved part around closed/frozen indices and does not explain the code here? perhaps rephrase as a todo.
private static boolean canPerformOperationBasedRecovery( IndexMetaData indexMetaData, TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore, DiscoveryNode replicaNode, TransportNodesListShardStoreMetaData.StoreFilesMetaData replicaStore) { if (replicaStore.isEmpty()) { // a corrupted store - it won't be able to perform operation-based recovery return false; } // If an index is closed or frozen, we can perform an operation-based recovery only if the last commit on the replica is safe and // has the same operations as the last commit on the primary. Here we must ignore the peer recovery retention lease as we don't // have the persisted global checkpoint from the replica to determine if the last commit is safe. However, the likelihood that // the last commit unsafe is very small. It's probably okay to use the retention lease if the sequence numbers of the last commit // from the primary and replica equal. if (indexMetaData.getState() == IndexMetaData.State.CLOSE || IndexMetaData.INDEX_BLOCKS_WRITE_SETTING.get(indexMetaData.getSettings())) { return false; } final String retentionLeaseId = ReplicationTracker.getPeerRecoveryRetentionLeaseId(replicaNode.getId()); return primaryStore.peerRecoveryRetentionLeases().stream().anyMatch(lease -> lease.id().equals(retentionLeaseId)); }	i don't see that the tests exercise this, but it'd be good if they did. that said, for closed and frozen indices i think it's ok not to treat them specially and simply to use the retention lease if a retention lease exists. it's going to be very rare that any recovery is needed and if it is we don't really have any option but to perform a file-based recovery anyway, so we may as well do that onto the same node as before. one issue we might face is that closed indices don't necessarily have a prrl for every shard copy, and i think this could result in bad behaviour on closed indices where the files don't match (and there's no sync id). we may need to defer special treatment of closed indices to a followup since i think this'll need a change to recoverysourcehandler to address.
private static boolean canPerformOperationBasedRecovery( IndexMetaData indexMetaData, TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore, DiscoveryNode replicaNode, TransportNodesListShardStoreMetaData.StoreFilesMetaData replicaStore) { if (replicaStore.isEmpty()) { // a corrupted store - it won't be able to perform operation-based recovery return false; } // If an index is closed or frozen, we can perform an operation-based recovery only if the last commit on the replica is safe and // has the same operations as the last commit on the primary. Here we must ignore the peer recovery retention lease as we don't // have the persisted global checkpoint from the replica to determine if the last commit is safe. However, the likelihood that // the last commit unsafe is very small. It's probably okay to use the retention lease if the sequence numbers of the last commit // from the primary and replica equal. if (indexMetaData.getState() == IndexMetaData.State.CLOSE || IndexMetaData.INDEX_BLOCKS_WRITE_SETTING.get(indexMetaData.getSettings())) { return false; } final String retentionLeaseId = ReplicationTracker.getPeerRecoveryRetentionLeaseId(replicaNode.getId()); return primaryStore.peerRecoveryRetentionLeases().stream().anyMatch(lease -> lease.id().equals(retentionLeaseId)); }	if we have two replicas that can both do operations based recovery, i think we should pick the one with least amount of operations to recover here? i think simply comparing the leases' retainingsequencenumber should suffice, since that should be progressed at regular intervals.
public static void toXContent(AliasMetaData aliasMetaData, XContentBuilder builder, ToXContent.Params params) throws IOException { builder.startObject(aliasMetaData.alias()); boolean binary = params.paramAsBoolean("binary", false); if (aliasMetaData.filter() != null) { if (binary) { builder.field("filter", aliasMetaData.filter.compressed()); } else { builder.field("filter", XContentHelper.convertToMap(new BytesArray(aliasMetaData.filter().uncompressed()), true).v2()); } } if (aliasMetaData.indexRouting() != null) { builder.field("index_routing", aliasMetaData.indexRouting()); } if (aliasMetaData.searchRouting() != null) { builder.field("search_routing", aliasMetaData.searchRouting()); } if (aliasMetaData.writeIndex() != null) { builder.field("is_write_index", aliasMetaData.writeIndex()); } builder.endObject(); }	i think you can use org.elasticsearch.common.xcontent.xcontentbuilder#field(java.lang.string, java.lang.boolean) ?
@SuppressWarnings("unchecked") public PutIndexTemplateRequest source(Map templateSource) { Map<String, Object> source = templateSource; for (Map.Entry<String, Object> entry : source.entrySet()) { String name = entry.getKey(); if (name.equals("template")) { // This is needed to allow for bwc (beats, logstash) with pre-5.0 templates (#21009) if(entry.getValue() instanceof String) { DEPRECATION_LOGGER.deprecated("Deprecated field [template] used, replaced by [index_patterns]"); patterns(Collections.singletonList((String) entry.getValue())); } } else if (name.equals("index_patterns")) { if(entry.getValue() instanceof String) { patterns(Collections.singletonList((String) entry.getValue())); } else if (entry.getValue() instanceof List) { List<String> elements = ((List<?>) entry.getValue()).stream().map(Object::toString).collect(Collectors.toList()); patterns(elements); } else { throw new IllegalArgumentException("Malformed [template] value, should be a string or a list of strings"); } } else if (name.equals("order")) { order(XContentMapValues.nodeIntegerValue(entry.getValue(), order())); } else if ("version".equals(name)) { if ((entry.getValue() instanceof Integer) == false) { throw new IllegalArgumentException("Malformed [version] value, should be an integer"); } version((Integer)entry.getValue()); } else if (name.equals("settings")) { if ((entry.getValue() instanceof Map) == false) { throw new IllegalArgumentException("Malformed [settings] section, should include an inner object"); } settings((Map<String, Object>) entry.getValue()); } else if (name.equals("mappings")) { Map<String, Object> mappings = (Map<String, Object>) entry.getValue(); for (Map.Entry<String, Object> entry1 : mappings.entrySet()) { if (!(entry1.getValue() instanceof Map)) { throw new IllegalArgumentException( "Malformed [mappings] section for type [" + entry1.getKey() + "], should include an inner object describing the mapping"); } mapping(entry1.getKey(), (Map<String, Object>) entry1.getValue()); } } else if (name.equals("aliases")) { aliases((Map<String, Object>) entry.getValue()); } else { // maybe custom? IndexMetaData.Custom proto = IndexMetaData.lookupPrototype(name); if (proto != null) { try { customs.put(name, proto.fromMap((Map<String, Object>) entry.getValue())); } catch (IOException e) { throw new ElasticsearchParseException("failed to parse custom metadata for [{}]", name); } } } } return this; }	@nik9000 i assume you mean here pre-6.0 templates. or will there be support for indext-patterns in the 5.x releases?
public static void initMockScripts() { SCRIPTS.put("initScript", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); state.put("collector", new ArrayList<Integer>()); return state; }); SCRIPTS.put("mapScript", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); ((List<Integer>) state.get("collector")).add(1); // just add 1 for each doc the script is run on return state; }); SCRIPTS.put("combineScript", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); return ((List<Integer>) state.get("collector")).stream().mapToInt(Integer::intValue).sum(); }); SCRIPTS.put("combineScriptNoop", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); return state; }); SCRIPTS.put("reduceScript", params -> { List<Integer> states = (List<Integer>) params.get("states"); return states.stream().mapToInt(Integer::intValue).sum(); }); SCRIPTS.put("initScriptScore", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); state.put("collector", new ArrayList<Double>()); return state; }); SCRIPTS.put("mapScriptScore", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); ((List<Double>) state.get("collector")).add(((Number) params.get("_score")).doubleValue()); return state; }); SCRIPTS.put("combineScriptScore", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); return ((List<Double>) state.get("collector")).stream().mapToDouble(Double::doubleValue).sum(); }); SCRIPTS.put("initScriptParams", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); Integer initialValue = (Integer)params.get("initialValue"); ArrayList<Integer> collector = new ArrayList<>(); collector.add(initialValue); state.put("collector", collector); return state; }); SCRIPTS.put("mapScriptParams", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); Integer itemValue = (Integer) params.get("itemValue"); ((List<Integer>) state.get("collector")).add(itemValue); return state; }); SCRIPTS.put("combineScriptParams", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); int multiplier = ((Integer) params.get("multiplier")); return ((List<Integer>) state.get("collector")).stream().mapToInt(Integer::intValue).map(i -> i * multiplier).sum(); }); SCRIPTS.put("reduceScriptParams", params -> ((List)params.get("states")).stream().mapToInt(i -> (int)i).sum() + (int)params.get("aggs_param") + (int)params.get("additional") - ((List)params.get("states")).size()*24*4 ); SCRIPTS.put("initScriptSelfRef", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); state.put("collector", new ArrayList<Integer>()); state.put("selfRef", state); return state; }); SCRIPTS.put("mapScriptSelfRef", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); state.put("selfRef", state); return state; }); SCRIPTS.put("combineScriptSelfRef", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); state.put("selfRef", state); return state; }); SCRIPTS.put("initScriptMakingArray", params -> { Map<String, Object> state = (Map<String, Object>) params.get("state"); state.put("array", new String[] {"foo", "bar"}); state.put("collector", new ArrayList<Integer>()); return state; }); }	we could check this for all aggs in the aggregatortestcase directly ?
public void setAggBuilder(AggregatorFactories.Builder aggBuilder) { for (AggregationBuilder aggregation : aggBuilder.getAggregatorFactories()) { final String type = aggregation.getType(); switch (type) { case MinAggregationBuilder.NAME: case MaxAggregationBuilder.NAME: case AvgAggregationBuilder.NAME: case SumAggregationBuilder.NAME: case CardinalityAggregationBuilder.NAME: break; default: // top term and percentile should be supported throw new IllegalArgumentException("Unsupported aggregation of type [" + type + "]"); } } for (PipelineAggregationBuilder aggregation : aggBuilder.getPipelineAggregatorFactories()) { // should not have pipeline aggregations final String type = aggregation.getType(); throw new IllegalArgumentException("Unsupported pipeline aggregation of type [" + type + "]"); } this.aggBuilder = aggBuilder; }	let's wait with this. i think i have some idea on how to make this more generic.
private int lat(double lat) { return (int) Math.round(pointYScale * (VectorTileUtils.latToSphericalMercator(lat) - rectangle.getMinY())) + extent; }	we should still probably get this in.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (params.paramAsBoolean(RestSearchAction.TYPED_KEYS_PARAM, false)) { // Concatenates the type and the name of the aggregation (ex: top_hits#foo) builder.startArray(String.join(InternalAggregation.TYPED_KEYS_DELIMITER, getType(), getName())); } else { builder.startArray(getName()); } for (Entry<?> entry : entries) { entry.toXContent(builder, params); } builder.endArray(); return builder; }	just a thought: maybe instead of referring to the delimiter in aggregations we can have an own constant in suggest and either declare the same symbol there or delegate to internalaggregation.typed_keys_delimiter? the delimiter doesn't need to be the same in aggs and suggestions, only the parsing part needs to refer to it (although its good to have the same like we do now i think)
@Override public Query termQuery(Object value, SearchExecutionContext context) { failIfNotIndexedNorDocValues(); return type.termQuery(name(), value, isIndexed() == false); }	do you think we should have an expensive query check here as well?
@Override protected void parseCreateField(ParseContext context, List<Field> fields) throws IOException { if (!enabled) { return; } if (!fieldType.stored()) { return; } if (context.flyweight()) { return; } BytesReference source = context.source(); boolean filtered = (includes != null && includes.length > 0) || (excludes != null && excludes.length > 0); if (filtered) { // we don't update the context source if we filter, we want to keep it as is... Tuple<XContentType, Map<String, Object>> mapTuple = XContentHelper.convertToMap(source, true); Map<String, Object> filteredSource = XContentMapValues.filter(mapTuple.v2(), includes, excludes); BytesStreamOutput bStream = new BytesStreamOutput(); StreamOutput streamOutput = bStream; if (compress != null && compress && (compressThreshold == -1 || source.length() > compressThreshold)) { streamOutput = CompressorFactory.defaultCompressor().streamOutput(bStream); } XContentType contentType = formatContentType; if (contentType == null) { contentType = mapTuple.v1(); } XContentBuilder builder = XContentFactory.contentBuilder(contentType, streamOutput).map(filteredSource); builder.close(); source = bStream.bytes(); } else if (compress != null && compress && !CompressorFactory.isCompressed(source)) { if (compressThreshold == -1 || source.length() > compressThreshold) { BytesStreamOutput bStream = new BytesStreamOutput(); XContentType contentType = XContentFactory.xContentType(source); if (formatContentType != null && formatContentType != contentType) { XContentBuilder builder = XContentFactory.contentBuilder(formatContentType, CompressorFactory.defaultCompressor().streamOutput(bStream)); builder.copyCurrentStructure(XContentFactory.xContent(contentType).createParser(source)); builder.close(); } else { StreamOutput streamOutput = CompressorFactory.defaultCompressor().streamOutput(bStream); source.writeTo(streamOutput); streamOutput.close(); } source = bStream.bytes(); // update the data in the context, so it can be compressed and stored compressed outside... context.source(source); } } else if (formatContentType != null) { // see if we need to convert the content type Compressor compressor = CompressorFactory.compressor(source); if (compressor != null) { CompressedStreamInput compressedStreamInput = compressor.streamInput(source.streamInput()); XContentType contentType = XContentFactory.xContentType(compressedStreamInput); compressedStreamInput.resetToBufferStart(); if (contentType != formatContentType) { // we need to reread and store back, compressed.... BytesStreamOutput bStream = new BytesStreamOutput(); StreamOutput streamOutput = CompressorFactory.defaultCompressor().streamOutput(bStream); XContentBuilder builder = XContentFactory.contentBuilder(formatContentType, streamOutput); builder.copyCurrentStructure(XContentFactory.xContent(contentType).createParser(compressedStreamInput)); builder.close(); source = bStream.bytes(); // update the data in the context, so we store it in the translog in this format context.source(source); } else { compressedStreamInput.close(); } } else { XContentType contentType = XContentFactory.xContentType(source); if (contentType != formatContentType) { // we need to reread and store back // we need to reread and store back, compressed.... BytesStreamOutput bStream = new BytesStreamOutput(); XContentBuilder builder = XContentFactory.contentBuilder(formatContentType, bStream); builder.copyCurrentStructure(XContentFactory.xContent(contentType).createParser(source)); builder.close(); source = bStream.bytes(); // update the data in the context, so we store it in the translog in this format context.source(source); } } } if (!source.hasArray()) { source = source.toBytesArray(); } assert source.hasArray(); fields.add(new StoredField(names().indexName(), source.array(), source.arrayOffset(), source.length())); }	does this assert still make sense?
static void validateSplitIndex(ClusterState state, String sourceIndex, String targetIndexName, Settings targetIndexSettings) { IndexMetadata sourceMetadata = validateResize(state, sourceIndex, targetIndexName, targetIndexSettings); if ("snapshot".equals(INDEX_STORE_TYPE_SETTING.get(sourceMetadata.getSettings()))) { throw new IllegalArgumentException("can't split searchable snapshot index [" + sourceIndex + ']'); } IndexMetadata.selectSplitShard(0, sourceMetadata, INDEX_NUMBER_OF_SHARDS_SETTING.get(targetIndexSettings)); }	could we move searchablesnapshotsconstants#snapshot_directory_factory_key and searchablesnapshotsconstants#issearchablesnapshotstore to server rather than just using the literal "snapshot" throughout? either that or add an independent constant in server and then add a static constructor to o.e.x.c.searchablesnapshots.searchablesnapshotsconstants to assert that the constant in server is equal to snapshot_directory_factory_key?
static void validateCloneIndex(ClusterState state, String sourceIndex, String targetIndexName, Settings targetIndexSettings) { IndexMetadata sourceMetadata = validateResize(state, sourceIndex, targetIndexName, targetIndexSettings); if ("snapshot".equals(INDEX_STORE_TYPE_SETTING.get(sourceMetadata.getSettings()))) { for (Setting<?> nonCloneableSetting : List.of(INDEX_STORE_TYPE_SETTING, INDEX_RECOVERY_TYPE_SETTING)) { if (nonCloneableSetting.exists(targetIndexSettings) == false) { throw new IllegalArgumentException("can't clone searchable snapshot index [" + sourceIndex + "]; setting [" + nonCloneableSetting.getKey() + "] should be overridden"); } } } IndexMetadata.selectCloneShard(0, sourceMetadata, INDEX_NUMBER_OF_SHARDS_SETTING.get(targetIndexSettings)); }	nit: maybe use arrays.aslist() or some other thing that's jdk8 compatible to make the backport cleaner? :)
@Override public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { DiscoveryNode fromDiscoNode = allocation.nodes().resolveNode(fromNode); DiscoveryNode toDiscoNode = allocation.nodes().resolveNode(toNode); Decision decision = null; boolean found = false; RoutingNode fromRoutingNode = allocation.routingNodes().node(fromDiscoNode.getId()); if (fromRoutingNode == null && !fromDiscoNode.isDataNode()) { throw new IllegalArgumentException("[move_allocation] can't move " + shardId + " from " + fromDiscoNode.getName() + " to " + toDiscoNode.getName() + ": " + fromDiscoNode.getName() + " is not a data node."); } RoutingNode toRoutingNode = allocation.routingNodes().node(toDiscoNode.getId()); if (toRoutingNode == null && !toDiscoNode.isDataNode()) { throw new IllegalArgumentException("[move_allocation] can't move " + shardId + " from " + fromDiscoNode.getName() + " to " + toDiscoNode.getName() + ": " + toDiscoNode.getName() + " is not a data node."); } for (ShardRouting shardRouting : fromRoutingNode) { if (!shardRouting.shardId().getIndexName().equals(index)) { continue; } if (shardRouting.shardId().id() != shardId) { continue; } found = true; // TODO we can possibly support also relocating cases, where we cancel relocation and move... if (!shardRouting.started()) { if (explain) { return new RerouteExplanation(this, allocation.decision(Decision.NO, "move_allocation_command", "shard " + shardId + " has not been started")); } throw new IllegalArgumentException("[move_allocation] can't move " + shardId + ", shard is not started (state = " + shardRouting.state() + "]"); } decision = allocation.deciders().canAllocate(shardRouting, toRoutingNode, allocation); if (decision.type() == Decision.Type.NO) { if (explain) { return new RerouteExplanation(this, decision); } throw new IllegalArgumentException("[move_allocation] can't move " + shardId + ", from " + fromDiscoNode + ", to " + toDiscoNode + ", since its not allowed, reason: " + decision); } if (decision.type() == Decision.Type.THROTTLE) { // its being throttled, maybe have a flag to take it into account and fail? for now, just do it since the "user" wants it... } allocation.routingNodes().relocateShard(shardRouting, toRoutingNode.nodeId(), allocation.clusterInfo().getShardSize(shardRouting, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE), allocation.changes()); } if (!found) { if (explain) { return new RerouteExplanation(this, allocation.decision(Decision.NO, "move_allocation_command", "shard " + shardId + " not found")); } throw new IllegalArgumentException("[move_allocation] can't move " + shardId + ", failed to find it on node " + fromDiscoNode); } return new RerouteExplanation(this, decision); }	could you wrap these lines to <120 characters? we're trying to cut down on overlong lines.
public void testMoveShardToNonDataNode() { AllocationService allocation = createAllocationService(Settings.builder().put("cluster.routing.allocation.node_concurrent_recoveries", 10).build()); logger.info("creating an index with 1 shard, no replica"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(0)) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsNew(metaData.index("test")) .build(); ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)).metaData(metaData).routingTable(routingTable).build(); logger.info("--> adding two nodes"); clusterState = ClusterState.builder(clusterState).nodes( DiscoveryNodes.builder() .add(new DiscoveryNode("node1", "node1", "node1", "test1", "test1", buildNewFakeTransportAddress(), emptyMap(), MASTER_DATA_ROLES, Version.CURRENT)) .add(new DiscoveryNode("node2", "node2", "node2", "test2", "test2", buildNewFakeTransportAddress(), emptyMap(), new HashSet<>(randomSubsetOf(EnumSet.of(DiscoveryNode.Role.MASTER, DiscoveryNode.Role.INGEST))), Version.CURRENT))).build(); logger.info("start primary shard"); clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)); Index index = clusterState.getMetaData().index("test").getIndex(); MoveAllocationCommand command = new MoveAllocationCommand(index.getName(), 0, "node1", "node2"); RoutingAllocation routingAllocation = new RoutingAllocation(new AllocationDeciders(Settings.EMPTY, Collections.emptyList()), new RoutingNodes(clusterState, false), clusterState, ClusterInfo.EMPTY, System.nanoTime()); logger.info("--> executing move allocation command to non-data node"); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> command.execute(routingAllocation, false)); assertEquals("[move_allocation] can't move 0 from node1 to node2: node2 is not a data node.", e.getMessage()); }	excellent, thanks. i'm glad you did that, because it shows that this message isn't what i was expecting. the shard number on its own is likely to cause confusion, and also it's useful to have the full discoverynode.tostring() output rather than just using .getname(). something like this'd be great: https://github.com/davecturner/elasticsearch/commit/c53a9805a47557ab6a29d18e9207ac5240116673
@Override public void writeTo(StreamOutput out) throws IOException { out.writeByte(unit.id()); out.writeString(timeZone.getID()); } } static class UTCIntervalTimeZoneRounding extends TimeZoneRounding { final static byte ID = 4; private long interval; UTCIntervalTimeZoneRounding() { // for serialization } UTCIntervalTimeZoneRounding(long interval) { if (interval < 1) throw new ElasticsearchIllegalArgumentException("Negative time interval not supported"); this.interval = interval; } @Override public byte id() { return ID; } @Override public long roundKey(long utcMillis) { return Rounding.Interval.roundKey(utcMillis, interval); } @Override public long valueForKey(long key) { return Rounding.Interval.roundValue(key, interval); } @Override public long nextRoundingValue(long value) { return value + interval; } @Override public void readFrom(StreamInput in) throws IOException { interval = in.readVLong(); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeVLong(interval); } } static class TimeIntervalTimeZoneRounding extends TimeZoneRounding { final static byte ID = 5; private long interval; private DateTimeZone timeZone; TimeIntervalTimeZoneRounding() { // for serialization } TimeIntervalTimeZoneRounding(long interval, DateTimeZone timeZone) { if (interval < 1) throw new ElasticsearchIllegalArgumentException("Negative time interval not supported"); this.interval = interval; this.timeZone = timeZone; } @Override public byte id() { return ID; } @Override public long roundKey(long utcMillis) { long timeLocal = timeZone.convertUTCToLocal(utcMillis); return Rounding.Interval.roundKey(timeLocal, interval); } @Override public long valueForKey(long key) { long localTime = Rounding.Interval.roundValue(key, interval); return timeZone.convertLocalToUTC(localTime, true); } @Override public long nextRoundingValue(long value) { long timeLocal = timeZone.convertUTCToLocal(value); long next = timeLocal + interval; return timeZone.convertLocalToUTC(next, true); } @Override public void readFrom(StreamInput in) throws IOException { interval = in.readVLong(); timeZone = DateTimeZone.forID(in.readString()); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeVLong(interval); out.writeString(timeZone.getID()); } } static class DayIntervalTimeZoneRounding extends TimeZoneRounding { final static byte ID = 6; private long interval; private DateTimeZone timeZone; DayIntervalTimeZoneRounding() { // for serialization } DayIntervalTimeZoneRounding(long interval, DateTimeZone timeZone) { if (interval < 1) throw new ElasticsearchIllegalArgumentException("Negative time interval not supported"); this.interval = interval; this.timeZone = timeZone; } @Override public byte id() { return ID; } @Override public long roundKey(long utcMillis) { long time = utcMillis + timeZone.getOffset(utcMillis); return Rounding.Interval.roundKey(time, interval); } @Override public long valueForKey(long key) { long localTime = Rounding.Interval.roundValue(key, interval); return timeZone.convertLocalToUTC(localTime, true); } @Override public long nextRoundingValue(long value) { long timeLocal = timeZone.convertUTCToLocal(value); long next = timeLocal + interval; return timeZone.convertLocalToUTC(next, true); } @Override public void readFrom(StreamInput in) throws IOException { interval = in.readVLong(); timeZone = DateTimeZone.forID(in.readString()); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeVLong(interval); out.writeString(timeZone.getID()); }	thanks for adding this!
*/ public void getMultiple(String ids, boolean allowNoMatch, ActionListener<List<DataFrameAnalyticsConfig>> listener) { GetDataFrameAnalyticsAction.Request request = new GetDataFrameAnalyticsAction.Request(); request.setPageParams(new PageParams(0, MAX_CONFIGS_SIZE)); request.setResourceId(ids); request.setAllowNoResources(allowNoMatch); executeAsyncWithOrigin(client, ML_ORIGIN, GetDataFrameAnalyticsAction.INSTANCE, request, ActionListener.wrap( response -> listener.onResponse(response.getResources().results()), listener::onFailure)); }	@droberts195 this here is used for memory tracking and before that it'd would be limited to 100 jobs. unfortunately we're still limited but now the limit is 10k.
public void testDataStreamResolution() { { final User user = new User("data-stream-tester1", "data_stream_test1"); // Resolve data streams: SearchRequest searchRequest = new SearchRequest(); searchRequest.indices("logs-*"); searchRequest.indicesOptions(IndicesOptions.fromOptions(false, false, true, false, false, true, true, true, true)); final List<String> authorizedIndices = buildAuthorizedIndices(user, SearchAction.NAME, searchRequest); ResolvedIndices resolvedIndices = defaultIndicesResolver.resolveIndicesAndAliases(searchRequest, metadata, authorizedIndices); assertThat(resolvedIndices.getLocal(), contains("logs-foobar")); assertThat(resolvedIndices.getRemote(), emptyIterable()); // Data streams with allow no indices: searchRequest = new SearchRequest(); searchRequest.indices("logs-*"); searchRequest.indicesOptions(IndicesOptions.fromOptions(false, true, true, false, false, true, true, true, true)); resolvedIndices = defaultIndicesResolver.resolveIndicesAndAliases(searchRequest, metadata, authorizedIndices); // if data streams are to be ignored then this happens in IndexNameExpressionResolver: assertThat(resolvedIndices.getLocal(), contains("logs-foobar")); assertThat(resolvedIndices.getRemote(), emptyIterable()); } { final User user = new User("data-stream-tester2", "data_stream_test2"); // Resolve *all* data streams: SearchRequest searchRequest = new SearchRequest(); searchRequest.indices("logs-*"); searchRequest.indicesOptions(IndicesOptions.fromOptions(false, false, true, false, false, true, true, true, true)); final List<String> authorizedIndices = buildAuthorizedIndices(user, SearchAction.NAME, searchRequest); ResolvedIndices resolvedIndices = defaultIndicesResolver.resolveIndicesAndAliases(searchRequest, metadata, authorizedIndices); assertThat(resolvedIndices.getLocal(), containsInAnyOrder("logs-foo", "logs-foobar")); assertThat(resolvedIndices.getRemote(), emptyIterable()); } }	no backing indices as well. but regular indices and aliases are visible. same with backing indices. (i would expand the namespace granted to by the role to ensure those entities are ok - i.e. i'm not suggesting another test, just a more complete one). but i would also test a simple (not wildcard) data stream name.
public void testDataStreamResolution() { { final User user = new User("data-stream-tester1", "data_stream_test1"); // Resolve data streams: SearchRequest searchRequest = new SearchRequest(); searchRequest.indices("logs-*"); searchRequest.indicesOptions(IndicesOptions.fromOptions(false, false, true, false, false, true, true, true, true)); final List<String> authorizedIndices = buildAuthorizedIndices(user, SearchAction.NAME, searchRequest); ResolvedIndices resolvedIndices = defaultIndicesResolver.resolveIndicesAndAliases(searchRequest, metadata, authorizedIndices); assertThat(resolvedIndices.getLocal(), contains("logs-foobar")); assertThat(resolvedIndices.getRemote(), emptyIterable()); // Data streams with allow no indices: searchRequest = new SearchRequest(); searchRequest.indices("logs-*"); searchRequest.indicesOptions(IndicesOptions.fromOptions(false, true, true, false, false, true, true, true, true)); resolvedIndices = defaultIndicesResolver.resolveIndicesAndAliases(searchRequest, metadata, authorizedIndices); // if data streams are to be ignored then this happens in IndexNameExpressionResolver: assertThat(resolvedIndices.getLocal(), contains("logs-foobar")); assertThat(resolvedIndices.getRemote(), emptyIterable()); } { final User user = new User("data-stream-tester2", "data_stream_test2"); // Resolve *all* data streams: SearchRequest searchRequest = new SearchRequest(); searchRequest.indices("logs-*"); searchRequest.indicesOptions(IndicesOptions.fromOptions(false, false, true, false, false, true, true, true, true)); final List<String> authorizedIndices = buildAuthorizedIndices(user, SearchAction.NAME, searchRequest); ResolvedIndices resolvedIndices = defaultIndicesResolver.resolveIndicesAndAliases(searchRequest, metadata, authorizedIndices); assertThat(resolvedIndices.getLocal(), containsInAnyOrder("logs-foo", "logs-foobar")); assertThat(resolvedIndices.getRemote(), emptyIterable()); } }	should include the backing indices as well, right?
private void runElaticsearchBinScriptWithInput(String input, String tool, String... args) { try (InputStream byteArrayInputStream = new ByteArrayInputStream(input.getBytes(StandardCharsets.UTF_8))) { services.loggedExec(spec -> { spec.setEnvironment(getESEnvironment()); spec.workingDir(workingDir); spec.executable( OS.conditionalString() .onUnix(() -> "./bin/" + tool) .onWindows(() -> "cmd") .supply() ); spec.args( OS.<List<String>>conditional() .onWindows(() -> { ArrayList<String> result = new ArrayList<>(); result.add("/c"); result.add("bin\\\\" + tool + ".bat"); for (String arg : args) { result.add(arg); } return result; }) .onUnix(() -> Arrays.asList(args)) .supply() ); spec.setStandardInput(byteArrayInputStream); }); } catch (IOException e) { throw new UncheckedIOException(e); } }	we should probably add some methods to conditional that simply accept t instead of supplier<t> as it's unnecessary to make this evaluation lazy in almost every case given that calling supply() is going to evaluate those functions immediately.
public void testForceStaleReplicaToBePromotedForGreenIndex() { internalCluster().startMasterOnlyNode(Settings.EMPTY); final List<String> dataNodes = internalCluster().startDataOnlyNodes(2); final String idxName = "test"; assertAcked(client().admin().indices().prepareCreate(idxName) .setSettings(Settings.builder().put("index.number_of_shards", 1) .put("index.number_of_replicas", 1)).get()); ensureGreen(); final String nodeWithoutData = randomFrom(dataNodes); final int shardId = 0; Throwable iae = expectThrows( IllegalArgumentException.class, () -> client().admin().cluster().prepareReroute() .add(new AllocateStalePrimaryAllocationCommand(idxName, shardId, nodeWithoutData, true)).get()); assertThat( iae.getMessage(), equalTo("[allocate_stale_primary] primary [" + idxName+ "][" + shardId + "] is already assigned")); }	illegalargumentexception iae = ...
@Override void analyze(ScriptRoot scriptRoot, Locals locals) { this.settings = scriptRoot.getCompilerSettings(); if (block.statements.isEmpty()) { throw createError(new IllegalArgumentException("Cannot generate an empty function [" + name + "].")); } locals = Locals.newLocalScope(locals); block.lastSource = true; block.analyze(scriptRoot, locals); methodEscape = block.methodEscape; if (!methodEscape && returnType != void.class) { throw createError(new IllegalArgumentException("Not all paths provide a return value for method [" + name + "].")); } if (settings.getMaxLoopCounter() > 0) { loop = locals.getVariable(null, Locals.LOOP); } }	this is only used for getmaxloopcounter(), consider just saving that setting.
public final BulkRequest timeout(String timeout) { return timeout(TimeValue.parseTimeValue(timeout, null, "BulkRequest.timeout")); }	just a suggestion, would it make sense to use bulkrequest.class.getsimplename() + ".timeout" i am afraid we missing this stuff in refactorings
public List<Phase> getOrderedPhases(Map<String, Phase> phases) { List<Phase> orderedPhases = new ArrayList<>(VALID_PHASES.size()); for (String phaseName : VALID_PHASES) { Phase phase = phases.get(phaseName); if (phase != null) { Map<String, LifecycleAction> actions = phase.getActions(); if (actions.containsKey(UnfollowAction.NAME) == false && (actions.containsKey(RolloverAction.NAME) || actions.containsKey(ShrinkAction.NAME) || actions.containsKey(SearchableSnapshotAction.NAME))) { Map<String, LifecycleAction> actionMap = new HashMap<>(phase.getActions()); actionMap.put(UnfollowAction.NAME, new UnfollowAction()); phase = new Phase(phase.getName(), phase.getMinimumAge(), actionMap); } if (shouldMigrateDataToTiers(phase)) { Map<String, LifecycleAction> actionMap = new HashMap<>(phase.getActions()); actionMap.put(MigrateAction.NAME, new MigrateAction(true)); phase = new Phase(phase.getName(), phase.getMinimumAge(), actionMap); } orderedPhases.add(phase); } } return orderedPhases; }	this might be better named shouldinjectmigratestepforphase i think
public List<Phase> getOrderedPhases(Map<String, Phase> phases) { List<Phase> orderedPhases = new ArrayList<>(VALID_PHASES.size()); for (String phaseName : VALID_PHASES) { Phase phase = phases.get(phaseName); if (phase != null) { Map<String, LifecycleAction> actions = phase.getActions(); if (actions.containsKey(UnfollowAction.NAME) == false && (actions.containsKey(RolloverAction.NAME) || actions.containsKey(ShrinkAction.NAME) || actions.containsKey(SearchableSnapshotAction.NAME))) { Map<String, LifecycleAction> actionMap = new HashMap<>(phase.getActions()); actionMap.put(UnfollowAction.NAME, new UnfollowAction()); phase = new Phase(phase.getName(), phase.getMinimumAge(), actionMap); } if (shouldMigrateDataToTiers(phase)) { Map<String, LifecycleAction> actionMap = new HashMap<>(phase.getActions()); actionMap.put(MigrateAction.NAME, new MigrateAction(true)); phase = new Phase(phase.getName(), phase.getMinimumAge(), actionMap); } orderedPhases.add(phase); } } return orderedPhases; }	this could use your definesallocationrules(allocateaction) helper here instead of manually checking, though i think maybe that helper needs to use isempty rather than checking for null so that an allocate action with "include": {} doesn't trip the assertion
@Override public InternalGeoCentroid reduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) { double lonSum = Double.NaN; double latSum = Double.NaN; long totalCount = 0; for (InternalAggregation aggregation : aggregations) { InternalGeoCentroid centroidAgg = (InternalGeoCentroid) aggregation; if (centroidAgg.count > 0) { totalCount += centroidAgg.count; if (Double.isNaN(lonSum)) { lonSum = centroidAgg.count * centroidAgg.centroid.getLon(); latSum = centroidAgg.count * centroidAgg.centroid.getLat(); } else { lonSum += (centroidAgg.count * centroidAgg.centroid.getLon()); latSum += (centroidAgg.count * centroidAgg.centroid.getLat()); } } } final GeoPoint result = (Double.isNaN(lonSum)) ? null : new GeoPoint(latSum/totalCount, lonSum/totalCount); return new InternalGeoCentroid(name, result, totalCount, getMetadata()); }	huh! the count member variable was always a long!
public void testReduceMaxCount() { InternalGeoCentroid maxValueGeoCentroid = new InternalGeoCentroid("agg", new GeoPoint(10, 0), Long.MAX_VALUE, Collections.emptyMap()); InternalGeoCentroid reducingGeoCentroid = new InternalGeoCentroid("agg", new GeoPoint(10, 0), randomNonNegativeLong(), Collections.emptyMap()); InternalGeoCentroid reducedGeoCentroid = reducingGeoCentroid .reduce(Collections.singletonList(maxValueGeoCentroid), null); assertThat(reducedGeoCentroid.count(), equalTo(Long.MAX_VALUE)); }	usually the reducing result is in the list, right?
public static String aswktShape(Object v) { return GeoProcessor.GeoOperation.ASWKT_SHAPE.apply(v).toString(); }	minor: you could rename the param to wkt
private Response mockPerformRequest(Request request) throws IOException { assertThat(request.getOptions().getHeaders(), hasSize(1)); Header httpHeader = request.getOptions().getHeaders().get(0); final Response mockResponse = mock(Response.class); when(mockResponse.getHost()).thenReturn(new HttpHost("localhost", 9200)); ProtocolVersion protocol = new ProtocolVersion("HTTP", 1, 1); when(mockResponse.getStatusLine()).thenReturn(new BasicStatusLine(protocol, 200, "OK")); MainResponse response = new MainResponse(httpHeader.getValue(), Version.CURRENT, ClusterName.DEFAULT, "_na", "dynamic-"+ClusterName.DEFAULT, Build.CURRENT); BytesRef bytesRef = XContentHelper.toXContent(response, XContentType.JSON, false).toBytesRef(); when(mockResponse.getEntity()).thenReturn(new ByteArrayEntity(bytesRef.bytes, ContentType.APPLICATION_JSON)); RequestLine requestLine = new BasicRequestLine(HttpGet.METHOD_NAME, ENDPOINT, protocol); when(mockResponse.getRequestLine()).thenReturn(requestLine); return mockResponse; }	*usually* we add a space on both sides of the +.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("name", nodeName); builder.field("cluster_name", clusterName.value()); builder.field("cluster_uuid", clusterUuid); if (!Strings.isEmpty(clusterDynamicName)) { builder.field("cluster_dynamic_name", clusterDynamicName); } builder.startObject("version") .field("number", version.toString()) .field("build_flavor", build.flavor().displayName()) .field("build_type", build.type().displayName()) .field("build_hash", build.shortHash()) .field("build_date", build.date()) .field("build_snapshot", build.isSnapshot()) .field("lucene_version", version.luceneVersion.toString()) .field("minimum_wire_compatibility_version", version.minimumCompatibilityVersion().toString()) .field("minimum_index_compatibility_version", version.minimumIndexCompatibilityVersion().toString()) .endObject(); builder.field("tagline", "You Know, for Search"); builder.endObject(); return builder; }	false == strings is our style and i've grown to rather like it! i find it a little more obvious.
protected Response handle(final Request request) throws IOException { final String nonAuthorizedPath = "* " + request.getMethod() + " " + request.getPath(); final RequestHandler nonAuthorizedHandler = handlers.retrieve(nonAuthorizedPath, request.getParameters()); if (nonAuthorizedHandler != null) { return nonAuthorizedHandler.handle(request); } final String authorizedPath = "A " + request.getMethod() + " " + request.getPath(); final RequestHandler handler = handlers.retrieve(authorizedPath, request.getParameters()); if (handler != null) { final String authorization = request.getHeader("Authorization"); final String permittedBucket; if (authorization != null && authorization.contains("s3_integration_test_permanent_access_key")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken != null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Unexpected session token", ""); } permittedBucket = permanentBucketName; } else if (authorization != null && authorization.contains("s3_integration_test_temporary_access_key")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken == null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } if (sessionToken.equals("s3_integration_test_temporary_session_token") == false) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = temporaryBucketName; } else if (authorization != null && authorization.contains("securitycredentials42_KEYID")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken == null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } if (sessionToken.equals("securitycredentials42_TKN") == false) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = temporaryBucketName; } else { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad access key", ""); } if (handler != null) { final String bucket = request.getParam("bucket"); if (bucket != null && permittedBucket.equals(bucket) == false) { // allow a null bucket to support the multi-object-delete API which // passes the bucket name in the host header instead of the URL. if (buckets.containsKey(bucket)) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad bucket", ""); } else { return newBucketNotFoundError(request.getId(), bucket); } } return handler.handle(request); } else { return newInternalError(request.getId(), "No handler defined for request [" + request + "]"); } } return null; }	i'm not a big fan of the * vs a thing, at least not as literals. could you introduce constants so these things have more descriptive names?
protected Response handle(final Request request) throws IOException { final String nonAuthorizedPath = "* " + request.getMethod() + " " + request.getPath(); final RequestHandler nonAuthorizedHandler = handlers.retrieve(nonAuthorizedPath, request.getParameters()); if (nonAuthorizedHandler != null) { return nonAuthorizedHandler.handle(request); } final String authorizedPath = "A " + request.getMethod() + " " + request.getPath(); final RequestHandler handler = handlers.retrieve(authorizedPath, request.getParameters()); if (handler != null) { final String authorization = request.getHeader("Authorization"); final String permittedBucket; if (authorization != null && authorization.contains("s3_integration_test_permanent_access_key")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken != null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Unexpected session token", ""); } permittedBucket = permanentBucketName; } else if (authorization != null && authorization.contains("s3_integration_test_temporary_access_key")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken == null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } if (sessionToken.equals("s3_integration_test_temporary_session_token") == false) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = temporaryBucketName; } else if (authorization != null && authorization.contains("securitycredentials42_KEYID")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken == null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } if (sessionToken.equals("securitycredentials42_TKN") == false) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = temporaryBucketName; } else { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad access key", ""); } if (handler != null) { final String bucket = request.getParam("bucket"); if (bucket != null && permittedBucket.equals(bucket) == false) { // allow a null bucket to support the multi-object-delete API which // passes the bucket name in the host header instead of the URL. if (buckets.containsKey(bucket)) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad bucket", ""); } else { return newBucketNotFoundError(request.getId(), bucket); } } return handler.handle(request); } else { return newInternalError(request.getId(), "No handler defined for request [" + request + "]"); } } return null; }	could we handle the authorisation == null case first to avoid these null checks proliferating? it's also helpful to distinguish no authorisation header vs a bad one, which this would achieve.
protected Response handle(final Request request) throws IOException { final String nonAuthorizedPath = "* " + request.getMethod() + " " + request.getPath(); final RequestHandler nonAuthorizedHandler = handlers.retrieve(nonAuthorizedPath, request.getParameters()); if (nonAuthorizedHandler != null) { return nonAuthorizedHandler.handle(request); } final String authorizedPath = "A " + request.getMethod() + " " + request.getPath(); final RequestHandler handler = handlers.retrieve(authorizedPath, request.getParameters()); if (handler != null) { final String authorization = request.getHeader("Authorization"); final String permittedBucket; if (authorization != null && authorization.contains("s3_integration_test_permanent_access_key")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken != null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Unexpected session token", ""); } permittedBucket = permanentBucketName; } else if (authorization != null && authorization.contains("s3_integration_test_temporary_access_key")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken == null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } if (sessionToken.equals("s3_integration_test_temporary_session_token") == false) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = temporaryBucketName; } else if (authorization != null && authorization.contains("securitycredentials42_KEYID")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken == null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } if (sessionToken.equals("securitycredentials42_TKN") == false) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = temporaryBucketName; } else { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad access key", ""); } if (handler != null) { final String bucket = request.getParam("bucket"); if (bucket != null && permittedBucket.equals(bucket) == false) { // allow a null bucket to support the multi-object-delete API which // passes the bucket name in the host header instead of the URL. if (buckets.containsKey(bucket)) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad bucket", ""); } else { return newBucketNotFoundError(request.getId(), bucket); } } return handler.handle(request); } else { return newInternalError(request.getId(), "No handler defined for request [" + request + "]"); } } return null; }	could we extract securitycredentials42 out as a named constant so it's clear why it is what it is?
protected Response handle(final Request request) throws IOException { final String nonAuthorizedPath = "* " + request.getMethod() + " " + request.getPath(); final RequestHandler nonAuthorizedHandler = handlers.retrieve(nonAuthorizedPath, request.getParameters()); if (nonAuthorizedHandler != null) { return nonAuthorizedHandler.handle(request); } final String authorizedPath = "A " + request.getMethod() + " " + request.getPath(); final RequestHandler handler = handlers.retrieve(authorizedPath, request.getParameters()); if (handler != null) { final String authorization = request.getHeader("Authorization"); final String permittedBucket; if (authorization != null && authorization.contains("s3_integration_test_permanent_access_key")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken != null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Unexpected session token", ""); } permittedBucket = permanentBucketName; } else if (authorization != null && authorization.contains("s3_integration_test_temporary_access_key")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken == null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } if (sessionToken.equals("s3_integration_test_temporary_session_token") == false) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = temporaryBucketName; } else if (authorization != null && authorization.contains("securitycredentials42_KEYID")) { final String sessionToken = request.getHeader("x-amz-security-token"); if (sessionToken == null) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } if (sessionToken.equals("securitycredentials42_TKN") == false) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = temporaryBucketName; } else { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad access key", ""); } if (handler != null) { final String bucket = request.getParam("bucket"); if (bucket != null && permittedBucket.equals(bucket) == false) { // allow a null bucket to support the multi-object-delete API which // passes the bucket name in the host header instead of the URL. if (buckets.containsKey(bucket)) { return newError(request.getId(), RestStatus.FORBIDDEN, "AccessDenied", "Bad bucket", ""); } else { return newBucketNotFoundError(request.getId(), bucket); } } return handler.handle(request); } else { return newInternalError(request.getId(), "No handler defined for request [" + request + "]"); } } return null; }	as commented above, i think this should have its own bucket.
private void contextScrollKeepAlive(SearchContext context, long keepAlive) throws IOException { if (keepAlive > maxKeepAlive) { throw new IllegalArgumentException( "Keep alive for scroll (" + TimeValue.timeValueMillis(keepAlive).format() + ") is too large. " + "It must be less than (" + TimeValue.timeValueMillis(maxKeepAlive).format() + "). " + "This limit can be set by changing the [" + MAX_KEEPALIVE_SETTING.getKey() + "] cluster level setting."); } context.keepAlive(keepAlive); }	do you think it would be possible to have a unit test for this too?
@Override protected AggregatorFactory<?> doBuild(SearchContext context, AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException { int maxFilters = context.indexShard().indexSettings().getMaxAdjacencyMatrixFilters(); if (filters.size() > maxFilters){ throw new IllegalArgumentException( "Number of filters is too large, must be less than or equal to: [" + maxFilters + "] but was [" + filters.size() + "]." + "This limit can be set by changing the [" + IndexSettings.MAX_ADJACENCY_MATRIX_FILTERS_SETTING.getKey() + "] index level setting."); } List<KeyedFilter> rewrittenFilters = new ArrayList<>(filters.size()); for (KeyedFilter kf : filters) { rewrittenFilters.add(new KeyedFilter(kf.key(), Rewriteable.rewrite(kf.filter(), context.getQueryShardContext(), true))); } return new AdjacencyMatrixAggregatorFactory(name, rewrittenFilters, separator, context, parent, subFactoriesBuilder, metaData); }	could we have a unit test for this change too?
protected static PolygonBuilder parsePolygon(CoordinateNode coordinates) { LineStringBuilder shell = parseLineString(coordinates.children.get(0)); PolygonBuilder polygon = new PolygonBuilder(shell.points); for (int i = 1; i < coordinates.children.size(); i++) { polygon.hole(parseLineString(coordinates.children.get(i))).close(); } return polygon.close(); }	this is not going to be backwards compatible and it may surprise people by "properly" closing polygons that previously would have failed because they were not closed _without_ any mechanism to throw a validation error of some sort as suggested by you in #4085. personally, i think that i prefer the concept of autoclosing being non-default behavior because it matches existing documentation. however, maybe in a major release (2.0 perhaps or even a 1.x where x is a few releases later?) this could be changed to default to autoclose because it prevents the need to waste sending the extra point.
@Override public IndexCommitRef acquireIndexCommit(final boolean safeCommit, final boolean flushFirst) throws EngineException { // we have to flush outside of the readlock otherwise we might have a problem upgrading // the to a write lock when we fail the engine in this operation if (flushFirst) { logger.trace("start flush for snapshot"); flush(false, true); logger.trace("finish flush for snapshot"); } final IndexCommit snapshotCommit = combinedDeletionPolicy.acquireIndexCommit(safeCommit); return new Engine.IndexCommitRef(snapshotCommit, () -> combinedDeletionPolicy.releaseCommit(snapshotCommit)); }	this means a potential change to the exception type. can you double check it's ok?
public void testAcquireIndexCommit() throws Exception { final AtomicLong globalCheckpoint = new AtomicLong(); final UUID translogUUID = UUID.randomUUID(); TranslogDeletionPolicy translogPolicy = createTranslogDeletionPolicy(); CombinedDeletionPolicy indexPolicy = new CombinedDeletionPolicy(OPEN_INDEX_AND_TRANSLOG, translogPolicy, globalCheckpoint::get); final long maxSeqNo1 = between(1, 1000); final long translogGen1 = between(1, 100); final MockIndexCommit c1 = mockIndexCommit(maxSeqNo1, translogUUID, translogGen1); final long maxSeqNo2 = maxSeqNo1 + between(1, 1000); final long translogGen2 = translogGen1 + between(1, 100); final MockIndexCommit c2 = mockIndexCommit(maxSeqNo2, translogUUID, translogGen2); globalCheckpoint.set(randomLongBetween(0, maxSeqNo2 - 1)); // Keep both c1 and c2. indexPolicy.onCommit(Arrays.asList(c1, c2)); final IndexCommit ref1 = indexPolicy.acquireIndexCommit(true); assertThat(ref1, equalTo(c1)); expectThrows(UnsupportedOperationException.class, ref1::delete); final IndexCommit ref2 = indexPolicy.acquireIndexCommit(false); assertThat(ref2, equalTo(c2)); expectThrows(UnsupportedOperationException.class, ref2::delete); assertThat(translogPolicy.getMinTranslogGenerationForRecovery(), lessThanOrEqualTo(100L)); globalCheckpoint.set(randomLongBetween(maxSeqNo2, Long.MAX_VALUE)); indexPolicy.onCommit(Arrays.asList(c1, c2)); // Policy keeps c2 only, but c1 is snapshotted. assertThat(c1.deleteTimes(), equalTo(0)); final IndexCommit ref3 = indexPolicy.acquireIndexCommit(true); assertThat(ref3, equalTo(c2)); assertThat(translogPolicy.getMinTranslogGenerationForRecovery(), equalTo(translogGen1)); indexPolicy.releaseCommit(ref1); // release acquired commit releases translog and commit indexPolicy.onCommit(Arrays.asList(c1, c2)); // Flush new commit deletes c1 assertThat(c1.deleteTimes(), equalTo(1)); }	shouldn't this be exactly transloggen1?
public void testAcquireIndexCommit() throws Exception { final AtomicLong globalCheckpoint = new AtomicLong(); final UUID translogUUID = UUID.randomUUID(); TranslogDeletionPolicy translogPolicy = createTranslogDeletionPolicy(); CombinedDeletionPolicy indexPolicy = new CombinedDeletionPolicy(OPEN_INDEX_AND_TRANSLOG, translogPolicy, globalCheckpoint::get); final long maxSeqNo1 = between(1, 1000); final long translogGen1 = between(1, 100); final MockIndexCommit c1 = mockIndexCommit(maxSeqNo1, translogUUID, translogGen1); final long maxSeqNo2 = maxSeqNo1 + between(1, 1000); final long translogGen2 = translogGen1 + between(1, 100); final MockIndexCommit c2 = mockIndexCommit(maxSeqNo2, translogUUID, translogGen2); globalCheckpoint.set(randomLongBetween(0, maxSeqNo2 - 1)); // Keep both c1 and c2. indexPolicy.onCommit(Arrays.asList(c1, c2)); final IndexCommit ref1 = indexPolicy.acquireIndexCommit(true); assertThat(ref1, equalTo(c1)); expectThrows(UnsupportedOperationException.class, ref1::delete); final IndexCommit ref2 = indexPolicy.acquireIndexCommit(false); assertThat(ref2, equalTo(c2)); expectThrows(UnsupportedOperationException.class, ref2::delete); assertThat(translogPolicy.getMinTranslogGenerationForRecovery(), lessThanOrEqualTo(100L)); globalCheckpoint.set(randomLongBetween(maxSeqNo2, Long.MAX_VALUE)); indexPolicy.onCommit(Arrays.asList(c1, c2)); // Policy keeps c2 only, but c1 is snapshotted. assertThat(c1.deleteTimes(), equalTo(0)); final IndexCommit ref3 = indexPolicy.acquireIndexCommit(true); assertThat(ref3, equalTo(c2)); assertThat(translogPolicy.getMinTranslogGenerationForRecovery(), equalTo(translogGen1)); indexPolicy.releaseCommit(ref1); // release acquired commit releases translog and commit indexPolicy.onCommit(Arrays.asList(c1, c2)); // Flush new commit deletes c1 assertThat(c1.deleteTimes(), equalTo(1)); }	can we also release c2 and see that it is not deleted? i would also appreciate some randomness here - i.e., a series of commits, choose snapshot at random times. release at random order and check that all is ok.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(Fields.JVM); builder.field(Fields.PID, pid); builder.field(Fields.VERSION, version); builder.field(Fields.VM_NAME, vmName); builder.field(Fields.VM_VERSION, vmVersion); builder.field(Fields.VM_VENDOR, vmVendor); builder.dateValueField(Fields.START_TIME_IN_MILLIS, Fields.START_TIME, startTime); builder.startObject(Fields.MEM); builder.byteSizeField(Fields.HEAP_INIT_IN_BYTES, Fields.HEAP_INIT, mem.heapInit); builder.byteSizeField(Fields.HEAP_MAX_IN_BYTES, Fields.HEAP_MAX, mem.heapMax); builder.byteSizeField(Fields.NON_HEAP_INIT_IN_BYTES, Fields.NON_HEAP_INIT, mem.nonHeapInit); builder.byteSizeField(Fields.NON_HEAP_MAX_IN_BYTES, Fields.NON_HEAP_MAX, mem.nonHeapMax); builder.byteSizeField(Fields.DIRECT_MAX_IN_BYTES, Fields.DIRECT_MAX, mem.directMemoryMax); builder.endObject(); builder.field(Fields.GC_COLLECTORS, gcCollectors); builder.field(Fields.MEMORY_POOLS, memoryPools); builder.field(Fields.USING_COMPRESSED_OOPS, useCompressedOops); builder.endObject(); return builder; }	if it can be null we should skip it here.
@Override public void readFrom(StreamInput in) throws IOException { pid = in.readLong(); version = in.readString(); vmName = in.readString(); vmVersion = in.readString(); vmVendor = in.readString(); startTime = in.readLong(); inputArguments = new String[in.readInt()]; for (int i = 0; i < inputArguments.length; i++) { inputArguments[i] = in.readString(); } bootClassPath = in.readString(); classPath = in.readString(); systemProperties = new HashMap<>(); int size = in.readInt(); for (int i = 0; i < size; i++) { systemProperties.put(in.readString(), in.readString()); } mem = new Mem(); mem.readFrom(in); gcCollectors = in.readStringArray(); memoryPools = in.readStringArray(); useCompressedOops = in.readOptionalString(); }	can it be null?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeString(cursor); if (out.getVersion().onOrAfter(Version.V_8_3_0)) { out.writeOptionalBoolean(binaryCommunication); } }	shouldn't the version be 8.2?
public static SqlStreamInput fromString(String base64encoded, NamedWriteableRegistry namedWriteableRegistry, Version version) throws IOException { byte[] bytes = Base64.getDecoder().decode(base64encoded); StreamInput in = StreamInput.wrap(bytes); Version inVersion = Version.readVersion(in); if (version.compareTo(inVersion) != 0) { throw new QlVersionMismatchException("cursor", inVersion, version); } InputStreamStreamInput uncompressingIn = new InputStreamStreamInput(CompressorFactory.COMPRESSOR.threadLocalInputStream(in)); return new SqlStreamInput(uncompressingIn, namedWriteableRegistry, inVersion); }	check whether the upstream (rest endpoint) accepts ql exceptions and not sql ones - if that's the case either keep the sql exception in place or introduce a ql one for sql.
@Override public Geometry visit(Point point) { double[] latlon = new double[]{point.getX(), point.getY()}; normalizePoint(latlon); return new Point(latlon[0], latlon[1]); }	is there a difference between normalizepoint and calling normalizelon and normalizelat directly on these values?
private static Query tryRewriteLongSort(SearchContext searchContext, IndexReader reader, Query query, boolean hasFilterCollector) throws IOException { if (searchContext.searchAfter() != null) return null; if (searchContext.scrollContext() != null) return null; if (searchContext.collapse() != null) return null; if (searchContext.trackScores()) return null; if (searchContext.aggregations() != null) return null; Sort sort = searchContext.sort().sort; SortField sortField = sort.getSort()[0]; if (SortField.Type.LONG.equals(IndexSortConfig.getSortFieldType(sortField)) == false) return null; // check if this is a field of type Long or Date, that is indexed and has doc values String fieldName = sortField.getField(); if (fieldName == null) return null; // happens when _score or _doc is the 1st sort field if (searchContext.mapperService() == null) return null; // mapperService can be null in tests final MappedFieldType fieldType = searchContext.mapperService().fullName(fieldName); if (fieldType == null) return null; // for unmapped fields, default behaviour depending on "unmapped_type" flag if ((fieldType.typeName().equals("long") == false) && (fieldType instanceof DateFieldType == false)) return null; if (fieldType.indexOptions() == IndexOptions.NONE) return null; //TODO: change to pointDataDimensionCount() when implemented if (fieldType.hasDocValues() == false) return null; // check that all sorts are actual document fields or _doc for (int i = 1; i < sort.getSort().length; i++) { SortField sField = sort.getSort()[i]; String sFieldName = sField.getField(); if (sFieldName == null) { if (SortField.FIELD_DOC.equals(sField) == false) return null; } else { if (searchContext.mapperService().fullName(sFieldName) == null) return null; // could be _script field that uses _score } } // check that setting of missing values allows optimization if (sortField.getMissingValue() == null) return null; Long missingValue = (Long) sortField.getMissingValue(); boolean missingValuesAccordingToSort = (sortField.getReverse() && (missingValue == Long.MIN_VALUE)) || ((sortField.getReverse() == false) && (missingValue == Long.MAX_VALUE)); if (missingValuesAccordingToSort == false) return null; int docCount = PointValues.getDocCount(reader, fieldName); // is not worth to run optimization on small index, also estimation of duplicate data doesn't work well on small index if (docCount <= 512) return null; // check for multiple values if (PointValues.size(reader, fieldName) != docCount) return null; //TODO: handle multiple values // check if the optimization makes sense with the track_total_hits setting if (searchContext.trackTotalHitsUpTo() == Integer.MAX_VALUE) { // with filter, we can't pre-calculate hitsCount, we need to explicitly calculate them => optimization does't make sense if (hasFilterCollector) return null; // if we can't pre-calculate hitsCount based on the query type, optimization does't make sense if (shortcutTotalHitCount(reader, query) == -1) return null; } byte[] minValueBytes = PointValues.getMinPackedValue(reader, fieldName); byte[] maxValueBytes = PointValues.getMaxPackedValue(reader, fieldName); if ((maxValueBytes == null) || (minValueBytes == null)) return null; long minValue = LongPoint.decodeDimension(minValueBytes, 0); long maxValue = LongPoint.decodeDimension(maxValueBytes, 0); Query rewrittenQuery; if (minValue == maxValue) { rewrittenQuery = new DocValuesFieldExistsQuery(fieldName); } else { if (indexFieldHasDuplicateData(reader, fieldName)) return null; long origin = (sortField.getReverse()) ? maxValue : minValue; long pivotDistance = (maxValue - minValue) >>> 1; // division by 2 on the unsigned representation to avoid overflow if (pivotDistance == 0) { // 0 if maxValue = (minValue + 1) pivotDistance = 1; } rewrittenQuery = LongPoint.newDistanceFeatureQuery(sortField.getField(), 1, origin, pivotDistance); } rewrittenQuery = new BooleanQuery.Builder() .add(query, BooleanClause.Occur.FILTER) // filter for original query .add(rewrittenQuery, BooleanClause.Occur.SHOULD) //should for rewrittenQuery .build(); return rewrittenQuery; }	i understand that we need a threshold but i don't understand this comment "estimation of duplicate data doesn't work well on small index", can you explain ?
static boolean indexFieldHasDuplicateData(IndexReader reader, String field) throws IOException { int duplicateSegments = 0; int noDuplicateSegments = 0; for (LeafReaderContext lrc : reader.leaves()) { PointValues pointValues = lrc.reader().getPointValues(field); int thresholdDocCount = pointValues.getDocCount()/2; byte[] minValueAsBytes = pointValues.getMinPackedValue(); byte[] maxValueAsBytes = pointValues.getMaxPackedValue(); long minValue = LongPoint.decodeDimension(minValueAsBytes, 0); long maxValue = LongPoint.decodeDimension(maxValueAsBytes, 0); long medianCount = estimateMedianCount(pointValues, minValue, maxValue, thresholdDocCount); if (medianCount > thresholdDocCount) { duplicateSegments++; } else { noDuplicateSegments++; } } return (duplicateSegments >= noDuplicateSegments); }	can we compute the global count of all medians and compare with the total doc count divided by 2 ? currently the heuristic does not take the size of segments into account.
public void testNumericLongOrDateSortOptimization() throws Exception { final String fieldNameLong = "long-field"; final String fieldNameDate = "date-field"; MappedFieldType fieldTypeLong = new NumberFieldMapper.NumberFieldType(NumberFieldMapper.NumberType.LONG); MappedFieldType fieldTypeDate = new DateFieldMapper.Builder(fieldNameDate).fieldType(); MapperService mapperService = mock(MapperService.class); when(mapperService.fullName(fieldNameLong)).thenReturn(fieldTypeLong); when(mapperService.fullName(fieldNameDate)).thenReturn(fieldTypeDate); TestSearchContext searchContext = spy(new TestSearchContext(null, indexShard)); when(searchContext.mapperService()).thenReturn(mapperService); final int numDocs = 10000; Directory dir = newDirectory(); IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(null)); for (int i = 0; i < numDocs; ++i) { Document doc = new Document(); long longValue = randomLongBetween(-10000000L, 10000000L); doc.add(new LongPoint(fieldNameLong, longValue)); doc.add(new NumericDocValuesField(fieldNameLong, longValue)); longValue = randomLongBetween(0, 3000000000000L); doc.add(new LongPoint(fieldNameDate, longValue)); doc.add(new NumericDocValuesField(fieldNameDate, longValue)); writer.addDocument(doc); } writer.close(); final IndexReader reader = DirectoryReader.open(dir); IndexSearcher searcher = getAssertingSortOptimizedSearcher(reader, 0); // 1. Test a sort on long field final SortField sortFieldLong = new SortField(fieldNameLong, SortField.Type.LONG); sortFieldLong.setMissingValue(Long.MAX_VALUE); final Sort longSort = new Sort(sortFieldLong); SortAndFormats sortAndFormats = new SortAndFormats(longSort, new DocValueFormat[]{DocValueFormat.RAW}); searchContext.sort(sortAndFormats); searchContext.parsedQuery(new ParsedQuery(new MatchAllDocsQuery())); searchContext.setTask(new SearchTask(123L, "", "", "", null, Collections.emptyMap())); searchContext.setSize(10); QueryPhase.execute(searchContext, searcher, checkCancelled -> {}); assertSortResults(searchContext.queryResult().topDocs().topDocs, (long) numDocs, false); // 2. Test a sort on long field + date field final SortField sortFieldDate = new SortField(fieldNameDate, SortField.Type.LONG); DocValueFormat dateFormat = fieldTypeDate.docValueFormat(null, null); final Sort longDateSort = new Sort(sortFieldLong, sortFieldDate); sortAndFormats = new SortAndFormats(longDateSort, new DocValueFormat[]{DocValueFormat.RAW, dateFormat}); searchContext.sort(sortAndFormats); QueryPhase.execute(searchContext, searcher, checkCancelled -> {}); assertSortResults(searchContext.queryResult().topDocs().topDocs, (long) numDocs, true); // 3. Test a sort on date field sortFieldDate.setMissingValue(Long.MAX_VALUE); final Sort dateSort = new Sort(sortFieldDate); sortAndFormats = new SortAndFormats(dateSort, new DocValueFormat[]{dateFormat}); searchContext.sort(sortAndFormats); QueryPhase.execute(searchContext, searcher, checkCancelled -> {}); assertSortResults(searchContext.queryResult().topDocs().topDocs, (long) numDocs, false); // 4. Test a sort on date field + long field final Sort dateLongSort = new Sort(sortFieldDate, sortFieldLong); sortAndFormats = new SortAndFormats(dateLongSort, new DocValueFormat[]{dateFormat, DocValueFormat.RAW}); searchContext.sort(sortAndFormats); QueryPhase.execute(searchContext, searcher, checkCancelled -> {}); assertSortResults(searchContext.queryResult().topDocs().topDocs, (long) numDocs, true); reader.close(); dir.close(); }	that's a big number, can we test with smaller values ?
*/ @SuppressWarnings("rawtypes") public Query toFilter(ShardSearchRequest request, QueryShardContext context) { final MappedFieldType type = context.getFieldType(field); if (type == null) { throw new IllegalArgumentException("field " + field + " not found"); } int shardIndex = request.shardIndex() != -1 ? request.shardIndex() : request.shardId().id(); int numShards = request.shardIndex() != -1 ? request.numberOfShards() : context.getIndexSettings().getNumberOfShards(); String field = this.field; boolean useTermQuery = false; if (IdFieldMapper.NAME.equals(field)) { useTermQuery = true; } else if (type.hasDocValues() == false) { throw new IllegalArgumentException("cannot load numeric doc values on " + field); } else { IndexFieldData ifm = context.getForField(type); if (ifm instanceof IndexNumericFieldData == false) { throw new IllegalArgumentException("cannot load numeric doc values on " + field); } } if (numShards == 1) { return useTermQuery ? new TermsSliceQuery(field, id, max) : new DocValuesSliceQuery(field, id, max); } if (max >= numShards) { // the number of slices is greater than the number of shards // in such case we can reduce the number of requested shards by slice // first we check if the slice is responsible of this shard int targetShard = id % numShards; if (targetShard != shardIndex) { // the shard is not part of this slice, we can skip it. return new MatchNoDocsQuery("this shard is not part of the slice"); } // compute the number of slices where this shard appears int numSlicesInShard = max / numShards; int rest = max % numShards; if (rest > targetShard) { numSlicesInShard++; } if (numSlicesInShard == 1) { // this shard has only one slice so we must check all the documents return new MatchAllDocsQuery(); } // get the new slice id for this shard int shardSlice = id / numShards; return useTermQuery ? new TermsSliceQuery(field, shardSlice, numSlicesInShard) : new DocValuesSliceQuery(field, shardSlice, numSlicesInShard); } // the number of shards is greater than the number of slices // check if the shard is assigned to the slice int targetSlice = shardIndex % max; if (id != targetSlice) { // the shard is not part of this slice, we can skip it. return new MatchNoDocsQuery("this shard is not part of the slice"); } return new MatchAllDocsQuery(); }	i guess a case when shardidex() = -1 (when a coordinating node is v < 8.0) and there is a preference or indexrouting present in scroll slice is a very rare case, and we can disregard it.
@Override public String toString() { StringBuilder sb = new StringBuilder(); sb.append(major).append('.').append(minor).append('.').append(revision); if (isAlpha()) { sb.append("-alpha"); sb.append(build); } else if (isBeta()) { if (major >= 2) { sb.append("-beta"); } else { sb.append(".Beta"); } sb.append(major < 5 ? build : build-25); } else if (build < 99) { if (major >= 2) { sb.append("-rc"); } else { sb.append(".RC"); } sb.append(build - 50); } return sb.toString(); }	i foresee being confused about whether we should use this or tostring. maybe we should just use tostring in this case?
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final ClusterStateRequest clusterStateRequest = Requests.clusterStateRequest(); clusterStateRequest.indicesOptions(IndicesOptions.fromRequest(request, clusterStateRequest.indicesOptions())); clusterStateRequest.local(request.paramAsBoolean("local", clusterStateRequest.local())); clusterStateRequest.masterNodeTimeout(request.paramAsTime("master_timeout", clusterStateRequest.masterNodeTimeout())); if (request.hasParam("wait_for_metadata_version")) { clusterStateRequest.waitForMetadataVersion(request.paramAsLong("wait_for_metadata_version", 0)); } clusterStateRequest.waitForTimeout(request.paramAsTime("wait_for_timeout", ClusterStateRequest.DEFAULT_WAIT_FOR_NODE_TIMEOUT)); final String[] indices = Strings.splitStringByCommaToArray(request.param("indices", "_all")); boolean isAllIndicesOnly = indices.length == 1 && "_all".equals(indices[0]); if (!isAllIndicesOnly) { clusterStateRequest.indices(indices); } if (request.hasParam("metric")) { EnumSet<ClusterState.Metric> metrics = ClusterState.Metric.parseString(request.param("metric"), true); // do not ask for what we do not need. clusterStateRequest.nodes(metrics.contains(ClusterState.Metric.NODES) || metrics.contains(ClusterState.Metric.MASTER_NODE)); /* * there is no distinction in Java api between routing_table and routing_nodes, it's the same info set over the wire, one single * flag to ask for it */ clusterStateRequest.routingTable( metrics.contains(ClusterState.Metric.ROUTING_TABLE) || metrics.contains(ClusterState.Metric.ROUTING_NODES)); clusterStateRequest.metadata(metrics.contains(ClusterState.Metric.METADATA)); clusterStateRequest.blocks(metrics.contains(ClusterState.Metric.BLOCKS)); clusterStateRequest.customs(metrics.contains(ClusterState.Metric.CUSTOMS)); } settingsFilter.addFilterSettingParams(request); return channel -> client.admin().cluster().state(clusterStateRequest, new RestActionListener<>(channel) { @Override protected void processResponse(ClusterStateResponse response) { final long startTimeMs = threadPool.relativeTimeInMillis(); // Process serialization on MANAGEMENT pool since the serialization of the cluster state to XContent // can be too slow to execute on an IO thread threadPool.executor(ThreadPool.Names.MANAGEMENT).execute( ActionRunnable.wrap(this, l -> new RestBuilderListener<ClusterStateResponse>(channel) { @Override public RestResponse buildResponse(final ClusterStateResponse response, final XContentBuilder builder) throws Exception { if (clusterStateRequest.local() == false && threadPool.relativeTimeInMillis() - startTimeMs > clusterStateRequest.masterNodeTimeout().millis()) { throw new ElasticsearchTimeoutException("Timed out getting cluster state"); } builder.startObject(); if (clusterStateRequest.waitForMetadataVersion() != null) { builder.field(Fields.WAIT_FOR_TIMED_OUT, response.isWaitForTimedOut()); } builder.field(Fields.CLUSTER_NAME, response.getClusterName().value()); ToXContent.Params params = new ToXContent.DelegatingMapParams( singletonMap(Metadata.CONTEXT_MODE_PARAM, Metadata.CONTEXT_MODE_API), request); response.getState().toXContent(builder, params); builder.endObject(); return new BytesRestResponse(RestStatus.OK, builder); } }.onResponse(response))); } }); }	seems a bit weird to create the listener just to immediately complete it. why not inline this?
public void testWaitForMetadataVersion() throws Exception { ClusterStateRequest clusterStateRequest = new ClusterStateRequest(); clusterStateRequest.waitForTimeout(TimeValue.timeValueHours(1)); ClusterStateResponse response = client().admin().cluster().state(clusterStateRequest).get(10L, TimeUnit.SECONDS); assertThat(response.isWaitForTimedOut(), is(false)); long metadataVersion = response.getState().getMetadata().version(); // Verify that cluster state api returns after the cluster settings have been updated: clusterStateRequest = new ClusterStateRequest(); clusterStateRequest.waitForMetadataVersion(metadataVersion + 1); ActionFuture<ClusterStateResponse> future2 = client().admin().cluster().state(clusterStateRequest); assertThat(future2.isDone(), is(false)); ClusterUpdateSettingsRequest updateSettingsRequest = new ClusterUpdateSettingsRequest(); // Pick an arbitrary dynamic cluster setting and change it. Just to get metadata version incremented: updateSettingsRequest.transientSettings(Settings.builder().put("cluster.max_shards_per_node", 999)); assertAcked(client().admin().cluster().updateSettings(updateSettingsRequest).actionGet()); response = future2.get(10L, TimeUnit.SECONDS); assertThat(response.isWaitForTimedOut(), is(false)); assertThat(response.getState().metadata().version(), equalTo(metadataVersion + 1)); // Verify that the timed out property has been set" metadataVersion = response.getState().getMetadata().version(); clusterStateRequest.waitForMetadataVersion(metadataVersion + 1); clusterStateRequest.waitForTimeout(TimeValue.timeValueMillis(500)); // Fail fast ActionFuture<ClusterStateResponse> future3 = client().admin().cluster().state(clusterStateRequest); response = future3.get(10L, TimeUnit.SECONDS); assertThat(response.isWaitForTimedOut(), is(true)); assertThat(response.getState(), nullValue()); // Remove transient setting, otherwise test fails with the reason that this test leaves state behind: updateSettingsRequest = new ClusterUpdateSettingsRequest(); updateSettingsRequest.transientSettings(Settings.builder().put("cluster.max_shards_per_node", (String) null)); assertAcked(client().admin().cluster().updateSettings(updateSettingsRequest).actionGet()); }	i kept this test cleanup even though it's not necessary now with the transport action changes reverted because busy asserting on futures was just weird ...
private static Bucket createFromStream(StreamInput in, DocValueFormat format, boolean keyed) throws IOException { String key = in.getVersion().equals(Version.V_8_0_0) ? in.readString() : in.getVersion().onOrAfter(Version.V_7_17_1) ? in.readOptionalString() : in.readString(); BytesRef from = in.readBoolean() ? in.readBytesRef() : null; BytesRef to = in.readBoolean() ? in.readBytesRef() : null; long docCount = in.readLong(); InternalAggregations aggregations = InternalAggregations.readFrom(in); return new Bucket(format, keyed, key, from, to, docCount, aggregations); }	maybe good to put a comment here in word saying what this does.
@Override public void writeTo(StreamOutput out) throws IOException { if (out.getVersion().equals(Version.V_8_0_0)) { out.writeString(key == null ? generateKey(from, to, format) : key); } else { if (out.getVersion().onOrAfter(Version.V_7_17_1)) { out.writeOptionalString(key); } else { out.writeString(key == null ? generateKey(from, to, format) : key); } } out.writeBoolean(from != null); if (from != null) { out.writeBytesRef(from); } out.writeBoolean(to != null); if (to != null) { out.writeBytesRef(to); } out.writeLong(docCount); aggregations.writeTo(out); }	maybe more readable with } else if {
@Override public void writeTo(StreamOutput out) throws IOException { if (out.getVersion().equals(Version.V_8_0_0)) { out.writeString(key == null ? generateKey(from, to, format) : key); } else { if (out.getVersion().onOrAfter(Version.V_7_17_1)) { out.writeOptionalString(key); } else { out.writeString(key == null ? generateKey(from, to, format) : key); } } out.writeDouble(from); if (out.getVersion().onOrAfter(Version.V_7_17_0) && out.getVersion().before(Version.V_8_2_0)) { out.writeOptionalDouble(from); } out.writeDouble(to); if (out.getVersion().onOrAfter(Version.V_7_17_0) && out.getVersion().before(Version.V_8_2_0)) { out.writeOptionalDouble(to); } out.writeVLong(docCount); aggregations.writeTo(out); }	i think you should move the v_8_2_0 thing to a different pr. it'll save a byte which is great but it makes the pr hard to backport.
public void execute(Task task) { checkstyleDir.mkdirs(); try (InputStream stream = checkstyleConfUrl.openStream()) { Files.copy(stream, checkstyleConf.toPath(), StandardCopyOption.REPLACE_EXISTING); } catch (IOException e) { throw new UncheckedIOException(e); } try (InputStream stream = checkstyleSuppressionsUrl.openStream()) { Files.copy(stream, checkstyleSuppressions.toPath(), StandardCopyOption.REPLACE_EXISTING); } catch (IOException e) { throw new UncheckedIOException(e); } }	setenabled has been deprecated
private String runForbiddenAPIsCli() throws IOException { ByteArrayOutputStream errorOut = new ByteArrayOutputStream(); ExecResult result = getProject().javaexec(spec -> { if (javaHome != null) { spec.setExecutable(javaHome + "/bin/java"); } spec.classpath( getForbiddenAPIsConfiguration(), getRuntimeConfiguration(), getProject().getConfigurations().getByName(CompileOnlyResolvePlugin.RESOLVEABLE_COMPILE_ONLY_CONFIGURATION_NAME) ); spec.jvmArgs("-Xmx1g"); spec.getMainClass().set("de.thetaphi.forbiddenapis.cli.CliMain"); spec.args("-f", getSignatureFile().getAbsolutePath(), "-d", getJarExpandDir(), "--allowmissingclasses"); spec.setErrorOutput(errorOut); if (getLogger().isInfoEnabled() == false) { spec.setStandardOutput(new NullOutputStream()); } spec.setIgnoreExitValue(true); }); if (OS.current().equals(OS.LINUX) && result.getExitValue() == SIG_KILL_EXIT_VALUE) { throw new IllegalStateException("Third party audit was killed buy SIGKILL, could be a victim of the Linux OOM killer"); } final String forbiddenApisOutput; try (ByteArrayOutputStream outputStream = errorOut) { forbiddenApisOutput = outputStream.toString(StandardCharsets.UTF_8.name()); } if (EXPECTED_EXIT_CODES.contains(result.getExitValue()) == false) { throw new IllegalStateException("Forbidden APIs cli failed: " + forbiddenApisOutput); } return forbiddenApisOutput; }	getmain has been deprecated
ResolvedIndices resolveIndicesAndAliases(String action, IndicesRequest indicesRequest) { assert false == requiresWildcardExpansion(indicesRequest) : "indices must not require wildcard expansion"; final String[] indices = indicesRequest.indices(); if (indices == null || indices.length == 0) { throw new IllegalArgumentException("the action " + action + " requires explicit index names, but none were provided"); } if (IndexNameExpressionResolver.isAllIndices(Arrays.asList(indices))) { throw new IllegalArgumentException( "the action " + action + " does not support accessing all indices;" + " the provided index expression [" + Strings.arrayToCommaDelimitedString(indices) + "] is not allowed" ); } // TODO: Shard level requests have wildcard expanded already and do not need go through this check final List<String> wildcards = Stream.of(indices).filter(Regex::isSimpleMatchPattern).collect(Collectors.toList()); if (wildcards.isEmpty() == false) { throw new IllegalArgumentException( "the action " + action + " does not support wildcards;" + " the provided index expression(s) [" + Strings.collectionToCommaDelimitedString(wildcards) + "] are not allowed" ); } //NOTE: shard level requests do support wildcards (as they hold the original indices options) but don't support // replacing their indices. //That is fine though because they never contain wildcards, as they get replaced as part of the authorization of their //corresponding parent request on the coordinating node. Hence wildcards don't need to get replaced nor exploded for // shard level requests. final List<String> localIndices = new ArrayList<>(indices.length); for (String name : indices) { localIndices.add(nameExpressionResolver.resolveDateMathExpression(name)); } return new ResolvedIndices(localIndices, List.of()); }	similarly, this loop is *not* needed if the request is not directly created for rest level request. that is, for request like shardsearchrequest, we can skip it for some performance.
static String convertResponseToJson(Response response) throws IOException { HttpEntity entity = response.getEntity(); if (entity == null) { throw new IllegalStateException("Response body expected but not returned"); } if (entity.getContentType() == null) { throw new IllegalStateException("Elasticsearch didn't return the [Content-Type] header, unable to parse response body"); } XContentType xContentType = XContentType.fromMediaTypeOrFormat(entity.getContentType().getValue()); if (xContentType == null) { throw new IllegalStateException("Unsupported Content-Type: " + entity.getContentType().getValue()); } if (xContentType == XContentType.JSON) { // No changes is required return Streams.copyToString(new InputStreamReader(response.getEntity().getContent(), StandardCharsets.UTF_8)); } else { // Need to convert into JSON try (InputStream stream = response.getEntity().getContent(); XContentParser parser = XContentFactory.xContent(xContentType).createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, stream)) { parser.nextToken(); XContentBuilder builder = XContentFactory.jsonBuilder(); builder.copyCurrentStructure(parser); return Strings.toString(builder); } } }	would it be ok for us to do entityutils.tostring and leave it at that? do we need to parse the response? if we make the request with the json content-type header we can be fairly sure it'll come back json. right?
public static SignificantTermsAggregationBuilder significantTerms(String name) { return new SignificantTermsAggregationBuilder(name); } /** * Create a new {@link SignificantTextAggregationBuilder}	i'm slightly surprised to not have found a method in this collection for creating a raretermsaggregationbuilder, but there isn't one.
static <VB extends CompositeValuesSourceBuilder<VB>, T> void declareValuesSourceFields(AbstractObjectParser<VB, T> objectParser, ValueType expectedValueType) { objectParser.declareField(VB::field, XContentParser::text, new ParseField("field"), ObjectParser.ValueType.STRING); objectParser.declareBoolean(VB::missingBucket, new ParseField("missing_bucket")); objectParser.declareField(VB::valueType, p -> { ValueType valueType = ValueType.resolveForScript(p.text()); if (expectedValueType != null && valueType.isNotA(expectedValueType)) { throw new ParsingException(p.getTokenLocation(), "Aggregation [" + objectParser.getName() + "] was configured with an incompatible value type [" + valueType + "]. It can only work on value off type [" + expectedValueType + "]"); } return valueType; }, new ParseField("value_type"), ObjectParser.ValueType.STRING); objectParser.declareField(VB::script, (parser, context) -> Script.parse(parser), Script.SCRIPT_PARSE_FIELD, ObjectParser.ValueType.OBJECT_OR_STRING); objectParser.declareField(VB::order, XContentParser::text, new ParseField("order"), ObjectParser.ValueType.STRING); }	i think this added a typo :) "of" -> "off"
@Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, HISTOGRAM(EquivalenceType.HISTOGRAM) { @Override public ValuesSource getEmpty() { // TODO: Is this the correct exception type here? throw new IllegalArgumentException("Can't deal with unmapped ValuesSource type " + this.value()); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { throw new AggregationExecutionException("value source of type [" + this.value() + "] is not supported by scripts"); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { final IndexFieldData<?> indexFieldData = fieldContext.indexFieldData(); if (!(indexFieldData instanceof IndexHistogramFieldData)) { throw new IllegalArgumentException("Expected histogram type on field [" + fieldContext.field() + "], but got [" + fieldContext.fieldType().typeName() + "]"); } return new ValuesSource.Histogram.Fielddata((IndexHistogramFieldData) indexFieldData); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, // TODO: Ordinal Numbering sync with types from master IP(EquivalenceType.STRING) { @Override public ValuesSource getEmpty() { return BYTES.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return BYTES.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return BYTES.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return BYTES.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return DocValueFormat.IP; } }, DATE(EquivalenceType.NUMBER) { @Override public ValuesSource getEmpty() { return NUMERIC.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return NUMERIC.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return NUMERIC.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return NUMERIC.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return new DocValueFormat.DateTime( format == null ? DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER : DateFormatter.forPattern(format), tz == null ? ZoneOffset.UTC : tz, // If we were just looking at fields, we could read the resolution from the field settings, but we need to deal with script // output, which has no way to indicate the resolution, so we need to default to something. Milliseconds is the standard. DateFieldMapper.Resolution.MILLISECONDS); } }, BOOLEAN(EquivalenceType.NUMBER) { @Override public ValuesSource getEmpty() { return NUMERIC.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return NUMERIC.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return NUMERIC.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return NUMERIC.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return DocValueFormat.BOOLEAN; } } ; enum EquivalenceType { STRING, NUMBER, GEO, RANGE, HISTOGRAM; } EquivalenceType equivalenceType; CoreValuesSourceType(EquivalenceType equivalenceType) { this.equivalenceType = equivalenceType; } @Override public boolean isCastableTo(ValuesSourceType valuesSourceType) { if (valuesSourceType instanceof CoreValuesSourceType == false) { return false; } CoreValuesSourceType other = (CoreValuesSourceType) valuesSourceType; return this.equivalenceType == other.equivalenceType; } public static ValuesSourceType fromString(String name) { return valueOf(name.trim().toUpperCase(Locale.ROOT)); } public static ValuesSourceType fromStream(StreamInput in) throws IOException { return in.readEnum(CoreValuesSourceType.class); } @Override public void writeTo(StreamOutput out) throws IOException { CoreValuesSourceType state = this; out.writeEnum(state); }	the three new corevaluessourcetypes exist mosty to hold formatters, functionality migrated from valuetype. the goal is for valuesourcetype to hold all the information the aggregation needs to parse values, rather than having it spread between two or more classes.
@Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, HISTOGRAM(EquivalenceType.HISTOGRAM) { @Override public ValuesSource getEmpty() { // TODO: Is this the correct exception type here? throw new IllegalArgumentException("Can't deal with unmapped ValuesSource type " + this.value()); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { throw new AggregationExecutionException("value source of type [" + this.value() + "] is not supported by scripts"); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { final IndexFieldData<?> indexFieldData = fieldContext.indexFieldData(); if (!(indexFieldData instanceof IndexHistogramFieldData)) { throw new IllegalArgumentException("Expected histogram type on field [" + fieldContext.field() + "], but got [" + fieldContext.fieldType().typeName() + "]"); } return new ValuesSource.Histogram.Fielddata((IndexHistogramFieldData) indexFieldData); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, // TODO: Ordinal Numbering sync with types from master IP(EquivalenceType.STRING) { @Override public ValuesSource getEmpty() { return BYTES.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return BYTES.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return BYTES.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return BYTES.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return DocValueFormat.IP; } }, DATE(EquivalenceType.NUMBER) { @Override public ValuesSource getEmpty() { return NUMERIC.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return NUMERIC.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return NUMERIC.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return NUMERIC.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return new DocValueFormat.DateTime( format == null ? DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER : DateFormatter.forPattern(format), tz == null ? ZoneOffset.UTC : tz, // If we were just looking at fields, we could read the resolution from the field settings, but we need to deal with script // output, which has no way to indicate the resolution, so we need to default to something. Milliseconds is the standard. DateFieldMapper.Resolution.MILLISECONDS); } }, BOOLEAN(EquivalenceType.NUMBER) { @Override public ValuesSource getEmpty() { return NUMERIC.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return NUMERIC.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return NUMERIC.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return NUMERIC.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return DocValueFormat.BOOLEAN; } } ; enum EquivalenceType { STRING, NUMBER, GEO, RANGE, HISTOGRAM; } EquivalenceType equivalenceType; CoreValuesSourceType(EquivalenceType equivalenceType) { this.equivalenceType = equivalenceType; } @Override public boolean isCastableTo(ValuesSourceType valuesSourceType) { if (valuesSourceType instanceof CoreValuesSourceType == false) { return false; } CoreValuesSourceType other = (CoreValuesSourceType) valuesSourceType; return this.equivalenceType == other.equivalenceType; } public static ValuesSourceType fromString(String name) { return valueOf(name.trim().toUpperCase(Locale.ROOT)); } public static ValuesSourceType fromStream(StreamInput in) throws IOException { return in.readEnum(CoreValuesSourceType.class); } @Override public void writeTo(StreamOutput out) throws IOException { CoreValuesSourceType state = this; out.writeEnum(state); }	is this basically the less-bad version of the valuetype#isa() stuff we were discussing earlier? do we feel this is temporary or permanent?
@Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, HISTOGRAM(EquivalenceType.HISTOGRAM) { @Override public ValuesSource getEmpty() { // TODO: Is this the correct exception type here? throw new IllegalArgumentException("Can't deal with unmapped ValuesSource type " + this.value()); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { throw new AggregationExecutionException("value source of type [" + this.value() + "] is not supported by scripts"); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { final IndexFieldData<?> indexFieldData = fieldContext.indexFieldData(); if (!(indexFieldData instanceof IndexHistogramFieldData)) { throw new IllegalArgumentException("Expected histogram type on field [" + fieldContext.field() + "], but got [" + fieldContext.fieldType().typeName() + "]"); } return new ValuesSource.Histogram.Fielddata((IndexHistogramFieldData) indexFieldData); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, // TODO: Ordinal Numbering sync with types from master IP(EquivalenceType.STRING) { @Override public ValuesSource getEmpty() { return BYTES.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return BYTES.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return BYTES.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return BYTES.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return DocValueFormat.IP; } }, DATE(EquivalenceType.NUMBER) { @Override public ValuesSource getEmpty() { return NUMERIC.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return NUMERIC.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return NUMERIC.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return NUMERIC.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return new DocValueFormat.DateTime( format == null ? DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER : DateFormatter.forPattern(format), tz == null ? ZoneOffset.UTC : tz, // If we were just looking at fields, we could read the resolution from the field settings, but we need to deal with script // output, which has no way to indicate the resolution, so we need to default to something. Milliseconds is the standard. DateFieldMapper.Resolution.MILLISECONDS); } }, BOOLEAN(EquivalenceType.NUMBER) { @Override public ValuesSource getEmpty() { return NUMERIC.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return NUMERIC.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return NUMERIC.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return NUMERIC.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } @Override public DocValueFormat getFormatter(String format, ZoneId tz) { return DocValueFormat.BOOLEAN; } } ; enum EquivalenceType { STRING, NUMBER, GEO, RANGE, HISTOGRAM; } EquivalenceType equivalenceType; CoreValuesSourceType(EquivalenceType equivalenceType) { this.equivalenceType = equivalenceType; } @Override public boolean isCastableTo(ValuesSourceType valuesSourceType) { if (valuesSourceType instanceof CoreValuesSourceType == false) { return false; } CoreValuesSourceType other = (CoreValuesSourceType) valuesSourceType; return this.equivalenceType == other.equivalenceType; } public static ValuesSourceType fromString(String name) { return valueOf(name.trim().toUpperCase(Locale.ROOT)); } public static ValuesSourceType fromStream(StreamInput in) throws IOException { return in.readEnum(CoreValuesSourceType.class); } @Override public void writeTo(StreamOutput out) throws IOException { CoreValuesSourceType state = this; out.writeEnum(state); }	this function, along with the whole equivalencetype enum exists to mimic the old valuetype.isa logic (and i re-implemented isa in terms of this function). the whole concept right now is pretty dodgy, since it lets you treat booleans as dates for example, but removing it entirely in this pr is too much, so i settled for making it explicit here. definitely needs a follow up discussion, and possibly removal.
@Override protected final void doWriteTo(StreamOutput out) throws IOException { if (serializeTargetValueType(out.getVersion())) { out.writeOptionalWriteable(null); } out.writeOptionalString(field); boolean hasScript = script != null; out.writeBoolean(hasScript); if (hasScript) { script.writeTo(out); } boolean hasValueType = userValueTypeHint != null; out.writeBoolean(hasValueType); if (hasValueType) { userValueTypeHint.writeTo(out); } out.writeOptionalString(format); out.writeGenericValue(missing); out.writeOptionalZoneId(timeZone); innerWriteTo(out); }	based on my code analysis, we always wrote a null anyway, so this should be backwards compatible. "should" being the key word.
*/ @SuppressWarnings("unchecked") public AB userValueTypeHint(ValueType valueType) { if (valueType == null) { // TODO: This is nonsense. We allow the value to be null (via constructor), but don't allow it to be set to null. This means // thing looking to copy settings (like RollupRequestTranslator) need to check if userValueTypeHint is not null, and then // set it if and only if it is non-null. throw new IllegalArgumentException("[valueType] must not be null: [" + name + "]"); } this.userValueTypeHint = valueType; return (AB) this; }	this made me laugh. and then cry.
public static ValuesSourceConfig resolve( QueryShardContext context, ValueType userValueTypeHint, String field, Script script, Object missing, ZoneId timeZone, String format, Function<Script, ValuesSourceType> defaultValueSourceType, String aggregationName) { ValuesSourceConfig config; MappedFieldType fieldType = null; ValuesSourceType valuesSourceType; if (field == null) { // Stand Alone Script Case if (script == null) { throw new IllegalStateException( "value source config is invalid; must have either a field context or a script or marked as unwrapped"); } /* * This is the Stand Alone Script path. We should have a script that will produce a value independent of the presence or * absence of any one field. The type of the script is given by the userValueTypeHint field, and defaults to bytes if not * specified. */ // TODO: Not pluggable, should always be userValueTypeHint if specified, BYTES if not. // TODO: Probably should validate that the resulting type is valid for this agg. That needs to be plugable. valuesSourceType = userValueTypeHint != null ? userValueTypeHint.getValuesSourceType() : CoreValuesSourceType.ANY; if (valuesSourceType == CoreValuesSourceType.ANY) { // the specific value source type is undefined, but for scripts, // we need to have a specific value source // type to know how to handle the script values, so we fallback // on Bytes valuesSourceType = defaultValueSourceType.apply(script); } config = new ValuesSourceConfig(valuesSourceType); config.script(createScript(script, context)); config.scriptValueType(userValueTypeHint); } else { // Field case fieldType = context.fieldMapper(field); if (fieldType == null) { /* Unmapped Field Case * We got here because the user specified a field, but it doesn't exist on this index, possibly because of a wildcard index * pattern. In this case, we're going to end up using the EMPTY variant of the ValuesSource, and possibly applying a user * specified missing value. */ // TODO: This should be pluggable too; Effectively that will replace the missingAny() case from toValuesSource() // TODO: PLAN - get rid of unmapped; it's only used by valid(), and we're intending to get rid of that. // TODO: Once we no longer care about unmapped, we can merge this case with the mapped case. valuesSourceType = userValueTypeHint != null ? userValueTypeHint.getValuesSourceType() : CoreValuesSourceType.ANY; if (valuesSourceType == CoreValuesSourceType.ANY) { valuesSourceType = defaultValueSourceType.apply(script); } config = new ValuesSourceConfig(valuesSourceType); config.unmapped(true); if (userValueTypeHint != null) { // todo do we really need this for unmapped? config.scriptValueType(userValueTypeHint); } } else { IndexFieldData<?> indexFieldData = context.getForField(fieldType); valuesSourceType = ValuesSourceRegistry.getInstance().getValuesSourceType(fieldType, indexFieldData, aggregationName, userValueTypeHint, script, defaultValueSourceType); config = new ValuesSourceConfig(valuesSourceType); config.fieldContext(new FieldContext(field, indexFieldData, fieldType)); config.script(createScript(script, context)); } } config.format(resolveFormat(format, valuesSourceType, timeZone, fieldType)); config.missing(missing); config.timezone(timeZone); return config; }	again, there's no reason i can see to bounce through an any here.
public void testGetAppropriateRounding() { RoundingInfo[] roundings = new RoundingInfo[6]; DateTimeZone timeZone = DateTimeZone.UTC; roundings[0] = new RoundingInfo(createRounding(DateTimeUnit.SECOND_OF_MINUTE, timeZone), 1000L, 1000); roundings[1] = new RoundingInfo(createRounding(DateTimeUnit.MINUTES_OF_HOUR, timeZone), 60 * 1000L, 1, 5, 10, 30); roundings[2] = new RoundingInfo(createRounding(DateTimeUnit.HOUR_OF_DAY, timeZone), 60 * 60 * 1000L, 1, 3, 12); OffsetDateTime timestamp = Instant.parse("2018-01-01T00:00:01.000Z").atOffset(ZoneOffset.UTC); int result = InternalAutoDateHistogram.getAppropriateRounding(timestamp.toEpochSecond()*1000, timestamp.plusDays(1).toEpochSecond()*1000, 0, roundings, 25); assertThat(result, equalTo(2)); }	using a large innerinterval for the first rounding used by the function is important to trigger the problematic code path
public void testGetAppropriateRounding() { RoundingInfo[] roundings = new RoundingInfo[6]; DateTimeZone timeZone = DateTimeZone.UTC; roundings[0] = new RoundingInfo(createRounding(DateTimeUnit.SECOND_OF_MINUTE, timeZone), 1000L, 1000); roundings[1] = new RoundingInfo(createRounding(DateTimeUnit.MINUTES_OF_HOUR, timeZone), 60 * 1000L, 1, 5, 10, 30); roundings[2] = new RoundingInfo(createRounding(DateTimeUnit.HOUR_OF_DAY, timeZone), 60 * 60 * 1000L, 1, 3, 12); OffsetDateTime timestamp = Instant.parse("2018-01-01T00:00:01.000Z").atOffset(ZoneOffset.UTC); int result = InternalAutoDateHistogram.getAppropriateRounding(timestamp.toEpochSecond()*1000, timestamp.plusDays(1).toEpochSecond()*1000, 0, roundings, 25); assertThat(result, equalTo(2)); }	we've also gotta start with an index that we know won't be used: to trigger the code path, the function needs to iterate past the first rounding.
@Override public void shardStarted(ShardRouting initializingShard, ShardRouting startedShard) { addAllocationId(startedShard); if (startedShard.primary() // started shard has to have null recoverySource; have to pick up recoverySource from its initializing state && (initializingShard.recoverySource() instanceof RecoverySource.ExistingStoreRecoverySource || initializingShard.recoverySource() instanceof RecoverySource.SnapshotRecoverySource)) { Updates updates = changes(startedShard.shardId()); updates.removedAllocationIds.add(RecoverySource.ExistingStoreRecoverySource.FORCED_ALLOCATION_ID); } }	why are snapshot and restore relevant here? i hope we can make this more explicit, depending on existingstorerecoverysource#force_stale_primary_instance in some form or fashion
@Override public void shardStarted(ShardRouting initializingShard, ShardRouting startedShard) { addAllocationId(startedShard); if (startedShard.primary() // started shard has to have null recoverySource; have to pick up recoverySource from its initializing state && (initializingShard.recoverySource() instanceof RecoverySource.ExistingStoreRecoverySource || initializingShard.recoverySource() instanceof RecoverySource.SnapshotRecoverySource)) { Updates updates = changes(startedShard.shardId()); updates.removedAllocationIds.add(RecoverySource.ExistingStoreRecoverySource.FORCED_ALLOCATION_ID); } }	can we make sure this is the only one?
public static void main(String... args) throws Exception { System.out.println("checking for wrong usages of ESLogger..."); boolean[] wrongUsageFound = new boolean[1]; // checkLoggerUsage(wrongLoggerUsage -> { // System.err.println(wrongLoggerUsage.getErrorLines()); // wrongUsageFound[0] = true; // }, args); if (wrongUsageFound[0]) { throw new Exception("Wrong logger usages found"); } else { System.out.println("No wrong usages found"); } }	i still need to figure out how to fix this test
public abstract DeleteResult delete(Delete delete); /** * Base class for index and delete operation results * Holds result meta data (e.g. translog location, updated version) * for an executed write {@link Operation}	a java doc commetn would be great here while we are at it
public abstract DeleteResult delete(Delete delete); /** * Base class for index and delete operation results * Holds result meta data (e.g. translog location, updated version) * for an executed write {@link Operation}	maybe use private final setonce<boolean> frozen here it will barf if you freeze multiple times?
@Override public IndexResult index(Index index) { IndexResult result; try (ReleasableLock lock = readLock.acquire()) { ensureOpen(); if (index.origin().isRecovery()) { // Don't throttle recovery operations result = innerIndex(index); } else { try (Releasable r = throttle.acquireThrottle()) { result = innerIndex(index); } } } catch (Exception e) { result = new IndexResult(checkIfDocumentFailureOrThrow(index, e), index.version()); } return result; }	can we please move the maybefailengine(operation.operationtype().getlowercase(), failure)) one line above and assign it to a variable. i really wanna make sure no short circuit logic here prevents it from being called
@Override public IndexResult index(Index index) { IndexResult result; try (ReleasableLock lock = readLock.acquire()) { ensureOpen(); if (index.origin().isRecovery()) { // Don't throttle recovery operations result = innerIndex(index); } else { try (Releasable r = throttle.acquireThrottle()) { result = innerIndex(index); } } } catch (Exception e) { result = new IndexResult(checkIfDocumentFailureOrThrow(index, e), index.version()); } return result; }	maybe document that this is a hack to retrhow the original
default Engine.Index preIndex(Engine.Index operation) { return operation; } /** * Called after the indexing operation occurred. Implementations should * check {@link Engine.IndexResult#hasFailure()} for operation failures * and delegate to {@link #postIndex(Engine.Index, Exception)} with * {@link Engine.IndexResult#getFailure()}	i don't think we need to say this? how about "note that this method is also called when indexing a document didn't succeed because of document related failures. see {@link #postindex(..)} for engine level failures.
@Override public void postIndex(Engine.Index index, Engine.IndexResult result) { if (result.hasFailure() == false) { if (!index.origin().isRecovery()) { long took = result.getTook(); totalStats.indexMetric.inc(took); totalStats.indexCurrent.dec(); StatsHolder typeStats = typeStats(index.type()); typeStats.indexMetric.inc(took); typeStats.indexCurrent.dec(); } } else { postIndex(index, result.getFailure()); } }	should we have a write failures statistic too? @bleskes
public void testTook() throws Exception { XContentBuilder mapping = jsonBuilder() .startObject() .startObject("type1") .startObject("properties") .startObject("field") .field("type", "text") .field("term_vector", "with_positions_offsets_payloads") .endObject() .endObject() .endObject() .endObject(); assertAcked(prepareCreate("test").addAlias(new Alias("alias")).addMapping("type1", mapping)); ensureGreen(); client().prepareIndex("test", "type1", "0").setSource("field", "foo bar").setRefresh(true).execute().get(); long start = System.nanoTime(); TermVectorsResponse response = client().termVectors(new TermVectorsRequest(indexOrAlias(), "type1", "0")).get(); assertThat(response, notNullValue()); assertThat(response.getTookInMillis(), greaterThanOrEqualTo(0L)); assertThat(response.getTookInMillis(), lessThanOrEqualTo(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start))); }	this test doesn't protect against insidious bugs. for example, if we modify: java termvectorsresponse.settookinmillis(timeunit.nanoseconds.tomillis(system.nanotime() - starttime)); to java termvectorsresponse.settookinmillis((system.nanotime() - starttime) / 10000000l); then we've introduced a unit conversion bug but the test will not capture that because the test will still pass.
public void testUpdatesAreRejected() { String indexName = randomFrom("<test-{2015.05.05||+1d}>", "test"); assertAcked(client().admin().indices().prepareCreate(indexName) .setMapping("id", "type=keyword", "field1", "type=text", "field2", "type=text") .setSettings(Settings.builder() .put("index.number_of_replicas", 0) .put("index.number_of_shards", 1)) ); client().prepareIndex(indexName).setId("1").setSource("id", "1", "field1", "value1") .setRefreshPolicy(IMMEDIATE) .get(); ElasticsearchSecurityException exception = expectThrows(ElasticsearchSecurityException.class, () -> { client().filterWithHeader(Collections.singletonMap(BASIC_AUTH_HEADER, basicAuthHeaderValue("user1", USERS_PASSWD))) .prepareUpdate(indexName, "1") .setDoc(Requests.INDEX_CONTENT_TYPE, "field2", "value2") .get(); }); assertThat(exception.getDetailedMessage(), containsString("Can't execute an update request if field or document level security")); BulkResponse bulkResponse = client().filterWithHeader(Collections.singletonMap(BASIC_AUTH_HEADER, basicAuthHeaderValue("user1", USERS_PASSWD))) .prepareBulk() .add(client().prepareUpdate(indexName, "1") .setDoc(Requests.INDEX_CONTENT_TYPE, "field2", "value2")) .get(); assertThat(bulkResponse.getItems().length, is(1)); assertThat(bulkResponse.getItems()[0].getFailureMessage(), containsString("Can't execute a bulk item request with update requests" + " embedded if field or document level security is enabled")); }	why would we randomise on this rather than just have 2 tests? it feels like having a permanent test with datemath would be a good thing.
public void intercept(RequestInfo requestInfo, AuthorizationEngine authzEngine, AuthorizationInfo authorizationInfo, ActionListener<Void> listener) { boolean shouldIntercept = licenseState.isSecurityEnabled(); var featureUsageChecker = new MemoizedSupplier<>(() -> licenseState.checkFeature(Feature.SECURITY_DLS_FLS)); if (requestInfo.getRequest() instanceof BulkShardRequest && shouldIntercept) { IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY); BulkShardRequest bulkShardRequest = (BulkShardRequest) requestInfo.getRequest(); // this uses the {@code BulkShardRequest#index()} because the {@code bulkItemRequest#index()} // can still be an unresolved date math expression IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(bulkShardRequest.index()); // TODO replace if condition with assertion if (indexAccessControl != null) { for (BulkItemRequest bulkItemRequest : bulkShardRequest.items()) { boolean found = false; if (bulkItemRequest.request() instanceof UpdateRequest) { boolean fls = indexAccessControl.getFieldPermissions().hasFieldLevelSecurity(); boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions(); // the feature usage checker is a "last-ditch" verification, it doesn't have practical importance if ((fls || dls) && featureUsageChecker.get()) { found = true; logger.trace("aborting bulk item update request for index [{}]", bulkShardRequest.index()); bulkItemRequest.abort(bulkItemRequest.index(), new ElasticsearchSecurityException("Can't execute a bulk " + "item request with update requests embedded if field or document level security is enabled", RestStatus.BAD_REQUEST)); } } if (found == false) { logger.trace("intercepted bulk request for index [{}] without any update requests, continuing execution", bulkShardRequest.index()); } } } } listener.onResponse(null); }	the previous name seems more appropriate to me. the purpose is to check the license allows the feature, not to track usage.
public void testParameters() throws IOException { String tvString = randomTimeValue(1, 100); doTestParameter("keep_alive", tvString, TimeValue.parseTimeValue(tvString, ""), SubmitAsyncSearchRequest::getKeepAlive); doTestParameter("wait_for_completion_timeout", tvString, TimeValue.parseTimeValue(tvString, ""), SubmitAsyncSearchRequest::getWaitForCompletionTimeout); boolean keepOnCompletion = randomBoolean(); doTestParameter("keep_on_completion", Boolean.toString(keepOnCompletion), keepOnCompletion, SubmitAsyncSearchRequest::isKeepOnCompletion); boolean requestCache = randomBoolean(); doTestParameter("request_cache", Boolean.toString(requestCache), requestCache, r -> r.getSearchRequest().requestCache()); int batchedReduceSize = randomIntBetween(2, 50); doTestParameter("batched_reduce_size", Integer.toString(batchedReduceSize), batchedReduceSize, r -> r.getSearchRequest().getBatchedReduceSize()); }	it would be cleaner to also clean up the channels being tracked due to the dispatchrequest calls from this test. i gave it a try though and it was not trivial, and the same should be done in restcancellablenodeclienttests where we actually assume that we may have channels in the map already, hence we only check the delta since the test has started. i wonder if it's worth looking into cleaning the channels up after both tests have executed, or not.
protected List<Engine.Operation> generateSingleDocHistory(boolean forReplica, VersionType versionType, long primaryTerm, int minOpCount, int maxOpCount, String docId) { final int numOfOps = randomIntBetween(minOpCount, maxOpCount); final List<Engine.Operation> ops = new ArrayList<>(); final Term id = newUid(docId); final int startWithSeqNo = 0; final String valuePrefix = (forReplica ? "r_" : "p_" ) + docId + "_"; final boolean incrementTermWhenIntroducingSeqNo = randomBoolean(); for (int i = 0; i < numOfOps; i++) { final Engine.Operation op; final long version; switch (versionType) { case INTERNAL: version = forReplica ? i : Versions.MATCH_ANY; break; case EXTERNAL: version = i; break; case EXTERNAL_GTE: version = randomBoolean() ? Math.max(i - 1, 0) : i; break; case FORCE: version = randomNonNegativeLong(); break; default: throw new UnsupportedOperationException("unknown version type: " + versionType); } if (randomBoolean()) { op = new Engine.Index(id, testParsedDocument(docId, null, testDocumentWithTextField(valuePrefix + i), B_1, null), forReplica && i >= startWithSeqNo ? i * 2 : SequenceNumbers.UNASSIGNED_SEQ_NO, forReplica && i >= startWithSeqNo && incrementTermWhenIntroducingSeqNo ? primaryTerm + 1 : primaryTerm, version, forReplica ? versionType.versionTypeForReplicationAndRecovery() : versionType, forReplica ? REPLICA : PRIMARY, System.currentTimeMillis(), -1, false ); } else { op = new Engine.Delete("test", docId, id, forReplica && i >= startWithSeqNo ? i * 2 : SequenceNumbers.UNASSIGNED_SEQ_NO, forReplica && i >= startWithSeqNo && incrementTermWhenIntroducingSeqNo ? primaryTerm + 1 : primaryTerm, version, forReplica ? versionType.versionTypeForReplicationAndRecovery() : versionType, forReplica ? REPLICA : PRIMARY, System.currentTimeMillis()); } ops.add(op); } return ops; }	just to check, this is only for master (i.e. >= 7.0.0), right?
public void testConcurrentOutOfOrderDocsOnReplica() throws IOException, InterruptedException { final List<Engine.Operation> opsDoc1 = generateSingleDocHistory(true, randomFrom(VersionType.INTERNAL, VersionType.EXTERNAL), 2, 100, 300, "1"); final Engine.Operation lastOpDoc1 = opsDoc1.get(opsDoc1.size() - 1); final String lastFieldValueDoc1; if (lastOpDoc1 instanceof Engine.Index) { Engine.Index index = (Engine.Index) lastOpDoc1; lastFieldValueDoc1 = index.docs().get(0).get("value"); } else { // delete lastFieldValueDoc1 = null; } final List<Engine.Operation> opsDoc2 = generateSingleDocHistory(true, randomFrom(VersionType.INTERNAL, VersionType.EXTERNAL), 2, 100, 300, "2"); final Engine.Operation lastOpDoc2 = opsDoc2.get(opsDoc2.size() - 1); final String lastFieldValueDoc2; if (lastOpDoc2 instanceof Engine.Index) { Engine.Index index = (Engine.Index) lastOpDoc2; lastFieldValueDoc2 = index.docs().get(0).get("value"); } else { // delete lastFieldValueDoc2 = null; } // randomly interleave AtomicLong seqNoGenerator = new AtomicLong(); Function<Engine.Operation, Engine.Operation> seqNoUpdater = operation -> { final long newSeqNo = seqNoGenerator.getAndIncrement(); if (operation instanceof Engine.Index) { Engine.Index index = (Engine.Index) operation; return new Engine.Index(index.uid(), index.parsedDoc(), newSeqNo, index.primaryTerm(), index.version(), index.versionType(), index.origin(), index.startTime(), index.getAutoGeneratedIdTimestamp(), index.isRetry()); } else { Engine.Delete delete = (Engine.Delete) operation; return new Engine.Delete(delete.type(), delete.id(), delete.uid(), newSeqNo, delete.primaryTerm(), delete.version(), delete.versionType(), delete.origin(), delete.startTime()); } }; final List<Engine.Operation> allOps = new ArrayList<>(); Iterator<Engine.Operation> iter1 = opsDoc1.iterator(); Iterator<Engine.Operation> iter2 = opsDoc2.iterator(); while (iter1.hasNext() && iter2.hasNext()) { final Engine.Operation next = randomBoolean() ? iter1.next() : iter2.next(); allOps.add(seqNoUpdater.apply(next)); } iter1.forEachRemaining(o -> allOps.add(seqNoUpdater.apply(o))); iter2.forEachRemaining(o -> allOps.add(seqNoUpdater.apply(o))); // insert some duplicates allOps.addAll(randomSubsetOf(allOps)); shuffle(allOps, random()); concurrentlyApplyOps(allOps, engine); engine.refresh("test"); if (lastFieldValueDoc1 != null) { try (Searcher searcher = engine.acquireSearcher("test")) { final TotalHitCountCollector collector = new TotalHitCountCollector(); searcher.searcher().search(new TermQuery(new Term("value", lastFieldValueDoc1)), collector); assertThat(collector.getTotalHits(), equalTo(1)); } } if (lastFieldValueDoc2 != null) { try (Searcher searcher = engine.acquireSearcher("test")) { final TotalHitCountCollector collector = new TotalHitCountCollector(); searcher.searcher().search(new TermQuery(new Term("value", lastFieldValueDoc2)), collector); assertThat(collector.getTotalHits(), equalTo(1)); } } int totalExpectedOps = 0; if (lastFieldValueDoc1 != null) { totalExpectedOps++; } if (lastFieldValueDoc2 != null) { totalExpectedOps++; } assertVisibleCount(engine, totalExpectedOps); }	this duplication isn't to my taste - i think i'd try and pull the notion of "doc" out into a class of its own and have this kind of thing be methods there.
public void testConcurrentOutOfOrderDocsOnReplica() throws IOException, InterruptedException { final List<Engine.Operation> opsDoc1 = generateSingleDocHistory(true, randomFrom(VersionType.INTERNAL, VersionType.EXTERNAL), 2, 100, 300, "1"); final Engine.Operation lastOpDoc1 = opsDoc1.get(opsDoc1.size() - 1); final String lastFieldValueDoc1; if (lastOpDoc1 instanceof Engine.Index) { Engine.Index index = (Engine.Index) lastOpDoc1; lastFieldValueDoc1 = index.docs().get(0).get("value"); } else { // delete lastFieldValueDoc1 = null; } final List<Engine.Operation> opsDoc2 = generateSingleDocHistory(true, randomFrom(VersionType.INTERNAL, VersionType.EXTERNAL), 2, 100, 300, "2"); final Engine.Operation lastOpDoc2 = opsDoc2.get(opsDoc2.size() - 1); final String lastFieldValueDoc2; if (lastOpDoc2 instanceof Engine.Index) { Engine.Index index = (Engine.Index) lastOpDoc2; lastFieldValueDoc2 = index.docs().get(0).get("value"); } else { // delete lastFieldValueDoc2 = null; } // randomly interleave AtomicLong seqNoGenerator = new AtomicLong(); Function<Engine.Operation, Engine.Operation> seqNoUpdater = operation -> { final long newSeqNo = seqNoGenerator.getAndIncrement(); if (operation instanceof Engine.Index) { Engine.Index index = (Engine.Index) operation; return new Engine.Index(index.uid(), index.parsedDoc(), newSeqNo, index.primaryTerm(), index.version(), index.versionType(), index.origin(), index.startTime(), index.getAutoGeneratedIdTimestamp(), index.isRetry()); } else { Engine.Delete delete = (Engine.Delete) operation; return new Engine.Delete(delete.type(), delete.id(), delete.uid(), newSeqNo, delete.primaryTerm(), delete.version(), delete.versionType(), delete.origin(), delete.startTime()); } }; final List<Engine.Operation> allOps = new ArrayList<>(); Iterator<Engine.Operation> iter1 = opsDoc1.iterator(); Iterator<Engine.Operation> iter2 = opsDoc2.iterator(); while (iter1.hasNext() && iter2.hasNext()) { final Engine.Operation next = randomBoolean() ? iter1.next() : iter2.next(); allOps.add(seqNoUpdater.apply(next)); } iter1.forEachRemaining(o -> allOps.add(seqNoUpdater.apply(o))); iter2.forEachRemaining(o -> allOps.add(seqNoUpdater.apply(o))); // insert some duplicates allOps.addAll(randomSubsetOf(allOps)); shuffle(allOps, random()); concurrentlyApplyOps(allOps, engine); engine.refresh("test"); if (lastFieldValueDoc1 != null) { try (Searcher searcher = engine.acquireSearcher("test")) { final TotalHitCountCollector collector = new TotalHitCountCollector(); searcher.searcher().search(new TermQuery(new Term("value", lastFieldValueDoc1)), collector); assertThat(collector.getTotalHits(), equalTo(1)); } } if (lastFieldValueDoc2 != null) { try (Searcher searcher = engine.acquireSearcher("test")) { final TotalHitCountCollector collector = new TotalHitCountCollector(); searcher.searcher().search(new TermQuery(new Term("value", lastFieldValueDoc2)), collector); assertThat(collector.getTotalHits(), equalTo(1)); } } int totalExpectedOps = 0; if (lastFieldValueDoc1 != null) { totalExpectedOps++; } if (lastFieldValueDoc2 != null) { totalExpectedOps++; } assertVisibleCount(engine, totalExpectedOps); }	i don't think it needs to be an atomiclong - it's only updated on this thread.
public void testConcurrentOutOfOrderDocsOnReplica() throws IOException, InterruptedException { final List<Engine.Operation> opsDoc1 = generateSingleDocHistory(true, randomFrom(VersionType.INTERNAL, VersionType.EXTERNAL), 2, 100, 300, "1"); final Engine.Operation lastOpDoc1 = opsDoc1.get(opsDoc1.size() - 1); final String lastFieldValueDoc1; if (lastOpDoc1 instanceof Engine.Index) { Engine.Index index = (Engine.Index) lastOpDoc1; lastFieldValueDoc1 = index.docs().get(0).get("value"); } else { // delete lastFieldValueDoc1 = null; } final List<Engine.Operation> opsDoc2 = generateSingleDocHistory(true, randomFrom(VersionType.INTERNAL, VersionType.EXTERNAL), 2, 100, 300, "2"); final Engine.Operation lastOpDoc2 = opsDoc2.get(opsDoc2.size() - 1); final String lastFieldValueDoc2; if (lastOpDoc2 instanceof Engine.Index) { Engine.Index index = (Engine.Index) lastOpDoc2; lastFieldValueDoc2 = index.docs().get(0).get("value"); } else { // delete lastFieldValueDoc2 = null; } // randomly interleave AtomicLong seqNoGenerator = new AtomicLong(); Function<Engine.Operation, Engine.Operation> seqNoUpdater = operation -> { final long newSeqNo = seqNoGenerator.getAndIncrement(); if (operation instanceof Engine.Index) { Engine.Index index = (Engine.Index) operation; return new Engine.Index(index.uid(), index.parsedDoc(), newSeqNo, index.primaryTerm(), index.version(), index.versionType(), index.origin(), index.startTime(), index.getAutoGeneratedIdTimestamp(), index.isRetry()); } else { Engine.Delete delete = (Engine.Delete) operation; return new Engine.Delete(delete.type(), delete.id(), delete.uid(), newSeqNo, delete.primaryTerm(), delete.version(), delete.versionType(), delete.origin(), delete.startTime()); } }; final List<Engine.Operation> allOps = new ArrayList<>(); Iterator<Engine.Operation> iter1 = opsDoc1.iterator(); Iterator<Engine.Operation> iter2 = opsDoc2.iterator(); while (iter1.hasNext() && iter2.hasNext()) { final Engine.Operation next = randomBoolean() ? iter1.next() : iter2.next(); allOps.add(seqNoUpdater.apply(next)); } iter1.forEachRemaining(o -> allOps.add(seqNoUpdater.apply(o))); iter2.forEachRemaining(o -> allOps.add(seqNoUpdater.apply(o))); // insert some duplicates allOps.addAll(randomSubsetOf(allOps)); shuffle(allOps, random()); concurrentlyApplyOps(allOps, engine); engine.refresh("test"); if (lastFieldValueDoc1 != null) { try (Searcher searcher = engine.acquireSearcher("test")) { final TotalHitCountCollector collector = new TotalHitCountCollector(); searcher.searcher().search(new TermQuery(new Term("value", lastFieldValueDoc1)), collector); assertThat(collector.getTotalHits(), equalTo(1)); } } if (lastFieldValueDoc2 != null) { try (Searcher searcher = engine.acquireSearcher("test")) { final TotalHitCountCollector collector = new TotalHitCountCollector(); searcher.searcher().search(new TermQuery(new Term("value", lastFieldValueDoc2)), collector); assertThat(collector.getTotalHits(), equalTo(1)); } } int totalExpectedOps = 0; if (lastFieldValueDoc1 != null) { totalExpectedOps++; } if (lastFieldValueDoc2 != null) { totalExpectedOps++; } assertVisibleCount(engine, totalExpectedOps); }	nit: extra blank line
public ClusterState execute(ClusterState currentState) { DiscoveryNodes.Builder nodesBuilder; synchronized (pendingJoinRequests) { if (pendingJoinRequests.isEmpty()) { return currentState; } nodesBuilder = DiscoveryNodes.builder(currentState.nodes()); Iterator<Map.Entry<DiscoveryNode, List<MembershipAction.JoinCallback>>> iterator = pendingJoinRequests.entrySet().iterator(); while (iterator.hasNext()) { Map.Entry<DiscoveryNode, List<MembershipAction.JoinCallback>> entry = iterator.next(); final DiscoveryNode node = entry.getKey(); iterator.remove(); final String preflight = nodesBuilder.preflightPut(node); if (preflight != null) { Throwable failure = new IllegalStateException(preflight); for (MembershipAction.JoinCallback callback: entry.getValue()) { joinCallbacksToFail.add(new Tuple<>(callback, failure)); } } else if (currentState.nodes().nodeExists(node.getId())) { logger.debug("received a join request for an existing node [{}]", node); joinCallbacksToRespondTo.addAll(entry.getValue()); } else { nodeAdded = true; nodesBuilder.put(node); joinCallbacksToRespondTo.addAll(entry.getValue()); } assert entry.getValue().stream().allMatch(cb -> joinCallbacksToRespondTo.contains(cb) ^ joinCallbacksToFail.stream().filter(tuple -> tuple.v1().equals(cb)).count() > 0 ) : "failed to add " + entry.getValue() + "to joinCallbacksToRespondTo or joinCallbacksToFail"; } } // we must return a new cluster state instance to force publishing. This is important // for the joining node to finalize it's join and set us as a master final ClusterState.Builder newState = ClusterState.builder(currentState); if (nodeAdded) { newState.nodes(nodesBuilder); } return newState.build(); }	if a node with same persisted node id but different address tries to join the cluster (because the data folder was copied from another node) it will not manage to join the cluster. it will not have any meaningful error message either except that it couldn't join. can we tell the offending node why it failed using joincallbackstofail?
public void testRejectingJoinWithSameAddress() throws InterruptedException, ExecutionException { ClusterState state = clusterService.state(); final DiscoveryNode other_node = new DiscoveryNode("other_node", state.nodes().getLocalNode().getAddress(), emptyMap(), emptySet(), Version.CURRENT); ExecutionException e = expectThrows(ExecutionException.class, () -> joinNode(other_node)); assertThat(e.getMessage(), containsString("found existing node")); }	also add test for situation i described above (same node id but different address)
protected void warnAboutSlowTaskIfNeeded(TimeValue executionTime, String source) { if (executionTime.getMillis() > slowTaskLoggingThreshold.getMillis()) { logger.warn("cluster state update task [{}] took [{}] which is above the warn threshold of [{}]", source, executionTime, slowTaskLoggingThreshold); } }	an interesting follow-up would be to change the slow logging in masterservice so that it only applies to calculatetaskoutputs, and then reduce the timeout from 30s to 10s. currently, the slow logging in masterservice measures cluster state update calculation, publication + application. the latter two are covered by your pr here now
public static void addDataFrameAnalyticsFields(XContentBuilder builder) throws IOException { builder.startObject(DataFrameAnalyticsConfig.ID.getPreferredName()) .field(TYPE, KEYWORD) .endObject() .startObject(DataFrameAnalyticsConfig.SOURCE.getPreferredName()) .field(TYPE, KEYWORD) .endObject() .startObject(DataFrameAnalyticsConfig.DEST.getPreferredName()) .field(TYPE, KEYWORD) .endObject() .startObject(DataFrameAnalyticsConfig.ANALYSES.getPreferredName()) .startObject(PROPERTIES) .startObject("outlier_detection") .startObject(PROPERTIES) .startObject("number_neighbours") .field(TYPE, INTEGER) .endObject() .startObject("method") .field(TYPE, KEYWORD) .endObject() .endObject() .endObject() .endObject() .endObject(); }	you are re-using the ml configuration index? i wonder if there aren't already fields called id? apart from that: have we thought about a separate index for data frame analytics? i think we do not need the coupling.
private void processData(String jobId, DataFrameDataExtractor dataExtractor, AnalyticsProcess process, AnalyticsResultProcessor resultProcessor, Consumer<Exception> finishHandler) { try { writeHeaderRecord(dataExtractor, process); writeDataRows(dataExtractor, process); process.writeEndOfDataMessage(); process.flushStream(); LOGGER.info("[{}] Waiting for result processor to complete", jobId); resultProcessor.awaitForCompletion(); LOGGER.info("[{}] Result processor has completed", jobId); } catch (IOException e) { LOGGER.error(new ParameterizedMessage("[{}] Error writing data to the process", jobId), e); finishHandler.accept(e); } finally { LOGGER.info("[{}] Closing process", jobId); try { process.close(); LOGGER.info("[{}] Closed process", jobId); // This results in marking the persistent task as complete finishHandler.accept(null); } catch (IOException e) { LOGGER.error("[{}] Error closing data frame analyzer process", jobId); finishHandler.accept(e); } } }	looks to me like finishhandler could be called twice, here and as part of the finally block. i guess this isn't the way you want it.
public void process(AnalyticsProcess process) { try { Iterator<AnalyticsResult> iterator = process.readAnalyticsResults(); while (iterator.hasNext()) { try { AnalyticsResult result = iterator.next(); if (dataExtractor.hasNext() == false) { return; } if (currentDataFrameRows == null) { Optional<List<DataFrameDataExtractor.Row>> nextBatch = dataExtractor.next(); if (nextBatch.isPresent() == false) { return; } currentDataFrameRows = nextBatch.get(); currentResults = new ArrayList<>(currentDataFrameRows.size()); } currentResults.add(result); if (currentResults.size() == currentDataFrameRows.size()) { joinCurrentResults(); currentDataFrameRows = null; } } catch (Exception e) { LOGGER.warn("Error processing dataframe result", e); } } } catch (Exception e) { LOGGER.error("Error parsing dataframe output", e); } finally { completionLatch.countDown(); process.consumeAndCloseOutputStream(); } }	you know, data frame ;-)
@Override public void onResponse(SearchResponse response) { searchResponse.get().updateFinalResponse(response); executeCompletionListeners(); }	+1, https://github.com/elastic/elasticsearch/pull/55683 has the same change
public static String name(Expression e) { if (e instanceof NamedExpression) { return ((NamedExpression) e).name(); } else if (e instanceof Literal) { return e.toString(); } else { return e.nodeName(); } }	i feel like we'd be better off being more oo here and adding a colunmname function to expression or something like that. the fix is fine with me as it stands but i think something like that would make a good follow up change.
private void assertSearchSlicesWithPointInTime(String sliceField, String sortField, String pointInTimeId, int numSlice, int numDocs) { int totalResults = 0; List<String> keys = new ArrayList<>(); for (int id = 0; id < numSlice; id++) { int numSliceResults = 0; SearchRequestBuilder request = client().prepareSearch() .slice(new SliceBuilder(sliceField, id, numSlice)) .setPointInTime(new PointInTimeBuilder(pointInTimeId)) .addSort(SortBuilders.fieldSort(sortField)) .setPreference(null) .setSize(randomIntBetween(10, 100)); SearchResponse searchResponse = request.get(); int expectedSliceResults = (int) searchResponse.getHits().getTotalHits().value; while (true) { int numHits = searchResponse.getHits().getHits().length; if (numHits == 0) { break; } totalResults += numHits; numSliceResults += numHits; for (SearchHit hit : searchResponse.getHits().getHits()) { assertTrue(keys.add(hit.getId())); } Object[] sortValues = searchResponse.getHits().getHits()[numHits - 1].getSortValues(); request.setIndices(Strings.EMPTY_ARRAY); searchResponse = request.searchAfter(sortValues).get(); } assertThat(numSliceResults, equalTo(expectedSliceResults)); } assertThat(totalResults, equalTo(numDocs)); assertThat(keys.size(), equalTo(numDocs)); assertThat(new HashSet<>(keys).size(), equalTo(numDocs)); }	out of curiosity, why set the preference to null explicitly?
@Override public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; boolean scroll = scroll() != null; if (scroll) { if (source != null) { if (source.trackTotalHitsUpTo() != null && source.trackTotalHitsUpTo() != SearchContext.TRACK_TOTAL_HITS_ACCURATE) { validationException = addValidationError( "disabling [track_total_hits] is not allowed in a scroll context", validationException ); } if (source.from() > 0) { validationException = addValidationError("using [from] is not allowed in a scroll context", validationException); } if (source.size() == 0) { validationException = addValidationError("[size] cannot be [0] in a scroll context", validationException); } if (source.rescores() != null && source.rescores().isEmpty() == false) { validationException = addValidationError("using [rescore] is not allowed in a scroll context", validationException); } } if (requestCache != null && requestCache) { validationException = addValidationError("[request_cache] cannot be used in a scroll context", validationException); } } if (source != null) { if (source.aggregations() != null) { validationException = source.aggregations().validate(validationException); } } if (pointInTimeBuilder() != null) { if (scroll) { validationException = addValidationError("using [point in time] is not allowed in a scroll context", validationException); } if (indices.length > 0) { validationException = addValidationError("[indices] cannot be used with point in time", validationException); } if (routing != null) { validationException = addValidationError("[routing] cannot be used with point in time", validationException); } if (preference != null) { validationException = addValidationError("[preference] cannot be used with point in time", validationException); } } else if (source != null && source.sorts() != null) { for (SortBuilder<?> sortBuilder : source.sorts()) { if (sortBuilder instanceof FieldSortBuilder && ShardDocSortField.NAME.equals(((FieldSortBuilder) sortBuilder).getFieldName())) { validationException = addValidationError( "[" + FieldSortBuilder.SHARD_DOC_FIELD_NAME + "] sort field cannot be used without [point in time]", validationException ); } } } if (minCompatibleShardNode() != null) { if (isCcsMinimizeRoundtrips()) { validationException = addValidationError( "[ccs_minimize_roundtrips] cannot be [true] when setting a minimum compatible " + "shard version", validationException ); } } if (pointInTimeBuilder() != null && waitForCheckpoints.isEmpty() == false) { validationException = addValidationError("using [point in time] is not allowed with wait_for_checkpoints", validationException); } return validationException; }	thanks, ++ on having these checks as part of the request validation
public static MultiSearchRequest parseRequest( RestRequest restRequest, NamedWriteableRegistry namedWriteableRegistry, boolean allowExplicitIndex, TriFunction<String, Object, SearchRequest, Boolean> extraParamParser ) throws IOException { if (restRequest.getRestApiVersion() == RestApiVersion.V_7 && restRequest.hasParam("type")) { restRequest.param("type"); } MultiSearchRequest multiRequest = new MultiSearchRequest(); IndicesOptions indicesOptions = IndicesOptions.fromRequest(restRequest, multiRequest.indicesOptions()); multiRequest.indicesOptions(indicesOptions); if (restRequest.hasParam("max_concurrent_searches")) { multiRequest.maxConcurrentSearchRequests(restRequest.paramAsInt("max_concurrent_searches", 0)); } Integer preFilterShardSize = null; if (restRequest.hasParam("pre_filter_shard_size")) { preFilterShardSize = restRequest.paramAsInt("pre_filter_shard_size", SearchRequest.DEFAULT_PRE_FILTER_SHARD_SIZE); } final Integer maxConcurrentShardRequests; if (restRequest.hasParam("max_concurrent_shard_requests")) { // only set if we have the parameter since we auto adjust the max concurrency on the coordinator // based on the number of nodes in the cluster maxConcurrentShardRequests = restRequest.paramAsInt("max_concurrent_shard_requests", Integer.MIN_VALUE); } else { maxConcurrentShardRequests = null; } parseMultiLineRequest(restRequest, multiRequest.indicesOptions(), allowExplicitIndex, (searchRequest, parser) -> { searchRequest.source(SearchSourceBuilder.fromXContent(parser, false)); RestSearchAction.checkRestTotalHits(restRequest, searchRequest); if (searchRequest.pointInTimeBuilder() != null) { if (restRequest.paramAsBoolean("ccs_minimize_roundtrips", false)) { throw new IllegalArgumentException("[ccs_minimize_roundtrips] cannot be used with point in time"); } if (searchRequest.indicesOptions().equals(DEFAULT_INDICES_OPTIONS) == false) { throw new IllegalArgumentException("[indicesOptions] cannot be used with point in time"); } } else { searchRequest.setCcsMinimizeRoundtrips( restRequest.paramAsBoolean("ccs_minimize_roundtrips", searchRequest.isCcsMinimizeRoundtrips()) ); } multiRequest.add(searchRequest); }, extraParamParser); List<SearchRequest> requests = multiRequest.requests(); for (SearchRequest request : requests) { // preserve if it's set on the request if (preFilterShardSize != null && request.getPreFilterShardSize() == null) { request.setPreFilterShardSize(preFilterShardSize); } if (maxConcurrentShardRequests != null) { request.setMaxConcurrentShardRequests(maxConcurrentShardRequests); } } return multiRequest; } /** * Parses a multi-line {@link RestRequest} body, instantiating a {@link SearchRequest}	i wonder why we need these at the rest layer, shouldn't these checks be performed as part of the request validation anyways. would we fail anyways down the line without these additional checks?
public static void parseSearchRequest( SearchRequest searchRequest, RestRequest request, XContentParser requestContentParser, NamedWriteableRegistry namedWriteableRegistry, IntConsumer setSize, BiConsumer<RestRequest, SearchRequest> extraParamParser ) throws IOException { if (request.getRestApiVersion() == RestApiVersion.V_7 && request.hasParam("type")) { request.param("type"); deprecationLogger.compatibleCritical("search_with_types", TYPES_DEPRECATION_MESSAGE); } if (searchRequest.source() == null) { searchRequest.source(new SearchSourceBuilder()); } searchRequest.indices(Strings.splitStringByCommaToArray(request.param("index"))); if (requestContentParser != null) { searchRequest.source().parseXContent(requestContentParser, true); } final int batchedReduceSize = request.paramAsInt("batched_reduce_size", searchRequest.getBatchedReduceSize()); searchRequest.setBatchedReduceSize(batchedReduceSize); if (request.hasParam("pre_filter_shard_size")) { searchRequest.setPreFilterShardSize(request.paramAsInt("pre_filter_shard_size", SearchRequest.DEFAULT_PRE_FILTER_SHARD_SIZE)); } if (request.hasParam("enable_fields_emulation")) { // this flag is a no-op from 8.0 on, we only want to consume it so its presence doesn't cause errors request.paramAsBoolean("enable_fields_emulation", false); } if (request.hasParam("max_concurrent_shard_requests")) { // only set if we have the parameter since we auto adjust the max concurrency on the coordinator // based on the number of nodes in the cluster final int maxConcurrentShardRequests = request.paramAsInt( "max_concurrent_shard_requests", searchRequest.getMaxConcurrentShardRequests() ); searchRequest.setMaxConcurrentShardRequests(maxConcurrentShardRequests); } if (request.hasParam("allow_partial_search_results")) { // only set if we have the parameter passed to override the cluster-level default searchRequest.allowPartialSearchResults(request.paramAsBoolean("allow_partial_search_results", null)); } searchRequest.searchType(request.param("search_type")); parseSearchSource(searchRequest.source(), request, setSize); searchRequest.requestCache(request.paramAsBoolean("request_cache", searchRequest.requestCache())); String scroll = request.param("scroll"); if (scroll != null) { searchRequest.scroll(new Scroll(parseTimeValue(scroll, null, "scroll"))); } searchRequest.routing(request.param("routing")); searchRequest.preference(request.param("preference")); searchRequest.indicesOptions(IndicesOptions.fromRequest(request, searchRequest.indicesOptions())); checkRestTotalHits(request, searchRequest); if (searchRequest.pointInTimeBuilder() != null) { if (request.paramAsBoolean("ccs_minimize_roundtrips", false)) { throw new IllegalArgumentException("[ccs_minimize_roundtrips] cannot be used with point in time"); } if (searchRequest.indicesOptions().equals(DEFAULT_INDICES_OPTIONS) == false) { throw new IllegalArgumentException("[indicesOptions] cannot be used with point in time"); } } else { searchRequest.setCcsMinimizeRoundtrips( request.paramAsBoolean("ccs_minimize_roundtrips", searchRequest.isCcsMinimizeRoundtrips()) ); } extraParamParser.accept(request, searchRequest); }	along the same lines as above, i wonder if we should try and share the validation that we already have in searchrequest
public void testRejectPointInTimeWithIndices() throws Exception { String authorizedUser = randomFrom("user1", "user2"); final String pitId = openPointInTime(new String[] { "index-" + authorizedUser }, authorizedUser); try { final Request request = new Request("POST", "/_async_search"); setRunAsHeader(request, authorizedUser); request.addParameter("wait_for_completion_timeout", "10m"); request.addParameter("keep_on_completion", "true"); if (randomBoolean()) { request.addParameter("index", "index-" + authorizedUser); } else { request.addParameter("index", "*"); } final XContentBuilder requestBody = JsonXContent.contentBuilder() .startObject() .startObject("pit") .field("id", pitId) .field("keep_alive", "1m") .endObject() .endObject(); request.setJsonEntity(Strings.toString(requestBody)); final ResponseException exc = expectThrows(ResponseException.class, () -> client().performRequest(request)); assertThat(exc.getResponse().getStatusLine().getStatusCode(), equalTo(400)); assertThat(exc.getMessage(), containsString("[indices] cannot be used with point in time")); } finally { closePointInTime(pitId, authorizedUser); } }	interesting, this was a mistake that was not though causing any failure?
private Query rethrowUnlessLenient(RuntimeException e) { if (settings.lenient()) { return Queries.newMatchNoDocsQuery("failed query, caused by " + e.getMessage()); } throw e; }	nit: can you update the comment to reflect that we return matchnodocs instead of null now?
static DeprecationIssue checkIndexMatrixFiltersSetting(IndexMetadata indexMetadata) { return checkRemovedSetting(indexMetadata.getSettings(), IndexSettings.MAX_ADJACENCY_MATRIX_FILTERS_SETTING, "https://www.elastic.co/guide/en/elasticsearch/reference/master/migrating-8.0.html#breaking_80_settings_changes", DeprecationIssue.Level.WARNING ); }	this needs to be a link to our 7.x documentation. maybe https://www.elastic.co/guide/en/elasticsearch/reference/7.15/search-aggregations-bucket-adjacency-matrix-aggregation.html#adjacency-matrix-agg-filter-limits? (related, i just noticed that that link says "this setting is deprecated and will be **repaced**").
public void testCreateSplitIndexToN() throws IOException { int[][] possibleShardSplits = new int[][]{{2, 4, 8}, {3, 6, 12}, {1, 2, 4}}; int[] shardSplits = randomFrom(possibleShardSplits); splitToN(shardSplits); }	i guess you had an explicit reason not to have a shrink to one shard then split again test (where we can take values in the split that doesn't compute with the source index)? alternatively we can explicitly set the routing shards on the source index to something that doesn't make sense when we start.
public void testCalculateNumRoutingShards() { assertEquals(1024, MetaDataCreateIndexService.calculateNumRoutingShards(1, Version.CURRENT)); assertEquals(1024, MetaDataCreateIndexService.calculateNumRoutingShards(2, Version.CURRENT)); assertEquals(1536, MetaDataCreateIndexService.calculateNumRoutingShards(3, Version.CURRENT)); assertEquals(1152, MetaDataCreateIndexService.calculateNumRoutingShards(9, Version.CURRENT)); assertEquals(1024, MetaDataCreateIndexService.calculateNumRoutingShards(512, Version.CURRENT)); assertEquals(2048, MetaDataCreateIndexService.calculateNumRoutingShards(1024, Version.CURRENT)); assertEquals(4096, MetaDataCreateIndexService.calculateNumRoutingShards(2048, Version.CURRENT)); Version latestV6 = VersionUtils.getPreviousVersion(Version.V_7_0_0_alpha1); int numShards = randomIntBetween(1, 1000); assertEquals(numShards, MetaDataCreateIndexService.calculateNumRoutingShards(numShards, latestV6)); assertEquals(numShards, MetaDataCreateIndexService.calculateNumRoutingShards(numShards, VersionUtils.randomVersionBetween(random(), VersionUtils.getFirstVersion(), latestV6))); for (int i = 0; i < 1000; i++) { int randomNumShards = randomIntBetween(1, 10000); int numRoutingShards = MetaDataCreateIndexService.calculateNumRoutingShards(randomNumShards, Version.CURRENT); double ratio = numRoutingShards / randomNumShards; int intRatio = (int) ratio; assertEquals(ratio, (double)(intRatio), 0.0d); assertTrue(1 < ratio); assertTrue(ratio <= 1024); assertEquals(0, intRatio % 2); } }	should we assert that intratio is a power of two by checking that intratio & (intratio - 1) is zero? (or intratio == integer.highestonebit(intratio) if you find it easier to read
@Override public Query rangeQuery(Object lowerTerm, Object upperTerm, boolean includeLower, boolean includeUpper, ShapeRelation relation, @Nullable ZoneId timeZone, @Nullable DateMathParser forcedDateParser, QueryShardContext context) { failIfNotIndexed(); if (relation == ShapeRelation.DISJOINT) { throw new IllegalArgumentException("Field [" + name() + "] of type [" + typeName() + "] does not support DISJOINT ranges"); } DateMathParser parser; if (forcedDateParser == null) { if (lowerTerm instanceof Integer || upperTerm instanceof Integer) { // force epoch_millis parser = EPOCH_MILLIS_PARSER; } else { parser = dateMathParser; } } else { parser = forcedDateParser; } return dateRangeQuery(lowerTerm, upperTerm, includeLower, includeUpper, timeZone, parser, context, resolution, (l, u) -> { Query query = LongPoint.newRangeQuery(name(), l, u); if (hasDocValues()) { Query dvQuery = SortedNumericDocValuesField.newSlowRangeQuery(name(), l, u); query = new IndexOrDocValuesQuery(query, dvQuery); if (context.indexSortedOnField(name())) { query = new IndexSortSortedNumericDocValuesRangeQuery(name(), l, u, query); } } return query; }); }	i think number is better here. that, or long and integer. its kind of plain luck that the range of numbers that cause the bug are small and so parsed as an integer instead of a long or whatever. but tests and stuff could still end up with a long.
@Override public Query rangeQuery(Object lowerTerm, Object upperTerm, boolean includeLower, boolean includeUpper, ShapeRelation relation, @Nullable ZoneId timeZone, @Nullable DateMathParser forcedDateParser, QueryShardContext context) { failIfNotIndexed(); if (relation == ShapeRelation.DISJOINT) { throw new IllegalArgumentException("Field [" + name() + "] of type [" + typeName() + "] does not support DISJOINT ranges"); } DateMathParser parser; if (forcedDateParser == null) { if (lowerTerm instanceof Integer || upperTerm instanceof Integer) { // force epoch_millis parser = EPOCH_MILLIS_PARSER; } else { parser = dateMathParser; } } else { parser = forcedDateParser; } return dateRangeQuery(lowerTerm, upperTerm, includeLower, includeUpper, timeZone, parser, context, resolution, (l, u) -> { Query query = LongPoint.newRangeQuery(name(), l, u); if (hasDocValues()) { Query dvQuery = SortedNumericDocValuesField.newSlowRangeQuery(name(), l, u); query = new IndexOrDocValuesQuery(query, dvQuery); if (context.indexSortedOnField(name())) { query = new IndexSortSortedNumericDocValuesRangeQuery(name(), l, u, query); } } return query; }); }	should we just cast them to a number and call longvalue on them? it feels like a shame to tostring them and then parse them if we know they are numbers.
static Mapping createDynamicUpdate(Mapping mapping, DocumentMapper docMapper, List<Mapper> dynamicMappers) { if (dynamicMappers.isEmpty()) { return null; } // We build a mapping by first sorting the mappers, so that all mappers containing a common prefix // will be processed in a contiguous block. When the prefix is no longer seen, we pop the extra elements // off the stack, merging them upwards into the existing mappers. Collections.sort(dynamicMappers, (Mapper o1, Mapper o2) -> o1.name().compareTo(o2.name())); Iterator<Mapper> dynamicMapperItr = dynamicMappers.iterator(); List<ObjectMapper> parentMappers = new ArrayList<>(); Mapper firstUpdate = dynamicMapperItr.next(); parentMappers.add(createUpdate(mapping.root(), firstUpdate.name().split("\\\\."), 0, firstUpdate)); Mapper previousMapper = null; while (dynamicMapperItr.hasNext()) { Mapper newMapper = dynamicMapperItr.next(); if (previousMapper != null && newMapper.name().equals(previousMapper.name())) { // We can see the same mapper more than once, for example, if we had foo.bar and foo.baz, where // foo did not yet exist. This will create 2 copies in dynamic mappings, which should be identical. // Here we just skip over the duplicates, but we merge them to ensure there are no conflicts. newMapper.merge(previousMapper, false); continue; } previousMapper = newMapper; String[] nameParts = newMapper.name().split("\\\\."); // find common elements with the previously processed dynamic mapper int keepBefore = 1; while (keepBefore < parentMappers.size() && parentMappers.get(keepBefore).simpleName().equals(nameParts[keepBefore - 1])) { ++keepBefore; } popMappers(parentMappers, keepBefore, true); if (keepBefore < nameParts.length) { String updateParentName = nameParts[keepBefore - 1]; final ObjectMapper lastParent = parentMappers.get(parentMappers.size() - 1); Mapper updateParent = lastParent.getMapper(updateParentName); if (updateParent == null) { // the parent we need is not on the stack, so look it up in the full mappings if (keepBefore > 1) { // only prefix with parent mapper if the parent mapper isn't the root (which has a fake name) updateParentName = lastParent.name() + '.' + updateParentName; } updateParent = docMapper.objectMappers().get(updateParentName); } assert updateParent instanceof ObjectMapper; newMapper = createUpdate((ObjectMapper)updateParent, nameParts, keepBefore, newMapper); } if (newMapper instanceof ObjectMapper) { parentMappers.add((ObjectMapper)newMapper); } else { addToLastMapper(parentMappers, newMapper, true); } } popMappers(parentMappers, 1, true); assert parentMappers.size() == 1; return mapping.mappingUpdate(parentMappers.get(0)); }	how is it supposed to work if the intermediate parent does not exist? i tried this pr agains the following document (mappings are initially empty): put test/test/1 { "a.b": 3 } and got the below exception: [elasticsearch] caused by: java.lang.nullpointerexception [elasticsearch] at org.elasticsearch.index.mapper.documentparser.addtolastmapper(documentparser.java:295) [elasticsearch] at org.elasticsearch.index.mapper.documentparser.createupdate(documentparser.java:314) [elasticsearch] at org.elasticsearch.index.mapper.documentparser.createdynamicupdate(documentparser.java:230) [elasticsearch] at org.elasticsearch.index.mapper.documentparser.parsedocument(documentparser.java:105) [elasticsearch] at org.elasticsearch.index.mapper.documentmapper.parse(documentmapper.java:286)
@Override protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageResponse> listener) { final ActionListener<List<XPackFeatureSet.Usage>> usageActionListener = listener.delegateFailure((l, usages) -> l.onResponse(new XPackUsageResponse(usages))); final AtomicReferenceArray<Usage> featureSetUsages = new AtomicReferenceArray<>(usageActions.size()); final AtomicInteger position = new AtomicInteger(0); final BiConsumer<XPackUsageFeatureAction, ActionListener<List<Usage>>> consumer = (featureUsageAction, iteratingListener) -> { // Since we're executing the actions locally we should create a new request // to avoid mutating the original request and setting the wrong parent task final XPackUsageRequest childRequest = new XPackUsageRequest(); childRequest.setParentTask(request.getParentTask()); client.executeLocally(featureUsageAction, childRequest, iteratingListener.delegateFailure((l, usageResponse) -> { featureSetUsages.set(position.getAndIncrement(), usageResponse.getUsage()); // the value sent back doesn't matter since our predicate keeps iterating l.onResponse(Collections.emptyList()); })); }; IteratingActionListener<List<XPackFeatureSet.Usage>, XPackUsageFeatureAction> iteratingActionListener = new IteratingActionListener<>(usageActionListener, consumer, usageActions, threadPool.getThreadContext(), (ignore) -> { final List<Usage> usageList = new ArrayList<>(featureSetUsages.length()); for (int i = 0; i < featureSetUsages.length(); i++) { usageList.add(featureSetUsages.get(i)); } return usageList; }, (ignore) -> true); iteratingActionListener.run(); }	just to check, does this mean that we exit the iteration early on cancellation (because the parent task would be banned from running any children)? perhaps add a comment to that effect if so.
public static DataType commonType(DataType left, DataType right) { if (left == right) { return left; } if (DataTypes.isNull(left)) { return right; } if (DataTypes.isNull(right)) { return left; } if (left.isString() && right.isString()) { if (left == TEXT) { return TEXT; } return right; } if (left.isNumeric() && right.isNumeric()) { // if one is int if (left.isInteger()) { // promote the highest int if (right.isInteger()) { return left.size > right.size ? left : right; } // promote the rational return right; } // try the other side if (right.isInteger()) { return left; } // promote the highest rational return left.size > right.size ? left : right; } if (left.isString()) { if (right.isNumeric()) { return right; } } if (right.isString()) { if (left.isNumeric()) { return left; } } // interval and dates if (left == DATE) { if (DataTypes.isInterval(right)) { return left; } } if (right == DATE) { if (DataTypes.isInterval(left)) { return right; } } if (left == TIME) { if (right == DATE) { return DATETIME; } if (DataTypes.isInterval(right)) { return left; } } if (right == TIME) { if (left == DATE) { return DATETIME; } if (DataTypes.isInterval(left)) { return right; } } if (left == DATETIME) { if (right == DATE || right == TIME) { return left; } if (DataTypes.isInterval(right)) { return left; } } if (right == DATETIME) { if (left == DATE || left == TIME) { return right; } if (DataTypes.isInterval(left)) { return right; } } // Interval * integer is a valid operation if (DataTypes.isInterval(left)) { if (right.isInteger()) { return left; } } if (DataTypes.isInterval(right)) { if (left.isInteger()) { return right; } } if (DataTypes.isInterval(left)) { // intervals widening if (DataTypes.isInterval(right)) { // null returned for incompatible intervals return DataTypes.compatibleInterval(left, right); } } // none found return null; }	why text and not keyword?
@Override public void afterRefresh(boolean didRefresh) throws IOException { if (Assertions.ENABLED) { assert callingThread != null : "afterRefresh called but not beforeRefresh"; assert callingThread == Thread.currentThread() : "beforeRefreshed called by a different thread. current [" + Thread.currentThread().getName() + "], thread that called beforeRefresh [" + callingThread.getName() + "]"; callingThread = null; } refreshMetric.inc(System.nanoTime() - currentRefreshStartTime); } } private EngineConfig.TombstoneDocSupplier tombstoneDocSupplier() { final RootObjectMapper.Builder noopRootMapper = new RootObjectMapper.Builder("__noop"); final DocumentMapper noopDocumentMapper = new DocumentMapper.Builder(noopRootMapper, mapperService).build(mapperService); return new EngineConfig.TombstoneDocSupplier() { @Override public ParsedDocument newDeleteTombstoneDoc(String type, String id) { return docMapper(type).getDocumentMapper().createDeleteTombstoneDoc(shardId.getIndexName(), type, id); } @Override public ParsedDocument newNoopTombstoneDoc(String reason) { return noopDocumentMapper.createNoopTombstoneDoc(shardId.getIndexName(), reason); } }; } /** * Rollback the current engine to the safe commit, then replay local translog up to the global checkpoint. */ void resetEngineToGlobalCheckpoint() throws IOException { assert getActiveOperationsCount() == 0 : "Ongoing writes [" + getActiveOperations() + "]"; sync(); // persist the global checkpoint to disk final SeqNoStats seqNoStats = seqNoStats(); final TranslogStats translogStats = translogStats(); // flush to make sure the latest commit, which will be opened by the read-only engine, includes all operations. flush(new FlushRequest()); synchronized (mutex) { verifyNotClosed(); // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata). final Engine readOnlyEngine = new ReadOnlyEngine(newEngineConfig(), seqNoStats, translogStats, false, Function.identity()); IOUtils.close(currentEngineReference.getAndSet(readOnlyEngine)); } Engine newEngine = null; try { final long globalCheckpoint = getGlobalCheckpoint(); trimUnsafeCommits(); synchronized (mutex) { // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata). newEngine = engineFactory.newReadWriteEngine(newEngineConfig()); onNewEngine(newEngine); } newEngine.advanceMaxSeqNoOfUpdatesOrDeletes(globalCheckpoint); final Engine.TranslogRecoveryRunner translogRunner = (engine, snapshot) -> runTranslogRecovery( engine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { // TODO: add a dedicate recovery stats for the reset translog }); newEngine.recoverFromTranslog(translogRunner, globalCheckpoint); synchronized (mutex) { verifyNotClosed(); IOUtils.close(currentEngineReference.getAndSet(newEngine)); // We set active because we are now writing operations to the engine; this way, // if we go idle after some time and become inactive, we still give sync'd flush a chance to run. active.set(true); newEngine = null; } // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during // which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine. onSettingsChanged(); } finally { IOUtils.close(newEngine); } } /** * Returns the maximum sequence number of either update or delete operations have been processed in this shard * or the sequence number from {@link #advanceMaxSeqNoOfUpdatesOrDeletes(long)}. An index request is considered * as an update operation if it overwrites the existing documents in Lucene index with the same document id. * <p> * The primary captures this value after executes a replication request, then transfers it to a replica before * executing that replication request on a replica. */ public long getMaxSeqNoOfUpdatesOrDeletes() { return getEngine().getMaxSeqNoOfUpdatesOrDeletes(); }	do we need to set waitifongoing to be 100% sure it's finished?
void resetEngineToGlobalCheckpoint() throws IOException { assert getActiveOperationsCount() == 0 : "Ongoing writes [" + getActiveOperations() + "]"; sync(); // persist the global checkpoint to disk final SeqNoStats seqNoStats = seqNoStats(); final TranslogStats translogStats = translogStats(); // flush to make sure the latest commit, which will be opened by the read-only engine, includes all operations. flush(new FlushRequest()); synchronized (mutex) { verifyNotClosed(); // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata). final Engine readOnlyEngine = new ReadOnlyEngine(newEngineConfig(), seqNoStats, translogStats, false, Function.identity()); IOUtils.close(currentEngineReference.getAndSet(readOnlyEngine)); } Engine newEngine = null; try { final long globalCheckpoint = getGlobalCheckpoint(); trimUnsafeCommits(); synchronized (mutex) { // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata). newEngine = engineFactory.newReadWriteEngine(newEngineConfig()); onNewEngine(newEngine); } newEngine.advanceMaxSeqNoOfUpdatesOrDeletes(globalCheckpoint); final Engine.TranslogRecoveryRunner translogRunner = (engine, snapshot) -> runTranslogRecovery( engine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { // TODO: add a dedicate recovery stats for the reset translog }); newEngine.recoverFromTranslog(translogRunner, globalCheckpoint); synchronized (mutex) { verifyNotClosed(); IOUtils.close(currentEngineReference.getAndSet(newEngine)); // We set active because we are now writing operations to the engine; this way, // if we go idle after some time and become inactive, we still give sync'd flush a chance to run. active.set(true); newEngine = null; } // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during // which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine. onSettingsChanged(); } finally { IOUtils.close(newEngine); } } /** * Returns the maximum sequence number of either update or delete operations have been processed in this shard * or the sequence number from {@link #advanceMaxSeqNoOfUpdatesOrDeletes(long)}	i think we need a verifynotclosed here?
void resetEngineToGlobalCheckpoint() throws IOException { assert getActiveOperationsCount() == 0 : "Ongoing writes [" + getActiveOperations() + "]"; sync(); // persist the global checkpoint to disk final SeqNoStats seqNoStats = seqNoStats(); final TranslogStats translogStats = translogStats(); // flush to make sure the latest commit, which will be opened by the read-only engine, includes all operations. flush(new FlushRequest()); synchronized (mutex) { verifyNotClosed(); // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata). final Engine readOnlyEngine = new ReadOnlyEngine(newEngineConfig(), seqNoStats, translogStats, false, Function.identity()); IOUtils.close(currentEngineReference.getAndSet(readOnlyEngine)); } Engine newEngine = null; try { final long globalCheckpoint = getGlobalCheckpoint(); trimUnsafeCommits(); synchronized (mutex) { // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata). newEngine = engineFactory.newReadWriteEngine(newEngineConfig()); onNewEngine(newEngine); } newEngine.advanceMaxSeqNoOfUpdatesOrDeletes(globalCheckpoint); final Engine.TranslogRecoveryRunner translogRunner = (engine, snapshot) -> runTranslogRecovery( engine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { // TODO: add a dedicate recovery stats for the reset translog }); newEngine.recoverFromTranslog(translogRunner, globalCheckpoint); synchronized (mutex) { verifyNotClosed(); IOUtils.close(currentEngineReference.getAndSet(newEngine)); // We set active because we are now writing operations to the engine; this way, // if we go idle after some time and become inactive, we still give sync'd flush a chance to run. active.set(true); newEngine = null; } // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during // which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine. onSettingsChanged(); } finally { IOUtils.close(newEngine); } } /** * Returns the maximum sequence number of either update or delete operations have been processed in this shard * or the sequence number from {@link #advanceMaxSeqNoOfUpdatesOrDeletes(long)}	onnewengine does't publish the new engine right?
protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener) { if (enabled == false) { MachineLearningFeatureSetUsage usage = new MachineLearningFeatureSetUsage(licenseState.isMachineLearningAllowed(), enabled, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), 0); listener.onResponse(new XPackUsageFeatureResponse(usage)); return; } Map<String, Object> jobsUsage = new LinkedHashMap<>(); Map<String, Object> datafeedsUsage = new LinkedHashMap<>(); Map<String, Object> analyticsUsage = new LinkedHashMap<>(); int nodeCount = mlNodeCount(state); // Step 3. Extract usage from data frame analytics stats and return usage response ActionListener<GetDataFrameAnalyticsStatsAction.Response> dataframeAnalyticsListener = ActionListener.wrap( response -> { addDataFrameAnalyticsUsage(response, analyticsUsage); MachineLearningFeatureSetUsage usage = new MachineLearningFeatureSetUsage(licenseState.isMachineLearningAllowed(), enabled, jobsUsage, datafeedsUsage, analyticsUsage, nodeCount); listener.onResponse(new XPackUsageFeatureResponse(usage)); }, listener::onFailure ); // Step 2. Extract usage from datafeeds stats and return usage response ActionListener<GetDatafeedsStatsAction.Response> datafeedStatsListener = ActionListener.wrap(response -> { addDatafeedsUsage(response, datafeedsUsage); GetDataFrameAnalyticsStatsAction.Request dataframeAnalyticsStatsRequest = new GetDataFrameAnalyticsStatsAction.Request(GetDatafeedsStatsAction.ALL); client.execute(GetDataFrameAnalyticsStatsAction.INSTANCE, dataframeAnalyticsStatsRequest, dataframeAnalyticsListener); }, listener::onFailure); // Step 1. Extract usage from jobs stats and then request stats for all datafeeds GetJobsStatsAction.Request jobStatsRequest = new GetJobsStatsAction.Request(MetaData.ALL); ActionListener<GetJobsStatsAction.Response> jobStatsListener = ActionListener.wrap( response -> { jobManagerHolder.getJobManager().expandJobs(MetaData.ALL, true, ActionListener.wrap(jobs -> { addJobsUsage(response, jobs.results(), jobsUsage); GetDatafeedsStatsAction.Request datafeedStatsRequest = new GetDatafeedsStatsAction.Request( GetDatafeedsStatsAction.ALL); client.execute(GetDatafeedsStatsAction.INSTANCE, datafeedStatsRequest, datafeedStatsListener); }, listener::onFailure)); }, listener::onFailure); // Step 0. Kick off the chain of callbacks by requesting jobs stats client.execute(GetJobsStatsAction.INSTANCE, jobStatsRequest, jobStatsListener); }	i think we should set the size to 10000 here.
public void testPendingRefreshWithIntervalChange() throws Exception { Settings.Builder builder = Settings.builder(); builder.put(IndexSettings.INDEX_SEARCH_IDLE_AFTER.getKey(), TimeValue.ZERO); IndexService indexService = createIndex("test", builder.build()); assertFalse(indexService.getIndexSettings().isExplicitRefresh()); ensureGreen(); assertNoSearchHits(client().prepareSearch().get()); client().prepareIndex("test", "test", "0").setSource("{\\"foo\\" : \\"bar\\"}", XContentType.JSON).get(); IndexShard shard = indexService.getShard(0); assertFalse(shard.scheduledRefresh()); assertTrue(shard.isSearchIdle()); CountDownLatch refreshLatch = new CountDownLatch(1); client().admin().indices().prepareRefresh() .execute(ActionListener.wrap(refreshLatch::countDown));// async on purpose to make sure it happens concurrently assertHitCount(client().prepareSearch().get(), 1); client().prepareIndex("test", "test", "1").setSource("{\\"foo\\" : \\"bar\\"}", XContentType.JSON).get(); assertFalse(shard.scheduledRefresh()); // now disable background refresh and make sure the refresh happens CountDownLatch updateSettingsLatch = new CountDownLatch(1); client().admin().indices() .prepareUpdateSettings("test") .setSettings(Settings.builder().put(IndexSettings.INDEX_REFRESH_INTERVAL_SETTING.getKey(), -1).build()) .execute(ActionListener.wrap(updateSettingsLatch::countDown)); assertHitCount(client().prepareSearch().get(), 2); // wait for both to ensure we don't have in-flight operations updateSettingsLatch.await(); refreshLatch.await(); // We need to ensure a `scheduledRefresh` triggered by the internal refresh setting update is executed before we index a new doc; // otherwise, it will compete with the `scheduledRefresh` that we are going to verify. Since the refresh in single-node tests // is a single thread executor, we can make sure that all scheduled refresh tasks are done by submitting a task then wait until // that task is completed. Note that using ThreadPoolStats is not watertight as both queue and active count can be 0 when // ThreadPoolExecutor(in #runWorker) just takes a task out the queue but before marking it active (i.e., lock the worker). CountDownLatch scheduledRefreshDone = new CountDownLatch(1); indexService.getThreadPool().executor(ThreadPool.Names.REFRESH).execute(scheduledRefreshDone::countDown); scheduledRefreshDone.await(); client().prepareIndex("test", "test", "2").setSource("{\\"foo\\" : \\"bar\\"}", XContentType.JSON).get(); assertTrue(shard.scheduledRefresh()); assertTrue(shard.isSearchIdle()); assertHitCount(client().prepareSearch().get(), 3); }	why is it a single thread executor? i can't see where this is enforced
public void toXContentFragment(XContentBuilder builder) throws IOException { builder.field(User.Fields.USERNAME.getPreferredName(), user.principal()); builder.array(User.Fields.ROLES.getPreferredName(), user.roles()); builder.field(User.Fields.FULL_NAME.getPreferredName(), user.fullName()); builder.field(User.Fields.EMAIL.getPreferredName(), user.email()); if (isServiceAccount()) { final String tokenName = (String) getMetadata().get(ServiceAccountSettings.TOKEN_NAME_FIELD); assert tokenName != null : "token name cannot be null"; final String tokenSource = (String) getMetadata().get(ServiceAccountSettings.TOKEN_SOURCE_FIELD); assert tokenSource != null : "token source cannot be null"; builder.field(User.Fields.TOKEN.getPreferredName(), Map.of("name", tokenName, "type", ServiceAccountSettings.REALM_TYPE + "_" + tokenSource)); } builder.field(User.Fields.METADATA.getPreferredName(), user.metadata()); builder.field(User.Fields.ENABLED.getPreferredName(), user.enabled()); builder.startObject(User.Fields.AUTHENTICATION_REALM.getPreferredName()); builder.field(User.Fields.REALM_NAME.getPreferredName(), getAuthenticatedBy().getName()); builder.field(User.Fields.REALM_TYPE.getPreferredName(), getAuthenticatedBy().getType()); builder.endObject(); builder.startObject(User.Fields.LOOKUP_REALM.getPreferredName()); if (getLookedUpBy() != null) { builder.field(User.Fields.REALM_NAME.getPreferredName(), getLookedUpBy().getName()); builder.field(User.Fields.REALM_TYPE.getPreferredName(), getLookedUpBy().getType()); } else { builder.field(User.Fields.REALM_NAME.getPreferredName(), getAuthenticatedBy().getName()); builder.field(User.Fields.REALM_TYPE.getPreferredName(), getAuthenticatedBy().getType()); } builder.endObject(); builder.field(User.Fields.AUTHENTICATION_TYPE.getPreferredName(), getAuthenticationType().name().toLowerCase(Locale.ROOT)); if (isApiKey()) { builder.field("api_key", Map.of( "id", this.metadata.get(ApiKeyServiceField.API_KEY_ID_KEY), "name", this.metadata.get(ApiKeyServiceField.API_KEY_NAME_KEY) ) ); // authentication.api_key={"id":"abc123", "name":"my-api-key"} } }	there are older api keys that can potentially have null name (#59485) and we should handle that.
public void testAuthenticateResponseApiKey() throws IOException { final Map<String, Object> createApiKeyRequestBody = Map.of("name", "my-api-key-name", "metadata", Map.of("a", "b")); final Request createApiKeyRequest = new Request("POST", "_security/api_key"); createApiKeyRequest.setJsonEntity(XContentTestUtils.convertToXContent(createApiKeyRequestBody, XContentType.JSON).utf8ToString()); final Response createApiKeyResponse = adminClient().performRequest(createApiKeyRequest); final Map<String, Object> createApiKeyResponseAsMap = responseAsMap(createApiKeyResponse); // keys: id, name, api_key, encoded final String encoded = (String) createApiKeyResponseAsMap.get("encoded"); // encoded=Base64(id:api_key) final Request authenticateRequest = new Request("GET", "_security/_authenticate"); authenticateRequest.setOptions(authenticateRequest.getOptions().toBuilder().addHeader( "Authorization", "ApiKey " + encoded)); final Response authenticateResponse = client().performRequest(authenticateRequest); assertOK(authenticateResponse); final Map<String, Object> authenticate = responseAsMap(authenticateResponse); // keys: username, roles, full_name, etc // If authentication type is API_KEY, authentication.api_key={"id":"abc123","name":"my-api-key"} // If authentication type is other, authentication.api_key not present. assertThat(authenticate.get("api_key"), instanceOf(Map.class)); // implies hasKey() final Map<?, ?> apiKey = (Map<?, ?>) authenticate.get("api_key"); // assert Map<String,Object> below assertThat(apiKey.keySet(), allOf( everyItem(instanceOf(String.class)), // assert apiKey Map<?,?> is safe to cast to Map<String,?> containsInAnyOrder("id", "name")) // assert apiKey Map<String,?> exactly contains these keys (and no others) ); assertThat(apiKey.get("id"), allOf(instanceOf(String.class), not(equalTo("")))); assertThat(apiKey.get("name"), allOf(instanceOf(String.class), not(equalTo("")))); }	i think these can be simplified with something like: java assertthat(authenticate, hasentry("api_key", map.of("id", "...", "name": "my-api-key-name"));
private void createOAuth2Tokens(String accessToken, String refreshToken, Version tokenVersion, SecurityIndexManager tokensIndex, Authentication authentication, Authentication originatingClientAuth, Map<String, Object> metadata, ActionListener<CreateTokenResult> listener) { assert accessToken.length() == TOKEN_LENGTH : "We assume token ids have a fixed length for nodes of a certain version." + " When changing the token length, be careful that the inferences about its length still hold."; ensureEnabled(); if (authentication == null) { listener.onFailure(traceLog("create token", new IllegalArgumentException("authentication must be provided"))); } else if (originatingClientAuth == null) { listener.onFailure(traceLog("create token", new IllegalArgumentException("originating client authentication must be provided"))); } else { final Authentication tokenAuth = new Authentication(authentication.getUser(), authentication.getAuthenticatedBy(), authentication.getLookedUpBy(), tokenVersion, AuthenticationType.TOKEN, authentication.getMetadata()); final String storedAccessToken; final String storedRefreshToken; if (tokenVersion.onOrAfter(VERSION_HASHED_TOKENS)) { storedAccessToken = hashTokenString(accessToken); storedRefreshToken = (null == refreshToken) ? null : hashTokenString(refreshToken); } else { storedAccessToken = accessToken; storedRefreshToken = refreshToken; } final UserToken userToken = new UserToken(storedAccessToken, tokenVersion, tokenAuth, getExpirationTime(), metadata); final BytesReference tokenDocument = createTokenDocument(userToken, storedRefreshToken, originatingClientAuth); final String documentId = getTokenDocumentId(storedAccessToken); final IndexRequest indexTokenRequest = client.prepareIndex(tokensIndex.aliasName()).setId(documentId) .setOpType(OpType.CREATE) .setSource(tokenDocument, XContentType.JSON) .setRefreshPolicy(RefreshPolicy.WAIT_UNTIL) .request(); tokensIndex.prepareIndexIfNeededThenExecute( ex -> listener.onFailure(traceLog("prepare tokens index [" + tokensIndex.aliasName() + "]", documentId, ex)), () -> executeAsyncWithOrigin(client, SECURITY_ORIGIN, IndexAction.INSTANCE, indexTokenRequest, ActionListener.wrap(indexResponse -> { if (indexResponse.getResult() == Result.CREATED) { final String versionedAccessToken = prependVersionAndEncodeAccessToken(tokenVersion, accessToken); if (tokenVersion.onOrAfter(VERSION_TOKENS_INDEX_INTRODUCED)) { final String versionedRefreshToken = refreshToken != null ? prependVersionAndEncodeRefreshToken(tokenVersion, refreshToken) : null; listener.onResponse(new CreateTokenResult(versionedAccessToken, versionedRefreshToken, authentication)); } else { // prior versions of the refresh token are not version-prepended, as nodes on those // versions don't expect it. // Such nodes might exist in a mixed cluster during a rolling upgrade. listener.onResponse(new CreateTokenResult(versionedAccessToken, refreshToken,authentication)); } } else { listener.onFailure(traceLog("create token", new ElasticsearchException("failed to create token document [{}]", indexResponse))); } }, listener::onFailure))); } }	it looks like there's some unnecessary formatting changes here and in a couple of other places.
*/ private void cacheRepositoryData(RepositoryData updated) { if (bestEffortConsistency == false) { try { final int len = BytesReference.bytes(updated.snapshotsToXContent(XContentFactory.jsonBuilder(), true)).length(); if (len > ByteSizeUnit.KB.toBytes(500)) { logger.debug("Not caching repository data of size [{}] for repository [{}] because it is larger than 500KB in" + " serialized size", len, metadata.name()); if (len > ByteSizeUnit.MB.toBytes(5)) { logger.warn("Your repository metadata blob for repository [{}] is larger than 5MB. Consider moving to a fresh" + " repository for new snapshots or deleting unneeded snapshots from your repository to ensure stable" + " repository behavior going forward.", metadata.name()); } // Set empty repository data to not waste heap for an outdated cached value latestKnownRepositoryData.set(RepositoryData.EMPTY); return; } } catch (IOException e) { throw new AssertionError("Impossible, no IO happens here", e); } latestKnownRepositoryData.updateAndGet(known -> { if (known.getGenId() > updated.getGenId()) { return known; } return updated; }); } }	maybe make repositorydata implements accountable? should we craft an estimating function instead of serializing the whole repositorydata back again? if we were serializing back to xcontent we could pass a filteroutputstream to the xcontentbuilder so that it just count bytes and not build everything in heap. or if we follow yannick's suggestion on caching the serialized and compressed bytes then maybe we should keep track of the length of the original blob.
*/ private void cacheRepositoryData(RepositoryData updated) { if (bestEffortConsistency == false) { try { final int len = BytesReference.bytes(updated.snapshotsToXContent(XContentFactory.jsonBuilder(), true)).length(); if (len > ByteSizeUnit.KB.toBytes(500)) { logger.debug("Not caching repository data of size [{}] for repository [{}] because it is larger than 500KB in" + " serialized size", len, metadata.name()); if (len > ByteSizeUnit.MB.toBytes(5)) { logger.warn("Your repository metadata blob for repository [{}] is larger than 5MB. Consider moving to a fresh" + " repository for new snapshots or deleting unneeded snapshots from your repository to ensure stable" + " repository behavior going forward.", metadata.name()); } // Set empty repository data to not waste heap for an outdated cached value latestKnownRepositoryData.set(RepositoryData.EMPTY); return; } } catch (IOException e) { throw new AssertionError("Impossible, no IO happens here", e); } latestKnownRepositoryData.updateAndGet(known -> { if (known.getGenId() > updated.getGenId()) { return known; } return updated; }); } }	if the node can't serialize the repositorydata, it would die here. perhaps just catch and log (while having an assert as well for our tests)
* @param rootBlobs Blobs at the repository root * @return RepositoryData */ private RepositoryData safeRepositoryData(long repositoryStateId, Map<String, BlobMetaData> rootBlobs) { final long generation = latestGeneration(rootBlobs.keySet()); final long genToLoad; final RepositoryData cached; if (bestEffortConsistency) { genToLoad = latestKnownRepoGen.updateAndGet(known -> Math.max(known, repositoryStateId)); cached = null; } else { genToLoad = latestKnownRepoGen.get(); cached = latestKnownRepositoryData.get(); } if (genToLoad > generation) { // It's always a possibility to not see the latest index-N in the listing here on an eventually consistent blob store, just // debug log it. Any blobs leaked as a result of an inconsistent listing here will be cleaned up in a subsequent cleanup or // snapshot delete run anyway. logger.debug("Determined repository's generation from its contents to [" + generation + "] but " + "current generation is at least [" + genToLoad + "]"); } if (genToLoad != repositoryStateId) { throw new RepositoryException(metadata.name(), "concurrent modification of the index-N file, expected current generation [" + repositoryStateId + "], actual current generation [" + genToLoad + "]"); } if (cached != null && cached.getGenId() == genToLoad) { return cached; } return getRepositoryData(genToLoad); } /** * After updating the {@link RepositoryData}	maybe we can just cache the serialized (and compressed) bytes instead of the object. decompressing should still be fast.
protected void awaitMasterFinishRepoOperations() throws Exception { logger.info("--> waiting for master to finish all repo operations on its SNAPSHOT pool"); final ThreadPool masterThreadPool = internalCluster().getCurrentMasterNodeInstance(ThreadPool.class); assertBusy(() -> { for (ThreadPoolStats.Stats stat : masterThreadPool.stats()) { if (ThreadPool.Names.SNAPSHOT.equals(stat.getName())) { assertEquals(stat.getActive(), 0); break; } } }); }	this actually fixes a bug, we just didn't see the bug in practice yet because we only used this method in tests with a single master node ...
private static ObjectParser createParser(Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings) { final ObjectParser<Map<String, Map<String, Map<String, FieldMappingMetaData>>>, String> parser = new ObjectParser<>(MAPPINGS.getPreferredName(), true, () -> mappings); parser.declareField((p, b, index) -> { final Map<String, Object> map = p.map(); final Map<String, Map<String, FieldMappingMetaData>> typeMappings = new HashMap<>(); for (Map.Entry<String, Object> typeObjectEntry : map.entrySet()) { final Map<String, FieldMappingMetaData> fieldsMapping = new HashMap<>(); typeMappings.put(typeObjectEntry.getKey(), fieldsMapping); final Object o1 = typeObjectEntry.getValue(); if (!(o1 instanceof Map)) { throw new ParsingException(p.getTokenLocation(), "Nested type mapping at " + index + "." + typeObjectEntry.getKey() + " is not found"); } final Map<String, Object> map2 = (Map) o1; for (Map.Entry<String, Object> e : map2.entrySet()) { final Object o2 = e.getValue(); if (!(o2 instanceof Map)) { throw new ParsingException(p.getTokenLocation(), "Nested field mapping at " + index + "." + typeObjectEntry.getKey() + "." + e.getKey() + " is not found"); } final Map<String, Object> map3 = (Map) o2; final String fullName = (String) map3.get(FieldMappingMetaData.FULL_NAME.getPreferredName()); final XContentBuilder jsonBuilder = jsonBuilder(); final Map<String, ?> values = (Map<String, ?>) map3.get(FieldMappingMetaData.MAPPING.getPreferredName()); jsonBuilder.map(values); final BytesReference source = BytesReference.bytes(jsonBuilder); final FieldMappingMetaData metaData = new FieldMappingMetaData(fullName, source); fieldsMapping.put(e.getKey(), metaData); } } mappings.put(index, typeMappings); }, MAPPINGS, ObjectParser.ValueType.OBJECT); return parser; }	it is much more normal to create the objectparser statically. i see that you did it this way so that you could collect the results into something, but that isn't really what object parser is *for*. i mean, you can do it by passing the mappings map as the context to the parse method and throwing out the return value, but that is weird. why not have the objectparser return the typemappings?
protected void execute(Terminal terminal, OptionSet options) throws Exception { final Map<String, String> settings = new HashMap<>(); for (final KeyValuePair kvp : settingOption.values(options)) { if (kvp.value.isEmpty()) { throw new UserException(ExitCodes.USAGE, "setting [" + kvp.key + "] must not be empty"); } if (settings.containsKey(kvp.key)) { final String message = String.format( Locale.ROOT, "setting [%s] already set, saw [%s] and [%s]", kvp.key, settings.get(kvp.key), kvp.value); throw new UserException(ExitCodes.USAGE, message); } settings.put(kvp.key, kvp.value); } putSystemPropertyIfSettingIsMissing(settings, "path.data", "es.path.data"); putSystemPropertyIfSettingIsMissing(settings, "path.home", "es.path.home"); putSystemPropertyIfSettingIsMissing(settings, "path.logs", "es.path.logs"); final Path pathConf = Paths.get(System.getProperty("es.path.conf")); execute(terminal, options, createEnv(terminal, settings, pathConf)); } /** Create an {@link Environment}	should there be a clearer error if the property wasn't set? while that shouldn't happen (since all our scripts explicitly set it), if it's missing then you get a not-very-helpful npe.
public void testShardStats() throws IOException { createIndex("test"); ensureGreen(); IndicesService indicesService = getInstanceFromNode(IndicesService.class); IndexService test = indicesService.indexService("test"); IndexShard shard = test.shard(0); ShardStats stats = new ShardStats(shard, new CommonStatsFlags()); assertEquals(shard.shardPath().getRootDataPath().toString(), stats.getDataPath()); assertEquals(shard.shardPath().getRootStatePath().toString(), stats.getStatePath()); assertEquals(shard.shardPath().isCustomDataPath(), stats.isCustomDataPath()); if (randomBoolean() || true) { // try to serialize it to ensure values survive the serialization BytesStreamOutput out = new BytesStreamOutput(); stats.writeTo(out); StreamInput in = StreamInput.wrap(out.bytes()); stats = ShardStats.readShardStats(in); } XContentBuilder builder = XContentFactory.jsonBuilder(); builder.startObject(); stats.toXContent(builder, EMPTY_PARAMS); builder.endObject(); String xContent = builder.string(); StringBuilder expectedSubSequence = new StringBuilder("\\"shard_path\\":{\\"state_path\\":\\""); expectedSubSequence.append(shard.shardPath().getRootStatePath().toString()); expectedSubSequence.append("\\",\\"data_path\\":\\""); expectedSubSequence.append(shard.shardPath().getRootDataPath().toString()); expectedSubSequence.append("\\",\\"is_custom_data_path\\":").append(shard.shardPath().isCustomDataPath()).append("}"); System.out.println(expectedSubSequence); System.out.println(xContent); assertTrue(xContent.contains(expectedSubSequence)); } // "shard_path":{"state_path":"/private/var/folders/qj/rsr2js6n275f3r88r1z5bbgw0000gn/T/org.elasticsearch.index.shard.IndexShardTests_EE3FC3D62D02988A-001/tempDir-001/data/single-node-cluster-CHILD_VM=[0]-CLUSTER_SEED=[9012992977055748205]-HASH=[13FDF896A8B7DEC0]/nodes/0","data_path":"/private/var/folders/qj/rsr2js6n275f3r88r1z5bbgw0000gn/T/org.elasticsearch.index.shard.IndexShardTests_EE3FC3D62D02988A-001/tempDir-001/data/single-node-cluster-CHILD_VM=[0]-CLUSTER_SEED=[9012992977055748205]-HASH=[13FDF896A8B7DEC0]/nodes/0","is_custom_data_path":"false} // "shard_path":{"state_path":"/private/var/folders/qj/rsr2js6n275f3r88r1z5bbgw0000gn/T/org.elasticsearch.index.shard.IndexShardTests_EE3FC3D62D02988A-001/tempDir-001/data/single-node-cluster-CHILD_VM=[0]-CLUSTER_SEED=[9012992977055748205]-HASH=[13FDF896A8B7DEC0]/nodes/0","data_path":"/private/var/folders/qj/rsr2js6n275f3r88r1z5bbgw0000gn/T/org.elasticsearch.index.shard.IndexShardTests_EE3FC3D62D02988A-001/tempDir-001/data/single-node-cluster-CHILD_VM=[0]-CLUSTER_SEED=[9012992977055748205]-HASH=[13FDF896A8B7DEC0]/nodes/0","is_custom_data_path":false}	didn't this cause forbidden apis to freak out?
public void testDnsTtl() { String propertyValue = System.getProperty("networkaddress.cache.ttl"); if (Strings.isNullOrEmpty(propertyValue)) { assertEquals(JvmInfo.jvmInfo().getDnsCacheExpiration(), "unlimited"); } else { assertEquals(JvmInfo.jvmInfo().getDnsCacheExpiration(), propertyValue); } }	as you mention in the comment... i think it is safe to grab the property here, set it to the the desired value, test it, then set it back to the original value in a finally block. assuming we don't execute any of the unit tests within the same jvm concurrently (i don't think we do, but not 100%).
*/ public void setHosts(HttpHost... hosts) { if (hosts == null) { throw new IllegalArgumentException("hosts must not be null"); } setHosts(Arrays.asList(hosts), hostTuple.metaResolver); } /** * Replaces the hosts that the client communicates with and the * {@link HostMetadata} used by any {@link HostSelector}	you lost a synchronized here, it's pretty important that only one thread at a time write modify the hosts. we should add a test for this maybe as a followup.
*/ public void setHosts(HttpHost... hosts) { if (hosts == null) { throw new IllegalArgumentException("hosts must not be null"); } setHosts(Arrays.asList(hosts), hostTuple.metaResolver); } /** * Replaces the hosts that the client communicates with and the * {@link HostMetadata} used by any {@link HostSelector}	why we don't we fail early in case hosts is empty? i see that the length check has been moved below
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	nit: shall we rather initialize to empty and replace this with an empty check? i see that empty is currently not an option anyways.
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	as far as i can see there are now two reasons why we need more rounds: 1) concurrent modifications to the blacklist (like before this change) 2) the selector rejects all hosts (introduced with this change) . instead of trying max 10 times, can we figure out whether the reason for having no hosts is either the former or the latter and act based on that? in the first case we can just retry (indefinitely) while in the second case i am not sure. do we fail the request or do we try a node that the selector doesn't want us to go to? probably the former but just double checking.
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	food for thought: are we sure that is the way to go? should we throw exception when the selector rejected all of the hosts? or try one that the selector doesn't like? or make this behaviour configurable in some way?
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	if we made the distinction that i suggested above on not having hosts for one or the other reason, maybe we could throw exception directly in the nexthostsonetime method, then this class would not be needed as all the needed info would be part of the exception message already?
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	because they are **marked** dead ?
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	from the list **of provided hosts** ? also remove the dot at the end of the line?
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	revived **from the blacklist** ?
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	maybe it would be nice to also print out how many nodes were provided as well (i know such info is not part of this class but it could help figuring out why we can't send the request)
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	nit: add an empty line?
*/ private HostTuple<Iterator<HttpHost>> nextHost(HostSelector hostSelector) throws IOException { int attempts = 0; NextHostsResult result; /* * Try to fetch the hosts to which we can send the request. It is possible that * this returns an empty collection because of concurrent modification to the * blacklist. */ do { final HostTuple<Set<HttpHost>> hostTuple = this.hostTuple; result = nextHostsOneTime(hostTuple, blacklist, lastHostIndex, System.nanoTime(), hostSelector); if (result.hosts == null) { if (logger.isDebugEnabled()) { logger.debug("No hosts avialable. Will retry. Failure is " + result.describeFailure()); } } else { // Success! return new HostTuple<>(result.hosts.iterator(), hostTuple.authCache, hostTuple.metaResolver); } attempts++; } while (attempts < MAX_NEXT_HOSTS_ATTEMPTS); throw new IOException("No hosts available for request. Last failure was " + result.describeFailure()); }	rename to selecthosts ?
public void testWithHostSelector() throws IOException { /* * note that this *doesn't* stopRandomHost(); because it might stop * the first host which we use for testing the selector. */ HostSelector firstPositionOnly = new HostSelector() { @Override public boolean select(HttpHost host, HostMetadata meta) { return httpHosts[0] == host; } }; RestClientActions withHostSelector = restClient.withHostSelector(firstPositionOnly); Response response = withHostSelector.performRequest("GET", "/firstOnly"); assertEquals(httpHosts[0], response.getHost()); restClient.close(); try { withHostSelector.performRequest("GET", "/firstOnly"); fail("expected a failure"); } catch (IllegalStateException e) { assertThat(e.getMessage(), containsString("status: STOPPED")); } }	maybe you want to run this in a loop a few times just to make sure?
Response getResponse() { if (response instanceof Response) { return (Response) response; } if (response instanceof ResponseException) { return ((ResponseException) response).getResponse(); } if (response instanceof Exception) { throw new AssertionError("unexpected response " + response.getClass(), (Exception) response); } throw new AssertionError("unexpected response " + response.getClass()); }	this isn't actually needed by the code *now* but it was useful for debugging when i made mistakes.
public static void startHttpServer() throws Exception { if (randomBoolean()) { pathPrefixWithoutLeadingSlash = "testPathPrefix/" + randomAsciiOfLengthBetween(1, 5); pathPrefix = "/" + pathPrefixWithoutLeadingSlash; } else { pathPrefix = pathPrefixWithoutLeadingSlash = ""; } int numHttpServers = randomIntBetween(2, 4); httpServers = new HttpServer[numHttpServers]; httpHosts = new HttpHost[numHttpServers]; for (int i = 0; i < numHttpServers; i++) { HttpServer httpServer = createHttpServer(); httpServers[i] = httpServer; httpHosts[i] = new HttpHost(httpServer.getAddress().getHostString(), httpServer.getAddress().getPort()); } httpServers[0].createContext(pathPrefix + "/firstOnly", new ResponseHandler(200)); }	do you need to register this other endpoint? can't you call the existing /200 endpoint instead?
public static void startHttpServer() throws Exception { if (randomBoolean()) { pathPrefixWithoutLeadingSlash = "testPathPrefix/" + randomAsciiOfLengthBetween(1, 5); pathPrefix = "/" + pathPrefixWithoutLeadingSlash; } else { pathPrefix = pathPrefixWithoutLeadingSlash = ""; } int numHttpServers = randomIntBetween(2, 4); httpServers = new HttpServer[numHttpServers]; httpHosts = new HttpHost[numHttpServers]; for (int i = 0; i < numHttpServers; i++) { HttpServer httpServer = createHttpServer(); httpServers[i] = httpServer; httpHosts[i] = new HttpHost(httpServer.getAddress().getHostString(), httpServer.getAddress().getPort()); } httpServers[0].createContext(pathPrefix + "/firstOnly", new ResponseHandler(200)); }	i think i would prefer to leave the client creation and shutdown before and after all tests rather than between each test. maybe we could find a different way to test that when the client is closed, the view is useless too.
public void testSetHostsWithMetadataResolver() throws IOException { HostMetadataResolver firstPositionIsClient = new HostMetadataResolver() { @Override public HostMetadata resolveMetadata(HttpHost host) { HostMetadata.Roles roles; if (host == httpHosts[0]) { roles = new HostMetadata.Roles(false, false, false); } else { roles = new HostMetadata.Roles(true, true, true); } return new HostMetadata("dummy", roles); } }; restClient.setHosts(Arrays.asList(httpHosts), firstPositionIsClient); assertSame(firstPositionIsClient, restClient.getHostMetadataResolver()); Response response = restClient.withHostSelector(HostSelector.NOT_MASTER).performRequest("GET", "/200"); assertEquals(httpHosts[0], response.getHost()); }	maybe you want to run this a few times in a loop?
private static void readHost(String nodeId, JsonParser parser, Scheme scheme, List<HttpHost> hosts, Map<HttpHost, HostMetadata> hostMetadata) throws IOException { HttpHost publishedHost = null; List<HttpHost> boundHosts = new ArrayList<>(); String fieldName = null; String version = null; boolean sawRoles = false; boolean master = false; boolean data = false; boolean ingest = false; while (parser.nextToken() != JsonToken.END_OBJECT) { if (parser.getCurrentToken() == JsonToken.FIELD_NAME) { fieldName = parser.getCurrentName(); } else if (parser.getCurrentToken() == JsonToken.START_OBJECT) { if ("http".equals(fieldName)) { while (parser.nextToken() != JsonToken.END_OBJECT) { if (parser.getCurrentToken() == JsonToken.VALUE_STRING && "publish_address".equals(parser.getCurrentName())) { URI publishAddressAsURI = URI.create(scheme + "://" + parser.getValueAsString()); publishedHost = new HttpHost(publishAddressAsURI.getHost(), publishAddressAsURI.getPort(), publishAddressAsURI.getScheme()); } else if (parser.currentToken() == JsonToken.START_ARRAY && "bound_address".equals(parser.getCurrentName())) { while (parser.nextToken() != JsonToken.END_ARRAY) { URI boundAddressAsURI = URI.create(scheme + "://" + parser.getValueAsString()); boundHosts.add(new HttpHost(boundAddressAsURI.getHost(), boundAddressAsURI.getPort(), boundAddressAsURI.getScheme())); } } else if (parser.getCurrentToken() == JsonToken.START_OBJECT) { parser.skipChildren(); } } } else { parser.skipChildren(); } } else if (parser.currentToken() == JsonToken.START_ARRAY) { if ("roles".equals(fieldName)) { sawRoles = true; while (parser.nextToken() != JsonToken.END_ARRAY) { switch (parser.getText()) { case "master": master = true; break; case "data": data = true; break; case "ingest": ingest = true; break; default: logger.warn("unknown role [" + parser.getText() + "] on node [" + nodeId + "]"); } } } else { parser.skipChildren(); } } else if (parser.currentToken().isScalarValue()) { if ("version".equals(fieldName)) { version = parser.getText(); } } } //http section is not present if http is not enabled on the node, ignore such nodes if (publishedHost == null) { logger.debug("skipping node [" + nodeId + "] with http disabled"); } else { logger.trace("adding node [" + nodeId + "]"); assert sawRoles : "didn't see roles for [" + nodeId + "]"; hosts.add(publishedHost); HostMetadata meta = new HostMetadata(version, new Roles(master, data, ingest)); for (HttpHost bound: boundHosts) { hostMetadata.put(bound, meta); } } }	this will only work in 5.x+ see #16963 . so maybe this should not be an assertion and we should also parse roles in the way 2.x returns them?
private static void readHost(String nodeId, JsonParser parser, Scheme scheme, List<HttpHost> hosts, Map<HttpHost, HostMetadata> hostMetadata) throws IOException { HttpHost publishedHost = null; List<HttpHost> boundHosts = new ArrayList<>(); String fieldName = null; String version = null; boolean sawRoles = false; boolean master = false; boolean data = false; boolean ingest = false; while (parser.nextToken() != JsonToken.END_OBJECT) { if (parser.getCurrentToken() == JsonToken.FIELD_NAME) { fieldName = parser.getCurrentName(); } else if (parser.getCurrentToken() == JsonToken.START_OBJECT) { if ("http".equals(fieldName)) { while (parser.nextToken() != JsonToken.END_OBJECT) { if (parser.getCurrentToken() == JsonToken.VALUE_STRING && "publish_address".equals(parser.getCurrentName())) { URI publishAddressAsURI = URI.create(scheme + "://" + parser.getValueAsString()); publishedHost = new HttpHost(publishAddressAsURI.getHost(), publishAddressAsURI.getPort(), publishAddressAsURI.getScheme()); } else if (parser.currentToken() == JsonToken.START_ARRAY && "bound_address".equals(parser.getCurrentName())) { while (parser.nextToken() != JsonToken.END_ARRAY) { URI boundAddressAsURI = URI.create(scheme + "://" + parser.getValueAsString()); boundHosts.add(new HttpHost(boundAddressAsURI.getHost(), boundAddressAsURI.getPort(), boundAddressAsURI.getScheme())); } } else if (parser.getCurrentToken() == JsonToken.START_OBJECT) { parser.skipChildren(); } } } else { parser.skipChildren(); } } else if (parser.currentToken() == JsonToken.START_ARRAY) { if ("roles".equals(fieldName)) { sawRoles = true; while (parser.nextToken() != JsonToken.END_ARRAY) { switch (parser.getText()) { case "master": master = true; break; case "data": data = true; break; case "ingest": ingest = true; break; default: logger.warn("unknown role [" + parser.getText() + "] on node [" + nodeId + "]"); } } } else { parser.skipChildren(); } } else if (parser.currentToken().isScalarValue()) { if ("version".equals(fieldName)) { version = parser.getText(); } } } //http section is not present if http is not enabled on the node, ignore such nodes if (publishedHost == null) { logger.debug("skipping node [" + nodeId + "] with http disabled"); } else { logger.trace("adding node [" + nodeId + "]"); assert sawRoles : "didn't see roles for [" + nodeId + "]"; hosts.add(publishedHost); HostMetadata meta = new HostMetadata(version, new Roles(master, data, ingest)); for (HttpHost bound: boundHosts) { hostMetadata.put(bound, meta); } } }	why do we need to add metadata for each bound hosts?
public void testSnapshotPrimaryRelocations() { final int masterNodeCount = randomFrom(1, 3, 5); setupTestCluster(masterNodeCount, randomIntBetween(2, 10)); String repoName = "repo"; String snapshotName = "snapshot"; final String index = "test"; final int shards = randomIntBetween(1, 10); final TestClusterNode masterNode = testClusterNodes.currentMaster(testClusterNodes.nodes.values().iterator().next().clusterService.state()); final AtomicBoolean createdSnapshot = new AtomicBoolean(); final AdminClient masterAdminClient = masterNode.client.admin(); masterAdminClient.cluster().preparePutRepository(repoName) .setType(FsRepository.TYPE).setSettings(Settings.builder().put("location", randomAlphaOfLength(10))) .execute( assertNoFailureListener( () -> masterAdminClient.indices().create( new CreateIndexRequest(index).waitForActiveShards(ActiveShardCount.ALL) .settings(defaultIndexSettings(shards)), assertNoFailureListener( () -> masterAdminClient.cluster().state(new ClusterStateRequest(), assertNoFailureListener( clusterStateResponse -> { final ShardRouting shardToRelocate = clusterStateResponse.getState().routingTable().allShards(index).get(0); final TestClusterNode currentPrimaryNode = testClusterNodes.nodeById(shardToRelocate.currentNodeId()); final TestClusterNode otherNode = testClusterNodes.randomDataNodeSafe(currentPrimaryNode.node.getName()); final Runnable maybeForceAllocate = new Runnable() { @Override public void run() { masterAdminClient.cluster().state(new ClusterStateRequest(), assertNoFailureListener( resp -> { final ShardRouting shardRouting = resp.getState().routingTable() .shardRoutingTable(shardToRelocate.shardId()).primaryShard(); if (shardRouting.unassigned() && shardRouting.unassignedInfo().getReason() == UnassignedInfo.Reason.NODE_LEFT) { if (masterNodeCount > 1) { scheduleNow(() -> testClusterNodes.stopNode(masterNode)); } testClusterNodes.randomDataNodeSafe().client.admin().cluster() .prepareCreateSnapshot(repoName, snapshotName) .execute(ActionListener.wrap(() -> { testClusterNodes.randomDataNodeSafe().client.admin().cluster() .deleteSnapshot( new DeleteSnapshotRequest(repoName, snapshotName), noopListener()); createdSnapshot.set(true); })); scheduleNow( () -> testClusterNodes.randomMasterNodeSafe().client.admin().cluster().reroute( new ClusterRerouteRequest().add( new AllocateEmptyPrimaryAllocationCommand( index, shardRouting.shardId().id(), otherNode.node.getName(), true) ), noopListener())); } else { scheduleSoon(this); } } )); } }; scheduleNow(() -> testClusterNodes.stopNode(currentPrimaryNode)); scheduleNow(maybeForceAllocate); } )))))); runUntil(() -> { final Optional<TestClusterNode> randomMaster = testClusterNodes.randomMasterNode(); if (randomMaster.isPresent()) { final SnapshotsInProgress snapshotsInProgress = randomMaster.get().clusterService.state().custom(SnapshotsInProgress.TYPE); return (snapshotsInProgress == null || snapshotsInProgress.entries().isEmpty()) && createdSnapshot.get(); } return false; }, TimeUnit.MINUTES.toMillis(20L)); clearDisruptionsAndAwaitSync(); assertTrue(createdSnapshot.get()); final SnapshotsInProgress finalSnapshotsInProgress = testClusterNodes.randomDataNodeSafe() .clusterService.state().custom(SnapshotsInProgress.TYPE); assertThat(finalSnapshotsInProgress.entries(), empty()); final Repository repository = masterNode.repositoriesService.repository(repoName); Collection<SnapshotId> snapshotIds = repository.getRepositoryData().getSnapshotIds(); assertThat(snapshotIds, either(hasSize(1)).or(hasSize(0))); }	poor man's primary relocation simulation until we have non-blocking recoveries.
public void connectToNodes(DiscoveryNodes discoveryNodes) { // override this method as it does blocking calls boolean callSuper = true; for (final DiscoveryNode node : discoveryNodes) { try { transportService.connectToNode(node); } catch (Exception e) { callSuper = false; } } if (callSuper) { super.connectToNodes(discoveryNodes); } }	we have to bail here if a connect fails, otherwise will block while trying to reconnect.
public void testExecuteUntilFirstNonClusterStateStep() throws IOException { setStateToKey(secondStepKey); Step startStep = policyStepsRegistry.getStep(index.getName(), secondStepKey); long now = randomNonNegativeLong(); ExecuteStepsUpdateTask task = new ExecuteStepsUpdateTask(mixedPolicyName, index, startStep, policyStepsRegistry, () -> now); ClusterState newState = task.execute(clusterState); StepKey currentStepKey = IndexLifecycleRunner.getCurrentStepKey(newState.metaData().index(index).getSettings()); assertThat(currentStepKey, equalTo(thirdStepKey)); assertThat(firstStep.getExecuteCount(), equalTo(0L)); assertThat(secondStep.getExecuteCount(), equalTo(1L)); assertThat(LifecycleSettings.LIFECYCLE_PHASE_TIME_SETTING.get(newState.metaData().index(index).getSettings()), equalTo(-1L)); assertThat(LifecycleSettings.LIFECYCLE_ACTION_TIME_SETTING.get(newState.metaData().index(index).getSettings()), equalTo(-1L)); assertThat(LifecycleSettings.LIFECYCLE_STEP_INFO_SETTING.get(newState.metaData().index(index).getSettings()), equalTo("")); }	i'd love to hear feedback about these tests, whether they should be removed or what?
public void testExecuteUntilFirstNonClusterStateStep() throws IOException { setStateToKey(secondStepKey); Step startStep = policyStepsRegistry.getStep(index.getName(), secondStepKey); long now = randomNonNegativeLong(); ExecuteStepsUpdateTask task = new ExecuteStepsUpdateTask(mixedPolicyName, index, startStep, policyStepsRegistry, () -> now); ClusterState newState = task.execute(clusterState); StepKey currentStepKey = IndexLifecycleRunner.getCurrentStepKey(newState.metaData().index(index).getSettings()); assertThat(currentStepKey, equalTo(thirdStepKey)); assertThat(firstStep.getExecuteCount(), equalTo(0L)); assertThat(secondStep.getExecuteCount(), equalTo(1L)); assertThat(LifecycleSettings.LIFECYCLE_PHASE_TIME_SETTING.get(newState.metaData().index(index).getSettings()), equalTo(-1L)); assertThat(LifecycleSettings.LIFECYCLE_ACTION_TIME_SETTING.get(newState.metaData().index(index).getSettings()), equalTo(-1L)); assertThat(LifecycleSettings.LIFECYCLE_STEP_INFO_SETTING.get(newState.metaData().index(index).getSettings()), equalTo("")); }	i think this just needs to be made more general and not just test what it means to be an invalid first step, but an invalid step in general? i don't see other "invalid" input tests here and this was at least one of them
public void testExecuteUntilFirstNonClusterStateStep() throws IOException { setStateToKey(secondStepKey); Step startStep = policyStepsRegistry.getStep(index.getName(), secondStepKey); long now = randomNonNegativeLong(); ExecuteStepsUpdateTask task = new ExecuteStepsUpdateTask(mixedPolicyName, index, startStep, policyStepsRegistry, () -> now); ClusterState newState = task.execute(clusterState); StepKey currentStepKey = IndexLifecycleRunner.getCurrentStepKey(newState.metaData().index(index).getSettings()); assertThat(currentStepKey, equalTo(thirdStepKey)); assertThat(firstStep.getExecuteCount(), equalTo(0L)); assertThat(secondStep.getExecuteCount(), equalTo(1L)); assertThat(LifecycleSettings.LIFECYCLE_PHASE_TIME_SETTING.get(newState.metaData().index(index).getSettings()), equalTo(-1L)); assertThat(LifecycleSettings.LIFECYCLE_ACTION_TIME_SETTING.get(newState.metaData().index(index).getSettings()), equalTo(-1L)); assertThat(LifecycleSettings.LIFECYCLE_STEP_INFO_SETTING.get(newState.metaData().index(index).getSettings()), equalTo("")); }	i think the check that we hit the end of our step.nextstepkey == null check can be added to the end of testexecutealluntilendofphase if we go that extra step in the execution in that test, we can get rid of this one
public static SortField[] readSortFields(StreamInput in) throws IOException { SortField[] fields = new SortField[in.readVInt()]; for (int i = 0; i < fields.length; i++) { fields[i] = readSortField(in); } return fields; }	can't you use writearray with a writer? i think that would be good instead of iterating manually. same on teh read end
*/ default Map<String, Supplier<UnicastHostsProvider>> getZenHostsProviders(TransportService transportService, NetworkService networkService) { return Collections.emptyMap(); } /** * Returns a consumer that validate the initial join cluster state. The validator, unless <code>null</code> is called exactly once per * join attempt but might be called multiple times during the lifetime of a node. Validators are expected to throw a * {@link IllegalStateException}	nit - since we call it both on the master and the joining node, i think we should say "if the node and the cluster state are incompatible"
public ClusterState deleteIndices(ClusterState currentState, Set<Index> indices) { final Metadata meta = currentState.metadata(); final Set<Index> indicesToDelete = new HashSet<>(); final Map<Index, DataStream> backingIndices = new HashMap<>(); for (Index index : indices) { IndexMetadata im = meta.getIndexSafe(index); IndexAbstraction.DataStream parent = meta.getIndicesLookup().get(im.getIndex().getName()).getParentDataStream(); if (parent != null) { if (parent.getWriteIndex().equals(im.getIndex())) { throw new IllegalArgumentException( "index [" + index.getName() + "] is the write index for data stream [" + parent.getName() + "] and cannot be deleted" ); } else { backingIndices.put(index, parent.getDataStream()); } } indicesToDelete.add(im.getIndex()); } // Check if index deletion conflicts with any running snapshots Set<Index> snapshottingIndices = SnapshotsService.snapshottingIndices(currentState, indicesToDelete); if (snapshottingIndices.isEmpty() == false) { throw new SnapshotInProgressException( "Cannot delete indices that are being snapshotted: " + snapshottingIndices + ". Try again after snapshot finishes or cancel the currently running snapshot." ); } RoutingTable.Builder routingTableBuilder = RoutingTable.builder(currentState.routingTable()); Metadata.Builder metadataBuilder = Metadata.builder(meta); ClusterBlocks.Builder clusterBlocksBuilder = ClusterBlocks.builder().blocks(currentState.blocks()); final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metadataBuilder.indexGraveyard()); final int previousGraveyardSize = graveyardBuilder.tombstones().size(); for (final Index index : indices) { String indexName = index.getName(); logger.info("{} deleting index", index); routingTableBuilder.remove(indexName); clusterBlocksBuilder.removeIndexBlocks(indexName); metadataBuilder.remove(indexName); if (backingIndices.containsKey(index)) { DataStream parent = metadataBuilder.dataStream(backingIndices.get(index).getName()); metadataBuilder.put(parent.removeBackingIndex(index)); } } // add tombstones to the cluster state for each deleted index final IndexGraveyard currentGraveyard = graveyardBuilder.addTombstones(indices).build(settings); metadataBuilder.indexGraveyard(currentGraveyard); // the new graveyard set on the metadata logger.trace( "{} tombstones purged from the cluster state. Previous tombstone size: {}. Current tombstone size: {}.", graveyardBuilder.getNumPurged(), previousGraveyardSize, currentGraveyard.getTombstones().size() ); final ClusterState.Builder builder = ClusterState.builder(currentState) .routingTable(routingTableBuilder.build()) .blocks(clusterBlocksBuilder.build()) .metadata(metadataBuilder.build()); ImmutableOpenMap.Builder<String, ClusterState.Custom> customBuilder = null; // update snapshot restore entries final RestoreInProgress restoreInProgress = currentState.custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY); RestoreInProgress updatedRestoreInProgress = RestoreService.updateRestoreStateWithDeletedIndices(restoreInProgress, indices); if (updatedRestoreInProgress != restoreInProgress) { customBuilder = ImmutableOpenMap.builder(currentState.getCustoms()); customBuilder.put(RestoreInProgress.TYPE, updatedRestoreInProgress); } // update snapshot(s) marked as to delete final SnapshotDeletionsPending deletionsInPending = currentState.custom( SnapshotDeletionsPending.TYPE, SnapshotDeletionsPending.EMPTY ); final SnapshotDeletionsPending updatedPendingDeletes = updateSnapshotDeletionsPending( deletionsInPending, indicesToDelete, currentState ); if (updatedPendingDeletes != deletionsInPending) { if (customBuilder == null) { customBuilder = ImmutableOpenMap.builder(currentState.getCustoms()); } customBuilder.put(SnapshotDeletionsPending.TYPE, updatedPendingDeletes); } if (customBuilder != null) { builder.customs(customBuilder.build()); } return allocationService.reroute(builder.build(), "deleted indices [" + indices + "]"); } /** * This method updates the list of snapshots marked as to be deleted if one or more searchable snapshots are deleted. * * The snapshots cannot be deleted at the same time of the searchable snapshots indices because deleting one or more snapshot requires a * consistent view of their repositories data, and getting the consistent views cannot be done in the same cluster state update. It is * also possible than one (or more) snapshot cannot be deleted immediately because the snapshot is involved in another restore or * cloning or the repository might not be writeable etc. To address those conflicting situations this method only captures the snapshot * information that are required to later delete the snapshot and stores them in a {@link SnapshotDeletionsPending.Entry}	perhaps we should fail the deletion if one of the snapshots we cannot delete is one of the ones we pass in here? i think we can defer this to follow-ups if you prefer.
private SnapshotDeletionsPending updateSnapshotDeletionsPending( final SnapshotDeletionsPending pendingDeletions, final Set<Index> indicesToDelete, final ClusterState state ) { final List<Settings> deletedIndicesSettings = indicesToDelete.stream() .map(index -> state.metadata().getIndexSafe(index).getSettings()) .filter(SearchableSnapshotsSettings::isSearchableSnapshotStore) .filter(indexSettings -> indexSettings.getAsBoolean(SEARCHABLE_SNAPSHOTS_DELETE_SNAPSHOT_ON_INDEX_DELETION, false)) .collect(Collectors.toList()); if (deletedIndicesSettings.isEmpty()) { return pendingDeletions; } final Set<SnapshotId> activeSearchableSnapshots = state.metadata() .indices() .stream() .map(Map.Entry::getValue) .filter(index -> indicesToDelete.contains(index.getIndex()) == false) .map(IndexMetadata::getSettings) .filter(SearchableSnapshotsSettings::isSearchableSnapshotStore) .map(MetadataDeleteIndexService::toSnapshotId) .collect(Collectors.toUnmodifiableSet()); final RepositoriesMetadata repositories = state.metadata().custom(RepositoriesMetadata.TYPE); // also used to deduplicate snapshots that were used by multiple deleted indices final Map<SnapshotId, Tuple<String, String>> snapshotsWithRepository = new HashMap<>(); // also used to log a warning for snapshots with unknown repository final Map<SnapshotId, Tuple<String, String>> snapshotsWithoutRepository = new HashMap<>(); for (Settings deletedIndexSettings : deletedIndicesSettings) { SnapshotId snapshotId = toSnapshotId(deletedIndexSettings); if (activeSearchableSnapshots.contains(snapshotId) == false) { String repositoryUuid = deletedIndexSettings.get(SEARCHABLE_SNAPSHOTS_REPOSITORY_UUID_SETTING_KEY); String repositoryName = deletedIndexSettings.get(SEARCHABLE_SNAPSHOTS_REPOSITORY_NAME_SETTING_KEY); Optional<RepositoryMetadata> repository = findRepositoryForPendingDeletion(repositories, repositoryName, repositoryUuid); if (repository.isPresent()) { snapshotsWithRepository.putIfAbsent(snapshotId, Tuple.tuple(repositoryName, repositoryUuid)); } else { snapshotsWithoutRepository.putIfAbsent(snapshotId, Tuple.tuple(repositoryName, repositoryUuid)); } } } final int maxPendingDeletions = SnapshotDeletionsPending.MAX_PENDING_DELETIONS_SETTING.get(settings); final SnapshotDeletionsPending.Builder builder = new SnapshotDeletionsPending.Builder( pendingDeletions, evicted -> logger.warn( () -> new ParameterizedMessage( "maximum number of snapshots [{}] awaiting deletion has been reached in " + "cluster state before snapshot [{}] deleted on [{}] in repository [{}/{}] could be deleted", maxPendingDeletions, evicted.getSnapshotId(), Instant.ofEpochMilli(evicted.getIndexDeletionTime()).atZone(ZoneOffset.UTC), evicted.getRepositoryName(), evicted.getRepositoryUuid() ) ) ); final long timestamp = Instant.now().toEpochMilli(); for (Map.Entry<SnapshotId, Tuple<String, String>> entry : snapshotsWithRepository.entrySet()) { logger.debug("snapshot [{}:{}] added to the list of snapshots pending deletion", entry.getValue().v1(), entry.getKey()); builder.add(entry.getValue().v1(), entry.getValue().v2(), entry.getKey(), timestamp); } for (Map.Entry<SnapshotId, Tuple<String, String>> entry : snapshotsWithoutRepository.entrySet()) { // TODO also log that it will stay as pending for a given time/attempts and then be removed? logger.warn( "snapshot [{}] added to the list of snapshots pending deletion but refers to an unregistered repository [{}/{}]", entry.getKey(), entry.getValue().v1(), entry.getValue().v2() ); builder.add(entry.getValue().v1(), entry.getValue().v2(), entry.getKey(), timestamp); } return builder.build(settings); }	i think we will log once that it is added to the pending list and then also when it is removed since it exceeds the list? i think that should be enough, i.e., we can remove this todo imo.
* @param oldGen previous safe repository generation * @param newGen new safe repository generation * @return updated cluster state */ private ClusterState updateRepositoryGenerationsIfNecessary(ClusterState state, long oldGen, long newGen) { final String repoName = metadata.name(); final SnapshotsInProgress updatedSnapshotsInProgress; boolean changedSnapshots = false; final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>(); final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); for (SnapshotsInProgress.Entry entry : state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).forRepo(repoName)) { if (entry.repositoryStateId() == oldGen) { snapshotEntries.add(entry.withRepoGen(newGen)); changedSnapshots = true; } else { snapshotEntries.add(entry); } } updatedSnapshotsInProgress = changedSnapshots ? snapshotsInProgress.withUpdatedEntriesForRepo(repoName, snapshotEntries) : null; final SnapshotDeletionsInProgress updatedDeletionsInProgress; boolean changedDeletions = false; final List<SnapshotDeletionsInProgress.Entry> deletionEntries = new ArrayList<>(); for (SnapshotDeletionsInProgress.Entry entry : state.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY) .getEntries()) { if (entry.repository().equals(repoName) && entry.repositoryStateId() == oldGen) { deletionEntries.add(entry.withRepoGen(newGen)); changedDeletions = true; } else { deletionEntries.add(entry); } } updatedDeletionsInProgress = changedDeletions ? SnapshotDeletionsInProgress.of(deletionEntries) : null; final SnapshotDeletionsPending pendingDeletions = state.custom(SnapshotDeletionsPending.TYPE); return SnapshotsService.updateWithSnapshots(state, updatedSnapshotsInProgress, updatedDeletionsInProgress, pendingDeletions); }	could we not simply pass null as the last arg here instead? looks like all we want is to copy the existing pending deletions, which looks like it is the default anyway?
@Override public void applyClusterState(ClusterChangedEvent event) { try { if (event.localNodeMaster()) { // We don't remove old master when master flips anymore. So, we need to check for change in master SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); final boolean newMaster = event.previousState().nodes().isLocalNodeElectedMaster() == false; processExternalChanges( newMaster || removedNodesCleanupNeeded(snapshotsInProgress, event.nodesDelta().removedNodes()), event.routingTableChanged() && waitingShardsStartedOrUnassigned(snapshotsInProgress, event) ); } else { if (snapshotCompletionListeners.isEmpty() == false) { // We have snapshot listeners but are not the master any more. Fail all waiting listeners except for those that already // have their snapshots finalizing (those that are already finalizing will fail on their own from to update the cluster // state). for (Snapshot snapshot : Set.copyOf(snapshotCompletionListeners.keySet())) { if (endingSnapshots.add(snapshot)) { failSnapshotCompletionListeners(snapshot, new SnapshotException(snapshot, "no longer master")); } } } } triggerSnapshotsPendingDeletions(event); } catch (Exception e) { assert false : new AssertionError(e); logger.warn("Failed to update snapshot state ", e); } assert assertConsistentWithClusterState(event.state()); assert assertNoDanglingSnapshots(event.state()); }	i fail to see the change here except for the extra curly brace around the if? perhaps revert this bit.
public void testDeleteIndexWithSnapshotDeletion() { final boolean deleteSnapshot = randomBoolean(); final IndexMetadata indexMetadata = IndexMetadata.builder("test") .settings( Settings.builder() .put("index.version.created", VersionUtils.randomVersion(random())) .put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOT_STORE_TYPE) .put(SEARCHABLE_SNAPSHOTS_REPOSITORY_NAME_SETTING_KEY, "repo_name") .put(SEARCHABLE_SNAPSHOTS_REPOSITORY_UUID_SETTING_KEY, randomBoolean() ? null : "repo_uuid") .put(SEARCHABLE_SNAPSHOTS_SNAPSHOT_NAME_SETTING_KEY, "snap_name") .put(SEARCHABLE_SNAPSHOTS_SNAPSHOT_UUID_SETTING_KEY, "snap_uuid") .put(SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOTS_DELETE_SNAPSHOT_ON_INDEX_DELETION, deleteSnapshot) .build() ) .numberOfShards(1) .numberOfReplicas(1) .build(); final ClusterState initialState = ClusterState.builder(ClusterName.DEFAULT) .metadata( Metadata.builder() .put(indexMetadata, false) .putCustom( RepositoriesMetadata.TYPE, new RepositoriesMetadata(List.of(new RepositoryMetadata("repo_name", "fs", Settings.EMPTY).withUuid("repo_uuid"))) ) ) .routingTable(RoutingTable.builder().addAsNew(indexMetadata).build()) .blocks(ClusterBlocks.builder().addBlocks(indexMetadata)) .build(); final ClusterState updatedState = service.deleteIndices(initialState, Set.of(indexMetadata.getIndex())); assertThat(updatedState.metadata().getIndices().get("test"), nullValue()); assertThat(updatedState.blocks().indices().get("test"), nullValue()); assertThat(updatedState.routingTable().index("test"), nullValue()); final SnapshotDeletionsPending updatedPendingDeletions = updatedState.custom(SnapshotDeletionsPending.TYPE); if (deleteSnapshot) { assertThat(updatedPendingDeletions, notNullValue()); assertThat(updatedPendingDeletions.isEmpty(), equalTo(false)); assertThat(updatedPendingDeletions.contains(new SnapshotId("snap_name", "snap_uuid")), equalTo(true)); } else { assertThat(updatedPendingDeletions, nullValue()); } }	can we also validate the it has size 1, i.e., only led to one pending deletion?
public void testDeleteIndexWithSnapshotDeletion() { final boolean deleteSnapshot = randomBoolean(); final IndexMetadata indexMetadata = IndexMetadata.builder("test") .settings( Settings.builder() .put("index.version.created", VersionUtils.randomVersion(random())) .put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOT_STORE_TYPE) .put(SEARCHABLE_SNAPSHOTS_REPOSITORY_NAME_SETTING_KEY, "repo_name") .put(SEARCHABLE_SNAPSHOTS_REPOSITORY_UUID_SETTING_KEY, randomBoolean() ? null : "repo_uuid") .put(SEARCHABLE_SNAPSHOTS_SNAPSHOT_NAME_SETTING_KEY, "snap_name") .put(SEARCHABLE_SNAPSHOTS_SNAPSHOT_UUID_SETTING_KEY, "snap_uuid") .put(SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOTS_DELETE_SNAPSHOT_ON_INDEX_DELETION, deleteSnapshot) .build() ) .numberOfShards(1) .numberOfReplicas(1) .build(); final ClusterState initialState = ClusterState.builder(ClusterName.DEFAULT) .metadata( Metadata.builder() .put(indexMetadata, false) .putCustom( RepositoriesMetadata.TYPE, new RepositoriesMetadata(List.of(new RepositoryMetadata("repo_name", "fs", Settings.EMPTY).withUuid("repo_uuid"))) ) ) .routingTable(RoutingTable.builder().addAsNew(indexMetadata).build()) .blocks(ClusterBlocks.builder().addBlocks(indexMetadata)) .build(); final ClusterState updatedState = service.deleteIndices(initialState, Set.of(indexMetadata.getIndex())); assertThat(updatedState.metadata().getIndices().get("test"), nullValue()); assertThat(updatedState.blocks().indices().get("test"), nullValue()); assertThat(updatedState.routingTable().index("test"), nullValue()); final SnapshotDeletionsPending updatedPendingDeletions = updatedState.custom(SnapshotDeletionsPending.TYPE); if (deleteSnapshot) { assertThat(updatedPendingDeletions, notNullValue()); assertThat(updatedPendingDeletions.isEmpty(), equalTo(false)); assertThat(updatedPendingDeletions.contains(new SnapshotId("snap_name", "snap_uuid")), equalTo(true)); } else { assertThat(updatedPendingDeletions, nullValue()); } }	can we also validate the repo uuid and repo name are correct?
public void testDeleteMultipleIndicesWithSnapshotDeletion() { RepositoryMetadata repositoryMetadata = new RepositoryMetadata(randomAlphaOfLength(10), "fs", Settings.EMPTY); if (randomBoolean()) { repositoryMetadata = repositoryMetadata.withUuid(UUIDs.randomBase64UUID()); } final Metadata.Builder metadataBuilder = Metadata.builder(); metadataBuilder.putCustom(RepositoriesMetadata.TYPE, new RepositoriesMetadata(List.of(repositoryMetadata))); final RoutingTable.Builder routingBuilder = RoutingTable.builder(); final SnapshotId snapshotId = new SnapshotId(randomAlphaOfLength(10), UUIDs.randomBase64UUID()); final Set<Index> indices = new HashSet<>(); final int nbIndices = randomIntBetween(2, 10); for (int i = 0; i < nbIndices; i++) { Settings.Builder indexSettingsBuilder = Settings.builder() .put("index.version.created", VersionUtils.randomVersion(random())) .put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOT_STORE_TYPE) .put(SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOTS_DELETE_SNAPSHOT_ON_INDEX_DELETION, true) .put(SEARCHABLE_SNAPSHOTS_REPOSITORY_NAME_SETTING_KEY, repositoryMetadata.name()) .put(SEARCHABLE_SNAPSHOTS_SNAPSHOT_NAME_SETTING_KEY, snapshotId.getName()) .put(SEARCHABLE_SNAPSHOTS_SNAPSHOT_UUID_SETTING_KEY, snapshotId.getUUID()); if (randomBoolean()) { indexSettingsBuilder.put(SEARCHABLE_SNAPSHOTS_REPOSITORY_UUID_SETTING_KEY, repositoryMetadata.uuid()); } IndexMetadata indexMetadata = IndexMetadata.builder(randomAlphaOfLength(10) + i) .settings(indexSettingsBuilder.build()) .numberOfShards(randomIntBetween(1, 3)) .numberOfReplicas(randomInt(1)) .build(); metadataBuilder.put(indexMetadata, false); routingBuilder.addAsNew(indexMetadata); indices.add(indexMetadata.getIndex()); } ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT) .routingTable(routingBuilder.build()) .metadata(metadataBuilder) .build(); SnapshotDeletionsPending pendingDeletions = clusterState.custom(SnapshotDeletionsPending.TYPE, SnapshotDeletionsPending.EMPTY); while (indices.size() > 0) { assertThat(pendingDeletions.isEmpty(), equalTo(true)); List<Index> indicesToDelete = randomSubsetOf(randomIntBetween(1, Math.max(1, indices.size() - 1)), indices); clusterState = service.deleteIndices(clusterState, Set.copyOf(indicesToDelete)); indicesToDelete.forEach(indices::remove); for (Index deletedIndex : indicesToDelete) { assertThat(clusterState.metadata().index(deletedIndex), nullValue()); assertThat(clusterState.routingTable().index(deletedIndex), nullValue()); } pendingDeletions = clusterState.custom(SnapshotDeletionsPending.TYPE, SnapshotDeletionsPending.EMPTY); } assertThat(pendingDeletions.isEmpty(), equalTo(false)); assertThat(pendingDeletions.contains(snapshotId), equalTo(true)); }	can we also validate the it has size 1, i.e., only led to one pending deletion?
@Override protected ShardRefreshResponse shardOperation(ShardRefreshRequest request) throws ElasticsearchException { IndexShard indexShard = indicesService.indexServiceSafe(request.shardId().getIndex()).shardSafe(request.shardId().id()); indexShard.refresh("api"); logger.trace("{} refresh request executed, force: [{}]", indexShard.shardId()); return new ShardRefreshResponse(request.shardId()); }	should remove the "force:[{}]" in trace logger. @s1monw
@Override public ClusterState execute(ClusterState currentState) throws Exception { final ImmutableOpenMap<String, IndexMetadata> indexMetadataMap = currentState.metadata().indices(); final List<IndexMetadata> updatedMetadata = new ArrayList<>(); for (ObjectObjectCursor<String, IndexMetadata> cursor : indexMetadataMap) { if (cursor.value != lastIndexMetadataMap.get(cursor.key)) { final boolean isSystem = systemIndices.isSystemIndex(cursor.value.getIndex()) || systemIndices.isSystemIndexBackingDataStream(cursor.value.getIndex().getName()); if (isSystem != cursor.value.isSystem()) { updatedMetadata.add(IndexMetadata.builder(cursor.value).system(cursor.value.isSystem() == false).build()); } if (isSystem) { if (cursor.value.getSettings().getAsBoolean(IndexMetadata.SETTING_INDEX_HIDDEN, false)) { throw new IllegalStateException("Cannot define index [" + cursor.value.getIndex().getName() + "] as a system index because it has the [index.hidden] setting set to true."); } } } } if (updatedMetadata.isEmpty() == false) { final Metadata.Builder builder = Metadata.builder(currentState.metadata()); updatedMetadata.forEach(idxMeta -> builder.put(idxMeta, true)); return ClusterState.builder(currentState).metadata(builder).build(); } return currentState; }	i don't believe this should throw an error but rather ensure that the setting is set to false and if not, correct it, in this task. this validation should happen elsewhere. maybe in the metadataupdatesettingsservice or the setting validator?
protected void masterOperation( Task task, GetFeatureUpgradeStatusRequest request, ClusterState state, ActionListener<GetFeatureUpgradeStatusResponse> listener ) throws Exception { List<GetFeatureUpgradeStatusResponse.FeatureUpgradeStatus> features = systemIndices.getFeatures() .entrySet() .stream() .sorted(Map.Entry.comparingByKey()) .map(entry -> getFeatureUpgradeStatus(state, entry)) .collect(Collectors.toList()); GetFeatureUpgradeStatusResponse.UpgradeStatus status = features.stream() .map(GetFeatureUpgradeStatusResponse.FeatureUpgradeStatus::getUpgradeStatus) .reduce(GetFeatureUpgradeStatusResponse.UpgradeStatus::combine) .orElseGet(() -> { assert false : "get feature statuses API doesn't have any features"; return NO_UPGRADE_NEEDED; }); listener.onResponse(new GetFeatureUpgradeStatusResponse(features, status)); }	we should probably pull this version number out into a constant to make it more visible and to use it in tests.
public abstract Translog.Snapshot newLuceneChangesSnapshot(String source, MapperService mapperService, long minSeqNo, long maxSeqNo, boolean requiredFullRange) throws IOException; /** * Creates a new translog snapshot for reading operations whose seq# is in the provided range. * The returned snapshot can be retrieved from either Lucene index or translog files depending on * {@link org.elasticsearch.index.IndexSettings#INDEX_SOFT_DELETES_USE_IN_PEER_RECOVERY_SETTING}	nit: shall we rename this to getoperationhistroy? i think we can start moving from the the translog universe.
public void resync(final IndexShard indexShard, final ActionListener<ResyncTask> listener) { ActionListener<ResyncTask> resyncListener = null; try { final long startingSeqNo = indexShard.getGlobalCheckpoint() + 1; Translog.Snapshot snapshot = indexShard.newTranslogSnapshot("primary-replica-resync", startingSeqNo, Long.MAX_VALUE); resyncListener = new ActionListener<ResyncTask>() { @Override public void onResponse(final ResyncTask resyncTask) { try { snapshot.close(); listener.onResponse(resyncTask); } catch (final Exception e) { onFailure(e); } } @Override public void onFailure(final Exception e) { try { snapshot.close(); } catch (final Exception inner) { e.addSuppressed(inner); } finally { listener.onFailure(e); } } }; ShardId shardId = indexShard.shardId(); // Wrap translog snapshot to make it synchronized as it is accessed by different threads through SnapshotSender. // Even though those calls are not concurrent, snapshot.next() uses non-synchronized state and is not multi-thread-compatible // Also fail the resync early if the shard is shutting down Translog.Snapshot wrappedSnapshot = new Translog.Snapshot() { @Override public synchronized void close() throws IOException { snapshot.close(); } @Override public synchronized int totalOperations() { return snapshot.totalOperations(); } @Override public synchronized Translog.Operation next() throws IOException { IndexShardState state = indexShard.state(); if (state == IndexShardState.CLOSED) { throw new IndexShardClosedException(shardId); } else { assert state == IndexShardState.STARTED : "resync should only happen on a started shard, but state was: " + state; } return snapshot.next(); } }; resync(shardId, indexShard.routingEntry().allocationId().getId(), indexShard.getPrimaryTerm(), wrappedSnapshot, startingSeqNo, resyncListener); } catch (Exception e) { if (resyncListener != null) { resyncListener.onFailure(e); } else { listener.onFailure(e); } } }	this is tricky - we need to make sure that we do a refresh upon promotion so all operations are visible. instead of doing this here, how about making two explicit methods for now - one for the translog and one like you have now (with a fallback to the translog if soft deletes are not in lucene). then we can use the first one for here and do a follow up to shift the primary/replica syncing to lucene as well (and make sure we have proprer testing)
public void testGetTransform() throws IOException { String sourceIndex = "transform-source"; createIndex(sourceIndex); String id = "test-get"; DataFrameTransformConfig transform = validDataFrameTransformConfig(id, sourceIndex, "pivot-dest"); DataFrameClient client = highLevelClient().dataFrame(); putTransform(transform); GetDataFrameTransformRequest getRequest = new GetDataFrameTransformRequest(id); GetDataFrameTransformResponse getResponse = execute(getRequest, client::getDataFrameTransform, client::getDataFrameTransformAsync); assertNull(getResponse.getInvalidTransforms()); assertThat(getResponse.getTransformConfigurations(), hasSize(1)); assertEquals(transform, getResponse.getTransformConfigurations().get(0)); }	it feels that the paradigm we've followed with ml jobs where we have a cleaner class (see mlteststatecleaner) might be nice to use here too. it frees up the tests from having to worry about cleaning up and thus makes the test easier to write/read and less error prone. of course, we can push this fix as is and refactor in a different time & pr.
public void testGetStats() throws Exception { String sourceIndex = "transform-source"; createIndex(sourceIndex); indexData(sourceIndex); GroupConfig groupConfig = GroupConfig.builder().groupBy("reviewer", TermsGroupSource.builder().setField("user_id").build()).build(); AggregatorFactories.Builder aggBuilder = new AggregatorFactories.Builder(); aggBuilder.addAggregator(AggregationBuilders.avg("avg_rating").field("stars")); PivotConfig pivotConfig = PivotConfig.builder().setGroups(groupConfig).setAggregations(aggBuilder).build(); String id = "test-get-stats"; DataFrameTransformConfig transform = DataFrameTransformConfig.builder() .setId(id) .setSource(SourceConfig.builder().setIndex(sourceIndex).setQuery(new MatchAllQueryBuilder()).build()) .setDest(DestConfig.builder().setIndex("pivot-dest").build()) .setPivotConfig(pivotConfig) .setDescription("transform for testing stats") .build(); DataFrameClient client = highLevelClient().dataFrame(); putTransform(transform); GetDataFrameTransformStatsResponse statsResponse = execute(new GetDataFrameTransformStatsRequest(id), client::getDataFrameTransformStats, client::getDataFrameTransformStatsAsync); assertEquals(1, statsResponse.getTransformsStateAndStats().size()); DataFrameTransformStateAndStats stats = statsResponse.getTransformsStateAndStats().get(0); assertEquals(DataFrameTransformTaskState.STOPPED, stats.getTransformState().getTaskState()); assertEquals(IndexerState.STOPPED, stats.getTransformState().getIndexerState()); DataFrameIndexerTransformStats zeroIndexerStats = new DataFrameIndexerTransformStats(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L); assertEquals(zeroIndexerStats, stats.getTransformStats()); // start the transform StartDataFrameTransformResponse startTransformResponse = execute(new StartDataFrameTransformRequest(id), client::startDataFrameTransform, client::startDataFrameTransformAsync); assertThat(startTransformResponse.isAcknowledged(), is(true)); assertBusy(() -> { GetDataFrameTransformStatsResponse response = execute(new GetDataFrameTransformStatsRequest(id), client::getDataFrameTransformStats, client::getDataFrameTransformStatsAsync); DataFrameTransformStateAndStats stateAndStats = response.getTransformsStateAndStats().get(0); assertNotEquals(zeroIndexerStats, stateAndStats.getTransformStats()); assertNotNull(stateAndStats.getTransformState().getProgress()); assertThat(stateAndStats.getTransformState().getProgress().getPercentComplete(), equalTo(100.0)); assertThat(stateAndStats.getTransformState().getProgress().getTotalDocs(), greaterThan(0L)); assertThat(stateAndStats.getTransformState().getProgress().getRemainingDocs(), equalTo(0L)); assertThat(stateAndStats.getTransformState().getReason(), is(nullValue())); }); }	maybe use isoneof(started, stopped)
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); clusterName = ClusterName.readClusterName(in); clusterState = ClusterState.Builder.readFrom(in, null); }	maybe it's more intuitive to have a readfrom(streaminput input) that delegates than passing null here?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeString(cause); out.writeString(name); out.writeString(template); out.writeInt(order); out.writeBoolean(create); writeSettingsToStream(settings, out); out.writeVInt(mappings.size()); for (Map.Entry<String, String> entry : mappings.entrySet()) { out.writeString(entry.getKey()); out.writeString(entry.getValue()); } out.writeVInt(customs.size()); for (Map.Entry<String, IndexClusterStatePart> entry : customs.entrySet()) { IndexClusterStatePart.Factory<IndexClusterStatePart> factory = IndexMetaData.FACTORY.lookupFactorySafe(entry.getKey()); if(factory.addedIn().onOrAfter(out.getVersion())) { out.writeString(entry.getKey()); factory.writeTo(entry.getValue(), out); } } out.writeVInt(aliases.size()); for (Alias alias : aliases) { alias.writeTo(out); } }	i have seen this loop twice maybe we can have a utility method for this?
static TaskInfo randomTaskInfo() { return randomTaskInfo(randomLong()); }	i think randomnonnegativelong() is reasonable here as well.
public final void testConcurrentToXContent() throws IOException, InterruptedException, ExecutionException { XContentType xContentType = randomFrom(XContentType.values()); T testInstance = createXContextTestInstance(xContentType); ToXContent.Params params = new ToXContent.MapParams(singletonMap(RestSearchAction.TYPED_KEYS_PARAM, "true")); boolean humanReadable = randomBoolean(); BytesRef firstTimeBytes = toXContent(testInstance, xContentType, params, humanReadable).toBytesRef(); concurrentTest(() -> { try { for (int r = 0; r < 500; r++) { assertEquals(firstTimeBytes, toXContent(testInstance, xContentType, params, humanReadable).toBytesRef()); } } catch (IOException e) { throw new AssertionError(e); } }); } /** * Parses to a new instance using the provided {@link XContentParser}	i would randomize these and in concurrent tests as well, so we vary it.
public static void validateTemplateForDataStream(String dataStreamName, Metadata metadata) { final String v2Template = MetadataIndexTemplateService.findV2Template(metadata, dataStreamName, false); if (v2Template == null) { throw new IllegalArgumentException("no matching index template found for data stream [" + dataStreamName + "]"); } IndexTemplateV2 indexTemplateV2 = metadata.templatesV2().get(v2Template); if (indexTemplateV2.getDataStreamTemplate() == null) { throw new IllegalArgumentException("matching index template [" + v2Template + "] for data stream [" + dataStreamName + "] has no data stream template"); } }	should we enforce that the timestamp field on the create data stream request is the same as the timestamp field in the data stream template? this can be different if a user provided a different timestamp field via the create data stream api. if we do enforce this then i think that we should remove the timestamp field from the create data stream request and always use the timestamp field provided in the data stream template inside a itv2.
private void executeSearch(SearchTask task, SearchTimeProvider timeProvider, SearchRequest searchRequest, OriginalIndices localIndices, Map<String, OriginalIndices> remoteClusterIndices, List<SearchShardIterator> remoteShardIterators, BiFunction<String, String, DiscoveryNode> remoteConnections, ClusterState clusterState, Map<String, AliasFilter> remoteAliasMap, ActionListener<SearchResponse> listener, int nodeCount, SearchResponse.Clusters clusters) { clusterState.blocks().globalBlockedRaiseException(ClusterBlockLevel.READ); // TODO: I think startTime() should become part of ActionRequest and that should be used both for index name // date math expressions and $now in scripts. This way all apis will deal with now in the same way instead // of just for the _search api final Index[] indices; if (localIndices.indices().length == 0 && remoteClusterIndices.isEmpty() == false) { indices = Index.EMPTY_ARRAY; // don't search on _all if only remote indices were specified } else { indices = indexNameExpressionResolver.concreteIndices(clusterState, searchRequest.indicesOptions(), timeProvider.getAbsoluteStartMillis(), localIndices.indices()); } Map<String, AliasFilter> aliasFilter = buildPerIndexAliasFilter(searchRequest, clusterState, indices, remoteAliasMap); Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, searchRequest.routing(), searchRequest.indices()); routingMap = routingMap == null ? Collections.emptyMap() : Collections.unmodifiableMap(routingMap); String[] concreteIndices = new String[indices.length]; for (int i = 0; i < indices.length; i++) { concreteIndices[i] = indices[i].getName(); } Map<String, Long> nodeSearchCounts = searchTransportService.getPendingSearchRequests(); GroupShardsIterator<ShardIterator> localShardsIterator = clusterService.operationRouting().searchShards(clusterState, concreteIndices, routingMap, searchRequest.preference(), searchService.getResponseCollectorService(), nodeSearchCounts); GroupShardsIterator<SearchShardIterator> shardIterators = mergeShardsIterators(localShardsIterator, localIndices, remoteShardIterators); failIfOverShardCountLimit(clusterService, shardIterators.size()); Map<String, Float> concreteIndexBoosts = resolveIndexBoosts(searchRequest, clusterState); // optimize search type for cases where there is only one shard group to search on if (shardIterators.size() == 1) { // if we only have one group, then we always want Q_T_F, no need for DFS, and no need to do THEN since we hit one shard searchRequest.searchType(QUERY_THEN_FETCH); } if (searchRequest.allowPartialSearchResults() == null) { // No user preference defined in search request - apply cluster service default searchRequest.allowPartialSearchResults(searchService.defaultAllowPartialSearchResults()); } if (searchRequest.isSuggestOnly()) { // disable request cache if we have only suggest searchRequest.requestCache(false); switch (searchRequest.searchType()) { case DFS_QUERY_THEN_FETCH: // convert to Q_T_F if we have only suggest searchRequest.searchType(QUERY_THEN_FETCH); break; } } final DiscoveryNodes nodes = clusterState.nodes(); BiFunction<String, String, Transport.Connection> connectionLookup = (clusterName, nodeId) -> { final DiscoveryNode discoveryNode = clusterName == null ? nodes.get(nodeId) : remoteConnections.apply(clusterName, nodeId); if (discoveryNode == null) { throw new IllegalStateException("no node found for id: " + nodeId); } return searchTransportService.getConnection(clusterName, discoveryNode); }; if (searchRequest.isMaxConcurrentShardRequestsSet() == false) { /* * We try to set a default of max concurrent shard requests based on the node count but upper-bound it by 256 by default to keep * it sane. A single search request that fans out to lots of shards should not hit a cluster too hard while 256 is already a * lot. */ searchRequest.setMaxConcurrentShardRequests(Math.min(256, nodeCount)); } boolean preFilterSearchShards = shouldPreFilterSearchShards(searchRequest, shardIterators); searchAsyncAction(task, searchRequest, shardIterators, timeProvider, connectionLookup, clusterState.version(), Collections.unmodifiableMap(aliasFilter), concreteIndexBoosts, routingMap, listener, preFilterSearchShards, clusters).start(); }	+1, we could multiply nodecount with a small constant to favor latency over throughput but the reasoning for the default value is to make sure that we hit all shards in a default index even when there is a single node so this change is consistent with the new default number of shards.
public void testUnhealthyNodesGetsRemoved() { AtomicReference<StatusInfo> healthStatusInfo = new AtomicReference<>( new StatusInfo(HEALTHY, "healthy-info")); try (Cluster cluster = new Cluster(3)) { cluster.runRandomly(); cluster.stabilise(); final ClusterNode leader = cluster.getAnyLeader(); logger.info("--> adding two new healthy nodes"); ClusterNode newNode1 = cluster.new ClusterNode(nextNodeIndex.getAndIncrement(), true, leader.nodeSettings, () -> healthStatusInfo.get()); ClusterNode newNode2 = cluster.new ClusterNode(nextNodeIndex.getAndIncrement(), true, leader.nodeSettings, () -> healthStatusInfo.get()); cluster.clusterNodes.add(newNode1); cluster.clusterNodes.add(newNode2); cluster.stabilise(); { assertThat(leader.coordinator.getMode(), is(Mode.LEADER)); final VotingConfiguration lastCommittedConfiguration = leader.getLastAppliedClusterState().getLastCommittedConfiguration(); assertThat(lastCommittedConfiguration + " should be all nodes", lastCommittedConfiguration.getNodeIds(), equalTo(cluster.clusterNodes.stream().map(ClusterNode::getId).collect(Collectors.toSet()))); } logger.info("setting auto-shrink reconfiguration to true"); leader.submitSetAutoShrinkVotingConfiguration(true); cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY); assertTrue(CLUSTER_AUTO_SHRINK_VOTING_CONFIGURATION.get(leader.getLastAppliedClusterState().metadata().settings())); logger.info("--> changing health of newly added nodes to unhealthy"); healthStatusInfo.getAndSet(new StatusInfo(UNHEALTHY, "unhealthy-info")); cluster.stabilise(); { final ClusterNode newLeader = cluster.getAnyLeader(); final VotingConfiguration lastCommittedConfiguration = newLeader.getLastAppliedClusterState().getLastCommittedConfiguration(); assertThat(lastCommittedConfiguration + " should be 3 nodes", lastCommittedConfiguration.getNodeIds().size(), equalTo(3)); assertFalse(lastCommittedConfiguration.getNodeIds().contains(newNode1.getId())); assertFalse(lastCommittedConfiguration.getNodeIds().contains(newNode2.getId())); } } }	is there a shorter time-bound for stabilisation here?
final SearchContext createContext(ShardSearchRequest request) throws IOException { final DefaultSearchContext context = createSearchContext(request, defaultSearchTimeout); if (activeContexts.size() >= Node.MAX_SEARCH_CONTEXT_SETTING.get(settings)) { throw new IllegalStateException( "Trying to create too many search contexts. Must be less than or equal to: [" + Node.MAX_SEARCH_CONTEXT_SETTING.get(settings) + "]. This limit can be set by changing the [" + Node.MAX_SEARCH_CONTEXT_SETTING.getKey() + "] node level setting."); } try { if (request.scroll() != null) { context.scrollContext(new ScrollContext()); context.scrollContext().scroll = request.scroll(); } parseSource(context, request.source()); // if the from and size are still not set, default them if (context.from() == -1) { context.from(0); } if (context.size() == -1) { context.size(10); } // pre process dfsPhase.preProcess(context); queryPhase.preProcess(context); fetchPhase.preProcess(context); // compute the context keep alive long keepAlive = defaultKeepAlive; if (request.scroll() != null && request.scroll().keepAlive() != null) { keepAlive = request.scroll().keepAlive().millis(); } contextScrollKeepAlive(context, keepAlive); context.lowLevelCancellation(lowLevelCancellation); } catch (Exception e) { context.close(); throw ExceptionsHelper.convertToRuntime(e); } return context; }	should we rather add this check at the beginning of createandputcontext? this method only creates a new context, it doesn't add it to the list of active contexts?
final SearchContext createContext(ShardSearchRequest request) throws IOException { final DefaultSearchContext context = createSearchContext(request, defaultSearchTimeout); if (activeContexts.size() >= Node.MAX_SEARCH_CONTEXT_SETTING.get(settings)) { throw new IllegalStateException( "Trying to create too many search contexts. Must be less than or equal to: [" + Node.MAX_SEARCH_CONTEXT_SETTING.get(settings) + "]. This limit can be set by changing the [" + Node.MAX_SEARCH_CONTEXT_SETTING.getKey() + "] node level setting."); } try { if (request.scroll() != null) { context.scrollContext(new ScrollContext()); context.scrollContext().scroll = request.scroll(); } parseSource(context, request.source()); // if the from and size are still not set, default them if (context.from() == -1) { context.from(0); } if (context.size() == -1) { context.size(10); } // pre process dfsPhase.preProcess(context); queryPhase.preProcess(context); fetchPhase.preProcess(context); // compute the context keep alive long keepAlive = defaultKeepAlive; if (request.scroll() != null && request.scroll().keepAlive() != null) { keepAlive = request.scroll().keepAlive().millis(); } contextScrollKeepAlive(context, keepAlive); context.lowLevelCancellation(lowLevelCancellation); } catch (Exception e) { context.close(); throw ExceptionsHelper.convertToRuntime(e); } return context; }	i think elasticsearchexception would allow to return a http status code that indicates that the request should be retried later, this would be better than a 500/internal error?
public void testMaxSearchContexts() throws IOException { createIndex("index"); final SearchService service = getInstanceFromNode(SearchService.class); final IndicesService indicesService = getInstanceFromNode(IndicesService.class); final IndexService indexService = indicesService.indexServiceSafe(resolveIndex("index")); final IndexShard indexShard = indexService.getShard(0); for (int i = 0; i < Node.MAX_SEARCH_CONTEXT_SETTING.get(Settings.EMPTY); i++) { SearchContext context = service.createAndPutContext( new ShardSearchLocalRequest( indexShard.shardId(), 1, SearchType.DEFAULT, new SearchSourceBuilder(), new String[0], false, new AliasFilter(null, Strings.EMPTY_ARRAY), 1.0f, true) ); } try (SearchContext context = service.createAndPutContext(new ShardSearchLocalRequest(indexShard.shardId(), 1, SearchType.DEFAULT, new SearchSourceBuilder(), new String[0], false, new AliasFilter(null, Strings.EMPTY_ARRAY), 1.0f, true))) { assertNotNull(context); } catch (IllegalStateException ex) { assertEquals( "Trying to create too many search contexts. Must be less than or equal to: [100]. " + "This limit can be set by changing the [node.max_search_context] node level setting.", ex.getMessage()); } }	can you use expectthrows to check for the exception/message?
@Override protected void restoreFiles(List<FileInfo> filesToRecover, Store store) throws IOException { logger.trace("[{}] starting CCR restore of {} files", shardId, filesToRecover); try (MultiFileWriter multiFileWriter = new MultiFileWriter(store, recoveryState.getIndex(), "", logger, () -> {})) { final LocalCheckpointTracker requestSeqIdTracker = new LocalCheckpointTracker(NO_OPS_PERFORMED, NO_OPS_PERFORMED); final AtomicReference<Tuple<StoreFileMetaData, Exception>> error = new AtomicReference<>(); outer: for (FileInfo fileInfo : filesToRecover) { if (error.get() != null) { break outer; } final long fileLength = fileInfo.length(); long offset = 0; while (offset < fileLength) { if (error.get() != null) { break outer; } final long requestSeqId = requestSeqIdTracker.generateSeqNo(); try { requestSeqIdTracker.waitForOpsToComplete(requestSeqId - ccrSettings.getMaxConcurrentFileChunks()); final int bytesRequested = Math.toIntExact( Math.min(ccrSettings.getChunkSize().getBytes(), fileLength - offset)); offset += bytesRequested; final GetCcrRestoreFileChunkRequest request = new GetCcrRestoreFileChunkRequest(node, sessionUUID, fileInfo.name(), bytesRequested); logger.trace("[{}] [{}] fetching chunk for file [{}], expected offset: {}, size: {}", shardId, snapshotId, fileInfo.name(), offset, bytesRequested); remoteClient.execute(GetCcrRestoreFileChunkAction.INSTANCE, request, ActionListener.wrap( r -> threadPool.generic().execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { error.compareAndSet(null, Tuple.tuple(fileInfo.metadata(), e)); requestSeqIdTracker.markSeqNoAsCompleted(requestSeqId); } @Override protected void doRun() throws Exception { final int actualChunkSize = r.getChunk().length(); logger.trace("[{}] [{}] got response for file [{}], offset: {}, length: {}", shardId, snapshotId, fileInfo.name(), r.getOffset(), actualChunkSize); final long nanosPaused = ccrSettings.getRateLimiter().maybePause(actualChunkSize); throttleListener.accept(nanosPaused); final boolean lastChunk = r.getOffset() + actualChunkSize >= fileLength; multiFileWriter.writeFileChunk(fileInfo.metadata(), r.getOffset(), r.getChunk(), lastChunk); requestSeqIdTracker.markSeqNoAsCompleted(requestSeqId); } }), e -> { error.compareAndSet(null, Tuple.tuple(fileInfo.metadata(), e)); requestSeqIdTracker.markSeqNoAsCompleted(requestSeqId); } )); } catch (Exception e) { error.compareAndSet(null, Tuple.tuple(fileInfo.metadata(), e)); requestSeqIdTracker.markSeqNoAsCompleted(requestSeqId); break outer; } } } try { requestSeqIdTracker.waitForOpsToComplete(requestSeqIdTracker.getMaxSeqNo()); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new ElasticsearchException(e); } if (error.get() != null) { handleError(store, error.get().v2()); } } logger.trace("[{}] completed CCR restore", shardId); }	i think we can remove this check here since the check inside the while loop is good enough.
@Before public void setupTest() throws IOException { installation = runContainer( distribution(), builder().envVars(Map.of("ingest.geoip.downloader.enabled", "false", "ELASTIC_PASSWORD", PASSWORD)) ); tempDir = createTempDir(DockerTests.class.getSimpleName()); }	i would suggest we randomly disable security (set xpack.security.enabled to false). i think these tests are the only place where we can reliably assert the security "status" of a distribution, although until now these tests were security agnostic.
public <T extends TransportResponse> void sendRequest(Transport.Connection connection, String action, TransportRequest request, TransportRequestOptions options, TransportResponseHandler<T> handler) { final boolean requireAuth = XPackSettings.SECURITY_ENABLED.get(settings); // the transport in core normally does this check, BUT since we are serializing to a string header we need to do it // ourselves otherwise we wind up using a version newer than what we can actually send final Version minVersion = Version.min(connection.getVersion(), Version.CURRENT); // Sometimes a system action gets executed like a internal create index request or update mappings request // which means that the user is copied over to system actions so we need to change the user if (AuthorizationUtils.shouldReplaceUserWithSystem(threadPool.getThreadContext(), action)) { securityContext.executeAsUser(SystemUser.INSTANCE, (original) -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original) , handler), sender, requireAuth), minVersion); } else if (AuthorizationUtils.shouldSetUserBasedOnActionOrigin(threadPool.getThreadContext())) { AuthorizationUtils.switchUserBasedOnActionOriginAndExecute(threadPool.getThreadContext(), securityContext, (original) -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original) , handler), sender, requireAuth)); } else if (securityContext.getAuthentication() != null && securityContext.getAuthentication().getVersion().equals(minVersion) == false) { // re-write the authentication since we want the authentication version to match the version of the connection securityContext.executeAfterRewritingAuthentication(original -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original), handler), sender, requireAuth), minVersion); } else { sendWithUser(connection, action, request, options, handler, sender, requireAuth); } }	i don't think you need to add any code in this class (only remove).
protected Settings nodeSettings() { return Settings.builder() .put(NodeRoleSettings.NODE_ROLES_SETTING.getKey(), "master, data, ingest") .put(XPackSettings.SECURITY_ENABLED.getKey(), false) // TODO Change this to run with security enabled .build(); }	since you already did the work to identify the tests that maybe need to work with security, do you mind opening a meta issue with checkboxes with all of these (and referring the issue in comments)? i think this would be very helpful in tracking the work :)
protected ChannelHandler getServerChannelInitializer(String name, Settings settings) { return new ServerChannelInitializer(name, settings); }	is there a reason not to inline this method and the one below?
private void onNoLongerPrimary(Exception failure) { final boolean nodeIsClosing = failure instanceof NodeClosedException || (failure instanceof SendRequestTransportException && ShardStateAction.SHARD_FAILED_ACTION_NAME.equals(((SendRequestTransportException) failure).action()) && failure.getCause() instanceof TransportException && "TransportService is closed stopped can't send request".equals(failure.getCause().getMessage())); final String message; if (nodeIsClosing) { message = String.format(Locale.ROOT, "node with primary [%s] is shutting down while failing replica shard", primary.routingEntry()); // We prefer not to fail the primary to avoid unnecessary warning log // when the node with the primary shard is gracefully shutting down. } else { if (Assertions.ENABLED) { if (failure instanceof ShardStateAction.NoLongerPrimaryShardException == false) { throw new AssertionError("unexpected failure", failure); } } // we are no longer the primary, fail ourselves and start over message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard", primary.routingEntry()); primary.failShard(message, failure); } finishAsFailed(new RetryOnPrimaryException(primary.routingEntry().shardId(), message, failure)); }	perhaps use exceptionshelper.unwrapcause? sendrequesttransportexception is implementing elasticsearchwrapperexception.
private void exchangeCodeForToken(AuthorizationCode code, ActionListener<Tuple<AccessToken, JWT>> tokensListener) { try { final AuthorizationCodeGrant codeGrant = new AuthorizationCodeGrant(code, rpConfig.getRedirectUri()); final HttpPost httpPost = new HttpPost(opConfig.getTokenEndpoint()); httpPost.setHeader("Content-type", "application/x-www-form-urlencoded"); final List<NameValuePair> params = new ArrayList<>(); for (Map.Entry<String, List<String>> entry : codeGrant.toParameters().entrySet()) { // All parameters of AuthorizationCodeGrant are singleton lists params.add(new BasicNameValuePair(entry.getKey(), entry.getValue().get(0))); } if (rpConfig.getClientAuthenticationMethod().equals(ClientAuthenticationMethod.CLIENT_SECRET_BASIC)) { UsernamePasswordCredentials creds = new UsernamePasswordCredentials(URLEncoder.encode(rpConfig.getClientId().getValue(), StandardCharsets.UTF_8), URLEncoder.encode(rpConfig.getClientSecret().toString(), StandardCharsets.UTF_8)); httpPost.addHeader(new BasicScheme().authenticate(creds, httpPost, null)); } else if (rpConfig.getClientAuthenticationMethod().equals(ClientAuthenticationMethod.CLIENT_SECRET_POST)) { params.add(new BasicNameValuePair("client_id", rpConfig.getClientId().getValue())); params.add(new BasicNameValuePair("client_secret", rpConfig.getClientSecret().toString())); } else if (rpConfig.getClientAuthenticationMethod().equals(ClientAuthenticationMethod.CLIENT_SECRET_JWT)) { ClientSecretJWT clientSecretJWT = new ClientSecretJWT(rpConfig.getClientId(), opConfig.getTokenEndpoint(), rpConfig.getClientAuthenticationJwtAlgorithm(), new Secret(rpConfig.getClientSecret().toString())); for (Map.Entry<String, List<String>> entry : clientSecretJWT.toParameters().entrySet()) { // Both client_assertion and client_assertion_type are singleton lists params.add(new BasicNameValuePair(entry.getKey(), entry.getValue().get(0))); } } else { tokensListener.onFailure(new ElasticsearchSecurityException("Failed to exchange code for Id Token using Token Endpoint." + "Expected client authentication method to be one of " + OpenIdConnectRealmSettings.CLIENT_AUTH_METHODS + " but was " + rpConfig.getClientAuthenticationMethod())); } httpPost.setEntity(new UrlEncodedFormEntity(params)); SpecialPermission.check(); AccessController.doPrivileged((PrivilegedAction<Void>) () -> { httpClient.execute(httpPost, new FutureCallback<HttpResponse>() { @Override public void completed(HttpResponse result) { handleTokenResponse(result, tokensListener); } @Override public void failed(Exception ex) { tokensListener.onFailure( new ElasticsearchSecurityException("Failed to exchange code for Id Token using the Token Endpoint.", ex)); } @Override public void cancelled() { final String message = "Failed to exchange code for Id Token using the Token Endpoint. Request was cancelled"; tokensListener.onFailure(new ElasticsearchSecurityException(message)); } }); return null; }); } catch (AuthenticationException | UnsupportedEncodingException | JOSEException e) { tokensListener.onFailure( new ElasticsearchSecurityException("Failed to exchange code for Id Token using the Token Endpoint.", e)); } }	maybe i am missing something obvious. but client_id is not added as one of the request parameters? the map returned from clientsecretjwt.toparameters() only contains client_assertion and client_assertion_type if i read the code correctly.
private void exchangeCodeForToken(AuthorizationCode code, ActionListener<Tuple<AccessToken, JWT>> tokensListener) { try { final AuthorizationCodeGrant codeGrant = new AuthorizationCodeGrant(code, rpConfig.getRedirectUri()); final HttpPost httpPost = new HttpPost(opConfig.getTokenEndpoint()); httpPost.setHeader("Content-type", "application/x-www-form-urlencoded"); final List<NameValuePair> params = new ArrayList<>(); for (Map.Entry<String, List<String>> entry : codeGrant.toParameters().entrySet()) { // All parameters of AuthorizationCodeGrant are singleton lists params.add(new BasicNameValuePair(entry.getKey(), entry.getValue().get(0))); } if (rpConfig.getClientAuthenticationMethod().equals(ClientAuthenticationMethod.CLIENT_SECRET_BASIC)) { UsernamePasswordCredentials creds = new UsernamePasswordCredentials(URLEncoder.encode(rpConfig.getClientId().getValue(), StandardCharsets.UTF_8), URLEncoder.encode(rpConfig.getClientSecret().toString(), StandardCharsets.UTF_8)); httpPost.addHeader(new BasicScheme().authenticate(creds, httpPost, null)); } else if (rpConfig.getClientAuthenticationMethod().equals(ClientAuthenticationMethod.CLIENT_SECRET_POST)) { params.add(new BasicNameValuePair("client_id", rpConfig.getClientId().getValue())); params.add(new BasicNameValuePair("client_secret", rpConfig.getClientSecret().toString())); } else if (rpConfig.getClientAuthenticationMethod().equals(ClientAuthenticationMethod.CLIENT_SECRET_JWT)) { ClientSecretJWT clientSecretJWT = new ClientSecretJWT(rpConfig.getClientId(), opConfig.getTokenEndpoint(), rpConfig.getClientAuthenticationJwtAlgorithm(), new Secret(rpConfig.getClientSecret().toString())); for (Map.Entry<String, List<String>> entry : clientSecretJWT.toParameters().entrySet()) { // Both client_assertion and client_assertion_type are singleton lists params.add(new BasicNameValuePair(entry.getKey(), entry.getValue().get(0))); } } else { tokensListener.onFailure(new ElasticsearchSecurityException("Failed to exchange code for Id Token using Token Endpoint." + "Expected client authentication method to be one of " + OpenIdConnectRealmSettings.CLIENT_AUTH_METHODS + " but was " + rpConfig.getClientAuthenticationMethod())); } httpPost.setEntity(new UrlEncodedFormEntity(params)); SpecialPermission.check(); AccessController.doPrivileged((PrivilegedAction<Void>) () -> { httpClient.execute(httpPost, new FutureCallback<HttpResponse>() { @Override public void completed(HttpResponse result) { handleTokenResponse(result, tokensListener); } @Override public void failed(Exception ex) { tokensListener.onFailure( new ElasticsearchSecurityException("Failed to exchange code for Id Token using the Token Endpoint.", ex)); } @Override public void cancelled() { final String message = "Failed to exchange code for Id Token using the Token Endpoint. Request was cancelled"; tokensListener.onFailure(new ElasticsearchSecurityException(message)); } }); return null; }); } catch (AuthenticationException | UnsupportedEncodingException | JOSEException e) { tokensListener.onFailure( new ElasticsearchSecurityException("Failed to exchange code for Id Token using the Token Endpoint.", e)); } }	nit: the name or value is usually placed inside a pair of brackets in error messages, .e.g ... but was [xxx].
public void testInvalidClientAuthenticationMethodThrowsError() { final Settings.Builder settingsBuilder = Settings.builder() .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.OP_AUTHORIZATION_ENDPOINT), "https://op.example.com/login") .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.OP_ISSUER), "https://op.example.com") .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.OP_JWKSET_PATH), "https://op.example.com/jwks.json") .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.OP_TOKEN_ENDPOINT), "https://op.example.com/token") .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.PRINCIPAL_CLAIM.getClaim()), "sub") .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.RP_REDIRECT_URI), "https://rp.my.com") .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.RP_CLIENT_ID), "rp-my") .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.RP_CLIENT_AUTH_METHOD), "none") .put(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.RP_RESPONSE_TYPE), "code"); settingsBuilder.setSecureSettings(getSecureSettings()); IllegalArgumentException exception = expectThrows(IllegalArgumentException.class, () -> { new OpenIdConnectRealm(buildConfig(settingsBuilder.build()), null, null); }); assertThat(exception.getMessage(), Matchers.containsString(getFullSettingKey(REALM_NAME, OpenIdConnectRealmSettings.RP_CLIENT_AUTH_METHOD))); }	"none", "none", "none", "none", "none" ... that we will not have this issue.
private TemporalAccessor doParse(String input) { if (parsers.size() > 1) { for (DateTimeFormatter formatter : parsers) { ParsePosition pos = new ParsePosition(0); Object object = formatter.toFormat().parseObject(input, pos); if (parssingSucceeded(object, input, pos) == true) { return (TemporalAccessor) object; } } throw new DateTimeParseException("Failed to parse with all parsers from a composite parser", input, 0); } return firstParser().parse(input); }	it's implicit that this is a composite parser, as the format will be returned in the iae?
private TemporalAccessor doParse(String input) { if (parsers.size() > 1) { for (DateTimeFormatter formatter : parsers) { ParsePosition pos = new ParsePosition(0); Object object = formatter.toFormat().parseObject(input, pos); if (parssingSucceeded(object, input, pos) == true) { return (TemporalAccessor) object; } } throw new DateTimeParseException("Failed to parse with all parsers from a composite parser", input, 0); } return firstParser().parse(input); }	useless nit: one s less?
public void testGroupByHavingNonGrouped() { assertEquals("1:48: Cannot filter HAVING on non-aggregate [int]; consider using WHERE instead", error("SELECT AVG(int) FROM test GROUP BY text HAVING int > 10")); }	why are we losing here the text proposal? can we still have it?
public <T> void declareFieldArray(BiConsumer<Value, List<T>> consumer, ContextParser<Context, T> itemParser, ParseField field, ValueType type) { declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type); } /** * Declares a set of fields that are required for parsing to succeed. Only one of the values * provided per String[] must be matched. * * E.g. <code>declareRequiredFieldSet("foo", "bar");</code> means at least one of "foo" or * "bar" fields must be present. If neither of those fields are present, an exception will be thrown. * * Multiple required sets can be configured: * * <pre><code> * parser.declareRequiredFieldSet("foo", "bar"); * parser.declareRequiredFieldSet("bizz", "buzz"); * </code></pre> * * requires that one of "foo" or "bar" fields are present, and also that one of "bizz" or * "buzz" fields are present. * * In JSON, it means any of these combinations are acceptable: * * <ul> * <li><code>{"foo":"...", "bizz": "..."}</code></li> * <li><code>{"bar":"...", "bizz": "..."}</code></li> * <li><code>{"foo":"...", "buzz": "..."}</code></li> * <li><code>{"bar":"...", "buzz": "..."}</code></li> * <li><code>{"foo":"...", "bar":"...", "bizz": "..."}</code></li> * <li><code>{"foo":"...", "bar":"...", "buzz": "..."}</code></li> * <li><code>{"foo":"...", "bizz":"...", "buzz": "..."}</code></li> * <li><code>{"bar":"...", "bizz":"...", "buzz": "..."}</code></li> * <li><code>{"foo":"...", "bar":"...", "bizz": "...", "buzz": "..."}</code></li> * </ul> * * The following would however be rejected: * * <table summary="failure cases"> * <tr><th>Provided JSON</th><th>Reason for failure</th></tr> * <tr><td><code>{"foo":"..."}</code></td><td>Missing "bizz" or "buzz" field</td></tr> * <tr><td><code>{"bar":"..."}</code></td><td>Missing "bizz" or "buzz" field</td></tr> * <tr><td><code>{"bizz": "..."}</code></td><td>Missing "foo" or "bar" field</td></tr> * <tr><td><code>{"buzz": "..."}</code></td><td>Missing "foo" or "bar" field</td></tr> * <tr><td><code>{"foo":"...", "bar": "..."}</code></td><td>Missing "bizz" or "buzz" field</td></tr> * <tr><td><code>{"bizz":"...", "buzz": "..."}</code></td><td>Missing "foo" or "bar" field</td></tr> * <tr><td><code>{"unrelated":"..."}	typo: requriedset -> requiredset
public <T> void declareFieldArray(BiConsumer<Value, List<T>> consumer, ContextParser<Context, T> itemParser, ParseField field, ValueType type) { declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type); } /** * Declares a set of fields that are required for parsing to succeed. Only one of the values * provided per String[] must be matched. * * E.g. <code>declareRequiredFieldSet("foo", "bar");</code> means at least one of "foo" or * "bar" fields must be present. If neither of those fields are present, an exception will be thrown. * * Multiple required sets can be configured: * * <pre><code> * parser.declareRequiredFieldSet("foo", "bar"); * parser.declareRequiredFieldSet("bizz", "buzz"); * </code></pre> * * requires that one of "foo" or "bar" fields are present, and also that one of "bizz" or * "buzz" fields are present. * * In JSON, it means any of these combinations are acceptable: * * <ul> * <li><code>{"foo":"...", "bizz": "..."}</code></li> * <li><code>{"bar":"...", "bizz": "..."}</code></li> * <li><code>{"foo":"...", "buzz": "..."}</code></li> * <li><code>{"bar":"...", "buzz": "..."}</code></li> * <li><code>{"foo":"...", "bar":"...", "bizz": "..."}</code></li> * <li><code>{"foo":"...", "bar":"...", "buzz": "..."}</code></li> * <li><code>{"foo":"...", "bizz":"...", "buzz": "..."}</code></li> * <li><code>{"bar":"...", "bizz":"...", "buzz": "..."}</code></li> * <li><code>{"foo":"...", "bar":"...", "bizz": "...", "buzz": "..."}</code></li> * </ul> * * The following would however be rejected: * * <table summary="failure cases"> * <tr><th>Provided JSON</th><th>Reason for failure</th></tr> * <tr><td><code>{"foo":"..."}</code></td><td>Missing "bizz" or "buzz" field</td></tr> * <tr><td><code>{"bar":"..."}</code></td><td>Missing "bizz" or "buzz" field</td></tr> * <tr><td><code>{"bizz": "..."}</code></td><td>Missing "foo" or "bar" field</td></tr> * <tr><td><code>{"buzz": "..."}</code></td><td>Missing "foo" or "bar" field</td></tr> * <tr><td><code>{"foo":"...", "bar": "..."}</code></td><td>Missing "bizz" or "buzz" field</td></tr> * <tr><td><code>{"bizz":"...", "buzz": "..."}</code></td><td>Missing "foo" or "bar" field</td></tr> * <tr><td><code>{"unrelated":"..."}	typo in the arg name too
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; if (super.equals(o) == false) return false; GeoContextMapping that = (GeoContextMapping) o; if (precision != that.precision) return false; return !(fieldName != null ? !fieldName.equals(that.fieldName) : that.fieldName != null); }	do we have plans to enforce the idea templates for generating these equals methods?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeBoolean(value); } } public void testWriteMapWithConsistentOrder() throws IOException { Map<String, Object> map = new HashMap<>(); map.put("gWrgS", randomAsciiOfLength(5)); map.put("HLRYi", randomAsciiOfLength(5)); map.put("HyKnF", randomAsciiOfLength(5)); Map<String, Object> reOrderMap = new HashMap<>(map); List<String> mapKeys = map.entrySet().stream().map(Map.Entry::getKey).collect(Collectors.toList()); List<String> reOrderMapKeys = reOrderMap.entrySet().stream().map(Map.Entry::getKey).collect(Collectors.toList()); assertNotEquals(mapKeys, reOrderMapKeys); BytesStreamOutput output = new BytesStreamOutput(); BytesStreamOutput reOrderMapOutput = new BytesStreamOutput(); output.writeMapWithConsistentOrder(map); reOrderMapOutput.writeMapWithConsistentOrder(reOrderMap); assertEquals(output.bytes(), reOrderMapOutput.bytes()); } public void testReadMapByUsingWriteMapWithConsistentOrder() throws IOException { Map<String, String> streamOutMap = randomHashMap(randomIntBetween(2, 20), () -> randomAsciiOfLength(5), () -> randomAsciiOfLength(5)); BytesStreamOutput streamOut = new BytesStreamOutput(); streamOut.writeMapWithConsistentOrder(streamOutMap); StreamInput in = StreamInput.wrap(BytesReference.toBytes(streamOut.bytes())); Map<String, Object> streamInMap = in.readMap(); assertEquals(streamOutMap, streamInMap); } public void testWriteMapWithConsistentOrderWithLinkedHashMapShouldThrowAssertError() throws IOException { BytesStreamOutput output1 = new BytesStreamOutput(); Map<String, Object> map1 = new LinkedHashMap<>(); try { output1.writeMapWithConsistentOrder(map1); fail("should throw assert error when write consistent order map with LinkedHashMap"); } catch (Error e) { assertTrue(e instanceof AssertionError); } }	why hardcode these test keys? for the hashmap default **capacity** is 16, and **threshold** is 12(default_load_factor(0.75) * default_initial_capacity(16)). for these keys, their slots should be: | key | slot | | --- | --- | | gwrgs | 10 | | hlryi | 4 | | hyknf | 12 | and for map<string, object> reordermap = new hashmap<>(map); it will recalculate capacity size and threshold, capacity: 8, threshold: 6, and the generated slots should be: | key | slot | | --- | --- | | gwrgs | 2 | | hlryi | 4 | | hyknf | 4 (collision!!! ) | when met **collision**, it will append this key into the existed node. for these test keys, when met collision,this two maps order will not equal. but these two map generated **bytes** still should be equal.
void processBufferedChildBuckets() throws IOException { if (bucketBuffer.isEmpty()) { return; } final int prevParentDoc = parentDocs.prevSetBit(currentParentDoc - 1); int childDocId = childDocs.docID(); if (childDocId <= prevParentDoc) { childDocId = childDocs.advance(prevParentDoc + 1); } for (; childDocId < currentParentDoc; childDocId = childDocs.nextDoc()) { cachedScorer.doc = childDocId; final int docId = childDocId; bucketBuffer.stream().mapToLong(Long::longValue).forEach(bucket -> { try { collectBucket(sub, docId, bucket); } catch (IOException e) { throw new UncheckedIOException(e); } }); } bucketBuffer.clear(); } } private static class CachedScorable extends Scorable { int doc; float score; @Override public final float score() { return score; } @Override public int docID() { return doc; }	maybe use a regular for loop here? i notice that we have been avoiding to use steam() in hot code paths. i think we should do this here too? but maybe i'm wrong here.
public final void postProcess(Function<String, MappedFieldType> fieldTypeLookup) { for (Map.Entry<String, List<Object>> entry : fields().entrySet()) { MappedFieldType fieldType = fieldTypeLookup.apply(entry.getKey()); List<Object> fieldValues = entry.getValue(); for (int i = 0; i < fieldValues.size(); i++) { fieldValues.set(i, fieldType.valueForDisplay(fieldValues.get(i))); } } }	i moved this logic to the caller, it is now part of the lookup function that is provided.
public void testResyncDoesNotBlockOnPrimaryAction() throws Exception { try (ClusterService clusterService = createClusterService(threadPool)) { final String indexName = randomAlphaOfLength(5); setState(clusterService, state(indexName, true, ShardRoutingState.STARTED)); setState(clusterService, ClusterState.builder(clusterService.state()).blocks(ClusterBlocks.builder() .addGlobalBlock(NoMasterBlockService.NO_MASTER_BLOCK_ALL) .addIndexBlock(indexName, IndexMetaData.INDEX_WRITE_BLOCK))); try (MockNioTransport transport = new MockNioTransport(Settings.EMPTY, Version.CURRENT, threadPool, new NetworkService(emptyList()), PageCacheRecycler.NON_RECYCLING_INSTANCE, new NamedWriteableRegistry(emptyList()), new NoneCircuitBreakerService())) { final MockTransportService transportService = new MockTransportService(Settings.EMPTY, transport, threadPool, NOOP_TRANSPORT_INTERCEPTOR, x -> clusterService.localNode(), null, Collections.emptySet()); transportService.start(); transportService.acceptIncomingRequests(); final ShardStateAction shardStateAction = new ShardStateAction(clusterService, transportService, null, null, threadPool); final IndexMetaData indexMetaData = clusterService.state().metaData().index(indexName); final Index index = indexMetaData.getIndex(); final ShardId shardId = new ShardId(index, 0); final IndexShardRoutingTable shardRoutingTable = clusterService.state().routingTable().shardRoutingTable(shardId); final ShardRouting primaryShardRouting = clusterService.state().routingTable().shardRoutingTable(shardId).primaryShard(); final String allocationId = primaryShardRouting.allocationId().getId(); final long primaryTerm = indexMetaData.primaryTerm(shardId.id()); final AtomicInteger acquiredPermits = new AtomicInteger(); final IndexShard indexShard = mock(IndexShard.class); when(indexShard.shardId()).thenReturn(shardId); when(indexShard.routingEntry()).thenReturn(primaryShardRouting); when(indexShard.getPendingPrimaryTerm()).thenReturn(primaryTerm); when(indexShard.getOperationPrimaryTerm()).thenReturn(primaryTerm); when(indexShard.getActiveOperationsCount()).then(i -> acquiredPermits.get()); doAnswer(invocation -> { ActionListener<Releasable> callback = (ActionListener<Releasable>) invocation.getArguments()[0]; acquiredPermits.incrementAndGet(); callback.onResponse(acquiredPermits::decrementAndGet); return null; }).when(indexShard).acquirePrimaryOperationPermit(any(ActionListener.class), anyString(), anyObject()); when(indexShard.getReplicationGroup()).thenReturn( new ReplicationGroup(shardRoutingTable, clusterService.state().metaData().index(index).inSyncAllocationIds(shardId.id()), shardRoutingTable.getAllAllocationIds())); final IndexService indexService = mock(IndexService.class); when(indexService.getShard(eq(shardId.id()))).thenReturn(indexShard); final IndicesService indexServices = mock(IndicesService.class); when(indexServices.indexServiceSafe(eq(index))).thenReturn(indexService); final IndexNameExpressionResolver resolver = new IndexNameExpressionResolver(); final TransportResyncReplicationAction action = new TransportResyncReplicationAction(Settings.EMPTY, transportService, clusterService, indexServices, threadPool, shardStateAction, new ActionFilters(new HashSet<>()), resolver); assertThat(action.globalBlockLevel(), nullValue()); assertThat(action.indexBlockLevel(), nullValue()); final Task task = mock(Task.class); when(task.getId()).thenReturn(randomNonNegativeLong()); final byte[] bytes = "{}".getBytes(Charset.forName("UTF-8")); final ResyncReplicationRequest request = new ResyncReplicationRequest(shardId, 42L, 100, new Translog.Operation[]{new Translog.Index("type", 0, primaryTerm, 0L, bytes, null, -1)}); final PlainActionFuture<ResyncReplicationResponse> listener = new PlainActionFuture<>(); action.sync(request, task, allocationId, primaryTerm, listener); assertThat(listener.get().getShardInfo().getFailed(), equalTo(0)); assertThat(listener.isDone(), is(true)); } } }	we should keep "id" instead of "type" here.
private void applyOperation(Engine engine, Engine.Operation operation) throws IOException { switch (operation.operationType()) { case INDEX: Engine.Index engineIndex = (Engine.Index) operation; Mapping update = engineIndex.parsedDoc().dynamicMappingsUpdate(); if (engineIndex.parsedDoc().dynamicMappingsUpdate() != null) { recoveredTypes.compute("", (k, mapping) -> mapping == null ? update : mapping.merge(update)); } engine.index(engineIndex); break; case DELETE: engine.delete((Engine.Delete) operation); break; case NO_OP: engine.noOp((Engine.NoOp) operation); break; default: throw new IllegalStateException("No operation defined for [" + operation + "]"); } }	i think we should remove "recoveredtypes" and adjust the related tests.
private void checkAllowExpensiveQueries(QueryShardContext context) { if (context.allowExpensiveQueries() == false) { throw new IllegalArgumentException( "queries cannot be executed against [script] fields while [" + ALLOW_EXPENSIVE_QUERIES.getKey() + "] is set to [false]." ); } }	nit: use the constant from the mapper? content type i think it is called
private void checkAllowExpensiveQueries(QueryShardContext context) { if (context.allowExpensiveQueries() == false) { throw new IllegalArgumentException( "queries cannot be executed against [script] fields while [" + ALLOW_EXPENSIVE_QUERIES.getKey() + "] is set to [false]." ); } }	maybe one day we will a base class for runtime mapper field types that does this in one place.
public void validateDetectorsAreUnique() { Set<Detector> canonicalDetectors = new HashSet<>(); for (Detector detector : this.analysisConfig.getDetectors()) { // While testing for equality, ignore detectorIndex field as this field is auto-generated. Detector canonicalDetector = new Detector.Builder(detector).setDetectorIndex(0).build(); if (!canonicalDetectors.add(canonicalDetector)) { throw new IllegalArgumentException( Messages.getMessage(Messages.JOB_CONFIG_DUPLICATE_DETECTORS_DISALLOWED, detector.getDetectorDescription())); } } } /** * Builds a job with the given {@code createTime} and the current version. * This should be used when a new job is created as opposed to {@link #build()}	nit: we prefer to avoid using ! and use == false instead because the former is easy to miss while reading the code.
@Override public Collection<SystemIndexDescriptor> getSystemIndexDescriptors(Settings settings) { return List.of( SystemIndexDescriptor.builder() .setIndexPattern(MlMetaIndex.indexName() + "*") .setPrimaryIndex(MlMetaIndex.indexName()) .setDescription("Contains scheduling and anomaly tracking metadata") .setMappings(MlMetaIndex.mapping()) .setSettings(MlMetaIndex.settings()) .setVersionMetaKey("version") .setOrigin(ML_ORIGIN) .build(), SystemIndexDescriptor.builder() .setIndexPattern(MlConfigIndex.indexName() + "*") .setPrimaryIndex(MlConfigIndex.indexName()) .setDescription("Contains ML configuration data") .setMappings(MlConfigIndex.mapping()) .setSettings(MlConfigIndex.settings()) .setVersionMetaKey("version") .setOrigin(ML_ORIGIN) .build(), SystemIndexDescriptor.builder() .setIndexPattern(InferenceIndexConstants.INDEX_PATTERN) .setPrimaryIndex(InferenceIndexConstants.LATEST_INDEX_NAME) .setDescription("Contains ML model configuration and statistics") .setMappings(InferenceIndexConstants.mapping()) .setSettings(InferenceIndexConstants.settings()) .setVersionMetaKey("version") .setOrigin(ML_ORIGIN) .build() ); }	inference does ever update the existing mappings of the old index. so, on a new node where inferenceindexconstants.latest_index_name is newer than other nodes in the index, a new index is created. do system indices still apply the latest index mapping, even if that new node is not master? previously we got around this by manually adding the template in teh analytics job assignment, but this pr removes that.
private RebalanceDecision decideRebalance(final ShardRouting shard) { if (shard.started() == false) { // cannot rebalance a shard that isn't started return RebalanceDecision.NOT_TAKEN; } Decision canRebalance = allocation.deciders().canRebalance(shard, allocation); if (canRebalance.type() != Type.YES) { // pick the first NO rebalance decision and use its explanation for the final explanation String explanation = null; Type explanationCause = null; for (Decision subDecision : canRebalance.getDecisions()) { if ((subDecision.type() == Type.NO && (explanation == null || explanationCause == Type.THROTTLE)) || (subDecision.type() == Type.THROTTLE && explanation == null)) { explanation = subDecision.getExplanation(); explanationCause = subDecision.type(); } } return new RebalanceDecision(canRebalance, Type.NO, explanation); } if (allocation.hasPendingAsyncFetch()) { return new RebalanceDecision( canRebalance, Type.NO, "cannot rebalance due to in-flight shard store fetches, otherwise allocation may prematurely rebalance a shard to " + "a node that is soon to receive another shard assignment upon completion of the shard store fetch, " + "rendering the cluster imbalanced again" ); } sorter.reset(shard.getIndexName()); ModelNode[] modelNodes = sorter.modelNodes; final String currentNodeId = shard.currentNodeId(); // find currently assigned node ModelNode currentNode = null; for (ModelNode node : modelNodes) { if (node.getNodeId().equals(currentNodeId)) { currentNode = node; break; } } assert currentNode != null : "currently assigned node could not be found"; // balance the shard, if a better node can be found final float currentWeight = sorter.weight(currentNode); final AllocationDeciders deciders = allocation.deciders(); final String idxName = shard.getIndexName(); Map<String, NodeRebalanceDecision> nodeDecisions = new HashMap<>(); Type rebalanceDecisionType = Type.NO; String assignedNodeId = null; for (ModelNode node : modelNodes) { if (node == currentNode) { continue; // skip over node we're currently allocated to it } final Decision canAllocate = deciders.canAllocate(shard, node.getRoutingNode(), allocation); final float nodeWeight = sorter.weight(node); final boolean betterWeightThanCurrent = nodeWeight <= currentWeight; final boolean rebalanceConditionsMet; final boolean deltaAboveThreshold; final float weightWithShardAdded; if (betterWeightThanCurrent) { final float currentDelta = absDelta(nodeWeight, currentWeight); deltaAboveThreshold = lessThan(currentDelta, threshold) == false; weightWithShardAdded = weight.weightShardAdded(this, node, idxName); final float proposedDelta = weightWithShardAdded - weight.weightShardRemoved(this, currentNode, idxName); rebalanceConditionsMet = deltaAboveThreshold && proposedDelta < currentDelta; if (rebalanceConditionsMet && canAllocate.type().higherThan(rebalanceDecisionType)) { // rebalance to the node, only will get overwritten if the decision here is to // THROTTLE and we get a decision with YES on another node rebalanceDecisionType = canAllocate.type(); assignedNodeId = node.getNodeId(); } } else { rebalanceConditionsMet = false; deltaAboveThreshold = false; weightWithShardAdded = Float.POSITIVE_INFINITY; } nodeDecisions.put(node.getNodeId(), new NodeRebalanceDecision( rebalanceConditionsMet ? canAllocate.type() : Type.NO, canAllocate, betterWeightThanCurrent, deltaAboveThreshold, nodeWeight, weightWithShardAdded) ); } return RebalanceDecision.decision( canRebalance, rebalanceDecisionType, assignedNodeId, nodeDecisions, currentWeight, threshold ); }	should we have a break statement after this since your comment says we pick the first explanation?
private RebalanceDecision decideRebalance(final ShardRouting shard) { if (shard.started() == false) { // cannot rebalance a shard that isn't started return RebalanceDecision.NOT_TAKEN; } Decision canRebalance = allocation.deciders().canRebalance(shard, allocation); if (canRebalance.type() != Type.YES) { // pick the first NO rebalance decision and use its explanation for the final explanation String explanation = null; Type explanationCause = null; for (Decision subDecision : canRebalance.getDecisions()) { if ((subDecision.type() == Type.NO && (explanation == null || explanationCause == Type.THROTTLE)) || (subDecision.type() == Type.THROTTLE && explanation == null)) { explanation = subDecision.getExplanation(); explanationCause = subDecision.type(); } } return new RebalanceDecision(canRebalance, Type.NO, explanation); } if (allocation.hasPendingAsyncFetch()) { return new RebalanceDecision( canRebalance, Type.NO, "cannot rebalance due to in-flight shard store fetches, otherwise allocation may prematurely rebalance a shard to " + "a node that is soon to receive another shard assignment upon completion of the shard store fetch, " + "rendering the cluster imbalanced again" ); } sorter.reset(shard.getIndexName()); ModelNode[] modelNodes = sorter.modelNodes; final String currentNodeId = shard.currentNodeId(); // find currently assigned node ModelNode currentNode = null; for (ModelNode node : modelNodes) { if (node.getNodeId().equals(currentNodeId)) { currentNode = node; break; } } assert currentNode != null : "currently assigned node could not be found"; // balance the shard, if a better node can be found final float currentWeight = sorter.weight(currentNode); final AllocationDeciders deciders = allocation.deciders(); final String idxName = shard.getIndexName(); Map<String, NodeRebalanceDecision> nodeDecisions = new HashMap<>(); Type rebalanceDecisionType = Type.NO; String assignedNodeId = null; for (ModelNode node : modelNodes) { if (node == currentNode) { continue; // skip over node we're currently allocated to it } final Decision canAllocate = deciders.canAllocate(shard, node.getRoutingNode(), allocation); final float nodeWeight = sorter.weight(node); final boolean betterWeightThanCurrent = nodeWeight <= currentWeight; final boolean rebalanceConditionsMet; final boolean deltaAboveThreshold; final float weightWithShardAdded; if (betterWeightThanCurrent) { final float currentDelta = absDelta(nodeWeight, currentWeight); deltaAboveThreshold = lessThan(currentDelta, threshold) == false; weightWithShardAdded = weight.weightShardAdded(this, node, idxName); final float proposedDelta = weightWithShardAdded - weight.weightShardRemoved(this, currentNode, idxName); rebalanceConditionsMet = deltaAboveThreshold && proposedDelta < currentDelta; if (rebalanceConditionsMet && canAllocate.type().higherThan(rebalanceDecisionType)) { // rebalance to the node, only will get overwritten if the decision here is to // THROTTLE and we get a decision with YES on another node rebalanceDecisionType = canAllocate.type(); assignedNodeId = node.getNodeId(); } } else { rebalanceConditionsMet = false; deltaAboveThreshold = false; weightWithShardAdded = Float.POSITIVE_INFINITY; } nodeDecisions.put(node.getNodeId(), new NodeRebalanceDecision( rebalanceConditionsMet ? canAllocate.type() : Type.NO, canAllocate, betterWeightThanCurrent, deltaAboveThreshold, nodeWeight, weightWithShardAdded) ); } return RebalanceDecision.decision( canRebalance, rebalanceDecisionType, assignedNodeId, nodeDecisions, currentWeight, threshold ); }	should this be checked prior to the decision check, since it is a little lighter weight?
private RebalanceDecision decideRebalance(final ShardRouting shard) { if (shard.started() == false) { // cannot rebalance a shard that isn't started return RebalanceDecision.NOT_TAKEN; } Decision canRebalance = allocation.deciders().canRebalance(shard, allocation); if (canRebalance.type() != Type.YES) { // pick the first NO rebalance decision and use its explanation for the final explanation String explanation = null; Type explanationCause = null; for (Decision subDecision : canRebalance.getDecisions()) { if ((subDecision.type() == Type.NO && (explanation == null || explanationCause == Type.THROTTLE)) || (subDecision.type() == Type.THROTTLE && explanation == null)) { explanation = subDecision.getExplanation(); explanationCause = subDecision.type(); } } return new RebalanceDecision(canRebalance, Type.NO, explanation); } if (allocation.hasPendingAsyncFetch()) { return new RebalanceDecision( canRebalance, Type.NO, "cannot rebalance due to in-flight shard store fetches, otherwise allocation may prematurely rebalance a shard to " + "a node that is soon to receive another shard assignment upon completion of the shard store fetch, " + "rendering the cluster imbalanced again" ); } sorter.reset(shard.getIndexName()); ModelNode[] modelNodes = sorter.modelNodes; final String currentNodeId = shard.currentNodeId(); // find currently assigned node ModelNode currentNode = null; for (ModelNode node : modelNodes) { if (node.getNodeId().equals(currentNodeId)) { currentNode = node; break; } } assert currentNode != null : "currently assigned node could not be found"; // balance the shard, if a better node can be found final float currentWeight = sorter.weight(currentNode); final AllocationDeciders deciders = allocation.deciders(); final String idxName = shard.getIndexName(); Map<String, NodeRebalanceDecision> nodeDecisions = new HashMap<>(); Type rebalanceDecisionType = Type.NO; String assignedNodeId = null; for (ModelNode node : modelNodes) { if (node == currentNode) { continue; // skip over node we're currently allocated to it } final Decision canAllocate = deciders.canAllocate(shard, node.getRoutingNode(), allocation); final float nodeWeight = sorter.weight(node); final boolean betterWeightThanCurrent = nodeWeight <= currentWeight; final boolean rebalanceConditionsMet; final boolean deltaAboveThreshold; final float weightWithShardAdded; if (betterWeightThanCurrent) { final float currentDelta = absDelta(nodeWeight, currentWeight); deltaAboveThreshold = lessThan(currentDelta, threshold) == false; weightWithShardAdded = weight.weightShardAdded(this, node, idxName); final float proposedDelta = weightWithShardAdded - weight.weightShardRemoved(this, currentNode, idxName); rebalanceConditionsMet = deltaAboveThreshold && proposedDelta < currentDelta; if (rebalanceConditionsMet && canAllocate.type().higherThan(rebalanceDecisionType)) { // rebalance to the node, only will get overwritten if the decision here is to // THROTTLE and we get a decision with YES on another node rebalanceDecisionType = canAllocate.type(); assignedNodeId = node.getNodeId(); } } else { rebalanceConditionsMet = false; deltaAboveThreshold = false; weightWithShardAdded = Float.POSITIVE_INFINITY; } nodeDecisions.put(node.getNodeId(), new NodeRebalanceDecision( rebalanceConditionsMet ? canAllocate.type() : Type.NO, canAllocate, betterWeightThanCurrent, deltaAboveThreshold, nodeWeight, weightWithShardAdded) ); } return RebalanceDecision.decision( canRebalance, rebalanceDecisionType, assignedNodeId, nodeDecisions, currentWeight, threshold ); }	minor optimization, but we can do new hashmap<>(modelnodes.length - 1); to avoid hashmap resizing (if you want)
private RebalanceDecision decideRebalance(final ShardRouting shard) { if (shard.started() == false) { // cannot rebalance a shard that isn't started return RebalanceDecision.NOT_TAKEN; } Decision canRebalance = allocation.deciders().canRebalance(shard, allocation); if (canRebalance.type() != Type.YES) { // pick the first NO rebalance decision and use its explanation for the final explanation String explanation = null; Type explanationCause = null; for (Decision subDecision : canRebalance.getDecisions()) { if ((subDecision.type() == Type.NO && (explanation == null || explanationCause == Type.THROTTLE)) || (subDecision.type() == Type.THROTTLE && explanation == null)) { explanation = subDecision.getExplanation(); explanationCause = subDecision.type(); } } return new RebalanceDecision(canRebalance, Type.NO, explanation); } if (allocation.hasPendingAsyncFetch()) { return new RebalanceDecision( canRebalance, Type.NO, "cannot rebalance due to in-flight shard store fetches, otherwise allocation may prematurely rebalance a shard to " + "a node that is soon to receive another shard assignment upon completion of the shard store fetch, " + "rendering the cluster imbalanced again" ); } sorter.reset(shard.getIndexName()); ModelNode[] modelNodes = sorter.modelNodes; final String currentNodeId = shard.currentNodeId(); // find currently assigned node ModelNode currentNode = null; for (ModelNode node : modelNodes) { if (node.getNodeId().equals(currentNodeId)) { currentNode = node; break; } } assert currentNode != null : "currently assigned node could not be found"; // balance the shard, if a better node can be found final float currentWeight = sorter.weight(currentNode); final AllocationDeciders deciders = allocation.deciders(); final String idxName = shard.getIndexName(); Map<String, NodeRebalanceDecision> nodeDecisions = new HashMap<>(); Type rebalanceDecisionType = Type.NO; String assignedNodeId = null; for (ModelNode node : modelNodes) { if (node == currentNode) { continue; // skip over node we're currently allocated to it } final Decision canAllocate = deciders.canAllocate(shard, node.getRoutingNode(), allocation); final float nodeWeight = sorter.weight(node); final boolean betterWeightThanCurrent = nodeWeight <= currentWeight; final boolean rebalanceConditionsMet; final boolean deltaAboveThreshold; final float weightWithShardAdded; if (betterWeightThanCurrent) { final float currentDelta = absDelta(nodeWeight, currentWeight); deltaAboveThreshold = lessThan(currentDelta, threshold) == false; weightWithShardAdded = weight.weightShardAdded(this, node, idxName); final float proposedDelta = weightWithShardAdded - weight.weightShardRemoved(this, currentNode, idxName); rebalanceConditionsMet = deltaAboveThreshold && proposedDelta < currentDelta; if (rebalanceConditionsMet && canAllocate.type().higherThan(rebalanceDecisionType)) { // rebalance to the node, only will get overwritten if the decision here is to // THROTTLE and we get a decision with YES on another node rebalanceDecisionType = canAllocate.type(); assignedNodeId = node.getNodeId(); } } else { rebalanceConditionsMet = false; deltaAboveThreshold = false; weightWithShardAdded = Float.POSITIVE_INFINITY; } nodeDecisions.put(node.getNodeId(), new NodeRebalanceDecision( rebalanceConditionsMet ? canAllocate.type() : Type.NO, canAllocate, betterWeightThanCurrent, deltaAboveThreshold, nodeWeight, weightWithShardAdded) ); } return RebalanceDecision.decision( canRebalance, rebalanceDecisionType, assignedNodeId, nodeDecisions, currentWeight, threshold ); }	i feel like these methods (the comparisons, deltas, etc), would benefit from some comments explaining what's happening for people reading this code
private RebalanceDecision decideRebalance(final ShardRouting shard) { if (shard.started() == false) { // cannot rebalance a shard that isn't started return RebalanceDecision.NOT_TAKEN; } Decision canRebalance = allocation.deciders().canRebalance(shard, allocation); if (canRebalance.type() != Type.YES) { // pick the first NO rebalance decision and use its explanation for the final explanation String explanation = null; Type explanationCause = null; for (Decision subDecision : canRebalance.getDecisions()) { if ((subDecision.type() == Type.NO && (explanation == null || explanationCause == Type.THROTTLE)) || (subDecision.type() == Type.THROTTLE && explanation == null)) { explanation = subDecision.getExplanation(); explanationCause = subDecision.type(); } } return new RebalanceDecision(canRebalance, Type.NO, explanation); } if (allocation.hasPendingAsyncFetch()) { return new RebalanceDecision( canRebalance, Type.NO, "cannot rebalance due to in-flight shard store fetches, otherwise allocation may prematurely rebalance a shard to " + "a node that is soon to receive another shard assignment upon completion of the shard store fetch, " + "rendering the cluster imbalanced again" ); } sorter.reset(shard.getIndexName()); ModelNode[] modelNodes = sorter.modelNodes; final String currentNodeId = shard.currentNodeId(); // find currently assigned node ModelNode currentNode = null; for (ModelNode node : modelNodes) { if (node.getNodeId().equals(currentNodeId)) { currentNode = node; break; } } assert currentNode != null : "currently assigned node could not be found"; // balance the shard, if a better node can be found final float currentWeight = sorter.weight(currentNode); final AllocationDeciders deciders = allocation.deciders(); final String idxName = shard.getIndexName(); Map<String, NodeRebalanceDecision> nodeDecisions = new HashMap<>(); Type rebalanceDecisionType = Type.NO; String assignedNodeId = null; for (ModelNode node : modelNodes) { if (node == currentNode) { continue; // skip over node we're currently allocated to it } final Decision canAllocate = deciders.canAllocate(shard, node.getRoutingNode(), allocation); final float nodeWeight = sorter.weight(node); final boolean betterWeightThanCurrent = nodeWeight <= currentWeight; final boolean rebalanceConditionsMet; final boolean deltaAboveThreshold; final float weightWithShardAdded; if (betterWeightThanCurrent) { final float currentDelta = absDelta(nodeWeight, currentWeight); deltaAboveThreshold = lessThan(currentDelta, threshold) == false; weightWithShardAdded = weight.weightShardAdded(this, node, idxName); final float proposedDelta = weightWithShardAdded - weight.weightShardRemoved(this, currentNode, idxName); rebalanceConditionsMet = deltaAboveThreshold && proposedDelta < currentDelta; if (rebalanceConditionsMet && canAllocate.type().higherThan(rebalanceDecisionType)) { // rebalance to the node, only will get overwritten if the decision here is to // THROTTLE and we get a decision with YES on another node rebalanceDecisionType = canAllocate.type(); assignedNodeId = node.getNodeId(); } } else { rebalanceConditionsMet = false; deltaAboveThreshold = false; weightWithShardAdded = Float.POSITIVE_INFINITY; } nodeDecisions.put(node.getNodeId(), new NodeRebalanceDecision( rebalanceConditionsMet ? canAllocate.type() : Type.NO, canAllocate, betterWeightThanCurrent, deltaAboveThreshold, nodeWeight, weightWithShardAdded) ); } return RebalanceDecision.decision( canRebalance, rebalanceDecisionType, assignedNodeId, nodeDecisions, currentWeight, threshold ); }	i think it's a little strange to define these three variables as final in this for loop, then potentially set them only once, then use them in the map .put call, instead i think it'd be better to define them before the if check as: java boolean rebalanceconditionsmet = false; boolean deltaabovethreshold = false; float weightwithshardadded = float.positive_infinity; and then you can remove this else branch entirely.
private RebalanceDecision decideRebalance(final ShardRouting shard) { if (shard.started() == false) { // cannot rebalance a shard that isn't started return RebalanceDecision.NOT_TAKEN; } Decision canRebalance = allocation.deciders().canRebalance(shard, allocation); if (canRebalance.type() != Type.YES) { // pick the first NO rebalance decision and use its explanation for the final explanation String explanation = null; Type explanationCause = null; for (Decision subDecision : canRebalance.getDecisions()) { if ((subDecision.type() == Type.NO && (explanation == null || explanationCause == Type.THROTTLE)) || (subDecision.type() == Type.THROTTLE && explanation == null)) { explanation = subDecision.getExplanation(); explanationCause = subDecision.type(); } } return new RebalanceDecision(canRebalance, Type.NO, explanation); } if (allocation.hasPendingAsyncFetch()) { return new RebalanceDecision( canRebalance, Type.NO, "cannot rebalance due to in-flight shard store fetches, otherwise allocation may prematurely rebalance a shard to " + "a node that is soon to receive another shard assignment upon completion of the shard store fetch, " + "rendering the cluster imbalanced again" ); } sorter.reset(shard.getIndexName()); ModelNode[] modelNodes = sorter.modelNodes; final String currentNodeId = shard.currentNodeId(); // find currently assigned node ModelNode currentNode = null; for (ModelNode node : modelNodes) { if (node.getNodeId().equals(currentNodeId)) { currentNode = node; break; } } assert currentNode != null : "currently assigned node could not be found"; // balance the shard, if a better node can be found final float currentWeight = sorter.weight(currentNode); final AllocationDeciders deciders = allocation.deciders(); final String idxName = shard.getIndexName(); Map<String, NodeRebalanceDecision> nodeDecisions = new HashMap<>(); Type rebalanceDecisionType = Type.NO; String assignedNodeId = null; for (ModelNode node : modelNodes) { if (node == currentNode) { continue; // skip over node we're currently allocated to it } final Decision canAllocate = deciders.canAllocate(shard, node.getRoutingNode(), allocation); final float nodeWeight = sorter.weight(node); final boolean betterWeightThanCurrent = nodeWeight <= currentWeight; final boolean rebalanceConditionsMet; final boolean deltaAboveThreshold; final float weightWithShardAdded; if (betterWeightThanCurrent) { final float currentDelta = absDelta(nodeWeight, currentWeight); deltaAboveThreshold = lessThan(currentDelta, threshold) == false; weightWithShardAdded = weight.weightShardAdded(this, node, idxName); final float proposedDelta = weightWithShardAdded - weight.weightShardRemoved(this, currentNode, idxName); rebalanceConditionsMet = deltaAboveThreshold && proposedDelta < currentDelta; if (rebalanceConditionsMet && canAllocate.type().higherThan(rebalanceDecisionType)) { // rebalance to the node, only will get overwritten if the decision here is to // THROTTLE and we get a decision with YES on another node rebalanceDecisionType = canAllocate.type(); assignedNodeId = node.getNodeId(); } } else { rebalanceConditionsMet = false; deltaAboveThreshold = false; weightWithShardAdded = Float.POSITIVE_INFINITY; } nodeDecisions.put(node.getNodeId(), new NodeRebalanceDecision( rebalanceConditionsMet ? canAllocate.type() : Type.NO, canAllocate, betterWeightThanCurrent, deltaAboveThreshold, nodeWeight, weightWithShardAdded) ); } return RebalanceDecision.decision( canRebalance, rebalanceDecisionType, assignedNodeId, nodeDecisions, currentWeight, threshold ); }	this syntax is a little funky, it feels like bracket syntax, but for a function call.
@Override @Nullable public String getExplanation() { // multi-decisions have no explanation return null; }	should multi-decisions not be a concatenation of their sub-decision explanations?
public static ClusterState state(String index, final int numberOfNodes, final int numberOfPrimaries) { DiscoveryNodes.Builder discoBuilder = DiscoveryNodes.builder(); Set<String> nodes = new HashSet<>(); for (int i = 0; i < numberOfNodes; i++) { final DiscoveryNode node = newNode(i); discoBuilder = discoBuilder.add(node); nodes.add(node.getId()); } discoBuilder.localNodeId(newNode(0).getId()); discoBuilder.masterNodeId(newNode(0).getId()); IndexMetaData indexMetaData = IndexMetaData.builder(index).settings(Settings.builder() .put(SETTING_VERSION_CREATED, Version.CURRENT) .put(SETTING_NUMBER_OF_SHARDS, numberOfPrimaries).put(SETTING_NUMBER_OF_REPLICAS, 0) .put(SETTING_CREATION_DATE, System.currentTimeMillis())).build(); RoutingTable.Builder routing = new RoutingTable.Builder(); routing.addAsNew(indexMetaData); IndexRoutingTable.Builder indexRoutingTable = IndexRoutingTable.builder(indexMetaData.getIndex()); for (int i = 0; i < numberOfPrimaries; i++) { ShardId shardId = new ShardId(indexMetaData.getIndex(), i); IndexShardRoutingTable.Builder indexShardRoutingBuilder = new IndexShardRoutingTable.Builder(shardId); indexShardRoutingBuilder.addShard( TestShardRouting.newShardRouting(shardId, randomFrom(nodes), true, ShardRoutingState.STARTED)); indexRoutingTable.addIndexShard(indexShardRoutingBuilder.build()); } ClusterState.Builder state = ClusterState.builder(new ClusterName("test")); state.nodes(discoBuilder); state.metaData(MetaData.builder().put(indexMetaData, false).generateClusterUuidIfNeeded()); state.routingTable(RoutingTable.builder().add(indexRoutingTable).build()); return state.build(); }	why is this change ok?
private synchronized void consumeInternal(QuerySearchResult querySearchResult) { if (querySearchResult.isNull() == false) { if (index == bufferSize) { if (hasAggs) { InternalAggregations reducedAggs = InternalAggregations.topLevelReduce( Arrays.stream(aggsBuffer).map(Supplier::get).collect(toList()), aggReduceContextBuilder.forPartialReduction()); Arrays.fill(aggsBuffer, null); aggsBuffer[0] = () -> reducedAggs; } if (hasTopDocs) { TopDocs reducedTopDocs = mergeTopDocs(Arrays.asList(topDocsBuffer), // we have to merge here in the same way we collect on a shard topNSize, 0); Arrays.fill(topDocsBuffer, null); topDocsBuffer[0] = reducedTopDocs; } numReducePhases++; index = 1; if (hasAggs || hasTopDocs) { progressListener.notifyPartialReduce(SearchProgressListener.buildSearchShards(processedShards), topDocsStats.getTotalHits(), hasAggs ? aggsBuffer[0].get() : null, numReducePhases); } } final int i = index++; if (hasAggs) { aggsBuffer[i] = querySearchResult.consumeAggs(); } if (hasTopDocs) { final TopDocsAndMaxScore topDocs = querySearchResult.consumeTopDocs(); // can't be null topDocsStats.add(topDocs, querySearchResult.searchTimedOut(), querySearchResult.terminatedEarly()); setShardIndex(topDocs.topDocs, querySearchResult.getShardIndex()); topDocsBuffer[i] = topDocs.topDocs; } } processedShards[querySearchResult.getShardIndex()] = querySearchResult.getSearchShardTarget(); }	should we nullify the rest of the array to make the reduced aggs eligible for gc ?
public void writeToNoId(StreamOutput out) throws IOException { out.writeVInt(from); out.writeVInt(size); if (sortValueFormats == null) { out.writeVInt(0); } else { out.writeVInt(1 + sortValueFormats.length); for (int i = 0; i < sortValueFormats.length; ++i) { out.writeNamedWriteable(sortValueFormats[i]); } } writeTopDocs(out, topDocsAndMaxScore); if (aggregations == null) { out.writeBoolean(false); } else { out.writeBoolean(true); if (out.getVersion().before(Version.V_8_0_0)) { aggregations.get().writeTo(out); } else { aggregations.writeTo(out); } } if (out.getVersion().before(Version.V_7_2_0)) { //Earlier versions expect sibling pipeline aggs separately as they used to be set to QuerySearchResult directly, //while later versions expect them in InternalAggregations. Note that despite serializing sibling pipeline aggs as part of //InternalAggregations is supported since 6.7.0, the shards set sibling pipeline aggs to InternalAggregations only from 7.1 on. if (aggregations == null) { out.writeNamedWriteableList(Collections.emptyList()); } else { out.writeNamedWriteableList(aggregations.get().getTopLevelPipelineAggregators()); } } if (suggest == null) { out.writeBoolean(false); } else { out.writeBoolean(true); suggest.writeTo(out); } out.writeBoolean(searchTimedOut); out.writeOptionalBoolean(terminatedEarly); out.writeOptionalWriteable(profileShardResults); out.writeZLong(serviceTimeEWMA); out.writeInt(nodeQueueSize); }	we can maybe get the aggs once if the remote node is in a version before v8 (instead of calling get here and below to get the pipeline aggs) ?
@Override public void usage(ActionListener<XPackFeatureSet.Usage> listener) { ClusterState state = clusterService.state(); new Retriever(client, MlMetadata.getMlMetadata(state), available(), enabled()).execute(listener); }	it was never really worth handling this case differently to ml metadata existing but being empty, as this state only existed for a fraction of a second on first cluster startup. now this state will exist for longer - until the first job is created - but handling it like empty ml metadata is actually closer to the historical behaviour.
public Map<String, Processor.Factory> getProcessors(Processor.Parameters parameters) { Map<String, Processor.Factory> processors = new HashMap<>(); processors.put(DateProcessor.TYPE, new DateProcessor.Factory()); processors.put(SetProcessor.TYPE, new SetProcessor.Factory(parameters.templateService)); processors.put(AppendProcessor.TYPE, new AppendProcessor.Factory(parameters.templateService)); processors.put(RenameProcessor.TYPE, new RenameProcessor.Factory()); processors.put(RemoveProcessor.TYPE, new RemoveProcessor.Factory()); processors.put(SplitProcessor.TYPE, new SplitProcessor.Factory()); processors.put(JoinProcessor.TYPE, new JoinProcessor.Factory()); processors.put(UppercaseProcessor.TYPE, new UppercaseProcessor.Factory()); processors.put(LowercaseProcessor.TYPE, new LowercaseProcessor.Factory()); processors.put(TrimProcessor.TYPE, new TrimProcessor.Factory()); processors.put(ConvertProcessor.TYPE, new ConvertProcessor.Factory()); processors.put(GsubProcessor.TYPE, new GsubProcessor.Factory()); processors.put(FailProcessor.TYPE, new FailProcessor.Factory(parameters.templateService)); processors.put(ForEachProcessor.TYPE, new ForEachProcessor.Factory()); processors.put(DateIndexNameProcessor.TYPE, new DateIndexNameProcessor.Factory()); processors.put(SortProcessor.TYPE, new SortProcessor.Factory()); processors.put(GrokProcessor.TYPE, new GrokProcessor.Factory(builtinPatterns)); processors.put(ScriptProcessor.TYPE, new ScriptProcessor.Factory(parameters.scriptService)); processors.put(DotExpanderProcessor.TYPE, new DotExpanderProcessor.Factory()); processors.put(JsonProcessor.TYPE, new JsonProcessor.Factory()); processors.put(KeyValueProcessor.TYPE, new KeyValueProcessor.Factory()); return Collections.unmodifiableMap(processors); }	it would be best to leave templatable field values, since that was the previously supported behavior
@Override public RemoveProcessor create(Map<String, Processor.Factory> registry, String processorTag, Map<String, Object> config) throws Exception { List<String> fields = ConfigurationUtils.readList(TYPE, processorTag, config, "field"); return new RemoveProcessor(processorTag, fields); }	i think it would be nice to support both singular strings *and* lists. what do you think? will also help with backwards compatibility

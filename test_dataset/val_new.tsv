private void checkMappingsCompatibility(IndexMetaData indexMetaData) { try { // We cannot instantiate real analysis server at this point because the node might not have // been started yet. However, we don't really need real analyzers at this stage - so we can fake it IndexSettings indexSettings = new IndexSettings(indexMetaData, this.settings); SimilarityService similarityService = new SimilarityService(indexSettings, Collections.emptyMap()); final NamedAnalyzer fakeDefault = new NamedAnalyzer("fake_default", new Analyzer() { @Override protected TokenStreamComponents createComponents(String fieldName) { throw new UnsupportedOperationException("shouldn't be here"); } }); final Map<String, NamedAnalyzer> analyzerMap = new AbstractMap<String, NamedAnalyzer>() { @Override public NamedAnalyzer get(Object key) { return new NamedAnalyzer((String)key, fakeDefault.analyzer()); } @Override public Set<Entry<String, NamedAnalyzer>> entrySet() { return Collections.singletonMap(fakeDefault.name(), fakeDefault).entrySet(); } }; try (IndexAnalyzers fakeIndexAnalzyers = new IndexAnalyzers(indexSettings, fakeDefault, fakeDefault, fakeDefault, analyzerMap)) { MapperService mapperService = new MapperService(indexSettings, fakeIndexAnalzyers, similarityService, mapperRegistry, () -> null); for (ObjectCursor<MappingMetaData> cursor : indexMetaData.getMappings().values()) { MappingMetaData mappingMetaData = cursor.value; mapperService.merge(mappingMetaData.type(), mappingMetaData.source(), MapperService.MergeReason.MAPPING_RECOVERY, false); } } } catch (Exception ex) { // Wrap the inner exception so we have the index name in the exception message throw new IllegalStateException("unable to upgrade the mappings for the index [" + indexMetaData.getIndex() + "]", ex); } }	tostring on the key?
public void testScaleDown_WhenMemoryIsInaccurate() { List<DiscoveryNode> nodes = withMlNodes("foo", "bar", "baz"); DiscoveryNode badNode = randomFrom(nodes); NodeLoad badLoad = NodeLoad.builder(badNode.getId()).setUseMemory(false).build(); when(nodeLoadDetector.detectNodeLoad(any(), anyBoolean(), any(), anyInt(), anyInt(), anyBoolean(), anyBoolean())) .thenReturn(NodeLoad.builder(badNode.getId()).setUseMemory(true).build()); when(nodeLoadDetector.detectNodeLoad(any(), anyBoolean(), eq(badNode), anyInt(), anyInt(), anyBoolean(), anyBoolean())) .thenReturn(badLoad); MlAutoscalingDeciderService service = buildService(); MlScalingReason.Builder reasonBuilder = new MlScalingReason.Builder(); try { service.checkForScaleDown(nodes, ClusterState.EMPTY_STATE, Long.MAX_VALUE, new NativeMemoryCapacity(ByteSizeValue.ofGb(3).getBytes(), ByteSizeValue.ofGb(1).getBytes()), reasonBuilder); } catch (AssertionError ae) { // the scale method should thrown an assertion failure return; } assert false : "call for scale down should have thrown an assertion error"; }	does expectthrows work with assertionerror? if it does then that would be the more idiomatic way to write this.
static List<DataStream> getDataStreams(ClusterState clusterState, Request request) { Map<String, DataStream> dataStreams = clusterState.metadata().dataStreams(); // return all data streams if no name was specified final String[] requestedNames = request.names == null || request.names.length == 0 ? new String[]{"*"} : request.names; boolean hasWildcard = false; final Set<DataStream> results = new HashSet<>(); for (String requestedName : requestedNames) { if (Regex.isSimpleMatchPattern(requestedName)) { hasWildcard = true; for (Map.Entry<String, DataStream> entry : dataStreams.entrySet()) { if (Regex.simpleMatch(requestedName, entry.getKey())) { results.add(entry.getValue()); } } } else if (dataStreams.containsKey(requestedName)) { results.add(dataStreams.get(requestedName)); } } if (hasWildcard == false && results.size() == 0) { throw new ResourceNotFoundException( "data stream(s) matching [" + Strings.arrayToCommaDelimitedString(request.names) + "] not found"); } List<DataStream> resultList = new ArrayList<>(results); resultList.sort(Comparator.comparing(DataStream::getName)); return resultList; }	i think the wildcard logic is incorrect here with security enabled? this test fails: org.elasticsearch.xpack.security.corewithsecurityclientyamltestsuiteit > test {yaml=indices.data_stream/10_basic/get data stream} in this case there are no matching resources, so the expression gets rewritten to: *,-*. with this logic here, this means that all data streams get included, because there is no logic that removes data streams if -* is used. we can add this logic (logic that deals with -[something] here, or we can use the index name expressions resolver and filter out anything that isn't a data stream?
public static boolean isClusterAction(String actionName) { return actionName.startsWith("cluster:") || actionName.startsWith("indices:admin/template/") || actionName.startsWith("indices:admin/index_template/"); }	at least this hack is removed :)
public Recycler.V<BytesRef> get() { ByteBuf byteBuf = ALLOCATOR.heapBuffer(PageCacheRecycler.BYTE_PAGE_SIZE, PageCacheRecycler.BYTE_PAGE_SIZE); assert byteBuf.hasArray(); BytesRef bytesRef = new BytesRef(byteBuf.array(), byteBuf.arrayOffset(), byteBuf.writableBytes()); return new Recycler.V<>() { @Override public BytesRef v() { return bytesRef; } @Override public boolean isRecycled() { return true; } @Override public void close() { byteBuf.release(); } }; }	nit: perhaps use bytebuf.capacity() instead, seems more appropriate (but same result).
public static Match fromXContent(XContentParser parser) { return PARSER.apply(parser, null); }	can you change this and the other getters to package private so they can be used by tests only?
public static Disjunction fromXContent(XContentParser parser) throws IOException { return PARSER.parse(parser, null); }	these here would be nice as well if package private (if possible).
public void hitsExecute(SearchContext context, SearchHit[] hits) throws IOException { if ((context.innerHits() != null && context.innerHits().getInnerHits().size() > 0) == false) { return; } for (Map.Entry<String, InnerHitsContext.InnerHitSubContext> entry : context.innerHits().getInnerHits().entrySet()) { InnerHitsContext.InnerHitSubContext innerHits = entry.getValue(); TopDocsAndMaxScore[] topDocs = innerHits.topDocs(hits); for (int i = 0; i < hits.length; i++) { SearchHit hit = hits[i]; TopDocsAndMaxScore topDoc = topDocs[i]; Map<String, SearchHits> results = hit.getInnerHits(); if (results == null) { hit.setInnerHits(results = new HashMap<>()); } innerHits.queryResult().topDocs(topDoc, innerHits.sort() == null ? null : innerHits.sort().formats); int[] docIdsToLoad = new int[topDoc.topDocs.scoreDocs.length]; for (int j = 0; j < topDoc.topDocs.scoreDocs.length; j++) { docIdsToLoad[j] = topDoc.topDocs.scoreDocs[j].doc; } innerHits.docIdsToLoad(docIdsToLoad, 0, docIdsToLoad.length); innerHits.setId(hit.getId()); fetchPhase.execute(innerHits); FetchSearchResult fetchResult = innerHits.fetchResult(); SearchHit[] internalHits = fetchResult.fetchResult().hits().getHits(); for (int j = 0; j < internalHits.length; j++) { ScoreDoc scoreDoc = topDoc.topDocs.scoreDocs[j]; SearchHit searchHitFields = internalHits[j]; searchHitFields.score(scoreDoc.score); if (scoreDoc instanceof FieldDoc) { FieldDoc fieldDoc = (FieldDoc) scoreDoc; searchHitFields.sortValues(fieldDoc.fields, innerHits.sort().formats); } } results.put(entry.getKey(), fetchResult.hits()); } } }	we set the root document's _source here, but when processing inner hits we didn't use it and instead reloaded the _source. so this is safe to remove for now. in a follow-up pr, i will fix this to make sure the _source is loaded only once for the root doc + inner hits.
protected void masterOperation( Task task, CreateIndexRequest request, ClusterState state, ActionListener<CreateIndexResponse> finalListener ) { AtomicReference<String> indexNameRef = new AtomicReference<>(); ActionListener<AcknowledgedResponse> listener = ActionListener.wrap(response -> { String indexName = indexNameRef.get(); assert indexName != null; if (response.isAcknowledged()) { activeShardsObserver.waitForActiveShards( new String[] { indexName }, ActiveShardCount.DEFAULT, request.timeout(), shardsAcked -> { finalListener.onResponse(new CreateIndexResponse(true, shardsAcked, indexName)); }, finalListener::onFailure ); } else { finalListener.onResponse(new CreateIndexResponse(false, false, indexName)); } }, finalListener::onFailure); AckedClusterStateUpdateTask clusterTask = new AckedClusterStateUpdateTask(Priority.URGENT, request, listener) { @Override public ClusterState execute(ClusterState currentState) throws Exception { final SystemDataStreamDescriptor dataStreamDescriptor = systemIndices.validateDataStreamAccess( request.index(), threadPool.getThreadContext() ); final boolean isSystemDataStream = dataStreamDescriptor != null; final boolean isSystemIndex = isSystemDataStream == false && systemIndices.isSystemIndex(request.index()); final ComposableIndexTemplate template = resolveTemplate(request, currentState.metadata()); final boolean isDataStream = isSystemIndex == false && (isSystemDataStream || (template != null && template.getDataStreamTemplate() != null)); if (isDataStream) { // This expression only evaluates to true when the argument is non-null and false if (isSystemDataStream == false && Boolean.FALSE.equals(template.getAllowAutoCreate())) { throw new IndexNotFoundException( "composable template " + template.indexPatterns() + " forbids index auto creation" ); } CreateDataStreamClusterStateUpdateRequest createRequest = new CreateDataStreamClusterStateUpdateRequest( request.index(), dataStreamDescriptor, request.masterNodeTimeout(), request.timeout() ); ClusterState clusterState = metadataCreateDataStreamService.createDataStream(createRequest, currentState); indexNameRef.set(clusterState.metadata().dataStreams().get(request.index()).getIndices().get(0).getName()); return clusterState; } else { String indexName = indexNameExpressionResolver.resolveDateMathExpression(request.index()); indexNameRef.set(indexName); if (isSystemIndex) { if (indexName.equals(request.index()) == false) { throw new IllegalStateException("system indices do not support date math expressions"); } } else { // This will throw an exception if the index does not exist and creating it is prohibited final boolean shouldAutoCreate = autoCreateIndex.shouldAutoCreate(indexName, currentState); if (shouldAutoCreate == false) { // The index already exists. return currentState; } } final SystemIndexDescriptor mainDescriptor = isSystemIndex ? systemIndices.findMatchingDescriptor(indexName) : null; final boolean isManagedSystemIndex = mainDescriptor != null && mainDescriptor.isAutomaticallyManaged(); final CreateIndexClusterStateUpdateRequest updateRequest; if (isManagedSystemIndex) { final SystemIndexDescriptor descriptor = mainDescriptor.getDescriptorCompatibleWith( state.nodes().getSmallestNonClientNodeVersion() ); if (descriptor == null) { final String message = mainDescriptor.getMinimumNodeVersionMessage("auto-create index"); logger.warn(message); throw new IllegalStateException(message); } updateRequest = buildSystemIndexUpdateRequest(indexName, descriptor); } else if (isSystemIndex) { updateRequest = buildUpdateRequest(indexName); if (Objects.isNull(request.settings())) { updateRequest.settings(SystemIndexDescriptor.DEFAULT_SETTINGS); } else if (false == request.settings().hasValue(SETTING_INDEX_HIDDEN)) { updateRequest.settings(Settings.builder().put(request.settings()).put(SETTING_INDEX_HIDDEN, true).build()); } else if ("false".equals(request.settings().get(SETTING_INDEX_HIDDEN))) { final String message = "Cannot auto-create system index [" + indexName + "] with [index.hidden] set to 'false'"; logger.warn(message); throw new IllegalStateException(message); } } else { updateRequest = buildUpdateRequest(indexName); } return createIndexService.applyCreateIndexRequest(currentState, updateRequest, false); } } private CreateIndexClusterStateUpdateRequest buildUpdateRequest(String indexName) { CreateIndexClusterStateUpdateRequest updateRequest = new CreateIndexClusterStateUpdateRequest( request.cause(), indexName, request.index() ).ackTimeout(request.timeout()).performReroute(false).masterNodeTimeout(request.masterNodeTimeout()); logger.debug("Auto-creating index {}", indexName); return updateRequest; } private CreateIndexClusterStateUpdateRequest buildSystemIndexUpdateRequest( String indexName, SystemIndexDescriptor descriptor ) { String mappings = descriptor.getMappings(); Settings settings = descriptor.getSettings(); String aliasName = descriptor.getAliasName(); // if we are writing to the alias name, we should create the primary index here String concreteIndexName = indexName.equals(aliasName) ? descriptor.getPrimaryIndex() : indexName; CreateIndexClusterStateUpdateRequest updateRequest = new CreateIndexClusterStateUpdateRequest( request.cause(), concreteIndexName, request.index() ).ackTimeout(request.timeout()).masterNodeTimeout(request.masterNodeTimeout()).performReroute(false); updateRequest.waitForActiveShards(ActiveShardCount.ALL); if (mappings != null) { updateRequest.mappings(mappings); } if (settings != null) { updateRequest.settings(settings); } if (aliasName != null) { updateRequest.aliases(Set.of(new Alias(aliasName).isHidden(true))); } if (logger.isDebugEnabled()) { if (concreteIndexName.equals(indexName) == false) { logger.debug("Auto-creating backing system index {} for alias {}", concreteIndexName, indexName); } else { logger.debug("Auto-creating system index {}", concreteIndexName); } } return updateRequest; } }; clusterService.submitStateUpdateTask("auto create [" + request.index() + "]", clusterTask, clusterTask, executor, clusterTask); }	lets add a todo here to move this away from ackedclusterstateupdatetask and use a custom task object.
public void testCombinedFieldsQueryHighlight() throws IOException { XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("_doc") .startObject("properties") .startObject("field1") .field("type", "text") .field("index_options", "offsets") .field("term_vector", "with_positions_offsets") .endObject() .startObject("field2") .field("type", "text") .field("index_options", "offsets") .field("term_vector", "with_positions_offsets") .endObject() .endObject() .endObject().endObject(); assertAcked(prepareCreate("test").setMapping(mapping)); ensureGreen(); client().prepareIndex("test") .setSource("field1", "The quick brown fox jumps over", "field2", "The quick brown fox jumps over") .get(); refresh(); for (String highlighterType : ALL_TYPES) { CombinedFieldsQueryBuilder multiMatchQueryBuilder = combinedFieldsQuery("the quick brown fox", "field1", "field2"); SearchSourceBuilder source = searchSource() .query(multiMatchQueryBuilder) .highlighter(highlight() .highlighterType(highlighterType) .field(new Field("field1").requireFieldMatch(true).preTags("<field1>").postTags("</field1>"))); SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet(); assertHitCount(searchResponse, 1L); assertHighlight(searchResponse, 0, "field1", 0, anyOf(equalTo("<field1>The quick brown fox</field1> jumps over"), equalTo("<field1>The</field1> <field1>quick</field1> <field1>brown</field1> <field1>fox</field1> jumps over"))); } }	i think it can never be "<field1>the quick brown fox</field1> jumps over"? no? isn't this only applicable for phrase and phrase_prefix search?
@Override protected void extractUnknownQuery(Query query, Map<String, WeightedSpanTerm> terms) throws IOException { extractWeightedTerms(terms, query, 1F); }	can you explain why this has changed?
private void doFinish() { if (finished.compareAndSet(false, true)) { decrementCounter(indexShard.get()); final ShardId shardId = shardIt.shardId(); final ActionWriteResponse.ShardInfo.Failure[] failuresArray; if (!shardReplicaFailures.isEmpty()) { int slot = 0; failuresArray = new ActionWriteResponse.ShardInfo.Failure[shardReplicaFailures.size()]; for (Map.Entry<String, Throwable> entry : shardReplicaFailures.entrySet()) { RestStatus restStatus = ExceptionsHelper.status(entry.getValue()); failuresArray[slot++] = new ActionWriteResponse.ShardInfo.Failure( shardId.getIndex(), shardId.getId(), entry.getKey(), entry.getValue(), restStatus, false ); } } else { failuresArray = ActionWriteResponse.EMPTY; } finalResponse.setShardInfo(new ActionWriteResponse.ShardInfo( totalShards, success.get(), failuresArray ) ); listener.onResponse(finalResponse); } } } /** * Internal request class that gets built on each node. Holds the original request plus additional info. */ protected class InternalRequest { final Request request; String concreteIndex; InternalRequest(Request request) { this.request = request; } public Request request() { return request; } void concreteIndex(String concreteIndex) { this.concreteIndex = concreteIndex; } public String concreteIndex() { return concreteIndex; } } void decrementCounter(@Nullable IndexShard indexShard) { if (indexShard != null) { indexShard.decrementOperationCounter(); }	i think we can simplify this a bit and maybe have a class that does that ie this: java static class indexshardreference { private final atomicreference<indexshard> ref = new atomicreference(); void setreference(indexshard shard) { indexshard.incrementoperationcounter(); assert ref.get() == null; ref.set(indexshard); } void removereference() { indexshard shard = ref.get(); try { shard. decrementoperationcounter(); } finally { ref.set(null); } } } this seems cleaner? or do i miss something
private final EngineConfig newEngineConfig() { final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(mapperService, mapperAnalyzer, queryParserService, indexAliasesService, indexCache) { @Override protected void operationProcessed() { assert recoveryState != null; recoveryState.getTranslog().incrementRecoveredOperations(); } }; return new EngineConfig(shardId, threadPool, indexingService, indexSettingsService, warmer, store, deletionPolicy, translog, mergePolicyProvider, mergeScheduler, mapperAnalyzer, similarityService.similarity(), codecService, failedEngineListener, translogRecoveryPerformer); }	maybe we can log here that we are done with this shard?
@Override public void messageReceived(ClusterStateRequest request, TransportChannel channel, Task task) throws Exception { if (ignoreRequest.get()) { return; } DiscoveryNodes discoveryNodes = DiscoveryNodes.builder().add(transportService.getLocalDiscoNode()).build(); ClusterState build = ClusterState.builder(ClusterName.DEFAULT).nodes(discoveryNodes).build(); channel.sendResponse(new ClusterStateResponse(ClusterName.DEFAULT, build, 0L, false)); }	nit: maybe change the assertion message too like "... marked as busy" or so?
@Override @SuppressWarnings("unchecked") protected Map<String, Function<Map<String, Object>, Object>> pluginScripts() { Map<String, Function<Map<String, Object>, Object>> scripts = new HashMap<>(); scripts.put("1", vars -> 1.0d); scripts.put("get score value", vars -> ((ScoreAccessor) vars.get("_score")).doubleValue()); scripts.put("return (doc['num'].value)", vars -> { Map<?, ?> doc = (Map) vars.get("doc"); ScriptDocValues.Longs num = (ScriptDocValues.Longs) doc.get("num"); return num.getValue(); }); scripts.put("doc['random_score']", vars -> { Map<?, ?> doc = (Map) vars.get("doc"); ScriptDocValues.Doubles randomScore = (ScriptDocValues.Doubles) doc.get("random_score"); if (randomScore != null) {return randomScore.getValue();} else return 0; }); return scripts; } } public void testScriptScoresNested() throws IOException { createIndex(INDEX); index(INDEX, TYPE, "1", jsonBuilder().startObject().field("dummy_field", 1).endObject()); refresh(); Script scriptOne = new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "1", Collections.emptyMap()); Script scriptTwo = new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "get score value", Collections.emptyMap()); SearchResponse response = client().search( searchRequest().source( searchSource().query( functionScoreQuery( functionScoreQuery( functionScoreQuery(scriptFunction(scriptOne)), scriptFunction(scriptTwo)), scriptFunction(scriptTwo) ) ) ) ).actionGet(); assertSearchResponse(response); assertThat(response.getHits().getAt(0).getScore(), equalTo(1.0f)); } public void testScriptScoresWithAgg() throws IOException { createIndex(INDEX); index(INDEX, TYPE, "1", jsonBuilder().startObject().field("dummy_field", 1).endObject()); refresh(); Script script = new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "get score value", Collections.emptyMap()); SearchResponse response = client().search( searchRequest().source( searchSource() .query(functionScoreQuery(scriptFunction(script))) .aggregation(terms("score_agg").script(script)) ) ).actionGet(); assertSearchResponse(response); assertThat(response.getHits().getAt(0).getScore(), equalTo(1.0f)); assertThat(((Terms) response.getAggregations().asMap().get("score_agg")).getBuckets().get(0).getKeyAsString(), equalTo("1.0")); assertThat(((Terms) response.getAggregations().asMap().get("score_agg")).getBuckets().get(0).getDocCount(), is(1L)); } public void testMinScoreFunctionScoreBasic() throws IOException { index(INDEX, TYPE, jsonBuilder().startObject().field("num", 2).endObject()); refresh(); float score = randomFloat(); float minScore = randomFloat(); index(INDEX, TYPE, jsonBuilder().startObject() .field("num", 2) .field("random_score", score) // Pass the random score as a document field so that it can be extracted in the script .endObject()); refresh(); ensureYellow(); Script script = new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "doc['random_score']", Collections.emptyMap()); SearchResponse searchResponse = client().search( searchRequest().source(searchSource().query(functionScoreQuery(scriptFunction(script)).setMinScore(minScore))) ).actionGet(); if (score < minScore) { assertThat(searchResponse.getHits().getTotalHits(), is(0L)); } else { assertThat(searchResponse.getHits().getTotalHits(), is(1L)); } searchResponse = client().search( searchRequest().source(searchSource().query(functionScoreQuery(new MatchAllQueryBuilder(), new FilterFunctionBuilder[] { new FilterFunctionBuilder(scriptFunction(script)), new FilterFunctionBuilder(scriptFunction(script)) }).scoreMode(FunctionScoreQuery.ScoreMode.AVG).setMinScore(minScore))) ).actionGet(); if (score < minScore) { assertThat(searchResponse.getHits().getTotalHits(), is(0L)); } else { assertThat(searchResponse.getHits().getTotalHits(), is(1L)); } } public void testMinScoreFunctionScoreManyDocsAndRandomMinScore() throws IOException, ExecutionException, InterruptedException { List<IndexRequestBuilder> docs = new ArrayList<>(); int numDocs = randomIntBetween(1, 100); int scoreOffset = randomIntBetween(-2 * numDocs, 2 * numDocs); int minScore = randomIntBetween(-2 * numDocs, 2 * numDocs); for (int i = 0; i < numDocs; i++) { docs.add(client().prepareIndex(INDEX, TYPE, Integer.toString(i)).setSource("num", i + scoreOffset)); } indexRandom(true, docs); Script script = new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "return (doc['num'].value)", Collections.emptyMap()); int numMatchingDocs = numDocs + scoreOffset - minScore; if (numMatchingDocs < 0) { numMatchingDocs = 0; } if (numMatchingDocs > numDocs) { numMatchingDocs = numDocs; } SearchResponse searchResponse = client().search( searchRequest().source(searchSource().query(functionScoreQuery(scriptFunction(script)) .setMinScore(minScore)).size(numDocs))).actionGet(); assertMinScoreSearchResponses(numDocs, searchResponse, numMatchingDocs); searchResponse = client().search( searchRequest().source(searchSource().query(functionScoreQuery(new MatchAllQueryBuilder(), new FilterFunctionBuilder[] { new FilterFunctionBuilder(scriptFunction(script)), new FilterFunctionBuilder(scriptFunction(script)) }).scoreMode(FunctionScoreQuery.ScoreMode.AVG).setMinScore(minScore)).size(numDocs))).actionGet(); assertMinScoreSearchResponses(numDocs, searchResponse, numMatchingDocs); } protected void assertMinScoreSearchResponses(int numDocs, SearchResponse searchResponse, int numMatchingDocs) { assertSearchResponse(searchResponse); assertThat((int) searchResponse.getHits().getTotalHits(), is(numMatchingDocs)); int pos = 0; for (int hitId = numDocs - 1; (numDocs - hitId) < searchResponse.getHits().getTotalHits(); hitId--) { assertThat(searchResponse.getHits().getAt(pos).getId(), equalTo(Integer.toString(hitId))); pos++; } } /** make sure min_score works if functions is empty, see https://github.com/elastic/elasticsearch/issues/10253 */ public void testWithEmptyFunctions() throws IOException, ExecutionException, InterruptedException { assertAcked(prepareCreate("test")); index("test", "testtype", "1", jsonBuilder().startObject().field("text", "test text").endObject()); refresh(); SearchResponse termQuery = client().search( searchRequest().source( searchSource().explain(true).query( termQuery("text", "text")))).get(); assertSearchResponse(termQuery); assertThat(termQuery.getHits().getTotalHits(), equalTo(1L)); float termQueryScore = termQuery.getHits().getAt(0).getScore(); for (CombineFunction combineFunction : CombineFunction.values()) { testMinScoreApplied(combineFunction, termQueryScore); } }	if this is going to be on a single line, please use a ternary statement. the same applies to the other tests here that were changed in a similar way.
public void testBinaryFieldWithNoDocValues() { assertEquals("1:31: Binary field [binary] cannot be used for filtering unless it has the doc_values setting enabled", error("SELECT binary FROM test WHERE binary IS NOT NULL")); assertEquals("1:34: Binary field [binary] cannot be used in aggregations unless it has the doc_values setting enabled", error("SELECT binary FROM test GROUP BY binary")); assertEquals("1:45: Binary field [binary] cannot be used for filtering unless it has the doc_values setting enabled", error("SELECT count(binary) FROM test HAVING count(binary) > 1")); assertEquals("1:34: Binary field [binary] cannot be used for ordering unless it has the doc_values setting enabled", error("SELECT binary FROM test ORDER BY binary")); }	these are tested in the rest tests too. not sure why there and not here, but wondering if it'd be worth removing them from there, dunno if it's worth having them in both places.
public void testBinaryFieldWithNoDocValues() { assertEquals("1:31: Binary field [binary] cannot be used for filtering unless it has the doc_values setting enabled", error("SELECT binary FROM test WHERE binary IS NOT NULL")); assertEquals("1:34: Binary field [binary] cannot be used in aggregations unless it has the doc_values setting enabled", error("SELECT binary FROM test GROUP BY binary")); assertEquals("1:45: Binary field [binary] cannot be used for filtering unless it has the doc_values setting enabled", error("SELECT count(binary) FROM test HAVING count(binary) > 1")); assertEquals("1:34: Binary field [binary] cannot be used for ordering unless it has the doc_values setting enabled", error("SELECT binary FROM test ORDER BY binary")); }	this message is rather confusing... score() is not in a function in the query...
public void testWhereOnNested() { assertEquals("1:33: WHERE isn't (yet) compatible with scalar functions on nested fields [dep.start_date]", error("SELECT int FROM test WHERE YEAR(dep.start_date) + 10 > 0")); assertEquals("1:13: WHERE isn't (yet) compatible with scalar functions on nested fields [dep.start_date]", error("SELECT YEAR(dep.start_date) + 10 AS a FROM test WHERE int > 10 AND (int < 3 OR NOT (a > 5))")); accept("SELECT int FROM test WHERE dep.start_date > '2020-01-30'::date AND (int > 10 OR dep.end_date IS NULL)"); accept("SELECT int FROM test WHERE dep.start_date > '2020-01-30'::date AND (int > 10 OR dep.end_date IS NULL) " + "OR NOT(dep.start_date >= '2020-01-01')"); String operator = randomFrom("<", "<="); assertEquals("1:46: WHERE isn't (yet) compatible with scalar functions on nested fields [dep.location]", error("SELECT geo_shape FROM test " + "WHERE ST_Distance(dep.location, ST_WKTToSQL('point (10 20)')) " + operator + " 25")); }	any reason for these changes? guessing that we'd change, but not extend the coverage by it?
* @param stepInfo the new step info to update * @return Updated cluster state with <code>stepInfo</code> if changed, otherwise the same cluster state * if no changes to step info exist * @throws IOException if parsing step info fails */ static ClusterState addStepInfoToClusterState(Index index, ClusterState clusterState, ToXContentObject stepInfo) throws IOException { IndexMetaData idxMeta = clusterState.getMetaData().index(index); XContentBuilder infoXContentBuilder = JsonXContent.contentBuilder(); stepInfo.toXContent(infoXContentBuilder, ToXContent.EMPTY_PARAMS); String stepInfoString = BytesReference.bytes(infoXContentBuilder).utf8ToString(); if (stepInfoString.equals(LifecycleSettings.LIFECYCLE_STEP_INFO_SETTING.get(idxMeta.getSettings()))) { return clusterState; } Settings.Builder indexSettings = Settings.builder().put(idxMeta.getSettings()) .put(LifecycleSettings.LIFECYCLE_STEP_INFO_SETTING.getKey(), stepInfoString); ClusterState.Builder newClusterStateBuilder = newClusterStateWithIndexSettings(index, clusterState, indexSettings); return newClusterStateBuilder.build(); }	i know this isn't part of this, but while you're in here, can you put this in a try-with-resources block?
public void testCreateAndDeleteIndexConcurrently() throws InterruptedException { createIndex("test"); final AtomicInteger indexVersion = new AtomicInteger(0); final Object indexVersionLock = new Object(); final CountDownLatch latch = new CountDownLatch(1); int numDocs = randomIntBetween(1, 10); for (int i = 0; i < numDocs; i++) { client().prepareIndex("test", "test").setSource("index_version", indexVersion.get()).get(); } synchronized (indexVersionLock) { // not necessarily needed here but for completeness we lock here too indexVersion.incrementAndGet(); } client().admin().indices().prepareDelete("test").execute(new ActionListener<DeleteIndexResponse>() { // this happens async!!! @Override public void onResponse(DeleteIndexResponse deleteIndexResponse) { Thread thread = new Thread() { @Override public void run() { try { client().prepareIndex("test", "test").setSource("index_version", indexVersion.get()).get(); // recreate that index synchronized (indexVersionLock) { // we sync here since we have to ensure that all indexing operations below for a given ID are done before we increment the // index version otherwise a doc that is in-flight could make it into an index that it was supposed to be deleted for and our assertion fail... indexVersion.incrementAndGet(); } assertAcked(client().admin().indices().prepareDelete("test").get()); // from here on all docs with index_version == 0|1 must be gone!!!! only 2 are ok; } finally { latch.countDown(); } } }; thread.start(); } @Override public void onFailure(Throwable e) { throw new RuntimeException(e); } } ); numDocs = randomIntBetween(100, 200); for (int i = 0; i < numDocs; i++) { try { synchronized (indexVersionLock) { client().prepareIndex("test", "test").setSource("index_version", indexVersion.get()).setTimeout(TimeValue.timeValueSeconds(10)).get(); } } catch (IndexNotFoundException inf) { // fine index has been deleted } catch (ProcessClusterEventTimeoutException ex) { // fine request timed out since we where waiting for a shard and maybe index was deleted? } catch (UnavailableShardsException ex) { assertEquals(ex.getCause().getClass(), IndexNotFoundException.class); // fine we run into a delete index while retrying } } latch.await(); refresh(); // we only really assert that we never reuse segments of old indices or anything like this here and that nothing fails with crazy exceptions SearchResponse expected = client().prepareSearch("test").setIndicesOptions(IndicesOptions.lenientExpandOpen()).setQuery(new RangeQueryBuilder("index_version").from(indexVersion.get(), true)).get(); SearchResponse all = client().prepareSearch("test").setIndicesOptions(IndicesOptions.lenientExpandOpen()).get(); assertEquals(expected + " vs. " + all, expected.getHits().getTotalHits(), all.getHits().getTotalHits()); logger.info("total: {}", expected.getHits().getTotalHits()); }	in what cases do you we expect a custer event timeout due to index deletion? we do have #14932 , but that's a valid issue?
public void testCreateAndDeleteIndexConcurrently() throws InterruptedException { createIndex("test"); final AtomicInteger indexVersion = new AtomicInteger(0); final Object indexVersionLock = new Object(); final CountDownLatch latch = new CountDownLatch(1); int numDocs = randomIntBetween(1, 10); for (int i = 0; i < numDocs; i++) { client().prepareIndex("test", "test").setSource("index_version", indexVersion.get()).get(); } synchronized (indexVersionLock) { // not necessarily needed here but for completeness we lock here too indexVersion.incrementAndGet(); } client().admin().indices().prepareDelete("test").execute(new ActionListener<DeleteIndexResponse>() { // this happens async!!! @Override public void onResponse(DeleteIndexResponse deleteIndexResponse) { Thread thread = new Thread() { @Override public void run() { try { client().prepareIndex("test", "test").setSource("index_version", indexVersion.get()).get(); // recreate that index synchronized (indexVersionLock) { // we sync here since we have to ensure that all indexing operations below for a given ID are done before we increment the // index version otherwise a doc that is in-flight could make it into an index that it was supposed to be deleted for and our assertion fail... indexVersion.incrementAndGet(); } assertAcked(client().admin().indices().prepareDelete("test").get()); // from here on all docs with index_version == 0|1 must be gone!!!! only 2 are ok; } finally { latch.countDown(); } } }; thread.start(); } @Override public void onFailure(Throwable e) { throw new RuntimeException(e); } } ); numDocs = randomIntBetween(100, 200); for (int i = 0; i < numDocs; i++) { try { synchronized (indexVersionLock) { client().prepareIndex("test", "test").setSource("index_version", indexVersion.get()).setTimeout(TimeValue.timeValueSeconds(10)).get(); } } catch (IndexNotFoundException inf) { // fine index has been deleted } catch (ProcessClusterEventTimeoutException ex) { // fine request timed out since we where waiting for a shard and maybe index was deleted? } catch (UnavailableShardsException ex) { assertEquals(ex.getCause().getClass(), IndexNotFoundException.class); // fine we run into a delete index while retrying } } latch.await(); refresh(); // we only really assert that we never reuse segments of old indices or anything like this here and that nothing fails with crazy exceptions SearchResponse expected = client().prepareSearch("test").setIndicesOptions(IndicesOptions.lenientExpandOpen()).setQuery(new RangeQueryBuilder("index_version").from(indexVersion.get(), true)).get(); SearchResponse all = client().prepareSearch("test").setIndicesOptions(IndicesOptions.lenientExpandOpen()).get(); assertEquals(expected + " vs. " + all, expected.getHits().getTotalHits(), all.getHits().getTotalHits()); logger.info("total: {}", expected.getHits().getTotalHits()); }	maybe this should throw the indexnotfoundexception instead?
static Request getMappings(GetMappingsRequest getMappingsRequest) { String[] indices = getMappingsRequest.indices() == null ? Strings.EMPTY_ARRAY : getMappingsRequest.indices(); Request request = new Request(HttpGet.METHOD_NAME, RequestConverters.endpoint(indices, "_mapping")); RequestConverters.Params parameters = new RequestConverters.Params(request); parameters.withMasterTimeout(getMappingsRequest.masterNodeTimeout()); parameters.withIndicesOptions(getMappingsRequest.indicesOptions()); parameters.withLocal(getMappingsRequest.local()); return request; }	maybe add @deprecated even if we only use this internally. it makes it more obvious and serves as a grep-able reminder that we can remove this in 8.0.
private IndexRequest createRequest(String docId, ToXContentObject body) { try (XContentBuilder builder = XContentFactory.jsonBuilder()) { XContentBuilder source = body.toXContent(builder, FOR_INTERNAL_STORAGE_PARAMS); return new IndexRequest() .opType(DocWriteRequest.OpType.CREATE) .id(docId) .source(source); } catch (IOException ex) { // This should never happen. If we were able to deserialize the object (from Native or REST) and then fail to serialize it again // that is not the users fault. We did something wrong and should throw. throw new ElasticsearchStatusException("Unexpected serialization exception for [{}]", RestStatus.INTERNAL_SERVER_ERROR, ex, docId); } }	nit: could use exceptionshelper.servererror. not necessary but for future reference.
public IngestDocument execute(IngestDocument document) { IngestScript.Factory factory = scriptService.compile(script, IngestScript.CONTEXT); factory.newInstance(script.getParams()).execute(new DeprecationMap(document.getSourceAndMetadata(), DEPRECATIONS)); CollectionUtils.ensureNoSelfReferences(document.getSourceAndMetadata(), "ingest script"); return document; }	i noticed that deprecationmap uses deprecated instead of deprecatedandmaybelog. is this something we might want to change?
public static CompletionSuggestion reduceTo(List<Suggest.Suggestion<Entry>> toReduce) { if (toReduce.isEmpty()) { return null; } else { final CompletionSuggestion leader = (CompletionSuggestion) toReduce.get(0); final Entry leaderEntry = leader.getEntries().get(0); final String name = leader.getName(); int size = leader.getSize(); if (toReduce.size() == 1) { return leader; } else { // combine suggestion entries from participating shards on the coordinating node // the global top <code>size</code> entries are collected from the shard results // using a priority queue PriorityQueue<SuggestionRef> pq = new PriorityQueue(leader.getSize()); for (Suggest.Suggestion<Entry> suggestion : toReduce) { assert suggestion.getName().equals(name) : "name should be identical across all suggestions"; Iterator<Entry.Option> it = ((CompletionSuggestion) suggestion).getOptions().iterator(); if (it.hasNext()) { pq.add(new SuggestionRef(it)); } } // Dedup duplicate suggestions (based on the surface form) if skip duplicates is activated final CharArraySet seenSurfaceForms = leader.skipDuplicates ? new CharArraySet(leader.getSize(), false) : null; final Entry entry = new Entry(leaderEntry.getText(), leaderEntry.getOffset(), leaderEntry.getLength()); final List<Entry.Option> options = entry.getOptions(); while (pq.isEmpty() == false) { final SuggestionRef ref = pq.poll(); final Entry.Option current = ref.current; if (ref.hasNext()) { ref.next(); pq.add(ref); } if (leader.skipDuplicates && seenSurfaceForms.add(current.getText().toString()) == false) { continue; } options.add(current); if (options.size() >= size) { break; } } final CompletionSuggestion suggestion = new CompletionSuggestion(leader.getName(), leader.getSize(), leader.skipDuplicates); suggestion.addTerm(entry); return suggestion; } } }	would you mind adding <> after new priorityqueue ? otherwise this is an unchecked assignment.
public static CompletionSuggestion reduceTo(List<Suggest.Suggestion<Entry>> toReduce) { if (toReduce.isEmpty()) { return null; } else { final CompletionSuggestion leader = (CompletionSuggestion) toReduce.get(0); final Entry leaderEntry = leader.getEntries().get(0); final String name = leader.getName(); int size = leader.getSize(); if (toReduce.size() == 1) { return leader; } else { // combine suggestion entries from participating shards on the coordinating node // the global top <code>size</code> entries are collected from the shard results // using a priority queue PriorityQueue<SuggestionRef> pq = new PriorityQueue(leader.getSize()); for (Suggest.Suggestion<Entry> suggestion : toReduce) { assert suggestion.getName().equals(name) : "name should be identical across all suggestions"; Iterator<Entry.Option> it = ((CompletionSuggestion) suggestion).getOptions().iterator(); if (it.hasNext()) { pq.add(new SuggestionRef(it)); } } // Dedup duplicate suggestions (based on the surface form) if skip duplicates is activated final CharArraySet seenSurfaceForms = leader.skipDuplicates ? new CharArraySet(leader.getSize(), false) : null; final Entry entry = new Entry(leaderEntry.getText(), leaderEntry.getOffset(), leaderEntry.getLength()); final List<Entry.Option> options = entry.getOptions(); while (pq.isEmpty() == false) { final SuggestionRef ref = pq.poll(); final Entry.Option current = ref.current; if (ref.hasNext()) { ref.next(); pq.add(ref); } if (leader.skipDuplicates && seenSurfaceForms.add(current.getText().toString()) == false) { continue; } options.add(current); if (options.size() >= size) { break; } } final CompletionSuggestion suggestion = new CompletionSuggestion(leader.getName(), leader.getSize(), leader.skipDuplicates); suggestion.addTerm(entry); return suggestion; } } }	long comment, sorry :) i initially found the suggestionref class hard to read, as well as the way it's used within a priority queue and the fact that it implements both iterator and comparable which i find confusing at first glance. i was about to propose something like the following, that i find easier to reason about as all the options are added to the priority queue and then eventually skipped if needed: priorityqueue<entry.option> priorityqueue = new priorityqueue<>(comparator); for (suggest.suggestion<entry> suggestion : toreduce) { assert suggestion.getname().equals(name) : "name should be identical across all suggestions"; assert leader.skipduplicates == ((completionsuggestion) suggestion).skipduplicates; priorityqueue.addall(((completionsuggestion) suggestion).getoptions()); } final completionsuggestion suggestion = new completionsuggestion(leader.getname(), leader.getsize(), leader.skipduplicates); final entry entry = new entry(leaderentry.gettext(), leaderentry.getoffset(), leaderentry.getlength()); final chararrayset seensurfaceforms = leader.skipduplicates ? new chararrayset(leader.getsize(), false) : null; list<entry.option> options = entry.getoptions(); while(priorityqueue.size() > 0) { entry.option current = priorityqueue.poll(); if (leader.skipduplicates == false || seensurfaceforms.add(current.gettext().tostring())) { options.add(current); if (options.size() >= leader.getsize()) { break; } } } suggestion.addterm(entry); return suggestion; there are performance implications with this though as we add all the options to the queue (though add is o log(n) ). and we no longer take advantage of the fact that each shard won't return duplicates. i then played a bit and came up with the following that is very similar to what you have: private static final class optionpriorityqueue extends org.apache.lucene.util.priorityqueue<shardoptions> { optionpriorityqueue(int maxsize) { super(maxsize); } @override protected boolean lessthan(shardoptions a, shardoptions b) { return comparator.compare(a.current, b.current) < 0; } } private static class shardoptions { final iterator<entry.option> optionsiterator; entry.option current; private shardoptions(iterator<entry.option> optionsiterator) { assert optionsiterator.hasnext(); this.optionsiterator = optionsiterator; this.current = optionsiterator.next(); } boolean advancetonextoption() { if (optionsiterator.hasnext()) { current = optionsiterator.next(); return true; } else { return false; } } } optionpriorityqueue pq = new optionpriorityqueue(toreduce.size()); for (suggest.suggestion<entry> suggestion : toreduce) { assert suggestion.getname().equals(name) : "name should be identical across all suggestions"; iterator<entry.option> it = ((completionsuggestion) suggestion).getoptions().iterator(); if (it.hasnext()) { pq.add(new shardoptions(it)); } } // dedup duplicate suggestions (based on the surface form) if skip duplicates is activated final chararrayset seensurfaceforms = leader.skipduplicates ? new chararrayset(leader.getsize(), false) : null; final entry entry = new entry(leaderentry.gettext(), leaderentry.getoffset(), leaderentry.getlength()); final list<entry.option> options = entry.getoptions(); while (pq.size() > 0) { shardoptions top = pq.top(); entry.option current = top.current; if (top.advancetonextoption()) { pq.updatetop(); } else { //options exhausted for this shard pq.pop(); } if (leader.skipduplicates && seensurfaceforms.add(current.gettext().tostring()) == false) { continue; } options.add(current); if (options.size() >= size) { break; } } final completionsuggestion suggestion = new completionsuggestion(leader.getname(), leader.getsize(), leader.skipduplicates); suggestion.addterm(entry); return suggestion; i think that not implementing iterator and comparable and renaming suggestionref to shardoptions make things clearer (at least for me, it may be subjective!). using lucene's priorityqueue allows us to use updatetop without having to poll and add back each item, which should make things faster.
public static CompletionSuggestion reduceTo(List<Suggest.Suggestion<Entry>> toReduce) { if (toReduce.isEmpty()) { return null; } else { final CompletionSuggestion leader = (CompletionSuggestion) toReduce.get(0); final Entry leaderEntry = leader.getEntries().get(0); final String name = leader.getName(); int size = leader.getSize(); if (toReduce.size() == 1) { return leader; } else { // combine suggestion entries from participating shards on the coordinating node // the global top <code>size</code> entries are collected from the shard results // using a priority queue PriorityQueue<SuggestionRef> pq = new PriorityQueue(leader.getSize()); for (Suggest.Suggestion<Entry> suggestion : toReduce) { assert suggestion.getName().equals(name) : "name should be identical across all suggestions"; Iterator<Entry.Option> it = ((CompletionSuggestion) suggestion).getOptions().iterator(); if (it.hasNext()) { pq.add(new SuggestionRef(it)); } } // Dedup duplicate suggestions (based on the surface form) if skip duplicates is activated final CharArraySet seenSurfaceForms = leader.skipDuplicates ? new CharArraySet(leader.getSize(), false) : null; final Entry entry = new Entry(leaderEntry.getText(), leaderEntry.getOffset(), leaderEntry.getLength()); final List<Entry.Option> options = entry.getOptions(); while (pq.isEmpty() == false) { final SuggestionRef ref = pq.poll(); final Entry.Option current = ref.current; if (ref.hasNext()) { ref.next(); pq.add(ref); } if (leader.skipDuplicates && seenSurfaceForms.add(current.getText().toString()) == false) { continue; } options.add(current); if (options.size() >= size) { break; } } final CompletionSuggestion suggestion = new CompletionSuggestion(leader.getName(), leader.getSize(), leader.skipDuplicates); suggestion.addTerm(entry); return suggestion; } } }	minor nit: for readability, i would rather do if (leader.skipduplicates == false || seensurfaceforms.add(current.gettext().tostring())) { options.add(current); if (options.size() >= size) { break; } } that makes the if with continue not needed as we are at the end of the while block.
public void testToReduceWithDuplicates() { List<Suggest.Suggestion<CompletionSuggestion.Entry>> shardSuggestions = new ArrayList<>(); int nShards = randomIntBetween(2, 10); String name = randomAlphaOfLength(10); int size = randomIntBetween(10, 100); int totalResults = size * nShards; int numSurfaceForms = randomIntBetween(1, size); String[] surfaceForms = new String[numSurfaceForms]; for (int i = 0; i < numSurfaceForms; i++) { surfaceForms[i] = randomAlphaOfLength(20); } List<CompletionSuggestion.Entry.Option> options = new ArrayList<>(); for (int i = 0; i < nShards; i++) { CompletionSuggestion suggestion = new CompletionSuggestion(name, size, true); CompletionSuggestion.Entry entry = new CompletionSuggestion.Entry(new Text(""), 0, 0); suggestion.addTerm(entry); int maxScore = randomIntBetween(totalResults, totalResults*2); for (int j = 0; j < size; j++) { String surfaceForm = randomFrom(surfaceForms); CompletionSuggestion.Entry.Option newOption = new CompletionSuggestion.Entry.Option(j, new Text(surfaceForm), maxScore - j, Collections.emptyMap()); entry.addOption(newOption); options.add(newOption); } shardSuggestions.add(suggestion); } List<CompletionSuggestion.Entry.Option> expected = options.stream() .sorted((o1, o2) -> COMPARATOR.compare(o1, o2)) .distinct() .limit(size) .collect(Collectors.toList()); CompletionSuggestion reducedSuggestion = CompletionSuggestion.reduceTo(shardSuggestions); assertNotNull(reducedSuggestion); assertThat(reducedSuggestion.getOptions().size(), lessThanOrEqualTo(size)); assertEquals(expected, reducedSuggestion.getOptions()); }	the lambda can be replaced with comparator ?
private SeqNoStats loadSeqNoStats(EngineConfig.OpenMode openMode) throws IOException { final SeqNoStats seqNoStats; switch (openMode) { case OPEN_INDEX_AND_TRANSLOG: final long globalCheckpoint = Translog.readGlobalCheckpoint(engineConfig.getTranslogConfig().getTranslogPath()); seqNoStats = store.loadSeqNoStats(globalCheckpoint); break; case OPEN_INDEX_CREATE_TRANSLOG: seqNoStats = store.loadSeqNoStats(SequenceNumbers.UNASSIGNED_SEQ_NO); break; case CREATE_INDEX_AND_TRANSLOG: seqNoStats = new SeqNoStats( SequenceNumbers.NO_OPS_PERFORMED, SequenceNumbers.NO_OPS_PERFORMED, SequenceNumbers.UNASSIGNED_SEQ_NO); break; default: throw new IllegalArgumentException(openMode.toString()); } return seqNoStats; }	i'd just return store.loadseqnostats() here rather than breaking and returning at the bottom. not sure if that's against our style tho?
public void testSimpleCommit() throws IOException { final int operations = randomIntBetween(1, 4096); long seqNo = 0; for (int i = 0; i < operations; i++) { translog.add(new Translog.NoOp(seqNo++, 0, "test'")); if (rarely()) { translog.rollGeneration(); } } long lastGen = randomLongBetween(1, translog.currentFileGeneration()); commit(translog, randomLongBetween(1, lastGen), lastGen); }	nit: } should be on its own line
public void testRecoveryWithOutOfOrderDelete() throws Exception { try (ReplicationGroup shards = createGroup(1)) { shards.startAll(); // create out of order delete and index op on replica final IndexShard orgReplica = shards.getReplicas().get(0); orgReplica.applyDeleteOperationOnReplica(1, 2, "type", "id", VersionType.EXTERNAL, u -> {}); orgReplica.getTranslog().rollGeneration(); // isolate the delete in it's own generation orgReplica.applyIndexOperationOnReplica(0, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, SourceToParse.source(orgReplica.shardId().getIndexName(), "type", "id", new BytesArray("{}"), XContentType.JSON), u -> {}); // index a second item into the second generation, skipping seq# 2. Local checkpoint is now 1, which will make this generation // stick around orgReplica.applyIndexOperationOnReplica(3, 1, VersionType.EXTERNAL, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, SourceToParse.source(orgReplica.shardId().getIndexName(), "type", "id2", new BytesArray("{}"), XContentType.JSON), u -> {}); final int translogOps = 4; // 3 ops + seqno gaps if (randomBoolean()) { flushShard(orgReplica); } final IndexShard orgPrimary = shards.getPrimary(); shards.promoteReplicaToPrimary(orgReplica).get(); // wait for primary/replica sync to make sure seq# gap is closed. IndexShard newReplica = shards.addReplicaWithExistingPath(orgPrimary.shardPath(), orgPrimary.routingEntry().currentNodeId()); shards.recoverReplica(newReplica); shards.assertAllEqual(1); assertThat(newReplica.getTranslog().totalOperations(), equalTo(translogOps)); } }	this removes the tests for the cases where we've updated the index settings. is that ok?
@Override public Collection<Object> createComponents( final Client client, final ClusterService clusterService, final ThreadPool threadPool, final ResourceWatcherService resourceWatcherService, final ScriptService scriptService, final NamedXContentRegistry xContentRegistry, final Environment environment, final NodeEnvironment nodeEnvironment, final NamedWriteableRegistry registry, final IndexNameExpressionResolver resolver, final Supplier<RepositoriesService> repositoriesServiceSupplier ) { final List<Object> components = new ArrayList<>(); this.repositoriesServiceSupplier = repositoriesServiceSupplier; this.threadPool.set(threadPool); this.failShardsListener.set(new FailShardsOnInvalidLicenseClusterListener(getLicenseState(), clusterService.getRerouteService())); if (DiscoveryNode.canContainData(settings)) { final CacheService cacheService = new CacheService(settings, clusterService, threadPool, new PersistentCache(nodeEnvironment)); this.cacheService.set(cacheService); final FrozenCacheService frozenCacheService = new FrozenCacheService(nodeEnvironment, settings, threadPool); this.frozenCacheService.set(frozenCacheService); components.add(cacheService); final BlobStoreCacheService blobStoreCacheService = new BlobStoreCacheService( clusterService, client, SNAPSHOT_BLOB_CACHE_INDEX, threadPool::absoluteTimeInMillis ); this.blobStoreCacheService.set(blobStoreCacheService); components.add(blobStoreCacheService); } else { PersistentCache.cleanUp(settings, nodeEnvironment); } this.allocator.set(new SearchableSnapshotAllocator(client, clusterService.getRerouteService(), frozenCacheInfoService)); components.add(new FrozenCacheServiceSupplier(frozenCacheService.get())); components.add(new CacheServiceSupplier(cacheService.get())); new SearchableSnapshotIndexMetadataUpgrader(clusterService, threadPool).initialize(); clusterService.addListener(new RepositoryUuidWatcher(clusterService.getRerouteService())); return Collections.unmodifiableList(components); }	maybe register the listener only on master eligible nodes?
public void testPositionIncrement() throws IOException { Analyzer analyzer = new Analyzer() { @Override protected TokenStreamComponents createComponents(String fieldName) { Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false); TokenFilter filters = new EdgeNGramTokenFilter(tokenizer, 1, 2, false); filters = new UniqueTokenFilter(filters); return new TokenStreamComponents(tokenizer, filters); } }; assertAnalyzesTo(analyzer, "foo bar bro", new String[]{"f", "fo", "b", "ba", "br"}, new int[]{0, 0, 4, 4, 8}, new int[]{3, 3, 7, 7, 11}, new int[]{1, 0, 1, 0, 1}); analyzer.close(); }	could we also include multiple skipped tokens in here? e.g. test for "foo bar bro bar bro baz"
protected void commitIndexWriter(final IndexWriter writer, final Translog translog) throws IOException { ensureCanFlush(); try { final long localCheckpoint = localCheckpointTracker.getProcessedCheckpoint(); writer.setLiveCommitData(() -> { /* * The user data captured above (e.g. local checkpoint) contains data that must be evaluated *before* Lucene flushes * segments, including the local checkpoint amongst other values. The maximum sequence number is different, we never want * the maximum sequence number to be less than the last sequence number to go into a Lucene commit, otherwise we run the * risk of re-using a sequence number for two different documents when restoring from this commit point and subsequently * writing new documents to the index. Since we only know which Lucene documents made it into the final commit after the * {@link IndexWriter#commit()} call flushes all documents, we defer computation of the maximum sequence number to the time * of invocation of the commit data iterator (which occurs after all documents have been flushed to Lucene). */ final Map<String, String> commitData = new HashMap<>(7); commitData.put(Translog.TRANSLOG_UUID_KEY, translog.getTranslogUUID()); commitData.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(localCheckpoint)); commitData.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(localCheckpointTracker.getMaxSeqNo())); commitData.put(MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, Long.toString(maxUnsafeAutoIdTimestamp.get())); commitData.put(HISTORY_UUID_KEY, historyUUID); final String currentForceMergeUUID = forceMergeUUID; if (currentForceMergeUUID != null) { commitData.put(FORCE_MERGE_UUID_KEY, currentForceMergeUUID); } commitData.put(Engine.MIN_RETAINED_SEQNO, Long.toString(softDeletesPolicy.getMinRetainedSeqNo())); commitData.put(ES_VERSION, Version.CURRENT.toString()); logger.trace("committing writer with commit data [{}]", commitData); return commitData.entrySet().iterator(); }); shouldPeriodicallyFlushAfterBigMerge.set(false); writer.commit(); } catch (final Exception ex) { try { failEngine("lucene commit failed", ex); } catch (final Exception inner) { ex.addSuppressed(inner); } throw ex; } catch (final AssertionError e) { /* * If assertions are enabled, IndexWriter throws AssertionError on commit if any files don't exist, but tests that randomly * throw FileNotFoundException or NoSuchFileException can also hit this. */ if (ExceptionsHelper.stackTrace(e).contains("org.apache.lucene.index.IndexWriter.filesExist")) { final EngineException engineException = new EngineException(shardId, "failed to commit engine", e); try { failEngine("lucene commit failed", engineException); } catch (final Exception inner) { engineException.addSuppressed(inner); } throw engineException; } else { throw e; } } }	i think we need to add this in a few other places like store.trimunsafecommits and storerecovery.addindices? can we perhaps add an assertion about the previous version being before or on current version? and maybe even add an assertion about this being filled in if lucene version is new enough? we can at least keep that in master though we may have to remove it in 7.x depending on whether a lucene version upgrade happens for the release where this lands.
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeLong(recoveryId); shardId.writeTo(out); out.writeVInt(phase1FileNames.size()); for (String phase1FileName : phase1FileNames) { out.writeString(phase1FileName); } out.writeVInt(phase1FileSizes.size()); for (Long phase1FileSize : phase1FileSizes) { out.writeVLong(phase1FileSize); } out.writeVInt(phase1ExistingFileNames.size()); for (String phase1ExistingFileName : phase1ExistingFileNames) { out.writeString(phase1ExistingFileName); } out.writeVInt(phase1ExistingFileSizes.size()); for (Long phase1ExistingFileSize : phase1ExistingFileSizes) { out.writeVLong(phase1ExistingFileSize); } out.writeVInt(totalTranslogOps); if (out.getVersion().onOrAfter(RecoverySettings.SEQ_NO_SNAPSHOT_RECOVERIES_SUPPORTED_VERSION)) { out.writeBoolean(deleteRecoveredFiles); } }	i think we can add following? suggestion } else { assert false : "target of recovery must be at least same version as source"; }
@Override public void receiveFileInfo(List<String> phase1FileNames, List<Long> phase1FileSizes, List<String> phase1ExistingFileNames, List<Long> phase1ExistingFileSizes, int totalTranslogOps, boolean deleteRecoveredFiles, ActionListener<Void> listener) { ActionListener.completeWith(listener, () -> { if (deleteRecoveredFiles) { multiFileWriter.deleteTempFiles(); } indexShard.resetRecoveryStage(); indexShard.prepareForIndexRecovery(); final RecoveryState.Index index = state().getIndex(); for (int i = 0; i < phase1ExistingFileNames.size(); i++) { index.addFileDetail(phase1ExistingFileNames.get(i), phase1ExistingFileSizes.get(i), true); } for (int i = 0; i < phase1FileNames.size(); i++) { index.addFileDetail(phase1FileNames.get(i), phase1FileSizes.get(i), false); } index.setFileDetailsComplete(); state().getTranslog().totalOperations(totalTranslogOps); state().getTranslog().totalOperationsOnStart(totalTranslogOps); return null; }); }	i wonder if this would not even work without the deleterecoveredfiles flag? it seems logical that if the target receives a new file info, it should clean up any temp files already downloaded. and initially, there simply are none.
private Optional<ShardSnapshot> fetchSnapshotFiles(GetShardSnapshotResponse shardSnapshotResponse) { assert Thread.currentThread().getName().contains(ThreadPool.Names.GENERIC); final Optional<ShardSnapshotInfo> latestShardSnapshotOpt = shardSnapshotResponse.getLatestShardSnapshot(); if (latestShardSnapshotOpt.isEmpty()) { return Optional.empty(); } final ShardSnapshotInfo latestShardSnapshot = latestShardSnapshotOpt.get(); try { final Snapshot snapshot = latestShardSnapshot.getSnapshot(); final Repository repository = repositoriesService.repository(snapshot.getRepository()); if (repository instanceof BlobStoreRepository == false) { return Optional.empty(); } BlobStoreRepository blobStoreRepository = (BlobStoreRepository) repository; BlobContainer blobContainer = blobStoreRepository.shardContainer(latestShardSnapshot.getIndexId(), latestShardSnapshot.getShardId().getId()); BlobStoreIndexShardSnapshot blobStoreIndexShardSnapshot = blobStoreRepository.loadShardSnapshot(blobContainer, snapshot.getSnapshotId()); Map<String, StoreFileMetadata> snapshotFiles = blobStoreIndexShardSnapshot.indexFiles() .stream() .map(BlobStoreIndexShardSnapshot.FileInfo::metadata) .collect(Collectors.toMap(StoreFileMetadata::name, Function.identity())); InMemoryDirectory directory = new InMemoryDirectory(snapshotFiles); SegmentInfos segmentCommitInfos = Lucene.readSegmentInfos(directory); Map<String, String> userData = segmentCommitInfos.userData; return Optional.of(new ShardSnapshot(latestShardSnapshot, blobStoreIndexShardSnapshot.indexFiles(), userData)); } catch (Exception e) { logger.warn(new ParameterizedMessage("Unable to fetch shard snapshot files for {}", latestShardSnapshot), e); return Optional.empty(); } }	in the (albeit edge) case that this runs on an older version than the replica and the snapshot is taken on the newer version, this no longer allows file-comparison based recover from snapshot. i think that is ok, but also that it deserves a comment here on this and why it is unlikely to be an issue.
private ShardRecoveryPlan computeRecoveryPlanWithSnapshots(@Nullable String shardStateIdentifier, Store.MetadataSnapshot sourceMetadata, Store.MetadataSnapshot targetMetadata, long startingSeqNo, int translogOps, Optional<ShardSnapshot> latestSnapshotOpt) { Store.RecoveryDiff sourceTargetDiff = sourceMetadata.recoveryDiff(targetMetadata); List<StoreFileMetadata> filesMissingInTarget = concatLists(sourceTargetDiff.missing, sourceTargetDiff.different); if (latestSnapshotOpt.isEmpty()) { // If we couldn't find any valid snapshots, fallback to the source return getRecoveryPlanUsingSourceNode(sourceMetadata, sourceTargetDiff, filesMissingInTarget, startingSeqNo, translogOps); } ShardSnapshot latestSnapshot = latestSnapshotOpt.get(); Version snapshotVersion = latestSnapshot.getVersion(); // Primary failed over after the snapshot was taken if (latestSnapshot.isLogicallyEquivalent(shardStateIdentifier) && latestSnapshot.hasDifferentPhysicalFiles(sourceMetadata) && snapshotVersion != null && snapshotVersion.onOrBefore(Version.CURRENT) && sourceTargetDiff.identical.isEmpty()) { // Use the current primary as a fallback if the download fails half-way ShardRecoveryPlan fallbackPlan = getRecoveryPlanUsingSourceNode(sourceMetadata, sourceTargetDiff, filesMissingInTarget, startingSeqNo, translogOps); ShardRecoveryPlan.SnapshotFilesToRecover snapshotFilesToRecover = new ShardRecoveryPlan.SnapshotFilesToRecover( latestSnapshot.getIndexId(), latestSnapshot.getRepository(), latestSnapshot.getSnapshotFiles() ); return new ShardRecoveryPlan(snapshotFilesToRecover, Collections.emptyList(), Collections.emptyList(), startingSeqNo, translogOps, latestSnapshot.getMetadataSnapshot(), fallbackPlan ); } Store.MetadataSnapshot filesToRecoverFromSourceSnapshot = toMetadataSnapshot(filesMissingInTarget); Store.RecoveryDiff snapshotDiff = filesToRecoverFromSourceSnapshot.recoveryDiff(latestSnapshot.getMetadataSnapshot()); final ShardRecoveryPlan.SnapshotFilesToRecover snapshotFilesToRecover; if (snapshotDiff.identical.isEmpty()) { snapshotFilesToRecover = ShardRecoveryPlan.SnapshotFilesToRecover.EMPTY; } else { snapshotFilesToRecover = new ShardRecoveryPlan.SnapshotFilesToRecover(latestSnapshot.getIndexId(), latestSnapshot.getRepository(), latestSnapshot.getSnapshotFilesMatching(snapshotDiff.identical)); } return new ShardRecoveryPlan(snapshotFilesToRecover, concatLists(snapshotDiff.missing, snapshotDiff.different), sourceTargetDiff.identical, startingSeqNo, translogOps, sourceMetadata ); }	let us add a comment here that we check against source version and why it is ok (the allocation decider). also, i think we can do following instead: suggestion (snapshotversion == null || snapshotversion.onorbefore(version.current)) && and add a comment that no snapshotcommitversion means that it was taken before 7.16? that would make this new path work from 7.16 with snapshots taken in 7.15-.
@Override public Settings nodeSettings(int nodeOrdinal) { final Path home = nodePath(nodeOrdinal); final Path xpackConf = home.resolve("config"); try { Files.createDirectories(xpackConf); } catch (IOException e) { throw new UncheckedIOException(e); } writeFile(xpackConf, "roles.yml", configRoles()); writeFile(xpackConf, "users", configUsers()); writeFile(xpackConf, "users_roles", configUsersRoles()); Settings.Builder builder = Settings.builder() .put(XPackSettings.SECURITY_ENABLED.getKey(), true) .put(NetworkModule.TRANSPORT_TYPE_KEY, SecurityField.NIO)//randomBoolean() ? SecurityField.NAME4 : SecurityField.NIO) .put(NetworkModule.HTTP_TYPE_KEY, randomBoolean() ? SecurityField.NAME4 : SecurityField.NIO) //TODO: for now isolate security tests from watcher & monitoring (randomize this later) .put(XPackSettings.WATCHER_ENABLED.getKey(), false) .put(XPackSettings.MONITORING_ENABLED.getKey(), false) .put(XPackSettings.AUDIT_ENABLED.getKey(), randomBoolean()) .put(LoggingAuditTrail.EMIT_HOST_ADDRESS_SETTING.getKey(), randomBoolean()) .put(LoggingAuditTrail.EMIT_HOST_NAME_SETTING.getKey(), randomBoolean()) .put(LoggingAuditTrail.EMIT_NODE_NAME_SETTING.getKey(), randomBoolean()) .put(LoggingAuditTrail.EMIT_NODE_ID_SETTING.getKey(), randomBoolean()) .put("xpack.security.authc.realms." + FileRealmSettings.TYPE + ".file.order", 0) .put("xpack.security.authc.realms." + NativeRealmSettings.TYPE + ".index.order", "1") .put("xpack.license.self_generated.type", "trial"); addNodeSSLSettings(builder); return builder.build(); }	i assume this is left over debugging?
@Override public SortField sortField(@Nullable Object missingValue, MultiValueMode sortMode, XFieldComparatorSource.Nested nested, boolean reverse) { XFieldComparatorSource source = new BytesRefFieldComparatorSource(this, missingValue, sortMode, nested); return new SortField(getFieldName(), source, reverse); }	nit: this fits in one line ?
public final Index index() { return index; }	is this still needed ? i don't see where it is used.
@Override protected void doAssertLuceneQuery(MatchPhrasePrefixQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException { assertThat(query, notNullValue()); if (query instanceof MatchAllDocsQuery) { assertThat(queryBuilder.zeroTermsQuery(), equalTo(ZeroTermsQuery.ALL)); return; } assertThat(query, either(instanceOf(MultiPhrasePrefixQuery.class)) .or(instanceOf(SynonymQuery.class)) .or(instanceOf(MatchNoDocsQuery.class))); }	as we are asserting lucene queries there, i would make this test in reverse: if (querybuilder.zerotermsquery().equals(zerotermsquery.all)) { assert that query instanceof matchalldocsquery; }
protected void doExecute(Task task, Request request, ActionListener<Response> listener) { if (!licenseState.isAllowed(XPackLicenseState.Feature.TRANSFORM)) { listener.onFailure(LicenseUtils.newComplianceException(XPackField.TRANSFORM)); return; } final ClusterState clusterState = clusterService.state(); XPackPlugin.checkReadyForXPackCustomMetadata(clusterState); final DiscoveryNodes nodes = clusterState.nodes(); if (nodes.isLocalNodeElectedMaster() == false) { // Delegates update transform to elected master node so it becomes the coordinating node. if (nodes.getMasterNode() == null) { listener.onFailure(new MasterNotDiscoveredException()); } else { transportService.sendRequest( nodes.getMasterNode(), actionName, request, new ActionListenerResponseHandler<>(listener, Response::fromStreamWithBWC) ); } } else { // set headers to run transform as calling user Map<String, String> filteredHeaders = threadPool.getThreadContext() .getHeaders() .entrySet() .stream() .filter(e -> ClientHelper.SECURITY_HEADER_FILTERS.contains(e.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); TransformConfigUpdate update = request.getUpdate(); update.setHeaders(filteredHeaders); // GET transform and attempt to update // We don't want the update to complete if the config changed between GET and INDEX transformConfigManager.getTransformConfigurationForUpdate(request.getId(), ActionListener.wrap(configAndVersion -> { final TransformConfig config = configAndVersion.v1(); // If it is a noop don't bother even writing the doc, save the cycles, just return here. if (update.isNoop(config)) { listener.onResponse(new Response(config)); return; } TransformConfig updatedConfig = update.apply(config); final ActionListener<Response> updateListener; if (update.changesSettings(config)) { PersistentTasksCustomMetadata tasksMetadata = PersistentTasksCustomMetadata.getPersistentTasksCustomMetadata( clusterState ); PersistentTasksCustomMetadata.PersistentTask<?> transformTask = tasksMetadata.getTask(request.getId()); // to send a request to apply new settings at runtime, several requirements must be met: // - transform must be running, meaning a task exists // - transform is not failed (stopped transforms do not have a task) // - the node where transform is executed on is at least 7.8.0 in order to understand the request if (transformTask != null && transformTask.getState() instanceof TransformState && ((TransformState) transformTask.getState()).getTaskState() != TransformTaskState.FAILED && clusterState.nodes().get(transformTask.getExecutorNode()).getVersion().onOrAfter(Version.V_8_0_0) // todo: // V_7_8_0 ) { request.setNodes(transformTask.getExecutorNode()); updateListener = ActionListener.wrap(updateResponse -> { request.setConfig(updateResponse.getConfig()); super.doExecute(task, request, listener); }, listener::onFailure); } else { updateListener = listener; } } else { updateListener = listener; } sourceDestValidator.validate( clusterState, updatedConfig.getSource().getIndex(), updatedConfig.getDestination().getIndex(), request.isDeferValidation() ? SourceDestValidations.NON_DEFERABLE_VALIDATIONS : SourceDestValidations.ALL_VALIDATIONS, ActionListener.wrap( validationResponse -> { checkPriviledgesAndUpdateTransform(request, clusterState, updatedConfig, configAndVersion.v2(), updateListener); }, listener::onFailure ) ); }, listener::onFailure)); } }	early return might end up being more readable.
*/ public void applyNewSettings(SettingsConfig newSettings) { auditor.info(transformConfig.getId(), "Transform settings have been updated."); logger.info("[{}] transform settings have been updated.", transformConfig.getId()); docsPerSecond = newSettings.getDocsPerSecond() != null ? newSettings.getDocsPerSecond() : -1; if (newSettings.getMaxPageSearchSize() != null) { pageSize = newSettings.getMaxPageSearchSize(); } rethrottle(); }	how can the user revert the page size to the default?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); { builder.field("snapshot_uuid", getSnapshotId().getUUID()); builder.field("index_uuid", getIndexId().getId()); builder.startObject("shard"); { builder.field("id", shardRouting.shardId()); builder.field("state", shardRouting.state()); builder.field("primary", shardRouting.primary()); builder.field("node", shardRouting.currentNodeId()); if (shardRouting.relocatingNodeId() != null) { builder.field("relocating_node", shardRouting.relocatingNodeId()); } } builder.endObject(); builder.startArray("files"); { List<CacheIndexInputStats> stats = inputStats.stream() .sorted(Comparator.comparing(CacheIndexInputStats::getFileName)).collect(toList()); for (CacheIndexInputStats stat : stats) { stat.toXContent(builder, params); } } builder.endArray(); } return builder.endObject(); }	kinda unrelated change, but this was useful for debugging.
@Override protected Settings nodeSettings(int nodeOrdinal) { final Settings.Builder builder = Settings.builder().put(super.nodeSettings(nodeOrdinal)); builder.put(LicenseService.SELF_GENERATED_LICENSE_TYPE.getKey(), "trial"); if (randomBoolean()) { builder.put( CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), rarely() ? randomBoolean() ? new ByteSizeValue(randomIntBetween(0, 10), ByteSizeUnit.KB) : new ByteSizeValue(randomIntBetween(0, 1000), ByteSizeUnit.BYTES) : new ByteSizeValue(randomIntBetween(1, 10), ByteSizeUnit.MB) ); } if (randomBoolean()) { builder.put( CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(), rarely() ? new ByteSizeValue(randomIntBetween(4, 1024), ByteSizeUnit.KB) : new ByteSizeValue(randomIntBetween(1, 10), ByteSizeUnit.MB) ); } return builder.build(); }	prior to this change the smallest nonzero cache size was 1kb which was typically larger than many of the files seen in this test, so i thought this might improve coverage a bit.
public void testFieldAliasesForMetaFields() throws Exception { XContentBuilder mapping = XContentFactory.jsonBuilder() .startObject() .startObject("type") .startObject("properties") .startObject("id-alias") .field("type", "alias") .field("path", "_id") .endObject() .startObject("routing-alias") .field("type", "alias") .field("path", "_routing") .endObject() .endObject() .endObject() .endObject(); assertAcked(prepareCreate("test").addMapping("type", mapping)); IndexRequestBuilder indexRequest = client().prepareIndex("test", "type") .setId("1") .setRouting("custom") .setSource("field", "value"); indexRandom(true, false, indexRequest); SearchResponse searchResponse = client().prepareSearch() .setQuery(termQuery("routing-alias", "custom")) .addDocValueField("id-alias") .get(); assertHitCount(searchResponse, 1L); SearchHit hit = searchResponse.getHits().getAt(0); assertEquals(2, hit.getFields().size()); assertTrue(hit.getFields().containsKey("id-alias")); DocumentField field = hit.getFields().get("id-alias"); assertThat(field.getValue().toString(), equalTo("1")); }	maybe undo changes to this file since they are unrelated
@Override public void execute(SearchContext context) { context.fetchResult().setTaskInfo(context.getTaskInfo()); if (LOGGER.isTraceEnabled()) { LOGGER.trace("{}", new SearchContextSourcePrinter(context)); } final FieldsVisitor fieldsVisitor; Map<String, Set<String>> storedToRequestedFields = new HashMap<>(); StoredFieldsContext storedFieldsContext = context.storedFieldsContext(); if (storedFieldsContext == null) { // no fields specified, default to return source if no explicit indication if (!context.hasScriptFields() && !context.hasFetchSourceContext()) { context.fetchSourceContext(new FetchSourceContext(true)); } fieldsVisitor = new FieldsVisitor(context.sourceRequested()); } else if (storedFieldsContext.fetchFields() == false) { // disable stored fields entirely fieldsVisitor = null; } else { for (String fieldNameOrPattern : context.storedFieldsContext().fieldNames()) { if (fieldNameOrPattern.equals(SourceFieldMapper.NAME)) { FetchSourceContext fetchSourceContext = context.hasFetchSourceContext() ? context.fetchSourceContext() : FetchSourceContext.FETCH_SOURCE; context.fetchSourceContext(new FetchSourceContext(true, fetchSourceContext.includes(), fetchSourceContext.excludes())); continue; } Collection<String> fieldNames = context.mapperService().simpleMatchToFullName(fieldNameOrPattern); for (String fieldName : fieldNames) { MappedFieldType fieldType = context.smartNameFieldType(fieldName); if (fieldType == null) { // Only fail if we know it is a object field, missing paths / fields shouldn't fail. if (context.getObjectMapper(fieldName) != null) { throw new IllegalArgumentException("field [" + fieldName + "] isn't a leaf field"); } } else { String storedField = fieldType.name(); Set<String> requestedFields = storedToRequestedFields.computeIfAbsent( storedField, key -> new HashSet<>()); requestedFields.add(fieldName); } } } boolean loadSource = context.sourceRequested(); if (storedToRequestedFields.isEmpty()) { // empty list specified, default to disable _source if no explicit indication fieldsVisitor = new FieldsVisitor(loadSource); } else { fieldsVisitor = new CustomFieldsVisitor(storedToRequestedFields.keySet(), loadSource); } } try { SearchHit[] hits = new SearchHit[context.docIdsToLoadSize()]; FetchSubPhase.HitContext hitContext = new FetchSubPhase.HitContext(); for (int index = 0; index < context.docIdsToLoadSize(); index++) { if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } int docId = context.docIdsToLoad()[context.docIdsToLoadFrom() + index]; int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves()); LeafReaderContext subReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex); int subDocId = docId - subReaderContext.docBase; final SearchHit searchHit; int rootDocId = findRootDocumentIfNested(context, subReaderContext, subDocId); if (rootDocId != -1) { searchHit = createNestedSearchHit(context, docId, subDocId, rootDocId, storedToRequestedFields, subReaderContext); } else { searchHit = createSearchHit(context, fieldsVisitor, docId, subDocId, storedToRequestedFields, subReaderContext); } hits[index] = searchHit; hitContext.reset(searchHit, subReaderContext, subDocId, context.searcher()); for (FetchSubPhase fetchSubPhase : fetchSubPhases) { fetchSubPhase.hitExecute(context, hitContext); } } if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } for (FetchSubPhase fetchSubPhase : fetchSubPhases) { fetchSubPhase.hitsExecute(context, hits); if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } } TotalHits totalHits = context.queryResult().getTotalHits(); context.fetchResult().hits(new SearchHits(hits, totalHits, context.queryResult().getMaxScore())); } catch (IOException e) { throw ExceptionsHelper.convertToElastic(e); } }	it's too early to set the task info since we want to report the elapsed time and other infos that should only be set when the task is finished. it should be easier to do it in transportsearchservice where you can set the task info just before we send the response, see [here]( https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/action/search/searchtransportservice.java#l311) for the dfs phase for instance. it should be possible to have a generic action listener for searchphaseresults that sets the task info when onresponse is called and then delegate to the channel action listener ?
@Override public void execute(SearchContext searchContext) throws QueryPhaseExecutionException { searchContext.queryResult().setTaskInfo(searchContext.getTaskInfo()); if (searchContext.hasOnlySuggest()) { suggestPhase.execute(searchContext); searchContext.queryResult().topDocs(new TopDocsAndMaxScore( new TopDocs(new TotalHits(0, TotalHits.Relation.EQUAL_TO), Lucene.EMPTY_SCORE_DOCS), Float.NaN), new DocValueFormat[0]); return; } if (LOGGER.isTraceEnabled()) { LOGGER.trace("{}", new SearchContextSourcePrinter(searchContext)); } // Pre-process aggregations as late as possible. In the case of a DFS_Q_T_F // request, preProcess is called on the DFS phase phase, this is why we pre-process them // here to make sure it happens during the QUERY phase aggregationPhase.preProcess(searchContext); final ContextIndexSearcher searcher = searchContext.searcher(); boolean rescore = execute(searchContext, searchContext.searcher(), searcher::setCheckCancelled); if (rescore) { // only if we do a regular search rescorePhase.execute(searchContext); } suggestPhase.execute(searchContext); aggregationPhase.execute(searchContext); if (searchContext.getProfilers() != null) { ProfileShardResult shardResults = SearchProfileShardResults .buildShardResults(searchContext.getProfilers()); searchContext.queryResult().profileResults(shardResults); } }	we shouldn't set the task info here, transportsearchservice seems a better fit.
public T getValue() { return isSet ? value : defaultValue.get(); }	are you thinking of removing the old getvalue in a follow up?
public void prepareIndexIfNeededThenExecute(final Consumer<Exception> consumer, final Runnable andThen) { final State indexState = this.indexState; // use a local copy so all checks execute against the same state! try { // TODO we should improve this so we don't fire off a bunch of requests to do the same thing (create or update mappings) if (indexState == State.UNRECOVERED_STATE) { throw new ElasticsearchStatusException( "Cluster state has not been recovered yet, cannot write to the [" + indexState.concreteIndexName + "] index", RestStatus.SERVICE_UNAVAILABLE); } else if (indexState.indexExists() && indexState.isIndexUpToDate == false) { throw new IllegalStateException("Index [" + indexState.concreteIndexName + "] is not on the current version." + "Security features relying on the index will not be available until the upgrade API is run on the index"); } else if (indexState.indexExists() == false) { assert indexState.concreteIndexName != null; logger.info("security index does not exist. Creating [{}] with alias [{}]", indexState.concreteIndexName, this.aliasName); final byte[] mappingSource = mappingSourceSupplier.get(); final Tuple<String, Settings> mappingAndSettings = parseMappingAndSettingsFromTemplateBytes(mappingSource); CreateIndexRequest request = new CreateIndexRequest(indexState.concreteIndexName) .alias(new Alias(this.aliasName)) .mapping(MapperService.SINGLE_MAPPING_NAME, mappingAndSettings.v1(), XContentType.JSON) .waitForActiveShards(ActiveShardCount.ALL) .settings(mappingAndSettings.v2()); executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, request, new ActionListener<CreateIndexResponse>() { @Override public void onResponse(CreateIndexResponse createIndexResponse) { if (createIndexResponse.isAcknowledged()) { andThen.run(); } else { consumer.accept(new ElasticsearchException("Failed to create security index")); } } @Override public void onFailure(Exception e) { final Throwable cause = ExceptionsHelper.unwrapCause(e); if (cause instanceof ResourceAlreadyExistsException) { // the index already exists - it was probably just created so this // node hasn't yet received the cluster state update with the index andThen.run(); } else { consumer.accept(e); } } }, client.admin().indices()::create); } else if (indexState.mappingUpToDate == false) { logger.info("Index [{}] (alias [{}]) is not up to date. Updating mapping", indexState.concreteIndexName, this.aliasName); final byte[] mappingSource = mappingSourceSupplier.get(); final Tuple<String, Settings> mappingAndSettings = parseMappingAndSettingsFromTemplateBytes(mappingSource); PutMappingRequest request = new PutMappingRequest(indexState.concreteIndexName) .source(mappingAndSettings.v1(), XContentType.JSON) .type(MapperService.SINGLE_MAPPING_NAME); executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, request, ActionListener.<AcknowledgedResponse>wrap(putMappingResponse -> { if (putMappingResponse.isAcknowledged()) { andThen.run(); } else { consumer.accept(new IllegalStateException("put mapping request was not acknowledged")); } }, consumer), client.admin().indices()::putMapping); } else { andThen.run(); } } catch(Exception e) { consumer.accept(e); } }	nit: suggestion } catch (exception e) {
public void testIndexWithFaultyMappingOnDisk() { SecurityIndexManager.State state = new SecurityIndexManager.State(randomBoolean() ? Instant.now() : null, true, randomBoolean(), false, null, "not_important", null, null); Runnable runnable = mock(Runnable.class); manager = new SecurityIndexManager(mock(Client.class), RestrictedIndicesNames.SECURITY_MAIN_ALIAS, RestrictedIndicesNames.INTERNAL_SECURITY_MAIN_INDEX_7, SecurityIndexManager.INTERNAL_MAIN_INDEX_FORMAT, () -> { throw new RuntimeException(); }, state); AtomicReference<Exception> exceptionConsumer = new AtomicReference<>(); manager.prepareIndexIfNeededThenExecute(e -> exceptionConsumer.set(e), runnable); verify(runnable, never()).run(); assertThat(exceptionConsumer.get(), is(notNullValue())); }	can you extract the supplier into a local so that the variable name/type makes it more obvious what it is? as it reads now, you have to jump to the constructor to work out what the 5th argument is, and why throwing an exception there achieves the purpose fo this test.
public void run() { try { IOUtils.close(node, spawner); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); if (node != null && node.awaitClose(10, TimeUnit.SECONDS) == false) { throw new IOException("Node didn't stop within 10 seconds. " + "Any outstanding requests or tasks might get killed."); } } catch (IOException ex) { throw new ElasticsearchException("failed to stop node", ex); } catch (InterruptedException e) { Thread.currentThread().interrupt(); LogManager.getLogger(Bootstrap.class).warn("Thread got interrupted while waiting for the node to shutdown."); } }	i think illegalstateexception might be more appropriate?
public void run() { try { IOUtils.close(node, spawner); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); if (node != null && node.awaitClose(10, TimeUnit.SECONDS) == false) { throw new IOException("Node didn't stop within 10 seconds. " + "Any outstanding requests or tasks might get killed."); } } catch (IOException ex) { throw new ElasticsearchException("failed to stop node", ex); } catch (InterruptedException e) { Thread.currentThread().interrupt(); LogManager.getLogger(Bootstrap.class).warn("Thread got interrupted while waiting for the node to shutdown."); } }	how about we revert the order here (i.e. first log the message, then restore interrupt status)?
static void stop() throws IOException { try { IOUtils.close(INSTANCE.node, INSTANCE.spawner); if (INSTANCE.node != null && INSTANCE.node.awaitClose(10, TimeUnit.SECONDS) == false) { throw new IOException("Node didn't stop within 10 seconds. Any outstanding requests or tasks might get killed."); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); LogManager.getLogger(Bootstrap.class).warn("Thread got interrupted while waiting for the node to shutdown."); } finally { INSTANCE.keepAliveLatch.countDown(); } } /** * This method is invoked by {@link Elasticsearch#main(String[])}	i think illegalstateexception might be more appropriate?
static void stop() throws IOException { try { IOUtils.close(INSTANCE.node, INSTANCE.spawner); if (INSTANCE.node != null && INSTANCE.node.awaitClose(10, TimeUnit.SECONDS) == false) { throw new IOException("Node didn't stop within 10 seconds. Any outstanding requests or tasks might get killed."); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); LogManager.getLogger(Bootstrap.class).warn("Thread got interrupted while waiting for the node to shutdown."); } finally { INSTANCE.keepAliveLatch.countDown(); } } /** * This method is invoked by {@link Elasticsearch#main(String[])}	also here i suggest to revert the order.
@Override protected void doClose() throws IOException { indicesRefCount.decRef(); } /** * Wait for this {@link IndicesService} to be effectively closed. When this returns, all shard and shard stores are closed and all * shard {@link CacheHelper#addClosedListener(org.apache.lucene.index.IndexReader.ClosedListener) closed listeners} have run. * However some {@link IndexEventListener#onStoreClosed(ShardId) shard closed listeners}	i think the comment is not entirely accurate? the guarantees described here only hold if this method returns true. if it returns false it timed out waiting for the condition to happen.
@Override public void close() throws IOException { assert Thread.holdsLock(InternalTestCluster.this); try { resetClient(); } finally { closed.set(true); markNodeDataDirsAsPendingForWipe(node); node.close(); try { if (node.awaitClose(10, TimeUnit.SECONDS) == false) { throw new IOException("Node didn't close within 10 seconds."); } } catch (InterruptedException e) { throw new AssertionError("Interruption while waiting for the node to close", e); } } }	i think illegalstateexception might be more appropriate?
void processResult(AutodetectResult result) { if (processKilled) { return; } Bucket bucket = result.getBucket(); if (bucket != null) { if (deleteInterimRequired) { // Delete any existing interim results generated by a Flush command // which have not been replaced or superseded by new results. LOGGER.trace("[{}] Deleting interim results", jobId); persister.deleteInterimResults(jobId); deleteInterimRequired = false; } // persist after deleting interim results in case the new // results are also interim timingStatsReporter.reportBucket(bucket); bulkResultsPersister.persistBucket(bucket).executeRequest(); ++bucketCount; } List<AnomalyRecord> records = result.getRecords(); if (records != null && !records.isEmpty()) { bulkResultsPersister.persistRecords(records); } List<Influencer> influencers = result.getInfluencers(); if (influencers != null && !influencers.isEmpty()) { bulkResultsPersister.persistInfluencers(influencers); } CategoryDefinition categoryDefinition = result.getCategoryDefinition(); if (categoryDefinition != null) { persister.persistCategoryDefinition(categoryDefinition); } ModelPlot modelPlot = result.getModelPlot(); if (modelPlot != null) { bulkResultsPersister.persistModelPlot(modelPlot); } Forecast forecast = result.getForecast(); if (forecast != null) { bulkResultsPersister.persistForecast(forecast); } ForecastRequestStats forecastRequestStats = result.getForecastRequestStats(); if (forecastRequestStats != null) { LOGGER.trace("Received Forecast Stats [{}]", forecastRequestStats.getId()); bulkResultsPersister.persistForecastRequestStats(forecastRequestStats); // execute the bulk request only in some cases or in doubt // otherwise rely on the count-based trigger switch (forecastRequestStats.getStatus()) { case OK: case STARTED: break; case FAILED: case SCHEDULED: case FINISHED: default: bulkResultsPersister.executeRequest(); } } ModelSizeStats modelSizeStats = result.getModelSizeStats(); if (modelSizeStats != null) { processModelSizeStats(modelSizeStats); } ModelSnapshot modelSnapshot = result.getModelSnapshot(); if (modelSnapshot != null) { // We need to refresh in order for the snapshot to be available when we try to update the job with it IndexResponse indexResponse = persister.persistModelSnapshot(modelSnapshot, WriteRequest.RefreshPolicy.IMMEDIATE); if (indexResponse.getResult() == DocWriteResponse.Result.CREATED) { updateModelSnapshotOnJob(modelSnapshot); } } Quantiles quantiles = result.getQuantiles(); if (quantiles != null) { LOGGER.debug("[{}] Parsed Quantiles with timestamp {}", jobId, quantiles.getTimestamp()); persister.persistQuantiles(quantiles); bulkResultsPersister.executeRequest(); if (processKilled == false && renormalizer.isEnabled()) { // We need to make all results written up to these quantiles available for renormalization persister.commitResultWrites(jobId); LOGGER.debug("[{}] Quantiles queued for renormalization", jobId); renormalizer.renormalize(quantiles); } } FlushAcknowledgement flushAcknowledgement = result.getFlushAcknowledgement(); if (flushAcknowledgement != null) { LOGGER.debug("[{}] Flush acknowledgement parsed from output for ID {}", jobId, flushAcknowledgement.getId()); // Commit previous writes here, effectively continuing // the flush from the C++ autodetect process right // through to the data store try { bulkResultsPersister.executeRequest(); persister.commitResultWrites(jobId); } catch (Exception e) { LOGGER.error("[" + jobId + "] failed to bulk persist results and commit writes during flush acknowledgement.", e); } finally { LOGGER.debug("[{}] Flush acknowledgement sent to listener for ID {}", jobId, flushAcknowledgement.getId()); flushListener.acknowledgeFlush(flushAcknowledgement); } // Interim results may have been produced by the flush, // which need to be // deleted when the next finalized results come through deleteInterimRequired = true; } }	if there's been an exception it would be better if the caller received the exception rather than an acknowledgement saying the flush worked fine. i think this can be achieved by: 1. making flushlistener.acknowledgeflush take two arguments, namely a flushacknowledgement and an exception, one of which should be null. 2. making flushacknowledgementholder hold an exception as well as a latch and a flushacknowledgement. 3. making flushlistener.waitforflush throw the exception in the holder if it's non-null. this will get propagated to the response of the flush call by abstractrunnable.run.
public void execute(BulkRequest bulkRequest, long executionId) { boolean bulkRequestSetupSuccessful = false; boolean acquired = false; try { listener.beforeBulk(executionId, bulkRequest); semaphore.acquire(); acquired = true; CountDownLatch latch = new CountDownLatch(1); retry.withBackoff(consumer, bulkRequest, new ActionListener<BulkResponse>() { @Override public void onResponse(BulkResponse response) { try { listener.afterBulk(executionId, bulkRequest, response); } finally { semaphore.release(); latch.countDown(); } } @Override public void onFailure(Exception e) { try { listener.afterBulk(executionId, bulkRequest, e); } finally { semaphore.release(); latch.countDown(); } } }, Settings.EMPTY); bulkRequestSetupSuccessful = true; if (concurrentRequests == 0) { latch.await(); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); logger.info((Supplier<?>) () -> new ParameterizedMessage("Bulk request {} has been cancelled.", executionId), e); listener.afterBulk(executionId, bulkRequest, e); } catch (Exception e) { logger.warn((Supplier<?>) () -> new ParameterizedMessage("Failed to execute bulk request {}.", executionId), e); listener.afterBulk(executionId, bulkRequest, e); } finally { if (bulkRequestSetupSuccessful == false && acquired) { // if we fail on client.bulk() release the semaphore semaphore.release(); } } }	does this need to be public and also does this class need to be subclassable
public void testParseFromXContent() throws IOException { final int iters = randomIntBetween(10, 50); for (int i = 0; i < iters; i++) { { float floatValue = randomFloat(); XContentBuilder json = jsonBuilder().startObject() .field(Fuzziness.X_FIELD_NAME, floatValue) .endObject(); XContentParser parser = createParser(json); assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_NUMBER)); Fuzziness parse = Fuzziness.parse(parser); assertThat(parse.asFloat(), equalTo(floatValue)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT)); } { Integer intValue = frequently() ? randomIntBetween(0, 2) : randomIntBetween(0, 100); Float floatRep = randomFloat(); Number value = intValue; if (randomBoolean()) { value = new Float(floatRep += intValue); } XContentBuilder json = jsonBuilder().startObject() .field(Fuzziness.X_FIELD_NAME, randomBoolean() ? value.toString() : value) .endObject(); XContentParser parser = createParser(json); assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME)); assertThat(parser.nextToken(), anyOf(equalTo(XContentParser.Token.VALUE_NUMBER), equalTo(XContentParser.Token.VALUE_STRING))); Fuzziness parse = Fuzziness.parse(parser); if (value.intValue() >= 1) { assertThat(parse.asDistance(), equalTo(Math.min(2, value.intValue()))); } assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT)); if (intValue.equals(value)) { switch (intValue) { case 1: assertThat(parse, sameInstance(Fuzziness.ONE)); break; case 2: assertThat(parse, sameInstance(Fuzziness.TWO)); break; case 0: assertThat(parse, sameInstance(Fuzziness.ZERO)); break; default: break; } } } { XContentBuilder json; boolean defaultAutoFuzziness = randomBoolean(); if (defaultAutoFuzziness) { json = Fuzziness.AUTO.toXContent(jsonBuilder().startObject(), null).endObject(); } else { String auto = randomBoolean() ? "AUTO" : "auto"; if (randomBoolean()) { auto += ":" + randomIntBetween(1, 3) + "," + randomIntBetween(4, 10); } json = jsonBuilder().startObject() .field(Fuzziness.X_FIELD_NAME, auto) .endObject(); } XContentParser parser = createParser(json); assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING)); Fuzziness parse = Fuzziness.parse(parser); if (defaultAutoFuzziness) { assertThat(parse, sameInstance(Fuzziness.AUTO)); } assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT)); } } }	i think that you should think of a name that is more descriptive for the variable "parse"
public void testParseFromXContent() throws IOException { final int iters = randomIntBetween(10, 50); for (int i = 0; i < iters; i++) { { float floatValue = randomFloat(); XContentBuilder json = jsonBuilder().startObject() .field(Fuzziness.X_FIELD_NAME, floatValue) .endObject(); XContentParser parser = createParser(json); assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_NUMBER)); Fuzziness parse = Fuzziness.parse(parser); assertThat(parse.asFloat(), equalTo(floatValue)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT)); } { Integer intValue = frequently() ? randomIntBetween(0, 2) : randomIntBetween(0, 100); Float floatRep = randomFloat(); Number value = intValue; if (randomBoolean()) { value = new Float(floatRep += intValue); } XContentBuilder json = jsonBuilder().startObject() .field(Fuzziness.X_FIELD_NAME, randomBoolean() ? value.toString() : value) .endObject(); XContentParser parser = createParser(json); assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME)); assertThat(parser.nextToken(), anyOf(equalTo(XContentParser.Token.VALUE_NUMBER), equalTo(XContentParser.Token.VALUE_STRING))); Fuzziness parse = Fuzziness.parse(parser); if (value.intValue() >= 1) { assertThat(parse.asDistance(), equalTo(Math.min(2, value.intValue()))); } assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT)); if (intValue.equals(value)) { switch (intValue) { case 1: assertThat(parse, sameInstance(Fuzziness.ONE)); break; case 2: assertThat(parse, sameInstance(Fuzziness.TWO)); break; case 0: assertThat(parse, sameInstance(Fuzziness.ZERO)); break; default: break; } } } { XContentBuilder json; boolean defaultAutoFuzziness = randomBoolean(); if (defaultAutoFuzziness) { json = Fuzziness.AUTO.toXContent(jsonBuilder().startObject(), null).endObject(); } else { String auto = randomBoolean() ? "AUTO" : "auto"; if (randomBoolean()) { auto += ":" + randomIntBetween(1, 3) + "," + randomIntBetween(4, 10); } json = jsonBuilder().startObject() .field(Fuzziness.X_FIELD_NAME, auto) .endObject(); } XContentParser parser = createParser(json); assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME)); assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING)); Fuzziness parse = Fuzziness.parse(parser); if (defaultAutoFuzziness) { assertThat(parse, sameInstance(Fuzziness.AUTO)); } assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT)); } } }	in a boolean variable, the name should be more descriptive. something like **is**defaultautofuzziness is more readable and easier to understand.
public void testToQueryWithStringFieldDefinedFuzziness() throws IOException { assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0); String query = "{\\\\n" + " \\\\"fuzzy\\\\":{\\\\n" + " \\\\"" + STRING_FIELD_NAME + "\\\\":{\\\\n" + " \\\\"value\\\\":\\\\"sh\\\\",\\\\n" + " \\\\"fuzziness\\\\": \\\\"AUTO:2,5\\\\",\\\\n" + " \\\\"prefix_length\\\\":1,\\\\n" + " \\\\"boost\\\\":2.0\\\\n" + " }\\\\n" + " }\\\\n" + "}"; Query parsedQuery = parseQuery(query).toQuery(createShardContext()); assertThat(parsedQuery, instanceOf(BoostQuery.class)); BoostQuery boostQuery = (BoostQuery) parsedQuery; assertThat(boostQuery.getBoost(), equalTo(2.0f)); assertThat(boostQuery.getQuery(), instanceOf(FuzzyQuery.class)); FuzzyQuery fuzzyQuery = (FuzzyQuery) boostQuery.getQuery(); assertThat(fuzzyQuery.getTerm(), equalTo(new Term(STRING_FIELD_NAME, "sh"))); assertThat(fuzzyQuery.getMaxEdits(), equalTo(Fuzziness.AUTO.asDistance("sh"))); assertThat(fuzzyQuery.getPrefixLength(), equalTo(1)); }	instead of query1, query2, and query3, i believe you should rename the queries with the goal that you are trying to accomplish with each one. i believe that i should be able to know what the variable means without having to look at the entire test to do so.
@Override protected Settings nodeSettings() { Settings.Builder settings = Settings.builder().put(super.nodeSettings()); settings.put(LifecycleSettings.LIFECYCLE_HISTORY_INDEX_ENABLED_SETTING.getKey(), false); settings.put(XPackSettings.MACHINE_LEARNING_ENABLED.getKey(), false); settings.put(XPackSettings.SECURITY_ENABLED.getKey(), false); settings.put(XPackSettings.WATCHER_ENABLED.getKey(), false); settings.put(XPackSettings.MONITORING_ENABLED.getKey(), false); settings.put(XPackSettings.GRAPH_ENABLED.getKey(), false); settings.put(Environment.PATH_REPO_SETTING.getKey(), repositoryLocation); settings.put(XPackSettings.SNAPSHOT_LIFECYCLE_ENABLED.getKey(), true); return settings.build(); }	can you explain this change, just so i can better understand?
private void resolveRoleNames(Set<String> roleNames, ActionListener<RolesRetrievalResult> listener) { roleDescriptors(roleNames, ActionListener.wrap(rolesRetrievalResult -> { logDeprecatedRoles(rolesRetrievalResult.getRoleDescriptors()); final boolean missingRoles = rolesRetrievalResult.getMissingRoles().isEmpty() == false; if (missingRoles) { logger.debug(() -> new ParameterizedMessage("Could not find roles with names {}", rolesRetrievalResult.getMissingRoles())); } final Set<RoleDescriptor> effectiveDescriptors; Set<RoleDescriptor> roleDescriptors = rolesRetrievalResult.getRoleDescriptors(); if (roleDescriptors.stream().anyMatch(RoleDescriptor::isUsingDocumentOrFieldLevelSecurity) && DOCUMENT_LEVEL_SECURITY_FEATURE.checkWithoutTracking(licenseState) == false) { effectiveDescriptors = skipRolesUsingDocumentOrFieldLevelSecurity(roleDescriptors); } else { effectiveDescriptors = roleDescriptors; } logger.trace( () -> new ParameterizedMessage( "Exposing effective role descriptors [{}] for role names [{}]", effectiveDescriptors, roleNames ) ); effectiveRoleDescriptorsConsumer.accept(Collections.unmodifiableCollection(effectiveDescriptors)); // TODO: why not populate negativeLookupCache here with missing roles? // TODO: replace with a class that better represent the result, e.g. carry info for disabled role final RolesRetrievalResult finalResult = new RolesRetrievalResult(); finalResult.addDescriptors(effectiveDescriptors); finalResult.setMissingRoles(rolesRetrievalResult.getMissingRoles()); if (false == rolesRetrievalResult.isSuccess()) { finalResult.setFailure(); } listener.onResponse(finalResult); }, listener::onFailure)); }	the new skiprolesusingdocumentorfieldlevelsecurity method has the assumptions that (1) incompatible license and (2) there are role descriptors with either dls or fls. i'd add these as assertions in the beginning of the method. but the assertions are identical functional wise to the if check here. given this situation, my intuition would be moving the whole if/else block into the new method and call it something like maybeskiprolesusingdocumentorfieldlevelsecurity which feels more self-contained.
private Set<RoleDescriptor> skipRolesUsingDocumentOrFieldLevelSecurity(Set<RoleDescriptor> roleDescriptors) { final Map<Boolean, Set<RoleDescriptor>> roles = roleDescriptors.stream() .collect(Collectors.partitioningBy(RoleDescriptor::isUsingDocumentOrFieldLevelSecurity, Collectors.toSet())); final Set<RoleDescriptor> rolesToSkip = roles.get(true); logger.warn( "User roles [{}] are disabled since they require document or field level security to determine user access. " + "These security features [{}, {}] are not available under the current license. " + "Access to documents or fields granted by above roles will be denied. " + "To re-enable the roles, upgrade license to [{}] or above, or renew if it's expired.", DOCUMENT_LEVEL_SECURITY_FEATURE.getName(), FIELD_LEVEL_SECURITY_FEATURE.getName(), rolesToSkip.stream().map(RoleDescriptor::getName).collect(Collectors.joining(",")), DOCUMENT_LEVEL_SECURITY_FEATURE.getMinimumOperationMode() ); return roles.get(false); }	i checked other messages related to licensing issues and we can probably simplify this message to: > user roles [{}] are disabled because they require field or document level security. the current license is non-compliant for [field and document level security]. for a few reasons: 1. a bit more concise 2. access to documents or fields granted by above roles will be denied. this is not entirely correct because the same access can be granted by other enabled roles. but it's hard to explain the details. so it's better to just leave it out and only state the direct impact (instead of the possible implications). 3. license expiry has its own warning messages (see licenseservice). downgraded license also has its dedicated messages. so i think we don't need suggest upgrade or renew license here.
private Set<RoleDescriptor> skipRolesUsingDocumentOrFieldLevelSecurity(Set<RoleDescriptor> roleDescriptors) { final Map<Boolean, Set<RoleDescriptor>> roles = roleDescriptors.stream() .collect(Collectors.partitioningBy(RoleDescriptor::isUsingDocumentOrFieldLevelSecurity, Collectors.toSet())); final Set<RoleDescriptor> rolesToSkip = roles.get(true); logger.warn( "User roles [{}] are disabled since they require document or field level security to determine user access. " + "These security features [{}, {}] are not available under the current license. " + "Access to documents or fields granted by above roles will be denied. " + "To re-enable the roles, upgrade license to [{}] or above, or renew if it's expired.", DOCUMENT_LEVEL_SECURITY_FEATURE.getName(), FIELD_LEVEL_SECURITY_FEATURE.getName(), rolesToSkip.stream().map(RoleDescriptor::getName).collect(Collectors.joining(",")), DOCUMENT_LEVEL_SECURITY_FEATURE.getMinimumOperationMode() ); return roles.get(false); }	this should be the first argument after the string message.
* @param precision the tile zoom-level * * @return the number of tiles set by the shape */ @Override public int setValues(GeoShapeCellValues values, MultiGeoValues.GeoValue geoValue, int precision) { MultiGeoValues.BoundingBox bounds = geoValue.boundingBox(); assert bounds.minX() <= bounds.maxX(); if (precision == 0) { values.resizeCell(1); values.add(0, GeoTileUtils.longEncodeTiles(0, 0, 0)); return 1; } // geo tiles are not defined at the extreme latitudes due to them // tiling the world as a square. if ((bounds.top > GeoTileUtils.LATITUDE_MASK && bounds.bottom > GeoTileUtils.LATITUDE_MASK) || (bounds.top < -GeoTileUtils.LATITUDE_MASK && bounds.bottom < -GeoTileUtils.LATITUDE_MASK)) { return 0; } final double tiles = 1 << precision; int minXTile = GeoTileUtils.getXTile(bounds.minX(), (long) tiles); int minYTile = GeoTileUtils.getYTile(bounds.maxY(), (long) tiles); int maxXTile = GeoTileUtils.getXTile(bounds.maxX(), (long) tiles); int maxYTile = GeoTileUtils.getYTile(bounds.minY(), (long) tiles); int count = (maxXTile - minXTile + 1) * (maxYTile - minYTile + 1); if (count == 1) { values.resizeCell(1); values.add(0, GeoTileUtils.longEncodeTiles(precision, minXTile, minYTile)); return 1; } else if (count <= precision) { return setValuesByBruteForceScan(values, geoValue, precision, minXTile, minYTile, maxXTile, maxYTile); } else { return setValuesByRasterization(0, 0, 0, values, 0, precision, geoValue); } }	i believe this can be improved, maybe there can be a way to wrap the shape such that all the latitudes above the map can be normalized down to the edge tile
@Override protected GeoDistanceSortBuilder mutate(GeoDistanceSortBuilder original) throws IOException { GeoDistanceSortBuilder result = new GeoDistanceSortBuilder(original); int parameter = randomIntBetween(0, 9); switch (parameter) { case 0: while (Arrays.deepEquals(original.points(), result.points())) { GeoPoint pt = RandomGeoGenerator.randomPoint(random()); result.point(pt.getLat(), pt.getLon()); } break; case 1: result.points(points(original.points())); break; case 2: result.geoDistance(geoDistance(original.geoDistance())); break; case 3: result.unit(unit(original.unit())); break; case 4: result.order(ESTestCase.randomValueOtherThan(original.order(), () -> randomFrom(SortOrder.values()))); break; case 5: result.sortMode(mode(original.sortMode())); break; case 6: result.setNestedFilter(ESTestCase.randomValueOtherThan( original.getNestedFilter(), () -> randomNestedFilter())); break; case 7: result.setNestedPath(ESTestCase.randomValueOtherThan( result.getNestedPath(), () -> ESTestCase.randomAsciiOfLengthBetween(1, 10))); break; case 8: result.coerce(! original.coerce()); break; case 9: // ignore malformed will only be set if coerce is set to true result.coerce(false); result.ignoreMalformed(! original.ignoreMalformed()); break; } return result; }	nit: i think you can leave out estestcase here.
@Override protected GeoDistanceSortBuilder mutate(GeoDistanceSortBuilder original) throws IOException { GeoDistanceSortBuilder result = new GeoDistanceSortBuilder(original); int parameter = randomIntBetween(0, 9); switch (parameter) { case 0: while (Arrays.deepEquals(original.points(), result.points())) { GeoPoint pt = RandomGeoGenerator.randomPoint(random()); result.point(pt.getLat(), pt.getLon()); } break; case 1: result.points(points(original.points())); break; case 2: result.geoDistance(geoDistance(original.geoDistance())); break; case 3: result.unit(unit(original.unit())); break; case 4: result.order(ESTestCase.randomValueOtherThan(original.order(), () -> randomFrom(SortOrder.values()))); break; case 5: result.sortMode(mode(original.sortMode())); break; case 6: result.setNestedFilter(ESTestCase.randomValueOtherThan( original.getNestedFilter(), () -> randomNestedFilter())); break; case 7: result.setNestedPath(ESTestCase.randomValueOtherThan( result.getNestedPath(), () -> ESTestCase.randomAsciiOfLengthBetween(1, 10))); break; case 8: result.coerce(! original.coerce()); break; case 9: // ignore malformed will only be set if coerce is set to true result.coerce(false); result.ignoreMalformed(! original.ignoreMalformed()); break; } return result; }	nit: i think you can leave out estestcase here.
@Override protected GeoDistanceSortBuilder mutate(GeoDistanceSortBuilder original) throws IOException { GeoDistanceSortBuilder result = new GeoDistanceSortBuilder(original); int parameter = randomIntBetween(0, 9); switch (parameter) { case 0: while (Arrays.deepEquals(original.points(), result.points())) { GeoPoint pt = RandomGeoGenerator.randomPoint(random()); result.point(pt.getLat(), pt.getLon()); } break; case 1: result.points(points(original.points())); break; case 2: result.geoDistance(geoDistance(original.geoDistance())); break; case 3: result.unit(unit(original.unit())); break; case 4: result.order(ESTestCase.randomValueOtherThan(original.order(), () -> randomFrom(SortOrder.values()))); break; case 5: result.sortMode(mode(original.sortMode())); break; case 6: result.setNestedFilter(ESTestCase.randomValueOtherThan( original.getNestedFilter(), () -> randomNestedFilter())); break; case 7: result.setNestedPath(ESTestCase.randomValueOtherThan( result.getNestedPath(), () -> ESTestCase.randomAsciiOfLengthBetween(1, 10))); break; case 8: result.coerce(! original.coerce()); break; case 9: // ignore malformed will only be set if coerce is set to true result.coerce(false); result.ignoreMalformed(! original.ignoreMalformed()); break; } return result; }	nit: i think you can leave out estestcase here.
public static GeoDistanceSortBuilder randomGeoDistanceSortBuilder() { String fieldName = randomAsciiOfLengthBetween(1, 10); GeoDistanceSortBuilder result = null; int id = randomIntBetween(0, 2); switch(id) { case 0: int count = randomIntBetween(1, 10); String[] geohashes = new String[count]; for (int i = 0; i < count; i++) { geohashes[i] = RandomGeoGenerator.randomPoint(random()).geohash(); } result = new GeoDistanceSortBuilder(fieldName, geohashes); break; case 1: GeoPoint pt = RandomGeoGenerator.randomPoint(random()); result = new GeoDistanceSortBuilder(fieldName, pt.getLat(), pt.getLon()); break; case 2: result = new GeoDistanceSortBuilder(fieldName, points(new GeoPoint[0])); break; default: throw new IllegalStateException("one of three geo initialisation strategies must be used"); } if (randomBoolean()) { result.geoDistance(geoDistance(result.geoDistance())); } if (randomBoolean()) { result.unit(unit(result.unit())); } if (randomBoolean()) { result.order(randomFrom(SortOrder.values())); } if (randomBoolean()) { result.sortMode(mode(result.sortMode())); } if (randomBoolean()) { result.setNestedFilter(randomNestedFilter()); } if (randomBoolean()) { result.setNestedPath( ESTestCase.randomValueOtherThan( result.getNestedPath(), () -> ESTestCase.randomAsciiOfLengthBetween(1, 10))); } if (randomBoolean()) { result.coerce(! result.coerce()); } if (randomBoolean()) { result.ignoreMalformed(! result.ignoreMalformed()); } return result; }	nit: i think you can leave out estestcase here.
@Override protected ScoreSortBuilder mutate(ScoreSortBuilder original) throws IOException { ScoreSortBuilder result = new ScoreSortBuilder(); result.order(ESTestCase.randomValueOtherThan(original.order(), () -> randomFrom(SortOrder.values()))); return result; }	nit: i think you can leave out estestcase here.
public static ScriptSortBuilder randomScriptSortBuilder() { ScriptSortType type = randomBoolean() ? ScriptSortType.NUMBER : ScriptSortType.STRING; ScriptSortBuilder builder = new ScriptSortBuilder(new Script(randomAsciiOfLengthBetween(5, 10)), type); if (randomBoolean()) { builder.order(randomFrom(SortOrder.values())); } if (randomBoolean()) { if (type == ScriptSortType.NUMBER) { builder.sortMode(ESTestCase.randomValueOtherThan(builder.sortMode(), () -> randomFrom(SortMode.values()))); } else { Set<SortMode> exceptThis = new HashSet<>(); exceptThis.add(SortMode.SUM); exceptThis.add(SortMode.AVG); exceptThis.add(SortMode.MEDIAN); builder.sortMode(ESTestCase.randomValueOtherThanMany(exceptThis::contains, () -> randomFrom(SortMode.values()))); } } if (randomBoolean()) { builder.setNestedFilter(randomNestedFilter()); } if (randomBoolean()) { builder.setNestedPath(ESTestCase.randomAsciiOfLengthBetween(1, 10)); } return builder; }	nit: i think you can leave out estestcase here.
public static ScriptSortBuilder randomScriptSortBuilder() { ScriptSortType type = randomBoolean() ? ScriptSortType.NUMBER : ScriptSortType.STRING; ScriptSortBuilder builder = new ScriptSortBuilder(new Script(randomAsciiOfLengthBetween(5, 10)), type); if (randomBoolean()) { builder.order(randomFrom(SortOrder.values())); } if (randomBoolean()) { if (type == ScriptSortType.NUMBER) { builder.sortMode(ESTestCase.randomValueOtherThan(builder.sortMode(), () -> randomFrom(SortMode.values()))); } else { Set<SortMode> exceptThis = new HashSet<>(); exceptThis.add(SortMode.SUM); exceptThis.add(SortMode.AVG); exceptThis.add(SortMode.MEDIAN); builder.sortMode(ESTestCase.randomValueOtherThanMany(exceptThis::contains, () -> randomFrom(SortMode.values()))); } } if (randomBoolean()) { builder.setNestedFilter(randomNestedFilter()); } if (randomBoolean()) { builder.setNestedPath(ESTestCase.randomAsciiOfLengthBetween(1, 10)); } return builder; }	nit: i think you can leave out estestcase here.
public static String toYaml(final Settings settings) { try (XContentBuilder builder = XContentBuilder.builder(XContentType.YAML.xContent())) { builder.startObject(); settings.toXContent(builder, new ToXContent.MapParams(Collections.singletonMap("flat_settings", "true"))); builder.endObject(); return Strings.toString(builder); } catch (IOException e) { throw new UncheckedIOException(e); } }	this method does not seem to be used anywhere? it also looks like something belonging to the test code. can we remove or relocated it?
public void testGetTokenWhenKeyCacheHasExpired() throws Exception { TokenService tokenService = createTokenService(tokenServiceEnabledSettings, systemUTC()); // This test only makes sense in mixed clusters with pre v7.1.0 nodes where the Key is actually used if (null == oldNode) { oldNode = addAnotherDataNodeWithVersion(this.clusterService, randomFrom(Version.V_6_7_0, Version.V_7_0_0)); } Authentication authentication = new Authentication(new User("joe", "admin"), new RealmRef("native_realm", "native", "node1"), null); PlainActionFuture<Tuple<String, String>> tokenFuture = new PlainActionFuture<>(); final String userTokenId = UUIDs.randomBase64UUID(); final String refreshToken = UUIDs.randomBase64UUID(); tokenService.createOAuth2Tokens(userTokenId, refreshToken, authentication, authentication, Collections.emptyMap(), tokenFuture); String accessToken = tokenFuture.get().v1(); assertThat(accessToken, notNullValue()); tokenService.clearActiveKeyCache(); tokenService.createOAuth2Tokens(userTokenId, refreshToken, authentication, authentication, Collections.emptyMap(), tokenFuture); accessToken = tokenFuture.get().v1(); assertThat(accessToken, notNullValue()); }	minor, this feature is now in v7.2, not v7.1 suggestion oldnode = addanotherdatanodewithversion(this.clusterservice, randomfrom(version.v_6_7_0, version.v_7_1_0));
private boolean check(String action, String index) { assert index != null; return check(action) && (indexNameMatcher.test(index) && ((false == RestrictedIndicesNames.NAMES_SET.contains(index)) // if it is not restricted no more checks are required || IndexPrivilege.MONITOR.predicate().test(action) // allow monitor as a special case, even for restricted || allowRestrictedIndices) ); }	i suggest we reorder this check to: (indexnamematcher.test(index) && (allowrestrictedindices || false == restrictedindicesnames.names_set.contains(index) || indexprivilege.monitor.predicate().test(action))
public String generateSnapshotName(Context context) { List<String> candidates = DATE_MATH_RESOLVER.resolve(context, Collections.singletonList(this.name)); if (candidates.size() != 1) { throw new IllegalStateException("resolving snapshot name " + this.name + " generated more than one candidate: " + candidates); } // TODO: we are breaking the rules of UUIDs by lowercasing this here, find an alternative (snapshot names must be lowercase) return candidates.get(0) + "-" + UUIDs.randomBase64UUID().toLowerCase(Locale.ROOT); }	what rules are we breaking? the [spec i can find](https://www.itu.int/rec/t-rec-x.667-201210-i/en) says that stuff that produces uuids shouldn't emit upper-case characters (although uppercased uuids should be accepted, which isn't the case here i guess)
@Override protected InternalEngine.IndexingStrategy indexingStrategyForOperation(final Index index) throws IOException { preFlight(index); /* * A note about optimization using sequence numbers: * * 1. Indexing operations are processed concurrently in an engine. However, operations of the same docID are processed * one by one under the docID lock. * * 2. An engine itself can resolve correctly if an operation is delivered multiple times. However, if an operation is * optimized and delivered multiple times, it will be appended into Lucene more than once. We void this issue by * not executing operations which have been processed before (using LocalCheckpointTracker). * * 3. When replicating operations to replicas or followers, we also carry the max seq_no_of_updates_or_deletes on the * leader to followers. This transfer guarantees the MUS on a follower when operation O is processed at least the * MUS on the leader when it was executed [every operation O => MSU_r(O) >= MSU_p(O)]. * * 4. The following proves that docID(O) does not exist on a follower when operation O is applied if MSU_r(O) <= LCP < seqno(O): * * 4.1) Given two operations O and O' with docID(O) = docID(O) and seqno(O) < seqno(O) then MSU_p(O') on the primary * must be at least seqno(O). Moreover, the MSU_r on a follower >= min(seqno(O), seqno(O')) after these operations * arrive in any order. * * 4.2) If such operation O' with docID(O) = docID(O) and LCP < seqno(O) then MSU_r(O) >= min(seqno(O), seqno(O')) > LCP * because both arrived on the follower[4.1]. This contradicts the assumption [MSU_r(O) <= LCP]. * * 4.3) MSU(O) < seqno(O) then docID(O) does not exist when O is applied on a leader. This means docID(O) does not exist * after we apply every operation with docID = docID(O) and seqno < seqno(O). On the follower, we have applied every * operation with seqno <= LCP, and there is no such O' with docID(O) = docID(O) and LCP < seqno(O)[4.2]. * These mean the follower has applied every operation with docID = docID(O) and seqno < seqno(O). * Thus docID(O) does not exist on the follower. */ final long maxSeqNoOfUpdatesOrDeletes = getMaxSeqNoOfUpdatesOrDeletes(); assert maxSeqNoOfUpdatesOrDeletes != SequenceNumbers.UNASSIGNED_SEQ_NO : "max_seq_no_of_updates is not initialized"; if (hasBeenProcessedBefore(index)) { return IndexingStrategy.processButSkipLucene(false, index.seqNo(), index.version()); } else if (maxSeqNoOfUpdatesOrDeletes <= getLocalCheckpoint()) { assert maxSeqNoOfUpdatesOrDeletes < index.seqNo() : "seq_no[" + index.seqNo() + "] <= msu[" + maxSeqNoOfUpdatesOrDeletes + "]"; numOfOptimizedIndexing.inc(); return InternalEngine.IndexingStrategy.optimizedAppendOnly(index.seqNo(), index.version()); } else { return planIndexingAsNonPrimary(index); } }	i think you mean msu_p(o') must be at least seqno(o') (as o' is an update)
@Override protected InternalEngine.IndexingStrategy indexingStrategyForOperation(final Index index) throws IOException { preFlight(index); /* * A note about optimization using sequence numbers: * * 1. Indexing operations are processed concurrently in an engine. However, operations of the same docID are processed * one by one under the docID lock. * * 2. An engine itself can resolve correctly if an operation is delivered multiple times. However, if an operation is * optimized and delivered multiple times, it will be appended into Lucene more than once. We void this issue by * not executing operations which have been processed before (using LocalCheckpointTracker). * * 3. When replicating operations to replicas or followers, we also carry the max seq_no_of_updates_or_deletes on the * leader to followers. This transfer guarantees the MUS on a follower when operation O is processed at least the * MUS on the leader when it was executed [every operation O => MSU_r(O) >= MSU_p(O)]. * * 4. The following proves that docID(O) does not exist on a follower when operation O is applied if MSU_r(O) <= LCP < seqno(O): * * 4.1) Given two operations O and O' with docID(O) = docID(O) and seqno(O) < seqno(O) then MSU_p(O') on the primary * must be at least seqno(O). Moreover, the MSU_r on a follower >= min(seqno(O), seqno(O')) after these operations * arrive in any order. * * 4.2) If such operation O' with docID(O) = docID(O) and LCP < seqno(O) then MSU_r(O) >= min(seqno(O), seqno(O')) > LCP * because both arrived on the follower[4.1]. This contradicts the assumption [MSU_r(O) <= LCP]. * * 4.3) MSU(O) < seqno(O) then docID(O) does not exist when O is applied on a leader. This means docID(O) does not exist * after we apply every operation with docID = docID(O) and seqno < seqno(O). On the follower, we have applied every * operation with seqno <= LCP, and there is no such O' with docID(O) = docID(O) and LCP < seqno(O)[4.2]. * These mean the follower has applied every operation with docID = docID(O) and seqno < seqno(O). * Thus docID(O) does not exist on the follower. */ final long maxSeqNoOfUpdatesOrDeletes = getMaxSeqNoOfUpdatesOrDeletes(); assert maxSeqNoOfUpdatesOrDeletes != SequenceNumbers.UNASSIGNED_SEQ_NO : "max_seq_no_of_updates is not initialized"; if (hasBeenProcessedBefore(index)) { return IndexingStrategy.processButSkipLucene(false, index.seqNo(), index.version()); } else if (maxSeqNoOfUpdatesOrDeletes <= getLocalCheckpoint()) { assert maxSeqNoOfUpdatesOrDeletes < index.seqNo() : "seq_no[" + index.seqNo() + "] <= msu[" + maxSeqNoOfUpdatesOrDeletes + "]"; numOfOptimizedIndexing.inc(); return InternalEngine.IndexingStrategy.optimizedAppendOnly(index.seqNo(), index.version()); } else { return planIndexingAsNonPrimary(index); } }	> moreover, the msu_r on a follower >= min(seqno(o), seqno(o')) after these operations i still don't follow this.
public FieldSortBuilder randomFieldSortBuilder() { String fieldName = rarely() ? FieldSortBuilder.DOC_FIELD_NAME : randomAlphaOfLengthBetween(1, 10); FieldSortBuilder builder = new FieldSortBuilder(fieldName); if (randomBoolean()) { builder.order(randomFrom(SortOrder.values())); } if (randomBoolean()) { builder.missing(randomFrom(missingContent)); } if (randomBoolean()) { builder.unmappedType(randomAlphaOfLengthBetween(1, 10)); } if (randomBoolean()) { builder.sortMode(randomFrom(SortMode.values())); } if (randomBoolean()) { builder.setNestedSort(createRandomNestedSort(3)); } // the following are alternative ways to setNestedSort for nested sorting if (randomBoolean()) { builder.setNestedFilter(randomNestedFilter()); } if (randomBoolean()) { builder.setNestedPath(randomAlphaOfLengthBetween(1, 10)); } return builder; }	not related to tests but this function should be deprecated.
@Override public void close() throws SQLException { // this cursor doesn't hold any resource - no need to clean up } } static String escapeStringPattern(String pattern, EscapeWildcard strategy) { if (StringUtils.hasText(pattern) == false || "%".equals(pattern)) { return pattern; } char esc = '\\\\\\\\'; switch (strategy) { case NEVER: return pattern; case ALWAYS: return pattern.replace("_", esc + "_").replace("%", esc + "%"); default: // first detect if there's any escaping in the string, if so bail out boolean escaped = false; boolean hasEscaping = false; for (int i = 0; i < pattern.length(); i++) { char curr = pattern.charAt(i); if (escaped == false && (curr == esc) && esc != 0) { escaped = true; } else { if ((curr == '%' || curr == '_' || curr == esc) && (escaped == true)) { hasEscaping = true; break; } escaped = false; } } // looks like the string is using escaping, bail out if (hasEscaping) { return pattern; } // escape _ only if not escaped (% is ignored for now) StringBuilder wildcard = new StringBuilder(pattern.length()); for (int i = 0; i < pattern.length(); i++) { char curr = pattern.charAt(i); if (escaped == false && (curr == esc) && esc != 0) { escaped = true; } else { // if (curr == '_' && (escaped == false)) { wildcard.append(esc); } escaped = false; } wildcard.append(curr); } return wildcard.toString(); }	esc != 0 ? esc doesn't seem to change value, it's like a constant, no?
@Override public void close() throws SQLException { // this cursor doesn't hold any resource - no need to clean up } } static String escapeStringPattern(String pattern, EscapeWildcard strategy) { if (StringUtils.hasText(pattern) == false || "%".equals(pattern)) { return pattern; } char esc = '\\\\\\\\'; switch (strategy) { case NEVER: return pattern; case ALWAYS: return pattern.replace("_", esc + "_").replace("%", esc + "%"); default: // first detect if there's any escaping in the string, if so bail out boolean escaped = false; boolean hasEscaping = false; for (int i = 0; i < pattern.length(); i++) { char curr = pattern.charAt(i); if (escaped == false && (curr == esc) && esc != 0) { escaped = true; } else { if ((curr == '%' || curr == '_' || curr == esc) && (escaped == true)) { hasEscaping = true; break; } escaped = false; } } // looks like the string is using escaping, bail out if (hasEscaping) { return pattern; } // escape _ only if not escaped (% is ignored for now) StringBuilder wildcard = new StringBuilder(pattern.length()); for (int i = 0; i < pattern.length(); i++) { char curr = pattern.charAt(i); if (escaped == false && (curr == esc) && esc != 0) { escaped = true; } else { // if (curr == '_' && (escaped == false)) { wildcard.append(esc); } escaped = false; } wildcard.append(curr); } return wildcard.toString(); }	here you could just return pattern; since you break from the loop and then you just return.
@Override public void close() throws SQLException { // this cursor doesn't hold any resource - no need to clean up } } static String escapeStringPattern(String pattern, EscapeWildcard strategy) { if (StringUtils.hasText(pattern) == false || "%".equals(pattern)) { return pattern; } char esc = '\\\\\\\\'; switch (strategy) { case NEVER: return pattern; case ALWAYS: return pattern.replace("_", esc + "_").replace("%", esc + "%"); default: // first detect if there's any escaping in the string, if so bail out boolean escaped = false; boolean hasEscaping = false; for (int i = 0; i < pattern.length(); i++) { char curr = pattern.charAt(i); if (escaped == false && (curr == esc) && esc != 0) { escaped = true; } else { if ((curr == '%' || curr == '_' || curr == esc) && (escaped == true)) { hasEscaping = true; break; } escaped = false; } } // looks like the string is using escaping, bail out if (hasEscaping) { return pattern; } // escape _ only if not escaped (% is ignored for now) StringBuilder wildcard = new StringBuilder(pattern.length()); for (int i = 0; i < pattern.length(); i++) { char curr = pattern.charAt(i); if (escaped == false && (curr == esc) && esc != 0) { escaped = true; } else { // if (curr == '_' && (escaped == false)) { wildcard.append(esc); } escaped = false; } wildcard.append(curr); } return wildcard.toString(); }	don't we need to reset escaped = false before this loop?
public void testReplicaShardPreferenceIters() throws Exception { AllocationService strategy = createAllocationService(Settings.builder() .put("cluster.routing.allocation.node_concurrent_recoveries", 10) .build()); OperationRouting operationRouting = new OperationRouting(Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS)); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(2).numberOfReplicas(2)) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsNew(metaData.index("test")) .build(); ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)).metaData(metaData).routingTable(routingTable).build(); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .add(newNode("node1")) .add(newNode("node2")) .add(newNode("node3")) .localNodeId("node1") ).build(); clusterState = strategy.reroute(clusterState, "reroute"); clusterState = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)); String[] removedPreferences = {"_primary", "_primary_first", "_replica", "_replica_first"}; for (String pref : removedPreferences) { try{ operationRouting.searchShards(clusterState, new String[]{"test"}, null, pref); fail("Should have failed because the shard preference [" + pref + "] is removed."); }catch (IllegalArgumentException ex){ assertThat(ex.getMessage(), containsString(pref)); } } }	this can be replaced with expectthrows.
public void testCorruptTranslogTruncationOfReplica() throws Exception { internalCluster().startNodes(2, Settings.EMPTY); final String primaryNode = internalCluster().getNodeNames()[0]; final String replicaNode = internalCluster().getNodeNames()[1]; assertAcked(prepareCreate("test").setSettings(Settings.builder() .put("index.number_of_shards", 1) .put("index.number_of_replicas", 1) .put("index.refresh_interval", "-1") .put(MockEngineSupport.DISABLE_FLUSH_ON_CLOSE.getKey(), true) // never flush - always recover from translog .put("index.routing.allocation.exclude._name", replicaNode) )); ensureYellow(); assertAcked(client().admin().indices().prepareUpdateSettings("test").setSettings(Settings.builder() .put("index.routing.allocation.exclude._name", (String)null) )); ensureGreen(); // Index some documents int numDocsToKeep = randomIntBetween(0, 100); logger.info("--> indexing [{}] docs to be kept", numDocsToKeep); IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocsToKeep]; for (int i = 0; i < builders.length; i++) { builders[i] = client().prepareIndex("test", "type").setSource("foo", "bar"); } indexRandom(false, false, false, Arrays.asList(builders)); flush("test"); disableTranslogFlush("test"); // having no extra docs is an interesting case for seq no based recoveries - test it more often int numDocsToTruncate = randomBoolean() ? 0 : randomIntBetween(0, 100); logger.info("--> indexing [{}] more docs to be truncated", numDocsToTruncate); builders = new IndexRequestBuilder[numDocsToTruncate]; for (int i = 0; i < builders.length; i++) { builders[i] = client().prepareIndex("test", "type").setSource("foo", "bar"); } indexRandom(false, false, false, Arrays.asList(builders)); final int totalDocs = numDocsToKeep + numDocsToTruncate; // sample the replica node translog dirs final ShardId shardId = new ShardId(resolveIndex("test"), 0); Set<Path> translogDirs = getTranslogDirs(replicaNode, shardId); // stop the cluster nodes. we don't use full restart so the node start up order will be the same // and shard roles will be maintained internalCluster().stopRandomDataNode(); internalCluster().stopRandomDataNode(); // Corrupt the translog file(s) logger.info("--> corrupting translog"); corruptTranslogFiles(translogDirs); // Restart the single node logger.info("--> starting node"); internalCluster().startNode(); ensureYellow(); // Run a search and make sure it succeeds assertHitCount(client().prepareSearch("test").setQuery(matchAllQuery()).get(), totalDocs); TruncateTranslogCommand ttc = new TruncateTranslogCommand(); MockTerminal t = new MockTerminal(); OptionParser parser = ttc.getParser(); for (Path translogDir : translogDirs) { final Path idxLocation = translogDir.getParent().resolve("index"); assertBusy(() -> { logger.info("--> checking that lock has been released for {}", idxLocation); try (Directory dir = FSDirectory.open(idxLocation, NativeFSLockFactory.INSTANCE); Lock writeLock = dir.obtainLock(IndexWriter.WRITE_LOCK_NAME)) { // Great, do nothing, we just wanted to obtain the lock } catch (LockObtainFailedException lofe) { logger.info("--> failed acquiring lock for {}", idxLocation); fail("still waiting for lock release at [" + idxLocation + "]"); } catch (IOException ioe) { fail("Got an IOException: " + ioe); } }); OptionSet options = parser.parse("-d", translogDir.toAbsolutePath().toString(), "-b"); logger.info("--> running truncate translog command for [{}]", translogDir.toAbsolutePath()); ttc.execute(t, options, null /* TODO: env should be real here, and ttc should actually use it... */); logger.info("--> output:\\\\n{}", t.getOutput()); } logger.info("--> starting the replica node to test recovery"); internalCluster().startNode(); ensureGreen("test"); for(String node : internalCluster().nodesInclude("test")){ assertHitCount(client().prepareSearch("test").setPreference("_only_nodes:"+node).setQuery(matchAllQuery()).get(), totalDocs); } final RecoveryResponse recoveryResponse = client().admin().indices().prepareRecoveries("test").setActiveOnly(false).get(); final RecoveryState replicaRecoveryState = recoveryResponse.shardRecoveryStates().get("test").stream() .filter(recoveryState -> recoveryState.getPrimary() == false).findFirst().get(); // the replica translog was disabled so it doesn't know what hte global checkpoint is and thus can't do ops based recovery assertThat(replicaRecoveryState.getIndex().toString(), replicaRecoveryState.getIndex().recoveredFileCount(), greaterThan(0)); }	can you add extra spacing like: for( -> for ( )){ -> )) { your ide can help with this.
public void testCorruptTranslogTruncationOfReplica() throws Exception { internalCluster().startNodes(2, Settings.EMPTY); final String primaryNode = internalCluster().getNodeNames()[0]; final String replicaNode = internalCluster().getNodeNames()[1]; assertAcked(prepareCreate("test").setSettings(Settings.builder() .put("index.number_of_shards", 1) .put("index.number_of_replicas", 1) .put("index.refresh_interval", "-1") .put(MockEngineSupport.DISABLE_FLUSH_ON_CLOSE.getKey(), true) // never flush - always recover from translog .put("index.routing.allocation.exclude._name", replicaNode) )); ensureYellow(); assertAcked(client().admin().indices().prepareUpdateSettings("test").setSettings(Settings.builder() .put("index.routing.allocation.exclude._name", (String)null) )); ensureGreen(); // Index some documents int numDocsToKeep = randomIntBetween(0, 100); logger.info("--> indexing [{}] docs to be kept", numDocsToKeep); IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocsToKeep]; for (int i = 0; i < builders.length; i++) { builders[i] = client().prepareIndex("test", "type").setSource("foo", "bar"); } indexRandom(false, false, false, Arrays.asList(builders)); flush("test"); disableTranslogFlush("test"); // having no extra docs is an interesting case for seq no based recoveries - test it more often int numDocsToTruncate = randomBoolean() ? 0 : randomIntBetween(0, 100); logger.info("--> indexing [{}] more docs to be truncated", numDocsToTruncate); builders = new IndexRequestBuilder[numDocsToTruncate]; for (int i = 0; i < builders.length; i++) { builders[i] = client().prepareIndex("test", "type").setSource("foo", "bar"); } indexRandom(false, false, false, Arrays.asList(builders)); final int totalDocs = numDocsToKeep + numDocsToTruncate; // sample the replica node translog dirs final ShardId shardId = new ShardId(resolveIndex("test"), 0); Set<Path> translogDirs = getTranslogDirs(replicaNode, shardId); // stop the cluster nodes. we don't use full restart so the node start up order will be the same // and shard roles will be maintained internalCluster().stopRandomDataNode(); internalCluster().stopRandomDataNode(); // Corrupt the translog file(s) logger.info("--> corrupting translog"); corruptTranslogFiles(translogDirs); // Restart the single node logger.info("--> starting node"); internalCluster().startNode(); ensureYellow(); // Run a search and make sure it succeeds assertHitCount(client().prepareSearch("test").setQuery(matchAllQuery()).get(), totalDocs); TruncateTranslogCommand ttc = new TruncateTranslogCommand(); MockTerminal t = new MockTerminal(); OptionParser parser = ttc.getParser(); for (Path translogDir : translogDirs) { final Path idxLocation = translogDir.getParent().resolve("index"); assertBusy(() -> { logger.info("--> checking that lock has been released for {}", idxLocation); try (Directory dir = FSDirectory.open(idxLocation, NativeFSLockFactory.INSTANCE); Lock writeLock = dir.obtainLock(IndexWriter.WRITE_LOCK_NAME)) { // Great, do nothing, we just wanted to obtain the lock } catch (LockObtainFailedException lofe) { logger.info("--> failed acquiring lock for {}", idxLocation); fail("still waiting for lock release at [" + idxLocation + "]"); } catch (IOException ioe) { fail("Got an IOException: " + ioe); } }); OptionSet options = parser.parse("-d", translogDir.toAbsolutePath().toString(), "-b"); logger.info("--> running truncate translog command for [{}]", translogDir.toAbsolutePath()); ttc.execute(t, options, null /* TODO: env should be real here, and ttc should actually use it... */); logger.info("--> output:\\\\n{}", t.getOutput()); } logger.info("--> starting the replica node to test recovery"); internalCluster().startNode(); ensureGreen("test"); for(String node : internalCluster().nodesInclude("test")){ assertHitCount(client().prepareSearch("test").setPreference("_only_nodes:"+node).setQuery(matchAllQuery()).get(), totalDocs); } final RecoveryResponse recoveryResponse = client().admin().indices().prepareRecoveries("test").setActiveOnly(false).get(); final RecoveryState replicaRecoveryState = recoveryResponse.shardRecoveryStates().get("test").stream() .filter(recoveryState -> recoveryState.getPrimary() == false).findFirst().get(); // the replica translog was disabled so it doesn't know what hte global checkpoint is and thus can't do ops based recovery assertThat(replicaRecoveryState.getIndex().toString(), replicaRecoveryState.getIndex().recoveredFileCount(), greaterThan(0)); }	extra spacing here: for( -> for ( )){ -> )) {
public void testCorruptTranslogTruncationOfReplica() throws Exception { internalCluster().startNodes(2, Settings.EMPTY); final String primaryNode = internalCluster().getNodeNames()[0]; final String replicaNode = internalCluster().getNodeNames()[1]; assertAcked(prepareCreate("test").setSettings(Settings.builder() .put("index.number_of_shards", 1) .put("index.number_of_replicas", 1) .put("index.refresh_interval", "-1") .put(MockEngineSupport.DISABLE_FLUSH_ON_CLOSE.getKey(), true) // never flush - always recover from translog .put("index.routing.allocation.exclude._name", replicaNode) )); ensureYellow(); assertAcked(client().admin().indices().prepareUpdateSettings("test").setSettings(Settings.builder() .put("index.routing.allocation.exclude._name", (String)null) )); ensureGreen(); // Index some documents int numDocsToKeep = randomIntBetween(0, 100); logger.info("--> indexing [{}] docs to be kept", numDocsToKeep); IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocsToKeep]; for (int i = 0; i < builders.length; i++) { builders[i] = client().prepareIndex("test", "type").setSource("foo", "bar"); } indexRandom(false, false, false, Arrays.asList(builders)); flush("test"); disableTranslogFlush("test"); // having no extra docs is an interesting case for seq no based recoveries - test it more often int numDocsToTruncate = randomBoolean() ? 0 : randomIntBetween(0, 100); logger.info("--> indexing [{}] more docs to be truncated", numDocsToTruncate); builders = new IndexRequestBuilder[numDocsToTruncate]; for (int i = 0; i < builders.length; i++) { builders[i] = client().prepareIndex("test", "type").setSource("foo", "bar"); } indexRandom(false, false, false, Arrays.asList(builders)); final int totalDocs = numDocsToKeep + numDocsToTruncate; // sample the replica node translog dirs final ShardId shardId = new ShardId(resolveIndex("test"), 0); Set<Path> translogDirs = getTranslogDirs(replicaNode, shardId); // stop the cluster nodes. we don't use full restart so the node start up order will be the same // and shard roles will be maintained internalCluster().stopRandomDataNode(); internalCluster().stopRandomDataNode(); // Corrupt the translog file(s) logger.info("--> corrupting translog"); corruptTranslogFiles(translogDirs); // Restart the single node logger.info("--> starting node"); internalCluster().startNode(); ensureYellow(); // Run a search and make sure it succeeds assertHitCount(client().prepareSearch("test").setQuery(matchAllQuery()).get(), totalDocs); TruncateTranslogCommand ttc = new TruncateTranslogCommand(); MockTerminal t = new MockTerminal(); OptionParser parser = ttc.getParser(); for (Path translogDir : translogDirs) { final Path idxLocation = translogDir.getParent().resolve("index"); assertBusy(() -> { logger.info("--> checking that lock has been released for {}", idxLocation); try (Directory dir = FSDirectory.open(idxLocation, NativeFSLockFactory.INSTANCE); Lock writeLock = dir.obtainLock(IndexWriter.WRITE_LOCK_NAME)) { // Great, do nothing, we just wanted to obtain the lock } catch (LockObtainFailedException lofe) { logger.info("--> failed acquiring lock for {}", idxLocation); fail("still waiting for lock release at [" + idxLocation + "]"); } catch (IOException ioe) { fail("Got an IOException: " + ioe); } }); OptionSet options = parser.parse("-d", translogDir.toAbsolutePath().toString(), "-b"); logger.info("--> running truncate translog command for [{}]", translogDir.toAbsolutePath()); ttc.execute(t, options, null /* TODO: env should be real here, and ttc should actually use it... */); logger.info("--> output:\\\\n{}", t.getOutput()); } logger.info("--> starting the replica node to test recovery"); internalCluster().startNode(); ensureGreen("test"); for(String node : internalCluster().nodesInclude("test")){ assertHitCount(client().prepareSearch("test").setPreference("_only_nodes:"+node).setQuery(matchAllQuery()).get(), totalDocs); } final RecoveryResponse recoveryResponse = client().admin().indices().prepareRecoveries("test").setActiveOnly(false).get(); final RecoveryState replicaRecoveryState = recoveryResponse.shardRecoveryStates().get("test").stream() .filter(recoveryState -> recoveryState.getPrimary() == false).findFirst().get(); // the replica translog was disabled so it doesn't know what hte global checkpoint is and thus can't do ops based recovery assertThat(replicaRecoveryState.getIndex().toString(), replicaRecoveryState.getIndex().recoveredFileCount(), greaterThan(0)); }	spacing here: :"+node -> :" + node
public void testNewReplicasWork() throws Exception { if (runningAgainstOldCluster) { XContentBuilder mappingsAndSettings = jsonBuilder(); mappingsAndSettings.startObject(); { mappingsAndSettings.startObject("settings"); mappingsAndSettings.field("number_of_shards", 1); mappingsAndSettings.field("number_of_replicas", 0); mappingsAndSettings.endObject(); } { mappingsAndSettings.startObject("mappings"); mappingsAndSettings.startObject("doc"); mappingsAndSettings.startObject("properties"); { mappingsAndSettings.startObject("field"); mappingsAndSettings.field("type", "text"); mappingsAndSettings.endObject(); } mappingsAndSettings.endObject(); mappingsAndSettings.endObject(); mappingsAndSettings.endObject(); } mappingsAndSettings.endObject(); client().performRequest("PUT", "/" + index, Collections.emptyMap(), new StringEntity(mappingsAndSettings.string(), ContentType.APPLICATION_JSON)); int numDocs = randomIntBetween(2000, 3000); indexRandomDocuments(numDocs, true, false, i -> { return JsonXContent.contentBuilder().startObject() .field("field", "value") .endObject(); }); logger.info("Refreshing [{}]", index); client().performRequest("POST", "/" + index + "/_refresh"); } else { final int numReplicas = 1; final long startTime = System.currentTimeMillis(); logger.debug("--> creating [{}] replicas for index [{}]", numReplicas, index); String requestBody = "{ \\\\"index\\\\": { \\\\"number_of_replicas\\\\" : " + numReplicas + " }}"; Response response = client().performRequest("PUT", "/" + index + "/_settings", Collections.emptyMap(), new StringEntity(requestBody, ContentType.APPLICATION_JSON)); assertEquals(200, response.getStatusLine().getStatusCode()); Map<String, String> params = new HashMap<>(); params.put("timeout", "2m"); params.put("wait_for_status", "green"); params.put("wait_for_no_relocating_shards", "true"); params.put("wait_for_events", "languid"); Map<String, Object> healthRsp = toMap(client().performRequest("GET", "/_cluster/health/" + index, params)); assertEquals("green", healthRsp.get("status")); assertFalse((Boolean) healthRsp.get("timed_out")); logger.debug("--> index [{}] is green, took [{}] ms", index, (System.currentTimeMillis() - startTime)); Map<String, Object> recoverRsp = toMap(client().performRequest("GET", "/" + index + "/_recovery")); logger.debug("--> recovery status:\\\\n{}", recoverRsp); Set<Integer> counts = new HashSet<>(); for(String node : dataNodes(index, client())){ Map<String, Object> responseBody = toMap(client().performRequest("GET", "/" + index + "/_search", Collections.singletonMap("preference", "_only_nodes:"+node))); assertNoFailures(responseBody); int hits = (int) XContentMapValues.extractValue("hits.total", responseBody); counts.add(hits); } assertEquals("All nodes should have a consistent number of documents", 1, counts.size()); } }	spacing here: for( -> for ( )){ -> )) {
public void testNewReplicasWork() throws Exception { if (runningAgainstOldCluster) { XContentBuilder mappingsAndSettings = jsonBuilder(); mappingsAndSettings.startObject(); { mappingsAndSettings.startObject("settings"); mappingsAndSettings.field("number_of_shards", 1); mappingsAndSettings.field("number_of_replicas", 0); mappingsAndSettings.endObject(); } { mappingsAndSettings.startObject("mappings"); mappingsAndSettings.startObject("doc"); mappingsAndSettings.startObject("properties"); { mappingsAndSettings.startObject("field"); mappingsAndSettings.field("type", "text"); mappingsAndSettings.endObject(); } mappingsAndSettings.endObject(); mappingsAndSettings.endObject(); mappingsAndSettings.endObject(); } mappingsAndSettings.endObject(); client().performRequest("PUT", "/" + index, Collections.emptyMap(), new StringEntity(mappingsAndSettings.string(), ContentType.APPLICATION_JSON)); int numDocs = randomIntBetween(2000, 3000); indexRandomDocuments(numDocs, true, false, i -> { return JsonXContent.contentBuilder().startObject() .field("field", "value") .endObject(); }); logger.info("Refreshing [{}]", index); client().performRequest("POST", "/" + index + "/_refresh"); } else { final int numReplicas = 1; final long startTime = System.currentTimeMillis(); logger.debug("--> creating [{}] replicas for index [{}]", numReplicas, index); String requestBody = "{ \\\\"index\\\\": { \\\\"number_of_replicas\\\\" : " + numReplicas + " }}"; Response response = client().performRequest("PUT", "/" + index + "/_settings", Collections.emptyMap(), new StringEntity(requestBody, ContentType.APPLICATION_JSON)); assertEquals(200, response.getStatusLine().getStatusCode()); Map<String, String> params = new HashMap<>(); params.put("timeout", "2m"); params.put("wait_for_status", "green"); params.put("wait_for_no_relocating_shards", "true"); params.put("wait_for_events", "languid"); Map<String, Object> healthRsp = toMap(client().performRequest("GET", "/_cluster/health/" + index, params)); assertEquals("green", healthRsp.get("status")); assertFalse((Boolean) healthRsp.get("timed_out")); logger.debug("--> index [{}] is green, took [{}] ms", index, (System.currentTimeMillis() - startTime)); Map<String, Object> recoverRsp = toMap(client().performRequest("GET", "/" + index + "/_recovery")); logger.debug("--> recovery status:\\\\n{}", recoverRsp); Set<Integer> counts = new HashSet<>(); for(String node : dataNodes(index, client())){ Map<String, Object> responseBody = toMap(client().performRequest("GET", "/" + index + "/_search", Collections.singletonMap("preference", "_only_nodes:"+node))); assertNoFailures(responseBody); int hits = (int) XContentMapValues.extractValue("hits.total", responseBody); counts.add(hits); } assertEquals("All nodes should have a consistent number of documents", 1, counts.size()); } }	spacing here: :"+node -> :" + node
public MatchQueryBuilder zeroTermsQuery(MatchQuery.ZeroTermsQuery zeroTermsQuery) { if (zeroTermsQuery == null) { throw new IllegalArgumentException("[" + NAME + "] requires zeroTermsQuery to be non-null"); } this.zeroTermsQuery = zeroTermsQuery; return this; } /** * Sets query to use when single term synonyms overlap positions * {@link MatchQuery.SynonymQueryStyle#BLENDED_TERMS}, but can be set to * {@link MatchQuery.SynonymQueryStyle#MOST_TERMS} or {@link MatchQuery.SynonymQueryStyle#BEST_TERMS}	wouldn't it be better if the exception uses synonym_query_style instead of synquerystyle. it would be more clear, i think.
public MultiMatchQueryBuilder synonymQueryStyle(MatchQuery.SynonymQueryStyle synQueryStyle) { if (synonymQueryStyle == null) { throw new IllegalArgumentException("[" + NAME + "] requires synonym query style to be non-null"); } this.synonymQueryStyle = synQueryStyle; return this; }	here it is more clean, but again i think using synonym_query_style would be better
@Override protected Query newSynonymQuery(Term[] terms) { switch (this.synonymQueryStyle) { case BEST_TERMS: List<Query> currPosnClauses = new ArrayList<Query>(terms.length); for (Term term : terms) { currPosnClauses.add(newTermQuery(term)); } return new DisjunctionMaxQuery(currPosnClauses, 0.0f); case MOST_TERMS: BooleanQuery.Builder builder = new BooleanQuery.Builder(); for (Term term : terms) { builder.add(newTermQuery(term), BooleanClause.Occur.SHOULD); } return builder.build(); case BLENDED_TERMS: return blendTermsQuery(terms, mapper); default: throw new IllegalStateException("unrecognized synonymQueryStyle passed when creating newSynonymQuery"); } }	for easier debugging it would be good to print the (unknown) value of this.synonymquerystyle
static void prohibitAppendOnlyWritesInBackingIndices(DocWriteRequest<?> writeRequest, Metadata metadata) { if (writeRequest.index().startsWith(".ds-") == false) { // This is definitively not a backing index of data stream, skip further checking return; } // Extract data stream name and check if a composable template with a data stream definition would match with it: int indexOfLastDash = writeRequest.index().lastIndexOf('-'); if (indexOfLastDash == -1 || indexOfLastDash == 3) { return; } String dataStreamName = writeRequest.index().substring(4, indexOfLastDash); String templateId = findV2Template(metadata, dataStreamName, false); if (templateId == null) { return; } ComposableIndexTemplate template = metadata.templatesV2().get(templateId); if (template.getDataStreamTemplate() == null) { return; } // At this point with write op is targeting a data stream directly,so // checking if write op is append-only and if so fail. // (Updates and deletes are allowed to target a backing index) DocWriteRequest.OpType opType = writeRequest.opType(); // CREATE op_type is considered append-only and // INDEX op_type is considered append-only when no if_primary_term and if_seq_no is specified. // (the latter maybe an update, but at this stage we can't determine that. In order to determine // that an engine level change is needed and for now this check is sufficient.) if (opType == DocWriteRequest.OpType.CREATE || (opType == DocWriteRequest.OpType.INDEX && writeRequest.ifPrimaryTerm() == UNASSIGNED_PRIMARY_TERM && writeRequest.ifSeqNo() == UNASSIGNED_SEQ_NO)) { throw new IllegalArgumentException("append-only write targeting backing indices is disallowed," + "target corresponding data stream instead"); } }	i would prefer to lookup the index and check if it is part of a data-stream. once we can adopt indices this check will no longer be sufficient.
static void prohibitAppendOnlyWritesInBackingIndices(DocWriteRequest<?> writeRequest, Metadata metadata) { if (writeRequest.index().startsWith(".ds-") == false) { // This is definitively not a backing index of data stream, skip further checking return; } // Extract data stream name and check if a composable template with a data stream definition would match with it: int indexOfLastDash = writeRequest.index().lastIndexOf('-'); if (indexOfLastDash == -1 || indexOfLastDash == 3) { return; } String dataStreamName = writeRequest.index().substring(4, indexOfLastDash); String templateId = findV2Template(metadata, dataStreamName, false); if (templateId == null) { return; } ComposableIndexTemplate template = metadata.templatesV2().get(templateId); if (template.getDataStreamTemplate() == null) { return; } // At this point with write op is targeting a data stream directly,so // checking if write op is append-only and if so fail. // (Updates and deletes are allowed to target a backing index) DocWriteRequest.OpType opType = writeRequest.opType(); // CREATE op_type is considered append-only and // INDEX op_type is considered append-only when no if_primary_term and if_seq_no is specified. // (the latter maybe an update, but at this stage we can't determine that. In order to determine // that an engine level change is needed and for now this check is sufficient.) if (opType == DocWriteRequest.OpType.CREATE || (opType == DocWriteRequest.OpType.INDEX && writeRequest.ifPrimaryTerm() == UNASSIGNED_PRIMARY_TERM && writeRequest.ifSeqNo() == UNASSIGNED_SEQ_NO)) { throw new IllegalArgumentException("append-only write targeting backing indices is disallowed," + "target corresponding data stream instead"); } }	using the lookup instead, we can also find the data-stream directly here. also, i think that if the index does not yet exist, there is no reason for special handling and thus no reason to look for a template?
public void testKerberosRealmWithInvalidKeytabPathConfigurations() throws IOException { final String keytabPathCase = randomFrom("keytabPathAsDirectory", "keytabFileDoesNotExist", "keytabPathWithNoReadPermissions"); final String expectedErrorMessage; final String keytabPath; final Set<PosixFilePermission> filePerms; switch (keytabPathCase) { case "keytabPathAsDirectory": final String dirName = randomAlphaOfLength(5); Files.createDirectory(dir.resolve(dirName)); keytabPath = dir.resolve(dirName).toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] is a directory"; break; case "keytabFileDoesNotExist": keytabPath = dir.resolve(randomAlphaOfLength(5) + ".keytab").toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] does not exist"; break; case "keytabPathWithNoReadPermissions": final String fileName = randomAlphaOfLength(5); final Path keytabFilePath = Files.createTempFile(dir, fileName, ".keytab"); Files.write(keytabFilePath, randomAlphaOfLength(5).getBytes(StandardCharsets.UTF_8)); final Set<String> supportedAttributes = keytabFilePath.getFileSystem().supportedFileAttributeViews(); if (supportedAttributes.contains("posix")) { final PosixFileAttributeView fileAttributeView = Files.getFileAttributeView(keytabFilePath, PosixFileAttributeView.class); fileAttributeView.setPermissions(PosixFilePermissions.fromString("---------")); } else if (supportedAttributes.contains("acl")) { final UserPrincipal principal = Files.getOwner(keytabFilePath); final AclFileAttributeView view = Files.getFileAttributeView(keytabFilePath, AclFileAttributeView.class); final AclEntry entry = AclEntry.newBuilder() .setType(AclEntryType.DENY) .setPrincipal(principal) .setPermissions(AclEntryPermission.READ_DATA, AclEntryPermission.READ_ATTRIBUTES).build(); final List<AclEntry> acl = view.getAcl(); acl.add(0, entry); view.setAcl(acl); } else if (supportedAttributes.contains("dos")) { final DosFileAttributeView fileAttributeView = Files.getFileAttributeView(keytabFilePath, DosFileAttributeView.class); fileAttributeView.setReadOnly(false); } else { throw new UnsupportedOperationException("Unsupported file attributes for this test"); } keytabPath = keytabFilePath.toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] must have read permission"; break; default: throw new IllegalArgumentException("Unknown test case :" + keytabPathCase); } settings = KerberosTestCase.buildKerberosRealmSettings(keytabPath, 100, "10m", true, randomBoolean()); config = new RealmConfig("test-kerb-realm", settings, globalSettings, TestEnvironment.newEnvironment(globalSettings), new ThreadContext(globalSettings)); mockNativeRoleMappingStore = roleMappingStore(Arrays.asList("user")); mockKerberosTicketValidator = mock(KerberosTicketValidator.class); final IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () -> new KerberosRealm(config, mockNativeRoleMappingStore, mockKerberosTicketValidator, threadPool, null)); assertThat(iae.getMessage(), is(equalTo(expectedErrorMessage))); }	? how does making it not-read only help?
public void testAccountUpdateSettings() throws Exception { final Setting<SecureString> secureSetting = SecureSetting.secureString("xpack.notification.test.account.x.secure", null); final Setting<String> setting = Setting.simpleString("xpack.notification.test.account.x.dynamic", Setting.Property.Dynamic, Setting.Property.NodeScope); final AtomicReference<String> secureSettingValue = new AtomicReference<String>(randomAlphaOfLength(4)); final AtomicReference<String> settingValue = new AtomicReference<String>(randomAlphaOfLength(4)); final Map<String, char[]> secureSettingsMap = new HashMap<>(); final AtomicInteger validationInvocationCount = new AtomicInteger(0); secureSettingsMap.put(secureSetting.getKey(), secureSettingValue.get().toCharArray()); final Settings.Builder settingsBuilder = Settings.builder() .put(setting.getKey(), settingValue.get()) .setSecureSettings(secureSettingsFromMap(secureSettingsMap)); final TestNotificationService service = new TestNotificationService(settingsBuilder.build(), Arrays.asList(secureSetting), (String name, Settings accountSettings) -> { assertThat(accountSettings.get("dynamic"), is(settingValue.get())); assertThat(SecureSetting.secureString("secure", null).get(accountSettings), is(secureSettingValue.get())); validationInvocationCount.incrementAndGet(); }); assertThat(validationInvocationCount.get(), is(0)); service.getAccount(null); assertThat(validationInvocationCount.get(), is(1)); // update secure setting only secureSettingValue.set(randomAlphaOfLength(4)); secureSettingsMap.put(secureSetting.getKey(), secureSettingValue.get().toCharArray()); service.reload(settingsBuilder.build()); assertThat(validationInvocationCount.get(), is(1)); service.getAccount(null); assertThat(validationInvocationCount.get(), is(2)); // update dynamic cluster setting only settingValue.set(randomAlphaOfLength(4)); settingsBuilder.put(setting.getKey(), settingValue.get()); service.clusterSettingsConsumer(settingsBuilder.build()); assertThat(validationInvocationCount.get(), is(2)); service.getAccount(null); assertThat(validationInvocationCount.get(), is(3)); // update both if (randomBoolean()) { // update secure first secureSettingValue.set(randomAlphaOfLength(4)); secureSettingsMap.put(secureSetting.getKey(), secureSettingValue.get().toCharArray()); service.reload(settingsBuilder.build()); // update cluster second settingValue.set(randomAlphaOfLength(4)); settingsBuilder.put(setting.getKey(), settingValue.get()); service.clusterSettingsConsumer(settingsBuilder.build()); } else { // update cluster first settingValue.set(randomAlphaOfLength(4)); settingsBuilder.put(setting.getKey(), settingValue.get()); service.clusterSettingsConsumer(settingsBuilder.build()); // update secure second secureSettingValue.set(randomAlphaOfLength(4)); secureSettingsMap.put(secureSetting.getKey(), secureSettingValue.get().toCharArray()); service.reload(settingsBuilder.build()); } assertThat(validationInvocationCount.get(), is(3)); service.getAccount(null); assertThat(validationInvocationCount.get(), is(4)); }	putting these three line calls in dedicated methods allows you to get rid of the "first/second" comments and its easy to figure out the order when reading
@Override public void onRemoval(ShardId shardId, Accountable accountable) { } }), (ft, idxName, lookup) -> ft.fielddataBuilder(idxName, lookup) .build(new IndexFieldDataCache.None(), new NoneCircuitBreakerService()), mapperService, mapperService.mappingLookup(), similarityService, null, parserConfig(), writableRegistry(), null, searcher, () -> nowInMillis, null, null, () -> true, null, Collections.emptyMap() ); } protected BiFunction<MappedFieldType, Supplier<SearchLookup>, IndexFieldData<?>> fieldDataLookup() { return (mft, lookupSource) -> mft.fielddataBuilder("test", lookupSource) .build(new IndexFieldDataCache.None(), new NoneCircuitBreakerService()); } protected final String syntheticSource(DocumentMapper mapper, CheckedConsumer<XContentBuilder, IOException> build) throws IOException { try (Directory directory = newDirectory()) { RandomIndexWriter iw = new RandomIndexWriter(random(), directory); iw.addDocument(mapper.parse(source(build)).rootDoc()); iw.close(); try (DirectoryReader reader = DirectoryReader.open(directory)) { SourceLoader loader = mapper.sourceMapper().newSourceLoader(mapper.mapping().getRoot()); String syntheticSource = loader.leaf(getOnlyLeafReader(reader)).source(null, 0).utf8ToString(); try (Directory roundTripDirectory = newDirectory()) { RandomIndexWriter roundTripIw = new RandomIndexWriter(random(), roundTripDirectory); roundTripIw.addDocument(mapper.parse(source(build)).rootDoc()); roundTripIw.close(); try (DirectoryReader roundTripReader = DirectoryReader.open(directory)) { String roundTripSyntheticSource = loader.leaf(getOnlyLeafReader(reader)).source(null, 0).utf8ToString(); assertThat(roundTripSyntheticSource, equalTo(syntheticSource)); assertReaderEquals("round trip " + syntheticSource, reader, roundTripReader); } } return syntheticSource; } } }	this line is wrong. oops.
public void testSuccessfulSnapshotAndRestore() { setupTestCluster(randomFrom(1, 3, 5), randomIntBetween(2, 10)); String repoName = "repo"; String snapshotName = "snapshot"; final String index = "test"; final int shards = randomIntBetween(1, 10); final int documents = randomIntBetween(0, 100); final StepListener<AcknowledgedResponse> createRepositoryListener = new StepListener<>(); final TestClusterNode masterNode = testClusterNodes.currentMaster(testClusterNodes.nodes.values().iterator().next().clusterService.state()); masterNode.client.admin().cluster().preparePutRepository(repoName) .setType(FsRepository.TYPE).setSettings(Settings.builder().put("location", randomAlphaOfLength(10))) .execute(createRepositoryListener); final StepListener<CreateIndexResponse> createIndexResponseStepListener = new StepListener<>(); createRepositoryListener.whenComplete(acknowledgedResponse -> masterNode.client.admin().indices().create( new CreateIndexRequest(index).waitForActiveShards(ActiveShardCount.ALL).settings(defaultIndexSettings(shards)), createIndexResponseStepListener), SnapshotResiliencyTests::rethrowAssertion); final StepListener<CreateSnapshotResponse> createSnapshotResponseListener = new StepListener<>(); createIndexResponseStepListener.whenComplete(createIndexResponse -> { final Runnable afterIndexing = () -> masterNode.client.admin().cluster().prepareCreateSnapshot(repoName, snapshotName) .setWaitForCompletion(true).execute(createSnapshotResponseListener); final AtomicInteger countdown = new AtomicInteger(documents); for (int i = 0; i < documents; ++i) { masterNode.client.bulk( new BulkRequest().add(new IndexRequest(index).source(Collections.singletonMap("foo", "bar" + i))) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE), assertNoFailureListener( bulkResponse -> { assertFalse("Failures in bulk response: " + bulkResponse.buildFailureMessage(), bulkResponse.hasFailures()); if (countdown.decrementAndGet() == 0) { afterIndexing.run(); } })); } if (documents == 0) { afterIndexing.run(); } }, SnapshotResiliencyTests::rethrowAssertion); final StepListener<AcknowledgedResponse> deleteIndexListener = new StepListener<>(); final AtomicBoolean createdSnapshot = new AtomicBoolean(); createSnapshotResponseListener.whenComplete( createSnapshotResponse -> { createdSnapshot.set(true); masterNode.client.admin().indices().delete(new DeleteIndexRequest(index), deleteIndexListener); }, SnapshotResiliencyTests::rethrowAssertion); final StepListener<RestoreSnapshotResponse> restoreSnapshotResponseListener = new StepListener<>(); deleteIndexListener.whenComplete(ignored -> masterNode.client.admin().cluster().restoreSnapshot( new RestoreSnapshotRequest(repoName, snapshotName).waitForCompletion(true), restoreSnapshotResponseListener), SnapshotResiliencyTests::rethrowAssertion); final StepListener<SearchResponse> searchResponseListener = new StepListener<>(); final AtomicBoolean snapshotRestored = new AtomicBoolean(); restoreSnapshotResponseListener.whenComplete(restoreSnapshotResponse -> { snapshotRestored.set(true); assertEquals(shards, restoreSnapshotResponse.getRestoreInfo().totalShards()); masterNode.client.search( new SearchRequest(index).source(new SearchSourceBuilder().size(0).trackTotalHits(true)), searchResponseListener); }, SnapshotResiliencyTests::rethrowAssertion); final AtomicBoolean documentCountVerified = new AtomicBoolean(); searchResponseListener.whenComplete(r -> { assertEquals(documents, Objects.requireNonNull(r.getHits().getTotalHits()).value); documentCountVerified.set(true); }, SnapshotResiliencyTests::rethrowAssertion); runUntil(documentCountVerified::get, TimeUnit.MINUTES.toMillis(5L)); assertTrue(createdSnapshot.get()); assertTrue(snapshotRestored.get()); assertTrue(documentCountVerified.get()); SnapshotsInProgress finalSnapshotsInProgress = masterNode.clusterService.state().custom(SnapshotsInProgress.TYPE); assertFalse(finalSnapshotsInProgress.entries().stream().anyMatch(entry -> entry.state().completed() == false)); final Repository repository = masterNode.repositoriesService.repository(repoName); Collection<SnapshotId> snapshotIds = repository.getRepositoryData().getSnapshotIds(); assertThat(snapshotIds, hasSize(1)); final SnapshotInfo snapshotInfo = repository.getSnapshotInfo(snapshotIds.iterator().next()); assertEquals(SnapshotState.SUCCESS, snapshotInfo.state()); assertThat(snapshotInfo.indices(), containsInAnyOrder(index)); assertEquals(shards, snapshotInfo.successfulShards()); assertEquals(0, snapshotInfo.failedShards()); }	1. if we are using bulk, why not to prepare the single bulk request and send it just once and avoid countdownlatch usage? 2. after doing it, i would create separate bulkresponselistener and chain it with createsnapshotresponselistener.
public void testSuccessfulSnapshotAndRestore() { setupTestCluster(randomFrom(1, 3, 5), randomIntBetween(2, 10)); String repoName = "repo"; String snapshotName = "snapshot"; final String index = "test"; final int shards = randomIntBetween(1, 10); final int documents = randomIntBetween(0, 100); final StepListener<AcknowledgedResponse> createRepositoryListener = new StepListener<>(); final TestClusterNode masterNode = testClusterNodes.currentMaster(testClusterNodes.nodes.values().iterator().next().clusterService.state()); masterNode.client.admin().cluster().preparePutRepository(repoName) .setType(FsRepository.TYPE).setSettings(Settings.builder().put("location", randomAlphaOfLength(10))) .execute(createRepositoryListener); final StepListener<CreateIndexResponse> createIndexResponseStepListener = new StepListener<>(); createRepositoryListener.whenComplete(acknowledgedResponse -> masterNode.client.admin().indices().create( new CreateIndexRequest(index).waitForActiveShards(ActiveShardCount.ALL).settings(defaultIndexSettings(shards)), createIndexResponseStepListener), SnapshotResiliencyTests::rethrowAssertion); final StepListener<CreateSnapshotResponse> createSnapshotResponseListener = new StepListener<>(); createIndexResponseStepListener.whenComplete(createIndexResponse -> { final Runnable afterIndexing = () -> masterNode.client.admin().cluster().prepareCreateSnapshot(repoName, snapshotName) .setWaitForCompletion(true).execute(createSnapshotResponseListener); final AtomicInteger countdown = new AtomicInteger(documents); for (int i = 0; i < documents; ++i) { masterNode.client.bulk( new BulkRequest().add(new IndexRequest(index).source(Collections.singletonMap("foo", "bar" + i))) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE), assertNoFailureListener( bulkResponse -> { assertFalse("Failures in bulk response: " + bulkResponse.buildFailureMessage(), bulkResponse.hasFailures()); if (countdown.decrementAndGet() == 0) { afterIndexing.run(); } })); } if (documents == 0) { afterIndexing.run(); } }, SnapshotResiliencyTests::rethrowAssertion); final StepListener<AcknowledgedResponse> deleteIndexListener = new StepListener<>(); final AtomicBoolean createdSnapshot = new AtomicBoolean(); createSnapshotResponseListener.whenComplete( createSnapshotResponse -> { createdSnapshot.set(true); masterNode.client.admin().indices().delete(new DeleteIndexRequest(index), deleteIndexListener); }, SnapshotResiliencyTests::rethrowAssertion); final StepListener<RestoreSnapshotResponse> restoreSnapshotResponseListener = new StepListener<>(); deleteIndexListener.whenComplete(ignored -> masterNode.client.admin().cluster().restoreSnapshot( new RestoreSnapshotRequest(repoName, snapshotName).waitForCompletion(true), restoreSnapshotResponseListener), SnapshotResiliencyTests::rethrowAssertion); final StepListener<SearchResponse> searchResponseListener = new StepListener<>(); final AtomicBoolean snapshotRestored = new AtomicBoolean(); restoreSnapshotResponseListener.whenComplete(restoreSnapshotResponse -> { snapshotRestored.set(true); assertEquals(shards, restoreSnapshotResponse.getRestoreInfo().totalShards()); masterNode.client.search( new SearchRequest(index).source(new SearchSourceBuilder().size(0).trackTotalHits(true)), searchResponseListener); }, SnapshotResiliencyTests::rethrowAssertion); final AtomicBoolean documentCountVerified = new AtomicBoolean(); searchResponseListener.whenComplete(r -> { assertEquals(documents, Objects.requireNonNull(r.getHits().getTotalHits()).value); documentCountVerified.set(true); }, SnapshotResiliencyTests::rethrowAssertion); runUntil(documentCountVerified::get, TimeUnit.MINUTES.toMillis(5L)); assertTrue(createdSnapshot.get()); assertTrue(snapshotRestored.get()); assertTrue(documentCountVerified.get()); SnapshotsInProgress finalSnapshotsInProgress = masterNode.clusterService.state().custom(SnapshotsInProgress.TYPE); assertFalse(finalSnapshotsInProgress.entries().stream().anyMatch(entry -> entry.state().completed() == false)); final Repository repository = masterNode.repositoriesService.repository(repoName); Collection<SnapshotId> snapshotIds = repository.getRepositoryData().getSnapshotIds(); assertThat(snapshotIds, hasSize(1)); final SnapshotInfo snapshotInfo = repository.getSnapshotInfo(snapshotIds.iterator().next()); assertEquals(SnapshotState.SUCCESS, snapshotInfo.state()); assertThat(snapshotInfo.indices(), containsInAnyOrder(index)); assertEquals(shards, snapshotInfo.successfulShards()); assertEquals(0, snapshotInfo.failedShards()); }	we can avoid this atomic and atomics below and use, listener.result() instead
public static TermVectorsResponse getTermVectors(IndexShard indexShard, TermVectorsRequest request) { final long startTime = System.currentTimeMillis(); final TermVectorsResponse termVectorsResponse = new TermVectorsResponse(indexShard.shardId().getIndex().getName(), request.type(), request.id()); final Term uidTerm = new Term(UidFieldMapper.NAME, Uid.createUidAsBytes(request.type(), request.id())); Engine.GetResult get = indexShard.get(new Engine.Get(request.realtime(), uidTerm).version(request.version()).versionType(request.versionType())); Fields termVectorsByField = null; boolean docFromTranslog = get.source() != null; AggregatedDfs dfs = null; TermVectorsFilter termVectorsFilter = null; /* fetched from translog is treated as an artificial document */ if (docFromTranslog) { request.doc(get.source().source, false); termVectorsResponse.setDocVersion(get.version()); } /* handle potential wildcards in fields */ if (request.selectedFields() != null) { handleFieldWildcards(indexShard, request); } final Engine.Searcher searcher = indexShard.acquireSearcher("term_vector"); try { Fields topLevelFields = MultiFields.getFields(get.searcher() != null ? get.searcher().reader() : searcher.reader()); Versions.DocIdAndVersion docIdAndVersion = get.docIdAndVersion(); /* from an artificial document */ if (request.doc() != null) { termVectorsByField = generateTermVectorsFromDoc(indexShard, request, !docFromTranslog); // if no document indexed in shard, take the queried document itself for stats if (topLevelFields == null) { topLevelFields = termVectorsByField; } termVectorsResponse.setArtificial(!docFromTranslog); termVectorsResponse.setExists(true); } /* or from an existing document */ else if (docIdAndVersion != null) { // fields with stored term vectors termVectorsByField = docIdAndVersion.context.reader().getTermVectors(docIdAndVersion.docId); Set<String> selectedFields = request.selectedFields(); // generate tvs for fields where analyzer is overridden if (selectedFields == null && request.perFieldAnalyzer() != null) { selectedFields = getFieldsToGenerate(request.perFieldAnalyzer(), termVectorsByField); } // fields without term vectors if (selectedFields != null) { termVectorsByField = addGeneratedTermVectors(indexShard, get, termVectorsByField, request, selectedFields); } termVectorsResponse.setDocVersion(docIdAndVersion.version); termVectorsResponse.setExists(true); } /* no term vectors generated or found */ else { termVectorsResponse.setExists(false); } /* if there are term vectors, optional compute dfs and/or terms filtering */ if (termVectorsByField != null) { if (request.filterSettings() != null) { termVectorsFilter = new TermVectorsFilter(termVectorsByField, topLevelFields, request.selectedFields(), dfs); termVectorsFilter.setSettings(request.filterSettings()); try { termVectorsFilter.selectBestTerms(); } catch (IOException e) { throw new ElasticsearchException("failed to select best terms", e); } } // write term vectors termVectorsResponse.setFields(termVectorsByField, request.selectedFields(), request.getFlags(), topLevelFields, dfs, termVectorsFilter); } termVectorsResponse.setTookInMillis(Math.max(1, System.currentTimeMillis() - startTime)); } catch (Throwable ex) { throw new ElasticsearchException("failed to execute term vector request", ex); } finally { searcher.close(); get.release(); } return termVectorsResponse; }	it's better to use a relative time source like system#nanotime to calculate the length of time instead of an absolute time source (which is subject to crazy things like ntp adjustments, dst modifications, or just end-user changes).
@Override protected void createRepository(String repoName) { AcknowledgedResponse putRepositoryResponse = client().admin().cluster().preparePutRepository(repoName) .setType("azure") .setSettings(Settings.builder() .put("container", System.getProperty("test.azure.container")) .put("base_path", System.getProperty("test.azure.base")) ).get(); assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true)); if (Strings.hasText(System.getProperty("test.azure.sas_token"))) { ensureSasTokenHasMinimalPermissions(); } }	i'd call this ensuresastokenpersmissions() and maybe also test other operations like listing all root blobs or delete the container?
static BinaryFieldMapper createQueryBuilderFieldBuilder(BuilderContext context) { BinaryFieldMapper.Builder builder = new BinaryFieldMapper.Builder(QUERY_BUILDER_FIELD_NAME); builder.docValues(true); return builder.build(context); }	note to self - these were the defaults so they are safe to drop.
@Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } IndexMetaData that = (IndexMetaData) o; if (version != that.version) { return false; } if (!aliases.equals(that.aliases)) { return false; } if (!index.equals(that.index)) { return false; } if (!mappings.equals(that.mappings)) { return false; } if (!settings.equals(that.settings)) { return false; } if (state != that.state) { return false; } if (!customs.equals(that.customs)) { return false; } if (Arrays.equals(primaryTerms, that.primaryTerms) == false) { return false; } return true; }	there's a [built-in (int long#hashcode(long))](https://docs.oracle.com/javase/8/docs/api/java/lang/long.html#hashcode-long-) for computing the hash code of a long since java 8.
*/ public final long write(final T state, final Path... locations) throws WriteStateException { if (locations == null) { throw new IllegalArgumentException("Locations must not be null"); } if (locations.length <= 0) { throw new IllegalArgumentException("One or more locations required"); } final long oldGenerationId, newGenerationId; try { oldGenerationId = findMaxGenerationId(prefix, locations); newGenerationId = oldGenerationId + 1; } catch (Exception e) { throw new WriteStateException(false, "exception during looking up new generation id", e); } assert newGenerationId >= 0 : "newGenerationId must be positive but was: [" + oldGenerationId + "]"; final String fileName = getStateFileName(newGenerationId); final String tmpFileName = fileName + ".tmp"; List<Tuple<Path, Directory>> directories = new ArrayList<>(); try { for (Path location : locations) { Path stateLocation = location.resolve(STATE_DIR_NAME); try { directories.add(new Tuple<>(location, newDirectory(stateLocation))); } catch (IOException e) { throw new WriteStateException(false, "failed to open state directory " + stateLocation, e); } } writeStateToFirstLocation(state, directories.get(0).v1(), directories.get(0).v2(), tmpFileName); copyStateToExtraLocations(directories, tmpFileName); performRenames(tmpFileName, fileName, directories); performStateDirectoriesFsync(directories); } catch (WriteStateException e) { cleanupOldFiles(oldGenerationId, locations); throw e; } finally { for (Tuple<Path, Directory> pathAndDirectory : directories) { deleteFileIgnoreExceptions(pathAndDirectory.v1(), pathAndDirectory.v2(), tmpFileName); IOUtils.closeWhileHandlingException(pathAndDirectory.v2()); } } return newGenerationId; }	i think this is dangerous and can lead to data loss. assume that you've successfully written a cluster state that contains an index "test" with state file of generation 1. then you try to write an updated cluster state for index with state file generation 2. writing the state file for the index is successful, but there's a failure later to write the state file for another index. now the node crashes or the clean-up logic fails. when you then handle the next cluster state, you will try to write generation 3, which, if it fails, will clean up everything except generation 2. i think that within metadatastateformat, you can only clean up files within the write method that you know that you have written. in particular, you can't assume (given the manifest-based approach) that the highest generation you see here is the file to keep around (as the manifest might not be pointing to the one with the highest generation). this clean-up logic will have to be handled at a higher level (gatewaymetastate)
@Override public void clusterChanged(ClusterChangedEvent event) { if (event.localNodeMaster() && refreshAndRescheduleRunnable.get() == null) { logger.trace("elected as master, scheduling cluster info update tasks"); executeRefresh(clusterService.state(), "became master"); final RefreshAndRescheduleRunnable newRunnable = new RefreshAndRescheduleRunnable(); refreshAndRescheduleRunnable.set(newRunnable); threadPool.scheduleUnlessShuttingDown(updateFrequency, REFRESH_EXECUTOR, newRunnable); } else if (event.localNodeMaster() == false) { refreshAndRescheduleRunnable.set(null); return; } if (enabled == false) { return; } // Refresh if a data node was added for (DiscoveryNode addedNode : event.nodesDelta().addedNodes()) { if (addedNode.isDataNode()) { executeRefresh(event.state(), "data node added"); break; } } // Clean up info for any removed nodes for (DiscoveryNode removedNode : event.nodesDelta().removedNodes()) { if (removedNode.isDataNode()) { logger.trace("Removing node from cluster info: {}", removedNode.getId()); if (leastAvailableSpaceUsages.containsKey(removedNode.getId())) { ImmutableOpenMap.Builder<String, DiskUsage> newMaxUsages = ImmutableOpenMap.builder(leastAvailableSpaceUsages); newMaxUsages.remove(removedNode.getId()); leastAvailableSpaceUsages = newMaxUsages.build(); } if (mostAvailableSpaceUsages.containsKey(removedNode.getId())) { ImmutableOpenMap.Builder<String, DiskUsage> newMinUsages = ImmutableOpenMap.builder(mostAvailableSpaceUsages); newMinUsages.remove(removedNode.getId()); mostAvailableSpaceUsages = newMinUsages.build(); } } } }	maybe just event.state() here?
public void testMessageOverrideWithNoValue() throws IOException { //message field is meant to be overriden (see custom.test config), but is not provided. //Expected is that it will be emptied final Logger testLogger = LogManager.getLogger("custom.test"); testLogger.info(ESLogMessage.message("some message")); final Path path = PathUtils.get(System.getProperty("es.logs.base_path"), System.getProperty("es.logs.cluster_name") + "_custom.json"); try (Stream<Map<String, String>> stream = JsonLogsStream.mapStreamFrom(path)) { List<Map<String, String>> jsonLogs = stream .collect(Collectors.toList()); assertThat(jsonLogs, contains( allOf( hasEntry("type", "custom"), hasEntry("level", "INFO"), hasEntry("component", "c.test"), hasEntry("cluster.name", "elasticsearch"), hasEntry("node.name", "sample-name")) ) ); } }	@atorok how about this approach for building messages? (pr is inconsistent at the moment, and if that approach is ok i will clean it up) this is influenced by @felixbarny [ecs testcase](https://github.com/elastic/java-ecs-logging/blob/master/log4j2-ecs-layout/src/test/java/co/elastic/logging/log4j2/parameterizedstructuredmessage.java) should work out of the box if we migrate to ecs
@Override public synchronized void close() throws IOException { if (lifecycle.started()) { stop(); } if (!lifecycle.moveToClosed()) { return; } Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings)); logger.info("closing ..."); List<Closeable> toClose = new ArrayList<>(); StopWatch stopWatch = new StopWatch("node_close"); toClose.add(() -> stopWatch.start("tribe")); toClose.add(injector.getInstance(TribeService.class)); toClose.add(() -> stopWatch.stop().start("node_service")); toClose.add(nodeService); toClose.add(() -> stopWatch.stop().start("http")); if (NetworkModule.HTTP_ENABLED.get(settings)) { toClose.add(injector.getInstance(HttpServerTransport.class)); } toClose.add(() -> stopWatch.stop().start("snapshot_service")); toClose.add(injector.getInstance(SnapshotsService.class)); toClose.add(injector.getInstance(SnapshotShardsService.class)); toClose.add(() -> stopWatch.stop().start("client")); Releasables.close(injector.getInstance(Client.class)); toClose.add(() -> stopWatch.stop().start("indices_cluster")); toClose.add(injector.getInstance(IndicesClusterStateService.class)); toClose.add(() -> stopWatch.stop().start("indices")); toClose.add(injector.getInstance(IndicesService.class)); // close filter/fielddata caches after indices toClose.add(injector.getInstance(IndicesStore.class)); toClose.add(() -> stopWatch.stop().start("routing")); toClose.add(injector.getInstance(RoutingService.class)); toClose.add(() -> stopWatch.stop().start("cluster")); toClose.add(injector.getInstance(ClusterService.class)); toClose.add(() -> stopWatch.stop().start("node_connections_service")); toClose.add(injector.getInstance(NodeConnectionsService.class)); toClose.add(() -> stopWatch.stop().start("discovery")); toClose.add(injector.getInstance(Discovery.class)); toClose.add(() -> stopWatch.stop().start("monitor")); toClose.add(nodeService.getMonitorService()); toClose.add(() -> stopWatch.stop().start("gateway")); toClose.add(injector.getInstance(GatewayService.class)); toClose.add(() -> stopWatch.stop().start("search")); toClose.add(injector.getInstance(SearchService.class)); toClose.add(() -> stopWatch.stop().start("transport")); toClose.add(injector.getInstance(TransportService.class)); for (LifecycleComponent plugin : pluginLifecycleComponents) { toClose.add(() -> stopWatch.stop().start("plugin(" + plugin.getClass().getName() + ")")); toClose.add(plugin); } toClose.addAll(pluginsService.filterPlugins(Plugin.class)); toClose.add(() -> stopWatch.stop().start("script")); toClose.add(injector.getInstance(ScriptService.class)); toClose.add(() -> stopWatch.stop().start("thread_pool")); // TODO this should really use ThreadPool.terminate() toClose.add(() -> injector.getInstance(ThreadPool.class).shutdown()); toClose.add(() -> { try { injector.getInstance(ThreadPool.class).awaitTermination(10, TimeUnit.SECONDS); } catch (InterruptedException e) { // ignore } }); toClose.add(() -> stopWatch.stop().start("thread_pool_force_shutdown")); toClose.add(() -> injector.getInstance(ThreadPool.class).shutdownNow()); toClose.add(() -> stopWatch.stop()); toClose.add(injector.getInstance(NodeEnvironment.class)); toClose.add(injector.getInstance(BigArrays.class)); if (logger.isTraceEnabled()) { logger.trace("Close times for each service:\\\\n{}", stopWatch.prettyPrint()); } IOUtils.close(toClose); logger.info("closed"); }	this is redundant. it seems to be already closed by nodeservice. it's a bit weird that nodeservice closes the monitor service but doesn't call own calling the stop / start methods. not a big deal though.
@Override public long ramBytesUsed() { return SHALLOW_SIZE + RamUsageEstimator.sizeOf(index) + RamUsageEstimator.sizeOf(id); }	since this doesn't change during the lifecycle of the object, it can be computed as a constant inside the constructor. as a separate concern, the index string is cached during the same request to reduce object churn which the estimation should take into account otherwise it overestimates. which against a lot of objects becomes an issue.
public long implicitTiebreaker() { return implicitTiebreaker; }	same as above - since the fields are final and this method is going to be called multiple times it's worth considering caching the long (and thus increasing its memory size) to save on virtual calls.
@Override public long ramBytesUsed() { long size = SHALLOW_SIZE; if (until != null) { size += until.ramBytesUsed(); } for (SequenceGroup sg : groups) { if (sg != null) { size += sg.ramBytesUsed(); } } size += RamUsageEstimator.shallowSizeOf(groups); return size; } } private final int listSize; /** for each key, associate the frame per state (determined by index) */ private final Map<SequenceKey, SequenceEntry> keyToSequences; KeyToSequences(int listSize) { this.listSize = listSize; this.keyToSequences = new LinkedHashMap<>(); } SequenceGroup groupIfPresent(int stage, SequenceKey key) { SequenceEntry sequenceEntry = keyToSequences.get(key); return sequenceEntry == null ? null : sequenceEntry.groups[stage]; } UntilGroup untilIfPresent(SequenceKey key) { SequenceEntry sequenceEntry = keyToSequences.get(key); return sequenceEntry == null ? null : sequenceEntry.until; } long add(int stage, Sequence sequence) { long ramBytesUsed = 0; SequenceKey key = sequence.key(); SequenceEntry info = keyToSequences.get(key); if (info == null) { info = new SequenceEntry(listSize); keyToSequences.put(key, info); ramBytesUsed += info.ramBytesUsed(); } ramBytesUsed += info.add(stage, sequence); return ramBytesUsed; } long until(Iterable<KeyAndOrdinal> until) { long ramBytesUsed = 0; for (KeyAndOrdinal keyAndOrdinal : until) { // ignore unknown keys SequenceKey key = keyAndOrdinal.key(); SequenceEntry sequenceEntry = keyToSequences.get(key); if (sequenceEntry != null) { ramBytesUsed += sequenceEntry.until(keyAndOrdinal.ordinal); } } return ramBytesUsed; } long remove(int stage, SequenceKey key) { SequenceEntry info = keyToSequences.get(key); return info.remove(stage); } /** * Remove all matches except the latest occurring _before_ the given ordinal. */ void trimToTail(Ordinal ordinal) { for (Iterator<SequenceEntry> it = keyToSequences.values().iterator(); it.hasNext(); ) { SequenceEntry seqs = it.next(); // remember the last item found (will be ascending) // to trim unneeded until that occur before it Sequence firstTail = null; // remove any empty keys boolean keyIsEmpty = true; for (SequenceGroup group : seqs.groups) { if (group != null) { Sequence sequence = group.trimBeforeLast(ordinal); if (firstTail == null) { firstTail = sequence; } keyIsEmpty &= group.isEmpty(); } } // there are no sequences on any stage for this key, drop it if (keyIsEmpty) { it.remove(); } if (firstTail != null) { // drop any possible UNTIL that occurs before the last tail UntilGroup until = seqs.until; if (until != null) { until.trimBefore(firstTail.ordinal()); } } } } public void clear() { keyToSequences.clear(); } @Override public long ramBytesUsed() { long size = SHALLOW_SIZE; size += RamUsageEstimator.primitiveSizes.get(int.class); size += RamUsageEstimator.sizeOfMap(keyToSequences); return size; } @Override public String toString() { return LoggerMessageFormat.format(null, "Keys=[{}	accounting/ram usage should not leak into the return values of add/remove/until.
protected void masterOperation( Task task, PutTrainedModelAliasAction.Request request, ClusterState state, ActionListener<AcknowledgedResponse> listener ) throws Exception { final boolean mlSupported = MachineLearningField.ML_API_FEATURE.check(licenseState); final Predicate<TrainedModelConfig> isLicensed = (model) -> mlSupported || licenseState.isAllowedByLicense(model.getLicenseLevel()); final String oldModelId = ModelAliasMetadata.fromState(state).getModelId(request.getModelAlias()); if (oldModelId != null && (request.isReassign() == false)) { listener.onFailure(ExceptionsHelper.badRequestException( "cannot assign model_alias [{}] to model_id [{}] as model_alias already refers to [{}]. " + "Set parameter [reassign] to [true] if model_alias should be reassigned.", request.getModelAlias(), request.getModelId(), oldModelId)); return; } Set<String> modelIds = new HashSet<>(); modelIds.add(request.getModelAlias()); modelIds.add(request.getModelId()); if (oldModelId != null) { modelIds.add(oldModelId); } trainedModelProvider.getTrainedModels(modelIds, GetTrainedModelsAction.Includes.empty(), true, ActionListener.wrap( models -> { TrainedModelConfig newModel = null; TrainedModelConfig oldModel = null; for (TrainedModelConfig config : models) { if (config.getModelId().equals(request.getModelId())) { newModel = config; } if (config.getModelId().equals(oldModelId)) { oldModel = config; } if (config.getModelId().equals(request.getModelAlias())) { listener.onFailure( ExceptionsHelper.badRequestException("model_alias cannot be the same as an existing trained model_id") ); return; } } if (newModel == null) { listener.onFailure( ExceptionsHelper.missingTrainedModel(request.getModelId()) ); return; } if (isLicensed.test(newModel) == false) { listener.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)); return; } if (newModel.getModelType() == TrainedModelType.PYTORCH) { listener.onFailure(ExceptionsHelper.badRequestException("model_alias is not supported on pytorch models")); return; } // if old model is null, none of these validations matter // we should still allow reassignment even if the old model was some how deleted and the alias still refers to it if (oldModel != null) { // validate inference configs are the same type. Moving an alias from regression -> classification seems dangerous if (newModel.getInferenceConfig() != null && oldModel.getInferenceConfig() != null) { if (newModel.getInferenceConfig().getName().equals(oldModel.getInferenceConfig().getName()) == false) { listener.onFailure( ExceptionsHelper.badRequestException( "cannot reassign model_alias [{}] to model [{}] " + "with inference config type [{}] from model [{}] with type [{}]", request.getModelAlias(), newModel.getModelId(), newModel.getInferenceConfig().getName(), oldModel.getModelId(), oldModel.getInferenceConfig().getName() ) ); return; } } Set<String> oldInputFields = new HashSet<>(oldModel.getInput().getFieldNames()); Set<String> newInputFields = new HashSet<>(newModel.getInput().getFieldNames()); // TODO should we fail in this case??? if (Sets.difference(oldInputFields, newInputFields).size() > (oldInputFields.size() / 2) || Sets.intersection(newInputFields, oldInputFields).size() < (oldInputFields.size() / 2)) { String warning = Messages.getMessage( TRAINED_MODEL_INPUTS_DIFFER_SIGNIFICANTLY, request.getModelId(), oldModelId); auditor.warning(oldModelId, warning); logger.warn("[{}] {}", oldModelId, warning); HeaderWarning.addWarning(warning); } } clusterService.submitStateUpdateTask("update-model-alias", new AckedClusterStateUpdateTask(request, listener) { @Override public ClusterState execute(final ClusterState currentState) { return updateModelAlias(currentState, request); } }); }, listener::onFailure )); }	i am aware it's the existing code. but should we use licensestate.copycurrentlicensestate() for the two license checks?
public void testGetStepMultithreaded() throws Exception { Client client = mock(Client.class); Mockito.when(client.settings()).thenReturn(Settings.EMPTY); LifecyclePolicy policy = LifecyclePolicyTests.randomTimeseriesLifecyclePolicyWithAllPhases("policy"); String phaseName = randomFrom(policy.getPhases().keySet()); Phase phase = policy.getPhases().get(phaseName); LifecycleExecutionState lifecycleState = LifecycleExecutionState.builder() .setPhaseDefinition(Strings.toString(new PhaseExecutionInfo(policy.getName(), phase, 1, randomNonNegativeLong()))) .build(); IndexMetadata indexMetadata = IndexMetadata.builder("test") .settings( Settings.builder() .put("index.number_of_shards", 1) .put("index.number_of_replicas", 0) .put("index.version.created", Version.CURRENT) .put(LifecycleSettings.LIFECYCLE_NAME, "policy") .build() ) .putCustom(ILM_CUSTOM_METADATA_KEY, lifecycleState.asMap()) .build(); SortedMap<String, LifecyclePolicyMetadata> metas = new TreeMap<>(); metas.put("policy", new LifecyclePolicyMetadata(policy, Collections.emptyMap(), 1, randomNonNegativeLong())); IndexLifecycleMetadata meta = new IndexLifecycleMetadata(metas, OperationMode.RUNNING); PolicyStepsRegistry registry = new PolicyStepsRegistry(REGISTRY, client, null); registry.update(meta); // test a variety of getStep calls with random actions and steps for (int i = 0; i < scaledRandomIntBetween(100, 1000); i++) { LifecycleAction action = randomValueOtherThan(MigrateAction.DISABLED, () -> randomFrom(phase.getActions().values())); Step step = randomFrom(action.toSteps(client, phaseName, MOCK_STEP_KEY, null)); Step actualStep = registry.getStep(indexMetadata, step.getKey()); assertThat(actualStep.getKey(), equalTo(step.getKey())); } final CountDownLatch latch = new CountDownLatch(1); final AtomicBoolean done = new AtomicBoolean(false); // now, in another thread, update the registry repeatedly as fast as possible. // updating the registry has the side effect of clearing the cache. Thread t = new Thread(() -> { latch.countDown(); // signal that we're starting while (done.get() == false) { registry.update(meta); } }); t.start(); try { latch.await(); // wait until the other thread started // and, while the cache is being repeatedly cleared, // test a variety of getStep calls with random actions and steps for (int i = 0; i < scaledRandomIntBetween(100, 1000); i++) { LifecycleAction action = randomValueOtherThan(MigrateAction.DISABLED, () -> randomFrom(phase.getActions().values())); Step step = randomFrom(action.toSteps(client, phaseName, MOCK_STEP_KEY, null)); Step actualStep = registry.getStep(indexMetadata, step.getKey()); assertThat(actualStep.getKey(), equalTo(step.getKey())); } } finally { // tell the other thread we're finished and wait for it to die done.set(true); t.join(1000); } }	i'm a bit confused as to what this tests. there'll be a new step added, by-passing the cache, on every iteration - i'm quite confused specifically as we don't have any steps defined/registered before, yet we do setup some metadatas (both indexmetadata and indexlifecyclemetadata). apologies if i'm missing something very obvious but could we document the intent here? (i'm _guessing_ we want to populate the cache? )
*/ public static void warnIfNoTransformNodes(ClusterState clusterState) { long transformNodes = getNumberOfTransformNodes(clusterState); if (transformNodes == 0) { HeaderWarning.addWarning(TransformMessages.REST_WARN_NO_TRANSFORM_NODES); } } /** * Select any node among provided nodes that satisfies all of the following: * - is a transform node * - is a remote cluster client node * - runs at least version 7.13 * * @param nodes nodes to select from * @return selected node or {@code Optional.empty()}	this isn't strictly true given the implementation of this method. it's only true because this method only gets called after determining that a remote cluster node is required. nodecanrunthistransform() only enforces remote cluster client nodes if the transform requires one.
*/ public static void warnIfNoTransformNodes(ClusterState clusterState) { long transformNodes = getNumberOfTransformNodes(clusterState); if (transformNodes == 0) { HeaderWarning.addWarning(TransformMessages.REST_WARN_NO_TRANSFORM_NODES); } } /** * Select any node among provided nodes that satisfies all of the following: * - is a transform node * - is a remote cluster client node * - runs at least version 7.13 * * @param nodes nodes to select from * @return selected node or {@code Optional.empty()}	nit: it sounds like the node is a remote node, but it is a remote _client_ node. maybe just name it selectnodethatcanrunthistransform? the code doc explains it nicely anyway, nit: add the reasons, e.g. 7.13 because we added _validate there, right?
@Override protected void doExecute(Task task, Request request, ActionListener<Response> finalListener) { final ClusterState clusterState = clusterService.state(); TransformNodes.warnIfNoTransformNodes(clusterState); final DiscoveryNodes nodes = clusterState.nodes(); transformConfigManager.expandTransformIds( request.getId(), request.getPageParams(), request.isAllowNoMatch(), ActionListener.wrap(hitsAndIds -> { if (hitsAndIds.v2().v2().stream().anyMatch(config -> config.getSource().requiresRemoteCluster()) && (isRemoteClusterClientNode == false)) { // remote_cluster_client role is required but the current node is not remote_cluster_client, find another node. Optional<DiscoveryNode> remoteClusterClientNode = TransformNodes.selectAnyTransformRemoteNode(nodes); if (remoteClusterClientNode.isPresent()) { // Redirect the request to a remote_cluster_client node transportService.sendRequest( remoteClusterClientNode.get(), actionName, request, new ActionListenerResponseHandler<>(finalListener, Response::new)); } else { // There are no remote_cluster_client nodes in the cluster, fail finalListener.onFailure(ExceptionsHelper.badRequestException("No remote_cluster_client node to run on")); } return; } request.setExpandedIds(hitsAndIds.v2().v1()); final TransformNodeAssignments transformNodeAssignments = TransformNodes.transformTaskNodes(hitsAndIds.v2().v1(), clusterState); ActionListener<Response> doExecuteListener = ActionListener.wrap(response -> { PersistentTasksCustomMetadata tasksInProgress = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE); if (tasksInProgress != null) { // Mutates underlying state object with the assigned node attributes response.getTransformsStats().forEach(dtsasi -> setNodeAttributes(dtsasi, tasksInProgress, clusterState)); } collectStatsForTransformsWithoutTasks( request, response, transformNodeAssignments.getWaitingForAssignment(), clusterState, ActionListener.wrap( finalResponse -> finalListener.onResponse( new Response( finalResponse.getTransformsStats(), hitsAndIds.v1(), finalResponse.getTaskFailures(), finalResponse.getNodeFailures() ) ), finalListener::onFailure ) ); }, finalListener::onFailure); if (transformNodeAssignments.getExecutorNodes().size() > 0) { request.setNodes(transformNodeAssignments.getExecutorNodes().toArray(new String[0])); super.doExecute(task, request, doExecuteListener); } else { doExecuteListener.onResponse(new Response(Collections.emptyList(), 0L)); } }, e -> { // If the index to search, or the individual config is not there, just return empty if (e instanceof ResourceNotFoundException) { finalListener.onResponse(new Response(Collections.emptyList(), 0L)); } else { finalListener.onFailure(e); } } ) ); }	i think this is not necessary at this level: if a transform is running, the call is redirected to the node that runs the transform to get _live data_ from that task. if the transform requires remote client, the transform running node must be transform and remote client by definition. but we hit a problem for collectstatsfortransformswithouttasks: this is executed on _this_ node. transformnodeassignments tells you which transforms are waitingforassignment or stopped. i would use your added selectanytransformremotenode method to send a request only with non running transforms to an appropriate transform node with or without remote client.
@Override protected void doExecute(Task task, Request request, ActionListener<Response> finalListener) { final ClusterState clusterState = clusterService.state(); TransformNodes.warnIfNoTransformNodes(clusterState); final DiscoveryNodes nodes = clusterState.nodes(); transformConfigManager.expandTransformIds( request.getId(), request.getPageParams(), request.isAllowNoMatch(), ActionListener.wrap(hitsAndIds -> { if (hitsAndIds.v2().v2().stream().anyMatch(config -> config.getSource().requiresRemoteCluster()) && (isRemoteClusterClientNode == false)) { // remote_cluster_client role is required but the current node is not remote_cluster_client, find another node. Optional<DiscoveryNode> remoteClusterClientNode = TransformNodes.selectAnyTransformRemoteNode(nodes); if (remoteClusterClientNode.isPresent()) { // Redirect the request to a remote_cluster_client node transportService.sendRequest( remoteClusterClientNode.get(), actionName, request, new ActionListenerResponseHandler<>(finalListener, Response::new)); } else { // There are no remote_cluster_client nodes in the cluster, fail finalListener.onFailure(ExceptionsHelper.badRequestException("No remote_cluster_client node to run on")); } return; } request.setExpandedIds(hitsAndIds.v2().v1()); final TransformNodeAssignments transformNodeAssignments = TransformNodes.transformTaskNodes(hitsAndIds.v2().v1(), clusterState); ActionListener<Response> doExecuteListener = ActionListener.wrap(response -> { PersistentTasksCustomMetadata tasksInProgress = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE); if (tasksInProgress != null) { // Mutates underlying state object with the assigned node attributes response.getTransformsStats().forEach(dtsasi -> setNodeAttributes(dtsasi, tasksInProgress, clusterState)); } collectStatsForTransformsWithoutTasks( request, response, transformNodeAssignments.getWaitingForAssignment(), clusterState, ActionListener.wrap( finalResponse -> finalListener.onResponse( new Response( finalResponse.getTransformsStats(), hitsAndIds.v1(), finalResponse.getTaskFailures(), finalResponse.getNodeFailures() ) ), finalListener::onFailure ) ); }, finalListener::onFailure); if (transformNodeAssignments.getExecutorNodes().size() > 0) { request.setNodes(transformNodeAssignments.getExecutorNodes().toArray(new String[0])); super.doExecute(task, request, doExecuteListener); } else { doExecuteListener.onResponse(new Response(Collections.emptyList(), 0L)); } }, e -> { // If the index to search, or the individual config is not there, just return empty if (e instanceof ResourceNotFoundException) { finalListener.onResponse(new Response(Collections.emptyList(), 0L)); } else { finalListener.onFailure(e); } } ) ); }	is it possible that the real reason could be that there are no transform nodes?
protected void doExecute(Task task, Request request, ActionListener<Response> listener) { final ClusterState clusterState = clusterService.state(); TransformNodes.warnIfNoTransformNodes(clusterState); final DiscoveryNodes nodes = clusterState.nodes(); // Redirection can only be performed between nodes that are at least 7.13. if (nodes.getMinNodeVersion().onOrAfter(Version.V_7_13_0) && request.getConfig().getSource().requiresRemoteCluster() && (isRemoteClusterClientNode == false)) { // remote_cluster_client role is required but the current node is not remote_cluster_client, find another node. Optional<DiscoveryNode> remoteClusterClientNode = TransformNodes.selectAnyTransformRemoteNode(nodes); if (remoteClusterClientNode.isPresent()) { // Redirect the request to a remote_cluster_client node transportService.sendRequest( remoteClusterClientNode.get(), actionName, request, new ActionListenerResponseHandler<>(listener, Response::new)); } else { // There are no remote_cluster_client nodes in the cluster, fail listener.onFailure(ExceptionsHelper.badRequestException("No remote_cluster_client node to run on")); } return; } final TransformConfig config = request.getConfig(); sourceDestValidator.validate( clusterState, config.getSource().getIndex(), config.getDestination().getIndex(), config.getDestination().getPipeline(), SourceDestValidations.getValidationsForPreview(config.getAdditionalValidations()), ActionListener.wrap(r -> { // create the function for validation final Function function = FunctionFactory.create(config); function.validateConfig(ActionListener.wrap(functionValidationResponse -> { getPreview( config.getId(), // note: @link{PreviewTransformAction} sets an id, so this is never null function, config.getSource(), config.getDestination().getPipeline(), config.getDestination().getIndex(), config.getSyncConfig(), listener ); }, listener::onFailure)); }, listener::onFailure) ); }	i think it makes sense to make this a hard check, if the cluster lacks a transform node, preview should not be possible. this sounds like a breaking change, but seems acceptable to me, as it makes no sense to call preview without a transform node. if there are concerns, we could make this check only on master.
protected void doExecute(Task task, Request request, ActionListener<Response> listener) { final ClusterState clusterState = clusterService.state(); TransformNodes.warnIfNoTransformNodes(clusterState); final DiscoveryNodes nodes = clusterState.nodes(); // Redirection can only be performed between nodes that are at least 7.13. if (nodes.getMinNodeVersion().onOrAfter(Version.V_7_13_0) && request.getConfig().getSource().requiresRemoteCluster() && (isRemoteClusterClientNode == false)) { // remote_cluster_client role is required but the current node is not remote_cluster_client, find another node. Optional<DiscoveryNode> remoteClusterClientNode = TransformNodes.selectAnyTransformRemoteNode(nodes); if (remoteClusterClientNode.isPresent()) { // Redirect the request to a remote_cluster_client node transportService.sendRequest( remoteClusterClientNode.get(), actionName, request, new ActionListenerResponseHandler<>(listener, Response::new)); } else { // There are no remote_cluster_client nodes in the cluster, fail listener.onFailure(ExceptionsHelper.badRequestException("No remote_cluster_client node to run on")); } return; } final TransformConfig config = request.getConfig(); sourceDestValidator.validate( clusterState, config.getSource().getIndex(), config.getDestination().getIndex(), config.getDestination().getPipeline(), SourceDestValidations.getValidationsForPreview(config.getAdditionalValidations()), ActionListener.wrap(r -> { // create the function for validation final Function function = FunctionFactory.create(config); function.validateConfig(ActionListener.wrap(functionValidationResponse -> { getPreview( config.getId(), // note: @link{PreviewTransformAction} sets an id, so this is never null function, config.getSource(), config.getDestination().getPipeline(), config.getDestination().getIndex(), config.getSyncConfig(), listener ); }, listener::onFailure)); }, listener::onFailure) ); }	i think it would be simpler and better for the future to always redirect preview to a transform node, that means checking if _this_ node is sufficient(transform+remote_cluster_if_needed) and redirect using selectanytransformremotenode if not.
* @param shardTarget the shard target for this failure * @param e the failure reason */ @Override public final void onShardFailure(final int shardIndex, SearchShardTarget shardTarget, Exception e) { // we don't aggregate shard on failures due to the internal cancellation, // but do keep the header counts right if ((requestCancelled.get() && isTaskCancelledException(e)) == false) { AtomicArray<ShardSearchFailure> shardFailures = this.shardFailures.get(); // lazily create shard failures, so we can early build the empty shard failure list in most cases (no failures) if (shardFailures == null) { // this is double checked locking but it's fine since SetOnce uses a volatile read internally synchronized (shardFailuresMutex) { shardFailures = this.shardFailures.get(); // read again otherwise somebody else has created it? if (shardFailures == null) { // still null so we are the first and create a new instance shardFailures = new AtomicArray<>(getNumShards()); this.shardFailures.set(shardFailures); } } } ShardSearchFailure failure = shardFailures.get(shardIndex); if (failure == null) { shardFailures.set(shardIndex, new ShardSearchFailure(e, shardTarget)); } else { // the failure is already present, try and not override it with an exception that is less meaningless // for example, getting illegal shard state if (TransportActions.isReadOverrideException(e)) { shardFailures.set(shardIndex, new ShardSearchFailure(e, shardTarget)); } } if (results.hasResult(shardIndex)) { assert failure == null : "shard failed before but shouldn't: " + failure; successfulOps.decrementAndGet(); // if this shard was successful before (initial phase) we have to adjust the counter } } results.consumeShardFailure(shardIndex); }	the many different shard not available exceptions have a variety of rest status codes. i wonder if it is best to wrap them or just create a noshardavailableactionexception with the message from the inner exception - to get a 503 out every time. otherwise we risk seeing among others a 400 not found, which is a bit weird. also, whether the shard was unavailable at the beginning of search or we detected it during the first phase of search is largely irrelevant to clients i think, using the same exception in all those cases could allow clients to filter out those specific errors if they want to.
public static void setupThreadPool() { int bulkThreadPoolSize = randomIntBetween(1, 2); int bulkThreadPoolQueueSize = randomIntBetween(1, 2); threadPool = new TestThreadPool("IndexShardOperationsLockTests", Settings.builder() .put("thread_pool." + ThreadPool.Names.BULK + ".size", bulkThreadPoolSize) .put("thread_pool." + ThreadPool.Names.BULK + ".queue_size", bulkThreadPoolQueueSize) .build()); assertThat(threadPool.executor(ThreadPool.Names.BULK), instanceOf(EsThreadPoolExecutor.class)); assertThat(((EsThreadPoolExecutor) threadPool.executor(ThreadPool.Names.BULK)).getCorePoolSize(), equalTo(bulkThreadPoolSize)); assertThat(((EsThreadPoolExecutor) threadPool.executor(ThreadPool.Names.BULK)).getMaximumPoolSize(), equalTo(bulkThreadPoolSize)); assertThat(((EsThreadPoolExecutor) threadPool.executor(ThreadPool.Names.BULK)).getQueue().remainingCapacity(), equalTo(bulkThreadPoolQueueSize)); }	++ to randomly use limited capacity.
public String resolveWriteIndexRouting(@Nullable String routing, String aliasOrIndex) { if (aliasOrIndex == null) { return routing; } AliasOrIndex result = getAliasAndIndexLookup().get(aliasOrIndex); if (result == null || result.isAlias() == false) { return routing; } AliasOrIndex.Alias alias = (AliasOrIndex.Alias) result; IndexMetaData writeIndex = alias.getWriteIndex(); if (writeIndex == null) { throw new IllegalArgumentException("alias [" + aliasOrIndex + "] does not have a write index"); } AliasMetaData aliasMd = writeIndex.getAliases().get(alias.getAliasName()); if (aliasMd.indexRouting() != null) { if (aliasMd.indexRouting().indexOf(',') != -1) { throw new IllegalArgumentException("index/alias [" + aliasOrIndex + "] provided with routing value [" + aliasMd.getIndexRouting() + "] that resolved to several routing values, rejecting operation"); } if (routing != null) { if (!routing.equals(aliasMd.indexRouting())) { throw new IllegalArgumentException("Alias [" + aliasOrIndex + "] has index routing associated with it [" + aliasMd.indexRouting() + "], and was provided with routing value [" + routing + "], rejecting operation"); } } // Alias routing overrides the parent routing (if any). return aliasMd.indexRouting(); } return routing; }	i wonder if we should validate that when people set is_write_index to true as well
public void testResolveIndexRouting() { IndexMetaData.Builder builder = IndexMetaData.builder("index") .settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)) .numberOfShards(1) .numberOfReplicas(0) .putAlias(AliasMetaData.builder("alias0").build()) .putAlias(AliasMetaData.builder("alias1").routing("1").build()) .putAlias(AliasMetaData.builder("alias2").routing("1,2").build()); MetaData metaData = MetaData.builder().put(builder).build(); // no alias, no index assertEquals(metaData.resolveIndexRouting(null, null), null); assertEquals(metaData.resolveIndexRouting("0", null), "0"); // index, no alias assertEquals(metaData.resolveIndexRouting(null, "index"), null); assertEquals(metaData.resolveIndexRouting("0", "index"), "0"); // alias with no index routing assertEquals(metaData.resolveIndexRouting(null, "alias0"), null); assertEquals(metaData.resolveIndexRouting("0", "alias0"), "0"); // alias with index routing. assertEquals(metaData.resolveIndexRouting(null, "alias1"), "1"); try { metaData.resolveIndexRouting("0", "alias1"); fail("should fail"); } catch (IllegalArgumentException ex) { assertThat(ex.getMessage(), is("Alias [alias1] has index routing associated with it [1], and was provided with routing value [0], rejecting operation")); } // alias with invalid index routing. try { metaData.resolveIndexRouting(null, "alias2"); fail("should fail"); } catch (IllegalArgumentException ex) { assertThat(ex.getMessage(), is("index/alias [alias2] provided with routing value [1,2] that resolved to several routing values, rejecting operation")); } try { metaData.resolveIndexRouting("1", "alias2"); fail("should fail"); } catch (IllegalArgumentException ex) { assertThat(ex.getMessage(), is("index/alias [alias2] provided with routing value [1,2] that resolved to several routing values, rejecting operation")); } IndexMetaData.Builder builder2 = IndexMetaData.builder("index2") .settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)) .numberOfShards(1) .numberOfReplicas(0) .putAlias(AliasMetaData.builder("alias0").build()); MetaData metaDataTwoIndices = MetaData.builder(metaData).put(builder2).build(); // alias with multiple indices IllegalArgumentException exception = expectThrows(IllegalArgumentException.class, () -> metaDataTwoIndices.resolveIndexRouting("1", "alias0")); assertThat(exception.getMessage(), startsWith("Alias [alias0] has more than one index associated with it")); }	we don't have an explicit is_write_index set to true in this test. set it here randomly?
public void testResolveIndexRouting() { IndexMetaData.Builder builder = IndexMetaData.builder("index") .settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)) .numberOfShards(1) .numberOfReplicas(0) .putAlias(AliasMetaData.builder("alias0").build()) .putAlias(AliasMetaData.builder("alias1").routing("1").build()) .putAlias(AliasMetaData.builder("alias2").routing("1,2").build()); MetaData metaData = MetaData.builder().put(builder).build(); // no alias, no index assertEquals(metaData.resolveIndexRouting(null, null), null); assertEquals(metaData.resolveIndexRouting("0", null), "0"); // index, no alias assertEquals(metaData.resolveIndexRouting(null, "index"), null); assertEquals(metaData.resolveIndexRouting("0", "index"), "0"); // alias with no index routing assertEquals(metaData.resolveIndexRouting(null, "alias0"), null); assertEquals(metaData.resolveIndexRouting("0", "alias0"), "0"); // alias with index routing. assertEquals(metaData.resolveIndexRouting(null, "alias1"), "1"); try { metaData.resolveIndexRouting("0", "alias1"); fail("should fail"); } catch (IllegalArgumentException ex) { assertThat(ex.getMessage(), is("Alias [alias1] has index routing associated with it [1], and was provided with routing value [0], rejecting operation")); } // alias with invalid index routing. try { metaData.resolveIndexRouting(null, "alias2"); fail("should fail"); } catch (IllegalArgumentException ex) { assertThat(ex.getMessage(), is("index/alias [alias2] provided with routing value [1,2] that resolved to several routing values, rejecting operation")); } try { metaData.resolveIndexRouting("1", "alias2"); fail("should fail"); } catch (IllegalArgumentException ex) { assertThat(ex.getMessage(), is("index/alias [alias2] provided with routing value [1,2] that resolved to several routing values, rejecting operation")); } IndexMetaData.Builder builder2 = IndexMetaData.builder("index2") .settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)) .numberOfShards(1) .numberOfReplicas(0) .putAlias(AliasMetaData.builder("alias0").build()); MetaData metaDataTwoIndices = MetaData.builder(metaData).put(builder2).build(); // alias with multiple indices IllegalArgumentException exception = expectThrows(IllegalArgumentException.class, () -> metaDataTwoIndices.resolveIndexRouting("1", "alias0")); assertThat(exception.getMessage(), startsWith("Alias [alias0] has more than one index associated with it")); }	i don't believe we check rejection of multi value routings with is_write_index set to true. can we add a test that it fails (preferably but rejecting it when building the metadata objects?)
private void handleJoinRequest(JoinRequest joinRequest, JoinHelper.JoinCallback joinCallback) { assert Thread.holdsLock(mutex) == false; assert getLocalNode().isMasterNode() : getLocalNode() + " received a join but is not master-eligible"; logger.trace("handleJoinRequest: as {}, handling {}", mode, joinRequest); transportService.connectToNode(joinRequest.getSourceNode()); final ClusterState stateForJoinValidation = getStateForMasterService(); if (stateForJoinValidation.nodes().isLocalNodeElectedMaster()) { onJoinValidators.forEach(a -> a.accept(joinRequest.getSourceNode(), stateForJoinValidation)); if (stateForJoinValidation.getBlocks().hasGlobalBlock(STATE_NOT_RECOVERED_BLOCK) == false) { // we do this in a couple of places including the cluster update thread. This one here is really just best effort // to ensure we fail as fast as possible. JoinTaskExecutor.ensureMajorVersionBarrier(joinRequest.getSourceNode().getVersion(), stateForJoinValidation.getNodes().getMinNodeVersion()); } sendValidateJoinRequest(stateForJoinValidation, joinRequest, joinCallback); } else { processJoinRequest(joinRequest, joinCallback); } }	can you move this test to the same package as coordinator, and make this method package-visible instead?
static void loadConfigWithSubstitutions(Settings.Builder output, Path configFile, Function<String, String> substitutions) throws IOException { long existingSize = Files.size(configFile); StringBuilder builder = new StringBuilder((int) existingSize); try (BufferedReader reader = Files.newBufferedReader(configFile, StandardCharsets.UTF_8)) { String line; while ((line = reader.readLine()) != null) { int dollarNdx; int nextNdx = 0; while ((dollarNdx = line.indexOf("${", nextNdx)) != -1) { int closeNdx = line.indexOf('}', dollarNdx + 2); if (closeNdx == -1) { // No close substitution was found. Break to leniently copy the rest of the line as is. break; } // copy up to the dollar if (dollarNdx > nextNdx) { builder.append(line, nextNdx, dollarNdx); } nextNdx = closeNdx + 1; String substKey = line.substring(dollarNdx + 2, closeNdx); String substValue = substitutions.apply(substKey); if (substValue != null) { builder.append(substValue); } else { // the substitution name doesn't exist, defer to setting based substitution after yaml parsing builder.append(line, dollarNdx, nextNdx); } } if (nextNdx < line.length()) { builder.append(line, nextNdx, line.length()); } builder.append(System.lineSeparator()); } } var is = new ByteArrayInputStream(builder.toString().getBytes(StandardCharsets.UTF_8)); output.loadFromStream(configFile.getFileName().toString(), is, false); }	i'm confused why this is necessary. doesn't the later call to initializesettings take care of replacing any remaining values? how many "passes" are necessary?
@Override public MetaData.Custom fromXContent(XContentParser parser) throws IOException { Map<String, PipelineConfiguration> pipelines = new HashMap<>(); List<PipelineConfiguration> configs = INGEST_METADATA_PARSER.parse(parser); for (PipelineConfiguration pipeline : configs) { pipelines.put(pipeline.getId(), pipeline); } return new IngestMetadata(pipelines); }	+1 this is much nicer. i like the fact we can tell objectparser to just work with arraylist. i kind of thought that a custom pojo was always needed...
private boolean updateCustoms(ClusterState currentState, List<ClusterChangedEvent> tasks, MetaData.Builder metaData) { boolean clusterStateChanged = false; Set<String> changedCustomMetaDataTypeSet = tasks.stream() .map(ClusterChangedEvent::changedCustomMetaDataSet) .flatMap(Collection::stream) .collect(Collectors.toSet()); final List<Node> tribeClientNodes = TribeService.this.nodes; Map<String, MetaData.Custom> mergedCustomMetaDataMap = mergeChangedCustomMetaData(changedCustomMetaDataTypeSet, customMetaDataType -> tribeClientNodes.stream() .map(TribeService::getClusterService).map(ClusterService::state) .map(ClusterState::metaData) .map(clusterMetaData -> ((MetaData.Custom) clusterMetaData.custom(customMetaDataType))) .filter(custom1 -> custom1 != null && custom1 instanceof MergableCustomMetaData) .map(custom2 -> (MergableCustomMetaData) custom2) .collect(Collectors.toList()) ); for (String changedCustomMetaDataType : changedCustomMetaDataTypeSet) { MetaData.Custom mergedCustomMetaData = mergedCustomMetaDataMap.get(changedCustomMetaDataType); if (mergedCustomMetaData == null) { // we ignore merging custom md which doesn't implement MergableCustomMetaData interface if (currentState.metaData().custom(changedCustomMetaDataType) instanceof MergableCustomMetaData) { // custom md has been removed clusterStateChanged = true; logger.info("[{}] removing custom meta data type [{}]", tribeName, changedCustomMetaDataType); metaData.removeCustom(changedCustomMetaDataType); } } else { // custom md has been changed clusterStateChanged = true; logger.info("[{}] updating custom meta data type [{}] data [{}]", tribeName, changedCustomMetaDataType, mergedCustomMetaData); metaData.putCustom(changedCustomMetaDataType, mergedCustomMetaData); } } return clusterStateChanged; }	i'm wondering if we really need this check - when is it relevant?
public void testInvalidMissing() { final Settings settings = Settings.builder() .put("index.sort.field", "field1") .put("index.sort.missing", "default") .build(); IllegalArgumentException exc = expectThrows(IllegalArgumentException.class, () -> indexSettings(settings)); assertThat(exc.getMessage(), containsString("Illegal missing value:[default]," + " must be one of [_last, _first]")); }	this was moved from runtimefieldmappertests
@Override protected void masterOperation( Task task, final MountSearchableSnapshotRequest request, final ClusterState state, final ActionListener<RestoreSnapshotResponse> listener ) { SearchableSnapshots.ensureValidLicense(licenseState); final String mountedIndexName = request.mountedIndexName(); if (systemIndices.isSystemIndex(mountedIndexName)) { throw new ElasticsearchException("system index [{}] cannot be mounted as searchable snapshots", mountedIndexName); } final String repoName = request.repositoryName(); final String snapName = request.snapshotName(); final String indexName = request.snapshotIndexName(); // Retrieve IndexId and SnapshotId instances, which are then used to create a new restore // request, which is then sent on to the actual snapshot restore mechanism final Repository repository = repositoriesService.repository(repoName); final StepListener<RepositoryData> repositoryDataListener = new StepListener<>(); repository.getRepositoryData(repositoryDataListener); repositoryDataListener.whenComplete(repoData -> { final Map<String, IndexId> indexIds = repoData.getIndices(); if (indexIds.containsKey(indexName) == false) { throw new IndexNotFoundException("index [" + indexName + "] not found in repository [" + repoName + "]"); } final IndexId indexId = indexIds.get(indexName); final Optional<SnapshotId> matchingSnapshotId = repoData.getSnapshotIds() .stream() .filter(s -> snapName.equals(s.getName())) .findFirst(); if (matchingSnapshotId.isEmpty()) { throw new ElasticsearchException("snapshot [" + snapName + "] not found in repository [" + repoName + "]"); } final SnapshotId snapshotId = matchingSnapshotId.get(); final String[] ignoreIndexSettings = Arrays.copyOf(request.ignoreIndexSettings(), request.ignoreIndexSettings().length + 1); ignoreIndexSettings[ignoreIndexSettings.length - 1] = IndexMetadata.SETTING_DATA_PATH; client.admin() .cluster() .restoreSnapshot( new RestoreSnapshotRequest(repoName, snapName) // Restore the single index specified .indices(indexName) // Always rename it to the desired mounted index name .renamePattern(".+") .renameReplacement(mountedIndexName) // Pass through index settings, adding the index-level settings required to use searchable snapshots .indexSettings( Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) // can be overridden .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, false) // can be overridden .put(request.indexSettings()) .put(buildIndexSettings(request.repositoryName(), snapshotId, indexId)) .build() ) // Pass through ignored index settings .ignoreIndexSettings(ignoreIndexSettings) // Don't include global state .includeGlobalState(false) // Don't include aliases .includeAliases(false) // Pass through the wait-for-completion flag .waitForCompletion(request.waitForCompletion()) // Pass through the master-node timeout .masterNodeTimeout(request.masterNodeTimeout()) // Fail the restore if the snapshot found above is swapped out from under us before the restore happens .snapshotUuid(snapshotId.getUUID()), listener ); }, listener::onFailure); }	this looks better than the previous approach
@Override public GetResult get(Get get, BiFunction<String, SearcherScope, Engine.Searcher> searcherFactory) throws EngineException { assert Objects.equals(get.uid().field(), IdFieldMapper.NAME) : get.uid().field(); try (ReleasableLock ignored = readLock.acquire()) { ensureOpen(); SearcherScope scope; if (get.realtime()) { VersionValue versionValue = null; try (Releasable ignore = versionMap.acquireLock(get.uid().bytes())) { // we need to lock here to access the version map to do this truly in RT versionValue = getVersionFromMap(get.uid().bytes()); } if (versionValue != null) { if (versionValue.isDelete()) { return GetResult.NOT_EXISTS; } if (get.versionType().isVersionConflictForReads(versionValue.version, get.version())) { throw new VersionConflictEngineException(shardId, get.id(), get.versionType().explainConflictForReads(versionValue.version, get.version())); } if (get.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO && ( get.getIfSeqNo() != versionValue.seqNo || get.getIfPrimaryTerm() != versionValue.term )) { throw new VersionConflictEngineException(shardId, get.id(), get.getIfSeqNo(), get.getIfPrimaryTerm(), versionValue.seqNo, versionValue.term); } if (get.isReadFromTranslog()) { // this is only used for updates - API _GET calls will always read form a reader for consistency // the update call doesn't need the consistency since it's source only + _parent but parent can go away in 7.0 if (versionValue.getLocation() != null) { try { Translog.Operation operation = translog.readOperation(versionValue.getLocation()); if (operation != null) { // in the case of a already pruned translog generation we might get null here - yet very unlikely final Translog.Index index = (Translog.Index) operation; TranslogLeafReader reader = new TranslogLeafReader(index); return new GetResult(new Engine.Searcher("realtime_get", reader, IndexSearcher.getDefaultSimilarity(), null, IndexSearcher.getDefaultQueryCachingPolicy(), reader), new VersionsAndSeqNoResolver.DocIdAndVersion(0, index.version(), index.seqNo(), index.primaryTerm(), reader, 0)); } } catch (IOException e) { maybeFailEngine("realtime_get", e); // lets check if the translog has failed with a tragic event throw new EngineException(shardId, "failed to read operation from translog", e); } } else { trackTranslogLocation.set(true); } } refreshIfNeeded("realtime_get", versionValue.seqNo); } scope = SearcherScope.INTERNAL; } else { // we expose what has been externally expose in a point in time snapshot via an explicit refresh scope = SearcherScope.EXTERNAL; } // no version, get the version from the index, we know that we refresh on flush return getFromSearcher(get, searcherFactory, scope); } }	can you add an assertion that versionvalue.seqno >= 0?
public void testReadCorruptedArraySize() throws IOException { try (BytesStreamOutput output = new BytesStreamOutput(0)) { output.writeVInt(10); for (int i = 0; i < 10; i ++) { output.writeInt(i); } output.writeVInt(100); for (int i = 0; i < 10; i ++) { output.writeInt(i); } try (StreamInput streamInput = output.bytes().streamInput()) { int[] ints = streamInput.readIntArray(); for (int i = 0; i < 10; i ++) { assertEquals(i, ints[i]); } EOFException eofException = expectThrows(EOFException.class, () -> streamInput.readIntArray()); assertEquals("tried to read: 100 bytes but this stream is limited to: 82", eofException.getMessage()); } } }	why did this change?
private void startCluster() { final ClusterState initialClusterState = new ClusterState.Builder(ClusterName.DEFAULT).nodes(testClusterNodes.randomDiscoveryNodes()).build(); testClusterNodes.nodes.values().forEach(testClusterNode -> testClusterNode.start(initialClusterState)); deterministicTaskQueue.advanceTime(); if (deterministicTaskQueue.hasRunnableTasks()) { deterministicTaskQueue.runAllRunnableTasks(); } final BootstrapConfiguration bootstrapConfiguration = new BootstrapConfiguration( testClusterNodes.nodes.values().stream().filter(n -> n.node.isMasterNode()) .map(node -> new BootstrapConfiguration.NodeDescription(node.node)) .distinct() .collect(Collectors.toList())); testClusterNodes.nodes.values().stream().filter(n -> n.node.isMasterNode()).forEach( testClusterNode -> testClusterNode.coordinator.setInitialConfiguration(bootstrapConfiguration) ); runUntil( () -> { List<String> masterNodeIds = testClusterNodes.nodes.values().stream() .map(node -> node.clusterService.state().nodes().getMasterNodeId()) .distinct().collect(Collectors.toList()); return masterNodeIds.size() == 1 && masterNodeIds.contains(null) == false; }, TimeUnit.SECONDS.toMillis(30L) ); }	renamed this to something more appropriate.
/** @see DocumentMapper#merge(Mapping, MergeReason) */ public Mapping merge(Mapping mergeWith, MergeReason reason) { RootObjectMapper mergedRoot = root.merge(mergeWith.root, reason); Map<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> mergedMetadataMappers = new HashMap<>(metadataMappersMap); for (MetadataFieldMapper metaMergeWith : mergeWith.metadataMappers) { MetadataFieldMapper mergeInto = mergedMetadataMappers.get(metaMergeWith.getClass()); MetadataFieldMapper merged; if (mergeInto == null || reason == MergeReason.INDEX_TEMPLATE) { merged = metaMergeWith; } else { merged = (MetadataFieldMapper) mergeInto.merge(metaMergeWith); } mergedMetadataMappers.put(merged.getClass(), merged); } Map<String, Object> mergedMeta; if (mergeWith.meta == null) { mergedMeta = meta; } else if (meta == null || reason != MergeReason.INDEX_TEMPLATE) { mergedMeta = mergeWith.meta; } else { mergedMeta = new HashMap<>(mergeWith.meta); XContentHelper.mergeDefaults(mergedMeta, meta); } return new Mapping(indexCreated, mergedRoot, mergedMetadataMappers.values().toArray(new MetadataFieldMapper[0]), mergedMeta); }	maybe add a small comment explaining the logic for meta and metadata fields ?
protected void doMerge(final ObjectMapper mergeWith, MergeReason reason) { nested().merge(mergeWith.nested(), reason); if (mergeWith.dynamic != null) { this.dynamic = mergeWith.dynamic; } if (reason == MergeReason.INDEX_TEMPLATE) { if (mergeWith.enabled.explicit()) { this.enabled = mergeWith.enabled; } } else if (isEnabled() != mergeWith.isEnabled()) { throw new MapperException("the [enabled] parameter can't be updated for the object mapping [" + name() + "]"); } for (Mapper mergeWithMapper : mergeWith) { Mapper mergeIntoMapper = mappers.get(mergeWithMapper.simpleName()); Mapper merged; if (mergeIntoMapper == null) { merged = mergeWithMapper; } else if (mergeIntoMapper instanceof ObjectMapper) { ObjectMapper objectMapper = (ObjectMapper) mergeIntoMapper; merged = objectMapper.merge(mergeWithMapper, reason); } else { assert mergeIntoMapper instanceof FieldMapper || mergeIntoMapper instanceof FieldAliasMapper; if (mergeWithMapper instanceof ObjectMapper) { throw new IllegalArgumentException("can't merge a non object mapping [" + mergeWithMapper.name() + "] with an object mapping"); } // If we're merging template mappings when creating an index, then a field definition always // replaces an existing one. if (reason == MergeReason.INDEX_TEMPLATE) { merged = mergeWithMapper; } else { merged = mergeIntoMapper.merge(mergeWithMapper); } } putMapper(merged); } }	i wonder if we should nullify the underlying mappers if enabled is false ? it's not related to this pr specifically but we don't need to keep children when the parent is disabled ?
static BitSet bitSetFromDocIterator(DocIdSetIterator iter, int maxDoc) throws IOException { // TODO: This snippet is copied from Lucene Bitset#of. Should we integrate it to Lucene? final long cost = iter.cost(); final int threshold = maxDoc >>> 7; final BitSet set; if (cost < threshold) { set = new SparseFixedBitSet(maxDoc); } else { set = new FixedBitSet(maxDoc); } if (iter.docID() != -1) { throw new IllegalStateException("Must be an unpositioned iterator, got current position = " + iter.docID()); } int matches = 0; for (int doc = iter.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = iter.nextDoc()) { matches++; set.set(doc); } if (matches == maxDoc) { return new MatchAllRoleBitSet(maxDoc); } else { return set; } }	the bitset#or implementations seem to have some optimizations, maybe we should do set.or(it) and then compare set.cardinality() with maxdoc.
private static String getSearchParseExceptionMessage(String currentFieldName, String aggregationName) { return "Failed when parsing field with name '" + currentFieldName + "' found inside aggregation with name '" + aggregationName + "'." + " Few Possible Causes for the error:" + "1)Misspelling the field name. 2)Giving a field name which is not recognized by Geo Distance Aggregator." + "3)Assigning the field to a value with unallowed object type. "; }	enumerating all possible clauses of a parsing failure doesn't look helpful to me. i think we just need to include the field name in the exception that we create on line 148?
NodesDeprecationCheckAction.NodeResponse nodeOperation( NodesDeprecationCheckAction.NodeRequest request, List< DeprecationChecks.NodeDeprecationCheck< Settings, PluginsAndModules, ClusterState, XPackLicenseState, DeprecationIssue>> nodeSettingsChecks ) { Settings filteredNodeSettings = settings.filter(setting -> Regex.simpleMatch(skipTheseDeprecations, setting) == false); Metadata metadata = clusterService.state().metadata(); Settings transientSettings = metadata.transientSettings() .filter(setting -> Regex.simpleMatch(skipTheseDeprecations, setting) == false); Settings persistentSettings = metadata.persistentSettings() .filter(setting -> Regex.simpleMatch(skipTheseDeprecations, setting) == false); ClusterState filteredClusterState = ClusterState.builder(clusterService.state()) .metadata(Metadata.builder(metadata).transientSettings(transientSettings).persistentSettings(persistentSettings).build()) .build(); List<DeprecationIssue> issues = DeprecationInfoAction.filterChecks( nodeSettingsChecks, (c) -> c.apply(filteredNodeSettings, pluginsService.info(), filteredClusterState, licenseState) ); addDiskUsageWarnings(issues, filteredNodeSettings, filteredClusterState.metadata().settings()); return new NodesDeprecationCheckAction.NodeResponse(transportService.getLocalNode(), issues); }	i think if we make this take a clusterinfo and nodeid string (and probably a clustersettings object) then it could be made static, and would be much easier to unit test (since we can construct a fake cluster info object)
private void addDiskUsageWarnings(List<DeprecationIssue> issues, Settings nodeSettings, Settings clusterSettings) { DiskUsage usage = clusterInfoService.getClusterInfo().getNodeMostAvailableDiskUsages().get(transportService.getLocalNode().getId()); if (usage != null) { long freeBytes = usage.getFreeBytes(); double freeDiskPercentage = usage.getFreeDiskAsPercentage(); final boolean emitWarning; if (exceedsLowWatermark(nodeSettings, freeBytes, freeDiskPercentage) || exceedsLowWatermark(clusterSettings, freeBytes, freeDiskPercentage)) { emitWarning = true; } else { emitWarning = false; } if (emitWarning) { issues.add( new DeprecationIssue( DeprecationIssue.Level.CRITICAL, "Disk usage exceeds low watermark", "https://ela.st/es-deprecation-7-disk-watermark-exceeded", String.format( Locale.ROOT, "Disk usage exceeds low watermark, which will prevent reindexing indices during upgrade. Get disk usage on " + "all nodes below the value specified in %s", CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey() ), false, null ) ); } } }	the use of this boolean seems a little strange. why do we have it when it could be: java if (exceedslowwatermark(...) || exceedslowwatermark(...)) { issues.add(...); } and then we don't need an else branch, since it's not used anywhere?
public static SpatialArgs getArgs(Geometry shape, ShapeRelation relation) { switch (relation) { case DISJOINT: return new SpatialArgs(SpatialOperation.IsDisjointTo, buildS4J(shape)); case INTERSECTS: return new SpatialArgs(SpatialOperation.Intersects, buildS4J(shape)); case WITHIN: return new SpatialArgs(SpatialOperation.IsWithin, buildS4J(shape)); case CONTAINS: return new SpatialArgs(SpatialOperation.Contains, buildS4J(shape)); default: throw new IllegalArgumentException("invalid relation [" + relation + "]"); } }	i think in the next iteration i am going to move this into abstractgeometryindexer and make it obtainable from queryshardcontext.
public final void getAsync(GetRequest getRequest, ActionListener<GetResponse> listener, Header... headers) { performRequestAsyncAndParseEntity(getRequest, Request::get, GetResponse::fromXContent, listener, singleton(404), headers); }	thank you, seems like i left this out :)
public void testOpenExistingIndex() throws IOException { String[] indices = randomIndices(1, 5); for (String index : indices) { createIndex(index); closeIndex(index); ResponseException exception = expectThrows(ResponseException.class, () -> client().performRequest("GET", index + "/_search")); assertThat(exception.getResponse().getStatusLine().getStatusCode(), equalTo(RestStatus.BAD_REQUEST.getStatus())); assertThat(exception.getMessage().contains(index), equalTo(true)); } OpenIndexRequest openIndexRequest = new OpenIndexRequest(indices); OpenIndexResponse openIndexResponse = execute(openIndexRequest, highLevelClient().indices()::openIndex, highLevelClient().indices()::openIndexAsync); assertTrue(openIndexResponse.isAcknowledged()); for (String index : indices) { client().performRequest("GET", index + "/_search"); } }	check that 200 was returned?
public void testOpenExistingIndex() throws IOException { String[] indices = randomIndices(1, 5); for (String index : indices) { createIndex(index); closeIndex(index); ResponseException exception = expectThrows(ResponseException.class, () -> client().performRequest("GET", index + "/_search")); assertThat(exception.getResponse().getStatusLine().getStatusCode(), equalTo(RestStatus.BAD_REQUEST.getStatus())); assertThat(exception.getMessage().contains(index), equalTo(true)); } OpenIndexRequest openIndexRequest = new OpenIndexRequest(indices); OpenIndexResponse openIndexResponse = execute(openIndexRequest, highLevelClient().indices()::openIndex, highLevelClient().indices()::openIndexAsync); assertTrue(openIndexResponse.isAcknowledged()); for (String index : indices) { client().performRequest("GET", index + "/_search"); } }	would you mind checking also that with indicesoptions.lenientexpandopen we don't get back an error?
public void testOpenExistingIndex() throws IOException { String[] indices = randomIndices(1, 5); for (String index : indices) { createIndex(index); closeIndex(index); ResponseException exception = expectThrows(ResponseException.class, () -> client().performRequest("GET", index + "/_search")); assertThat(exception.getResponse().getStatusLine().getStatusCode(), equalTo(RestStatus.BAD_REQUEST.getStatus())); assertThat(exception.getMessage().contains(index), equalTo(true)); } OpenIndexRequest openIndexRequest = new OpenIndexRequest(indices); OpenIndexResponse openIndexResponse = execute(openIndexRequest, highLevelClient().indices()::openIndex, highLevelClient().indices()::openIndexAsync); assertTrue(openIndexResponse.isAcknowledged()); for (String index : indices) { client().performRequest("GET", index + "/_search"); } }	maybe we can remove this method and always call the other one?
public void sync() throws IOException { if (syncNeeded()) { synchronized (this) { ensureOpen(); // this call gives a better exception that the incRef if we are closed by a tragic event final long offsetToSync; final int opsCounter; outputStream.flush(); offsetToSync = totalOffset; opsCounter = operationCounter; try { checkpoint(offsetToSync, opsCounter, generation, channel, path); } catch (Throwable ex) { closeWithTragicEvent(ex); throw ex; } lastSyncedOffset = offsetToSync; } } }	comment here is stale (no more incref)?
public void sync() throws IOException { if (syncNeeded()) { synchronized (this) { ensureOpen(); // this call gives a better exception that the incRef if we are closed by a tragic event final long offsetToSync; final int opsCounter; outputStream.flush(); offsetToSync = totalOffset; opsCounter = operationCounter; try { checkpoint(offsetToSync, opsCounter, generation, channel, path); } catch (Throwable ex) { closeWithTragicEvent(ex); throw ex; } lastSyncedOffset = offsetToSync; } } }	hmm, i think if this hits throwable we should also closewithtragicevent, and we should javadoc this method that it declares tragedy on exceptions so callers don't need to?
public boolean isAllowCustomRouting() { return allowCustomRouting; }	perhaps add @nullable to the method here?
public void validate(Function<String, IndexMetadata> imSupplier) { if (indexMode == IndexMode.TIME_SERIES) { List<Tuple<String, Tuple<Instant, Instant>>> startAndEndTimes = indices.stream() .map(index -> imSupplier.apply(index.getName())) .map(im -> { Instant start = IndexSettings.TIME_SERIES_START_TIME.get(im.getSettings()); Instant end = IndexSettings.TIME_SERIES_END_TIME.get(im.getSettings()); assert end.isAfter(start); // This is also validated by TIME_SERIES_END_TIME setting. return new Tuple<>(im.getIndex().getName(), new Tuple<>(start, end)); }) .sorted(Comparator.comparing(entry -> entry.v2().v1())) .collect(Collectors.toList()); Tuple<String, Tuple<Instant, Instant>> previous = null; for (Tuple<String, Tuple<Instant, Instant>> startAndEndTime : startAndEndTimes) { if (previous == null) { previous = startAndEndTime; } else { // previous end time should be on or before current start time if (previous.v2().v2().compareTo(startAndEndTime.v2().v1()) > 0) { throw new IllegalArgumentException( "backing index [" + previous.v1() + "] is overlapping with backing index [" + startAndEndTime.v1() + "]" ); } } } } }	are the ranges inclusive or exclusive? i am just curious whether start time is inclusive and end time is exclusive, or how that works?
public void validate(Function<String, IndexMetadata> imSupplier) { if (indexMode == IndexMode.TIME_SERIES) { List<Tuple<String, Tuple<Instant, Instant>>> startAndEndTimes = indices.stream() .map(index -> imSupplier.apply(index.getName())) .map(im -> { Instant start = IndexSettings.TIME_SERIES_START_TIME.get(im.getSettings()); Instant end = IndexSettings.TIME_SERIES_END_TIME.get(im.getSettings()); assert end.isAfter(start); // This is also validated by TIME_SERIES_END_TIME setting. return new Tuple<>(im.getIndex().getName(), new Tuple<>(start, end)); }) .sorted(Comparator.comparing(entry -> entry.v2().v1())) .collect(Collectors.toList()); Tuple<String, Tuple<Instant, Instant>> previous = null; for (Tuple<String, Tuple<Instant, Instant>> startAndEndTime : startAndEndTimes) { if (previous == null) { previous = startAndEndTime; } else { // previous end time should be on or before current start time if (previous.v2().v2().compareTo(startAndEndTime.v2().v1()) > 0) { throw new IllegalArgumentException( "backing index [" + previous.v1() + "] is overlapping with backing index [" + startAndEndTime.v1() + "]" ); } } } } }	can you include the time ranges in the error message so that a user can tell what the overlap is?
public boolean isAllowCustomRouting() { return allowCustomRouting; }	perhaps also add @nullable here?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(NAME_FIELD.getPreferredName(), name); builder.field(TIMESTAMP_FIELD_FIELD.getPreferredName(), timeStampField); builder.xContentList(INDICES_FIELD.getPreferredName(), indices); builder.field(GENERATION_FIELD.getPreferredName(), generation); if (metadata != null) { builder.field(METADATA_FIELD.getPreferredName(), metadata); } builder.field(HIDDEN_FIELD.getPreferredName(), hidden); builder.field(REPLICATED_FIELD.getPreferredName(), replicated); builder.field(SYSTEM_FIELD.getPreferredName(), system); builder.field(ALLOW_CUSTOM_ROUTING.getPreferredName(), allowCustomRouting); if (indexMode != null) { builder.field(INDEX_MODE.getPreferredName(), indexMode); } builder.endObject(); return builder; }	and also here, for outputting a lowercased version of the index mode
public Builder dataStreams(Map<String, DataStream> dataStreams, Map<String, DataStreamAlias> dataStreamAliases) { previousIndicesLookup = null; for (DataStream dataStream : dataStreams.values()) { dataStream.validate(indices::get); } this.customs.put(DataStreamMetadata.TYPE, new DataStreamMetadata(dataStreams, dataStreamAliases)); return this; }	feels a little strange to do the validation in the builder building methods, can we do the validation in the actual .build() method? that way we don't do validation multiple times if a caller uses this method more than once?
public Settings getAdditionalIndexSettings( String indexName, String dataStreamName, ComposableIndexTemplate matchingTemplate, Metadata metadata, long resolvedAt, Settings allSettings ) { if (dataStreamName != null) { DataStream dataStream = metadata.dataStreams().get(dataStreamName); IndexMode indexMode; if (dataStream != null) { indexMode = dataStream.getIndexMode(); } else { indexMode = matchingTemplate.getDataStreamTemplate().getIndexMode(); } if (indexMode != null) { Settings.Builder builder = Settings.builder(); builder.put(IndexSettings.MODE.getKey(), indexMode.name().toLowerCase(Locale.ROOT)); if (indexMode == IndexMode.TIME_SERIES) { TimeValue lookAheadTime = allSettings.getAsTime( IndexSettings.LOOK_AHEAD_TIME.getKey(), IndexSettings.LOOK_AHEAD_TIME.getDefault(allSettings) ); Instant start; if (dataStream == null) { start = Instant.ofEpochMilli(resolvedAt).minusMillis(lookAheadTime.getMillis()); } else { IndexMetadata currentLatestBackingIndex = metadata.index(dataStream.getWriteIndex()); if (currentLatestBackingIndex.getSettings().hasValue(IndexSettings.TIME_SERIES_END_TIME.getKey()) == false) { throw new IllegalStateException( String.format( Locale.ROOT, "backing index [%s] in tsdb mode doesn't have the [%s] index setting", currentLatestBackingIndex.getIndex().getName(), IndexSettings.TIME_SERIES_START_TIME.getKey() ) ); } start = IndexSettings.TIME_SERIES_END_TIME.get(currentLatestBackingIndex.getSettings()); } builder.put(IndexSettings.TIME_SERIES_START_TIME.getKey(), FORMATTER.format(start)); Instant end = Instant.ofEpochMilli(resolvedAt).plusMillis(lookAheadTime.getMillis()); builder.put(IndexSettings.TIME_SERIES_END_TIME.getKey(), FORMATTER.format(end)); } return builder.build(); } } return Settings.EMPTY; }	same comment about perhaps having a dedicated tostring() on indexmode that serializes it in a good way
public Settings getAdditionalIndexSettings( String indexName, String dataStreamName, ComposableIndexTemplate matchingTemplate, Metadata metadata, long resolvedAt, Settings allSettings ) { if (dataStreamName != null) { DataStream dataStream = metadata.dataStreams().get(dataStreamName); IndexMode indexMode; if (dataStream != null) { indexMode = dataStream.getIndexMode(); } else { indexMode = matchingTemplate.getDataStreamTemplate().getIndexMode(); } if (indexMode != null) { Settings.Builder builder = Settings.builder(); builder.put(IndexSettings.MODE.getKey(), indexMode.name().toLowerCase(Locale.ROOT)); if (indexMode == IndexMode.TIME_SERIES) { TimeValue lookAheadTime = allSettings.getAsTime( IndexSettings.LOOK_AHEAD_TIME.getKey(), IndexSettings.LOOK_AHEAD_TIME.getDefault(allSettings) ); Instant start; if (dataStream == null) { start = Instant.ofEpochMilli(resolvedAt).minusMillis(lookAheadTime.getMillis()); } else { IndexMetadata currentLatestBackingIndex = metadata.index(dataStream.getWriteIndex()); if (currentLatestBackingIndex.getSettings().hasValue(IndexSettings.TIME_SERIES_END_TIME.getKey()) == false) { throw new IllegalStateException( String.format( Locale.ROOT, "backing index [%s] in tsdb mode doesn't have the [%s] index setting", currentLatestBackingIndex.getIndex().getName(), IndexSettings.TIME_SERIES_START_TIME.getKey() ) ); } start = IndexSettings.TIME_SERIES_END_TIME.get(currentLatestBackingIndex.getSettings()); } builder.put(IndexSettings.TIME_SERIES_START_TIME.getKey(), FORMATTER.format(start)); Instant end = Instant.ofEpochMilli(resolvedAt).plusMillis(lookAheadTime.getMillis()); builder.put(IndexSettings.TIME_SERIES_END_TIME.getKey(), FORMATTER.format(end)); } return builder.build(); } } return Settings.EMPTY; }	can we use indexsettings.look_ahead_time.get(allsettings) which should resolve the default value appropriately here instead?
public Settings getAdditionalIndexSettings( String indexName, String dataStreamName, ComposableIndexTemplate matchingTemplate, Metadata metadata, long resolvedAt, Settings allSettings ) { if (dataStreamName != null) { DataStream dataStream = metadata.dataStreams().get(dataStreamName); IndexMode indexMode; if (dataStream != null) { indexMode = dataStream.getIndexMode(); } else { indexMode = matchingTemplate.getDataStreamTemplate().getIndexMode(); } if (indexMode != null) { Settings.Builder builder = Settings.builder(); builder.put(IndexSettings.MODE.getKey(), indexMode.name().toLowerCase(Locale.ROOT)); if (indexMode == IndexMode.TIME_SERIES) { TimeValue lookAheadTime = allSettings.getAsTime( IndexSettings.LOOK_AHEAD_TIME.getKey(), IndexSettings.LOOK_AHEAD_TIME.getDefault(allSettings) ); Instant start; if (dataStream == null) { start = Instant.ofEpochMilli(resolvedAt).minusMillis(lookAheadTime.getMillis()); } else { IndexMetadata currentLatestBackingIndex = metadata.index(dataStream.getWriteIndex()); if (currentLatestBackingIndex.getSettings().hasValue(IndexSettings.TIME_SERIES_END_TIME.getKey()) == false) { throw new IllegalStateException( String.format( Locale.ROOT, "backing index [%s] in tsdb mode doesn't have the [%s] index setting", currentLatestBackingIndex.getIndex().getName(), IndexSettings.TIME_SERIES_START_TIME.getKey() ) ); } start = IndexSettings.TIME_SERIES_END_TIME.get(currentLatestBackingIndex.getSettings()); } builder.put(IndexSettings.TIME_SERIES_START_TIME.getKey(), FORMATTER.format(start)); Instant end = Instant.ofEpochMilli(resolvedAt).plusMillis(lookAheadTime.getMillis()); builder.put(IndexSettings.TIME_SERIES_END_TIME.getKey(), FORMATTER.format(end)); } return builder.build(); } } return Settings.EMPTY; }	i think this will throw an exception when someone converts a data stream from "standard" to "time_series" mode right? because the current backing index wouldn't have the end time, but the new one will look for the prior index's end date? (it might not be something we're trying to fix for right, just trying to understand the behavior)
ClusterState addIndexTemplateV2(final ClusterState currentState, final boolean create, final String name, final IndexTemplateV2 template) throws Exception { if (create && currentState.metadata().templatesV2().containsKey(name)) { throw new IllegalArgumentException("index template [" + name + "] already exists"); } Map<String, List<String>> overlaps = findConflictingV2Templates(currentState, name, template.indexPatterns(), true, template.priority()); if (overlaps.size() > 0) { String error = String.format(Locale.ROOT, "index template [%s] has index patterns %s matching patterns from " + "existing templates [%s] with patterns (%s) that have the same priority [%d]", name, template.indexPatterns(), Strings.collectionToCommaDelimitedString(overlaps.keySet()), overlaps.entrySet().stream() .map(e -> e.getKey() + " => " + e.getValue()) .collect(Collectors.joining(",")), template.priority()); throw new IllegalArgumentException(error); } overlaps = findConflictingV1Templates(currentState, name, template.indexPatterns()); if (overlaps.size() > 0) { String warning = String.format(Locale.ROOT, "index template [%s] has index patterns %s matching patterns from " + "existing older templates [%s] with patterns (%s); this template [%s] will take precedence during new index creation", name, template.indexPatterns(), Strings.collectionToCommaDelimitedString(overlaps.keySet()), overlaps.entrySet().stream() .map(e -> e.getKey() + " => " + e.getValue()) .collect(Collectors.joining(",")), name); logger.warn(warning); deprecationLogger.deprecated(warning); } IndexTemplateV2 finalIndexTemplate = template; Template innerTemplate = template.template(); if (innerTemplate != null) { // We may need to normalize index settings, so do that also Settings finalSettings = innerTemplate.settings(); if (finalSettings != null) { finalSettings = Settings.builder() .put(finalSettings).normalizePrefix(IndexMetadata.INDEX_SETTING_PREFIX) .build(); } // If an inner template was specified, its mappings may need to be // adjusted (to add _doc) and it should be validated CompressedXContent mappings = innerTemplate.mappings(); String stringMappings = mappings == null ? null : mappings.string(); validateTemplate(finalSettings, stringMappings, indicesService, xContentRegistry); // Mappings in index templates don't include _doc, so update the mappings to include this single type if (stringMappings != null) { Map<String, Object> parsedMappings = MapperService.parseMapping(xContentRegistry, stringMappings); if (parsedMappings.size() > 0) { stringMappings = Strings.toString(XContentFactory.jsonBuilder() .startObject() .field(MapperService.SINGLE_MAPPING_NAME, parsedMappings) .endObject()); } } final Template finalTemplate = new Template(finalSettings, stringMappings == null ? null : new CompressedXContent(stringMappings), innerTemplate.aliases()); finalIndexTemplate = new IndexTemplateV2(template.indexPatterns(), finalTemplate, template.composedOf(), template.priority(), template.version(), template.metadata()); } logger.info("adding index template [{}]", name); return ClusterState.builder(currentState) .metadata(Metadata.builder(currentState.metadata()).put(name, finalIndexTemplate)) .build(); }	i think we can be a little more explicit with what the user needs to do, maybe something like: index template [foo] has index patterns [bar*] matching patterns from existing templates [baz] with patterns (bar*) that have the same priority [5], multiple index templates may not match during index creation, please use a different priority what do you think?
static Map<String, List<String>> findConflictingV2Templates(final ClusterState state, final String candidateName, final List<String> indexPatterns, boolean checkPriority, Long priority) { Automaton v1automaton = Regex.simpleMatchToAutomaton(indexPatterns.toArray(Strings.EMPTY_ARRAY)); Map<String, List<String>> overlappingTemplates = new HashMap<>(); for (Map.Entry<String, IndexTemplateV2> entry : state.metadata().templatesV2().entrySet()) { String name = entry.getKey(); IndexTemplateV2 template = entry.getValue(); Automaton v2automaton = Regex.simpleMatchToAutomaton(template.indexPatterns().toArray(Strings.EMPTY_ARRAY)); if (Operations.isEmpty(Operations.intersection(v1automaton, v2automaton)) == false) { if(checkPriority == false || Objects.equals(priority, template.priority())) { logger.debug("old template {} and index template {} would overlap: {} <=> {}", candidateName, name, indexPatterns, template.indexPatterns()); overlappingTemplates.put(name, template.indexPatterns()); } } } return overlappingTemplates; }	super minor nit suggestion if (checkpriority == false || objects.equals(priority, template.priority())) {
public void testAddIndexTemplateV2() throws Exception { ClusterState state = ClusterState.EMPTY_STATE; final MetadataIndexTemplateService metadataIndexTemplateService = getMetadataIndexTemplateService(); IndexTemplateV2 template = IndexTemplateV2Tests.randomInstance(); state = metadataIndexTemplateService.addIndexTemplateV2(state, false, "foo", template); assertNotNull(state.metadata().templatesV2().get("foo")); assertTemplatesEqual(state.metadata().templatesV2().get("foo"), template); IndexTemplateV2 newTemplate = IndexTemplateV2Tests.randomInstance(); final ClusterState throwState = ClusterState.builder(state).build(); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> metadataIndexTemplateService.addIndexTemplateV2(throwState, true, "foo", newTemplate)); assertThat(e.getMessage(), containsString("index template [foo] already exists")); state = metadataIndexTemplateService.addIndexTemplateV2(state, randomBoolean(), "bar", newTemplate); assertNotNull(state.metadata().templatesV2().get("bar")); }	it's suuuuper unlikely, but this might be better as: java indextemplatev2 newtemplate = randomvalueotherthanmany(t -> objects.equals(template.priority(), t.priority(), () -> indextemplatev2tests.randominstance()); to prevent generating a new template that accidentally has the same index patterns and priority (highly unlikely but not impossible)
@Override public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) { InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg = (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation; List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets(); BucketAggregationScript.Factory factory = reduceContext.scriptService().compile(script, BucketAggregationScript.CONTEXT); List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>(); for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) { Map<String, Object> vars = new HashMap<>(); if (script.getParams() != null) { vars.putAll(script.getParams()); } boolean skipBucket = false; for (Map.Entry<String, String> entry : bucketsPathsMap.entrySet()) { String varName = entry.getKey(); String bucketsPath = entry.getValue(); Double value = resolveBucketValue(originalAgg, bucket, bucketsPath, gapPolicy); if (GapPolicy.SKIP == gapPolicy && (value == null || Double.isNaN(value))) { skipBucket = true; break; } vars.put(varName, value); } if (skipBucket) { newBuckets.add(bucket); } else { Double returned = factory.newInstance(vars).execute(); if (returned == null) { newBuckets.add(bucket); } else { final List<InternalAggregation> aggs = StreamSupport.stream(bucket.getAggregations().spliterator(), false).map( (p) -> (InternalAggregation) p).collect(Collectors.toList()); aggs.add(new InternalSimpleValue(name(), returned, formatter, new ArrayList<>(), metaData())); InternalMultiBucketAggregation.InternalBucket newBucket = originalAgg.createBucket(new InternalAggregations(aggs), bucket); newBuckets.add(newBucket); } } } return originalAgg.create(newBuckets); }	should this else clause be returned to what it was before the original refactoring pr? https://github.com/elastic/elasticsearch/pull/32068/files#diff-c94184ea4ef180f10817aa2bbd41a8edl119
public static TermsGroupSource fromXContent(final XContentParser parser, boolean lenient) throws IOException { return lenient ? LENIENT_PARSER.apply(parser, null) : STRICT_PARSER.apply(parser, null); }	it may be a good idea to put in some logic for when changedbuckets.size() > 10_000.
private long getTimeStampForTimeBasedSynchronization(SyncConfig syncConfig, long timestamp) { if (syncConfig instanceof TimeSyncConfig) { TimeSyncConfig timeSyncConfig = (TimeSyncConfig) syncConfig; return timestamp - timeSyncConfig.getDelay().millis(); } return 0L; }	it would be great to figure out how to avoid instanceof here. in particular, anything that has to do with the notion time in the checkpoint service should probably be factored out in a class that implements an interface the service talks to. then depending on the type of sync we can pass the relevant instance. i haven't read through all the code to understand the details, but that seems like it would be the ideal solution if we ever add other types of syncing.
@Override protected SearchRequest buildSearchRequest() { SearchRequest searchRequest = new SearchRequest(getConfig().getSource().getIndex()); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(pivot.buildAggregation(getPosition(), pageSize)); sourceBuilder.size(0); QueryBuilder pivotQueryBuilder = getConfig().getSource().getQueryConfig().getQuery(); DataFrameTransformConfig config = getConfig(); if (config.getSyncConfig() != null) { if (inProgressOrLastCheckpoint == null) { throw new RuntimeException("in progress checkpoint not found"); } BoolQueryBuilder filteredQuery = new BoolQueryBuilder(). filter(pivotQueryBuilder). filter(config.getSyncConfig().getRangeQuery(inProgressOrLastCheckpoint)); if (changedBuckets != null && changedBuckets.isEmpty() == false) { QueryBuilder pivotFilter = pivot.filterBuckets(changedBuckets); if (pivotFilter != null) { filteredQuery.filter(pivotFilter); } } logger.trace("running filtered query: {}", filteredQuery); sourceBuilder.query(filteredQuery); } else { sourceBuilder.query(pivotQueryBuilder); } searchRequest.source(sourceBuilder); return searchRequest; } /** * Handle the circuit breaking case: A search consumed to much memory and got aborted. * * Going out of memory we smoothly reduce the page size which reduces memory consumption. * * Implementation details: We take the values from the circuit breaker as a hint, but * note that it breaks early, that's why we also reduce using * * @param e Exception thrown, only {@link CircuitBreakingException}	so, if there are *no* changed buckets, we query the entire timeframe?
private void getChangedBuckets(DataFrameTransformCheckpoint oldCheckpoint, DataFrameTransformCheckpoint newCheckpoint, ActionListener<Map<String, List<String>>> listener) { SearchRequest searchRequest = new SearchRequest(getConfig().getSource().getIndex()); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); // we do not need the sub-aggs CompositeAggregationBuilder changesAgg = pivot.buildChangedBucketsAggregation(pageSize); sourceBuilder.aggregation(changesAgg); sourceBuilder.size(0); QueryBuilder pivotQueryBuilder = getConfig().getSource().getQueryConfig().getQuery(); DataFrameTransformConfig config = getConfig(); if (config.getSyncConfig() != null) { BoolQueryBuilder filteredQuery = new BoolQueryBuilder(). filter(pivotQueryBuilder). filter(config.getSyncConfig().getRangeQuery(oldCheckpoint, newCheckpoint)); logger.trace("Gathering changes using query {}", filteredQuery); sourceBuilder.query(filteredQuery); } else { logger.trace("No sync configured"); listener.onResponse(null); return; } searchRequest.source(sourceBuilder); searchRequest.allowPartialSearchResults(false); // pre-fill the hashmap with the expected keys // note that changed bucket might be empty due to the query HashMap<String, List<String>> keys = new HashMap<>(); for(Entry<String, SingleGroupSource> entry: config.getPivotConfig().getGroupConfig().getGroups().entrySet()) { keys.put(entry.getKey(), new ArrayList<>()); } collectChangedBuckets(searchRequest, changesAgg, keys, ActionListener.wrap(listener::onResponse, e -> { // fall back if bucket collection failed logger.error("Failed to retrieve changed buckets, fall back to complete retrieval", e); listener.onResponse(null); })); }	it seems to me that we also only care about the terms agg. if we keep the histogram and date_histogram in the composite agg, we could have tons of buckets to scroll through for no reason.
private void getChangedBuckets(DataFrameTransformCheckpoint oldCheckpoint, DataFrameTransformCheckpoint newCheckpoint, ActionListener<Map<String, List<String>>> listener) { SearchRequest searchRequest = new SearchRequest(getConfig().getSource().getIndex()); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); // we do not need the sub-aggs CompositeAggregationBuilder changesAgg = pivot.buildChangedBucketsAggregation(pageSize); sourceBuilder.aggregation(changesAgg); sourceBuilder.size(0); QueryBuilder pivotQueryBuilder = getConfig().getSource().getQueryConfig().getQuery(); DataFrameTransformConfig config = getConfig(); if (config.getSyncConfig() != null) { BoolQueryBuilder filteredQuery = new BoolQueryBuilder(). filter(pivotQueryBuilder). filter(config.getSyncConfig().getRangeQuery(oldCheckpoint, newCheckpoint)); logger.trace("Gathering changes using query {}", filteredQuery); sourceBuilder.query(filteredQuery); } else { logger.trace("No sync configured"); listener.onResponse(null); return; } searchRequest.source(sourceBuilder); searchRequest.allowPartialSearchResults(false); // pre-fill the hashmap with the expected keys // note that changed bucket might be empty due to the query HashMap<String, List<String>> keys = new HashMap<>(); for(Entry<String, SingleGroupSource> entry: config.getPivotConfig().getGroupConfig().getGroups().entrySet()) { keys.put(entry.getKey(), new ArrayList<>()); } collectChangedBuckets(searchRequest, changesAgg, keys, ActionListener.wrap(listener::onResponse, e -> { // fall back if bucket collection failed logger.error("Failed to retrieve changed buckets, fall back to complete retrieval", e); listener.onResponse(null); })); }	similar to above, i think a non-trivial optimization would be to only do terms
private void getChangedBuckets(DataFrameTransformCheckpoint oldCheckpoint, DataFrameTransformCheckpoint newCheckpoint, ActionListener<Map<String, List<String>>> listener) { SearchRequest searchRequest = new SearchRequest(getConfig().getSource().getIndex()); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); // we do not need the sub-aggs CompositeAggregationBuilder changesAgg = pivot.buildChangedBucketsAggregation(pageSize); sourceBuilder.aggregation(changesAgg); sourceBuilder.size(0); QueryBuilder pivotQueryBuilder = getConfig().getSource().getQueryConfig().getQuery(); DataFrameTransformConfig config = getConfig(); if (config.getSyncConfig() != null) { BoolQueryBuilder filteredQuery = new BoolQueryBuilder(). filter(pivotQueryBuilder). filter(config.getSyncConfig().getRangeQuery(oldCheckpoint, newCheckpoint)); logger.trace("Gathering changes using query {}", filteredQuery); sourceBuilder.query(filteredQuery); } else { logger.trace("No sync configured"); listener.onResponse(null); return; } searchRequest.source(sourceBuilder); searchRequest.allowPartialSearchResults(false); // pre-fill the hashmap with the expected keys // note that changed bucket might be empty due to the query HashMap<String, List<String>> keys = new HashMap<>(); for(Entry<String, SingleGroupSource> entry: config.getPivotConfig().getGroupConfig().getGroups().entrySet()) { keys.put(entry.getKey(), new ArrayList<>()); } collectChangedBuckets(searchRequest, changesAgg, keys, ActionListener.wrap(listener::onResponse, e -> { // fall back if bucket collection failed logger.error("Failed to retrieve changed buckets, fall back to complete retrieval", e); listener.onResponse(null); })); }	if the composite aggs request also includes the *histograms, then there is a strong possibility that the list of changed terms will have many duplicates.
@Override public Object extract(SearchHit hit) { Object value = null; if (useDocValue) { DocumentField field = hit.field(fieldName); if (field != null) { value = unwrapMultiValue(field.getValues()); } } else { // if the field was ignored because it was malformed and ignore_malformed was turned on if (fullFieldName != null && hit.getFields().containsKey(IgnoredFieldMapper.NAME) && isFromDocValuesOnly(dataType) == false && (dataType.isNumeric() || dataType == DataTypes.IP)) { /* * ignore_malformed makes sense for extraction from _source for numeric and IP fields only. * And we check here that the data type is actually a numeric or IP one to rule out * any non-numeric sub-fields (for which the "parent" field should actually be extracted from _source). * For example, in the case of a malformed number, a "byte" field with "ignore_malformed: true" * with a "text" sub-field should return "null" for the "byte" parent field and the actual malformed * data for the "text" sub-field. Also, the _ignored section of the response contains the full field * name, thus the need to do the comparison with that and not only the field name. */ if (hit.getFields().get(IgnoredFieldMapper.NAME).getValues().contains(fullFieldName)) { return null; } } Map<String, Object> source = hit.getSourceAsMap(); if (source != null) { value = extractFromSource(source); } } return value; }	are there any cases where the type is relevant for ignore malformed is set and the return value should not be null? it is currently available for numeric, date , geo-points and ip (https://www.elastic.co/guide/en/elasticsearch/reference/current/ignore-malformed.html). i would argue that if it is set, null should be returned regardless of the data type.
public static void setupAll() { Assume.assumeFalse(Architecture.current() == Architecture.AARCH64); }	for now we just ignore the unit tests on aarch64 as we filter out there anything < 7.12
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Bucket bucket = (Bucket) o; if (docCount != bucket.docCount) return false; return Objects.equals(key, bucket.key) && Objects.equals(from, bucket.from) && Objects.equals(to, bucket.to) && Objects.equals(aggregations, bucket.aggregations); }	keyed and format are in the hashcode but not in equals. i think it is fine not to consider them in equals since they only have an impact on formatting, but then hashcode should not use them either?
@Override public void authenticate(AuthenticationToken authToken, ActionListener<AuthenticationResult> listener) { assert delegatedRealms != null : "Realm has not been initialized correctly"; X509AuthenticationToken token = (X509AuthenticationToken)authToken; try { final BytesKey fingerprint = computeFingerprint(token.credentials()[0]); User user = cache.get(fingerprint); if (user != null) { if (delegatedRealms.hasDelegation()) { delegatedRealms.resolveUser(token.principal(), listener); } else { listener.onResponse(AuthenticationResult.success(user)); } } else if (isCertificateChainTrusted(trustManager, token, logger) == false) { listener.onResponse(AuthenticationResult.unsuccessful("Certificate for " + token.dn() + " is not trusted", null)); } else { final ActionListener<AuthenticationResult> cachingListener = ActionListener.wrap(result -> { if (result.isAuthenticated()) { try (ReleasableLock ignored = readLock.acquire()) { cache.put(fingerprint, result.getUser()); } } listener.onResponse(result); }, listener::onFailure); if (delegatedRealms.hasDelegation()) { delegatedRealms.resolveUser(token.principal(), cachingListener); } else { this.buildUser(token, cachingListener); } } } catch (CertificateEncodingException e) { listener.onResponse(AuthenticationResult.unsuccessful("Certificate for " + token.dn() + " has encoding issues", e)); } }	do we want to cache when we are using delegated realms for resolving user? because we are doing lookups in any case
@Override public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException { Map<String, BlobMetaData> builder = new HashMap<>(); blobNamePrefix = blobNamePrefix == null ? "" : blobNamePrefix; try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, blobNamePrefix + "*")) { for (Path file : stream) { final BasicFileAttributes attrs; try { attrs = Files.readAttributes(file, BasicFileAttributes.class); } catch (FileNotFoundException e) { continue; } if (attrs.isRegularFile()) { builder.put(file.getFileName().toString(), new PlainBlobMetaData(file.getFileName().toString(), attrs.size())); } } } return unmodifiableMap(builder); }	this might need to be nosuchfileexception | filenotfoundexception.
@Override public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException { Map<String, BlobMetaData> builder = new HashMap<>(); blobNamePrefix = blobNamePrefix == null ? "" : blobNamePrefix; try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, blobNamePrefix + "*")) { for (Path file : stream) { final BasicFileAttributes attrs; try { attrs = Files.readAttributes(file, BasicFileAttributes.class); } catch (FileNotFoundException e) { continue; } if (attrs.isRegularFile()) { builder.put(file.getFileName().toString(), new PlainBlobMetaData(file.getFileName().toString(), attrs.size())); } } } return unmodifiableMap(builder); }	can you add a comment here saying why it's ok to ignore?
@Override public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException { Map<String, BlobMetaData> builder = new HashMap<>(); blobNamePrefix = blobNamePrefix == null ? "" : blobNamePrefix; try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, blobNamePrefix + "*")) { for (Path file : stream) { final BasicFileAttributes attrs; try { attrs = Files.readAttributes(file, BasicFileAttributes.class); } catch (FileNotFoundException e) { continue; } if (attrs.isRegularFile()) { builder.put(file.getFileName().toString(), new PlainBlobMetaData(file.getFileName().toString(), attrs.size())); } } } return unmodifiableMap(builder); }	perhaps easier as the two suggestions above is to use files.isregularfile(file) here which takes care of all this.
public void testSortsWWWAuthenticateHeaderValues() { final String basicAuthScheme = "Basic realm=\\\\"" + XPackField.SECURITY + "\\\\" charset=\\\\"UTF-8\\\\""; final String bearerAuthScheme = "Bearer realm=\\\\"" + XPackField.SECURITY + "\\\\""; final String negotiateAuthScheme = randomFrom("Negotiate", "Negotiate Ijoijksdk"); final Map<String, List<String>> failureResponeHeaders = new HashMap<>(); failureResponeHeaders.put("WWW-Authenticate", Arrays.asList(basicAuthScheme, bearerAuthScheme, negotiateAuthScheme)); final DefaultAuthenticationFailureHandler failuerHandler = new DefaultAuthenticationFailureHandler(failureResponeHeaders); final ElasticsearchSecurityException ese = failuerHandler.exceptionProcessingRequest(Mockito.mock(RestRequest.class), null, new ThreadContext(Settings.builder().build())); assertThat(ese, is(notNullValue())); assertThat(ese.getHeader("WWW-Authenticate"), is(notNullValue())); assertThat(ese.getMessage(), equalTo("error attempting to authenticate request")); assertWWWAuthenticateWithSchemes(ese, negotiateAuthScheme, bearerAuthScheme, basicAuthScheme); }	we should randomize the order of this list, ie collections.shuffle(list, random())
* @param objects a list of objects the parsed one will be added to * @param <T> the type of the object to parse * @return the parsed object * @throws IOException if anything went wrong during parsing or if the type or name cannot be derived * from the field's name */ public static <T> void parseTypedKeysObject(XContentParser parser, String delimiter, Class<T> objectClass, List<T> objects) throws IOException { assert parser.currentToken() == XContentParser.Token.START_OBJECT; String currentFieldName = parser.currentName(); if (Strings.hasLength(currentFieldName)) { int position = currentFieldName.indexOf(delimiter); if (position > 0) { String type = currentFieldName.substring(0, position); String name = currentFieldName.substring(position + 1); objects.add(parser.namedObject(objectClass, type, name)); return; } } // this will only happen when the field name is empty or we don't find a delimiter parser.skipChildren(); }	i think that both methods will be useful, one that returns the object and one that accepts a consumer<t> to consume the result of the previous method. so that it doesn't force calling methods to create a list of objects when the first result will always be used.
* @param objects a list of objects the parsed one will be added to * @param <T> the type of the object to parse * @return the parsed object * @throws IOException if anything went wrong during parsing or if the type or name cannot be derived * from the field's name */ public static <T> void parseTypedKeysObject(XContentParser parser, String delimiter, Class<T> objectClass, List<T> objects) throws IOException { assert parser.currentToken() == XContentParser.Token.START_OBJECT; String currentFieldName = parser.currentName(); if (Strings.hasLength(currentFieldName)) { int position = currentFieldName.indexOf(delimiter); if (position > 0) { String type = currentFieldName.substring(0, position); String name = currentFieldName.substring(position + 1); objects.add(parser.namedObject(objectClass, type, name)); return; } } // this will only happen when the field name is empty or we don't find a delimiter parser.skipChildren(); }	maybe also explain why we skip children (forward comp etc.)
@Override protected void removeDataBefore(Job job, long cutoffEpochMs, ActionListener<Boolean> listener) { listener.onResponse(Boolean.TRUE); } } private OriginSettingClient originSettingClient; private Client client; @Before public void setUpTests() { client = mock(Client.class); originSettingClient = MockOriginSettingClient.mockOriginSettingClient(client, ClientHelper.ML_ORIGIN); } static SearchResponse createSearchResponse(List<? extends ToXContent> toXContents) throws IOException { return createSearchResponse(toXContents, toXContents.size()); } @SuppressWarnings("unchecked") static void givenJobs(Client client, List<Job> jobs) throws IOException { SearchResponse response = AbstractExpiredJobDataRemoverTests.createSearchResponse(jobs); doAnswer(invocationOnMock -> { ActionListener<SearchResponse> listener = (ActionListener<SearchResponse>) invocationOnMock.getArguments()[2]; listener.onResponse(response); return null; }).when(client).execute(eq(SearchAction.INSTANCE), any(), any()); } private static SearchResponse createSearchResponse(List<? extends ToXContent> toXContents, int totalHits) throws IOException { SearchHit[] hitsArray = new SearchHit[toXContents.size()]; for (int i = 0; i < toXContents.size(); i++) { hitsArray[i] = new SearchHit(randomInt()); XContentBuilder jsonBuilder = JsonXContent.contentBuilder(); toXContents.get(i).toXContent(jsonBuilder, ToXContent.EMPTY_PARAMS); hitsArray[i].sourceRef(BytesReference.bytes(jsonBuilder)); } SearchHits hits = new SearchHits(hitsArray, new TotalHits(totalHits, TotalHits.Relation.EQUAL_TO), 1.0f); SearchResponse searchResponse = mock(SearchResponse.class); when(searchResponse.getHits()).thenReturn(hits); return searchResponse; } public void testRemoveGivenNoJobs() throws IOException { SearchResponse response = createSearchResponse(Collections.emptyList()); mockSearchResponse(response); TestListener listener = new TestListener(); ConcreteExpiredJobDataRemover remover = new ConcreteExpiredJobDataRemover(originSettingClient); remover.remove(listener, () -> false); listener.waitToCompletion(); assertThat(listener.success, is(true)); assertEquals(0, remover.getRetentionDaysCallCount); } @SuppressWarnings("unchecked") public void testRemoveGivenMultipleBatches() throws IOException { // This is testing AbstractExpiredJobDataRemover.WrappedBatchedJobsIterator int totalHits = 7; List<SearchResponse> responses = new ArrayList<>(); responses.add(createSearchResponse(Arrays.asList( JobTests.buildJobBuilder("job1").build(), JobTests.buildJobBuilder("job2").build(), JobTests.buildJobBuilder("job3").build() ), totalHits)); responses.add(createSearchResponse(Arrays.asList( JobTests.buildJobBuilder("job4").build(), JobTests.buildJobBuilder("job5").build(), JobTests.buildJobBuilder("job6").build() ), totalHits)); responses.add(createSearchResponse(Collections.singletonList( JobTests.buildJobBuilder("job7").build() ), totalHits)); AtomicInteger searchCount = new AtomicInteger(0); doAnswer(invocationOnMock -> { ActionListener<SearchResponse> listener = (ActionListener<SearchResponse>) invocationOnMock.getArguments()[2]; listener.onResponse(responses.get(searchCount.getAndIncrement())); return null; }).when(client).execute(eq(SearchAction.INSTANCE), any(), any()); TestListener listener = new TestListener(); ConcreteExpiredJobDataRemover remover = new ConcreteExpiredJobDataRemover(originSettingClient); remover.remove(listener, () -> false); listener.waitToCompletion(); assertThat(listener.success, is(true)); assertEquals(3, searchCount.get()); assertEquals(7, remover.getRetentionDaysCallCount); } public void testRemoveGivenTimeOut() throws IOException { int totalHits = 3; SearchResponse response = createSearchResponse(Arrays.asList( JobTests.buildJobBuilder("job1").build(), JobTests.buildJobBuilder("job2").build(), JobTests.buildJobBuilder("job3").build() ), totalHits); final int timeoutAfter = randomIntBetween(0, totalHits - 1); AtomicInteger attemptsLeft = new AtomicInteger(timeoutAfter); mockSearchResponse(response); TestListener listener = new TestListener(); ConcreteExpiredJobDataRemover remover = new ConcreteExpiredJobDataRemover(originSettingClient); remover.remove(listener, () -> (attemptsLeft.getAndDecrement() <= 0)); listener.waitToCompletion(); assertThat(listener.success, is(false)); assertEquals(timeoutAfter, remover.getRetentionDaysCallCount); } @SuppressWarnings("unchecked") private void mockSearchResponse(SearchResponse searchResponse) { doAnswer(invocationOnMock -> { ActionListener<SearchResponse> listener = (ActionListener<SearchResponse>) invocationOnMock.getArguments()[2]; listener.onResponse(searchResponse); return null; }).when(client).execute(eq(SearchAction.INSTANCE), any(), any()); } static class TestListener implements ActionListener<Boolean> { boolean success; private final CountDownLatch latch = new CountDownLatch(1); @Override public void onResponse(Boolean aBoolean) { success = aBoolean; latch.countDown(); } @Override public void onFailure(Exception e) { latch.countDown(); } void waitToCompletion() { try { latch.await(3, TimeUnit.SECONDS); } catch (InterruptedException e) { fail("listener timed out before completing"); } }	looks like this is indented more than the line above.
protected final IndexingStrategy planIndexingAsNonPrimary(Index index) throws IOException { assert assertNonPrimaryOrigin(index); final IndexingStrategy plan; final boolean appendOnlyRequest = canOptimizeAddDocument(index); if (appendOnlyRequest && mayHaveBeenIndexedBefore(index) == false && index.seqNo() > maxSeqNoOfNonAppendOnlyOperations.get()) { /* * As soon as an append-only request was indexed into the primary, it can be exposed to a search then users can issue * a follow-up operation on it. In rare cases, the follow up operation can be arrived and processed on a replica before * the original append-only. In this case we can't simply proceed with the append only without consulting the version map. * If a replica has seen a non-append-only operation with a higher seqno than the seqno of an append-only, it may have seen * the document of that append-only request. However if the seqno of an append-only is higher than seqno of any non-append-only * requests, we can assert the replica have not seen the document of that append-only request, thus we can apply optimization. */ assert index.version() == 1L : "can optimize on replicas but incoming version is [" + index.version() + "]"; plan = IndexingStrategy.optimizedAppendOnly(1L); } else { if (appendOnlyRequest == false) { maxSeqNoOfNonAppendOnlyOperations.updateAndGet(curr -> Math.max(index.seqNo(), curr)); assert maxSeqNoOfNonAppendOnlyOperations.get() >= index.seqNo() : "max_seqno of non-append-only was not updated;" + "max_seqno non-append-only [" + maxSeqNoOfNonAppendOnlyOperations.get() + "], seqno of index [" + index.seqNo() + "]"; } // unlike the primary, replicas don't really care to about creation status of documents // this allows to ignore the case where a document was found in the live version maps in // a delete state and return false for the created flag in favor of code simplicity final long localCheckpoint = localCheckpointTracker.getProcessedCheckpoint(); final long maxSeqNoOfUpdatesOrDeletes = getMaxSeqNoOfUpdatesOrDeletes(); if (index.seqNo() <= localCheckpoint) { // the operation seq# is lower then the current local checkpoint and thus was already put into lucene // this can happen during recovery where older operations are sent from the translog that are already // part of the lucene commit (either from a peer recovery or a local translog) // or due to concurrent indexing & recovery. For the former it is important to skip lucene as the operation in // question may have been deleted in an out of order op that is not replayed. // See testRecoverFromStoreWithOutOfOrderDelete for an example of local recovery // See testRecoveryWithOutOfOrderDelete for an example of peer recovery plan = IndexingStrategy.processButSkipLucene(false, index.version()); // See Engine#getMaxSeqNoOfUpdatesOrDeletes for the explanation of the optimization using sequence numbers } else if (softDeleteEnabled && maxSeqNoOfUpdatesOrDeletes <= localCheckpoint && hasBeenProcessedBefore(index) == false) { assert maxSeqNoOfUpdatesOrDeletes < index.seqNo() : index.seqNo() + ">=" + maxSeqNoOfUpdatesOrDeletes; plan = IndexingStrategy.optimizedAppendOnly(index.version()); } else { versionMap.enforceSafeAccess(); final OpVsLuceneDocStatus opVsLucene = compareOpToLuceneDocBasedOnSeqNo(index); if (opVsLucene == OpVsLuceneDocStatus.OP_STALE_OR_EQUAL) { plan = IndexingStrategy.processAsStaleOp(softDeleteEnabled, index.version()); } else { plan = IndexingStrategy.processNormally(opVsLucene == OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND, index.version()); } } } return plan; }	i will work on a follow-up to replace index.seqno() <= localcheckpoint with hasbeenprocessedbefore(index).
@Override public void visitRegex(ERegex userRegexNode, SemanticScope semanticScope) { String pattern = userRegexNode.getPattern(); String flags = userRegexNode.getFlags(); if (semanticScope.getCondition(userRegexNode, Write.class)) { throw userRegexNode.createError(new IllegalArgumentException( "invalid assignment: cannot assign a value to regex constant [" + pattern + "] with flags [" + flags + "]")); } if (semanticScope.getCondition(userRegexNode, Read.class) == false) { throw userRegexNode.createError(new IllegalArgumentException( "not a statement: regex constant [" + pattern + "] with flags [" + flags + "] not used")); } if (semanticScope.getScriptScope().getCompilerSettings().areRegexesEnabled() == CompilerSettings.RegexEnabled.FALSE) { throw userRegexNode.createError(new IllegalStateException("Regexes are disabled. Set [script.painless.regex.enabled] to " + "[limited] in elasticsearch.yaml to allow them with some protection against super long regexes. You can set it to " + "[true] to allow them without an protection against long backtracking but this usually a bad idea.")); } switch (userRegexNode.getFlavor()) { case JAVA: /* * It's important for backwards compatibility that `/foo/` is * a java.util.regex.Pattern. It's been that way for years and * that class has a zillion things on it. */ semanticScope.putDecoration(userRegexNode, new ValueType(Pattern.class)); semanticScope.putDecoration(userRegexNode, new StandardConstant(compileJavaPattern(userRegexNode))); return; case GROK: FlavoredPattern grok = new FlavoredPattern.ForGrok(compileGrok(userRegexNode, semanticScope)); semanticScope.putDecoration(userRegexNode, new ValueType(FlavoredPattern.class)); semanticScope.putDecoration(userRegexNode, new StandardConstant(grok)); return; case DISECT: FlavoredPattern dissect = new FlavoredPattern.ForDissect(compileDissect(userRegexNode)); semanticScope.putDecoration(userRegexNode, new ValueType(FlavoredPattern.class)); semanticScope.putDecoration(userRegexNode, new StandardConstant(dissect)); return; default: throw userRegexNode.createError( new IllegalArgumentException("Unsupported regex flavor [" + userRegexNode.getFlavor() + "]") ); } }	when i was debugging *stuff* i noticed that i wasn't getting good error message from bad java regexes. it turns out that pse.getdescription() has the actual cause of the syntax error in it and it isn't included in tostring! i've added this because i wanted it when i was debugging stuff but it isn't really related to the rest of the change.
@Override public void visitRegex(ERegex userRegexNode, ScriptScope scriptScope) { String memberFieldName = scriptScope.getNextSyntheticName("regex"); Class<?> type = scriptScope.getDecoration(userRegexNode, ValueType.class).getValueType(); FieldNode irFieldNode = new FieldNode(userRegexNode.getLocation()); irFieldNode.attachDecoration(new IRDModifiers(Modifier.PUBLIC | Modifier.STATIC)); irFieldNode.attachDecoration(new IRDFieldType(type)); irFieldNode.attachDecoration(new IRDName(memberFieldName)); irClassNode.addFieldNode(irFieldNode); scriptScope.addStaticConstant( memberFieldName, scriptScope.getDecoration(userRegexNode, StandardConstant.class).getStandardConstant() ); LoadFieldMemberNode irLoadFieldMemberNode = new LoadFieldMemberNode(userRegexNode.getLocation()); irLoadFieldMemberNode.attachDecoration(new IRDExpressionType(type)); irLoadFieldMemberNode.attachDecoration(new IRDName(memberFieldName)); irLoadFieldMemberNode.attachCondition(IRCStatic.class); scriptScope.putDecoration(userRegexNode, new IRNodeDecoration(irLoadFieldMemberNode)); }	instead of compiling the pattern again in the script we now pass it through as a static constant. this kind of nice for patterns but it makes my life a lot simpler for grok and dissect. now we can build the thing we need in the semantic analysis phase, stuff it on the ir tree, and just attache it when we build the script object.
public void testNoConflictWithRegex() { assertEquals(1, exec("int d = 1; return d/1;")); assertEquals(1, exec("int g = 1; return g/1;")); assertEquals(1, exec("int j = 1; return j/1;")); }	we want to be extra paranoid here that the compiler doesn't see g/ as the key for a grok regex when it is really division. it doesn't, but let's add an explicit test to make sure it never does.
@Override public SetProcessor create(Map<String, Processor.Factory> registry, String processorTag, String description, Map<String, Object> config) throws Exception { String field = ConfigurationUtils.readStringProperty(TYPE, processorTag, config, "field"); String copyFrom = ConfigurationUtils.readOptionalStringProperty(TYPE, processorTag, config, "copy_from"); String mimeType = ConfigurationUtils.readMimeTypeProperty(TYPE, processorTag, config, "mime_type", "application/json"); ValueSource valueSource = null; if (copyFrom == null) { Object value = ConfigurationUtils.readObject(TYPE, processorTag, config, "value"); valueSource = ValueSource.wrap(value, scriptService, Map.of(Script.CONTENT_TYPE_OPTION, mimeType)); } else { Object value = config.remove("value"); if (value != null) { throw newConfigurationException(TYPE, processorTag, "copy_from", "cannot set both `copy_from` and `value` in the same processor"); } } boolean overrideEnabled = ConfigurationUtils.readBooleanProperty(TYPE, processorTag, config, "override", true); TemplateScript.Factory compiledTemplate = ConfigurationUtils.compileTemplate(TYPE, processorTag, "field", field, scriptService); boolean ignoreEmptyValue = ConfigurationUtils.readBooleanProperty(TYPE, processorTag, config, "ignore_empty_value", false); return new SetProcessor( processorTag, description, compiledTemplate, valueSource, copyFrom, overrideEnabled, ignoreEmptyValue); }	are we sure we want mime_type as a name here? i thought mime was replace with media_type but wouldn't content_type be more consistent with what we have in mustache templating engine?
@Override public boolean hasNext() { try { if (position >= length) { return false; } if (cacheBuffer == null) { cacheBuffer = ByteBuffer.allocate(1024); } cacheBuffer.limit(4); int bytesRead = Channels.readFromFileChannel(channel, position, cacheBuffer); if (bytesRead < 0) { // the snapshot is acquired under a write lock. we should never read beyond the EOF throw new EOFException("read past EOF. pos [" + position + "] length: [" + cacheBuffer.limit() + "] end: [" + channel.size() + "]"); } assert bytesRead == 4; cacheBuffer.flip(); int opSize = cacheBuffer.getInt(); position += 4; if ((position + opSize) > length) { // the snapshot is acquired under a write lock. we should never read beyond the EOF position -= 4; throw new EOFException("opSize of [" + opSize + "] pointed beyond EOF. position [" + position + "] length [" + length + "]"); } if (cacheBuffer.capacity() < opSize) { cacheBuffer = ByteBuffer.allocate(opSize); } cacheBuffer.clear(); cacheBuffer.limit(opSize); bytesRead = Channels.readFromFileChannel(channel, position, cacheBuffer); if (bytesRead < 0) { throw new EOFException("tried to read past EOF. opSize [" + opSize + "] position [" + position + "] length [" + length + "]"); } cacheBuffer.flip(); position += opSize; lastOperationRead = TranslogStreams.readTranslogOperation(new BytesStreamInput(cacheBuffer.array(), 0, opSize, true)); return true; } catch (Exception e) { return false; } }	unrelated but i think we should fix java public void close() throws elasticsearchexception { raf.decreaserefcount(true); } this needs to be protected from double decrementing if closed twice!
public FsChannelSnapshot snapshot() throws TranslogException { rwl.writeLock().lock(); try { if (!raf.increaseRefCount()) { return null; } return new FsChannelSnapshot(this.id, raf, lastWrittenPosition, operationCounter); } catch (Exception e) { throw new TranslogException(shardId, "Failed to snapshot", e); } finally { rwl.writeLock().unlock(); } }	can we acquire the ref outside of the lock?
protected Translog create() { return new FsTranslog(shardId, ImmutableSettings.settingsBuilder() .put("index.translog.fs.type", FsTranslogFile.Type.BUFFERED.name()) .put("index.translog.fs.buffer_size", 10 + randomInt(128 * 1024)) .build(), new File("data/fs-buf-translog") ); }	cool stuff can we create a new issue to randomize the translog impl all the time?
public static Version getPreviousVersion(Version version) { for (int i = RELEASED_VERSIONS.size() - 1; i >= 0; i--) { Version v = RELEASED_VERSIONS.get(i); if (v.before(version)) { return v; } } throw new IllegalArgumentException("couldn't find any released versions before [" + version + "]"); } /** * Get the released version before {@link Version#CURRENT}	this change is required for getpreviousversion(current) to work. we do that fairly frequently.
public static Version getPreviousVersion(Version version) { for (int i = RELEASED_VERSIONS.size() - 1; i >= 0; i--) { Version v = RELEASED_VERSIONS.get(i); if (v.before(version)) { return v; } } throw new IllegalArgumentException("couldn't find any released versions before [" + version + "]"); } /** * Get the released version before {@link Version#CURRENT}	i did this because i was upset that it was so inefficient.
synchronized void shutDown() { this.state.set(WatcherState.STOPPING); shutDown = true; clearAllocationIds(); watcherService.shutDown(null); this.state.set(WatcherState.STOPPED); }	personal preference: can you not make it nullable but pass an empty listener instead?
*/ @Override public void clusterChanged(ClusterChangedEvent event) { if (event.state().blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK) || shutDown) { clearAllocationIds(); // wait until the gateway has recovered from disk, otherwise we think may not have .watches and // a .triggered_watches index, but they may not have been restored from the cluster state on disk return; } if (Strings.isNullOrEmpty(event.state().nodes().getMasterNodeId())) { pauseExecution("no master node"); return; } if (event.state().getBlocks().hasGlobalBlockWithLevel(ClusterBlockLevel.WRITE)) { pauseExecution("write level cluster block"); return; } boolean isWatcherStoppedManually = isWatcherStoppedManually(event.state()); // if this is not a data node, we need to start it ourselves possibly if (event.state().nodes().getLocalNode().isDataNode() == false && isWatcherStoppedManually == false && (this.state.get() == WatcherState.STOPPED || this.state.get() == WatcherState.STOPPING)) { this.state.set(WatcherState.STARTING); watcherService.start(event.state(), () -> this.state.set(WatcherState.STARTED)); return; } if (isWatcherStoppedManually) { if (this.state.get() == WatcherState.STARTED) { clearAllocationIds(); this.state.set(WatcherState.STOPPING); //waiting to set state to stopped until after all currently running watches are finished watcherService.stop("watcher manually marked to shutdown by cluster state update", a -> { //ensure that Watcher wasn't restarted between stopping and now. if (state.get() == WatcherState.STOPPING) { state.set(WatcherState.STOPPED); } }); } return; } DiscoveryNode localNode = event.state().nodes().getLocalNode(); RoutingNode routingNode = event.state().getRoutingNodes().node(localNode.getId()); if (routingNode == null) { pauseExecution("routing node in cluster state undefined. network issue?"); return; } IndexMetaData watcherIndexMetaData = WatchStoreUtils.getConcreteIndex(Watch.INDEX, event.state().metaData()); if (watcherIndexMetaData == null) { pauseExecution("no watcher index found"); return; } String watchIndex = watcherIndexMetaData.getIndex().getName(); List<ShardRouting> localShards = routingNode.shardsWithState(watchIndex, RELOCATING, STARTED); // no local shards, empty out watcher and dont waste resources! if (localShards.isEmpty()) { pauseExecution("no local watcher shards found"); return; } // also check if non local shards have changed, as loosing a shard on a // remote node or adding a replica on a remote node needs to trigger a reload too Set<ShardId> localShardIds = localShards.stream().map(ShardRouting::shardId).collect(Collectors.toSet()); List<ShardRouting> allShards = event.state().routingTable().index(watchIndex).shardsWithState(STARTED); allShards.addAll(event.state().routingTable().index(watchIndex).shardsWithState(RELOCATING)); List<ShardRouting> localAffectedShardRoutings = allShards.stream() .filter(shardRouting -> localShardIds.contains(shardRouting.shardId())) // shardrouting is not comparable, so we need some order mechanism .sorted(Comparator.comparing(ShardRouting::hashCode)) .collect(Collectors.toList()); if (previousShardRoutings.get().equals(localAffectedShardRoutings) == false) { if (watcherService.validate(event.state())) { previousShardRoutings.set(localAffectedShardRoutings); if (state.get() == WatcherState.STARTED) { watcherService.reload(event.state(), "new local watcher shard allocation ids"); } else if (state.get() == WatcherState.STOPPED || state.get() == WatcherState.STOPPING) { this.state.set(WatcherState.STARTING); watcherService.start(event.state(), () -> this.state.set(WatcherState.STARTED)); } } else { clearAllocationIds(); this.state.set(WatcherState.STOPPED); } } }	this looks racy to me. as it is an atomicreference you can set it atomically and may want to log something if the state was not set properly?
public void stop(String reason, Consumer<Void> stoppedListener) { assert stoppedListener != null; logger.info("stopping watch service, reason [{}]", reason); executionService.pause(stoppedListener); triggerService.pauseExecution(); } /** * shuts down the trigger service as well to make sure there are no lingering threads * also no need to check anything, as this is final, we just can go to status STOPPED * * @param stopListener The listener that will set Watcher state to: {@link WatcherState#STOPPED}, may be {@code null} assuming the * {@link WatcherState#STOPPED}	consistent naming: stoplistener vs. stoppedlistener
public void remove(String id) { lock.lock(); try { currentExecutions.remove(id); if (currentExecutions.isEmpty()) { empty.signal(); } } finally { lock.unlock(); } } /** * Calling this method makes the class stop accepting new executions and throws and exception instead. * In addition it waits for a certain amount of time for current executions to finish before returning * * @param maxStopTimeout The maximum wait time to wait to current executions to finish * @param stoppedListener The listener that will set Watcher state to: {@link WatcherState#STOPPED}, may be {@code null} assuming the * {@link WatcherState#STOPPED}	you could pass a runnable here, which is still a functional interface but ignores the return value if it is void anyway? same applies for the consumers below, won't repeat.
public void testOutgoingThrottlesAllocation() { TestGatewayAllocator gatewayAllocator = new TestGatewayAllocator(); AllocationService strategy = createAllocationService(Settings.builder() .put("cluster.routing.allocation.node_concurrent_outgoing_recoveries", 1) .build(), gatewayAllocator); logger.info("Building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(2)) .build(); ClusterState clusterState = createRecoveryStateAndInitalizeAllocations(metaData, gatewayAllocator); logger.info("with one node, do reroute, only 1 should initialize"); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(2)); logger.info("start initializing"); clusterState = strategy.applyStartedShards(clusterState, clusterState.routingTable().shardsWithState(INITIALIZING)); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(2)); logger.info("start one more node, first non-primary should start being allocated"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node2"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(1)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("start initializing non-primary"); clusterState = strategy.applyStartedShards(clusterState, clusterState.routingTable().shardsWithState(INITIALIZING)); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(1)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 0); logger.info("start one more node, initializing second non-primary"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node3"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(0)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("start one more node"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node4"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("move started non-primary to new node"); AllocationService.CommandsResult commandsResult = strategy.reroute(clusterState, new AllocationCommands( new MoveAllocationCommand("test", 0, "node2", "node4")), true, false); assertEquals(commandsResult.explanations().explanations().size(), 1); assertEquals(commandsResult.explanations().explanations().get(0).decisions().type(), Decision.Type.THROTTLE); for(Decision decision : commandsResult.explanations().explanations().get(0).decisions().getDecisions()){ if(decision.label().equals(ThrottlingAllocationDecider.NAME)){ assertEquals("reached the limit of outgoing shard recoveries [1] on the node [node1] which holds the primary, " + "cluster setting [cluster.routing.allocation.node_concurrent_outgoing_recoveries=1] (can also be set via [cluster.routing.allocation.node_concurrent_recoveries])",decision.getExplanation()); } } // even though it is throttled, move command still forces allocation clusterState = commandsResult.getClusterState(); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(RELOCATING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(0)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 2); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node2"), 0); }	nit: please add spaces after the for and before the {.
public void testOutgoingThrottlesAllocation() { TestGatewayAllocator gatewayAllocator = new TestGatewayAllocator(); AllocationService strategy = createAllocationService(Settings.builder() .put("cluster.routing.allocation.node_concurrent_outgoing_recoveries", 1) .build(), gatewayAllocator); logger.info("Building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(2)) .build(); ClusterState clusterState = createRecoveryStateAndInitalizeAllocations(metaData, gatewayAllocator); logger.info("with one node, do reroute, only 1 should initialize"); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(2)); logger.info("start initializing"); clusterState = strategy.applyStartedShards(clusterState, clusterState.routingTable().shardsWithState(INITIALIZING)); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(2)); logger.info("start one more node, first non-primary should start being allocated"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node2"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(1)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("start initializing non-primary"); clusterState = strategy.applyStartedShards(clusterState, clusterState.routingTable().shardsWithState(INITIALIZING)); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(1)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 0); logger.info("start one more node, initializing second non-primary"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node3"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(0)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("start one more node"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node4"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("move started non-primary to new node"); AllocationService.CommandsResult commandsResult = strategy.reroute(clusterState, new AllocationCommands( new MoveAllocationCommand("test", 0, "node2", "node4")), true, false); assertEquals(commandsResult.explanations().explanations().size(), 1); assertEquals(commandsResult.explanations().explanations().get(0).decisions().type(), Decision.Type.THROTTLE); for(Decision decision : commandsResult.explanations().explanations().get(0).decisions().getDecisions()){ if(decision.label().equals(ThrottlingAllocationDecider.NAME)){ assertEquals("reached the limit of outgoing shard recoveries [1] on the node [node1] which holds the primary, " + "cluster setting [cluster.routing.allocation.node_concurrent_outgoing_recoveries=1] (can also be set via [cluster.routing.allocation.node_concurrent_recoveries])",decision.getExplanation()); } } // even though it is throttled, move command still forces allocation clusterState = commandsResult.getClusterState(); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(RELOCATING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(0)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 2); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node2"), 0); }	nit: please add spaces after the if and before the {.
public void testOutgoingThrottlesAllocation() { TestGatewayAllocator gatewayAllocator = new TestGatewayAllocator(); AllocationService strategy = createAllocationService(Settings.builder() .put("cluster.routing.allocation.node_concurrent_outgoing_recoveries", 1) .build(), gatewayAllocator); logger.info("Building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(2)) .build(); ClusterState clusterState = createRecoveryStateAndInitalizeAllocations(metaData, gatewayAllocator); logger.info("with one node, do reroute, only 1 should initialize"); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(2)); logger.info("start initializing"); clusterState = strategy.applyStartedShards(clusterState, clusterState.routingTable().shardsWithState(INITIALIZING)); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(2)); logger.info("start one more node, first non-primary should start being allocated"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node2"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(1)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("start initializing non-primary"); clusterState = strategy.applyStartedShards(clusterState, clusterState.routingTable().shardsWithState(INITIALIZING)); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(0)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(1)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 0); logger.info("start one more node, initializing second non-primary"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node3"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(0)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("start one more node"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).add(newNode("node4"))).build(); clusterState = strategy.reroute(clusterState, "reroute"); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 1); logger.info("move started non-primary to new node"); AllocationService.CommandsResult commandsResult = strategy.reroute(clusterState, new AllocationCommands( new MoveAllocationCommand("test", 0, "node2", "node4")), true, false); assertEquals(commandsResult.explanations().explanations().size(), 1); assertEquals(commandsResult.explanations().explanations().get(0).decisions().type(), Decision.Type.THROTTLE); for(Decision decision : commandsResult.explanations().explanations().get(0).decisions().getDecisions()){ if(decision.label().equals(ThrottlingAllocationDecider.NAME)){ assertEquals("reached the limit of outgoing shard recoveries [1] on the node [node1] which holds the primary, " + "cluster setting [cluster.routing.allocation.node_concurrent_outgoing_recoveries=1] (can also be set via [cluster.routing.allocation.node_concurrent_recoveries])",decision.getExplanation()); } } // even though it is throttled, move command still forces allocation clusterState = commandsResult.getClusterState(); assertThat(clusterState.routingTable().shardsWithState(STARTED).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(RELOCATING).size(), equalTo(1)); assertThat(clusterState.routingTable().shardsWithState(INITIALIZING).size(), equalTo(2)); assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(0)); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node1"), 2); assertEquals(clusterState.getRoutingNodes().getOutgoingRecoveries("node2"), 0); }	this line is longer than 120 characters so ci will reject it.
Predicate<AuditEventMetaInfo> ignorePredicate() { return eventInfo -> eventInfo.principal != null && ignorePrincipalsPredicate.test(eventInfo.principal) && eventInfo.realm != null && ignoreRealmsPredicate.test(eventInfo.realm) && eventInfo.action != null && (ignoreIndexPrivilegesPredicateTest(eventInfo.action) && ignoreClusterPrivilegesPredicateTest(eventInfo.action)) && eventInfo.roles.get().allMatch(role -> role != null && ignoreRolesPredicate.test(role)) && eventInfo.indices.get().allMatch(index -> index != null && ignoreIndicesPredicate.test(index)); }	i think the question @albertzaharovits was asking about manage_own_api_key etc depends on the decision here. due to the history of this change, we're treating this as though it is a predicate on action name only. however, it doesn't have to be that way. all the audit events that have cluster actions also have a transportrequest and an authentication, so we _could_ decide that this predicate actually operates over the action,request,authentication triplet and uses the existing clusterpermission.check() there are consequences of that choice - it has potential performance implications, and perhaps we would prefer to keep audit filtering simple and say that it can only filter on the action name, but i think we should evaluate the options & make an explicit choice here: 1. _either_ we filter on action name only, and it is an error to specify a privilege that needs to inspect the request or authentication (this option is less useful, but more efficient) 2. _or_ we explicitly filter on privilege and pass through all the parameters that privileges need. my leaning is towards (2), but albert probably has a better sense of how this would affect auditing performance.
private static IllegalArgumentException illegalArgument(String message) { assert false : message; return new IllegalArgumentException(message); }	i don't think we need this change anymore, but if we do, then we should move this method to somewhere more appropriate rather than just make it public here.
public void testNoAutoReleaseOfIndicesOnReplacementNodes() { AtomicReference<Set<String>> indicesToMarkReadOnly = new AtomicReference<>(); AtomicReference<Set<String>> indicesToRelease = new AtomicReference<>(); AllocationService allocation = createAllocationService(Settings.builder() .put("cluster.routing.allocation.node_concurrent_recoveries", 10).build()); Metadata metadata = Metadata.builder() .put(IndexMetadata.builder("test_1").settings(settings(Version.CURRENT)).numberOfShards(2).numberOfReplicas(1)) .put(IndexMetadata.builder("test_2").settings(settings(Version.CURRENT)).numberOfShards(2).numberOfReplicas(1)) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsNew(metadata.index("test_1")) .addAsNew(metadata.index("test_2")) .build(); final ClusterState clusterState = applyStartedShardsUntilNoChange( ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)) .metadata(metadata).routingTable(routingTable) .nodes(DiscoveryNodes.builder().add(newNormalNode("node1")).add(newNormalNode("node2"))).build(), allocation); assertThat(clusterState.getRoutingTable().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(8)); final ImmutableOpenMap.Builder<ClusterInfo.NodeAndPath, ClusterInfo.ReservedSpace> reservedSpacesBuilder = ImmutableOpenMap.builder(); final int reservedSpaceNode1 = between(0, 10); reservedSpacesBuilder.put(new ClusterInfo.NodeAndPath("node1", "/foo/bar"), new ClusterInfo.ReservedSpace.Builder().add(new ShardId("", "", 0), reservedSpaceNode1).build()); final int reservedSpaceNode2 = between(0, 10); reservedSpacesBuilder.put(new ClusterInfo.NodeAndPath("node2", "/foo/bar"), new ClusterInfo.ReservedSpace.Builder().add(new ShardId("", "", 0), reservedSpaceNode2).build()); ImmutableOpenMap<ClusterInfo.NodeAndPath, ClusterInfo.ReservedSpace> reservedSpaces = reservedSpacesBuilder.build(); DiskThresholdMonitor monitor = new DiskThresholdMonitor(Settings.EMPTY, () -> clusterState, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), null, () -> 0L, (reason, priority, listener) -> { assertNotNull(listener); assertThat(priority, equalTo(Priority.HIGH)); listener.onResponse(clusterState); }) { @Override protected void updateIndicesReadOnly(Set<String> indicesToUpdate, ActionListener<Void> listener, boolean readOnly) { if (readOnly) { assertTrue(indicesToMarkReadOnly.compareAndSet(null, indicesToUpdate)); } else { assertTrue(indicesToRelease.compareAndSet(null, indicesToUpdate)); } listener.onResponse(null); } }; indicesToMarkReadOnly.set(null); indicesToRelease.set(null); ImmutableOpenMap.Builder<String, DiskUsage> builder = ImmutableOpenMap.builder(); builder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(0, 4))); builder.put("node2", new DiskUsage("node2", "node2", "/foo/bar", 100, between(0, 4))); monitor.onNewInfo(clusterInfo(builder.build(), reservedSpaces)); assertEquals(new HashSet<>(Arrays.asList("test_1", "test_2")), indicesToMarkReadOnly.get()); assertNull(indicesToRelease.get()); // Reserved space is ignored when applying block indicesToMarkReadOnly.set(null); indicesToRelease.set(null); builder = ImmutableOpenMap.builder(); builder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(5, 90))); builder.put("node2", new DiskUsage("node2", "node2", "/foo/bar", 100, between(5, 90))); monitor.onNewInfo(clusterInfo(builder.build(), reservedSpaces)); assertNull(indicesToMarkReadOnly.get()); assertNull(indicesToRelease.get()); // Change cluster state so that "test_2" index is blocked (read only) IndexMetadata indexMetadata = IndexMetadata.builder(clusterState.metadata().index("test_2")).settings(Settings.builder() .put(clusterState.metadata() .index("test_2").getSettings()) .put(IndexMetadata.INDEX_BLOCKS_READ_ONLY_ALLOW_DELETE_SETTING.getKey(), true)).build(); final String sourceNode; final String targetNode; if (randomBoolean()) { sourceNode = "node1"; targetNode = "node2"; } else { sourceNode = "node2"; targetNode = "node1"; } ClusterState clusterStateWithBlocks = ClusterState.builder(clusterState) .metadata(Metadata.builder(clusterState.metadata()) .put(indexMetadata, true) .putCustom(NodesShutdownMetadata.TYPE, new NodesShutdownMetadata(Collections.singletonMap(sourceNode, SingleNodeShutdownMetadata.builder() .setNodeId(sourceNode) .setReason("testing") .setType(SingleNodeShutdownMetadata.Type.REPLACE) .setTargetNodeName(targetNode) .setStartedAtMillis(randomNonNegativeLong()) .build()))) .build()) .blocks(ClusterBlocks.builder().addBlocks(indexMetadata).build()) .build(); assertTrue(clusterStateWithBlocks.blocks().indexBlocked(ClusterBlockLevel.WRITE, "test_2")); monitor = new DiskThresholdMonitor(Settings.EMPTY, () -> clusterStateWithBlocks, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), null, () -> 0L, (reason, priority, listener) -> { assertNotNull(listener); assertThat(priority, equalTo(Priority.HIGH)); listener.onResponse(clusterStateWithBlocks); }) { @Override protected void updateIndicesReadOnly(Set<String> indicesToUpdate, ActionListener<Void> listener, boolean readOnly) { if (readOnly) { assertTrue(indicesToMarkReadOnly.compareAndSet(null, indicesToUpdate)); } else { assertTrue(indicesToRelease.compareAndSet(null, indicesToUpdate)); } listener.onResponse(null); } }; // When free disk on any of node1 or node2 goes below 5% flood watermark, then apply index block on indices not having the block indicesToMarkReadOnly.set(null); indicesToRelease.set(null); builder = ImmutableOpenMap.builder(); builder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(0, 100))); builder.put("node2", new DiskUsage("node2", "node2", "/foo/bar", 100, between(0, 4))); monitor.onNewInfo(clusterInfo(builder.build(), reservedSpaces)); assertThat(indicesToMarkReadOnly.get(), contains("test_1")); assertNull(indicesToRelease.get()); // While the REPLACE is ongoing the lock will not be removed from the index indicesToMarkReadOnly.set(null); indicesToRelease.set(null); builder = ImmutableOpenMap.builder(); builder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(10, 100))); builder.put("node2", new DiskUsage("node2", "node2", "/foo/bar", 100, between(10, 100))); monitor.onNewInfo(clusterInfo(builder.build(), reservedSpaces)); assertNull(indicesToMarkReadOnly.get()); assertNull(indicesToRelease.get()); ClusterState clusterStateNoShutdown = ClusterState.builder(clusterState) .metadata(Metadata.builder(clusterState.metadata()) .put(indexMetadata, true) .removeCustom(NodesShutdownMetadata.TYPE) .build()) .blocks(ClusterBlocks.builder().addBlocks(indexMetadata).build()) .build(); assertTrue(clusterStateNoShutdown.blocks().indexBlocked(ClusterBlockLevel.WRITE, "test_2")); monitor = new DiskThresholdMonitor(Settings.EMPTY, () -> clusterStateNoShutdown, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), null, () -> 0L, (reason, priority, listener) -> { assertNotNull(listener); assertThat(priority, equalTo(Priority.HIGH)); listener.onResponse(clusterStateNoShutdown); }) { @Override protected void updateIndicesReadOnly(Set<String> indicesToUpdate, ActionListener<Void> listener, boolean readOnly) { if (readOnly) { assertTrue(indicesToMarkReadOnly.compareAndSet(null, indicesToUpdate)); } else { assertTrue(indicesToRelease.compareAndSet(null, indicesToUpdate)); } listener.onResponse(null); } }; // Now that the REPLACE is gone, auto-releasing can occur for the index indicesToMarkReadOnly.set(null); indicesToRelease.set(null); builder = ImmutableOpenMap.builder(); builder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(10, 100))); builder.put("node2", new DiskUsage("node2", "node2", "/foo/bar", 100, between(10, 100))); monitor.onNewInfo(clusterInfo(builder.build(), reservedSpaces)); assertNull(indicesToMarkReadOnly.get()); assertThat(indicesToRelease.get(), contains("test_2")); }	i think it would be clearer to either refer the clusterstate to use via an atomic reference or make a specific sub-class with clusterstate, indicestomarkreadonly and indicestorelease fields. as it stands now we reuse the latter two but use a new cluster state field - and the monitor subclass code is repeated 3 times.
private static DiscoveryNode newNormalNode(String nodeId) { Set<DiscoveryNodeRole> randomRoles = new HashSet<>(randomSubsetOf(DiscoveryNodeRole.roles())); Set<DiscoveryNodeRole> roles = Sets.union(randomRoles, Set.of(randomFrom(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.DATA_CONTENT_NODE_ROLE, DiscoveryNodeRole.DATA_HOT_NODE_ROLE, DiscoveryNodeRole.DATA_WARM_NODE_ROLE, DiscoveryNodeRole.DATA_COLD_NODE_ROLE))); return newNode(nodeId, nodeId, roles); }	i think this means that we no longer have any nodes with duplicate names. i wonder if we need to add a test checking that the monitor survives duplicate names (which i am sure it does now, but is a property we want to continue supporting).
void runRandomly() { assert disconnectedNodes.isEmpty() : "may reconnect disconnected nodes, probably unexpected: " + disconnectedNodes; assert blackholedNodes.isEmpty() : "may reconnect blackholed nodes, probably unexpected: " + blackholedNodes; final int randomSteps = scaledRandomIntBetween(10, 10000); logger.info("--> start of safety phase of at least [{}] steps", randomSteps); deterministicTaskQueue.setExecutionDelayVariabilityMillis(EXTREME_DELAY_VARIABILITY); int step = 0; long finishTime = -1; while (finishTime == -1 || deterministicTaskQueue.getCurrentTimeMillis() <= finishTime) { step++; if (randomSteps <= step && finishTime == -1) { finishTime = deterministicTaskQueue.getLatestDeferredExecutionTime(); deterministicTaskQueue.setExecutionDelayVariabilityMillis(DEFAULT_DELAY_VARIABILITY); logger.debug("----> [runRandomly {}] reducing delay variability and running until [{}]", step, finishTime); } try { if (rarely()) { final ClusterNode clusterNode = getAnyNodePreferringLeaders(); final int newValue = randomInt(); logger.debug("----> [runRandomly {}] proposing new value [{}] to [{}]", step, newValue, clusterNode.getId()); clusterNode.submitValue(newValue); } else if (rarely()) { final ClusterNode clusterNode = getAnyNode(); logger.debug("----> [runRandomly {}] forcing {} to become candidate", step, clusterNode.getId()); synchronized (clusterNode.coordinator.mutex) { clusterNode.coordinator.becomeCandidate("runRandomly"); } } else if (rarely()) { final ClusterNode clusterNode = getAnyNode(); final String id = clusterNode.getId(); disconnectedNodes.remove(id); blackholedNodes.remove(id); switch (randomInt(2)) { case 0: logger.debug("----> [runRandomly {}] connecting {}", step, id); break; case 1: logger.debug("----> [runRandomly {}] disconnecting {}", step, id); disconnectedNodes.add(id); break; case 2: logger.debug("----> [runRandomly {}] blackholing {}", step, id); blackholedNodes.add(id); break; } } else { if (deterministicTaskQueue.hasDeferredTasks() && randomBoolean()) { deterministicTaskQueue.advanceTime(); } else if (deterministicTaskQueue.hasRunnableTasks()) { deterministicTaskQueue.runRandomTask(); } } // TODO other random steps: // - reboot a node // - abdicate leadership // - bootstrap } catch (CoordinationStateRejectedException ignored) { // This is ok: it just means a message couldn't currently be handled. } assertConsistentStates(); } disconnectedNodes.clear(); blackholedNodes.clear(); }	maybe use hamcrest matchers? assertthat(disconnectednodes, empty())?
@Override public void handleResponse(ClusterStateResponse response) { clusterStateResponses.put(nodeToPing, response); latch.countDown(); closeConnection(); }	maybe we should unify the latch.countdown() and closeconnection() into a single method called "ondone" on the abstractrunnable that everyone calls? this it's less trappy and people wouldn't forget to do one but not the other.
@Override public synchronized boolean maybeTriggerAsyncJob(long now) { if (transformConfig == null) { CountDownLatch latch = new CountDownLatch(1); transformsConfigManager.getTransformConfiguration(transformId, new LatchedActionListener<>(ActionListener.wrap(config -> { transformConfig = config; }, e -> { throw new RuntimeException( DataFrameMessages.getMessage(DataFrameMessages.FAILED_TO_LOAD_TRANSFORM_CONFIGURATION, transformId), e); }), latch)); try { latch.await(LOAD_TRANSFORM_TIMEOUT_IN_SECONDS, TimeUnit.SECONDS); } catch (InterruptedException e) { throw new RuntimeException( DataFrameMessages.getMessage(DataFrameMessages.FAILED_TO_LOAD_TRANSFORM_CONFIGURATION, transformId), e); } } // todo: set job into failed state if (transformConfig.isValid() == false) { throw new RuntimeException( DataFrameMessages.getMessage(DataFrameMessages.DATA_FRAME_TRANSFORM_CONFIGURATION_INVALID, transformId)); } if (fieldMappings == null) { CountDownLatch latch = new CountDownLatch(1); SchemaUtil.getDestinationFieldMappings(client, transformConfig.getDestination(), new LatchedActionListener<>( ActionListener.wrap( destinationMappings -> fieldMappings = destinationMappings, e -> { throw new RuntimeException( DataFrameMessages.getMessage(DataFrameMessages.DATA_FRAME_UNABLE_TO_GATHER_FIELD_MAPPINGS, transformConfig.getDestination()), e); }), latch)); try { latch.await(LOAD_TRANSFORM_TIMEOUT_IN_SECONDS, TimeUnit.SECONDS); } catch (InterruptedException e) { throw new RuntimeException( DataFrameMessages.getMessage(DataFrameMessages.DATA_FRAME_UNABLE_TO_GATHER_FIELD_MAPPINGS, transformConfig.getDestination()), e); } } return super.maybeTriggerAsyncJob(now); }	til: i thought fieldmappings would have to be volatile for the assignment to be visible to the awaiting thread but latch.await is a synchronisation point. https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html#memoryvisibility. i presume we are blocking here because super.maybetriggerasyncjob(now); must execute on the calling thread.
public void testSliceEquals() { int length = randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5)); ByteArray ba1 = bigarrays.newByteArray(length, false); BytesReference pbr = new PagedBytesReference(bigarrays, ba1, length); // test equality of slices int sliceFrom = randomIntBetween(0, pbr.length()); int sliceLength = randomIntBetween(pbr.length() - sliceFrom, pbr.length() - sliceFrom); BytesReference slice1 = pbr.slice(sliceFrom, sliceLength); BytesReference slice2 = pbr.slice(sliceFrom, sliceLength); assertArrayEquals(BytesReference.toBytes(slice1), BytesReference.toBytes(slice2)); // test a slice with same offset but different length, // unless randomized testing gave us a 0-length slice. if (sliceLength > 0) { BytesReference slice3 = pbr.slice(sliceFrom, sliceLength / 2); assertFalse(Arrays.equals(BytesReference.toBytes(slice1), BytesReference.toBytes(slice3))); } }	can we get several iterations to make sure that both the single page and multiple pages cases are tested every time?
public void expandIds(String idExpression, boolean allowNoResources, @Nullable PageParams pageParams, ActionListener<Tuple<Long, Set<String>>> idsListener) { String[] tokens = Strings.tokenizeToStringArray(idExpression, ","); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder() .sort(SortBuilders.fieldSort(TrainedModelConfig.MODEL_ID.getPreferredName()) // If there are no resources, there might be no mapping for the id field. // This makes sure we don't get an error if that happens. .unmappedType("long")) .query(buildQueryIdExpressionQuery(tokens, TrainedModelConfig.MODEL_ID.getPreferredName())); if (pageParams != null) { sourceBuilder.from(pageParams.getFrom()).size(pageParams.getSize()); } sourceBuilder.trackTotalHits(true) // we only care about the item id's .fetchSource(TrainedModelConfig.MODEL_ID.getPreferredName(), null); IndicesOptions indicesOptions = SearchRequest.DEFAULT_INDICES_OPTIONS; SearchRequest searchRequest = new SearchRequest(InferenceIndexConstants.INDEX_PATTERN) .indicesOptions(IndicesOptions.fromOptions(true, indicesOptions.allowNoIndices(), indicesOptions.expandWildcardsOpen(), indicesOptions.expandWildcardsClosed(), indicesOptions)) .source(sourceBuilder); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest, ActionListener.<SearchResponse>wrap( response -> { Set<String> foundResourceIds = new LinkedHashSet<>(); long totalHitCount = response.getHits().getTotalHits().value; for (SearchHit hit : response.getHits().getHits()) { Map<String, Object> docSource = hit.getSourceAsMap(); if (docSource == null) { continue; } Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName()); if (idValue instanceof String) { foundResourceIds.add(idValue.toString()); } } ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources); requiredMatches.filterMatchedIds(foundResourceIds); if (requiredMatches.hasUnmatchedIds()) { idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString())); } else { idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds)); } }, idsListener::onFailure ), client::search); }	we are expanding the ids in other parts of our codebase. shouldn't this logic (splitting by comma, wildcards) be reused?
* @throws IOException if the cache file failed to be fsync * @throws java.nio.file.NoSuchFileException if the cache file does not exist */ public SortedSet<Tuple<Long, Long>> fsync() throws IOException { if (refCounter.tryIncRef()) { try { if (needsFsync.compareAndSet(true, false)) { boolean success = false; try { // Capture the completed ranges before fsyncing; ranges that are completed after this point won't be considered as // persisted on disk by the caller of this method, even if they are fully written to disk at the time the file // fsync is effectively executed final SortedSet<Tuple<Long, Long>> completedRanges = tracker.getCompletedRanges(); assert completedRanges != null; assert completedRanges.isEmpty() == false; IOUtils.fsync(file, false, false); success = true; return completedRanges; } finally { if (success == false) { markAsNeedsFSync(); } } } } finally { refCounter.decRef(); } } else { assert evicted.get(); } return Collections.emptySortedSet(); }	this will add it back on the queue, so we continually try to fsync a bad file. i saw the comments armin made in other places about this, we can tackle this in the same follow-up.
protected CacheService randomCacheService() { final Settings.Builder cacheSettings = Settings.builder(); if (randomBoolean()) { cacheSettings.put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), randomCacheSize()); } if (randomBoolean()) { cacheSettings.put(CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(), randomCacheRangeSize()); } if (randomBoolean()) { cacheSettings.put( CacheService.SNAPSHOT_CACHE_SYNC_INTERVAL_SETTING.getKey(), TimeValue.timeValueSeconds(randomLongBetween(MIN_SNAPSHOT_CACHE_SYNC_INTERVAL.getSeconds(), Long.MAX_VALUE)) ); } return new CacheService(cacheSettings.build(), clusterService, threadPool, AbstractSearchableSnapshotsTestCase::noOpCacheCleaner); } /** * @return a new {@link CacheService}	i think we should bias towards smaller values, perhaps using scaledrandomint?
public void testCacheSynchronization() throws Exception { final int numShards = randomIntBetween(1, 3); final Index index = new Index(randomAlphaOfLength(5).toLowerCase(Locale.ROOT), UUIDs.randomBase64UUID(random())); final SnapshotId snapshotId = new SnapshotId("_snapshot_name", UUIDs.randomBase64UUID(random())); final IndexId indexId = new IndexId("_index_name", UUIDs.randomBase64UUID(random())); logger.debug("--> creating shard cache directories on disk"); final Path[] shardsCacheDirs = new Path[numShards]; for (int i = 0; i < numShards; i++) { final Path shardDataPath = randomFrom(nodeEnvironment.availableShardPaths(new ShardId(index, i))); assertFalse(Files.exists(shardDataPath)); logger.debug("--> creating directories [{}] for shard [{}]", shardDataPath.toAbsolutePath(), i); shardsCacheDirs[i] = Files.createDirectories(CacheService.resolveSnapshotCache(shardDataPath).resolve(snapshotId.getUUID())); } try (CacheService cacheService = defaultCacheService()) { cacheService.start(); logger.debug("--> setting large cache sync interval (explicit cache synchronization calls in test)"); cacheService.setCacheSyncInterval(TimeValue.timeValueMillis(Long.MAX_VALUE)); // Keep a count of the number of writes for every cache file existing in the cache final Map<CacheKey, Tuple<CacheFile, Integer>> previous = new HashMap<>(); for (int iteration = 0; iteration < between(1, 10); iteration++) { final Map<CacheKey, Tuple<CacheFile, Integer>> updates = new HashMap<>(); logger.trace("--> more random reads/writes from existing cache files"); for (Map.Entry<CacheKey, Tuple<CacheFile, Integer>> cacheEntry : randomSubsetOf(previous.entrySet())) { final CacheKey cacheKey = cacheEntry.getKey(); final CacheFile cacheFile = cacheEntry.getValue().v1(); final CacheFile.EvictionListener listener = evictedCacheFile -> {}; cacheFile.acquire(listener); final SortedSet<Tuple<Long, Long>> newCacheRanges = randomPopulateAndReads(cacheFile); assertThat(cacheService.isCacheFileToSync(cacheFile), is(newCacheRanges.isEmpty() == false)); if (newCacheRanges.isEmpty() == false) { final int numberOfWrites = cacheEntry.getValue().v2() + 1; updates.put(cacheKey, Tuple.tuple(cacheFile, numberOfWrites)); } cacheFile.release(listener); } logger.trace("--> creating new cache files and randomly read/write them"); for (int i = 0; i < between(1, 25); i++) { final ShardId shardId = new ShardId(index, randomIntBetween(0, numShards - 1)); final String fileName = String.format(Locale.ROOT, "file_%d_%d", iteration, i); final CacheKey cacheKey = new CacheKey(snapshotId, indexId, shardId, fileName); final CacheFile cacheFile = cacheService.get(cacheKey, randomIntBetween(1, 10_000), shardsCacheDirs[shardId.id()]); final CacheFile.EvictionListener listener = evictedCacheFile -> {}; cacheFile.acquire(listener); final SortedSet<Tuple<Long, Long>> newRanges = randomPopulateAndReads(cacheFile); assertThat(cacheService.isCacheFileToSync(cacheFile), is(newRanges.isEmpty() == false)); updates.put(cacheKey, Tuple.tuple(cacheFile, newRanges.isEmpty() ? 0 : 1)); cacheFile.release(listener); } logger.trace("--> evicting random cache files"); for (CacheKey evictedCacheKey : randomSubsetOf(Sets.union(previous.keySet(), updates.keySet()))) { cacheService.removeFromCache(evictedCacheKey::equals); previous.remove(evictedCacheKey); updates.remove(evictedCacheKey); } logger.trace("--> capturing expected number of fsyncs per cache directory before synchronization"); final Map<Path, Integer> cacheDirFSyncs = new HashMap<>(); for (int i = 0; i < shardsCacheDirs.length; i++) { final Path shardCacheDir = shardsCacheDirs[i]; final ShardId shardId = new ShardId(index, i); final Integer numberOfFSyncs = fileSystemProvider.getNumberOfFSyncs(shardCacheDir); if (updates.entrySet() .stream() .filter(update -> update.getValue().v2() != null) .filter(update -> update.getValue().v2() > 0) .anyMatch(update -> update.getKey().getShardId().equals(shardId))) { cacheDirFSyncs.put(shardCacheDir, numberOfFSyncs == null ? 1 : numberOfFSyncs + 1); } else { cacheDirFSyncs.put(shardCacheDir, numberOfFSyncs); } } logger.debug("--> synchronizing cache files [#{}]", iteration); cacheService.synchronizeCache(); logger.trace("--> verifying cache synchronization correctness"); updates.forEach( (cacheKey, cacheFileAndExpectedNumberOfFSyncs) -> assertFalse( cacheService.isCacheFileToSync(cacheFileAndExpectedNumberOfFSyncs.v1()) ) ); cacheDirFSyncs.forEach( (dir, expectedNumberOfFSyncs) -> assertThat(fileSystemProvider.getNumberOfFSyncs(dir), equalTo(expectedNumberOfFSyncs)) ); previous.putAll(updates); previous.forEach((key, cacheFileAndExpectedNumberOfFSyncs) -> { CacheFile cacheFile = cacheFileAndExpectedNumberOfFSyncs.v1(); assertThat(cacheService.isCacheFileToSync(cacheFile), is(false)); assertThat(fileSystemProvider.getNumberOfFSyncs(cacheFile.getFile()), equalTo(cacheFileAndExpectedNumberOfFSyncs.v2())); }); } } }	would be good to verify that once we evicted, we do not fsync these cachefiles anymore.
public void testTimestampAsNative() throws Exception { ZonedDateTime now = ZonedDateTime.now(Clock.tick(Clock.system(ZoneId.of("Z")), Duration.ofMillis(1))); assertThat(convertAsNative(now, EsType.DATETIME), instanceOf(Timestamp.class)); assertEquals(now.toInstant().toEpochMilli(), ((Timestamp) convertAsNative(now, EsType.DATETIME)).getTime()); }	why using clock here? maybe we could just use zoneddatetime.now(zoneoffset.utc)
private Object unwrapMultiValue(Object values) { if (values == null) { return null; } if (values instanceof List) { List<?> list = (List<?>) values; if (list.isEmpty()) { return null; } else { if (ARRAYS_LENIENCY || list.size() == 1) { return unwrapMultiValue(list.get(0)); } else { throw new SqlIllegalArgumentException("Arrays (returned by [{}]) are not supported", fieldName); } } } if (values instanceof Map) { throw new SqlIllegalArgumentException("Objects (returned by [{}]) are not supported", fieldName); } if (dataType == DataType.DATETIME) { if (values instanceof String) { return DateUtils.asDateTime(Long.parseLong(values.toString())); } } if (values instanceof Long || values instanceof Double || values instanceof String || values instanceof Boolean) { return values; } throw new SqlIllegalArgumentException("Type {} (returned by [{}]) is not supported", values.getClass().getSimpleName(), fieldName); }	@costin i'm not sure about this, but doesn't seem to break something. should we check more in depth and add tests?
@Override public Literal visitTimeEscapedLiteral(TimeEscapedLiteralContext ctx) { String string = string(ctx.string()); Source source = source(ctx); // parse HH:mm:ss LocalTime lt = null; try { lt = LocalTime.parse(string, ISO_LOCAL_TIME); } catch (DateTimeParseException ex) { throw new ParsingException(source, "Invalid time received; {}", ex.getMessage()); } throw new SqlIllegalArgumentException("Time (only) literals are not supported; a date component is required as well"); }	i see iso_local_time deals, also, with format as hh:mm (not only hh:mm:ss) and the previous joda implementation seems to have looked at hh:mm:ss only... not sure if this is an issue or not, meaning if there are 0 seconds, does it return 12:13 only or 12:13:00?
@Override public Literal visitTimestampEscapedLiteral(TimestampEscapedLiteralContext ctx) { String string = string(ctx.string()); Source source = source(ctx); // parse yyyy-mm-dd hh:mm:ss(.f...) ZonedDateTime zdt = null; try { DateTimeFormatter formatter = new DateTimeFormatterBuilder() .append(ISO_LOCAL_DATE) .appendLiteral(" ") .append(ISO_LOCAL_TIME) .toFormatter(); zdt = ZonedDateTime.parse(string, formatter.withZone(UTC)); return new Literal(source, zdt, DataType.DATETIME); } catch (DateTimeParseException ex) { throw new ParsingException(source, "Invalid timestamp received; {}", ex.getMessage()); } }	is there a formatter in dateformatters that we can use here? if not, maybe we could make this static final field
@Override public void messageReceived(final RecoveryFileChunkRequest request, TransportChannel channel) throws Exception { try (RecoveryStatus onGoingRecovery = onGoingRecoveries.getStatusSafe(request.recoveryId(), request.shardId())) { final Store store = onGoingRecovery.store(); IndexOutput indexOutput; if (request.position() == 0) { indexOutput = onGoingRecovery.openAndPutIndexOutput(request.name(), request.metadata(), store); } else { indexOutput = onGoingRecovery.getOpenIndexOutput(request.name()); } if (recoverySettings.rateLimiter() != null) { recoverySettings.rateLimiter().pause(request.content().length()); } BytesReference content = request.content(); if (!content.hasArray()) { content = content.toBytesArray(); } indexOutput.writeBytes(content.array(), content.arrayOffset(), content.length()); onGoingRecovery.state().getIndex().addRecoveredByteCount(content.length()); RecoveryState.File file = onGoingRecovery.state().getIndex().file(request.name()); if (file != null) { file.updateRecovered(request.length()); } if (indexOutput.getFilePointer() >= request.length() || request.lastChunk()) { Store.verify(indexOutput); // we are done indexOutput.close(); // write the checksum onGoingRecovery.legacyChecksums().add(request.metadata()); store.directory().sync(Collections.singleton(request.name())); IndexOutput remove = onGoingRecovery.removeOpenIndexOutputs(request.name()); onGoingRecovery.state().getIndex().addRecoveredFileCount(1); assert remove == null || remove == indexOutput; // remove maybe null if we got finished } } channel.sendResponse(TransportResponse.Empty.INSTANCE); }	i think since you change the finally part you should do this like: java try { store.verify(indexoutput); } finally { indexoutput.close(); } just to make sure we are closing the stream asap
private void doRecovery(final long recoveryId) { final StartRecoveryRequest request; final CancellableThreads cancellableThreads; final RecoveryState.Timer timer; try (RecoveryRef recoveryRef = onGoingRecoveries.getRecovery(recoveryId)) { if (recoveryRef == null) { logger.trace("not running recovery with id [{}] - can not find it (probably finished)", recoveryId); return; } final RecoveryTarget recoveryTarget = recoveryRef.target(); cancellableThreads = recoveryTarget.cancellableThreads(); timer = recoveryTarget.state().getTimer(); try { assert recoveryTarget.sourceNode() != null : "can not do a recovery without a source node"; request = getStartRecoveryRequest(recoveryTarget); logger.trace("{} preparing shard for peer recovery", recoveryTarget.shardId()); recoveryTarget.indexShard().prepareForIndexRecovery(); } catch (final Exception e) { // this will be logged as warning later on... logger.trace("unexpected error while preparing shard for peer recovery, failing recovery", e); onGoingRecoveries.failRecovery(recoveryId, new RecoveryFailedException(recoveryTarget.state(), "failed to prepare shard for recovery", e), true); return; } } Consumer<Throwable> handleException = e -> { if (logger.isTraceEnabled()) { logger.trace(() -> new ParameterizedMessage( "[{}][{}] Got exception on recovery", request.shardId().getIndex().getName(), request.shardId().id()), e); } Throwable cause = ExceptionsHelper.unwrapCause(e); if (cause instanceof CancellableThreads.ExecutionCancelledException) { // this can also come from the source wrapped in a RemoteTransportException onGoingRecoveries.failRecovery(recoveryId, new RecoveryFailedException(request, "source has canceled the recovery", cause), false); return; } if (cause instanceof RecoveryEngineException) { // unwrap an exception that was thrown as part of the recovery cause = cause.getCause(); } // do it twice, in case we have double transport exception cause = ExceptionsHelper.unwrapCause(cause); if (cause instanceof RecoveryEngineException) { // unwrap an exception that was thrown as part of the recovery cause = cause.getCause(); } // here, we would add checks against exception that need to be retried (and not removeAndClean in this case) if (cause instanceof IllegalIndexShardStateException || cause instanceof IndexNotFoundException || cause instanceof ShardNotFoundException) { // if the target is not ready yet, retry retryRecovery( recoveryId, "remote shard not ready", recoverySettings.retryDelayStateSync(), recoverySettings.activityTimeout()); return; } if (cause instanceof DelayRecoveryException) { retryRecovery(recoveryId, cause, recoverySettings.retryDelayStateSync(), recoverySettings.activityTimeout()); return; } if (cause instanceof ConnectTransportException) { logger.debug("delaying recovery of {} for [{}] due to networking error [{}]", request.shardId(), recoverySettings.retryDelayNetwork(), cause.getMessage()); retryRecovery(recoveryId, cause.getMessage(), recoverySettings.retryDelayNetwork(), recoverySettings.activityTimeout()); return; } if (cause instanceof AlreadyClosedException) { onGoingRecoveries.failRecovery(recoveryId, new RecoveryFailedException(request, "source shard is closed", cause), false); return; } onGoingRecoveries.failRecovery(recoveryId, new RecoveryFailedException(request, e), true); }; try { logger.trace("{} starting recovery from {}", request.shardId(), request.sourceNode()); transportService.submitRequest(request.sourceNode(), PeerRecoverySourceService.Actions.START_RECOVERY, request, new TransportResponseHandler<RecoveryResponse>() { @Override public void handleResponse(RecoveryResponse recoveryResponse) { final TimeValue recoveryTime = new TimeValue(timer.time()); // do this through ongoing recoveries to remove it from the collection onGoingRecoveries.markRecoveryAsDone(recoveryId); if (logger.isTraceEnabled()) { StringBuilder sb = new StringBuilder(); sb.append('[').append(request.shardId().getIndex().getName()).append(']') .append('[').append(request.shardId().id()).append("] "); sb.append("recovery completed from ").append(request.sourceNode()).append(", took[").append(recoveryTime) .append("]\\\\n"); sb.append(" phase1: recovered_files [").append(recoveryResponse.phase1FileNames.size()).append("]") .append(" with total_size of [").append(new ByteSizeValue(recoveryResponse.phase1TotalSize)).append("]") .append(", took [").append(timeValueMillis(recoveryResponse.phase1Time)).append("], throttling_wait [") .append(timeValueMillis(recoveryResponse.phase1ThrottlingWaitTime)).append(']').append("\\\\n"); sb.append(" : reusing_files [").append(recoveryResponse.phase1ExistingFileNames.size()) .append("] with total_size of [").append(new ByteSizeValue(recoveryResponse.phase1ExistingTotalSize)) .append("]\\\\n"); sb.append(" phase2: start took [").append(timeValueMillis(recoveryResponse.startTime)).append("]\\\\n"); sb.append(" : recovered [").append(recoveryResponse.phase2Operations).append("]") .append(" transaction log operations") .append(", took [").append(timeValueMillis(recoveryResponse.phase2Time)).append("]") .append("\\\\n"); logger.trace("{}", sb); } else { logger.debug("{} recovery done from [{}], took [{}]", request.shardId(), request.sourceNode(), recoveryTime); } } @Override public void handleException(TransportException e) { handleException.accept(e); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public RecoveryResponse read(StreamInput in) throws IOException { RecoveryResponse recoveryResponse = new RecoveryResponse(); recoveryResponse.readFrom(in); return recoveryResponse; } }); } catch (Exception e) { handleException.accept(e); } }	just consumer<exception>. no need for throwable
private void doRecovery(final long recoveryId) { final StartRecoveryRequest request; final CancellableThreads cancellableThreads; final RecoveryState.Timer timer; try (RecoveryRef recoveryRef = onGoingRecoveries.getRecovery(recoveryId)) { if (recoveryRef == null) { logger.trace("not running recovery with id [{}] - can not find it (probably finished)", recoveryId); return; } final RecoveryTarget recoveryTarget = recoveryRef.target(); cancellableThreads = recoveryTarget.cancellableThreads(); timer = recoveryTarget.state().getTimer(); try { assert recoveryTarget.sourceNode() != null : "can not do a recovery without a source node"; request = getStartRecoveryRequest(recoveryTarget); logger.trace("{} preparing shard for peer recovery", recoveryTarget.shardId()); recoveryTarget.indexShard().prepareForIndexRecovery(); } catch (final Exception e) { // this will be logged as warning later on... logger.trace("unexpected error while preparing shard for peer recovery, failing recovery", e); onGoingRecoveries.failRecovery(recoveryId, new RecoveryFailedException(recoveryTarget.state(), "failed to prepare shard for recovery", e), true); return; } } Consumer<Throwable> handleException = e -> { if (logger.isTraceEnabled()) { logger.trace(() -> new ParameterizedMessage( "[{}][{}] Got exception on recovery", request.shardId().getIndex().getName(), request.shardId().id()), e); } Throwable cause = ExceptionsHelper.unwrapCause(e); if (cause instanceof CancellableThreads.ExecutionCancelledException) { // this can also come from the source wrapped in a RemoteTransportException onGoingRecoveries.failRecovery(recoveryId, new RecoveryFailedException(request, "source has canceled the recovery", cause), false); return; } if (cause instanceof RecoveryEngineException) { // unwrap an exception that was thrown as part of the recovery cause = cause.getCause(); } // do it twice, in case we have double transport exception cause = ExceptionsHelper.unwrapCause(cause); if (cause instanceof RecoveryEngineException) { // unwrap an exception that was thrown as part of the recovery cause = cause.getCause(); } // here, we would add checks against exception that need to be retried (and not removeAndClean in this case) if (cause instanceof IllegalIndexShardStateException || cause instanceof IndexNotFoundException || cause instanceof ShardNotFoundException) { // if the target is not ready yet, retry retryRecovery( recoveryId, "remote shard not ready", recoverySettings.retryDelayStateSync(), recoverySettings.activityTimeout()); return; } if (cause instanceof DelayRecoveryException) { retryRecovery(recoveryId, cause, recoverySettings.retryDelayStateSync(), recoverySettings.activityTimeout()); return; } if (cause instanceof ConnectTransportException) { logger.debug("delaying recovery of {} for [{}] due to networking error [{}]", request.shardId(), recoverySettings.retryDelayNetwork(), cause.getMessage()); retryRecovery(recoveryId, cause.getMessage(), recoverySettings.retryDelayNetwork(), recoverySettings.activityTimeout()); return; } if (cause instanceof AlreadyClosedException) { onGoingRecoveries.failRecovery(recoveryId, new RecoveryFailedException(request, "source shard is closed", cause), false); return; } onGoingRecoveries.failRecovery(recoveryId, new RecoveryFailedException(request, e), true); }; try { logger.trace("{} starting recovery from {}", request.shardId(), request.sourceNode()); transportService.submitRequest(request.sourceNode(), PeerRecoverySourceService.Actions.START_RECOVERY, request, new TransportResponseHandler<RecoveryResponse>() { @Override public void handleResponse(RecoveryResponse recoveryResponse) { final TimeValue recoveryTime = new TimeValue(timer.time()); // do this through ongoing recoveries to remove it from the collection onGoingRecoveries.markRecoveryAsDone(recoveryId); if (logger.isTraceEnabled()) { StringBuilder sb = new StringBuilder(); sb.append('[').append(request.shardId().getIndex().getName()).append(']') .append('[').append(request.shardId().id()).append("] "); sb.append("recovery completed from ").append(request.sourceNode()).append(", took[").append(recoveryTime) .append("]\\\\n"); sb.append(" phase1: recovered_files [").append(recoveryResponse.phase1FileNames.size()).append("]") .append(" with total_size of [").append(new ByteSizeValue(recoveryResponse.phase1TotalSize)).append("]") .append(", took [").append(timeValueMillis(recoveryResponse.phase1Time)).append("], throttling_wait [") .append(timeValueMillis(recoveryResponse.phase1ThrottlingWaitTime)).append(']').append("\\\\n"); sb.append(" : reusing_files [").append(recoveryResponse.phase1ExistingFileNames.size()) .append("] with total_size of [").append(new ByteSizeValue(recoveryResponse.phase1ExistingTotalSize)) .append("]\\\\n"); sb.append(" phase2: start took [").append(timeValueMillis(recoveryResponse.startTime)).append("]\\\\n"); sb.append(" : recovered [").append(recoveryResponse.phase2Operations).append("]") .append(" transaction log operations") .append(", took [").append(timeValueMillis(recoveryResponse.phase2Time)).append("]") .append("\\\\n"); logger.trace("{}", sb); } else { logger.debug("{} recovery done from [{}], took [{}]", request.shardId(), request.sourceNode(), recoveryTime); } } @Override public void handleException(TransportException e) { handleException.accept(e); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public RecoveryResponse read(StreamInput in) throws IOException { RecoveryResponse recoveryResponse = new RecoveryResponse(); recoveryResponse.readFrom(in); return recoveryResponse; } }); } catch (Exception e) { handleException.accept(e); } }	this does a refresh, and also cleans up temporary files on disk, so it should not run on the network thread. let's change the executor of the response listener to threadpool.names.generic.
public ActionRequestValidationException validate(ActionRequestValidationException validationException) { if (index.isEmpty()) { validationException = addValidationError("index.isEmpty()", validationException); } return validationException; }	should this get a proper error message?
@Override public boolean equals(Object other) { if (this == other) { return true; } if (other == null || getClass() != other.getClass()) { return false; } final QueryConfig that = (QueryConfig) other; return Objects.equals(this.source, that.source) && Objects.equals(this.query, that.query); }	should this get a proper error message?
private Table buildTable( RestRequest request, GetIndexTemplatesResponse getIndexTemplatesResponse, GetComposableIndexTemplateAction.Response getComposableIndexTemplatesResponse, String[] requestedNames ) { final Predicate<String> namePredicate = getNamePredicate(requestedNames); final Table table = getTableWithHeader(request); for (IndexTemplateMetadata indexData : getIndexTemplatesResponse.getIndexTemplates()) { assert namePredicate.test(indexData.getName()); table.startRow(); table.addCell(indexData.name()); table.addCell("[" + String.join(", ", indexData.patterns()) + "]"); table.addCell(indexData.getOrder()); table.addCell(indexData.getVersion()); table.addCell(""); table.endRow(); } for (Map.Entry<String, ComposableIndexTemplate> entry : getComposableIndexTemplatesResponse.indexTemplates().entrySet()) { final String name = entry.getKey(); if (namePredicate.test(name)) { final ComposableIndexTemplate template = entry.getValue(); table.startRow(); table.addCell(name); table.addCell("[" + String.join(", ", template.indexPatterns()) + "]"); table.addCell(template.priorityOrZero()); table.addCell(template.version()); table.addCell("[" + String.join(", ", template.composedOf()) + "]"); table.endRow(); } } return table; }	why do we need this, shouldn't we rely on the template request to only returns stuff matching our initial cat request since we pass on the index name? (i'm sure we do, but i can't really figure out why i must admit :))
@Override protected RestChannelConsumer doCatRequest(final RestRequest request, NodeClient client) { final String[] templateNames = Strings.splitStringByCommaToArray(request.param("name", "")); final GetIndexTemplatesRequest getIndexTemplatesRequest = new GetIndexTemplatesRequest(templateNames); getIndexTemplatesRequest.local(request.paramAsBoolean("local", getIndexTemplatesRequest.local())); getIndexTemplatesRequest.masterNodeTimeout(request.paramAsTime("master_timeout", getIndexTemplatesRequest.masterNodeTimeout())); final StepListener<GetIndexTemplatesResponse> getIndexTemplatesStep = new StepListener<>(); final GetComposableIndexTemplateAction.Request getComposableTemplatesRequest = new GetComposableIndexTemplateAction.Request(); getComposableTemplatesRequest.local(request.paramAsBoolean("local", getComposableTemplatesRequest.local())); getComposableTemplatesRequest.masterNodeTimeout( request.paramAsTime("master_timeout", getComposableTemplatesRequest.masterNodeTimeout())); final StepListener<GetComposableIndexTemplateAction.Response> getComposableTemplatesStep = new StepListener<>(); return channel -> { client.admin().indices().getTemplates(getIndexTemplatesRequest, getIndexTemplatesStep); client.execute(GetComposableIndexTemplateAction.INSTANCE, getComposableTemplatesRequest, getComposableTemplatesStep); final ActionListener<Table> tableListener = new RestResponseListener<>(channel) { @Override public RestResponse buildResponse(Table table) throws Exception { return RestTable.buildResponse(table, channel); } }; getIndexTemplatesStep.whenComplete(getIndexTemplatesResponse -> getComposableTemplatesStep.whenComplete(getComposableIndexTemplatesResponse -> ActionListener.completeWith(tableListener, () -> buildTable( request, getIndexTemplatesResponse, getComposableIndexTemplatesResponse, templateNames) ), tableListener::onFailure ), tableListener::onFailure); }; }	maybe move this into the channel -> {} lambda, it's kind of weird having it outside of its scope when it's only used in the lambda? (at least it took me a little to reason out the failure paths here.
*/ final boolean syncUpTo(long offset) throws IOException { if (lastSyncedCheckpoint.offset < offset && syncNeeded()) { synchronized (syncLock) { // only one sync/checkpoint should happen concurrently but we wait if (lastSyncedCheckpoint.offset < offset && syncNeeded()) { // double checked locking - we don't want to fsync unless we have to and now that we have // the lock we should check again since if this code is busy we might have fsynced enough already final Checkpoint checkpointToSync; final LongArrayList flushedSequenceNumbers; final ReleasableBytesReference toWrite; try (ReleasableLock toClose = writeLock.acquire()) { synchronized (this) { ensureOpen(); checkpointToSync = getCheckpoint(); toWrite = pollOpsToWrite(); if (nonFsyncedSequenceNumbers.isEmpty()) { flushedSequenceNumbers = null; } else { flushedSequenceNumbers = nonFsyncedSequenceNumbers; nonFsyncedSequenceNumbers = new LongArrayList(64); } } try { // Write ops will release operations. writeAndReleaseOps(toWrite); } catch (final Exception ex) { closeWithTragicEvent(ex); throw ex; } } // now do the actual fsync outside of the synchronized block such that // we can continue writing to the buffer etc. try { if (lastSyncedCheckpoint.offset != checkpointToSync.offset) { channel.force(false); } writeCheckpoint(checkpointChannel, checkpointPath, checkpointToSync); } catch (final Exception ex) { closeWithTragicEvent(ex); throw ex; } if (flushedSequenceNumbers != null) { flushedSequenceNumbers.forEach((LongProcedure) persistedSequenceNumberConsumer::accept); } assert lastSyncedCheckpoint.offset <= checkpointToSync.offset : "illegal state: " + lastSyncedCheckpoint.offset + " <= " + checkpointToSync.offset; lastSyncedCheckpoint = checkpointToSync; // write protected by syncLock return true; } } } return false; }	this seems wrong, it leaves us with a (synced) checkpoint that refers to data that isn't synced and might vanish if, say, you pull the power out after executing this line.
private void writeAndReleaseOps(ReleasableBytesReference toWrite) throws IOException { try (ReleasableBytesReference toClose = toWrite) { assert writeLock.isHeldByCurrentThread(); final int length = toWrite.length(); if (length == 0) { return; } ByteBuffer ioBuffer = DiskIoBufferPool.maybeGetDirectIOBuffer(); if (ioBuffer == null) { // not using a direct buffer for writes from the current thread so just write without copying to the io buffer BytesRefIterator iterator = toWrite.iterator(); BytesRef current; while ((current = iterator.next()) != null) { Channels.writeToChannel(ByteBuffer.wrap(current.bytes, current.offset, current.length), channel); } return; } BytesRefIterator iterator = toWrite.iterator(); BytesRef current; while ((current = iterator.next()) != null) { int currentBytesConsumed = 0; while (currentBytesConsumed != current.length) { int nBytesToWrite = Math.min(current.length - currentBytesConsumed, ioBuffer.remaining()); ioBuffer.put(current.bytes, current.offset + currentBytesConsumed, nBytesToWrite); currentBytesConsumed += nBytesToWrite; if (ioBuffer.hasRemaining() == false) { ioBuffer.flip(); writeToFile(ioBuffer); ioBuffer.clear(); } } } ioBuffer.flip(); writeToFile(ioBuffer); } }	is there a case where we are writing using other than write or flush threads? if so, i wonder if we need to add a test or if this is already covered by existing tests?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeInt(waitForNodes); out.writeOptionalTimeValue(timeout); out.writeStringList(requiredNodes); }	there should be a corresponding change to the deserialisation code (in the constructor) and also to the tests.
public static FetchFieldsContext create( String indexName, MapperService mapperService, SearchLookup searchLookup, List<FieldAndFormat> fields ) { DocumentMapper documentMapper = mapperService.documentMapper(); if (documentMapper.sourceMapper().enabled() == false) { throw new IllegalArgumentException( "Unable to retrieve the requested [fields] since _source is " + "disabled in the mappings for index [" + indexName + "]" ); } return new FetchFieldsContext((m, s) -> FieldValueRetriever.create(m, s, fields)); }	could we just store and expose fields on this context? it feels confusing to me to store a fieldvalueretriever supplier instead. then we could take a simple approach where fetchfieldsphase just constructs the fieldvalueretriever directly.
@Override public boolean isReadOnly() { return readOnly; } /** * Writing a new index generation is a three step process. * First, the {@link RepositoriesState.State} entry for this repository is set into a pending state by incrementing its * pending generation {@code P} while its safe generation {@code N} remains unchanged. * Second, the updated {@link RepositoryData} is written to generation {@code P + 1}. * Lastly, the {@link RepositoriesState.State} entry for this repository is updated to the new generation {@code P + 1} and thus * pending and safe generation are set to the same value marking the end of the update of the repository data. * * @param repositoryData RepositoryData to write * @param expectedGen expected repository generation at the start of the operation * @param writeShardGens whether to write {@link ShardGenerations} to the new {@link RepositoryData}	is this step fault-tolerant against master failovers? do we want it to be?
@Override public boolean isReadOnly() { return readOnly; } /** * Writing a new index generation is a three step process. * First, the {@link RepositoriesState.State} entry for this repository is set into a pending state by incrementing its * pending generation {@code P} while its safe generation {@code N} remains unchanged. * Second, the updated {@link RepositoryData} is written to generation {@code P + 1}. * Lastly, the {@link RepositoriesState.State} entry for this repository is updated to the new generation {@code P + 1} and thus * pending and safe generation are set to the same value marking the end of the update of the repository data. * * @param repositoryData RepositoryData to write * @param expectedGen expected repository generation at the start of the operation * @param writeShardGens whether to write {@link ShardGenerations} to the new {@link RepositoryData}	maybe mention generation + repo here
public void testRetrieveSnapshots() throws Exception { final Client client = client(); final Path location = ESIntegTestCase.randomRepoPath(node().settings()); final String repositoryName = "test-repo"; logger.info("--> creating repository"); AcknowledgedResponse putRepositoryResponse = client.admin().cluster().preparePutRepository(repositoryName) .setType(REPO_TYPE) .setSettings(Settings.builder().put(node().settings()).put("location", location)) .get(); assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true)); logger.info("--> creating an index and indexing documents"); final String indexName = "test-idx"; createIndex(indexName); ensureGreen(); int numDocs = randomIntBetween(10, 20); for (int i = 0; i < numDocs; i++) { String id = Integer.toString(i); client().prepareIndex(indexName).setId(id).setSource("text", "sometext").get(); } client().admin().indices().prepareFlush(indexName).get(); logger.info("--> create first snapshot"); CreateSnapshotResponse createSnapshotResponse = client.admin() .cluster() .prepareCreateSnapshot(repositoryName, "test-snap-1") .setWaitForCompletion(true) .setIndices(indexName) .get(); final SnapshotId snapshotId1 = createSnapshotResponse.getSnapshotInfo().snapshotId(); logger.info("--> create second snapshot"); createSnapshotResponse = client.admin() .cluster() .prepareCreateSnapshot(repositoryName, "test-snap-2") .setWaitForCompletion(true) .setIndices(indexName) .get(); final SnapshotId snapshotId2 = createSnapshotResponse.getSnapshotInfo().snapshotId(); logger.info("--> make sure the node's repository can resolve the snapshots"); final RepositoriesService repositoriesService = getInstanceFromNode(RepositoriesService.class); final BlobStoreRepository repository = (BlobStoreRepository) repositoriesService.repository(repositoryName); final List<SnapshotId> originalSnapshots = Arrays.asList(snapshotId1, snapshotId2); List<SnapshotId> snapshotIds = ESBlobStoreRepositoryIntegTestCase.getRepositoryData(repository).getSnapshotIds().stream() .sorted((s1, s2) -> s1.getName().compareTo(s2.getName())).collect(Collectors.toList()); assertThat(snapshotIds, equalTo(originalSnapshots)); }	the generation here will now be something other than -1 because we are using the same repository name across tests. i intentionally didn't add a repository cleanup step between tests here, since this serves as a neat test (and illustration) of how the pending generation is kept consistent no matter what happens to the repo contents.
private void cleanupOldFiles(final String currentStateFile, Path[] locations) throws IOException { for (Path location : locations) { Path stateLocation = location.resolve(STATE_DIR_NAME); try (Directory stateDir = newDirectory(stateLocation)) { for (String file : stateDir.listAll()) { if (file.startsWith(prefix) && file.equals(currentStateFile) == false) { deleteTempFile(stateLocation, stateDir, file); logger.trace("cleanupOldFiles: cleaned up {}", stateLocation.resolve(file)); } } } } }	this is already logged part of deletetempfile?
public void testGuessRootCause() { { ElasticsearchException exception = new ElasticsearchException("foo", new ElasticsearchException("bar", new IndexNotFoundException("foo", new RuntimeException("foobar")))); ElasticsearchException[] rootCauses = exception.guessRootCauses(); assertEquals(rootCauses.length, 1); assertEquals(ElasticsearchException.getExceptionName(rootCauses[0]), "index_not_found_exception"); assertEquals(rootCauses[0].getMessage(), "no such index"); ShardSearchFailure failure = new ShardSearchFailure(new ParsingException(1, 2, "foobar", null), new SearchShardTarget("node_1", new Index("foo", "_na_"), 1, null)); ShardSearchFailure failure1 = new ShardSearchFailure(new ParsingException(1, 2, "foobar", null), new SearchShardTarget("node_1", new Index("foo", "_na_"), 2, null)); SearchPhaseExecutionException ex = new SearchPhaseExecutionException("search", "all shards failed", new ShardSearchFailure[]{failure, failure1}); if (randomBoolean()) { rootCauses = (randomBoolean() ? new RemoteTransportException("remoteboom", ex) : ex).guessRootCauses(); } else { rootCauses = ElasticsearchException.guessRootCauses(randomBoolean() ? new RemoteTransportException("remoteboom", ex) : ex); } assertEquals("parsing_exception", ElasticsearchException.getExceptionName(rootCauses[0])); assertEquals("foobar", rootCauses[0].getMessage()); ElasticsearchException oneLevel = new ElasticsearchException("foo", new RuntimeException("foobar")); rootCauses = oneLevel.guessRootCauses(); assertEquals("exception", ElasticsearchException.getExceptionName(rootCauses[0])); assertEquals("foo", rootCauses[0].getMessage()); } { ShardSearchFailure failure = new ShardSearchFailure( new ParsingException(1, 2, "foobar", null), new SearchShardTarget("node_1", new Index("foo", "_na_"), 1, null)); ShardSearchFailure failure1 = new ShardSearchFailure(new QueryShardException(new Index("foo1", "_na_"), "foobar", null), new SearchShardTarget("node_1", new Index("foo1", "_na_"), 1, null)); ShardSearchFailure failure2 = new ShardSearchFailure(new QueryShardException(new Index("foo1", "_na_"), "foobar", null), new SearchShardTarget("node_1", new Index("foo1", "_na_"), 2, null)); SearchPhaseExecutionException ex = new SearchPhaseExecutionException("search", "all shards failed", new ShardSearchFailure[]{failure, failure1, failure2}); final ElasticsearchException[] rootCauses = ex.guessRootCauses(); assertEquals(rootCauses.length, 2); assertEquals(ElasticsearchException.getExceptionName(rootCauses[0]), "parsing_exception"); assertEquals(rootCauses[0].getMessage(), "foobar"); assertEquals(1, ((ParsingException) rootCauses[0]).getLineNumber()); assertEquals(2, ((ParsingException) rootCauses[0]).getColumnNumber()); assertEquals("query_shard_exception", ElasticsearchException.getExceptionName(rootCauses[1])); assertEquals("foo1", rootCauses[1].getIndex().getName()); assertEquals("foobar", rootCauses[1].getMessage()); } { final ElasticsearchException[] foobars = ElasticsearchException.guessRootCauses(new IllegalArgumentException("foobar")); assertEquals(foobars.length, 1); assertThat(foobars[0], instanceOf(ElasticsearchException.class)); assertEquals("foobar", foobars[0].getMessage()); assertEquals(IllegalArgumentException.class, foobars[0].getCause().getClass()); assertEquals("illegal_argument_exception", foobars[0].getExceptionName()); } { XContentParseException inner = new XContentParseException(null, "inner"); XContentParseException outer = new XContentParseException(null, "outer", inner); final ElasticsearchException[] causes = ElasticsearchException.guessRootCauses(outer); assertEquals(causes.length, 1); assertThat(causes[0], instanceOf(ElasticsearchException.class)); assertEquals("inner", causes[0].getMessage()); assertEquals("x_content_parse_exception", causes[0].getExceptionName()); } { ElasticsearchException inner = new ElasticsearchException("inner"); XContentParseException outer = new XContentParseException(null, "outer", inner); final ElasticsearchException[] causes = ElasticsearchException.guessRootCauses(outer); assertEquals(causes.length, 1); assertThat(causes[0], instanceOf(ElasticsearchException.class)); assertEquals("inner", causes[0].getMessage()); assertEquals("exception", causes[0].getExceptionName()); } }	these were all backwards....
public void run() { final PeriodicMaintenanceTask maintenanceTask = this; assert assertGenericThread(); try { ensureOpen(); if (pointIntTimeId == null) { final OpenPointInTimeRequest openRequest = new OpenPointInTimeRequest(SNAPSHOT_BLOB_CACHE_INDEX); openRequest.keepAlive(keepAlive); clientWithOrigin.execute(OpenPointInTimeAction.INSTANCE, openRequest, new ActionListener<>() { @Override public void onResponse(OpenPointInTimeResponse response) { logger.trace("periodic maintenance task initialized with point-in-time id [{}]", response.getPointInTimeId()); maintenanceTask.pointIntTimeId = response.getPointInTimeId(); executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { if (TransportActions.isShardNotAvailableException(e)) { complete(maintenanceTask, null); } else { complete(maintenanceTask, e); } } }); return; } final String pitId = pointIntTimeId; assert Strings.hasLength(pitId); if (searchResponse == null) { final SearchSourceBuilder searchSource = new SearchSourceBuilder(); searchSource.trackScores(false); searchSource.sort("_shard_doc"); searchSource.size(batchSize); final PointInTimeBuilder pointInTime = new PointInTimeBuilder(pitId); searchSource.pointInTimeBuilder(pointInTime); pointInTime.setKeepAlive(keepAlive); final SearchRequest searchRequest = new SearchRequest(); searchRequest.source(searchSource); if (searchAfter != null) { searchSource.searchAfter(searchAfter); } clientWithOrigin.execute(SearchAction.INSTANCE, searchRequest, new ActionListener<>() { @Override public void onResponse(SearchResponse response) { maintenanceTask.total.compareAndSet(0L, response.getHits().getTotalHits().value); maintenanceTask.searchResponse = response; maintenanceTask.searchAfter = null; executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { complete(maintenanceTask, e); } }); return; } final SearchHit[] searchHits = searchResponse.getHits().getHits(); if (searchHits != null && searchHits.length > 0) { final ClusterState state = clusterService.state(); final BulkRequest bulkRequest = new BulkRequest(); final Set<Tuple<String, String>> knownSnapshots = new HashSet<>(); final Set<Tuple<String, String>> missingSnapshots = new HashSet<>(); final Set<String> knownRepositories = state.metadata() .custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY) .repositories() .stream() .map(RepositoryMetadata::name) .collect(Collectors.toSet()); Object[] lastSortValues = null; for (SearchHit searchHit : searchHits) { assert searchHit.getId() != null; assert searchHit.hasSource(); // See {@link BlobStoreCacheService#generateId} // doc id = {repository name}/{snapshot id}/{snapshot index id}/{shard id}/{file name}/@{file offset} final String[] parts = Objects.requireNonNull(searchHit.getId()).split("/"); assert parts.length == 6 : Arrays.toString(parts) + " vs " + searchHit.getId(); boolean delete = false; lastSortValues = searchHit.getSortValues(); final String repositoryName = parts[0]; if (knownRepositories.contains(repositoryName) == false) { logger.trace( "deleting blob store cache entry [id:{}, repository:{}, reason: repository does not exist]", searchHit.getId(), repositoryName ); delete = true; } else { final Tuple<String, String> snapshot = Tuple.tuple(parts[1], parts[2]); boolean isMissing = missingSnapshots.contains(snapshot); boolean isKnown = knownSnapshots.contains(snapshot); if (isMissing || (isKnown == false && hasSearchableSnapshotWith(state, snapshot.v1(), snapshot.v2()) == false)) { logger.trace( "deleting blob store cache entry [id:{}, snapshotId:{}, indexId:{}, reason: unused]", searchHit.getId(), snapshot.v1(), snapshot.v2() ); if (isMissing == false) { missingSnapshots.add(snapshot); } delete = true; } else if (isKnown == false) { knownSnapshots.add(snapshot); } } if (delete) { final DeleteRequest deleteRequest = new DeleteRequest().index(searchHit.getIndex()); deleteRequest.id(searchHit.getId()); deleteRequest.setIfSeqNo(searchHit.getSeqNo()); deleteRequest.setIfPrimaryTerm(searchHit.getPrimaryTerm()); bulkRequest.add(deleteRequest); } } assert lastSortValues != null; if (bulkRequest.numberOfActions() == 0) { this.searchResponse = null; this.searchAfter = lastSortValues; executeNext(this); return; } final Object[] finalSearchAfter = lastSortValues; clientWithOrigin.execute(BulkAction.INSTANCE, bulkRequest, new ActionListener<>() { @Override public void onResponse(BulkResponse response) { for (BulkItemResponse itemResponse : response.getItems()) { if (itemResponse.isFailed() == false) { assert itemResponse.getResponse() instanceof DeleteResponse; maintenanceTask.deletes.incrementAndGet(); } } maintenanceTask.searchResponse = null; maintenanceTask.searchAfter = finalSearchAfter; executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { complete(maintenanceTask, e); } }); return; } // we're done, complete the task complete(this, null); } catch (Exception e) { complete(this, e); } }	we keep around the snapshots in order to avoid reiterating over all indices in cluster state.
public void run() { final PeriodicMaintenanceTask maintenanceTask = this; assert assertGenericThread(); try { ensureOpen(); if (pointIntTimeId == null) { final OpenPointInTimeRequest openRequest = new OpenPointInTimeRequest(SNAPSHOT_BLOB_CACHE_INDEX); openRequest.keepAlive(keepAlive); clientWithOrigin.execute(OpenPointInTimeAction.INSTANCE, openRequest, new ActionListener<>() { @Override public void onResponse(OpenPointInTimeResponse response) { logger.trace("periodic maintenance task initialized with point-in-time id [{}]", response.getPointInTimeId()); maintenanceTask.pointIntTimeId = response.getPointInTimeId(); executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { if (TransportActions.isShardNotAvailableException(e)) { complete(maintenanceTask, null); } else { complete(maintenanceTask, e); } } }); return; } final String pitId = pointIntTimeId; assert Strings.hasLength(pitId); if (searchResponse == null) { final SearchSourceBuilder searchSource = new SearchSourceBuilder(); searchSource.trackScores(false); searchSource.sort("_shard_doc"); searchSource.size(batchSize); final PointInTimeBuilder pointInTime = new PointInTimeBuilder(pitId); searchSource.pointInTimeBuilder(pointInTime); pointInTime.setKeepAlive(keepAlive); final SearchRequest searchRequest = new SearchRequest(); searchRequest.source(searchSource); if (searchAfter != null) { searchSource.searchAfter(searchAfter); } clientWithOrigin.execute(SearchAction.INSTANCE, searchRequest, new ActionListener<>() { @Override public void onResponse(SearchResponse response) { maintenanceTask.total.compareAndSet(0L, response.getHits().getTotalHits().value); maintenanceTask.searchResponse = response; maintenanceTask.searchAfter = null; executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { complete(maintenanceTask, e); } }); return; } final SearchHit[] searchHits = searchResponse.getHits().getHits(); if (searchHits != null && searchHits.length > 0) { final ClusterState state = clusterService.state(); final BulkRequest bulkRequest = new BulkRequest(); final Set<Tuple<String, String>> knownSnapshots = new HashSet<>(); final Set<Tuple<String, String>> missingSnapshots = new HashSet<>(); final Set<String> knownRepositories = state.metadata() .custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY) .repositories() .stream() .map(RepositoryMetadata::name) .collect(Collectors.toSet()); Object[] lastSortValues = null; for (SearchHit searchHit : searchHits) { assert searchHit.getId() != null; assert searchHit.hasSource(); // See {@link BlobStoreCacheService#generateId} // doc id = {repository name}/{snapshot id}/{snapshot index id}/{shard id}/{file name}/@{file offset} final String[] parts = Objects.requireNonNull(searchHit.getId()).split("/"); assert parts.length == 6 : Arrays.toString(parts) + " vs " + searchHit.getId(); boolean delete = false; lastSortValues = searchHit.getSortValues(); final String repositoryName = parts[0]; if (knownRepositories.contains(repositoryName) == false) { logger.trace( "deleting blob store cache entry [id:{}, repository:{}, reason: repository does not exist]", searchHit.getId(), repositoryName ); delete = true; } else { final Tuple<String, String> snapshot = Tuple.tuple(parts[1], parts[2]); boolean isMissing = missingSnapshots.contains(snapshot); boolean isKnown = knownSnapshots.contains(snapshot); if (isMissing || (isKnown == false && hasSearchableSnapshotWith(state, snapshot.v1(), snapshot.v2()) == false)) { logger.trace( "deleting blob store cache entry [id:{}, snapshotId:{}, indexId:{}, reason: unused]", searchHit.getId(), snapshot.v1(), snapshot.v2() ); if (isMissing == false) { missingSnapshots.add(snapshot); } delete = true; } else if (isKnown == false) { knownSnapshots.add(snapshot); } } if (delete) { final DeleteRequest deleteRequest = new DeleteRequest().index(searchHit.getIndex()); deleteRequest.id(searchHit.getId()); deleteRequest.setIfSeqNo(searchHit.getSeqNo()); deleteRequest.setIfPrimaryTerm(searchHit.getPrimaryTerm()); bulkRequest.add(deleteRequest); } } assert lastSortValues != null; if (bulkRequest.numberOfActions() == 0) { this.searchResponse = null; this.searchAfter = lastSortValues; executeNext(this); return; } final Object[] finalSearchAfter = lastSortValues; clientWithOrigin.execute(BulkAction.INSTANCE, bulkRequest, new ActionListener<>() { @Override public void onResponse(BulkResponse response) { for (BulkItemResponse itemResponse : response.getItems()) { if (itemResponse.isFailed() == false) { assert itemResponse.getResponse() instanceof DeleteResponse; maintenanceTask.deletes.incrementAndGet(); } } maintenanceTask.searchResponse = null; maintenanceTask.searchAfter = finalSearchAfter; executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { complete(maintenanceTask, e); } }); return; } // we're done, complete the task complete(this, null); } catch (Exception e) { complete(this, e); } }	i noticed that the repository name is part of the document id :( using our own maintenance task here could help to reindex the docs without the repository name, if we think the snapshot uuid + index uuid are unique enough to avoid collisions.
public void run() { final PeriodicMaintenanceTask maintenanceTask = this; assert assertGenericThread(); try { ensureOpen(); if (pointIntTimeId == null) { final OpenPointInTimeRequest openRequest = new OpenPointInTimeRequest(SNAPSHOT_BLOB_CACHE_INDEX); openRequest.keepAlive(keepAlive); clientWithOrigin.execute(OpenPointInTimeAction.INSTANCE, openRequest, new ActionListener<>() { @Override public void onResponse(OpenPointInTimeResponse response) { logger.trace("periodic maintenance task initialized with point-in-time id [{}]", response.getPointInTimeId()); maintenanceTask.pointIntTimeId = response.getPointInTimeId(); executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { if (TransportActions.isShardNotAvailableException(e)) { complete(maintenanceTask, null); } else { complete(maintenanceTask, e); } } }); return; } final String pitId = pointIntTimeId; assert Strings.hasLength(pitId); if (searchResponse == null) { final SearchSourceBuilder searchSource = new SearchSourceBuilder(); searchSource.trackScores(false); searchSource.sort("_shard_doc"); searchSource.size(batchSize); final PointInTimeBuilder pointInTime = new PointInTimeBuilder(pitId); searchSource.pointInTimeBuilder(pointInTime); pointInTime.setKeepAlive(keepAlive); final SearchRequest searchRequest = new SearchRequest(); searchRequest.source(searchSource); if (searchAfter != null) { searchSource.searchAfter(searchAfter); } clientWithOrigin.execute(SearchAction.INSTANCE, searchRequest, new ActionListener<>() { @Override public void onResponse(SearchResponse response) { maintenanceTask.total.compareAndSet(0L, response.getHits().getTotalHits().value); maintenanceTask.searchResponse = response; maintenanceTask.searchAfter = null; executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { complete(maintenanceTask, e); } }); return; } final SearchHit[] searchHits = searchResponse.getHits().getHits(); if (searchHits != null && searchHits.length > 0) { final ClusterState state = clusterService.state(); final BulkRequest bulkRequest = new BulkRequest(); final Set<Tuple<String, String>> knownSnapshots = new HashSet<>(); final Set<Tuple<String, String>> missingSnapshots = new HashSet<>(); final Set<String> knownRepositories = state.metadata() .custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY) .repositories() .stream() .map(RepositoryMetadata::name) .collect(Collectors.toSet()); Object[] lastSortValues = null; for (SearchHit searchHit : searchHits) { assert searchHit.getId() != null; assert searchHit.hasSource(); // See {@link BlobStoreCacheService#generateId} // doc id = {repository name}/{snapshot id}/{snapshot index id}/{shard id}/{file name}/@{file offset} final String[] parts = Objects.requireNonNull(searchHit.getId()).split("/"); assert parts.length == 6 : Arrays.toString(parts) + " vs " + searchHit.getId(); boolean delete = false; lastSortValues = searchHit.getSortValues(); final String repositoryName = parts[0]; if (knownRepositories.contains(repositoryName) == false) { logger.trace( "deleting blob store cache entry [id:{}, repository:{}, reason: repository does not exist]", searchHit.getId(), repositoryName ); delete = true; } else { final Tuple<String, String> snapshot = Tuple.tuple(parts[1], parts[2]); boolean isMissing = missingSnapshots.contains(snapshot); boolean isKnown = knownSnapshots.contains(snapshot); if (isMissing || (isKnown == false && hasSearchableSnapshotWith(state, snapshot.v1(), snapshot.v2()) == false)) { logger.trace( "deleting blob store cache entry [id:{}, snapshotId:{}, indexId:{}, reason: unused]", searchHit.getId(), snapshot.v1(), snapshot.v2() ); if (isMissing == false) { missingSnapshots.add(snapshot); } delete = true; } else if (isKnown == false) { knownSnapshots.add(snapshot); } } if (delete) { final DeleteRequest deleteRequest = new DeleteRequest().index(searchHit.getIndex()); deleteRequest.id(searchHit.getId()); deleteRequest.setIfSeqNo(searchHit.getSeqNo()); deleteRequest.setIfPrimaryTerm(searchHit.getPrimaryTerm()); bulkRequest.add(deleteRequest); } } assert lastSortValues != null; if (bulkRequest.numberOfActions() == 0) { this.searchResponse = null; this.searchAfter = lastSortValues; executeNext(this); return; } final Object[] finalSearchAfter = lastSortValues; clientWithOrigin.execute(BulkAction.INSTANCE, bulkRequest, new ActionListener<>() { @Override public void onResponse(BulkResponse response) { for (BulkItemResponse itemResponse : response.getItems()) { if (itemResponse.isFailed() == false) { assert itemResponse.getResponse() instanceof DeleteResponse; maintenanceTask.deletes.incrementAndGet(); } } maintenanceTask.searchResponse = null; maintenanceTask.searchAfter = finalSearchAfter; executeNext(maintenanceTask); } @Override public void onFailure(Exception e) { complete(maintenanceTask, e); } }); return; } // we're done, complete the task complete(this, null); } catch (Exception e) { complete(this, e); } }	rather than maintain this extra state and loop through the indices for every snapshot we see, i wonder if we could simply find the set of snapshots to keep after opening the pit? since the pit will not return docs indexed after it was opened, that should be relatively safe. there is a race condition though, in that we risk a searchable snapshot in the process of being mounted but this node does not know about it yet. that race also exists with the current structure. i can think of a few options: 1. remember max seq-no from last round and only allow deleting entries with lower seq-no than that. 2. wait 10 sec after opening pit before proceeding. 3. collect the snapshot ids to delete but wait a round before deleting, rechecking that they can still be deleted. 4. ignore the issue, put in a comment and accept that we rarely delete a bit too eagerly. 5. add a timestamp to each entry, allowing us to not delete entries newer than an hour or so. do you have other ideas/options?
public void clusterChanged(ClusterChangedEvent event) { final ClusterState state = event.state(); if (state.getBlocks().hasGlobalBlock(STATE_NOT_RECOVERED_BLOCK)) { return; // state not fully recovered } final ShardRouting primary = systemIndexPrimaryShard(state); if (primary == null || Objects.equals(state.nodes().getLocalNodeId(), primary.currentNodeId()) == false) { // system index primary shard does not exist or is not assigned to this data node stopPeriodicTask(); return; } if (event.indicesDeleted().isEmpty() == false) { threadPool.generic().execute(new DeletedIndicesMaintenanceTask(event)); } if (periodicTask == null || periodicTask.isCancelled()) { schedulePeriodic = true; schedulePeriodicTask(); } }	nit: since we have stop in it's own method, i would prefer to also add a start method.
private static boolean hasSearchableSnapshotWith(final ClusterState state, final String snapshotId, final String indexId) { for (IndexMetadata indexMetadata : state.metadata()) { final Settings indexSettings = indexMetadata.getSettings(); if (SearchableSnapshotsSettings.isSearchableSnapshotStore(indexSettings)) { if (Objects.equals(snapshotId, SNAPSHOT_SNAPSHOT_ID_SETTING.get(indexSettings)) && Objects.equals(indexId, SNAPSHOT_INDEX_ID_SETTING.get(indexSettings))) { return true; } } } return false; }	note: this method does not check if the snapshot belongs to the same repository. since snapshot uuid and index uuid are unique ids i think the risk of collision is very limited, given that in case of collision the cached docs are not deleted.
* @param options options on the request to perform * @return the response returned by Elasticsearch * @throws IOException in case of a problem or the connection was aborted * @throws ClientProtocolException in case of an http protocol error * @throws ResponseException in case Elasticsearch responded with a status code that indicated an error */ public Response performRequest(Request request, RequestOptions options) throws IOException { SyncResponseListener listener = new SyncResponseListener(maxRetryTimeoutMillis); performRequestAsyncNoCatch(request, options, listener); return listener.get(); } /** * Sends a request to the Elasticsearch cluster that the client points to. * The request is executed asynchronously and the provided * {@link ResponseListener} gets notified upon request completion or * failure. Selects a host out of the provided ones in a round-robin * fashion. Failing hosts are marked dead and retried after a certain * amount of time (minimum 1 minute, maximum 30 minutes), depending on how * many times they previously failed (the more failures, the later they * will be retried). In case of failures all of the alive nodes (or dead * nodes that deserve a retry) are retried until one responds or none of * them does, in which case an {@link IOException} will be thrown. * * @param request the request to perform * @param options options on the request to perform * @param responseListener the {@link ResponseListener}	i would try not to touch these signatures. the idea could be to make options a subclass of request, and also an instance member of request, so that the low-level rest client's methods remain the same, only to way to set the options changes (we haven't released yet the new request flavoured methods anyways).
@Override public String toString() { return "SnapshotFiles{" + indexFiles.toString() + "}"; }	can we add shardstateidentifier here while we're at it ? :) (also .tostring() isn't necessary?
private static void ensureMutable(String name) { if ((name.startsWith("segments_") || name.startsWith("pending_segments_") || name.matches("^recovery\\\\\\\\..*\\\\\\\\.segments_.*$")) == false) { throw new ImmutableDirectoryException("file [" + name + "] is not mutable"); } }	nit: drop empty line
@Override public Optional<EngineFactory> getEngineFactory(IndexSettings indexSettings) { if (SearchableSnapshotsConstants.isSearchableSnapshotStore(indexSettings.getSettings()) && indexSettings.getSettings().getAsBoolean("index.frozen", false) == false) { return Optional.of( engineConfig -> new ReadOnlyEngine(engineConfig, null, new TranslogStats(), false, Function.identity(), false) { // present an empty IndexCommit to the snapshot mechanism so that we copy no shard data to the repository private final IndexCommit emptyIndexCommit; { try { final Directory directory = engineConfig.getStore().directory(); final String oldestSegmentsFile = Arrays.stream(directory.listAll()) .filter(s -> s.startsWith(IndexFileNames.SEGMENTS + "_")) .min(Comparator.naturalOrder()) .orElseThrow(() -> new IOException("segments_N file not found")); final SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major); segmentInfos.updateGeneration(SegmentInfos.readCommit(directory, oldestSegmentsFile)); emptyIndexCommit = Lucene.getIndexCommit(segmentInfos, directory); } catch (IOException e) { throw new UncheckedIOException(e); } } @Override public IndexCommitRef acquireIndexCommitForSnapshot() throws EngineException { store.incRef(); return new IndexCommitRef(emptyIndexCommit, store::decRef); } } ); } return Optional.empty(); }	oops, this will almost always work except for cases where we add another character to the encoded generation (think "1" < "10" < "2"). i'll address this.
@Override public Optional<EngineFactory> getEngineFactory(IndexSettings indexSettings) { if (SearchableSnapshotsConstants.isSearchableSnapshotStore(indexSettings.getSettings()) && indexSettings.getSettings().getAsBoolean("index.frozen", false) == false) { return Optional.of( engineConfig -> new ReadOnlyEngine(engineConfig, null, new TranslogStats(), false, Function.identity(), false) { // present an empty IndexCommit to the snapshot mechanism so that we copy no shard data to the repository private final IndexCommit emptyIndexCommit; { try { final Directory directory = engineConfig.getStore().directory(); final String oldestSegmentsFile = Arrays.stream(directory.listAll()) .filter(s -> s.startsWith(IndexFileNames.SEGMENTS + "_")) .min(Comparator.naturalOrder()) .orElseThrow(() -> new IOException("segments_N file not found")); final SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major); segmentInfos.updateGeneration(SegmentInfos.readCommit(directory, oldestSegmentsFile)); emptyIndexCommit = Lucene.getIndexCommit(segmentInfos, directory); } catch (IOException e) { throw new UncheckedIOException(e); } } @Override public IndexCommitRef acquireIndexCommitForSnapshot() throws EngineException { store.incRef(); return new IndexCommitRef(emptyIndexCommit, store::decRef); } } ); } return Optional.empty(); }	just for my own learning probably, but why do we need this? can't we just always use generation 0? probably not :) => can we add a comment with why we need this?
public void collect(int doc, long bucket) throws IOException { weights = bigArrays.grow(weights, bucket + 1); sums = bigArrays.grow(sums, bucket + 1); sumCompensations = bigArrays.grow(sumCompensations, bucket + 1); weightCompensations = bigArrays.grow(weightCompensations, bucket + 1); if (docValues.advanceExact(doc) && docWeights.advanceExact(doc)) { if (docWeights.docValueCount() > 1) { throw new AggregationExecutionException("Encountered more than one weight for a " + "single document. Use a script to combine multiple weights-per-doc into a single value."); } // There should always be one weight if advanceExact lands us here, either // a real weight or a `missing` value assert docWeights.docValueCount() == 1; final double weight = docWeights.nextValue(); final int numValues = docValues.docValueCount(); assert numValues > 0; for (int i = 0; i < numValues; i++) { kahanSum(docValues.nextValue() * weight, sums, sumCompensations, bucket); kahanSum(weight, weights, weightCompensations, bucket); } } }	nit: missing value -> missing weight
public static Response randomTransformResponse() { List<TransformConfig> configs = new ArrayList<>(); for (int i = 0; i < randomInt(10); ++i) { configs.add(TransformConfigTests.randomTransformConfig()); } return new Response(configs, randomNonNegativeLong()); }	this is generating a different random int for the stopping point on every iteration. it will mean that getting to the high numbers like 8, 9, 10 is extremely rare. it would be easier to understand if int numconfigs = randomint(10); was pulled out of the loop condition and then numconfigs used in the loop condition.
@Override public final ShardSearchRequest buildShardSearchRequest(SearchShardIterator shardIt, int shardIndex) { AliasFilter filter = aliasFilter.get(shardIt.shardId().getIndex().getUUID()); assert filter != null; float indexBoost = concreteIndexBoosts.getOrDefault(shardIt.shardId().getIndex().getUUID(), DEFAULT_INDEX_BOOST); long waitForCheckpoint; if (request.getWaitForCheckpoints().length == 0) { waitForCheckpoint = SequenceNumbers.NO_OPS_PERFORMED; } else { waitForCheckpoint = request.getWaitForCheckpoints()[shardIndex]; } ShardSearchRequest shardRequest = new ShardSearchRequest(shardIt.getOriginalIndices(), request, shardIt.shardId(), shardIndex, getNumShards(), filter, indexBoost, timeProvider.getAbsoluteStartMillis(), shardIt.getClusterAlias(), shardIt.getSearchContextId(), shardIt.getSearchContextKeepAlive(), waitForCheckpoint); // if we already received a search result we can inform the shard that it // can return a null response if the request rewrites to match none rather // than creating an empty response in the search thread pool. // Note that, we have to disable this shortcut for queries that create a context (scroll and search context). shardRequest.canReturnNullResponseIfMatchNoDocs(hasShardResponse.get() && shardRequest.scroll() == null); return shardRequest; }	perhaps add: assert request.getwaitforcheckpoints().length > shardindex; just to be sure an array index out of bounds is not handled, reported as failed shard and ignored in some tests.
public RefreshStats refreshStats() { int listeners = refreshListeners.pendingLocationListenerCount(); return new RefreshStats( refreshMetric.count(), TimeUnit.NANOSECONDS.toMillis(refreshMetric.sum()), externalRefreshMetric.count(), TimeUnit.NANOSECONDS.toMillis(externalRefreshMetric.sum()), listeners); }	i think we should include the checkpoint listeners here too? perhaps keep the method named pendingcount and add the numbers in it?
private void onNewEngine(Engine newEngine) { assert Thread.holdsLock(engineMutex); refreshListeners.setCurrentRefreshLocationSupplier(newEngine::getTranslogLastWriteLocation); refreshListeners.setCurrentProcessedSeqNoSupplier(newEngine::getProcessedLocalCheckpoint); }	perhaps rename setcurrentprocessedseqnosupplier to setcurrentprocessedcheckpointsupplier?
* @param listener for the refresh. * @return did we call the listener (true) or register the listener to call later (false)? */ public boolean addOrNotify(long checkpoint, ActionListener<Void> listener) { assert checkpoint >= SequenceNumbers.NO_OPS_PERFORMED; if (checkpoint <= lastRefreshedSeqNo) { listener.onResponse(null); return true; } long processedSequenceNumber = processedSeqNoSupplier.getAsLong(); if (checkpoint > processedSequenceNumber) { IllegalArgumentException e = new IllegalArgumentException("Cannot wait for unprocessed checkpoint [wait_for_checkpoint=" + checkpoint + ", processed_checkpoint=" + processedSequenceNumber + "]"); listener.onFailure(e); return true; } synchronized (this) { if (closed) { listener.onFailure(new IllegalStateException("can't wait for refresh on a closed index")); return true; } List<Tuple<Long, ActionListener<Void>>> listeners = seqNoRefreshListeners; final int maxRefreshes = getMaxRefreshListeners.getAsInt(); if (refreshForcers == 0 && roomForListener(maxRefreshes, locationRefreshListeners, listeners)) { ActionListener<Void> contextPreservingListener = ContextPreservingActionListener.wrapPreservingContext(listener, threadContext); if (listeners == null) { listeners = new ArrayList<>(); } // We have a free slot so register the listener listeners.add(new Tuple<>(checkpoint, contextPreservingListener)); seqNoRefreshListeners = listeners; return false; } } // No free slot so force a refresh and call the listener in this thread forceRefresh.run(); listener.onResponse(null); return true; }	this method is unused except in tests. perhaps remove it, if you agree to having a cmbined pendingcount method
public void testLotsOfThreads() throws Exception { int threadCount = between(3, 10); maxListeners = between(1, threadCount * 2); // This thread just refreshes every once in a while to cause trouble. Cancellable refresher = threadPool.scheduleWithFixedDelay(() -> engine.refresh("because test"), timeValueMillis(100), Names.SAME); // These threads add and block until the refresh makes the change visible and then do a non-realtime get. Thread[] indexers = new Thread[threadCount]; for (int thread = 0; thread < threadCount; thread++) { final String threadId = String.format(Locale.ROOT, "%04d", thread); indexers[thread] = new Thread(() -> { for (int iteration = 1; iteration <= 50; iteration++) { try { String testFieldValue = String.format(Locale.ROOT, "%s%04d", threadId, iteration); Engine.IndexResult index = index(threadId, testFieldValue); assertEquals(iteration, index.getVersion()); DummyRefreshListener listener = new DummyRefreshListener(); listeners.addOrNotify(index.getTranslogLocation(), listener); DummySeqNoListener seqNoListener = new DummySeqNoListener(); long processedLocalCheckpoint = engine.getProcessedLocalCheckpoint(); // Cannot listen for a sequence number higher than processed local checkpoint currently. listeners.addOrNotify(Math.min(index.getSeqNo(), processedLocalCheckpoint), seqNoListener); assertBusy(() -> { assertNotNull("location listener never called", listener.forcedRefresh.get()); if (seqNoListener.error != null) { logger.error(seqNoListener.error); } assertTrue("seqNo listener never called", seqNoListener.isDone.get()); }, 1, TimeUnit.MINUTES); if ((threadCount * 2) < maxListeners) { assertFalse(listener.forcedRefresh.get()); } listener.assertNoError(); Engine.Get get = new Engine.Get(false, false, threadId); MapperService mapperService = EngineTestCase.createMapperService(); try (Engine.GetResult getResult = engine.get(get, mapperService.mappingLookup(), mapperService.documentParser(), EngineTestCase.randomSearcherWrapper())) { assertTrue("document not found", getResult.exists()); assertEquals(iteration, getResult.version()); org.apache.lucene.document.Document document = getResult.docIdAndVersion().reader.document(getResult.docIdAndVersion().docId); assertThat(document.getValues("test"), arrayContaining(testFieldValue)); } } catch (Exception t) { throw new RuntimeException("failure on the [" + iteration + "] iteration of thread [" + threadId + "]", t); } } }); indexers[thread].start(); } for (Thread indexer : indexers) { indexer.join(); } refresher.cancel(); }	you could throw a non-assertion error here, that should be passed out as an uncaught exception.
public void testDisallowAddListeners() throws Exception { assertEquals(0, listeners.pendingLocationListenerCount()); assertEquals(0, listeners.pendingSeqNoListenersCount()); DummyRefreshListener listener = new DummyRefreshListener(); assertFalse(listeners.addOrNotify(index("1").getTranslogLocation(), listener)); DummySeqNoListener seqNoListener = new DummySeqNoListener(); assertFalse(listeners.addOrNotify(index("1").getSeqNo(), seqNoListener)); engine.refresh("I said so"); assertFalse(listener.forcedRefresh.get()); listener.assertNoError(); assertTrue(seqNoListener.isDone.get()); try (Releasable releaseable1 = listeners.forceRefreshes()) { listener = new DummyRefreshListener(); assertTrue(listeners.addOrNotify(index("1").getTranslogLocation(), listener)); assertTrue(listener.forcedRefresh.get()); listener.assertNoError(); assertTrue(listener.forcedRefresh.get()); seqNoListener = new DummySeqNoListener(); assertTrue(listeners.addOrNotify(index("1").getSeqNo(), seqNoListener)); assertTrue(seqNoListener.isDone.get()); assertEquals(0, listeners.pendingLocationListenerCount()); assertEquals(0, listeners.pendingSeqNoListenersCount()); try (Releasable releaseable2 = listeners.forceRefreshes()) { listener = new DummyRefreshListener(); assertTrue(listeners.addOrNotify(index("1").getTranslogLocation(), listener)); assertTrue(listener.forcedRefresh.get()); listener.assertNoError(); seqNoListener = new DummySeqNoListener(); assertTrue(listeners.addOrNotify(index("1").getSeqNo(), seqNoListener)); assertTrue(seqNoListener.isDone.get()); assertEquals(0, listeners.pendingLocationListenerCount()); assertEquals(0, listeners.pendingSeqNoListenersCount()); } listener = new DummyRefreshListener(); assertTrue(listeners.addOrNotify(index("1").getTranslogLocation(), listener)); assertTrue(listener.forcedRefresh.get()); listener.assertNoError(); seqNoListener = new DummySeqNoListener(); assertTrue(listeners.addOrNotify(index("1").getSeqNo(), seqNoListener)); assertTrue(seqNoListener.isDone.get()); assertEquals(0, listeners.pendingLocationListenerCount()); assertEquals(0, listeners.pendingSeqNoListenersCount()); } assertFalse(listeners.addOrNotify(index("1").getTranslogLocation(), new DummyRefreshListener())); assertFalse(listeners.addOrNotify(index("1").getSeqNo(), new DummySeqNoListener())); assertEquals(1, listeners.pendingLocationListenerCount()); assertEquals(1, listeners.pendingSeqNoListenersCount()); }	this line is the same as 3 lines above?
public void testWaitOnRefresh() throws Exception { createIndex("index"); final SearchService service = getInstanceFromNode(SearchService.class); final IndicesService indicesService = getInstanceFromNode(IndicesService.class); final IndexService indexService = indicesService.indexServiceSafe(resolveIndex("index")); final IndexShard indexShard = indexService.getShard(0); SearchRequest searchRequest = new SearchRequest().allowPartialSearchResults(true); SearchShardTask task = new SearchShardTask(123L, "", "", "", null, Collections.emptyMap()); PlainActionFuture<SearchPhaseResult> future = PlainActionFuture.newFuture(); ShardSearchRequest request = new ShardSearchRequest(OriginalIndices.NONE, searchRequest, indexShard.shardId(), 0, 1, new AliasFilter(null, Strings.EMPTY_ARRAY), 1.0f, -1, null, null, null, 0); request.source(SearchSourceBuilder.searchSource().timeout(TimeValue.timeValueSeconds(30))); service.executeQueryPhase(request, task, future); final IndexResponse response = client().prepareIndex("index").setSource("id", "1").get(); assertEquals(RestStatus.CREATED, response.status()); searchRequest.indices("index").source(new SearchSourceBuilder().query(new TermQueryBuilder("id", "1"))); SearchPhaseResult searchPhaseResult = future.actionGet(); assertEquals(1, searchPhaseResult.queryResult().getTotalHits().value); }	this line looks like it belongs somewhat higher up, before calling executequeryphase?
public void testAuthenticateWithWrongToken() throws IOException { final RestHighLevelClient restClient = new TestRestHighLevelClient(); CreateTokenResponse response = restClient.security().createToken(CreateTokenRequest.passwordGrant( SecuritySettingsSource.TEST_USER_NAME, SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()), SECURITY_REQUEST_OPTIONS); assertNotNull(response.getRefreshToken()); // First check that the correct access token works by getting cluster health with token assertNoTimeout(client() .filterWithHeader(Collections.singletonMap("Authorization", "Bearer " + response.getAccessToken())) .admin().cluster().prepareHealth().get()); // Now attempt to authenticate with an invalid access token string RequestOptions wrongAuthOptions = RequestOptions.DEFAULT.toBuilder().addHeader("Authorization", "Bearer " + randomAlphaOfLengthBetween(0, 128)).build(); ElasticsearchStatusException e = expectThrows(ElasticsearchStatusException.class, () -> restClient.security().authenticate(wrongAuthOptions)); assertEquals(RestStatus.UNAUTHORIZED, e.status()); }	i was expecting you'd invalidate the created token and try another time with it, rather than ~creating a new~ using a random string. we have a few formats (lengths) for tokens. if you wish to test some/most of them, i would suggest we craft them on purpose, so that we cover the branches in decodetoken. i think a long random string is insufficient to cover for "invalid" tokens, generally. if you only test the rest response code, which i believe you are aiming for, a random string is enough (no need to create another token).
public BytesReference cacheKey(CheckedBiConsumer<ShardSearchRequest, StreamOutput, IOException> differentiator) throws IOException { BytesStreamOutput out = scratch.get(); try { this.innerWriteTo(out, true); if (differentiator != null) { differentiator.accept(this, out); } return new BytesArray(getHashedCacheKey(out.bytes())); } finally { out.reset(); } }	i think we can use messagedigests.digest(out.bytes(), messagedigests.sha256()) here rather than adding a new method?
public void testTransportRespondsEventually() throws InterruptedException { internalCluster().setBootstrapMasterNodeIndex(0); internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(3, 5)); final NetworkDisruption.DisruptedLinks disruptedLinks; if (randomBoolean()) { disruptedLinks = TwoPartitions.random(random(), internalCluster().getNodeNames()); } else { disruptedLinks = NetworkDisruption.Bridge.random(random(), internalCluster().getNodeNames()); } NetworkDisruption networkDisruption = new NetworkDisruption(disruptedLinks, randomFrom(new NetworkDisruption.NetworkUnresponsive(), new NetworkDisruption.NetworkDisconnect(), NetworkDisruption.NetworkDelay.random(random()))); internalCluster().setDisruptionScheme(networkDisruption); networkDisruption.startDisrupting(); CountDownLatch latch = new CountDownLatch(100); for (int i = 0; i < 100; ++i) { TransportService transportService = internalCluster().getInstance(TransportService.class); TransportService target = internalCluster().getInstance(TransportService.class); transportService.sendRequest(target.getLocalNode(), ClusterHealthAction.NAME, new ClusterHealthRequest(), new TransportResponseHandler<>() { @Override public void handleResponse(TransportResponse response) { latch.countDown(); } @Override public void handleException(TransportException exp) { latch.countDown(); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public TransportResponse read(StreamInput in) throws IOException { return ClusterHealthResponse.readResponseFrom(in); } }); } // give a bit of time to send something under disruption. latch.await(500, TimeUnit.MILLISECONDS); networkDisruption.stopDisrupting(); latch.await(30, TimeUnit.SECONDS); assertEquals("All requests must respond", 0, latch.getCount()); }	suggest renaming this to source since there's another transportservice in play.
public void testTransportRespondsEventually() throws InterruptedException { internalCluster().setBootstrapMasterNodeIndex(0); internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(3, 5)); final NetworkDisruption.DisruptedLinks disruptedLinks; if (randomBoolean()) { disruptedLinks = TwoPartitions.random(random(), internalCluster().getNodeNames()); } else { disruptedLinks = NetworkDisruption.Bridge.random(random(), internalCluster().getNodeNames()); } NetworkDisruption networkDisruption = new NetworkDisruption(disruptedLinks, randomFrom(new NetworkDisruption.NetworkUnresponsive(), new NetworkDisruption.NetworkDisconnect(), NetworkDisruption.NetworkDelay.random(random()))); internalCluster().setDisruptionScheme(networkDisruption); networkDisruption.startDisrupting(); CountDownLatch latch = new CountDownLatch(100); for (int i = 0; i < 100; ++i) { TransportService transportService = internalCluster().getInstance(TransportService.class); TransportService target = internalCluster().getInstance(TransportService.class); transportService.sendRequest(target.getLocalNode(), ClusterHealthAction.NAME, new ClusterHealthRequest(), new TransportResponseHandler<>() { @Override public void handleResponse(TransportResponse response) { latch.countDown(); } @Override public void handleException(TransportException exp) { latch.countDown(); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public TransportResponse read(StreamInput in) throws IOException { return ClusterHealthResponse.readResponseFrom(in); } }); } // give a bit of time to send something under disruption. latch.await(500, TimeUnit.MILLISECONDS); networkDisruption.stopDisrupting(); latch.await(30, TimeUnit.SECONDS); assertEquals("All requests must respond", 0, latch.getCount()); }	i like to clarify that we're not double-counting anything when using a countdownlatch. how about something like this for an atomicboolean responded: suggestion asserttrue(responded.compareandset(false, true)); latch.countdown();
public void testTransportRespondsEventually() throws InterruptedException { internalCluster().setBootstrapMasterNodeIndex(0); internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(3, 5)); final NetworkDisruption.DisruptedLinks disruptedLinks; if (randomBoolean()) { disruptedLinks = TwoPartitions.random(random(), internalCluster().getNodeNames()); } else { disruptedLinks = NetworkDisruption.Bridge.random(random(), internalCluster().getNodeNames()); } NetworkDisruption networkDisruption = new NetworkDisruption(disruptedLinks, randomFrom(new NetworkDisruption.NetworkUnresponsive(), new NetworkDisruption.NetworkDisconnect(), NetworkDisruption.NetworkDelay.random(random()))); internalCluster().setDisruptionScheme(networkDisruption); networkDisruption.startDisrupting(); CountDownLatch latch = new CountDownLatch(100); for (int i = 0; i < 100; ++i) { TransportService transportService = internalCluster().getInstance(TransportService.class); TransportService target = internalCluster().getInstance(TransportService.class); transportService.sendRequest(target.getLocalNode(), ClusterHealthAction.NAME, new ClusterHealthRequest(), new TransportResponseHandler<>() { @Override public void handleResponse(TransportResponse response) { latch.countDown(); } @Override public void handleException(TransportException exp) { latch.countDown(); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public TransportResponse read(StreamInput in) throws IOException { return ClusterHealthResponse.readResponseFrom(in); } }); } // give a bit of time to send something under disruption. latch.await(500, TimeUnit.MILLISECONDS); networkDisruption.stopDisrupting(); latch.await(30, TimeUnit.SECONDS); assertEquals("All requests must respond", 0, latch.getCount()); }	is there a way we could assert that this _does_ time out? suggestion assertfalse(latch.await(500, timeunit.milliseconds)); however this won't work if none the messages we send are across disrupted links. can we try and ensure that there is always at least one disrupted message?
public void testTransportRespondsEventually() throws InterruptedException { internalCluster().setBootstrapMasterNodeIndex(0); internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(3, 5)); final NetworkDisruption.DisruptedLinks disruptedLinks; if (randomBoolean()) { disruptedLinks = TwoPartitions.random(random(), internalCluster().getNodeNames()); } else { disruptedLinks = NetworkDisruption.Bridge.random(random(), internalCluster().getNodeNames()); } NetworkDisruption networkDisruption = new NetworkDisruption(disruptedLinks, randomFrom(new NetworkDisruption.NetworkUnresponsive(), new NetworkDisruption.NetworkDisconnect(), NetworkDisruption.NetworkDelay.random(random()))); internalCluster().setDisruptionScheme(networkDisruption); networkDisruption.startDisrupting(); CountDownLatch latch = new CountDownLatch(100); for (int i = 0; i < 100; ++i) { TransportService transportService = internalCluster().getInstance(TransportService.class); TransportService target = internalCluster().getInstance(TransportService.class); transportService.sendRequest(target.getLocalNode(), ClusterHealthAction.NAME, new ClusterHealthRequest(), new TransportResponseHandler<>() { @Override public void handleResponse(TransportResponse response) { latch.countDown(); } @Override public void handleException(TransportException exp) { latch.countDown(); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public TransportResponse read(StreamInput in) throws IOException { return ClusterHealthResponse.readResponseFrom(in); } }); } // give a bit of time to send something under disruption. latch.await(500, TimeUnit.MILLISECONDS); networkDisruption.stopDisrupting(); latch.await(30, TimeUnit.SECONDS); assertEquals("All requests must respond", 0, latch.getCount()); }	i think we should assert that this doesn't time out: suggestion asserttrue("all requests completed", latch.await(30, timeunit.seconds));
public void testTransportRespondsEventually() throws InterruptedException { internalCluster().setBootstrapMasterNodeIndex(0); internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(3, 5)); final NetworkDisruption.DisruptedLinks disruptedLinks; if (randomBoolean()) { disruptedLinks = TwoPartitions.random(random(), internalCluster().getNodeNames()); } else { disruptedLinks = NetworkDisruption.Bridge.random(random(), internalCluster().getNodeNames()); } NetworkDisruption networkDisruption = new NetworkDisruption(disruptedLinks, randomFrom(new NetworkDisruption.NetworkUnresponsive(), new NetworkDisruption.NetworkDisconnect(), NetworkDisruption.NetworkDelay.random(random()))); internalCluster().setDisruptionScheme(networkDisruption); networkDisruption.startDisrupting(); CountDownLatch latch = new CountDownLatch(100); for (int i = 0; i < 100; ++i) { TransportService transportService = internalCluster().getInstance(TransportService.class); TransportService target = internalCluster().getInstance(TransportService.class); transportService.sendRequest(target.getLocalNode(), ClusterHealthAction.NAME, new ClusterHealthRequest(), new TransportResponseHandler<>() { @Override public void handleResponse(TransportResponse response) { latch.countDown(); } @Override public void handleException(TransportException exp) { latch.countDown(); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public TransportResponse read(StreamInput in) throws IOException { return ClusterHealthResponse.readResponseFrom(in); } }); } // give a bit of time to send something under disruption. latch.await(500, TimeUnit.MILLISECONDS); networkDisruption.stopDisrupting(); latch.await(30, TimeUnit.SECONDS); assertEquals("All requests must respond", 0, latch.getCount()); }	... and then we don't need this. suggestion
protected void doExecute(Task task, GetCalendarsAction.Request request, ActionListener<GetCalendarsAction.Response> listener) { final String[] calendarIds = Strings.splitStringByCommaToArray(request.getCalendarId()); PageParams pageParams = request.getPageParams(); if (pageParams == null) { pageParams = PageParams.defaultParams(); } getCalendars(calendarIds, pageParams, listener); }	the get request docs will need updating to add the standard description of a wildcard-able id parameter "comma-separated list of calendar ids and wildcard expressions. you can get all by using _all, or * or by omitting the id"
@Override public void setScorer(Scorer s) { scorer = s; try { // We have a new binding for the scorer so we need to reset the values values = source.getValues(leaf, new DoubleValues() { @Override public double doubleValue() throws IOException { return scorer.score(); } @Override public boolean advanceExact(int doc) throws IOException { return true; } }); } catch (IOException e) { throw new IllegalStateException("Can't get values using " + compiledScript, e); } }	you could reuse doublevaluessource.fromscorer(scorer) here
public DockerRun uid(Integer uid, Integer gid) { if (uid == null) { if (gid != null) { throw new IllegalArgumentException("Cannot override GID without also overriding UID"); } } this.uid = uid; this.gid = gid; return this; }	the intention here was to then specifically add a docker packaging test which leverages this to set the container memory limit and assert that the auto configured heap is as we'd expect.
public static void maybeConfigure(TaskContainer tasks, String name, Action<? super Task> config) { tasks.matching(t -> t.getName().equals(name)).configureEach( t-> config.execute(t)); }	calling matching here doesn't eagerly resolve all tasks? wouldn't it have to if the predicate filtered on anything other than name?
public static VotingConfiguration of(DiscoveryNode... nodes) { // this could be used in many more places - TODO use this where appropriate return new VotingConfiguration(Arrays.stream(nodes).map(DiscoveryNode::getId).collect(Collectors.toSet())); }	this method (and the other describe methods) are without context in their respective classes. i would prefer to have the full construction of the output in warnclusterformationfailed
@SuppressWarnings("unchecked") private static void setupData(CommonEqlActionTestCase tc) throws Exception { if (isSetUp) { return; } CreateIndexRequest request = new CreateIndexRequest(testIndexName) .mapping(Streams.readFully(CommonEqlActionTestCase.class.getResourceAsStream("/mapping-default.json")), XContentType.JSON); tc.highLevelClient().indices().create(request, RequestOptions.DEFAULT); BulkRequest bulk = new BulkRequest(); bulk.setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); try (XContentParser parser = tc.createParser(JsonXContent.jsonXContent, CommonEqlActionTestCase.class.getResourceAsStream("/test_data.json"))) { List<Object> list = parser.list(); for (Object item : list) { assertThat(item, instanceOf(HashMap.class)); HashMap<String, Object> entry = (HashMap<String, Object>) item; // Adjust the structure of the document with additional event.category and @timestamp fields // Add event.category field HashMap<String, Object> objEvent = new HashMap<>(); objEvent.put("category", entry.get("event_type")); entry.put("event", objEvent); // Add @timestamp field entry.put("@timestamp", entry.get("timestamp")); bulk.add(new IndexRequest(testIndexName).source(entry, XContentType.JSON)); } } if (bulk.numberOfActions() > 0) { BulkResponse bulkResponse = tc.highLevelClient().bulk(bulk, RequestOptions.DEFAULT); assertEquals(RestStatus.OK, bulkResponse.status()); assertFalse(bulkResponse.hasFailures()); isSetUp = true; } }	needed mapping to ip type, this creating the index with mapping.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { assert index != null; assert type != null; assert id != null; builder.field(FieldStrings._INDEX, index); builder.field(FieldStrings._TYPE, type); builder.field(FieldStrings._ID, id); builder.field(FieldStrings._VERSION, docVersion); builder.field(FieldStrings.FOUND, isExists()); if (!isExists()) { return builder; } builder.startObject(FieldStrings.TERM_VECTORS); final CharsRef spare = new CharsRef(); Fields theFields = getFields(); Iterator<String> fieldIter = theFields.iterator(); while (fieldIter.hasNext()) { buildField(builder, spare, theFields, fieldIter); } builder.endObject(); return builder; }	was the additional endobject causing a bug? if it's not needed anymore it means it closed an object one too many times before this fix?
public void testRepeatCleanupsDontRemove() throws Exception { final ActionFuture<CleanupRepositoryResponse> cleanupFuture = startBlockedCleanup("test-repo"); logger.info("--> sending another cleanup"); assertFutureThrows(client().admin().cluster().prepareCleanupRepository("test-repo").execute(), IllegalStateException.class); logger.info("--> ensure cleanup is still in progress"); final RepositoryCleanupInProgress cleanup = client().admin().cluster().prepareState().get().getState().custom(RepositoryCleanupInProgress.TYPE); assertTrue(cleanup.hasCleanupInProgress()); logger.info("--> unblocking master node"); unblockNode("test-repo", internalCluster().getMasterName()); logger.info("--> wait for cleanup to finish and disappear from cluster state"); awaitClusterState(state -> state.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY).hasCleanupInProgress() == false); try { cleanupFuture.get(); } catch (ExecutionException e) { // ignored and expected } }	can't we be more specific about the exception that we expect here?
public void testWaitOnIndexReady() throws Exception { final GetGlobalCheckpointsAction.Request request = new GetGlobalCheckpointsAction.Request( "not-yet-existing", false, EMPTY_ARRAY, TEN_SECONDS ); long start = System.nanoTime(); ActionFuture<GetGlobalCheckpointsAction.Response> future = client().execute(GetGlobalCheckpointsAction.INSTANCE, request); Thread.sleep(randomIntBetween(10, 100)); client().admin() .indices() .prepareCreate("not-yet-existing") .setSettings( Settings.builder() .put(IndexSettings.INDEX_TRANSLOG_DURABILITY_SETTING.getKey(), Translog.Durability.REQUEST) .put("index.number_of_shards", 1) .put("index.number_of_replicas", 0) ) .get(); GetGlobalCheckpointsAction.Response response = future.actionGet(); long elapsed = TimeValue.timeValueNanos(System.nanoTime() - start).seconds(); assertThat(elapsed, lessThan(30L)); assertFalse(response.timedOut()); }	can we also add a test that validates that if the shard becomes available within the timeout, it is succesful? i.e., must like this test, just remove the include.node setting while the request is going on.
protected void doExecute(Task task, Request request, ActionListener<Response> listener) { final ClusterState state = clusterService.state(); final Index index; try { index = resolver.concreteSingleIndex(state, request); } catch (IndexNotFoundException e) { handleIndexNotReady(request, listener); return; } final IndexMetadata indexMetadata = state.getMetadata().index(index); final IndexRoutingTable routingTable = state.routingTable().index(index); if (routingTable.allPrimaryShardsActive()) { new CheckpointFetcher(client, request, listener, indexMetadata, request.timeout()).run(); } else { handleIndexNotReady(request, listener); } }	should we fail out immediately if there are no wait_for_xxx options specified? see also comment below.
protected void doExecute(Task task, Request request, ActionListener<Response> listener) { final ClusterState state = clusterService.state(); final Index index; try { index = resolver.concreteSingleIndex(state, request); } catch (IndexNotFoundException e) { handleIndexNotReady(request, listener); return; } final IndexMetadata indexMetadata = state.getMetadata().index(index); final IndexRoutingTable routingTable = state.routingTable().index(index); if (routingTable.allPrimaryShardsActive()) { new CheckpointFetcher(client, request, listener, indexMetadata, request.timeout()).run(); } else { handleIndexNotReady(request, listener); } }	should this also check the waitforadvance flag (or another waitforyellow flag) to still allow a get checkpoints that just returns the available checkpoints regardless of primary failures. i think we should strive to return current state immediately if there are no wait_for_xxx options enabled on the request.
*/ public boolean awaitClose(long timeout, TimeUnit unit) throws InterruptedException { try { lock.lock(); if (closed) { return true; } closed = true; this.cancellableFlushTask.cancel(); if (bulkRequest.numberOfActions() > 0) { execute(); } try { return this.bulkRequestHandler.awaitClose(timeout, unit); } finally { onClose.run(); } }finally { lock.unlock(); } } /** * Adds an {@link IndexRequest} to the list of actions to execute. Follows the same behavior of {@link IndexRequest}	i did some quick searching here because im just not too familiar w/ this stuff, and it looks like most of the time the lock.lock() is outside of the try, which i assume is to throw the interruptedexception, rather than to execute the lock.unlock() before throwing. should the lock.lock() be in the try ?
public void testConcurrentExecutions() throws Exception { final AtomicBoolean called = new AtomicBoolean(false); int estimatedTimeForTest = Integer.MAX_VALUE; final int simulateWorkTimeInMillis = 5; int concurrentClients= 0; int concurrentBulkRequests = 0; int expectedExecutions= 0; int maxBatchSize = 0; int maxDocuments = 0; int iterations = 0; //find some randoms that allow this test to take under ~ 10 seconds while (estimatedTimeForTest > 10_000) { if (iterations++ > 1_000) { fail("failed to find random values that allows test to run quickly"); //extremely unlikely } maxBatchSize = randomIntBetween(1, 100); maxDocuments = randomIntBetween(maxBatchSize, 1_000_000); concurrentClients = randomIntBetween(1, 20); concurrentBulkRequests = randomIntBetween(0, 20); expectedExecutions = maxDocuments / maxBatchSize; estimatedTimeForTest = (expectedExecutions * simulateWorkTimeInMillis) / Math.min(concurrentBulkRequests + 1, concurrentClients); } BulkResponse bulkResponse = new BulkResponse(new BulkItemResponse[]{new BulkItemResponse()}, 0); AtomicInteger failureCount = new AtomicInteger(0); AtomicInteger successCount = new AtomicInteger(0); AtomicInteger requestCount = new AtomicInteger(0); AtomicInteger docCount = new AtomicInteger(0); BiConsumer<BulkRequest, ActionListener<BulkResponse>> consumer = (request, listener) -> { try { Thread.sleep(simulateWorkTimeInMillis); //simulate work } catch (InterruptedException e) { failureCount.getAndIncrement(); logger.error("interrupted while sleeping. There is likely something wrong with this test!", e); } listener.onResponse(bulkResponse); }; BulkProcessor bulkProcessor = new BulkProcessor(consumer, BackoffPolicy.noBackoff(), countingListener(requestCount, successCount, failureCount, docCount), concurrentBulkRequests, maxBatchSize, new ByteSizeValue(Integer.MAX_VALUE), null, (command, delay, executor) -> null, () -> called.set(true), BulkRequest::new); ExecutorService executorService = Executors.newFixedThreadPool(concurrentClients); IndexRequest indexRequest = new IndexRequest(); String bulkRequest = "{ \\\\"index\\\\" : { \\\\"_index\\\\" : \\\\"test\\\\", \\\\"_id\\\\" : \\\\"1\\\\" } }\\\\n" + "{ \\\\"field1\\\\" : \\\\"value1\\\\" }\\\\n"; BytesReference bytesReference = BytesReference.fromByteBuffers(new ByteBuffer[]{ByteBuffer.wrap(bulkRequest.getBytes(StandardCharsets.UTF_8))}); List<Future> futures = new ArrayList<>(); for (final AtomicInteger i = new AtomicInteger(0); i.getAndIncrement() < maxDocuments;) { //alternate between ways to add to the bulk processor futures.add(executorService.submit(() -> { if(i.get() % 2 == 0) { bulkProcessor.add(indexRequest); }else{ try { bulkProcessor.add(bytesReference, null, null, XContentType.JSON); } catch (Exception e) { throw new RuntimeException(e); } } })); } for (Future f : futures) { try { f.get(10, TimeUnit.SECONDS); }catch (Exception e){ failureCount.incrementAndGet(); logger.error("failure while getting future", e); } } executorService.shutdown(); executorService.awaitTermination(10, TimeUnit.SECONDS); if (failureCount.get() > 0 || successCount.get() != expectedExecutions || requestCount.get() != successCount.get()) { fail("\\\\nExpected Bulks: " + expectedExecutions + "\\\\n" + "Requested Bulks: " + requestCount.get() + "\\\\n" + "Successful Bulks: " + successCount.get() + "\\\\n" + "Failed Bulks: " + failureCount.get() + "\\\\n" + "Max Documents: " + maxDocuments + "\\\\n" + "Max Batch Size: " + maxBatchSize + "\\\\n" + "Concurrent Clients: " + concurrentClients + "\\\\n" + "Concurrent Bulk Requests: " + concurrentBulkRequests + "\\\\n" ); } bulkProcessor.close(); //count total docs after processor is closed since there may have been partial batches that are flushed on close. assertEquals(docCount.get(), maxDocuments); }	instead of failing the test, use an assumetrue/assumefalse?
public void testConcurrentExecutions() throws Exception { final AtomicBoolean called = new AtomicBoolean(false); int estimatedTimeForTest = Integer.MAX_VALUE; final int simulateWorkTimeInMillis = 5; int concurrentClients= 0; int concurrentBulkRequests = 0; int expectedExecutions= 0; int maxBatchSize = 0; int maxDocuments = 0; int iterations = 0; //find some randoms that allow this test to take under ~ 10 seconds while (estimatedTimeForTest > 10_000) { if (iterations++ > 1_000) { fail("failed to find random values that allows test to run quickly"); //extremely unlikely } maxBatchSize = randomIntBetween(1, 100); maxDocuments = randomIntBetween(maxBatchSize, 1_000_000); concurrentClients = randomIntBetween(1, 20); concurrentBulkRequests = randomIntBetween(0, 20); expectedExecutions = maxDocuments / maxBatchSize; estimatedTimeForTest = (expectedExecutions * simulateWorkTimeInMillis) / Math.min(concurrentBulkRequests + 1, concurrentClients); } BulkResponse bulkResponse = new BulkResponse(new BulkItemResponse[]{new BulkItemResponse()}, 0); AtomicInteger failureCount = new AtomicInteger(0); AtomicInteger successCount = new AtomicInteger(0); AtomicInteger requestCount = new AtomicInteger(0); AtomicInteger docCount = new AtomicInteger(0); BiConsumer<BulkRequest, ActionListener<BulkResponse>> consumer = (request, listener) -> { try { Thread.sleep(simulateWorkTimeInMillis); //simulate work } catch (InterruptedException e) { failureCount.getAndIncrement(); logger.error("interrupted while sleeping. There is likely something wrong with this test!", e); } listener.onResponse(bulkResponse); }; BulkProcessor bulkProcessor = new BulkProcessor(consumer, BackoffPolicy.noBackoff(), countingListener(requestCount, successCount, failureCount, docCount), concurrentBulkRequests, maxBatchSize, new ByteSizeValue(Integer.MAX_VALUE), null, (command, delay, executor) -> null, () -> called.set(true), BulkRequest::new); ExecutorService executorService = Executors.newFixedThreadPool(concurrentClients); IndexRequest indexRequest = new IndexRequest(); String bulkRequest = "{ \\\\"index\\\\" : { \\\\"_index\\\\" : \\\\"test\\\\", \\\\"_id\\\\" : \\\\"1\\\\" } }\\\\n" + "{ \\\\"field1\\\\" : \\\\"value1\\\\" }\\\\n"; BytesReference bytesReference = BytesReference.fromByteBuffers(new ByteBuffer[]{ByteBuffer.wrap(bulkRequest.getBytes(StandardCharsets.UTF_8))}); List<Future> futures = new ArrayList<>(); for (final AtomicInteger i = new AtomicInteger(0); i.getAndIncrement() < maxDocuments;) { //alternate between ways to add to the bulk processor futures.add(executorService.submit(() -> { if(i.get() % 2 == 0) { bulkProcessor.add(indexRequest); }else{ try { bulkProcessor.add(bytesReference, null, null, XContentType.JSON); } catch (Exception e) { throw new RuntimeException(e); } } })); } for (Future f : futures) { try { f.get(10, TimeUnit.SECONDS); }catch (Exception e){ failureCount.incrementAndGet(); logger.error("failure while getting future", e); } } executorService.shutdown(); executorService.awaitTermination(10, TimeUnit.SECONDS); if (failureCount.get() > 0 || successCount.get() != expectedExecutions || requestCount.get() != successCount.get()) { fail("\\\\nExpected Bulks: " + expectedExecutions + "\\\\n" + "Requested Bulks: " + requestCount.get() + "\\\\n" + "Successful Bulks: " + successCount.get() + "\\\\n" + "Failed Bulks: " + failureCount.get() + "\\\\n" + "Max Documents: " + maxDocuments + "\\\\n" + "Max Batch Size: " + maxBatchSize + "\\\\n" + "Concurrent Clients: " + concurrentClients + "\\\\n" + "Concurrent Bulk Requests: " + concurrentBulkRequests + "\\\\n" ); } bulkProcessor.close(); //count total docs after processor is closed since there may have been partial batches that are flushed on close. assertEquals(docCount.get(), maxDocuments); }	i am not sure this is the best way to test this. ideally you want all of your threads ready to go prior to starting work. usually we use a cyclic barrier or countdown latch so that we know all threads are ready prior to execution of the concurrent tasks. an example use of a cyclic barrier would be https://github.com/elastic/elasticsearch/blob/0531987a0466fd7e4af367c76cf6b8911b4df51c/server/src/test/java/org/elasticsearch/common/cache/cachetests.java#l843-l878.
public void testConcurrentExecutions() throws Exception { final AtomicBoolean called = new AtomicBoolean(false); int estimatedTimeForTest = Integer.MAX_VALUE; final int simulateWorkTimeInMillis = 5; int concurrentClients= 0; int concurrentBulkRequests = 0; int expectedExecutions= 0; int maxBatchSize = 0; int maxDocuments = 0; int iterations = 0; //find some randoms that allow this test to take under ~ 10 seconds while (estimatedTimeForTest > 10_000) { if (iterations++ > 1_000) { fail("failed to find random values that allows test to run quickly"); //extremely unlikely } maxBatchSize = randomIntBetween(1, 100); maxDocuments = randomIntBetween(maxBatchSize, 1_000_000); concurrentClients = randomIntBetween(1, 20); concurrentBulkRequests = randomIntBetween(0, 20); expectedExecutions = maxDocuments / maxBatchSize; estimatedTimeForTest = (expectedExecutions * simulateWorkTimeInMillis) / Math.min(concurrentBulkRequests + 1, concurrentClients); } BulkResponse bulkResponse = new BulkResponse(new BulkItemResponse[]{new BulkItemResponse()}, 0); AtomicInteger failureCount = new AtomicInteger(0); AtomicInteger successCount = new AtomicInteger(0); AtomicInteger requestCount = new AtomicInteger(0); AtomicInteger docCount = new AtomicInteger(0); BiConsumer<BulkRequest, ActionListener<BulkResponse>> consumer = (request, listener) -> { try { Thread.sleep(simulateWorkTimeInMillis); //simulate work } catch (InterruptedException e) { failureCount.getAndIncrement(); logger.error("interrupted while sleeping. There is likely something wrong with this test!", e); } listener.onResponse(bulkResponse); }; BulkProcessor bulkProcessor = new BulkProcessor(consumer, BackoffPolicy.noBackoff(), countingListener(requestCount, successCount, failureCount, docCount), concurrentBulkRequests, maxBatchSize, new ByteSizeValue(Integer.MAX_VALUE), null, (command, delay, executor) -> null, () -> called.set(true), BulkRequest::new); ExecutorService executorService = Executors.newFixedThreadPool(concurrentClients); IndexRequest indexRequest = new IndexRequest(); String bulkRequest = "{ \\\\"index\\\\" : { \\\\"_index\\\\" : \\\\"test\\\\", \\\\"_id\\\\" : \\\\"1\\\\" } }\\\\n" + "{ \\\\"field1\\\\" : \\\\"value1\\\\" }\\\\n"; BytesReference bytesReference = BytesReference.fromByteBuffers(new ByteBuffer[]{ByteBuffer.wrap(bulkRequest.getBytes(StandardCharsets.UTF_8))}); List<Future> futures = new ArrayList<>(); for (final AtomicInteger i = new AtomicInteger(0); i.getAndIncrement() < maxDocuments;) { //alternate between ways to add to the bulk processor futures.add(executorService.submit(() -> { if(i.get() % 2 == 0) { bulkProcessor.add(indexRequest); }else{ try { bulkProcessor.add(bytesReference, null, null, XContentType.JSON); } catch (Exception e) { throw new RuntimeException(e); } } })); } for (Future f : futures) { try { f.get(10, TimeUnit.SECONDS); }catch (Exception e){ failureCount.incrementAndGet(); logger.error("failure while getting future", e); } } executorService.shutdown(); executorService.awaitTermination(10, TimeUnit.SECONDS); if (failureCount.get() > 0 || successCount.get() != expectedExecutions || requestCount.get() != successCount.get()) { fail("\\\\nExpected Bulks: " + expectedExecutions + "\\\\n" + "Requested Bulks: " + requestCount.get() + "\\\\n" + "Successful Bulks: " + successCount.get() + "\\\\n" + "Failed Bulks: " + failureCount.get() + "\\\\n" + "Max Documents: " + maxDocuments + "\\\\n" + "Max Batch Size: " + maxBatchSize + "\\\\n" + "Concurrent Clients: " + concurrentClients + "\\\\n" + "Concurrent Bulk Requests: " + concurrentBulkRequests + "\\\\n" ); } bulkProcessor.close(); //count total docs after processor is closed since there may have been partial batches that are flushed on close. assertEquals(docCount.get(), maxDocuments); }	typically we avoid things like since timing could get messed up by a slow machine or gc pause. we can just use future#get
@Override public QuerySearchResult queryResult() { return queryResult; }	looks like if you do indexservice.mapperservice() you can go ahead and remove mapperservice() from searchcontext.
public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (privileges.isEmpty()) { validationException = addValidationError("At least one application privilege must be provided", validationException); } else { for (ApplicationPrivilegeDescriptor privilege : privileges) { try { ApplicationPrivilege.validateApplicationName(privilege.getApplication()); } catch (IllegalArgumentException e) { validationException = addValidationError(e.getMessage(), validationException); } try { ApplicationPrivilege.validatePrivilegeName(privilege.getName()); } catch (IllegalArgumentException e) { validationException = addValidationError(e.getMessage(), validationException); } if (privilege.getActions().isEmpty()) { validationException = addValidationError("Application privileges must have at least one action", validationException); } for (String action : privilege.getActions()) { try { ApplicationPrivilege.validateActionName(action); } catch (IllegalArgumentException e) { validationException = addValidationError(e.getMessage(), validationException); } } if (MetadataUtils.containsReservedMetadata(privilege.getMetadata())) { validationException = addValidationError( "metadata keys may not start with [" + MetadataUtils.RESERVED_PREFIX + "] (in privilege " + privilege.getApplication() + ' ' + privilege.getName() + ")", validationException ); } } } return validationException; } /** * Should this request trigger a refresh ({@linkplain RefreshPolicy#IMMEDIATE}, the default), wait for a refresh ( * {@linkplain RefreshPolicy#WAIT_UNTIL}), or proceed ignore refreshes entirely ({@linkplain RefreshPolicy#NONE}	i found it confusing to have a method called validateactionname which does validation on action name, but then also calls through to validateprivilegeoractionname.
*/ public static void validatePrivilegeName(String name) { if (isValidPrivilegeName(name) == false) { throw new IllegalArgumentException( "Application privilege names must match the pattern " + VALID_NAME.pattern() + " (found '" + name + "')" ); } }	does it make sense for a new validateactionname method to call a new isvalidactionname method? for example, there is an existing validateprivilegename method which calls isvalidprivilegename. does it make sense to follow that pattern? in other words, i found it confusing to have a method called validateactionname which does validation on action name, but then also calls through to validateprivilegeoractionname.
public void setInitialConfiguration(final VotingConfiguration votingConfiguration) { synchronized (mutex) { final ClusterState currentState = getStateForMasterService(); if (isInitialConfigurationSet()) { throw new CoordinationStateRejectedException("Cannot set initial configuration: configuration has already been set"); } assert currentState.term() == 0 : currentState; assert currentState.version() == 0 : currentState; if (mode != Mode.CANDIDATE) { throw new CoordinationStateRejectedException("Cannot set initial configuration in mode " + mode); } final List<DiscoveryNode> knownNodes = new ArrayList<>(); knownNodes.add(getLocalNode()); peerFinder.getFoundPeers().forEach(knownNodes::add); if (votingConfiguration.hasQuorum(knownNodes.stream().map(DiscoveryNode::getId).collect(Collectors.toList())) == false) { throw new CoordinationStateRejectedException("not enough nodes discovered to form a quorum in the initial configuration " + "[knownNodes=" + knownNodes + ", " + votingConfiguration + "]"); } logger.info("setting initial configuration to {}", votingConfiguration); final Builder builder = masterService.incrementVersion(currentState); builder.lastAcceptedConfiguration(votingConfiguration); builder.lastCommittedConfiguration(votingConfiguration); MetaData.Builder metaDataBuilder = MetaData.builder(); // automatically generate a UID for the metadata if we need to metaDataBuilder.generateClusterUuidIfNeeded(); // TODO generate UUID in bootstrapping tool? metaDataBuilder.persistentSettings(Settings.builder().put(CLUSTER_MASTER_NODES_FAILURE_TOLERANCE.getKey(), (votingConfiguration.getNodeIds().size() - 1) / 2).build()); // TODO set this in bootstrapping tool? builder.metaData(metaDataBuilder); coordinationState.get().setInitialState(builder.build()); preVoteCollector.update(getPreVoteResponse(), null); // pick up the change to last-accepted version startElectionScheduler(); } }	this is safe to extract because getstateformasterservice depends only on things protected by mutex.
*/ private boolean handleWakeUp() { assert holdsLock() : "PeerFinder mutex not held"; boolean peersRemoved = false; for (final Peer peer : peersByAddress.values()) { peersRemoved = peer.handleWakeUp() || peersRemoved; // care: avoid short-circuiting, each peer needs waking up } if (active == false) { logger.trace("not active"); return peersRemoved; } logger.trace("probing master nodes from cluster state: {}", lastAcceptedNodes); for (ObjectCursor<DiscoveryNode> discoveryNodeObjectCursor : lastAcceptedNodes.getMasterNodes().values()) { startProbe(discoveryNodeObjectCursor.value.getAddress()); } configuredHostsResolver.resolveConfiguredHosts(providedAddresses -> { synchronized (mutex) { logger.trace("probing resolved transport addresses {}", providedAddresses); providedAddresses.forEach(this::startProbe); } }); try { transportService.getThreadPool().schedule(findPeersInterval, Names.GENERIC, new AbstractRunnable() { @Override public boolean isForceExecution() { return true; } @Override public void onFailure(Exception e) { assert false : e; logger.debug("unexpected exception in wakeup", e); } @Override protected void doRun() { synchronized (mutex) { if (handleWakeUp() == false) { return; } } onFoundPeersUpdated(); } @Override public String toString() { return "PeerFinder handling wakeup"; } }); } catch (EsRejectedExecutionException e) { if (e.isExecutorShutdown()) { logger.debug("couldn't schedule new execution of peer finder, executor is shutting down", e); } else { throw e; } } return peersRemoved; }	the changes to this file are just for esrejectedexecutionexception-handling, which arose in test failures. no test to verify this specifically is added here.
private boolean clusterStateHasNode(DiscoveryNode node) { return node.equals(MasterServiceTests.discoveryState(masterService).nodes().get(node.getId())); }	this class was extracted to the top level.
public static GetMappingsResponse fromXContent(XContentParser parser) throws IOException { if (parser.currentToken() == null) { parser.nextToken(); } assert parser.currentToken() == XContentParser.Token.START_OBJECT; Map<String, Object> parts = parser.map(); ImmutableOpenMap.Builder<String, ImmutableOpenMap<String, MappingMetaData>> builder = new ImmutableOpenMap.Builder<>(); for (Map.Entry<String, Object> entry : parts.entrySet()) { final String indexName = entry.getKey(); assert entry.getValue() instanceof Map : "expected a map as type mapping, but got: " + entry.getValue().getClass(); ImmutableOpenMap.Builder<String, MappingMetaData> typeBuilder = new ImmutableOpenMap.Builder<>(); @SuppressWarnings("unchecked") final Map<String, Object> fieldMappings = (Map<String, Object>) ((Map<String, ?>) entry.getValue()) .get(MAPPINGS.getPreferredName()); if (fieldMappings.isEmpty() == false) { assert fieldMappings instanceof Map : "expected a map as inner type mapping, but got: " + fieldMappings.getClass(); MappingMetaData mmd = new MappingMetaData(MapperService.SINGLE_MAPPING_NAME, fieldMappings); typeBuilder.put(MapperService.SINGLE_MAPPING_NAME, mmd); } builder.put(indexName, typeBuilder.build()); } return new GetMappingsResponse(builder.build()); }	i'm assuming that we'll revisit the interface to getmappingsresponse for the hlrc, as we plan to do for getindexresponse.
public void testTypeUrlParamerterDeprecation() throws Exception { Map<String, String> params = new HashMap<>(); params.put(INCLUDE_TYPE_NAME_PARAMETER, "true"); RestRequest request = new FakeRestRequest.Builder(xContentRegistry()) .withMethod(RestRequest.Method.GET) .withParams(params) .withPath("some_index/some_type/_mapping/some_field") .build(); RestGetMappingAction handler = new RestGetMappingAction(Settings.EMPTY, mock(RestController.class)); handler.prepareRequest(request, mock(NodeClient.class)); assertWarnings(RestGetMappingAction.TYPES_DEPRECATION_MESSAGE); }	i think we could just do controller().dispatchrequest here, as we do in the test above.
public static void toXContent(IndexMetaData indexMetaData, XContentBuilder builder, ToXContent.Params params) throws IOException { MetaData.XContentContext context = MetaData.XContentContext.valueOf(params.param(CONTEXT_MODE_PARAM, "API")); builder.startObject(indexMetaData.getIndex().getName()); builder.field(KEY_VERSION, indexMetaData.getVersion()); builder.field(KEY_MAPPING_VERSION, indexMetaData.getMappingVersion()); builder.field(KEY_SETTINGS_VERSION, indexMetaData.getSettingsVersion()); builder.field(KEY_ALIASES_VERSION, indexMetaData.getAliasesVersion()); builder.field(KEY_ROUTING_NUM_SHARDS, indexMetaData.getRoutingNumShards()); builder.field(KEY_STATE, indexMetaData.getState().toString().toLowerCase(Locale.ENGLISH)); boolean binary = params.paramAsBoolean("binary", false); builder.startObject(KEY_SETTINGS); if (context != MetaData.XContentContext.API) { indexMetaData.getSettings().toXContent(builder, new MapParams(Collections.singletonMap("flat_settings", "true"))); } else { indexMetaData.getSettings() .toXContent(builder, new MapParams(Collections.singletonMap("flat_settings", params.param("flat_settings")))); } builder.endObject(); if (context != MetaData.XContentContext.API) { builder.startArray(KEY_MAPPINGS); MappingMetaData mmd = indexMetaData.mapping(); if (mmd != null) { if (binary) { builder.value(mmd.source().compressed()); } else { builder.map(XContentHelper.convertToMap(new BytesArray(mmd.source().uncompressed()), true).v2()); } } builder.endArray(); } else { builder.startObject(KEY_MAPPINGS); MappingMetaData mmd = indexMetaData.mapping(); if (mmd != null) { Map<String, Object> mapping = XContentHelper.convertToMap(new BytesArray(mmd.source().uncompressed()), false).v2(); if (mapping.size() == 1 && mapping.containsKey(mmd.type())) { // the type name is the root value, reduce it mapping = (Map<String, Object>) mapping.get(mmd.type()); } builder.field(mmd.type()); builder.map(mapping); } builder.endObject(); } for (ObjectObjectCursor<String, DiffableStringMap> cursor : indexMetaData.customData) { builder.field(cursor.key); builder.map(cursor.value); } if (context != MetaData.XContentContext.API) { builder.startObject(KEY_ALIASES); for (ObjectCursor<AliasMetaData> cursor : indexMetaData.getAliases().values()) { AliasMetaData.Builder.toXContent(cursor.value, builder, params); } builder.endObject(); builder.startArray(KEY_PRIMARY_TERMS); for (int i = 0; i < indexMetaData.getNumberOfShards(); i++) { builder.value(indexMetaData.primaryTerm(i)); } builder.endArray(); } else { builder.startArray(KEY_ALIASES); for (ObjectCursor<String> cursor : indexMetaData.getAliases().keys()) { builder.value(cursor.value); } builder.endArray(); builder.startObject(IndexMetaData.KEY_PRIMARY_TERMS); for (int shard = 0; shard < indexMetaData.getNumberOfShards(); shard++) { builder.field(Integer.toString(shard), indexMetaData.primaryTerm(shard)); } builder.endObject(); } builder.startObject(KEY_IN_SYNC_ALLOCATIONS); for (IntObjectCursor<Set<String>> cursor : indexMetaData.inSyncAllocationIds) { builder.startArray(String.valueOf(cursor.key)); for (String allocationId : cursor.value) { builder.value(allocationId); } builder.endArray(); } builder.endObject(); builder.startObject(KEY_ROLLOVER_INFOS); for (ObjectCursor<RolloverInfo> cursor : indexMetaData.getRolloverInfos().values()) { cursor.value.toXContent(builder, params); } builder.endObject(); builder.endObject(); }	please put the else on the same line as closing bracket (same for other occurrences in this pr)
public static void toXContent(IndexMetaData indexMetaData, XContentBuilder builder, ToXContent.Params params) throws IOException { MetaData.XContentContext context = MetaData.XContentContext.valueOf(params.param(CONTEXT_MODE_PARAM, "API")); builder.startObject(indexMetaData.getIndex().getName()); builder.field(KEY_VERSION, indexMetaData.getVersion()); builder.field(KEY_MAPPING_VERSION, indexMetaData.getMappingVersion()); builder.field(KEY_SETTINGS_VERSION, indexMetaData.getSettingsVersion()); builder.field(KEY_ALIASES_VERSION, indexMetaData.getAliasesVersion()); builder.field(KEY_ROUTING_NUM_SHARDS, indexMetaData.getRoutingNumShards()); builder.field(KEY_STATE, indexMetaData.getState().toString().toLowerCase(Locale.ENGLISH)); boolean binary = params.paramAsBoolean("binary", false); builder.startObject(KEY_SETTINGS); if (context != MetaData.XContentContext.API) { indexMetaData.getSettings().toXContent(builder, new MapParams(Collections.singletonMap("flat_settings", "true"))); } else { indexMetaData.getSettings() .toXContent(builder, new MapParams(Collections.singletonMap("flat_settings", params.param("flat_settings")))); } builder.endObject(); if (context != MetaData.XContentContext.API) { builder.startArray(KEY_MAPPINGS); MappingMetaData mmd = indexMetaData.mapping(); if (mmd != null) { if (binary) { builder.value(mmd.source().compressed()); } else { builder.map(XContentHelper.convertToMap(new BytesArray(mmd.source().uncompressed()), true).v2()); } } builder.endArray(); } else { builder.startObject(KEY_MAPPINGS); MappingMetaData mmd = indexMetaData.mapping(); if (mmd != null) { Map<String, Object> mapping = XContentHelper.convertToMap(new BytesArray(mmd.source().uncompressed()), false).v2(); if (mapping.size() == 1 && mapping.containsKey(mmd.type())) { // the type name is the root value, reduce it mapping = (Map<String, Object>) mapping.get(mmd.type()); } builder.field(mmd.type()); builder.map(mapping); } builder.endObject(); } for (ObjectObjectCursor<String, DiffableStringMap> cursor : indexMetaData.customData) { builder.field(cursor.key); builder.map(cursor.value); } if (context != MetaData.XContentContext.API) { builder.startObject(KEY_ALIASES); for (ObjectCursor<AliasMetaData> cursor : indexMetaData.getAliases().values()) { AliasMetaData.Builder.toXContent(cursor.value, builder, params); } builder.endObject(); builder.startArray(KEY_PRIMARY_TERMS); for (int i = 0; i < indexMetaData.getNumberOfShards(); i++) { builder.value(indexMetaData.primaryTerm(i)); } builder.endArray(); } else { builder.startArray(KEY_ALIASES); for (ObjectCursor<String> cursor : indexMetaData.getAliases().keys()) { builder.value(cursor.value); } builder.endArray(); builder.startObject(IndexMetaData.KEY_PRIMARY_TERMS); for (int shard = 0; shard < indexMetaData.getNumberOfShards(); shard++) { builder.field(Integer.toString(shard), indexMetaData.primaryTerm(shard)); } builder.endObject(); } builder.startObject(KEY_IN_SYNC_ALLOCATIONS); for (IntObjectCursor<Set<String>> cursor : indexMetaData.inSyncAllocationIds) { builder.startArray(String.valueOf(cursor.key)); for (String allocationId : cursor.value) { builder.value(allocationId); } builder.endArray(); } builder.endObject(); builder.startObject(KEY_ROLLOVER_INFOS); for (ObjectCursor<RolloverInfo> cursor : indexMetaData.getRolloverInfos().values()) { cursor.value.toXContent(builder, params); } builder.endObject(); builder.endObject(); }	should we just pass on params instead of just extracting flat_settings?
void finalizeValues() { if (version.isPresent() == false) { throw new IllegalArgumentException("version not specified for jdk [" + name + "]"); } if (platform.isPresent() == false) { throw new IllegalArgumentException("platform not specified for jdk [" + name + "]"); } if (vendor.isPresent() == false) { throw new IllegalArgumentException("vendor not specified for jdk [" + name + "]"); } version.finalizeValue(); platform.finalizeValue(); vendor.finalizeValue(); ; }	maybe just remove this empty statement?
@Override public XContentBuilder toXContent(final XContentBuilder builder, final Params params) throws IOException { builder.startObject(); { builder.field("index", index); if (renamedIndex != null) { builder.field("renamed_index", renamedIndex); } if (indexSettings != null) { builder.startObject("index_settings"); { indexSettings.toXContent(builder, params); } builder.endObject(); } if (ignoreIndexSettings != null) { builder.array("ignore_index_settings", ignoreIndexSettings); } } builder.endObject(); return builder; }	thanks for catching this!
@Override public void close(Mode newMode) { assert closed == false : "CandidateJoinAccumulator closed"; closed = true; if (newMode == Mode.LEADER) { final JoinTask joinTask = new JoinTask(joinRequestAccumulator.entrySet().stream().map(entry -> { final DiscoveryNode discoveryNode = entry.getKey(); final ActionListener<Void> listener = entry.getValue(); return new JoinTask.NodeJoinTask( discoveryNode, joinReasonService.getJoinReason(discoveryNode, Mode.CANDIDATE), listener ); }).collect(Collectors.toList()), true); joinTaskExecutor = joinTaskExecutorGenerator.get(); masterService.submitStateUpdateTask( "elected-as-master ([" + joinTask.nodeCount() + "] nodes joined)", joinTask, ClusterStateTaskConfig.build(Priority.URGENT), joinTaskExecutor ); } else { assert newMode == Mode.FOLLOWER : newMode; joinTaskExecutor = null; joinRequestAccumulator.values() .forEach(joinCallback -> joinCallback.onFailure(new CoordinationStateRejectedException("became follower"))); } // CandidateJoinAccumulator is only closed when becoming leader or follower, otherwise it accumulates all joins received // regardless of term. }	please consider using .tolist() instead of .collect(collectors.tolist()) in new code. it is concise and also produces immutable list
@Override protected void write(WriteScope writeScope) { MethodWriter methodWriter = writeScope.getMethodWriter(); methodWriter.writeStatementOffset(getLocation()); Variable variable = writeScope.defineVariable(exceptionType, symbol); Label jump = new Label(); methodWriter.mark(jump); methodWriter.visitVarInsn(variable.getAsmType().getOpcode(Opcodes.ISTORE), variable.getSlot()); if (blockNode != null) { blockNode.write(writeScope.newTryScope(null, null, null)); } methodWriter.visitTryCatchBlock( writeScope.getTryBeginLabel(), writeScope.getTryEndLabel(), jump, variable.getAsmType().getInternalName()); if (writeScope.getCatchesEndLabel() != null && (blockNode == null || blockNode.doAllEscape() == false)) { methodWriter.goTo(writeScope.getCatchesEndLabel()); } }	why does method stuff things into the write scope to just pull them out? maybe visittrycatchblock can take in a writescope?
private void internalCloseSession(String sessionUUID, boolean throwIfSessionMissing) { final RestoreSession restore; synchronized (this) { restore = onGoingRestores.remove(sessionUUID); if (restore == null) { if (throwIfSessionMissing) { logger.debug("could not close session [{}] because session not found", sessionUUID); throw new IllegalArgumentException("session [" + sessionUUID + "] not found"); } else { return; } } closeSessionListeners.forEach(c -> c.accept(sessionUUID)); HashSet<String> sessions = sessionsForShard.get(restore.indexShard); assert sessions != null : "No session UUIDs for shard even though one [" + sessionUUID + "] is active in ongoing restores"; if (sessions != null) { boolean removed = sessions.remove(sessionUUID); assert removed : "No session found for UUID [" + sessionUUID + "]"; if (sessions.isEmpty()) { sessionsForShard.remove(restore.indexShard); } } } restore.decRef(); }	should we run this outside of the mutex?
public void testGetSessionDoesNotLeakFileIfClosed() throws IOException { Settings settings = Settings.builder().put("index.merge.enabled", false).build(); IndexShard indexShard = newStartedShard(true, settings); for (int i = 0; i < 5; i++) { indexDoc(indexShard, "_doc", Integer.toString(i)); flushShard(indexShard, true); } final String sessionUUID = UUIDs.randomBase64UUID(); restoreSourceService.openSession(sessionUUID, indexShard); ArrayList<StoreFileMetaData> files = new ArrayList<>(); indexShard.snapshotStoreMetadata().forEach(files::add); try (CcrRestoreSourceService.SessionReader sessionReader = restoreSourceService.getSessionReader(sessionUUID)) { sessionReader.readFileBytes(files.get(0).name(), new BytesArray(new byte[10])); } // Request a second file to ensure that original file is not leaked try (CcrRestoreSourceService.SessionReader sessionReader = restoreSourceService.getSessionReader(sessionUUID)) { sessionReader.readFileBytes(files.get(1).name(), new BytesArray(new byte[10])); } restoreSourceService.closeSession(sessionUUID); closeShards(indexShard); // Exception will be thrown if file is not closed. }	why does this full section need to be wrapped into assertbusy?
public void testGetSessionDoesNotLeakFileIfClosed() throws IOException { Settings settings = Settings.builder().put("index.merge.enabled", false).build(); IndexShard indexShard = newStartedShard(true, settings); for (int i = 0; i < 5; i++) { indexDoc(indexShard, "_doc", Integer.toString(i)); flushShard(indexShard, true); } final String sessionUUID = UUIDs.randomBase64UUID(); restoreSourceService.openSession(sessionUUID, indexShard); ArrayList<StoreFileMetaData> files = new ArrayList<>(); indexShard.snapshotStoreMetadata().forEach(files::add); try (CcrRestoreSourceService.SessionReader sessionReader = restoreSourceService.getSessionReader(sessionUUID)) { sessionReader.readFileBytes(files.get(0).name(), new BytesArray(new byte[10])); } // Request a second file to ensure that original file is not leaked try (CcrRestoreSourceService.SessionReader sessionReader = restoreSourceService.getSessionReader(sessionUUID)) { sessionReader.readFileBytes(files.get(1).name(), new BytesArray(new byte[10])); } restoreSourceService.closeSession(sessionUUID); closeShards(indexShard); // Exception will be thrown if file is not closed. }	timing-dependent tests are evil and can easily fail on ci. we have an alternative way to write these tests, so that they are fully reproducible. deterministictaskqueue gives you a fake threadpool that allows for full mocking of timing. see deterministictaskqueuetests.testthreadpoolschedulesperiodicfuturetasks on how to use this.
public static void setUpCacheSettings() { blobCacheMaxLength = new ByteSizeValue(randomLongBetween(64L, 128L), ByteSizeUnit.KB); final Settings.Builder builder = Settings.builder(); // Cold (full copy) cache should be unlimited to not cause evictions builder.put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES)); // Align ranges to match the blob cache max length builder.put(CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(), blobCacheMaxLength); builder.put(CacheService.SNAPSHOT_CACHE_RECOVERY_RANGE_SIZE_SETTING.getKey(), blobCacheMaxLength); // Frozen (shared cache) cache should be large enough to not cause direct reads builder.put(SnapshotsService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), ByteSizeValue.ofMb(128)); // Align ranges to match the blob cache max length builder.put(SnapshotsService.SNAPSHOT_CACHE_REGION_SIZE_SETTING.getKey(), blobCacheMaxLength); builder.put(SnapshotsService.SHARED_CACHE_RANGE_SIZE_SETTING.getKey(), blobCacheMaxLength); builder.put(FrozenCacheService.FROZEN_CACHE_RECOVERY_RANGE_SIZE_SETTING.getKey(), blobCacheMaxLength); cacheSettings = builder.build(); }	i'm trying to randomize these settings.
private void analyzeCompound(final CompilerSettings settings, final Definition definition, final Variables variables) { final ALink last = links.get(links.size() - 1); expression.analyze(settings, definition, variables); if (operation == Operation.MUL) { promote = AnalyzerCaster.promoteNumeric(definition, last.after, expression.actual, true, true); } else if (operation == Operation.DIV) { promote = AnalyzerCaster.promoteNumeric(definition, last.after, expression.actual, true, true); } else if (operation == Operation.REM) { promote = AnalyzerCaster.promoteNumeric(definition, last.after, expression.actual, true, true); } else if (operation == Operation.ADD) { promote = AnalyzerCaster.promoteAdd(definition, last.after, expression.actual); } else if (operation == Operation.SUB) { promote = AnalyzerCaster.promoteNumeric(definition, last.after, expression.actual, true, true); } else if (operation == Operation.LSH) { promote = AnalyzerCaster.promoteNumeric(definition, last.after, false, true); } else if (operation == Operation.RSH) { promote = AnalyzerCaster.promoteNumeric(definition, last.after, false, true); } else if (operation == Operation.USH) { promote = AnalyzerCaster.promoteNumeric(definition, last.after, false, true); } else if (operation == Operation.BWAND) { promote = AnalyzerCaster.promoteXor(definition, last.after, expression.actual); } else if (operation == Operation.XOR) { promote = AnalyzerCaster.promoteXor(definition, last.after, expression.actual); } else if (operation == Operation.BWOR) { promote = AnalyzerCaster.promoteXor(definition, last.after, expression.actual); } else { throw new IllegalStateException(error("Illegal tree structure.")); } if (promote == null) { throw new ClassCastException("Cannot apply compound assignment " + "[" + operation.symbol + "=] to types [" + last.after + "] and [" + expression.actual + "]."); } cat = operation == Operation.ADD && promote.sort == Sort.STRING; if (cat) { if (expression instanceof EBinary && ((EBinary)expression).operation == Operation.ADD && expression.actual.sort == Sort.STRING) { ((EBinary)expression).cat = true; } expression.expected = expression.actual; } else if (operation == Operation.LSH || operation == Operation.RSH || operation == Operation.USH) { expression.expected = definition.intType; expression.explicit = true; } else { expression.expected = promote; } expression = expression.cast(settings, definition, variables); exact = !settings.getNumericOverflow() && (operation == Operation.MUL || operation == Operation.DIV || operation == Operation.REM || operation == Operation.ADD || operation == Operation.SUB); there = AnalyzerCaster.getLegalCast(definition, location, last.after, promote, false); back = AnalyzerCaster.getLegalCast(definition, location, promote, last.after, true); if (last instanceof ADefLink) { final ADefLink lastDef = (ADefLink) last; // Unfortunately, we don't know the real type because we load from DEF and store to DEF! lastDef.storeValueType = last.after; } statement = true; actual = read ? last.after : definition.voidType; }	> for the regular case it should be the same, but moved into the else block hi jack, you see this was moved up to else block!
@Override public void handleException(TransportException exp) { final Throwable rootCause = exp.getRootCause(); assertThat(rootCause, instanceOf(IllegalArgumentException.class)); assertThat(rootCause.getMessage(), equalTo("cluster bootstrapping is not supported by discovery type [zen2]")); countDownLatch.countDown(); }	um, cluster bootstrapping _is only_ supported by discovery type zen2 so this seems wrong. i think the right fix is to pass another discovery type into the transportbootstrapclusteraction constructor above, choosing randomly from zen, single-node and some other random string.
@Override public void handleException(TransportException exp) { final Throwable rootCause = exp.getRootCause(); assertThat(rootCause, instanceOf(IllegalArgumentException.class)); assertThat(rootCause.getMessage(), equalTo("discovered nodes are not exposed by discovery type [zen2]")); countDownLatch.countDown(); }	similarly, discovered nodes _are only_ supported by discovery type zen2 so this seems wrong. i think the right fix is to pass another discovery type into the transportgetdiscoverednodesaction constructor above, choosing randomly from zen, single-node and some other random string.
@Override public TemporalAccessor parse(String input) { if (Strings.isNullOrEmpty(input)) { throw new IllegalArgumentException("cannot parse empty date"); } try { return doParse(input); } catch (DateTimeParseException e) { throw new IllegalArgumentException("failed to parse date field [" + input + "] with format [" + format + "]", e); } }	this means that this method will never throw datetimeparseexception as mentioned in the javadoc of dateformatter#parse. can you please update the javadoc there to reflect this change?
private long parseDateTime(String value, ZoneId timeZone, boolean roundUpIfNoTime) { if (Strings.isNullOrEmpty(value)) { throw new IllegalArgumentException("cannot parse empty date"); } Function<String,TemporalAccessor> formatter = roundUpIfNoTime ? this.roundUpFormatter::parse : this.formatter::parse; try { if (timeZone == null) { return DateFormatters.from(formatter.apply(value)).toInstant().toEpochMilli(); } else { TemporalAccessor accessor = formatter.apply(value); ZoneId zoneId = TemporalQueries.zone().queryFrom(accessor); if (zoneId != null) { timeZone = zoneId; } return DateFormatters.from(accessor).withZoneSameLocal(timeZone).toInstant().toEpochMilli(); } } catch (IllegalArgumentException | DateTimeException e) {; throw new ElasticsearchParseException("failed to parse date field [{}] with format [{}]: [{}]", e, value, format, e.getMessage()); } }	nit: there is an additional unnecessary semicolon at the end of the line.
private long parseDateTime(String value, ZoneId timeZone, boolean roundUpIfNoTime) { if (Strings.isNullOrEmpty(value)) { throw new IllegalArgumentException("cannot parse empty date"); } Function<String,TemporalAccessor> formatter = roundUpIfNoTime ? this.roundUpFormatter::parse : this.formatter::parse; try { if (timeZone == null) { return DateFormatters.from(formatter.apply(value)).toInstant().toEpochMilli(); } else { TemporalAccessor accessor = formatter.apply(value); ZoneId zoneId = TemporalQueries.zone().queryFrom(accessor); if (zoneId != null) { timeZone = zoneId; } return DateFormatters.from(accessor).withZoneSameLocal(timeZone).toInstant().toEpochMilli(); } } catch (IllegalArgumentException | DateTimeException e) {; throw new ElasticsearchParseException("failed to parse date field [{}] with format [{}]: [{}]", e, value, format, e.getMessage()); } }	nit: is there a reason for the additional empty line here?
private synchronized void closeWithTragicEvent(final Exception ex) { if (tragedy.setTragicException(ex)) { try { close(); } catch (final IOException | RuntimeException e) { ex.addSuppressed(e); } } }	why change the semantics here to only call close when setting the tragedy the first time? let's keep the existing semantics and make settragicexception return void
public void testInOnFieldTextWithNoKeyword() { assertEquals("1:26: [IN] cannot operate on field of data type [text]: " + "No keyword/multi-field defined exact matches for [text]; define one or use MATCH/QUERY instead", error("SELECT * FROM test WHERE text IN ('foo', 'bar')")); }	i like this one.
synchronized void startInternal(Client client, Settings settings, String nodeName, String clusterName) throws IOException, InterruptedException { if (process != null) { throw new IllegalStateException("Already started"); } List<String> params = new ArrayList<>(); if (!Constants.WINDOWS) { params.add("bin/elasticsearch"); } else { params.add("bin/elasticsearch.bat"); } params.add("-Des.cluster.name=" + clusterName); params.add("-Des.node.name=" + nodeName); ImmutableSettings.Builder externaNodeSettingsBuilder = ImmutableSettings.builder(); for (Map.Entry<String, String> entry : settings.getAsMap().entrySet()) { switch (entry.getKey()) { case "cluster.name": case "node.name": case "path.home": case "node.mode": case "node.local": case TransportModule.TRANSPORT_TYPE_KEY: case DiscoveryModule.DISCOVERY_TYPE_KEY: case "gateway.type": case TransportModule.TRANSPORT_SERVICE_TYPE_KEY: case "config.ignore_system_properties": continue; default: externaNodeSettingsBuilder.put(entry.getKey(), entry.getValue()); } } this.externalNodeSettings = externaNodeSettingsBuilder.put(REQUIRED_SETTINGS).build(); for (Map.Entry<String, String> entry : externalNodeSettings.getAsMap().entrySet()) { params.add("-Des." + entry.getKey() + "=" + entry.getValue()); } params.add("-Des.path.home=" + new File("").getAbsolutePath()); params.add("-Des.path.conf=" + path + "/config"); ProcessBuilder builder = new ProcessBuilder(params); builder.directory(path); builder.inheritIO(); boolean success = false; try { logger.debug("starting external node [{}] with: {}", nodeName, builder.command()); process = builder.start(); this.nodeInfo = null; if (waitForNode(client, nodeName)) { nodeInfo = nodeInfo(client, nodeName); assert nodeInfo != null; logger.debug("external node {} found, version [{}], build {}", nodeInfo.getNode(), nodeInfo.getVersion(), nodeInfo.getBuild()); } else { throw new IllegalStateException("Node [" + nodeName + "] didn't join the cluster"); } success = true; } finally { if (!success) { stop(); } } }	maybe info level so it is always logged even if debug logging is off?
public BatchResult<ShardRoutingEntry> execute(ClusterState currentState, List<ShardRoutingEntry> tasks) throws Exception { BatchResult.Builder<ShardRoutingEntry> batchResultBuilder = BatchResult.builder(); Set<ShardRoutingEntry> nonTrivialTasks = Collections.newSetFromMap(new IdentityHashMap<>()); List<FailedRerouteAllocation.FailedShard> failedShards = new ArrayList<>(tasks.size()); for (ShardRoutingEntry task : tasks) { RoutingNodes.RoutingNodeIterator routingNodeIterator = currentState.getRoutingNodes().routingNodeIter(task.getShardRouting().currentNodeId()); if (routingNodeIterator != null) { for (ShardRouting maybe : routingNodeIterator) { if (task.getShardRouting().isSameAllocation(maybe)) { nonTrivialTasks.add(task); failedShards.add(new FailedRerouteAllocation.FailedShard(task.shardRouting, task.message, task.failure)); break; } } } if (!nonTrivialTasks.contains(task)) { // the requested shard does not exist batchResultBuilder.success(task); } } ClusterState maybeUpdatedState = currentState; try { RoutingAllocation.Result result = allocationService.applyFailedShards(currentState, failedShards); if (result.changed()) { maybeUpdatedState = ClusterState.builder(currentState).routingResult(result).build(); } batchResultBuilder.successes(nonTrivialTasks); } catch (Throwable t) { batchResultBuilder.failures(nonTrivialTasks, t); } return batchResultBuilder.build(maybeUpdatedState); }	can we call these - missingshardtasks or alreadyremovedtasks?
public BatchResult<ShardRoutingEntry> execute(ClusterState currentState, List<ShardRoutingEntry> tasks) throws Exception { BatchResult.Builder<ShardRoutingEntry> batchResultBuilder = BatchResult.builder(); Set<ShardRoutingEntry> nonTrivialTasks = Collections.newSetFromMap(new IdentityHashMap<>()); List<FailedRerouteAllocation.FailedShard> failedShards = new ArrayList<>(tasks.size()); for (ShardRoutingEntry task : tasks) { RoutingNodes.RoutingNodeIterator routingNodeIterator = currentState.getRoutingNodes().routingNodeIter(task.getShardRouting().currentNodeId()); if (routingNodeIterator != null) { for (ShardRouting maybe : routingNodeIterator) { if (task.getShardRouting().isSameAllocation(maybe)) { nonTrivialTasks.add(task); failedShards.add(new FailedRerouteAllocation.FailedShard(task.shardRouting, task.message, task.failure)); break; } } } if (!nonTrivialTasks.contains(task)) { // the requested shard does not exist batchResultBuilder.success(task); } } ClusterState maybeUpdatedState = currentState; try { RoutingAllocation.Result result = allocationService.applyFailedShards(currentState, failedShards); if (result.changed()) { maybeUpdatedState = ClusterState.builder(currentState).routingResult(result).build(); } batchResultBuilder.successes(nonTrivialTasks); } catch (Throwable t) { batchResultBuilder.failures(nonTrivialTasks, t); } return batchResultBuilder.build(maybeUpdatedState); }	are you worried about duplicates here? when can that happen?
public void testShardNotFound() throws InterruptedException { final String index = "test"; clusterService.setState(stateWithStartedPrimary(index, true, randomInt(5))); String indexUUID = clusterService.state().metaData().index(index).getIndexUUID(); AtomicBoolean success = new AtomicBoolean(); CountDownLatch latch = new CountDownLatch(1); ShardRouting failedShard = getRandomShardRouting(index); RoutingTable routingTable = RoutingTable.builder(clusterService.state().getRoutingTable()).remove(index).build(); clusterService.setState(ClusterState.builder(clusterService.state()).routingTable(routingTable)); shardStateAction.shardFailed(failedShard, indexUUID, "test", getSimulatedFailure(), new ShardStateAction.Listener() { @Override public void onSuccess() { success.set(true); latch.countDown(); } @Override public void onFailure(Throwable t) { success.set(false); latch.countDown(); assert false; } }); CapturingTransport.CapturedRequest[] capturedRequests = transport.getCapturedRequestsAndClear(); transport.handleResponse(capturedRequests[0].requestId, TransportResponse.Empty.INSTANCE); latch.await(); assertTrue(success.get()); }	how about inlining this with multiple commands and check that everything works ok in batch mode? this can be a separate test...
public void registerLicense(final PutLicenseRequest request, final ActionListener<PutLicenseResponse> listener) { final License newLicense = request.license(); final License.LicenseType licenseType; try { licenseType = License.LicenseType.resolve(newLicense); } catch (Exception e) { listener.onFailure(e); return; } final long now = clock.millis(); if (!LicenseVerifier.verifyLicense(newLicense) || newLicense.issueDate() > now || newLicense.startDate() > now) { listener.onResponse(new PutLicenseResponse(true, LicensesStatus.INVALID)); } else if (licenseType == License.LicenseType.BASIC) { listener.onFailure(new IllegalArgumentException("Registering basic licenses is not allowed.")); } else if (isAllowedLicenseType(licenseType) == false) { listener.onFailure(new IllegalArgumentException( "Registering [" + licenseType.getTypeName() + "] licenses is not allowed on this cluster")); } else if (newLicense.expiryDate() < now) { listener.onResponse(new PutLicenseResponse(true, LicensesStatus.EXPIRED)); } else { if (!request.acknowledged()) { // TODO: ack messages should be generated on the master, since another node's cluster state may be behind... final License currentLicense = getLicense(); if (currentLicense != null) { Map<String, String[]> acknowledgeMessages = getAckMessages(newLicense, currentLicense); if (acknowledgeMessages.isEmpty() == false) { // needs acknowledgement listener.onResponse(new PutLicenseResponse(false, LicensesStatus.VALID, ACKNOWLEDGEMENT_HEADER, acknowledgeMessages)); return; } } } // This check would be incorrect if "basic" licenses were allowed here // because the defaults there mean that security can be "off", even if the setting is "on" // BUT basic licenses are explicitly excluded earlier in this method, so we don't need to worry if (XPackSettings.SECURITY_ENABLED.get(settings)) { // TODO we should really validate that all nodes have xpack installed and are consistently configured but this // should happen on a different level and not in this code if (XPackLicenseState.isTransportTlsRequired(newLicense, settings) && XPackSettings.TRANSPORT_SSL_ENABLED.get(settings) == false && isProductionMode(settings, clusterService.localNode())) { // security is on but TLS is not configured we gonna fail the entire request and throw an exception throw new IllegalStateException("Cannot install a [" + newLicense.operationMode() + "] license unless TLS is configured or security is disabled"); } else if (XPackSettings.FIPS_MODE_ENABLED.get(settings) && newLicense.operationMode() != License.OperationMode.PLATINUM && newLicense.operationMode() != License.OperationMode.TRIAL) { throw new IllegalStateException("Cannot install a [" + newLicense.operationMode() + "] license unless FIPS mode is disabled"); } } clusterService.submitStateUpdateTask("register license [" + newLicense.uid() + "]", new AckedClusterStateUpdateTask<PutLicenseResponse>(request, listener) { @Override protected PutLicenseResponse newResponse(boolean acknowledged) { return new PutLicenseResponse(acknowledged, LicensesStatus.VALID); } @Override public ClusterState execute(ClusterState currentState) throws Exception { XPackPlugin.checkReadyForXPackCustomMetadata(currentState); final Version oldestNodeVersion = currentState.nodes().getSmallestNonClientNodeVersion(); if (licenseIsCompatible(newLicense, oldestNodeVersion) == false) { throw new IllegalStateException("The provided license is not compatible with node version [" + oldestNodeVersion + "]"); } MetaData currentMetadata = currentState.metaData(); LicensesMetaData licensesMetaData = currentMetadata.custom(LicensesMetaData.TYPE); Version trialVersion = null; if (licensesMetaData != null) { trialVersion = licensesMetaData.getMostRecentTrialVersion(); } MetaData.Builder mdBuilder = MetaData.builder(currentMetadata); mdBuilder.putCustom(LicensesMetaData.TYPE, new LicensesMetaData(newLicense, trialVersion)); return ClusterState.builder(currentState).metaData(mdBuilder).build(); } }); } }	we should not consume any part of the license before we validate its signature first
public StoreStats storeStats() { try { return store.stats(); } catch (IOException e) { failShard("Failing Shard as IOException was found.",e); throw new ElasticsearchException("io exception while building 'store stats'", e); } catch (AlreadyClosedException ex) { return null; // already closed } }	can you change the message to the same one used in the exception we throw?
protected Settings prepareBackwardsDataDir(Path backwardsIndex, Object... settings) throws IOException { Path indexDir = createTempDir(); Path dataDir = indexDir.resolve("data"); try (InputStream stream = Files.newInputStream(backwardsIndex)) { TestUtil.unzip(stream, indexDir); } assertTrue(Files.exists(dataDir)); // list clusters in the datapath, ignoring anything from extrasfs final Path[] list; try (DirectoryStream<Path> stream = Files.newDirectoryStream(dataDir)) { List<Path> dirs = new ArrayList<>(); for (Path p : stream) { if (!p.getFileName().toString().startsWith("extra")) { dirs.add(p); } } list = dirs.toArray(new Path[0]); } if (list.length != 1) { throw new IllegalStateException("Backwards index must contain exactly one cluster\\\\n" + StringUtils.join(list, "\\\\n")); } Path src = list[0]; Path dest = dataDir.resolve(internalCluster().getClusterName()); assertTrue(Files.exists(src)); Files.move(src, dest); assertFalse(Files.exists(src)); assertTrue(Files.exists(dest)); ImmutableSettings.Builder builder = ImmutableSettings.builder() .put(settings) .put("path.data", dataDir.toAbsolutePath()); Path configDir = indexDir.resolve("config"); if (Files.exists(configDir)) { builder.put("path.conf", configDir.toAbsolutePath()); } return builder.build(); }	wondering if extrafs should expose the pattern it uses to inject files...
public static Builder builder(Client client, Listener listener) { if (client == null) { throw new Exception("The client you specified while building a BulkProcessor is null"); } return new Builder(client, listener); }	can you make it a nullpointerexception or an illegalargumentexception instead?
public int readInt() throws IOException { return ((readByte() & 0xFF) << 24) | ((readByte() & 0xFF) << 16) | ((readByte() & 0xFF) << 8) | (readByte() & 0xFF); }	can you add javadocs?
public void writeOptionalString(@Nullable String str) throws IOException { if (str == null) { writeBoolean(false); } else { writeBoolean(true); writeString(str); } }	can you add javadocs?
static DeprecationIssue checkSingleDataNodeWatermarkSetting( final Settings settings, final PluginsAndModules pluginsAndModules, final ClusterState clusterState, final XPackLicenseState licenseState ) { if (DiskThresholdDecider.ENABLE_FOR_SINGLE_DATA_NODE.get(settings) == false && DiskThresholdDecider.ENABLE_FOR_SINGLE_DATA_NODE.exists(settings)) { String key = DiskThresholdDecider.ENABLE_FOR_SINGLE_DATA_NODE.getKey(); String disableDiskDecider = DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(); return new DeprecationIssue( DeprecationIssue.Level.CRITICAL, String.format(Locale.ROOT, "Setting [%s=false] is deprecated", key), "https://ela.st/es-deprecation-7-disk-watermark-enable-for-single-node-setting", String.format( Locale.ROOT, "Disk watermarks do not treat single-node clusters differently in versions 8.0 and later, and [%s] may not be set to " + "[false] in these versions. Set [%s] to [true] to adopt the future behavior before upgrading. " + "If desired you may also set [%s] to [false] to completely disable disk-based allocation.", key, key, disableDiskDecider ), false, null ); } if (DiskThresholdDecider.ENABLE_FOR_SINGLE_DATA_NODE.get(settings) == false && clusterState.getNodes().getDataNodes().size() == 1 && clusterState.getNodes().getLocalNode().isMasterNode()) { String key = DiskThresholdDecider.ENABLE_FOR_SINGLE_DATA_NODE.getKey(); String disableDiskDecider = DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(); return new DeprecationIssue( DeprecationIssue.Level.WARNING, "Disk watermarks do not treat single-node clusters differently in versions 8.0 and later.", "https://ela.st/es-deprecation-7-disk-watermark-enable-for-single-node-setting", String.format( Locale.ROOT, "Disk watermarks do not treat single-node clusters differently in versions 8.0 and later, which will affect the " + "behavior of this cluster. Set [%s] to [true] to adopt the future behaviour before upgrading. " + "If desired you may also set [%s] to [false] to completely disable disk-based allocation.", key, disableDiskDecider ), false, null ); } return null; }	i wonder if we should avoid this part of the message, since it also disables flood stage monitoring? we made this setting an operator-only setting too, i.e., may not be settable by users in all environments. same comment for all 4 checks.
public boolean allocateUnassigned(final RoutingAllocation allocation) { boolean changed = false; RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); unassigned.sort(new PriorityComparator() { @Override protected Settings getIndexSettings(String index) { IndexMetaData indexMetaData = allocation.metaData().index(index); return indexMetaData.getSettings(); } }); // sort for priority ordering changed |= primaryShardAllocator.allocateUnassigned(allocation); changed |= replicaShardAllocator.allocateUnassigned(allocation); return changed; }	can we add an assertion going out that going out of the primary shard allocator we don't have any primary shards in the unassigned list, unless we expect them to be there? (primaryallocatedpostapi is false or restoresource != null)
private Directory writeStateToFirstLocation(final T state, Path stateLocation, String tmpFileName) throws WriteStateException { try { Directory stateDir = newDirectory(stateLocation); try { deleteFileIfExists(stateLocation, stateDir, tmpFileName); try (IndexOutput out = stateDir.createOutput(tmpFileName, IOContext.DEFAULT)) { CodecUtil.writeHeader(out, STATE_FILE_CODEC, STATE_FILE_VERSION); out.writeInt(FORMAT.index()); try (XContentBuilder builder = newXContentBuilder(FORMAT, new IndexOutputOutputStream(out) { @Override public void close() throws IOException { // this is important since some of the XContentBuilders write bytes on close. // in order to write the footer we need to prevent closing the actual index input. } })) { builder.startObject(); { toXContent(builder, state); } builder.endObject(); } CodecUtil.writeFooter(out); } stateDir.sync(Collections.singleton(tmpFileName)); } catch (Exception e) { // perform clean up only in case of exception, we need to keep directory open and temporary file on disk // if everything is ok for the next algorithm steps performDirectoryCleanup(stateLocation, stateDir, tmpFileName); throw e; } return stateDir; } catch (Exception e) { throw new WriteStateException(false, "failed to write state to the first location tmp file", e); } }	this { block } fits the pattern we use elsewhere, but feels unnecessary in this context.
private Directory writeStateToFirstLocation(final T state, Path stateLocation, String tmpFileName) throws WriteStateException { try { Directory stateDir = newDirectory(stateLocation); try { deleteFileIfExists(stateLocation, stateDir, tmpFileName); try (IndexOutput out = stateDir.createOutput(tmpFileName, IOContext.DEFAULT)) { CodecUtil.writeHeader(out, STATE_FILE_CODEC, STATE_FILE_VERSION); out.writeInt(FORMAT.index()); try (XContentBuilder builder = newXContentBuilder(FORMAT, new IndexOutputOutputStream(out) { @Override public void close() throws IOException { // this is important since some of the XContentBuilders write bytes on close. // in order to write the footer we need to prevent closing the actual index input. } })) { builder.startObject(); { toXContent(builder, state); } builder.endObject(); } CodecUtil.writeFooter(out); } stateDir.sync(Collections.singleton(tmpFileName)); } catch (Exception e) { // perform clean up only in case of exception, we need to keep directory open and temporary file on disk // if everything is ok for the next algorithm steps performDirectoryCleanup(stateLocation, stateDir, tmpFileName); throw e; } return stateDir; } catch (Exception e) { throw new WriteStateException(false, "failed to write state to the first location tmp file", e); } }	perhaps i'm missing something, but it looks like we do performdirectorycleanup on every directory in a finally block in the top-level write method. is this extra call also necessary?
private Directory writeStateToFirstLocation(final T state, Path stateLocation, String tmpFileName) throws WriteStateException { try { Directory stateDir = newDirectory(stateLocation); try { deleteFileIfExists(stateLocation, stateDir, tmpFileName); try (IndexOutput out = stateDir.createOutput(tmpFileName, IOContext.DEFAULT)) { CodecUtil.writeHeader(out, STATE_FILE_CODEC, STATE_FILE_VERSION); out.writeInt(FORMAT.index()); try (XContentBuilder builder = newXContentBuilder(FORMAT, new IndexOutputOutputStream(out) { @Override public void close() throws IOException { // this is important since some of the XContentBuilders write bytes on close. // in order to write the footer we need to prevent closing the actual index input. } })) { builder.startObject(); { toXContent(builder, state); } builder.endObject(); } CodecUtil.writeFooter(out); } stateDir.sync(Collections.singleton(tmpFileName)); } catch (Exception e) { // perform clean up only in case of exception, we need to keep directory open and temporary file on disk // if everything is ok for the next algorithm steps performDirectoryCleanup(stateLocation, stateDir, tmpFileName); throw e; } return stateDir; } catch (Exception e) { throw new WriteStateException(false, "failed to write state to the first location tmp file", e); } }	i think it'd be useful to see the filenames in the exception message.
private Directory writeStateToFirstLocation(final T state, Path stateLocation, String tmpFileName) throws WriteStateException { try { Directory stateDir = newDirectory(stateLocation); try { deleteFileIfExists(stateLocation, stateDir, tmpFileName); try (IndexOutput out = stateDir.createOutput(tmpFileName, IOContext.DEFAULT)) { CodecUtil.writeHeader(out, STATE_FILE_CODEC, STATE_FILE_VERSION); out.writeInt(FORMAT.index()); try (XContentBuilder builder = newXContentBuilder(FORMAT, new IndexOutputOutputStream(out) { @Override public void close() throws IOException { // this is important since some of the XContentBuilders write bytes on close. // in order to write the footer we need to prevent closing the actual index input. } })) { builder.startObject(); { toXContent(builder, state); } builder.endObject(); } CodecUtil.writeFooter(out); } stateDir.sync(Collections.singleton(tmpFileName)); } catch (Exception e) { // perform clean up only in case of exception, we need to keep directory open and temporary file on disk // if everything is ok for the next algorithm steps performDirectoryCleanup(stateLocation, stateDir, tmpFileName); throw e; } return stateDir; } catch (Exception e) { throw new WriteStateException(false, "failed to write state to the first location tmp file", e); } }	perhaps i'm missing something, but it looks like we do performdirectorycleanup on every directory in a finally block in the top-level write method. is this extra call also necessary?
public void testFailWriteAndReadAnyState() throws IOException { Path path = createTempDir(); Format format = new Format("foo-"); Set<DummyState> possibleStates = new HashSet<>(); DummyState initialState = writeAndReadStateSuccessfully(format, path); possibleStates.add(initialState); for (int i = 0; i < randomIntBetween(1, 5); i++) { format.failOnMethods(Format.FAIL_FSYNC_STATE_DIRECTORY); DummyState newState = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 100), randomInt(), randomLong(), randomDouble(), randomBoolean()); possibleStates.add(newState); WriteStateException ex = expectThrows(WriteStateException.class, () -> format.write(newState, path)); assertTrue(ex.isDirty()); format.noFailures(); assertTrue(possibleStates.contains(format.loadLatestState(logger, NamedXContentRegistry.EMPTY, path))); } writeAndReadStateSuccessfully(format, path); }	we no longer need to declare throws writestateexception since it's a subclass of ioexception.
public void testFailCopyTmpFileToExtraLocation() throws IOException, WriteStateException { Path paths[] = new Path[randomIntBetween(2, 5)]; for (int i = 0; i < paths.length; i++) { paths[i] = createTempDir(); } Format format = new Format("foo-"); DummyState initialState = writeAndReadStateSuccessfully(format, paths); for (int i = 0; i < randomIntBetween(1, 5); i++) { format.failOnMethods(Format.FAIL_OPEN_STATE_FILE_WHEN_COPYING); DummyState newState = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 100), randomInt(), randomLong(), randomDouble(), randomBoolean()); WriteStateException ex = expectThrows(WriteStateException.class, () -> format.write(newState, paths)); assertFalse(ex.isDirty()); format.noFailures(); assertEquals(initialState, format.loadLatestState(logger, NamedXContentRegistry.EMPTY, paths)); } writeAndReadStateSuccessfully(format, paths); }	nit: extra blank line added
public void testFailRandomlyAndReadAnyState() throws IOException { Path paths[] = new Path[randomIntBetween(1, 5)]; for (int i = 0; i < paths.length; i++) { paths[i] = createTempDir(); } Format format = new Format("foo-"); Set<DummyState> possibleStates = new HashSet<>(); DummyState initialState = writeAndReadStateSuccessfully(format, paths); possibleStates.add(initialState); for (int i = 0; i < randomIntBetween(1, 5); i++) { format.failRandomly(); DummyState newState = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 100), randomInt(), randomLong(), randomDouble(), randomBoolean()); try { format.write(newState, paths); //if write is successful, the only possible state we can read is the one that was just written possibleStates.clear(); possibleStates.add(newState); } catch (WriteStateException e) { //if dirty flag is not set, read might return only old state if (e.isDirty()) { //if dirty flag is set, read might return old state or new state possibleStates.add(newState); } } format.noFailures(); //we call loadLatestState not on full path set, but only on random paths from this set. This is to emulate disk failures. Path[] randomPaths = randomSubsetOf(randomIntBetween(1, paths.length), paths).toArray(new Path[0]); DummyState stateOnDisk = format.loadLatestState(logger, NamedXContentRegistry.EMPTY, randomPaths); assertTrue(possibleStates.contains(stateOnDisk)); } writeAndReadStateSuccessfully(format, paths); }	s/might/must/. but i think i'd prefer to omit this comment and the next since they don't say anything that isn't in the javadocs for writestateexception.
public Builder setModelId(String modelId) { this.modelId = modelId; return this; }	does this really need to be public? seems like it is just the serialization who should use it. suggestion private builder setmodeltype(string modeltype) {
@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, ActionListener<Void> listener) { if (isReadOnly()) { listener.onFailure(new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository")); } else { SnapshotInfo snapshot = null; try { snapshot = getSnapshotInfo(snapshotId); } catch (SnapshotMissingException ex) { listener.onFailure(ex); return; } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) { logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex); } // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots final RepositoryData updatedRepositoryData; final RepositoryData repositoryData; final Map<String, BlobContainer> foundIndices; final Set<String> rootBlobs; try { repositoryData = getRepositoryData(); updatedRepositoryData = repositoryData.removeSnapshot(snapshotId); // Cache the indices that were found before writing out the new index-N blob so that a stuck master will never // delete an index that was created by another master node after writing this index-N blob. foundIndices = blobStore().blobContainer(basePath().add("indices")).children(); rootBlobs = blobContainer().listBlobs().keySet(); writeIndexGen(updatedRepositoryData, repositoryStateId); } catch (Exception ex) { listener.onFailure(new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex)); return; } final SnapshotInfo finalSnapshotInfo = snapshot; try { blobContainer().deleteBlobsIgnoringIfNotExists( Arrays.asList(snapshotFormat.blobName(snapshotId.getUUID()), globalMetaDataFormat.blobName(snapshotId.getUUID()))); } catch (IOException e) { logger.warn(() -> new ParameterizedMessage("[{}] Unable to delete global metadata files", snapshotId), e); } final var survivingIndices = updatedRepositoryData.getIndices(); deleteIndices( Optional.ofNullable(finalSnapshotInfo) .map(info -> info.indices().stream().filter(survivingIndices::containsKey) .map(updatedRepositoryData::resolveIndexId).collect(Collectors.toList())) .orElse(Collections.emptyList()), snapshotId, ActionListener.map(listener, v -> { cleanupStaleIndices(foundIndices, survivingIndices); // Cleaning up according to repository data before the delete so we don't accidentally identify the two just deleted // blobs for the current snapshot as stale. cleanupStaleRootFiles(rootBlobs, repositoryData); return null; }) ); } }	i find this too subtle, in particular because the cleanupstalerootfiles method now operates on an old repository data, which is not clear when looking at that method itself. can you instead do something like the following: diff --git a/server/src/main/java/org/elasticsearch/repositories/blobstore/blobstorerepository.java b/server/src/main/java/org/elasticsearch/repositories/blobstore/blobstorerepository.java index 8b731f05a39..5c6695a93e0 100644 --- a/server/src/main/java/org/elasticsearch/repositories/blobstore/blobstorerepository.java +++ b/server/src/main/java/org/elasticsearch/repositories/blobstore/blobstorerepository.java @@ -59,6 +59,7 @@ import org.elasticsearch.common.settings.setting; import org.elasticsearch.common.settings.settings; import org.elasticsearch.common.unit.bytesizeunit; import org.elasticsearch.common.unit.bytesizevalue; +import org.elasticsearch.common.util.set.sets; import org.elasticsearch.common.xcontent.loggingdeprecationhandler; import org.elasticsearch.common.xcontent.namedxcontentregistry; import org.elasticsearch.common.xcontent.xcontentfactory; @@ -101,6 +102,7 @@ import java.util.arraylist; import java.util.arrays; import java.util.collection; import java.util.collections; +import java.util.hashset; import java.util.list; import java.util.map; import java.util.optional; @@ -394,11 +396,10 @@ public abstract class blobstorerepository extends abstractlifecyclecomponent imp } // delete snapshot from the index file, since it is the maintainer of truth of active snapshots final repositorydata updatedrepositorydata; - final repositorydata repositorydata; final map<string, blobcontainer> foundindices; final set<string> rootblobs; try { - repositorydata = getrepositorydata(); + final repositorydata repositorydata = getrepositorydata(); updatedrepositorydata = repositorydata.removesnapshot(snapshotid); // cache the indices that were found before writing out the new index-n blob so that a stuck master will never // delete an index that was created by another master node after writing this index-n blob. @@ -410,9 +411,10 @@ public abstract class blobstorerepository extends abstractlifecyclecomponent imp return; } final snapshotinfo finalsnapshotinfo = snapshot; + final list<string> snapmetafilestodelete = + arrays.aslist(snapshotformat.blobname(snapshotid.getuuid()), globalmetadataformat.blobname(snapshotid.getuuid())); try { - blobcontainer().deleteblobsignoringifnotexists( - arrays.aslist(snapshotformat.blobname(snapshotid.getuuid()), globalmetadataformat.blobname(snapshotid.getuuid()))); + blobcontainer().deleteblobsignoringifnotexists(snapmetafilestodelete); } catch (ioexception e) { logger.warn(() -> new parameterizedmessage("[{}] unable to delete global metadata files", snapshotid), e); } @@ -425,9 +427,8 @@ public abstract class blobstorerepository extends abstractlifecyclecomponent imp snapshotid, actionlistener.map(listener, v -> { cleanupstaleindices(foundindices, survivingindices); - // cleaning up according to repository data before the delete so we don't accidentally identify the two just deleted - // blobs for the current snapshot as stale. - cleanupstalerootfiles(rootblobs, repositorydata); + // remove snapmetafilestodelete, which have been deleted in a prior step, so that they are not identified as stale + cleanupstalerootfiles(sets.difference(rootblobs, new hashset<>(snapmetafilestodelete)), updatedrepositorydata); return null; }) );
private void updateGlobalCheckpoint(final String allocationId, final long globalCheckpoint, LongConsumer ifUpdated) { final CheckpointState cps = checkpoints.get(allocationId); assert !this.shardAllocationId.equals(allocationId) || cps != null; if (cps != null && globalCheckpoint > cps.globalCheckpoint) { ifUpdated.accept(cps.globalCheckpoint); cps.globalCheckpoint = globalCheckpoint; onGlobalCheckpointUpdated.accept(globalCheckpoint); } } /** * Initializes the global checkpoint tracker in primary mode (see {@link #primaryMode}	i'm doubting whether this should be called out of lock. i'm tending to say yes. thoughts?
private Node stop() { if (!lifecycle.moveToStopped()) { return this; } Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings)); logger.info("stopping ..."); injector.getInstance(TribeService.class).stop(); injector.getInstance(ResourceWatcherService.class).stop(); if (NetworkModule.HTTP_ENABLED.get(settings)) { injector.getInstance(HttpServer.class).stop(); } injector.getInstance(SnapshotsService.class).stop(); injector.getInstance(SnapshotShardsService.class).stop(); // stop any changes happening as a result of cluster state changes injector.getInstance(IndicesClusterStateService.class).stop(); injector.getInstance(Discovery.class).stop(); // we close indices first, so operations won't be allowed on it injector.getInstance(IndicesTTLService.class).stop(); injector.getInstance(RoutingService.class).stop(); injector.getInstance(ClusterService.class).stop(); injector.getInstance(NodeConnectionsService.class).stop(); injector.getInstance(MonitorService.class).stop(); injector.getInstance(GatewayService.class).stop(); injector.getInstance(SearchService.class).stop(); injector.getInstance(RestController.class).stop(); injector.getInstance(TransportService.class).stop(); pluginLifecycleComponents.forEach(LifecycleComponent::stop); // we should stop this last since it waits for resources to get released // if we had scroll searchers etc or recovery going on we wait for to finish. injector.getInstance(IndicesService.class).stop(); logger.info("stopped"); return this; }	if the position of this is important i'd leave a comment about why. even if it is just to stop spammy log messages.
protected final void afterInternal(boolean afterClass) throws Exception { boolean success = false; try { final Scope currentClusterScope = getCurrentClusterScope(); clearDisruptionScheme(); try { if (cluster() != null) { if (currentClusterScope != Scope.TEST) { MetaData metaData = client().admin().cluster().prepareState().execute().actionGet().getState().getMetaData(); final Map<String, String> persistent = metaData.persistentSettings().getAsMap(); assertThat("test leaves persistent cluster metadata behind: " + persistent, persistent.size(), equalTo(0)); final Map<String, String> transientSettings = new HashMap<>(metaData.transientSettings().getAsMap()); // this is set by the test infra transientSettings.remove(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey()); assertThat("test leaves transient cluster metadata behind: " + transientSettings, transientSettings.keySet(), empty()); } ensureClusterSizeConsistency(); ensureClusterStateConsistency(); if (isInternalCluster()) { // check no pending cluster states are leaked for (Discovery discovery : internalCluster().getInstances(Discovery.class)) { if (discovery instanceof ZenDiscovery) { final ZenDiscovery zenDiscovery = (ZenDiscovery) discovery; assertBusy(() -> { final ClusterState[] states = zenDiscovery.pendingClusterStates(); assertThat(zenDiscovery.localNode().getName() + " still having pending states:\\\\n" + Stream.of(states).map(ClusterState::toString).collect(Collectors.joining("\\\\n")), states, emptyArray()); }); } } } beforeIndexDeletion(); cluster().wipe(excludeTemplates()); // wipe after to make sure we fail in the test that didn't ack the delete if (afterClass || currentClusterScope == Scope.TEST) { cluster().close(); } cluster().assertAfterTest(); } } finally { if (currentClusterScope == Scope.TEST) { clearClusters(); // it is ok to leave persistent / transient cluster state behind if scope is TEST } } success = true; } finally { if (!success) { // if we failed here that means that something broke horribly so we should clear all clusters // TODO: just let the exception happen, WTF is all this horseshit // afterTestRule.forceFailure(); } } }	should you only remove this if the annotation asks for this to be automatically managed?
public synchronized Async<List<String>> startNodesAsync(final int numNodes, final Settings settings) { final int defaultMinMasterNodes; if (autoManageMinMasterNodes) { int mastersDelta = Node.NODE_MASTER_SETTING.get(settings) ? numNodes : 0; defaultMinMasterNodes = updateMinMasterNodes(getMasterNodesCount() + mastersDelta); } else { defaultMinMasterNodes = -1; } final List<Async<String>> asyncs = new ArrayList<>(); for (int i = 0; i < numNodes; i++) { asyncs.add(startNodeAsync(settings, defaultMinMasterNodes)); } return () -> { List<String> ids = new ArrayList<>(); for (Async<String> async : asyncs) { ids.add(async.get()); } return ids; }; }	this code block is very similar to something i already saw above, shall we factor it out?
public void testSubqueryGroupByFilterAndOrderByByRealiased() { optimizeAndPlan("SELECT g as h FROM (SELECT date AS f, int AS g FROM test) WHERE h IS NOT NULL GROUP BY h ORDER BY h ASC"); }	for this and the other tests, it would be nice to also have a simple query (without subselect) and check that the plans are equivalent.
@SuppressForbidden(reason = "access /sys/fs/cgroup/memory") String readSysFsCgroupMemoryUsageInBytes(final String controlGroup) throws IOException { return readSingleLine(PathUtils.get("/sys/fs/cgroup/memory", controlGroup, "memory.usage_in_bytes")); } /** * The total current memory usage by processes in the cgroup (in bytes). * If there is no limit then some Linux versions return the maximum value that can be stored in an * unsigned 64 bit number, and this will overflow a long, hence the result type is <code>String</code>. * (The alternative would have been <code>BigInteger</code> but then it would not be possible to index * the OS stats document into Elasticsearch without losing information, as <code>BigInteger</code> is * not a supported Elasticsearch type.) * * @param controlGroup the control group for the Elasticsearch process for the {@code memory} subsystem * @return the total current memory usage by processes in the cgroup (in bytes) * @throws IOException if an I/O exception occurs reading {@code memory.current}	i think this is fine since here we are not actually parsing the line, but elsewhere in this file we do a bare split on the control group line, which doesn't allow for colons in the underlying path. looks like an existing issue in this code, but thought i would mention. to ensure we protect against it (could be a followup). see https://bugs.java.com/bugdatabase/view_bug.do?bug_id=jdk-8272124
public void testReadCharsFromStdin() throws Exception { assertPassphraseRead("\\\\n", ""); assertPassphraseRead("\\\\r", ""); assertPassphraseRead("\\\\r\\\\n", ""); assertPassphraseRead("hello", "hello"); assertPassphraseRead("hello\\\\n", "hello"); assertPassphraseRead("hello\\\\r", "hello"); assertPassphraseRead("hello\\\\r\\\\n", "hello"); assertPassphraseRead("hellohello", "hellohello"); assertPassphraseRead("hellohello\\\\n", "hellohello"); assertPassphraseRead("hellohello\\\\r", "hellohello"); assertPassphraseRead("hellohello\\\\r\\\\n", "hellohello"); assertPassphraseRead("hello\\\\nhi\\\\n", "hello"); assertPassphraseRead("hello\\\\rhi\\\\r", "hello"); assertPassphraseRead("hello\\\\r\\\\nhi\\\\r\\\\n", "hello"); }	what about negative assertions, ie passphrase too long?
@Override public char[] readSecret(String prompt) { return CONSOLE.readPassword("%s", prompt); } } /** visible for testing */ static class SystemTerminal extends Terminal { private static final PrintWriter WRITER = newWriter(); private BufferedReader reader; private boolean ignoreNextNewline = false; SystemTerminal() { super(System.lineSeparator()); } @SuppressForbidden(reason = "Writer for System.out") private static PrintWriter newWriter() { return new PrintWriter(System.out); } /** visible for testing */ BufferedReader getReader() { if (reader == null) { reader = new BufferedReader( new InputStreamReader(System.in, Charset.defaultCharset()), STDIN_READER_BUFFER_SIZE); } return reader; } @Override public PrintWriter getWriter() { return WRITER; } @Override public String readText(String text) { getErrorWriter().print(text); // prompts should go to standard error to avoid mixing with list output try { final String line = getReader().readLine(); if (line == null) { throw new IllegalStateException("unable to read from standard input; is standard input open and a tty attached?"); } return line; } catch (IOException ioe) { throw new RuntimeException(ioe); } } @Override public char[] readSecret(String text) { return readText(text).toCharArray(); } @Override public char[] readSecret(String text, int maxLength) { getErrorWriter().println(text); return readLineToCharArray(getReader(), maxLength); } /** * This method has to keep track of a little bit of state in order to account for * line breaks in a system-independent way. * * Visible for testing. */ char[] readLineToCharArray(Reader reader, int maxLength) { char[] buf = new char[maxLength]; try { int len = 0; int next; while ((next = reader.read()) != -1) { char nextChar = (char) next; if (nextChar == '\\\\n') { if (ignoreNextNewline) { ignoreNextNewline = false; continue; } else { break; } } if (nextChar == '\\\\r') { ignoreNextNewline = true; break; } ignoreNextNewline = false; if (len < maxLength) { buf[len] = nextChar; } len++; } if (len < maxLength) { char[] shortResult = Arrays.copyOf(buf, len); Arrays.fill(buf, '\\\\0'); return shortResult; } if (len > maxLength) { Arrays.fill(buf, '\\\\0'); throw new RuntimeException("password too long"); } return buf; } catch (IOException e) { throw new RuntimeException(e); } }	shouldn't this message be to stdout, not stderr?
@Override public char[] readSecret(String prompt) { return CONSOLE.readPassword("%s", prompt); } } /** visible for testing */ static class SystemTerminal extends Terminal { private static final PrintWriter WRITER = newWriter(); private BufferedReader reader; private boolean ignoreNextNewline = false; SystemTerminal() { super(System.lineSeparator()); } @SuppressForbidden(reason = "Writer for System.out") private static PrintWriter newWriter() { return new PrintWriter(System.out); } /** visible for testing */ BufferedReader getReader() { if (reader == null) { reader = new BufferedReader( new InputStreamReader(System.in, Charset.defaultCharset()), STDIN_READER_BUFFER_SIZE); } return reader; } @Override public PrintWriter getWriter() { return WRITER; } @Override public String readText(String text) { getErrorWriter().print(text); // prompts should go to standard error to avoid mixing with list output try { final String line = getReader().readLine(); if (line == null) { throw new IllegalStateException("unable to read from standard input; is standard input open and a tty attached?"); } return line; } catch (IOException ioe) { throw new RuntimeException(ioe); } } @Override public char[] readSecret(String text) { return readText(text).toCharArray(); } @Override public char[] readSecret(String text, int maxLength) { getErrorWriter().println(text); return readLineToCharArray(getReader(), maxLength); } /** * This method has to keep track of a little bit of state in order to account for * line breaks in a system-independent way. * * Visible for testing. */ char[] readLineToCharArray(Reader reader, int maxLength) { char[] buf = new char[maxLength]; try { int len = 0; int next; while ((next = reader.read()) != -1) { char nextChar = (char) next; if (nextChar == '\\\\n') { if (ignoreNextNewline) { ignoreNextNewline = false; continue; } else { break; } } if (nextChar == '\\\\r') { ignoreNextNewline = true; break; } ignoreNextNewline = false; if (len < maxLength) { buf[len] = nextChar; } len++; } if (len < maxLength) { char[] shortResult = Arrays.copyOf(buf, len); Arrays.fill(buf, '\\\\0'); return shortResult; } if (len > maxLength) { Arrays.fill(buf, '\\\\0'); throw new RuntimeException("password too long"); } return buf; } catch (IOException e) { throw new RuntimeException(e); } }	since we need to copy the line to a new array anyways (that is appropriately sized), could we simplify this method to copy until \\\\n is seen, then check if we want 1 or 2 less characters depending if the second to last char is \\\\r?
@Override public char[] readSecret(String prompt) { return CONSOLE.readPassword("%s", prompt); } } /** visible for testing */ static class SystemTerminal extends Terminal { private static final PrintWriter WRITER = newWriter(); private BufferedReader reader; private boolean ignoreNextNewline = false; SystemTerminal() { super(System.lineSeparator()); } @SuppressForbidden(reason = "Writer for System.out") private static PrintWriter newWriter() { return new PrintWriter(System.out); } /** visible for testing */ BufferedReader getReader() { if (reader == null) { reader = new BufferedReader( new InputStreamReader(System.in, Charset.defaultCharset()), STDIN_READER_BUFFER_SIZE); } return reader; } @Override public PrintWriter getWriter() { return WRITER; } @Override public String readText(String text) { getErrorWriter().print(text); // prompts should go to standard error to avoid mixing with list output try { final String line = getReader().readLine(); if (line == null) { throw new IllegalStateException("unable to read from standard input; is standard input open and a tty attached?"); } return line; } catch (IOException ioe) { throw new RuntimeException(ioe); } } @Override public char[] readSecret(String text) { return readText(text).toCharArray(); } @Override public char[] readSecret(String text, int maxLength) { getErrorWriter().println(text); return readLineToCharArray(getReader(), maxLength); } /** * This method has to keep track of a little bit of state in order to account for * line breaks in a system-independent way. * * Visible for testing. */ char[] readLineToCharArray(Reader reader, int maxLength) { char[] buf = new char[maxLength]; try { int len = 0; int next; while ((next = reader.read()) != -1) { char nextChar = (char) next; if (nextChar == '\\\\n') { if (ignoreNextNewline) { ignoreNextNewline = false; continue; } else { break; } } if (nextChar == '\\\\r') { ignoreNextNewline = true; break; } ignoreNextNewline = false; if (len < maxLength) { buf[len] = nextChar; } len++; } if (len < maxLength) { char[] shortResult = Arrays.copyOf(buf, len); Arrays.fill(buf, '\\\\0'); return shortResult; } if (len > maxLength) { Arrays.fill(buf, '\\\\0'); throw new RuntimeException("password too long"); } return buf; } catch (IOException e) { throw new RuntimeException(e); } }	this should be maxlength + 2 right? so we can account for a \\\\r\\\\n on windows being in the buffer?
char[] readLineToCharArray(Reader reader, int maxLength) { char[] buf = new char[maxLength]; try { int len = 0; int next; while ((next = reader.read()) != -1) { char nextChar = (char) next; if (nextChar == '\\\\n') { if (ignoreNextNewline) { ignoreNextNewline = false; continue; } else { break; } } if (nextChar == '\\\\r') { ignoreNextNewline = true; break; } ignoreNextNewline = false; if (len < maxLength) { buf[len] = nextChar; } len++; } if (len < maxLength) { char[] shortResult = Arrays.copyOf(buf, len); Arrays.fill(buf, '\\\\0'); return shortResult; } if (len > maxLength) { Arrays.fill(buf, '\\\\0'); throw new RuntimeException("password too long"); } return buf; } catch (IOException e) { throw new RuntimeException(e); } }	we should say what the maximum length is
@Before public void setup() throws Exception { assumeFalse(failed); // skip rest of tests once one fails sh = newShell(); if (installation != null && Files.exists(installation.logs.resolve("elasticsearch.log"))) { FileUtils.rm(installation.logs.resolve("elasticsearch.log")); } } /** The {@link Distribution}	while i understand the desire to have a minimal log file when debugging failures, unfortunately i think this will hurt more than it helps. we have historically had failures in packaging tests where we do not fail the test correctly, so several more test cases may be run before an unrelated failure occurs, yet we would then only see the latest log file. i'm open, though, to any mitigation ideas for the above problem, as i do like this change in principle.
static SecureSettings loadSecureSettings(Environment initialEnv) throws BootstrapException { final KeyStoreWrapper keystore; try { keystore = KeyStoreWrapper.load(initialEnv.configFile()); } catch (IOException e) { throw new BootstrapException(e); } char[] passChars; try { if (keystore != null && keystore.hasPassword()) { passChars = readPassphrase(System.in, KeyStoreAwareCommand.MAX_PASSPHRASE_LENGTH); } else { passChars = new char[0]; } } catch (IOException e) { throw new BootstrapException(e); } try (SecureString password = new SecureString(passChars)) { if (keystore == null) { final KeyStoreWrapper keyStoreWrapper = KeyStoreWrapper.create(); keyStoreWrapper.save(initialEnv.configFile(), new char[0]); return keyStoreWrapper; } else { keystore.decrypt(password.getChars()); KeyStoreWrapper.upgrade(keystore, initialEnv.configFile(), password.getChars()); } } catch (Exception e) { throw new BootstrapException(e); } return keystore; }	shouldn't we only be reading until we see \\\\n?
public static Map<String, Object> entityAsMap(Response response) throws IOException { XContentType xContentType = XContentType.fromMediaTypeOrFormat(response.getEntity().getContentType().getValue()); // EMPTY and THROW are fine here because `.map` doesn't use named x content or deprecation try (XContentParser parser = xContentType.xContent().createParser( NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, response.getEntity().getContent())) { return parser.map(); } }	i made this static so that we could use it easily from feature. i feel like if client() is static then this can be.
@SuppressWarnings("unchecked") private void deleteAllDatafeeds() throws IOException { final Request datafeedsRequest = new Request("GET", "/_xpack/ml/datafeeds"); datafeedsRequest.addParameter("filter_path", "datafeeds"); final Response datafeedsResponse = adminClient.performRequest(datafeedsRequest); @SuppressWarnings("unchecked") final List<Map<String, Object>> datafeeds = (List<Map<String, Object>>) XContentMapValues.extractValue("datafeeds", ESRestTestCase.entityAsMap(datafeedsResponse)); if (datafeeds == null) { return; } try { int statusCode = adminClient.performRequest("POST", "/_xpack/ml/datafeeds/_all/_stop") .getStatusLine().getStatusCode(); if (statusCode != 200) { logger.error("Got status code " + statusCode + " when stopping datafeeds"); } } catch (Exception e1) { logger.warn("failed to stop all datafeeds. Forcing stop", e1); try { int statusCode = adminClient .performRequest("POST", "/_xpack/ml/datafeeds/_all/_stop?force=true") .getStatusLine().getStatusCode(); if (statusCode != 200) { logger.error("Got status code " + statusCode + " when stopping datafeeds"); } } catch (Exception e2) { logger.warn("Force-closing all data feeds failed", e2); } throw new RuntimeException( "Had to resort to force-stopping datafeeds, something went wrong?", e1); } for (Map<String, Object> datafeed : datafeeds) { String datafeedId = (String) datafeed.get("datafeed_id"); int statusCode = adminClient.performRequest("DELETE", "/_xpack/ml/datafeeds/" + datafeedId).getStatusLine().getStatusCode(); if (statusCode != 200) { logger.error("Got status code " + statusCode + " when deleting datafeed " + datafeedId); } } }	checkstyle forced me to change this and the rest of the changes to this file come about because of the change here.
@SuppressWarnings("unchecked") private void deleteAllJobs() throws Exception { Response response = adminClient.performRequest("GET", "/_xpack/rollup/job/_all"); Map<String, Object> jobs = ESRestTestCase.entityAsMap(response); @SuppressWarnings("unchecked") List<Map<String, Object>> jobConfigs = (List<Map<String, Object>>) XContentMapValues.extractValue("jobs", jobs); if (jobConfigs == null) { return; } for (Map<String, Object> jobConfig : jobConfigs) { logger.debug(jobConfig); String jobId = (String) ((Map<String, Object>) jobConfig.get("config")).get("id"); logger.debug("Deleting job " + jobId); try { response = adminClient.performRequest("DELETE", "/_xpack/rollup/job/" + jobId); } catch (Exception e) { // ok } } }	checkstyle forced me to change this and the rest of the changes to this file come about because of the change here.
*/ public void writeToChannel(WriteOperation writeOperation) { assertOnSelectorThread(); SocketChannelContext context = writeOperation.getChannel(); boolean shouldFlush = context.readyForFlush() == false; try { SelectionKeyUtils.setWriteInterested(context.getSelectionKey()); context.queueWriteOperation(writeOperation); } catch (Exception e) { shouldFlush = false; executeFailedListener(writeOperation.getListener(), e); } if (shouldFlush) { handleWrite(context); eventHandler.postSocketChannelHandling(context); } }	this logic doesn't look intuitive to me. why should we flush if the context is not ready for it? maybe it's a naming issue?
* @param nodeName the node name to be used as a logging prefix */ // visible for testing static void check(final boolean enforceLimits, final List<Check> checks, final String nodeName) { final ESLogger logger = Loggers.getLogger(BootstrapCheck.class, nodeName); final List<IllegalStateException> exceptions = checks.stream() .filter(BootstrapCheck.Check::check) .map(check -> new IllegalStateException(check.errorMessage())) .collect(Collectors.toList()); if (!exceptions.isEmpty()) { final List<String> messages = new ArrayList<>(1 + exceptions.size()); messages.add("bootstrap checks failed"); exceptions.forEach(e -> messages.add(e.getMessage())); if (enforceLimits) { final RuntimeException re = new RuntimeException(String.join("\\\\n", messages)); exceptions.forEach(re::addSuppressed); throw re; } else { messages.forEach(message -> logger.warn(message)); } } }	this is what i meant, feel free to change or keep as is: final list<string> errors = checks.stream() .filter(bootstrapcheck.check::check) .map(bootstrapcheck.check::errormessage) .collect(collectors.tolist()); if (!errors.isempty()) { final list<string> messages = new arraylist<>(1 + errors.size()); messages.add("bootstrap checks failed"); if (enforcelimits) { final runtimeexception re = new runtimeexception(string.join("\\\\n", messages)); errors.foreach(e -> re.addsuppressed(new illegalstateexception(e))); throw re; } else { messages.foreach(message -> logger.warn(message)); } }
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Request request = (Request) o; if (rollupIndex.equals(request.rollupIndex) == false) return false; if (rollupRequest.equals(request.rollupRequest) == false) return false; if (Arrays.equals(dimensionFields, request.dimensionFields) == false) return false; return Arrays.equals(metricFields, request.metricFields); } } public static class RequestBuilder extends ActionRequestBuilder<Request, Response> { protected RequestBuilder(ElasticsearchClient client, RollupIndexerAction action) { super(client, action, new Request()); } } public static class Response extends BroadcastResponse implements Writeable, ToXContentObject { private final boolean created; public Response(boolean created) { super(0, 0, 0, null); this.created = created; } public Response(StreamInput in) throws IOException { super(in); created = in.readBoolean(); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeBoolean(created); } public boolean isCreated() { return created; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("created", created); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Response response = (Response) o; return created == response.created; } @Override public int hashCode() { return Objects.hash(created); } } public static class ShardRequest extends BroadcastShardRequest { private final Request request; public ShardRequest(StreamInput in) throws IOException { super(in); this.request = new Request(in); } public ShardRequest(ShardId shardId, Request request) { super(shardId, request); this.request = request; } public String getRollupIndex() { return request.getRollupIndex(); } public RollupActionConfig getRollupConfig() { return request.getRollupRequest().getRollupConfig(); } public String[] getDimensionFields() { return request.getDimensionFields(); } public String[] getMetricFields() { return request.getMetricFields(); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); request.writeTo(out); } } public static class ShardResponse extends BroadcastShardResponse { public ShardResponse(StreamInput in) throws IOException { super(in); } public ShardResponse(ShardId shardId) { super(shardId); }	it's worth double checking that this hashes the arrays properly. i *think* it just calls .hashcode on them which doesn't hash their contents.
public static boolean isIngestEnabled(Settings settings) { return settings.getAsBoolean("node.ingest", true); }	maybe this static helper should be in ingestproxyactionfilter? it doesn't seem to be used elsewhere.
public static void writeRawField(String field, BytesReference source, XContentBuilder builder, ToXContent.Params params) throws IOException { Compressor compressor = CompressorFactory.compressor(source); if (compressor != null) { InputStream compressedStreamInput = compressor.streamInput(source.streamInput()); XContentType contentType = XContentFactory.xContentType(compressedStreamInput); if (compressedStreamInput.markSupported() == false) { compressedStreamInput = new BufferedInputStream(compressedStreamInput); } if (contentType == builder.contentType()) { builder.rawField(field, compressedStreamInput); } else { try (XContentParser parser = XContentFactory.xContent(contentType).createParser(compressedStreamInput)) { parser.nextToken(); builder.field(field); builder.copyCurrentStructure(parser); } } } else { XContentType contentType = XContentFactory.xContentType(source); if (contentType == builder.contentType()) { builder.rawField(field, source); } else { try (XContentParser parser = XContentFactory.xContent(contentType).createParser(source)) { parser.nextToken(); builder.field(field); builder.copyCurrentStructure(parser); } } } }	shouldn't this be before the call to xcontentfactory.xcontenttype?
@Override public BytesReference value(Object value) { if (value == null) { return null; } BytesReference bytes; if (value instanceof BytesRef) { bytes = new BytesArray((BytesRef) value); } else if (value instanceof BytesReference) { bytes = (BytesReference) value; } else if (value instanceof byte[]) { bytes = new BytesArray((byte[]) value); } else { try { bytes = new BytesArray(Base64.decode(value.toString())); } catch (IOException e) { throw new ElasticsearchParseException("failed to convert bytes", e); } } try { if (indexCreatedBefore2x) { try { return CompressorFactory.uncompressIfNeeded(bytes); } catch (NotXContentException e) { // This is a BUG! We try to decompress by detecting a header in // the stored bytes but since we accept arbitrary bytes, we have // no guarantee that uncompressed bytes will be detected as // compressed! } } return bytes; } catch (IOException e) { throw new ElasticsearchParseException("failed to decompress source", e); } }	this is scary to see a comment saying "this is a bug!" and then doing nothing with the exception...
@Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null || getClass() != obj.getClass()) return false; if (super.equals(obj) == false) return false; ArrayValuesSourceAggregationBuilder<?, ?> that = (ArrayValuesSourceAggregationBuilder<?, ?>) obj; return valuesSourceType == that.valuesSourceType && targetValueType == that.targetValueType && Objects.equals(fields, that.fields) && valueType == that.valueType && Objects.equals(format, that.format) && Objects.equals(missing, that.missing); }	is there a reason for the mix of objects.equals() vs == here?
@Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null || getClass() != obj.getClass()) return false; if (super.equals(obj) == false) return false; PercentilesAggregationBuilder other = (PercentilesAggregationBuilder) obj; if (!Objects.equals(method, other.method)) { return false; } boolean equalSettings = false; switch (method) { case HDR: equalSettings = Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits); break; case TDIGEST: equalSettings = Objects.equals(compression, other.compression); break; default: throw new IllegalStateException("Illegal method [" + method.toString() + "]"); } return equalSettings && Objects.deepEquals(percents, other.percents) && Objects.equals(keyed, other.keyed) && Objects.equals(method, other.method); }	while we're making changes, would you mind fixing this !objects.equals(...) too? thanks!
@Override public ANode visitSource(SourceContext ctx) { List<SFunction> functions = new ArrayList<>(); for (FunctionContext function : ctx.function()) { functions.add((SFunction)visit(function)); } List<AStatement> statements = new ArrayList<>(); for (StatementContext statement : ctx.statement()) { statements.add((AStatement)visit(statement)); } String returnCanonicalTypeName = PainlessLookupUtility.typeToCanonicalTypeName(scriptClassInfo.getExecuteMethodReturnType()); List<String> paramTypes = new ArrayList<>(); List<String> paramNames = new ArrayList<>(); for (ScriptClassInfo.MethodArgument argument : scriptClassInfo.getExecuteArguments()) { paramTypes.add(PainlessLookupUtility.typeToCanonicalTypeName(argument.getClazz())); paramNames.add(argument.getName()); } SFunction execute = new SFunction(location(ctx), returnCanonicalTypeName, "execute", paramTypes, paramNames, new SBlock( location(ctx), statements), true, false, false, true); functions.add(execute); return new SClass(scriptClassInfo, sourceName, sourceText, debugStream, location(ctx), functions); }	consider adding a comment here "handle execute" or something similar.
@Override public ANode visitSource(SourceContext ctx) { List<SFunction> functions = new ArrayList<>(); for (FunctionContext function : ctx.function()) { functions.add((SFunction)visit(function)); } List<AStatement> statements = new ArrayList<>(); for (StatementContext statement : ctx.statement()) { statements.add((AStatement)visit(statement)); } String returnCanonicalTypeName = PainlessLookupUtility.typeToCanonicalTypeName(scriptClassInfo.getExecuteMethodReturnType()); List<String> paramTypes = new ArrayList<>(); List<String> paramNames = new ArrayList<>(); for (ScriptClassInfo.MethodArgument argument : scriptClassInfo.getExecuteArguments()) { paramTypes.add(PainlessLookupUtility.typeToCanonicalTypeName(argument.getClazz())); paramNames.add(argument.getName()); } SFunction execute = new SFunction(location(ctx), returnCanonicalTypeName, "execute", paramTypes, paramNames, new SBlock( location(ctx), statements), true, false, false, true); functions.add(execute); return new SClass(scriptClassInfo, sourceName, sourceText, debugStream, location(ctx), functions); }	> true, false, false, true ugh.
@Override protected void write(ClassWriter classWriter, MethodWriter methodWriter, Globals globals, ScopeTable scopeTable) { int access = Opcodes.ACC_PUBLIC; if (isStatic) { access |= Opcodes.ACC_STATIC; } else { scopeTable.defineVariable(Object.class, "#this"); } if (isSynthetic) { access |= Opcodes.ACC_SYNTHETIC; } Type asmReturnType = MethodWriter.getType(returnType); Type[] asmParameterTypes = new Type[typeParameters.size()]; for (int index = 0; index < asmParameterTypes.length; ++index) { Class<?> type = typeParameters.get(index); String name = parameterNames.get(index); scopeTable.defineVariable(type, name); asmParameterTypes[index] = MethodWriter.getType(typeParameters.get(index)); } Method method = new Method(name, asmReturnType, asmParameterTypes); methodWriter = classWriter.newMethodWriter(access, method); methodWriter.visitCode(); // TODO: do not specialize for execute Label startTry = new Label(); Label endTry = new Label(); Label startExplainCatch = new Label(); Label startOtherCatch = new Label(); Label endCatch = new Label(); if ("execute".equals(name)) { methodWriter.mark(startTry); for (int getMethodIndex = 0; getMethodIndex < scriptRoot.getScriptClassInfo().getGetMethods().size(); ++getMethodIndex) { Method getMethod = scriptRoot.getScriptClassInfo().getGetMethods().get(getMethodIndex); Class<?> returnType = scriptRoot.getScriptClassInfo().getGetReturns().get(getMethodIndex); String name = getMethod.getName().substring(3); name = Character.toLowerCase(name.charAt(0)) + name.substring(1); if (scriptRoot.getUsedVariables().contains(name)) { Variable variable = scopeTable.defineVariable(returnType, name); methodWriter.loadThis(); methodWriter.invokeVirtual(Type.getType(scriptRoot.getScriptClassInfo().getBaseClass()), getMethod); methodWriter.visitVarInsn(getMethod.getReturnType().getOpcode(Opcodes.ISTORE), variable.getSlot()); } } } // TODO: end if (maxLoopCounter > 0) { // if there is infinite loop protection, we do this once: // int #loop = settings.getMaxLoopCounter() Variable loop = scopeTable.defineInternalVariable(int.class, "loop"); methodWriter.push(maxLoopCounter); methodWriter.visitVarInsn(Opcodes.ISTORE, loop.getSlot()); } blockNode.write(classWriter, methodWriter, globals, scopeTable.newScope()); if (doesMethodEscape == false) { if (returnType == void.class) { methodWriter.returnValue(); } else if (doAutoReturn) { if (returnType == boolean.class) { methodWriter.push(false); } else if (returnType == byte.class || returnType == char.class || returnType == short.class || returnType == int.class) { methodWriter.push(0); } else if (returnType == long.class) { methodWriter.push(0L); } else if (returnType == float.class) { methodWriter.push(0f); } else if (returnType == double.class) { methodWriter.push(0d); } else { methodWriter.visitInsn(Opcodes.ACONST_NULL); } methodWriter.returnValue(); } else { throw getLocation().createError(new IllegalStateException("not all paths provide a return value " + "for function [" + name + "] with [" + typeParameters.size() + "] parameters")); } } // TODO: do not specialize for execute if ("execute".equals(name)) { methodWriter.mark(endTry); methodWriter.goTo(endCatch); // This looks like: // } catch (PainlessExplainError e) { // throw this.convertToScriptException(e, e.getHeaders($DEFINITION)) // } methodWriter.visitTryCatchBlock(startTry, endTry, startExplainCatch, PAINLESS_EXPLAIN_ERROR_TYPE.getInternalName()); methodWriter.mark(startExplainCatch); methodWriter.loadThis(); methodWriter.swap(); methodWriter.dup(); methodWriter.getStatic(CLASS_TYPE, "$DEFINITION", DEFINITION_TYPE); methodWriter.invokeVirtual(PAINLESS_EXPLAIN_ERROR_TYPE, PAINLESS_EXPLAIN_ERROR_GET_HEADERS_METHOD); methodWriter.invokeInterface(BASE_INTERFACE_TYPE, CONVERT_TO_SCRIPT_EXCEPTION_METHOD); methodWriter.throwException(); // This looks like: // } catch (PainlessError | BootstrapMethodError | OutOfMemoryError | StackOverflowError | Exception e) { // throw this.convertToScriptException(e, e.getHeaders()) // }	can you do something like: try { } catch (xxx) { }
@Override protected void doRun() throws Exception { indexVersionChecker.accept(response.getIndexMetadataVersion(), e -> { if (e != null) { if (retryCounter.incrementAndGet() <= PROCESSOR_RETRY_LIMIT) { handleResponse(to, response); } else { handler.accept(new ElasticsearchException("retrying failed [" + retryCounter.get() + "] times, aborting...", e)); } return; } final BulkShardOperationsRequest request = new BulkShardOperationsRequest(followerShard, response.getOperations()); followerClient.execute(BulkShardOperationsAction.INSTANCE, request, new ActionListener<BulkShardOperationsResponse>() { @Override public void onResponse(final BulkShardOperationsResponse bulkShardOperationsResponse) { handler.accept(null); } @Override public void onFailure(final Exception e) { // No retry mechanism here, because if a failure is being redirected to this place it is considered // non recoverable. assert e != null; handler.accept(e); } } ); }); }	do we want a retry on every exception type?
@Override public void buildCacheKey(StreamOutput out) throws IOException { assert false == (queries == null && limitedByQueries == null) : "one of queries and limited-by queries must be non-null"; if (queries != null) { assert evaluatedQueries != null : "queries are not evaluated"; out.writeBoolean(true); out.writeCollection(evaluatedQueries, StreamOutput::writeString); } else { out.writeBoolean(false); } if (limitedByQueries != null) { assert evaluatedLimitedByQueries != null : "limited-by queries are not evaluated"; out.writeBoolean(true); out.writeCollection(evaluatedLimitedByQueries, StreamOutput::writeString); } else { out.writeBoolean(false); } }	this feels wrong. it means that it's only possible to call this method if you have called some other method first. but in fact there's no reason for the caller to know that they should do that, or to even expect them to be linked. it might be what we need to do right now, but i'm very reluctant to leave it this way. the obvious answers are to either: 1. inject enough services into the constructor (or a builder) to evaluate the queries immediately 2. split this class into 2, so that there's a separate evaluateddocumentpermissions class that implements cachekey. i don't know if either of those are feasible.
*/ public BooleanQuery filter(User user, ScriptService scriptService, ShardId shardId, Function<ShardId, SearchExecutionContext> searchExecutionContextProvider) throws IOException { if (hasDocumentLevelPermissions()) { evaluateQueries(user, scriptService); BooleanQuery.Builder filter; if (evaluatedQueries != null && evaluatedLimitedByQueries != null) { filter = new BooleanQuery.Builder(); BooleanQuery.Builder scopedFilter = new BooleanQuery.Builder(); buildRoleQuery(shardId, searchExecutionContextProvider, evaluatedLimitedByQueries, scopedFilter); filter.add(scopedFilter.build(), FILTER); buildRoleQuery(shardId, searchExecutionContextProvider, evaluatedQueries, filter); } else if (evaluatedQueries != null) { filter = new BooleanQuery.Builder(); buildRoleQuery(shardId, searchExecutionContextProvider, evaluatedQueries, filter); } else if (evaluatedLimitedByQueries != null) { filter = new BooleanQuery.Builder(); buildRoleQuery(shardId, searchExecutionContextProvider, evaluatedLimitedByQueries, filter); } else { assert false : "one of queries and limited-by queries must be non-null"; return null; } return filter.build(); } return null; }	this only appears to be called from filter - why is it public?
public void testRegressionWithNumericFeatureAndFewDocuments() throws Exception { String sourceIndex = "test-regression-with-numeric-feature-and-few-docs"; BulkRequestBuilder bulkRequestBuilder = client().prepareBulk(); bulkRequestBuilder.setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); List<Double> featureValues = Arrays.asList(1.0, 2.0, 3.0); List<Double> dependentVariableValues = Arrays.asList(10.0, 20.0, 30.0); for (int i = 0; i < 350; i++) { Double field = featureValues.get(i % 3); Double value = dependentVariableValues.get(i % 3); IndexRequest indexRequest = new IndexRequest(sourceIndex); if (i < 300) { indexRequest.source("feature", field, "variable", value); } else { indexRequest.source("feature", field); } bulkRequestBuilder.add(indexRequest); } BulkResponse bulkResponse = bulkRequestBuilder.get(); if (bulkResponse.hasFailures()) { fail("Failed to index data: " + bulkResponse.buildFailureMessage()); } String id = "test_regression_with_numeric_feature_and_few_docs"; DataFrameAnalyticsConfig config = buildRegressionAnalytics(id, new String[] {sourceIndex}, sourceIndex + "-results", null, "variable"); registerAnalytics(config); putAnalytics(config); assertState(id, DataFrameAnalyticsState.STOPPED); startAnalytics(id); waitUntilAnalyticsIsStopped(id); SearchResponse sourceData = client().prepareSearch(sourceIndex).get(); for (SearchHit hit : sourceData.getHits()) { GetResponse destDocGetResponse = client().prepareGet().setIndex(config.getDest().getIndex()).setId(hit.getId()).get(); assertThat(destDocGetResponse.isExists(), is(true)); Map<String, Object> sourceDoc = hit.getSourceAsMap(); Map<String, Object> destDoc = destDocGetResponse.getSource(); for (String field : sourceDoc.keySet()) { assertThat(destDoc.containsKey(field), is(true)); assertThat(destDoc.get(field), equalTo(sourceDoc.get(field))); } assertThat(destDoc.containsKey("ml"), is(true)); @SuppressWarnings("unchecked") Map<String, Object> resultsObject = (Map<String, Object>) destDoc.get("ml"); int resultsWithPrediction = 0; if (resultsObject.containsKey("prediction")) { resultsWithPrediction++; double featureValue = (double) destDoc.get("feature"); double predictionValue = (double) resultsObject.get("prediction"); // it seems for this case values can be as far off as 1.5 assertThat(predictionValue, closeTo(10 * featureValue, 1.5)); } assertThat(resultsWithPrediction, greaterThan(0)); } }	i'm wondering, should this declaration and assertion from line 429 be moved out of the loop?
public void testRecoveryStateRecoveredBytesMatchPhysicalCacheState() throws Exception { final String fsRepoName = randomAlphaOfLength(10); final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final String restoredIndexName = randomBoolean() ? indexName : randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final String snapshotName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); createRepository(fsRepoName, "fs"); final Settings.Builder originalIndexSettings = Settings.builder(); originalIndexSettings.put(INDEX_SOFT_DELETES_SETTING.getKey(), true); originalIndexSettings.put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1); createAndPopulateIndex(indexName, originalIndexSettings); final SnapshotInfo snapshotInfo = createFullSnapshot(fsRepoName, snapshotName); assertAcked(client().admin().indices().prepareDelete(indexName)); mountSnapshot(fsRepoName, snapshotName, indexName, restoredIndexName, Settings.EMPTY); ensureGreen(restoredIndexName); final Index restoredIndex = client().admin() .cluster() .prepareState() .clear() .setMetadata(true) .get() .getState() .metadata() .index(restoredIndexName) .getIndex(); assertExecutorIsIdle(SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME); assertExecutorIsIdle(SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME); RecoveryState recoveryState = getRecoveryState(restoredIndexName); assertThat(recoveryState.getStage(), equalTo(RecoveryState.Stage.DONE)); long recoveredBytes = recoveryState.getIndex().recoveredBytes(); long physicalCacheSize = getPhysicalCacheSize(restoredIndex, snapshotInfo.snapshotId().getUUID()); assertThat("Physical cache size doesn't match with recovery state data", physicalCacheSize, equalTo(recoveredBytes)); assertThat("Expected to recover 100% of files", recoveryState.getIndex().recoveredBytesPercent(), equalTo(100.0f)); }	would be great to use more than 100 docs
public void testRecoveryStateRecoveredBytesMatchPhysicalCacheState() throws Exception { final String fsRepoName = randomAlphaOfLength(10); final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final String restoredIndexName = randomBoolean() ? indexName : randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final String snapshotName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); createRepository(fsRepoName, "fs"); final Settings.Builder originalIndexSettings = Settings.builder(); originalIndexSettings.put(INDEX_SOFT_DELETES_SETTING.getKey(), true); originalIndexSettings.put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1); createAndPopulateIndex(indexName, originalIndexSettings); final SnapshotInfo snapshotInfo = createFullSnapshot(fsRepoName, snapshotName); assertAcked(client().admin().indices().prepareDelete(indexName)); mountSnapshot(fsRepoName, snapshotName, indexName, restoredIndexName, Settings.EMPTY); ensureGreen(restoredIndexName); final Index restoredIndex = client().admin() .cluster() .prepareState() .clear() .setMetadata(true) .get() .getState() .metadata() .index(restoredIndexName) .getIndex(); assertExecutorIsIdle(SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME); assertExecutorIsIdle(SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME); RecoveryState recoveryState = getRecoveryState(restoredIndexName); assertThat(recoveryState.getStage(), equalTo(RecoveryState.Stage.DONE)); long recoveredBytes = recoveryState.getIndex().recoveredBytes(); long physicalCacheSize = getPhysicalCacheSize(restoredIndex, snapshotInfo.snapshotId().getUUID()); assertThat("Physical cache size doesn't match with recovery state data", physicalCacheSize, equalTo(recoveredBytes)); assertThat("Expected to recover 100% of files", recoveryState.getIndex().recoveredBytesPercent(), equalTo(100.0f)); }	i think we need to wait for the recovery to be done before executing synchronizecache()? otherwise there is a risk that not all cache files are fully prewarmed and synchronized on disk this fails: > assertthat("expected to reuse all data from the persistent cache but it didn't", 0l, equalto(recoveredbytes));
public void testFilesStoredInThePersistentCacheAreMarkedAsReusedInRecoveryState() throws Exception { final String fsRepoName = randomAlphaOfLength(10); final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final String restoredIndexName = randomBoolean() ? indexName : randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final String snapshotName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); createRepository(fsRepoName, "test-fs"); int numberOfShards = 1; createAndPopulateIndex( indexName, Settings.builder().put(INDEX_SOFT_DELETES_SETTING.getKey(), true).put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, numberOfShards) ); final SnapshotInfo snapshotInfo = createFullSnapshot(fsRepoName, snapshotName); assertAcked(client().admin().indices().prepareDelete(indexName)); mountSnapshot(fsRepoName, snapshotName, indexName, restoredIndexName, Settings.EMPTY); ensureGreen(restoredIndexName); for (CacheService cacheService : internalCluster().getDataNodeInstances(CacheService.class)) { cacheService.synchronizeCache(); } internalCluster().restartRandomDataNode(); ensureGreen(restoredIndexName); final Index restoredIndex = client().admin() .cluster() .prepareState() .clear() .setMetadata(true) .get() .getState() .metadata() .index(restoredIndexName) .getIndex(); assertExecutorIsIdle(SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME); assertExecutorIsIdle(SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME); RecoveryState recoveryState = getRecoveryState(restoredIndexName); assertThat(recoveryState.getStage(), equalTo(RecoveryState.Stage.DONE)); long recoveredBytes = recoveryState.getIndex().recoveredBytes(); long physicalCacheSize = getPhysicalCacheSize(restoredIndex, snapshotInfo.snapshotId().getUUID()); assertThat("Expected to reuse all data from the persistent cache but it didn't", 0L, equalTo(recoveredBytes)); final Repository repository = internalCluster().getDataNodeInstance(RepositoriesService.class).repository(fsRepoName); assertThat(repository, instanceOf(BlobStoreRepository.class)); final BlobStoreRepository blobStoreRepository = (BlobStoreRepository) repository; final RepositoryData repositoryData = ESBlobStoreRepositoryIntegTestCase.getRepositoryData(repository); final IndexId indexId = repositoryData.resolveIndexId(indexName); long inMemoryCacheSize = 0; for (int shardId = 0; shardId < numberOfShards; shardId++) { final BlobStoreIndexShardSnapshot snapshot = blobStoreRepository.loadShardSnapshot( blobStoreRepository.shardContainer(indexId, shardId), snapshotInfo.snapshotId() ); inMemoryCacheSize += snapshot.indexFiles() .stream() .filter(f -> f.metadata().hashEqualsContents()) .mapToLong(BlobStoreIndexShardSnapshot.FileInfo::length) .sum(); } assertThat(physicalCacheSize + inMemoryCacheSize, equalTo(recoveryState.getIndex().reusedBytes())); assertThat("Expected to recover 100% of files", recoveryState.getIndex().recoveredBytesPercent(), equalTo(100.0f)); for (RecoveryState.FileDetail fileDetail : recoveryState.getIndex().fileDetails()) { assertThat(fileDetail.name() + " wasn't mark as reused", fileDetail.reused(), equalTo(true)); } }	i supposed that we could compute the physicalcachesize from the blobstoreindexshardsnapshot too? maybe also check that getphysicalcachesize() is equal to physicalcachesize?
public void performAction(IndexMetaData indexMetaData, ClusterState clusterState, ClusterStateObserver observer, Listener listener) { final RoutingNodes routingNodes = clusterState.getRoutingNodes(); RoutingAllocation allocation = new RoutingAllocation(ALLOCATION_DECIDERS, routingNodes, clusterState, null, System.nanoTime()); List<String> validNodeIds = new ArrayList<>(); final Map<ShardId, List<ShardRouting>> routingsByShardId = clusterState.getRoutingTable() .allShards(indexMetaData.getIndex().getName()) .stream() .collect(Collectors.groupingBy(ShardRouting::shardId)); if (routingsByShardId.isEmpty() == false) { for (RoutingNode node : routingNodes) { boolean canAllocateOneCopyOfEachShard = routingsByShardId.values().stream() // For each shard .allMatch(shardRoutings -> shardRoutings.stream() // Can we allocate at least one shard copy to this node? .map(shardRouting -> ALLOCATION_DECIDERS.canAllocate(shardRouting, node, allocation).type()) .anyMatch(Decision.Type.YES::equals)); if (canAllocateOneCopyOfEachShard) { validNodeIds.add(node.node().getId()); } } // Shuffle the list of nodes so the one we pick is random Randomness.shuffle(validNodeIds); Optional<String> nodeId = validNodeIds.stream().findAny(); if (nodeId.isPresent()) { Settings settings = Settings.builder() .put(IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getKey() + "_id", nodeId.get()).build(); UpdateSettingsRequest updateSettingsRequest = new UpdateSettingsRequest(indexMetaData.getIndex().getName()) .settings(settings); getClient().admin().indices().updateSettings(updateSettingsRequest, ActionListener.wrap(response -> listener.onResponse(true), listener::onFailure)); } else { // No nodes currently match the allocation rules so just wait until there is one that does listener.onResponse(false); } } else { // There are no shards for the index, the index might be gone listener.onFailure(new IndexNotFoundException(indexMetaData.getIndex())); } }	for now can you add a log message (probably at debug level) for if there are no valid node ids? hopefully we don't hit that case, but it's nice to have debug logging for it if we do in case we have to diagnose a user's situation.
*/ public void validateQueryAsync(ValidateQueryRequest validateQueryRequest, RequestOptions options, ActionListener<ValidateQueryResponse> listener) { restHighLevelClient.performRequestAsyncAndParseEntity(validateQueryRequest, IndicesRequestConverters::validateQuery, options, ValidateQueryResponse::fromXContent, listener, emptySet()); } /** * Gets index templates which includes deprecated doc types using the Index Templates API * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html"> Index Templates API * on elastic.co</a> * @param getIndexTemplatesRequest the request * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT} if nothing needs to be customized * @return the response * @throws IOException in case there is a problem sending the request or parsing back the response * @deprecated use {@link #getIndexTemplate(GetIndexTemplatesRequest, RequestOptions)}	small comment: maybe we could use the same wording as we do in putmapping for consistency: https://github.com/elastic/elasticsearch/pull/37280/files#diff-944e3b65663d286958065a2ebc8d1aedr178
@SuppressWarnings("unchecked") public void testPutUntypedTemplate() throws Exception { PutIndexTemplateRequest putTemplateRequest = new PutIndexTemplateRequest() .name("my-template") .patterns(Arrays.asList("pattern-1", "name-*")) .order(10) .create(randomBoolean()) .settings(Settings.builder().put("number_of_shards", "3").put("number_of_replicas", "0")) .mapping("{ \\\\"properties\\\\":{" + "\\\\"host_name\\\\": {\\\\"type\\\\":\\\\"keyword\\\\"}" + "}" + "}", XContentType.JSON) .alias(new Alias("alias-1").indexRouting("abc")).alias(new Alias("{index}-write").searchRouting("xyz")); AcknowledgedResponse putTemplateResponse = execute(putTemplateRequest, highLevelClient().indices()::putTemplate, highLevelClient().indices()::putTemplateAsync); assertThat(putTemplateResponse.isAcknowledged(), equalTo(true)); Map<String, Object> templates = getAsMap("/_template/my-template"); assertThat(templates.keySet(), hasSize(1)); assertThat(extractValue("my-template.order", templates), equalTo(10)); assertThat(extractRawValues("my-template.index_patterns", templates), contains("pattern-1", "name-*")); assertThat(extractValue("my-template.settings.index.number_of_shards", templates), equalTo("3")); assertThat(extractValue("my-template.settings.index.number_of_replicas", templates), equalTo("0")); assertThat(extractValue("my-template.mappings.properties.host_name.type", templates), equalTo("keyword")); assertThat((Map<String, String>) extractValue("my-template.aliases.alias-1", templates), hasEntry("index_routing", "abc")); assertThat((Map<String, String>) extractValue("my-template.aliases.{index}-write", templates), hasEntry("search_routing", "xyz")); }	also for consistency with 'put mapping', i think we should rename testputtemplate -> testputtemplatewithtypes, and then rename this one testputuntypedtemplate -> testputtemplate.
public void testCustomScriptBinaryField() throws Exception { final byte[] randomBytesDoc1 = getRandomBytes(15); final byte[] randomBytesDoc2 = getRandomBytes(16); assertAcked( client().admin().indices().prepareCreate("my-index") .addMapping("my-type", createMappingSource("binary")) .setSettings(indexSettings()) ); client().prepareIndex("my-index", "my-type", "1") .setSource(jsonBuilder().startObject().field("binaryData", Base64.getEncoder().encodeToString(randomBytesDoc1)).endObject()) .get(); flush(); client().prepareIndex("my-index", "my-type", "2") .setSource(jsonBuilder().startObject().field("binaryData", Base64.getEncoder().encodeToString(randomBytesDoc2)).endObject()) .get(); flush(); refresh(); SearchResponse response = client().prepareSearch() .setQuery(scriptQuery( new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "doc['binaryData'].get(0).length > 15", Collections.emptyMap()))) .addScriptField("sbinaryData", new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "doc['binaryData'].get(0).length", Collections.emptyMap())) .get(); assertThat(response.getHits().totalHits(), equalTo(1L)); assertThat(response.getHits().getAt(0).id(), equalTo("2")); assertThat(response.getHits().getAt(0).fields().get("sbinaryData").values().get(0), equalTo(16)); }	this line is too long, it is causing checkstyle to fail. we have a line limit of 140 characters that we started enforcing a few months ago but only on files that didn't have any violations. we're super super slowly fixing the violations as we go. i'll fix this before i merge.
public void testCustomScriptBinaryField() throws Exception { final byte[] randomBytesDoc1 = getRandomBytes(15); final byte[] randomBytesDoc2 = getRandomBytes(16); assertAcked( client().admin().indices().prepareCreate("my-index") .addMapping("my-type", createMappingSource("binary")) .setSettings(indexSettings()) ); client().prepareIndex("my-index", "my-type", "1") .setSource(jsonBuilder().startObject().field("binaryData", Base64.getEncoder().encodeToString(randomBytesDoc1)).endObject()) .get(); flush(); client().prepareIndex("my-index", "my-type", "2") .setSource(jsonBuilder().startObject().field("binaryData", Base64.getEncoder().encodeToString(randomBytesDoc2)).endObject()) .get(); flush(); refresh(); SearchResponse response = client().prepareSearch() .setQuery(scriptQuery( new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "doc['binaryData'].get(0).length > 15", Collections.emptyMap()))) .addScriptField("sbinaryData", new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, "doc['binaryData'].get(0).length", Collections.emptyMap())) .get(); assertThat(response.getHits().totalHits(), equalTo(1L)); assertThat(response.getHits().getAt(0).id(), equalTo("2")); assertThat(response.getHits().getAt(0).fields().get("sbinaryData").values().get(0), equalTo(16)); }	this api call is forbidden and fails the build. random().nextbytes(randombytes); is fine though. i'll fix it before i merge.
public void testQueueSize() throws InterruptedException { final int capacity = randomIntBetween(1, 32); final BlockingQueue<Integer> blockingQueue = new ArrayBlockingQueue<>(capacity); final SizeBlockingQueue<Integer> sizeBlockingQueue = new SizeBlockingQueue<>(blockingQueue, capacity); // fill the queue to capacity for (int i = 0; i < capacity; i++) { sizeBlockingQueue.offer(i); } final CountDownLatch latch = new CountDownLatch(1); final CyclicBarrier barrier = new CyclicBarrier(2); final AtomicInteger maxSize = new AtomicInteger(); // this thread will repeatedly poll the size of the queue keeping track of the maximum size that it sees final int iterations = 1 << 16; final Thread queueSizeThread = new Thread(() -> { try { latch.await(); } catch (final InterruptedException e) { throw new RuntimeException(e); } for (int i = 0; i < iterations; i++) { try { // synchronize each iteration of checking the size with each iteration of offering, each iteration is a race barrier.await(); } catch (final BrokenBarrierException | InterruptedException e) { throw new RuntimeException(e); } maxSize.set(Math.max(maxSize.get(), sizeBlockingQueue.size())); } }); queueSizeThread.start(); // this thread will try to offer items to the queue while the queue size thread is polling the size final Thread queueOfferThread = new Thread(() -> { try { latch.await(); } catch (final InterruptedException e) { throw new RuntimeException(e); } for (int i = 0; i < iterations; i++) { try { // synchronize each iteration of checking the size with each iteration of offering, each iteration is a race barrier.await(); } catch (final BrokenBarrierException | InterruptedException e) { throw new RuntimeException(e); } sizeBlockingQueue.offer(capacity + i); } }); queueOfferThread.start(); // synchronize the start of the two threads latch.countDown(); // wait for the threads to finish queueOfferThread.join(); queueSizeThread.join(); // the maximum size of the queue should be equal to the capacity assertThat(maxSize.get(), equalTo(capacity)); }	looking at this i was wondering if the latch was needed with the introduction of the barrier, it can probably be removed.
public Object transformPredictedValue(Double value, String stringRep) { if (value == null) { return null; } switch(this) { case STRING: return stringRep == null ? value.toString() : stringRep; case BOOLEAN: if (isNumberQuickCheck(stringRep)) { try { // 1 is true, 0 is false return Integer.parseInt(stringRep) == 1; } catch (NumberFormatException nfe) { // do nothing, allow fall through to final fromDouble } } else if (isBoolQuickCheck(stringRep)) { // if we start with t/f case insensitive, it indicates boolean string return Boolean.parseBoolean(stringRep); } return fromDouble(value); case NUMBER: // Quick check to verify that the string rep is LIKELY a number // Still handles the case where it throws and then returns the underlying value if (isNumberQuickCheck(stringRep)) { try { return Long.parseLong(stringRep); } catch (NumberFormatException nfe) { // do nothing, allow fall through to final return } } return value; default: return value; } }	there is one place where this might cause a mistake. lets assume the class labels are something that are not true or false, taco and hamburger for classes 0 and 1 respectively if the user specifically selects the boolean prediction value type on inference, it will flag as follows: taco will be false (since it starts with t but is not the word true) hamburger will be true (since it doesn't start with t or f, we revert to the class number). to me, this is acceptable, as the user would have to specify the boolean result type. knowing their classes labels were not actually boolean :/.
public void writeJson() throws IOException { buffer.append('['); boolean first = true; for (ScheduledEvent event : scheduledEvents) { if (first) { first = false; } else { buffer.append(','); } try (XContentBuilder contentBuilder = XContentFactory.jsonBuilder()) { buffer.append(Strings.toString(event.writeJson(bucketSpan, contentBuilder))); } } buffer.append(']'); }	it's very non-standard in elasticsearch to have a json document whose outermost level is an array. that's why you've had to hand-craft a writer instead of using standard mechanisms. i think instead you should use a format similar to the [get scheduled events api response](https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-calendar-event.html#ml-get-calendar-event-example), with the document being an object at its outermost level, and containing a field events that is the array of scheduled events. if you can tolerate the superfluous count you could actually reuse getcalendareventsaction.response to create and print this. or create a new very simple toxcontentobject that contains a list<scheduledevent> and uses builder.field("events", scheduledevents) in its toxcontent method. then you'll need to change the c++ parser to accept the extra object layer and events field, but there needs to be a second pr on the c++ side anyway and since the current c++ code is disabled this pr can still be merged before the second round of c++ updates.
public void testSnapshotWithDateMath() { final String repo = "repo"; final AdminClient admin = client().admin(); final IndexNameExpressionResolver nameExpressionResolver = new IndexNameExpressionResolver(Settings.EMPTY); final String snapshotName = "<snapshot-{now/d}>"; final String expression = nameExpressionResolver.resolveDateMathExpression(snapshotName); logger.info("--> creating repository"); assertAcked(admin.cluster().preparePutRepository(repo).setType("fs") .setSettings(Settings.builder().put("location", randomRepoPath()) .put("compress", randomBoolean()))); logger.info("--> creating date math snapshot"); CreateSnapshotResponse snapshotResponse = admin.cluster().prepareCreateSnapshot(repo, snapshotName) .setIncludeGlobalState(true) .setWaitForCompletion(true) .execute().actionGet(); assertThat(snapshotResponse.status(), equalTo(RestStatus.OK)); SnapshotsStatusResponse response = admin.cluster().prepareSnapshotStatus(repo) .setSnapshots(expression) .execute().actionGet(); assertThat(response.getSnapshots(), hasSize(1)); assertThat(response.getSnapshots().get(0).getState().completed(), equalTo(true)); }	i think that if there is a day rollover between resolving the expression here and taking the snapshot, the test will fail. a fix might be to resolve the expression a second time after the snapshot was taken, and then use both resolved expressions when asking for the snapshot status.
public void testSnapshotWithDateMath() { final String repo = "repo"; final AdminClient admin = client().admin(); final IndexNameExpressionResolver nameExpressionResolver = new IndexNameExpressionResolver(Settings.EMPTY); final String snapshotName = "<snapshot-{now/d}>"; final String expression = nameExpressionResolver.resolveDateMathExpression(snapshotName); logger.info("--> creating repository"); assertAcked(admin.cluster().preparePutRepository(repo).setType("fs") .setSettings(Settings.builder().put("location", randomRepoPath()) .put("compress", randomBoolean()))); logger.info("--> creating date math snapshot"); CreateSnapshotResponse snapshotResponse = admin.cluster().prepareCreateSnapshot(repo, snapshotName) .setIncludeGlobalState(true) .setWaitForCompletion(true) .execute().actionGet(); assertThat(snapshotResponse.status(), equalTo(RestStatus.OK)); SnapshotsStatusResponse response = admin.cluster().prepareSnapshotStatus(repo) .setSnapshots(expression) .execute().actionGet(); assertThat(response.getSnapshots(), hasSize(1)); assertThat(response.getSnapshots().get(0).getState().completed(), equalTo(true)); }	just .get() will do. also no need for setincludeglobalstate(true), that's irrelevant for the test.
public void testPipelineIngestWithModelAliases() throws Exception { String regressionModelId = "test_regression_1"; putModel(regressionModelId, REGRESSION_CONFIG); String regressionModelId2 = "test_regression_2"; putModel(regressionModelId2, REGRESSION_CONFIG); String modelAlias = "test_regression"; postUpdateModelAlias(modelAlias, null, regressionModelId); client().performRequest(putPipeline("simple_regression_pipeline", pipelineDefinition(modelAlias, "regression"))); for (int i = 0; i < 10; i++) { client().performRequest(indexRequest("index_for_inference_test", "simple_regression_pipeline", generateSourceDoc())); } postUpdateModelAlias(modelAlias, regressionModelId, regressionModelId2); // Need to assert busy as loading the model and then switching the model alias can take time assertBusy(() -> { String source = "{\\\\n" + " \\\\"docs\\\\": [\\\\n" + " {\\\\"_source\\\\": {\\\\n" + " \\\\"col1\\\\": \\\\"female\\\\",\\\\n" + " \\\\"col2\\\\": \\\\"M\\\\",\\\\n" + " \\\\"col3\\\\": \\\\"none\\\\",\\\\n" + " \\\\"col4\\\\": 10\\\\n" + " }}]\\\\n" + "}"; Request request = new Request("POST", "_ingest/pipeline/simple_regression_pipeline/_simulate"); request.setJsonEntity(source); Response response = client().performRequest(request); String responseString = EntityUtils.toString(response.getEntity()); assertThat(responseString, containsString("\\\\"model_id\\\\":\\\\"test_regression_2\\\\"")); }, 30, TimeUnit.SECONDS); for (int i = 0; i < 5; i++) { client().performRequest(indexRequest("index_for_inference_test", "simple_regression_pipeline", generateSourceDoc())); } client().performRequest(new Request("DELETE", "_ingest/pipeline/simple_regression_pipeline")); client().performRequest(new Request("POST", "index_for_inference_test/_refresh")); Response searchResponse = client().performRequest(searchRequest("index_for_inference_test", QueryBuilders.boolQuery() .filter( QueryBuilders.existsQuery("ml.inference.regression.predicted_value")))); assertThat(EntityUtils.toString(searchResponse.getEntity()), containsString("\\\\"value\\\\":15")); searchResponse = client().performRequest(searchRequest("index_for_inference_test", QueryBuilders.boolQuery() .filter( QueryBuilders.termQuery("ml.inference.regression.model_id.keyword", regressionModelId)))); assertThat(EntityUtils.toString(searchResponse.getEntity()), containsString("\\\\"value\\\\":10")); searchResponse = client().performRequest(searchRequest("index_for_inference_test", QueryBuilders.boolQuery() .filter( QueryBuilders.termQuery("ml.inference.regression.model_id.keyword", regressionModelId2)))); assertThat(EntityUtils.toString(searchResponse.getEntity()), containsString("\\\\"value\\\\":5")); assertBusy(() -> { try (XContentParser parser = createParser(JsonXContent.jsonXContent, client().performRequest(new Request("GET", "_ml/trained_models/" + modelAlias + "/_stats")).getEntity().getContent())) { GetTrainedModelsStatsResponse response = GetTrainedModelsStatsResponse.fromXContent(parser); assertThat(response.toString(), response.getTrainedModelStats(), hasSize(1)); TrainedModelStats trainedModelStats = response.getTrainedModelStats().get(0); assertThat(trainedModelStats.getModelId(), equalTo(regressionModelId2)); assertThat(trainedModelStats.getInferenceStats(), is(notNullValue())); } catch (ResponseException ex) { //this could just mean shard failures. fail(ex.getMessage()); } }); }	value: 15 is the hit count? same where you filter by model id below? it took me a long time to realise value is hit count rather than a field written by the model could you add a comment please.
protected void masterOperation(Task task, PutTrainedModelAction.Request request, ClusterState state, ActionListener<Response> listener) { try { request.getTrainedModelConfig().ensureParsedDefinition(xContentRegistry); request.getTrainedModelConfig().getModelDefinition().getTrainedModel().validate(); } catch (IOException ex) { listener.onFailure(ExceptionsHelper.badRequestException("Failed to parse definition for [{}]", ex, request.getTrainedModelConfig().getModelId())); return; } catch (ElasticsearchException ex) { listener.onFailure(ExceptionsHelper.badRequestException("Definition for [{}] has validation failures.", ex, request.getTrainedModelConfig().getModelId())); return; } if (request.getTrainedModelConfig() .getInferenceConfig() .isTargetTypeSupported(request.getTrainedModelConfig() .getModelDefinition() .getTrainedModel() .targetType()) == false) { listener.onFailure(ExceptionsHelper.badRequestException( "Model [{}] inference config type [{}] does not support definition target type [{}]", request.getTrainedModelConfig().getModelId(), request.getTrainedModelConfig().getInferenceConfig().getName(), request.getTrainedModelConfig() .getModelDefinition() .getTrainedModel() .targetType())); return; } Version minCompatibilityVersion = request.getTrainedModelConfig() .getModelDefinition() .getTrainedModel() .getMinimalCompatibilityVersion(); if (state.nodes().getMinNodeVersion().before(minCompatibilityVersion)) { listener.onFailure(ExceptionsHelper.badRequestException( "Definition for [{}] requires that all nodes are at least version [{}]", request.getTrainedModelConfig().getModelId(), minCompatibilityVersion.toString())); return; } TrainedModelConfig trainedModelConfig = new TrainedModelConfig.Builder(request.getTrainedModelConfig()) .setVersion(Version.CURRENT) .setCreateTime(Instant.now()) .setCreatedBy("api_user") .setLicenseLevel(License.OperationMode.PLATINUM.description()) .setEstimatedHeapMemory(request.getTrainedModelConfig().getModelDefinition().ramBytesUsed()) .setEstimatedOperations(request.getTrainedModelConfig().getModelDefinition().getTrainedModel().estimatedNumOperations()) .build(); final ModelAliasMetadata currentMetadata = state.metadata().custom(ModelAliasMetadata.NAME); if (currentMetadata != null && currentMetadata.getModelId(trainedModelConfig.getModelId()) != null) { listener.onFailure(ExceptionsHelper.badRequestException( "requested model_id [{}] is the same as an existing model_alias. Model model_aliases and ids must be unique", request.getTrainedModelConfig().getModelId() )); return; } ActionListener<Void> tagsModelIdCheckListener = ActionListener.wrap( r -> trainedModelProvider.storeTrainedModel(trainedModelConfig, ActionListener.wrap( bool -> { TrainedModelConfig configToReturn = new TrainedModelConfig.Builder(trainedModelConfig).clearDefinition().build(); listener.onResponse(new PutTrainedModelAction.Response(configToReturn)); }, listener::onFailure )), listener::onFailure ); ActionListener<Void> modelIdTagCheckListener = ActionListener.wrap( r -> checkTagsAgainstModelIds(request.getTrainedModelConfig().getTags(), tagsModelIdCheckListener), listener::onFailure ); checkModelIdAgainstTags(request.getTrainedModelConfig().getModelId(), modelIdTagCheckListener); }	it is a common pattern to extract the meta from the state and check it is not null. consider a static helper modelaliasmetadata.fromstate(clusterstate s) that returns an static final _empty_ value in the case of null.
protected Query asQuery(org.elasticsearch.xpack.ql.expression.predicate.logical.BinaryLogic e, TranslatorHandler handler) { if (e instanceof And) { return and(e.source(), toQuery(e.left(), handler), toQuery(e.right(), handler)); } if (e instanceof Or) { return or(e.source(), toQuery(e.left(), handler), toQuery(e.right(), handler)); } return null; } } public static Object valueOf(Expression e) { if (e.foldable()) { return e.fold(); } throw new QlIllegalArgumentException("Cannot determine value for {}", e); } public static class Scalars extends ExpressionTranslator<ScalarFunction> { @Override protected Query asQuery(ScalarFunction f, TranslatorHandler handler) { return doTranslate(f, handler); } public static Query doTranslate(ScalarFunction f, TranslatorHandler handler) { Query q = ExpressionTranslators.Scalars.doKnownTranslate(f, handler); if (q != null) { return q; } if (f instanceof CIDRMatch) { CIDRMatch cm = (CIDRMatch) f; if (cm.input() instanceof FieldAttribute && Expressions.foldable(cm.addresses())) { String targetFieldName = handler.nameOf(((FieldAttribute) cm.input()).exactAttribute()); Set<Object> set = new LinkedHashSet<>(CollectionUtils.mapSize(cm.addresses().size())); for (Expression e : cm.addresses()) { set.add(valueOf(e)); } return new TermsQuery(f.source(), targetFieldName, set); } } return handler.wrapFunctionQuery(f, f, new ScriptQuery(f.source(), f.asScript())); } } public static class CaseSensitiveScalarFunctions extends ExpressionTranslator<CaseSensitiveScalarFunction> { @Override protected Query asQuery(CaseSensitiveScalarFunction f, TranslatorHandler handler) { return f.isCaseSensitive() ? doTranslate(f, handler) : null; } public static Query doTranslate(CaseSensitiveScalarFunction f, TranslatorHandler handler) { Expression field = null; Expression constant = null; if (f instanceof StringContains) { StringContains sc = (StringContains) f; field = sc.string(); constant = sc.substring(); } else if (f instanceof EndsWith) { EndsWith ew = (EndsWith) f; field = ew.input(); constant = ew.pattern(); } else { return null; } if (field instanceof FieldAttribute && constant.foldable()) { String targetFieldName = handler.nameOf(((FieldAttribute) field).exactAttribute()); String substring = (String) constant.fold(); String query = "*" + substring + (f instanceof StringContains ? "*" : ""); return new WildcardQuery(f.source(), targetFieldName, query); } return null; }	only if you make any other changes to this pr, i think this method should be placed after dotranslate.
static Mapping createDynamicUpdate(Mapping mapping, DocumentMapper docMapper, List<Mapper> dynamicMappers, List<RuntimeFieldType> dynamicRuntimeFields) { if (dynamicMappers.isEmpty() && dynamicRuntimeFields.isEmpty()) { return null; } RootObjectMapper root; if (dynamicMappers.isEmpty() == false) { root = createDynamicUpdate(mapping.root, docMapper, dynamicMappers); } else { root = mapping.root.copyAndReset(); } root.addRuntimeFields(dynamicRuntimeFields); return mapping.mappingUpdate(root); }	i think we should rework this to use objectmapper.builder so that we can make the actual mappings immutable, but let's do that in a followup.
public void testPropagateDynamicRuntimeWithDynamicMapper() throws Exception { DocumentMapper mapper = createDocumentMapper(topMapping(b -> { b.field("dynamic", false); b.startObject("properties"); { b.startObject("foo"); { b.field("type", "object"); b.field("dynamic", "runtime"); b.startObject("properties").endObject(); } b.endObject(); } b.endObject(); })); ParsedDocument doc = mapper.parse(source(b -> { b.startObject("foo"); { b.field("baz", "test"); b.startObject("bar").field("baz", "something").endObject(); } b.endObject(); })); assertNull(doc.rootDoc().getField("foo.bar.baz")); assertEquals("{\\\\"_doc\\\\":{\\\\"dynamic\\\\":\\\\"false\\\\"," + "\\\\"runtime\\\\":{\\\\"foo.bar.baz\\\\":{\\\\"type\\\\":\\\\"string\\\\"},\\\\"foo.baz\\\\":{\\\\"type\\\\":\\\\"string\\\\"}}," + "\\\\"properties\\\\":{\\\\"foo\\\\":{\\\\"dynamic\\\\":\\\\"runtime\\\\",\\\\"properties\\\\":{\\\\"bar\\\\":{\\\\"type\\\\":\\\\"object\\\\"}}}}}}", Strings.toString(doc.dynamicMappingsUpdate())); }	are we happy that this intermediate object with no concrete leaf fields gets added? it feels a bit weird to me, but i can see how it ends up being necessary because of all the book-keeping we do while we create dynamic mappings during parsing.
public void writeTo(StreamOutput out) throws IOException { InternalUserSerializationHelper.writeTo(user, out); authenticatedBy.writeTo(out); if (lookedUpBy != null) { out.writeBoolean(true); lookedUpBy.writeTo(out); } else { out.writeBoolean(false); } out.writeVInt(type.ordinal()); out.writeMap(metadata); }	can we please have a javadoc for this method? mention that this is used for ownership tests. i understand that: * two users authenticated by realms are considered "the same" if they have equal principals and that their realms have equal types. in addition, unless the realms are singletons, the realm names are also compared for equality. * an api key authentication passes this same user test only with itself. the owner user is not relevant. because of the api key special case, i would name this method something like equivalentownershipto, or maybe more clear samesubject (although we don't have a subject notion). anyway, just a nit.
public boolean sameUserAs(Authentication other) { if (AuthenticationType.API_KEY == getAuthenticationType() && AuthenticationType.API_KEY == other.getAuthenticationType()) { assert getUser().principal().equals(other.getUser().principal()) : "The same API key ID cannot be attributed to two different usernames"; return getMetadata().get(API_KEY_ID_KEY).equals(other.getMetadata().get(API_KEY_ID_KEY)); } if (false == getUser().principal().equals(other.getUser().principal())) { return false; } final RealmRef thisRealm = getSourceRealm(); final RealmRef otherRealm = other.getSourceRealm(); if (FileRealmSettings.TYPE.equals(thisRealm.getType()) || NativeRealmSettings.TYPE.equals(thisRealm.getType())) { return thisRealm.getType().equals(otherRealm.getType()); } return thisRealm.getName().equals(otherRealm.getName()) && thisRealm.getType().equals(otherRealm.getType()); }	looks like a misplaced assert. should be after the id equality check?
public boolean sameUserAs(Authentication other) { if (AuthenticationType.API_KEY == getAuthenticationType() && AuthenticationType.API_KEY == other.getAuthenticationType()) { assert getUser().principal().equals(other.getUser().principal()) : "The same API key ID cannot be attributed to two different usernames"; return getMetadata().get(API_KEY_ID_KEY).equals(other.getMetadata().get(API_KEY_ID_KEY)); } if (false == getUser().principal().equals(other.getUser().principal())) { return false; } final RealmRef thisRealm = getSourceRealm(); final RealmRef otherRealm = other.getSourceRealm(); if (FileRealmSettings.TYPE.equals(thisRealm.getType()) || NativeRealmSettings.TYPE.equals(thisRealm.getType())) { return thisRealm.getType().equals(otherRealm.getType()); } return thisRealm.getName().equals(otherRealm.getName()) && thisRealm.getType().equals(otherRealm.getType()); }	if the intention is to not consider the realm names when there could only be one realm of a given type, i suggest we extend this with the kerberos realm (and reserved, but that's less important because it is handled by the existing checks).
public boolean sameUserAs(Authentication other) { if (AuthenticationType.API_KEY == getAuthenticationType() && AuthenticationType.API_KEY == other.getAuthenticationType()) { assert getUser().principal().equals(other.getUser().principal()) : "The same API key ID cannot be attributed to two different usernames"; return getMetadata().get(API_KEY_ID_KEY).equals(other.getMetadata().get(API_KEY_ID_KEY)); } if (false == getUser().principal().equals(other.getUser().principal())) { return false; } final RealmRef thisRealm = getSourceRealm(); final RealmRef otherRealm = other.getSourceRealm(); if (FileRealmSettings.TYPE.equals(thisRealm.getType()) || NativeRealmSettings.TYPE.equals(thisRealm.getType())) { return thisRealm.getType().equals(otherRealm.getType()); } return thisRealm.getName().equals(otherRealm.getName()) && thisRealm.getType().equals(otherRealm.getType()); }	i think any user should have access to the resources owned by the anonymous user. if you agree, i propose we handle it in a follow-up.
private static boolean checkWaitRequirements(GetDiscoveredNodesRequest request, Set<DiscoveryNode> nodes) { List<String> requirements = request.getRequiredNodes(); if (requirements.size() != new HashSet<>(requirements).size()) { throw new IllegalArgumentException("There are duplicate entries in [cluster.initial_master_nodes]"); } if (nodes.size() < request.getWaitForNodes()) { return false; } final Set<DiscoveryNode> selectedNodes = new HashSet<>(); for (final String requirement : requirements) { final Set<DiscoveryNode> matchingNodes = nodes.stream().filter(n -> matchesRequirement(n, requirement)).collect(Collectors.toSet()); if (matchingNodes.isEmpty()) { return false; } if (matchingNodes.size() > 1) { throw new IllegalArgumentException("[" + requirement + "] matches " + matchingNodes); } for (final DiscoveryNode matchingNode : matchingNodes) { if (selectedNodes.add(matchingNode) == false) { throw new IllegalArgumentException("[" + matchingNode + "] matches " + requirements.stream().filter(r -> matchesRequirement(matchingNode, requirement)) .collect(Collectors.toList())); } } } return true; }	i think we don't need this, and if you omit it then you get more useful messages (i.e. it specifies one of the problematic requirements).
public Collection<FunctionDefinition> listFunctions(String pattern) { // It is worth double checking if we need this copy. These are immutable anyway. Pattern p = Strings.hasText(pattern) ? Pattern.compile(normalize(pattern)) : null; return defs.entrySet().stream() .filter(e -> p == null || p.matcher(e.getKey()).matches()) .map(e -> cloneDefinition(e.getKey(), e.getValue())) .collect(toList()); }	i find this approach great, clearer intent in the immutable objects strategy. (_winking towards replacechildren*_).
@Override public UnaryOperator<RestHandler> getRestHandlerWrapper(ThreadContext threadContext) { final boolean ssl = enabled && HTTP_SSL_ENABLED.get(settings); final SSLConfiguration httpSSLConfig = getSslService().getHttpTransportSSLConfiguration(); boolean extractClientCertificate = ssl && getSslService().isSSLClientAuthEnabled(httpSSLConfig); return handler -> new SecurityRestFilter(getLicenseState(), threadContext, authcService.get(), secondayAuthc.get(), handler, extractClientCertificate); }	this is the main part of the pr - it's the thing that actually achieves the result we're after, but i don't see anywhere where we test that this pr delivers the behaviour that we've decided on. that is, we need a testsecurityhandlerisalwaysinstalled
public void parse(DocumentParserContext context) throws IOException { try { if (hasScript) { throw new IllegalArgumentException("Cannot index data directly into a field with a [script] parameter"); } parseCreateField(context); } catch (Exception e) { String valuePreview = ""; try { XContentParser parser = context.parser(); Object complexValue = AbstractXContentParser.readValue(parser, HashMap::new); if (complexValue == null) { valuePreview = "null"; } else { valuePreview = complexValue.toString(); if (UnicodeUtil.UTF16toUTF8( valuePreview, 0, valuePreview.length(), new byte[UnicodeUtil.maxUTF8Length(valuePreview.length())] ) > BYTE_BLOCK_SIZE - 2) { valuePreview = valuePreview.substring(0, 30) + "..."; } } } catch (Exception innerException) { throw new MapperParsingException( "failed to parse field [{}] of type [{}] in document with id '{}'. " + "Could not parse field value preview,", e, fieldType().name(), fieldType().typeName(), context.sourceToParse().id() ); } throw new MapperParsingException( "failed to parse field [{}] of type [{}] in document with id '{}'. " + "Preview of field's value: '{}'", e, fieldType().name(), fieldType().typeName(), context.sourceToParse().id(), valuePreview ); } multiFields.parse(this, context); } /** * Parse the field value and populate the fields on {@link DocumentParserContext#doc()}	i'm not sure about this change here. it is a bit unrelated to the direct issue this pr is addressing, and needs discussion with a larger group of folks. can you back out this change?
*/ public synchronized void addAffixGroupUpdateConsumer(List<Setting.AffixSetting<?>> settings, BiConsumer<String, Settings> consumer) { List<SettingUpdater> affixUpdaters = new ArrayList<>(settings.size()); for (Setting.AffixSetting<?> setting : settings) { ensureSettingIsRegistered(setting); affixUpdaters.add(setting.newAffixUpdater((a,b)-> {}, logger, (a,b)-> {})); } addSettingsUpdater(new SettingUpdater<Map<String, Settings>>() { @Override public boolean hasChanged(Settings current, Settings previous) { return affixUpdaters.stream().anyMatch(au -> au.hasChanged(current, previous)); } @Override public Map<String, Settings> getValue(Settings current, Settings previous) { Set<String> namespaces = new HashSet<>(); Consumer<String> aConsumer = namespaces::add; for (Setting.AffixSetting<?> setting : settings) { SettingUpdater affixUpdaterA = setting.newAffixUpdater((k, v) -> aConsumer.accept(k), logger, (a, b) ->{}); affixUpdaterA.apply(current, previous); } Map<String, Settings> namespaceToSettings = new HashMap<>(namespaces.size()); for (String namespace : namespaces) { Set<String> concreteSettings = new HashSet<>(settings.size()); for (Setting.AffixSetting<?> setting : settings) { concreteSettings.add(setting.getConcreteSettingForNamespace(namespace).getKey()); } namespaceToSettings.put(namespace, current.filter(concreteSettings::contains)); } return namespaceToSettings; } @Override public void apply(Map<String, Settings> values, Settings current, Settings previous) { for (Map.Entry<String, Settings> entry : values.entrySet()) { consumer.accept(entry.getKey(), entry.getValue()); } } }); }	do we need the consumer<string> aconsumer = namespaces::add;? i mean we can just call namespaces.add(k) instead?
private void sendBanParentRequests(Map<String, List<DiscoveryNode>> childConnections, ActionListener<Void> listener, Function<List<DiscoveryNode>, BanParentTaskRequest> requestGenerator) { if (childConnections.isEmpty()) { listener.onResponse(null); return; } final int groupSize = childConnections.entrySet().stream() .mapToInt(e -> e.getKey().equals(RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY) ? e.getValue().size() : 1) .sum(); final GroupedActionListener<Void> groupedListener = new GroupedActionListener<>(ActionListener.map(listener, r -> null), groupSize); for (Map.Entry<String, List<DiscoveryNode>> entry : childConnections.entrySet()) { final String clusterAlias = entry.getKey(); if (clusterAlias.equals(RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY)) { final BanParentTaskRequest request = requestGenerator.apply(List.of()); for (DiscoveryNode node : entry.getValue()) { sendBanParentRequest(() -> transportService.getConnection(node), request, groupedListener); } } else { final ArrayList<DiscoveryNode> subNodes = new ArrayList<>(entry.getValue()); final DiscoveryNode targetNode = subNodes.remove(0); if (targetNode.getVersion().onOrAfter(Version.V_8_0_0)) { BanParentTaskRequest request = requestGenerator.apply(subNodes); sendBanParentRequest(() -> transportService.getRemoteClusterService().getConnection(targetNode, clusterAlias), request, groupedListener); } else { groupedListener.onResponse(null); // old versions do not support cancellation cross clusters } } } }	i also explored an alternative that tracks connections instead of pairs of <cluster, node>. it's simpler than the current approach, but we won't be able to reduce the cross-cluster messages. i think this optimization is important for the heartbeat component.
private void sendBanParentRequests(Map<String, List<DiscoveryNode>> childConnections, ActionListener<Void> listener, Function<List<DiscoveryNode>, BanParentTaskRequest> requestGenerator) { if (childConnections.isEmpty()) { listener.onResponse(null); return; } final int groupSize = childConnections.entrySet().stream() .mapToInt(e -> e.getKey().equals(RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY) ? e.getValue().size() : 1) .sum(); final GroupedActionListener<Void> groupedListener = new GroupedActionListener<>(ActionListener.map(listener, r -> null), groupSize); for (Map.Entry<String, List<DiscoveryNode>> entry : childConnections.entrySet()) { final String clusterAlias = entry.getKey(); if (clusterAlias.equals(RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY)) { final BanParentTaskRequest request = requestGenerator.apply(List.of()); for (DiscoveryNode node : entry.getValue()) { sendBanParentRequest(() -> transportService.getConnection(node), request, groupedListener); } } else { final ArrayList<DiscoveryNode> subNodes = new ArrayList<>(entry.getValue()); final DiscoveryNode targetNode = subNodes.remove(0); if (targetNode.getVersion().onOrAfter(Version.V_8_0_0)) { BanParentTaskRequest request = requestGenerator.apply(subNodes); sendBanParentRequest(() -> transportService.getRemoteClusterService().getConnection(targetNode, clusterAlias), request, groupedListener); } else { groupedListener.onResponse(null); // old versions do not support cancellation cross clusters } } } }	in case of "proxy mode" connections (see also proxyconnectionstrategy), targetnode.getversion() will always return version.current.minimumcompatibilityversion() afaics, which means that this request won't be send to those nodes. the actual version of the node on the other side is available using channel.getversion(). on the other hand channel.getnode will return the (possibly fake) discoverynode object that was used to create the connection. /cc: @tbrooks8 this is quite trappy, anything we can change in the transport in the short term to avoid this?
private void maybeCleanupOldBanMarkers(long nowInMillis) { assert Thread.holdsLock(banedParents); if (oldestBanMarkerInMillis - nowInMillis > banParentRetainingIntervalInMillis) { final Iterator<Map.Entry<TaskId, BanReason>> banIterator = banedParents.entrySet().iterator(); oldestBanMarkerInMillis = Long.MAX_VALUE; while (banIterator.hasNext()) { final Map.Entry<TaskId, BanReason> entry = banIterator.next(); final long elapsed = nowInMillis - entry.getValue().timeInMillis; // TODO: extends the time to live of the marker if (elapsed > banParentRetainingIntervalInMillis) { logger.debug("Clean up ban for the parent task [{}] after [{}]", entry.getKey(), TimeValue.timeValueMillis(elapsed)); banIterator.remove(); } else { oldestBanMarkerInMillis = Math.min(oldestBanMarkerInMillis, entry.getValue().timeInMillis); } } } }	we might need to implement the heartbeat component before this change.
* @param onChildTasksCompleted called when all child tasks are completed or failed * @return the set of current nodes that have outstanding child tasks */ public Map<String, List<DiscoveryNode>> startBanOnChildrenNodes(long taskId, Runnable onChildTasksCompleted) { final CancellableTaskHolder holder = cancellableTasks.get(taskId); if (holder != null) { return holder.startBan(onChildTasksCompleted); } else { onChildTasksCompleted.run(); return Collections.emptyMap(); } }	do we still need some of this for bwc?
public void sendPublishRequest(DiscoveryNode destination, PublishRequest publishRequest, ActionListener<PublishWithJoinResponse> responseActionListener) { assert publishRequest.getAcceptedState() == clusterChangedEvent.state() : "state got switched on us"; if (destination.equals(nodes.getLocalNode())) { // if publishing to self, use original request instead (see currentPublishRequestToSelf for explanation) final PublishRequest previousRequest = currentPublishRequestToSelf.getAndSet(publishRequest); assert previousRequest == null; final ActionListener<PublishWithJoinResponse> originalListener = responseActionListener; responseActionListener = new ActionListener<PublishWithJoinResponse>() { @Override public void onResponse(PublishWithJoinResponse publishWithJoinResponse) { final PublishRequest previousRequest = currentPublishRequestToSelf.getAndSet(null); assert previousRequest == publishRequest; originalListener.onResponse(publishWithJoinResponse); } @Override public void onFailure(Exception e) { final PublishRequest previousRequest = currentPublishRequestToSelf.getAndSet(null); assert previousRequest == publishRequest; originalListener.onFailure(e); } }; } if (sendFullVersion || !previousState.nodes().nodeExists(destination)) { logger.trace("sending full cluster state version {} to {}", newState.version(), destination); PublicationTransportHandler.this.sendFullClusterState(newState, serializedStates, destination, responseActionListener); } else { logger.trace("sending cluster state diff for version {} to {}", newState.version(), destination); PublicationTransportHandler.this.sendClusterStateDiff(newState, serializedDiffs, serializedStates, destination, responseActionListener); } }	i'd rather this used a different variable rather than mutating the parameter.
public Similarity getSimilarity() { return similarity; } } @Override protected boolean doEquals(HasChildQueryBuilder that) { return Objects.equals(query, that.query) && Objects.equals(type, that.type) && Objects.equals(scoreMode, that.scoreMode) && Objects.equals(minChildren, that.minChildren) && Objects.equals(maxChildren, that.maxChildren) && Objects.equals(innerHitBuilder, that.innerHitBuilder) && Objects.equals(ignoreUnmapped, that.ignoreUnmapped); } @Override protected int doHashCode() { return Objects.hash(query, type, scoreMode, minChildren, maxChildren, innerHitBuilder, ignoreUnmapped); } @Override protected QueryBuilder doRewrite(QueryRewriteContext queryShardContext) throws IOException { QueryBuilder rewrittenQuery = query.rewrite(queryShardContext); if (rewrittenQuery != query) { HasChildQueryBuilder hasChildQueryBuilder = new HasChildQueryBuilder(type, rewrittenQuery, minChildren, maxChildren, scoreMode, innerHitBuilder); hasChildQueryBuilder.ignoreUnmapped(ignoreUnmapped); return hasChildQueryBuilder; } return this; } @Override protected void extractInnerHitBuilders(Map<String, InnerHitContextBuilder> innerHits) { if (innerHitBuilder != null) { if (innerHits.containsKey(innerHitBuilder.getName())) { throw new IllegalArgumentException("innerHits already contains an entry for key [" + innerHitBuilder.getName() + "]"); } Map<String, InnerHitContextBuilder> children = new HashMap<>(); InnerHitContextBuilder.extractInnerHits(query, children); String name = innerHitBuilder.getName() != null ? innerHitBuilder.getName() : type; InnerHitContextBuilder innerHitContextBuilder = new ParentChildInnerHitContextBuilder(type, true, query, innerHitBuilder, children); innerHits.put(name, innerHitContextBuilder); }	if innerhitbuilder.getname() is null we use the type as the name (see below). can you build the name first and check in the inner hits map with the inferred one ?: string name = innerhitbuilder.getname() != null ? innerhitbuilder.getname() : type; if (innerhits.containskey(name)) { ...
public Similarity getSimilarity() { return similarity; } } @Override protected boolean doEquals(HasChildQueryBuilder that) { return Objects.equals(query, that.query) && Objects.equals(type, that.type) && Objects.equals(scoreMode, that.scoreMode) && Objects.equals(minChildren, that.minChildren) && Objects.equals(maxChildren, that.maxChildren) && Objects.equals(innerHitBuilder, that.innerHitBuilder) && Objects.equals(ignoreUnmapped, that.ignoreUnmapped); } @Override protected int doHashCode() { return Objects.hash(query, type, scoreMode, minChildren, maxChildren, innerHitBuilder, ignoreUnmapped); } @Override protected QueryBuilder doRewrite(QueryRewriteContext queryShardContext) throws IOException { QueryBuilder rewrittenQuery = query.rewrite(queryShardContext); if (rewrittenQuery != query) { HasChildQueryBuilder hasChildQueryBuilder = new HasChildQueryBuilder(type, rewrittenQuery, minChildren, maxChildren, scoreMode, innerHitBuilder); hasChildQueryBuilder.ignoreUnmapped(ignoreUnmapped); return hasChildQueryBuilder; } return this; } @Override protected void extractInnerHitBuilders(Map<String, InnerHitContextBuilder> innerHits) { if (innerHitBuilder != null) { if (innerHits.containsKey(innerHitBuilder.getName())) { throw new IllegalArgumentException("innerHits already contains an entry for key [" + innerHitBuilder.getName() + "]"); } Map<String, InnerHitContextBuilder> children = new HashMap<>(); InnerHitContextBuilder.extractInnerHits(query, children); String name = innerHitBuilder.getName() != null ? innerHitBuilder.getName() : type; InnerHitContextBuilder innerHitContextBuilder = new ParentChildInnerHitContextBuilder(type, true, query, innerHitBuilder, children); innerHits.put(name, innerHitContextBuilder); }	we should use the same format for the message than the option in the request, can you change innerhits to [inner_hits] ?
@Override protected void extractInnerHitBuilders(Map<String, InnerHitContextBuilder> innerHits) { if (innerHitBuilder != null) { if (innerHits.containsKey(innerHitBuilder.getName())) { throw new IllegalArgumentException("innerHits already contains an entry for key [" + innerHitBuilder.getName() + "]"); } Map<String, InnerHitContextBuilder> children = new HashMap<>(); InnerHitContextBuilder.extractInnerHits(query, children); String name = innerHitBuilder.getName() != null ? innerHitBuilder.getName() : type; InnerHitContextBuilder innerHitContextBuilder = new ParentChildInnerHitContextBuilder(type, false, query, innerHitBuilder, children); innerHits.put(name, innerHitContextBuilder); } }	see above, we should check with the inferred name as well if innerhitbuilder.getname is null.
@Override protected void extractInnerHitBuilders(Map<String, InnerHitContextBuilder> innerHits) { if (innerHitBuilder != null) { if (innerHits.containsKey(innerHitBuilder.getName())) { throw new IllegalArgumentException("innerHits already contains an entry for key [" + innerHitBuilder.getName() + "]"); } Map<String, InnerHitContextBuilder> children = new HashMap<>(); InnerHitContextBuilder.extractInnerHits(query, children); String name = innerHitBuilder.getName() != null ? innerHitBuilder.getName() : type; InnerHitContextBuilder innerHitContextBuilder = new ParentChildInnerHitContextBuilder(type, false, query, innerHitBuilder, children); innerHits.put(name, innerHitContextBuilder); } }	same here, innerhits-> [inner_hits]
@Override public void extractInnerHitBuilders(Map<String, InnerHitContextBuilder> innerHits) { if (innerHitBuilder != null) { if (innerHits.containsKey(innerHitBuilder.getName())) { throw new IllegalArgumentException("innerHits already contains an entry for key [" + innerHitBuilder.getName() + "]"); } Map<String, InnerHitContextBuilder> children = new HashMap<>(); InnerHitContextBuilder.extractInnerHits(query, children); String name = innerHitBuilder.getName() != null ? innerHitBuilder.getName() : path; InnerHitContextBuilder innerHitContextBuilder = new NestedInnerHitContextBuilder(path, query, innerHitBuilder, children); innerHits.put(name, innerHitContextBuilder); } }	here the path is used if the name is null, can you infer the name first and then check inside the map ?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeOptionalWriteable(aggregations); out.writeOptionalBoolean(explain); out.writeOptionalStreamable(fetchSourceContext); out.writeGenericValue(docValueFields); out.writeGenericValue(storedFieldNames); out.writeVInt(from); out.writeOptionalWriteable(highlightBuilder); boolean hasIndexBoost = indexBoost != null; out.writeBoolean(hasIndexBoost); if (hasIndexBoost) { writeIndexBoost(out); } out.writeOptionalFloat(minScore); out.writeOptionalNamedWriteable(postQueryBuilder); out.writeOptionalNamedWriteable(queryBuilder); boolean hasRescoreBuilders = rescoreBuilders != null; out.writeBoolean(hasRescoreBuilders); if (hasRescoreBuilders) { out.writeVInt(rescoreBuilders.size()); for (RescoreBuilder<?> rescoreBuilder : rescoreBuilders) { out.writeNamedWriteable(rescoreBuilder); } } boolean hasScriptFields = scriptFields != null; out.writeBoolean(hasScriptFields); if (hasScriptFields) { out.writeVInt(scriptFields.size()); for (ScriptField scriptField : scriptFields) { scriptField.writeTo(out); } } out.writeVInt(size); boolean hasSorts = sorts != null; out.writeBoolean(hasSorts); if (hasSorts) { out.writeVInt(sorts.size()); for (SortBuilder<?> sort : sorts) { out.writeNamedWriteable(sort); } } boolean hasStats = stats != null; out.writeBoolean(hasStats); if (hasStats) { out.writeVInt(stats.size()); for (String stat : stats) { out.writeString(stat); } } out.writeOptionalWriteable(suggestBuilder); out.writeVInt(terminateAfter); out.writeOptionalWriteable(timeout); out.writeBoolean(trackScores); out.writeOptionalBoolean(version); out.writeOptionalBytesReference(ext); out.writeBoolean(profile); out.writeOptionalWriteable(searchAfterBuilder); out.writeOptionalWriteable(sliceBuilder); }	while we are able to break the wire protocol, can you remove the boolean and just use size=0 on read to avoid building the map? may as well size a boolean over the wire and in the cache key
public void createAndIndexInferenceModelMetadata(ModelSizeInfo inferenceModelSize) { if (readyToStoreNewModel.compareAndSet(true, false) == false) { failureHandler.accept(ExceptionsHelper.serverError( "new inference model is attempting to be stored before completion previous model storage" )); return; } TrainedModelConfig trainedModelConfig = createTrainedModelConfig(inferenceModelSize); CountDownLatch latch = storeTrainedModelMetadata(trainedModelConfig); try { if (latch.await(STORE_TIMEOUT_SEC, TimeUnit.SECONDS) == false) { LOGGER.error("[{}] Timed out (30s) waiting for inference model metadata to be stored", analytics.getId()); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); this.readyToStoreNewModel.set(true); failureHandler.accept(ExceptionsHelper.serverError("interrupted waiting for inference model metadata to be stored")); } }	is this valid here? i.e. should you be allowing this to start storing new models before logging, etc. seems like there is a race to reset the currentmodelid.
public static LimitedRole createLimitedRole(Role fromRole, Role limitedByRole) { Objects.requireNonNull(limitedByRole, "limited by role is required to create limited role"); return new LimitedRole(fromRole.cluster(), fromRole.indices(), fromRole.application(), fromRole.runAs(), limitedByRole); }	i understand fromrole.names() is close to non-sense. but i wonder whether it is marginally justifiable to still keep them around just in case? i can be easily persuaded. :)
@Override public Automaton allowedActionsMatcher(String index) { final Automaton allowedMatcher = super.allowedActionsMatcher(index); final Automaton limitedByMatcher = limitedBy.allowedActionsMatcher(index); return Automatons.intersectAndMinimize(allowedMatcher, limitedByMatcher); } /** * Check if indices permissions allow for the given action, also checks whether the limited by role allows the given actions * * @param action indices action * @return {@code true} if action is allowed else returns {@code false}	gosh! is this a bug!?
X509ExtendedKeyManager createKeyManager(@Nullable Environment environment) { try { PrivateKey privateKey = readPrivateKey(keyPath, keyPassword, environment); if (privateKey == null) { throw new IllegalArgumentException("private key [" + keyPath + "] could not be loaded"); } Certificate[] certificateChain = getCertificateChain(environment); return CertParsingUtils.keyManager(certificateChain, privateKey, keyPassword.getChars()); } catch (IOException | UnrecoverableKeyException | NoSuchAlgorithmException | CertificateException | KeyStoreException e) { throw new ElasticsearchException("failed to initialize SSL KeyManagerFactory", e); } }	nit: maybe have a constant for certificate and truststore in trustconfig
private static PrivateKey readPrivateKey(String keyPath, SecureString keyPassword, Environment environment) throws IOException { final Path key = CertParsingUtils.resolvePath(keyPath, environment); try { return PemUtils.readPrivateKey(key, keyPassword::getChars); } catch (FileNotFoundException | NoSuchFileException fileException) { throw missingKeyConfigFile(fileException, "key", key); } catch (AccessDeniedException accessException) { throw unreadableKeyConfigFile(accessException, "key", key); } catch (AccessControlException securityException) { throw blockedKeyConfigFile(securityException, environment, "key", key); } }	nit: maybe have a constant for key and keystore in keyconfig
protected Expression rule(Expression e) { if (e instanceof IsNotNull) { if (((IsNotNull) e).field().nullable() == false) { return new Literal(e.location(), Expressions.name(e), Boolean.TRUE, DataType.BOOLEAN); } // see https://github.com/elastic/elasticsearch/issues/34876 // similar for IsNull once it gets introduced } else if (e instanceof In) { In in = (In) e; if (canFoldToNullLiteral(in.value())) { return Literal.of(in, null); } } else if (e instanceof BinaryLogic) { BinaryLogic bl = (BinaryLogic) e; // The expression can also be in the SELECT clause where we must keep the 3vl if (canFoldToNullLiteral(bl.left()) && canFoldToNullLiteral(bl.right())) { return Literal.of(bl, null); } } else if (canFoldToNullLiteral(e)) { return Literal.of(e, null); } return e; }	shouldn't be needed since the fallback if should nullify the value in the first run then fold in in the second.
protected Expression rule(Expression e) { if (e instanceof IsNotNull) { if (((IsNotNull) e).field().nullable() == false) { return new Literal(e.location(), Expressions.name(e), Boolean.TRUE, DataType.BOOLEAN); } // see https://github.com/elastic/elasticsearch/issues/34876 // similar for IsNull once it gets introduced } else if (e instanceof In) { In in = (In) e; if (canFoldToNullLiteral(in.value())) { return Literal.of(in, null); } } else if (e instanceof BinaryLogic) { BinaryLogic bl = (BinaryLogic) e; // The expression can also be in the SELECT clause where we must keep the 3vl if (canFoldToNullLiteral(bl.left()) && canFoldToNullLiteral(bl.right())) { return Literal.of(bl, null); } } else if (canFoldToNullLiteral(e)) { return Literal.of(e, null); } return e; }	this shouldn't be needed if and/or.nullable return the proper value.
@Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } Request other = (Request) obj; return Objects.equals(id, other.id) && Objects.equals(pageParams, other.pageParams); } } public static class Response extends BaseTasksResponse implements ToXContentObject { private List<DataFrameTransformStateAndStats> transformsStateAndStats; private long totalCount; public Response(List<DataFrameTransformStateAndStats> transformsStateAndStats) { super(Collections.emptyList(), Collections.emptyList()); this.transformsStateAndStats = ExceptionsHelper.requireNonNull(transformsStateAndStats, "transformsStateAndStats"); this.totalCount = transformsStateAndStats.size(); } public Response(List<DataFrameTransformStateAndStats> transformsStateAndStats, List<TaskOperationFailure> taskFailures, List<? extends ElasticsearchException> nodeFailures) { super(taskFailures, nodeFailures); this.transformsStateAndStats = ExceptionsHelper.requireNonNull(transformsStateAndStats, "transformsStateAndStats"); this.totalCount = transformsStateAndStats.size(); } public Response(StreamInput in) throws IOException { super(in); transformsStateAndStats = in.readList(DataFrameTransformStateAndStats::new); if (in.getVersion().onOrAfter(Version.V_7_3_0)) { totalCount = in.readLong(); } else { totalCount = transformsStateAndStats.size(); } } // Set the total count if it is different than transformsStateAndStats.size() public Response setTotalCount(long totalCount) { assert totalCount >= transformsStateAndStats.size(); this.totalCount = totalCount; return this; } public List<DataFrameTransformStateAndStats> getTransformsStateAndStats() { return transformsStateAndStats; } public long getTotalCount() { return totalCount; } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeList(transformsStateAndStats); if (out.getVersion().onOrAfter(Version.V_7_3_0)) { out.writeLong(totalCount); } } @Override public void readFrom(StreamInput in) { throw new UnsupportedOperationException("usage of Streamable is to be replaced by Writeable"); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); toXContentCommon(builder, params); builder.field(DataFrameField.COUNT.getPreferredName(), totalCount); builder.field(DataFrameField.TRANSFORMS.getPreferredName(), transformsStateAndStats); builder.endObject(); return builder; } @Override public int hashCode() { return Objects.hash(transformsStateAndStats, totalCount); } @Override public boolean equals(Object other) { if (this == other) { return true; } if (other == null || getClass() != other.getClass()) { return false; } final Response that = (Response) other; return Objects.equals(this.transformsStateAndStats, that.transformsStateAndStats) && this.totalCount == that.totalCount; } @Override public final String toString() { return Strings.toString(this); }	should we consider expecting the count in the constructor instead of setting it via a setter? i'm worried the constructor does not stress out the need to set the count. looking at anomaly jobs, we expect a querypage in the corresponding class. that includes the count. should we switch to that?
public boolean isBottomSortShardDisjoint(QueryShardContext context, SearchSortValuesAndFormats bottomSortValues) throws IOException { if (bottomSortValues == null || bottomSortValues.getRawSortValues().length == 0) { return false; } if (canRewriteToMatchNone() == false) { return false; } MappedFieldType fieldType = context.fieldMapper(fieldName); if (fieldType == null) { // unmapped return false; } if (fieldType.indexOptions() == IndexOptions.NONE) { return false; } DocValueFormat docValueFormat = bottomSortValues.getSortValueFormats()[0]; final DateMathParser dateMathParser; if (docValueFormat instanceof DocValueFormat.DateTime) { if (fieldType instanceof DateFieldType && ((DateFieldType) fieldType).resolution() == NANOSECONDS) { // no matter what docValueFormat = DocValueFormat.withNanosecondResolution(docValueFormat); } dateMathParser = ((DocValueFormat.DateTime) docValueFormat).getDateMathParser(); } else { dateMathParser = null; } Object bottomSortValue = bottomSortValues.getFormattedSortValues()[0]; Object minValue = order() == SortOrder.DESC ? bottomSortValue : null; Object maxValue = order() == SortOrder.DESC ? null : bottomSortValue; MappedFieldType.Relation relation = fieldType.isFieldWithinQuery(context.getIndexReader(), minValue, maxValue, true, true, null, dateMathParser, context); return relation == MappedFieldType.Relation.DISJOINT; }	is my understanding correct that we need this for the case when a shard is mapped as a date_nanos field while other fields are mapped as date fields?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(jobId); out.writeLong(timestamp.getTime()); out.writeDouble(anomalyScore); out.writeLong(bucketSpan); out.writeDouble(initialAnomalyScore); // bwc for recordCount if (out.getVersion().before(Version.V_5_5_0)) { out.writeInt(0); } out.writeList(records); out.writeLong(eventCount); out.writeBoolean(isInterim); out.writeList(bucketInfluencers); out.writeLong(processingTimeMs); // bwc for perPartitionMaxProbability if (out.getVersion().before(Version.V_5_5_0)) { out.writeGenericValue(Collections.emptyMap()); } // bwc for perPartitionNormalization if (out.getVersion().before(Version.V_6_5_0)) { out.writeGenericValue(Collections.emptyList()); } if (out.getVersion().onOrAfter(Version.V_6_2_0)) { out.writeStringList(scheduledEvents); } }	i was expecting this to be out.writelist(collections.emptylist());. did you try that out?
public RepositoryData removeSnapshot(final SnapshotId snapshotId, final ShardGenerations updatedShardGenerations) { Map<String, SnapshotId> newSnapshotIds = snapshotIds.values().stream() .filter(id -> !snapshotId.equals(id)) .collect(Collectors.toMap(SnapshotId::getUUID, Function.identity())); if (newSnapshotIds.size() == snapshotIds.size()) { throw new ResourceNotFoundException("Attempting to remove non-existent snapshot [{}] from repository data", snapshotId); } Map<String, SnapshotState> newSnapshotStates = new HashMap<>(snapshotStates); newSnapshotStates.remove(snapshotId.getUUID()); final Map<String, Version> newSnapshotVersions = new HashMap<>(snapshotVersions); newSnapshotVersions.remove(snapshotId.getUUID()); Map<IndexId, List<SnapshotId>> indexSnapshots = new HashMap<>(); for (final IndexId indexId : indices.values()) { List<SnapshotId> remaining; List<SnapshotId> snapshotIds = this.indexSnapshots.get(indexId); assert snapshotIds != null; final int listIndex = snapshotIds.indexOf(snapshotId); if (listIndex > -1) { if (snapshotIds.size() == 1) { // removing the snapshot will mean no more snapshots // have this index, so just skip over it continue; } remaining = new ArrayList<>(snapshotIds); remaining.remove(listIndex); remaining = List.copyOf(remaining); } else { remaining = snapshotIds; } indexSnapshots.put(indexId, remaining); } return new RepositoryData(genId, newSnapshotIds, newSnapshotStates, newSnapshotVersions, indexSnapshots, ShardGenerations.builder().putAll(shardGenerations).putAll(updatedShardGenerations) .retainIndicesAndPruneDeletes(indexSnapshots.keySet()).build() ); }	same thing here, copy of copy
public void testCloseCursor() throws SQLException, IOException { index("library", "1", builder -> { builder.field("name", "foo"); }); index("library", "2", builder -> { builder.field("name", "bar"); }); index("library", "3", builder -> { builder.field("name", "baz"); }); try (Connection connection = createConnection(connectionProperties())) { try (Statement statement = connection.createStatement()) { statement.setFetchSize(1); ResultSet results = statement.executeQuery(" SELECT name FROM library"); assertTrue(results.next()); results.close(); // force sending a cursor close since more pages are available assertTrue(results.isClosed()); } } }	wondering if something like testclosenocursor or testclosemissingcursor might be more fitting, since the cursor isn't actually present, but empty.
public static MediaType getResponseMediaType(RestRequest request, SqlClearCursorRequest sqlRequest) { if (Mode.isDedicatedClient(sqlRequest.requestInfo().mode()) && (sqlRequest.binaryCommunication() == null || sqlRequest.binaryCommunication())) { // enforce CBOR response for drivers and CLI (unless instructed differently through the config param) return XContentType.CBOR; } return mediaTypeFromHeaders(request); }	this shouldn't be necessary, see my previous comment.
private void failOnCompileConfigurationResolution(SourceSet sourceSet) { project.getConfigurations() .getByName(sourceSet.getCompileConfigurationName()) .getIncoming() .beforeResolve(resolvableDependencies -> { throw new GradleException( "Resolving configuration " + sourceSet.getCompileConfigurationName() + " is no longer supported. Use " + sourceSet.getImplementationConfigurationName() + " instead." ); }); }	we should always fail on resolution. no matter if the configuration actually contains a dependency or not.
boolean isJava8() { assert "Oracle Corporation".equals(jvmVendor()); return JavaVersion.current().equals(JavaVersion.parse("1.8")); } } static class AllPermissionCheck implements BootstrapCheck { @Override public final BootstrapCheckResult check(BootstrapContext context) { if (isAllPermissionGranted()) { return BootstrapCheck.BootstrapCheckResult.failure("granting the all permission effectively disables security"); } return BootstrapCheckResult.success(); } boolean isAllPermissionGranted() { final SecurityManager sm = System.getSecurityManager(); assert sm != null; try { sm.checkPermission(new AllPermission()); } catch (final SecurityException e) { return false; } return true; } } static class DiscoveryConfiguredCheck implements BootstrapCheck { @Override public BootstrapCheckResult check(BootstrapContext context) { if (ClusterBootstrapService.discoveryIsConfigured(context.settings)) { return BootstrapCheckResult.success(); } return BootstrapCheckResult.failure("the default discovery settings are unsuitable for production use"); }	explicitly mention the settings which need to be set?
public final String getValue() { return get(0); } } public static final class BytesRefs extends ScriptDocValues<BytesRef> { private final BinaryDocValuesField binaryDocValuesField; public BytesRefs(BinaryDocValuesField binaryDocValuesField) { this.binaryDocValuesField = binaryDocValuesField; } @Override public void setNextDocId(int docId) throws IOException { throw new UnsupportedOperationException(); } public BytesRef getValue() { throwIfEmpty(); return binaryDocValuesField.getValue(null); // default is ignored } @Override public BytesRef get(int index) { throwIfEmpty(); return binaryDocValuesField.getValue(null); // default is ignored } @Override public int size() { return binaryDocValuesField.size(); }	instead of passing in a null, do you think we should expose a getvalue without a default for the scriptdocvalues use? for number fields, having it's be even weirder to have an unused default.
* @param listener snapshot creation listener */ public void createSnapshot(final CreateSnapshotRequest request, final ActionListener<Snapshot> listener) { final String repositoryName = request.repository(); final String snapshotName = indexNameExpressionResolver.resolveDateMathExpression(request.snapshot()); validate(repositoryName, snapshotName); final SnapshotId snapshotId = new SnapshotId(snapshotName, UUIDs.randomBase64UUID()); // new UUID for the snapshot Repository repository = repositoriesService.repository(request.repository()); if (repository.isReadOnly()) { listener.onFailure( new RepositoryException(repository.getMetadata().name(), "cannot create snapshot in a readonly repository")); return; } final Snapshot snapshot = new Snapshot(repositoryName, snapshotId); final Map<String, Object> userMeta = repository.adaptUserMetadata(request.userMetadata()); repository.executeConsistentStateUpdate(repositoryData -> new ClusterStateUpdateTask() { private SnapshotsInProgress.Entry newEntry; @Override public ClusterState execute(ClusterState currentState) { // check if the snapshot name already exists in the repository if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName))) { throw new InvalidSnapshotNameException( repository.getMetadata().name(), snapshotName, "snapshot with the same name already exists"); } validate(repositoryName, snapshotName, currentState); final SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY); if (deletionsInProgress.hasDeletionsInProgress()) { throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, "cannot snapshot while a snapshot deletion is in-progress in [" + deletionsInProgress + "]"); } final RepositoryCleanupInProgress repositoryCleanupInProgress = currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY); if (repositoryCleanupInProgress.hasCleanupInProgress()) { throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, "cannot snapshot while a repository cleanup is in-progress in [" + repositoryCleanupInProgress + "]"); } final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); // Fail if there are any concurrently running snapshots. The only exception to this being a snapshot in INIT state from a // previous master that we can simply ignore and remove from the cluster state because we would clean it up from the // cluster state anyway in #applyClusterState. if (snapshots.entries().stream().anyMatch(entry -> entry.state() != State.INIT)) { throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, " a snapshot is already running"); } // Store newSnapshot here to be processed in clusterStateProcessed List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndexNames(currentState, request.indicesOptions(), true, request.indices())); final List<String> dataStreams = indexNameExpressionResolver.dataStreamNames(currentState, request.indicesOptions(), request.indices()); logger.trace("[{}][{}] creating snapshot for indices [{}]", repositoryName, snapshotName, indices); final List<IndexId> indexIds = repositoryData.resolveNewIndices(indices); final Version version = minCompatibleVersion(currentState.nodes().getMinNodeVersion(), repositoryData, null); ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards = shards(currentState, indexIds, useShardGenerations(version), repositoryData); if (request.partial() == false) { Set<String> missing = new HashSet<>(); for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards) { if (entry.value.state() == ShardState.MISSING) { missing.add(entry.key.getIndex().getName()); } } if (missing.isEmpty() == false) { // TODO: We should just throw here instead of creating a FAILED and hence useless snapshot in the repository newEntry = new SnapshotsInProgress.Entry( new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), false, State.FAILED, indexIds, dataStreams, threadPool.absoluteTimeInMillis(), repositoryData.getGenId(), shards, "Indices don't have primary shards " + missing, userMeta, version); } } if (newEntry == null) { newEntry = new SnapshotsInProgress.Entry( new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), request.partial(), State.STARTED, indexIds, dataStreams, threadPool.absoluteTimeInMillis(), repositoryData.getGenId(), shards, null, userMeta, version); } return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, SnapshotsInProgress.of(List.of(newEntry))).build(); } @Override public void onFailure(String source, Exception e) { logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to create snapshot", repositoryName, snapshotName), e); listener.onFailure(e); } @Override public void clusterStateProcessed(String source, ClusterState oldState, final ClusterState newState) { try { logger.info("snapshot [{}] started", snapshot); listener.onResponse(snapshot); } finally { if (newEntry.state().completed() || newEntry.shards().isEmpty()) { endSnapshot(newEntry, newState.metadata()); } } } @Override public TimeValue timeout() { return request.masterNodeTimeout(); } }, "create_snapshot [" + snapshotName + ']', listener::onFailure); }	just like for indices we should respect the data streams constraints given by the request and not simply always snapshot all data-streams when doing a global snapshot.
private static XContentBuilder getDynamicTemplates(XContentBuilder builder) throws IOException { return builder.startArray("dynamic_templates") .startObject() .startObject("strings") .field("match_mapping_type", "string") .startObject("mapping") .field("type", "keyword") .endObject() .endObject() .endObject() .endArray(); } /** * Creates the rollup mapping properties from the provided {@link RollupActionConfig}	can you preserve the indentation
@Override public ClusterState execute(ClusterState currentState) throws IOException { IndexMetadata rollupIndexMetadata = currentState.getMetadata().index(rollupIndexName); Index rollupIndex = rollupIndexMetadata.getIndex(); Map<String, String> idxMetadata = currentState.getMetadata().index(originalIndexName) .getCustomData(RollupIndexMetadata.TYPE); // Add the source index name to the rollup index metadata. If the original index is a rollup index itself // we will add the name of the raw index that we initially rolled up. String rollupSourceIndexName = idxMetadata != null ? idxMetadata.get(RollupIndexMetadata.SOURCE_INDEX_NAME_META_FIELD) : originalIndexName; Map<String, String> rollupIndexRollupMetadata = new HashMap<>(); rollupIndexRollupMetadata.put(RollupIndexMetadata.SOURCE_INDEX_NAME_META_FIELD, rollupSourceIndexName); // Add metadata about the rollup configuration RollupActionDateHistogramGroupConfig dateConfig = config.getGroupConfig().getDateHistogram(); WriteableZoneId rollupDateZoneId = WriteableZoneId.of(dateConfig.getTimeZone()); Map<String, List<String>> metricsConfig = new HashMap<>(); for (MetricConfig mconfig : config.getMetricsConfig()) { metricsConfig.put(mconfig.getField(), mconfig.getMetrics()); } RollupIndexMetadata rollupInfo = new RollupIndexMetadata(dateConfig.getInterval(), rollupDateZoneId, metricsConfig); // Serialize the metadata as JSON string and store it try (XContentBuilder builder = XContentFactory.jsonBuilder()) { rollupInfo.toXContent(builder, ToXContent.EMPTY_PARAMS); rollupIndexRollupMetadata.put(RollupIndexMetadata.ROLLUP_META_FIELD, Strings.toString(builder)); } Metadata.Builder metadataBuilder = Metadata.builder(currentState.metadata()) .put(IndexMetadata.builder(rollupIndexMetadata).putCustom(RollupIndexMetadata.TYPE, rollupIndexRollupMetadata)); IndexAbstraction originalIndex = currentState.getMetadata().getIndicesLookup().get(originalIndexName); if (originalIndex.getParentDataStream() != null) { // If rolling up a backing index of a data stream, add rolled up index to backing data stream DataStream originalDataStream = originalIndex.getParentDataStream().getDataStream(); List<Index> backingIndices = new ArrayList<>(originalDataStream.getIndices().size() + 1); // Adding rollup indices to the beginning of the list will prevent rollup indices from ever being // considered a write index backingIndices.add(rollupIndex); backingIndices.addAll(originalDataStream.getIndices()); DataStream dataStream = new DataStream(originalDataStream.getName(), originalDataStream.getTimeStampField(), backingIndices, originalDataStream.getGeneration(), originalDataStream.getMetadata()); metadataBuilder.put(dataStream); } return ClusterState.builder(currentState).metadata(metadataBuilder.build()).build(); }	that should be enough for now. the rest of the information will be recorded in the mapping so no need to introduce the rollup metadata object below.
public void testIndicesAliasesRequestTargetDataStreams() { final String dataStreamName = "my-data-stream"; IndexMetadata backingIndex = createBackingIndex(dataStreamName, 1).build(); Metadata.Builder mdBuilder = Metadata.builder() .put(backingIndex, false) .put(new DataStream(dataStreamName, createTimestampField("ts"), org.elasticsearch.common.collect.List.of(backingIndex.getIndex()), 1)); ClusterState state = ClusterState.builder(new ClusterName("_name")).metadata(mdBuilder).build(); { IndicesAliasesRequest.AliasActions aliasActions = IndicesAliasesRequest.AliasActions.add().index(dataStreamName); IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () -> indexNameExpressionResolver.concreteIndexNames(state, aliasActions)); assertEquals("The provided expression [" + dataStreamName + "] matches a data stream, specify the corresponding " + "concrete indices instead.", iae.getMessage()); } { IndicesAliasesRequest.AliasActions aliasActions = IndicesAliasesRequest.AliasActions.add().index("my-data-*").alias("my-data"); IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () -> indexNameExpressionResolver.concreteIndexNames(state, aliasActions)); assertEquals("The provided expression [my-data-*] matches a data stream, specify the corresponding concrete indices instead.", iae.getMessage()); } //{ // IndicesAliasesRequest.AliasActions aliasActions = IndicesAliasesRequest.AliasActions.add().index(dataStreamName) // .alias("my-data"); // String[] indices = indexNameExpressionResolver.concreteIndexNames(state, aliasActions); // assertEquals(1, indices.length); // assertEquals(backingIndex.getIndex().getName(), indices[0]); //} }	why is this commented out?
@SuppressForbidden(reason = "Object#notifyAll") private void updateCheckpoint() { assert Thread.holdsLock(this); assert getBitArrayForSeqNo(checkpoint + 1).get(seqNoToBitArrayOffset(checkpoint + 1)) : "updateCheckpoint is called but the bit following the checkpoint is not set"; try { // keep it simple for now, get the checkpoint one by one; in the future we can optimize and read words long bitArrayKey = getBitArrayKey(checkpoint); FixedBitSet current = processedSeqNo.get(bitArrayKey); if (current == null) { // the bit set corresponding to the checkpoint has already been removed, set ourselves up for the next bit set assert checkpoint % bitArraysSize == bitArraysSize - 1; current = processedSeqNo.get(++bitArrayKey); } do { checkpoint++; /* * The checkpoint always falls in the current bit set or we have already cleaned it; if it falls on the last bit of the * current bit set, we can clean it. */ if (checkpoint == lastSeqNoInBitArray(bitArrayKey)) { assert current != null; final FixedBitSet removed = processedSeqNo.remove(bitArrayKey); assert removed == current; current = processedSeqNo.get(++bitArrayKey); } } while (current != null && current.get(seqNoToBitArrayOffset(checkpoint + 1))); } finally { // notifies waiters in waitForOpsToComplete this.notifyAll(); } }	maybe use indexof(...) and then indexinsert(...) and indexget(...) respectively to avoid determining what the slot is for a key several times? java final int slot = processedseqno.indexof(bitarraykey); if (processedseqno.indexexists(slot) == false) { processedseqno.indexinsert(slot, bitarraykey, new fixedbitset(bitarrayssize)); } return processedseqno.indexget(slot);
@Override public void execute(SearchContext context) { LOGGER.trace("{}", new SearchContextRequestLog(context)); final FieldsVisitor fieldsVisitor; Map<String, Set<String>> storedToRequestedFields = new HashMap<>(); StoredFieldsContext storedFieldsContext = context.storedFieldsContext(); if (storedFieldsContext == null) { // no fields specified, default to return source if no explicit indication if (!context.hasScriptFields() && !context.hasFetchSourceContext()) { context.fetchSourceContext(new FetchSourceContext(true)); } fieldsVisitor = new FieldsVisitor(context.sourceRequested()); } else if (storedFieldsContext.fetchFields() == false) { // disable stored fields entirely fieldsVisitor = null; } else { for (String fieldNameOrPattern : context.storedFieldsContext().fieldNames()) { if (fieldNameOrPattern.equals(SourceFieldMapper.NAME)) { FetchSourceContext fetchSourceContext = context.hasFetchSourceContext() ? context.fetchSourceContext() : FetchSourceContext.FETCH_SOURCE; context.fetchSourceContext(new FetchSourceContext(true, fetchSourceContext.includes(), fetchSourceContext.excludes())); continue; } Collection<String> fieldNames = context.mapperService().simpleMatchToFullName(fieldNameOrPattern); for (String fieldName : fieldNames) { MappedFieldType fieldType = context.smartNameFieldType(fieldName); if (fieldType == null) { // Only fail if we know it is a object field, missing paths / fields shouldn't fail. if (context.getObjectMapper(fieldName) != null) { throw new IllegalArgumentException("field [" + fieldName + "] isn't a leaf field"); } } else { String storedField = fieldType.name(); Set<String> requestedFields = storedToRequestedFields.computeIfAbsent( storedField, key -> new HashSet<>()); requestedFields.add(fieldName); } } } boolean loadSource = context.sourceRequested(); if (storedToRequestedFields.isEmpty()) { // empty list specified, default to disable _source if no explicit indication fieldsVisitor = new FieldsVisitor(loadSource); } else { fieldsVisitor = new CustomFieldsVisitor(storedToRequestedFields.keySet(), loadSource); } } try { SearchHit[] hits = new SearchHit[context.docIdsToLoadSize()]; FetchSubPhase.HitContext hitContext = new FetchSubPhase.HitContext(); for (int index = 0; index < context.docIdsToLoadSize(); index++) { if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } int docId = context.docIdsToLoad()[context.docIdsToLoadFrom() + index]; int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves()); LeafReaderContext subReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex); int subDocId = docId - subReaderContext.docBase; final SearchHit searchHit; int rootDocId = findRootDocumentIfNested(context, subReaderContext, subDocId); if (rootDocId != -1) { searchHit = createNestedSearchHit(context, docId, subDocId, rootDocId, storedToRequestedFields, subReaderContext); } else { searchHit = createSearchHit(context, fieldsVisitor, docId, subDocId, storedToRequestedFields, subReaderContext); } hits[index] = searchHit; hitContext.reset(searchHit, subReaderContext, subDocId, context.searcher()); for (FetchSubPhase fetchSubPhase : fetchSubPhases) { fetchSubPhase.hitExecute(context, hitContext); } } if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } for (FetchSubPhase fetchSubPhase : fetchSubPhases) { fetchSubPhase.hitsExecute(context, hits); if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } } TotalHits totalHits = context.queryResult().getTotalHits(); long totalHitsAsLong = totalHits.relation == Relation.EQUAL_TO ? totalHits.value : -1; context.fetchResult().hits(new SearchHits(hits, totalHitsAsLong, context.queryResult().getMaxScore())); } catch (IOException e) { throw ExceptionsHelper.convertToElastic(e); } }	it might make sense to guard this log statement? (i.e. if logger.istraceenabled())?)
@Override public void execute(SearchContext searchContext) throws QueryPhaseExecutionException { if (searchContext.hasOnlySuggest()) { suggestPhase.execute(searchContext); // TODO: fix this once we can fetch docs for suggestions searchContext.queryResult().topDocs(new TopDocsAndMaxScore( new TopDocs(new TotalHits(0, TotalHits.Relation.EQUAL_TO), Lucene.EMPTY_SCORE_DOCS), Float.NaN), new DocValueFormat[0]); return; } LOGGER.trace("{}", new SearchContextRequestLog(searchContext)); // Pre-process aggregations as late as possible. In the case of a DFS_Q_T_F // request, preProcess is called on the DFS phase phase, this is why we pre-process them // here to make sure it happens during the QUERY phase aggregationPhase.preProcess(searchContext); final ContextIndexSearcher searcher = searchContext.searcher(); boolean rescore = execute(searchContext, searchContext.searcher(), searcher::setCheckCancelled); if (rescore) { // only if we do a regular search rescorePhase.execute(searchContext); } suggestPhase.execute(searchContext); aggregationPhase.execute(searchContext); if (searchContext.getProfilers() != null) { ProfileShardResult shardResults = SearchProfileShardResults .buildShardResults(searchContext.getProfilers()); searchContext.queryResult().profileResults(shardResults); } }	it might make sense to guard this log statement? (i.e. if logger.istraceenabled())?)
public static ClientYamlSuiteRestSpec parseFrom(FileSystem fileSystem, String optionalPathPrefix, String... paths) throws IOException { ClientYamlSuiteRestSpec restSpec = new ClientYamlSuiteRestSpec(); ClientYamlSuiteRestApiParser restApiParser = new ClientYamlSuiteRestApiParser(); for (String path : paths) { for (Path jsonFile : FileUtils.findJsonSpec(fileSystem, optionalPathPrefix, path)) { try (InputStream stream = Files.newInputStream(jsonFile)) { String filename = jsonFile.getFileName().toString(); try (XContentParser parser = JsonXContent.jsonXContent.createParser(NamedXContentRegistry.EMPTY, stream)) { if (filename.equals("_common.json")) { String currentFieldName = null; while (parser.nextToken() != XContentParser.Token.END_OBJECT) { if (parser.currentToken() == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (parser.currentToken() == XContentParser.Token.START_OBJECT && "params".equals(currentFieldName)) { while (parser.nextToken() == XContentParser.Token.FIELD_NAME) { String param = parser.currentName(); if (restSpec.globalParameters.contains(param)) { throw new IllegalArgumentException("Found duplicate global param [" + param + "]"); } restSpec.globalParameters.add(param); parser.nextToken(); if (parser.currentToken() != XContentParser.Token.START_OBJECT) { throw new IllegalArgumentException("Expected params field in rest api definition to " + "contain an object"); } parser.skipChildren(); } } } } else { ClientYamlSuiteRestApi restApi = restApiParser.parse(jsonFile.toString(), parser); String expectedApiName = filename.substring(0, filename.lastIndexOf('.')); if (restApi.getName().equals(expectedApiName) == false) { throw new IllegalArgumentException("found api [" + restApi.getName() + "] in [" + jsonFile.toString() + "]. " + "Each api is expected to have the same name as the file that defines it."); } restSpec.addApi(restApi); } } } catch (Exception ex) { throw new IOException("Can't parse rest spec file: [" + jsonFile + "]", ex); } } } return restSpec; }	i think it'd be easier to read if this were objectparser stuff.
public static ClientYamlSuiteRestSpec parseFrom(FileSystem fileSystem, String optionalPathPrefix, String... paths) throws IOException { ClientYamlSuiteRestSpec restSpec = new ClientYamlSuiteRestSpec(); ClientYamlSuiteRestApiParser restApiParser = new ClientYamlSuiteRestApiParser(); for (String path : paths) { for (Path jsonFile : FileUtils.findJsonSpec(fileSystem, optionalPathPrefix, path)) { try (InputStream stream = Files.newInputStream(jsonFile)) { String filename = jsonFile.getFileName().toString(); try (XContentParser parser = JsonXContent.jsonXContent.createParser(NamedXContentRegistry.EMPTY, stream)) { if (filename.equals("_common.json")) { String currentFieldName = null; while (parser.nextToken() != XContentParser.Token.END_OBJECT) { if (parser.currentToken() == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (parser.currentToken() == XContentParser.Token.START_OBJECT && "params".equals(currentFieldName)) { while (parser.nextToken() == XContentParser.Token.FIELD_NAME) { String param = parser.currentName(); if (restSpec.globalParameters.contains(param)) { throw new IllegalArgumentException("Found duplicate global param [" + param + "]"); } restSpec.globalParameters.add(param); parser.nextToken(); if (parser.currentToken() != XContentParser.Token.START_OBJECT) { throw new IllegalArgumentException("Expected params field in rest api definition to " + "contain an object"); } parser.skipChildren(); } } } } else { ClientYamlSuiteRestApi restApi = restApiParser.parse(jsonFile.toString(), parser); String expectedApiName = filename.substring(0, filename.lastIndexOf('.')); if (restApi.getName().equals(expectedApiName) == false) { throw new IllegalArgumentException("found api [" + restApi.getName() + "] in [" + jsonFile.toString() + "]. " + "Each api is expected to have the same name as the file that defines it."); } restSpec.addApi(restApi); } } } catch (Exception ex) { throw new IOException("Can't parse rest spec file: [" + jsonFile + "]", ex); } } } return restSpec; }	this looks like a good candidate for xcontentparser#map. not quite the same, but close enough?
@Override public Query existsQuery(QueryShardContext context) { if (hasDocValues()) { return new DocValuesFieldExistsQuery(name()); } else { return new TermQuery(new Term(FieldNamesFieldMapper.NAME, name())); } }	this is not related to this pr but keyedjsonfieldtype#existsquery should also use the _field_names field instead of a [prefix query](https://github.com/elastic/elasticsearch/blob/b2e0b996811ef80f020c5fa8690c7a1471f4f53c/server/src/main/java/org/elasticsearch/index/mapper/jsonfieldmapper.java#l292) ?
public MetaData build() { // TODO: We should move these datastructures to IndexNameExpressionResolver, this will give the following benefits: // 1) The datastructures will be rebuilt only when needed. Now during serializing we rebuild these datastructures // while these datastructures aren't even used. // 2) The aliasAndIndexLookup can be updated instead of rebuilding it all the time. final Set<String> allIndices = new HashSet<>(indices.size()); final List<String> visibleIndices = new ArrayList<>(); final List<String> allOpenIndices = new ArrayList<>(); final List<String> visibleOpenIndices = new ArrayList<>(); final List<String> allClosedIndices = new ArrayList<>(); final List<String> visibleClosedIndices = new ArrayList<>(); final Set<String> duplicateAliasesIndices = new HashSet<>(); for (ObjectCursor<IndexMetaData> cursor : indices.values()) { final IndexMetaData indexMetaData = cursor.value; final String name = indexMetaData.getIndex().getName(); boolean added = allIndices.add(name); assert added : "double index named [" + name + "]"; final boolean visible = IndexMetaData.INDEX_HIDDEN_SETTING.get(indexMetaData.getSettings()) == false; if (visible) { visibleIndices.add(name); } if (indexMetaData.getState() == IndexMetaData.State.OPEN) { allOpenIndices.add(name); if (visible) { visibleOpenIndices.add(name); } } else if (indexMetaData.getState() == IndexMetaData.State.CLOSE) { allClosedIndices.add(name); if (visible) { visibleClosedIndices.add(name); } } indexMetaData.getAliases().keysIt().forEachRemaining(duplicateAliasesIndices::add); } duplicateAliasesIndices.retainAll(allIndices); if (duplicateAliasesIndices.isEmpty() == false) { // iterate again and constructs a helpful message ArrayList<String> duplicates = new ArrayList<>(); for (ObjectCursor<IndexMetaData> cursor : indices.values()) { for (String alias : duplicateAliasesIndices) { if (cursor.value.getAliases().containsKey(alias)) { duplicates.add(alias + " (alias of " + cursor.value.getIndex() + ")"); } } } assert duplicates.size() > 0; throw new IllegalStateException("index and alias names need to be unique, but the following duplicates were found [" + Strings.collectionToCommaDelimitedString(duplicates) + "]"); } SortedMap<String, AliasOrIndex> aliasAndIndexLookup = Collections.unmodifiableSortedMap(buildAliasAndIndexLookup()); DataStreamMetadata dsMetadata = (DataStreamMetadata) customs.get(DataStreamMetadata.TYPE); if (dsMetadata != null) { for (DataStream ds : dsMetadata.dataStreams().values()) { if (aliasAndIndexLookup.containsKey(ds.getName())) { throw new IllegalStateException("data stream [" + ds.getName() + "] conflicts with existing index or alias"); } final String backingIndexPrefix = (ds.getName().startsWith(".") ? "" : ".") + ds.getName() + "-"; for (String indexName : allIndices) { if (indexName.startsWith(backingIndexPrefix)) { throw new IllegalStateException( "data stream [" + ds.getName() + "] could create backing indices that conflict with existing indices"); } } } } // build all concrete indices arrays: // TODO: I think we can remove these arrays. it isn't worth the effort, for operations on all indices. // When doing an operation across all indices, most of the time is spent on actually going to all shards and // do the required operations, the bottleneck isn't resolving expressions into concrete indices. String[] allIndicesArray = allIndices.toArray(Strings.EMPTY_ARRAY); String[] visibleIndicesArray = visibleIndices.toArray(Strings.EMPTY_ARRAY); String[] allOpenIndicesArray = allOpenIndices.toArray(Strings.EMPTY_ARRAY); String[] visibleOpenIndicesArray = visibleOpenIndices.toArray(Strings.EMPTY_ARRAY); String[] allClosedIndicesArray = allClosedIndices.toArray(Strings.EMPTY_ARRAY); String[] visibleClosedIndicesArray = visibleClosedIndices.toArray(Strings.EMPTY_ARRAY); return new MetaData(clusterUUID, clusterUUIDCommitted, version, coordinationMetaData, transientSettings, persistentSettings, hashesOfConsistentSettings, indices.build(), templates.build(), customs.build(), allIndicesArray, visibleIndicesArray, allOpenIndicesArray, visibleOpenIndicesArray, allClosedIndicesArray, visibleClosedIndicesArray, aliasAndIndexLookup); }	i'm not thrilled with this nested loop that will run in a cluster state update thread, but i think we need to validate against potential backing index conflicts.
public void setRoleDescriptors(List<RoleDescriptor> roleDescriptors) { this.roleDescriptors = (roleDescriptors == null) ? List.of() : Collections.unmodifiableList(roleDescriptors); }	if you are going to use list.of then collections.unmodifiablelist( should also be replaced by list.copyof . but, we don't have a general guidance, so i would go with the "surroundings", and raise a bigger refactoring issue for all requests? maybe worth raising a discussion in the team's meeting.
@Override public int hashCode() { return Objects.hash(id, type, nestedIdentity, version, source, fields, getHighlightFields(), Arrays.hashCode(matchedQueries), explanation, shard, innerHits, index, clusterAlias); }	seems like a leftover.
public static SearchHit createTestItem(XContentType xContentType, boolean withOptionalInnerHits, boolean withShardTarget) { int internalId = randomInt(); String uid = randomAlphaOfLength(10); Text type = new Text(randomAlphaOfLengthBetween(5, 10)); NestedIdentity nestedIdentity = null; if (randomBoolean()) { nestedIdentity = NestedIdentityTests.createTestItem(randomIntBetween(0, 2)); } Map<String, DocumentField> fields = new HashMap<>(); if (randomBoolean()) { fields = GetResultTests.randomDocumentFields(xContentType).v2(); } SearchHit hit = new SearchHit(internalId, uid, type, nestedIdentity, fields); if (frequently()) { if (rarely()) { hit.score(Float.NaN); } else { hit.score(randomFloat()); } } if (frequently()) { hit.sourceRef(RandomObjects.randomSource(random(), xContentType)); } if (randomBoolean()) { hit.version(randomLong()); } if (randomBoolean()) { hit.sortValues(SearchSortValuesTests.createTestItem()); } if (randomBoolean()) { int size = randomIntBetween(0, 5); Map<String, HighlightField> highlightFields = new HashMap<>(size); for (int i = 0; i < size; i++) { HighlightField testItem = HighlightFieldTests.createTestItem(); highlightFields.put(testItem.getName(), testItem); } hit.highlightFields(highlightFields); } if (randomBoolean()) { int size = randomIntBetween(0, 5); String[] matchedQueries = new String[size]; for (int i = 0; i < size; i++) { matchedQueries[i] = randomAlphaOfLength(5); } hit.matchedQueries(matchedQueries); } if (randomBoolean()) { hit.explanation(createExplanation(randomIntBetween(0, 5))); } if (withOptionalInnerHits) { int innerHitsSize = randomIntBetween(0, 3); if (innerHitsSize > 0) { Map<String, SearchHits> innerHits = new HashMap<>(innerHitsSize); for (int i = 0; i < innerHitsSize; i++) { innerHits.put(randomAlphaOfLength(5), SearchHitsTests.createTestItem(xContentType, false, withShardTarget)); } hit.setInnerHits(innerHits); } } if (withShardTarget) { String index = randomAlphaOfLengthBetween(5, 10); String clusterAlias = randomBoolean() ? null : randomAlphaOfLengthBetween(5, 10); hit.shard(new SearchShardTarget(randomAlphaOfLengthBetween(5, 10), new ShardId(new Index(index, IndexMetaData.INDEX_UUID_NA_VALUE), randomInt()), clusterAlias, OriginalIndices.NONE)); } return hit; }	should this still be v1?
public static <T> XContentTester<T> xContentTester( CheckedBiFunction<XContent, BytesReference, XContentParser, IOException> createParser, Supplier<T> instanceSupplier, CheckedBiConsumer<T, XContentBuilder, IOException> toXContent, CheckedFunction<XContentParser, T, IOException> fromXContent) { return new XContentTester<>( createParser, x -> instanceSupplier.get(), (testInstance, xContentType) -> { try (XContentBuilder builder = XContentBuilder.builder(xContentType.xContent())) { toXContent.accept(testInstance, builder); return BytesReference.bytes(builder); } }, fromXContent); }	i see! usually you don't need an xcontenttype before you make the instance, but for search hits it is different!
* @throws IOException If problem when generating the content */ private IndexRequest createSampleIndexRequest() throws IOException { final XContentType xContentType = randomFrom(XContentType.values()); IndexRequest indexRequest = new IndexRequest("index", "type"); indexRequest.source(XContentBuilder.builder(xContentType.xContent()).startObject().field("test", "test").endObject()); return indexRequest; }	if this is only used for testing deletions, i don't think we need to randomize the content type when indexing. let's use json? we can also provide it as a string.
@Override public InferencePipelineAggregationBuilder rewrite(QueryRewriteContext context) { if (model != null) { return this; } SetOnce<LocalModel> loadedModel = new SetOnce<>(); BiConsumer<Client, ActionListener<?>> modelLoadAction = (client, listener) -> modelLoadingService.get().getModelForSearch(modelId, listener.delegateFailure((delegate, model) -> { loadedModel.set(model); boolean isLicensed = licenseState.checkFeature(XPackLicenseState.Feature.MACHINE_LEARNING) || licenseState.isAllowedByLicense(model.getLicenseLevel()); if (isLicensed) { delegate.onResponse(null); } else { delegate.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)); } })); context.registerAsyncAction((client, listener) -> { if (XPackSettings.SECURITY_ENABLED.get(settings)) { // check the user has ml privileges SecurityContext securityContext = new SecurityContext(Settings.EMPTY, client.threadPool().getThreadContext()); useSecondaryAuthIfAvailable(securityContext, () -> { final String username = securityContext.getUser().principal(); final HasPrivilegesRequest privRequest = new HasPrivilegesRequest(); privRequest.username(username); privRequest.clusterPrivileges(GetTrainedModelsAction.NAME); privRequest.indexPrivileges(new RoleDescriptor.IndicesPrivileges[]{}); privRequest.applicationPrivileges(new RoleDescriptor.ApplicationResourcePrivileges[]{}); ActionListener<HasPrivilegesResponse> privResponseListener = ActionListener.wrap( r -> { if (r.isCompleteMatch()) { modelLoadAction.accept(client, listener); } else { listener.onFailure(Exceptions.authorizationError("user [" + username + "] does not have the privilege to get trained models so cannot use ml inference")); } }, listener::onFailure); client.execute(HasPrivilegesAction.INSTANCE, privRequest, privResponseListener); }); } else { modelLoadAction.accept(client, listener); } }); return new InferencePipelineAggregationBuilder(name, bucketPathMap, loadedModel::get, modelId, inferenceConfig, licenseState, settings); }	meh, pass the settings to the securitycontext as well, please.
public List<Realm> asList() { final XPackLicenseState licenseStateSnapshot = licenseState.copyCurrentLicenseState(); if ( XPackSettings.SECURITY_ENABLED.get(settings) == false) { return Collections.emptyList(); } if (licenseStateSnapshot.checkFeature(Feature.SECURITY_ALL_REALMS)) { return realms; } else if (licenseStateSnapshot.checkFeature(Feature.SECURITY_STANDARD_REALMS)) { return standardRealmsOnly; } else { // native realms are basic licensed, and always allowed, even for an expired license return nativeRealmsOnly; } }	the two above are part of the same story that we don't have to check for security enabled in all the trappy places to account for security enabled status to change while something was in-progress. i would remove them, but replacing these with asserts is an option too.
public void write(MethodWriter writer) { final org.objectweb.asm.Type type; final Class<?> clazz; if (augmentation != null) { assert java.lang.reflect.Modifier.isStatic(modifiers); clazz = augmentation; type = org.objectweb.asm.Type.getType(augmentation); } else { clazz = owner.clazz; type = owner.type; } if (java.lang.reflect.Modifier.isStatic(modifiers)) { // special case for interfaces where the interface function needs to be set to true // to reference the appropriate class file when calling a static interface method // java 8 did not check, but java 9 and 10 do if (java.lang.reflect.Modifier.isInterface(clazz.getModifiers())) { writer.visitMethodInsn(Opcodes.INVOKESTATIC, type.getInternalName(), name, getMethodType().toMethodDescriptorString(), true); } else { writer.invokeStatic(type, method); } } else if (java.lang.reflect.Modifier.isInterface(clazz.getModifiers())) { writer.invokeInterface(type, method); } else { writer.invokeVirtual(type, method); } }	i think this'd be more clear if you said something like "invokestatic assumes that it is not invoking a method on an interface."
static Request syncedFlush(SyncedFlushRequest syncedFlushRequest) { String[] indices = syncedFlushRequest.indices() == null ? Strings.EMPTY_ARRAY : syncedFlushRequest.indices(); String endpoint = endpoint(indices, "_flush", "synced"); Params syncedFlushparameters = Params.builder(); // This request takes no other parameters other than the indices. syncedFlushparameters.withIndicesOptions(syncedFlushRequest.indicesOptions()); return new Request(HttpPost.METHOD_NAME, endpoint, syncedFlushparameters.getParams(), null); }	i would remove this comment
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(Fields._SHARDS); shardCounts.toXContent(builder, params); builder.endObject(); for (Map.Entry<String, List<ShardsSyncedFlushResult>> indexEntry : shardsResultPerIndex.entrySet()) { List<ShardsSyncedFlushResult> indexResult = indexEntry.getValue(); builder.startObject(indexEntry.getKey()); ShardCounts indexShardCounts = calculateShardCounts(indexResult); indexShardCounts.toXContent(builder, params); if (indexShardCounts.failed > 0) { builder.startArray(Fields.FAILURES); for (ShardsSyncedFlushResult shardResults : indexResult) { if (shardResults.failed()) { builder.startObject(); builder.field(Fields.SHARD, shardResults.shardId().id()); builder.field(Fields.REASON, shardResults.failureReason()); builder.field(Fields.TOTAL_COPIES, shardResults.totalShards()); builder.field(Fields.SUCCESSFUL_COPIES, shardResults.successfulShards()); builder.endObject(); continue; } Map<ShardRouting, SyncedFlushService.ShardSyncedFlushResponse> failedShards = shardResults.failedShards(); for (Map.Entry<ShardRouting, SyncedFlushService.ShardSyncedFlushResponse> shardEntry : failedShards.entrySet()) { builder.startObject(); builder.field(Fields.SHARD, shardResults.shardId().id()); builder.field(Fields.REASON, shardEntry.getValue().failureReason()); builder.field(Fields.TOTAL_COPIES, shardResults.totalShards()); builder.field(Fields.SUCCESSFUL_COPIES, shardResults.successfulShards()); builder.field(Fields.ROUTING, shardEntry.getKey()); builder.endObject(); } } builder.endArray(); } builder.endObject(); } return builder; }	why adding these two new fields to the response here and above?
private static SyncedFlushResponse innerFromXContent(XContentParser parser) throws IOException { ShardCounts totalShardCounts = null; Map<String, ShardCounts> shardsCountsPerIndex = new HashMap<>(); Map<String, List<ShardsSyncedFlushResult>> shardsResultPerIndex = new HashMap<>(); // If it is an object we try to parse it for Fields._SHARD or for an index entry for (Token curToken = parser.currentToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, parser.currentToken(), parser::getTokenLocation); String fieldName = parser.currentName(); curToken = parser.nextToken(); Integer totalShards = null; Integer successfulShards = null; Integer failedShards = null; Map<ShardId, List<FailureContainer>> failures = new HashMap<>(); if (curToken == Token.START_OBJECT) { // Start parsing for _shard or for index for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { if (curToken == Token.FIELD_NAME) { String level2FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level2FieldName) { case Fields.TOTAL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalShards = parser.intValue(); break; case Fields.SUCCESSFUL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulShards = parser.intValue(); break; case Fields.FAILED: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); failedShards = parser.intValue(); break; case Fields.FAILURES: if (!fieldName.equals(Fields._SHARDS)) { ensureExpectedToken(Token.START_ARRAY, curToken, parser::getTokenLocation); for (curToken = parser.nextToken(); curToken != Token.END_ARRAY; curToken = parser.nextToken()) { ensureExpectedToken(Token.START_OBJECT, curToken, parser::getTokenLocation); ShardRouting routing = null; String failureReason = null; Integer totalCopies = null; Integer successfulCopies = null; ShardId shardId = null; XContentLocation startLocation = parser.getTokenLocation(); for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, curToken, parser::getTokenLocation); String level3FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level3FieldName) { case Fields.SHARD: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); shardId = new ShardId( fieldName, IndexMetaData.INDEX_UUID_NA_VALUE, parser.intValue() ); break; case Fields.REASON: ensureExpectedToken(Token.VALUE_STRING, curToken, parser::getTokenLocation); failureReason = parser.text(); break; case Fields.TOTAL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalCopies = parser.intValue(); break; case Fields.SUCCESSFUL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulCopies = parser.intValue(); break; case Fields.ROUTING: routing = ShardRouting.fromXContent(parser); break; default: // If something else skip it parser.skipChildren(); break; } } if (failureReason != null && shardId != null && totalCopies != null && successfulCopies != null) { // This is ugly but there is only one ShardsSyncedFlushResult for each shardId // so this will work. if (!failures.containsKey(shardId)) { failures.put(shardId, new ArrayList<>()); } failures.get(shardId).add( new FailureContainer(shardId, failureReason, totalCopies, successfulCopies, routing) ); } else { throw new ParsingException(startLocation, "Unable to construct ShardsSyncedFlushResult"); } } } else { parser.skipChildren(); } break; default: parser.skipChildren(); break; } } else { parser.skipChildren(); } } if (totalShards != null && successfulShards != null && failedShards != null) { ShardCounts shardCount = new ShardCounts(totalShards, successfulShards, failedShards); if (fieldName.equals(Fields._SHARDS)) { totalShardCounts = shardCount; } else { List<ShardsSyncedFlushResult> results = new ArrayList<>(); // All failures in this list belong to the same index for (Map.Entry<ShardId, List<FailureContainer>> entry: failures.entrySet()) { Map<ShardRouting, SyncedFlushService.ShardSyncedFlushResponse> shardResponses = new HashMap<>(); for (FailureContainer container: entry.getValue()) { if (container.shardRouting != null) { shardResponses.put(container.shardRouting, new SyncedFlushService.ShardSyncedFlushResponse(container.failureReason) ); } } // Size of entry.getValue() will at least be one FailureContainer container = entry.getValue().get(0); if (!shardResponses.isEmpty()) { results.add( new ShardsSyncedFlushResult(container.shardId, null, container.totalCopies, container.successfulCopies, shardResponses) ); } else { results.add( new ShardsSyncedFlushResult(container.shardId, container.totalCopies, container.successfulCopies, container.failureReason) ); } } shardsCountsPerIndex.put(fieldName, shardCount); shardsResultPerIndex.put(fieldName, results); } } } else { // Else leave this tree alone parser.skipChildren(); } } return new SyncedFlushResponse(totalShardCounts, shardsResultPerIndex, shardsCountsPerIndex); }	i am missing why we need this if. where do we print out _shards under failures?
private static SyncedFlushResponse innerFromXContent(XContentParser parser) throws IOException { ShardCounts totalShardCounts = null; Map<String, ShardCounts> shardsCountsPerIndex = new HashMap<>(); Map<String, List<ShardsSyncedFlushResult>> shardsResultPerIndex = new HashMap<>(); // If it is an object we try to parse it for Fields._SHARD or for an index entry for (Token curToken = parser.currentToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, parser.currentToken(), parser::getTokenLocation); String fieldName = parser.currentName(); curToken = parser.nextToken(); Integer totalShards = null; Integer successfulShards = null; Integer failedShards = null; Map<ShardId, List<FailureContainer>> failures = new HashMap<>(); if (curToken == Token.START_OBJECT) { // Start parsing for _shard or for index for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { if (curToken == Token.FIELD_NAME) { String level2FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level2FieldName) { case Fields.TOTAL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalShards = parser.intValue(); break; case Fields.SUCCESSFUL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulShards = parser.intValue(); break; case Fields.FAILED: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); failedShards = parser.intValue(); break; case Fields.FAILURES: if (!fieldName.equals(Fields._SHARDS)) { ensureExpectedToken(Token.START_ARRAY, curToken, parser::getTokenLocation); for (curToken = parser.nextToken(); curToken != Token.END_ARRAY; curToken = parser.nextToken()) { ensureExpectedToken(Token.START_OBJECT, curToken, parser::getTokenLocation); ShardRouting routing = null; String failureReason = null; Integer totalCopies = null; Integer successfulCopies = null; ShardId shardId = null; XContentLocation startLocation = parser.getTokenLocation(); for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, curToken, parser::getTokenLocation); String level3FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level3FieldName) { case Fields.SHARD: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); shardId = new ShardId( fieldName, IndexMetaData.INDEX_UUID_NA_VALUE, parser.intValue() ); break; case Fields.REASON: ensureExpectedToken(Token.VALUE_STRING, curToken, parser::getTokenLocation); failureReason = parser.text(); break; case Fields.TOTAL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalCopies = parser.intValue(); break; case Fields.SUCCESSFUL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulCopies = parser.intValue(); break; case Fields.ROUTING: routing = ShardRouting.fromXContent(parser); break; default: // If something else skip it parser.skipChildren(); break; } } if (failureReason != null && shardId != null && totalCopies != null && successfulCopies != null) { // This is ugly but there is only one ShardsSyncedFlushResult for each shardId // so this will work. if (!failures.containsKey(shardId)) { failures.put(shardId, new ArrayList<>()); } failures.get(shardId).add( new FailureContainer(shardId, failureReason, totalCopies, successfulCopies, routing) ); } else { throw new ParsingException(startLocation, "Unable to construct ShardsSyncedFlushResult"); } } } else { parser.skipChildren(); } break; default: parser.skipChildren(); break; } } else { parser.skipChildren(); } } if (totalShards != null && successfulShards != null && failedShards != null) { ShardCounts shardCount = new ShardCounts(totalShards, successfulShards, failedShards); if (fieldName.equals(Fields._SHARDS)) { totalShardCounts = shardCount; } else { List<ShardsSyncedFlushResult> results = new ArrayList<>(); // All failures in this list belong to the same index for (Map.Entry<ShardId, List<FailureContainer>> entry: failures.entrySet()) { Map<ShardRouting, SyncedFlushService.ShardSyncedFlushResponse> shardResponses = new HashMap<>(); for (FailureContainer container: entry.getValue()) { if (container.shardRouting != null) { shardResponses.put(container.shardRouting, new SyncedFlushService.ShardSyncedFlushResponse(container.failureReason) ); } } // Size of entry.getValue() will at least be one FailureContainer container = entry.getValue().get(0); if (!shardResponses.isEmpty()) { results.add( new ShardsSyncedFlushResult(container.shardId, null, container.totalCopies, container.successfulCopies, shardResponses) ); } else { results.add( new ShardsSyncedFlushResult(container.shardId, container.totalCopies, container.successfulCopies, container.failureReason) ); } } shardsCountsPerIndex.put(fieldName, shardCount); shardsResultPerIndex.put(fieldName, results); } } } else { // Else leave this tree alone parser.skipChildren(); } } return new SyncedFlushResponse(totalShardCounts, shardsResultPerIndex, shardsCountsPerIndex); }	i think that given our responses, this is always true?
private static SyncedFlushResponse innerFromXContent(XContentParser parser) throws IOException { ShardCounts totalShardCounts = null; Map<String, ShardCounts> shardsCountsPerIndex = new HashMap<>(); Map<String, List<ShardsSyncedFlushResult>> shardsResultPerIndex = new HashMap<>(); // If it is an object we try to parse it for Fields._SHARD or for an index entry for (Token curToken = parser.currentToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, parser.currentToken(), parser::getTokenLocation); String fieldName = parser.currentName(); curToken = parser.nextToken(); Integer totalShards = null; Integer successfulShards = null; Integer failedShards = null; Map<ShardId, List<FailureContainer>> failures = new HashMap<>(); if (curToken == Token.START_OBJECT) { // Start parsing for _shard or for index for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { if (curToken == Token.FIELD_NAME) { String level2FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level2FieldName) { case Fields.TOTAL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalShards = parser.intValue(); break; case Fields.SUCCESSFUL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulShards = parser.intValue(); break; case Fields.FAILED: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); failedShards = parser.intValue(); break; case Fields.FAILURES: if (!fieldName.equals(Fields._SHARDS)) { ensureExpectedToken(Token.START_ARRAY, curToken, parser::getTokenLocation); for (curToken = parser.nextToken(); curToken != Token.END_ARRAY; curToken = parser.nextToken()) { ensureExpectedToken(Token.START_OBJECT, curToken, parser::getTokenLocation); ShardRouting routing = null; String failureReason = null; Integer totalCopies = null; Integer successfulCopies = null; ShardId shardId = null; XContentLocation startLocation = parser.getTokenLocation(); for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, curToken, parser::getTokenLocation); String level3FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level3FieldName) { case Fields.SHARD: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); shardId = new ShardId( fieldName, IndexMetaData.INDEX_UUID_NA_VALUE, parser.intValue() ); break; case Fields.REASON: ensureExpectedToken(Token.VALUE_STRING, curToken, parser::getTokenLocation); failureReason = parser.text(); break; case Fields.TOTAL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalCopies = parser.intValue(); break; case Fields.SUCCESSFUL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulCopies = parser.intValue(); break; case Fields.ROUTING: routing = ShardRouting.fromXContent(parser); break; default: // If something else skip it parser.skipChildren(); break; } } if (failureReason != null && shardId != null && totalCopies != null && successfulCopies != null) { // This is ugly but there is only one ShardsSyncedFlushResult for each shardId // so this will work. if (!failures.containsKey(shardId)) { failures.put(shardId, new ArrayList<>()); } failures.get(shardId).add( new FailureContainer(shardId, failureReason, totalCopies, successfulCopies, routing) ); } else { throw new ParsingException(startLocation, "Unable to construct ShardsSyncedFlushResult"); } } } else { parser.skipChildren(); } break; default: parser.skipChildren(); break; } } else { parser.skipChildren(); } } if (totalShards != null && successfulShards != null && failedShards != null) { ShardCounts shardCount = new ShardCounts(totalShards, successfulShards, failedShards); if (fieldName.equals(Fields._SHARDS)) { totalShardCounts = shardCount; } else { List<ShardsSyncedFlushResult> results = new ArrayList<>(); // All failures in this list belong to the same index for (Map.Entry<ShardId, List<FailureContainer>> entry: failures.entrySet()) { Map<ShardRouting, SyncedFlushService.ShardSyncedFlushResponse> shardResponses = new HashMap<>(); for (FailureContainer container: entry.getValue()) { if (container.shardRouting != null) { shardResponses.put(container.shardRouting, new SyncedFlushService.ShardSyncedFlushResponse(container.failureReason) ); } } // Size of entry.getValue() will at least be one FailureContainer container = entry.getValue().get(0); if (!shardResponses.isEmpty()) { results.add( new ShardsSyncedFlushResult(container.shardId, null, container.totalCopies, container.successfulCopies, shardResponses) ); } else { results.add( new ShardsSyncedFlushResult(container.shardId, container.totalCopies, container.successfulCopies, container.failureReason) ); } } shardsCountsPerIndex.put(fieldName, shardCount); shardsResultPerIndex.put(fieldName, results); } } } else { // Else leave this tree alone parser.skipChildren(); } } return new SyncedFlushResponse(totalShardCounts, shardsResultPerIndex, shardsCountsPerIndex); }	we try not to throw exception when parsing responses as they may become a problem when it comes to forward compatibility. the client should be able to speak to future versions that have added fields, arrays or objects, by just reading what it knows and ignoring the rest.
private static SyncedFlushResponse innerFromXContent(XContentParser parser) throws IOException { ShardCounts totalShardCounts = null; Map<String, ShardCounts> shardsCountsPerIndex = new HashMap<>(); Map<String, List<ShardsSyncedFlushResult>> shardsResultPerIndex = new HashMap<>(); // If it is an object we try to parse it for Fields._SHARD or for an index entry for (Token curToken = parser.currentToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, parser.currentToken(), parser::getTokenLocation); String fieldName = parser.currentName(); curToken = parser.nextToken(); Integer totalShards = null; Integer successfulShards = null; Integer failedShards = null; Map<ShardId, List<FailureContainer>> failures = new HashMap<>(); if (curToken == Token.START_OBJECT) { // Start parsing for _shard or for index for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { if (curToken == Token.FIELD_NAME) { String level2FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level2FieldName) { case Fields.TOTAL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalShards = parser.intValue(); break; case Fields.SUCCESSFUL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulShards = parser.intValue(); break; case Fields.FAILED: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); failedShards = parser.intValue(); break; case Fields.FAILURES: if (!fieldName.equals(Fields._SHARDS)) { ensureExpectedToken(Token.START_ARRAY, curToken, parser::getTokenLocation); for (curToken = parser.nextToken(); curToken != Token.END_ARRAY; curToken = parser.nextToken()) { ensureExpectedToken(Token.START_OBJECT, curToken, parser::getTokenLocation); ShardRouting routing = null; String failureReason = null; Integer totalCopies = null; Integer successfulCopies = null; ShardId shardId = null; XContentLocation startLocation = parser.getTokenLocation(); for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, curToken, parser::getTokenLocation); String level3FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level3FieldName) { case Fields.SHARD: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); shardId = new ShardId( fieldName, IndexMetaData.INDEX_UUID_NA_VALUE, parser.intValue() ); break; case Fields.REASON: ensureExpectedToken(Token.VALUE_STRING, curToken, parser::getTokenLocation); failureReason = parser.text(); break; case Fields.TOTAL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalCopies = parser.intValue(); break; case Fields.SUCCESSFUL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulCopies = parser.intValue(); break; case Fields.ROUTING: routing = ShardRouting.fromXContent(parser); break; default: // If something else skip it parser.skipChildren(); break; } } if (failureReason != null && shardId != null && totalCopies != null && successfulCopies != null) { // This is ugly but there is only one ShardsSyncedFlushResult for each shardId // so this will work. if (!failures.containsKey(shardId)) { failures.put(shardId, new ArrayList<>()); } failures.get(shardId).add( new FailureContainer(shardId, failureReason, totalCopies, successfulCopies, routing) ); } else { throw new ParsingException(startLocation, "Unable to construct ShardsSyncedFlushResult"); } } } else { parser.skipChildren(); } break; default: parser.skipChildren(); break; } } else { parser.skipChildren(); } } if (totalShards != null && successfulShards != null && failedShards != null) { ShardCounts shardCount = new ShardCounts(totalShards, successfulShards, failedShards); if (fieldName.equals(Fields._SHARDS)) { totalShardCounts = shardCount; } else { List<ShardsSyncedFlushResult> results = new ArrayList<>(); // All failures in this list belong to the same index for (Map.Entry<ShardId, List<FailureContainer>> entry: failures.entrySet()) { Map<ShardRouting, SyncedFlushService.ShardSyncedFlushResponse> shardResponses = new HashMap<>(); for (FailureContainer container: entry.getValue()) { if (container.shardRouting != null) { shardResponses.put(container.shardRouting, new SyncedFlushService.ShardSyncedFlushResponse(container.failureReason) ); } } // Size of entry.getValue() will at least be one FailureContainer container = entry.getValue().get(0); if (!shardResponses.isEmpty()) { results.add( new ShardsSyncedFlushResult(container.shardId, null, container.totalCopies, container.successfulCopies, shardResponses) ); } else { results.add( new ShardsSyncedFlushResult(container.shardId, container.totalCopies, container.successfulCopies, container.failureReason) ); } } shardsCountsPerIndex.put(fieldName, shardCount); shardsResultPerIndex.put(fieldName, results); } } } else { // Else leave this tree alone parser.skipChildren(); } } return new SyncedFlushResponse(totalShardCounts, shardsResultPerIndex, shardsCountsPerIndex); }	maybe for readability we could split this into a couple of methods for each section / object?
private static SyncedFlushResponse innerFromXContent(XContentParser parser) throws IOException { ShardCounts totalShardCounts = null; Map<String, ShardCounts> shardsCountsPerIndex = new HashMap<>(); Map<String, List<ShardsSyncedFlushResult>> shardsResultPerIndex = new HashMap<>(); // If it is an object we try to parse it for Fields._SHARD or for an index entry for (Token curToken = parser.currentToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, parser.currentToken(), parser::getTokenLocation); String fieldName = parser.currentName(); curToken = parser.nextToken(); Integer totalShards = null; Integer successfulShards = null; Integer failedShards = null; Map<ShardId, List<FailureContainer>> failures = new HashMap<>(); if (curToken == Token.START_OBJECT) { // Start parsing for _shard or for index for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { if (curToken == Token.FIELD_NAME) { String level2FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level2FieldName) { case Fields.TOTAL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalShards = parser.intValue(); break; case Fields.SUCCESSFUL: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulShards = parser.intValue(); break; case Fields.FAILED: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); failedShards = parser.intValue(); break; case Fields.FAILURES: if (!fieldName.equals(Fields._SHARDS)) { ensureExpectedToken(Token.START_ARRAY, curToken, parser::getTokenLocation); for (curToken = parser.nextToken(); curToken != Token.END_ARRAY; curToken = parser.nextToken()) { ensureExpectedToken(Token.START_OBJECT, curToken, parser::getTokenLocation); ShardRouting routing = null; String failureReason = null; Integer totalCopies = null; Integer successfulCopies = null; ShardId shardId = null; XContentLocation startLocation = parser.getTokenLocation(); for (curToken = parser.nextToken(); curToken != Token.END_OBJECT; curToken = parser.nextToken()) { ensureExpectedToken(Token.FIELD_NAME, curToken, parser::getTokenLocation); String level3FieldName = parser.currentName(); curToken = parser.nextToken(); switch (level3FieldName) { case Fields.SHARD: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); shardId = new ShardId( fieldName, IndexMetaData.INDEX_UUID_NA_VALUE, parser.intValue() ); break; case Fields.REASON: ensureExpectedToken(Token.VALUE_STRING, curToken, parser::getTokenLocation); failureReason = parser.text(); break; case Fields.TOTAL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); totalCopies = parser.intValue(); break; case Fields.SUCCESSFUL_COPIES: ensureExpectedToken(Token.VALUE_NUMBER, curToken, parser::getTokenLocation); successfulCopies = parser.intValue(); break; case Fields.ROUTING: routing = ShardRouting.fromXContent(parser); break; default: // If something else skip it parser.skipChildren(); break; } } if (failureReason != null && shardId != null && totalCopies != null && successfulCopies != null) { // This is ugly but there is only one ShardsSyncedFlushResult for each shardId // so this will work. if (!failures.containsKey(shardId)) { failures.put(shardId, new ArrayList<>()); } failures.get(shardId).add( new FailureContainer(shardId, failureReason, totalCopies, successfulCopies, routing) ); } else { throw new ParsingException(startLocation, "Unable to construct ShardsSyncedFlushResult"); } } } else { parser.skipChildren(); } break; default: parser.skipChildren(); break; } } else { parser.skipChildren(); } } if (totalShards != null && successfulShards != null && failedShards != null) { ShardCounts shardCount = new ShardCounts(totalShards, successfulShards, failedShards); if (fieldName.equals(Fields._SHARDS)) { totalShardCounts = shardCount; } else { List<ShardsSyncedFlushResult> results = new ArrayList<>(); // All failures in this list belong to the same index for (Map.Entry<ShardId, List<FailureContainer>> entry: failures.entrySet()) { Map<ShardRouting, SyncedFlushService.ShardSyncedFlushResponse> shardResponses = new HashMap<>(); for (FailureContainer container: entry.getValue()) { if (container.shardRouting != null) { shardResponses.put(container.shardRouting, new SyncedFlushService.ShardSyncedFlushResponse(container.failureReason) ); } } // Size of entry.getValue() will at least be one FailureContainer container = entry.getValue().get(0); if (!shardResponses.isEmpty()) { results.add( new ShardsSyncedFlushResult(container.shardId, null, container.totalCopies, container.successfulCopies, shardResponses) ); } else { results.add( new ShardsSyncedFlushResult(container.shardId, container.totalCopies, container.successfulCopies, container.failureReason) ); } } shardsCountsPerIndex.put(fieldName, shardCount); shardsResultPerIndex.put(fieldName, results); } } } else { // Else leave this tree alone parser.skipChildren(); } } return new SyncedFlushResponse(totalShardCounts, shardsResultPerIndex, shardsCountsPerIndex); }	when can it happen that some of these is null?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject() .field(Fields.STATE, state()) .field(Fields.PRIMARY, primary()) .field(Fields.NODE, currentNodeId()) .field(Fields.RELOCATING_NODE, relocatingNodeId()) .field(Fields.SHARD, id()) .field(Fields.INDEX, getIndexName()); if (expectedShardSize != UNAVAILABLE_EXPECTED_SHARD_SIZE) { builder.field(Fields.EXPECTED_SHARD_SIZE_IN_BYTES, expectedShardSize); } if (recoverySource != null) { builder.field(Fields.RECOVERY_SOURCE, recoverySource); } if (allocationId != null) { builder.field(Fields.ALLOCATION_ID); allocationId.toXContent(builder, params); } if (unassignedInfo != null) { unassignedInfo.toXContent(builder, params); } return builder.endObject(); }	we've stopped making these fields objects in a past year or so and just started to use string constants or even quoted strings.
public void createResponse(String docId, Map<String, String> headers, R response, ActionListener<IndexResponse> listener) throws IOException { try { // TODO: Integrate with circuit breaker final XContentBuilder source = XContentFactory.jsonBuilder() .startObject() .field(HEADERS_FIELD, headers) .field(EXPIRATION_TIME_FIELD, response.getExpirationTime()) .directFieldAsBase64(RESULT_FIELD, os -> writeResponse(response, os)) .endObject(); final IndexRequest indexRequest = new IndexRequest(index) .create(true) .id(docId) .source(source); clientWithOrigin.index(indexRequest, listener); } catch (Exception e) { listener.onFailure(e); } }	i wonder if indexrequest could take a toxcontent would let you could avoid another copy.
static Request getFeatures(GetFeaturesRequest getFeaturesRequest) { String endpoint = "/_features"; Request request = new Request(HttpGet.METHOD_NAME, endpoint); RequestConverters.Params parameters = new RequestConverters.Params(); parameters.withMasterTimeout(getFeaturesRequest.masterNodeTimeout()); request.addParameters(parameters.asMap()); return request; }	i would remove this comment or add javadocs to the method
@TestLogging(value = "org.elasticsearch.transport.TransportService.tracer:trace") public void testTracerLog() throws Exception { TransportRequestHandler<TransportRequest> handler = (request, channel, task) -> channel.sendResponse(new StringMessageResponse("")); TransportRequestHandler<StringMessageRequest> handlerWithError = (request, channel, task) -> { if (request.timeout() > 0) { Thread.sleep(request.timeout); } channel.sendResponse(new RuntimeException("")); }; TransportResponseHandler<StringMessageResponse> noopResponseHandler = new TransportResponseHandler<StringMessageResponse>() { @Override public StringMessageResponse read(StreamInput in) throws IOException { return new StringMessageResponse(in); } @Override public void handleResponse(StringMessageResponse response) { } @Override public void handleException(TransportException exp) { } @Override public String executor() { return ThreadPool.Names.SAME; } }; serviceA.registerRequestHandler("internal:test", StringMessageRequest::new, ThreadPool.Names.SAME, handler); serviceA.registerRequestHandler("internal:testNotSeen", StringMessageRequest::new, ThreadPool.Names.SAME, handler); serviceA.registerRequestHandler("internal:testError", StringMessageRequest::new, ThreadPool.Names.SAME, handlerWithError); serviceB.registerRequestHandler("internal:test", StringMessageRequest::new, ThreadPool.Names.SAME, handler); serviceB.registerRequestHandler("internal:testNotSeen", StringMessageRequest::new, ThreadPool.Names.SAME, handler); serviceB.registerRequestHandler("internal:testError", StringMessageRequest::new, ThreadPool.Names.SAME, handlerWithError); String includeSettings; String excludeSettings; if (randomBoolean()) { // sometimes leave include empty (default) includeSettings = randomBoolean() ? "*" : ""; excludeSettings = "internal:testNotSeen"; } else { includeSettings = "internal:test,internal:testError"; excludeSettings = "DOESN'T_MATCH"; } clusterSettingsA.applySettings(Settings.builder() .put(TransportSettings.TRACE_LOG_INCLUDE_SETTING.getKey(), includeSettings) .put(TransportSettings.TRACE_LOG_EXCLUDE_SETTING.getKey(), excludeSettings) .build()); MockLogAppender appender = new MockLogAppender(); appender.start(); Loggers.addAppender(LogManager.getLogger("org.elasticsearch.transport.TransportService.tracer"), appender); try { final String requestSent = ".*\\\\\\\\[internal:test].*sent to.*\\\\\\\\{TS_B}.*"; final MockLogAppender.LoggingExpectation requestSentExpectation = new MockLogAppender.PatternSeenEventExpectation( "sent request", "org.elasticsearch.transport.TransportService.tracer", Level.TRACE, requestSent); final String requestReceived = ".*\\\\\\\\[internal:test].*received request.*"; final MockLogAppender.LoggingExpectation requestReceivedExpectation = new MockLogAppender.PatternSeenEventExpectation( "received request", "org.elasticsearch.transport.TransportService.tracer", Level.TRACE, requestReceived); final String responseSent = ".*\\\\\\\\[internal:test].*sent response.*"; final MockLogAppender.LoggingExpectation responseSentExpectation = new MockLogAppender.PatternSeenEventExpectation( "sent response", "org.elasticsearch.transport.TransportService.tracer", Level.TRACE, responseSent); final String responseReceived = ".*\\\\\\\\[internal:test].*received response from.*\\\\\\\\{TS_B}.*"; final MockLogAppender.LoggingExpectation responseReceivedExpectation = new MockLogAppender.PatternSeenEventExpectation( "received response", "org.elasticsearch.transport.TransportService.tracer", Level.TRACE, responseReceived); appender.addExpectation(requestSentExpectation); appender.addExpectation(requestReceivedExpectation); appender.addExpectation(responseSentExpectation); appender.addExpectation(responseReceivedExpectation); StringMessageRequest request = new StringMessageRequest("", 10); serviceA.sendRequest(nodeB, "internal:test", request, TransportRequestOptions.EMPTY, noopResponseHandler); assertBusy(appender::assertAllExpectationsMatched); final String errorResponseSent = ".*\\\\\\\\[internal:testError].*sent error response.*"; final MockLogAppender.LoggingExpectation errorResponseSentExpectation = new MockLogAppender.PatternSeenEventExpectation( "sent error response", "org.elasticsearch.transport.TransportService.tracer", Level.TRACE, errorResponseSent); final String errorResponseReceived = ".*\\\\\\\\[internal:testError].*received response from.*\\\\\\\\{TS_B}.*"; final MockLogAppender.LoggingExpectation errorResponseReceivedExpectation = new MockLogAppender.PatternSeenEventExpectation( "received error response", "org.elasticsearch.transport.TransportService.tracer", Level.TRACE, errorResponseReceived); appender.addExpectation(errorResponseSentExpectation); appender.addExpectation(errorResponseReceivedExpectation); serviceA.sendRequest(nodeB, "internal:testError", new StringMessageRequest(""), noopResponseHandler); assertBusy(appender::assertAllExpectationsMatched); final String notSeenSent = "*[internal:testNotSeen]*sent to*"; final MockLogAppender.LoggingExpectation notSeenSentExpectation = new MockLogAppender.UnseenEventExpectation( "not seen request sent", "org.elasticsearch.transport.TransportService.tracer", Level.TRACE, notSeenSent); final String notSeenReceived = ".*\\\\\\\\[internal:testNotSeen].*received request.*"; final MockLogAppender.LoggingExpectation notSeenReceivedExpectation = new MockLogAppender.PatternSeenEventExpectation( "not seen request received", "org.elasticsearch.transport.TransportService.tracer", Level.TRACE, notSeenReceived); appender.addExpectation(notSeenSentExpectation); appender.addExpectation(notSeenReceivedExpectation); PlainTransportFuture<StringMessageResponse> future = new PlainTransportFuture<>(noopResponseHandler); serviceA.sendRequest(nodeB, "internal:testNotSeen", new StringMessageRequest(""), future); future.txGet(); assertBusy(appender::assertAllExpectationsMatched); } finally { Loggers.removeAppender(LogManager.getLogger("org.elasticsearch.transport.TransportService.tracer"), appender); appender.stop(); } }	instead maybe to move this line into the try/finally after starting the appender?
public Engine.Operation convertToEngineOp(Translog.Operation operation, Engine.Operation.Origin origin) { switch (operation.opType()) { case INDEX: final Translog.Index index = (Translog.Index) operation; final String indexName = mapperService.index().getName(); final Engine.Index engineIndex = IndexShard.prepareIndex(docMapper(index.type()), new SourceToParse(indexName, index.type(), index.id(), index.source(), XContentHelper.xContentType(index.source()), index.routing()), index.seqNo(), index.primaryTerm(), index.version(), origin == Engine.Operation.Origin.PRIMARY ? VersionType.EXTERNAL : null, origin, index.getAutoGeneratedIdTimestamp(), true, SequenceNumbers.UNASSIGNED_SEQ_NO, 0); return engineIndex; case DELETE: final Translog.Delete delete = (Translog.Delete) operation; final Engine.Delete engineDelete = new Engine.Delete(delete.type(), delete.id(), delete.uid(), delete.seqNo(), delete.primaryTerm(), delete.version(), origin == Engine.Operation.Origin.PRIMARY ? VersionType.EXTERNAL : null, origin, System.nanoTime(), SequenceNumbers.UNASSIGNED_SEQ_NO, 0); return engineDelete; case NO_OP: final Translog.NoOp noOp = (Translog.NoOp) operation; final Engine.NoOp engineNoOp = new Engine.NoOp(noOp.seqNo(), noOp.primaryTerm(), origin, System.nanoTime(), noOp.reason()); return engineNoOp; default: throw new IllegalStateException("No operation defined for [" + operation + "]"); } }	perhaps write the code the same way as in indexshard: // if a translog op is replayed on the primary (eg. ccr), we need to use external instead of null for its version type. final versiontype versiontype = (origin == engine.operation.origin.primary) ? versiontype.external : null;
private void fetchOperations(AtomicBoolean stopped, AtomicLong lastFetchedSeqNo, InternalEngine leader, FollowingEngine follower) throws IOException { final MapperService mapperService = EngineTestCase.createMapperService("test"); final TranslogHandler translogHandler = new TranslogHandler(xContentRegistry(), follower.config().getIndexSettings()); while (stopped.get() == false) { final long checkpoint = leader.getProcessedLocalCheckpoint(); final long lastSeqNo = lastFetchedSeqNo.get(); if (lastSeqNo < checkpoint) { final long nextSeqNo = randomLongBetween(lastSeqNo + 1, checkpoint); if (lastFetchedSeqNo.compareAndSet(lastSeqNo, nextSeqNo)) { // extends the fetch range so we may deliver some overlapping operations more than once. final long fromSeqNo = randomLongBetween(Math.max(lastSeqNo - 5, 0), lastSeqNo + 1); final long toSeqNo = randomLongBetween(nextSeqNo, Math.min(nextSeqNo + 5, checkpoint)); try (Translog.Snapshot snapshot = shuffleSnapshot(leader.newChangesSnapshot("test", mapperService, fromSeqNo, toSeqNo, true))) { follower.advanceMaxSeqNoOfUpdatesOrDeletes(leader.getMaxSeqNoOfUpdatesOrDeletes()); Translog.Operation op; while ((op = snapshot.next()) != null) { EngineTestCase.applyOperation(follower, translogHandler.convertToEngineOp(op, randomFrom(Engine.Operation.Origin.values()))); } follower.syncTranslog(); } } } } }	transloghandler is no longer needed now
public Set<String> resolveExpressions(ClusterState state, String... expressions) { Context context = new Context(state, IndicesOptions.lenientExpandOpen(), true, false); List<String> resolvedExpressions = Arrays.asList(expressions); for (ExpressionResolver expressionResolver : expressionResolvers) { resolvedExpressions = expressionResolver.resolve(context, resolvedExpressions); } return Collections.unmodifiableSet(new HashSet<>(resolvedExpressions)); } /** * Iterates through the list of indices and selects the effective list of filtering aliases for the * given index. * <p>Only aliases with filters are returned. If the indices list contains a non-filtering reference to * the index itself - null is returned. Returns {@code null} if no filtering is required. * <b>NOTE</b>: The provided expressions must have been resolved already via {@link #resolveExpressions}	i think we should also fix the expressionresolver to take and return a set<string>. i can see wildcardexpressionresolver is already converting from set to list internally. i am ok with doing this as a followup. either way is fine
*/ public Set<KType> keySet() { final Iterator<KType> keysIterator = keysIt(); return new AbstractSet<>() { @Override public Iterator<KType> iterator() { return keysIterator; } @Override public int size() { return map.size(); } }; }	this doesn't look like it provides a complete set view of the keys, there should also be some other methods overriden for contains/etc?
*/ public Set<KType> keySet() { final Iterator<KType> keysIterator = keysIt(); return new AbstractSet<>() { @Override public Iterator<KType> iterator() { return keysIterator; } @Override public int size() { return map.size(); } }; }	shouldn't iterator() create a new iterator, ie call keysit()?
public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject("nodes"); { for (ObjectObjectCursor<String, DiskUsage> c : this.leastAvailableSpaceUsage) { builder.startObject(c.key); { // node builder.field("node_name", c.value.getNodeName()); builder.startObject("least_available"); { c.value.toShortXContent(builder); } builder.endObject(); // end "least_available" builder.startObject("most_available"); { DiskUsage most = this.mostAvailableSpaceUsage.get(c.key); if (most != null) { most.toShortXContent(builder); } } builder.endObject(); // end "most_available" } builder.endObject(); // end $nodename } } builder.endObject(); // end "nodes" builder.startObject("shard_sizes"); { for (ObjectObjectCursor<ShardId, Long> c : this.shardSizes) { final String shardKey; if (BWC_SHARD_ID_UUID.equals(c.key.getIndex().getUUID())) { shardKey = c.key.getIndexName(); } else { shardKey = c.key.toString(); } builder.humanReadableField(shardKey + "_bytes", shardKey, new ByteSizeValue(c.value)); } } builder.endObject(); // end "shard_sizes" builder.startObject("shard_paths"); { for (ObjectObjectCursor<ShardRouting, String> c : this.routingToDataPath) { builder.field(c.key.toString(), c.value); } } builder.endObject(); // end "shard_paths" return builder; }	this is arguably a breaking change since we will no longer include the [p] or [r] suffix in the shard sizes reported by the allocation explain output. we could fake these out for bwc, reporting the size of each shard twice, and allowing users to opt-in to the future behaviour here with a system property. tbd.
public void testAutoCreationDisabled() { Settings settings = Settings.builder().put(AutoCreateIndex.AUTO_CREATE_INDEX_SETTING.getKey(), false).build(); AutoCreateIndex autoCreateIndex = newAutoCreateIndex(settings); String randomIndex = randomAlphaOfLengthBetween(1, 10); IndexNotFoundException e = expectThrows(IndexNotFoundException.class, () -> autoCreateIndex.shouldAutoCreate(randomIndex, false, buildClusterState())); assertEquals("no such index [" + randomIndex + "] and [action.auto_create_index] is [false]", e.getMessage()); }	where does the defaulting of request-level setting to index-level setting happen?
public static char StringTochar(final String value) { if (value == null) { throw new ClassCastException( "cannot cast " + "null " + String.class.getCanonicalName() + " to " + char.class.getCanonicalName() ); } if (value.length() != 1) { throw new ClassCastException( "cannot cast " + String.class.getCanonicalName() + " with length not equal to one to " + char.class.getCanonicalName() ); } return value.charAt(0); }	just the number is fine, no need to spend lines on adding the ordinal suffix.
private void replaceCallWithConstant( InvokeCallMemberNode irInvokeCallMemberNode, Consumer<ExpressionNode> scope, Method javaMethod, Object receiver ) { Object[] args = new Object[irInvokeCallMemberNode.getArgumentNodes().size()]; for (int i = 0; i < irInvokeCallMemberNode.getArgumentNodes().size(); i++) { ExpressionNode argNode = irInvokeCallMemberNode.getArgumentNodes().get(i); IRDConstant constantDecoration = argNode.getDecoration(IRDConstant.class); if (constantDecoration == null) { // offering the symbol name in error message (CastNode was evolved from ESymbol) String argumentName = argNode instanceof CastNode ? ((CastNode) argNode).getChildNode().getDecoration(IRDName.class).getValue() : ""; throw irInvokeCallMemberNode .getLocation() .createError(new IllegalArgumentException(String.format( Locale.ROOT, "All arguments of the [%s] method must be constants, but the [%s] argument [%s] is not", javaMethod.getName(), Utility.toOrdinal(i+1), argumentName ))); } args[i] = constantDecoration.getValue(); } Object result; try { result = javaMethod.invoke(receiver, args); } catch (IllegalAccessException | IllegalArgumentException e) { throw irInvokeCallMemberNode.getLocation() .createError(new IllegalArgumentException("error invoking [" + irInvokeCallMemberNode + "] at compile time", e)); } catch (InvocationTargetException e) { throw irInvokeCallMemberNode.getLocation() .createError(new IllegalArgumentException("error invoking [" + irInvokeCallMemberNode + "] at compile time", e.getCause())); } ConstantNode replacement = new ConstantNode(irInvokeCallMemberNode.getLocation()); replacement.attachDecoration(new IRDConstant(result)); replacement.attachDecoration(irInvokeCallMemberNode.getDecoration(IRDExpressionType.class)); scope.accept(replacement); }	does this handle cases when there are multiple arguments that should be constants?
public static Mode fromString(String mode) { try { return Mode.valueOf(mode.toUpperCase(Locale.ROOT)); } catch (Exception e) { return PLAIN; } }	i think we should just thrown an exception stating the mode is invalid. i think it's good to be lenient but at the same time simply don't set up a mode instead of setting an incorrect one.
@Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } Request other = (Request) obj; return Objects.equals(id, other.id) && Objects.equals(pageParams, other.pageParams); } } public static class Response extends BaseTasksResponse implements Writeable, ToXContentObject { private List<DataFrameTransformStateAndStats> transformsStateAndStats; public Response(List<DataFrameTransformStateAndStats> transformsStateAndStats) { super(Collections.emptyList(), Collections.emptyList()); this.transformsStateAndStats = transformsStateAndStats; } public Response(List<DataFrameTransformStateAndStats> transformsStateAndStats, List<TaskOperationFailure> taskFailures, List<? extends ElasticsearchException> nodeFailures) { super(taskFailures, nodeFailures); this.transformsStateAndStats = transformsStateAndStats; } public Response() { super(Collections.emptyList(), Collections.emptyList()); this.transformsStateAndStats = Collections.emptyList(); } public Response(StreamInput in) throws IOException { super(in); readFrom(in); } public List<DataFrameTransformStateAndStats> getTransformsStateAndStats() { return transformsStateAndStats; } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); transformsStateAndStats = in.readList(DataFrameTransformStateAndStats::new); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeList(transformsStateAndStats); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); toXContentCommon(builder, params); builder.field(DataFrameField.COUNT.getPreferredName(), transformsStateAndStats.size()); builder.field(DataFrameField.TRANSFORMS.getPreferredName(), transformsStateAndStats); builder.endObject(); return builder; } @Override public int hashCode() { return Objects.hash(transformsStateAndStats); } @Override public boolean equals(Object other) { if (this == other) { return true; } if (other == null || getClass() != other.getClass()) { return false; } final Response that = (Response) other; return Objects.equals(this.transformsStateAndStats, that.transformsStateAndStats); } @Override public final String toString() { return Strings.toString(this); }	i waffled on putting the expandedids in here or not, since they are a setting that is set later in the object's life cycle, i was not sure.
public static DataFrameTransformStateAndStats randomDataFrameTransformStateAndStats(String id) { return new DataFrameTransformStateAndStats(id, DataFrameTransformStateTests.randomDataFrameTransformState(), DataFrameIndexerTransformStatsTests.randomStats(null)); }	using null here as when dataframeindexertransformstatstests is serialized into an xcontentbuilder, it will not include the id. if the id was included here, we would fail the equality tests.
private void finishWithFailure(Exception exc) { try { doSaveState(finishAndSetState(), position.get(), () -> onFailure(exc)); } catch (Exception e) { onFailure(exc); onFailure(new RuntimeException("Failed to save State", e)); } }	dosavestate is async so there is no need to try/catch this call. we ignore exceptions in dosavestate, this is ok imo since saving the state here is just to ensure that we'll start from the last commit point.
private void finishWithFailure(Exception exc) { try { doSaveState(finishAndSetState(), position.get(), () -> onFailure(exc)); } catch (Exception e) { onFailure(exc); onFailure(new RuntimeException("Failed to save State", e)); } }	that's the odd part of this pr. we have 2 failures, the one this method has been called with and the other one which happened during save. any better idea than calling the callback twice?
@Override public String getWriteableName() { return TYPE; } } /** * An individual tombstone entry for representing a deleted index. */ public static final class Tombstone implements ToXContent, Writeable { private static final String INDEX_KEY = "index"; private static final String DELETE_DATE_IN_MILLIS_KEY = "delete_date_in_millis"; private static final String DELETE_DATE_KEY = "delete_date"; private static final ObjectParser<Tombstone.Builder, Void> TOMBSTONE_PARSER; static { TOMBSTONE_PARSER = new ObjectParser<>("tombstoneEntry", Tombstone.Builder::new); TOMBSTONE_PARSER.declareObject(Tombstone.Builder::index, (parser, context) -> Index.fromXContent(parser), new ParseField(INDEX_KEY)); TOMBSTONE_PARSER.declareLong(Tombstone.Builder::deleteDateInMillis, new ParseField(DELETE_DATE_IN_MILLIS_KEY)); TOMBSTONE_PARSER.declareString((b, s) -> {}, new ParseField(DELETE_DATE_KEY)); } static ContextParser<Void, Tombstone> getParser() { return (parser, context) -> TOMBSTONE_PARSER.apply(parser, null).build(); } private final Index index; private final long deleteDateInMillis; private Tombstone(final Index index, final long deleteDateInMillis) { Objects.requireNonNull(index); if (deleteDateInMillis < 0L) { throw new IllegalArgumentException("invalid deleteDateInMillis [" + deleteDateInMillis + "]"); } this.index = index; this.deleteDateInMillis = deleteDateInMillis; } // create from stream private Tombstone(StreamInput in) throws IOException { index = new Index(in); deleteDateInMillis = in.readLong(); } /** * The deleted index. */ public Index getIndex() { return index; } /** * The date in milliseconds that the index deletion event occurred, used for logging/debugging. */ public long getDeleteDateInMillis() { return deleteDateInMillis; } @Override public void writeTo(final StreamOutput out) throws IOException { index.writeTo(out); out.writeLong(deleteDateInMillis); } @Override public boolean equals(final Object other) { if (this == other) { return true; } if (other == null || getClass() != other.getClass()) { return false; } @SuppressWarnings("unchecked") Tombstone that = (Tombstone) other; return index.equals(that.index) && deleteDateInMillis == that.deleteDateInMillis; } @Override public int hashCode() { int result = index.hashCode(); result = 31 * result + Long.hashCode(deleteDateInMillis); return result; } @Override public String toString() { return "[index=" + index + ", deleteDate=" + Joda.getStrictStandardDateFormatter().printer().print(deleteDateInMillis) + "]"; } @Override public XContentBuilder toXContent(final XContentBuilder builder, final Params params) throws IOException { builder.startObject(); builder.field(INDEX_KEY); index.toXContent(builder, params); builder.timeValueField(DELETE_DATE_IN_MILLIS_KEY, DELETE_DATE_KEY, deleteDateInMillis, TimeUnit.MILLISECONDS); return builder.endObject(); } public static Tombstone fromXContent(final XContentParser parser) throws IOException { return TOMBSTONE_PARSER.parse(parser, null).build(); } /** * A builder for building tombstone entries. */ private static final class Builder { private Index index; private long deleteDateInMillis = -1L; public void index(final Index index) { this.index = index; } public void deleteDateInMillis(final long deleteDate) { this.deleteDateInMillis = deleteDate; } public Tombstone build() { assert index != null; assert deleteDateInMillis > -1L; return new Tombstone(index, deleteDateInMillis); } }	maybe this should become nocontextparser? in a followup?
public static URI getBuildSrcCodeSource() { try { return Util.class.getProtectionDomain().getCodeSource().getLocation().toURI(); } catch (URISyntaxException e) { throw new GradleException("Error determining build tools JAR location", e); } }	this is a bit of an awkward way of defining a filter. instead of creating a function that takes a filetree and returns another filetree why not model the filter as an action<? super patternfilterable>? then your methods that take this could just look like: getjavamainsourceresources(project, spec -> spec.include(**/foo/**)) this also avoids the need to create a patternset via patternsetfactory which is technically an internal api.
@Override public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + "[T#" + threadNumber.getAndIncrement() + "]", 0); t.setDaemon(true); return t; } } /** * Cannot instantiate. */ private EsExecutors() {} static class ExecutorScalingQueue<E> extends LinkedTransferQueue<E> { ThreadPoolExecutor executor; ExecutorScalingQueue() {} @Override public boolean offer(E e) { // first try to transfer to a waiting worker thread if (tryTransfer(e) == false) { // check if there might be spare capacity in the thread // pool executor int left = executor.getMaximumPoolSize() - executor.getCorePoolSize(); if (left > 0) { // reject queuing the task to force the thread pool // executor to add a worker if it can; combined // with ForceQueuePolicy, this causes the thread // pool to always scale up to max pool size and we // only queue when there is no spare capacity return false; } else { return super.offer(e); } } else { return true; } } } /** * A handler for rejected tasks that adds the specified element to this queue, * waiting if necessary for space to become available. */ static class ForceQueuePolicy extends EsRejectedExecutionHandler { private final boolean rejectAfterShutdown; /** * @param rejectAfterShutdown indicates if {@link Runnable} should be rejected once the thread pool is shutting down */ ForceQueuePolicy(boolean rejectAfterShutdown) { this.rejectAfterShutdown = rejectAfterShutdown; } @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { if (rejectAfterShutdown && executor.isShutdown()) { throw newRejectedException(r, executor, true); } try { // force queue policy should only be used with a scaling queue assert executor.getQueue() instanceof ExecutorScalingQueue; executor.getQueue().put(r); } catch (final InterruptedException e) { // a scaling queue never blocks so a put to it can never be interrupted throw new AssertionError(e); } }	is there a potential for a race condition here, though very unlikely? if we get into rejectedexecution due to all threads active but pool is shutdown here and then all threads go inactive too before we put the runnable on the queue?
public void testScalingThreadPoolRejectAfterShutdown() throws Exception { final boolean rejectAfterShutdown = randomBoolean(); final int min = randomIntBetween(1, 4); final int max = randomIntBetween(min, 16); final EsThreadPoolExecutor scalingExecutor = EsExecutors.newScaling( getTestName().toLowerCase(Locale.ROOT), min, max, randomLongBetween(0, 100), TimeUnit.MILLISECONDS, rejectAfterShutdown, EsExecutors.daemonThreadFactory(getTestName().toLowerCase(Locale.ROOT)), new ThreadContext(Settings.EMPTY) ); try { final AtomicInteger executed = new AtomicInteger(); final AtomicInteger rejected = new AtomicInteger(); final AtomicInteger failed = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(max); final CountDownLatch block = new CountDownLatch(1); for (int i = 0; i < max; i++) { execute(scalingExecutor, () -> { try { latch.countDown(); block.await(); } catch (InterruptedException e) { fail(e.toString()); } }, executed, rejected, failed); } latch.await(); assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); final int queued = randomIntBetween(1, 100); for (int i = 0; i < queued; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(queued)); scalingExecutor.shutdown(); final int queuedAfterShutdown = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterShutdown; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(queued) : equalTo(queued + queuedAfterShutdown)); block.countDown(); assertBusy(() -> assertTrue(scalingExecutor.isTerminated())); assertThat(scalingExecutor.getActiveCount(), equalTo(0)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); assertThat( scalingExecutor.getCompletedTaskCount(), rejectAfterShutdown ? equalTo((long) max + queued) : greaterThanOrEqualTo((long) max + queued) ); assertThat( ((EsRejectedExecutionHandler) scalingExecutor.getRejectedExecutionHandler()).rejected(), rejectAfterShutdown ? equalTo((long) queuedAfterShutdown) : equalTo(0L) ); } finally { if (scalingExecutor.isShutdown() == false) { ThreadPool.terminate(scalingExecutor, 10, TimeUnit.SECONDS); } } }	can we also validate that rejected has 0 or queuedaftershutdown dependent on rejectaftershutdown here?
public void testScalingThreadPoolRejectAfterShutdown() throws Exception { final boolean rejectAfterShutdown = randomBoolean(); final int min = randomIntBetween(1, 4); final int max = randomIntBetween(min, 16); final EsThreadPoolExecutor scalingExecutor = EsExecutors.newScaling( getTestName().toLowerCase(Locale.ROOT), min, max, randomLongBetween(0, 100), TimeUnit.MILLISECONDS, rejectAfterShutdown, EsExecutors.daemonThreadFactory(getTestName().toLowerCase(Locale.ROOT)), new ThreadContext(Settings.EMPTY) ); try { final AtomicInteger executed = new AtomicInteger(); final AtomicInteger rejected = new AtomicInteger(); final AtomicInteger failed = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(max); final CountDownLatch block = new CountDownLatch(1); for (int i = 0; i < max; i++) { execute(scalingExecutor, () -> { try { latch.countDown(); block.await(); } catch (InterruptedException e) { fail(e.toString()); } }, executed, rejected, failed); } latch.await(); assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); final int queued = randomIntBetween(1, 100); for (int i = 0; i < queued; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(queued)); scalingExecutor.shutdown(); final int queuedAfterShutdown = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterShutdown; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(queued) : equalTo(queued + queuedAfterShutdown)); block.countDown(); assertBusy(() -> assertTrue(scalingExecutor.isTerminated())); assertThat(scalingExecutor.getActiveCount(), equalTo(0)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); assertThat( scalingExecutor.getCompletedTaskCount(), rejectAfterShutdown ? equalTo((long) max + queued) : greaterThanOrEqualTo((long) max + queued) ); assertThat( ((EsRejectedExecutionHandler) scalingExecutor.getRejectedExecutionHandler()).rejected(), rejectAfterShutdown ? equalTo((long) queuedAfterShutdown) : equalTo(0L) ); } finally { if (scalingExecutor.isShutdown() == false) { ThreadPool.terminate(scalingExecutor, 10, TimeUnit.SECONDS); } } }	should we also verify adding new tasks after termination are rejected?
public void testScalingThreadPoolRejectAfterShutdown() throws Exception { final boolean rejectAfterShutdown = randomBoolean(); final int min = randomIntBetween(1, 4); final int max = randomIntBetween(min, 16); final EsThreadPoolExecutor scalingExecutor = EsExecutors.newScaling( getTestName().toLowerCase(Locale.ROOT), min, max, randomLongBetween(0, 100), TimeUnit.MILLISECONDS, rejectAfterShutdown, EsExecutors.daemonThreadFactory(getTestName().toLowerCase(Locale.ROOT)), new ThreadContext(Settings.EMPTY) ); try { final AtomicInteger executed = new AtomicInteger(); final AtomicInteger rejected = new AtomicInteger(); final AtomicInteger failed = new AtomicInteger(); final CountDownLatch latch = new CountDownLatch(max); final CountDownLatch block = new CountDownLatch(1); for (int i = 0; i < max; i++) { execute(scalingExecutor, () -> { try { latch.countDown(); block.await(); } catch (InterruptedException e) { fail(e.toString()); } }, executed, rejected, failed); } latch.await(); assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); final int queued = randomIntBetween(1, 100); for (int i = 0; i < queued; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(queued)); scalingExecutor.shutdown(); final int queuedAfterShutdown = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterShutdown; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(queued) : equalTo(queued + queuedAfterShutdown)); block.countDown(); assertBusy(() -> assertTrue(scalingExecutor.isTerminated())); assertThat(scalingExecutor.getActiveCount(), equalTo(0)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); assertThat( scalingExecutor.getCompletedTaskCount(), rejectAfterShutdown ? equalTo((long) max + queued) : greaterThanOrEqualTo((long) max + queued) ); assertThat( ((EsRejectedExecutionHandler) scalingExecutor.getRejectedExecutionHandler()).rejected(), rejectAfterShutdown ? equalTo((long) queuedAfterShutdown) : equalTo(0L) ); } finally { if (scalingExecutor.isShutdown() == false) { ThreadPool.terminate(scalingExecutor, 10, TimeUnit.SECONDS); } } }	should this not check against isterminated? perhaps just remove the if?
public static ScalingExecutorBuilder[] executorBuilders(Settings settings) { final int processors = EsExecutors.allocatedProcessors(settings); // searchable snapshots cache thread pools should always reject tasks once they are shutting down, otherwise some threads might be // waiting for some cache file regions to be populated but this will never happen once the thread pool is shutting down. In order to // prevent these threads to be blocked the cache thread pools will reject after shutdown. final boolean rejectAfterShutdown = true; return new ScalingExecutorBuilder[] { new ScalingExecutorBuilder( CACHE_FETCH_ASYNC_THREAD_POOL_NAME, 0, Math.min(processors * 3, 50), TimeValue.timeValueSeconds(30L), rejectAfterShutdown, CACHE_FETCH_ASYNC_THREAD_POOL_SETTING ), new ScalingExecutorBuilder( CACHE_PREWARMING_THREAD_POOL_NAME, 0, 16, TimeValue.timeValueSeconds(30L), rejectAfterShutdown, CACHE_PREWARMING_THREAD_POOL_SETTING ) }; }	did you look into adding a test provoking this specific issue consistently?
@Benchmark public void singleBucketIntoMulti(Blackhole bh) { try (LongKeyedBucketOrds ords = LongKeyedBucketOrds.build(bigArrays, CardinalityUpperBound.MANY)) { for (long i = 0; i < LIMIT; i++) { ords.add(0, i % DISTINCT_VALUES); } if (ords.size() != DISTINCT_VALUES) { throw new IllegalArgumentException("Expected [" + DISTINCT_VALUES + "] but found [" + ords.size() + "]"); } bh.consume(ords); } } /** * Emulates an aggregation that collects from a single bucket "by accident". * This can happen if an aggregation is under, say, a {@code terms}	this is the same comment as the previous benchmark? would be more helpful if it highlighted what this one is doing differently, imho.
@Override public void close() { ords.close(); } } public static class FromManySmall extends LongKeyedBucketOrds { private final LongHash ords; private final int owningBucketOrdShift; private final long owningBucketOrdMask; public FromManySmall(BigArrays bigArrays, int owningBucketOrdShift) { ords = new LongHash(2, bigArrays); this.owningBucketOrdShift = owningBucketOrdShift; this.owningBucketOrdMask = -1L << owningBucketOrdShift; } private long encode(long owningBucketOrd, long value) { // This is in the critical path for collecting lots of aggs. Be careful of performance. return (owningBucketOrd << owningBucketOrdShift) | value; } @Override public long add(long owningBucketOrd, long value) { // This is in the critical path for collecting lots of aggs. Be careful of performance. long enc = encode(owningBucketOrd, value); if (owningBucketOrd != (enc >>> owningBucketOrdShift) && (enc & ~owningBucketOrdMask) != value) { throw new IllegalArgumentException( String.format( Locale.ROOT, "[%s] and [%s] must fit in [%s..%s] bits", owningBucketOrd, value, 64 - owningBucketOrdShift, owningBucketOrdShift ) ); } return ords.add(enc); } @Override public long find(long owningBucketOrd, long value) { if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return -1; } if ((value & owningBucketOrdMask) != 0) { return -1; } return ords.find(encode(owningBucketOrd, value)); } @Override public long get(long ordinal) { return ords.get(ordinal) & ~owningBucketOrdMask; } @Override public long bucketsInOrd(long owningBucketOrd) { // TODO it'd be faster to count the number of buckets in a list of these ords rather than one at a time if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return 0; } long count = 0; long enc = owningBucketOrd << owningBucketOrdShift; for (long i = 0; i < ords.size(); i++) { if ((ords.get(i) & owningBucketOrdMask) == enc) { count++; } } return count; } @Override public long size() { return ords.size(); } @Override public long maxOwningBucketOrd() { // TODO this is fairly expensive to compute. Can we avoid needing it? long max = -1; for (long i = 0; i < ords.size(); i++) { max = Math.max(max, (ords.get(i) & owningBucketOrdMask) >>> owningBucketOrdShift); } return max; } @Override public String decribe() { return "many bucket ords packed using [" + (64 - owningBucketOrdShift) + "/" + owningBucketOrdShift + "] bits"; } @Override public BucketOrdsEnum ordsEnum(long owningBucketOrd) { // TODO it'd be faster to iterate many ords at once rather than one at a time if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return BucketOrdsEnum.EMPTY; } final long encodedOwningBucketOrd = owningBucketOrd << owningBucketOrdShift; return new BucketOrdsEnum() { private long ord = -1; private long value; @Override public boolean next() { while (true) { ord++; if (ord >= ords.size()) { return false; } long encoded = ords.get(ord); if ((encoded & owningBucketOrdMask) == encodedOwningBucketOrd) { value = encoded & ~owningBucketOrdMask; return true; } } } @Override public long value() { return value; } @Override public long ord() { return ord; } }; } @Override public void close() { ords.close(); }	let's add some javadoc for new public classes, please. i think a short note explaining when this implementation can be used would be good, and maybe a plain language description of the encoding. it's not too hard to figure the encoding out from the code, but explicit is better than implicit.
@Override public void close() { ords.close(); } } public static class FromManySmall extends LongKeyedBucketOrds { private final LongHash ords; private final int owningBucketOrdShift; private final long owningBucketOrdMask; public FromManySmall(BigArrays bigArrays, int owningBucketOrdShift) { ords = new LongHash(2, bigArrays); this.owningBucketOrdShift = owningBucketOrdShift; this.owningBucketOrdMask = -1L << owningBucketOrdShift; } private long encode(long owningBucketOrd, long value) { // This is in the critical path for collecting lots of aggs. Be careful of performance. return (owningBucketOrd << owningBucketOrdShift) | value; } @Override public long add(long owningBucketOrd, long value) { // This is in the critical path for collecting lots of aggs. Be careful of performance. long enc = encode(owningBucketOrd, value); if (owningBucketOrd != (enc >>> owningBucketOrdShift) && (enc & ~owningBucketOrdMask) != value) { throw new IllegalArgumentException( String.format( Locale.ROOT, "[%s] and [%s] must fit in [%s..%s] bits", owningBucketOrd, value, 64 - owningBucketOrdShift, owningBucketOrdShift ) ); } return ords.add(enc); } @Override public long find(long owningBucketOrd, long value) { if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return -1; } if ((value & owningBucketOrdMask) != 0) { return -1; } return ords.find(encode(owningBucketOrd, value)); } @Override public long get(long ordinal) { return ords.get(ordinal) & ~owningBucketOrdMask; } @Override public long bucketsInOrd(long owningBucketOrd) { // TODO it'd be faster to count the number of buckets in a list of these ords rather than one at a time if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return 0; } long count = 0; long enc = owningBucketOrd << owningBucketOrdShift; for (long i = 0; i < ords.size(); i++) { if ((ords.get(i) & owningBucketOrdMask) == enc) { count++; } } return count; } @Override public long size() { return ords.size(); } @Override public long maxOwningBucketOrd() { // TODO this is fairly expensive to compute. Can we avoid needing it? long max = -1; for (long i = 0; i < ords.size(); i++) { max = Math.max(max, (ords.get(i) & owningBucketOrdMask) >>> owningBucketOrdShift); } return max; } @Override public String decribe() { return "many bucket ords packed using [" + (64 - owningBucketOrdShift) + "/" + owningBucketOrdShift + "] bits"; } @Override public BucketOrdsEnum ordsEnum(long owningBucketOrd) { // TODO it'd be faster to iterate many ords at once rather than one at a time if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return BucketOrdsEnum.EMPTY; } final long encodedOwningBucketOrd = owningBucketOrd << owningBucketOrdShift; return new BucketOrdsEnum() { private long ord = -1; private long value; @Override public boolean next() { while (true) { ord++; if (ord >= ords.size()) { return false; } long encoded = ords.get(ord); if ((encoded & owningBucketOrdMask) == encodedOwningBucketOrd) { value = encoded & ~owningBucketOrdMask; return true; } } } @Override public long value() { return value; } @Override public long ord() { return ord; } }; } @Override public void close() { ords.close(); }	it's not clear to me why this works. shouldn't we have to encode ordinal before getting it?
private void initiateFollowing( final Client client, final PutFollowAction.Request request, final ActionListener<PutFollowAction.Response> listener) { assert request.waitForActiveShards() != ActiveShardCount.DEFAULT : "PutFollowAction does not support DEFAULT."; activeShardsObserver.waitForActiveShards(new String[]{request.getFollowerIndex()}, request.waitForActiveShards(), request.timeout(), result -> { if (result) { FollowParameters parameters = request.getParameters(); ResumeFollowAction.Request resumeFollowRequest = new ResumeFollowAction.Request(); resumeFollowRequest.setFollowerIndex(request.getFollowerIndex()); resumeFollowRequest.getParameters().setMaxOutstandingReadRequests(parameters.getMaxOutstandingReadRequests()); resumeFollowRequest.getParameters().setMaxOutstandingWriteRequests(parameters.getMaxOutstandingWriteRequests()); resumeFollowRequest.getParameters().setMaxReadRequestOperationCount(parameters.getMaxReadRequestOperationCount()); resumeFollowRequest.getParameters().setMaxWriteRequestOperationCount( parameters.getMaxWriteRequestOperationCount()); resumeFollowRequest.getParameters().setMaxReadRequestSize(parameters.getMaxReadRequestSize()); resumeFollowRequest.getParameters().setMaxWriteRequestSize(parameters.getMaxWriteRequestSize()); resumeFollowRequest.getParameters().setMaxWriteBufferCount(parameters.getMaxWriteBufferCount()); resumeFollowRequest.getParameters().setMaxWriteBufferSize(parameters.getMaxWriteBufferSize()); resumeFollowRequest.getParameters().setReadPollTimeout(parameters.getReadPollTimeout()); resumeFollowRequest.getParameters().setMaxRetryDelay(parameters.getMaxRetryDelay()); client.execute(ResumeFollowAction.INSTANCE, resumeFollowRequest, ActionListener.wrap( r -> listener.onResponse(new PutFollowAction.Response(true, true, r.isAcknowledged())), listener::onFailure )); } else { listener.onResponse(new PutFollowAction.Response(true, false, false)); } }, listener::onFailure); }	maybe add a copy constructor to followparameters so we won't forget copying new fields?
public void attemptSyncedFlush(final String[] aliasesOrIndices, IndicesOptions indicesOptions, final ActionListener<IndicesSyncedFlushResult> listener) { final ClusterState state = clusterService.state(); final String[] concreteIndices = state.metaData().concreteIndices(indicesOptions, aliasesOrIndices); final Map<String, List<ShardsSyncedFlushResult>> results = ConcurrentCollections.newConcurrentMap(); int totalNumberOfShards = 0; int numberOfShards = 0; for (String index : concreteIndices) { final IndexMetaData indexMetaData = state.metaData().index(index); totalNumberOfShards += indexMetaData.totalNumberOfShards(); numberOfShards += indexMetaData.getNumberOfShards(); results.put(index, Collections.synchronizedList(new ArrayList<ShardsSyncedFlushResult>())); } final int finalTotalNumberOfShards = totalNumberOfShards; final CountDown countDown = new CountDown(numberOfShards); for (final String index : concreteIndices) { final int indexNumberOfShards = state.metaData().index(index).getNumberOfShards(); for (int shard = 0; shard < indexNumberOfShards; shard++) { final ShardId shardId = new ShardId(index, shard); attemptSyncedFlush(shardId, new ActionListener<ShardsSyncedFlushResult>() { @Override public void onResponse(ShardsSyncedFlushResult syncedFlushResult) { results.get(index).add(syncedFlushResult); if (countDown.countDown()) { listener.onResponse(new IndicesSyncedFlushResult(results)); } } @Override public void onFailure(Throwable e) { logger.debug("{} unexpected error while executing synced flush", shardId); results.get(index).add(new ShardsSyncedFlushResult(shardId, finalTotalNumberOfShards, e.getMessage())); if (countDown.countDown()) { listener.onResponse(new IndicesSyncedFlushResult(results)); } } }); } } }	flush request will hang if there are no concrete indices. i think we have to check this case and then call listener.onresponse()
public void apply(Project project) { project.getRootProject().getPluginManager().apply(DockerSupportPlugin.class); if (BuildParams.isInternal()) { project.getPlugins().apply(InternalDistributionDownloadPlugin.class); } else { project.getPlugins().apply(DistributionDownloadPlugin.class); } project.getPluginManager().apply("elasticsearch.build"); Provider<DockerSupportService> dockerSupport = GradleUtils.getBuildService( project.getGradle().getSharedServices(), DockerSupportPlugin.DOCKER_SUPPORT_SERVICE_NAME ); // TODO: it would be useful to also have the SYSTEM_JAVA_HOME setup in the root project, so that running from GCP only needs // a java for gradle to run, and the tests are self sufficient and consistent with the java they use Version upgradeVersion = getUpgradeVersion(project); Provider<Directory> distributionsDir = project.getLayout().getBuildDirectory().dir("packaging/distributions"); Provider<Directory> upgradeDir = project.getLayout().getBuildDirectory().dir("packaging/upgrade"); List<ElasticsearchDistribution> distributions = configureDistributions(project, upgradeVersion); TaskProvider<Copy> copyDistributionsTask = configureCopyDistributionsTask(project, distributionsDir); TaskProvider<Copy> copyUpgradeTask = configureCopyUpgradeTask(project, upgradeVersion, upgradeDir); Map<ElasticsearchDistribution.Type, TaskProvider<?>> lifecyleTasks = lifecyleTasks(project, "destructiveDistroTest"); TaskProvider<Task> destructiveDistroTest = project.getTasks().register("destructiveDistroTest"); Configuration examplePlugin = configureExamplePlugin(project); for (ElasticsearchDistribution distribution : distributions) { TaskProvider<?> destructiveTask = configureDistroTest(project, distribution, dockerSupport, examplePlugin); destructiveDistroTest.configure(t -> t.dependsOn(destructiveTask)); lifecyleTasks.get(distribution.getType()).configure(t -> t.dependsOn(destructiveTask)); } TaskProvider<BatsTestTask> batsUpgradeTest = configureBatsTest( project, "upgrade", distributionsDir, copyDistributionsTask, copyUpgradeTask ); batsUpgradeTest.configure(t -> t.setUpgradeDir(upgradeDir)); project.subprojects(vmProject -> { vmProject.getPluginManager().apply(VagrantBasePlugin.class); vmProject.getPluginManager().apply(JdkDownloadPlugin.class); List<Object> vmDependencies = new ArrayList<>(configureVM(vmProject)); vmDependencies.add(project.getConfigurations().getByName("testRuntimeClasspath")); Map<ElasticsearchDistribution.Type, TaskProvider<?>> vmLifecyleTasks = lifecyleTasks(vmProject, "distroTest"); TaskProvider<Task> distroTest = vmProject.getTasks().register("distroTest"); for (ElasticsearchDistribution distribution : distributions) { String destructiveTaskName = destructiveDistroTestTaskName(distribution); Platform platform = distribution.getPlatform(); // this condition ensures windows boxes get windows distributions, and linux boxes get linux distributions if (isWindows(vmProject) == (platform == Platform.WINDOWS)) { TaskProvider<GradleDistroTestTask> vmTask = configureVMWrapperTask( vmProject, distribution.getName() + " distribution", destructiveTaskName, vmDependencies ); vmTask.configure(t -> t.dependsOn(distribution, examplePlugin)); vmLifecyleTasks.get(distribution.getType()).configure(t -> t.dependsOn(vmTask)); distroTest.configure(t -> { // Only VM sub-projects that are specifically opted-in to testing Docker should // have the Docker task added as a dependency. Although we control whether Docker // is installed in the VM via `Vagrantfile` and we could auto-detect its presence // in the VM, the test tasks e.g. `destructiveDistroTest.default-docker` are defined // on the host during Gradle's configuration phase and not in the VM, so // auto-detection doesn't work. // // The shouldTestDocker property could be null, hence we use Boolean.TRUE.equals() boolean shouldExecute = distribution.getType() != Type.DOCKER || Boolean.TRUE.equals(vmProject.findProperty("shouldTestDocker")); if (shouldExecute) { t.dependsOn(vmTask); } }); } } configureVMWrapperTask(vmProject, "bats upgrade", batsUpgradeTest.getName(), vmDependencies).configure(t -> { t.setProgressHandler(new BatsProgressLogger(project.getLogger())); t.onlyIf(spec -> isWindows(vmProject) == false); // bats doesn't run on windows t.dependsOn(copyDistributionsTask, copyUpgradeTask); }); }); }	we kicked the can of differentiating between internal and external builds further to here for now. is this plugin even considered to be used externally. otherwise we can remove this differentiation here and just apply the internal plugin
public void apply(Project project) { project.getRootProject().getPluginManager().apply(GlobalBuildInfoPlugin.class); if (BuildParams.isInternal()) { project.getPlugins().apply(InternalDistributionDownloadPlugin.class); } else { project.getPlugins().apply(DistributionDownloadPlugin.class); } project.getRootProject().getPluginManager().apply(ReaperPlugin.class); ReaperService reaper = project.getRootProject().getExtensions().getByType(ReaperService.class); // enable the DSL to describe clusters NamedDomainObjectContainer<ElasticsearchCluster> container = createTestClustersContainerExtension(project, reaper); // provide a task to be able to list defined clusters. createListClustersTask(project, container); // register cluster registry as a global build service project.getGradle().getSharedServices().registerIfAbsent(REGISTRY_SERVICE_NAME, TestClustersRegistry.class, noop()); // register throttle so we only run at most max-workers/2 nodes concurrently project.getGradle() .getSharedServices() .registerIfAbsent( THROTTLE_SERVICE_NAME, TestClustersThrottle.class, spec -> spec.getMaxParallelUsages().set(Math.max(1, project.getGradle().getStartParameter().getMaxWorkerCount() / 2)) ); // register cluster hooks project.getRootProject().getPluginManager().apply(TestClustersHookPlugin.class); }	we kicked the can of differentiating between internal and external builds further to here for now. we might want to apply similar internal / external plugin distinction later for that plugin too
public void resetClient(InetSocketAddress[] addresses, Settings settings) throws IOException, RestException { if (restClient == null) { restClient = new RestClient(restSpec, settings, addresses); } else { //re-initialize the REST client if the addresses have changed //might happen if there's a failure and we restart the global cluster due to that if (restClient.httpAddresses().length != addresses.length) { restClient.close(); restClient = new RestClient(restSpec, settings, addresses); return; } for (InetSocketAddress address : addresses) { boolean found = false; for (InetSocketAddress previousAddress : restClient.httpAddresses()) { if (previousAddress.equals(address)) { found = true; break; } } if (!found) { restClient.close(); restClient = new RestClient(restSpec, settings, addresses); return; } } } }	can't we just use a set< inetsocketaddress> and compare the set?
private void removeFinishedSnapshotFromClusterState(ClusterChangedEvent event) { if (event.previousState().nodes().isLocalNodeElectedMaster() == false) { SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE); if (snapshotsInProgress != null && !snapshotsInProgress.entries().isEmpty()) { for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) { if (entry.state().completed()) { endSnapshot(entry); } } } } }	what is this change?
public void bootstrapNewHistory() throws IOException { metadataLock.writeLock().lock(); try { final SegmentInfos segmentCommitInfos = readLastCommittedSegmentsInfo(); final Map<String, String> userData = segmentCommitInfos.getUserData(); final String rawLocalCheckpoint = userData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY); final String rawMaxSeqNo = userData.get(SequenceNumbers.MAX_SEQ_NO); if (rawLocalCheckpoint == null) { assert rawMaxSeqNo == null : "Local checkpoint null but max sequence number: " + rawMaxSeqNo; // If the local checkpoint is null we expect that this is Lucene version 6 or earlier assert segmentCommitInfos.getCommitLuceneVersion().major < 7 : "Found Lucene version: " + segmentCommitInfos.getCommitLuceneVersion().major; } final SequenceNumbers.CommitInfo seqno = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet()); bootstrapNewHistory(seqno.localCheckpoint, seqno.maxSeqNo); } finally { metadataLock.writeLock().unlock(); } }	you can remove the if block and instead write the following 2 assertions: assert (rawlocalcheckpoint == null) == (rawmaxseqno == null) : "local checkpoint was " + rawlocalcheckpoint + " but max seq no was " + rawmaxseqno; assert rawlocalcheckpoint != null || segmentcommitinfos.getcommitluceneversion().major < 7 : "found lucene version: " + segmentcommitinfos.getcommitluceneversion().major;
public static NamedClusterPrivilege resolve(String name) { name = Objects.requireNonNull(name).toLowerCase(Locale.ROOT); if (isClusterAction(name)) { return new ActionClusterPrivilege(name, Set.of(actionToPattern(name))); } final NamedClusterPrivilege fixedPrivilege = VALUES.get(name); if (fixedPrivilege != null) { return fixedPrivilege; } logger.error("failed to resolve cluster privilege [" + name + "]."); throw new IllegalArgumentException("unknown cluster privilege [" + name + "]. a privilege must be either " + "one of the predefined cluster privilege names [" + Strings.collectionToCommaDelimitedString(VALUES.keySet()) + "] or a pattern over one of the available " + "cluster actions"); }	i don't think this message is as helpful as it could be. what do we expect someone to do if they read this? why is it an error ?
private void waitForSnapshotToFinish(String repo, String snapshot) throws Exception { assertBusy(() -> { SnapshotsStatusResponse response = client().admin().cluster().prepareSnapshotStatus(repo).setSnapshots(snapshot).get(); assertThat(response.getSnapshots().get(0).getState(), is(SnapshotsInProgress.State.SUCCESS)); }); // The status of the snapshot in the repository can become SUCCESS before it is fully finalized in the cluster state so wait for // it to disappear from the cluster state as well assertBusy(() -> { SnapshotsInProgress response = client().admin().cluster().state(new ClusterStateRequest()).get().getState().custom(SnapshotsInProgress.TYPE); assertThat(response.entries(), Matchers.empty()); }); }	@original-brownbear you've got my rubber stamp lgtm , but shouldn't we fix the preparesnapshotstatus(repo).setsnapshots(snapshot) api to not return success if there is still an snapshotsinprogress.type entry?
public void testInvalidTypeForNumericFunction_WithTwoArgs() { assertEquals("1:8: [TRUNCATE('foo', 2)] first argument must be [NUMERIC], found value ['foo'] type [KEYWORD]", error("SELECT TRUNCATE('foo', 2)")); assertEquals("1:8: [TRUNCATE(1.2, 'bar')] second argument must be [NUMERIC], found value ['bar'] type [KEYWORD]", error("SELECT TRUNCATE(1.2, 'bar')")); }	what's with the uppercasing of the types? it is used as lowercase since that's the convention used by es itself.
public RestResponse buildResponse(ClusterStateResponse response, XContentBuilder builder) throws Exception { builder.startObject(); builder.field(Fields.TIMED_OUT, response.isWaitForTimedOut()); builder.field(Fields.CLUSTER_NAME, response.getClusterName().value()); builder.humanReadableField(Fields.CLUSTER_STATE_SIZE_IN_BYTES, Fields.CLUSTER_STATE_SIZE, response.getTotalCompressedSize()); response.getState().toXContent(builder, request); builder.endObject(); return new BytesRestResponse(RestStatus.OK, builder); }	i wonder if this field should be called wait_for_timed_out? and i wonder if we should only include it if wait_for_metadata_version was set on the request?
private Expression replaceRawBoolFieldWithEquals(Expression e) { if (e instanceof FieldAttribute && e.dataType() == BOOLEAN) { e = new Equals(e.source(), e, Literal.of(e, Boolean.TRUE)); } return e; }	the rule could check whether the filter is resolved before applying the equality.
static Map<String, Object> mergeFailingOnReplacement(Map<String, Object> first, Map<String, Object> second) { Objects.requireNonNull(first, "merging requires two non-null maps but the first map was null"); Objects.requireNonNull(second, "merging requires two non-null maps but the second map was null"); Map<String, Object> results = new HashMap<>(first); Set<String> prefixes = second.keySet().stream().map(MetadataCreateIndexService::prefix).collect(Collectors.toSet()); List<String> matchedPrefixes = new ArrayList<>(); results.keySet().forEach(k -> { if (prefixes.contains(prefix(k))) { matchedPrefixes.add(k); } }); if (matchedPrefixes.size() > 0) { throw new IllegalArgumentException("mapping fields " + matchedPrefixes + " cannot be replaced during template composition"); } results.putAll(second); return results; }	i thnik this could be expressed as, i find it a bit more readable, but maybe it's a personal preference so feel free to ignore list<string> matchedprefixes = results.keyset().stream().filter(k -> prefixes.contains(prefix(k))).collect(collectors.tolist())
private static void validateCompositeTemplate(final ClusterState state, final String templateName, final ComposableIndexTemplate template, final IndicesService indicesService, final NamedXContentRegistry xContentRegistry) throws Exception { final ClusterState stateWithTemplate = ClusterState.builder(state) .metadata(Metadata.builder(state.metadata()).put(templateName, template)) .build(); Index createdIndex = null; final String temporaryIndexName = "validate-template-" + UUIDs.randomBase64UUID().toLowerCase(Locale.ROOT); try { Settings resolvedSettings = resolveSettings(stateWithTemplate.metadata(), templateName); // use the provided values, otherwise just pick valid dummy values int dummyPartitionSize = IndexMetadata.INDEX_ROUTING_PARTITION_SIZE_SETTING.get(resolvedSettings); int dummyShards = resolvedSettings.getAsInt(IndexMetadata.SETTING_NUMBER_OF_SHARDS, dummyPartitionSize == 1 ? 1 : dummyPartitionSize + 1); int shardReplicas = resolvedSettings.getAsInt(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0); //create index service for parsing and validating "mappings" Settings dummySettings = Settings.builder() .put(IndexMetadata.SETTING_VERSION_CREATED, Version.CURRENT) .put(resolvedSettings) .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, dummyShards) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, shardReplicas) .put(IndexMetadata.SETTING_INDEX_UUID, UUIDs.randomBase64UUID()) .build(); // Validate index metadata (settings) final ClusterState stateWithIndex = ClusterState.builder(stateWithTemplate) .metadata(Metadata.builder(stateWithTemplate.metadata()) .put(IndexMetadata.builder(temporaryIndexName).settings(dummySettings)) .build()) .build(); final IndexMetadata tmpIndexMetadata = stateWithIndex.metadata().index(temporaryIndexName); IndexService dummyIndexService = indicesService.createIndex(tmpIndexMetadata, Collections.emptyList(), false); createdIndex = dummyIndexService.index(); // Validate aliases MetadataCreateIndexService.resolveAndValidateAliases(temporaryIndexName, Collections.emptySet(), MetadataIndexTemplateService.resolveAliases(stateWithIndex.metadata(), templateName), stateWithIndex.metadata(), new AliasValidator(), // the context is only used for validation so it's fine to pass fake values for the // shard id and the current timestamp xContentRegistry, dummyIndexService.newQueryShardContext(0, null, () -> 0L, null)); // Parse mappings to ensure they are valid after being composed List<CompressedXContent> mappings = resolveMappings(stateWithIndex, templateName); Map<String, Object> finalMappings = MetadataCreateIndexService.parseV2Mappings("{}", mappings, xContentRegistry); MapperService dummyMapperService = dummyIndexService.mapperService(); if (finalMappings.isEmpty() == false) { assert finalMappings.size() == 1 : finalMappings; // TODO: Eventually change this to: // dummyMapperService.merge(MapperService.SINGLE_MAPPING_NAME, mapping, MergeReason.INDEX_TEMPLATE); dummyMapperService.merge(MapperService.SINGLE_MAPPING_NAME, finalMappings, MergeReason.MAPPING_UPDATE); } } finally { if (createdIndex != null) { indicesService.removeIndex(createdIndex, NO_LONGER_ASSIGNED, "created for validating template addition"); } } }	is this comment obsolete or misplaced?
private static void validateCompositeTemplate(final ClusterState state, final String templateName, final ComposableIndexTemplate template, final IndicesService indicesService, final NamedXContentRegistry xContentRegistry) throws Exception { final ClusterState stateWithTemplate = ClusterState.builder(state) .metadata(Metadata.builder(state.metadata()).put(templateName, template)) .build(); Index createdIndex = null; final String temporaryIndexName = "validate-template-" + UUIDs.randomBase64UUID().toLowerCase(Locale.ROOT); try { Settings resolvedSettings = resolveSettings(stateWithTemplate.metadata(), templateName); // use the provided values, otherwise just pick valid dummy values int dummyPartitionSize = IndexMetadata.INDEX_ROUTING_PARTITION_SIZE_SETTING.get(resolvedSettings); int dummyShards = resolvedSettings.getAsInt(IndexMetadata.SETTING_NUMBER_OF_SHARDS, dummyPartitionSize == 1 ? 1 : dummyPartitionSize + 1); int shardReplicas = resolvedSettings.getAsInt(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0); //create index service for parsing and validating "mappings" Settings dummySettings = Settings.builder() .put(IndexMetadata.SETTING_VERSION_CREATED, Version.CURRENT) .put(resolvedSettings) .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, dummyShards) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, shardReplicas) .put(IndexMetadata.SETTING_INDEX_UUID, UUIDs.randomBase64UUID()) .build(); // Validate index metadata (settings) final ClusterState stateWithIndex = ClusterState.builder(stateWithTemplate) .metadata(Metadata.builder(stateWithTemplate.metadata()) .put(IndexMetadata.builder(temporaryIndexName).settings(dummySettings)) .build()) .build(); final IndexMetadata tmpIndexMetadata = stateWithIndex.metadata().index(temporaryIndexName); IndexService dummyIndexService = indicesService.createIndex(tmpIndexMetadata, Collections.emptyList(), false); createdIndex = dummyIndexService.index(); // Validate aliases MetadataCreateIndexService.resolveAndValidateAliases(temporaryIndexName, Collections.emptySet(), MetadataIndexTemplateService.resolveAliases(stateWithIndex.metadata(), templateName), stateWithIndex.metadata(), new AliasValidator(), // the context is only used for validation so it's fine to pass fake values for the // shard id and the current timestamp xContentRegistry, dummyIndexService.newQueryShardContext(0, null, () -> 0L, null)); // Parse mappings to ensure they are valid after being composed List<CompressedXContent> mappings = resolveMappings(stateWithIndex, templateName); Map<String, Object> finalMappings = MetadataCreateIndexService.parseV2Mappings("{}", mappings, xContentRegistry); MapperService dummyMapperService = dummyIndexService.mapperService(); if (finalMappings.isEmpty() == false) { assert finalMappings.size() == 1 : finalMappings; // TODO: Eventually change this to: // dummyMapperService.merge(MapperService.SINGLE_MAPPING_NAME, mapping, MergeReason.INDEX_TEMPLATE); dummyMapperService.merge(MapperService.SINGLE_MAPPING_NAME, finalMappings, MergeReason.MAPPING_UPDATE); } } finally { if (createdIndex != null) { indicesService.removeIndex(createdIndex, NO_LONGER_ASSIGNED, "created for validating template addition"); } } }	would using indicesservice.withtempindexservice be better as it avoids adding.removing the index from the indexservice and also handles closing the indexservice? (we're using this in transportsimulateindextemplateaction)
public void testRemoveComponentTemplateInUse() throws Exception { ComposableIndexTemplate template = new ComposableIndexTemplate(Collections.singletonList("a"), null, Collections.singletonList("ct"), null, null, null); ComponentTemplate ct = new ComponentTemplate(new Template(null, new CompressedXContent("{}"), null), null, null); final MetadataIndexTemplateService service = getMetadataIndexTemplateService(); CountDownLatch ctLatch = new CountDownLatch(1); service.putComponentTemplate("api", false, "ct", TimeValue.timeValueSeconds(5), ct, ActionListener.wrap(r -> ctLatch.countDown(), e -> fail("unexpected error"))); ctLatch.await(5, TimeUnit.SECONDS); CountDownLatch latch = new CountDownLatch(1); service.putIndexTemplateV2("api", false, "template", TimeValue.timeValueSeconds(30), template, ActionListener.wrap(r -> latch.countDown(), e -> fail("unexpected error"))); latch.await(5, TimeUnit.SECONDS); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> { AtomicReference<Exception> err = new AtomicReference<>(); CountDownLatch errLatch = new CountDownLatch(1); service.removeComponentTemplate("c*", TimeValue.timeValueSeconds(30), ActionListener.wrap(r -> fail("should have failed!"), exception -> { err.set(exception); errLatch.countDown(); })); errLatch.await(5, TimeUnit.SECONDS); if (err.get() != null) { throw err.get(); } }); assertThat(e.getMessage(), containsString("component templates [ct] cannot be removed as they are still in use by index templates [template]")); }	this and the next test names are rather confusing to me. it's quite difficult to pick up what they are testing and what's the invalid case. (maybe related to the previous comment too, but "composable composite" adds to the confusion for me) what do you think of including the cause of failure in the test name? what do you think of something along the lines of testindextemplatefailstooverridecomponenttemplatemappingfield and respectively testupdatecomponenttemplatefailsifresolvedindextemplateswouldbeinvalid ?
private LogicalPlan pullUpTopClauseFromQuerySpecification(LogicalPlan plan) { if (plan instanceof UnaryPlan) { UnaryPlan unary = (UnaryPlan) plan; if (unary.child() instanceof Limit) { Limit limit = (Limit) unary.child(); return limit.replaceChildrenSameSize(Arrays.asList( unary.replaceChildrenSameSize(Arrays.asList( limit.child())))); } } return plan; }	i'd prefer a more focused approach in handling top. top is handled through visitqueryspecification while order by is handled inside querynowith hence why one can check if the limit exists in case of an orderby and do the switch then - replace the child of the limit with the new orderby which points to the limit child.
private LogicalPlan pullUpTopClauseFromQuerySpecification(LogicalPlan plan) { if (plan instanceof UnaryPlan) { UnaryPlan unary = (UnaryPlan) plan; if (unary.child() instanceof Limit) { Limit limit = (Limit) unary.child(); return limit.replaceChildrenSameSize(Arrays.asList( unary.replaceChildrenSameSize(Arrays.asList( limit.child())))); } } return plan; }	nit: collections#singletonlist() might work too.
private static void checkForNonCollapsableSubselects(PhysicalPlan plan, List<Failure> failures) { // use a comparator to ensure deterministic order of the error messages Set<LimitExec> limits = new TreeSet<>(Comparator.comparing(l -> l.source().text())); plan.forEachDown(p -> { if (p instanceof OrderExec || p instanceof FilterExec || p instanceof PivotExec || p instanceof AggregateExec) { p.forEachDown(LimitExec.class, limits::add); } }); for (LimitExec limit : limits) { failures.add( fail(limit, "LIMIT or TOP cannot be used in a subquery if outer query contains GROUP BY, WHERE, ORDER BY or PIVOT") ); } }	please order the upper cased keywords alphabetically (for consistency).
public static RestClientBuilder builder(Node... nodes) { return new RestClientBuilder(nodes == null ? null : Arrays.asList(nodes)); } /** * Returns a new {@link RestClientBuilder} to help with {@link RestClient} creation. * Creates a new builder instance and sets the nodes that the client will send requests to. * <p> * You can use this if you do not have metadata up front about the nodes. If you do, prefer * {@link #builder(Node...)}	i think adding an example of what a cloudid looks like to the javadoc would help for people who don't know what that is.
public static RestClientBuilder builder(String cloudId) { // there is an optional first portion of the cloudId that is a human readable string, but it is not used. if (cloudId.contains(":")) { if (cloudId.indexOf(":") == cloudId.length() - 1) { throw new IllegalStateException("cloudId " + cloudId + " is invalid"); } cloudId = cloudId.substring(cloudId.indexOf(":") + 1); } String decoded = new String(Base64.getDecoder().decode(cloudId)); // once decoded the parts are separated by a $ character String[] decodedParts = decoded.split("\\\\\\\\$"); if (decodedParts.length != 3) { throw new IllegalStateException("cloudId " + cloudId + " did not contain the correct number of parts"); } String url = decodedParts[1] + "." + decodedParts[0]; return builder(new HttpHost(url, 443, "https")); } /** * Returns a new {@link RestClientBuilder} to help with {@link RestClient} creation. * Creates a new builder instance and sets the hosts that the client will send requests to. * <p> * Prefer this to {@link #builder(HttpHost...)}	this error will be really confusing for users who have no idea that cloudids contain multiple parts, or that they are even encoded at all. maybe we can change it to something like: "clouldid ... is invalid and does not decode to a cluster identifier correctly"
public Optional<ValidationException> validate() { if (indexPatterns == null || indexPatterns.size() == 0) { return Optional.of(ValidationException.withError("index patterns are missing")); } return Optional.empty(); }	i think that even tho this is like a builder and a request, we should put things that required (like in this case the indexpatterns) into a constructor for the builder. wdyt? then we move the validation to the constructor and remove this validate method altogether.
protected LogicalPlan rule(UnaryPlan plan) { if (plan.child() instanceof SubQueryAlias) { SubQueryAlias a = (SubQueryAlias) plan.child(); return plan.transformExpressionsDown(FieldAttribute.class, f -> { if (f.qualifier() != null && f.qualifier().equals(a.alias())) { // Find the underlying concrete relation (EsIndex) and its name as the new qualifier List<LogicalPlan> children = a.collectFirstChildren(p -> p instanceof EsRelation); if (children.isEmpty() == false) { return f.withQualifier(((EsRelation) children.get(0)).index().name()); } } return f; }); } return plan; }	wondering if you have any better ideas for getting only the first child, and not first children out of which to extract the first. not a biggie, though.
@Override public void accept(ClusterState state) { minNodeVersion = state.nodes().getMinNodeVersion(); Metadata metadata = state.getMetadata(); if (metadata == null) { currentInferenceProcessors = 0; return; } IngestMetadata ingestMetadata = metadata.custom(IngestMetadata.TYPE); if (ingestMetadata == null) { currentInferenceProcessors = 0; return; } int count = 0; for (PipelineConfiguration configuration : ingestMetadata.getPipelines().values()) { Map<String, Object> configMap = configuration.getConfigAsMap(); try { List<Map<String, Object>> processorConfigs = ConfigurationUtils.readList(null, null, configMap, PROCESSORS_KEY); for (Map<String, Object> processorConfigWithKey : processorConfigs) { for (Map.Entry<String, Object> entry : processorConfigWithKey.entrySet()) { if (TYPE.equals(entry.getKey())) { ++count; } // Special handling as `foreach` processors allow a `processor` to be defined if (FOREACH_PROCESSOR_NAME.equals(entry.getKey())) { if (entry.getValue() instanceof Map<?, ?>) { Object processorDefinition = ((Map<?, ?>)entry.getValue()).get("processor"); if (processorDefinition instanceof Map<?, ?>) { if (((Map<?, ?>) processorDefinition).keySet().contains(TYPE)) { ++count; } } } } } } // We cannot throw any exception here. It might break other pipelines. } catch (Exception ex) { logger.debug( () -> new ParameterizedMessage("failed gathering processors for pipeline [{}]", configuration.getId()), ex); } } currentInferenceProcessors = count; }	the for-each processor and the onfailure directive are the only scenarios i know of that result in child processors. both of those can be nested to an indefinite number of levels. i'm not sure how far you want to go down that rabbit hole, though.
@Override public BootstrapCheckResult check(BootstrapContext context) { final long initialHeapSize = getInitialHeapSize(); final long maxHeapSize = getMaxHeapSize(); if (initialHeapSize != 0 && maxHeapSize != 0 && initialHeapSize != maxHeapSize) { final String message; if (isMemoryLocked()) { message = String.format( Locale.ROOT, "initial heap size [%d] not equal to maximum heap size [%d]; " + "this can cause resize pauses", getInitialHeapSize(), getMaxHeapSize()); } else { message = String.format( Locale.ROOT, "initial heap size [%d] not equal to maximum heap size [%d]; " + "this can cause resize pauses and prevents memory locking from locking the entire heap", getInitialHeapSize(), getMaxHeapSize()); } return BootstrapCheckResult.failure(message); } else { return BootstrapCheckResult.success(); } }	shouldn't the memory locking part be in the other condition's message, since this is the *not* memory locked case?
private static int getNumDocs(LeafReader reader, Query roleQuery, BitSet roleQueryBits) throws IOException, ExecutionException { IndexReader.CacheHelper cacheHelper = reader.getReaderCacheHelper(); // this one takes deletes into account if (cacheHelper == null) { return computeNumDocs(reader, roleQueryBits); } final boolean[] added = new boolean[] { false }; Cache<Query, Integer> perReaderCache = NUM_DOCS_CACHE.computeIfAbsent(cacheHelper.getKey(), key -> { added[0] = true; return CacheBuilder.<Query, Integer>builder() // Not configurable, this limit only exists so that if a role query is updated // then we won't risk OOME because of old role queries that are not used anymore .setMaximumWeight(1000) .weigher((k, v) -> 1) // just count .build(); }); if (added[0]) { IndexReader.ClosedListener closedListener = NUM_DOCS_CACHE::remove; try { cacheHelper.addClosedListener(closedListener); } catch (AlreadyClosedException e) { closedListener.onClose(cacheHelper.getKey()); throw e; } } return perReaderCache.computeIfAbsent(roleQuery, q -> computeNumDocs(reader, roleQueryBits)); }	this change was necessary as documentsubsetreader was a bad citizen. it required the leafreader that it wrapped to have a readercachehelper, but then at the same time did not expose a reader cache helper by its own wrapping leafreader, so essentially double-standards. i don't see a reason for this reader not to work when caching isn't available (single-document case)
public static void assertBulkItemResponse(BulkItemResponse expected, BulkItemResponse actual) { assertEquals(expected.getItemId(), actual.getItemId()); assertEquals(expected.getIndex(), actual.getIndex()); assertEquals(expected.getType(), actual.getType()); assertEquals(expected.getId(), actual.getId()); assertEquals(expected.getOpType(), actual.getOpType()); assertEquals(expected.getVersion(), actual.getVersion()); assertEquals(expected.isFailed(), actual.isFailed()); if (expected.isFailed()) { BulkItemResponse.Failure expectedFailure = expected.getFailure(); BulkItemResponse.Failure actualFailure = actual.getFailure(); assertEquals(expectedFailure.getIndex(), actualFailure.getIndex()); assertEquals(expectedFailure.getType(), actualFailure.getType()); assertEquals(expectedFailure.getId(), actualFailure.getId()); assertEquals(expectedFailure.getMessage(), actualFailure.getMessage()); assertEquals(expectedFailure.getStatus(), actualFailure.getStatus()); assertDeepEquals((ElasticsearchException) expectedFailure.getCause(), (ElasticsearchException) actualFailure.getCause()); } else { if (expected.getOpType() == DocWriteRequest.OpType.UPDATE) { UpdateResponseTests.assertUpdateResponse(expected.getResponse(), actual.getResponse()); } else { IndexResponseTests.assertDocWriteResponse(expected.getResponse(), actual.getResponse()); } } }	i was wondering what happened to delete and create, then realized that indexresponsetests.assertdocwriteresponse seems to work for all of them. if thats true, can you add a small comment?
public void testToAndFromXContent() throws IOException { final XContentType xContentType = randomFrom(XContentType.values()); for (DocWriteRequest.OpType opType : DocWriteRequest.OpType.values()) { int bulkItemId = randomIntBetween(0, 100); boolean humanReadable = randomBoolean(); Tuple<? extends DocWriteResponse, ? extends DocWriteResponse> randomDocWriteResponses = null; if (opType == DocWriteRequest.OpType.INDEX || opType == DocWriteRequest.OpType.CREATE) { randomDocWriteResponses = IndexResponseTests.randomIndexResponse(); } else if (opType == DocWriteRequest.OpType.DELETE) { randomDocWriteResponses = DeleteResponseTests.randomDeleteResponse(); } else if (opType == DocWriteRequest.OpType.UPDATE) { randomDocWriteResponses = UpdateResponseTests.randomUpdateResponse(xContentType); } else { fail("Test does not support opType [" + opType + "]"); } BulkItemResponse bulkItemResponse = new BulkItemResponse(bulkItemId, opType, randomDocWriteResponses.v1()); BulkItemResponse expectedBulkItemResponse = new BulkItemResponse(bulkItemId, opType, randomDocWriteResponses.v2()); BytesReference originalBytes = toXContent(bulkItemResponse, xContentType, humanReadable); // Shuffle the XContent fields if (randomBoolean()) { try (XContentParser parser = createParser(xContentType.xContent(), originalBytes)) { originalBytes = shuffleXContent(parser, randomBoolean()).bytes(); } } BulkItemResponse parsedBulkItemResponse; try (XContentParser parser = createParser(xContentType.xContent(), originalBytes)) { assertEquals(XContentParser.Token.START_OBJECT, parser.nextToken()); parsedBulkItemResponse = BulkItemResponse.fromXContent(parser, bulkItemId); assertNull(parser.nextToken()); } assertBulkItemResponse(expectedBulkItemResponse, parsedBulkItemResponse); } }	question: later in the deleteresponsetests you mention in a comment that "the random delete response can contain shard failures with exceptions". does this matter here?
* @param withShardFailures indicates if the generated ShardInfo must contain shard failures */ public static Tuple<ShardInfo, ShardInfo> randomShardInfo(Random random, boolean withShardFailures) { int total = randomIntBetween(random, 1, 10); if (withShardFailures == false) { return Tuple.tuple(new ShardInfo(total, total), new ShardInfo(total, total)); } int successful = randomIntBetween(random, 1, Math.max(1, (total - 1))); int failures = Math.max(1, (total - successful)); Failure[] actualFailures = new Failure[failures]; Failure[] expectedFailures = new Failure[failures]; for (int i = 0; i < failures; i++) { Tuple<Failure, Failure> failure = randomShardInfoFailure(random); actualFailures[i] = failure.v1(); expectedFailures[i] = failure.v2(); } return Tuple.tuple(new ShardInfo(total, successful, actualFailures), new ShardInfo(total, successful, expectedFailures)); } /** * Returns a tuple that contains a randomized {@link Failure} value (left side) and its corresponding * value (right side) after it has been printed out as a {@link ToXContent} and parsed back using a parsing * method like {@link ShardInfo.Failure#fromXContent(XContentParser)}	is it okay that we might use a good rest status code in the failure (e.g. 200)? i guess it doesn't matter or should we exclude those?
public void parse(FieldMapper mainField, Supplier<DocumentParserContext> contextSupplier) throws IOException { // TODO: multi fields are really just copy fields, we just need to expose "sub fields" or something that can be part // of the mappings if (mappers.isEmpty()) { return; } DocumentParserContext context = contextSupplier.get(); context.path().add(mainField.simpleName()); for (FieldMapper mapper : mappers.values()) { mapper.parse(context); context = contextSupplier.get(); } context.path().remove(); }	this is hacky but we need some context to call context.path().add and remove. i did this to keep things simple.
@Override protected void index(DocumentParserContext context, GeoPoint geometry) throws IOException { if (fieldType().isIndexed()) { context.doc().add(new LatLonPoint(fieldType().name(), geometry.lat(), geometry.lon())); } if (fieldType().hasDocValues()) { context.doc().add(new LatLonDocValuesField(fieldType().name(), geometry.lat(), geometry.lon())); } else if (fieldType().isStored() || fieldType().isIndexed()) { context.addToFieldNames(fieldType().name()); } if (fieldType().isStored()) { context.doc().add(new StoredField(fieldType().name(), geometry.toString())); } // TODO phase out geohash (which is currently used in the CompletionSuggester) // we only expose the geohash value and disallow advancing tokens, hence we can reuse the same parser throughout multiple sub-fields DocumentParserContext parserContext = context.switchParser(new GeoHashMultiFieldParser(context.parser(), geometry.geohash())); multiFields.parse(this, () -> parserContext); }	what's the plan here? from the code, it does not seem like we can phase out geohash given it's what any geopoint sub-field reads. i believe the comment is inaccurate as also keyword and text will get the geohash.
public NamedAnalyzer indexAnalyzer(String field, Function<String, NamedAnalyzer> unmappedFieldAnalyzer) { if (this.mapper == null) { return unmappedFieldAnalyzer.apply(field); } return this.mapper.mappers().indexAnalyzer(field, unmappedFieldAnalyzer); }	maybe rename unmappedfieldanalyzer into defaultanalyzer ? that feels wrong to rely on something called unmapped for runtime fields ?
@Override public void forceMerge(final boolean flush, int maxNumSegments, boolean onlyExpungeDeletes, final String forceMergeUUID) throws EngineException, IOException { if (onlyExpungeDeletes && maxNumSegments >= 0) { throw new IllegalArgumentException("only_expunge_deletes and max_num_segments are mutually exclusive"); } /* * We do NOT acquire the readlock here since we are waiting on the merges to finish * that's fine since the IW.rollback should stop all the threads and trigger an IOException * causing us to fail the forceMerge */ optimizeLock.lock(); try { ensureOpen(); store.incRef(); // increment the ref just to ensure nobody closes the store while we optimize try { if (onlyExpungeDeletes) { indexWriter.forceMergeDeletes(true /* blocks and waits for merges*/); } else if (maxNumSegments <= 0) { indexWriter.maybeMerge(); } else { indexWriter.forceMerge(maxNumSegments, true /* blocks and waits for merges*/); this.forceMergeUUID = forceMergeUUID; } if (flush) { flush(false, true); // If any merges happened then we need to release the unmerged input segments so they can be deleted. A periodic refresh // will do this eventually unless the user has disabled refreshes or isn't searching this shard frequently, in which // case we should do something here to ensure a timely refresh occurs. However there's no real need to defer it nor to // have any should-we-actually-refresh-here logic: we're already doing an expensive force-merge operation at the user's // request and therefore don't expect any further writes so we may as well do the final refresh immediately and get it // out of the way. refresh("force-merge"); } } finally { store.decRef(); } } catch (AlreadyClosedException ex) { /* in this case we first check if the engine is still open. If so this exception is just fine * and expected. We don't hold any locks while we block on forceMerge otherwise it would block * closing the engine as well. If we are not closed we pass it on to failOnTragicEvent which ensures * we are handling a tragic even exception here */ ensureOpen(ex); failOnTragicEvent(ex); throw ex; } catch (Exception e) { try { maybeFailEngine("force merge", e); } catch (Exception inner) { e.addSuppressed(inner); } throw e; } finally { optimizeLock.unlock(); } }	in the case of anyone force merging against an index that still receives writes, i think this risks writing a new segment for the refresh. i did not look whether it is possible, but it would be ideal if we could somehow refresh to the latest available segment?
public void testRequestFailureReplication() throws Exception { try (ReplicationGroup shards = createGroup(0)) { shards.startAll(); BulkItemResponse response = shards.index( new IndexRequest(index.getName(), "testRequestFailureException", "1") .source("{}", XContentType.JSON) .version(2) ); assertTrue(response.isFailed()); assertThat(response.getFailure().getCause(), instanceOf(VersionConflictEngineException.class)); shards.assertAllEqual(0); for (IndexShard indexShard : shards) { assertThat(indexShard.routingEntry() + " has the wrong number of ops in the translog", indexShard.translogStats().estimatedNumberOfOperations(), equalTo(0)); } // add some replicas int nReplica = randomIntBetween(1, 3); for (int i = 0; i < nReplica; i++) { shards.addReplica(); } shards.startReplicas(nReplica); response = shards.index( new IndexRequest(index.getName(), "testRequestFailureException", "1") .source("{}", XContentType.JSON) .version(2) ); assertTrue(response.isFailed()); assertThat(response.getFailure().getCause(), instanceOf(VersionConflictEngineException.class)); shards.assertAllEqual(0); for (IndexShard indexShard : shards) { assertThat(indexShard.routingEntry() + " has the wrong number of ops in the translog", indexShard.translogStats().estimatedNumberOfOperations(), equalTo(0)); } } }	can we call this testseqnocollision?
public void testTranslogDedupOperations() throws Exception { try (ReplicationGroup shards = createGroup(2)) { shards.startAll(); int initDocs = shards.indexDocs(randomInt(10)); List<IndexShard> replicas = shards.getReplicas(); IndexShard replica1 = replicas.get(0); IndexShard replica2 = replicas.get(1); logger.info("--> Isolate replica1"); IndexRequest indexDoc1 = new IndexRequest(index.getName(), "type", "d1").source("{}", XContentType.JSON); BulkShardRequest replicationRequest = indexOnPrimary(indexDoc1, shards.getPrimary()); for (int i = 1; i < replicas.size(); i++) { indexOnReplica(replicationRequest, replicas.get(i)); } final Translog.Operation op1; final List<Translog.Operation> initOperations = new ArrayList<>(initDocs); try (Translog.Snapshot snapshot = replica2.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 1)); for (int i = 0; i < initDocs; i++) { Translog.Operation op = snapshot.next(); assertThat(op, is(notNullValue())); initOperations.add(op); } op1 = snapshot.next(); assertThat(op1, notNullValue()); assertThat(snapshot.next(), nullValue()); assertThat(snapshot.overriddenOperations(), equalTo(0)); } // Make sure that replica2 receives translog ops (eg. op2) from replica1 and overwrites its stale operation (op1). logger.info("--> Promote replica1 as the primary"); shards.promoteReplicaToPrimary(replica1); shards.index(new IndexRequest(index.getName(), "type", "d2").source("{}", XContentType.JSON)); final Translog.Operation op2; try (Translog.Snapshot snapshot = replica2.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), greaterThanOrEqualTo(initDocs + 2)); op2 = snapshot.next(); assertThat(op2.seqNo(), equalTo(op1.seqNo())); assertThat(op2.primaryTerm(), greaterThan(op1.primaryTerm())); assertThat("Remaining of snapshot should contain init operations", snapshot, containsOperationsInAnyOrder(initOperations)); assertThat(snapshot.overriddenOperations(), greaterThanOrEqualTo(1)); } // Make sure that peer-recovery transfers all but non-overridden operations. IndexShard replica3 = shards.addReplica(); logger.info("--> Promote replica2 as the primary"); shards.promoteReplicaToPrimary(replica2); logger.info("--> Recover replica3 from replica2"); recoverReplica(replica3, replica2); try (Translog.Snapshot snapshot = replica3.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 1)); assertThat(snapshot.next(), equalTo(op2)); assertThat("Remaining of snapshot should contain init operations", snapshot, containsOperationsInAnyOrder(initOperations)); assertThat("Peer-recovery should not send overridden operations", snapshot.overriddenOperations(), equalTo(0)); } } }	can you just use replica2?
public void testTranslogDedupOperations() throws Exception { try (ReplicationGroup shards = createGroup(2)) { shards.startAll(); int initDocs = shards.indexDocs(randomInt(10)); List<IndexShard> replicas = shards.getReplicas(); IndexShard replica1 = replicas.get(0); IndexShard replica2 = replicas.get(1); logger.info("--> Isolate replica1"); IndexRequest indexDoc1 = new IndexRequest(index.getName(), "type", "d1").source("{}", XContentType.JSON); BulkShardRequest replicationRequest = indexOnPrimary(indexDoc1, shards.getPrimary()); for (int i = 1; i < replicas.size(); i++) { indexOnReplica(replicationRequest, replicas.get(i)); } final Translog.Operation op1; final List<Translog.Operation> initOperations = new ArrayList<>(initDocs); try (Translog.Snapshot snapshot = replica2.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 1)); for (int i = 0; i < initDocs; i++) { Translog.Operation op = snapshot.next(); assertThat(op, is(notNullValue())); initOperations.add(op); } op1 = snapshot.next(); assertThat(op1, notNullValue()); assertThat(snapshot.next(), nullValue()); assertThat(snapshot.overriddenOperations(), equalTo(0)); } // Make sure that replica2 receives translog ops (eg. op2) from replica1 and overwrites its stale operation (op1). logger.info("--> Promote replica1 as the primary"); shards.promoteReplicaToPrimary(replica1); shards.index(new IndexRequest(index.getName(), "type", "d2").source("{}", XContentType.JSON)); final Translog.Operation op2; try (Translog.Snapshot snapshot = replica2.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), greaterThanOrEqualTo(initDocs + 2)); op2 = snapshot.next(); assertThat(op2.seqNo(), equalTo(op1.seqNo())); assertThat(op2.primaryTerm(), greaterThan(op1.primaryTerm())); assertThat("Remaining of snapshot should contain init operations", snapshot, containsOperationsInAnyOrder(initOperations)); assertThat(snapshot.overriddenOperations(), greaterThanOrEqualTo(1)); } // Make sure that peer-recovery transfers all but non-overridden operations. IndexShard replica3 = shards.addReplica(); logger.info("--> Promote replica2 as the primary"); shards.promoteReplicaToPrimary(replica2); logger.info("--> Recover replica3 from replica2"); recoverReplica(replica3, replica2); try (Translog.Snapshot snapshot = replica3.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 1)); assertThat(snapshot.next(), equalTo(op2)); assertThat("Remaining of snapshot should contain init operations", snapshot, containsOperationsInAnyOrder(initOperations)); assertThat("Peer-recovery should not send overridden operations", snapshot.overriddenOperations(), equalTo(0)); } } }	why can't we test for hard equality?
public void testTranslogDedupOperations() throws Exception { try (ReplicationGroup shards = createGroup(2)) { shards.startAll(); int initDocs = shards.indexDocs(randomInt(10)); List<IndexShard> replicas = shards.getReplicas(); IndexShard replica1 = replicas.get(0); IndexShard replica2 = replicas.get(1); logger.info("--> Isolate replica1"); IndexRequest indexDoc1 = new IndexRequest(index.getName(), "type", "d1").source("{}", XContentType.JSON); BulkShardRequest replicationRequest = indexOnPrimary(indexDoc1, shards.getPrimary()); for (int i = 1; i < replicas.size(); i++) { indexOnReplica(replicationRequest, replicas.get(i)); } final Translog.Operation op1; final List<Translog.Operation> initOperations = new ArrayList<>(initDocs); try (Translog.Snapshot snapshot = replica2.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 1)); for (int i = 0; i < initDocs; i++) { Translog.Operation op = snapshot.next(); assertThat(op, is(notNullValue())); initOperations.add(op); } op1 = snapshot.next(); assertThat(op1, notNullValue()); assertThat(snapshot.next(), nullValue()); assertThat(snapshot.overriddenOperations(), equalTo(0)); } // Make sure that replica2 receives translog ops (eg. op2) from replica1 and overwrites its stale operation (op1). logger.info("--> Promote replica1 as the primary"); shards.promoteReplicaToPrimary(replica1); shards.index(new IndexRequest(index.getName(), "type", "d2").source("{}", XContentType.JSON)); final Translog.Operation op2; try (Translog.Snapshot snapshot = replica2.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), greaterThanOrEqualTo(initDocs + 2)); op2 = snapshot.next(); assertThat(op2.seqNo(), equalTo(op1.seqNo())); assertThat(op2.primaryTerm(), greaterThan(op1.primaryTerm())); assertThat("Remaining of snapshot should contain init operations", snapshot, containsOperationsInAnyOrder(initOperations)); assertThat(snapshot.overriddenOperations(), greaterThanOrEqualTo(1)); } // Make sure that peer-recovery transfers all but non-overridden operations. IndexShard replica3 = shards.addReplica(); logger.info("--> Promote replica2 as the primary"); shards.promoteReplicaToPrimary(replica2); logger.info("--> Recover replica3 from replica2"); recoverReplica(replica3, replica2); try (Translog.Snapshot snapshot = replica3.getTranslog().newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 1)); assertThat(snapshot.next(), equalTo(op2)); assertThat("Remaining of snapshot should contain init operations", snapshot, containsOperationsInAnyOrder(initOperations)); assertThat("Peer-recovery should not send overridden operations", snapshot.overriddenOperations(), equalTo(0)); } } }	can you also assert that the contents of the shards are the same - see replicationgroup#assertallequal
static DeprecationIssue checkTemplatesWithFieldNamesDisabled(ClusterState state) { Set<String> templatesContainingFieldNamed = new HashSet<>(); state.getMetaData().getTemplates().forEach((templateCursor) -> { String templateName = templateCursor.key; templateCursor.value.getMappings().forEach((mappingCursor) -> { Map<String, Object> map = XContentHelper.convertToMap(mappingCursor.value.compressedReference(), false, XContentType.JSON) .v2(); if (mapContainsFieldNamesDisabled(map)) { templatesContainingFieldNamed.add(templateName); } }); }); if (templatesContainingFieldNamed.isEmpty() == false) { return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, "Index templates contain _field_names settings.", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-8.0.html#fieldnames-enabling", "Index templates " + templatesContainingFieldNamed + " use the deprecated `enable` setting for the `" + FieldNamesFieldMapper.NAME + "` field. Using this setting in new index mappings will throw an error " + "in the next major version and needs to be removed from existing mappings and templates."); } return null; }	typo, templatescontainingfieldnamed -> templatescontainingfieldnames
static DeprecationIssue checkTemplatesWithFieldNamesDisabled(ClusterState state) { Set<String> templatesContainingFieldNamed = new HashSet<>(); state.getMetaData().getTemplates().forEach((templateCursor) -> { String templateName = templateCursor.key; templateCursor.value.getMappings().forEach((mappingCursor) -> { Map<String, Object> map = XContentHelper.convertToMap(mappingCursor.value.compressedReference(), false, XContentType.JSON) .v2(); if (mapContainsFieldNamesDisabled(map)) { templatesContainingFieldNamed.add(templateName); } }); }); if (templatesContainingFieldNamed.isEmpty() == false) { return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, "Index templates contain _field_names settings.", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-8.0.html#fieldnames-enabling", "Index templates " + templatesContainingFieldNamed + " use the deprecated `enable` setting for the `" + FieldNamesFieldMapper.NAME + "` field. Using this setting in new index mappings will throw an error " + "in the next major version and needs to be removed from existing mappings and templates."); } return null; }	do we need to use a recursive function here, or could we just check for it directly since we know _field_names will be at the top level of the mapping? also, there is a convenient method indexdeprecationchecks#findinpropertiesrecursively for iterating over a mapping.
static DeprecationIssue fieldNamesEnabling(IndexMetaData indexMetaData) { MappingMetaData mapping = indexMetaData.mapping(); if ((mapping != null) && ClusterDeprecationChecks.mapContainsFieldNamesDisabled(mapping.getSourceAsMap())) { return new DeprecationIssue(DeprecationIssue.Level.WARNING, "Index mapping contain explicit _field_names enabling settings.", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-8.0.html" + "#fieldnames-enabling", "The index mapping contains a deprecated `enabled` setting for `_field_names` that should be removed moving foward."); } return null; }	'index mapping contain' -> 'index mapping contains'
public static AnalysisStats of(Metadata metadata, Runnable ensureNotCancelled) { final Map<String, IndexFeatureStats> usedCharFilterTypes = new HashMap<>(); final Map<String, IndexFeatureStats> usedTokenizerTypes = new HashMap<>(); final Map<String, IndexFeatureStats> usedTokenFilterTypes = new HashMap<>(); final Map<String, IndexFeatureStats> usedAnalyzerTypes = new HashMap<>(); final Map<String, IndexFeatureStats> usedBuiltInCharFilters = new HashMap<>(); final Map<String, IndexFeatureStats> usedBuiltInTokenizers = new HashMap<>(); final Map<String, IndexFeatureStats> usedBuiltInTokenFilters = new HashMap<>(); final Map<String, IndexFeatureStats> usedBuiltInAnalyzers = new HashMap<>(); final Map<String, Integer> mappingCounts = Maps.newMapWithExpectedSize(metadata.getMappingsByHash().size()); for (IndexMetadata indexMetadata : metadata) { ensureNotCancelled.run(); if (indexMetadata.isSystem()) { // Don't include system indices in statistics about analysis, // we care about the user's indices. continue; } Set<String> indexCharFilters = new HashSet<>(); Set<String> indexTokenizers = new HashSet<>(); Set<String> indexTokenFilters = new HashSet<>(); Set<String> indexAnalyzerTypes = new HashSet<>(); Set<String> indexCharFilterTypes = new HashSet<>(); Set<String> indexTokenizerTypes = new HashSet<>(); Set<String> indexTokenFilterTypes = new HashSet<>(); Settings indexSettings = indexMetadata.getSettings(); Map<String, Settings> analyzerSettings = indexSettings.getGroups("index.analysis.analyzer"); usedBuiltInAnalyzers.keySet().removeAll(analyzerSettings.keySet()); for (Settings analyzerSetting : analyzerSettings.values()) { final String analyzerType = analyzerSetting.get("type", "custom"); IndexFeatureStats stats = usedAnalyzerTypes.computeIfAbsent(analyzerType, IndexFeatureStats::new); stats.count++; if (indexAnalyzerTypes.add(analyzerType)) { stats.indexCount++; } for (String charFilter : analyzerSetting.getAsList("char_filter")) { stats = usedBuiltInCharFilters.computeIfAbsent(charFilter, IndexFeatureStats::new); stats.count++; if (indexCharFilters.add(charFilter)) { stats.indexCount++; } } String tokenizer = analyzerSetting.get("tokenizer"); if (tokenizer != null) { stats = usedBuiltInTokenizers.computeIfAbsent(tokenizer, IndexFeatureStats::new); stats.count++; if (indexTokenizers.add(tokenizer)) { stats.indexCount++; } } for (String filter : analyzerSetting.getAsList("filter")) { stats = usedBuiltInTokenFilters.computeIfAbsent(filter, IndexFeatureStats::new); stats.count++; if (indexTokenFilters.add(filter)) { stats.indexCount++; } } } Map<String, Settings> charFilterSettings = indexSettings.getGroups("index.analysis.char_filter"); usedBuiltInCharFilters.keySet().removeAll(charFilterSettings.keySet()); aggregateAnalysisTypes(charFilterSettings.values(), usedCharFilterTypes, indexCharFilterTypes); Map<String, Settings> tokenizerSettings = indexSettings.getGroups("index.analysis.tokenizer"); usedBuiltInTokenizers.keySet().removeAll(tokenizerSettings.keySet()); aggregateAnalysisTypes(tokenizerSettings.values(), usedTokenizerTypes, indexTokenizerTypes); Map<String, Settings> tokenFilterSettings = indexSettings.getGroups("index.analysis.filter"); usedBuiltInTokenFilters.keySet().removeAll(tokenFilterSettings.keySet()); aggregateAnalysisTypes(tokenFilterSettings.values(), usedTokenFilterTypes, indexTokenFilterTypes); countMapping(mappingCounts, indexMetadata); } for (Map.Entry<String, Integer> mappingAndCount : mappingCounts.entrySet()) { Set<String> indexAnalyzers = new HashSet<>(); final MappingMetadata mappingMetadata = metadata.getMappingsByHash().get(mappingAndCount.getKey()); final int count = mappingAndCount.getValue(); MappingVisitor.visitMapping(mappingMetadata.getSourceAsMap(), (field, fieldMapping) -> { for (String key : new String[] { "analyzer", "search_analyzer", "search_quote_analyzer" }) { Object analyzerO = fieldMapping.get(key); if (analyzerO != null) { final String analyzer = analyzerO.toString(); IndexFeatureStats stats = usedBuiltInAnalyzers.computeIfAbsent(analyzer, IndexFeatureStats::new); stats.count += count; if (indexAnalyzers.add(analyzer)) { stats.indexCount += count; } } } }); } return new AnalysisStats( usedCharFilterTypes.values(), usedTokenizerTypes.values(), usedTokenFilterTypes.values(), usedAnalyzerTypes.values(), usedBuiltInCharFilters.values(), usedBuiltInTokenizers.values(), usedBuiltInTokenFilters.values(), usedBuiltInAnalyzers.values() ); }	i would find it more intuitive to use an identityhashmp or just a hash-map (since hash-code/equals compare on the hash anyway). is there a reason to use an explicit hash-value as key here?
public void testStartSnapshotWithSuccessfulShardClonePendingFinalization() throws Exception { final String masterName = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS); final String dataNode = internalCluster().startDataOnlyNode(); final String repoName = "test-repo"; createRepository(repoName, "mock"); final String indexName = "test-idx"; createIndexWithContent(indexName); final String sourceSnapshot = "source-snapshot"; createFullSnapshot(repoName, sourceSnapshot); blockMasterOnWriteIndexFile(repoName); final String cloneName = "clone-blocked"; final ActionFuture<AcknowledgedResponse> blockedClone = startClone(repoName, sourceSnapshot, cloneName, indexName); waitForBlock(masterName, repoName, TimeValue.timeValueSeconds(30L)); awaitNumberOfSnapshotsInProgress(1); blockNodeOnAnyFiles(repoName, dataNode); final ActionFuture<CreateSnapshotResponse> otherSnapshot = startFullSnapshot(repoName, "other-snapshot"); awaitNumberOfSnapshotsInProgress(2); assertFalse(blockedClone.isDone()); unblockNode(repoName, masterName); awaitNumberOfSnapshotsInProgress(1); awaitMasterFinishRepoOperations(); unblockNode(repoName, dataNode); assertAcked(blockedClone.get()); assertEquals(getSnapshot(repoName, cloneName).state(), SnapshotState.SUCCESS); assertSuccessful(otherSnapshot); }	i guess that it's enough to assert that the snapshot is successful to ensure that they get the corresponding generation number? otherwise it would be a partial snapshot, right?
public void testAbortNotStartedSnapshotWithoutIO() throws Exception { internalCluster().startMasterOnlyNode(); final String dataNode = internalCluster().startDataOnlyNode(); final String repoName = "test-repo"; createRepository(repoName, "mock"); createIndexWithContent("test-index"); final ActionFuture<CreateSnapshotResponse> createSnapshot1Future = startFullSnapshotBlockedOnDataNode("first-snapshot", repoName, dataNode); final String snapshotTwo = "second-snapshot"; final ActionFuture<CreateSnapshotResponse> createSnapshot2Future = startFullSnapshot(repoName, snapshotTwo); awaitNumberOfSnapshotsInProgress(2); assertAcked(startDeleteSnapshot(repoName, snapshotTwo).get()); final SnapshotException sne = expectThrows(SnapshotException.class, createSnapshot2Future::actionGet); assertFalse(createSnapshot1Future.isDone()); unblockNode(repoName, dataNode); assertSuccessful(createSnapshot1Future); assertThat(getRepositoryData(repoName).getGenId(), is(0L)); }	sorry for making this so complicated, but this required 4 new tests i'm afraid because you can have all possible combinations of clone or snapshot ready for a shard but not finalized and then wanting to run either a new clone or new snapshot => 2 x 2 = 4 possible buggy scenarios.
public void testMasterFailOverWithQueuedDeletes() throws Exception { internalCluster().startMasterOnlyNodes(3); final String dataNode = internalCluster().startDataOnlyNode(); final String repoName = "test-repo"; createRepository(repoName, "mock"); final String firstIndex = "index-one"; createIndexWithContent(firstIndex); final String firstSnapshot = "snapshot-one"; blockDataNode(repoName, dataNode); final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse = startFullSnapshotFromNonMasterClient(repoName, firstSnapshot); waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L)); final String dataNode2 = internalCluster().startDataOnlyNode(); ensureStableCluster(5); final String secondIndex = "index-two"; createIndexWithContent(secondIndex, dataNode2, dataNode); final String secondSnapshot = "snapshot-two"; final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot); logger.info("--> wait for snapshot on second data node to finish"); awaitClusterState(state -> { final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress); }); final ActionFuture<AcknowledgedResponse> firstDeleteFuture = startDeleteFromNonMasterClient(repoName, firstSnapshot); awaitNDeletionsInProgress(1); blockNodeOnAnyFiles(repoName, dataNode2); final ActionFuture<CreateSnapshotResponse> snapshotThreeFuture = startFullSnapshotFromNonMasterClient(repoName, "snapshot-three"); waitForBlock(dataNode2, repoName, TimeValue.timeValueSeconds(30L)); assertThat(firstSnapshotResponse.isDone(), is(false)); assertThat(secondSnapshotResponse.isDone(), is(false)); logger.info("--> waiting for all three snapshots to show up as in-progress"); assertBusy(() -> assertThat(currentSnapshots(repoName), hasSize(3)), 30L, TimeUnit.SECONDS); final ActionFuture<AcknowledgedResponse> deleteAllSnapshots = startDeleteFromNonMasterClient(repoName, "*"); logger.info("--> wait for delete to be enqueued in cluster state"); awaitClusterState(state -> { final SnapshotDeletionsInProgress deletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE); return deletionsInProgress.getEntries().size() == 1 && deletionsInProgress.getEntries().get(0).getSnapshots().size() == 3; }); logger.info("--> waiting for second snapshot to finish and the other two snapshots to become aborted"); assertBusy(() -> { assertThat(currentSnapshots(repoName), hasSize(2)); for (SnapshotsInProgress.Entry entry : clusterService().state().custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) { assertThat(entry.state(), is(SnapshotsInProgress.State.ABORTED)); assertThat(entry.snapshot().getSnapshotId().getName(), not(secondSnapshot)); } }, 30L, TimeUnit.SECONDS); logger.info("--> stopping current master node"); internalCluster().stopCurrentMasterNode(); unblockNode(repoName, dataNode); unblockNode(repoName, dataNode2); for (ActionFuture<AcknowledgedResponse> deleteFuture : Arrays.asList(firstDeleteFuture, deleteAllSnapshots)) { try { assertAcked(deleteFuture.actionGet()); } catch (RepositoryException rex) { // rarely the master node fails over twice when shutting down the initial master and fails the transport listener assertThat(rex.repository(), is("_all")); assertThat(rex.getMessage(), endsWith("Failed to update cluster state during repository operation")); } catch (SnapshotMissingException sme) { // very rarely a master node fail-over happens at such a time that the client on the data-node sees a disconnect exception // after the master has already started the delete, leading to the delete retry to run into a situation where the // snapshot has already been deleted potentially assertThat(sme.getSnapshotName(), is(firstSnapshot)); } } expectThrows(SnapshotException.class, snapshotThreeFuture::actionGet); logger.info("--> verify that all snapshots are gone and no more work is left in the cluster state"); assertBusy(() -> { assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty()); final ClusterState state = clusterService().state(); final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE); assertThat(snapshotsInProgress.entries(), empty()); final SnapshotDeletionsInProgress snapshotDeletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE); assertThat(snapshotDeletionsInProgress.getEntries(), empty()); }, 30L, TimeUnit.SECONDS); }	the fact that we would block on data files here even though snapshot 3 and snapshot 2 were snapshotting the same data actually already demonstrated the bug in hindsight. since this test forces an exception on this snapshot below this was hidden from us unfortunately.
*/ private static ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards( SnapshotsInProgress snapshotsInProgress, @Nullable SnapshotDeletionsInProgress deletionsInProgress, Metadata metadata, RoutingTable routingTable, List<IndexId> indices, boolean useShardGenerations, RepositoryData repositoryData, String repoName) { ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> builder = ImmutableOpenMap.builder(); final ShardGenerations shardGenerations = repositoryData.shardGenerations(); final InFlightShardSnapshotStates inFlightShardStates = InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries()); final boolean readyToExecute = deletionsInProgress == null || deletionsInProgress.getEntries().stream() .noneMatch(entry -> entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.STARTED); for (IndexId index : indices) { final String indexName = index.getName(); final boolean isNewIndex = repositoryData.getIndices().containsKey(indexName) == false; IndexMetadata indexMetadata = metadata.index(indexName); if (indexMetadata == null) { // The index was deleted before we managed to start the snapshot - mark it as missing. builder.put(new ShardId(indexName, IndexMetadata.INDEX_UUID_NA_VALUE, 0), ShardSnapshotStatus.MISSING); } else { final IndexRoutingTable indexRoutingTable = routingTable.index(indexName); assert indexRoutingTable != null; for (int i = 0; i < indexMetadata.getNumberOfShards(); i++) { final ShardId shardId = indexRoutingTable.shard(i).shardId(); final String shardRepoGeneration; if (useShardGenerations) { final String inFlightGeneration = inFlightShardStates.bestGeneration(index, shardId.id(), shardGenerations); if (inFlightGeneration == null && isNewIndex) { assert shardGenerations.getShardGen(index, shardId.getId()) == null : "Found shard generation for new index [" + index + "]"; shardRepoGeneration = ShardGenerations.NEW_SHARD_GEN; } else { shardRepoGeneration = inFlightGeneration; } } else { shardRepoGeneration = null; } final ShardSnapshotStatus shardSnapshotStatus; ShardRouting primary = indexRoutingTable.shard(i).primaryShard(); if (readyToExecute == false || inFlightShardStates.isBusy(shardId.getIndexName(), shardId.id())) { shardSnapshotStatus = ShardSnapshotStatus.UNASSIGNED_QUEUED; } else if (primary == null || !primary.assignedToNode()) { shardSnapshotStatus = new ShardSnapshotStatus(null, ShardState.MISSING, "primary shard is not allocated", shardRepoGeneration); } else if (primary.relocating() || primary.initializing()) { shardSnapshotStatus = new ShardSnapshotStatus( primary.currentNodeId(), ShardState.WAITING, shardRepoGeneration); } else if (!primary.started()) { shardSnapshotStatus = new ShardSnapshotStatus(primary.currentNodeId(), ShardState.MISSING, "primary shard hasn't been started yet", shardRepoGeneration); } else { shardSnapshotStatus = new ShardSnapshotStatus(primary.currentNodeId(), shardRepoGeneration); } builder.put(shardId, shardSnapshotStatus); } } } return builder.build(); }	double checking my understanding, this will also update the shard generation to use for waiting shard snapshots, right?
public ClusterState execute(ClusterState currentState) { final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); final List<SnapshotsInProgress.Entry> updatedEntries = new ArrayList<>(snapshotsInProgress.entries()); boolean changed = false; final String localNodeId = currentState.nodes().getLocalNodeId(); final String repoName = cloneEntry.repository(); final ShardGenerations shardGenerations = repoData.shardGenerations(); for (int i = 0; i < updatedEntries.size(); i++) { if (cloneEntry.snapshot().equals(updatedEntries.get(i).snapshot())) { final ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clonesBuilder = ImmutableOpenMap.builder(); final InFlightShardSnapshotStates inFlightShardStates = InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries()); for (Tuple<IndexId, Integer> count : counts) { for (int shardId = 0; shardId < count.v2(); shardId++) { final RepositoryShardId repoShardId = new RepositoryShardId(count.v1(), shardId); final String indexName = repoShardId.indexName(); if (inFlightShardStates.isBusy(indexName, shardId)) { clonesBuilder.put(repoShardId, ShardSnapshotStatus.UNASSIGNED_QUEUED); } else { clonesBuilder.put(repoShardId, new ShardSnapshotStatus(localNodeId, inFlightShardStates.bestGeneration(repoShardId.index(), shardId, shardGenerations))); } } } updatedEntry = cloneEntry.withClones(clonesBuilder.build()); updatedEntries.set(i, updatedEntry); changed = true; break; } } return updateWithSnapshots(currentState, changed ? SnapshotsInProgress.of(updatedEntries) : null, null); }	obvious bug here and in snapshot creation in hindsight ... fortunately this was not something that would corrupt the repository but it would lead to a lot of partial snapshots in practice (seen this in prod. logs as well not just this test failure).
protected void awaitMasterFinishRepoOperations() throws Exception { logger.info("--> waiting for master to finish all repo operations on its SNAPSHOT pool"); final ThreadPool masterThreadPool = internalCluster().getMasterNodeInstance(ThreadPool.class); assertBusy(() -> { for (ThreadPoolStats.Stats stat : masterThreadPool.stats()) { if (ThreadPool.Names.SNAPSHOT.equals(stat.getName())) { assertEquals(stat.getActive(), 0); break; } } }); }	the number of active threads relies on threadpoolexecutor#getactivecount() which is approximate, but having core == max should make this ok i think.
public static void configureCompile(Project project) { project.getExtensions().getExtraProperties().set("compactProfile", "full"); JavaPluginExtension java = project.getExtensions().getByType(JavaPluginExtension.class); java.setSourceCompatibility(BuildParams.getMinimumRuntimeVersion()); java.setTargetCompatibility(BuildParams.getMinimumRuntimeVersion()); Function<File, String> canonicalPath = file -> { try { return file.getCanonicalPath(); } catch (IOException e) { throw new GradleException("Failed to get canonical path for " + file, e); } }; project.afterEvaluate(p -> { project.getTasks().withType(JavaCompile.class).configureEach(compileTask -> { CompileOptions compileOptions = compileTask.getOptions(); /* * -path because gradle will send in paths that don't always exist. * -missing because we have tons of missing @returns and @param. * -serial because we don't use java serialization. */ // don't even think about passing args with -J-xxx, oracle will ask you to submit a bug report :) // fail on all javac warnings List<String> compilerArgs = compileOptions.getCompilerArgs(); compilerArgs.add("-Werror"); compilerArgs.add("-Xlint:all,-path,-serial,-options,-deprecation,-try"); compilerArgs.add("-Xdoclint:all"); compilerArgs.add("-Xdoclint:-missing"); // either disable annotation processor completely (default) or allow to enable them if an annotation processor is explicitly // defined if (compilerArgs.contains("-processor") == false) { compilerArgs.add("-proc:none"); } compileOptions.setEncoding("UTF-8"); compileOptions.setIncremental(true); final JavaVersion targetCompatibilityVersion = JavaVersion.toVersion(compileTask.getTargetCompatibility()); compileOptions.getRelease().set(Integer.parseInt(targetCompatibilityVersion.getMajorVersion())); }); // also apply release flag to groovy, which is used in build-tools project.getTasks().withType(GroovyCompile.class).configureEach(compileTask -> { // TODO: this probably shouldn't apply to groovy at all? final JavaVersion targetCompatibilityVersion = JavaVersion.toVersion(compileTask.getTargetCompatibility()); compileTask.getOptions().getRelease().set(Integer.parseInt(targetCompatibilityVersion.getMajorVersion())); }); }); }	should we use a provider here so we an avoid any future issues of configuration ordering if individual modules override target compatibility version?
private static boolean searchWithCollectorManager(SearchContext searchContext, ContextIndexSearcher searcher, Query query, CheckedConsumer<List<LeafReaderContext>, IOException> leafSorter, boolean timeoutSet) throws IOException { final IndexReader reader = searchContext.searcher().getIndexReader(); final int numHits = Math.min(searchContext.from() + searchContext.size(), Math.max(1, reader.numDocs())); final SortAndFormats sortAndFormats = searchContext.sort(); int totalHitsThreshold; TotalHits totalHits; if (searchContext.trackTotalHitsUpTo() == SearchContext.TRACK_TOTAL_HITS_DISABLED) { totalHitsThreshold = 1; totalHits = new TotalHits(0, TotalHits.Relation.GREATER_THAN_OR_EQUAL_TO); } else { int hitCount = shortcutTotalHitCount(reader, query); if (hitCount == -1) { totalHitsThreshold = searchContext.trackTotalHitsUpTo(); totalHits = null; // will be computed via the collector } else { totalHitsThreshold = 1; totalHits = new TotalHits(hitCount, TotalHits.Relation.EQUAL_TO); // don't compute hit counts via the collector } } CollectorManager<TopFieldCollector, TopFieldDocs> sharedManager = TopFieldCollector.createSharedManager( sortAndFormats.sort, numHits, null, totalHitsThreshold); List<LeafReaderContext> leaves = new ArrayList<>(searcher.getIndexReader().leaves()); leafSorter.accept(leaves); try { Weight weight = searcher.createWeight(searcher.rewrite(query), ScoreMode.TOP_SCORES, 1f); searcher.search(leaves, weight, sharedManager, searchContext.queryResult(), sortAndFormats.formats, totalHits); } catch (TimeExceededException e) { assert timeoutSet : "TimeExceededException thrown even though timeout wasn't set"; if (searchContext.request().allowPartialSearchResults() == false) { // Can't rethrow TimeExceededException because not serializable throw new QueryPhaseExecutionException(searchContext.shardTarget(), "Time exceeded"); } searchContext.queryResult().searchTimedOut(true); } return false; // no rescoring when sorting by field }	i am wondering why we had this statement before? i don't see anywhere in the code that we add a releasable with a scope of lifetime.collection.
@Ignore /* Ignored for feature branch, awaits fix: https://github.com/elastic/elasticsearch/issues/49469 */ public void test20CreateKeystoreManually() throws Exception { rmKeystoreIfExists(); createKeystore(); final Installation.Executables bin = installation.executables(); verifyKeystorePermissions(); String possibleSudo = distribution().isArchive() && Platforms.LINUX ? "sudo -u " + ARCHIVE_OWNER + " " : ""; Shell.Result r = sh.run(possibleSudo + bin.keystoreTool + " list"); assertThat(r.stdout, containsString("keystore.seed")); }	you can use keystoretool.run like above now right?
@Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) { assert msg instanceof ByteBuf; final boolean queued = queuedWrites.offer(new WriteOperation((ByteBuf) msg, promise)); assert queued; assert Transports.assertDefaultThreadContext(transport.getThreadPool().getThreadContext()); }	think we can keep this assertion? it relates to #57792.
private void internalSend(TcpChannel channel, SendContext sendContext) throws IOException { channel.getChannelStats().markAccessed(threadPool.relativeTimeInMillis()); BytesReference reference = sendContext.get(); try (ThreadContext.StoredContext existing = threadPool.getThreadContext().stashContext()) { channel.sendMessage(reference, sendContext); } catch (RuntimeException ex) { sendContext.onFailure(ex); CloseableChannel.closeChannel(channel); throw ex; } }	think we can keep this comment? it relates to #57792.
@Override public int compareTo(StoreStatus other) { if (storeException != null && other.storeException == null) { return 1; } else if (other.storeException != null && storeException == null) { return -1; } if (allocationId != null && other.allocationId == null) { return -1; } else if (allocationId == null && other.allocationId != null) { return 1; } else if (allocationId == null && other.allocationId == null) { return Integer.compare(allocationStatus.id, other.allocationStatus.id); } else { int compare = Integer.compare(allocationStatus.id, other.allocationStatus.id); if (compare == 0) { return allocationId.compareTo(other.allocationId); } return compare; } } } /** * Single node failure while retrieving shard store information */ public static class Failure extends DefaultShardOperationFailedException { private String nodeId; public Failure(String nodeId, String index, int shardId, Throwable reason) { super(index, shardId, reason); this.nodeId = nodeId; } private Failure(StreamInput in) throws IOException { if (in.getVersion().before(Version.V_7_4_0)) { nodeId = in.readString(); } readFrom(in, this); if (in.getVersion().onOrAfter(Version.V_7_4_0)) { nodeId = in.readString(); } } public String nodeId() { return nodeId; } static Failure readFailure(StreamInput in) throws IOException { return new Failure(in); } @Override public void writeTo(StreamOutput out) throws IOException { if (out.getVersion().before(Version.V_7_4_0)) { out.writeString(nodeId); } super.writeTo(out); if (out.getVersion().onOrAfter(Version.V_7_4_0)) { out.writeString(nodeId); } } @Override public XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException { builder.field("node", nodeId()); return super.innerToXContent(builder, params); } } private ImmutableOpenMap<String, ImmutableOpenIntMap<List<StoreStatus>>> storeStatuses; private List<Failure> failures; public IndicesShardStoresResponse(ImmutableOpenMap<String, ImmutableOpenIntMap<List<StoreStatus>>> storeStatuses, List<Failure> failures) { this.storeStatuses = storeStatuses; this.failures = failures; } IndicesShardStoresResponse() { this(ImmutableOpenMap.of(), Collections.emptyList()); } public IndicesShardStoresResponse(StreamInput in) throws IOException { readFrom(in); } /** * Returns {@link StoreStatus}s * grouped by their index names and shard ids. */ public ImmutableOpenMap<String, ImmutableOpenIntMap<List<StoreStatus>>> getStoreStatuses() { return storeStatuses; } /** * Returns node {@link Failure}s encountered * while executing the request */ public List<Failure> getFailures() { return failures; } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); int numResponse = in.readVInt(); ImmutableOpenMap.Builder<String, ImmutableOpenIntMap<List<StoreStatus>>> storeStatusesBuilder = ImmutableOpenMap.builder(); for (int i = 0; i < numResponse; i++) { String index = in.readString(); int indexEntries = in.readVInt(); ImmutableOpenIntMap.Builder<List<StoreStatus>> shardEntries = ImmutableOpenIntMap.builder(); for (int shardCount = 0; shardCount < indexEntries; shardCount++) { int shardID = in.readInt(); int nodeEntries = in.readVInt(); List<StoreStatus> storeStatuses = new ArrayList<>(nodeEntries); for (int nodeCount = 0; nodeCount < nodeEntries; nodeCount++) { storeStatuses.add(readStoreStatus(in)); } shardEntries.put(shardID, storeStatuses); } storeStatusesBuilder.put(index, shardEntries.build()); } int numFailure = in.readVInt(); List<Failure> failureBuilder = new ArrayList<>(); for (int i = 0; i < numFailure; i++) { failureBuilder.add(Failure.readFailure(in)); } storeStatuses = storeStatusesBuilder.build(); failures = Collections.unmodifiableList(failureBuilder); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeVInt(storeStatuses.size()); for (ObjectObjectCursor<String, ImmutableOpenIntMap<List<StoreStatus>>> indexShards : storeStatuses) { out.writeString(indexShards.key); out.writeVInt(indexShards.value.size()); for (IntObjectCursor<List<StoreStatus>> shardStatusesEntry : indexShards.value) { out.writeInt(shardStatusesEntry.key); out.writeVInt(shardStatusesEntry.value.size()); for (StoreStatus storeStatus : shardStatusesEntry.value) { storeStatus.writeTo(out); } } } out.writeVInt(failures.size()); for (Failure failure : failures) { failure.writeTo(out); } } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (failures.size() > 0) { builder.startArray(Fields.FAILURES); for (Failure failure : failures) { failure.toXContent(builder, params); } builder.endArray(); } builder.startObject(Fields.INDICES); for (ObjectObjectCursor<String, ImmutableOpenIntMap<List<StoreStatus>>> indexShards : storeStatuses) { builder.startObject(indexShards.key); builder.startObject(Fields.SHARDS); for (IntObjectCursor<List<StoreStatus>> shardStatusesEntry : indexShards.value) { builder.startObject(String.valueOf(shardStatusesEntry.key)); builder.startArray(Fields.STORES); for (StoreStatus storeStatus : shardStatusesEntry.value) { builder.startObject(); storeStatus.toXContent(builder, params); builder.endObject(); } builder.endArray(); builder.endObject(); } builder.endObject(); builder.endObject(); } builder.endObject(); return builder; }	i missed this, but this seems unnecessary too
public static DefaultShardOperationFailedException readShardOperationFailed(StreamInput in) throws IOException { return new DefaultShardOperationFailedException(in); }	i missed this, but this seems unnecessary too
public void processExistingRecoveries(RoutingAllocation allocation) { MetaData metaData = allocation.metaData(); RoutingNodes routingNodes = allocation.routingNodes(); List<Runnable> shardCancellationActions = new ArrayList<>(); for (RoutingNode routingNode : routingNodes) { for (ShardRouting shard : routingNode) { if (shard.primary()) { continue; } if (shard.initializing() == false) { continue; } if (shard.relocatingNodeId() != null) { continue; } // if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one... if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) { if (canRemainOnCurrentNode(shard, allocation) == false) { // cancel new initializing replica if allocation deciders say no UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "new replica allocation to [" + shard.currentNodeId() + "] cancelled by allocation deciders", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT, Sets.newHashSet(shard.currentNodeId())); shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } continue; } AsyncShardFetch.FetchResult<NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation); if (shardStores.hasData() == false) { logger.trace("{}: fetching new stores for initializing shard", shard); continue; // still fetching } ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId()); assert primaryShard != null : "the replica shard can be allocated on at least one node, so there must be an active primary"; assert primaryShard.currentNodeId() != null; final DiscoveryNode primaryNode = allocation.nodes().get(primaryShard.currentNodeId()); final TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryNode, shardStores); if (primaryStore == null) { // if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed) // just let the recovery find it out, no need to do anything about it for the initializing shard logger.trace("{}: no primary shard store found or allocated, letting actual allocation figure it out", shard); continue; } MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, true, primaryNode, primaryStore, shardStores, false); if (matchingNodes.getNodeWithHighestMatch() != null) { DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId()); DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch(); // current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider if (currentNode.equals(nodeWithHighestMatch) == false && matchingNodes.canPerformNoopRecovery(nodeWithHighestMatch) && canPerformOperationBasedRecovery(primaryStore, shardStores, currentNode) == false) { // we found a better match that can perform noop recovery, cancel the existing allocation. logger.debug("cancelling allocation of replica on [{}], can perform a noop recovery on node [{}]", currentNode, nodeWithHighestMatch); final Set<String> failedNodeIds = shard.unassignedInfo() == null ? Collections.emptySet() : shard.unassignedInfo().getFailedNodeIds(); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + currentNode + "] cancelled, can perform a noop recovery on ["+ nodeWithHighestMatch + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT, failedNodeIds); // don't cancel shard in the loop as it will cause a ConcurrentModificationException shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } } else if (canRemainOnCurrentNode(shard, allocation) == false) { // cancel replica recoveries if allocation deciders say no UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + shard.currentNodeId() + "] cancelled by allocation deciders", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT, Sets.newHashSet(shard.currentNodeId())); shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } } } for (Runnable action : shardCancellationActions) { action.run(); } }	this else means we don't cancel ongoing recoveries if matchingnodes.getnodewithhighestmatch() != null; i think we want to consider cancelling ongoing recoveries whether there are any matching nodes or not. why not check this once at the top of the loop, just before if (shard.relocatingnodeid() != null) {?
private boolean canRemainOnCurrentNode(ShardRouting shard, RoutingAllocation allocation) { return allocation.deciders().canRemain(shard, allocation.routingNodes().node(shard.currentNodeId()), allocation).type() == Decision.Type.YES; } /** * Is the allocator responsible for allocating the given {@link ShardRouting}	i don't think that canremain today will return throttle but i would still prefer to treat throttle as a yes here. suggestion return allocation.deciders().canremain(shard, allocation.routingnodes().node(shard.currentnodeid()), allocation).type() != decision.type.no; alternatively, rename this to cannotremainoncurrentnode (and use == decision.type.no) since we always negate the result of this method.
public void testCancelExistingRecoveriesOnDiskPassingHighWatermark() throws Exception { // start two data node String node0 = internalCluster().startNode(Settings.builder() .put(Environment.PATH_DATA_SETTING.getKey(), createTempDir()) .put("node.attr.tag", "node0").build()); String node1 = internalCluster().startNode(Settings.builder() .put(Environment.PATH_DATA_SETTING.getKey(), createTempDir()) .put("node.attr.tag", "node1").build()); final MockInternalClusterInfoService clusterInfoService = getMockInternalClusterInfoService(); clusterInfoService.setUpdateFrequency(TimeValue.timeValueMillis(50)); clusterInfoService.onMaster(); ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); // prevent any effects from in-flight recoveries, since we are only simulating a 100-byte disk clusterInfoService.shardSizeFunction = shardRouting -> 0L; // start with all nodes below the watermark clusterInfoService.diskUsageFunction = (discoveryNode, fsInfoPath) -> setDiskUsage(fsInfoPath, 100, between(10, 100)); final boolean watermarkBytes = randomBoolean(); // we have to consistently use bytes or percentage for the disk watermark settings assertAcked(client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder() .put(CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), watermarkBytes ? "10b" : "90%") .put(CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), watermarkBytes ? "10b" : "90%") .put(CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), watermarkBytes ? "0b" : "100%") .put(CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), "1s") .put(INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.getKey(), "10s") .put(INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), "1b"))); // Create an index with 2 shards with 1 replica, but only allow allocation on node0, so all replicas are left unassigned final String index = "test"; assertAcked(prepareCreate(index).setSettings(Settings.builder() .put("number_of_shards", 2) .put("number_of_replicas", 1) .put("refresh_interval", -1) .put("index.routing.allocation.include.tag", "node0"))); // ensure yellow assertBusy(() -> assertThat(client().admin().cluster().prepareHealth(index).get().getStatus(), equalTo(ClusterHealthStatus.YELLOW))); // index some docs final int docCount = 100; for (int i = 0; i < docCount; i++) { index(index, "_doc", JsonXContent.contentBuilder().startObject().endObject()); } // update routing allocation to include all nodes, so node1 should have allocated new replica assertAcked(client().admin().indices().prepareUpdateSettings().setSettings(Settings.builder() .put("index.routing.allocation.include.tag", "node0,node1").build()).get()); assertBusy(() -> { final Map<String, Integer> shardCountByNodeId = getShardCountByNodeId(false, ShardRoutingState.INITIALIZING); assertThat("node1 has more than one initializing replica shards", shardCountByNodeId.get(resolveNodeByName(node1, clusterState).nodeId()), greaterThanOrEqualTo(1)); }); clusterInfoService.diskUsageFunction = (discoveryNode, fsInfoPath) -> setDiskUsage(fsInfoPath, 100, discoveryNode.getId().equals(resolveNodeByName(node1, clusterState).nodeId()) ? between(0, 9) : between(10, 100)); clusterInfoService.refresh(); logger.info("--> waiting for shards to cancel recovery on node [{}]", resolveNodeByName(node1, clusterState).nodeId()); assertBusy(() -> { final Map<String, Integer> shardCountByNodeId = getShardCountByNodeId(false, ShardRoutingState.INITIALIZING); assertThat("node1 has 0 initializing replica shards", shardCountByNodeId.get(resolveNodeByName(node1, clusterState).nodeId()), equalTo(0)); }); // move all nodes below watermark again logger.info("--> backup disk function"); clusterInfoService.diskUsageFunction = (discoveryNode, fsInfoPath) -> setDiskUsage(fsInfoPath, 100, between(10, 100)); clusterInfoService.setUpdateFrequency(TimeValue.timeValueMillis(1000)); assertAcked(client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder() .put(INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), "40MB"))); ensureGreen(index); logger.info("test passed"); }	use .putnull() to remove the filter entirely?
public void testCancelExistingRecoveriesOnDiskPassingHighWatermark() throws Exception { // start two data node String node0 = internalCluster().startNode(Settings.builder() .put(Environment.PATH_DATA_SETTING.getKey(), createTempDir()) .put("node.attr.tag", "node0").build()); String node1 = internalCluster().startNode(Settings.builder() .put(Environment.PATH_DATA_SETTING.getKey(), createTempDir()) .put("node.attr.tag", "node1").build()); final MockInternalClusterInfoService clusterInfoService = getMockInternalClusterInfoService(); clusterInfoService.setUpdateFrequency(TimeValue.timeValueMillis(50)); clusterInfoService.onMaster(); ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); // prevent any effects from in-flight recoveries, since we are only simulating a 100-byte disk clusterInfoService.shardSizeFunction = shardRouting -> 0L; // start with all nodes below the watermark clusterInfoService.diskUsageFunction = (discoveryNode, fsInfoPath) -> setDiskUsage(fsInfoPath, 100, between(10, 100)); final boolean watermarkBytes = randomBoolean(); // we have to consistently use bytes or percentage for the disk watermark settings assertAcked(client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder() .put(CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), watermarkBytes ? "10b" : "90%") .put(CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), watermarkBytes ? "10b" : "90%") .put(CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), watermarkBytes ? "0b" : "100%") .put(CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), "1s") .put(INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.getKey(), "10s") .put(INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), "1b"))); // Create an index with 2 shards with 1 replica, but only allow allocation on node0, so all replicas are left unassigned final String index = "test"; assertAcked(prepareCreate(index).setSettings(Settings.builder() .put("number_of_shards", 2) .put("number_of_replicas", 1) .put("refresh_interval", -1) .put("index.routing.allocation.include.tag", "node0"))); // ensure yellow assertBusy(() -> assertThat(client().admin().cluster().prepareHealth(index).get().getStatus(), equalTo(ClusterHealthStatus.YELLOW))); // index some docs final int docCount = 100; for (int i = 0; i < docCount; i++) { index(index, "_doc", JsonXContent.contentBuilder().startObject().endObject()); } // update routing allocation to include all nodes, so node1 should have allocated new replica assertAcked(client().admin().indices().prepareUpdateSettings().setSettings(Settings.builder() .put("index.routing.allocation.include.tag", "node0,node1").build()).get()); assertBusy(() -> { final Map<String, Integer> shardCountByNodeId = getShardCountByNodeId(false, ShardRoutingState.INITIALIZING); assertThat("node1 has more than one initializing replica shards", shardCountByNodeId.get(resolveNodeByName(node1, clusterState).nodeId()), greaterThanOrEqualTo(1)); }); clusterInfoService.diskUsageFunction = (discoveryNode, fsInfoPath) -> setDiskUsage(fsInfoPath, 100, discoveryNode.getId().equals(resolveNodeByName(node1, clusterState).nodeId()) ? between(0, 9) : between(10, 100)); clusterInfoService.refresh(); logger.info("--> waiting for shards to cancel recovery on node [{}]", resolveNodeByName(node1, clusterState).nodeId()); assertBusy(() -> { final Map<String, Integer> shardCountByNodeId = getShardCountByNodeId(false, ShardRoutingState.INITIALIZING); assertThat("node1 has 0 initializing replica shards", shardCountByNodeId.get(resolveNodeByName(node1, clusterState).nodeId()), equalTo(0)); }); // move all nodes below watermark again logger.info("--> backup disk function"); clusterInfoService.diskUsageFunction = (discoveryNode, fsInfoPath) -> setDiskUsage(fsInfoPath, 100, between(10, 100)); clusterInfoService.setUpdateFrequency(TimeValue.timeValueMillis(1000)); assertAcked(client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder() .put(INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), "40MB"))); ensureGreen(index); logger.info("test passed"); }	looks leftover from development? suggestion
private Map<String, Integer> getShardCountByNodeId(boolean primary, ShardRoutingState... shardState) { final Map<String, Integer> shardCountByNodeId = new HashMap<>(); final ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); for (final RoutingNode node : clusterState.getRoutingNodes()) { List<ShardRouting> shards = node.shardsWithState(shardState); int count = 0; for (ShardRouting shardRouting : shards) { if (shardRouting.primary() == primary) { count++; } } shardCountByNodeId.put(node.nodeId(), count); logger.info("----> node {} has {} {} shards. primary: {}", node.nodeId(), count, Arrays.toString(shardState), primary); } return shardCountByNodeId; }	a method for this seems excessive. it is only used in expressions of the form resolvenodebyname(node1, clusterstate).nodeid() which returns the (fixed) node id of node1. better to simply obtain the node id of node1 at the start of the test.
@Override public boolean equals(Object obj) { if (obj.getClass().equals(this.getClass()) == false) { return false; } EpochMillisDateFormatter other = (EpochMillisDateFormatter) obj; return Objects.equals(pattern(), other.pattern()) && Objects.equals(zoneId, other.zoneId) && Objects.equals(locale, other.locale); }	i thought the pattern is constant in this class?
synchronized void coordinateReads() { if (isStopped()) { LOGGER.info("{} shard follow task has been stopped", params.getFollowShardId()); return; } LOGGER.trace("{} coordinate reads, lastRequestedSeqno={}, leaderGlobalCheckpoint={}", params.getFollowShardId(), lastRequestedSeqno, leaderGlobalCheckpoint); final int maxBatchOperationCount = params.getMaxBatchOperationCount(); while (hasReadBudget() && lastRequestedSeqno < leaderGlobalCheckpoint) { final long from = lastRequestedSeqno + 1; final long maxRequiredSeqNo = Math.min(leaderGlobalCheckpoint, from + maxBatchOperationCount - 1); final int requestBatchCount; if (numConcurrentReads == 0) { // If this is the only request, we can treat it as a peek read. requestBatchCount = maxBatchOperationCount; } else { requestBatchCount = Math.toIntExact(maxRequiredSeqNo - from + 1); } assert 0 < requestBatchCount && requestBatchCount <= maxBatchOperationCount : "request_batch_count=" + requestBatchCount; LOGGER.trace("{}[{} ongoing reads] read from_seqno={} max_required_seqno={} batch_count={}", params.getFollowShardId(), numConcurrentReads, from, maxRequiredSeqNo, requestBatchCount); numConcurrentReads++; sendShardChangesRequest(from, requestBatchCount, maxRequiredSeqNo); lastRequestedSeqno = maxRequiredSeqNo; } if (numConcurrentReads == 0 && hasReadBudget()) { assert lastRequestedSeqno == leaderGlobalCheckpoint; // We sneak peek if there is any thing new in the leader. // If there is we will happily accept numConcurrentReads++; long from = lastRequestedSeqno + 1; LOGGER.trace("{}[{}] peek read [{}]", params.getFollowShardId(), numConcurrentReads, from); sendShardChangesRequest(from, maxBatchOperationCount, lastRequestedSeqno); } }	add "add let it optimistically fetch more documents if possible (but not require it)"?
public void infer(TrainedModelDeploymentTask task, double[] inputs, ActionListener<PyTorchResult> listener) { ProcessContext processContext = processContextByAllocation.get(task.getAllocationId()); try { String requestId = processContext.process.get().writeInferenceRequest(inputs); waitForResult(processContext, requestId, listener); } catch (IOException e) { logger.error(new ParameterizedMessage("[{}] error writing to process", processContext.modelId), e); listener.onFailure(ExceptionsHelper.serverError("error writing to process", e)); return; } }	a future improvement would be to not block the thread here
public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (name == null) { validationException = addValidationError("name is missing", validationException); } if (indexPatterns == null || indexPatterns.size() == 0) { validationException = addValidationError("index_patterns is missing", validationException); } return validationException; }	i wonder if this should be "index patterns are missing" as these requests can also come in from the transport layer where there is no notion of a "index_patterns" field?
public AttributeMap<E> build() { // copy, in case someone would do a .build, .put, .build sequence AttributeMap<E> m = new AttributeMap<>(); m.addAll(map); return m; }	why not return the map directly? attributemap is immutable. to guarantee that the builder is not used *after* build() is called, a flag could be used instead of copying things over.
* * @see Searcher#close() */ public Searcher acquireSearcher(String source, SearcherScope scope) throws EngineException { /* Acquire order here is store -> manager since we need * to make sure that the store is not closed before * the searcher is acquired. */ if (store.tryIncRef() == false) { throw new AlreadyClosedException(shardId + " store is closed", failedEngine.get()); } Releasable releasable = store::decRef; try { EngineSearcher engineSearcher = new EngineSearcher(source, getReferenceManager(scope), store, logger); releasable = null; // success - hand over the reference to the engine searcher return engineSearcher; } catch (AlreadyClosedException ex) { throw ex; } catch (Exception ex) { ensureOpen(ex); // throw EngineCloseException here if we are already closed logger.error(() -> new ParameterizedMessage("failed to acquire searcher, source {}", source), ex); throw new EngineException(shardId, "failed to acquire searcher, source " + source, ex); } finally { Releasables.close(releasable); } }	should have a maybefailengine here?
*/ public void getRoleMappingsAsync(final GetRoleMappingsRequest request, final RequestOptions options, final ActionListener<GetRoleMappingsResponse> listener) { restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::getRoleMappings, options, GetRoleMappingsResponse::fromXContent, listener, emptySet()); } /** * Enable a native realm or built-in user synchronously. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html"> * the docs</a> for more. * * @param request the request with the user to enable * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT} if nothing needs to be customized * @return the response from the enable user call * @throws IOException in case there is a problem sending the request or parsing back the response * @deprecated use {@link #enableUser(RequestOptions, EnableUserRequest)}	this doesn't make much sense to me since it returns a boolean. suggestion * @return {@code true} if the request succeeded (the user is enabled)
List<FetchSubPhaseProcessor> getProcessors(SearchShardTarget target, FetchContext context, Profiler profiler) { try { List<FetchSubPhaseProcessor> processors = new ArrayList<>(); for (FetchSubPhase fsp : fetchSubPhases) { FetchSubPhaseProcessor processor = fsp.getProcessor(context); if (processor != null) { String type = fsp.getClass().getSimpleName().replaceAll("^Fetch", "").replaceAll("(FetchSub)?Phase$", ""); processors.add(profiler.profile(type, "", processor)); } } return processors; } catch (Exception e) { throw new FetchPhaseExecutionException(target, "Error building fetch sub-phases", e); } }	personally i think it's nice to simply include the class name without replacements. it's still very readable and makes it easier to find the corresponding code for the phase.
private MultiSearchResponse fetchMatchingDocCountResponses(Correction[] corrections, CompiledScript collateScript, boolean isFilter, PhraseSuggestionContext suggestions, BytesRef byteSpare, CharsRef spare) throws IOException { Map<String, Object> vars = suggestions.getCollateScriptParams(); MultiSearchResponse multiSearchResponse = null; MultiSearchRequestBuilder multiSearchRequestBuilder = client.prepareMultiSearch(); boolean requestAdded = false; for (Correction correction : corrections) { UnicodeUtil.UTF8toUTF16(correction.join(SEPARATOR, byteSpare, null, null), spare); vars.put(SUGGESTION_TEMPLATE_VAR_NAME, spare.toString()); ExecutableScript executable = scriptService.executable(collateScript, vars); BytesReference querySource = (BytesReference) executable.run(); requestAdded = true; SearchRequestBuilder req; if (isFilter) { req = client.prepareSearch() .setPreference(suggestions.getPreference()) .setQuery(QueryBuilders.constantScoreQuery(FilterBuilders.bytesFilter(querySource))) .setSearchType(SearchType.COUNT); } else { req = client.prepareSearch() .setPreference(suggestions.getPreference()) .setQuery(querySource) .setSearchType(SearchType.COUNT); } multiSearchRequestBuilder.add(req); } if (requestAdded) { multiSearchResponse = multiSearchRequestBuilder.get(); } return multiSearchResponse; }	maybe init the req only once with preference and search type and then set the query / filter in the if / else block?
public final ShardSearchTransportRequest buildShardSearchRequest(SearchShardIterator shardIt) { String clusterAlias = shardIt.getClusterAlias(); AliasFilter filter = aliasFilter.get(shardIt.shardId().getIndex().getUUID()); assert filter != null; float indexBoost = concreteIndexBoosts.getOrDefault(shardIt.shardId().getIndex().getUUID(), DEFAULT_INDEX_BOOST); int[] indexShards = getIndexShards(shardIt.shardId().getIndex()); int remapShardId = Arrays.binarySearch(indexShards, shardIt.shardId().getId()); assert remapShardId >= 0; return new ShardSearchTransportRequest(shardIt.getOriginalIndices(), request, shardIt.shardId(), remapShardId, indexShards.length, getNumShards(), filter, indexBoost, timeProvider.getAbsoluteStartMillis(), clusterAlias); }	this is quite a complex operation since we call if for every shard and then do consume the entire iterator again. i wonder if we can pre-sort the shardroutings in searchsharditerator and then calculate this on the fly and simply call searchsharditerator#getindexshardordinal() to get it?
@Override protected void doRun() throws Exception { beforeSendLatches.get(subRequest).await(); if (client.getLocalNodeId().equals(subRequest.node.getId()) && randomBoolean()) { try { client.executeLocally(TransportTestAction.ACTION, subRequest, latchedListener); } catch (TaskCancelledException e) { latchedListener.onFailure(new TransportException(e)); } } else { final Transport.Connection connection; if (subRequest.clusterAlias == null || subRequest.clusterAlias.equals(LOCAL_CLUSTER)) { connection = transportService.getConnection(subRequest.node); } else { connection = transportService.getRemoteClusterService().getConnection(subRequest.node, subRequest.clusterAlias); assertThat(connection.clusterAlias(), equalTo(subRequest.clusterAlias)); } transportService.sendRequest(connection, ACTION.name(), subRequest, TransportRequestOptions.EMPTY, new TransportResponseHandler<TestResponse>() { @Override public void handleResponse(TestResponse response) { latchedListener.onResponse(response); } @Override public void handleException(TransportException exp) { latchedListener.onFailure(exp); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public TestResponse read(StreamInput in) throws IOException { return new TestResponse(in); } }); } } }); } }	are we connected to all the nodes here? better we're not so we can test the proxy functionality.
void sendHeartbeatRequest(NodeAndClusterAlias node) { final Transport.Connection connection; try { connection = getConnection(node); } catch (TransportException ignored) { return; } if (connection.getVersion().onOrAfter(Version.V_8_0_0) && connection.getNode().getVersion().onOrAfter(Version.V_8_0_0)) { final HeartbeatRequest request = new HeartbeatRequest(localNodeId()); transportService.sendRequest(connection, HEARTBEAT_ACTION_NAME, request, TransportRequestOptions.EMPTY, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) { @Override public void handleException(TransportException exp) { assert ExceptionsHelper.unwrapCause(exp) instanceof ElasticsearchSecurityException == false; logger.debug("failed to send a task heartbeat to node {}", node); } }); } }	i wonder if node id is sufficient here, or whether we need to mention concrete tasks here. assume that a remote cluster is disconnected during a request, and then reconnects pretty quickly with parent tasks on source cluster going away (due to failure of child requests). in that case, we keep the child task alive on the remote cluster (as long as new requests are being processed), even though there's no point in doing so. another option here (instead of sending parent task ids to renew) might be to use some numbering scheme (periodically incrementing numbers which correspond to named leases that represent an abstract set of long-running requests)
public void testNonFractionalTimeValues() { final String s = randomAsciiOfLength(10) + randomTimeUnit(); final ElasticsearchParseException e = expectThrows(ElasticsearchParseException.class, () -> TimeValue.parseTimeValue(s, null, "test")); assertThat(e, hasToString(containsString("failed to parse [" + s + "]"))); assertThat(e, not(hasToString(containsString(FRACTIONAL_TIME_VALUES_ARE_NOT_SUPPORTED)))); assertThat(e.getCause(), instanceOf(NumberFormatException.class)); }	i don't think you need to divide at all - randomdouble is between 0 and 1, i believe.
public static ObjectParser<GeoGridAggregationBuilder, Void> createParser(String name, PrecisionParser precisionParser) { ObjectParser<GeoGridAggregationBuilder, Void> parser = new ObjectParser<>(name); ValuesSourceAggregationBuilder.declareFields(parser, false, false, false); parser.declareField((p, builder, context) -> builder.precision(precisionParser.parse(p)), FIELD_PRECISION, org.elasticsearch.common.xcontent.ObjectParser.ValueType.INT); parser.declareInt(GeoGridAggregationBuilder::size, FIELD_SIZE); parser.declareInt(GeoGridAggregationBuilder::shardSize, FIELD_SHARD_SIZE); parser.declareField((p, builder, context) -> { builder.setGeoBoundingBox(GeoBoundingBox.parseBoundingBox(p)); }, GeoBoundingBox.BOUNDS_FIELD, org.elasticsearch.common.xcontent.ObjectParser.ValueType.OBJECT); return parser; }	probably a thing for later more than now: three boolean parameters can be kind of hard to keep track of. i think intellij has something where it puts the names of parameters but that isn't universal. anyway, *something* else might be a little more descriptive. i'm not sure what though. it certainly can wait for later too.
* @return the object read from the stream. * */ @Override public SimpleQueryStringBuilder readFrom(StreamInput in) throws IOException { String text = in.readOptionalString(); float boost = in.readFloat(); int size = in.readInt(); Map<String, Float> fields = new HashMap<>(); for (int i = 0; i < size; i++) { String field = in.readString(); Float weight = in.readFloat(); fields.put(field, weight); } int flags = in.readInt(); String analyzer = in.readOptionalString(); Operator operator = null; Integer operatorId = in.readOptionalVInt(); if (operatorId != null) { switch (operatorId) { case 1: operator = Operator.OR; break; case 2: operator = Operator.AND; break; } } Boolean lowercaseExpandedTerms = in.readOptionalBoolean(); Boolean lenient = in.readOptionalBoolean(); Boolean analyzeWildcards = in.readOptionalBoolean(); Locale locale = null; if (in.readBoolean()) { String localeStr = in.readString(); locale = Locale.forLanguageTag(localeStr); } String queryName = (in.readOptionalString()); String minimumShouldMatch = in.readOptionalString(); return new SimpleQueryStringBuilder(text, boost, fields, analyzer, queryName, operator, minimumShouldMatch, flags, locale, lowercaseExpandedTerms, lenient, analyzeWildcards); } /** * Writes the current {@link SimpleQueryStringBuilder}	we can simply make the id part of the operator enum, and add a parsefromint method to it too that gets called from readfrom. writeto just does out.writevint(operator.id())
@Override protected void masterOperation(final RolloverRequest rolloverRequest, final ClusterState state, final ActionListener<RolloverResponse> listener) { final MetaData metaData = state.metaData(); validate(metaData, rolloverRequest); final AliasOrIndex aliasOrIndex = metaData.getAliasAndIndexLookup().get(rolloverRequest.getAlias()); final IndexMetaData indexMetaData = aliasOrIndex.getIndices().get(0); final String sourceProvidedName = indexMetaData.getSettings().get(IndexMetaData.SETTING_INDEX_PROVIDED_NAME, indexMetaData.getIndex().getName()); final String sourceIndexName = indexMetaData.getIndex().getName(); final String unresolvedName = (rolloverRequest.getNewIndexName() != null) ? rolloverRequest.getNewIndexName() : generateRolloverIndexName(sourceProvidedName, indexNameExpressionResolver); final String rolloverIndexName = indexNameExpressionResolver.resolveDateMathExpression(unresolvedName); MetaDataCreateIndexService.validateIndexName(rolloverIndexName, state); // will fail if the index already exists client.admin().indices().prepareStats(sourceIndexName).clear().setDocs(true).execute( new ActionListener<IndicesStatsResponse>() { @Override public void onResponse(IndicesStatsResponse statsResponse) { final Set<Condition.Result> conditionResults = evaluateConditions(rolloverRequest.getConditions(), statsResponse.getTotal().getDocs(), metaData.index(sourceIndexName)); if (rolloverRequest.isDryRun()) { listener.onResponse( new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, true, false, false, false)); return; } if (conditionResults.size() == 0 || conditionResults.stream().anyMatch(result -> result.matched)) { CreateIndexClusterStateUpdateRequest updateRequest = prepareCreateIndexRequest(unresolvedName, rolloverIndexName, rolloverRequest); createIndexService.createIndex(updateRequest, ActionListener.wrap(createIndexClusterStateUpdateResponse -> { // switch the alias to point to the newly created index indexAliasesService.indicesAliases( prepareRolloverAliasesUpdateRequest(sourceIndexName, rolloverIndexName, rolloverRequest), ActionListener.wrap(aliasClusterStateUpdateResponse -> { if (aliasClusterStateUpdateResponse.isAcknowledged()) { activeShardsObserver.waitForActiveShards(rolloverIndexName, rolloverRequest.getCreateIndexRequest().waitForActiveShards(), rolloverRequest.masterNodeTimeout(), isShardsAcked -> listener.onResponse(new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, false, true, true, isShardsAcked)), listener::onFailure); } else { listener.onResponse(new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, false, true, false, false)); } }, listener::onFailure)); }, listener::onFailure)); } else { // conditions not met listener.onResponse( new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, false, false, false, false) ); } } @Override public void onFailure(Exception e) { listener.onFailure(e); } } ); }	this also validates the index name. is it worth adding a small test to verify that we don't accept index names that are not valid too?
public void testEnrollmentOverwritesPreviousAutoconfigurationWhenForced() throws Exception { final List<String> vanillaFileLines = Files.readAllLines(getResourcePath("elasticsearch.yml"), StandardCharsets.UTF_8); assertEquals(vanillaFileLines, removePreviousAutoconfiguration(getResourcePath("elasticsearch.yml"))); assertEquals(vanillaFileLines, removePreviousAutoconfiguration(getResourcePath("elasticsearch_with_autoconfig.yml"))); }	the two files in question don't have to be the "real" es config right? since they are copies they could quickly get out of date. it seems like this test is really (or should be ) a unit test of removepreviousautoconfiguration, which could use synthetic+dynamic data to assert the behavior.
@Override public Map<String, Processor.Factory> getProcessors(Processor.Parameters parameters) { return Map.of( "default", (factories, tag, description, config) -> new AbstractProcessor(tag, description) { @Override public void execute(IngestDocument ingestDocument, BiConsumer<IngestDocument, Exception> handler) { randomFrom(parameters.genericExecutor, Runnable::run).accept(() -> { ingestDocument.setFieldValue("default", true); handler.accept(ingestDocument, null); }); } @Override public IngestDocument execute(IngestDocument ingestDocument) { throw new UnsupportedOperationException(); } @Override public String getType() { return "default"; } }, "final", (processorFactories, tag, description, config) -> { final String exists = (String) config.remove("exists"); return new AbstractProcessor(tag, description) { @Override public IngestDocument execute(final IngestDocument ingestDocument) throws Exception { // this asserts that this pipeline is the final pipeline executed if (exists != null) { if (ingestDocument.getSourceAndMetadata().containsKey(exists) == false) { throw new AssertionError( "expected document to contain [" + exists + "] but was [" + ingestDocument.getSourceAndMetadata()); } } ingestDocument.setFieldValue("final", true); return ingestDocument; } @Override public String getType() { return "final"; } }; }, "request", (processorFactories, tag, description, config) -> new AbstractProcessor(tag, description) { @Override public IngestDocument execute(final IngestDocument ingestDocument) throws Exception { ingestDocument.setFieldValue("request", true); return ingestDocument; } @Override public String getType() { return "request"; } }, "changing_dest", (processorFactories, tag, description, config) -> new AbstractProcessor(tag, description) { @Override public IngestDocument execute(final IngestDocument ingestDocument) throws Exception { ingestDocument.setFieldValue(IngestDocument.Metadata.INDEX.getFieldName(), "target"); return ingestDocument; } @Override public String getType() { return "changing_dest"; } } ); }	maybe this? suggestion throw new assertionerror("should not be called");
private boolean tryLockingPolicy(String policyName) { Semaphore runLock = policyLocks.computeIfAbsent(policyName, (name) -> new Semaphore(1)); return runLock.tryAcquire(); }	i think we should fail with a descriptive error if the lock has been taken?
@Override public void onFailure(Exception e) { releasePolicy(policyName); listener.onFailure(e); } } protected Runnable createPolicyRunner(String policyName, EnrichPolicy policy, ActionListener<PolicyExecutionResult> listener) { return new EnrichPolicyRunner(policyName, policy, listener, clusterService, client, indexNameExpressionResolver, nowSupplier, fetchSize); } public void runPolicy(String policyId, ActionListener<PolicyExecutionResult> listener) { // Look up policy in policy store and execute it EnrichPolicy policy = EnrichStore.getPolicy(policyId, clusterService.state()); if (policy == null) { throw new ElasticsearchException("Policy execution failed. Could not locate policy with id [{}]", policyId); } else { runPolicy(policyId, policy, listener); } } public void runPolicy(String policyName, EnrichPolicy policy, ActionListener<PolicyExecutionResult> listener) { if (tryLockingPolicy(policyName)) { try { Runnable runnable = createPolicyRunner(policyName, policy, new PolicyUnlockingListener(policyName, listener)); threadPool.executor(ThreadPool.Names.GENERIC).execute(runnable); } catch (Exception e) { // Be sure to unlock if submission failed. releasePolicy(policyName); throw e; } } else { throw new ElasticsearchException("Policy execution failed. Policy execution for [{}] is already in progress.", policyName); }	this might be tangential to this review, but should we also cap to total number of concurrent policies running ? by default the generic threadpool will scale up to 512 !
public void runPolicy(String policyName, EnrichPolicy policy, ActionListener<PolicyExecutionResult> listener) { if (tryLockingPolicy(policyName)) { try { Runnable runnable = createPolicyRunner(policyName, policy, new PolicyUnlockingListener(policyName, listener)); threadPool.executor(ThreadPool.Names.GENERIC).execute(runnable); } catch (Exception e) { // Be sure to unlock if submission failed. releasePolicy(policyName); throw e; } } else { throw new ElasticsearchException("Policy execution failed. Policy execution for [{}] is already in progress.", policyName); } }	what does this map to in the http response ? (should this be 429 ?)
protected LogicalPlan rule(LogicalPlan plan) { return plan.transformExpressionsUp(e -> { if (e instanceof UnresolvedFunction) { UnresolvedFunction uf = (UnresolvedFunction) e; if (uf.analyzed()) { return uf; } String name = uf.name(); if (uf.childrenResolved() == false) { return uf; } String functionName = functionRegistry.resolveAlias(name); if (functionRegistry.functionExists(functionName) == false) { return uf.missing(functionName, functionRegistry.listFunctions()); } FunctionDefinition def = functionRegistry.resolveFunction(functionName); Function f = uf.buildResolved(null, def); return f; } return e; }); } } private class MatchLiteralsOnTheRight extends AnalyzerRule<LogicalPlan> { @Override protected LogicalPlan rule(LogicalPlan plan) { return plan.transformExpressionsUp(e -> { if (e instanceof Match) { Match m = (Match) e; int size = m.children().size(); if ((m.children().get(size - 1) instanceof Literal) == false) { List<Expression> newChildren = new ArrayList<>(m.children().size()); newChildren.add(m.children().get(size - 1)); newChildren.addAll(m.children().subList(0, size - 1)); return m.replaceChildren(newChildren); } } return e; }); }	this should be an optimization rule not analysis (since the literal might not be folded). it also seems incomplete - what if there are multiple non-literals? is there any validation happening (verifier rule) in the chain?
* @param version repository metadata version used when writing the data to the repository */ private void cacheRepositoryData(RepositoryData repositoryData, Version version) { if (cacheRepositoryData == false) { return; } final RepositoryData toCache; if (version.onOrBefore(SnapshotsService.OLD_SNAPSHOT_FORMAT)) { // don't cache shard generations here as they may be unreliable toCache = repositoryData.withoutShardGenerations(); } else { toCache = repositoryData; } assert toCache.getGenId() >= 0 : "No need to cache abstract generations but attempted to cache [" + toCache.getGenId() + "]"; latestKnownRepositoryData.updateAndGet(known -> { if (known.getGenId() > toCache.getGenId()) { return known; } return toCache; }); }	why do we drop shard gens if <= old_snapshot_format rather than if < shard_gen_in_repo_data_version? seems like this is a change in behaviour if the min node version is v7.5.1 or v7.5.2 (and all snapshots have known versions). also just thinking about the other stuff that would have been dropped but isn't any more? i guess the cluster/repo uuids are ok to cache but what about index gens? can we at least assert that this is already indexmetadatagenerations.empty?
protected void extract(Query query, float boost, Map<String, WeightedSpanTerm> terms) throws IOException { // skip all geo queries, see https://issues.apache.org/jira/browse/LUCENE-7293 and // https://github.com/elastic/elasticsearch/issues/17537 if (query instanceof GeoPointInBBoxQuery == false) { super.extract(query, boost, terms); } }	do we also expose geopointdistancequery or geopointinpolygonquery?
public static boolean canOpenIndex(Path indexLocation) throws IOException { try (Directory dir = new SimpleFSDirectory(indexLocation)) { failIfCorrupted(dir, new ShardId("", 1)); Lucene.readSegmentInfos(dir); return true; } catch (Exception ex) { return false; } }	cat we add a trace log here with the exception? thinking it might be helpful to be able to know why...
Collection<Object> createComponents(Client client, ThreadPool threadPool, ClusterService clusterService, ResourceWatcherService resourceWatcherService) throws Exception { if (enabled == false) { return Collections.emptyList(); } threadContext.set(threadPool.getThreadContext()); List<Object> components = new ArrayList<>(); securityContext.set(new SecurityContext(settings, threadPool.getThreadContext())); components.add(securityContext.get()); // audit trails construction Set<AuditTrail> auditTrails = new LinkedHashSet<>(); if (XPackSettings.AUDIT_ENABLED.get(settings)) { List<String> outputs = AUDIT_OUTPUTS_SETTING.get(settings); if (outputs.isEmpty()) { throw new IllegalArgumentException("Audit logging is enabled but there are zero output types in " + XPackSettings.AUDIT_ENABLED.getKey()); } for (String output : outputs) { switch (output) { case LoggingAuditTrail.NAME: auditTrails.add(new LoggingAuditTrail(settings, clusterService, threadPool)); // also enabling the deprecated format. To disable it, remove it's associated // appender in the log4j2.properties file auditTrails.add(new DeprecatedLoggingAuditTrail(settings, clusterService, threadPool)); break; case IndexAuditTrail.NAME: indexAuditTrail.set(new IndexAuditTrail(settings, client, threadPool, clusterService)); auditTrails.add(indexAuditTrail.get()); break; default: throw new IllegalArgumentException("Unknown audit trail output [" + output + "]"); } } } final AuditTrailService auditTrailService = new AuditTrailService(settings, new ArrayList<>(auditTrails), getLicenseState()); components.add(auditTrailService); this.auditTrailService.set(auditTrailService); securityIndex.set(new SecurityIndexManager(settings, client, SecurityIndexManager.SECURITY_INDEX_NAME, clusterService)); final TokenService tokenService = new TokenService(settings, Clock.systemUTC(), client, securityIndex.get(), clusterService); this.tokenService.set(tokenService); components.add(tokenService); // realms construction final NativeUsersStore nativeUsersStore = new NativeUsersStore(settings, client, securityIndex.get()); final NativeRoleMappingStore nativeRoleMappingStore = new NativeRoleMappingStore(settings, client, securityIndex.get()); final AnonymousUser anonymousUser = new AnonymousUser(settings); final ReservedRealm reservedRealm = new ReservedRealm(env, settings, nativeUsersStore, anonymousUser, securityIndex.get(), threadPool); Map<String, Realm.Factory> realmFactories = new HashMap<>(InternalRealms.getFactories(threadPool, resourceWatcherService, getSslService(), nativeUsersStore, nativeRoleMappingStore, securityIndex.get())); for (SecurityExtension extension : securityExtensions) { Map<String, Realm.Factory> newRealms = extension.getRealms(resourceWatcherService); for (Map.Entry<String, Realm.Factory> entry : newRealms.entrySet()) { if (realmFactories.put(entry.getKey(), entry.getValue()) != null) { throw new IllegalArgumentException("Realm type [" + entry.getKey() + "] is already registered"); } } } final Realms realms = new Realms(settings, env, realmFactories, getLicenseState(), threadPool.getThreadContext(), reservedRealm); components.add(nativeUsersStore); components.add(nativeRoleMappingStore); components.add(realms); components.add(reservedRealm); securityIndex.get().addIndexStateListener(nativeRoleMappingStore::onSecurityIndexStateChange); final AuthenticationFailureHandler failureHandler = createAuthenticationFailureHandler(realms); authcService.set(new AuthenticationService(settings, realms, auditTrailService, failureHandler, threadPool, anonymousUser, tokenService)); components.add(authcService.get()); final NativePrivilegeStore privilegeStore = new NativePrivilegeStore(settings, client, securityIndex.get()); components.add(privilegeStore); final FileRolesStore fileRolesStore = new FileRolesStore(settings, env, resourceWatcherService, getLicenseState()); final NativeRolesStore nativeRolesStore = new NativeRolesStore(settings, client, getLicenseState(), securityIndex.get()); final ReservedRolesStore reservedRolesStore = new ReservedRolesStore(); List<BiConsumer<Set<String>, ActionListener<Set<RoleDescriptor>>>> rolesProviders = new ArrayList<>(); for (SecurityExtension extension : securityExtensions) { rolesProviders.addAll(extension.getRolesProviders(settings, resourceWatcherService)); } final CompositeRolesStore allRolesStore = new CompositeRolesStore(settings, fileRolesStore, nativeRolesStore, reservedRolesStore, privilegeStore, rolesProviders, threadPool.getThreadContext(), getLicenseState()); securityIndex.get().addIndexStateListener(allRolesStore::onSecurityIndexStateChange); // to keep things simple, just invalidate all cached entries on license change. this happens so rarely that the impact should be // minimal getLicenseState().addListener(allRolesStore::invalidateAll); final AuthorizationService authzService = new AuthorizationService(settings, allRolesStore, clusterService, auditTrailService, failureHandler, threadPool, anonymousUser); components.add(nativeRolesStore); // used by roles actions components.add(reservedRolesStore); // used by roles actions components.add(allRolesStore); // for SecurityFeatureSet and clear roles cache components.add(authzService); ipFilter.set(new IPFilter(settings, auditTrailService, clusterService.getClusterSettings(), getLicenseState())); components.add(ipFilter.get()); DestructiveOperations destructiveOperations = new DestructiveOperations(settings, clusterService.getClusterSettings()); securityInterceptor.set(new SecurityServerTransportInterceptor(settings, threadPool, authcService.get(), authzService, getLicenseState(), getSslService(), securityContext.get(), destructiveOperations, clusterService)); final Set<RequestInterceptor> requestInterceptors; if (XPackSettings.DLS_FLS_ENABLED.get(settings)) { requestInterceptors = Collections.unmodifiableSet(Sets.newHashSet( new SearchRequestInterceptor(settings, threadPool, getLicenseState()), new UpdateRequestInterceptor(settings, threadPool, getLicenseState()), new BulkShardRequestInterceptor(settings, threadPool, getLicenseState()), new ResizeRequestInterceptor(settings, threadPool, getLicenseState(), auditTrailService), new IndicesAliasesRequestInterceptor(threadPool.getThreadContext(), getLicenseState(), auditTrailService))); } else { requestInterceptors = Collections.emptySet(); } securityActionFilter.set(new SecurityActionFilter(settings, authcService.get(), authzService, getLicenseState(), requestInterceptors, threadPool, securityContext.get(), destructiveOperations)); clusterService.getClusterSettings().addSettingsUpdateConsumer(INDICES_ADMIN_FILTERED_FIELDS_SETTING, this::setIndicesAdminFilteredFields); return components; }	we should have documentation about how to disable this. i am fine if you want to handle this separately, but please open up a issue if that is what you choose to do
public ClusterState deleteIndices(ClusterState currentState, Set<Index> indices) { final Metadata meta = currentState.metadata(); final Set<Index> indicesToDelete = new HashSet<>(); final Map<Index, DataStream> backingIndices = new HashMap<>(); for (Index index : indices) { IndexMetadata im = meta.getIndexSafe(index); IndexAbstraction.DataStream parent = meta.getIndicesLookup().get(im.getIndex().getName()).getParentDataStream(); if (parent != null) { if (parent.getWriteIndex().equals(im)) { throw new IllegalArgumentException("index [" + index.getName() + "] is the write index for data stream [" + parent.getName() + "] and cannot be deleted"); } else { backingIndices.put(index, parent.getDataStream()); } } indicesToDelete.add(im.getIndex()); } // Check if index deletion conflicts with any running snapshots Set<Index> snapshottingIndices = SnapshotsService.snapshottingIndices(currentState, indicesToDelete); if (snapshottingIndices.isEmpty() == false) { throw new SnapshotInProgressException("Cannot delete indices that are being snapshotted: " + snapshottingIndices + ". Try again after snapshot finishes or cancel the currently running snapshot."); } RoutingTable.Builder routingTableBuilder = RoutingTable.builder(currentState.routingTable()); Metadata.Builder metadataBuilder = Metadata.builder(meta); ClusterBlocks.Builder clusterBlocksBuilder = ClusterBlocks.builder().blocks(currentState.blocks()); final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metadataBuilder.indexGraveyard()); final int previousGraveyardSize = graveyardBuilder.tombstones().size(); for (final Index index : indices) { String indexName = index.getName(); logger.info("{} deleting index", index); routingTableBuilder.remove(indexName); clusterBlocksBuilder.removeIndexBlocks(indexName); metadataBuilder.remove(indexName); if (backingIndices.containsKey(index)) { DataStream parent = backingIndices.get(index); metadataBuilder.put(parent.removeBackingIndex(index)); } } // add tombstones to the cluster state for each deleted index final IndexGraveyard currentGraveyard = graveyardBuilder.addTombstones(indices).build(settings); metadataBuilder.indexGraveyard(currentGraveyard); // the new graveyard set on the metadata logger.trace("{} tombstones purged from the cluster state. Previous tombstone size: {}. Current tombstone size: {}.", graveyardBuilder.getNumPurged(), previousGraveyardSize, currentGraveyard.getTombstones().size()); Metadata newMetadata = metadataBuilder.build(); ClusterBlocks blocks = clusterBlocksBuilder.build(); // update snapshot restore entries ImmutableOpenMap<String, ClusterState.Custom> customs = currentState.getCustoms(); final RestoreInProgress restoreInProgress = currentState.custom(RestoreInProgress.TYPE); if (restoreInProgress != null) { RestoreInProgress updatedRestoreInProgress = RestoreService.updateRestoreStateWithDeletedIndices(restoreInProgress, indices); if (updatedRestoreInProgress != restoreInProgress) { ImmutableOpenMap.Builder<String, ClusterState.Custom> builder = ImmutableOpenMap.builder(customs); builder.put(RestoreInProgress.TYPE, updatedRestoreInProgress); customs = builder.build(); } } return allocationService.reroute( ClusterState.builder(currentState) .routingTable(routingTableBuilder.build()) .metadata(newMetadata) .blocks(blocks) .customs(customs) .build(), "deleted indices [" + indices + "]"); }	nit: { on the previous line?
public void testRemoveBackingIndex() { int numBackingIndices = randomIntBetween(2, 32); int indexToRemove = randomIntBetween(1, numBackingIndices - 1); String dataStreamName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); List<Index> indices = new ArrayList<>(numBackingIndices); for (int k = 1; k <= numBackingIndices; k++) { indices.add(new Index(DataStream.getBackingIndexName(dataStreamName, k), UUIDs.randomBase64UUID(random()))); } DataStream original = new DataStream(dataStreamName, "@timestamp", indices); DataStream updated = original.removeBackingIndex(indices.get(indexToRemove - 1)); assertThat(updated.getName(), equalTo(original.getName())); assertThat(updated.getGeneration(), equalTo(original.getGeneration())); assertThat(updated.getTimeStampField(), equalTo(original.getTimeStampField())); for (int k = 0; k < (numBackingIndices - 1); k++) { assertThat(updated.getIndices().get(k), equalTo(original.getIndices().get(k < (indexToRemove - 1) ? k : k + 1))); } }	maybe also assert that the index that is supposed to be removed really is removed? i think we now just check that all other indices but the removed index exists. i think that for that, just checking that the number of backing indices is decremented by 1 should suffice.
public void testDeleteBackingIndexForDataStream() { int numBackingIndices = randomIntBetween(2, 5); String dataStreamName = randomAlphaOfLength(6).toLowerCase(Locale.ROOT); ClusterState before = DeleteDataStreamRequestTests.getClusterStateWithDataStreams( List.of(new Tuple<>(dataStreamName, numBackingIndices)), List.of()); int numIndexToDelete = randomIntBetween(1, numBackingIndices - 1); Index indexToDelete = before.metadata().index(DataStream.getBackingIndexName(dataStreamName, numIndexToDelete)).getIndex(); ClusterState after = service.deleteIndices(before, Set.of(indexToDelete)); assertNull(after.metadata().getIndices().get(indexToDelete.getName())); for (int k = 1; k <= numBackingIndices; k++) { if (k != numIndexToDelete) { assertNotNull(after.metadata().getIndices().get(DataStream.getBackingIndexName(dataStreamName, k))); } } }	nit: use assertthat(...) with isnull() as matcher instead? i think in general that is the preferred way of writing test assertions.
public void testDeleteBackingIndexForDataStream() { int numBackingIndices = randomIntBetween(2, 5); String dataStreamName = randomAlphaOfLength(6).toLowerCase(Locale.ROOT); ClusterState before = DeleteDataStreamRequestTests.getClusterStateWithDataStreams( List.of(new Tuple<>(dataStreamName, numBackingIndices)), List.of()); int numIndexToDelete = randomIntBetween(1, numBackingIndices - 1); Index indexToDelete = before.metadata().index(DataStream.getBackingIndexName(dataStreamName, numIndexToDelete)).getIndex(); ClusterState after = service.deleteIndices(before, Set.of(indexToDelete)); assertNull(after.metadata().getIndices().get(indexToDelete.getName())); for (int k = 1; k <= numBackingIndices; k++) { if (k != numIndexToDelete) { assertNotNull(after.metadata().getIndices().get(DataStream.getBackingIndexName(dataStreamName, k))); } } }	i think that instead of looping here just checking that the index that is supposed to be removed is removed and checking that the number of indices is decremented by one is sufficient.
public void testPublishAnyLocalV4() throws Exception { InetAddress expected[] = null; try { expected = NetworkUtils.getFirstNonLoopbackAddresses(); } catch (Exception e) { assumeNoException("test requires up-and-running non-loopback address", e); } NetworkUtils.sortAddresses(Arrays.asList(expected)); NetworkService service = new NetworkService(Settings.EMPTY); assertEquals(expected[0], service.resolvePublishHostAddresses(new String[] { "0.0.0.0" })); }	can you use assertequals against the whole array?
public List<Phase> getOrderedPhases(Map<String, Phase> phases) { List<Phase> orderedPhases = new ArrayList<>(VALID_PHASES.size()); for (String phaseName : VALID_PHASES) { Phase phase = phases.get(phaseName); if (phase != null) { Map<String, LifecycleAction> actions = phase.getActions(); if (actions.containsKey(UnfollowAction.NAME) == false && (actions.containsKey(RolloverAction.NAME) || actions.containsKey(ShrinkAction.NAME))) { Map<String, LifecycleAction> actionMap = new HashMap<>(phase.getActions()); actionMap.put(UnfollowAction.NAME, new UnfollowAction()); phase = new Phase(phase.getName(), phase.getMinimumAge(), actionMap); } orderedPhases.add(phase); } } return orderedPhases; }	can you add a unit test for this? hopefully you should be able to instantiate a timeserieslifecycletype and directly call this since it's public.
* @param snapshotIds snapshots for which to fetch snapshot information * @param ignoreUnavailable if true, snapshots that could not be read will only be logged with a warning, * if false, they will throw an error */ private void snapshots(SnapshotsInProgress snapshotsInProgress, String repositoryName, Collection<SnapshotId> snapshotIds, boolean ignoreUnavailable, CancellableTask task, ActionListener<List<SnapshotInfo>> listener) { if (task.isCancelled()) { listener.onFailure(new TaskCancelledException("task cancelled")); return; } final Set<SnapshotInfo> snapshotSet = new HashSet<>(); final Set<SnapshotId> snapshotIdsToIterate = new HashSet<>(snapshotIds); // first, look at the snapshots in progress final List<SnapshotsInProgress.Entry> entries = SnapshotsService.currentSnapshots( snapshotsInProgress, repositoryName, snapshotIdsToIterate.stream().map(SnapshotId::getName).collect(Collectors.toList())); for (SnapshotsInProgress.Entry entry : entries) { if (snapshotIdsToIterate.remove(entry.snapshot().getSnapshotId())) { snapshotSet.add(new SnapshotInfo(entry)); } } // then, look in the repository if there's any matching snapshots left final List<SnapshotInfo> snapshotInfos; if (snapshotIdsToIterate.isEmpty()) { snapshotInfos = Collections.emptyList(); } else { snapshotInfos = Collections.synchronizedList(new ArrayList<>()); } final ActionListener<Collection<Void>> allDoneListener = listener.delegateFailure((l, v) -> { final ArrayList<SnapshotInfo> snapshotList = new ArrayList<>(snapshotInfos); snapshotList.addAll(snapshotSet); CollectionUtil.timSort(snapshotList); listener.onResponse(unmodifiableList(snapshotList)); }); if (snapshotIdsToIterate.isEmpty()) { allDoneListener.onResponse(Collections.emptyList()); return; } // put snapshot info downloads into a task queue instead of pushing them all into the queue to not completely monopolize the // snapshot meta pool for a single request final int workers = Math.min(threadPool.info(ThreadPool.Names.SNAPSHOT_META).getMax(), snapshotIdsToIterate.size()); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT_META); final BlockingQueue<SnapshotId> queue = new LinkedBlockingQueue<>(snapshotIdsToIterate); final ActionListener<Void> workerDoneListener = new GroupedActionListener<>(allDoneListener, workers).delegateResponse((l, e) -> { queue.clear(); // Stop fetching the remaining snapshots once we've failed fetching one since the response is an error response // anyway in this case l.onFailure(e); }); final Repository repository = repositoriesService.repository(repositoryName); for (int i = 0; i < workers; i++) { getOneSnapshotInfo( ignoreUnavailable, repository, queue, snapshotInfos, task, executor, workerDoneListener ); } } /** * Tries to poll a {@link SnapshotId} to load {@link SnapshotInfo} for from the given {@code queue}. If it finds one in the queue, * loads the snapshot info from the repository on the given {@code executor} and adds it to the given {@code snapshotInfos} collect, * then invokes itself again to try and poll another task from the queue. * If the queue is empty resolves {@code}	this is always the same executor, just wondering why not get it from the threadpool each time rather than passing it in.
synchronized void stop(boolean tailLogs) { if (esProcess == null && tailLogs) { // This is a special case. If start() throws an exception the plugin will still call stop // Another exception here would eat the orriginal. return; } logger.info("Stopping `{}`, tailLogs: {}", this, tailLogs); requireNonNull(esProcess, "Can't stop `" + this + "` as it was not started or already stopped."); // Test clusters are not reused, don't spend time on a graceful shutdown stopHandle(esProcess.toHandle(), true); if (tailLogs) { logFileContents("Standard output of node", esStdoutFile); logFileContents("Standard error of node", esStderrFile); } esProcess = null; }	typo, should be "gracefully". not say we are not grateful
synchronized void stop(boolean tailLogs) { if (esProcess == null && tailLogs) { // This is a special case. If start() throws an exception the plugin will still call stop // Another exception here would eat the orriginal. return; } logger.info("Stopping `{}`, tailLogs: {}", this, tailLogs); requireNonNull(esProcess, "Can't stop `" + this + "` as it was not started or already stopped."); // Test clusters are not reused, don't spend time on a graceful shutdown stopHandle(esProcess.toHandle(), true); if (tailLogs) { logFileContents("Standard output of node", esStdoutFile); logFileContents("Standard error of node", esStderrFile); } esProcess = null; }	we can invert this, so: if (processhandle.isalive() == false) { return } logger.info("process did not terminate after {} {}, stopping it forcefully", es_destroy_timeout, es_destroy_timeout_unit); processhandle.destroyforcibly();
public synchronized long time() { if (startTime == 0) { return 0; } if (time >= 0) { return time; } long t = TimeValue.nsecToMSec(System.nanoTime()); return Math.max(0, t - startTime); }	why the assignment to t here? all of the other replacements didn't use any temporary variables (just curious)
*/ synchronized public PluginsInfo info() { if (refreshInterval.millis() != 0) { if (cachedPluginsInfo != null && (refreshInterval.millis() < 0 || TimeValue.nsecToMSec(System.nanoTime() - lastRefreshNS) < refreshInterval.millis())) { if (logger.isTraceEnabled()) { logger.trace("using cache to retrieve plugins info"); } return cachedPluginsInfo; } lastRefreshNS = System.nanoTime(); } if (logger.isTraceEnabled()) { logger.trace("starting to fetch info on plugins"); } cachedPluginsInfo = new PluginsInfo(); // We first add all JvmPlugins for (Tuple<PluginInfo, Plugin> plugin : this.plugins) { if (logger.isTraceEnabled()) { logger.trace("adding jvm plugin [{}]", plugin.v1()); } cachedPluginsInfo.add(plugin.v1()); } try { // We reload site plugins (in case of some changes) for (Tuple<PluginInfo, Plugin> plugin : loadSitePlugins()) { if (logger.isTraceEnabled()) { logger.trace("adding site plugin [{}]", plugin.v1()); } cachedPluginsInfo.add(plugin.v1()); } } catch (IOException ex) { logger.warn("can load site plugins info", ex); } return cachedPluginsInfo; }	why not (system.nanotime() - lastrefreshns) < refreshinterval.nanos()) to avoid the extra call to ns->ms conversion?
@Override public List<FieldCardinalityConstraint> getFieldCardinalityConstraints() { // This restriction is due to the fact that currently the C++ backend only supports binomial classification. return Collections.singletonList(FieldCardinalityConstraint.between(dependentVariable, 2, 30)); }	should 30 become a constant in this class?
private static void writeCheckpoint( final FileChannel fileChannel, final Path checkpointFile, final Checkpoint checkpoint) throws IOException { // Since we used a fixed size checkpoint, we overwrite the checkpoint each time. fileChannel.position(0); Checkpoint.write(fileChannel, checkpointFile, checkpoint); }	what's a bit subtle here is that we assume callers to not have any concurrent access to this method. also we assume that once an exception occurs, this method is no more called on the same filechannel. the surrounding context currently correctly implements this, but it is easy to break and can have disastrous effects. is there a way we can strengthen these assumptions via assertions? i was also wondering if we should use positional writes (see filechannel.write(bytebuffer src, long position), to ensure we truly always write starting with offset 0 in all cases, and move the positioning logic to the checkpoint.write method.
@Override public void clusterChanged(ClusterChangedEvent event) { if (enabled.get() == false) { return; } if (event.metaDataChanged() == false) { return; } PersistentTasksCustomMetaData previous = event.previousState().getMetaData().custom(PersistentTasksCustomMetaData.TYPE); PersistentTasksCustomMetaData current = event.state().getMetaData().custom(PersistentTasksCustomMetaData.TYPE); if (Objects.equals(previous, current)) { return; } Version minNodeVersion = event.state().nodes().getMinNodeVersion(); if (minNodeVersion.onOrAfter(Version.V_6_6_0)) { // ok to migrate mlConfigMigrator.migrateConfigsWithoutTasks(event.state(), ActionListener.wrap( response -> threadPool.executor(executorName()).execute(() -> auditChangesToMlTasks(current, previous, event.state())), e -> logger.error("error migrating ml configurations", e) )); } else { threadPool.executor(executorName()).execute(() -> auditChangesToMlTasks(current, previous, event.state())); } }	maybe a migration error should still proceed to audit changes to ml tasks?
public static SpatialStrategy fromString(String strategyName) { for (SpatialStrategy strategy : values()) { if (strategy.strategyName.equals(strategyName)) { return strategy; } } throw new IllegalArgumentException("Unknown strategy [" + strategyName + "]"); }	this ended up catching a pre-existing bug in geoshapequerytests, which was asking for term instead of term as a strategy, and therefore ending up falling back on null here, which was getting translated to recursive once the mapper had been parsed. silently falling back to recursive is a bug, i think, although on backport we will need to change this to emit a warning instead, as you can't change the strategy once it has been set and so existing mappings with badly-spelled term strategies will have to stay using recursive
protected static ShapeBuilder parse(XContentParser parser, AbstractShapeGeometryFieldMapper shapeMapper) throws IOException { GeoShapeType shapeType = null; DistanceUnit.Distance radius = null; CoordinateNode coordinateNode = null; GeometryCollectionBuilder geometryCollections = null; Orientation orientation = (shapeMapper == null) ? Orientation.RIGHT : shapeMapper.orientation(); boolean coerce = shapeMapper != null && shapeMapper.coerce(); boolean ignoreZValue = shapeMapper == null || shapeMapper.ignoreZValue(); String malformedException = null; XContentParser.Token token; try (XContentParser subParser = new XContentSubParser(parser)) { while ((token = subParser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { String fieldName = subParser.currentName(); if (ShapeParser.FIELD_TYPE.match(fieldName, subParser.getDeprecationHandler())) { subParser.nextToken(); final GeoShapeType type = GeoShapeType.forName(subParser.text()); if (shapeType != null && shapeType.equals(type) == false) { malformedException = ShapeParser.FIELD_TYPE + " already parsed as [" + shapeType + "] cannot redefine as [" + type + "]"; } else { shapeType = type; } } else if (ShapeParser.FIELD_COORDINATES.match(fieldName, subParser.getDeprecationHandler())) { subParser.nextToken(); CoordinateNode tempNode = parseCoordinates(subParser, ignoreZValue); if (coordinateNode != null && tempNode.numDimensions() != coordinateNode.numDimensions()) { throw new ElasticsearchParseException("Exception parsing coordinates: " + "number of dimensions do not match"); } coordinateNode = tempNode; } else if (ShapeParser.FIELD_GEOMETRIES.match(fieldName, subParser.getDeprecationHandler())) { if (shapeType == null) { shapeType = GeoShapeType.GEOMETRYCOLLECTION; } else if (shapeType.equals(GeoShapeType.GEOMETRYCOLLECTION) == false) { malformedException = "cannot have [" + ShapeParser.FIELD_GEOMETRIES + "] with type set to [" + shapeType + "]"; } subParser.nextToken(); geometryCollections = parseGeometries(subParser, shapeMapper); } else if (CircleBuilder.FIELD_RADIUS.match(fieldName, subParser.getDeprecationHandler())) { if (shapeType == null) { shapeType = GeoShapeType.CIRCLE; } else if (shapeType != null && shapeType.equals(GeoShapeType.CIRCLE) == false) { malformedException = "cannot have [" + CircleBuilder.FIELD_RADIUS + "] with type set to [" + shapeType + "]"; } subParser.nextToken(); radius = DistanceUnit.Distance.parseDistance(subParser.text()); } else if (ShapeParser.FIELD_ORIENTATION.match(fieldName, subParser.getDeprecationHandler())) { if (shapeType != null && (shapeType.equals(GeoShapeType.POLYGON) || shapeType.equals(GeoShapeType.MULTIPOLYGON)) == false) { malformedException = "cannot have [" + ShapeParser.FIELD_ORIENTATION + "] with type set to [" + shapeType + "]"; } subParser.nextToken(); orientation = ShapeBuilder.Orientation.fromString(subParser.text()); } else { subParser.nextToken(); subParser.skipChildren(); } } } } if (malformedException != null) { throw new ElasticsearchParseException(malformedException); } else if (shapeType == null) { throw new ElasticsearchParseException("shape type not included"); } else if (coordinateNode == null && GeoShapeType.GEOMETRYCOLLECTION != shapeType) { throw new ElasticsearchParseException("coordinates not included"); } else if (geometryCollections == null && GeoShapeType.GEOMETRYCOLLECTION == shapeType) { throw new ElasticsearchParseException("geometries not included"); } else if (radius != null && GeoShapeType.CIRCLE != shapeType) { throw new ElasticsearchParseException("field [{}] is supported for [{}] only", CircleBuilder.FIELD_RADIUS, CircleBuilder.TYPE); } if (shapeType.equals(GeoShapeType.GEOMETRYCOLLECTION)) { return geometryCollections; } return shapeType.getBuilder(coordinateNode, radius, orientation, coerce); }	whether or not a parameter has been explicitly set is of interest only to the mapper itself, and its serialization code; the pr changes these getters on the mappers to return simple booleans.
InferenceResults processResult(PyTorchResult pyTorchResult) { if (pyTorchResult.getInferenceResult().length < 1) { return new WarningInferenceResults("Sentiment analysis result has no data"); } if (pyTorchResult.getInferenceResult()[0].length < 2) { return new WarningInferenceResults("Expected 2 values in sentiment analysis result"); } double[] normalizedScores = NlpHelpers.convertToProbabilitiesBySoftMax(pyTorchResult.getInferenceResult()[0]); // the second score is usually the positive score so put that first return new SentimentAnalysisResults(classLabels.get(1), normalizedScores[1], classLabels.get(0), normalizedScores[0]); }	this confused me. now we have explicit labels for class 1 and class 2, shouldn't class 1 correspond to the first score (index 0) and class 2 to the second (index 1)?
private int readArraySize() throws IOException { final int arraySize = readVInt(); if (arraySize > ArrayUtil.MAX_ARRAY_LENGTH) { throw new IllegalStateException("array length must be <= to " + ArrayUtil.MAX_ARRAY_LENGTH + " but was: " + arraySize); } if (arraySize < 0) { throw new NegativeArraySizeException("array size must be positive but was: " + arraySize); } // lets do a sanity check that if we are reading an array size that is bigger that the remaining bytes we can safely // throw an exception instead of allocating the array based on the size. A simple corrutpted byte can make a node go OOM // if the size is large and for perf reasons we allocate arrays ahead of time ensureCanReadBytes(arraySize); return arraySize; } /** * This method throws an {@link EOFException}	does it need to be public or can it be made protected?
@Override public void messageReceived(final ShardActiveRequest request, final TransportChannel channel) throws Exception { IndexShard indexShard = getShard(request); // make sure shard is really there before register cluster state observer if (indexShard == null) { channel.sendResponse(new ShardActiveResponse(false, clusterService.localNode())); } // create observer just in case ClusterStateObserver observer = new ClusterStateObserver(clusterService, request.timeout, logger); // check if shard is active. if so, all is good boolean shardActive = shardActive(indexShard); if (shardActive) { channel.sendResponse(new ShardActiveResponse(true, clusterService.localNode())); } else { // shard is not active, might be POST_RECOVERY so check if cluster state changed inbetween or wait for next change observer.waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { sendResult(shardActive(getShard(request))); } @Override public void onClusterServiceClose() { sendResult(false); } @Override public void onTimeout(TimeValue timeout) { sendResult(shardActive(getShard(request))); } public void sendResult(boolean shardActive) { try { channel.sendResponse(new ShardActiveResponse(shardActive, clusterService.localNode())); } catch (IOException e) { logger.error("failed send response for shard active while trying to delete shard {} - shard will probably not be removed", e, request.shardId); } catch (EsRejectedExecutionException e) { logger.error("failed send response for shard active while trying to delete shard {} - shard will probably not be removed", e, request.shardId); } } }, new ClusterStateObserver.ValidationPredicate() { @Override protected boolean validate(ClusterState newState) { // the shard is not there in which case we want to send back a false, so the cluster state listener must be notified // or the shard is active in which case we want to send back that the shard is active IndexShard indexShard = getShard(request); return indexShard == null || shardActive(indexShard); } }); } }	we don't use this now here, right? no need to create just in case.
public void indexCleanup() throws Exception { final String masterNode = internalCluster().startNode(ImmutableSettings.builder().put("node.data", false)); final String node_1 = internalCluster().startNode(ImmutableSettings.builder().put("node.master", false)); final String node_2 = internalCluster().startNode(ImmutableSettings.builder().put("node.master", false)); logger.info("--> creating index [test] with one shard and on replica"); assertAcked(prepareCreate("test").setSettings( ImmutableSettings.builder().put(indexSettings()) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)) ); ensureGreen("test"); logger.info("--> making sure that shard and its replica are allocated on node_1 and node_2"); assertThat(Files.exists(shardDirectory(node_1, "test", 0)), equalTo(true)); assertThat(Files.exists(indexDirectory(node_1, "test")), equalTo(true)); assertThat(Files.exists(shardDirectory(node_2, "test", 0)), equalTo(true)); assertThat(Files.exists(indexDirectory(node_2, "test")), equalTo(true)); logger.info("--> starting node server3"); final String node_3 = internalCluster().startNode(ImmutableSettings.builder().put("node.master", false)); logger.info("--> running cluster_health"); ClusterHealthResponse clusterHealth = client().admin().cluster().prepareHealth() .setWaitForNodes("4") .setWaitForRelocatingShards(0) .get(); assertThat(clusterHealth.isTimedOut(), equalTo(false)); assertThat(Files.exists(shardDirectory(node_1, "test", 0)), equalTo(true)); assertThat(Files.exists(indexDirectory(node_1, "test")), equalTo(true)); assertThat(Files.exists(shardDirectory(node_2, "test", 0)), equalTo(true)); assertThat(Files.exists(indexDirectory(node_2, "test")), equalTo(true)); assertThat(Files.exists(shardDirectory(node_3, "test", 0)), equalTo(false)); assertThat(Files.exists(indexDirectory(node_3, "test")), equalTo(false)); logger.info("--> move shard from node_1 to node_3, and wait for relocation to finish"); SlowClusterStateProcessing disruption = null; if (randomBoolean()) { disruption = new SlowClusterStateProcessing(node_3, getRandom(), 0, 0, 1000, 2000); internalCluster().setDisruptionScheme(disruption); disruption.startDisrupting(); } internalCluster().client().admin().cluster().prepareReroute().add(new MoveAllocationCommand(new ShardId("test", 0), node_1, node_3)).get(); clusterHealth = client().admin().cluster().prepareHealth() .setWaitForNodes("4") .setWaitForRelocatingShards(0) .get(); assertThat(clusterHealth.isTimedOut(), equalTo(false)); assertThat(waitForShardDeletion(node_1, "test", 0), equalTo(false)); assertThat(waitForIndexDeletion(node_1, "test"), equalTo(false)); assertThat(Files.exists(shardDirectory(node_2, "test", 0)), equalTo(true)); assertThat(Files.exists(indexDirectory(node_2, "test")), equalTo(true)); assertThat(Files.exists(shardDirectory(node_3, "test", 0)), equalTo(true)); assertThat(Files.exists(indexDirectory(node_3, "test")), equalTo(true)); if (disruption != null) { disruption.stopDisrupting(); } }	not needed. the test infra cleans up disruptions..
public void testPreload() throws IOException { doTestPreload(); doTestPreload("nvd", "dvd", "tim"); doTestPreload("*"); Settings build = Settings.builder() .put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), IndexModule.Type.HYBRIDFS.name().toLowerCase(Locale.ROOT)) .putList(IndexModule.INDEX_STORE_PRE_LOAD_SETTING.getKey(), "dvd", "tmp") .build(); try (Directory directory = newDirectory(build)) { assertTrue(FsDirectoryFactory.isHybridFs(directory)); FsDirectoryFactory.HybridDirectory hybridDirectory = (FsDirectoryFactory.HybridDirectory) directory; assertTrue(hybridDirectory.useDelegate("foo.dvd", newIOContext(random()))); assertTrue(hybridDirectory.useDelegate("foo.nvd", newIOContext(random()))); assertTrue(hybridDirectory.useDelegate("foo.tim", newIOContext(random()))); assertTrue(hybridDirectory.useDelegate("foo.tip", newIOContext(random()))); assertTrue(hybridDirectory.useDelegate("foo.cfs", newIOContext(random()))); assertTrue(hybridDirectory.useDelegate("foo.dim", newIOContext(random()))); assertTrue(hybridDirectory.useDelegate("foo.kdd", newIOContext(random()))); assertTrue(hybridDirectory.useDelegate("foo.kdi", newIOContext(random()))); assertFalse(hybridDirectory.useDelegate("foo.kdi", Store.READONCE_CHECKSUM)); assertFalse(hybridDirectory.useDelegate("foo.tmp", newIOContext(random()))); MMapDirectory delegate = hybridDirectory.getDelegate(); assertThat(delegate, Matchers.instanceOf(FsDirectoryFactory.PreLoadMMapDirectory.class)); FsDirectoryFactory.PreLoadMMapDirectory preLoadMMapDirectory = (FsDirectoryFactory.PreLoadMMapDirectory) delegate; assertTrue(preLoadMMapDirectory.useDelegate("foo.dvd")); assertTrue(preLoadMMapDirectory.useDelegate("foo.tmp")); } }	why way this change necessary? bar is not a valid extension, just as tmp?
*/ public static CircuitBreakerService createCircuitBreakerService(Settings settings, ClusterSettings clusterSettings) { String type = BREAKER_TYPE_KEY.get(settings); if (type.equals("hierarchy")) { return new HierarchyCircuitBreakerService(settings, clusterSettings); } else if (type.equals("none")) { return new NoneCircuitBreakerService(); } else { throw new IllegalArgumentException("Unknown circuit breaker type [" + type + "]"); } } /** * Creates a new {@link BigArrays} instance used for this node. * This method can be overwritten by subclasses to change their {@link BigArrays}	maybe just call it createbigarrays? also make the method protected.
public void refreshMapping(final String index, final String indexUUID) { final RefreshTask refreshTask = new RefreshTask(index, indexUUID); clusterService.submitStateUpdateTask(String.format("refresh-mapping [%s/%s]", index, indexUUID), refreshTask, ClusterStateTaskConfig.build(Priority.HIGH), refreshExecutor, (source, e) -> logger.warn(() -> new ParameterizedMessage("failure during [{}]", source), e) ); }	let's add only the index name to the task name. also, we can just concatenate strings (i.e., "refresh-mapping[" + index + "]"); no need to use string.format.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); final GetMappingsRequest getMappingsRequest = new GetMappingsRequest(); getMappingsRequest.indices(indices); getMappingsRequest.indicesOptions(IndicesOptions.fromRequest(request, getMappingsRequest.indicesOptions())); getMappingsRequest.masterNodeTimeout(request.paramAsTime("master_timeout", getMappingsRequest.masterNodeTimeout())); getMappingsRequest.local(request.paramAsBoolean("local", getMappingsRequest.local())); return channel -> client.admin().indices().getMappings(getMappingsRequest, new RestActionListener<>(channel) { @Override protected void processResponse(GetMappingsResponse getMappingsResponse) { // Process serialization on GENERIC pool since the serialization of the raw mappings to XContent can be too slow to execute // on an IO thread threadPool.generic().execute(ActionRunnable.wrap(this, l -> new RestBuilderListener<GetMappingsResponse>(channel) { @Override public RestResponse buildResponse(final GetMappingsResponse response, final XContentBuilder builder) throws Exception { builder.startObject(); response.toXContent(builder, request); builder.endObject(); return new BytesRestResponse(RestStatus.OK, builder); } }.onResponse(getMappingsResponse))); } }); }	wdyt about checking for a timeout again here before we do all the serialisation work? if we use a bounded threadpool then these things might pile up, so aborting early might be a helpful way to push back.
@Override public List<ActionHandler<? extends ActionRequest, ? extends ActionResponse>> getActions() { return Arrays.asList( new ActionHandler<>(GetGlobalCheckpointsAction.INSTANCE, GetGlobalCheckpointsAction.TransportGetGlobalCheckpointsAction.class), new ActionHandler<>(GetGlobalCheckpointsShardAction.INSTANCE, GetGlobalCheckpointsShardAction.TransportAction.class) ); }	nit: just for consistency we should rename this to just transportaction too? completely optional.
public boolean isAggregatable() { try { fielddataBuilder(""); return true; } catch (IllegalArgumentException e) { return false; } } /** Generates a query that will only match documents that contain the given value. * The default implementation returns a {@link TermQuery} over the value bytes, * boosted by {@link #boost()}. * @throws IllegalArgumentException if {@code value} cannot be converted to the expected data type or if the field is not searchable * due to the way it is configured (eg. not indexed) * @throws ElasticsearchParseException if {@code value}	the last 2 javadoc "throws" descriptions give identical reasons?
* If false, only searchable field types are added. * @param acceptMetadataField Whether metadata fields should be added when a pattern is expanded. * @param fieldSuffix The suffix name to add to the expanded field names if a mapping exists for that name. * The original name of the field is kept if adding the suffix to the field name does not point to a valid field * in the mapping. */ public static Map<String, Float> resolveMappingField(QueryShardContext context, String fieldOrPattern, float weight, boolean acceptAllTypes, boolean acceptMetadataField, String fieldSuffix) { Collection<String> allFields = context.simpleMatchToIndexNames(fieldOrPattern); Map<String, Float> fields = new HashMap<>(); for (String fieldName : allFields) { if (fieldSuffix != null && context.fieldMapper(fieldName + fieldSuffix) != null) { fieldName = fieldName + fieldSuffix; } MappedFieldType ft = context.fieldMapper(fieldName); if (ft == null) { // Unmapped fields are not ignored fields.put(fieldOrPattern, weight); continue; } if (acceptMetadataField == false && ft.name().startsWith("_")) { // Ignore metadata fields continue; } if (acceptAllTypes == false) { try { ft.termQuery("", context); } catch (QueryShardException|UnsupportedOperationException e) { // field type is never searchable with term queries (eg. geo point): ignore continue; } catch (RuntimeException e) { // other exceptions are parsing errors or not indexed fields: keep } } fields.put(fieldName, weight); } checkForTooManyFields(fields); return fields; }	would this break bwc for some users? underscored fieldnames were always by convention "ours" but are we now being stronger on this (either with docs or enforcing naming conventions?)
* If false, only searchable field types are added. * @param acceptMetadataField Whether metadata fields should be added when a pattern is expanded. * @param fieldSuffix The suffix name to add to the expanded field names if a mapping exists for that name. * The original name of the field is kept if adding the suffix to the field name does not point to a valid field * in the mapping. */ public static Map<String, Float> resolveMappingField(QueryShardContext context, String fieldOrPattern, float weight, boolean acceptAllTypes, boolean acceptMetadataField, String fieldSuffix) { Collection<String> allFields = context.simpleMatchToIndexNames(fieldOrPattern); Map<String, Float> fields = new HashMap<>(); for (String fieldName : allFields) { if (fieldSuffix != null && context.fieldMapper(fieldName + fieldSuffix) != null) { fieldName = fieldName + fieldSuffix; } MappedFieldType ft = context.fieldMapper(fieldName); if (ft == null) { // Unmapped fields are not ignored fields.put(fieldOrPattern, weight); continue; } if (acceptMetadataField == false && ft.name().startsWith("_")) { // Ignore metadata fields continue; } if (acceptAllTypes == false) { try { ft.termQuery("", context); } catch (QueryShardException|UnsupportedOperationException e) { // field type is never searchable with term queries (eg. geo point): ignore continue; } catch (RuntimeException e) { // other exceptions are parsing errors or not indexed fields: keep } } fields.put(fieldName, weight); } checkForTooManyFields(fields); return fields; }	is there a similar capability-testing concern with field types that support phrase queries?
private void runFollowTest(CheckedBiConsumer<InternalEngine, FollowingEngine, Exception> task) throws Exception { final CheckedBiConsumer<InternalEngine, FollowingEngine, Exception> wrappedTask = (leader, follower) -> { Thread[] threads = new Thread[between(1, 8)]; AtomicBoolean taskIsCompleted = new AtomicBoolean(); AtomicLong lastFetchedSeqNo = new AtomicLong(follower.getProcessedLocalCheckpoint()); CountDownLatch latch = new CountDownLatch(threads.length + 1); for (int i = 0; i < threads.length; i++) { threads[i] = new Thread(() -> { try { latch.countDown(); latch.await(); fetchOperations(taskIsCompleted, lastFetchedSeqNo, leader, follower); } catch (Exception e) { throw new AssertionError(e); } }); threads[i].start(); } try { latch.countDown(); latch.await(); task.accept(leader, follower); EngineTestCase.waitForOpsToComplete(follower, leader.getProcessedLocalCheckpoint()); } finally { taskIsCompleted.set(true); for (Thread thread : threads) { thread.join(); } assertThat(follower.getMaxSeqNoOfUpdatesOrDeletes(), greaterThanOrEqualTo(leader.getMaxSeqNoOfUpdatesOrDeletes())); assertThat(getDocIds(follower, true), equalTo(getDocIds(leader, true))); EngineTestCase.assertConsistentHistoryBetweenTranslogAndLuceneIndex(follower); EngineTestCase.assertAtMostOneLuceneDocumentPerSequenceNumber(follower); } }; Settings leaderSettings = Settings.builder() .put("index.number_of_shards", 1).put("index.number_of_replicas", 0) .put("index.version.created", Version.CURRENT).build(); IndexMetadata leaderIndexMetadata = IndexMetadata.builder(index.getName()).settings(leaderSettings).build(); IndexSettings leaderIndexSettings = new IndexSettings(leaderIndexMetadata, leaderSettings); try (Store leaderStore = createStore(shardId, leaderIndexSettings, newDirectory())) { leaderStore.createEmpty(); EngineConfig leaderConfig = engineConfig(shardId, leaderIndexSettings, threadPool, leaderStore, logger, xContentRegistry()); leaderStore.associateIndexWithNewTranslog(Translog.createEmptyTranslog( leaderConfig.getTranslogConfig().getTranslogPath(), SequenceNumbers.NO_OPS_PERFORMED, shardId, 1L)); try (InternalEngine leaderEngine = new InternalEngine(leaderConfig)) { leaderEngine.skipTranslogRecovery(); Settings followerSettings = Settings.builder() .put("index.number_of_shards", 1).put("index.number_of_replicas", 0) .put("index.version.created", Version.CURRENT).put("index.xpack.ccr.following_index", true).build(); IndexMetadata followerIndexMetadata = IndexMetadata.builder(index.getName()).settings(followerSettings).build(); IndexSettings followerIndexSettings = new IndexSettings(followerIndexMetadata, leaderSettings); try (Store followerStore = createStore(shardId, followerIndexSettings, newDirectory())) { EngineConfig followerConfig = engineConfig( shardId, followerIndexSettings, threadPool, followerStore, logger, xContentRegistry()); try (FollowingEngine followingEngine = createEngine(followerStore, followerConfig)) { wrappedTask.accept(leaderEngine, followingEngine); } } } } }	i am not sure i understand why this needs to assertbusy?
public void testNoopPeerRecoveriesWhenIndexClosed() throws Exception { final String indexName = "noop-peer-recovery-test"; int numberOfReplicas = between(1, 2); internalCluster().ensureAtLeastNumDataNodes(numberOfReplicas + between(1, 2)); createIndex(indexName, Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, numberOfReplicas) .put("index.routing.rebalance.enable", "none") .build()); int iterations = between(1, 3); for (int iter = 0; iter < iterations; iter++) { indexRandom(randomBoolean(), randomBoolean(), randomBoolean(), IntStream.range(0, randomIntBetween(0, 50)) .mapToObj(n -> client().prepareIndex(indexName, "_doc").setSource("num", n)).collect(toList())); ensureGreen(indexName); // Closing an index should execute noop peer recovery assertAcked(client().admin().indices().prepareClose(indexName).get()); assertIndexIsClosed(indexName); ensureGreen(indexName); assertNoFileBasedRecovery(indexName); internalCluster().assertSameDocIdsOnShards(); // Open a closed index should execute noop recovery assertAcked(client().admin().indices().prepareOpen(indexName).get()); assertIndexIsOpened(indexName); ensureGreen(indexName); assertNoFileBasedRecovery(indexName); internalCluster().assertSameDocIdsOnShards(); } }	perhaps add a comment that says that this tests recovery of a replica of a closed index that has some docs missing that were on the primary, leading to a file-based recovery
public void testStacktraceWithJson() throws IOException { String json = "{" + LINE_SEPARATOR + " \\\\"terms\\\\" : {" + LINE_SEPARATOR + " \\\\"user\\\\" : [" + LINE_SEPARATOR + " \\\\"u1\\\\"," + LINE_SEPARATOR + " \\\\"u2\\\\"," + LINE_SEPARATOR + " \\\\"u3\\\\"" + LINE_SEPARATOR + " ]," + LINE_SEPARATOR + " \\\\"boost\\\\" : 1.0" + LINE_SEPARATOR + " }" + LINE_SEPARATOR + "}"; Exception thrown = new Exception(json); LogEvent event = Log4jLogEvent.newBuilder().setMessage(new SimpleMessage("message")).setThrown(thrown).build(); String result = format(event); // confirms exception is correctly parsed JsonLogLine jsonLogLine = JsonLogsStream.from(new BufferedReader(new StringReader(result)), JsonLogLine.ES_LOG_LINE) .findFirst() .orElseThrow(() -> new AssertionError("no logs parsed")); int jsonLength = json.split(LINE_SEPARATOR).length; int stacktraceLength = thrown.getStackTrace().length; assertThat( "stacktrace should formatted in multiple lines. JsonLogLine= " + jsonLogLine + " result= " + result, jsonLogLine.stacktrace().size(), equalTo(jsonLength + stacktraceLength) ); }	would this be any clearer? suggestion string json = """ { "terms": { "user": [ "u1", "u2", "u3" ], "boost": 1.0 } }\\\\ """.lines().collect(collectors.joining(system.lineseparator()));
private String format(LogEvent event) { StringBuilder builder = new StringBuilder(); converter.format(event, builder); String jsonStacktraceElement = builder.toString(); return "{\\\\"type\\\\": \\\\"console\\\\", \\\\"timestamp\\\\": \\\\"2019-01-03T16:30:53,058+0100\\\\", \\\\"level\\\\": \\\\"DEBUG\\\\", " + "\\\\"component\\\\": \\\\"o.e.a.s.TransportSearchAction\\\\", \\\\"cluster.name\\\\": \\\\"clustername\\\\", \\\\"node.name\\\\": \\\\"node-0\\\\", " + "\\\\"cluster.uuid\\\\": \\\\"OG5MkvOrR9azuClJhWvy6Q\\\\", \\\\"node.id\\\\": \\\\"VTShUqmcQG6SzeKY5nn7qA\\\\", \\\\"message\\\\": \\\\"msg msg\\\\" " + jsonStacktraceElement + "}"; }	we could maybe try something a bit like this if we still wanted the text block? suggestion return """ { "type": "console", "timestamp": "2019-01-03t16:30:53,058+0100", "level": "debug", "component": "o.e.a.s.transportsearchaction", "cluster.name": "clustername", "node.name": "node-0", "cluster.uuid": "og5mkvorr9azucljhwvy6q", "node.id": "vtshuqmcqg6szeky5nn7qa", "message": "msg msg" %s }""".formatted(jsonstacktraceelement).lines().map(string::trim).collect(collectors.join(" "));
public static Deployment fromString(String string) { for (Deployment deployment : values()) { if (deployment.deployment.equalsIgnoreCase(string)) { return deployment; } } throw new IllegalArgumentException("invalid value for deployment type [" + string + "]"); }	i'll drop this call and the ones like it in a follow up change, but it'll be quite large so i wanted to separate it from this one.
@Override protected synchronized void doStart() { Objects.requireNonNull(nodeConnectionsService, "please set the node connection service before starting"); Objects.requireNonNull(state.get(), "please set initial state before starting"); addListener(localNodeMasterListeners); threadPoolExecutor = EsExecutors.newSinglePrioritizing( nodeName + "/" + CLUSTER_UPDATE_THREAD_NAME, daemonThreadFactory(nodeName, CLUSTER_UPDATE_THREAD_NAME), threadPool.getThreadContext(), threadPool.scheduler()); }	i added this method to esexecutors because i like how explicit it is. we already have the node name in a few places and *don't* have settings in this one.
public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; for (ApplicationPrivilegeDescriptor privilege : privileges) { try { ApplicationPrivilege.validateApplicationName(privilege.getApplication()); } catch (IllegalArgumentException e) { validationException = addValidationError(e.getMessage(), validationException); } try { ApplicationPrivilege.validatePrivilegeName(privilege.getName()); } catch (IllegalArgumentException e) { validationException = addValidationError(e.getMessage(), validationException); } try { ApplicationPrivilege.validatePrivilegeName(privilege.getName()); } catch (IllegalArgumentException e) { validationException = addValidationError(e.getMessage(), validationException); } for (String action : privilege.getActions()) { if (action.indexOf('/') == -1 && action.indexOf('*') == -1 && action.indexOf(':') == -1) { validationException = addValidationError("action [" + action + "] must contain one of [ '/' , '*' , ':' ]", validationException); } try { ApplicationPrivilege.validatePrivilegeOrActionName(action); } catch (IllegalArgumentException e) { validationException = addValidationError(e.getMessage(), validationException); } } if (MetadataUtils.containsReservedMetadata(privilege.getMetadata())) { validationException = addValidationError("metadata keys may not start with [" + MetadataUtils.RESERVED_PREFIX + "] (in privilege " + privilege.getApplication() + ' ' + privilege.getName() + ")", validationException); } } return validationException; } /** * Should this request trigger a refresh ({@linkplain RefreshPolicy#IMMEDIATE}, the default), wait for a refresh ( * {@linkplain RefreshPolicy#WAIT_UNTIL}), or proceed ignore refreshes entirely ({@linkplain RefreshPolicy#NONE}	hmm i think this should be something else or removed?
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final ForceMergeRequest mergeRequest = new ForceMergeRequest(Strings.splitStringByCommaToArray(request.param("index"))); mergeRequest.indicesOptions(IndicesOptions.fromRequest(request, mergeRequest.indicesOptions())); mergeRequest.maxNumSegments(request.paramAsInt("max_num_segments", mergeRequest.maxNumSegments())); mergeRequest.onlyExpungeDeletes(request.paramAsBoolean("only_expunge_deletes", mergeRequest.onlyExpungeDeletes())); mergeRequest.flush(request.paramAsBoolean("flush", mergeRequest.flush())); if (request.paramAsBoolean("wait_for_completion", true)) { return channel -> client.admin().indices().forceMerge(mergeRequest, new RestToXContentListener<>(channel)); } else { mergeRequest.setShouldStoreResult(true); } /* * Let's try and validate before forking so the user gets some error. The * task can't totally validate until it starts but this is better than * nothing. */ ActionRequestValidationException validationException = mergeRequest.validate(); if (validationException != null) { throw validationException; } return sendTask( client.getLocalNodeId(), client.executeLocally(ForceMergeAction.INSTANCE, mergeRequest, LoggingTaskListener.instance()) ); }	validate is already called by the transport infrastructure. i don't think we need to create this "branch only" case where we sometimes validate the request based on the wait_for_completion parameter. i think we should remove this section.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final ForceMergeRequest mergeRequest = new ForceMergeRequest(Strings.splitStringByCommaToArray(request.param("index"))); mergeRequest.indicesOptions(IndicesOptions.fromRequest(request, mergeRequest.indicesOptions())); mergeRequest.maxNumSegments(request.paramAsInt("max_num_segments", mergeRequest.maxNumSegments())); mergeRequest.onlyExpungeDeletes(request.paramAsBoolean("only_expunge_deletes", mergeRequest.onlyExpungeDeletes())); mergeRequest.flush(request.paramAsBoolean("flush", mergeRequest.flush())); if (request.paramAsBoolean("wait_for_completion", true)) { return channel -> client.admin().indices().forceMerge(mergeRequest, new RestToXContentListener<>(channel)); } else { mergeRequest.setShouldStoreResult(true); } /* * Let's try and validate before forking so the user gets some error. The * task can't totally validate until it starts but this is better than * nothing. */ ActionRequestValidationException validationException = mergeRequest.validate(); if (validationException != null) { throw validationException; } return sendTask( client.getLocalNodeId(), client.executeLocally(ForceMergeAction.INSTANCE, mergeRequest, LoggingTaskListener.instance()) ); }	nit: shall we move this block in the else block so we don't have "leaking" returns?
public static void main(String[] args) { final String executor = ThreadPool.Names.GENERIC; final boolean waitForRequest = true; final ByteSizeValue payloadSize = new ByteSizeValue(100, ByteSizeUnit.BYTES); final int NUMBER_OF_CLIENTS = 10; final int NUMBER_OF_ITERATIONS = 100000; final byte[] payload = new byte[(int) payloadSize.bytes()]; final AtomicLong idGenerator = new AtomicLong(); final Type type = Type.NETTY; Settings settings = ImmutableSettings.settingsBuilder() .build(); NodeSettingsService settingsService = new NodeSettingsService(settings); DynamicSettings dynamicSettings = new DynamicSettings(); final ThreadPool serverThreadPool = new ThreadPool("server"); final TransportService serverTransportService = new TransportService(type.newTransport(settings, serverThreadPool), serverThreadPool).start(); final ThreadPool clientThreadPool = new ThreadPool("client"); final TransportService clientTransportService = new TransportService(type.newTransport(settings, clientThreadPool), clientThreadPool).start(); final DiscoveryNode node = new DiscoveryNode("server", serverTransportService.boundAddress().publishAddress(), Version.CURRENT); serverTransportService.registerHandler("benchmark", new BaseTransportRequestHandler<BenchmarkMessageRequest>() { @Override public BenchmarkMessageRequest newInstance() { return new BenchmarkMessageRequest(); } @Override public String executor() { return executor; } @Override public void messageReceived(BenchmarkMessageRequest request, TransportChannel channel) throws Exception { channel.sendResponse(new BenchmarkMessageResponse(request)); } }); clientTransportService.connectToNode(node); for (int i = 0; i < 10000; i++) { BenchmarkMessageRequest message = new BenchmarkMessageRequest(1, payload); clientTransportService.submitRequest(node, "benchmark", message, new BaseTransportResponseHandler<BenchmarkMessageResponse>() { @Override public BenchmarkMessageResponse newInstance() { return new BenchmarkMessageResponse(); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public void handleResponse(BenchmarkMessageResponse response) { } @Override public void handleException(TransportException exp) { exp.printStackTrace(); } }).txGet(); } Thread[] clients = new Thread[NUMBER_OF_CLIENTS]; final CountDownLatch latch = new CountDownLatch(NUMBER_OF_CLIENTS * NUMBER_OF_ITERATIONS); for (int i = 0; i < NUMBER_OF_CLIENTS; i++) { clients[i] = new Thread(new Runnable() { @Override public void run() { for (int j = 0; j < NUMBER_OF_ITERATIONS; j++) { final long id = idGenerator.incrementAndGet(); BenchmarkMessageRequest request = new BenchmarkMessageRequest(id, payload); BaseTransportResponseHandler<BenchmarkMessageResponse> handler = new BaseTransportResponseHandler<BenchmarkMessageResponse>() { @Override public BenchmarkMessageResponse newInstance() { return new BenchmarkMessageResponse(); } @Override public String executor() { return executor; } @Override public void handleResponse(BenchmarkMessageResponse response) { if (response.id() != id) { System.out.println("NO ID MATCH [" + response.id() + "] and [" + id + "]"); } latch.countDown(); } @Override public void handleException(TransportException exp) { exp.printStackTrace(); latch.countDown(); } }; if (waitForRequest) { clientTransportService.submitRequest(node, "benchmark", request, handler).txGet(); } else { clientTransportService.sendRequest(node, "benchmark", request, handler); } } } }); } StopWatch stopWatch = new StopWatch().start(); for (int i = 0; i < NUMBER_OF_CLIENTS; i++) { clients[i].start(); } try { latch.await(); } catch (InterruptedException e) { e.printStackTrace(); } stopWatch.stop(); System.out.println("Ran [" + NUMBER_OF_CLIENTS + "], each with [" + NUMBER_OF_ITERATIONS + "] iterations, payload [" + payloadSize + "]: took [" + stopWatch.totalTime() + "], TPS: " + (NUMBER_OF_CLIENTS * NUMBER_OF_ITERATIONS) / stopWatch.totalTime().secondsFrac()); clientTransportService.close(); clientThreadPool.shutdownNow(); serverTransportService.close(); serverThreadPool.shutdownNow(); }	defined, but not used?
protected MockTransportService build(Settings settings, Version version) { NodeSettingsService settingsService = new NodeSettingsService(settings); DynamicSettings dynamicSettings = new DynamicSettings(); MockTransportService transportService = new MockTransportService(ImmutableSettings.EMPTY, new LocalTransport(settings, threadPool, version), threadPool); transportService.start(); return transportService; }	defined but not used
public static Throwable unwrapCause(Throwable t) { int counter = 0; Throwable result = t; while (result instanceof ElasticsearchWrapperException) { if (result.getCause() == null) { return result; } if (result.getCause() == result) { return result; } if (counter++ > 10) { // dear god, if we got more than 10 levels down, WTF? just bail logger.warn("Exception cause unwrapping ran for 10 levels: {}", t.getMessage()); return result; } result = result.getCause(); } return result; }	i think we'd do better logging the first 10 levels or something. it'd be fairly painful not to have any stack trace.
@Test public void rerouteRecoveryTest() throws Exception { logger.info("--> start node A"); String nodeA = internalCluster().startNode(settingsBuilder().put("gateway.type", "local")); logger.info("--> create index on node: {}", nodeA); createAndPopulateIndex(INDEX_NAME, 1, SHARD_COUNT, REPLICA_COUNT); logger.info("--> start node B"); String nodeB = internalCluster().startNode(settingsBuilder().put("gateway.type", "local")); ensureGreen(); logger.info("--> slowing down recoveries"); assertTrue(client().admin().cluster().prepareUpdateSettings().setTransientSettings("max_bytes_per_sec: 1").get().isAcknowledged()); logger.info("--> move shard from: {} to: {}", nodeA, nodeB); client().admin().cluster().prepareReroute() .add(new MoveAllocationCommand(new ShardId(INDEX_NAME, 0), nodeA, nodeB)) .execute().actionGet().getState(); logger.info("--> request recoveries"); RecoveryResponse response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet(); List<ShardRecoveryResponse> shardResponses = response.shardResponses().get(INDEX_NAME); List<ShardRecoveryResponse> nodeAResponses = findRecoveriesForTargetNode(nodeA, shardResponses); assertThat(nodeAResponses.size(), equalTo(1)); List<ShardRecoveryResponse> nodeBResponses = findRecoveriesForTargetNode(nodeB, shardResponses); assertThat(nodeBResponses.size(), equalTo(1)); assertRecoveryState(nodeAResponses.get(0).recoveryState(), 0, Type.GATEWAY, Stage.DONE, nodeA, nodeA, false); validateIndexRecoveryState(nodeAResponses.get(0).recoveryState().getIndex()); assertOnGoingRecoveryState(nodeBResponses.get(0).recoveryState(), 0, Type.RELOCATION, nodeA, nodeB, false); validateIndexRecoveryState(nodeBResponses.get(0).recoveryState().getIndex()); logger.info("--> speeding up recoveries"); assertTrue(client().admin().cluster().prepareUpdateSettings().setTransientSettings("max_bytes_per_sec: 60mb").get().isAcknowledged()); // wait for it to be finished ensureGreen(); response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet(); shardResponses = response.shardResponses().get(INDEX_NAME); assertThat(shardResponses.size(), equalTo(1)); assertRecoveryState(shardResponses.get(0).recoveryState(), 0, Type.RELOCATION, Stage.DONE, nodeA, nodeB, false); validateIndexRecoveryState(shardResponses.get(0).recoveryState().getIndex()); logger.info("--> bump replica count"); client().admin().indices().prepareUpdateSettings(INDEX_NAME) .setSettings(settingsBuilder().put("number_of_replicas", 1)).execute().actionGet(); ensureGreen(); logger.info("--> start node C"); String nodeC = internalCluster().startNode(settingsBuilder().put("gateway.type", "local")); assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes("3").get().isTimedOut()); logger.info("--> slowing down recoveries"); assertTrue(client().admin().cluster().prepareUpdateSettings().setTransientSettings("max_bytes_per_sec: 1").get().isAcknowledged()); logger.info("--> move replica shard from: {} to: {}", nodeA, nodeC); client().admin().cluster().prepareReroute() .add(new MoveAllocationCommand(new ShardId(INDEX_NAME, 0), nodeA, nodeC)) .execute().actionGet().getState(); response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet(); shardResponses = response.shardResponses().get(INDEX_NAME); nodeAResponses = findRecoveriesForTargetNode(nodeA, shardResponses); assertThat(nodeAResponses.size(), equalTo(1)); nodeBResponses = findRecoveriesForTargetNode(nodeB, shardResponses); assertThat(nodeBResponses.size(), equalTo(1)); List<ShardRecoveryResponse> nodeCResponses = findRecoveriesForTargetNode(nodeC, shardResponses); assertThat(nodeCResponses.size(), equalTo(1)); assertRecoveryState(nodeAResponses.get(0).recoveryState(), 0, Type.REPLICA, Stage.DONE, nodeB, nodeA, false); validateIndexRecoveryState(nodeAResponses.get(0).recoveryState().getIndex()); assertRecoveryState(nodeBResponses.get(0).recoveryState(), 0, Type.RELOCATION, Stage.DONE, nodeA, nodeB, false); validateIndexRecoveryState(nodeBResponses.get(0).recoveryState().getIndex()); assertOnGoingRecoveryState(nodeCResponses.get(0).recoveryState(), 0, Type.RELOCATION, nodeA, nodeC, false); validateIndexRecoveryState(nodeCResponses.get(0).recoveryState().getIndex()); logger.info("--> speeding up recoveries"); assertTrue(client().admin().cluster().prepareUpdateSettings().setTransientSettings("max_bytes_per_sec: 60mb").get().isAcknowledged()); ensureGreen(); response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet(); shardResponses = response.shardResponses().get(INDEX_NAME); nodeAResponses = findRecoveriesForTargetNode(nodeA, shardResponses); assertThat(nodeAResponses.size(), equalTo(1)); nodeBResponses = findRecoveriesForTargetNode(nodeB, shardResponses); assertThat(nodeBResponses.size(), equalTo(1)); nodeCResponses = findRecoveriesForTargetNode(nodeC, shardResponses); assertThat(nodeCResponses.size(), equalTo(1)); assertRecoveryState(nodeAResponses.get(0).recoveryState(), 0, Type.REPLICA, Stage.DONE, nodeB, nodeA, false); validateIndexRecoveryState(nodeAResponses.get(0).recoveryState().getIndex()); assertRecoveryState(nodeBResponses.get(0).recoveryState(), 0, Type.RELOCATION, Stage.DONE, nodeA, nodeB, false); validateIndexRecoveryState(nodeBResponses.get(0).recoveryState().getIndex()); assertRecoveryState(nodeCResponses.get(0).recoveryState(), 0, Type.RELOCATION, Stage.DONE, nodeA, nodeC, false); validateIndexRecoveryState(nodeCResponses.get(0).recoveryState().getIndex()); }	i think you set the wrong setting here? it needs to be indices.recovery.max_bytes_per_sec (there is a constant for it (recoverysettings#indices_recovery_max_bytes_per_sec). also, much nicer to set using the explicitly settings object i would say.
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); if (out.getVersion().before(Version.V_8_0_0)) { // types not supported so send an empty array to previous versions out.writeString(null); } out.writeString(id); out.writeBoolean(doc != null); if (doc != null) { out.writeBytesReference(doc); out.writeEnum(xContentType); } out.writeOptionalString(routing); out.writeOptionalString(preference); long longFlags = 0; for (Flag flag : flagsEnum) { longFlags |= (1 << flag.ordinal()); } out.writeVLong(longFlags); if (selectedFields != null) { out.writeVInt(selectedFields.size()); for (String selectedField : selectedFields) { out.writeString(selectedField); } } else { out.writeVInt(0); } out.writeBoolean(perFieldAnalyzer != null); if (perFieldAnalyzer != null) { out.writeGenericValue(perFieldAnalyzer); } out.writeBoolean(filterSettings != null); if (filterSettings != null) { filterSettings.writeTo(out); } out.writeBoolean(realtime); out.writeByte(versionType.getValue()); out.writeLong(version); }	hmm, this would fail wouldn't it? i thought only writeoptionalstring would accept nulls? do you need to write _doc instead? let's also make sure at least one test goes through this path?
@Override protected Settings nodeSettings(int nodeOrdinal) { return Settings.builder() .put(super.nodeSettings(nodeOrdinal)) // Use an unbound cache so we can recover the searchable snapshot completely all the times .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES)) // Have a shared cache of reasonable size available on each node because tests randomize over frozen and cold allocation .put(SnapshotsService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), ByteSizeValue.ofMb(randomLongBetween(1, 10))) .build(); }	nit: we could set this only for data nodes
public synchronized void startDisrupting() { if (suspendedThreads == null) { boolean success = false; try { suspendedThreads = ConcurrentHashMap.newKeySet(); final String currentThreadNamme = Thread.currentThread().getName(); assert currentThreadNamme.contains("[" + disruptedNode + "]") == false : "current thread match pattern. thread name: " + currentThreadNamme + ", node: " + disruptedNode; // we spawn a background thread to protect against deadlock which can happen // if there are shared resources between caller thread and and suspended threads // see unsafeClasses to how to avoid that final AtomicReference<Exception> stoppingError = new AtomicReference<>(); final Thread stoppingThread = new Thread(new AbstractRunnable() { @Override public void onFailure(Exception e) { stoppingError.set(e); } @Override protected void doRun() throws Exception { while (stopNodeThreads(disruptedNode, suspendedThreads)) ; } }); stoppingThread.setName(currentThreadNamme + "[LongGCDisruption][threadStopper]"); stoppingThread.start(); try { stoppingThread.join(getStoppingTimeoutInMillis()); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } if (stoppingError.get() != null) { throw new RuntimeException("unknown error while stopping threads", stoppingError.get()); } if (stoppingThread.isAlive()) { logger.warn("failed to stop node [{}]'s threads within [{}] millis. Stopping thread stack trace:\\\\n {}" , disruptedNode, getStoppingTimeoutInMillis(), stackTrace(stoppingThread)); stoppingThread.interrupt(); // best effort; throw new RuntimeException("stopping node threads took too long"); } success = true; } finally { if (success == false) { // resume threads if failed resumeThreads(suspendedThreads); suspendedThreads = null; } } } else { throw new IllegalStateException("can't disrupt twice, call stopDisrupting() first"); } }	nit: currentthreadnamme -> currentthreadname
static Map<String, PreBuiltTokenFilterSpec> setupPreBuiltTokenFilters(List<AnalysisPlugin> plugins) { NamedRegistry<PreBuiltTokenFilterSpec> preBuiltTokenFilters = new NamedRegistry<>("pre built token_filter"); // Add filters available in lucene-core preBuiltTokenFilters.register("lowercase", new PreBuiltTokenFilterSpec(true, CachingStrategy.LUCENE, (inputs, version) -> new LowerCaseFilter(inputs))); preBuiltTokenFilters.register("standard", new PreBuiltTokenFilterSpec(false, CachingStrategy.LUCENE, (inputs, version) -> new StandardFilter(inputs))); /* Note that "stop" is available in lucene-core but it's pre-built version uses a set of English stop words that are in * lucene-analyzers-common so "stop" is defined in the analysis-common module. */ // Add token filers declared in PreBuiltTokenFilters until they have all been migrated for (PreBuiltTokenFilters preBuilt : PreBuiltTokenFilters.values()) { switch (preBuilt) { case LOWERCASE: // This has been migrated but has to stick around until PreBuiltTokenizers is removed. continue; default: preBuiltTokenFilters.register(preBuilt.name().toLowerCase(Locale.ROOT), new PreBuiltTokenFilterSpec(preBuilt.isMultiTermAware(), preBuilt.getCachingStrategy(), preBuilt::create)); } } preBuiltTokenFilters.extractAndRegister(plugins, AnalysisPlugin::getPreBuiltTokenFilters); return preBuiltTokenFilters.getRegistry(); }	i debated moving lowercase out of core and just keeping standard but there isn't really a technical reason to do that so i decided not to for now.
static Map<String, PreBuiltTokenFilterSpec> setupPreBuiltTokenFilters(List<AnalysisPlugin> plugins) { NamedRegistry<PreBuiltTokenFilterSpec> preBuiltTokenFilters = new NamedRegistry<>("pre built token_filter"); // Add filters available in lucene-core preBuiltTokenFilters.register("lowercase", new PreBuiltTokenFilterSpec(true, CachingStrategy.LUCENE, (inputs, version) -> new LowerCaseFilter(inputs))); preBuiltTokenFilters.register("standard", new PreBuiltTokenFilterSpec(false, CachingStrategy.LUCENE, (inputs, version) -> new StandardFilter(inputs))); /* Note that "stop" is available in lucene-core but it's pre-built version uses a set of English stop words that are in * lucene-analyzers-common so "stop" is defined in the analysis-common module. */ // Add token filers declared in PreBuiltTokenFilters until they have all been migrated for (PreBuiltTokenFilters preBuilt : PreBuiltTokenFilters.values()) { switch (preBuilt) { case LOWERCASE: // This has been migrated but has to stick around until PreBuiltTokenizers is removed. continue; default: preBuiltTokenFilters.register(preBuilt.name().toLowerCase(Locale.ROOT), new PreBuiltTokenFilterSpec(preBuilt.isMultiTermAware(), preBuilt.getCachingStrategy(), preBuilt::create)); } } preBuiltTokenFilters.extractAndRegister(plugins, AnalysisPlugin::getPreBuiltTokenFilters); return preBuiltTokenFilters.getRegistry(); }	this is the shim that i'll drop in a followup.
public TokenStream create(TokenStream tokenStream, Version version) { return new DelimitedPayloadTokenFilter(tokenStream, DelimitedPayloadTokenFilterFactory.DEFAULT_DELIMITER, DelimitedPayloadTokenFilterFactory.DEFAULT_ENCODER); } }, LIMIT(CachingStrategy.ONE) { @Override public TokenStream create(TokenStream tokenStream, Version version) { return new LimitTokenCountFilter(tokenStream, LimitTokenCountFilterFactory.DEFAULT_MAX_TOKEN_COUNT, LimitTokenCountFilterFactory.DEFAULT_CONSUME_ALL_TOKENS); } }, ; protected boolean isMultiTermAware() { return false; } public abstract TokenStream create(TokenStream tokenStream, Version version); protected final PreBuiltCacheFactory.PreBuiltCache<TokenFilterFactory> cache; private final CachingStrategy cachingStrategy; PreBuiltTokenFilters(CachingStrategy cachingStrategy) { this.cachingStrategy = cachingStrategy; cache = PreBuiltCacheFactory.getCache(cachingStrategy); } public CachingStrategy getCachingStrategy() { return cachingStrategy; } private interface MultiTermAwareTokenFilterFactory extends TokenFilterFactory, MultiTermAwareComponent {} public synchronized TokenFilterFactory getTokenFilterFactory(final Version version) { TokenFilterFactory factory = cache.get(version); if (factory == null) { final String finalName = name().toLowerCase(Locale.ROOT); if (isMultiTermAware()) { factory = new MultiTermAwareTokenFilterFactory() { @Override public String name() { return finalName; } @Override public TokenStream create(TokenStream tokenStream) { return PreBuiltTokenFilters.this.create(tokenStream, version); } @Override public Object getMultiTermComponent() { return this; } }; } else { factory = new TokenFilterFactory() { @Override public String name() { return finalName; } @Override public TokenStream create(TokenStream tokenStream) { return PreBuiltTokenFilters.this.create(tokenStream, version); } }; } cache.put(version, factory); }	i've preserved this method entirely for prebuiltanalyer.lowercase#getmultitermcomponent. i'll figure out how to move away from that at some point but i thought that too should wait for another pr.
@Override protected Map<String, Class<?>> getPreBuiltTokenFilters() { Map<String, Class<?>> filters = new TreeMap<>(super.getPreBuiltTokenFilters()); filters.put("asciifolding", null); filters.put("classic", null); filters.put("common_grams", null); filters.put("edge_ngram", null); filters.put("edgeNGram", null); filters.put("kstem", null); filters.put("length", null); filters.put("ngram", null); filters.put("nGram", null); filters.put("porter_stem", null); filters.put("reverse", ReverseStringFilterFactory.class); filters.put("stop", null); filters.put("trim", null); filters.put("truncate", null); filters.put("unique", Void.class); filters.put("uppercase", null); filters.put("word_delimiter", null); filters.put("word_delimiter_graph", null); return filters; } /** * Fails if a tokenizer is marked in the superclass with {@link MovedToAnalysisCommon}	i've sort of preserved this null behavior from the original test case. it is a bit funky and magical and i'm tempted to remove it but i've kept it for now to get opinions on it.
public void testPreBuiltMultiTermAware() { Collection<Object> expected = new HashSet<>(); Collection<Object> actual = new HashSet<>(); for (Map.Entry<PreBuiltTokenizers, Class<?>> entry : PREBUILT_TOKENIZERS.entrySet()) { PreBuiltTokenizers tokenizer = entry.getKey(); Class<?> luceneFactory = entry.getValue(); if (luceneFactory == Void.class) { continue; } assertTrue(TokenizerFactory.class.isAssignableFrom(luceneFactory)); if (tokenizer.getTokenizerFactory(Version.CURRENT) instanceof MultiTermAwareComponent) { actual.add(tokenizer); } if (org.apache.lucene.analysis.util.MultiTermAwareComponent.class.isAssignableFrom(luceneFactory)) { expected.add(tokenizer); } } Map<String, PreBuiltTokenFilterSpec> preBuiltTokenFilters = AnalysisModule.setupPreBuiltTokenFilters(singletonList(plugin)); for (Map.Entry<String, Class<?>> entry : getPreBuiltTokenFilters().entrySet()) { String name = entry.getKey(); Class<?> luceneFactory = entry.getValue(); if (luceneFactory == Void.class) { continue; } if (luceneFactory == null) { luceneFactory = TokenFilterFactory.lookupClass(toCamelCase(name)); } assertTrue(TokenFilterFactory.class.isAssignableFrom(luceneFactory)); PreBuiltTokenFilterSpec spec = preBuiltTokenFilters.get(name); assertNotNull("test claims pre built token filter [" + name + "] should be available but it wasn't", spec); if (spec.shouldUseFilterForMultitermQueries()) { actual.add("token filter [" + name + "]"); } if (org.apache.lucene.analysis.util.MultiTermAwareComponent.class.isAssignableFrom(luceneFactory)) { expected.add("token filter [" + name + "]"); } } for (Map.Entry<PreBuiltCharFilters, Class<?>> entry : PREBUILT_CHARFILTERS.entrySet()) { PreBuiltCharFilters charFilter = entry.getKey(); Class<?> luceneFactory = entry.getValue(); if (luceneFactory == Void.class) { continue; } assertTrue(CharFilterFactory.class.isAssignableFrom(luceneFactory)); if (charFilter.getCharFilterFactory(Version.CURRENT) instanceof MultiTermAwareComponent) { actual.add(charFilter); } if (org.apache.lucene.analysis.util.MultiTermAwareComponent.class.isAssignableFrom(luceneFactory)) { expected.add(charFilter); } } Set<Object> classesMissingMultiTermSupport = new HashSet<>(expected); classesMissingMultiTermSupport.removeAll(actual); assertTrue("Pre-built components are missing multi-term support: " + classesMissingMultiTermSupport, classesMissingMultiTermSupport.isEmpty()); Set<Object> classesThatShouldNotHaveMultiTermSupport = new HashSet<>(actual); classesThatShouldNotHaveMultiTermSupport.removeAll(expected); assertTrue("Pre-built components should not have multi-term support: " + classesThatShouldNotHaveMultiTermSupport, classesThatShouldNotHaveMultiTermSupport.isEmpty()); }	this is the other side of that magic null behavior that i wasn't super comfortable with. it is convenient though.
static DeprecationIssue checkImplicitlyDisabledSecurityOnBasicAndTrial(final Settings settings, final PluginsAndModules pluginsAndModules, final XPackLicenseState licenseState) { if ( XPackSettings.SECURITY_ENABLED.exists(settings) == false && (licenseState.getOperationMode().equals(License.OperationMode.BASIC) || licenseState.getOperationMode().equals(License.OperationMode.TRIAL))) { String details = "The default behavior of disabling security on " + licenseState.getOperationMode().description() + " licenses is deprecated. A later version of Elasticsearch will set [xpack.security.enabled] to \\\\"true\\\\" " + "for all licenses and enable security by default." + "See https://www.elastic.co/guide/en/elasticsearch/reference/" + Version.CURRENT.major + "." + Version.CURRENT.minor + "/security-minimal-setup.html to enable security, or explicitly disable security by " + "setting [xpack.security.enabled] to \\\\"false\\\\" in elasticsearch.yml"; return new DeprecationIssue( DeprecationIssue.Level.CRITICAL, "Security is enabled by default for all licenses in the next major version.", "https://www.elastic.co/guide/en/elasticsearch/reference/7.14/migrating-7.14.html#implicitly-disabled-security", details); } return null; }	could this be better phrased as "will change the default of [xpack.security.enabled] to 'true' regardless of the license level."?
public void testSearch() throws Exception { internalCluster().ensureAtLeastNumDataNodes(2); final int nodeCount = internalCluster().numDataNodes(); assertAcked( client().admin() .indices() .prepareCreate("test-matching") .setMapping("{\\\\"properties\\\\":{\\\\"message\\\\":{\\\\"type\\\\":\\\\"text\\\\"},\\\\"@timestamp\\\\":{\\\\"type\\\\":\\\\"date\\\\"}}}") .setSettings( Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, nodeCount * 6) ) ); assertAcked( client().admin() .indices() .prepareCreate("test-notmatching") .setMapping("{\\\\"properties\\\\":{\\\\"message\\\\":{\\\\"type\\\\":\\\\"text\\\\"},\\\\"@timestamp\\\\":{\\\\"type\\\\":\\\\"date\\\\"}}}") .setSettings( Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, nodeCount * 6) ) ); ensureGreen("test-matching", "test-notmatching"); final String matchingDate = "2021-11-17"; final String nonMatchingDate = "2021-01-01"; final BulkRequestBuilder bulkRequestBuilder = client().prepareBulk().setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); for (int i = 0; i < 1000; i++) { final boolean isMatching = randomBoolean(); final IndexRequestBuilder indexRequestBuilder = client().prepareIndex(isMatching ? "test-matching" : "test-notmatching"); indexRequestBuilder.setSource( "{\\\\"@timestamp\\\\":\\\\"" + (isMatching ? matchingDate : nonMatchingDate) + "\\\\",\\\\"message\\\\":\\\\"\\\\"}", XContentType.JSON ); bulkRequestBuilder.add(indexRequestBuilder); } assertFalse(bulkRequestBuilder.execute().actionGet(10, TimeUnit.SECONDS).hasFailures()); final APMTracer.CapturingSpanExporter spanExporter = APMTracer.CAPTURING_SPAN_EXPORTER; spanExporter.clear(); final Request searchRequest = new Request("GET", "_search"); searchRequest.addParameter("search_type", "query_then_fetch"); searchRequest.addParameter("pre_filter_shard_size", "1"); searchRequest.setJsonEntity("{\\\\"query\\\\":{\\\\"range\\\\":{\\\\"@timestamp\\\\":{\\\\"gt\\\\":\\\\"2021-11-01\\\\"}}}}"); searchRequest.setOptions( searchRequest.getOptions() .toBuilder() .addHeader( "Authorization", UsernamePasswordToken.basicAuthHeaderValue( SecuritySettingsSource.TEST_USER_NAME, new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()) ) ) ); final Response searchResponse = getRestClient().performRequest(searchRequest); assertTrue(spanExporter.findSpanByName(SearchAction.NAME).findAny().isPresent()); assertTrue(spanExporter.findSpanByName(SearchTransportService.QUERY_CAN_MATCH_NODE_NAME).findAny().isPresent()); }	issue the request at rest layer so it goes through all authentication/authorization layer. otherwise the first task is created without auth being triggered.
public ClusterState execute(ClusterState currentState) { ensureRepositoryExists(repositoryName, currentState); ensureSnapshotNameAvailableInRepo(repositoryData, snapshotName, repository); final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries(); ensureSnapshotNameNotRunning(runningSnapshots, repositoryName, snapshotName); validate(repositoryName, snapshotName, currentState); final SnapshotDeletionsInProgress deletionsInProgress = currentState.custom( SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY ); ensureNoCleanupInProgress(currentState, repositoryName, snapshotName, "create snapshot"); ensureBelowConcurrencyLimit(repositoryName, snapshotName, snapshots, deletionsInProgress); // Store newSnapshot here to be processed in clusterStateProcessed List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndexNames(currentState, request)); final Set<SnapshotFeatureInfo> featureStates = new HashSet<>(); final Set<String> systemDataStreamNames = new HashSet<>(); // if we have any feature states in the snapshot, we add their required indices to the snapshot indices if they haven't // been requested by the request directly final Set<String> indexNames = new HashSet<>(indices); for (String featureName : featureStatesSet) { SystemIndices.Feature feature = systemIndexDescriptorMap.get(featureName); Set<String> featureSystemIndices = feature.getIndexDescriptors() .stream() .flatMap(descriptor -> descriptor.getMatchingIndices(currentState.metadata()).stream()) .collect(Collectors.toSet()); Set<String> featureAssociatedIndices = feature.getAssociatedIndexDescriptors() .stream() .flatMap(descriptor -> descriptor.getMatchingIndices(currentState.metadata()).stream()) .collect(Collectors.toSet()); Set<String> featureSystemDataStreams = new HashSet<>(); Set<String> featureDataStreamBackingIndices = new HashSet<>(); for (SystemDataStreamDescriptor sdd : feature.getDataStreamDescriptors()) { List<String> backingIndexNames = sdd.getBackingIndexNames(currentState.metadata()); if (backingIndexNames.size() > 0) { featureDataStreamBackingIndices.addAll(backingIndexNames); featureSystemDataStreams.add(sdd.getDataStreamName()); } } if (featureSystemIndices.size() > 0 || featureAssociatedIndices.size() > 0 || featureDataStreamBackingIndices.size() > 0) { featureStates.add(new SnapshotFeatureInfo(featureName, new ArrayList<>(featureSystemIndices))); indexNames.addAll(featureSystemIndices); indexNames.addAll(featureAssociatedIndices); indexNames.addAll(featureDataStreamBackingIndices); systemDataStreamNames.addAll(featureSystemDataStreams); } indices = List.copyOf(indexNames); } final List<String> dataStreams = indexNameExpressionResolver.dataStreamNames( currentState, request.indicesOptions(), request.indices() ); dataStreams.addAll(systemDataStreamNames); logger.trace("[{}][{}] creating snapshot for indices [{}]", repositoryName, snapshotName, indices); final Map<String, IndexId> indexIds = repositoryData.resolveNewIndices( indices, getInFlightIndexIds(runningSnapshots, repositoryName) ); final Version version = minCompatibleVersion(currentState.nodes().getMinNodeVersion(), repositoryData, null); ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards = shards( snapshots, deletionsInProgress, currentState, indexIds.values(), useShardGenerations(version), repositoryData, repositoryName ); if (request.partial() == false) { Set<String> missing = new HashSet<>(); for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards) { if (entry.value.state() == ShardState.MISSING) { missing.add(entry.key.getIndex().getName()); } } if (missing.isEmpty() == false) { throw new SnapshotException( new Snapshot(repositoryName, snapshotId), "Indices don't have primary shards " + missing ); } } newEntry = SnapshotsInProgress.startedEntry( new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), request.partial(), indexIds, dataStreams, threadPool.absoluteTimeInMillis(), repositoryData.getGenId(), shards, userMeta, version, new ArrayList<>(featureStates) ); return ClusterState.builder(currentState) .putCustom(SnapshotsInProgress.TYPE, SnapshotsInProgress.of(CollectionUtils.appendToCopy(runningSnapshots, newEntry))) .build(); }	there's a bit of a bug here, that's not new but more explicit now in that we create snapshotfeatureinfo (something living in the cs) with a mutable list field. i'd just use list.copyof here instead of new arraylist for now to fix this spot.
@Override protected void advanceMaxSeqNoOfDeleteOnPrimary(long seqNo) { if (Assertions.ENABLED) { final long localCheckpoint = getProcessedLocalCheckpoint(); final long maxSeqNoOfUpdates = getMaxSeqNoOfUpdatesOrDeletes(); assert localCheckpoint < maxSeqNoOfUpdates || maxSeqNoOfUpdates >= seqNo : "maxSeqNoOfUpdates is not advanced local_checkpoint=" + localCheckpoint + " msu=" + maxSeqNoOfUpdates + " seq_no=" + seqNo; } super.advanceMaxSeqNoOfDeleteOnPrimary(seqNo); }	also plural here for consistency.
private void executeMultipartUpload(String blobName, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) { SocketAccess.doPrivilegedVoidException(() -> { final BlobServiceAsyncClient asyncClient = asyncClient(); final BlobAsyncClient blobAsyncClient = asyncClient.getBlobContainerAsyncClient(container) .getBlobAsyncClient(blobName); final BlockBlobAsyncClient blockBlobAsyncClient = blobAsyncClient.getBlockBlobAsyncClient(); final long partSize = getUploadBlockSize(); final Tuple<Long, Long> multiParts = numberOfMultiparts(blobSize, partSize); final int nbParts = multiParts.v1().intValue(); final long lastPartSize = multiParts.v2(); assert blobSize == (((nbParts - 1) * partSize) + lastPartSize) : "blobSize does not match multipart sizes"; final List<String> blockIds = new ArrayList<>(nbParts); for (int i = 0; i < nbParts; i++) { final long length = i < nbParts - 1 ? partSize : lastPartSize; Flux<ByteBuffer> byteBufferFlux = convertStreamToByteBuffer(inputStream, length, DEFAULT_UPLOAD_BUFFERS_SIZE); if (i == nbParts - 1) { byteBufferFlux.doOnComplete(() -> { try { if (inputStream.available() > 0) { long totalLength = blobSize + inputStream.available(); throw new IllegalStateException( "InputStream provided " + totalLength + " bytes, more than the expected " + totalLength + " bytes" ); } } catch (IOException e) { throw new RuntimeException(e); } }); } final String blockId = UUIDs.base64UUID(); blockBlobAsyncClient.stageBlock(blockId, byteBufferFlux, length).block(); blockIds.add(blockId); } blockBlobAsyncClient.commitBlockList(blockIds, failIfAlreadyExists == false).block(); }); } /** * Converts the provided input stream into a Flux of ByteBuffer. To avoid having large amounts of outstanding * memory this Flux reads the InputStream into ByteBuffers of {@code chunkSize}	i wonder if we should just drop this check altogether instead? it seems kind of weird to check that we drained the stream fully when we also give the size we want to write (and the check relies on the fact that the stream reports available() as well which is not guaranteed). maybe we should just drop this check and only check that currenttotallength == blobsize at the end? i think i'd prefer that option and it simplifies things?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(key); out.writeDouble(from); if (out.getVersion().onOrAfter(Version.V_8_0_0)) { out.writeOptionalDouble(originalFrom); } out.writeDouble(to); if (out.getVersion().onOrAfter(Version.V_8_0_0)) { out.writeOptionalDouble(originalTo); } out.writeVLong(docCount); aggregations.writeTo(out); }	if you merge #82435 first, you can change this value to v_7_1_7 before merging this pr
public static FormatDateTimeFormatter parseDateTimeFormatter(Object node) { if (node instanceof String) { return Joda.forPattern((String) node); } throw new IllegalArgumentException("Invalid format: [" + node.toString() + "]: expected string value"); }	instead of instance testing here i would suggest changing the method to only take a string argument (the format) and change all call sites to convert to string instead. i should be save to do so since we to it here anyway currently. that way we also don't need to throw an exception for bad formats, jodas "forpattern" will do so already.
private synchronized boolean checkSecurityEnabled() { return isSecurityEnabled(status.mode, isSecurityExplicitlyEnabled, isSecurityEnabled); }	the name of the method threw me off while initially reading the code. i don't have a good suggestion that isn't too long for a method name, but can we add a line of javadoc explaining that this checks if the current license is at least minimummode and the rest of the requirements are satisfied ?
public static License generateSignedLicense(String type, int version, long issueDate, TimeValue expiryDuration) throws Exception { long issue = (issueDate != -1L) ? issueDate : System.currentTimeMillis() - TimeValue.timeValueHours(2).getMillis(); final String licenseType; if (version < License.VERSION_NO_FEATURE_TYPE) { licenseType = randomFrom("subscription", "internal", "development"); } else { licenseType = (type != null) ? type : randomFrom("silver", "dev", "gold", "platinum"); } final License.Builder builder = License.builder() .uid(UUID.randomUUID().toString()) .version(version) .expiryDate(System.currentTimeMillis() + expiryDuration.getMillis()) .issueDate(issue) .type(licenseType ) .issuedTo("customer") .issuer("elasticsearch") .maxNodes(5); if (version == License.VERSION_START) { builder.subscriptionType((type != null) ? type : randomFrom("dev", "gold", "platinum", "silver")); builder.feature(randomAlphaOfLength(10)); } if ("enterprise".equals(licenseType)) { builder.version(License.VERSION_ENTERPRISE).maxResourceUnits(5).maxNodes(-1); } final LicenseSigner signer = new LicenseSigner(getTestPriKeyPath(), getTestPubKeyPath()); return signer.sign(builder.build()); }	nit: i think we generally prefer separate calls for builder as it is clearer to read, or at least split them in different lines. also maxresourceunits can be called with a randomintegerinrange
public void testConflictNewType() throws Exception { XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("_doc") .startObject("properties").startObject("foo").field("type", "long").endObject() .endObject().endObject().endObject(); MapperService mapperService = createIndex("test", Settings.builder().build(), mapping).mapperService(); XContentBuilder update = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("foo").field("type", "double").endObject() .endObject().endObject().endObject(); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> mapperService.merge("type", new CompressedXContent(Strings.toString(update)), MapperService.MergeReason.MAPPING_UPDATE)); assertThat(e.getMessage(), containsString("mapper [foo] of different type, current_type [long], merged_type [double]")); assertThat(((FieldMapper) mapperService.documentMapper().mapping().root().getMapper("foo")).fieldType().typeName(), equalTo("long")); }	to me the error message "mapper [foo] cannot be changed from type [long] to [double]" is a lot clearer, we could prefer that original wording. on a related note, it looks like mappedfieldtype#checktypename is now unused and can be removed.
public void testMergeConflicts() { Mapper.BuilderContext context = new Mapper.BuilderContext(SETTINGS, new ContentPath(1)); T builder1 = newBuilder(); T builder2 = newBuilder(); { FieldMapper mapper = (FieldMapper) builder1.build(context); FieldMapper toMerge = (FieldMapper) builder2.build(context); mapper.merge(toMerge); // identical mappers should merge with no issue } { FieldMapper mapper = (FieldMapper) newBuilder().build(context); FieldMapper toMerge = new FieldMapper("bogus", new MockFieldMapper.FakeFieldType(), new MockFieldMapper.FakeFieldType(), SETTINGS, FieldMapper.MultiFields.empty(), FieldMapper.CopyTo.empty()) { @Override protected void parseCreateField(ParseContext context) { } @Override protected String contentType() { return "bogustype"; } }; IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> mapper.merge(toMerge)); assertThat(e.getMessage(), containsString("different type")); assertThat(e.getMessage(), containsString("bogustype")); } for (Modifier modifier : modifiers) { builder1 = newBuilder(); builder2 = newBuilder(); modifier.apply(builder1, builder2); FieldMapper mapper = (FieldMapper) builder1.build(context); FieldMapper toMerge = (FieldMapper) builder2.build(context); if (modifier.updateable) { mapper.merge(toMerge); } else { IllegalArgumentException e = expectThrows(IllegalArgumentException.class, "Expected an error when merging property difference " + modifier.property, () -> mapper.merge(toMerge)); assertThat(e.getMessage(), containsString(modifier.property)); } } }	i think this could just be fieldmapper tomerge = new mockfieldmapper("bogus") ?
public void testEquals() { MappedFieldType ft1 = createNamedDefaultFieldType(); MappedFieldType ft2 = createNamedDefaultFieldType(); assertEquals(ft1, ft1); // reflexive assertEquals(ft1, ft2); // symmetric assertEquals(ft2, ft1); assertEquals(ft1.hashCode(), ft2.hashCode()); }	i just noticed we've lost test coverage for the equals method, which is used in fieldtypelookup. this is too bad, i wonder if we could we keep these modifiers around or switch to using equalshashcodetestutils#checkequalsandhashcode.
@Override public Filter parse(QueryParseContext parseContext) throws IOException, QueryParsingException { XContentParser parser = parseContext.parser(); FilterCachingPolicy cache = parseContext.autoFilterCachePolicy(); HashedBytesRef cacheKey = null; String fieldName = null; List<GeoPoint> shell = Lists.newArrayList(); boolean normalizeLon = false; boolean normalizeLat = false; String filterName = null; String currentFieldName = null; XContentParser.Token token; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_OBJECT) { fieldName = currentFieldName; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_ARRAY) { if (POINTS.equals(currentFieldName)) { while ((token = parser.nextToken()) != Token.END_ARRAY) { shell.add(GeoUtils.parseGeoPoint(parser)); } } else { throw new QueryParsingException(parseContext.index(), "[geo_polygon] filter does not support [" + currentFieldName + "]"); } } else { throw new QueryParsingException(parseContext.index(), "[geo_polygon] filter does not support token type [" + token.name() + "] under [" + currentFieldName + "]"); } } } else if (token.isValue()) { if ("_name".equals(currentFieldName)) { filterName = parser.text(); } else if ("_cache".equals(currentFieldName)) { cache = parseContext.parseFilterCachePolicy(); } else if ("_cache_key".equals(currentFieldName) || "_cacheKey".equals(currentFieldName)) { cacheKey = new HashedBytesRef(parser.text()); } else if ("normalize".equals(currentFieldName)) { normalizeLat = parser.booleanValue(); normalizeLon = parser.booleanValue(); } else { throw new QueryParsingException(parseContext.index(), "[geo_polygon] filter does not support [" + currentFieldName + "]"); } } else { throw new QueryParsingException(parseContext.index(), "[geo_polygon] unexpected token type [" + token.name() + "]"); } } if (shell.isEmpty()) { throw new QueryParsingException(parseContext.index(), "no points defined for geo_polygon filter"); } else { if (shell.size() < 3) { throw new QueryParsingException(parseContext.index(), "too few points defined for geo_polygon filter"); } GeoPoint start = shell.get(0); if (!start.equals(shell.get(shell.size() - 1))) { shell.add(start); } if (shell.size() < 4) { throw new QueryParsingException(parseContext.index(), "too few points defined for geo_polygon filter"); } } if (normalizeLat || normalizeLon) { for (GeoPoint point : shell) { GeoUtils.normalizePoint(point, normalizeLat, normalizeLon); } } MapperService.SmartNameFieldMappers smartMappers = parseContext.smartFieldMappers(fieldName); if (smartMappers == null || !smartMappers.hasMapper()) { throw new QueryParsingException(parseContext.index(), "failed to find geo_point field [" + fieldName + "]"); } FieldMapper<?> mapper = smartMappers.mapper(); if (!(mapper instanceof GeoPointFieldMapper)) { throw new QueryParsingException(parseContext.index(), "field [" + fieldName + "] is not a geo_point field"); } IndexGeoPointFieldData indexFieldData = parseContext.getForField(mapper); Filter filter = new GeoPolygonFilter(indexFieldData, shell.toArray(new GeoPoint[shell.size()])); if (cache != null) { filter = parseContext.cacheFilter(filter, cacheKey, cache); } filter = wrapSmartNameFilter(filter, smartMappers, parseContext); if (filterName != null) { parseContext.addNamedFilter(filterName, filter); } return filter; }	while it doesn't look like the normalize setting is currently documented for geo poly filter, it probably should be? also, changing this here means the default for the setting would be different for geo poly vs geo point...that seems like it might be confusing for users?
private static boolean pointInPolygon(GeoPoint[] points, double lat, double lon) { boolean inPoly = false; double lat0, lon0, lat1, lon1; if (lon < 0) { lon += 360; } if (lat <0) { lat += 180; } for (int i = 1; i < points.length; i++) { lon0 = (points[i-1].lon() < 0) ? points[i-1].lon() + 360.0 : points[i-1].lon(); lon1 = (points[i].lon() < 0) ? points[i].lon() + 360.0 : points[i].lon(); lat0 = (points[i-1].lat() < 0) ? points[i-1].lat() + 180.0 : points[i-1].lat(); lat1 = (points[i].lat() < 0) ? points[i].lat() + 180.0 : points[i].lat(); if (lon1 < lon && lon0 >= lon || lon0 < lon && lon1 >= lon) { if (lat1 + (lon - lon1) / (lon0 - lon1) * (lat0 - lat1) < lat) { inPoly = !inPoly; } } } return inPoly; }	nit: space between < and 0
private static boolean pointInPolygon(GeoPoint[] points, double lat, double lon) { boolean inPoly = false; double lat0, lon0, lat1, lon1; if (lon < 0) { lon += 360; } if (lat <0) { lat += 180; } for (int i = 1; i < points.length; i++) { lon0 = (points[i-1].lon() < 0) ? points[i-1].lon() + 360.0 : points[i-1].lon(); lon1 = (points[i].lon() < 0) ? points[i].lon() + 360.0 : points[i].lon(); lat0 = (points[i-1].lat() < 0) ? points[i-1].lat() + 180.0 : points[i-1].lat(); lat1 = (points[i].lat() < 0) ? points[i].lat() + 180.0 : points[i].lat(); if (lon1 < lon && lon0 >= lon || lon0 < lon && lon1 >= lon) { if (lat1 + (lon - lon1) / (lon0 - lon1) * (lat0 - lat1) < lat) { inPoly = !inPoly; } } } return inPoly; }	instead of reapplying the logic for the previous element on every iteration, you could initialize lon0 and lat0 to points[0] before the loop, and then add lon0 = lon1; lat0 = lat1; at the end of the loop?
private static boolean pointInPolygon(GeoPoint[] points, double lat, double lon) { boolean inPoly = false; double lat0, lon0, lat1, lon1; if (lon < 0) { lon += 360; } if (lat <0) { lat += 180; } for (int i = 1; i < points.length; i++) { lon0 = (points[i-1].lon() < 0) ? points[i-1].lon() + 360.0 : points[i-1].lon(); lon1 = (points[i].lon() < 0) ? points[i].lon() + 360.0 : points[i].lon(); lat0 = (points[i-1].lat() < 0) ? points[i-1].lat() + 180.0 : points[i-1].lat(); lat1 = (points[i].lat() < 0) ? points[i].lat() + 180.0 : points[i].lat(); if (lon1 < lon && lon0 >= lon || lon0 < lon && lon1 >= lon) { if (lat1 + (lon - lon1) / (lon0 - lon1) * (lat0 - lat1) < lat) { inPoly = !inPoly; } } } return inPoly; }	while i realize this is the same logic that existed before, it would be nice to add a short comment explaining the flip/flop logic here for determining whether the point is ultimately in the polygon (it took me a bit to think through).
private static Object deepCopy(Object value) { if (value instanceof Map) { Map<?, ?> mapValue = (Map<?, ?>) value; Map<Object, Object> copy = new HashMap<>(mapValue.size()); for (Map.Entry<?, ?> entry : mapValue.entrySet()) { copy.put(entry.getKey(), deepCopy(entry.getValue())); } return copy; } else if (value instanceof List) { List<?> listValue = (List<?>) value; List<Object> copy = new ArrayList<>(listValue.size()); for (Object itemValue : listValue) { copy.add(deepCopy(itemValue)); } return copy; } else if (value instanceof byte[]) { byte[] bytes = (byte[]) value; return Arrays.copyOf(bytes, bytes.length); } else if (value == null || value instanceof String || value instanceof Integer || value instanceof Long || value instanceof Float || value instanceof Double || value instanceof Boolean) { return value; } else if (value instanceof Date) { return ((Date) value).clone(); } else if (value instanceof ZonedDateTime) { ZonedDateTime zonedDateTime = (ZonedDateTime) value; return ZonedDateTime.ofInstant(zonedDateTime.toLocalDateTime(), zonedDateTime.getOffset(), zonedDateTime.getZone()); } else { throw new IllegalArgumentException("unexpected value type [" + value.getClass() + "]"); } }	tbh i'm wondering whether this could just use zoneddatetime directly, since it is supposed to have value semantics. put differently, why not also deep-copy the localdatetime and zoneid and so on?
@Override protected void setupFieldType(BuilderContext context) { super.setupFieldType(context); DateFormatter formatter = fieldType().dateTimeFormatter; if (fieldType().rangeType == RangeType.DATE) { if (Strings.hasLength(builder.pattern) && Objects.equals(builder.pattern, formatter.pattern()) == false || Objects.equals(builder.locale, formatter.locale()) == false) { fieldType().setDateTimeFormatter(DateFormatters.forPattern(pattern).withLocale(locale)); } } else if (pattern != null) { throw new IllegalArgumentException("field [" + name() + "] of type [" + fieldType().rangeType + "] should not define a dateTimeFormatter unless it is a " + RangeType.DATE + " type"); } }	just the matter of style i guess, but the expression here ended up pretty long we might as well extract and call it haspatternchanged ?
@Override public void onFailure(Throwable t) { onSecondPhaseFailure(t, querySearchRequest, shardIndex, dfsResult, counter); // the query might not have been executed at all (for example because thread pool rejected execution) // and the search context that was created in dfs phase might not be released. // release it again to be in the safe side sendReleaseSearchContext(querySearchRequest.id(), node); }	even if onsecondphasefailure is not supposed to throw exceptions, should we put onsecondphasefailure in a try block and sendreleasesearchcontext in the finally to be on the safe side?
@Override public final boolean decRef() { touch(); int i = refCount.decrementAndGet(); assert i >= 0; if (i == 0) { try { closeInternal(); } catch (Exception e) { assert false : e; throw e; } return true; } return false; }	can we add javadoc to closeinternal to explain that it must handle all exceptions itself, i.e. can never throw?
public List<Object> fetchValues(SourceLookup lookup) throws IOException { List<Object> nestedEntriesToReturn = new ArrayList<>(); Map<String, Object> filteredSource = new HashMap<>(); Map<String, Object> stub = createSourceMapStub(filteredSource); List<?> nestedValues = XContentMapValues.extractNestedValue(nestedFieldPath, lookup.source()); if (nestedValues == null) { return Collections.emptyList(); } for (Object entry : nestedValues) { // add this one entry only to the stub and use this as source lookup stub.put(nestedFieldName, entry); SourceLookup nestedSourceLookup = new SourceLookup() { @Override public int docId() { // TODO instead of the original docId from the main doc, we probably need the ones for nested docs here // but this at least positions the docId somewhere other than -1, which would lead to NPE later return lookup.docId(); } }; nestedSourceLookup.setSource(filteredSource); Map<String, DocumentField> fetchResult = nestedFieldFetcher.fetch(nestedSourceLookup); Map<String, Object> nestedEntry = new HashMap<>(); for (DocumentField field : fetchResult.values()) { List<Object> fetchValues = field.getValues(); if (fetchValues.isEmpty() == false) { String keyInNestedMap = field.getName().substring(nestedFieldPath.length() + 1); nestedEntry.put(keyInNestedMap, fetchValues); } } if (nestedEntry.isEmpty() == false) { nestedEntriesToReturn.add(nestedEntry); } } return nestedEntriesToReturn; }	i was able to remove this override and the test still passes. maybe it's necessary in a slightly different set-up? in any case, i understand wanting to avoid passing a nonsensical doc id here. what would you think of using the typical method to initialize sourcelookup, which feels more robust: sourcelookup nestedsourcelookup = new sourcelookup(); nestedsourcelookup.setsegmentanddocument(context, lookup.docid()); nestedsourcelookup.setsource(filteredsource); it would require hanging onto the leafreadercontext supplied in setnextreader, but this doesn't seem too bad at all.
public Builder localNodeId(String localNodeId) { this.localNodeId = localNodeId; return this; } /** * Checks that a node can be safely added to this node collection. * * @return null if all is OK or an error message explaining why a node can not be added. * * Note: if this method returns a non-null value, calling {@link #put(DiscoveryNode)}	preflight is the weirdest word and does not convey anything meaningful to me in the context of this class. maybe call it checksafeadd?
@Override public void startInitialJoin() { synchronized (clusterGroups) { ClusterGroup clusterGroup = clusterGroups.get(clusterName); if (clusterGroup == null) { clusterGroup = new ClusterGroup(); clusterGroups.put(clusterName, clusterGroup); } logger.debug("Connected to cluster [{}]", clusterName); Optional<LocalDiscovery> current = clusterGroup.members().stream().filter(other -> other.localNode().equals(this.localNode())) .findFirst(); if (current.isPresent()) { throw new IllegalStateException("current cluster group already contains a node with the same id. current " + current.get().localNode() + ", this node " + localNode()); } clusterGroup.members().add(this); LocalDiscovery firstMaster = null; for (LocalDiscovery localDiscovery : clusterGroup.members()) { if (localDiscovery.localNode().isMasterNode()) { firstMaster = localDiscovery; break; } } if (firstMaster != null && firstMaster.equals(this)) { // we are the first master (and the master) master = true; final LocalDiscovery master = firstMaster; clusterService.submitStateUpdateTask("local-disco-initial_connect(master)", new ClusterStateUpdateTask() { @Override public boolean runOnlyOnMaster() { return false; } @Override public ClusterState execute(ClusterState currentState) { DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder(); for (LocalDiscovery discovery : clusterGroups.get(clusterName).members()) { nodesBuilder.put(discovery.localNode()); } nodesBuilder.localNodeId(master.localNode().getId()).masterNodeId(master.localNode().getId()); // remove the NO_MASTER block in this case ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks()).removeGlobalBlock(discoverySettings.getNoMasterBlock()); return ClusterState.builder(currentState).nodes(nodesBuilder).blocks(blocks).build(); } @Override public void onFailure(String source, Throwable t) { logger.error("unexpected failure during [{}]", t, source); } }); } else if (firstMaster != null) { // tell the master to send the fact that we are here final LocalDiscovery master = firstMaster; firstMaster.clusterService.submitStateUpdateTask("local-disco-receive(from node[" + localNode() + "])", new ClusterStateUpdateTask() { @Override public boolean runOnlyOnMaster() { return false; } @Override public ClusterState execute(ClusterState currentState) { DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder(); for (LocalDiscovery discovery : clusterGroups.get(clusterName).members()) { nodesBuilder.put(discovery.localNode()); } nodesBuilder.localNodeId(master.localNode().getId()).masterNodeId(master.localNode().getId()); return ClusterState.builder(currentState).nodes(nodesBuilder).build(); } @Override public void onFailure(String source, Throwable t) { logger.error("unexpected failure during [{}]", t, source); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { // we reroute not in the same cluster state update since in certain areas we rely on // the node to be in the cluster state (sampled from ClusterService#state) to be there, also // shard transitions need to better be handled in such cases master.routingService.reroute("post_node_add"); } }); } } // else, no master node, the next node that will start will fill things in... }	this only checks that it does not have the same ephemeral id, not node id?
public BatchResult<DiscoveryNode> execute(ClusterState currentState, List<DiscoveryNode> joiningNodes) throws Exception { final DiscoveryNodes currentNodes = currentState.nodes(); final BatchResult.Builder<DiscoveryNode> results = BatchResult.builder(); boolean nodesChanged = false; ClusterState.Builder newState = ClusterState.builder(currentState); DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder(currentNodes); if (currentNodes.getMasterNode() == null && joiningNodes.contains(BECOME_MASTER_TASK)) { // use these joins to try and become the master. // Note that we don't have to do any validation of the amount of joining nodes - the commit // during the cluster state publishing guarantees that we have enough nodesBuilder.masterNodeId(currentNodes.getLocalNodeId()); ClusterBlocks clusterBlocks = ClusterBlocks.builder().blocks(currentState.blocks()) .removeGlobalBlock(discoverySettings.getNoMasterBlock()).build(); newState.blocks(clusterBlocks); newState.nodes(nodesBuilder); nodesChanged = true; // reroute now to remove any dead nodes (master may have stepped down when they left and didn't update the routing table) // Note: also do it now to avoid assigning shards to these nodes. We will have another reroute after the cluster // state is published. // TODO: this publishing of a cluster state with no nodes assigned to joining nodes shouldn't be needed anymore. remove. final ClusterState tmpState = newState.build(); RoutingAllocation.Result result = routingService.getAllocationService().reroute(tmpState, "nodes joined"); newState = ClusterState.builder(tmpState); if (result.changed()) { newState.routingResult(result); } nodesBuilder = DiscoveryNodes.builder(tmpState.nodes()); } if (nodesBuilder.isLocalNodeElectedMaster() == false) { logger.trace("processing node joins, but we are not the master. current master: {}", currentNodes.getMasterNode()); throw new NotMasterException("Node [" + currentNodes.getLocalNode() + "] not master for join request"); } for (final DiscoveryNode node : joiningNodes) { if (node.equals(BECOME_MASTER_TASK) || node.equals(FINISH_ELECTION_NOT_MASTER_TASK)) { // noop } else if (currentNodes.nodeExists(node)) { logger.debug("received a join request for an existing node [{}]", node); } else { try { nodesBuilder.put(node); nodesChanged = true; } catch (IllegalArgumentException e) { results.failure(node, e); continue; } } results.success(node); } if (nodesChanged) { newState.nodes(nodesBuilder); } // we must return a new cluster state instance to force publishing. This is important // for the joining node to finalize its join and set us as a master return results.build(newState.build()); }	we should do some debug logging here.
private static NodeMetaData loadOrCreateNodeMetaData(Settings settings, ESLogger logger, NodePath... nodePaths) throws IOException { List<Path> pathList = Arrays.stream(nodePaths).map(np -> np.path).collect(Collectors.toList()); final Path[] paths = pathList.toArray(new Path[pathList.size()]); NodeMetaData metaData = NodeMetaData.FORMAT.loadLatestState(logger, paths); if (metaData == null) { metaData = new NodeMetaData(generateNodeId(settings)); } // we write again to make sure all paths have the latest state file NodeMetaData.FORMAT.write(metaData, paths); return metaData; }	the above two lines can be simplified to final path[] paths = arrays.stream(nodepaths).map(np -> np.path).toarray(path[]::new);
public final T read(Path file) throws IOException { try (Directory dir = newDirectory(file.getParent())) { try (final IndexInput indexInput = dir.openInput(file.getFileName().toString(), IOContext.DEFAULT)) { // We checksum the entire file before we even go and parse it. If it's corrupted we barf right here. CodecUtil.checksumEntireFile(indexInput); final int fileVersion = CodecUtil.checkHeader(indexInput, STATE_FILE_CODEC, MIN_COMPATIBLE_STATE_FILE_VERSION, STATE_FILE_VERSION); final XContentType xContentType = XContentType.values()[indexInput.readInt()]; if (fileVersion == 0) { // format version 0, wrote a version that always came from the content state file and was never used indexInput.readLong(); // version currently unused } long filePointer = indexInput.getFilePointer(); long contentSize = indexInput.length() - CodecUtil.footerLength() - filePointer; try (IndexInput slice = indexInput.slice("state_xcontent", filePointer, contentSize)) { try (XContentParser parser = XContentFactory.xContent(xContentType).createParser(new InputStreamIndexInput(slice, contentSize))) { return fromXContent(parser); } } } catch(CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) { // we trick this into a dedicated exception with the original stacktrace throw new CorruptStateException(ex); } } }	magic constant. would be useful to know which es versions used this constant, see https://github.com/elastic/elasticsearch/pull/17987/files#diff-35f0f7b57ab6d12add07c462b761253br67
public void testDeltas() { Set<DiscoveryNode> nodesA = new HashSet<>(); nodesA.addAll(randomNodes(1 + randomInt(10))); Set<DiscoveryNode> nodesB = new HashSet<>(); nodesB.addAll(randomNodes(1 + randomInt(5))); for (DiscoveryNode node : randomSubsetOf(nodesA)) { if (randomBoolean()) { // change an attribute Map<String, String> attrs = new HashMap<>(node.getAttributes()); attrs.put("new", "new"); node = new DiscoveryNode(node.getName(), node.getId(), node.getAddress(), attrs, node.getRoles(), node.getVersion()); } nodesB.add(node); } DiscoveryNode masterA = randomBoolean() ? null : RandomPicks.randomFrom(random(), nodesA); DiscoveryNode masterB = randomBoolean() ? null : RandomPicks.randomFrom(random(), nodesB); DiscoveryNodes.Builder builderA = DiscoveryNodes.builder(); nodesA.stream().forEach(builderA::put); final String masterAId = masterA == null ? null : masterA.getId(); builderA.masterNodeId(masterAId); builderA.localNodeId(RandomPicks.randomFrom(random(), nodesA).getId()); DiscoveryNodes.Builder builderB = DiscoveryNodes.builder(); nodesB.stream().forEach(builderB::put); final String masterBId = masterB == null ? null : masterB.getId(); builderB.masterNodeId(masterBId); builderB.localNodeId(RandomPicks.randomFrom(random(), nodesB).getId()); final DiscoveryNodes discoNodesA = builderA.build(); final DiscoveryNodes discoNodesB = builderB.build(); logger.info("nodes A: {}", discoNodesA.prettyPrint()); logger.info("nodes B: {}", discoNodesB.prettyPrint()); DiscoveryNodes.Delta delta = discoNodesB.delta(discoNodesA); if (masterB == null || Objects.equals(masterAId, masterBId)) { assertFalse(delta.masterNodeChanged()); assertThat(delta.previousMasterNode(), nullValue()); assertThat(delta.newMasterNode(), nullValue()); } else { assertTrue(delta.masterNodeChanged()); assertThat(delta.newMasterNode().getId(), equalTo(masterBId)); assertThat(delta.previousMasterNode() != null ? delta.previousMasterNode().getId() : null, equalTo(masterAId)); } Set<DiscoveryNode> newNodes = new HashSet<>(nodesB); newNodes.removeAll(nodesA); assertThat(delta.added(), equalTo(newNodes.isEmpty() == false)); assertThat(delta.addedNodes(), containsInAnyOrder(newNodes.stream().collect(Collectors.toList()).toArray())); assertThat(delta.addedNodes().size(), equalTo(newNodes.size())); Set<DiscoveryNode> removedNodes = new HashSet<>(nodesA); removedNodes.removeAll(nodesB); assertThat(delta.removed(), equalTo(removedNodes.isEmpty() == false)); assertThat(delta.removedNodes(), containsInAnyOrder(removedNodes.stream().collect(Collectors.toList()).toArray())); assertThat(delta.removedNodes().size(), equalTo(removedNodes.size())); }	here you also generate a new ephemeral id, probably not intended...
public void handle(final HttpExchange exchange) throws IOException { final String request = exchange.getRequestMethod() + " " + exchange.getRequestURI().toString(); try { if (Regex.simpleMatch("PUT /bucket/*", request)) { blobs.put(exchange.getRequestURI().toString(), Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1); } else if (Regex.simpleMatch("GET /bucket/?prefix=*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); assertThat("Test must be adapted for GET Bucket (List Objects) Version 2", params.get("list-type"), nullValue()); final StringBuilder list = new StringBuilder(); list.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); list.append("<ListBucketResult>"); final String prefix = params.get("prefix"); if (prefix != null) { list.append("<Prefix>").append(prefix).append("</Prefix>"); } for (Map.Entry<String, BytesReference> blob : blobs.entrySet()) { if (prefix == null || blob.getKey().startsWith("/bucket/" + prefix)) { list.append("<Contents>"); list.append("<Key>").append(blob.getKey().replace("/bucket/", "")).append("</Key>"); list.append("<Size>").append(blob.getValue().length()).append("</Size>"); list.append("</Contents>"); } } list.append("</ListBucketResult>"); byte[] response = list.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else if (Regex.simpleMatch("GET /bucket/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().toString()); if (blob != null) { exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), blob.length()); blob.writeTo(exchange.getResponseBody()); } else { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); } } else if (Regex.simpleMatch("DELETE /bucket/*", request)) { int deletions = 0; for (Iterator<Map.Entry<String, BytesReference>> iterator = blobs.entrySet().iterator(); iterator.hasNext(); ) { Map.Entry<String, BytesReference> blob = iterator.next(); if (blob.getKey().startsWith(exchange.getRequestURI().toString())) { iterator.remove(); deletions++; } } exchange.sendResponseHeaders((deletions > 0 ? RestStatus.OK : RestStatus.NO_CONTENT).getStatus(), -1); } else if (Regex.simpleMatch("POST /bucket/?delete", request)) { final String requestBody = Streams.copyToString(new InputStreamReader(exchange.getRequestBody(), UTF_8)); final StringBuilder deletes = new StringBuilder(); deletes.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); deletes.append("<DeleteResult>"); for (Iterator<Map.Entry<String, BytesReference>> iterator = blobs.entrySet().iterator(); iterator.hasNext(); ) { Map.Entry<String, BytesReference> blob = iterator.next(); String key = blob.getKey().replace("/bucket/", ""); if (requestBody.contains("<Key>" + key + "</Key>")) { deletes.append("<Deleted><Key>").append(key).append("</Key></Deleted>"); iterator.remove(); } } deletes.append("</DeleteResult>"); byte[] response = deletes.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else { exchange.sendResponseHeaders(RestStatus.INTERNAL_SERVER_ERROR.getStatus(), -1); } } finally { exchange.close(); } } } /** * HTTP handler that injects random S3 service errors * * Note: it is not a good idea to allow this handler to simulates too many errors as it would * slow down the test suite and/or could trigger SDK client request throttling (and request * would fail before reaching the max retry attempts - this can be mitigated by disabling * {@link S3ClientSettings#USE_THROTTLE_RETRIES_SETTING}) */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an S3 endpoint") private class ErroneousHttpHandler implements HttpHandler { // first key is the remote address, second key is the HTTP request unique id provided by the AWS SDK client, // value is the number of times the request has been seen private final Map<String, AtomicInteger> requests; private final HttpHandler delegate; private final int maxErrorsPerRequest; private ErroneousHttpHandler(final HttpHandler delegate, final int maxErrorsPerRequest) { this.requests = new ConcurrentHashMap<>(); this.delegate = delegate; this.maxErrorsPerRequest = maxErrorsPerRequest; assert maxErrorsPerRequest > 1; } @Override public void handle(final HttpExchange exchange) throws IOException { final String requestId = exchange.getRequestHeaders().getFirst(AmazonHttpClient.HEADER_SDK_TRANSACTION_ID); assert Strings.hasText(requestId); final int count = requests.computeIfAbsent(requestId, (req) -> new AtomicInteger(0)).incrementAndGet(); if (count >= maxErrorsPerRequest || randomBoolean()) { requests.remove(requestId); delegate.handle(exchange); } else { handleAsError(exchange, requestId); } } private void handleAsError(final HttpExchange exchange, final String requestId) throws IOException { Streams.readFully(exchange.getRequestBody()); exchange.getResponseHeaders().add(Headers.REQUEST_ID, requestId); exchange.sendResponseHeaders(HttpStatus.SC_INTERNAL_SERVER_ERROR, -1); exchange.close(); }	nit: -s => simulates -> simulate
public void handle(final HttpExchange exchange) throws IOException { final String request = exchange.getRequestMethod() + " " + exchange.getRequestURI().toString(); try { if (Regex.simpleMatch("PUT /bucket/*", request)) { blobs.put(exchange.getRequestURI().toString(), Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1); } else if (Regex.simpleMatch("GET /bucket/?prefix=*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); assertThat("Test must be adapted for GET Bucket (List Objects) Version 2", params.get("list-type"), nullValue()); final StringBuilder list = new StringBuilder(); list.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); list.append("<ListBucketResult>"); final String prefix = params.get("prefix"); if (prefix != null) { list.append("<Prefix>").append(prefix).append("</Prefix>"); } for (Map.Entry<String, BytesReference> blob : blobs.entrySet()) { if (prefix == null || blob.getKey().startsWith("/bucket/" + prefix)) { list.append("<Contents>"); list.append("<Key>").append(blob.getKey().replace("/bucket/", "")).append("</Key>"); list.append("<Size>").append(blob.getValue().length()).append("</Size>"); list.append("</Contents>"); } } list.append("</ListBucketResult>"); byte[] response = list.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else if (Regex.simpleMatch("GET /bucket/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().toString()); if (blob != null) { exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), blob.length()); blob.writeTo(exchange.getResponseBody()); } else { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); } } else if (Regex.simpleMatch("DELETE /bucket/*", request)) { int deletions = 0; for (Iterator<Map.Entry<String, BytesReference>> iterator = blobs.entrySet().iterator(); iterator.hasNext(); ) { Map.Entry<String, BytesReference> blob = iterator.next(); if (blob.getKey().startsWith(exchange.getRequestURI().toString())) { iterator.remove(); deletions++; } } exchange.sendResponseHeaders((deletions > 0 ? RestStatus.OK : RestStatus.NO_CONTENT).getStatus(), -1); } else if (Regex.simpleMatch("POST /bucket/?delete", request)) { final String requestBody = Streams.copyToString(new InputStreamReader(exchange.getRequestBody(), UTF_8)); final StringBuilder deletes = new StringBuilder(); deletes.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); deletes.append("<DeleteResult>"); for (Iterator<Map.Entry<String, BytesReference>> iterator = blobs.entrySet().iterator(); iterator.hasNext(); ) { Map.Entry<String, BytesReference> blob = iterator.next(); String key = blob.getKey().replace("/bucket/", ""); if (requestBody.contains("<Key>" + key + "</Key>")) { deletes.append("<Deleted><Key>").append(key).append("</Key></Deleted>"); iterator.remove(); } } deletes.append("</DeleteResult>"); byte[] response = deletes.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else { exchange.sendResponseHeaders(RestStatus.INTERNAL_SERVER_ERROR.getStatus(), -1); } } finally { exchange.close(); } } } /** * HTTP handler that injects random S3 service errors * * Note: it is not a good idea to allow this handler to simulates too many errors as it would * slow down the test suite and/or could trigger SDK client request throttling (and request * would fail before reaching the max retry attempts - this can be mitigated by disabling * {@link S3ClientSettings#USE_THROTTLE_RETRIES_SETTING}) */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an S3 endpoint") private class ErroneousHttpHandler implements HttpHandler { // first key is the remote address, second key is the HTTP request unique id provided by the AWS SDK client, // value is the number of times the request has been seen private final Map<String, AtomicInteger> requests; private final HttpHandler delegate; private final int maxErrorsPerRequest; private ErroneousHttpHandler(final HttpHandler delegate, final int maxErrorsPerRequest) { this.requests = new ConcurrentHashMap<>(); this.delegate = delegate; this.maxErrorsPerRequest = maxErrorsPerRequest; assert maxErrorsPerRequest > 1; } @Override public void handle(final HttpExchange exchange) throws IOException { final String requestId = exchange.getRequestHeaders().getFirst(AmazonHttpClient.HEADER_SDK_TRANSACTION_ID); assert Strings.hasText(requestId); final int count = requests.computeIfAbsent(requestId, (req) -> new AtomicInteger(0)).incrementAndGet(); if (count >= maxErrorsPerRequest || randomBoolean()) { requests.remove(requestId); delegate.handle(exchange); } else { handleAsError(exchange, requestId); } } private void handleAsError(final HttpExchange exchange, final String requestId) throws IOException { Streams.readFully(exchange.getRequestBody()); exchange.getResponseHeaders().add(Headers.REQUEST_ID, requestId); exchange.sendResponseHeaders(HttpStatus.SC_INTERNAL_SERVER_ERROR, -1); exchange.close(); }	nit: can be static
public void handle(final HttpExchange exchange) throws IOException { final String request = exchange.getRequestMethod() + " " + exchange.getRequestURI().toString(); try { if (Regex.simpleMatch("PUT /bucket/*", request)) { blobs.put(exchange.getRequestURI().toString(), Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1); } else if (Regex.simpleMatch("GET /bucket/?prefix=*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); assertThat("Test must be adapted for GET Bucket (List Objects) Version 2", params.get("list-type"), nullValue()); final StringBuilder list = new StringBuilder(); list.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); list.append("<ListBucketResult>"); final String prefix = params.get("prefix"); if (prefix != null) { list.append("<Prefix>").append(prefix).append("</Prefix>"); } for (Map.Entry<String, BytesReference> blob : blobs.entrySet()) { if (prefix == null || blob.getKey().startsWith("/bucket/" + prefix)) { list.append("<Contents>"); list.append("<Key>").append(blob.getKey().replace("/bucket/", "")).append("</Key>"); list.append("<Size>").append(blob.getValue().length()).append("</Size>"); list.append("</Contents>"); } } list.append("</ListBucketResult>"); byte[] response = list.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else if (Regex.simpleMatch("GET /bucket/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().toString()); if (blob != null) { exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), blob.length()); blob.writeTo(exchange.getResponseBody()); } else { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); } } else if (Regex.simpleMatch("DELETE /bucket/*", request)) { int deletions = 0; for (Iterator<Map.Entry<String, BytesReference>> iterator = blobs.entrySet().iterator(); iterator.hasNext(); ) { Map.Entry<String, BytesReference> blob = iterator.next(); if (blob.getKey().startsWith(exchange.getRequestURI().toString())) { iterator.remove(); deletions++; } } exchange.sendResponseHeaders((deletions > 0 ? RestStatus.OK : RestStatus.NO_CONTENT).getStatus(), -1); } else if (Regex.simpleMatch("POST /bucket/?delete", request)) { final String requestBody = Streams.copyToString(new InputStreamReader(exchange.getRequestBody(), UTF_8)); final StringBuilder deletes = new StringBuilder(); deletes.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); deletes.append("<DeleteResult>"); for (Iterator<Map.Entry<String, BytesReference>> iterator = blobs.entrySet().iterator(); iterator.hasNext(); ) { Map.Entry<String, BytesReference> blob = iterator.next(); String key = blob.getKey().replace("/bucket/", ""); if (requestBody.contains("<Key>" + key + "</Key>")) { deletes.append("<Deleted><Key>").append(key).append("</Key></Deleted>"); iterator.remove(); } } deletes.append("</DeleteResult>"); byte[] response = deletes.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else { exchange.sendResponseHeaders(RestStatus.INTERNAL_SERVER_ERROR.getStatus(), -1); } } finally { exchange.close(); } } } /** * HTTP handler that injects random S3 service errors * * Note: it is not a good idea to allow this handler to simulates too many errors as it would * slow down the test suite and/or could trigger SDK client request throttling (and request * would fail before reaching the max retry attempts - this can be mitigated by disabling * {@link S3ClientSettings#USE_THROTTLE_RETRIES_SETTING}) */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an S3 endpoint") private class ErroneousHttpHandler implements HttpHandler { // first key is the remote address, second key is the HTTP request unique id provided by the AWS SDK client, // value is the number of times the request has been seen private final Map<String, AtomicInteger> requests; private final HttpHandler delegate; private final int maxErrorsPerRequest; private ErroneousHttpHandler(final HttpHandler delegate, final int maxErrorsPerRequest) { this.requests = new ConcurrentHashMap<>(); this.delegate = delegate; this.maxErrorsPerRequest = maxErrorsPerRequest; assert maxErrorsPerRequest > 1; } @Override public void handle(final HttpExchange exchange) throws IOException { final String requestId = exchange.getRequestHeaders().getFirst(AmazonHttpClient.HEADER_SDK_TRANSACTION_ID); assert Strings.hasText(requestId); final int count = requests.computeIfAbsent(requestId, (req) -> new AtomicInteger(0)).incrementAndGet(); if (count >= maxErrorsPerRequest || randomBoolean()) { requests.remove(requestId); delegate.handle(exchange); } else { handleAsError(exchange, requestId); } } private void handleAsError(final HttpExchange exchange, final String requestId) throws IOException { Streams.readFully(exchange.getRequestBody()); exchange.getResponseHeaders().add(Headers.REQUEST_ID, requestId); exchange.sendResponseHeaders(HttpStatus.SC_INTERNAL_SERVER_ERROR, -1); exchange.close(); }	nit: brackets around req are redundant
@Override public SnapshotInfo getSnapshotInfo(SnapshotId snapshotId) { assert LATEST.equals(snapshotId.getUUID()) : "RemoteClusterRepository only supports _latest_ as the UUID"; assert LATEST.equals(snapshotId.getName()) : "RemoteClusterRepository only supports _latest_ as the name"; Client remoteClient = client.getRemoteClusterClient(remoteClusterAlias); ClusterStateResponse response = remoteClient.admin().cluster().prepareState().clear().setMetaData(true).setNodes(true).get(); ImmutableOpenMap<String, IndexMetaData> indicesMap = response.getState().metaData().indices(); ArrayList<String> indices = new ArrayList<>(indicesMap.size()); indicesMap.keysIt().forEachRemaining(indices::add); Iterator<DiscoveryNode> dataNodes = response.getState().getNodes().getDataNodes().valuesIt(); if (dataNodes.hasNext() == false) { throw new IllegalStateException("Leader cluster has no data nodes in cluster state."); } Version maxVersion = dataNodes.next().getVersion(); while (dataNodes.hasNext()) { Version nodeVersion = dataNodes.next().getVersion(); if (nodeVersion.after(maxVersion)) { maxVersion = nodeVersion; } } return new SnapshotInfo(snapshotId, indices, SnapshotState.SUCCESS, maxVersion); }	for simplicity perhaps just discoverynodes.maxnonclientnodeversion. it also takes master nodes into account, but that's fine i think. saves us the extra code here
private void rangeCheck(Translog.Operation op) { if (op != null) { final long expectedSeqNo = lastSeenSeqNo + 1; if (op.seqNo() != expectedSeqNo) { throw new IllegalStateException("Not all operations between min_seqno [" + fromSeqNo + "] " + "and max_seqno [" + toSeqNo + "] found; expected seqno [" + expectedSeqNo + "]; found [" + op + "]"); } } }	i think you need to adapt this message
public void setFollowIndex(String followIndex) { this.followIndex = followIndex; }	also all the getters here aren't used - remove?
@Override protected AllocatedPersistentTask createTask(long id, String type, String action, TaskId parentTaskId, PersistentTasksCustomMetaData.PersistentTask<ShardFollowTask> taskInProgress, Map<String, String> headers) { ShardFollowTask params = taskInProgress.getParams(); logger.info("{} Creating node task to track leader shard {}, params [{}]", params.getFollowShardId(), params.getLeaderShardId(), params); final Client leaderClient; if (params.getLeaderClusterAlias() != null) { leaderClient = wrapClient(client.getRemoteClusterClient(params.getLeaderClusterAlias()), params); } else { leaderClient = wrapClient(client, params); } Client followerClient = wrapClient(client, params); BiConsumer<TimeValue, Runnable> scheduler = (delay, command) -> threadPool.schedule(delay, Ccr.CCR_THREAD_POOL_NAME, command); return new ShardFollowNodeTask(id, type, action, getDescription(taskInProgress), parentTaskId, headers, params, scheduler) { @Override protected void updateMapping(LongConsumer handler) { Index leaderIndex = params.getLeaderShardId().getIndex(); Index followIndex = params.getFollowShardId().getIndex(); ClusterStateRequest clusterStateRequest = new ClusterStateRequest(); clusterStateRequest.clear(); clusterStateRequest.metaData(true); clusterStateRequest.indices(leaderIndex.getName()); leaderClient.admin().cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> { IndexMetaData indexMetaData = clusterStateResponse.getState().metaData().getIndexSafe(leaderIndex); assert indexMetaData.getMappings().size() == 1 : "expected exactly one mapping, but got [" + indexMetaData.getMappings().size() + "]"; MappingMetaData mappingMetaData = indexMetaData.getMappings().iterator().next().value; PutMappingRequest putMappingRequest = new PutMappingRequest(followIndex.getName()); putMappingRequest.type(mappingMetaData.type()); putMappingRequest.source(mappingMetaData.source().string(), XContentType.JSON); followerClient.admin().indices().putMapping(putMappingRequest, ActionListener.wrap( putMappingResponse -> handler.accept(indexMetaData.getVersion()), e -> handleFailure(e, () -> updateMapping(handler)))); }, e -> handleFailure(e, () -> updateMapping(handler)))); } @Override protected void innerSendBulkShardOperationsRequest(Translog.Operation[] operations, LongConsumer handler, Consumer<Exception> errorHandler) { final BulkShardOperationsRequest request = new BulkShardOperationsRequest(params.getFollowShardId(), operations); followerClient.execute(BulkShardOperationsAction.INSTANCE, request, ActionListener.wrap(response -> handler.accept(response.getLocalCheckpoint()), errorHandler)); } @Override protected void innerSendShardChangesRequest(long from, long size, Consumer<ShardChangesAction.Response> handler, Consumer<Exception> errorHandler) { ShardChangesAction.Request request = new ShardChangesAction.Request(params.getLeaderShardId()); request.setFromSeqNo(from); request.setMaxOperationCount(size); request.setMaxOperationSizeInBytes(params.getMaxOperationSizeInBytes()); leaderClient.execute(ShardChangesAction.INSTANCE, request, ActionListener.wrap(handler::accept, errorHandler)); } }; }	i think this log line can be removed? we log on task start?
@Override protected AllocatedPersistentTask createTask(long id, String type, String action, TaskId parentTaskId, PersistentTasksCustomMetaData.PersistentTask<ShardFollowTask> taskInProgress, Map<String, String> headers) { ShardFollowTask params = taskInProgress.getParams(); logger.info("{} Creating node task to track leader shard {}, params [{}]", params.getFollowShardId(), params.getLeaderShardId(), params); final Client leaderClient; if (params.getLeaderClusterAlias() != null) { leaderClient = wrapClient(client.getRemoteClusterClient(params.getLeaderClusterAlias()), params); } else { leaderClient = wrapClient(client, params); } Client followerClient = wrapClient(client, params); BiConsumer<TimeValue, Runnable> scheduler = (delay, command) -> threadPool.schedule(delay, Ccr.CCR_THREAD_POOL_NAME, command); return new ShardFollowNodeTask(id, type, action, getDescription(taskInProgress), parentTaskId, headers, params, scheduler) { @Override protected void updateMapping(LongConsumer handler) { Index leaderIndex = params.getLeaderShardId().getIndex(); Index followIndex = params.getFollowShardId().getIndex(); ClusterStateRequest clusterStateRequest = new ClusterStateRequest(); clusterStateRequest.clear(); clusterStateRequest.metaData(true); clusterStateRequest.indices(leaderIndex.getName()); leaderClient.admin().cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> { IndexMetaData indexMetaData = clusterStateResponse.getState().metaData().getIndexSafe(leaderIndex); assert indexMetaData.getMappings().size() == 1 : "expected exactly one mapping, but got [" + indexMetaData.getMappings().size() + "]"; MappingMetaData mappingMetaData = indexMetaData.getMappings().iterator().next().value; PutMappingRequest putMappingRequest = new PutMappingRequest(followIndex.getName()); putMappingRequest.type(mappingMetaData.type()); putMappingRequest.source(mappingMetaData.source().string(), XContentType.JSON); followerClient.admin().indices().putMapping(putMappingRequest, ActionListener.wrap( putMappingResponse -> handler.accept(indexMetaData.getVersion()), e -> handleFailure(e, () -> updateMapping(handler)))); }, e -> handleFailure(e, () -> updateMapping(handler)))); } @Override protected void innerSendBulkShardOperationsRequest(Translog.Operation[] operations, LongConsumer handler, Consumer<Exception> errorHandler) { final BulkShardOperationsRequest request = new BulkShardOperationsRequest(params.getFollowShardId(), operations); followerClient.execute(BulkShardOperationsAction.INSTANCE, request, ActionListener.wrap(response -> handler.accept(response.getLocalCheckpoint()), errorHandler)); } @Override protected void innerSendShardChangesRequest(long from, long size, Consumer<ShardChangesAction.Response> handler, Consumer<Exception> errorHandler) { ShardChangesAction.Request request = new ShardChangesAction.Request(params.getLeaderShardId()); request.setFromSeqNo(from); request.setMaxOperationCount(size); request.setMaxOperationSizeInBytes(params.getMaxOperationSizeInBytes()); leaderClient.execute(ShardChangesAction.INSTANCE, request, ActionListener.wrap(handler::accept, errorHandler)); } }; }	do we really need a thread pool now? can't it be generic or something? we rarely use it.
@Override protected AllocatedPersistentTask createTask(long id, String type, String action, TaskId parentTaskId, PersistentTasksCustomMetaData.PersistentTask<ShardFollowTask> taskInProgress, Map<String, String> headers) { ShardFollowTask params = taskInProgress.getParams(); logger.info("{} Creating node task to track leader shard {}, params [{}]", params.getFollowShardId(), params.getLeaderShardId(), params); final Client leaderClient; if (params.getLeaderClusterAlias() != null) { leaderClient = wrapClient(client.getRemoteClusterClient(params.getLeaderClusterAlias()), params); } else { leaderClient = wrapClient(client, params); } Client followerClient = wrapClient(client, params); BiConsumer<TimeValue, Runnable> scheduler = (delay, command) -> threadPool.schedule(delay, Ccr.CCR_THREAD_POOL_NAME, command); return new ShardFollowNodeTask(id, type, action, getDescription(taskInProgress), parentTaskId, headers, params, scheduler) { @Override protected void updateMapping(LongConsumer handler) { Index leaderIndex = params.getLeaderShardId().getIndex(); Index followIndex = params.getFollowShardId().getIndex(); ClusterStateRequest clusterStateRequest = new ClusterStateRequest(); clusterStateRequest.clear(); clusterStateRequest.metaData(true); clusterStateRequest.indices(leaderIndex.getName()); leaderClient.admin().cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> { IndexMetaData indexMetaData = clusterStateResponse.getState().metaData().getIndexSafe(leaderIndex); assert indexMetaData.getMappings().size() == 1 : "expected exactly one mapping, but got [" + indexMetaData.getMappings().size() + "]"; MappingMetaData mappingMetaData = indexMetaData.getMappings().iterator().next().value; PutMappingRequest putMappingRequest = new PutMappingRequest(followIndex.getName()); putMappingRequest.type(mappingMetaData.type()); putMappingRequest.source(mappingMetaData.source().string(), XContentType.JSON); followerClient.admin().indices().putMapping(putMappingRequest, ActionListener.wrap( putMappingResponse -> handler.accept(indexMetaData.getVersion()), e -> handleFailure(e, () -> updateMapping(handler)))); }, e -> handleFailure(e, () -> updateMapping(handler)))); } @Override protected void innerSendBulkShardOperationsRequest(Translog.Operation[] operations, LongConsumer handler, Consumer<Exception> errorHandler) { final BulkShardOperationsRequest request = new BulkShardOperationsRequest(params.getFollowShardId(), operations); followerClient.execute(BulkShardOperationsAction.INSTANCE, request, ActionListener.wrap(response -> handler.accept(response.getLocalCheckpoint()), errorHandler)); } @Override protected void innerSendShardChangesRequest(long from, long size, Consumer<ShardChangesAction.Response> handler, Consumer<Exception> errorHandler) { ShardChangesAction.Request request = new ShardChangesAction.Request(params.getLeaderShardId()); request.setFromSeqNo(from); request.setMaxOperationCount(size); request.setMaxOperationSizeInBytes(params.getMaxOperationSizeInBytes()); leaderClient.execute(ShardChangesAction.INSTANCE, request, ActionListener.wrap(handler::accept, errorHandler)); } }; }	can we validate the index uuid of the index we got?
@Override protected AllocatedPersistentTask createTask(long id, String type, String action, TaskId parentTaskId, PersistentTasksCustomMetaData.PersistentTask<ShardFollowTask> taskInProgress, Map<String, String> headers) { ShardFollowTask params = taskInProgress.getParams(); logger.info("{} Creating node task to track leader shard {}, params [{}]", params.getFollowShardId(), params.getLeaderShardId(), params); final Client leaderClient; if (params.getLeaderClusterAlias() != null) { leaderClient = wrapClient(client.getRemoteClusterClient(params.getLeaderClusterAlias()), params); } else { leaderClient = wrapClient(client, params); } Client followerClient = wrapClient(client, params); BiConsumer<TimeValue, Runnable> scheduler = (delay, command) -> threadPool.schedule(delay, Ccr.CCR_THREAD_POOL_NAME, command); return new ShardFollowNodeTask(id, type, action, getDescription(taskInProgress), parentTaskId, headers, params, scheduler) { @Override protected void updateMapping(LongConsumer handler) { Index leaderIndex = params.getLeaderShardId().getIndex(); Index followIndex = params.getFollowShardId().getIndex(); ClusterStateRequest clusterStateRequest = new ClusterStateRequest(); clusterStateRequest.clear(); clusterStateRequest.metaData(true); clusterStateRequest.indices(leaderIndex.getName()); leaderClient.admin().cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> { IndexMetaData indexMetaData = clusterStateResponse.getState().metaData().getIndexSafe(leaderIndex); assert indexMetaData.getMappings().size() == 1 : "expected exactly one mapping, but got [" + indexMetaData.getMappings().size() + "]"; MappingMetaData mappingMetaData = indexMetaData.getMappings().iterator().next().value; PutMappingRequest putMappingRequest = new PutMappingRequest(followIndex.getName()); putMappingRequest.type(mappingMetaData.type()); putMappingRequest.source(mappingMetaData.source().string(), XContentType.JSON); followerClient.admin().indices().putMapping(putMappingRequest, ActionListener.wrap( putMappingResponse -> handler.accept(indexMetaData.getVersion()), e -> handleFailure(e, () -> updateMapping(handler)))); }, e -> handleFailure(e, () -> updateMapping(handler)))); } @Override protected void innerSendBulkShardOperationsRequest(Translog.Operation[] operations, LongConsumer handler, Consumer<Exception> errorHandler) { final BulkShardOperationsRequest request = new BulkShardOperationsRequest(params.getFollowShardId(), operations); followerClient.execute(BulkShardOperationsAction.INSTANCE, request, ActionListener.wrap(response -> handler.accept(response.getLocalCheckpoint()), errorHandler)); } @Override protected void innerSendShardChangesRequest(long from, long size, Consumer<ShardChangesAction.Response> handler, Consumer<Exception> errorHandler) { ShardChangesAction.Request request = new ShardChangesAction.Request(params.getLeaderShardId()); request.setFromSeqNo(from); request.setMaxOperationCount(size); request.setMaxOperationSizeInBytes(params.getMaxOperationSizeInBytes()); leaderClient.execute(ShardChangesAction.INSTANCE, request, ActionListener.wrap(handler::accept, errorHandler)); } }; }	naming : size -> maxoperationcount
*/ private void markRepoCorrupted(long corruptedGeneration, Exception originalException, ActionListener<Void> listener) { assert corruptedGeneration != RepositoryData.UNKNOWN_REPO_GEN; assert bestEffortConsistency == false; clusterService.submitStateUpdateTask("mark repository corrupted [" + metadata.name() + "][" + corruptedGeneration + "]", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { final RepositoriesMetaData state = currentState.metaData().custom(RepositoriesMetaData.TYPE); final RepositoryMetaData repoState = state.repository(metadata.name()); if (repoState.generation() != corruptedGeneration) { throw new IllegalStateException("Tried to mark repo generation [" + corruptedGeneration + "] as corrupted but its state concurrently changed to [" + repoState + "]"); } return ClusterState.builder(currentState).metaData(MetaData.builder(currentState.metaData()).putCustom( RepositoriesMetaData.TYPE, state.withUpdatedGeneration( metadata.name(), RepositoryData.CORRUPTED_REPO_GEN, repoState.pendingGeneration())).build()).build(); } @Override public void onFailure(String source, Exception e) { listener.onFailure(new RepositoryException(metadata.name(), "Failed marking repository state as corrupted", ExceptionsHelper.useOrSuppress(e, originalException))); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { listener.onResponse(null); } }); }	this is a little dirty and we talked about it in other prs .. the generation here is -1 due to the weird way the repository data is loaded initially and we have to eventually set it. i'll clean that up in a follow up, for now just setting it here where it matters (it doesn't matter in the above code when serializing filteredrepositorydata to the repo) seemed the driest.
public void testOutOfRangeValues() throws IOException { final List<Triple<String, Object, String>> inputs = Arrays.asList( Triple.of("byte", "128", "is out of range for a byte"), Triple.of("short", "32768", "is out of range for a short"), Triple.of("integer", "2147483648", "For input string"), Triple.of("long", "92233720368547758080", "For input string"), Triple.of("half_float", "65504.1", "[half_float] supports only finite values"), Triple.of("float", "3.4028235E39", "[float] supports only finite values"), Triple.of("double", "1.7976931348623157E309", "[double] supports only finite values"), Triple.of("half_float", Float.NaN, "[half_float] supports only finite values"), Triple.of("float", Float.NaN, "[float] supports only finite values"), Triple.of("double", Double.NaN, "[double] supports only finite values"), Triple.of("half_float", Float.POSITIVE_INFINITY, "[half_float] supports only finite values"), Triple.of("float", Float.POSITIVE_INFINITY, "[float] supports only finite values"), Triple.of("double", Double.POSITIVE_INFINITY, "[double] supports only finite values") ); for(Triple<String, Object, String> item: inputs) { try { parseRequest(item.type, createIndexRequest(item.value)); fail("Mapper parsing exception expected for [" + item.type + "] with value [" + item.value + "]"); } catch (MapperParsingException e) { assertThat("Incorrect error message for [" + item.type + "] with value [" + item.value + "]", e.getCause().getMessage(), containsString(item.message)); } } }	instead of this being a generic class with a generic name can we call this outofrangespec, remove the generic arguments and instead use k -> string, v -> number, m -> string. also i think it would be ok for you to declare this class here and then reuse it in the numberfieldtypetests below instead of re-defining it
private <T extends TransportRequest> void handleRequest(TcpChannel channel, Header header, InboundMessage message) throws IOException { final String action = header.getActionName(); final long requestId = header.getRequestId(); final Version version = header.getVersion(); if (header.isHandshake()) { messageListener.onRequestReceived(requestId, action); // Cannot short circuit handshakes assert message.isShortCircuit() == false; final StreamInput stream = namedWriteableStream(message.openOrGetStreamInput()); assertRemoteVersion(stream, header.getVersion()); final TransportChannel transportChannel = new TcpTransportChannel(outboundHandler, channel, action, requestId, version, header.isCompressed(), header.isHandshake(), message.takeBreakerReleaseControl()); try { handshaker.handleHandshake(transportChannel, requestId, stream); } catch (Exception e) { if (Version.CURRENT.isCompatible(header.getVersion())) { sendErrorResponse(action, transportChannel, e); } else { logger.warn(new ParameterizedMessage( "could not send error response to handshake received on [{}] using wire format version [{}], closing channel", channel, header.getVersion()), e); channel.close(); } } } else { final TransportChannel transportChannel = new TcpTransportChannel(outboundHandler, channel, action, requestId, version, header.isCompressed(), header.isHandshake(), message.takeBreakerReleaseControl()); try { messageListener.onRequestReceived(requestId, action); if (message.isShortCircuit()) { sendErrorResponse(action, transportChannel, message.getException()); } else { final StreamInput stream = namedWriteableStream(message.openOrGetStreamInput()); assertRemoteVersion(stream, header.getVersion()); final RequestHandlerRegistry<T> reg = requestHandlers.getHandler(action); assert reg != null; final T request = reg.newRequest(stream); try { request.remoteAddress(new TransportAddress(channel.getRemoteAddress())); // in case we throw an exception, i.e. when the limit is hit, we don't want to verify final int nextByte = stream.read(); // calling read() is useful to make sure the message is fully read, even if there some kind of EOS marker if (nextByte != -1) { throw new IllegalStateException("Message not fully read (request) for requestId [" + requestId + "], action [" + action + "], available [" + stream.available() + "]; resetting"); } final String executor = reg.getExecutor(); if (ThreadPool.Names.SAME.equals(executor)) { try { reg.processMessageReceived(request, transportChannel); } catch (Exception e) { sendErrorResponse(reg.getAction(), transportChannel, e); } } else { boolean success = false; if (request instanceof RefCounted) { ((RefCounted) request).incRef(); } try { threadPool.executor(executor).execute(new AbstractRunnable() { @Override protected void doRun() throws Exception { reg.processMessageReceived(request, transportChannel); } @Override public boolean isForceExecution() { return reg.isForceExecution(); } @Override public void onFailure(Exception e) { sendErrorResponse(reg.getAction(), transportChannel, e); } @Override public void onAfter() { RefCounted.decRef(request); } }); success = true; } finally { if (success == false) { RefCounted.decRef(request); } } } } finally { RefCounted.decRef(request); } } } catch (Exception e) { sendErrorResponse(action, transportChannel, e); } } }	inlined this so that the reference count handling is all in one place and it's "clear" that there's no path to leaking a request (except for some corner cases where the threadpool is shutting down but we have that issue with all kinds of ref counted things and its only relevant for tests).
@Override public boolean paramAsBoolean(String key, boolean defaultValue) { String rawParam = param(key); // treat the sheer presence of a parameter as "true" if (rawParam != null && rawParam.length() == 0) { return true; } else { return Booleans.parseBoolean(rawParam, defaultValue); } }	maybe "treat empty string as true because that allows the presence of the url parameter to mean "turn this on"".
*/ public void addDataFrameAnalyticsJobMemoryAndRefreshAllOthers(String id, long mem, ActionListener<Void> listener) { if (isMaster == false) { listener.onFailure(new NotMasterException("Request to put data frame analytics memory requirement on non-master node")); return; } memoryRequirementByDataFrameAnalyticsJob.put(id, mem + DataFrameAnalyticsConfig.PROCESS_MEMORY_OVERHEAD.getBytes()); PersistentTasksCustomMetadata persistentTasks = clusterService.state().getMetadata().custom(PersistentTasksCustomMetadata.TYPE); refresh(persistentTasks, listener); }	this pattern makes more sense to me but it will affect the behaviour of the open job and upgrade model snapshot actions which would have continued with the null response before. both of those actions are master node actions so the change will only be seen if the node stops being master when during processing the action and that would be a very difficult problem to solve. we will have to see what ci makes of the change.
boolean recoverFromLocalShards(BiConsumer<String, MappingMetaData> mappingUpdateConsumer, final IndexShard indexShard, final List<LocalShardSnapshot> shards) throws IOException { if (canRecover(indexShard)) { assert indexShard.recoveryState().getType() == RecoveryState.Type.LOCAL_SHARDS : "invalid recovery type: " + indexShard.recoveryState().getType(); if (indexShard.routingEntry().restoreSource() != null) { throw new IllegalStateException("can't recover - restore source is not null"); } if (shards.isEmpty()) { throw new IllegalArgumentException("shards must not be empty"); } Set<Index> indices = shards.stream().map((s) -> s.getIndex()).collect(Collectors.toSet()); if (indices.size() > 1) { throw new IllegalArgumentException("can't add shards from more than one index"); } for (ObjectObjectCursor<String, MappingMetaData> mapping : shards.get(0).getMappings()) { mappingUpdateConsumer.accept(mapping.key, mapping.value); } for (ObjectObjectCursor<String, MappingMetaData> mapping : shards.get(0).getMappings()) { indexShard.mapperService().merge(mapping.key,mapping.value.source(), MapperService.MergeReason.MAPPING_RECOVERY, true); } return executeRecovery(indexShard, () -> { logger.debug("starting recovery from local shards {}", shards); try { /* * TODO: once we upgraded to Lucene 6.1 use HardlinkCopyDirectoryWrapper to enable hardlinks if possible and enable it * in the security.policy: * * grant codeBase "${codebase.lucene-misc-6.1.0.jar}" { * // needed to allow shard shrinking to use hard-links if possible via lucenes HardlinkCopyDirectoryWrapper * permission java.nio.file.LinkPermission "hard"; * }; * */ final Directory directory = indexShard.store().directory(); // don't close this directory!! addIndices(indexShard.recoveryState().getIndex(), directory, shards.stream().map(s -> s.getSnapshotDirectory()) .collect(Collectors.toList()).toArray(new Directory[shards.size()])); internalRecoverFromStore(indexShard, true); // just trigger a merge to do housekeeping on the // copied segments - we will also see them in stats etc. indexShard.getEngine().forceMerge(false, -1, false, false, false); } catch (IOException ex) { throw new IndexShardRecoveryException(indexShard.shardId(), "failed to recover from local shards", ex); } }); } return false; }	can we add a static code assertion to make sure we don't forget?
private static void validateIndexAndExtractFields(Client client, String[] index, DataFrameAnalyticsConfig config, String resultsField, boolean isTaskRestarting, ActionListener<ExtractedFields> listener) { AtomicInteger docValueFieldsLimitHolder = new AtomicInteger(); // Step 4. Extract fields (if possible) and notify listener ActionListener<FieldCapabilitiesResponse> fieldCapabilitiesHandler = ActionListener.wrap( fieldCapabilitiesResponse -> listener.onResponse( new ExtractedFieldsDetector( index, config, resultsField, isTaskRestarting, docValueFieldsLimitHolder.get(), fieldCapabilitiesResponse) .detect()), listener::onFailure ); // Step 3. Get field capabilities necessary to build the information of how to extract fields ActionListener<SearchResponse> checkCardinalityHandler = ActionListener.wrap( searchResponse -> { Map<String, Long> fieldCardinalityLimits = config.getAnalysis().getFieldCardinalityLimits(); if (fieldCardinalityLimits.isEmpty() == false) { if (searchResponse == null) { listener.onFailure(ExceptionsHelper.badRequestException("searchResponse == null")); return; } Aggregations aggs = searchResponse.getAggregations(); if (aggs == null) { listener.onFailure(ExceptionsHelper.badRequestException("aggs == null")); return; } for (Map.Entry<String, Long> entry : fieldCardinalityLimits.entrySet()) { String fieldName = entry.getKey(); Long limit = entry.getValue(); Cardinality cardinality = aggs.get(fieldName); if (cardinality == null) { listener.onFailure(ExceptionsHelper.badRequestException("cardinality == null")); return; } if (cardinality.getValue() > limit) { listener.onFailure( ExceptionsHelper.badRequestException( "Field [{}] must have at most [{}] distinct values but there were [{}]", fieldName, limit, cardinality.getValue())); return; } } } FieldCapabilitiesRequest fieldCapabilitiesRequest = new FieldCapabilitiesRequest(); fieldCapabilitiesRequest.indices(index); fieldCapabilitiesRequest.indicesOptions(IndicesOptions.lenientExpandOpen()); fieldCapabilitiesRequest.fields("*"); ClientHelper.executeWithHeaders(config.getHeaders(), ClientHelper.ML_ORIGIN, client, () -> { client.execute(FieldCapabilitiesAction.INSTANCE, fieldCapabilitiesRequest, fieldCapabilitiesHandler); // This response gets discarded - the listener handles the real response return null; }); }, listener::onFailure ); // Step 2. Get cardinality of dependent variable in case of classification analysis. ActionListener<Integer> docValueFieldsLimitListener = ActionListener.wrap( docValueFieldsLimit -> { docValueFieldsLimitHolder.set(docValueFieldsLimit); Map<String, Long> fieldCardinalityLimits = config.getAnalysis().getFieldCardinalityLimits(); if (fieldCardinalityLimits.isEmpty()) { checkCardinalityHandler.onResponse(null); } else { SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().size(0); for (String fieldName : fieldCardinalityLimits.keySet()) { searchSourceBuilder.aggregation(AggregationBuilders.cardinality(fieldName).field(fieldName)); } SearchRequest searchRequest = new SearchRequest(config.getSource().getIndex()).source(searchSourceBuilder); ClientHelper.executeWithHeadersAsync( config.getHeaders(), ClientHelper.ML_ORIGIN, client, SearchAction.INSTANCE, searchRequest, checkCardinalityHandler); } }, listener::onFailure ); // Step 1. Get doc value fields limit getDocValueFieldsLimit(client, index, docValueFieldsLimitListener); }	since the target field is a required field, we should probably verify that the index has all the fields required before we gather the cardinality limits.
public static MappingStats of(Metadata metadata, Runnable ensureNotCancelled) { Map<String, FieldStats> fieldTypes = new HashMap<>(); Set<String> concreteFieldNames = new HashSet<>(); Map<String, RuntimeFieldStats> runtimeFieldTypes = new HashMap<>(); for (IndexMetadata indexMetadata : metadata) { ensureNotCancelled.run(); if (indexMetadata.isSystem()) { // Don't include system indices in statistics about mappings, // we care about the user's indices. continue; } Set<String> indexFieldTypes = new HashSet<>(); Set<String> indexRuntimeFieldTypes = new HashSet<>(); MappingMetadata mappingMetadata = indexMetadata.mapping(); if (mappingMetadata != null) { final Map<String, Object> map = mappingMetadata.getSourceAsMap(); MappingVisitor.visitMapping(map, (field, fieldMapping) -> { concreteFieldNames.add(field); String type = null; Object typeO = fieldMapping.get("type"); if (typeO != null) { type = typeO.toString(); } else if (fieldMapping.containsKey("properties")) { type = "object"; } if (type != null) { FieldStats stats = fieldTypes.computeIfAbsent(type, FieldStats::new); stats.count++; if (indexFieldTypes.add(type)) { stats.indexCount++; } Object scriptObject = fieldMapping.get("script"); if (scriptObject instanceof Map) { Map<?, ?> script = (Map<?, ?>) scriptObject; Object sourceObject = script.get("source"); stats.scriptCount++; updateScriptParams(sourceObject, stats.fieldScriptStats); Object langObject = script.get("lang"); if (langObject != null) { stats.scriptLangs.add(langObject.toString()); } } } }); MappingVisitor.visitRuntimeMapping(map, (field, fieldMapping) -> { Object typeObject = fieldMapping.get("type"); if (typeObject == null) { return; } String type = typeObject.toString(); RuntimeFieldStats stats = runtimeFieldTypes.computeIfAbsent(type, RuntimeFieldStats::new); stats.count++; if (indexRuntimeFieldTypes.add(type)) { stats.indexCount++; } if (concreteFieldNames.contains(field)) { stats.shadowedCount++; } Object scriptObject = fieldMapping.get("script"); if (scriptObject == null) { stats.scriptLessCount++; } else if (scriptObject instanceof Map) { Map<?, ?> script = (Map<?, ?>) scriptObject; Object sourceObject = script.get("source"); updateScriptParams(sourceObject, stats.fieldScriptStats); Object langObject = script.get("lang"); if (langObject != null) { stats.scriptLangs.add(langObject.toString()); } } }); } } return new MappingStats(fieldTypes.values(), runtimeFieldTypes.values()); }	just a quick fix here, the bigger fix would be to just not do redundant work if there's duplicate mappings. i'll look into that in a follow-up. this should give us a neat short-term improvement though.
private MemoryUsageEstimationResult runJob(String jobId, DataFrameAnalyticsConfig config, DataFrameDataExtractorFactory dataExtractorFactory) { DataFrameDataExtractor dataExtractor = dataExtractorFactory.newExtractor(false); DataFrameDataExtractor.DataSummary dataSummary = dataExtractor.collectDataSummary(); Set<String> categoricalFields = dataExtractor.getCategoricalFields(); if (dataSummary.rows == 0) { return new MemoryUsageEstimationResult(ByteSizeValue.ZERO, ByteSizeValue.ZERO); } AnalyticsProcessConfig processConfig = new AnalyticsProcessConfig( dataSummary.rows, dataSummary.cols, // For memory estimation the model memory limit here should be set high enough not to trigger an error when C++ code // compares the limit to the result of estimation. new ByteSizeValue(1, ByteSizeUnit.TB), 1, "", categoricalFields, config.getAnalysis()); ProcessHolder processHolder = new ProcessHolder(); AnalyticsProcess<MemoryUsageEstimationResult> process = processFactory.createAnalyticsProcess( jobId, processConfig, executorServiceForProcess, onProcessCrash(jobId, processHolder)); processHolder.process = process; if (process.isProcessAlive() == false) { String errorMsg = new ParameterizedMessage("[{}] Error while starting process", jobId).getFormattedMessage(); throw ExceptionsHelper.serverError(errorMsg); } try { return readResult(jobId, process); } catch (Exception e) { String errorMsg = new ParameterizedMessage("[{}] Error while processing result [{}]", jobId, e.getMessage()).getFormattedMessage(); throw ExceptionsHelper.serverError(errorMsg, e); } finally { process.consumeAndCloseOutputStream(); try { LOGGER.info("[{}] Closing process", jobId); process.close(); LOGGER.info("[{}] Closed process", jobId); } catch (Exception e) { String errorMsg = new ParameterizedMessage("[{}] Error while closing process [{}]", jobId, e.getMessage()).getFormattedMessage(); throw ExceptionsHelper.serverError(errorMsg, e); } } }	unless there is a side-effect, i wonder if we should make this even larger. the era of 1 tb rams is around the corner :-)
public void writeTo(StreamOutput out) throws IOException { if (out.getVersion().onOrAfter(Version.V_7_0_0)) { out.writeStringArray(seedNodes.toArray(new String[0])); } else { // versions prior to 7.0.0 received the resolved transport address of the seed nodes out.writeList(seedNodes .stream() .map( s -> { final Tuple<String, Integer> hostPort = RemoteClusterAware.parseHostPort(s); assert hostPort.v2() != null : s; try { return new TransportAddress( InetAddress.getByAddress(hostPort.v1(), TransportAddress.META_ADDRESS.getAddress()), hostPort.v2()); } catch (final UnknownHostException e) { throw new AssertionError(e); } }) .collect(Collectors.toList())); } if (out.getVersion().before(Version.V_7_0_0)) { /* * Versions before 7.0 sent the HTTP addresses of all nodes in the * remote cluster here but it was expensive to fetch and we * ultimately figured out how to do without it. So we removed it. * * When sending this request to a node that expects HTTP addresses * here we pretend that we didn't find any. This *should* be fine * because, after all, we haven't been using this information for * a while. */ out.writeList(emptyList()); } out.writeVInt(connectionsPerCluster); out.writeTimeValue(initialConnectionTimeout); out.writeVInt(numNodesConnected); out.writeString(clusterAlias); out.writeBoolean(skipUnavailable); }	is the bwc aspect of serialization sufficiently tested? i do see remoteclusterconnectiontests#testremoteconnectioninfobwcomp, just wondering whether that is good enough.
public void testSimple() throws Exception { final XContentBuilder mapping = XContentFactory.jsonBuilder(); mapping.startObject(); { mapping.startObject("_doc"); { mapping.startObject("properties"); { mapping.startObject("english_text"); mapping.field("type", "text"); mapping.endObject(); mapping.startObject("value"); mapping.field("type", "long"); mapping.endObject(); } mapping.endObject(); } mapping.endObject(); } mapping.endObject(); final String index = "test-index"; client().admin().indices().prepareCreate(index) .setMapping(mapping) .setSettings(Settings.builder().put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, between(1, 5))) .get(); ensureGreen(index); int numDocs = randomIntBetween(10, 100); for (int i = 0; i < numDocs; i++) { int value = randomIntBetween(1, 1024); final XContentBuilder doc = XContentFactory.jsonBuilder() .startObject() .field("english_text", English.intToEnglish(value)) .field("value", value) .endObject(); client().prepareIndex(index) .setId("id-" + i) .setSource(doc) .get(); } final boolean hasNorms = randomBoolean(); if (hasNorms) { final XContentBuilder doc = XContentFactory.jsonBuilder() .startObject() .field("english_text", "A long sentence to make sure that norms is non-zero") .endObject(); client().prepareIndex(index) .setId("id") .setSource(doc) .get(); } PlainActionFuture<AnalyzeIndexDiskUsageResponse> future = PlainActionFuture.newFuture(); client().execute(AnalyzeIndexDiskUsageAction.INSTANCE, new AnalyzeIndexDiskUsageRequest(new String[] {index}, AnalyzeIndexDiskUsageRequest.DEFAULT_INDICES_OPTIONS, true), future); AnalyzeIndexDiskUsageResponse resp = future.actionGet(); final IndexDiskUsageStats stats = resp.getStats().get(index); logger.info("--> stats {}", stats); assertNotNull(stats); assertThat(stats.getIndexSizeInBytes(), greaterThan(100L)); final IndexDiskUsageStats.PerFieldDiskUsage englishField = stats.getFields().get("english_text"); assertThat(englishField.getInvertedIndexBytes(), greaterThan(0L)); assertThat(englishField.getStoredFieldBytes(), equalTo(0L)); if (hasNorms) { assertThat(englishField.getNormsBytes(), greaterThan(0L)); } final IndexDiskUsageStats.PerFieldDiskUsage valueField = stats.getFields().get("value"); assertThat(valueField.getInvertedIndexBytes(), equalTo(0L)); assertThat(valueField.getStoredFieldBytes(), equalTo(0L)); assertThat(valueField.getPointsBytes(), greaterThan(0L)); assertThat(valueField.getDocValuesBytes(), greaterThan(0L)); assertMetadataFields(stats); }	maybe call this "forcenorms"? because we would still have norms when this variable here is false.
public static FieldFetcher create(QueryShardContext context, SearchLookup searchLookup, Collection<FieldAndFormat> fieldAndFormats) { Map<String, FieldContext> fieldContexts = new HashMap<>(); List<String> unmappedFetchPattern = new ArrayList<>(); Set<String> mappedToExclude = new HashSet<>(); boolean includeUnmapped = false; for (FieldAndFormat fieldAndFormat : fieldAndFormats) { String fieldPattern = fieldAndFormat.field; if (fieldAndFormat.includeUnmapped != null && fieldAndFormat.includeUnmapped) { unmappedFetchPattern.add(fieldAndFormat.field); includeUnmapped = true; } String format = fieldAndFormat.format; Collection<String> concreteFields = context.simpleMatchToIndexNames(fieldPattern); for (String field : concreteFields) { MappedFieldType ft = context.getFieldType(field); if (ft == null || context.isMetadataField(field)) { continue; } ValueFetcher valueFetcher = ft.valueFetcher(context, format); mappedToExclude.add(field); fieldContexts.put(field, new FieldContext(field, valueFetcher)); } } CharacterRunAutomaton unmappedFetchAutomaton = new CharacterRunAutomaton(Automata.makeEmpty()); if (unmappedFetchPattern.isEmpty() == false) { unmappedFetchAutomaton = new CharacterRunAutomaton( Regex.simpleMatchToAutomaton(unmappedFetchPattern.toArray(new String[unmappedFetchPattern.size()])) ); } return new FieldFetcher(fieldContexts, unmappedFetchAutomaton, mappedToExclude, includeUnmapped); }	i think it'd make sense to use a linkedhashmap here -- then the fields would come back in the order requested. for example if you passed "fields": [ "name", "title"], then it'd be nice to return "fields": { "name": [ "christoph"], "title": [ "engineer" ] } we wouldn't make a formal guarantee about order (since it's a json map), i think it helps readability for debugging, etc.
public static FieldFetcher create(QueryShardContext context, SearchLookup searchLookup, Collection<FieldAndFormat> fieldAndFormats) { Map<String, FieldContext> fieldContexts = new HashMap<>(); List<String> unmappedFetchPattern = new ArrayList<>(); Set<String> mappedToExclude = new HashSet<>(); boolean includeUnmapped = false; for (FieldAndFormat fieldAndFormat : fieldAndFormats) { String fieldPattern = fieldAndFormat.field; if (fieldAndFormat.includeUnmapped != null && fieldAndFormat.includeUnmapped) { unmappedFetchPattern.add(fieldAndFormat.field); includeUnmapped = true; } String format = fieldAndFormat.format; Collection<String> concreteFields = context.simpleMatchToIndexNames(fieldPattern); for (String field : concreteFields) { MappedFieldType ft = context.getFieldType(field); if (ft == null || context.isMetadataField(field)) { continue; } ValueFetcher valueFetcher = ft.valueFetcher(context, format); mappedToExclude.add(field); fieldContexts.put(field, new FieldContext(field, valueFetcher)); } } CharacterRunAutomaton unmappedFetchAutomaton = new CharacterRunAutomaton(Automata.makeEmpty()); if (unmappedFetchPattern.isEmpty() == false) { unmappedFetchAutomaton = new CharacterRunAutomaton( Regex.simpleMatchToAutomaton(unmappedFetchPattern.toArray(new String[unmappedFetchPattern.size()])) ); } return new FieldFetcher(fieldContexts, unmappedFetchAutomaton, mappedToExclude, includeUnmapped); }	could we remove this set, and instead rely on fieldcontexts.containskey(...)?
@Override public void applyClusterState(ClusterChangedEvent event) { clusterState = event.state(); }	make this package protected.
public static IndexMetaData newIndexMeta(String name, Settings indexSettings) { Settings build = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_CREATION_DATE, 1) .put(IndexMetaData.SETTING_INDEX_UUID, "BOOM") .put(IndexMetaData.SETTING_VERSION_UPGRADED, Version.V_7_0_0) .put(indexSettings) .build(); return IndexMetaData.builder(name).settings(build).build(); }	i think this should be version.current.minimumindexcompatibilityversion()?
* @param endpoint the path of the request (without host and port) * @param params the query_string parameters * @param headers the optional request headers * @return the response returned by Elasticsearch * @throws IOException in case of a problem or the connection was aborted * @throws ClientProtocolException in case of an http protocol error * @throws ResponseException in case Elasticsearch responded with a status code that indicated an error */ public Response performRequest(String method, String endpoint, Map<String, String> params, Header... headers) throws IOException { return performRequest(method, endpoint, params, (HttpEntity)null, headers); } /** * Sends a request to the Elasticsearch cluster that the client points to and waits for the corresponding response * to be returned. Shortcut to {@link #performRequest(String, String, Map, HttpEntity, HttpAsyncResponseConsumerFactory, Header...)} * which doesn't require specifying an {@link HttpAsyncResponseConsumerFactory} instance, * {@link HttpAsyncResponseConsumerFactory} will be used to create the needed instances of {@link HttpAsyncResponseConsumer}	maybe "shortcut to {@link #performrequest(string, string, map, httpentity, httpasyncresponseconsumerfactory, header...)} which defaults the {@link httpasyncresponseconsumerfactory} to {@link httpasyncresponseconsumerfactory#heap_buffer_max_20mb} which buffers responses from elasticsearch on a heap, failing the request if the response is larger than that. consider providing a {@link httpasyncresponseconsumerfactory} with a higher limit if scrolling." ?
private void addNativeRealms(List<Realm> realms) throws Exception { Realm.Factory fileRealm = factories.get(FileRealmSettings.TYPE); if (fileRealm != null) { var realmIdentifier = new RealmConfig.RealmIdentifier(FileRealmSettings.TYPE, FileRealmSettings.DEFAULT_NAME); realms.add(fileRealm.create(new RealmConfig( realmIdentifier, ensureOrderSetting(settings, realmIdentifier, Integer.MIN_VALUE + 1), env, threadContext))); } Realm.Factory indexRealmFactory = factories.get(NativeRealmSettings.TYPE); if (indexRealmFactory != null) { var realmIdentifier = new RealmConfig.RealmIdentifier(NativeRealmSettings.TYPE, NativeRealmSettings.DEFAULT_NAME); realms.add(indexRealmFactory.create(new RealmConfig( realmIdentifier, ensureOrderSetting(settings, realmIdentifier, Integer.MIN_VALUE + 2), env, threadContext))); } }	this is a a good change. not related to your change, but it's interesting that we decided to call this default_file as if there could be other file realms.
private static void assertShardStatesMatch(final IndexShardStateChangeListener stateChangeListener, final int numShards, final IndexShardState... shardStates) { CheckedRunnable<Exception> waitPredicate = () -> { assertEquals(stateChangeListener.shardStates.size(), numShards); for (List<IndexShardState> indexShardStates : stateChangeListener.shardStates.values()) { assertNotNull(indexShardStates); assertEquals(indexShardStates.size(), shardStates.length); for (int i = 0; i < shardStates.length; i++) { assertEquals(indexShardStates.get(i), shardStates[i]); } } }; try { assertBusy(waitPredicate, 1, TimeUnit.MINUTES); } catch (Exception e) { fail("failed to observe expect shard states\\\\n" + "expected: [" + numShards + "] shards with states: " + Strings.arrayToCommaDelimitedString(shardStates) + "\\\\n" + "observed:\\\\n" + stateChangeListener); } stateChangeListener.shardStates.clear(); }	future thought: perhaps assertbusy should have a "message" like many other asserts do, so that if the assertion timeout fails, we get a good error message and don't need to do this try/catch/fail dance.
public void sendRequest(Transport.Connection connection, long requestId, String action, TransportRequest request, TransportRequestOptions options) throws IOException { logger.info("--> sending request {} on {}", action, connection.getNode()); if (PeerRecoverySourceService.Actions.START_RECOVERY.equals(action) && count.incrementAndGet() == 1) { // ensures that it's considered as valid recovery attempt by source try { assertBusy(() -> assertFalse( "Expected there to be some initializing shards", client(blueNodeName).admin().cluster().prepareState().setLocal(true).get() .getState().getRoutingTable().index("test").shard(0).getAllInitializingShards().isEmpty())); } catch (Exception e) { throw new RuntimeException(e); } connection.sendRequest(requestId, action, request, options); try { Thread.sleep(disconnectAfterDelay.millis()); } catch (InterruptedException e) { throw new RuntimeException(e); } throw new ConnectTransportException(connection.getNode(), "DISCONNECT: simulation disconnect after successfully sending " + action + " request"); } else { connection.sendRequest(requestId, action, request, options); } }	this assertion would probably be more useful as an assertthat(..., empty()) since we would automatically get the still initializing shards in the failure.
public void sendRequest(Transport.Connection connection, long requestId, String action, TransportRequest request, TransportRequestOptions options) throws IOException { logger.info("--> sending request {} on {}", action, connection.getNode()); if (PeerRecoverySourceService.Actions.START_RECOVERY.equals(action) && count.incrementAndGet() == 1) { // ensures that it's considered as valid recovery attempt by source try { assertBusy(() -> assertFalse( "Expected there to be some initializing shards", client(blueNodeName).admin().cluster().prepareState().setLocal(true).get() .getState().getRoutingTable().index("test").shard(0).getAllInitializingShards().isEmpty())); } catch (Exception e) { throw new RuntimeException(e); } connection.sendRequest(requestId, action, request, options); try { Thread.sleep(disconnectAfterDelay.millis()); } catch (InterruptedException e) { throw new RuntimeException(e); } throw new ConnectTransportException(connection.getNode(), "DISCONNECT: simulation disconnect after successfully sending " + action + " request"); } else { connection.sendRequest(requestId, action, request, options); } }	i don't think this wrapping is necessary, since the assertbusy will throw an assertionerror?
@Override protected void nodeOperation(AllocatedPersistentTask task, TestParams params, PersistentTaskState state) { logger.info("started node operation for the task {}", task); try { TestTask testTask = (TestTask) task; AtomicInteger phase = new AtomicInteger(); while (true) { // wait for something to happen try { assertBusy(() -> assertTrue(testTask.isCancelled() || testTask.getOperation() != null || clusterService.lifecycleState() != Lifecycle.State.STARTED), // speedup finishing on closed nodes 45, TimeUnit.SECONDS); // This can take a while during large cluster restart } catch (Exception e) { throw new RuntimeException(e); } if (clusterService.lifecycleState() != Lifecycle.State.STARTED) { return; } if ("finish".equals(testTask.getOperation())) { task.markAsCompleted(); return; } else if ("fail".equals(testTask.getOperation())) { task.markAsFailed(new RuntimeException("Simulating failure")); return; } else if ("update_status".equals(testTask.getOperation())) { testTask.setOperation(null); CountDownLatch latch = new CountDownLatch(1); State newState = new State("phase " + phase.incrementAndGet()); logger.info("updating the task state to {}", newState); task.updatePersistentTaskState(newState, new ActionListener<PersistentTask<?>>() { @Override public void onResponse(PersistentTask<?> persistentTask) { logger.info("updating was successful"); latch.countDown(); } @Override public void onFailure(Exception e) { logger.info("updating failed", e); latch.countDown(); fail(e.toString()); } }); assertTrue(latch.await(10, TimeUnit.SECONDS)); } else if (testTask.isCancelled()) { // Cancellation make cause different ways for the task to finish if (randomBoolean()) { if (randomBoolean()) { task.markAsFailed(new TaskCancelledException(testTask.getReasonCancelled())); } else { task.markAsCompleted(); } } else { task.markAsFailed(new RuntimeException(testTask.getReasonCancelled())); } return; } else { fail("We really shouldn't be here"); } } } catch (InterruptedException e) { task.markAsFailed(e); } }	ditto: i don't think we need the try/catch?
public void waitForBlockOnAnyDataNode(String repository, TimeValue timeout) { try { assertBusy(() -> { for (RepositoriesService repositoriesService : internalCluster().getDataNodeInstances(RepositoriesService.class)) { MockRepository mockRepository = (MockRepository) repositoriesService.repository(repository); if (mockRepository.blocked()) { return; } } fail("No repository is blocked waiting on a data node"); }, timeout.millis(), TimeUnit.MILLISECONDS); } catch (Exception e) { fail("Timeout waiting for repository block on any data node!!!"); } }	i don't think this try/catch/fail is necessary? the assertbusy will fail and explain that it timed out
public static void ensureAllPagesAreReleased() throws Exception { final Map<Object, Throwable> masterCopy = new HashMap<>(ACQUIRED_PAGES); if (!masterCopy.isEmpty()) { // not empty, we might be executing on a shared cluster that keeps on obtaining // and releasing pages, lets make sure that after a reasonable timeout, all master // copy (snapshot) have been released try { assertBusy(() -> Sets.haveEmptyIntersection(masterCopy.keySet(), ACQUIRED_PAGES.keySet())); } catch (Exception e) { masterCopy.keySet().retainAll(ACQUIRED_PAGES.keySet()); ACQUIRED_PAGES.keySet().removeAll(masterCopy.keySet()); // remove all existing master copy we will report on if (!masterCopy.isEmpty()) { Iterator<Throwable> causes = masterCopy.values().iterator(); Throwable firstCause = causes.next(); RuntimeException exception = new RuntimeException(masterCopy.size() + " pages have not been released", firstCause); while (causes.hasNext()) { exception.addSuppressed(causes.next()); } throw exception; } } } }	perhaps we should catch assertionerror here specifically since that is what assertbusy should fail with when timing out? that is for all these try/catch around assertbusy
@Before public void setupForTests() throws Exception { assertBusy(() -> { final List<String> missingTemplates = new ArrayList<>(); for (String template : templatesToWaitFor()) { try { final Request headRequest = new Request("HEAD", "_template/" + template); headRequest.setOptions(allowTypesRemovalWarnings()); final boolean exists = adminClient() .performRequest(headRequest) .getStatusLine().getStatusCode() == 200; if (!exists) { missingTemplates.add(template); } logger.debug("template [{}] exists [{}]", template, exists); } catch (IOException e) { logger.warn("error calling template api", e); throw e; } } assertThat(missingTemplates, is(empty())); }); }	nit: == false please
public void testResetRootDocId() throws Exception { IndexWriterConfig iwc = new IndexWriterConfig(null); iwc.setMergePolicy(NoMergePolicy.INSTANCE); SeqNoFieldMapper.SequenceIDFields sequenceIDFields = SeqNoFieldMapper.SequenceIDFields.emptySeqID(); try (Directory directory = newDirectory()) { try (RandomIndexWriter iw = new RandomIndexWriter(random(), directory, iwc)) { List<Document> documents = new ArrayList<>(); // 1 segment with, 1 root document, with 3 nested sub docs Document document = new Document(); document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE)); document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE)); documents.add(document); document = new Document(); document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE)); document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE)); documents.add(document); document = new Document(); document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE)); document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE)); documents.add(document); document = new Document(); document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.FIELD_TYPE)); document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE)); document.add(sequenceIDFields.primaryTerm); documents.add(document); iw.addDocuments(documents); iw.commit(); documents.clear(); // 1 segment with: // 1 document, with 1 nested subdoc document = new Document(); document.add(new Field(UidFieldMapper.NAME, "type#2", UidFieldMapper.Defaults.NESTED_FIELD_TYPE)); document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE)); documents.add(document); document = new Document(); document.add(new Field(UidFieldMapper.NAME, "type#2", UidFieldMapper.Defaults.FIELD_TYPE)); document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE)); document.add(sequenceIDFields.primaryTerm); documents.add(document); iw.addDocuments(documents); documents.clear(); // and 1 document, with 1 nested subdoc document = new Document(); document.add(new Field(UidFieldMapper.NAME, "type#3", UidFieldMapper.Defaults.NESTED_FIELD_TYPE)); document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE)); documents.add(document); document = new Document(); document.add(new Field(UidFieldMapper.NAME, "type#3", UidFieldMapper.Defaults.FIELD_TYPE)); document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE)); document.add(sequenceIDFields.primaryTerm); documents.add(document); iw.addDocuments(documents); iw.commit(); iw.close(); } try (IndexReader indexReader = wrap(DirectoryReader.open(directory))) { NestedAggregationBuilder nestedBuilder = new NestedAggregationBuilder(NESTED_AGG, "nested_field"); MappedFieldType fieldType = new NumberFieldMapper.NumberFieldType( NumberFieldMapper.NumberType.LONG); fieldType.setName(VALUE_FIELD_NAME); BooleanQuery.Builder bq = new BooleanQuery.Builder(); bq.add(Queries.newNonNestedFilter(Version.CURRENT), BooleanClause.Occur.MUST); bq.add(new TermQuery(new Term(UidFieldMapper.NAME, "type#2")), BooleanClause.Occur.MUST_NOT); Nested nested = search(newSearcher(indexReader, false, true), new ConstantScoreQuery(bq.build()), nestedBuilder, fieldType); assertEquals(NESTED_AGG, nested.getName()); // The bug manifests if 6 docs are returned, because currentRootDoc isn't reset the previous child docs from the first segment are emitted as hits. assertEquals(4L, nested.getDocCount()); } } }	randomize version? the test should be able to handle both the old and new way.
private List<JobForecastId> findForecastsToDelete(SearchResponse searchResponse) { List<JobForecastId> forecastsToDelete = new ArrayList<>(); SearchHits hits = searchResponse.getHits(); if (hits.getTotalHits() > MAX_FORECASTS) { LOGGER.info("More than [{}] forecasts were found. This run will only delete [{}] of them", MAX_FORECASTS, MAX_FORECASTS); } for (SearchHit hit : hits.getHits()) { String expiryTime = stringFieldValueOrNull(hit, ForecastRequestStats.EXPIRY_TIME.getPreferredName()); if (expiryTime == null) { LOGGER.warn("Forecast request stats document [{}] has a null [{}] field", hit.getId(), ForecastRequestStats.EXPIRY_TIME.getPreferredName()); continue; } long expiryMs = TimeUtils.parseToEpochMs(expiryTime); if (expiryMs < cutoffEpochMs) { JobForecastId idPair = new JobForecastId( stringFieldValueOrNull(hit, Job.ID.getPreferredName()), stringFieldValueOrNull(hit, Forecast.FORECAST_ID.getPreferredName())); if (idPair.hasNullValue() == false) { forecastsToDelete.add(idPair); } } } return forecastsToDelete; }	in 6.8 the doc value could be a long rather than a string. it's why timefield has this case: https://github.com/elastic/elasticsearch/blob/cde202610c58def76e5a923892776c0d7e00f88a/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/extractor/timefield.java#l49 what this means in practice is that a user running 6.8 who first used ml in 5.x will end up seeing the warning on lines 139-140 repeatedly and won't get any cleanup. it's still ok to use stringfieldvalueornull() to extract fields mapped as keyword or text from hits, but for fields mapped as date in 6.8 the code needs to handle both long and string.
public void onResponse(SearchResponse searchResponse) { try { List<JobSnapshotId> snapshotIds = new ArrayList<>(); for (SearchHit hit : searchResponse.getHits()) { JobSnapshotId idPair = new JobSnapshotId( stringFieldValueOrNull(hit, Job.ID.getPreferredName()), stringFieldValueOrNull(hit, ModelSnapshotField.SNAPSHOT_ID.getPreferredName())); if (idPair.hasNullValue() == false) { snapshotIds.add(idPair); } } deleteModelSnapshots(new VolatileCursorIterator<>(snapshotIds), listener); } catch (Exception e) { onFailure(e); } }	the logic here is different to the later branches as it doesn't have the new model snapshot retention options added in #56125
public static ParentJoinFieldMapper getMapper(MapperService service) { MetaJoinFieldMapper.MetaJoinFieldType fieldType = (MetaJoinFieldMapper.MetaJoinFieldType) service.fieldType(MetaJoinFieldMapper.NAME); return fieldType == null ? null : (ParentJoinFieldMapper) service.fieldMapper(fieldType.getJoinField()); }	parentjoinfieldmapper was using updatefieldtype() to update its linked metajoinfieldmapper. this requires a small bit of hackery to get round, which i hope to fix in a followup by moving the various filter factory methods from the field mapper to the field type.
private static void parseDynamicValue(final ParseContext context, ObjectMapper parentMapper, String currentFieldName, XContentParser.Token token) throws IOException { ObjectMapper.Dynamic dynamic = dynamicOrDefault(parentMapper, context); if (dynamic == ObjectMapper.Dynamic.STRICT) { throw new StrictDynamicMappingException(parentMapper.fullPath(), currentFieldName); } if (dynamic == ObjectMapper.Dynamic.FALSE) { return; } final String path = context.path().pathAsText(currentFieldName); final Mapper.BuilderContext builderContext = new Mapper.BuilderContext(context.indexSettings().getSettings(), context.path()); final MappedFieldType existingFieldType = context.mapperService().fieldType(path); final Mapper.Builder builder; if (existingFieldType != null) { // create a builder of the same type builder = createBuilderFromFieldType(context, existingFieldType, currentFieldName); } else { builder = createBuilderFromDynamicValue(context, token, currentFieldName); } Mapper mapper = builder.build(builderContext); context.addDynamicMapper(mapper); parseObjectOrField(context, mapper); }	if i'm understanding correctly, we can now assume existingfieldtype will always be null, since there is only one type. in that case we could remove the logic above too, including the method createbuilderfromfieldtype.
private boolean assertMappersShareSameFieldType() { if (mapper != null) { List<FieldMapper> fieldMappers = new ArrayList<>(); Collections.addAll(fieldMappers, mapper.mapping().metadataMappers); MapperUtils.collect(mapper.root(), new ArrayList<>(), fieldMappers, new ArrayList<>()); for (FieldMapper fieldMapper : fieldMappers) { assert Objects.equals(fieldMapper.fieldType(), fieldTypes.get(fieldMapper.name())) : "Fieldtype mismatch: " + fieldMapper.name() + " " + fieldMapper.fieldType() + " != " + fieldTypes.get(fieldMapper.name()); } } return true; }	i think we could remove this whole method assertmapperssharesamefieldtype, it doesn't seem helpful now that we only have one type.
public MappedFieldType fieldType(String fullName) { return fieldTypes.get(fullName); }	this is unfortunately necessary as a shim to get round the parentjoinfieldmapper issue above. hopefully removed asap.
private void validateDataStreams(SortedMap<String, AliasOrIndex> aliasAndIndexLookup) { DataStreamMetadata dsMetadata = (DataStreamMetadata) customs.get(DataStreamMetadata.TYPE); if (dsMetadata != null) { for (DataStream ds : dsMetadata.dataStreams().values()) { if (aliasAndIndexLookup.containsKey(ds.getName())) { throw new IllegalStateException("data stream [" + ds.getName() + "] conflicts with existing index or alias"); } final String backingIndexPrefixFrom = (ds.getName().startsWith(".") ? "" : ".") + ds.getName() + "-"; final String backingIndexPrefixTo = (ds.getName().startsWith(".") ? "" : ".") + ds.getName() + "."; SortedMap<?, ?> map = aliasAndIndexLookup.subMap(backingIndexPrefixFrom, backingIndexPrefixTo); if (map.size() != 0) { throw new IllegalStateException( "data stream [" + ds.getName() + "] could create backing indices that conflict with existing indices"); } } } }	nit: a backing index could also conflict with an alias (or data stream in the future)
protected ForecastRequestStats getForecastStats(String jobId, String forecastId) { SearchResponse searchResponse = client().prepareSearch(AnomalyDetectorsIndex.jobResultsAliasedName(jobId)) .setQuery(new BoolQueryBuilder() .filter(new TermQueryBuilder("_id", ForecastRequestStats.documentId(jobId, forecastId)))) .get(); if (searchResponse.getHits().getHits().length == 0) { return null; } assertThat(searchResponse.getHits().getHits().length, equalTo(1)); try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser( NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, searchResponse.getHits().getHits()[0].getSourceRef().streamInput())) { return ForecastRequestStats.STRICT_PARSER.apply(parser, null); } catch (IOException e) { throw new IllegalStateException(e); } }	you could use an idsquerybuilder instead of the termquerybuilder and drop the boolquerybuilder
public void getForecastRequestStats(String jobId, String forecastId, Consumer<ForecastRequestStats> handler, Consumer<Exception> errorHandler) { String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId); SearchRequestBuilder forecastSearch = client.prepareSearch(indexName) .setTypes(ElasticsearchMappings.DOC_TYPE) .setQuery(new BoolQueryBuilder() .filter(new TermQueryBuilder("_id", ForecastRequestStats.documentId(jobId, forecastId)))); searchSingleResult(jobId, ForecastRequestStats.RESULTS_FIELD.getPreferredName(), forecastSearch, ForecastRequestStats.LENIENT_PARSER,result -> handler.accept(result.result), errorHandler, () -> null); }	same comment as above
static void setRandomWaitForActiveShards(Consumer<ActiveShardCount> setter, ActiveShardCount defaultActiveShardCount, Map<String, String> expectedParams) { if (randomBoolean()) { int waitForActiveShardsInt = randomIntBetween(-1, 5); String waitForActiveShardsString; if (waitForActiveShardsInt == -1) { waitForActiveShardsString = "all"; } else { waitForActiveShardsString = String.valueOf(waitForActiveShardsInt); } ActiveShardCount activeShardCount = ActiveShardCount.parseString(waitForActiveShardsString); setter.accept(activeShardCount); if (defaultActiveShardCount.equals(activeShardCount) == false) { expectedParams.put("wait_for_active_shards", waitForActiveShardsString); } } } /** * @deprecated Use {@link RefreshPolicy} rather than {@link WriteRequest.RefreshPolicy}	did this get deprecated *and* renamed? should we be doing both?
@Override public void applyClusterState(ClusterChangedEvent event) { try { if (event.localNodeMaster()) { // We don't remove old master when master flips anymore. So, we need to check for change in master final boolean newMaster = event.previousState().nodes().isLocalNodeElectedMaster() == false; final SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE); if (snapshotsInProgress != null) { if (removedNodesCleanupNeeded(snapshotsInProgress, event.nodesDelta().removedNodes(), newMaster)) { processSnapshotsOnRemovedNodes(newMaster); } if (event.routingTableChanged() && waitingShardsStartedOrUnassigned(snapshotsInProgress, event)) { processStartedShards(); } if (newMaster) { // Removes finished snapshots from the cluster state, that the previous master failed to end properly before dying. snapshotsInProgress.entries().stream().filter(entry -> entry.state().completed()).forEach(this::endSnapshot); } } if (newMaster) { finalizeSnapshotDeletionFromPreviousMaster(event); } } } catch (Exception e) { logger.warn("Failed to update snapshot state ", e); } }	this check was nested in all the operations this condition now covers, much easier to understand that this method pretty much only takes effect when there's a snapshotsinprogress in the state now imo.
private void finalizeSnapshotDeletionFromPreviousMaster(ClusterChangedEvent event) { SnapshotDeletionsInProgress deletionsInProgress = event.state().custom(SnapshotDeletionsInProgress.TYPE); if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) { assert deletionsInProgress.getEntries().size() == 1 : "only one in-progress deletion allowed per cluster"; SnapshotDeletionsInProgress.Entry entry = deletionsInProgress.getEntries().get(0); deleteSnapshotFromRepository(entry.getSnapshot(), null, entry.getRepositoryStateId()); } }	inlined as a one-liner into applyclusterstate
public void authorize(final Authentication authentication, final String action, final TransportRequest originalRequest, final ActionListener<Void> listener) { final String auditId; try { auditId = requireAuditId(authentication, action, originalRequest); } catch (ElasticsearchSecurityException e) { listener.onFailure(e); return; } if (checkOperatorPrivileges(authentication, action, originalRequest, listener) == false) { return; } // sometimes a request might be wrapped within another, which is the case for proxied // requests and concrete shard requests final TransportRequest unwrappedRequest = maybeUnwrapRequest(authentication, originalRequest, action, auditId); final RequestInfo requestInfo = new RequestInfo(authentication, unwrappedRequest, action); // Detect cases where a child action can be authorized automatically because the parent was authorized final AuthorizationEngine engine = getAuthorizationEngine(authentication); if (shouldAuthorizeAsChildAction(engine, requestInfo)) { authorizeChildAction(engine, requestInfo, auditId, listener); return; } /* authorization fills in certain transient headers, which must be observed in the listener (action handler execution) * as well, but which must not bleed across different action context (eg parent-child action contexts). * <p> * Therefore we begin by clearing the existing ones up, as they might already be set during the authorization of a * previous parent action that ran under the same thread context (also on the same node). * When the returned {@code StoredContext} is closed, ALL the original headers are restored. */ try (ThreadContext.StoredContext ignore = threadContext.newStoredContext(false, ACTION_SCOPE_AUTHORIZATION_KEYS)) { // this does not clear {@code AuthorizationServiceField.ORIGINATING_ACTION_KEY} // prior to doing any authorization lets set the originating action in the thread context // the originating action is the current action if no originating action has yet been set in the current thread context // if there is already an original action, that stays put (eg. the current action is a child action) putTransientIfNonExisting(ORIGINATING_ACTION_KEY, action); if (SystemUser.is(authentication.getUser())) { // this never goes async so no need to wrap the listener authorizeSystemUser(authentication, action, auditId, unwrappedRequest, listener); } else { final ActionListener<AuthorizationInfo> authzInfoListener = wrapPreservingContext(ActionListener.wrap( authorizationInfo -> { threadContext.putTransient(AUTHORIZATION_INFO_KEY, authorizationInfo); maybeAuthorizeRunAs(requestInfo, auditId, authorizationInfo, listener); }, listener::onFailure), threadContext); engine.resolveAuthorizationInfo(requestInfo, authzInfoListener); } } } /** * @return {@code true}	i don't think it can happen in today's code. so this is more of a philosophical question: do we need to worry about the situation where a child action has a different authentication than the parent, but somehow the same authorization info. a possible way to get to this situation is mis-use of threadcontext or other helper classes, e.g. clienthelper and does not clear all associated context entirely. to prevent this, maybe we can add an assert in authenticationservice#writeauthtocontext to ensure when a new authentication is serialised, there should be no authorization info>
public void authorize(final Authentication authentication, final String action, final TransportRequest originalRequest, final ActionListener<Void> listener) { final String auditId; try { auditId = requireAuditId(authentication, action, originalRequest); } catch (ElasticsearchSecurityException e) { listener.onFailure(e); return; } if (checkOperatorPrivileges(authentication, action, originalRequest, listener) == false) { return; } // sometimes a request might be wrapped within another, which is the case for proxied // requests and concrete shard requests final TransportRequest unwrappedRequest = maybeUnwrapRequest(authentication, originalRequest, action, auditId); final RequestInfo requestInfo = new RequestInfo(authentication, unwrappedRequest, action); // Detect cases where a child action can be authorized automatically because the parent was authorized final AuthorizationEngine engine = getAuthorizationEngine(authentication); if (shouldAuthorizeAsChildAction(engine, requestInfo)) { authorizeChildAction(engine, requestInfo, auditId, listener); return; } /* authorization fills in certain transient headers, which must be observed in the listener (action handler execution) * as well, but which must not bleed across different action context (eg parent-child action contexts). * <p> * Therefore we begin by clearing the existing ones up, as they might already be set during the authorization of a * previous parent action that ran under the same thread context (also on the same node). * When the returned {@code StoredContext} is closed, ALL the original headers are restored. */ try (ThreadContext.StoredContext ignore = threadContext.newStoredContext(false, ACTION_SCOPE_AUTHORIZATION_KEYS)) { // this does not clear {@code AuthorizationServiceField.ORIGINATING_ACTION_KEY} // prior to doing any authorization lets set the originating action in the thread context // the originating action is the current action if no originating action has yet been set in the current thread context // if there is already an original action, that stays put (eg. the current action is a child action) putTransientIfNonExisting(ORIGINATING_ACTION_KEY, action); if (SystemUser.is(authentication.getUser())) { // this never goes async so no need to wrap the listener authorizeSystemUser(authentication, action, auditId, unwrappedRequest, listener); } else { final ActionListener<AuthorizationInfo> authzInfoListener = wrapPreservingContext(ActionListener.wrap( authorizationInfo -> { threadContext.putTransient(AUTHORIZATION_INFO_KEY, authorizationInfo); maybeAuthorizeRunAs(requestInfo, auditId, authorizationInfo, listener); }, listener::onFailure), threadContext); engine.resolveAuthorizationInfo(requestInfo, authzInfoListener); } } } /** * @return {@code true}	i think we should add a comment on the fact that run-as does not need to be checked for child actions and hence auditing for run-as of child actions is skipped as well.
@Override public boolean isChildActionAuthorizedByParent(RequestInfo requestInfo, String parentAction, IndicesAccessControl parentAccessControl) { final String childAction = requestInfo.getAction(); if (IndexPrivilege.CREATE_INDEX_MATCHER.test(childAction) || childAction.equals(TransportShardBulkAction.ACTION_NAME)) { // These need special handling, so don't short circuit return false; } if (isScrollRelatedAction(childAction) || isAsyncRelatedAction(childAction)) { // These need special handling, so don't short circuit return false; } if (childAction.startsWith(parentAction) == false) { // Parent action is not a true parent // We want to treat shard level actions (those that append '[s]' and/or '[p]' & '[r]') // or similar (e.g. search phases) as children, but not every action that is triggered // within another action should be authorized this way return false; } final IndicesRequest indicesRequest; if (requestInfo.getRequest() instanceof IndicesRequest) { indicesRequest = (IndicesRequest) requestInfo.getRequest(); } else { // Can only handle indices request here return false; } final String[] indices = indicesRequest.indices(); if (indices == null || indices.length == 0) { // No indices to check return false; } return Arrays.stream(indices).allMatch(idx -> { if (Regex.isSimpleMatchPattern(idx)) { // The request contains a wildcard return false; } IndicesAccessControl.IndexAccessControl iac = parentAccessControl.getIndexPermissions(idx); // The parent context has already successfully authorized access to this index (by name) return iac != null && iac.isGranted(); }); }	i think this check is redundant since the logic already mandates the request must be an instance of indicesrequest.
@Override public boolean isChildActionAuthorizedByParent(RequestInfo requestInfo, String parentAction, IndicesAccessControl parentAccessControl) { final String childAction = requestInfo.getAction(); if (IndexPrivilege.CREATE_INDEX_MATCHER.test(childAction) || childAction.equals(TransportShardBulkAction.ACTION_NAME)) { // These need special handling, so don't short circuit return false; } if (isScrollRelatedAction(childAction) || isAsyncRelatedAction(childAction)) { // These need special handling, so don't short circuit return false; } if (childAction.startsWith(parentAction) == false) { // Parent action is not a true parent // We want to treat shard level actions (those that append '[s]' and/or '[p]' & '[r]') // or similar (e.g. search phases) as children, but not every action that is triggered // within another action should be authorized this way return false; } final IndicesRequest indicesRequest; if (requestInfo.getRequest() instanceof IndicesRequest) { indicesRequest = (IndicesRequest) requestInfo.getRequest(); } else { // Can only handle indices request here return false; } final String[] indices = indicesRequest.indices(); if (indices == null || indices.length == 0) { // No indices to check return false; } return Arrays.stream(indices).allMatch(idx -> { if (Regex.isSimpleMatchPattern(idx)) { // The request contains a wildcard return false; } IndicesAccessControl.IndexAccessControl iac = parentAccessControl.getIndexPermissions(idx); // The parent context has already successfully authorized access to this index (by name) return iac != null && iac.isGranted(); }); }	should we be more conservative and have childaction.startswith(parentaction + "["). otherwise it is possible for an action to be child of itself, which i don't think makes sense in the known context. also, it is not recommended for end users to grant privileges using raw action names. now it absolutely does not make sense if child action names are used. should we add some warning or throwing error for such usages when we can detect them? it can be a separate pr.
@Override public boolean isChildActionAuthorizedByParent(RequestInfo requestInfo, String parentAction, IndicesAccessControl parentAccessControl) { final String childAction = requestInfo.getAction(); if (IndexPrivilege.CREATE_INDEX_MATCHER.test(childAction) || childAction.equals(TransportShardBulkAction.ACTION_NAME)) { // These need special handling, so don't short circuit return false; } if (isScrollRelatedAction(childAction) || isAsyncRelatedAction(childAction)) { // These need special handling, so don't short circuit return false; } if (childAction.startsWith(parentAction) == false) { // Parent action is not a true parent // We want to treat shard level actions (those that append '[s]' and/or '[p]' & '[r]') // or similar (e.g. search phases) as children, but not every action that is triggered // within another action should be authorized this way return false; } final IndicesRequest indicesRequest; if (requestInfo.getRequest() instanceof IndicesRequest) { indicesRequest = (IndicesRequest) requestInfo.getRequest(); } else { // Can only handle indices request here return false; } final String[] indices = indicesRequest.indices(); if (indices == null || indices.length == 0) { // No indices to check return false; } return Arrays.stream(indices).allMatch(idx -> { if (Regex.isSimpleMatchPattern(idx)) { // The request contains a wildcard return false; } IndicesAccessControl.IndexAccessControl iac = parentAccessControl.getIndexPermissions(idx); // The parent context has already successfully authorized access to this index (by name) return iac != null && iac.isGranted(); }); }	nit: i think it's safe to consider it a child action if there is no indices. not sure if this would actually happen, i.e. child actions have no indices while parent has them.
@Override public boolean isChildActionAuthorizedByParent(RequestInfo requestInfo, String parentAction, IndicesAccessControl parentAccessControl) { final String childAction = requestInfo.getAction(); if (IndexPrivilege.CREATE_INDEX_MATCHER.test(childAction) || childAction.equals(TransportShardBulkAction.ACTION_NAME)) { // These need special handling, so don't short circuit return false; } if (isScrollRelatedAction(childAction) || isAsyncRelatedAction(childAction)) { // These need special handling, so don't short circuit return false; } if (childAction.startsWith(parentAction) == false) { // Parent action is not a true parent // We want to treat shard level actions (those that append '[s]' and/or '[p]' & '[r]') // or similar (e.g. search phases) as children, but not every action that is triggered // within another action should be authorized this way return false; } final IndicesRequest indicesRequest; if (requestInfo.getRequest() instanceof IndicesRequest) { indicesRequest = (IndicesRequest) requestInfo.getRequest(); } else { // Can only handle indices request here return false; } final String[] indices = indicesRequest.indices(); if (indices == null || indices.length == 0) { // No indices to check return false; } return Arrays.stream(indices).allMatch(idx -> { if (Regex.isSimpleMatchPattern(idx)) { // The request contains a wildcard return false; } IndicesAccessControl.IndexAccessControl iac = parentAccessControl.getIndexPermissions(idx); // The parent context has already successfully authorized access to this index (by name) return iac != null && iac.isGranted(); }); }	nit: extra empty line.
@Override public boolean isChildActionAuthorizedByParent(RequestInfo requestInfo, String parentAction, IndicesAccessControl parentAccessControl) { final String childAction = requestInfo.getAction(); if (IndexPrivilege.CREATE_INDEX_MATCHER.test(childAction) || childAction.equals(TransportShardBulkAction.ACTION_NAME)) { // These need special handling, so don't short circuit return false; } if (isScrollRelatedAction(childAction) || isAsyncRelatedAction(childAction)) { // These need special handling, so don't short circuit return false; } if (childAction.startsWith(parentAction) == false) { // Parent action is not a true parent // We want to treat shard level actions (those that append '[s]' and/or '[p]' & '[r]') // or similar (e.g. search phases) as children, but not every action that is triggered // within another action should be authorized this way return false; } final IndicesRequest indicesRequest; if (requestInfo.getRequest() instanceof IndicesRequest) { indicesRequest = (IndicesRequest) requestInfo.getRequest(); } else { // Can only handle indices request here return false; } final String[] indices = indicesRequest.indices(); if (indices == null || indices.length == 0) { // No indices to check return false; } return Arrays.stream(indices).allMatch(idx -> { if (Regex.isSimpleMatchPattern(idx)) { // The request contains a wildcard return false; } IndicesAccessControl.IndexAccessControl iac = parentAccessControl.getIndexPermissions(idx); // The parent context has already successfully authorized access to this index (by name) return iac != null && iac.isGranted(); }); }	this check means the performance boost does not apply to shard level requests if the original request has wildcard since indices() returns the originalindices. i think this is what we wanted for this pr to solve a particular problem. but it's worthwhile to build upon it and cover more shard level requests in future.
@Override public boolean isChildActionAuthorizedByParent(RequestInfo requestInfo, String parentAction, IndicesAccessControl parentAccessControl) { final String childAction = requestInfo.getAction(); if (IndexPrivilege.CREATE_INDEX_MATCHER.test(childAction) || childAction.equals(TransportShardBulkAction.ACTION_NAME)) { // These need special handling, so don't short circuit return false; } if (isScrollRelatedAction(childAction) || isAsyncRelatedAction(childAction)) { // These need special handling, so don't short circuit return false; } if (childAction.startsWith(parentAction) == false) { // Parent action is not a true parent // We want to treat shard level actions (those that append '[s]' and/or '[p]' & '[r]') // or similar (e.g. search phases) as children, but not every action that is triggered // within another action should be authorized this way return false; } final IndicesRequest indicesRequest; if (requestInfo.getRequest() instanceof IndicesRequest) { indicesRequest = (IndicesRequest) requestInfo.getRequest(); } else { // Can only handle indices request here return false; } final String[] indices = indicesRequest.indices(); if (indices == null || indices.length == 0) { // No indices to check return false; } return Arrays.stream(indices).allMatch(idx -> { if (Regex.isSimpleMatchPattern(idx)) { // The request contains a wildcard return false; } IndicesAccessControl.IndexAccessControl iac = parentAccessControl.getIndexPermissions(idx); // The parent context has already successfully authorized access to this index (by name) return iac != null && iac.isGranted(); }); }	i cannot think of a real issue for it. but would like to write down my thinking here in case others could find out more about it. is it possible for an index of child action having different indexaccesscontrol than the parent? so even when iac != null && iac.isgranted() returns true, iac.getdocumentpermissions may not be the same for them? a terms lookup can have this effect. but action of fetching document from another index is not a child action of the original search action (name wise). so it is not really an issue. but there maybe other things that i didn't think of?
public List<Event> events() { return events; } } public static class Hits implements Writeable, ToXContentFragment { public static final Hits EMPTY = new Hits(null, null, null); private final List<Event> events; private final List<Sequence> sequences; private final TotalHits totalHits; private static final class Fields { static final String HITS = "hits"; static final String TOTAL = "total"; static final String EVENTS = "events"; static final String SEQUENCES = "sequences"; } public Hits(@Nullable List<Event> events, @Nullable List<Sequence> sequences, @Nullable TotalHits totalHits) { this.events = events; this.sequences = sequences; this.totalHits = totalHits; } public Hits(StreamInput in) throws IOException { if (in.readBoolean()) { totalHits = Lucene.readTotalHits(in); } else { totalHits = null; } events = in.readBoolean() ? in.readList(Event::new) : null; sequences = in.readBoolean() ? in.readList(Sequence::new) : null; } @Override public void writeTo(StreamOutput out) throws IOException { final boolean hasTotalHits = totalHits != null; out.writeBoolean(hasTotalHits); if (hasTotalHits) { Lucene.writeTotalHits(out, totalHits); } if (events != null) { out.writeBoolean(true); out.writeList(events); } else { out.writeBoolean(false); } if (sequences != null) { out.writeBoolean(true); out.writeList(sequences); } else { out.writeBoolean(false); } } private static final ConstructingObjectParser<EqlSearchResponse.Hits, Void> PARSER = new ConstructingObjectParser<>("eql/search_response_hits", true, args -> { int i = 0; @SuppressWarnings("unchecked") List<Event> events = (List<Event>) args[i++]; @SuppressWarnings("unchecked") List<Sequence> sequences = (List<Sequence>) args[i++]; TotalHits totalHits = (TotalHits) args[i]; return new EqlSearchResponse.Hits(events, sequences, totalHits); }); static { PARSER.declareObjectArray(ConstructingObjectParser.optionalConstructorArg(), (p, c) -> Event.fromXContent(p), new ParseField(Fields.EVENTS)); PARSER.declareObjectArray(ConstructingObjectParser.optionalConstructorArg(), Sequence.PARSER, new ParseField(Fields.SEQUENCES)); PARSER.declareObject(ConstructingObjectParser.optionalConstructorArg(), (p, c) -> SearchHits.parseTotalHitsFragment(p), new ParseField(Fields.TOTAL)); } public static Hits fromXContent(XContentParser parser) throws IOException { return PARSER.parse(parser, null); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(Fields.HITS); if (totalHits != null) { builder.startObject(Fields.TOTAL); builder.field("value", totalHits.value); builder.field("relation", totalHits.relation == TotalHits.Relation.EQUAL_TO ? "eq" : "gte"); builder.endObject(); } if (events != null) { builder.startArray(Fields.EVENTS); for (Event event : events) { event.toXContent(builder, params); } builder.endArray(); } if (sequences != null) { builder.field(Fields.SEQUENCES, sequences); } builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } Hits that = (Hits) o; return Objects.equals(events, that.events) && Objects.equals(sequences, that.sequences) && Objects.equals(totalHits, that.totalHits); } @Override public int hashCode() { return Objects.hash(events, sequences, totalHits); } public List<Event> events() { return this.events; } public List<Sequence> sequences() { return this.sequences; } public TotalHits totalHits() { return this.totalHits; }	this will break requests in mixed clusters, are we ok with that?
private FieldCapabilitiesRequest randomRequest() { FieldCapabilitiesRequest request = new FieldCapabilitiesRequest(); int size = randomIntBetween(1, 20); String[] randomFields = new String[size]; for (int i = 0; i < size; i++) { randomFields[i] = randomAlphaOfLengthBetween(5, 10); } size = randomIntBetween(0, 20); String[] randomIndices = new String[size]; for (int i = 0; i < size; i++) { randomIndices[i] = randomAlphaOfLengthBetween(5, 10); } request.fields(randomFields); request.indices(randomIndices); if (randomBoolean()) { request.indicesOptions(randomBoolean() ? IndicesOptions.strictExpand() : IndicesOptions.lenientExpandOpen()); } return request; }	maybe you can use equalshashcodetestutils#checkequalsandhashcode and save yourself some boilerplate here?
public void testDetect_GivenFloatField() { FieldCapabilitiesResponse fieldCapabilities = new MockFieldCapsResponseBuilder() .addAggregatableField("some_float", "float").build(); ExtractedFieldsDetector extractedFieldsDetector = new ExtractedFieldsDetector( buildOutlierDetectionConfig(), 100, fieldCapabilities, Collections.emptyMap()); Tuple<ExtractedFields, List<FieldSelection>> fieldExtraction = extractedFieldsDetector.detect(); List<ExtractedField> allFields = fieldExtraction.v1().getAllFields(); assertThat(allFields, hasSize(1)); assertThat(allFields.get(0).getName(), equalTo("some_float")); assertThat(allFields.get(0).getMethod(), equalTo(ExtractedField.Method.DOC_VALUE)); assertFieldSelectionContains(fieldExtraction.v2(), FieldSelection.included("some_float", Collections.singleton("float"), false, FieldSelection.FeatureType.NUMERICAL)); }	is it now possible to remove source_index constant? or is it still used?
@Override public void deleteBlobsIgnoringIfNotExists(List<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } final PlainActionFuture<Collection<Void>> result = PlainActionFuture.newFuture(); final GroupedActionListener<Void> listener = new GroupedActionListener<>(result, blobNames.size()); final ExecutorService executor = threadPool.executor(AzureRepositoryPlugin.REPOSITORY_THREAD_POOL_NAME); // Executing deletes in parallel since Azure SDK 8 is using blocking IO while Azure does not provide a bulk delete API endpoint. // TODO: Upgrade to newer non-blocking Azure SDK 11 and execute delete requests in parallel that way. for (String blobName : blobNames) { executor.submit(new ActionRunnable<>(listener) { @Override protected void doRun() throws IOException { deleteBlobIgnoringIfNotExists(blobName); listener.onResponse(null); } }); } try { result.actionGet(); } catch (Exception e) { throw new IOException("Exception during bulk delete.", e); } }	remove the . at the end of the exception message.
@Override protected AggregatorFactory doBuild(SearchContext context, AggregatorFactory parent, Builder subFactoriesBuilder) throws IOException { int maxFilters = SearchModule.INDICES_MAX_CLAUSE_COUNT_SETTING.get(context.indexShard().indexSettings().getSettings()); if (filters.size() > maxFilters){ throw new IllegalArgumentException( "Number of filters is too large, must be less than or equal to: [" + maxFilters + "] but was [" + filters.size() + "]." + "This limit can be set by changing the [" + SearchModule.INDICES_MAX_CLAUSE_COUNT_SETTING.getKey() + "] setting."); } List<KeyedFilter> rewrittenFilters = new ArrayList<>(filters.size()); for (KeyedFilter kf : filters) { rewrittenFilters.add(new KeyedFilter(kf.key(), Rewriteable.rewrite(kf.filter(), context.getQueryShardContext(), true))); } return new AdjacencyMatrixAggregatorFactory(name, rewrittenFilters, separator, context, parent, subFactoriesBuilder, metaData); }	i don't think this is correct since you are looking up a node setting in the index settings?
public static String apiKeyHeaderValue(SecureString apiKey) { CharBuffer chars = CharBuffer.allocate(apiKey.length()); byte[] charBytes = null; try { chars.put(apiKey.getChars()); charBytes = CharArrays.toUtf8Bytes(chars.array()); //TODO we still have passwords in Strings in headers. Maybe we can look into using a CharSequence? String apiKeyToken = Base64.getEncoder().encodeToString(charBytes); return "ApiKey " + apiKeyToken; } finally { Arrays.fill(chars.array(), (char) 0); if (charBytes != null) { Arrays.fill(charBytes, (byte) 0); } } } /** * Returns a TrustManager to be used in a client SSLContext, which trusts all certificates that are signed * by a specific CA certificate ( identified by its SHA256 fingerprint, {@code pinnedCaCertFingerPrint}	it is intentional that we do pinning to the ca certificate instead of the leaf certificate. the enroll kibana api could* return multiple addresses for different existing nodes in the cluster and we wanted a way to be able to bootstrap trust for a connection to any of these nodes, with a singe piece of information. \\\\* it currently doesn't, it returns addresses from the node that serves the api request, but the original design did and future potential work could introduce this.
public void setHttpAsyncResponseConsumerFactory(HttpAsyncResponseConsumerFactory httpAsyncResponseConsumerFactory) { this.httpAsyncResponseConsumerFactory = Objects.requireNonNull(httpAsyncResponseConsumerFactory, "httpAsyncResponseConsumerFactory cannot be null"); } /** * Handler for warnings or null if the request should use * {@link RestClientBuilder#setStrictDeprecationMode}	i find myself having to think a lot about this sentence. almost like it needs brackets to parse the logic. maybe i just need another coffee.
public void completed(HttpResponse httpResponse) { try { RequestLogger.logResponse(logger, request, node.getHost(), httpResponse); int statusCode = httpResponse.getStatusLine().getStatusCode(); Response response = new Response(request.getRequestLine(), node.getHost(), httpResponse); if (isSuccessfulResponse(statusCode) || ignoreErrorCodes.contains(response.getStatusLine().getStatusCode())) { onResponse(node); if (response.hasWarnings() && thisWarningsHandler.warningsShouldFailRequest(response.getWarnings())) { listener.onDefinitiveFailure(new ResponseException(response)); } else { listener.onSuccess(response); } } else { ResponseException responseException = new ResponseException(response); if (isRetryStatus(statusCode)) { //mark host dead and retry against next one onFailure(node); retryIfPossible(responseException); } else { //mark host alive and don't retry, as the error should be a request problem onResponse(node); listener.onDefinitiveFailure(responseException); } } } catch(Exception e) { listener.onDefinitiveFailure(e); } }	here we lose the option to fail if an expected warning didn't materialise. is that ok? its conceivable some tests can be certain in their expectations of failure and we do need a way to fail when we fail to fail (so to speak).
protected RestClient buildClient(Settings settings, HttpHost[] hosts) throws IOException { RestClientBuilder builder = RestClient.builder(hosts); configureClient(builder, settings); builder.setStrictDeprecationMode(getStrictDeprecationMode()); return builder.build(); } /** * Whether the used REST client should return any response containing at * least one warning header as a failure. * @deprecated always run in strict mode and use * {@link RequestOptions.Builder#setWarningsHandler}	got it. now i see the relationship between warningshandler and setstrictdeprecationmode. maybe make that earlier javadoc more explicit about the fact setstrictdeprecationmode has been deprecated?
* @param loader the function to compute a value given a key * @return the current (existing or computed) non-null value associated with the specified key * @throws ExecutionException thrown if loader throws an exception or returns a null value */ public V computeIfAbsent(K key, CacheLoader<K, V> loader) throws ExecutionException { long now = now(); // we have to eagerly evict or our putIfAbsent call below will fail V value = get(key, now, e -> { try (ReleasableLock ignored = lruLock.acquire()) { evictEntry(e); } }); if (value == null) { // we need to synchronize loading of a value for a given key; however, holding the segment lock while // invoking load can lead to deadlock against another thread due to dependent key loading; therefore, we // need a mechanism to ensure that load is invoked at most once, but we are not invoking load while holding // the segment lock; to do this, we atomically put a future in the map that can load the value, and then // get the value from this future on the thread that won the race to place the future into the segment map CacheSegment<K, V> segment = getCacheSegment(key); CompletableFuture<Entry<K, V>> future; CompletableFuture<Entry<K, V>> completableFuture = new CompletableFuture<>(); try (ReleasableLock ignored = segment.writeLock.acquire()) { future = segment.map.putIfAbsent(key, completableFuture); } BiFunction<? super Entry<K, V>, Throwable, ? extends V> handler = (ok, ex) -> { if (ok != null) { try (ReleasableLock ignored = lruLock.acquire()) { promote(ok, now); } return ok.value; } else { try (ReleasableLock ignored = segment.writeLock.acquire()) { CompletableFuture<Entry<K, V>> sanity = segment.map.get(key); if (sanity != null && sanity.isCompletedExceptionally()) { segment.map.remove(key); } } return null; } }; CompletableFuture<V> completableValue; if (future == null) { future = completableFuture; completableValue = future.handle(handler); V loaded; try { loaded = loader.load(key); } catch (Exception e) { future.completeExceptionally(e); throw new ExecutionException(e); } if (loaded == null) { NullPointerException npe = new NullPointerException("loader returned a null value"); future.completeExceptionally(npe); throw new ExecutionException(npe); } else { future.complete(new Entry<>(key, loaded, now)); } } else { completableValue = future.handle(handler); } try { value = completableValue.get(); // check to ensure the future hasn't been completed with an exception if (future.isCompletedExceptionally()) { future.get(); // call get to force the exception to be thrown for other concurrent callers throw new IllegalStateException("the future was completed exceptionally but no exception was thrown"); } } catch (InterruptedException e) { throw new IllegalStateException(e); } } return value; }	have to eagerly evict or ... -> "have to eagerly evict expired values or ..."
private int readDirectly(long start, long end, byte[] buffer, int offset) throws IOException { final long length = end - start; final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, length))]; logger.trace(() -> new ParameterizedMessage("direct reading of range [{}-{}] for cache file [{}]", start, end, cacheFileReference)); int bytesCopied = 0; final long startTimeNanos = directory.statsCurrentTimeNanos(); try (InputStream input = openInputStream(start, length)) { stats.incrementInnerOpenCount(); long remaining = end - start; while (remaining > 0) { final int len = (remaining < copyBuffer.length) ? (int) remaining : copyBuffer.length; int bytesRead = input.read(copyBuffer, 0, len); if (bytesRead == -1) { throw new EOFException(String.format(Locale.ROOT, "unexpected EOF reading [%d-%d] ([%d] bytes remaining) from %s", start, end, remaining, cacheFileReference)); } System.arraycopy(copyBuffer, 0, buffer, offset + bytesCopied, bytesRead); bytesCopied += bytesRead; remaining -= bytesRead; } final long endTimeNanos = directory.statsCurrentTimeNanos(); stats.addDirectBytesRead(bytesCopied, endTimeNanos - startTimeNanos); } return bytesCopied; }	replaces len with bytesread in case of a partial read. i think this was technically ok before since we would proceed to overwrite the junk bytes with correct ones, but it was confusing.
public int paramAsInt(String key, int defaultValue) { String sValue = param(key); if (sValue == null) { return defaultValue; } try { return Integer.parseInt(sValue); } catch (NumberFormatException e) { throw new IllegalArgumentException("Failed to parse int parameter [" + key + "] with value [" + sValue + "]", e); } }	this method isn't needed since i think the boxing change should be reverted.
* {@link Relation#INTERSECTS}, which is always fine to return when there is * no way to check whether values are actually within bounds. */ public Relation isFieldWithinQuery( IndexReader reader, Object from, Object to, boolean includeLower, boolean includeUpper, ZoneId timeZone, DateMathParser dateMathParser, QueryRewriteContext context) throws IOException { return Relation.INTERSECTS; } /** * Return whether all values of the given {@link IndexReader} are within the range, * outside the range or cross the range. The default implementation returns * {@link Relation#INTERSECTS}, which is always fine to return when there is * no way to check whether values are actually within bounds. * * Use this instead of * {@link #isFieldWithinQuery(IndexReader, Object, Object, boolean, boolean, ZoneId, DateMathParser, QueryRewriteContext)} * when you *know* the {@code fromInclusive} and {@code toInclusive}	i think you mean they're always longs?
public static void assertResponse(Map<String, Object> expected, Map<String, Object> actual) { if (false == expected.equals(actual)) { NotEqualMessageBuilder message = new NotEqualMessageBuilder(); message.compareMaps(actual, expected); fail("Response does not match:\\\\n" + message.toString()); } } /** * Generates the map of a JSON object whose paths to a value are of a specified depth, having mapped all possible path combinations: * {"a": {"b": {"c": 1}}, "a": {"b.c": [2, 3]}, "a.b": [{"c": 4}, {"c": 5}], "a.b.c": ...} * Some of the paths are randomly multiplied: * {"a": [{"b": {"c": 1}}}, {"b": {"c": 1}}	why not passing the supplier to the constructor as well and initialize all the internal variables - paths, map, values - in the constructor and from outside you only call the geter methods. the way it is now, the caller of this class needs to call the constructor and, then, call the generate method to populate all the internal variables. and i don't see why it should leave room for trial&error from the user of this class. restricting everything to the constructor makes the user of the class just call the constructor (to have everything ready) and then get the internal variables values by calling geters.
public static void assertResponse(Map<String, Object> expected, Map<String, Object> actual) { if (false == expected.equals(actual)) { NotEqualMessageBuilder message = new NotEqualMessageBuilder(); message.compareMaps(actual, expected); fail("Response does not match:\\\\n" + message.toString()); } } /** * Generates the map of a JSON object whose paths to a value are of a specified depth, having mapped all possible path combinations: * {"a": {"b": {"c": 1}}, "a": {"b.c": [2, 3]}, "a.b": [{"c": 4}, {"c": 5}], "a.b.c": ...} * Some of the paths are randomly multiplied: * {"a": [{"b": {"c": 1}}}, {"b": {"c": 1}}	why new hashmap(map) and not returning the map itself?
public void testQueryRewrite() throws Exception { assertAcked(client().admin().indices().prepareCreate("index").addMapping("type", "s", "type=date") .setSettings(IndicesRequestCache.INDEX_CACHE_REQUEST_ENABLED_SETTING.getKey(), true, IndexMetaData.SETTING_NUMBER_OF_SHARDS, 5, IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .get()); indexRandom(true, client().prepareIndex("index", "type", "1").setRouting("1").setSource("s", "2016-03-19"), client().prepareIndex("index", "type", "2").setRouting("1").setSource("s", "2016-03-20"), client().prepareIndex("index", "type", "3").setRouting("1").setSource("s", "2016-03-21"), client().prepareIndex("index", "type", "4").setRouting("2").setSource("s", "2016-03-22"), client().prepareIndex("index", "type", "5").setRouting("2").setSource("s", "2016-03-23"), client().prepareIndex("index", "type", "6").setRouting("2").setSource("s", "2016-03-24"), client().prepareIndex("index", "type", "7").setRouting("3").setSource("s", "2016-03-25"), client().prepareIndex("index", "type", "8").setRouting("3").setSource("s", "2016-03-26"), client().prepareIndex("index", "type", "9").setRouting("3").setSource("s", "2016-03-27")); ensureSearchable("index"); assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L)); assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L)); final SearchResponse r1 = client().prepareSearch("index").setSearchType(SearchType.QUERY_THEN_FETCH).setSize(0) .setQuery(QueryBuilders.rangeQuery("s").gte("2016-03-19").lte("2016-03-25")).get(); assertSearchResponse(r1); assertThat(r1.getHits().getTotalHits(), equalTo(7L)); assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L)); assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(5L)); final SearchResponse r2 = client().prepareSearch("index").setSearchType(SearchType.QUERY_THEN_FETCH).setSize(0) .setQuery(QueryBuilders.rangeQuery("s").gte("2016-03-20").lte("2016-03-26")).get(); assertSearchResponse(r2); assertThat(r2.getHits().getTotalHits(), equalTo(7L)); assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(3L)); assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(7L)); final SearchResponse r3 = client().prepareSearch("index").setSearchType(SearchType.QUERY_THEN_FETCH).setSize(0) .setQuery(QueryBuilders.rangeQuery("s").gte("2016-03-21").lte("2016-03-27")).get(); assertSearchResponse(r3); assertThat(r3.getHits().getTotalHits(), equalTo(7L)); assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(6L)); assertThat(client().admin().indices().prepareStats("index").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(9L)); }	i don't think this is required after #19450. @abeyad, do you know for sure?
Query getRetentionQuery() { return new BooleanQuery.Builder() .add(LongPoint.newRangeQuery(SeqNoFieldMapper.NAME, getMinRetainedSeqNo(), Long.MAX_VALUE), BooleanClause.Occur.SHOULD) .add(NumericDocValuesField.newSlowRangeQuery(SeqNoFieldMapper.UPDATED_BY_SEQNO_NAME, globalCheckpointSupplier.getAsLong() + 1, Long.MAX_VALUE), BooleanClause.Occur.SHOULD) .build(); }	adding a doc-value query as a should clause might mean we need to do a linear scan to find matches of this query. we don't have much of a choice so i would still let the change in and benchmark, but we might want to still add a comment here.
public void testRestoreShrinkIndex() throws Exception { logger.info("--> starting a master node and a data node"); internalCluster().startMasterOnlyNode(); internalCluster().startDataOnlyNode(); final Client client = client(); final String repo = "test-repo"; final String snapshot = "test-snap"; final String sourceIdx = "test-idx"; final String shrunkIdx = "test-idx-shrunk"; logger.info("--> creating repository"); assertAcked(client.admin().cluster().preparePutRepository(repo).setType("fs") .setSettings(Settings.builder().put("location", randomRepoPath()) .put("compress", randomBoolean()))); assertAcked(prepareCreate(sourceIdx, 0, Settings.builder() .put("number_of_shards", between(1, 20)).put("number_of_replicas", 0))); ensureGreen(); logger.info("--> indexing some data"); IndexRequestBuilder[] builders = new IndexRequestBuilder[randomIntBetween(10, 100)]; for (int i = 0; i < builders.length; i++) { builders[i] = client().prepareIndex(sourceIdx, "type1", Integer.toString(i)).setSource("field1", "bar " + i); } indexRandom(true, builders); flushAndRefresh(); logger.info("--> shrink the index"); assertAcked(client.admin().indices().prepareUpdateSettings(sourceIdx) .setSettings(Settings.builder().put("index.blocks.write", true)).get()); assertAcked(client.admin().indices().prepareShrinkIndex(sourceIdx, shrunkIdx).get()); logger.info("--> snapshot the shrunk index"); CreateSnapshotResponse createResponse = client.admin().cluster() .prepareCreateSnapshot(repo, snapshot) .setWaitForCompletion(true).setIndices(shrunkIdx).get(); assertEquals(SnapshotState.SUCCESS, createResponse.getSnapshotInfo().state()); logger.info("--> close index and stop the data node"); assertAcked(client.admin().indices().prepareDelete(shrunkIdx).get()); internalCluster().stopRandomDataNode(); client().admin().cluster().prepareHealth().setTimeout("30s").setWaitForNodes("1"); logger.info("--> start a new data node"); final Settings dataSettings = Settings.builder() .put(Node.NODE_NAME_SETTING.getKey(), randomAlphaOfLength(5)) .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) // to get a new node id .build(); internalCluster().startDataOnlyNode(dataSettings); client().admin().cluster().prepareHealth().setTimeout("30s").setWaitForNodes("2"); logger.info("--> restore the shrunk index and ensure all shards are allocated"); RestoreSnapshotResponse restoreResponse = client().admin().cluster() .prepareRestoreSnapshot(repo, snapshot).setWaitForCompletion(true) .setIndices(shrunkIdx).get(); assertEquals(restoreResponse.getRestoreInfo().totalShards(), restoreResponse.getRestoreInfo().successfulShards()); ensureYellow(); }	removing dead code here
public static RankedListQualityMetric fromXContent(XContentParser parser, ParseFieldMatcherSupplier context) throws IOException { RankedListQualityMetric rc; Token token = parser.nextToken(); if (token != XContentParser.Token.FIELD_NAME) { throw new ParsingException(parser.getTokenLocation(), "[_na] missing required metric name"); } String metricName = parser.currentName(); switch (metricName) { case PrecisionAtN.NAME: rc = PrecisionAtN.fromXContent(parser, context); break; default: throw new ParsingException(parser.getTokenLocation(), "[_na] unknown query metric name [{}]", metricName); } if (parser.currentToken() == XContentParser.Token.END_OBJECT) { // if we are at END_OBJECT, move to the next one... parser.nextToken(); } return rc; }	i just use a switch statement here to select the actual metric implementation for parsing. we can think about using a registry or sth. later if we decide the evaluation metrics need to be plugable.
public void handleRequest(final RestRequest request, final RestChannel channel, final NodeClient client) throws IOException { RankEvalRequest rankEvalRequest = new RankEvalRequest(); BytesReference restContent = RestActions.hasBodyContent(request) ? RestActions.getRestContent(request) : null; try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) { QueryParseContext parseContext = new QueryParseContext(queryRegistry, parser, parseFieldMatcher); if (restContent != null) { parseRankEvalRequest(rankEvalRequest, request, new RankEvalContext(parseFieldMatcher, parseContext, aggregators, suggesters)); } } client.execute(RankEvalAction.INSTANCE, rankEvalRequest, new RestToXContentListener<RankEvalResponse>(channel)); }	i don't really like the fact that we need to pass the aggregator parsers and suggester parsers here, but they are needed for searchsourcebuilder.fromxcontent() when parsing the request. maybe something we can simplify along the road.
protected void doExecute(RankEvalRequest request, ActionListener<RankEvalResponse> listener) { RankEvalSpec qualityTask = request.getRankEvalSpec(); RankedListQualityMetric metric = qualityTask.getEvaluator(); double qualitySum = 0; Map<String, Collection<String>> unknownDocs = new HashMap<String, Collection<String>>(); Collection<QuerySpec> specifications = qualityTask.getSpecifications(); for (QuerySpec spec : specifications) { SearchSourceBuilder specRequest = spec.getTestRequest(); String[] indices = new String[spec.getIndices().size()]; spec.getIndices().toArray(indices); SearchRequest templatedRequest = new SearchRequest(indices, specRequest); TransportSearchAction transportSearchAction = new TransportSearchAction(settings, threadPool, searchPhaseController, transportService, searchTransportService, clusterService, actionFilters, indexNameExpressionResolver); ActionFuture<SearchResponse> searchResponse = transportSearchAction.execute(templatedRequest); SearchHits hits = searchResponse.actionGet().getHits(); EvalQueryQuality intentQuality = metric.evaluate(hits.getHits(), spec.getRatedDocs()); qualitySum += intentQuality.getQualityLevel(); unknownDocs.put(spec.getSpecId(), intentQuality.getUnknownDocs()); } RankEvalResponse response = new RankEvalResponse(); RankEvalResult result = new RankEvalResult(qualityTask.getTaskId(), qualitySum / specifications.size(), unknownDocs); response.setRankEvalResult(result); listener.onResponse(response); }	we do the summing and averaging of the single query quality metrics all here in this class. however, i think this should probably move to the individual metrics (e.g. precisionatn), unless we always want to average over the individual queries. also probably topic of a follow up task.
private void validateMappingUpdate(final String type, Mapping update) { final CountDownLatch latch = new CountDownLatch(1); final Throwable[] error = new Throwable[1]; mappingUpdatedAction.updateMappingOnMaster(indexService.index().name(), indexService.indexUUID(), type, update, new MappingUpdatedAction.MappingUpdateListener() { @Override public void onMappingUpdate() { latch.countDown(); } @Override public void onFailure(Throwable t) { latch.countDown(); error[0] = t; } }); cancellableThreads.execute(new CancellableThreads.Interruptable() { @Override public void run() throws InterruptedException { try { if (latch.await(waitForMappingUpdatePostRecovery.millis(), TimeUnit.MILLISECONDS) == false) { logger.debug("waited for mapping update on master for [{}], yet timed out", type); } else { if (error[0] != null) { throw new IndexShardGatewayRecoveryException(shardId, "Failed to propagate mappings on master post recovery", error[0]); } } } catch (InterruptedException e) { logger.debug("interrupted while waiting for mapping update"); throw e; } } }); }	waitformappingupdatepostrecovery defaults to 30s, give that the timeout now results in a failed shard (good!) i think we should be more lenient. especially given the fact that local gateway recovery runs on full cluster restart where the master might be overloaded by things to do. how about something very conservative like 15m (which is what we use for the same update mapping - see recoverysourcehandler#updatemappingonmaster)
boolean isOnlySystem(BulkRequest request, SortedMap<String, IndexAbstraction> indicesLookup, SystemIndices systemIndices) { final boolean onlySystem = request.getIndices().stream().allMatch(indexName -> { final IndexAbstraction abstraction = indicesLookup.get(indexName); if (abstraction != null) { return abstraction.isSystem(); } else { return systemIndices.isSystemIndex(indexName); } }); return onlySystem; }	why can't this be simplified to just abstraction.issystem()?
public void testIngestLocal() throws Exception { Exception exception = new Exception("fake exception"); BulkRequest bulkRequest = new BulkRequest(); IndexRequest indexRequest1 = new IndexRequest("index").id("id"); indexRequest1.source(Collections.emptyMap()); indexRequest1.setPipeline("testpipeline"); IndexRequest indexRequest2 = new IndexRequest("index").id("id"); indexRequest2.source(Collections.emptyMap()); indexRequest2.setPipeline("testpipeline"); bulkRequest.add(indexRequest1); bulkRequest.add(indexRequest2); AtomicBoolean responseCalled = new AtomicBoolean(false); AtomicBoolean failureCalled = new AtomicBoolean(false); ActionTestUtils.execute(action, null, bulkRequest, ActionListener.wrap( response -> { BulkItemResponse itemResponse = response.iterator().next(); assertThat(itemResponse.getFailure().getMessage(), containsString("fake exception")); responseCalled.set(true); }, e -> { assertThat(e, sameInstance(exception)); failureCalled.set(true); })); // check failure works, and passes through to the listener assertFalse(action.isExecuted); // haven't executed yet assertFalse(responseCalled.get()); assertFalse(failureCalled.get()); verify(ingestService).executeBulkRequest(eq(bulkRequest.numberOfActions()), bulkDocsItr.capture(), failureHandler.capture(), completionHandler.capture(), any(), eq(Names.WRITE)); completionHandler.getValue().accept(null, exception); assertTrue(failureCalled.get()); // now check success Iterator<DocWriteRequest<?>> req = bulkDocsItr.getValue().iterator(); failureHandler.getValue().accept(0, exception); // have an exception for our one index request indexRequest2.setPipeline(IngestService.NOOP_PIPELINE_NAME); // this is done by the real pipeline execution service when processing completionHandler.getValue().accept(DUMMY_WRITE_THREAD, null); assertTrue(action.isExecuted); assertFalse(responseCalled.get()); // listener would only be called by real index action, not our mocked one verifyZeroInteractions(transportService); }	there are a number of instances where we now verify that we're using the write threadpool here. could we add a test here to check the logic that determines when to use system_write?
private Version computePreviousFirstMinor() { List<Version> allVersions = DeclaredVersionsHolder.DECLARED_VERSIONS; for (int i = allVersions.size() - 1; i >= 0; i--) { Version v = allVersions.get(i); if (v.before(this) && (v.minor < this.minor || v.major < this.major)) { return fromId(v.major * 1000000 + v.minor * 10000 + 99); } } throw new IllegalArgumentException("couldn't find any released versions of the minor before [" + this + "]"); }	can't we do the same that we do for the previousmajor? why go through all the declared version?
@Override protected void masterOperation(Request request, ClusterState state, ActionListener<Response> listener) { IndexLifecycleMetadata metadata = clusterService.state().metaData().custom(IndexLifecycleMetadata.TYPE); if (metadata == null) { listener.onFailure(new ResourceNotFoundException("Lifecycle policy not found: {}", Arrays.toString(request.getPolicyNames()))); } else { List<LifecyclePolicyMetadata> requestedPolicies; // if no policies explicitly provided, behave as if `*` was specified if (request.getPolicyNames().length == 0) { requestedPolicies = new ArrayList<>(metadata.getPolicyMetadatas().values()); } else { requestedPolicies = new ArrayList<>(request.getPolicyNames().length); for (String name : request.getPolicyNames()) { LifecyclePolicyMetadata policyMetadata = metadata.getPolicyMetadatas().get(name); if (policyMetadata == null) { listener.onFailure(new ResourceNotFoundException("Lifecycle policy not found: {}", name)); return; } requestedPolicies.add(policyMetadata); } } List<LifecyclePolicyResponseItem> responseItems = new ArrayList<>(requestedPolicies.size()); for (LifecyclePolicyMetadata policyMetadata : requestedPolicies) { responseItems.add(new LifecyclePolicyResponseItem(policyMetadata.getPolicy(), policyMetadata.getVersion(), policyMetadata.getCreationDateString())); } listener.onResponse(new Response(responseItems)); } }	can we not create the list of lifecyclepolicyresponseitem directly rather than first creating the list of lifecyclepolicymetadata?
*/ public List<CancellableTask> setBan(TaskId parentTaskId, String reason) { logger.trace("setting ban for the parent task {} {}", parentTaskId, reason); // Set the ban first, so the newly created tasks cannot be registered synchronized (banedParents) { if (lastDiscoveryNodes.nodeExists(parentTaskId.getNodeId())) { // Only set the ban if the node is the part of the cluster banedParents.put(parentTaskId, reason); } } return cancellableTasks.values().stream() .filter(t -> t.hasParent(parentTaskId)) .map(t -> t.task) .collect(Collectors.toUnmodifiableList()); }	i wonder if we need to eventually optimize this, in case there might be a very large list of cancellable tasks. also, this does not have proper happens-before, as iterating a concurrent map after an element has been added is not guaranteed to yield the element (it's eventually consistent).
@Override protected void logLine(String line) { String progress = progressHandler.apply(line); if (progress != null) { progressLogger.progress(progress); } forward.println(" [" + extension.getBox() +"] " + line); }	this was actually leftover from debugging. i don't think we want this always going to stdout. in fact, this progressoutputstream is meant only to write to the progress logger.
public void execute(Action<VagrantExecSpec> action) { VagrantExecSpec vagrantSpec = new VagrantExecSpec(); action.execute(vagrantSpec); Objects.requireNonNull(vagrantSpec.command); project.exec(execSpec -> { execSpec.setExecutable("vagrant"); File vagrantfile = extension.getVagrantfile(); execSpec.setEnvironment(System.getenv()); // pass through env execSpec.environment("VAGRANT_CWD", vagrantfile.getParentFile().toString()); execSpec.environment("VAGRANT_VAGRANTFILE", vagrantfile.getName()); execSpec.environment("VAGRANT_LOG", "debug"); extension.getHostEnv().forEach(execSpec::environment); execSpec.args(vagrantSpec.command); if (vagrantSpec.subcommand != null) { execSpec.args(vagrantSpec.subcommand); } execSpec.args(extension.getBox()); if (vagrantSpec.args != null) { execSpec.args(Arrays.asList(vagrantSpec.args)); } // output from vagrant needs to be manually curated because --machine-readable isn't actually "readable" UnaryOperator<String> progressHandler = vagrantSpec.progressHandler; if (progressHandler == null) { progressHandler = new VagrantProgressLogger("==> " + extension.getBox() + ": "); } execSpec.setStandardOutput(new ProgressOutputStream(vagrantSpec.command, progressHandler, System.out)); try { execSpec.setErrorOutput( Files.newOutputStream(project.getBuildDir().toPath().resolve("vagrant.log"), CREATE, APPEND) ); } catch (IOException e) { throw new UncheckedIOException(e); } }); }	i think the tee was fine? we just need a wrapper around the tee output stream to prefix each line. we used to do this and lost it somewhere.
public void execute(Action<VagrantExecSpec> action) { VagrantExecSpec vagrantSpec = new VagrantExecSpec(); action.execute(vagrantSpec); Objects.requireNonNull(vagrantSpec.command); project.exec(execSpec -> { execSpec.setExecutable("vagrant"); File vagrantfile = extension.getVagrantfile(); execSpec.setEnvironment(System.getenv()); // pass through env execSpec.environment("VAGRANT_CWD", vagrantfile.getParentFile().toString()); execSpec.environment("VAGRANT_VAGRANTFILE", vagrantfile.getName()); execSpec.environment("VAGRANT_LOG", "debug"); extension.getHostEnv().forEach(execSpec::environment); execSpec.args(vagrantSpec.command); if (vagrantSpec.subcommand != null) { execSpec.args(vagrantSpec.subcommand); } execSpec.args(extension.getBox()); if (vagrantSpec.args != null) { execSpec.args(Arrays.asList(vagrantSpec.args)); } // output from vagrant needs to be manually curated because --machine-readable isn't actually "readable" UnaryOperator<String> progressHandler = vagrantSpec.progressHandler; if (progressHandler == null) { progressHandler = new VagrantProgressLogger("==> " + extension.getBox() + ": "); } execSpec.setStandardOutput(new ProgressOutputStream(vagrantSpec.command, progressHandler, System.out)); try { execSpec.setErrorOutput( Files.newOutputStream(project.getBuildDir().toPath().resolve("vagrant.log"), CREATE, APPEND) ); } catch (IOException e) { throw new UncheckedIOException(e); } }); }	loggedexec has an option to spool the output to disk (although we may need to convert the static exec method to support that option).
@Override protected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException { EqlSearchRequest eqlRequest; try (XContentParser parser = request.contentOrSourceParamParser()) { eqlRequest = EqlSearchRequest.fromXContent(parser); eqlRequest.indices(Strings.splitStringByCommaToArray(request.param("index"))); eqlRequest.indicesOptions(IndicesOptions.fromRequest(request, eqlRequest.indicesOptions())); if (request.hasParam("wait_for_completion_timeout")) { eqlRequest.waitForCompletionTimeout( request.paramAsTime("wait_for_completion_timeout", eqlRequest.waitForCompletionTimeout())); } if (request.hasParam("keep_alive")) { eqlRequest.keepAlive(request.paramAsTime("keep_alive", eqlRequest.keepAlive())); } eqlRequest.keepOnCompletion(request.paramAsBoolean("keep_on_completion", eqlRequest.keepOnCompletion())); } return channel -> client.execute(EqlSearchAction.INSTANCE, eqlRequest, new RestResponseListener<>(channel) { @Override public RestResponse buildResponse(EqlSearchResponse response) throws Exception { XContentBuilder builder = channel.newBuilder(request.getXContentType(), XContentType.JSON, true); response.toXContent(builder, request); return new BytesRestResponse(RestStatus.OK, builder); } }); }	you should use restcancellablenodeclient in order to get the automatic cancellation if the user closes the rest channel ?
public void testDanglingIndicesCanBeImported() throws Exception { internalCluster().startNodes(3, buildSettings(0, true, false)); final String stoppedNodeName = createDanglingIndices(INDEX_NAME); final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME); final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true); client().admin().cluster().importDanglingIndex(request).get(); assertTrue("Expected dangling index " + INDEX_NAME + " to be recovered", indexExists(INDEX_NAME)); }	getting us a proper stack trace :)
public void apply(Project project) { Attribute<Boolean> jdkAttribute = Attribute.of("jdk", Boolean.class); project.getDependencies().getAttributesSchema().attribute(jdkAttribute); project.getDependencies().getArtifactTypes().maybeCreate(ArtifactTypeDefinition.ZIP_TYPE); project.getDependencies().registerTransform(JdkUnzipTransform.class, transformSpec -> { transformSpec.getFrom() .attribute(ArtifactAttributes.ARTIFACT_FORMAT, ArtifactTypeDefinition.ZIP_TYPE) .attribute(jdkAttribute, true); transformSpec.getTo() .attribute(ArtifactAttributes.ARTIFACT_FORMAT, ArtifactTypeDefinition.DIRECTORY_TYPE) .attribute(jdkAttribute, true); ; }); ArtifactTypeDefinition tarArtifactTypeDefinition = project.getDependencies().getArtifactTypes().maybeCreate("tar.gz"); project.getDependencies().registerTransform(JdkSymbolicLinkPreservingUntarTransform.class, transformSpec -> { transformSpec.getFrom() .attribute(ArtifactAttributes.ARTIFACT_FORMAT, tarArtifactTypeDefinition.getName()) .attribute(jdkAttribute, true); transformSpec.getTo() .attribute(ArtifactAttributes.ARTIFACT_FORMAT, ArtifactTypeDefinition.DIRECTORY_TYPE) .attribute(jdkAttribute, true); }); NamedDomainObjectContainer<Jdk> jdksContainer = project.container(Jdk.class, name -> { Configuration configuration = project.getConfigurations().create("jdk_" + name); configuration.setCanBeConsumed(false); configuration.getAttributes().attribute(ArtifactAttributes.ARTIFACT_FORMAT, ArtifactTypeDefinition.DIRECTORY_TYPE); configuration.getAttributes().attribute(jdkAttribute, true); Jdk jdk = new Jdk(name, configuration, project.getObjects()); configuration.defaultDependencies(dependencies -> { jdk.finalizeValues(); setupRepository(project, jdk); dependencies.add(project.getDependencies().create(dependencyNotation(jdk))); }); return jdk; }); project.getExtensions().add(EXTENSION_NAME, jdksContainer); }	just to clear my confusion, this can be _resolved_ but not _consumed_ which just means it can't be used by another project, right?
@Override public void onRecoveryFailure(RecoveryState state, RecoveryFailedException e, boolean sendShardFailure) { handleRecoveryFailure(indexService, shardRouting, sendShardFailure, e); } } private void handleRecoveryFailure(IndexService indexService, ShardRouting shardRouting, boolean sendShardFailure, Throwable failure) { synchronized (mutex) { failAndRemoveShard(shardRouting, indexService, sendShardFailure, "failed recovery", failure); } } private void removeIndex(String index, String reason) { try { indicesService.removeIndex(index, reason); } catch (Throwable e) { logger.warn("failed to clean index ({})", e, reason); } clearSeenMappings(index); } private void clearSeenMappings(String index) { // clear seen mappings as well for (Tuple<String, String> tuple : seenMappings.keySet()) { if (tuple.v1().equals(index)) { seenMappings.remove(tuple); } } } private void deleteIndex(String index, String reason) { try { indicesService.deleteIndex(index, reason); } catch (Throwable e) { logger.warn("failed to delete index ({})", e, reason); } // clear seen mappings as well clearSeenMappings(index); } private void failAndRemoveShard(ShardRouting shardRouting, IndexService indexService, boolean sendShardFailure, String message, @Nullable Throwable failure) { if (indexService.hasShard(shardRouting.getId())) { try { indexService.removeShard(shardRouting.getId(), message); } catch (ShardNotFoundException e) { // the node got closed on us, ignore it } catch (Throwable e1) { logger.warn("[{}][{}] failed to remove shard after failure ([{}])", e1, shardRouting.getIndex(), shardRouting.getId(), message); } } if (sendShardFailure) { sendFailShard(shardRouting, indexService.indexUUID(), message, failure); } } private void sendFailShard(ShardRouting shardRouting, String indexUUID, String message, @Nullable Throwable failure) { try { logger.warn("[{}] marking and sending shard failed due to [{}]", failure, shardRouting.shardId(), message); failedShards.put(shardRouting.shardId(), new FailedShard(shardRouting.version())); shardStateAction.shardFailed(shardRouting, indexUUID, message, failure); } catch (Throwable e1) { logger.warn("[{}][{}] failed to mark shard as failed (because of [{}])", e1, shardRouting.getIndex(), shardRouting.getId(), message); } } private class FailedEngineHandler implements Callback<IndexShard.ShardFailure> { @Override public void handle(final IndexShard.ShardFailure shardFailure) { final IndexService indexService = indicesService.indexService(shardFailure.routing.shardId().index().name()); final ShardRouting shardRouting = shardFailure.routing; threadPool.generic().execute(() -> { synchronized (mutex) { failAndRemoveShard(shardRouting, indexService, true, "engine failure, reason [" + shardFailure.reason + "]", shardFailure.cause); } }); }	we used to protect agains null here -> i think we should still do this? i'm thinking about failures that happen during index deletion..
@Override public void handle(final IndexShard.ShardFailure shardFailure) { final IndexService indexService = indicesService.indexService(shardFailure.routing.shardId().index().name()); final ShardRouting shardRouting = shardFailure.routing; threadPool.generic().execute(() -> { synchronized (mutex) { failAndRemoveShard(shardRouting, indexService, true, "engine failure, reason [" + shardFailure.reason + "]", shardFailure.cause); } }); }	engine failure -> shard failure
*/ public void putComponentTemplate(final String cause, final boolean create, final String name, final TimeValue masterTimeout, final ComponentTemplate template, final ActionListener<AcknowledgedResponse> listener) { clusterService.submitStateUpdateTask("create-component-template [" + name + "], cause [" + cause + "]", new ClusterStateUpdateTask(Priority.URGENT) { @Override public TimeValue timeout() { return masterTimeout; } @Override public void onFailure(String source, Exception e) { listener.onFailure(e); } @Override public ClusterState execute(ClusterState currentState) { if (create && currentState.metaData().componentTemplates().containsKey(name)) { throw new IllegalArgumentException("component_template [" + name + "] already exists"); } // TODO: validation of component template // validateAndAddTemplate(request, templateBuilder, indicesService, xContentRegistry); logger.info("adding component template [{}]", name); return ClusterState.builder(currentState) .metaData(MetaData.builder(currentState.metaData()).put(name, template)) .build(); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { listener.onResponse(new AcknowledgedResponse(true)); } }); }	maybe move the body of this method and remove method into a static method? this will make writing unit tests easier.
* @param fileListGeneration the generation number of the snapshot index file * @param blobs list of blobs in the container * @param reason a reason explaining why the shard index file is written */ protected void finalize(final List<SnapshotFiles> snapshots, final int fileListGeneration, final Map<String, BlobMetaData> blobs, final String reason) { final String indexGeneration = Integer.toString(fileListGeneration); try { final List<String> blobsToDelete; if (snapshots.isEmpty()) { // If we deleted all snapshots, we don't need to create a new index file and simply delete all the blobs we found blobsToDelete = List.copyOf(blobs.keySet()); } else { final BlobStoreIndexShardSnapshots updatedSnapshots = new BlobStoreIndexShardSnapshots(snapshots); indexShardSnapshotsFormat.writeAtomic(updatedSnapshots, blobContainer, indexGeneration); // Delete all previous index-N, data-blobs that are not referenced by the new index-N and temporary blobs blobsToDelete = blobs.keySet().stream().filter(blob -> blob.startsWith(SNAPSHOT_INDEX_PREFIX) || blob.startsWith(DATA_BLOB_PREFIX) && updatedSnapshots.findNameFile(canonicalName(blob)) == null || FsBlobContainer.isTempBlobName(blob)).collect(Collectors.toList()); } try { blobContainer.deleteBlobsIgnoringIfNotExists(blobsToDelete); } catch (IOException e) { logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to delete blobs during finalization", snapshotId, shardId), e); } } catch (IOException e) { String message = "Failed to finalize " + reason + " with shard index [" + indexShardSnapshotsFormat.blobName(indexGeneration) + "]"; throw new IndexShardSnapshotFailedException(shardId, message, e); } }	the reason the previous deletion happened in the given order is that it wanted to ensure that an index file does not end up referencing data files which have been cleaned up. now, this does not hold true anymore for older index files. i'm not sure how much that matters, just wanted to note it.
* @param fileListGeneration the generation number of the snapshot index file * @param blobs list of blobs in the container * @param reason a reason explaining why the shard index file is written */ protected void finalize(final List<SnapshotFiles> snapshots, final int fileListGeneration, final Map<String, BlobMetaData> blobs, final String reason) { final String indexGeneration = Integer.toString(fileListGeneration); try { final List<String> blobsToDelete; if (snapshots.isEmpty()) { // If we deleted all snapshots, we don't need to create a new index file and simply delete all the blobs we found blobsToDelete = List.copyOf(blobs.keySet()); } else { final BlobStoreIndexShardSnapshots updatedSnapshots = new BlobStoreIndexShardSnapshots(snapshots); indexShardSnapshotsFormat.writeAtomic(updatedSnapshots, blobContainer, indexGeneration); // Delete all previous index-N, data-blobs that are not referenced by the new index-N and temporary blobs blobsToDelete = blobs.keySet().stream().filter(blob -> blob.startsWith(SNAPSHOT_INDEX_PREFIX) || blob.startsWith(DATA_BLOB_PREFIX) && updatedSnapshots.findNameFile(canonicalName(blob)) == null || FsBlobContainer.isTempBlobName(blob)).collect(Collectors.toList()); } try { blobContainer.deleteBlobsIgnoringIfNotExists(blobsToDelete); } catch (IOException e) { logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to delete blobs during finalization", snapshotId, shardId), e); } } catch (IOException e) { String message = "Failed to finalize " + reason + " with shard index [" + indexShardSnapshotsFormat.blobName(indexGeneration) + "]"; throw new IndexShardSnapshotFailedException(shardId, message, e); } }	what about the prior comment: // delete temporary index files first, as we might otherwise fail in the next step creating the new index file if an earlier // attempt to write an index file with this generation failed mid-way after creating the temporary file.
* @param fileListGeneration the generation number of the snapshot index file * @param blobs list of blobs in the container * @param reason a reason explaining why the shard index file is written */ protected void finalize(final List<SnapshotFiles> snapshots, final int fileListGeneration, final Map<String, BlobMetaData> blobs, final String reason) { final String indexGeneration = Integer.toString(fileListGeneration); try { final List<String> blobsToDelete; if (snapshots.isEmpty()) { // If we deleted all snapshots, we don't need to create a new index file and simply delete all the blobs we found blobsToDelete = List.copyOf(blobs.keySet()); } else { final BlobStoreIndexShardSnapshots updatedSnapshots = new BlobStoreIndexShardSnapshots(snapshots); indexShardSnapshotsFormat.writeAtomic(updatedSnapshots, blobContainer, indexGeneration); // Delete all previous index-N, data-blobs that are not referenced by the new index-N and temporary blobs blobsToDelete = blobs.keySet().stream().filter(blob -> blob.startsWith(SNAPSHOT_INDEX_PREFIX) || blob.startsWith(DATA_BLOB_PREFIX) && updatedSnapshots.findNameFile(canonicalName(blob)) == null || FsBlobContainer.isTempBlobName(blob)).collect(Collectors.toList()); } try { blobContainer.deleteBlobsIgnoringIfNotExists(blobsToDelete); } catch (IOException e) { logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to delete blobs during finalization", snapshotId, shardId), e); } } catch (IOException e) { String message = "Failed to finalize " + reason + " with shard index [" + indexShardSnapshotsFormat.blobName(indexGeneration) + "]"; throw new IndexShardSnapshotFailedException(shardId, message, e); } }	there's a slight behavioral change here: previously, we were re-throwing when failing to delete old index-n and just logging warnings when failing to delete data blobs. now we're just logging for index-n as well. i think that's a reasonable change though, deleting old index-n is not functionally necessary for the snapshot process to work out and they can just as well be deleted by the next round of operations.
@Override LeafBucketCollector getLeafCollector(Comparable value, LeafReaderContext context, LeafBucketCollector next) throws IOException { if (value.getClass() != BytesRef.class) { throw new IllegalArgumentException("Expected BytesRef, got " + value.getClass()); } final SortedBinaryDocValues dvs = docValuesFunc.apply(context); currentValue = dvs.normalizeValue((BytesRef) value); return new LeafBucketCollector() { @Override public void collect(int doc, long bucket) throws IOException { next.collect(doc, bucket); } }; }	i hit a pretty similar bug to this in #53315. i wonder if it'd be better to have a member on this class that is a function<bytesref, bytesref> that does the normalization. that way we don't need to build the sortedbinarydocvalues. sortedbinarydocvalues can be a fairly heavy thing.
@Override public void restoreShard(Store store, SnapshotId snapshotId, IndexId indexId, ShardId snapshotShardId, RecoveryState recoveryState, ActionListener<Void> listener) { final ShardId shardId = store.shardId(); final ActionListener<Void> restoreListener = ActionListener.delegateResponse(listener, (l, e) -> l.onFailure(new IndexShardRestoreFailedException(shardId, "failed to restore snapshot [" + snapshotId + "]", e))); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT); final BlobContainer container = shardContainer(indexId, snapshotShardId); executor.execute(ActionRunnable.wrap(restoreListener, l -> { final BlobStoreIndexShardSnapshot snapshot = loadShardSnapshot(container, snapshotId); final SnapshotFiles snapshotFiles = new SnapshotFiles(snapshot.snapshot(), snapshot.indexFiles()); new FileRestoreContext(metadata.name(), shardId, snapshotId, recoveryState) { @Override protected void restoreFiles(List<BlobStoreIndexShardSnapshot.FileInfo> filesToRecover, Store store, ActionListener<Void> listener) { if (filesToRecover.isEmpty()) { listener.onResponse(null); } else { // Start as many workers as fit into the snapshot pool at once at the most final int workers = Math.min(threadPool.info(ThreadPool.Names.SNAPSHOT).getMax(), snapshotFiles.indexFiles().size()); final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> files = new LinkedBlockingQueue<>(filesToRecover); final ActionListener<Void> allFilesListener = fileQueueListener(files, workers, ActionListener.map(listener, v -> null)); // restore the files from the snapshot to the Lucene store for (int i = 0; i < workers; ++i) { executor.execute(ActionRunnable.run(allFilesListener, () -> { store.incRef(); try { BlobStoreIndexShardSnapshot.FileInfo fileToRecover; while ((fileToRecover = files.poll(0L, TimeUnit.MILLISECONDS)) != null) { restoreFile(fileToRecover, store); } } finally { store.decRef(); } })); } } } private void restoreFile(BlobStoreIndexShardSnapshot.FileInfo fileInfo, Store store) throws IOException { boolean success = false; try (InputStream stream = maybeRateLimit(new SlicedInputStream(fileInfo.numberOfParts()) { @Override protected InputStream openSlice(long slice) throws IOException { return container.readBlob(fileInfo.partName(slice)); } }, restoreRateLimiter, restoreRateLimitingTimeInNanos)) { try (IndexOutput indexOutput = store.createVerifyingOutput(fileInfo.physicalName(), fileInfo.metadata(), IOContext.DEFAULT)) { final byte[] buffer = new byte[BUFFER_SIZE]; int length; while ((length = stream.read(buffer)) > 0) { indexOutput.writeBytes(buffer, 0, length); recoveryState.getIndex().addRecoveredBytesToFile(fileInfo.physicalName(), length); } Store.verify(indexOutput); indexOutput.close(); store.directory().sync(Collections.singleton(fileInfo.physicalName())); success = true; } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) { try { store.markStoreCorrupted(ex); } catch (IOException e) { logger.warn("store cannot be marked as corrupted", e); } throw ex; } finally { if (success == false) { store.deleteQuiet(fileInfo.physicalName()); } } } } }.restore(snapshotFiles, store, l); })); }	this is not very fair to other actions running on the node. this is essentially blocking the snapshot threadpool until all files have been restored for the shard. imagine a concurrent snapshot (in the future). we should avoid blocking threadpools for very long actions.
private static ActionListener<Void> fileQueueListener(BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> files, int workers, ActionListener<Collection<Void>> listener) { return ActionListener.delegateResponse(new GroupedActionListener<>(listener, workers), (l, e) -> { files.clear(); // Stop uploading the remaining files if we run into any exception l.onFailure(e); }); }	for extra safety can we call onfailure() in a finally block? java try { files.clear(); // stop uploading the remaining files if we run into any exception } finally { l.onfailure(e); }
@Override public void validate(String username, Map<Setting<?>, Object> settings) { final String namespace = HttpExporter.AUTH_USERNAME_SETTING.getNamespace( HttpExporter.AUTH_USERNAME_SETTING.getConcreteSetting(key)); final String password = (String) settings.get(AUTH_PASSWORD_SETTING.getConcreteSettingForNamespace(namespace)); // password must be specified along with username for any auth if (Strings.isNullOrEmpty(username) == false) { if (Strings.isNullOrEmpty(password)) { throw new SettingsException( "[" + AUTH_USERNAME_SETTING.getConcreteSettingForNamespace(namespace).getKey() + "] without [" + AUTH_PASSWORD_SETTING.getConcreteSettingForNamespace(namespace).getKey() + "]"); } } }	could we have a more explicit message? this is something a user sees that must be acted upon. perhaps something for the form "xxx is set but yyy is missing"
public static String validateMonotonicallyIncreasingPhaseTimings(Collection<Phase> phases) { List<String> errors = new ArrayList<>(); // Loop through all phases in order, for each phase with a min_age // configured, look at all the future phases to see if their ages are // >= the configured age. A min_age of 0 means that the age was not // configured, so we don't validate it. for (int i = 0; i < ORDERED_VALID_PHASES.size(); i++) { String phaseName = ORDERED_VALID_PHASES.get(i); // Check if this phase is present with a configured min_age Optional<Phase> maybePhase = phases.stream() .filter(p -> phaseName.equals(p.getName())) .filter(p -> p.getMinimumAge() != null && p.getMinimumAge().equals(TimeValue.ZERO) == false) .findFirst(); if (maybePhase.isPresent()) { Phase phase = maybePhase.get(); TimeValue phaseMinAge = phase.getMinimumAge(); Set<String> followingPhases = new HashSet<>(ORDERED_VALID_PHASES.subList(i + 1, ORDERED_VALID_PHASES.size())); Set<Phase> phasesWithBadAges = phases.stream() .filter(p -> followingPhases.contains(p.getName())) .filter(p -> p.getMinimumAge() != null && p.getMinimumAge().equals(TimeValue.ZERO) == false) .filter(p -> p.getMinimumAge().compareTo(phaseMinAge) < 0) .collect(Collectors.toSet()); if (phasesWithBadAges.size() > 0) { errors.add("phases [" + phasesWithBadAges.stream().map(Phase::getName).collect(Collectors.joining(",")) + "] configure a [min_age] value less than the [min_age] of [" + phase.getMinimumAge() + "] for the [" + phaseName + "] phase, configuration: " + phasesWithBadAges.stream().collect(Collectors.toMap(Phase::getName, Phase::getMinimumAge))); } } } // If we found any invalid phase timings, concatenate their messages and return the message return Strings.collectionToCommaDelimitedString(errors); }	i believe that would always be true, but maybe i'm misunderstanding the intent here?
public void testValidatingIncreasingAges() { { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.ZERO, Collections.emptyMap()); validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); List<Phase> phases = new ArrayList<>(); phases.add(hotPhase); if (randomBoolean()) { phases.add(warmPhase); } if (randomBoolean()) { phases.add(coldPhase); } if (randomBoolean()) { phases.add(frozenPhase); } if (randomBoolean()) { phases.add(deletePhase); } validateMonotonicallyIncreasingPhaseTimings(phases); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.timeValueHours(12), Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.ZERO, Collections.emptyMap()); String err = validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); assertThat(err, containsString("phases [cold] configure a [min_age] value less than the" + " [min_age] of [1d] for the [hot] phase, configuration: {cold=12h}")); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.timeValueDays(2), Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, null, Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.timeValueHours(36), Collections.emptyMap()); String err = validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); assertThat(err, containsString("phases [delete,frozen] configure a [min_age] value less " + "than the [min_age] of [2d] for the [warm] phase, configuration: {frozen=1d, delete=1.5d}")); } }	i believe we still need to assert the result here?
public void testValidatingIncreasingAges() { { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.ZERO, Collections.emptyMap()); validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); List<Phase> phases = new ArrayList<>(); phases.add(hotPhase); if (randomBoolean()) { phases.add(warmPhase); } if (randomBoolean()) { phases.add(coldPhase); } if (randomBoolean()) { phases.add(frozenPhase); } if (randomBoolean()) { phases.add(deletePhase); } validateMonotonicallyIncreasingPhaseTimings(phases); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.timeValueHours(12), Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.ZERO, Collections.emptyMap()); String err = validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); assertThat(err, containsString("phases [cold] configure a [min_age] value less than the" + " [min_age] of [1d] for the [hot] phase, configuration: {cold=12h}")); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.timeValueDays(2), Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, null, Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.timeValueHours(36), Collections.emptyMap()); String err = validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); assertThat(err, containsString("phases [delete,frozen] configure a [min_age] value less " + "than the [min_age] of [2d] for the [warm] phase, configuration: {frozen=1d, delete=1.5d}")); } }	i believe we still need to assert the result here?
public void testValidatingIncreasingAges() { { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.ZERO, Collections.emptyMap()); validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); List<Phase> phases = new ArrayList<>(); phases.add(hotPhase); if (randomBoolean()) { phases.add(warmPhase); } if (randomBoolean()) { phases.add(coldPhase); } if (randomBoolean()) { phases.add(frozenPhase); } if (randomBoolean()) { phases.add(deletePhase); } validateMonotonicallyIncreasingPhaseTimings(phases); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.timeValueHours(12), Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.ZERO, Collections.emptyMap()); String err = validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); assertThat(err, containsString("phases [cold] configure a [min_age] value less than the" + " [min_age] of [1d] for the [hot] phase, configuration: {cold=12h}")); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.timeValueDays(2), Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, null, Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.timeValueHours(36), Collections.emptyMap()); String err = validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); assertThat(err, containsString("phases [delete,frozen] configure a [min_age] value less " + "than the [min_age] of [2d] for the [warm] phase, configuration: {frozen=1d, delete=1.5d}")); } }	huge nit :) : could we have all the timings configured in one unit of measurement? i think it'd be easier to visually see what the test case is
public void testValidatingIncreasingAges() { { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.ZERO, Collections.emptyMap()); validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); List<Phase> phases = new ArrayList<>(); phases.add(hotPhase); if (randomBoolean()) { phases.add(warmPhase); } if (randomBoolean()) { phases.add(coldPhase); } if (randomBoolean()) { phases.add(frozenPhase); } if (randomBoolean()) { phases.add(deletePhase); } validateMonotonicallyIncreasingPhaseTimings(phases); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, TimeValue.timeValueHours(12), Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.ZERO, Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.ZERO, Collections.emptyMap()); String err = validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); assertThat(err, containsString("phases [cold] configure a [min_age] value less than the" + " [min_age] of [1d] for the [hot] phase, configuration: {cold=12h}")); } { Phase hotPhase = new Phase(HOT_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase warmPhase = new Phase(WARM_PHASE, TimeValue.timeValueDays(2), Collections.emptyMap()); Phase coldPhase = new Phase(COLD_PHASE, null, Collections.emptyMap()); Phase frozenPhase = new Phase(FROZEN_PHASE, TimeValue.timeValueDays(1), Collections.emptyMap()); Phase deletePhase = new Phase(DELETE_PHASE, TimeValue.timeValueHours(36), Collections.emptyMap()); String err = validateMonotonicallyIncreasingPhaseTimings(Arrays.asList(hotPhase, warmPhase, coldPhase, frozenPhase, deletePhase)); assertThat(err, containsString("phases [delete,frozen] configure a [min_age] value less " + "than the [min_age] of [2d] for the [warm] phase, configuration: {frozen=1d, delete=1.5d}")); } }	i suspect we might get more errors than we want (or maybe we do want them?) in the following case: hot 1d, warm 3d, cold 0, frozen 2d, delete 1d can we add a test for this please? (i think we either want to support to multiple errors in this case, or otherwise we need to add a break after this line https://github.com/elastic/elasticsearch/pull/70089/files#diff-5964cde83b3b053cf693079cb18a73f4a5743c59590f09fda7dd268613098ce9r400
public void testTranslogGenerationSizeThreshold() { final ByteSizeValue size = new ByteSizeValue(Math.abs(randomInt())); final String key = IndexSettings.INDEX_TRANSLOG_GENERATION_THRESHOLD_SIZE_SETTING.getKey(); final ByteSizeValue actualValue = ByteSizeValue.parseBytesSizeValue(size.toString(), key); final IndexMetaData metaData = newIndexMeta( "index", Settings.builder() .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT) .put(key, size.toString()) .build()); final IndexSettings settings = new IndexSettings(metaData, Settings.EMPTY); assertEquals(actualValue, settings.getGenerationThresholdSize()); final ByteSizeValue newSize = new ByteSizeValue(Math.abs(randomInt())); final ByteSizeValue actual = ByteSizeValue.parseBytesSizeValue(newSize.toString(), key); settings.updateIndexMetaData( newIndexMeta("index", Settings.builder().put(key, newSize.toString()).build())); assertEquals(actual, settings.getGenerationThresholdSize()); }	should we protect against 0? thinking about it more, we should at least fit one op in there. so i wonder if we should have a decent lower bound. the problem in making it too big is testing - how about making it 1kb?
static Request sourceExists(GetRequest getRequest) { Params parameters = new Params(); parameters.withPreference(getRequest.preference()); parameters.withRouting(getRequest.routing()); parameters.withRefresh(getRequest.refresh()); parameters.withRealtime(getRequest.realtime()); parameters.withFetchSourceContext(getRequest.fetchSourceContext()); // Version params are not currently supported by the _source API so are not passed String endpoint = endpoint(getRequest.index(), "_source", getRequest.id()); Request request = new Request(HttpHead.METHOD_NAME, endpoint); request.addParameters(parameters.asMap()); return request; }	maybe add a test for this in requestconverterstests?
* @return a warning value formatted according to RFC 7234 */ public static String formatWarning(final String s) { return formatWarning(DeprecationLogger.CRITICAL, s); }	another method seems unnecessary, the only non-test use is in this class, let's just update the few tests using this to pass the level?
public static void addWarning(String message, Object... params) { addWarning(THREAD_CONTEXT, DeprecationLogger.CRITICAL, message, params); }	rather than adding another public method, can you please update the existing uses? it looks like there are ~12 calls to the existing method, and most are in tests, it should be easy to update with eg intellij refactoring tools to add a parameter.
private boolean isDeprecatedWarningOnly() { return properties.contains(Property.DeprecatedWarning) && properties.contains(Property.Deprecated) == false; } /** * Returns <code>true</code> iff this setting is a group setting. Group settings represent a set of settings rather than a single value. * The key, see {@link #getKey()}, in contrast to non-group settings is a prefix like {@code cluster.store.}	we should assert somewhere that both properties do not exist, they should be mutually exclusive. then this method isn't needed, just check for contains the warning version?
private List<CreateApiKeyResponse> createApiKeys(String user, String runAsUser, int noOfApiKeys, TimeValue expiration, String... clusterPrivileges) { final Map<String, String> headers = Map.of("Authorization", UsernamePasswordToken.basicAuthHeaderValue(runAsUser, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING), "es-security-runas-user", user); return createApiKeys(headers, noOfApiKeys, expiration, clusterPrivileges); }	these seem backwards to me. you authenticate as runasuser but run-as user. i assume the parameters are just strangely named, but it probably makes sense to have authenticatinguser and owninguser or something like that.
boolean isSnapshot() { return Build.CURRENT.isSnapshot(); }	probably address this todo before merging to master?
public static <T extends Enum<T>> Setting<T> enumSetting(Class<T> clazz, String key, T defaultValue, Property... properties) { return new Setting<>(key, defaultValue.toString(), e -> parseEnum(clazz, key, e, isFiltered(properties)), properties); }	i wasn't sure wether or not to add all the signatures, which i copied from boolsetting. also wasn't sure about adding comments. the added functionality seems straightforward and use is similar to f.e. boolsetting
public void testGetExplicitlyMappedFields() { assertThat(new Classification("foo").getExplicitlyMappedFields(null, "results"), equalTo(Collections.singletonMap("results.feature_importance", MapUtils.featureImportanceMapping()))); assertThat(new Classification("foo").getExplicitlyMappedFields(Collections.emptyMap(), "results"), equalTo(Collections.singletonMap("results.feature_importance", MapUtils.featureImportanceMapping()))); assertThat( new Classification("foo").getExplicitlyMappedFields(Collections.singletonMap("foo", "not_a_map"), "results"), equalTo(Collections.singletonMap("results.feature_importance", MapUtils.featureImportanceMapping()))); Map<String, Object> explicitlyMappedFields = new Classification("foo").getExplicitlyMappedFields( Collections.singletonMap("foo", Collections.singletonMap("bar", "baz")), "results"); assertThat(explicitlyMappedFields, allOf( hasEntry("results.foo_prediction", Collections.singletonMap("bar", "baz")), hasEntry("results.top_classes.class_name", Collections.singletonMap("bar", "baz")))); assertThat(explicitlyMappedFields, hasEntry("results.feature_importance", MapUtils.featureImportanceMapping())); explicitlyMappedFields = new Classification("foo").getExplicitlyMappedFields( new HashMap<>() {{ put("foo", new HashMap<>() {{ put("type", "alias"); put("path", "bar"); }}); put("bar", Collections.singletonMap("type", "long")); }}, "results"); assertThat(explicitlyMappedFields, allOf( hasEntry("results.foo_prediction", Collections.singletonMap("type", "long")), hasEntry("results.top_classes.class_name", Collections.singletonMap("type", "long")))); assertThat(explicitlyMappedFields, hasEntry("results.feature_importance", MapUtils.featureImportanceMapping())); assertThat( new Classification("foo").getExplicitlyMappedFields( Collections.singletonMap("foo", new HashMap<>() {{ put("type", "alias"); put("path", "missing"); }}), "results"), equalTo(Collections.singletonMap("results.feature_importance", MapUtils.featureImportanceMapping()))); }	you could put the hasentry assertion into allof in line 257 where the other hasentry assertions are.
public void testSingleNumericFeatureAndMixedTrainingAndNonTrainingRows() throws Exception { initialize("classification_single_numeric_feature_and_mixed_data_set"); String predictedClassField = KEYWORD_FIELD + "_prediction"; indexData(sourceIndex, 300, 50, KEYWORD_FIELD); DataFrameAnalyticsConfig config = buildAnalytics(jobId, sourceIndex, destIndex, null, new Classification( KEYWORD_FIELD, BoostedTreeParams.builder().setNumTopFeatureImportanceValues(1).build(), null, null, null, null, null)); registerAnalytics(config); putAnalytics(config); assertIsStopped(jobId); assertProgress(jobId, 0, 0, 0, 0); startAnalytics(jobId); waitUntilAnalyticsIsStopped(jobId); client().admin().indices().refresh(new RefreshRequest(destIndex)); SearchResponse sourceData = client().prepareSearch(sourceIndex).setTrackTotalHits(true).setSize(1000).get(); for (SearchHit hit : sourceData.getHits()) { Map<String, Object> destDoc = getDestDoc(config, hit); Map<String, Object> resultsObject = getFieldValue(destDoc, "ml"); assertThat(getFieldValue(resultsObject, predictedClassField), is(in(KEYWORD_FIELD_VALUES))); assertThat(getFieldValue(resultsObject, "is_training"), is(destDoc.containsKey(KEYWORD_FIELD))); assertTopClasses(resultsObject, 2, KEYWORD_FIELD, KEYWORD_FIELD_VALUES); @SuppressWarnings("unchecked") List<Map<String, Object>> importanceArray = (List<Map<String, Object>>)resultsObject.get("feature_importance"); assertThat(importanceArray.size(), greaterThan(0)); } assertProgress(jobId, 100, 100, 100, 100); assertThat(searchStoredProgress(jobId).getHits().getTotalHits().value, equalTo(1L)); assertModelStatePersisted(stateDocId()); assertInferenceModelPersisted(jobId); assertMlResultsFieldMappings(destIndex, predictedClassField, "keyword"); assertThatAuditMessagesMatch(jobId, "Created analytics with analysis type [classification]", "Estimated memory usage for this analytics to be", "Starting analytics on node", "Started analytics", expectedDestIndexAuditMessage(), "Started reindexing to destination index [" + destIndex + "]", "Finished reindexing to destination index [" + destIndex + "]", "Started loading data", "Started analyzing", "Started writing results", "Finished analysis"); assertEvaluation(KEYWORD_FIELD, KEYWORD_FIELD_VALUES, "ml." + predictedClassField); }	you could use more idiomatic hassize(greaterthan(0)) here.
private static boolean checkWaitRequirements(GetDiscoveredNodesRequest request, Set<DiscoveryNode> nodes) { Set<String> requiredNodes = new HashSet<>(request.getRequiredNodes()); for (final DiscoveryNode node : nodes) { requiredNodes.remove(node.getAddress().toString()); requiredNodes.remove(node.getName()); if (requiredNodes.isEmpty()) { break; } } return requiredNodes.isEmpty() && nodes.size() >= request.getWaitForNodes(); }	we should return an exception if any requirement matches >1 node. i also wonder if we should return an exception if we specified a set of nodes and found nodes that didn't match this set. this statement is a note to myself to check this, no action is required yet.
public void testNodeVersionIsUpdated() throws IOException, NodeValidationException { TransportClient client = (TransportClient) internalCluster().client(); try (Node node = new MockNode(Settings.builder() .put(internalCluster().getDefaultSettings()) .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) .put("node.name", "testNodeVersionIsUpdated") .put("transport.type", getTestTransportType()) .put(Node.NODE_DATA_SETTING.getKey(), false) .put("cluster.name", "foobar") .put(TestZenDiscovery.USE_ZEN2.getKey(), true) .putList( ClusterBootstrapService.INITIAL_MASTER_NODES_SETTING.getKey(), Collections.singletonList("testNodeVersionIsUpdated") ).put(ClusterBootstrapService.INITIAL_MASTER_NODE_COUNT_SETTING.getKey(), 1) .build(), Arrays.asList(getTestTransportPlugin(), TestZenDiscovery.TestPlugin.class, MockHttpTransport.TestPlugin.class)).start()) { TransportAddress transportAddress = node.injector().getInstance(TransportService.class).boundAddress().publishAddress(); client.addTransportAddress(transportAddress); // since we force transport clients there has to be one node started that we connect to. assertThat(client.connectedNodes().size(), greaterThanOrEqualTo(1)); // connected nodes have updated version for (DiscoveryNode discoveryNode : client.connectedNodes()) { assertThat(discoveryNode.getVersion(), equalTo(Version.CURRENT)); } for (DiscoveryNode discoveryNode : client.listedNodes()) { assertThat(discoveryNode.getId(), startsWith("#transport#-")); assertThat(discoveryNode.getVersion(), equalTo(Version.CURRENT.minimumCompatibilityVersion())); } assertThat(client.filteredNodes().size(), equalTo(1)); for (DiscoveryNode discoveryNode : client.filteredNodes()) { assertThat(discoveryNode.getVersion(), equalTo(Version.CURRENT.minimumCompatibilityVersion())); } } }	i think we only need to set one or other of these. here i think it'd be better to set initial_master_nodes_setting.
private static boolean uncompressedOrSchemeDefinited(Header header) { return header.isCompressed() == false || header.getCompressionScheme() != null; }	can this be tightened to: suggestion return header.iscompressed() == (header.getcompressionscheme() != null);
*/ void sendResponse(final Version nodeVersion, final TcpChannel channel, final long requestId, final String action, final TransportResponse response, final Compression.Scheme compressionScheme, final boolean isHandshake) throws IOException { Version version = Version.min(this.version, nodeVersion); OutboundMessage.Response message = new OutboundMessage.Response(threadPool.getThreadContext(), response, version, requestId, isHandshake, compressionScheme); ActionListener<Void> listener = ActionListener.wrap(() -> messageListener.onResponseSent(requestId, action, response)); sendMessage(channel, message, listener); }	i wonder if we can now assert that if compressionscheme == lz4 we are on a new enough version. something like: assert compressionscheme != compression.scheme.lz4 || version.onorafter(compression.scheme.lz4_version); i am sure this will break a test or two and i am ok with deferring this if it adds too much trouble.
public void testPipelineHandling() throws IOException { final List<Tuple<MessageData, Exception>> expected = new ArrayList<>(); final List<Tuple<MessageData, Exception>> actual = new ArrayList<>(); final List<ReleasableBytesReference> toRelease = new ArrayList<>(); final BiConsumer<TcpChannel, InboundMessage> messageHandler = (c, m) -> { try { final Header header = m.getHeader(); final MessageData actualData; final Version version = header.getVersion(); final boolean isRequest = header.isRequest(); final long requestId = header.getRequestId(); final Compression.Scheme compressionScheme = header.getCompressionScheme(); if (header.isCompressed()) { assertNotNull(compressionScheme); } if (m.isShortCircuit()) { actualData = new MessageData(version, requestId, isRequest, compressionScheme, header.getActionName(), null); } else if (isRequest) { final TestRequest request = new TestRequest(m.openOrGetStreamInput()); actualData = new MessageData(version, requestId, isRequest, compressionScheme, header.getActionName(), request.value); } else { final TestResponse response = new TestResponse(m.openOrGetStreamInput()); actualData = new MessageData(version, requestId, isRequest, compressionScheme, null, response.value); } actual.add(new Tuple<>(actualData, m.getException())); } catch (IOException e) { throw new AssertionError(e); } }; final StatsTracker statsTracker = new StatsTracker(); final LongSupplier millisSupplier = () -> TimeValue.nsecToMSec(System.nanoTime()); final InboundDecoder decoder = new InboundDecoder(Version.CURRENT, PageCacheRecycler.NON_RECYCLING_INSTANCE); final String breakThisAction = "break_this_action"; final String actionName = "actionName"; final Predicate<String> canTripBreaker = breakThisAction::equals; final TestCircuitBreaker circuitBreaker = new TestCircuitBreaker(); circuitBreaker.startBreaking(); final InboundAggregator aggregator = new InboundAggregator(() -> circuitBreaker, canTripBreaker); final InboundPipeline pipeline = new InboundPipeline(statsTracker, millisSupplier, decoder, aggregator, messageHandler); final FakeTcpChannel channel = new FakeTcpChannel(); final int iterations = randomIntBetween(100, 500); long totalMessages = 0; long bytesReceived = 0; for (int i = 0; i < iterations; ++i) { actual.clear(); expected.clear(); toRelease.clear(); try (BytesStreamOutput streamOutput = new BytesStreamOutput()) { while (streamOutput.size() < BYTE_THRESHOLD) { final Version version = randomFrom(Version.CURRENT, Version.CURRENT.minimumCompatibilityVersion()); final String value = randomRealisticUnicodeOfCodepointLength(randomIntBetween(200, 400)); final boolean isRequest = randomBoolean(); Compression.Scheme compressionScheme = getCompressionScheme(version); final long requestId = totalMessages++; final MessageData messageData; Exception expectedExceptionClass = null; OutboundMessage message; if (isRequest) { if (rarely()) { messageData = new MessageData(version, requestId, true, compressionScheme, breakThisAction, null); message = new OutboundMessage.Request(threadContext, new TestRequest(value), version, breakThisAction, requestId, false, compressionScheme); expectedExceptionClass = new CircuitBreakingException("", CircuitBreaker.Durability.PERMANENT); } else { messageData = new MessageData(version, requestId, true, compressionScheme, actionName, value); message = new OutboundMessage.Request(threadContext, new TestRequest(value), version, actionName, requestId, false, compressionScheme); } } else { messageData = new MessageData(version, requestId, false, compressionScheme, null, value); message = new OutboundMessage.Response(threadContext, new TestResponse(value), version, requestId, false, compressionScheme); } expected.add(new Tuple<>(messageData, expectedExceptionClass)); final BytesReference reference = message.serialize(new BytesStreamOutput()); Streams.copy(reference.streamInput(), streamOutput); } final BytesReference networkBytes = streamOutput.bytes(); int currentOffset = 0; while (currentOffset != networkBytes.length()) { final int remainingBytes = networkBytes.length() - currentOffset; final int bytesToRead = Math.min(randomIntBetween(1, 32 * 1024), remainingBytes); final BytesReference slice = networkBytes.slice(currentOffset, bytesToRead); try (ReleasableBytesReference reference = new ReleasableBytesReference(slice, () -> {})) { toRelease.add(reference); bytesReceived += reference.length(); pipeline.handleBytes(channel, reference); currentOffset += bytesToRead; } } final int messages = expected.size(); for (int j = 0; j < messages; ++j) { final Tuple<MessageData, Exception> expectedTuple = expected.get(j); final Tuple<MessageData, Exception> actualTuple = actual.get(j); final MessageData expectedMessageData = expectedTuple.v1(); final MessageData actualMessageData = actualTuple.v1(); assertEquals(expectedMessageData.requestId, actualMessageData.requestId); assertEquals(expectedMessageData.isRequest, actualMessageData.isRequest); assertEquals(expectedMessageData.compressionScheme, actualMessageData.compressionScheme); assertEquals(expectedMessageData.actionName, actualMessageData.actionName); assertEquals(expectedMessageData.value, actualMessageData.value); if (expectedTuple.v2() != null) { assertNotNull(actualTuple.v2()); assertThat(actualTuple.v2(), instanceOf(expectedTuple.v2().getClass())); } } for (ReleasableBytesReference released : toRelease) { assertEquals(0, released.refCount()); } } assertEquals(bytesReceived, statsTracker.getBytesRead()); assertEquals(totalMessages, statsTracker.getMessagesReceived()); } }	can we add else assertnull(compressionscheme); ? (or embed it into the other assertion).
public void testEquals() { MappedFieldType ft1 = createNamedDefaultFieldType(); MappedFieldType ft2 = createNamedDefaultFieldType(); assertEquals(ft1, ft1); // reflexive assertEquals(ft1, ft2); // symmetric assertEquals(ft2, ft1); assertEquals(ft1.hashCode(), ft2.hashCode()); }	it looks like this test was lost in the refactor. do we have plans to replace it or make it no longer necessary?
public void setUpMocks() { auditor = MockTransformAuditor.createMockAuditor(); transformConfigManager = new InMemoryTransformConfigManager(); client = new NoOpClient(getTestName()); threadPool = new TestThreadPool(ThreadPool.Names.GENERIC, new ScalingExecutorBuilder(ThreadPool.Names.GENERIC, 4, 10, TimeValue.timeValueSeconds(30))); }	i am not sure about this change. this got recently introduced: https://github.com/elastic/elasticsearch/pull/65721 to fix: https://github.com/elastic/elasticsearch/issues/65542 the change reverts #65721 and we might get test failures again. however, i am not sure the test problem justifies keeping the executor name. :thinking:
static Request multiSearch(MultiSearchRequest multiSearchRequest) throws IOException { Request request = new Request(HttpPost.METHOD_NAME, "/_msearch"); Params params = new Params(); params.putParam(RestSearchAction.TYPED_KEYS_PARAM, "true"); if (multiSearchRequest.maxConcurrentSearchRequests() != MultiSearchRequest.MAX_CONCURRENT_SEARCH_REQUESTS_DEFAULT) { params.putParam("max_concurrent_searches", Integer.toString(multiSearchRequest.maxConcurrentSearchRequests())); } XContent xContent = REQUEST_BODY_CONTENT_TYPE.xContent(); byte[] source = MultiSearchRequest.writeMultiLineFormat(multiSearchRequest, xContent); request.setEntity(new NByteArrayEntity(source, createContentType(xContent.type()))); return request; }	there's no matching request.setparameters(params.getparam()) in this method, so the parameters are never actually added to the request. this is also the case for many other methods in this file, too many to comment on individually. this is why the test run failed as well.
public void testMappingVersionAfterDynamicMappingUpdate() throws Exception { createIndex("test", client().admin().indices().prepareCreate("test").addMapping("type")); final ClusterService clusterService = getInstanceFromNode(ClusterService.class); final long previousVersion = clusterService.state().metaData().index("test").getMappingVersion(); client().prepareIndex("test", "type", "1").setSource("field", "text").get(); // This assertBusy is necessary. When a mapping update is needed as a document is indexed, // the document is tried, rejected (due to mapping conflict), then a mapping update sent // off, the document is then *immediately* retried to see if the mapping change has occurred // quickly enough, and if it has, indexing does not wait for the next cluster state to occur // before moving ahead. In very rare cases this immediate retry succeeds, which causes the // indexing request to complete (because it was successful) but the new cluster state to not // be propagated entirely yet. In that case, we need to wait because the mapping version // will eventually be updated, it just hasn't been updated *yet*. assertBusy(() -> assertThat(clusterService.state().metaData().index("test").getMappingVersion(), equalTo(1 + previousVersion))); }	thanks for the detailed message, til. however, i think the commit message is sufficient for the detailed explanation. perhaps a shorter version like: //ensure that cluster state has been updated also, nitpick: s/mapping conflict/required mapping update
@Override public char[] readSecret(String text, Object... args) { return console.readPassword(text, args); } } private static class SystemTerminal extends Terminal { @Override @SuppressForbidden(reason = "System#out") public void doPrint(String msg) { System.out.print(msg); System.out.flush(); } @Override public String readText(String text, Object... args) { doPrint(text); BufferedReader reader = new BufferedReader(new InputStreamReader(System.in, Charset.defaultCharset())); try { return reader.readLine(); } catch (IOException ioe) { throw new RuntimeException(ioe); } } @Override public char[] readSecret(String text, Object... args) { return readText(text, args).toCharArray(); }	args are not used.. we should either remove those from the method sig (which will be consistent with the println methods) or pass string.format(text, args) to doprint
private Map<String, DocumentField> readFields(StreamInput in) throws IOException { Map<String, DocumentField> fields = null; int size = in.readVInt(); if (size == 0) { fields = emptyMap(); } else { fields = new HashMap<>(size); for (int i = 0; i < size; i++) { DocumentField field = DocumentField.readDocumentField(in); fields.put(field.getName(), field); } } return fields; }	reading and writing the document fields directly shoudln't be necessary. instead we can use in.readmap(streaminput::readstring, documentfield::new) and for writing out.writemap(map, streamoutput::writestring, documentfield::writeto)
@Override public Iterator<DocumentField> iterator() { // need to join the fields and metadata fields Map<String, DocumentField> allFields = this.getFields(); if (allFields == null) { return Collections.emptyIterator(); } return allFields.values().iterator(); }	this can never be null, right? we always create a new map in getfields() currently.
private SearchHit createSearchHit(SearchContext context, FieldsVisitor fieldsVisitor, int docId, int subDocId, Map<String, Set<String>> storedToRequestedFields, LeafReaderContext subReaderContext) { DocumentMapper documentMapper = context.mapperService().documentMapper(); Text typeText = documentMapper.typeText(); if (fieldsVisitor == null) { return new SearchHit(docId, null, typeText, null, null); } Map<String, DocumentField> searchFields = getSearchFields(context, fieldsVisitor, subDocId, storedToRequestedFields, subReaderContext); Map<String, DocumentField> metaFields = new HashMap<>(), documentFields = new HashMap<>(); splitSearchHitFields(searchFields, documentFields, metaFields); SearchHit searchHit = new SearchHit(docId, fieldsVisitor.uid().id(), typeText, documentFields, metaFields); // Set _source if requested. SourceLookup sourceLookup = context.lookup().source(); sourceLookup.setSegmentAndDocument(subReaderContext, subDocId); if (fieldsVisitor.source() != null) { sourceLookup.setSource(fieldsVisitor.source()); } return searchHit; }	nit: please use a separate line for declaration of documentfields, no need for a continuation of the type.
static Map<String, Boolean> evaluateConditions(final Collection<Condition<?>> conditions, @Nullable final Condition.Stats stats) { Objects.requireNonNull(conditions, "conditions must not be null"); if (stats != null) { return conditions.stream() .map(condition -> condition.evaluate(stats)) .collect(Collectors.toMap(result -> result.condition.toString(), result -> result.matched)); } else { // no conditions matched return conditions.stream() .collect(Collectors.toMap(Condition::toString, cond -> false)); } }	i'm not enormously thrilled about this, but .
public <T> boolean supportsNamedObject(Class<T> categoryClass, String name) { Map<String, Entry> parsers = registry.get(categoryClass); if (parsers == null || registry.isEmpty()) { return false; } return parsers.containsKey(name); }	i think the registry.isempty() check is superfluous here
@Override protected QueryBuilder doRewrite(QueryRewriteContext queryRewriteContext) throws IOException { if ("_index".equals(fieldName)) { // Special-case optimisation for canMatch phase: // We can skip querying this shard if the index name doesn't match the value of this query on the "_index" field. QueryShardContext shardContext = queryRewriteContext.convertToShardContext(); if (shardContext != null && shardContext.index().getName().startsWith(value) == false) { return new MatchNoneQueryBuilder(); } } return super.doRewrite(queryRewriteContext); }	we should try to use the indexnamematcher so that it works with aliases too?
@Override void innerPerformAction(String followerIndex, Listener listener) { PauseFollowAction.Request request = new PauseFollowAction.Request(followerIndex); getClient().execute(PauseFollowAction.INSTANCE, request, ActionListener.wrap( r -> { assert r.isAcknowledged() : "pause follow response is not acknowledged"; listener.onResponse(true); }, listener::onFailure )); }	is there any way that we can check whether the pause has already happened and skip it here? here's the scenario i'm concerned about: - pause gets executed - request is not acknowledged - assertion is thrown (i think i've seen users run with assertions enabled before) - index enters the error state - retries the pause - because the pause has already occurred, the transportpausefollowaction throws an exception as pausing twice throws an exception - now we're stuck in an error loop i think we should also handle the case where the index is manually paused and then the pause-follower-index step runs, it should skip the index.
@Override void innerPerformAction(String followerIndex, Listener listener) { UnfollowAction.Request request = new UnfollowAction.Request(followerIndex); getClient().execute(UnfollowAction.INSTANCE, request, ActionListener.wrap( r -> { assert r.isAcknowledged() : "unfollow response is not acknowledged"; listener.onResponse(true); }, exception -> { if (exception instanceof ElasticsearchException && ((ElasticsearchException) exception).getMetadata("es.failed_to_remove_retention_leases") != null) { List<String> leasesNotRemoved = ((ElasticsearchException) exception) .getMetadata("es.failed_to_remove_retention_leases"); logger.debug("failed to remove leader retention lease(s) {} while unfollowing index [{}], " + "continuing with lifecycle execution", leasesNotRemoved, followerIndex); listener.onResponse(true); } else { listener.onFailure(exception); } } )); }	same comment here about skipping if the index is already unfollowed, a double unfollow throws an error so we'd just be retrying forever if it were executed after the index was already unfollowed
@Override public void usage(ActionListener<XPackFeatureSet.Usage> listener) { if (enabled == false) { listener.onResponse( new DataFrameFeatureSetUsage(available(), enabled(), Collections.emptyMap(), new DataFrameIndexerTransformStats())); return; } GetDataFrameTransformsAction.Request transformsRequest = new GetDataFrameTransformsAction.Request(MetaData.ALL); // TODO Before 7.1, see https://github.com/elastic/elasticsearch/issues/39994 transformsRequest.setPageParams(new PageParams(0, 10_000)); client.execute(GetDataFrameTransformsAction.INSTANCE, transformsRequest, ActionListener.wrap( transforms -> { Set<String> transformIds = transforms.getTransformConfigurations() .stream() .map(DataFrameTransformConfig::getId) .collect(Collectors.toSet()); GetDataFrameTransformsStatsAction.Request transformStatsRequest = new GetDataFrameTransformsStatsAction.Request(MetaData.ALL); client.execute(GetDataFrameTransformsStatsAction.INSTANCE, transformStatsRequest, ActionListener.wrap(transformStatsResponse -> { Map<String, Long> transformsCountByState = new HashMap<>(); DataFrameIndexerTransformStats accumulatedStats = new DataFrameIndexerTransformStats(); transformStatsResponse.getTransformsStateAndStats().forEach(singleResult -> { transformIds.remove(singleResult.getId()); transformsCountByState.merge(singleResult.getTransformState().getIndexerState().value(), 1L, Long::sum); accumulatedStats.merge(singleResult.getTransformStats()); }); // If there is no state returned, assumed stopped transformIds.forEach(ignored -> transformsCountByState.merge(IndexerState.STOPPED.value(), 1L, Long::sum)); listener.onResponse(new DataFrameFeatureSetUsage(available(), enabled(), transformsCountByState, accumulatedStats)); }, listener::onFailure)); }, listener::onFailure )); }	we should start with a lower limit, we can increase it after verification. at the moment there is no reason to support 10k as this is an unreleased feature. setpageparams is unchecked, we should add a check there, if not upstream we can override it and enforce a lower limit in getdataframetransformsaction.
private static boolean assertGenerationConsistency( Map<String, Map<Integer, String>> generations, String indexName, int shardId, @Nullable String activeGeneration ) { final String bestGeneration = generations.getOrDefault(indexName, Collections.emptyMap()).get(shardId); if ((bestGeneration == null || activeGeneration == null || activeGeneration.equals(bestGeneration)) == false) { throw new AssertionFailedException("gnarf"); } return true; }	changing this assertion failure into a runtime exception is, i think, cheating ;)
public void removeIndexTemplateV2(final String name, final TimeValue masterTimeout, final ActionListener<AcknowledgedResponse> listener) { clusterService.submitStateUpdateTask("remove-index-template-v2 [" + name + "]", new ClusterStateUpdateTask(Priority.URGENT, masterTimeout) { @Override public void onFailure(String source, Exception e) { listener.onFailure(e); } @Override public ClusterState execute(ClusterState currentState) { return innerRemoveIndexTemplateV2(currentState, name); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { listener.onResponse(AcknowledgedResponse.TRUE); } }); }	the name should be a string[], and also the name field on request class. ~~otherwise the request can't be introspected correctly by security.~~
private void ensureWriteAllowed(Engine.Operation.Origin origin) throws IllegalIndexShardStateException { IndexShardState state = this.state; // one time volatile read if (origin.isRecovery()) { if (state != IndexShardState.RECOVERING) { throw new IllegalIndexShardStateException(shardId, state, "operation only allowed when recovering, origin [" + origin + "]"); } } else { if (origin == Engine.Operation.Origin.PRIMARY) { verifyPrimary(); } if (writeAllowedStates.contains(state) == false) { throw new IllegalIndexShardStateException(shardId, state, "operation only allowed when shard state is one of " + writeAllowedStates + ", origin [" + origin + "]"); } } }	isn't verifyreplicationtarget still good to check?
public void testReadCharsFromStdin() throws Exception { assertPassphraseRead("hello", "hello"); assertPassphraseRead("hello\\\\n", "hello"); assertPassphraseRead("hello\\\\r\\\\n", "hello"); assertPassphraseRead("hellohello", "hellohello"); assertPassphraseRead("hellohello\\\\n", "hellohello"); assertPassphraseRead("hellohello\\\\r\\\\n", "hellohello"); assertPassphraseRead("hello\\\\nhi\\\\n", "hello"); assertPassphraseRead("hello\\\\r\\\\nhi\\\\r\\\\n", "hello"); }	can we use expectthrows here instead of a @rule?
static SecureSettings loadSecureSettings(Environment initialEnv) throws BootstrapException { final KeyStoreWrapper keystore; try { keystore = KeyStoreWrapper.load(initialEnv.configFile()); } catch (IOException e) { throw new BootstrapException(e); } char[] passChars; try { if (keystore != null && keystore.hasPassword()) { passChars = readPassphrase(System.in, KeyStoreAwareCommand.MAX_PASSPHRASE_LENGTH); } else { passChars = new char[0]; } } catch (IOException e) { throw new BootstrapException(e); } try (SecureString password = new SecureString(passChars)) { if (keystore == null) { final KeyStoreWrapper keyStoreWrapper = KeyStoreWrapper.create(); keyStoreWrapper.save(initialEnv.configFile(), new char[0]); return keyStoreWrapper; } else { keystore.decrypt(password.getChars()); KeyStoreWrapper.upgrade(keystore, initialEnv.configFile(), password.getChars()); } } catch (Exception e) { throw new BootstrapException(e); } return keystore; }	future thought (doesn't need to be in this pr): maybe readpassphrase should return a securestring.
static SecureSettings loadSecureSettings(Environment initialEnv) throws BootstrapException { final KeyStoreWrapper keystore; try { keystore = KeyStoreWrapper.load(initialEnv.configFile()); } catch (IOException e) { throw new BootstrapException(e); } char[] passChars; try { if (keystore != null && keystore.hasPassword()) { passChars = readPassphrase(System.in, KeyStoreAwareCommand.MAX_PASSPHRASE_LENGTH); } else { passChars = new char[0]; } } catch (IOException e) { throw new BootstrapException(e); } try (SecureString password = new SecureString(passChars)) { if (keystore == null) { final KeyStoreWrapper keyStoreWrapper = KeyStoreWrapper.create(); keyStoreWrapper.save(initialEnv.configFile(), new char[0]); return keyStoreWrapper; } else { keystore.decrypt(password.getChars()); KeyStoreWrapper.upgrade(keystore, initialEnv.configFile(), password.getChars()); } } catch (Exception e) { throw new BootstrapException(e); } return keystore; }	isn't this duplicating the read lines method in terminal?
public void testSearch() throws Exception { internalCluster().ensureAtLeastNumDataNodes(2); final int nodeCount = internalCluster().numDataNodes(); assertAcked( client().admin() .indices() .prepareCreate("test-matching") .setMapping("{\\\\"properties\\\\":{\\\\"message\\\\":{\\\\"type\\\\":\\\\"text\\\\"},\\\\"@timestamp\\\\":{\\\\"type\\\\":\\\\"date\\\\"}}}") .setSettings( Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, nodeCount * 6) ) ); assertAcked( client().admin() .indices() .prepareCreate("test-notmatching") .setMapping("{\\\\"properties\\\\":{\\\\"message\\\\":{\\\\"type\\\\":\\\\"text\\\\"},\\\\"@timestamp\\\\":{\\\\"type\\\\":\\\\"date\\\\"}}}") .setSettings( Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, nodeCount * 6) ) ); ensureGreen("test-matching", "test-notmatching"); final String matchingDate = "2021-11-17"; final String nonMatchingDate = "2021-01-01"; final BulkRequestBuilder bulkRequestBuilder = client().prepareBulk().setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); for (int i = 0; i < 1000; i++) { final boolean isMatching = randomBoolean(); final IndexRequestBuilder indexRequestBuilder = client().prepareIndex(isMatching ? "test-matching" : "test-notmatching"); indexRequestBuilder.setSource( "{\\\\"@timestamp\\\\":\\\\"" + (isMatching ? matchingDate : nonMatchingDate) + "\\\\",\\\\"message\\\\":\\\\"\\\\"}", XContentType.JSON ); bulkRequestBuilder.add(indexRequestBuilder); } assertFalse(bulkRequestBuilder.execute().actionGet(10, TimeUnit.SECONDS).hasFailures()); APMTracer.CAPTURING_SPAN_EXPORTER.clear(); client().prepareSearch() .setQuery(new RangeQueryBuilder("@timestamp").gt("2021-11-01")) .setSearchType(SearchType.QUERY_THEN_FETCH) .setPreFilterShardSize(1) .execute() .actionGet(10, TimeUnit.SECONDS); assertBusy(() -> { final List<SpanData> capturedSpans = APMTracer.CAPTURING_SPAN_EXPORTER.getCapturedSpans(); boolean gotSearchSpan = false; boolean gotCanMatchSpan = false; for (SpanData capturedSpan : capturedSpans) { logger.info("--> captured span [{}]", capturedSpan); final String spanName = capturedSpan.getName(); if (spanName.equals(SearchAction.NAME)) { gotSearchSpan = true; } if (spanName.equals(SearchTransportService.QUERY_CAN_MATCH_NODE_NAME)) { gotCanMatchSpan = true; } } assertTrue(gotSearchSpan); assertTrue(gotCanMatchSpan); }); }	should we have all the spans captured by the time the search is complete?
public void testSearch() throws Exception { internalCluster().ensureAtLeastNumDataNodes(2); final int nodeCount = internalCluster().numDataNodes(); assertAcked( client().admin() .indices() .prepareCreate("test-matching") .setMapping("{\\\\"properties\\\\":{\\\\"message\\\\":{\\\\"type\\\\":\\\\"text\\\\"},\\\\"@timestamp\\\\":{\\\\"type\\\\":\\\\"date\\\\"}}}") .setSettings( Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, nodeCount * 6) ) ); assertAcked( client().admin() .indices() .prepareCreate("test-notmatching") .setMapping("{\\\\"properties\\\\":{\\\\"message\\\\":{\\\\"type\\\\":\\\\"text\\\\"},\\\\"@timestamp\\\\":{\\\\"type\\\\":\\\\"date\\\\"}}}") .setSettings( Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, nodeCount * 6) ) ); ensureGreen("test-matching", "test-notmatching"); final String matchingDate = "2021-11-17"; final String nonMatchingDate = "2021-01-01"; final BulkRequestBuilder bulkRequestBuilder = client().prepareBulk().setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); for (int i = 0; i < 1000; i++) { final boolean isMatching = randomBoolean(); final IndexRequestBuilder indexRequestBuilder = client().prepareIndex(isMatching ? "test-matching" : "test-notmatching"); indexRequestBuilder.setSource( "{\\\\"@timestamp\\\\":\\\\"" + (isMatching ? matchingDate : nonMatchingDate) + "\\\\",\\\\"message\\\\":\\\\"\\\\"}", XContentType.JSON ); bulkRequestBuilder.add(indexRequestBuilder); } assertFalse(bulkRequestBuilder.execute().actionGet(10, TimeUnit.SECONDS).hasFailures()); APMTracer.CAPTURING_SPAN_EXPORTER.clear(); client().prepareSearch() .setQuery(new RangeQueryBuilder("@timestamp").gt("2021-11-01")) .setSearchType(SearchType.QUERY_THEN_FETCH) .setPreFilterShardSize(1) .execute() .actionGet(10, TimeUnit.SECONDS); assertBusy(() -> { final List<SpanData> capturedSpans = APMTracer.CAPTURING_SPAN_EXPORTER.getCapturedSpans(); boolean gotSearchSpan = false; boolean gotCanMatchSpan = false; for (SpanData capturedSpan : capturedSpans) { logger.info("--> captured span [{}]", capturedSpan); final String spanName = capturedSpan.getName(); if (spanName.equals(SearchAction.NAME)) { gotSearchSpan = true; } if (spanName.equals(SearchTransportService.QUERY_CAN_MATCH_NODE_NAME)) { gotCanMatchSpan = true; } } assertTrue(gotSearchSpan); assertTrue(gotCanMatchSpan); }); }	nit: this could be simplified to: apmtracer.capturing_span_exporter.findspanbyname(..) once https://github.com/elastic/elasticsearch/pull/80758/files is merged
public DiscoveryNodes build() { ImmutableOpenMap.Builder<String, DiscoveryNode> dataNodesBuilder = ImmutableOpenMap.builder(); ImmutableOpenMap.Builder<String, DiscoveryNode> masterNodesBuilder = ImmutableOpenMap.builder(); ImmutableOpenMap.Builder<String, DiscoveryNode> ingestNodesBuilder = ImmutableOpenMap.builder(); Version minNodeVersion = Version.CURRENT; Version maxNodeVersion = null; // The node where we are building this on might not be a master or a data node, so we cannot assume // that there is a node with the current version as a part of the cluster. Version minNonClientNodeVersion = null; Version maxNonClientNodeVersion = null; for (ObjectObjectCursor<String, DiscoveryNode> nodeEntry : nodes) { if (nodeEntry.value.isDataNode()) { dataNodesBuilder.put(nodeEntry.key, nodeEntry.value); } if (nodeEntry.value.isMasterNode()) { masterNodesBuilder.put(nodeEntry.key, nodeEntry.value); } final Version version = nodeEntry.value.getVersion(); if (nodeEntry.value.isDataNode() || nodeEntry.value.isMasterNode()) { if (minNonClientNodeVersion == null) { minNonClientNodeVersion = version; maxNonClientNodeVersion = version; } else { minNonClientNodeVersion = Version.min(minNonClientNodeVersion, version); maxNonClientNodeVersion = Version.max(maxNonClientNodeVersion, version); } } if (nodeEntry.value.isIngestNode()) { ingestNodesBuilder.put(nodeEntry.key, nodeEntry.value); } minNodeVersion = Version.min(minNodeVersion, version); maxNodeVersion = maxNodeVersion == null ? version : Version.max(maxNodeVersion, version); } return new DiscoveryNodes( nodes.build(), dataNodesBuilder.build(), masterNodesBuilder.build(), ingestNodesBuilder.build(), masterNodeId, localNodeId, minNonClientNodeVersion == null ? Version.CURRENT : minNonClientNodeVersion, maxNonClientNodeVersion == null ? Version.CURRENT : maxNonClientNodeVersion, maxNodeVersion == null ? Version.CURRENT : maxNodeVersion, minNodeVersion ); }	i think i'm confused why there's an asymmetry between min and max, would you please explain (or fix)?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); this.state.writeTo(out); } } static class ValidateJoinRequestRequestHandler implements TransportRequestHandler<ValidateJoinRequest> { @Override public void messageReceived(ValidateJoinRequest request, TransportChannel channel) throws Exception { ensureNodesCompatibility(Version.CURRENT, request.state.getNodes()); ensureIndexCompatibility(Version.CURRENT, request.state.getMetaData()); // for now, the mere fact that we can serialize the cluster state acts as validation.... channel.sendResponse(TransportResponse.Empty.INSTANCE); } } /** * Ensures that all indices are compatible with the given node version. This will ensure that all indices in the given metadata * will not be created with a newer version of elasticsearch as well as that all indices are newer or equal to the minimum index * compatibility version. * @see Version#minimumIndexCompatibilityVersion() * @throws IllegalStateException if any index is incompatible with the given version */ static void ensureIndexCompatibility(final Version nodeVersion, MetaData metaData) { Version supportedIndexVersion = nodeVersion.minimumIndexCompatibilityVersion(); // we ensure that all indices in the cluster we join are compatible with us no matter if they are // closed or not we can't read mappings of these indices so we need to reject the join... for (IndexMetaData idxMetaData : metaData) { if (idxMetaData.getCreationVersion().after(nodeVersion)) { throw new IllegalStateException("index " + idxMetaData.getIndex() + " version not supported: " + idxMetaData.getCreationVersion() + " the node version is: " + nodeVersion); } if (idxMetaData.getCreationVersion().before(supportedIndexVersion)) { throw new IllegalStateException("index " + idxMetaData.getIndex() + " version not supported: " + idxMetaData.getCreationVersion() + " minimum compatible index version is: " + supportedIndexVersion); } } } /** ensures that the joining node has a version that's compatible all nodes in the cluster */ static void ensureNodesCompatibility(final Version joiningNodeVersion, DiscoveryNodes currentNodes) { final Version minNodeVersion = currentNodes.getMinNodeVersion(); final Version maxNodeVersion = currentNodes.getMaxNodeVersion(); ensureNodesCompatibility(joiningNodeVersion, minNodeVersion, maxNodeVersion); } /** ensures that the joining node has a version that's compatible all nodes in the cluster */ static void ensureNodesCompatibility(Version joiningNodeVersion, Version minClusterNodeVersion, Version maxClusterNodeVersion) { assert minClusterNodeVersion.onOrBefore(maxClusterNodeVersion) : minClusterNodeVersion + " > " + maxClusterNodeVersion; final byte clusterMajor = minClusterNodeVersion.major; if (joiningNodeVersion.major < clusterMajor) { throw new IllegalStateException("node version [" + joiningNodeVersion + "] is not supported. " + "All nodes in the cluster are of a higher major [" + clusterMajor + "]."); } if (joiningNodeVersion.isCompatible(maxClusterNodeVersion) == false) { throw new IllegalStateException("node version [" + joiningNodeVersion + "] is not supported. " + "The cluster contains nodes with version [" + maxClusterNodeVersion + "], which is incompatible."); } if (joiningNodeVersion.isCompatible(minClusterNodeVersion) == false) { throw new IllegalStateException("node version [" + joiningNodeVersion + "] is not supported." + "The cluster contains nodes with version [" + minClusterNodeVersion + "], which is incompatible."); } } public static class LeaveRequest extends TransportRequest { private DiscoveryNode node; public LeaveRequest() { } private LeaveRequest(DiscoveryNode node) { this.node = node; } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); node = new DiscoveryNode(in); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); node.writeTo(out); } } private class LeaveRequestRequestHandler implements TransportRequestHandler<LeaveRequest> { @Override public void messageReceived(LeaveRequest request, TransportChannel channel) throws Exception { listener.onLeave(request.node); channel.sendResponse(TransportResponse.Empty.INSTANCE); }	this javadoc is not quite right since there's no "cluster" (discoverynodes) being passed in here, only a min and max version.
public void testIsCompatible() { assertTrue(isCompatible(Version.CURRENT, Version.CURRENT.minimumCompatibilityVersion())); assertTrue(isCompatible(Version.V_5_5_0, Version.V_6_0_0_alpha2)); assertFalse(isCompatible(Version.fromId(2000099), Version.V_6_0_0_alpha2)); assertFalse(isCompatible(Version.fromId(2000099), Version.V_5_0_0)); assertTrue(isCompatible(Version.fromString("6.0.0"), Version.fromString("7.0.0"))); if (Version.CURRENT.isRelease()) { assertTrue(isCompatible(Version.CURRENT, Version.fromString("7.0.0"))); } else { assertFalse(isCompatible(Version.CURRENT, Version.fromString("7.0.0"))); } assertFalse("only compatible with the latest minor", isCompatible(VersionUtils.getPreviousMinorVersion(), Version.fromString("7.0.0"))); assertFalse(isCompatible(Version.V_5_0_0, Version.fromString("6.0.0"))); assertFalse(isCompatible(Version.V_5_0_0, Version.fromString("7.0.0"))); Version a = randomVersion(random()); Version b = randomVersion(random()); assertThat(a.isCompatible(b), equalTo(b.isCompatible(a))); }	nit: compact -> compat
public void testRejectingJoinWithIncompatibleVersion() throws InterruptedException, ExecutionException { addNodes(randomInt(5)); ClusterState state = discoveryState(masterService); final DiscoveryNode badNode = new DiscoveryNode("badNode", buildNewFakeTransportAddress(), emptyMap(), new HashSet<>(randomSubsetOf(Arrays.asList(DiscoveryNode.Role.values()))), getPreviousVersion(Version.CURRENT.minimumCompatibilityVersion())); final DiscoveryNode goodNode = new DiscoveryNode("goodNode", buildNewFakeTransportAddress(), emptyMap(), new HashSet<>(randomSubsetOf(Arrays.asList(DiscoveryNode.Role.values()))), Version.CURRENT); CountDownLatch latch = new CountDownLatch(1); // block cluster state masterService.submitStateUpdateTask("test", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) throws Exception { latch.await(); return currentState; } @Override public void onFailure(String source, Exception e) { throw new AssertionError(e); } }); final SimpleFuture badJoin; final SimpleFuture goodJoin; if (randomBoolean()) { badJoin = joinNodeAsync(badNode); goodJoin = joinNodeAsync(goodNode); } else { goodJoin = joinNodeAsync(goodNode); badJoin = joinNodeAsync(badNode); } assert goodJoin.isDone() == false; assert badJoin.isDone() == false; latch.countDown(); goodJoin.get(); ExecutionException e = expectThrows(ExecutionException.class, badJoin::get); assertThat(e.getCause(), instanceOf(IllegalStateException.class)); }	please assert on the message (to guard against illegalstateexception being thrown for another reason).
@Override public List<String> localCustomMetaDataTypes() { List<String> customMetaDataList = new ArrayList<>(MetaData.customPrototypes.keySet()); Collections.sort(customMetaDataList); return customMetaDataList; }	any better way to grab registered custom metadata than this?
@SuppressForbidden(reason = "Argument to Math.abs() is definitely not Long.MIN_VALUE") public boolean checkFeature(Feature feature) { boolean allowed = isAllowed(feature); LongAccumulator maxEpochAccumulator = lastUsed.get(feature); final long licenseExpiryDate = getLicenseExpiryDate(); final long diff = licenseExpiryDate - System.currentTimeMillis(); if (maxEpochAccumulator != null) { maxEpochAccumulator.accumulate(epochMillisProvider.getAsLong()); } if (feature.minimumOperationMode.compareTo(OperationMode.BASIC) > 0 && LICENSE_EXPIRATION_WARNING_PERIOD.getMillis() > diff) { final long days = TimeUnit.MILLISECONDS.toDays(diff); final String expiryMessage = (days == 0 && diff > 0)? "expires today": (diff > 0? String.format(Locale.ROOT, "will expire in [%d] days", days): String.format(Locale.ROOT, "expired on [%s]", LicenseService.DATE_FORMATTER.formatMillis(licenseExpiryDate))); HeaderWarning.addWarning("Your license {}. " + "Contact your administrator or update your license for continued use of features", expiryMessage); } return allowed; }	i guess we are fine with the message in warnigns and subtleties of todays and being close to midnight in a given timezone , right? i think we are, just asking for completeness.
public void testWarningHeader() throws Exception { Request request = new Request("GET", "/_security/user"); RequestOptions.Builder options = request.getOptions().toBuilder(); options.addHeader("Authorization", basicAuthHeaderValue(SecuritySettingsSource.TEST_USER_NAME, new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()))); request.setOptions(options); Response response = getRestClient().performRequest(request); List<String> beforeWarningHeaders = getWarningHeaders(response.getHeaders()); assertTrue(beforeWarningHeaders.isEmpty()); License.OperationMode mode = randomFrom(License.OperationMode.GOLD, License.OperationMode.PLATINUM, License.OperationMode.ENTERPRISE, License.OperationMode.STANDARD); long now = System.currentTimeMillis(); long newExpirationDate = now + LICENSE_EXPIRATION_WARNING_PERIOD.getMillis() - 1; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); List<String> afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertThat(afterWarningHeaders, Matchers.hasSize(1)); assertThat(afterWarningHeaders.get(0), Matchers.containsString("Your license will expire in [6] days. " + "Contact your administrator or update your license for continued use of features")); newExpirationDate = now + 300000; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertThat(afterWarningHeaders, Matchers.hasSize(1)); assertThat(afterWarningHeaders.get(0), Matchers.containsString("Your license expires today. " + "Contact your administrator or update your license for continued use of features")); newExpirationDate = now - 300000; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertThat(afterWarningHeaders, Matchers.hasSize(1)); long finalNewExpirationDate = newExpirationDate; String expiredMessage = String.format(Locale.ROOT, "Your license expired on [%s]. ", LicenseService.DATE_FORMATTER.formatMillis(finalNewExpirationDate)); assertThat(afterWarningHeaders.get(0), Matchers.containsString(expiredMessage + "Contact your administrator or update your license for continued use of features")); }	nit: do we need all the empty lines?
public void testWarningHeader() throws Exception { Request request = new Request("GET", "/_security/user"); RequestOptions.Builder options = request.getOptions().toBuilder(); options.addHeader("Authorization", basicAuthHeaderValue(SecuritySettingsSource.TEST_USER_NAME, new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()))); request.setOptions(options); Response response = getRestClient().performRequest(request); List<String> beforeWarningHeaders = getWarningHeaders(response.getHeaders()); assertTrue(beforeWarningHeaders.isEmpty()); License.OperationMode mode = randomFrom(License.OperationMode.GOLD, License.OperationMode.PLATINUM, License.OperationMode.ENTERPRISE, License.OperationMode.STANDARD); long now = System.currentTimeMillis(); long newExpirationDate = now + LICENSE_EXPIRATION_WARNING_PERIOD.getMillis() - 1; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); List<String> afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertThat(afterWarningHeaders, Matchers.hasSize(1)); assertThat(afterWarningHeaders.get(0), Matchers.containsString("Your license will expire in [6] days. " + "Contact your administrator or update your license for continued use of features")); newExpirationDate = now + 300000; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertThat(afterWarningHeaders, Matchers.hasSize(1)); assertThat(afterWarningHeaders.get(0), Matchers.containsString("Your license expires today. " + "Contact your administrator or update your license for continued use of features")); newExpirationDate = now - 300000; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertThat(afterWarningHeaders, Matchers.hasSize(1)); long finalNewExpirationDate = newExpirationDate; String expiredMessage = String.format(Locale.ROOT, "Your license expired on [%s]. ", LicenseService.DATE_FORMATTER.formatMillis(finalNewExpirationDate)); assertThat(afterWarningHeaders.get(0), Matchers.containsString(expiredMessage + "Contact your administrator or update your license for continued use of features")); }	nit suggestion : testnowarningheaderwhenauthenticationfailed
private void handleException(String actionType, RestRequest request, RestChannel channel, Exception e) { logger.debug(new ParameterizedMessage("{} failed for REST request [{}]", actionType, request.uri()), e); final RestStatus restStatus = ExceptionsHelper.status(e); try { channel.sendResponse(new BytesRestResponse(channel, restStatus, e) { @Override protected boolean skipStackTrace() { return restStatus == RestStatus.UNAUTHORIZED; } @Override public Map<String, List<String>> filterHeaders(Map<String, List<String>> headers) { if (headers.containsKey("Warning")) { return Maps.copyMapWithRemovedEntry(headers, "Warning"); } return headers; } }); } catch (Exception inner) { inner.addSuppressed(e); logger.error((Supplier<?>) () -> new ParameterizedMessage("failed to send failure response for uri [{}]", request.uri()), inner); } }	do we want to remove _all_ warning headers on authentication failure?
public void testNamedObject() throws IOException { Object test1 = new Object(); Object test2 = new Object(); NamedXContentRegistry registry = new NamedXContentRegistry(Arrays.asList( new NamedXContentRegistry.Entry(Object.class, new ParseField("test1"), p -> test1), new NamedXContentRegistry.Entry(Object.class, new ParseField("test2", "deprecated"), p -> test2), new NamedXContentRegistry.Entry(Object.class, new ParseField("str"), p -> p.text()))); XContentBuilder b = XContentBuilder.builder(xcontentType().xContent()); b.value("test"); XContentParser p = xcontentType().xContent().createParser(registry, LoggingDeprecationHandler.INSTANCE, BytesReference.bytes(b).streamInput()); assertEquals(test1, p.namedObject(Object.class, "test1", null)); assertEquals(test2, p.namedObject(Object.class, "test2", null)); assertEquals(test2, p.namedObject(Object.class, "deprecated", null)); assertWarnings("Deprecated field [deprecated] used, expected [test2] instead"); { p.nextToken(); assertEquals("test", p.namedObject(Object.class, "str", null)); XContentParseException e = expectThrows(XContentParseException.class, () -> p.namedObject(Object.class, "unknown", null)); assertThat(e.getMessage(), endsWith("unable to parse Object with name [unknown]: parser not found")); } { Exception e = expectThrows(XContentParseException.class, () -> p.namedObject(String.class, "doesn't matter", null)); assertEquals("unknown named object category [java.lang.String]", e.getMessage()); } { XContentParser emptyRegistryParser = xcontentType().xContent() .createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, new byte[] {}); Exception e = expectThrows(XContentParseException.class, () -> emptyRegistryParser.namedObject(String.class, "doesn't matter", null)); assertEquals("named objects are not supported for this parser", e.getMessage()); } }	i think this should be namedobejectnotfoundexception instead, right? it is more specific.
public void testUnknownFieldsExpection() throws IOException { String rescoreElement = "{\\\\n" + " \\\\"window_size\\\\" : 20,\\\\n" + " \\\\"bad_rescorer_name\\\\" : { }\\\\n" + "}\\\\n"; { XContentParser parser = createParser(rescoreElement); Exception e = expectThrows(XContentParseException.class, () -> RescorerBuilder.parseFromXContent(parser)); assertEquals("[3:27] unable to parse RescorerBuilder with name [bad_rescorer_name]: parser not found", e.getMessage()); } rescoreElement = "{\\\\n" + " \\\\"bad_fieldName\\\\" : 20\\\\n" + "}\\\\n"; { XContentParser parser = createParser(rescoreElement); Exception e = expectThrows(ParsingException.class, () -> RescorerBuilder.parseFromXContent(parser)); assertEquals("rescore doesn't support [bad_fieldName]", e.getMessage()); } rescoreElement = "{\\\\n" + " \\\\"window_size\\\\" : 20,\\\\n" + " \\\\"query\\\\" : [ ]\\\\n" + "}\\\\n"; { XContentParser parser = createParser(rescoreElement); Exception e = expectThrows(ParsingException.class, () -> RescorerBuilder.parseFromXContent(parser)); assertEquals("unexpected token [START_ARRAY] after [query]", e.getMessage()); } rescoreElement = "{ }"; { XContentParser parser = createParser(rescoreElement); Exception e = expectThrows(ParsingException.class, () -> RescorerBuilder.parseFromXContent(parser)); assertEquals("missing rescore type", e.getMessage()); } rescoreElement = "{\\\\n" + " \\\\"window_size\\\\" : 20,\\\\n" + " \\\\"query\\\\" : { \\\\"bad_fieldname\\\\" : 1.0 } \\\\n" + "}\\\\n"; { XContentParser parser = createParser(rescoreElement); Exception e = expectThrows(IllegalArgumentException.class, () -> RescorerBuilder.parseFromXContent(parser)); assertEquals("[query] unknown field [bad_fieldname], parser not found", e.getMessage()); } rescoreElement = "{\\\\n" + " \\\\"window_size\\\\" : 20,\\\\n" + " \\\\"query\\\\" : { \\\\"rescore_query\\\\" : { \\\\"unknown_queryname\\\\" : { } } } \\\\n" + "}\\\\n"; { XContentParser parser = createParser(rescoreElement); Exception e = expectThrows(ParsingException.class, () -> RescorerBuilder.parseFromXContent(parser)); assertEquals("[query] failed to parse field [rescore_query]", e.getMessage()); } rescoreElement = "{\\\\n" + " \\\\"window_size\\\\" : 20,\\\\n" + " \\\\"query\\\\" : { \\\\"rescore_query\\\\" : { \\\\"match_all\\\\" : { } } } \\\\n" + "}\\\\n"; XContentParser parser = createParser(rescoreElement); RescorerBuilder.parseFromXContent(parser); }	all of these should be namedobjectnotfoundexception i think.
static void checkForDeprecatedTZ(PutRollupJobAction.Request request) { String timeZone = request.getConfig().getGroupConfig().getDateHistogram().getTimeZone(); String modernTZ = DateUtils.DEPRECATED_LONG_TIMEZONES.get(timeZone); if (modernTZ != null) { deprecationLogger.deprecated("Creating Rollup job [" + request.getConfig().getId() + "] with timezone [" + timeZone + "], but [" + timeZone + "] has been deprecated by the IANA. Use [" + modernTZ +"] instead."); } }	dateutils.of can also do that for you, but you would get a different log message.. do you think it is worth using instead?
static QueryBuilder rewriteQuery(QueryBuilder builder, Set<RollupJobCaps> jobCaps) { if (builder == null) { return new MatchAllQueryBuilder(); } if (builder.getWriteableName().equals(BoolQueryBuilder.NAME)) { BoolQueryBuilder rewrittenBool = new BoolQueryBuilder(); ((BoolQueryBuilder)builder).must().forEach(query -> rewrittenBool.must(rewriteQuery(query, jobCaps))); ((BoolQueryBuilder)builder).mustNot().forEach(query -> rewrittenBool.mustNot(rewriteQuery(query, jobCaps))); ((BoolQueryBuilder)builder).should().forEach(query -> rewrittenBool.should(rewriteQuery(query, jobCaps))); ((BoolQueryBuilder)builder).filter().forEach(query -> rewrittenBool.filter(rewriteQuery(query, jobCaps))); return rewrittenBool; } else if (builder.getWriteableName().equals(ConstantScoreQueryBuilder.NAME)) { return new ConstantScoreQueryBuilder(rewriteQuery(((ConstantScoreQueryBuilder)builder).innerQuery(), jobCaps)); } else if (builder.getWriteableName().equals(BoostingQueryBuilder.NAME)) { return new BoostingQueryBuilder(rewriteQuery(((BoostingQueryBuilder)builder).negativeQuery(), jobCaps), rewriteQuery(((BoostingQueryBuilder)builder).positiveQuery(), jobCaps)); } else if (builder.getWriteableName().equals(DisMaxQueryBuilder.NAME)) { DisMaxQueryBuilder rewritten = new DisMaxQueryBuilder(); ((DisMaxQueryBuilder) builder).innerQueries().forEach(query -> rewritten.add(rewriteQuery(query, jobCaps))); return rewritten; } else if (builder.getWriteableName().equals(RangeQueryBuilder.NAME)) { RangeQueryBuilder range = (RangeQueryBuilder) builder; String fieldName = range.fieldName(); // Many range queries don't include the timezone because the default is UTC, but the query // builder will return null so we need to set it here String timeZone = range.timeZone() == null ? DateHistogramGroupConfig.DEFAULT_ZONEID_TIMEZONE.toString() : range.timeZone(); String rewrittenFieldName = rewriteFieldName(jobCaps, RangeQueryBuilder.NAME, fieldName, timeZone); RangeQueryBuilder rewritten = new RangeQueryBuilder(rewrittenFieldName) .from(range.from()) .to(range.to()) .includeLower(range.includeLower()) .includeUpper(range.includeUpper()) .timeZone(timeZone); if (range.format() != null) { rewritten.format(range.format()); } return rewritten; } else if (builder.getWriteableName().equals(TermQueryBuilder.NAME)) { TermQueryBuilder term = (TermQueryBuilder) builder; String fieldName = term.fieldName(); String rewrittenFieldName = rewriteFieldName(jobCaps, TermQueryBuilder.NAME, fieldName, null); return new TermQueryBuilder(rewrittenFieldName, term.value()); } else if (builder.getWriteableName().equals(TermsQueryBuilder.NAME)) { TermsQueryBuilder terms = (TermsQueryBuilder) builder; String fieldName = terms.fieldName(); String rewrittenFieldName = rewriteFieldName(jobCaps, TermQueryBuilder.NAME, fieldName, null); return new TermsQueryBuilder(rewrittenFieldName, terms.values()); } else if (builder.getWriteableName().equals(MatchAllQueryBuilder.NAME)) { // no-op return builder; } else { throw new IllegalArgumentException("Unsupported Query in search request: [" + builder.getWriteableName() + "]"); } }	maybe it would better to keep the zoneid type as long as it is not needed to be converted? so here and then passing to rewritefieldname where it would be converted to string?
@Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } UpdateSettingsRequest that = (UpdateSettingsRequest) o; return Objects.equals(settings, that.settings) && Objects.equals(indicesOptions, that.indicesOptions) && Objects.equals(preserveExisting, that.preserveExisting) && Arrays.equals(indices, that.indices); }	don't you need to take also timeout and masternodetimeout into account?
void possiblyValidateImplicitSecurityBehaviorOnUpdate(Settings settings, NodeMetadata nodeMetadata) { if (null == nodeMetadata) { // No Node metadata for given node return; } final XPackLicenseState licenseState = getLicenseState(); if (licenseState.getLicenseExpiryDate() == Long.MAX_VALUE) { // License hasn't been recovered yet from disk of cluster state return; } final License.OperationMode license = licenseState.getOperationMode(); final Version lastKnownVersion = nodeMetadata.nodeVersion(); // pre v7.2.0 nodes have Version.EMPTY and its id is 0, so Version#before handles this successfully if (lastKnownVersion.before(Version.V_8_0_0) && XPackSettings.SECURITY_ENABLED.exists(settings) == false && (license == License.OperationMode.BASIC || license == License.OperationMode.TRIAL)) { throw new IllegalStateException( "The default value for [" + XPackSettings.SECURITY_ENABLED.getKey() + "] has changed. See https://www.elastic.co/guide/en/elasticsearch/reference/" + Version.CURRENT.major + "." + Version.CURRENT.minor + "/security-minimal-setup.html to enable security, or explicitly disable security by " + "setting [xpack.security.enabled] to \\\\"false\\\\" in elasticsearch.yml before restarting the node" ); } }	is this condition ever false when running properly? we're running this from createcomponents, but i think licensestate represents the cluster state that's exposed by the clusterservice so the state isn't even initialised when this runs, let alone derived from disk. sorry if i'm missing something obvious here...
private void authorizeAction( final RequestInfo requestInfo, final String requestId, final AuthorizationInfo authzInfo, final ActionListener<Void> listener ) { final Authentication authentication = requestInfo.getAuthentication(); final TransportRequest request = requestInfo.getRequest(); final String action = requestInfo.getAction(); final AuthorizationEngine authzEngine = getAuthorizationEngine(authentication); final AuditTrail auditTrail = auditTrailService.get(); if (ClusterPrivilegeResolver.isClusterAction(action)) { final ActionListener<AuthorizationResult> clusterAuthzListener = wrapPreservingContext( new AuthorizationResultListener<>(result -> { threadContext.putTransient(INDICES_PERMISSIONS_KEY, IndicesAccessControl.allowAll()); listener.onResponse(null); }, listener::onFailure, requestInfo, requestId, authzInfo), threadContext ); authzEngine.authorizeClusterAction(requestInfo, authzInfo, ActionListener.wrap(result -> { if (false == result.isGranted() && QueryApiKeyAction.NAME.equals(action)) { assert request instanceof QueryApiKeyRequest : "request does not match action"; final QueryApiKeyRequest queryApiKeyRequest = (QueryApiKeyRequest) request; if (false == queryApiKeyRequest.isFilterForCurrentUser()) { queryApiKeyRequest.setFilterForCurrentUser(); authzEngine.authorizeClusterAction(requestInfo, authzInfo, clusterAuthzListener); return; } } clusterAuthzListener.onResponse(result); }, clusterAuthzListener::onFailure)); } else if (isIndexAction(action)) { final Metadata metadata = clusterService.state().metadata(); final AsyncSupplier<ResolvedIndices> resolvedIndicesAsyncSupplier = new CachingAsyncSupplier<>(resolvedIndicesListener -> { ResolvedIndices resolvedIndices = indicesAndAliasesResolver.tryResolveWithoutWildcards(action, request); if (resolvedIndices != null) { resolvedIndicesListener.onResponse(resolvedIndices); } else { try { resolvedIndicesListener.onResponse( indicesAndAliasesResolver.resolve( action, request, metadata, authzEngine.loadAuthorizedIndices(requestInfo, authzInfo, metadata) ) ); } catch (Exception e) { auditTrail.accessDenied(requestId, authentication, action, request, authzInfo); if (e instanceof IndexNotFoundException) { listener.onFailure(e); } else { listener.onFailure(denialException(authentication, action, request, e)); } } } }); authzEngine.authorizeIndexAction( requestInfo, authzInfo, resolvedIndicesAsyncSupplier, metadata.getIndicesLookup(), wrapPreservingContext( new AuthorizationResultListener<>( result -> handleIndexActionAuthorizationResult( result, requestInfo, requestId, authzInfo, authzEngine, resolvedIndicesAsyncSupplier, metadata, listener ), listener::onFailure, requestInfo, requestId, authzInfo ), threadContext ) ); } else { logger.warn("denying access as action [{}] is not an index or cluster action", action); auditTrail.accessDenied(requestId, authentication, action, request, authzInfo); listener.onFailure(denialException(authentication, action, request, null)); } }	logging for slow loadauthorizedindices is not easily relocatable with the proposed changes because the logic is now in server package and does not know anything about user or roles. one possibility is to pass a string description to that class just for logging purpose.
public void testFollowIndex() throws Exception { final int numberOfPrimaryShards = randomIntBetween(1, 3); int numberOfReplicas = between(0, 1); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, numberOfReplicas, singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("index1").setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderYellow("index1"); final int firstBatchNumDocs = randomIntBetween(800, 1200); final int flushPoint = (int) (firstBatchNumDocs * 0.75); logger.info("Indexing [{}] docs as first batch", firstBatchNumDocs); BulkRequestBuilder bulkRequestBuilder = leaderClient().prepareBulk(); for (int i = 0; i < flushPoint; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); IndexRequest indexRequest = new IndexRequest("index1", "doc", Integer.toString(i)) .source(source, XContentType.JSON) .timeout(TimeValue.timeValueSeconds(1)); bulkRequestBuilder.add(indexRequest); } bulkRequestBuilder.get(); leaderClient().admin().indices().prepareFlush("index1").setForce(true).setWaitIfOngoing(true).get(); // Index some docs after the flush that will be recovered in the normal index following operations for (int i = flushPoint; i < firstBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } boolean waitOnAll = randomBoolean(); final PutFollowAction.Request followRequest; if (waitOnAll) { followRequest = putFollow("index1", "index2", ActiveShardCount.ALL); } else { followRequest = putFollow("index1", "index2", ActiveShardCount.ONE); } PutFollowAction.Response response = followerClient().execute(PutFollowAction.INSTANCE, followRequest).get(); assertTrue(response.isFollowIndexCreated()); assertTrue(response.isFollowIndexShardsAcked()); assertTrue(response.isIndexFollowingStarted()); ClusterHealthRequest healthRequest = Requests.clusterHealthRequest("index2").waitForNoRelocatingShards(true); ClusterIndexHealth indexHealth = followerClient().admin().cluster().health(healthRequest).actionGet().getIndices().get("index2"); for (ClusterShardHealth shardHealth : indexHealth.getShards().values()) { if (waitOnAll) { assertTrue(shardHealth.isPrimaryActive()); assertEquals(1 + numberOfReplicas, shardHealth.getActiveShards()); } else { assertTrue(shardHealth.isPrimaryActive()); } } final Map<ShardId, Long> firstBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] firstBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : firstBatchShardStats) { if (shardStats.getShardRouting().primary()) { long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; firstBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, firstBatchNumDocsPerShard)); for (int i = 0; i < firstBatchNumDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); followerClient().execute(ResumeFollowAction.INSTANCE, followRequest.getFollowRequest()).get(); final int secondBatchNumDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as second batch", secondBatchNumDocs); for (int i = firstBatchNumDocs; i < firstBatchNumDocs + secondBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } final Map<ShardId, Long> secondBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] secondBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : secondBatchShardStats) { if (shardStats.getShardRouting().primary()) { final long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; secondBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, secondBatchNumDocsPerShard)); for (int i = firstBatchNumDocs; i < firstBatchNumDocs + secondBatchNumDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); assertMaxSeqNoOfUpdatesIsTransferred(resolveLeaderIndex("index1"), resolveFollowerIndex("index2"), numberOfPrimaryShards); }	how long does it take to index this? perhaps we can only do this rarely?
public void testFollowIndex() throws Exception { final int numberOfPrimaryShards = randomIntBetween(1, 3); int numberOfReplicas = between(0, 1); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, numberOfReplicas, singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("index1").setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderYellow("index1"); final int firstBatchNumDocs = randomIntBetween(800, 1200); final int flushPoint = (int) (firstBatchNumDocs * 0.75); logger.info("Indexing [{}] docs as first batch", firstBatchNumDocs); BulkRequestBuilder bulkRequestBuilder = leaderClient().prepareBulk(); for (int i = 0; i < flushPoint; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); IndexRequest indexRequest = new IndexRequest("index1", "doc", Integer.toString(i)) .source(source, XContentType.JSON) .timeout(TimeValue.timeValueSeconds(1)); bulkRequestBuilder.add(indexRequest); } bulkRequestBuilder.get(); leaderClient().admin().indices().prepareFlush("index1").setForce(true).setWaitIfOngoing(true).get(); // Index some docs after the flush that will be recovered in the normal index following operations for (int i = flushPoint; i < firstBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } boolean waitOnAll = randomBoolean(); final PutFollowAction.Request followRequest; if (waitOnAll) { followRequest = putFollow("index1", "index2", ActiveShardCount.ALL); } else { followRequest = putFollow("index1", "index2", ActiveShardCount.ONE); } PutFollowAction.Response response = followerClient().execute(PutFollowAction.INSTANCE, followRequest).get(); assertTrue(response.isFollowIndexCreated()); assertTrue(response.isFollowIndexShardsAcked()); assertTrue(response.isIndexFollowingStarted()); ClusterHealthRequest healthRequest = Requests.clusterHealthRequest("index2").waitForNoRelocatingShards(true); ClusterIndexHealth indexHealth = followerClient().admin().cluster().health(healthRequest).actionGet().getIndices().get("index2"); for (ClusterShardHealth shardHealth : indexHealth.getShards().values()) { if (waitOnAll) { assertTrue(shardHealth.isPrimaryActive()); assertEquals(1 + numberOfReplicas, shardHealth.getActiveShards()); } else { assertTrue(shardHealth.isPrimaryActive()); } } final Map<ShardId, Long> firstBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] firstBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : firstBatchShardStats) { if (shardStats.getShardRouting().primary()) { long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; firstBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, firstBatchNumDocsPerShard)); for (int i = 0; i < firstBatchNumDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); followerClient().execute(ResumeFollowAction.INSTANCE, followRequest.getFollowRequest()).get(); final int secondBatchNumDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as second batch", secondBatchNumDocs); for (int i = firstBatchNumDocs; i < firstBatchNumDocs + secondBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } final Map<ShardId, Long> secondBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] secondBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : secondBatchShardStats) { if (shardStats.getShardRouting().primary()) { final long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; secondBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, secondBatchNumDocsPerShard)); for (int i = firstBatchNumDocs; i < firstBatchNumDocs + secondBatchNumDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); assertMaxSeqNoOfUpdatesIsTransferred(resolveLeaderIndex("index1"), resolveFollowerIndex("index2"), numberOfPrimaryShards); }	is it necessary to force (i don't think so)?
public void testFollowIndex() throws Exception { final int numberOfPrimaryShards = randomIntBetween(1, 3); int numberOfReplicas = between(0, 1); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, numberOfReplicas, singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("index1").setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderYellow("index1"); final int firstBatchNumDocs = randomIntBetween(800, 1200); final int flushPoint = (int) (firstBatchNumDocs * 0.75); logger.info("Indexing [{}] docs as first batch", firstBatchNumDocs); BulkRequestBuilder bulkRequestBuilder = leaderClient().prepareBulk(); for (int i = 0; i < flushPoint; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); IndexRequest indexRequest = new IndexRequest("index1", "doc", Integer.toString(i)) .source(source, XContentType.JSON) .timeout(TimeValue.timeValueSeconds(1)); bulkRequestBuilder.add(indexRequest); } bulkRequestBuilder.get(); leaderClient().admin().indices().prepareFlush("index1").setForce(true).setWaitIfOngoing(true).get(); // Index some docs after the flush that will be recovered in the normal index following operations for (int i = flushPoint; i < firstBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } boolean waitOnAll = randomBoolean(); final PutFollowAction.Request followRequest; if (waitOnAll) { followRequest = putFollow("index1", "index2", ActiveShardCount.ALL); } else { followRequest = putFollow("index1", "index2", ActiveShardCount.ONE); } PutFollowAction.Response response = followerClient().execute(PutFollowAction.INSTANCE, followRequest).get(); assertTrue(response.isFollowIndexCreated()); assertTrue(response.isFollowIndexShardsAcked()); assertTrue(response.isIndexFollowingStarted()); ClusterHealthRequest healthRequest = Requests.clusterHealthRequest("index2").waitForNoRelocatingShards(true); ClusterIndexHealth indexHealth = followerClient().admin().cluster().health(healthRequest).actionGet().getIndices().get("index2"); for (ClusterShardHealth shardHealth : indexHealth.getShards().values()) { if (waitOnAll) { assertTrue(shardHealth.isPrimaryActive()); assertEquals(1 + numberOfReplicas, shardHealth.getActiveShards()); } else { assertTrue(shardHealth.isPrimaryActive()); } } final Map<ShardId, Long> firstBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] firstBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : firstBatchShardStats) { if (shardStats.getShardRouting().primary()) { long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; firstBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, firstBatchNumDocsPerShard)); for (int i = 0; i < firstBatchNumDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); followerClient().execute(ResumeFollowAction.INSTANCE, followRequest.getFollowRequest()).get(); final int secondBatchNumDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as second batch", secondBatchNumDocs); for (int i = firstBatchNumDocs; i < firstBatchNumDocs + secondBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } final Map<ShardId, Long> secondBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] secondBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : secondBatchShardStats) { if (shardStats.getShardRouting().primary()) { final long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; secondBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, secondBatchNumDocsPerShard)); for (int i = firstBatchNumDocs; i < firstBatchNumDocs + secondBatchNumDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); assertMaxSeqNoOfUpdatesIsTransferred(resolveLeaderIndex("index1"), resolveFollowerIndex("index2"), numberOfPrimaryShards); }	this comment is not true. there is no guarantee that these won't be flushed. change the wording from will to might
public void accept(DiscoveryNode node, ClusterState state) { License license = LicenseService.getLicense(state.metaData()); if (license != null && license.version() >= License.VERSION_CRYPTO_ALGORITHMS && node.getVersion().before(Version.V_6_4_0)) { throw new IllegalStateException("node " + node + " is on version [" + node.getVersion() + "] that cannot deserialize the license format [" + license.version() + "], upgrade node to at least 6.4.0"); } } } static final class ValidateLicenseForFIPS implements BiConsumer<DiscoveryNode, ClusterState> { private final boolean inFipsMode; ValidateLicenseForFIPS(boolean inFipsMode) { this.inFipsMode = inFipsMode; } @Override public void accept(DiscoveryNode node, ClusterState state) { if (inFipsMode) { License license = LicenseService.getLicense(state.metaData()); if (license != null && FIPS140LicenseBootstrapCheck.ALLOWED_LICENSE_OPERATION_MODES.contains(license.operationMode()) == false) { throw new IllegalStateException("License operation mode [" + license.operationMode() + "] is not valid for FIPS"); } } } }	maybe add a clearer message indicating that the problem is with fips_mode setting. how about throw new illegalstateexception("fips mode cannot be used with a [" + license.operationmode() + "] license. it is only allowed with a platinum or trial license.");
public XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException { if (scrollId != null) { builder.field(SCROLL_ID.getPreferredName(), scrollId); } builder.field(TOOK.getPreferredName(), tookInMillis); builder.field(TIMED_OUT.getPreferredName(), isTimedOut()); if (isTerminatedEarly() != null) { builder.field(TERMINATED_EARLY.getPreferredName(), isTerminatedEarly()); } if (getNumReducePhases() != 1) { builder.field(NUM_REDUCE_PHASES.getPreferredName(), getNumReducePhases()); } RestActions.buildBroadcastShardsHeader(builder, params, getTotalShards(), getSuccessfulShards(), getFailedShards(), getShardFailures()); internalResponse.toXContent(builder, params); return builder; }	nit: can you move the ensureexpectedtoken() first?
private static void updateIndexMappingsAndBuildSortOrder(IndexService indexService, Map<String, Object> mappings, @Nullable IndexMetaData sourceMetaData) throws IOException { MapperService mapperService = indexService.mapperService(); if (!mappings.isEmpty()) { assert mappings.size() == 1 : mappings; mapperService.merge(MapperService.SINGLE_MAPPING_NAME, mappings, MergeReason.MAPPING_UPDATE); mapperService.validateDynamicTemplates(); } if (sourceMetaData == null) { // now that the mapping is merged we can validate the index sort. // we cannot validate for index shrinking since the mapping is empty // at this point. The validation will take place later in the process // (when all shards are copied in a single place). indexService.getIndexSortSupplier().get(); } }	this seems to only validate dynamic templates at creation time, should we move validation to mapperservice#merge to make sure we cover all cases, including mapping updates that add dynamic templates on existing indices?
public void testDataStreamsWithWildcardExpression() { final String dataStream1 = "logs-mysql"; final String dataStream2 = "logs-redis"; IndexMetadata index1 = createBackingIndex(dataStream1, 1).build(); IndexMetadata index2 = createBackingIndex(dataStream1, 2).build(); IndexMetadata index3 = createBackingIndex(dataStream2, 1).build(); IndexMetadata index4 = createBackingIndex(dataStream2, 2).build(); Metadata.Builder mdBuilder = Metadata.builder() .put(index1, false) .put(index2, false) .put(index3, false) .put(index4, false) .put(new DataStream(dataStream1, createTimestampField("@timestamp"), List.of(index1.getIndex(), index2.getIndex()))) .put(new DataStream(dataStream2, createTimestampField("@timestamp"), List.of(index3.getIndex(), index4.getIndex()))); ClusterState state = ClusterState.builder(new ClusterName("_name")).metadata(mdBuilder).build(); { IndicesOptions indicesOptions = IndicesOptions.STRICT_EXPAND_OPEN; Index[] result = indexNameExpressionResolver.concreteIndices(state, indicesOptions, true, "logs-*"); Arrays.sort(result, Comparator.comparing(Index::getName)); assertThat(result.length, equalTo(4)); assertThat(result[0].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream1, 1))); assertThat(result[1].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream1, 2))); assertThat(result[2].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream2, 1))); assertThat(result[3].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream2, 2)));; } { IndicesOptions indicesOptions = IndicesOptions.STRICT_EXPAND_OPEN; Index[] result = indexNameExpressionResolver.concreteIndices(state, indicesOptions, true, "*"); Arrays.sort(result, Comparator.comparing(Index::getName)); assertThat(result.length, equalTo(4)); assertThat(result[0].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream1, 1))); assertThat(result[1].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream1, 2))); assertThat(result[2].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream2, 1))); assertThat(result[3].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream2, 2)));; } { IndicesOptions indicesOptions = IndicesOptions.STRICT_EXPAND_OPEN; Index[] result = indexNameExpressionResolver.concreteIndices(state, indicesOptions, true, "logs-m*"); Arrays.sort(result, Comparator.comparing(Index::getName)); assertThat(result.length, equalTo(2)); assertThat(result[0].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream1, 1))); assertThat(result[1].getName(), equalTo(DataStream.getDefaultBackingIndexName(dataStream1, 2))); } { IndicesOptions indicesOptions = IndicesOptions.STRICT_EXPAND_OPEN; // without include data streams Exception e = expectThrows(IllegalArgumentException.class, () -> indexNameExpressionResolver.concreteIndices(state, indicesOptions, "logs-*")); assertThat(e.getMessage(), equalTo("The provided expression [logs-*] matches a data stream, " + "specify the corresponding concrete indices instead.")); } }	maybe randomize between: *, _all and no args / empty array?
* @param repositoryData the {@link RepositoryData} of the repository to delete from * @param minNodeVersion minimum node version in the cluster */ private void deleteSnapshotsFromRepository( SnapshotDeletionsInProgress.Entry deleteEntry, RepositoryData repositoryData, Version minNodeVersion ) { if (repositoryOperations.startDeletion(deleteEntry.uuid())) { assert currentlyFinalizing.contains(deleteEntry.repository()); final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots(); assert deleteEntry.state() == SnapshotDeletionsInProgress.State.STARTED : "incorrect state for entry [" + deleteEntry + "]"; if (snapshotIds.isEmpty()) { // this deletion overlapped one or more deletions that were successfully processed and there is no remaining snapshot to // delete now, we can avoid reaching to the repository and can complete the deletion. removeSnapshotDeletionFromClusterState(deleteEntry, null, repositoryData); return; } repositoriesService.repository(deleteEntry.repository()) .deleteSnapshots( snapshotIds, repositoryData.getGenId(), minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds), ActionListener.wrap(updatedRepoData -> { logger.info("snapshots {} deleted", snapshotIds); removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData); }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)) ); } } /** * Removes a {@link SnapshotDeletionsInProgress.Entry} from {@link SnapshotDeletionsInProgress} in the cluster state after it executed * on the repository. * * @param deleteEntry delete entry to remove from the cluster state * @param failure failure encountered while executing the delete on the repository or {@code null} if the delete executed * successfully * @param repositoryData current {@link RepositoryData}	i think this is fine as an optimization but we could do even better imo. couldn't we just resolve the listener on deletes that become empty on a cs update and remove them from the cs right away, thus saving us another cs update?
PublishWithJoinResponse handlePublishRequest(PublishRequest publishRequest) { assert publishRequest.getAcceptedState().nodes().getLocalNode().equals(getLocalNode()) : publishRequest.getAcceptedState().nodes().getLocalNode() + " != " + getLocalNode(); synchronized (mutex) { final DiscoveryNode sourceNode = publishRequest.getAcceptedState().nodes().getMasterNode(); logger.trace("handlePublishRequest: handling [{}] from [{}]", publishRequest, sourceNode); if (sourceNode.equals(getLocalNode()) && mode != Mode.LEADER) { // Rare case in which we stood down as leader between starting this publication and receiving it ourselves. The publication // is already failed so there is no point in proceeding. throw new CoordinationStateRejectedException("no longer leading this publication's term: " + publishRequest); } if (publishRequest.getAcceptedState().term() == ZEN1_BWC_TERM && getCurrentTerm() == ZEN1_BWC_TERM && mode == Mode.FOLLOWER && Optional.of(sourceNode).equals(lastKnownLeader) == false) { logger.debug("received cluster state from {} but currently following {}, rejecting", sourceNode, lastKnownLeader); throw new CoordinationStateRejectedException("received cluster state from " + sourceNode + " but currently following " + lastKnownLeader + ", rejecting"); } if (publishRequest.getAcceptedState().term() > coordinationState.get().getLastAcceptedState().term()) { // only do join validation if we have not accepted state from this master yet onJoinValidators.stream().forEach(a -> a.accept(getLocalNode(), publishRequest.getAcceptedState())); } ensureTermAtLeast(sourceNode, publishRequest.getAcceptedState().term()); final PublishResponse publishResponse = coordinationState.get().handlePublishRequest(publishRequest); if (sourceNode.equals(getLocalNode())) { preVoteCollector.update(getPreVoteResponse(), getLocalNode()); } else { becomeFollower("handlePublishRequest", sourceNode); // also updates preVoteCollector } if (isInitialConfigurationSet()) { for (final ActionListener<Iterable<DiscoveryNode>> discoveredNodesListener : discoveredNodesListeners) { discoveredNodesListener.onFailure(new ClusterAlreadyBootstrappedException()); } } return new PublishWithJoinResponse(publishResponse, joinWithDestination(lastJoin, sourceNode, publishRequest.getAcceptedState().term())); } }	apparently the .stream() is unnecessary.
private void handleJoinRequest(JoinRequest joinRequest, JoinHelper.JoinCallback joinCallback) { assert Thread.holdsLock(mutex) == false; assert getLocalNode().isMasterNode() : getLocalNode() + " received a join but is not master-eligible"; logger.trace("handleJoinRequest: as {}, handling {}", mode, joinRequest); transportService.connectToNode(joinRequest.getSourceNode()); final ClusterState stateForJoinValidation = getStateForMasterService(); if (stateForJoinValidation.nodes().isLocalNodeElectedMaster()) { // we do this in a couple of places including the cluster update thread. This one here is really just best effort // to ensure we fail as fast as possible. onJoinValidators.stream().forEach(a -> a.accept(joinRequest.getSourceNode(), stateForJoinValidation)); if (stateForJoinValidation.getBlocks().hasGlobalBlock(STATE_NOT_RECOVERED_BLOCK) == false) { JoinTaskExecutor.ensureMajorVersionBarrier(joinRequest.getSourceNode().getVersion(), stateForJoinValidation.getNodes().getMinNodeVersion()); } // validate the join on the joining node, will throw a failure if it fails the validation joinHelper.sendValidateJoinRequest(joinRequest.getSourceNode(), stateForJoinValidation, new ActionListener<Empty>() { @Override public void onResponse(Empty empty) { try { processJoinRequest(joinRequest, joinCallback); } catch (Exception e) { joinCallback.onFailure(e); } } @Override public void onFailure(Exception e) { logger.warn(() -> new ParameterizedMessage("failed to validate incoming join request from node [{}]", joinRequest.getSourceNode()), e); joinCallback.onFailure(new IllegalStateException("failure when sending a validation request to node", e)); } }); } else { processJoinRequest(joinRequest, joinCallback); } }	is this true? i see that we call ensureindexcompatibility and ensurenodescompatibility in jointaskexecutor, but i don't see that we call any other join validators there.
private void handleJoinRequest(JoinRequest joinRequest, JoinHelper.JoinCallback joinCallback) { assert Thread.holdsLock(mutex) == false; assert getLocalNode().isMasterNode() : getLocalNode() + " received a join but is not master-eligible"; logger.trace("handleJoinRequest: as {}, handling {}", mode, joinRequest); transportService.connectToNode(joinRequest.getSourceNode()); final ClusterState stateForJoinValidation = getStateForMasterService(); if (stateForJoinValidation.nodes().isLocalNodeElectedMaster()) { // we do this in a couple of places including the cluster update thread. This one here is really just best effort // to ensure we fail as fast as possible. onJoinValidators.stream().forEach(a -> a.accept(joinRequest.getSourceNode(), stateForJoinValidation)); if (stateForJoinValidation.getBlocks().hasGlobalBlock(STATE_NOT_RECOVERED_BLOCK) == false) { JoinTaskExecutor.ensureMajorVersionBarrier(joinRequest.getSourceNode().getVersion(), stateForJoinValidation.getNodes().getMinNodeVersion()); } // validate the join on the joining node, will throw a failure if it fails the validation joinHelper.sendValidateJoinRequest(joinRequest.getSourceNode(), stateForJoinValidation, new ActionListener<Empty>() { @Override public void onResponse(Empty empty) { try { processJoinRequest(joinRequest, joinCallback); } catch (Exception e) { joinCallback.onFailure(e); } } @Override public void onFailure(Exception e) { logger.warn(() -> new ParameterizedMessage("failed to validate incoming join request from node [{}]", joinRequest.getSourceNode()), e); joinCallback.onFailure(new IllegalStateException("failure when sending a validation request to node", e)); } }); } else { processJoinRequest(joinRequest, joinCallback); } }	apparently the .stream() is unnecessary.
public static String charToString(final char value) { return "" + value; }	can we do this one as string.valueof? https://docs.oracle.com/javase/7/docs/api/java/lang/string.html#valueof%28char%29
public static ScriptTemplate nullSafeFilter(ScriptTemplate script) { return new ScriptTemplate(formatTemplate( format(Locale.ROOT, "{ql}.nullSafeFilter(%s)", script.template())), script.params(), DataTypes.BOOLEAN); }	this method seems to be used in one place as such extracting it in a different class doesn't add much value since there's no reuse. the name also indicates that - a method specific to cardinality/count.
static String field(AggregateFunction af) { Expression arg = af.field(); if (arg.foldable()) { return String.valueOf(((Literal) arg).value()); } if (arg instanceof FieldAttribute) { FieldAttribute field = (FieldAttribute) arg; // COUNT(DISTINCT) uses cardinality aggregation which works on exact values (not changed by analyzers or normalizers) if (af instanceof Count && ((Count) af).distinct()) { // use the `keyword` version of the field, if there is one return field.exactAttribute().name(); } return field.name(); } throw new SqlIllegalArgumentException("Does not know how to convert argument {} for function {}", arg.nodeString(), af.nodeString()); }	string.valueof(arg.fold()) there's rarely a reason to cast to a literal
private static Object topAggsFieldOrScript(AggregateFunction af, Expression e) { if (e == null) { return null; } if (e instanceof FieldAttribute) { return ((FieldAttribute) e).exactAttribute().name(); } if (e instanceof ScalarFunction) { return ((ScalarFunction) e).asScript(); } throw new SqlIllegalArgumentException("Does not know how to convert argument {} for function {}", e.nodeString(), af.nodeString()); }	this can be moved under the tophit agg class
private static Object topAggsFieldOrScript(AggregateFunction af, Expression e) { if (e == null) { return null; } if (e instanceof FieldAttribute) { return ((FieldAttribute) e).exactAttribute().name(); } if (e instanceof ScalarFunction) { return ((ScalarFunction) e).asScript(); } throw new SqlIllegalArgumentException("Does not know how to convert argument {} for function {}", e.nodeString(), af.nodeString()); }	get prefix is for getter - use something like tofield.. or asfield..
private static Object topAggsFieldOrScript(AggregateFunction af, Expression e) { if (e == null) { return null; } if (e instanceof FieldAttribute) { return ((FieldAttribute) e).exactAttribute().name(); } if (e instanceof ScalarFunction) { return ((ScalarFunction) e).asScript(); } throw new SqlIllegalArgumentException("Does not know how to convert argument {} for function {}", e.nodeString(), af.nodeString()); }	return isfieldorliteral(af) ? field(af) : ((scalarfunction) af.field()).asscript()
@Test public void mask0() { SearchResponse response = client().prepareSearch("idx") .addAggregation(ipRange("range") .field("ip") .addMaskRange("0.0.0.0/0")) .execute().actionGet(); assertSearchResponse(response); Range range = response.getAggregations().get("range"); assertThat(range, notNullValue()); assertThat(range.getName(), equalTo("range")); List<? extends Bucket> buckets = range.getBuckets(); assertThat(range.getBuckets().size(), equalTo(1)); Range.Bucket bucket = buckets.get(0); assertThat((String) bucket.getKey(), equalTo("0.0.0.0/0")); assertThat(bucket.getFromAsString(), nullValue()); assertThat(bucket.getToAsString(), nullValue()); assertThat(((Number) bucket.getTo()).doubleValue(), equalTo(Double.POSITIVE_INFINITY)); assertEquals(255l, bucket.getDocCount()); }	@jpountz this creates and index and adds documents to it. should the first part be moved to the setupsuitescopecluster(*)?
protected void doExecute(Task task, Request request, ActionListener<AcknowledgedResponse> listener) { if (MachineLearningField.ML_API_FEATURE.check(licenseState) == false) { listener.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)); return; } ActionListener<TrainedModelConfig> configActionListener = ActionListener.wrap(config -> { TrainedModelLocation location = config.getLocation(); if (location == null) { listener.onFailure( new ElasticsearchStatusException( "cannot put definition for model [{}] as location is null", RestStatus.BAD_REQUEST, request.getModelId() ) ); return; } if (config.getModelSize() > 0 && config.getModelSize() != request.getTotalDefinitionLength()) { listener.onFailure( ExceptionsHelper.badRequestException( "cannot put definition for model [{}] as [{}][{}] is not the same as the model's [{}][{}]", request.getModelId(), TrainedModelDefinitionDoc.TOTAL_DEFINITION_LENGTH, request.getTotalDefinitionLength(), TrainedModelConfig.MODEL_SIZE_BYTES, config.getModelSize() ) ); return; } final String indexName = ((IndexLocation) location).getIndexName(); trainedModelProvider.storeTrainedModelDefinitionDoc( new TrainedModelDefinitionDoc.Builder().setModelId(request.getModelId()) .setDocNum(request.getPart()) .setEos(request.isEos()) // XContentParser::binaryValue pulls out the raw, base64 decoded bytes automatically. So, we only need the length here .setDefinitionLength(request.getDefinition().length()) .setTotalDefinitionLength(request.getTotalDefinitionLength()) .setCompressionVersion(TrainedModelConfig.CURRENT_DEFINITION_COMPRESSION_VERSION) .setBinaryData(request.getDefinition()) .build(), indexName, ActionListener.wrap(stored -> handleDocStoredResponse(request, config, listener), listener::onFailure) ); }, listener::onFailure); trainedModelProvider.getTrainedModel(request.getModelId(), GetTrainedModelsAction.Includes.empty(), configActionListener); }	this could throw a version conflict no?
private LocalCheckpointTracker createLocalCheckpointTracker( BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier) { try { final SequenceNumbers.CommitInfo seqNoStats = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(store.readLastCommittedSegmentsInfo().userData.entrySet()); final long maxSeqNo = seqNoStats.maxSeqNo; final long localCheckpoint = seqNoStats.localCheckpoint; logger.trace("recovered maximum sequence number [{}] and local checkpoint [{}]", maxSeqNo, localCheckpoint); final LocalCheckpointTracker tracker = localCheckpointTrackerSupplier.apply(maxSeqNo, localCheckpoint); // scans existing sequence numbers in Lucene commit, then marks them as completed in the tracker. if (localCheckpoint < maxSeqNo && softDeleteEnabled) { try (Searcher engineSearcher = acquireSearcher("build_local_checkpoint_tracker", SearcherScope.INTERNAL)) { final DirectoryReader reader = Lucene.wrapAllDocsLive(engineSearcher.getDirectoryReader()); final IndexSearcher searcher = new IndexSearcher(reader); searcher.setQueryCache(null); final Query query = LongPoint.newRangeQuery(SeqNoFieldMapper.NAME, localCheckpoint + 1, maxSeqNo); for (LeafReaderContext leaf : reader.leaves()) { final Scorer scorer = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1.0f).scorer(leaf); if (scorer == null) { continue; } final DocIdSetIterator docIdSetIterator = scorer.iterator(); final NumericDocValues seqNoDocValues = leaf.reader().getNumericDocValues(SeqNoFieldMapper.NAME); int docId; while ((docId = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) { if (seqNoDocValues == null || seqNoDocValues.advanceExact(docId) == false) { throw new IllegalStateException("invalid seq_no doc_values for doc_id=" + docId); } final long seqNo = seqNoDocValues.longValue(); assert localCheckpoint < seqNo && seqNo <= maxSeqNo : "local_checkpoint=" + localCheckpoint + " seq_no=" + seqNo + " max_seq_no=" + maxSeqNo; tracker.markSeqNoAsCompleted(seqNo); } } } } return tracker; } catch (IOException ex) { throw new EngineCreationFailureException(shardId, "failed to create local checkpoint tracker", ex); } }	@bleskes let me know if you still prefer using the "snapshot" api.
private LocalCheckpointTracker createLocalCheckpointTracker( BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier) { try { final SequenceNumbers.CommitInfo seqNoStats = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(store.readLastCommittedSegmentsInfo().userData.entrySet()); final long maxSeqNo = seqNoStats.maxSeqNo; final long localCheckpoint = seqNoStats.localCheckpoint; logger.trace("recovered maximum sequence number [{}] and local checkpoint [{}]", maxSeqNo, localCheckpoint); final LocalCheckpointTracker tracker = localCheckpointTrackerSupplier.apply(maxSeqNo, localCheckpoint); // scans existing sequence numbers in Lucene commit, then marks them as completed in the tracker. if (localCheckpoint < maxSeqNo && softDeleteEnabled) { try (Searcher engineSearcher = acquireSearcher("build_local_checkpoint_tracker", SearcherScope.INTERNAL)) { final DirectoryReader reader = Lucene.wrapAllDocsLive(engineSearcher.getDirectoryReader()); final IndexSearcher searcher = new IndexSearcher(reader); searcher.setQueryCache(null); final Query query = LongPoint.newRangeQuery(SeqNoFieldMapper.NAME, localCheckpoint + 1, maxSeqNo); for (LeafReaderContext leaf : reader.leaves()) { final Scorer scorer = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1.0f).scorer(leaf); if (scorer == null) { continue; } final DocIdSetIterator docIdSetIterator = scorer.iterator(); final NumericDocValues seqNoDocValues = leaf.reader().getNumericDocValues(SeqNoFieldMapper.NAME); int docId; while ((docId = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) { if (seqNoDocValues == null || seqNoDocValues.advanceExact(docId) == false) { throw new IllegalStateException("invalid seq_no doc_values for doc_id=" + docId); } final long seqNo = seqNoDocValues.longValue(); assert localCheckpoint < seqNo && seqNo <= maxSeqNo : "local_checkpoint=" + localCheckpoint + " seq_no=" + seqNo + " max_seq_no=" + maxSeqNo; tracker.markSeqNoAsCompleted(seqNo); } } } } return tracker; } catch (IOException ex) { throw new EngineCreationFailureException(shardId, "failed to create local checkpoint tracker", ex); } }	this takes another approach than lucenechangessnapshot. @jimczi @s1monw can you please double check this?
public void testOutOfOrderOnFollower() throws Exception { try (ReplicationGroup leaderGroup = createGroup(0); ReplicationGroup followerGroup = createFollowGroup(0)) { leaderGroup.startAll(); followerGroup.startAll(); leaderGroup.indexDocs(3); // doc#1, doc#2, doc#3 IndexShard leadingPrimary = leaderGroup.getPrimary(); Translog.Operation[] operations = ShardChangesAction.getOperations(leadingPrimary, leadingPrimary.getGlobalCheckpoint(), 0, 3, leadingPrimary.getHistoryUUID(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES)); // replicate doc#1 and doc#3 and flush BulkShardOperationsRequest firstBulk = new BulkShardOperationsRequest(followerGroup.getPrimary().shardId(), followerGroup.getPrimary().getHistoryUUID(), Arrays.asList(operations[0], operations[2]), leadingPrimary.getMaxSeqNoOfUpdatesOrDeletes()); new CCRAction(firstBulk, new PlainActionFuture<>(), followerGroup).execute(); followerGroup.flush(); // replicate doc#2 BulkShardOperationsRequest secondBulk = new BulkShardOperationsRequest(followerGroup.getPrimary().shardId(), followerGroup.getPrimary().getHistoryUUID(), Arrays.asList(operations[1]), leadingPrimary.getMaxSeqNoOfUpdatesOrDeletes()); new CCRAction(secondBulk, new PlainActionFuture<>(), followerGroup).execute(); followerGroup.syncGlobalCheckpoint(); // add a new replica IndexShard newReplica = followerGroup.addReplica(); followerGroup.recoverReplica(newReplica); followerGroup.assertAllEqual(3); } }	it's not about out of order but rather over recovery of ops you already have. i checked and i can't find any existing tests that cover recovery (which can cover this case). i think we should have some (i.e., rename this test and extend it to cover general peer recovery (with and without holes). we also need concurrent recovery + indexing.
@Override protected RestChannelConsumer prepareRequest(final RestRequest restRequest, final NodeClient client) throws IOException { final ExecuteEnrichPolicyAction.Request request = new ExecuteEnrichPolicyAction.Request(restRequest.param("name")); request.setWaitForCompletion(restRequest.paramAsBoolean("wait_for_completion", true)); return channel -> client.execute(ExecuteEnrichPolicyAction.INSTANCE, request, new RestToXContentListener<>(channel)); }	i think we should add wait_for_completion parameter to the enrich.execute_policy.json rest spec (under params).
public void testAsyncTaskExecute() throws Exception { String policyName = "async-policy"; String sourceIndexName = "async-policy-source"; { IndexRequest indexRequest = new IndexRequest(sourceIndexName); indexRequest.source("key", "key", "value", "val1"); client().index(indexRequest).actionGet(); client().admin().indices().refresh(new RefreshRequest(sourceIndexName)).actionGet(); } EnrichPolicy enrichPolicy = new EnrichPolicy(EnrichPolicy.MATCH_TYPE, null, List.of(sourceIndexName), "key", List.of("value")); PutEnrichPolicyAction.Request request = new PutEnrichPolicyAction.Request(policyName, enrichPolicy); client().execute(PutEnrichPolicyAction.INSTANCE, request).actionGet(); ExecuteEnrichPolicyAction.Response executeResponse = client() .execute(ExecuteEnrichPolicyAction.INSTANCE, new ExecuteEnrichPolicyAction.Request(policyName).setWaitForCompletion(false)) .actionGet(); assertThat(executeResponse.getStatus(), is(nullValue())); assertThat(executeResponse.getTaskId(), is(not(nullValue()))); GetTaskRequest getPolicyTaskRequest = new GetTaskRequest().setTaskId(executeResponse.getTaskId()).setWaitForCompletion(true); GetTaskResponse taskResponse = client().execute(GetTaskAction.INSTANCE, getPolicyTaskRequest).actionGet(); assertThat(((ExecuteEnrichPolicyStatus) taskResponse.getTask().getTask().getStatus()).getPhase(), is(ExecuteEnrichPolicyStatus.PolicyPhases.COMPLETE)); String pipelineName = "test-pipeline"; String pipelineBody = "{\\\\"processors\\\\": [{\\\\"enrich\\\\": {\\\\"policy_name\\\\":\\\\"" + policyName + "\\\\", \\\\"field\\\\": \\\\"key\\\\", \\\\"target_field\\\\": \\\\"target\\\\"}}]}"; PutPipelineRequest putPipelineRequest = new PutPipelineRequest(pipelineName, new BytesArray(pipelineBody), XContentType.JSON); client().admin().cluster().putPipeline(putPipelineRequest).actionGet(); BulkRequest bulkRequest = new BulkRequest("my-index"); int numTestDocs = randomIntBetween(3, 10); for (int i = 0; i < numTestDocs; i++) { IndexRequest indexRequest = new IndexRequest("my-index"); indexRequest.id(Integer.toString(i)); indexRequest.setPipeline(pipelineName); indexRequest.source(Map.of("key", "key")); bulkRequest.add(indexRequest); } BulkResponse bulkResponse = client().bulk(bulkRequest).actionGet(); assertThat("Expected no failure, but " + bulkResponse.buildFailureMessage(), bulkResponse.hasFailures(), is(false)); for (int i = 0; i < numTestDocs; i++) { GetResponse getResponse = client().get(new GetRequest("my-index", Integer.toString(i))).actionGet(); Map<String, Object> source = getResponse.getSourceAsMap(); assertThat(source.size(), equalTo(2)); assertThat(source.get("target"), equalTo(List.of(Map.of("key", "key", "value", "val1")))); } }	i would place this inside an assertbusy(...), because there is no guarantee that the task is completed (especially on slow build machines).
public void invalidateAccessToken(String tokenString, ActionListener<Boolean> listener) { ensureEnabled(); if (Strings.isNullOrEmpty(tokenString)) { logger.trace("No token-string provided"); listener.onFailure(new IllegalArgumentException("token must be provided")); } else { maybeStartTokenRemover(); try { decodeToken(tokenString, ActionListener.wrap(userToken -> { if (userToken == null) { listener.onFailure(traceLog("invalidate token", tokenString, malformedTokenException())); } else { final long expirationEpochMilli = getExpirationTime().toEpochMilli(); indexBwcInvalidation(userToken, listener, new AtomicInteger(0), expirationEpochMilli); } }, listener::onFailure)); } catch (IOException e) { logger.error("received a malformed token as part of a invalidation request", e); listener.onFailure(malformedTokenException()); } } }	maybe warn ? this shouldn't be called with an empty token under normal circumstances ? same comment for invalidateaccesstoken and invalidaterefreshtoken below.
* @param listener the listener to notify upon completion * @param attemptCount the number of attempts to invalidate that have already been tried * @param expirationEpochMilli the expiration time as milliseconds since the epoch */ private void indexBwcInvalidation(UserToken userToken, ActionListener<Boolean> listener, AtomicInteger attemptCount, long expirationEpochMilli) { if (attemptCount.get() > 5) { logger.warn("Failed to invalidate token [{}] after [{}] attempts", userToken.getId(), attemptCount.get()); listener.onFailure(invalidGrantException("failed to invalidate token")); } else { final String invalidatedTokenId = getInvalidatedTokenDocumentId(userToken); IndexRequest indexRequest = client.prepareIndex(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE, invalidatedTokenId) .setOpType(OpType.CREATE) .setSource("doc_type", INVALIDATED_TOKEN_DOC_TYPE, "expiration_time", expirationEpochMilli) .setRefreshPolicy(RefreshPolicy.WAIT_UNTIL) .request(); final String tokenDocId = getTokenDocumentId(userToken); final Version version = userToken.getVersion(); securityIndex.prepareIndexIfNeededThenExecute(ex -> listener.onFailure(traceLog("prepare index", tokenDocId, ex)), () -> executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, indexRequest, ActionListener.<IndexResponse>wrap(indexResponse -> { ActionListener<Boolean> wrappedListener = ActionListener.wrap(ignore -> listener.onResponse(true), listener::onFailure); indexInvalidation(tokenDocId, version, wrappedListener, attemptCount, "access_token", 1L); }, e -> { Throwable cause = ExceptionsHelper.unwrapCause(e); traceLog("(bwc) invalidate token", tokenDocId, cause); if (cause instanceof VersionConflictEngineException) { // expected since something else could have invalidated ActionListener<Boolean> wrappedListener = ActionListener.wrap(ignore -> listener.onResponse(false), listener::onFailure); indexInvalidation(tokenDocId, version, wrappedListener, attemptCount, "access_token", 1L); } else if (isShardNotAvailableException(e)) { attemptCount.incrementAndGet(); indexBwcInvalidation(userToken, listener, attemptCount, expirationEpochMilli); } else { listener.onFailure(e); } }), client::index)); } }	maybe keep one of prepare index or prepare security for consistency ? or merge them to prepare security index ? i feel these identifiers offer little assistance to an es admin troubleshooting failures, but will be extremely helpful when we need to look into an issue. to be honest, i don't have any good ideas on how these could become more helpful for es admins without being extremely descriptive
* @param listener the listener to notify upon completion * @param attemptCount the number of attempts to invalidate that have already been tried * @param srcPrefix the prefix to use when constructing the doc to update * @param documentVersion the expected version of the document we will update */ private void indexInvalidation(String tokenDocId, Version version, ActionListener<Boolean> listener, AtomicInteger attemptCount, String srcPrefix, long documentVersion) { if (attemptCount.get() > 5) { logger.warn("Failed to invalidate token [{}] after [{}] attempts", tokenDocId, attemptCount.get()); listener.onFailure(invalidGrantException("failed to invalidate token")); } else { UpdateRequest request = client.prepareUpdate(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE, tokenDocId) .setDoc(srcPrefix, Collections.singletonMap("invalidated", true)) .setVersion(documentVersion) .setRefreshPolicy(RefreshPolicy.WAIT_UNTIL) .request(); securityIndex.prepareIndexIfNeededThenExecute(ex -> listener.onFailure(traceLog("prepare index", tokenDocId, ex)), () -> executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, request, ActionListener.<UpdateResponse>wrap(updateResponse -> { logger.debug("Invalidated [{}] for doc [{}]", srcPrefix, tokenDocId); if (updateResponse.getGetResult() != null && updateResponse.getGetResult().sourceAsMap().containsKey(srcPrefix) && ((Map<String, Object>) updateResponse.getGetResult().sourceAsMap().get(srcPrefix)) .containsKey("invalidated")) { final boolean prevInvalidated = (boolean) ((Map<String, Object>) updateResponse.getGetResult().sourceAsMap().get(srcPrefix)) .get("invalidated"); listener.onResponse(prevInvalidated == false); } else { listener.onResponse(true); } }, e -> { Throwable cause = ExceptionsHelper.unwrapCause(e); traceLog("invalidate token", tokenDocId, cause); if (cause instanceof DocumentMissingException) { if (version.onOrAfter(Version.V_6_2_0)) { // the document should always be there! listener.onFailure(e); } else { listener.onResponse(false); } } else if (cause instanceof VersionConflictEngineException || isShardNotAvailableException(cause)) { attemptCount.incrementAndGet(); executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, client.prepareGet(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE, tokenDocId).request(), ActionListener.<GetResponse>wrap(getResult -> { if (getResult.isExists()) { Map<String, Object> source = getResult.getSource(); Map<String, Object> accessTokenSource = (Map<String, Object>) source.get("access_token"); Consumer<Exception> onFailure = ex -> listener.onFailure(traceLog("get token", tokenDocId, ex)); if (accessTokenSource == null) { onFailure.accept(new IllegalArgumentException( "token document is missing access_token field")); } else { Boolean invalidated = (Boolean) accessTokenSource.get("invalidated"); if (invalidated == null) { onFailure.accept(new IllegalStateException( "token document missing invalidated value")); } else if (invalidated) { logger.trace("Token [{}] is already invalidated", tokenDocId); listener.onResponse(false); } else { indexInvalidation(tokenDocId, version, listener, attemptCount, srcPrefix, getResult.getVersion()); } } } else if (version.onOrAfter(Version.V_6_2_0)) { logger.warn("could not find token document [{}] but there should " + "be one as token has version [{}]", tokenDocId, version); listener.onFailure(invalidGrantException("could not invalidate the token")); } else { listener.onResponse(false); } }, e1 -> { traceLog("get token", tokenDocId, e1); if (isShardNotAvailableException(e1)) { // don't increment count; call again indexInvalidation(tokenDocId, version, listener, attemptCount, srcPrefix, documentVersion); } else { listener.onFailure(e1); } }), client::get); } else { listener.onFailure(e); } }), client::update)); } }	set a max_attempt_count (since it's used in 4 places already) to 5 and also use that then in the log message instead of attemptcount.get() ?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); searchRequest.writeTo(out); out.writeBoolean(abortOnVersionConflict); out.writeVInt(size); out.writeBoolean(refresh); timeout.writeTo(out); activeShardCount.writeTo(out); retryBackoffInitialTime.writeTo(out); out.writeVInt(maxRetries); out.writeFloat(requestsPerSecond); if (out.getVersion().onOrAfter(BulkByScrollTask.V_5_1_0_UNRELEASED)) { out.writeVInt(slices); } }	now that i think about this i think a better alternative is to switch this to illegalargumentexception and to catch that in assertversionserializable. then we would: 1. fail to process requests with slices if sending them to a node that doesn't support them. 2. return a 400 level error if this happens to happen over rest (unlikely, i think. maybe impossible) 3. have a way for other requests to solve this problem in ways that are less surprising than attempting to degenerate into another request.
CompletableFuture<Integer> populateAndRead( final Tuple<Long, Long> rangeToWrite, final Tuple<Long, Long> rangeToRead, final RangeAvailableHandler reader, final RangeMissingHandler writer, final Executor executor ) { ensureOpen(); final CompletableFuture<Integer> future = new CompletableFuture<>(); try { final FileChannelReference reference = getFileChannelReference(future); final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(rangeToWrite, rangeToRead, ActionListener.wrap(success -> { final int read = reader.onRangeAvailable(reference.fileChannel); assert read == rangeToRead.v2() - rangeToRead.v1() : "partial read [" + read + "] does not match the range to read [" + rangeToRead.v2() + '-' + rangeToRead.v1() + ']'; future.complete(read); }, future::completeExceptionally)); if (gaps.isEmpty() == false) { executor.execute(new AbstractRunnable() { @Override protected void doRun() { for (SparseFileTracker.Gap gap : gaps) { ensureOpen(); try { if (reference.tryIncRef() == false) { throw new AlreadyClosedException("Cache file channel has been released and closed"); } try { ensureOpen(); writer.fillCacheRange(reference.fileChannel, gap.start(), gap.end(), gap::onProgress); } finally { reference.decRef(); } gap.onCompletion(); } catch (Exception e) { gap.onFailure(e); } } } @Override public void onFailure(Exception e) { gaps.forEach(gap -> gap.onFailure(e)); } }); } } catch (Exception e) { future.completeExceptionally(e); } return future; } /** * Notifies the {@link RangeAvailableHandler} when {@code rangeToRead} is available to read from the file. If {@code rangeToRead} is * already available then the {@link RangeAvailableHandler} is called synchronously by this method; if not, but it is pending, then the * {@link RangeAvailableHandler} is notified when the pending ranges have completed. If it contains gaps that are not currently pending * then no listeners are registered and this method returns {@code null}. * * @return a future which returns the result of the {@link RangeAvailableHandler} once it has completed, or {@code null}	i'm not sure why this ensureopen() has been added here? it could be added before the for loop so that we fail all gaps directly, otherwise it should be included within the next try/catch block but it's already present there.
@Override protected Set<String> responseParams() { return textFormat == TextFormat.CSV ? Collections.singleton("delimiter") : Collections.emptySet(); }	can we use some static variable for "delimiter"?
public void testCsvFormatWithCustomDelimiterRegularData() { List<Character> forbidden = Arrays.asList('"', '\\\\r', '\\\\n', '\\\\t'); Character delim = randomValueOtherThanMany(forbidden::contains, () -> randomAlphaOfLength(1).charAt(0)); String text = CSV.format(reqWithParam("delimiter", String.valueOf(delim)), regularData()); List<String> terms = Arrays.asList("string", "number", "Along The River Bank", "708", "Mind Train", "280"); List<String> expectedTerms = terms.stream() .map(x -> x.contains(String.valueOf(delim)) ? '"' + x + '"' : x) .collect(Collectors.toList()); StringBuffer sb = new StringBuffer(); do { sb.append(expectedTerms.remove(0)); sb.append(delim); sb.append(expectedTerms.remove(0)); sb.append("\\\\r\\\\n"); } while (expectedTerms.size() > 0); assertEquals(sb.toString(), text); }	minor, a set would be faster.
void checkWarningHeaders(final List<String> warningHeaders, final Version masterVersion) { final List<String> unexpected = new ArrayList<>(); final List<String> unmatched = new ArrayList<>(); final List<String> missing = new ArrayList<>(); // LinkedHashSet so that missing expected warnings come back in a predictable order which is nice for testing final Set<String> expected = new LinkedHashSet<>(expectedWarningHeaders.stream().map(DeprecationLogger::escapeAndEncode).collect(Collectors.toList())); final Set<String> seen = new HashSet<>(); for (final String header : warningHeaders) { final Matcher matcher = WARNING_HEADER_PATTERN.matcher(header); final boolean matches = matcher.matches(); if (matches) { final String message = matcher.group(1); // noinspection StatementWithEmptyBody if (masterVersion.before(Version.V_7_0_0) && message.equals("the default number of shards will change from [5] to [1] in 7.0.0; " + "if you wish to continue using the default of [5] shards, " + "you must manage this on the create index request or with an index template")) { /* * This warning header will come back in the vast majority of our tests that create an index when running against an * older master. Rather than rewrite our tests to assert this warning header, we assume that it is expected. */ } else // noinspection StatementWithEmptyBody if (message.startsWith("[types removal]") || message.startsWith("[_data_frame/transforms/] is deprecated")) { /* * We skip warnings related to types deprecation and transform rename so that we can continue to run the many * mixed-version tests that used typed APIs. */ } else if (expected.contains(message) == false) { unexpected.add(header); } else { seen.add(message); } } else { unmatched.add(header); } } if (seen.size() != expected.size()) { for (final String header : expected) { if (seen.contains(header) == false) { missing.add(header); } } } if (unexpected.isEmpty() == false || unmatched.isEmpty() == false || missing.isEmpty() == false) { final StringBuilder failureMessage = new StringBuilder(); appendBadHeaders(failureMessage, unexpected, "got unexpected warning header" + (unexpected.size() > 1 ? "s" : "")); appendBadHeaders(failureMessage, unmatched, "got unmatched warning header" + (unmatched.size() > 1 ? "s" : "")); appendBadHeaders(failureMessage, missing, "did not get expected warning header" + (missing.size() > 1 ? "s" : "")); fail(failureMessage.toString()); } }	@nik9000 since you wrote the original logic, thought i would ping you. this allows for the same warning to be mentioned multiple times in the headers. this is required for ml since there are times when a warning appears more than once. the number of times a warning appears is non-deterministic in multi-node tests. this is because for ml we occasionally need to route to specific nodes (e.g. to a master node) and warnings can appear on parsing xcontent and on serializing between nodes.
@Override protected void initChannel(Channel ch) throws Exception { SSLEngine serverEngine = sslService.createSSLEngine(configuration, null, -1); serverEngine.setUseClientMode(false); final SslHandler sslHandler = new SslHandler(serverEngine); ch.pipeline().addFirst("sslhandler", sslHandler); super.initChannel(ch); } } protected ServerChannelInitializer getSslChannelInitializer(final String name, final SSLConfiguration configuration) { return new SslChannelInitializer(name, sslConfiguration); } private class SecurityClientChannelInitializer extends ClientChannelInitializer { private final boolean hostnameVerificationEnabled; private final SNIHostName serverName; SecurityClientChannelInitializer(DiscoveryNode node) { this.hostnameVerificationEnabled = sslEnabled && sslConfiguration.verificationMode().isHostnameVerificationEnabled(); String configuredServerName = node.getAttributes().get("server_name"); if (configuredServerName != null) { try { serverName = new SNIHostName(configuredServerName); } catch (IllegalArgumentException e) { throw new ConnectTransportException(node, "invalid DiscoveryNode server_name [" + configuredServerName + "]", e); } } else { serverName = null; } } @Override protected void initChannel(Channel ch) throws Exception { super.initChannel(ch); if (sslEnabled) { ch.pipeline().addFirst(new ClientSslHandlerInitializer(sslConfiguration, sslService, hostnameVerificationEnabled, serverName)); } } } private static class ClientSslHandlerInitializer extends ChannelOutboundHandlerAdapter { private final boolean hostnameVerificationEnabled; private final SSLConfiguration sslConfiguration; private final SSLService sslService; private final SNIServerName serverName; private ClientSslHandlerInitializer(SSLConfiguration sslConfiguration, SSLService sslService, boolean hostnameVerificationEnabled, SNIServerName serverName) { this.sslConfiguration = sslConfiguration; this.hostnameVerificationEnabled = hostnameVerificationEnabled; this.sslService = sslService; this.serverName = serverName; } @Override public void connect(ChannelHandlerContext ctx, SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) throws Exception { final SSLEngine sslEngine; if (hostnameVerificationEnabled) { InetSocketAddress inetSocketAddress = (InetSocketAddress) remoteAddress; // we create the socket based on the name given. don't reverse DNS sslEngine = sslService.createSSLEngine(sslConfiguration, inetSocketAddress.getHostString(), inetSocketAddress.getPort()); } else { sslEngine = sslService.createSSLEngine(sslConfiguration, null, -1); } sslEngine.setUseClientMode(true); if (serverName != null) { SSLParameters sslParameters = sslEngine.getSSLParameters(); sslParameters.setServerNames(Collections.singletonList(serverName)); sslEngine.setSSLParameters(sslParameters); } ctx.pipeline().replace(this, "ssl", new SslHandler(sslEngine)); super.connect(ctx, remoteAddress, localAddress, promise); }	can you add an assertion post super call that the ssl handler is still first? that way a future change does not accidentally mess up necessary ordering on handlers.
private void executeShardOperation(final ShardRequest request, final IndexShard indexShard) { final ShardId shardId = indexShard.shardId(); if (indexShard.getActiveOperationsCount() != 0) { throw new IllegalStateException("On-going operations in progress while checking index shard " + shardId + " before closing"); } final ClusterBlocks clusterBlocks = clusterService.state().blocks(); if (clusterBlocks.hasIndexBlock(shardId.getIndexName(), request.clusterBlock()) == false) { throw new IllegalStateException("Index shard " + shardId + " must be blocked by " + request.clusterBlock() + " before closing"); } indexShard.checkIndexBeforeClose(); indexShard.flush(new FlushRequest().force(true)); logger.trace("{} shard is ready for closing", shardId); }	perhaps call this method verifyshardbeforeindexclosing.
@Override public void checkGlobalCheckpointBeforeClose(final long globalCheckpoint) { final long maxSeqNo = getSeqNoStats(globalCheckpoint).getMaxSeqNo(); if (globalCheckpoint != maxSeqNo) { throw new IllegalStateException("Global checkpoint [" + globalCheckpoint + "] mismatches maximum sequence number [" + maxSeqNo + "] on index shard " + shardId); } }	i would move this implementation to the engine class, and have specific engines explicitly opt-out of this behavior by overriding with empty method
@Override public void checkGlobalCheckpointBeforeClose(final long globalCheckpoint) { final long maxSeqNo = getSeqNoStats(globalCheckpoint).getMaxSeqNo(); if (globalCheckpoint != maxSeqNo) { throw new IllegalStateException("Global checkpoint [" + globalCheckpoint + "] mismatches maximum sequence number [" + maxSeqNo + "] on index shard " + shardId); } }	internalengine has access to the global checkpoint through engineconfig.getglobalcheckpointsupplier().getaslong(), so no need to pass in the global checkpoint explicitly. we can then just call this verifyenginebeforeindexclosing
@After public void cleanUpTransformsAndLogAudits() throws Exception { for (String transformId : transformsToClean) { highLevelClient().transform() .stopTransform(new StopTransformRequest(transformId, Boolean.TRUE, null, false), RequestOptions.DEFAULT); } for (String transformId : transformsToClean) { highLevelClient().transform().deleteTransform(new DeleteTransformRequest(transformId), RequestOptions.DEFAULT); } transformsToClean = new ArrayList<>(); waitForPendingTasks(adminClient()); // using '*' to make this lenient and do not fail if the audit index does not exist SearchRequest searchRequest = new SearchRequest(".transform-notifications-*"); searchRequest.source(new SearchSourceBuilder().query(new MatchAllQueryBuilder()).size(100).sort("timestamp", SortOrder.ASC)); for (SearchHit hit : searchAll(searchRequest)) { Map<String, Object> source = hit.getSourceAsMap(); String level = (String) source.getOrDefault("level", "info"); logger.log( Level.getLevel(level.toUpperCase(Locale.ROOT)), "Transform audit: [{}] [{}] [{}] [{}]", Instant.ofEpochMilli((long) source.getOrDefault("timestamp", 0)), source.getOrDefault("transform_id", "n/a"), source.getOrDefault("message", "n/a"), source.getOrDefault("node_name", "n/a") ); } }	the same can be achieved with new searchrequest.indicesoptions(indicesoptions.lenient_expand_open)
public ExtractedFields detect() { Set<String> fields = new HashSet<>(fieldCapabilitiesResponse.get().keySet()); fields.removeAll(IGNORE_FIELDS); removeFieldsUnderResultsField(fields); includeAndExcludeFields(fields); removeFieldsWithIncompatibleTypes(fields); checkRequiredFieldsArePresent(fields); if (fields.isEmpty()) { Set<String> supportedTypes = new HashSet<>(NUMERICAL_TYPES); if (config.getAnalysis().supportsCategoricalFields()) { supportedTypes.addAll(CATEGORICAL_TYPES); } throw ExceptionsHelper.badRequestException("No compatible fields could be detected in index {}. Supported types are {}.", Arrays.toString(index), supportedTypes); } List<String> sortedFields = new ArrayList<>(fields); // We sort the fields to ensure the checksum for each document is deterministic Collections.sort(sortedFields); ExtractedFields extractedFields = ExtractedFields.build(sortedFields, Collections.emptySet(), fieldCapabilitiesResponse); if (extractedFields.getDocValueFields().size() > docValueFieldsLimit) { extractedFields = fetchFromSourceIfSupported(extractedFields); if (extractedFields.getDocValueFields().size() > docValueFieldsLimit) { throw ExceptionsHelper.badRequestException("[{}] fields must be retrieved from doc_values but the limit is [{}]; " + "please adjust the index level setting [{}]", extractedFields.getDocValueFields().size(), docValueFieldsLimit, IndexSettings.MAX_DOCVALUE_FIELDS_SEARCH_SETTING.getKey()); } } return extractedFields; }	nice! we should also do the same for the last error message in removefieldswithincompatibletypes()
public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting shardRouting = null; RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard : unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { shardRouting = shard; break; } } if (shardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "primary [" + index + "][" + shardId + "] is already assigned"); } if (shardRouting.recoverySource().getType() != RecoverySource.Type.EMPTY_STORE && acceptDataLoss == false) { String dataLossWarning = "allocating an empty primary for [" + index + "][" + shardId + "] can result in data loss. Please confirm by setting the accept_data_loss parameter to true"; return explainOrThrowRejectedCommand(explain, allocation, dataLossWarning); } UnassignedInfo unassignedInfoToUpdate = null; if (shardRouting.unassignedInfo().getReason() != UnassignedInfo.Reason.FORCED_EMPTY_PRIMARY) { String unassignedInfoMessage = "force empty allocation from previous reason " + shardRouting.unassignedInfo().getReason() + ", " + shardRouting.unassignedInfo().getMessage(); unassignedInfoToUpdate = new UnassignedInfo(UnassignedInfo.Reason.FORCED_EMPTY_PRIMARY, unassignedInfoMessage, shardRouting.unassignedInfo().getFailure(), 0, System.nanoTime(), System.currentTimeMillis(), false, shardRouting.unassignedInfo().getLastAllocationStatus()); } initializeUnassignedShard(allocation, routingNodes, routingNode, shardRouting, unassignedInfoToUpdate, EmptyStoreRecoverySource.INSTANCE); return new RerouteExplanation(this, allocation.decision(Decision.YES, name() + " (allocation command)", "ignore deciders")); }	nit: you can inline unassigned, it's only used in the following line.
public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting primaryShardRouting = null; for (RoutingNode node : allocation.routingNodes()) { for (ShardRouting shard : node) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { primaryShardRouting = shard; break; } } } if (primaryShardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate a replica shard [" + index + "][" + shardId + "], while corresponding primary shard is still unassigned"); } List<ShardRouting> replicaShardRoutings = new ArrayList<>(); RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard : unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary() == false) { replicaShardRoutings.add(shard); } } ShardRouting shardRouting; if (replicaShardRoutings.isEmpty()) { return explainOrThrowRejectedCommand(explain, allocation, "all copies of [" + index + "][" + shardId + "] are already assigned. Use the move allocation command instead"); } else { shardRouting = replicaShardRoutings.get(0); } Decision decision = allocation.deciders().canAllocate(shardRouting, routingNode, allocation); if (decision.type() == Decision.Type.NO) { // don't use explainOrThrowRejectedCommand to keep the original "NO" decision if (explain) { return new RerouteExplanation(this, decision); } throw new IllegalArgumentException("[" + name() + "] allocation of [" + index + "][" + shardId + "] on node " + discoNode + " is not allowed, reason: " + decision); } initializeUnassignedShard(allocation, routingNodes, routingNode, shardRouting); return new RerouteExplanation(this, decision); }	nit: you can inline unassigned, it's only used in the following line.
public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting shardRouting = null; RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard : unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { shardRouting = shard; break; } } if (shardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "primary [" + index + "][" + shardId + "] is already assigned"); } if (acceptDataLoss == false) { String dataLossWarning = "allocating an empty primary for [" + index + "][" + shardId + "] can result in data loss. Please " + "confirm by setting the accept_data_loss parameter to true"; return explainOrThrowRejectedCommand(explain, allocation, dataLossWarning); } if (shardRouting.recoverySource().getType() != RecoverySource.Type.EXISTING_STORE) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate an existing primary shard [" + index + "][" + shardId + "], while no such shard has ever been active"); } initializeUnassignedShard(allocation, routingNodes, routingNode, shardRouting, null, RecoverySource.ExistingStoreRecoverySource.FORCE_STALE_PRIMARY_INSTANCE); return new RerouteExplanation(this, allocation.decision(Decision.YES, name() + " (allocation command)", "ignore deciders")); }	nit: you can inline unassigned, it's only used in the following line.
public void testConflictingCommandsInSingleRequest() { AllocationService allocation = createAllocationService(Settings.builder() .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none") .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none") .build()); final String index1 = "test1"; final String index2 = "test2"; final String index3 = "test3"; logger.info("--> building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder(index1).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index2).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index3).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsRecovery(metaData.index(index1)) .addAsRecovery(metaData.index(index2)) .addAsRecovery(metaData.index(index3)) .build(); ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)) .metaData(metaData).routingTable(routingTable).build(); final String node1 = "node1"; final String node2 = "node2"; clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .add(newNode(node1)) .add(newNode(node2)) ).build(); clusterState = allocation.reroute(clusterState, "reroute"); logger.info("--> allocating same index primary in multiple commands should fail"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateStalePrimaryAllocationCommand(index1, 0, node1, true), new AllocateStalePrimaryAllocationCommand(index1, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index1 + "][0] is already assigned")); } try { allocation.reroute(clusterState, new AllocationCommands( new AllocateEmptyPrimaryAllocationCommand(index2, 0, node1, true), new AllocateEmptyPrimaryAllocationCommand(index2, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index2 + "][0] is already assigned")); } clusterState = allocation.reroute(clusterState, new AllocationCommands(new AllocateEmptyPrimaryAllocationCommand(index3, 0, node1, true)), false, false).getClusterState(); clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)); assertThat(clusterState.getRoutingNodes().node(node1).shardsWithState(STARTED).size(), equalTo(1)); logger.info("--> subsequent replica allocation fails as all configured replicas have been allocated"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateReplicaAllocationCommand(index3, 0, node2), new AllocateReplicaAllocationCommand(index3, 0, node2)), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("all copies of [" + index3 + "][0] are already assigned. Use the move allocation command instead")); } }	i think there's no need to call getclusterstate() here.
public void testConflictingCommandsInSingleRequest() { AllocationService allocation = createAllocationService(Settings.builder() .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none") .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none") .build()); final String index1 = "test1"; final String index2 = "test2"; final String index3 = "test3"; logger.info("--> building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder(index1).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index2).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index3).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsRecovery(metaData.index(index1)) .addAsRecovery(metaData.index(index2)) .addAsRecovery(metaData.index(index3)) .build(); ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)) .metaData(metaData).routingTable(routingTable).build(); final String node1 = "node1"; final String node2 = "node2"; clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .add(newNode(node1)) .add(newNode(node2)) ).build(); clusterState = allocation.reroute(clusterState, "reroute"); logger.info("--> allocating same index primary in multiple commands should fail"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateStalePrimaryAllocationCommand(index1, 0, node1, true), new AllocateStalePrimaryAllocationCommand(index1, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index1 + "][0] is already assigned")); } try { allocation.reroute(clusterState, new AllocationCommands( new AllocateEmptyPrimaryAllocationCommand(index2, 0, node1, true), new AllocateEmptyPrimaryAllocationCommand(index2, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index2 + "][0] is already assigned")); } clusterState = allocation.reroute(clusterState, new AllocationCommands(new AllocateEmptyPrimaryAllocationCommand(index3, 0, node1, true)), false, false).getClusterState(); clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)); assertThat(clusterState.getRoutingNodes().node(node1).shardsWithState(STARTED).size(), equalTo(1)); logger.info("--> subsequent replica allocation fails as all configured replicas have been allocated"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateReplicaAllocationCommand(index3, 0, node2), new AllocateReplicaAllocationCommand(index3, 0, node2)), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("all copies of [" + index3 + "][0] are already assigned. Use the move allocation command instead")); } }	this doesn't assert that the exception is actually thrown: the test still passes if the reroute succeeds. suggest using expectthrows() like this: { final clusterstate finalclusterstate = clusterstate; assertthat(expectthrows(illegalargumentexception.class, () -> { allocation.reroute(finalclusterstate, new allocationcommands( new allocatestaleprimaryallocationcommand(index1, 0, node1, true), new allocatestaleprimaryallocationcommand(index1, 0, node2, true) ), false, false); }).getmessage(), containsstring("primary [" + index1 + "][0] is already assigned")); }
public void testConflictingCommandsInSingleRequest() { AllocationService allocation = createAllocationService(Settings.builder() .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none") .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none") .build()); final String index1 = "test1"; final String index2 = "test2"; final String index3 = "test3"; logger.info("--> building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder(index1).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index2).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index3).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsRecovery(metaData.index(index1)) .addAsRecovery(metaData.index(index2)) .addAsRecovery(metaData.index(index3)) .build(); ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)) .metaData(metaData).routingTable(routingTable).build(); final String node1 = "node1"; final String node2 = "node2"; clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .add(newNode(node1)) .add(newNode(node2)) ).build(); clusterState = allocation.reroute(clusterState, "reroute"); logger.info("--> allocating same index primary in multiple commands should fail"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateStalePrimaryAllocationCommand(index1, 0, node1, true), new AllocateStalePrimaryAllocationCommand(index1, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index1 + "][0] is already assigned")); } try { allocation.reroute(clusterState, new AllocationCommands( new AllocateEmptyPrimaryAllocationCommand(index2, 0, node1, true), new AllocateEmptyPrimaryAllocationCommand(index2, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index2 + "][0] is already assigned")); } clusterState = allocation.reroute(clusterState, new AllocationCommands(new AllocateEmptyPrimaryAllocationCommand(index3, 0, node1, true)), false, false).getClusterState(); clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)); assertThat(clusterState.getRoutingNodes().node(node1).shardsWithState(STARTED).size(), equalTo(1)); logger.info("--> subsequent replica allocation fails as all configured replicas have been allocated"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateReplicaAllocationCommand(index3, 0, node2), new AllocateReplicaAllocationCommand(index3, 0, node2)), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("all copies of [" + index3 + "][0] are already assigned. Use the move allocation command instead")); } }	as above, no need for .getclusterstate().
public void testConflictingCommandsInSingleRequest() { AllocationService allocation = createAllocationService(Settings.builder() .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none") .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none") .build()); final String index1 = "test1"; final String index2 = "test2"; final String index3 = "test3"; logger.info("--> building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder(index1).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index2).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index3).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsRecovery(metaData.index(index1)) .addAsRecovery(metaData.index(index2)) .addAsRecovery(metaData.index(index3)) .build(); ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)) .metaData(metaData).routingTable(routingTable).build(); final String node1 = "node1"; final String node2 = "node2"; clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .add(newNode(node1)) .add(newNode(node2)) ).build(); clusterState = allocation.reroute(clusterState, "reroute"); logger.info("--> allocating same index primary in multiple commands should fail"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateStalePrimaryAllocationCommand(index1, 0, node1, true), new AllocateStalePrimaryAllocationCommand(index1, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index1 + "][0] is already assigned")); } try { allocation.reroute(clusterState, new AllocationCommands( new AllocateEmptyPrimaryAllocationCommand(index2, 0, node1, true), new AllocateEmptyPrimaryAllocationCommand(index2, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index2 + "][0] is already assigned")); } clusterState = allocation.reroute(clusterState, new AllocationCommands(new AllocateEmptyPrimaryAllocationCommand(index3, 0, node1, true)), false, false).getClusterState(); clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)); assertThat(clusterState.getRoutingNodes().node(node1).shardsWithState(STARTED).size(), equalTo(1)); logger.info("--> subsequent replica allocation fails as all configured replicas have been allocated"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateReplicaAllocationCommand(index3, 0, node2), new AllocateReplicaAllocationCommand(index3, 0, node2)), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("all copies of [" + index3 + "][0] are already assigned. Use the move allocation command instead")); } }	this should also use expectthrows().
public void testConflictingCommandsInSingleRequest() { AllocationService allocation = createAllocationService(Settings.builder() .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none") .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none") .build()); final String index1 = "test1"; final String index2 = "test2"; final String index3 = "test3"; logger.info("--> building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder(index1).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index2).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index3).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsRecovery(metaData.index(index1)) .addAsRecovery(metaData.index(index2)) .addAsRecovery(metaData.index(index3)) .build(); ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)) .metaData(metaData).routingTable(routingTable).build(); final String node1 = "node1"; final String node2 = "node2"; clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .add(newNode(node1)) .add(newNode(node2)) ).build(); clusterState = allocation.reroute(clusterState, "reroute"); logger.info("--> allocating same index primary in multiple commands should fail"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateStalePrimaryAllocationCommand(index1, 0, node1, true), new AllocateStalePrimaryAllocationCommand(index1, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index1 + "][0] is already assigned")); } try { allocation.reroute(clusterState, new AllocationCommands( new AllocateEmptyPrimaryAllocationCommand(index2, 0, node1, true), new AllocateEmptyPrimaryAllocationCommand(index2, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index2 + "][0] is already assigned")); } clusterState = allocation.reroute(clusterState, new AllocationCommands(new AllocateEmptyPrimaryAllocationCommand(index3, 0, node1, true)), false, false).getClusterState(); clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)); assertThat(clusterState.getRoutingNodes().node(node1).shardsWithState(STARTED).size(), equalTo(1)); logger.info("--> subsequent replica allocation fails as all configured replicas have been allocated"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateReplicaAllocationCommand(index3, 0, node2), new AllocateReplicaAllocationCommand(index3, 0, node2)), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("all copies of [" + index3 + "][0] are already assigned. Use the move allocation command instead")); } }	as above, no need for .getclusterstate().
public void testConflictingCommandsInSingleRequest() { AllocationService allocation = createAllocationService(Settings.builder() .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none") .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none") .build()); final String index1 = "test1"; final String index2 = "test2"; final String index3 = "test3"; logger.info("--> building initial routing table"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder(index1).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index2).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .put(IndexMetaData.builder(index3).settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1) .putInSyncAllocationIds(0, Collections.singleton("randomAllocID")) .putInSyncAllocationIds(1, Collections.singleton("randomAllocID2"))) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsRecovery(metaData.index(index1)) .addAsRecovery(metaData.index(index2)) .addAsRecovery(metaData.index(index3)) .build(); ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)) .metaData(metaData).routingTable(routingTable).build(); final String node1 = "node1"; final String node2 = "node2"; clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .add(newNode(node1)) .add(newNode(node2)) ).build(); clusterState = allocation.reroute(clusterState, "reroute"); logger.info("--> allocating same index primary in multiple commands should fail"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateStalePrimaryAllocationCommand(index1, 0, node1, true), new AllocateStalePrimaryAllocationCommand(index1, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index1 + "][0] is already assigned")); } try { allocation.reroute(clusterState, new AllocationCommands( new AllocateEmptyPrimaryAllocationCommand(index2, 0, node1, true), new AllocateEmptyPrimaryAllocationCommand(index2, 0, node2, true) ), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("primary [" + index2 + "][0] is already assigned")); } clusterState = allocation.reroute(clusterState, new AllocationCommands(new AllocateEmptyPrimaryAllocationCommand(index3, 0, node1, true)), false, false).getClusterState(); clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)); assertThat(clusterState.getRoutingNodes().node(node1).shardsWithState(STARTED).size(), equalTo(1)); logger.info("--> subsequent replica allocation fails as all configured replicas have been allocated"); try { allocation.reroute(clusterState, new AllocationCommands( new AllocateReplicaAllocationCommand(index3, 0, node2), new AllocateReplicaAllocationCommand(index3, 0, node2)), false, false).getClusterState(); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("all copies of [" + index3 + "][0] are already assigned. Use the move allocation command instead")); } }	this should also use expectthrows().
public void testAnonymousUser() { final String hashingAlgorithm = inFipsJvm() ? randomFrom("pbkdf2", "pbkdf2_1000") : randomFrom("pbkdf2", "pbkdf2_1000", "bcrypt", "bcrypt9"); Settings settings = Settings.builder().put(AnonymousUser.ROLES_SETTING.getKey(), "superuser") .put(XPackSettings.PASSWORD_HASHING_ALGORITHM.getKey(), hashingAlgorithm).build(); AnonymousUser anonymousUser = new AnonymousUser(settings); NativeUsersStore usersStore = mock(NativeUsersStore.class); TransportService transportService = new TransportService(Settings.EMPTY, mock(Transport.class), null, TransportService.NOOP_TRANSPORT_INTERCEPTOR, x -> null, null, Collections.emptySet()); TransportChangePasswordAction action = new TransportChangePasswordAction(settings, transportService, mock(ActionFilters.class), usersStore); // Request will fail before the request hashing algorithm is checked, but we use the same algorithm as in settings for consistency ChangePasswordRequest request = new ChangePasswordRequest(); request.username(anonymousUser.principal()); request.passwordHash(Hasher.resolve(hashingAlgorithm).hash(SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING)); final AtomicReference<Throwable> throwableRef = new AtomicReference<>(); final AtomicReference<ChangePasswordResponse> responseRef = new AtomicReference<>(); action.doExecute(mock(Task.class), request, new ActionListener<ChangePasswordResponse>() { @Override public void onResponse(ChangePasswordResponse changePasswordResponse) { responseRef.set(changePasswordResponse); } @Override public void onFailure(Exception e) { throwableRef.set(e); } }); assertThat(responseRef.get(), is(nullValue())); assertThat(throwableRef.get(), instanceOf(IllegalArgumentException.class)); assertThat(throwableRef.get().getMessage(), containsString("is anonymous and cannot be modified")); verifyZeroInteractions(usersStore); }	not essential, but would it be worth putting the choice of hashing algorithm in its own utility method?
* @return a map of blob names and their metadata */ Map<String, BlobMetaData> listBlobs(String prefix) throws IOException { return listBlobsByPrefix(prefix, ""); }	nit: can you fix javadoc?
* @return an InputStream */ InputStream readBlob(String blobName) throws IOException { final BlobId blobId = BlobId.of(bucket, blobName); final Blob blob = SocketAccess.doPrivilegedIOException(() -> storage.get(blobId)); if (blob == null) { throw new NoSuchFileException("Blob [" + blobName + "] does not exit."); } final ReadChannel readChannel = SocketAccess.doPrivilegedIOException(blob::reader); return java.nio.channels.Channels.newInputStream(new ReadableByteChannel() { @SuppressForbidden(reason = "Channel is based of a socket not a file.") @Override public int read(ByteBuffer dst) throws IOException { return SocketAccess.doPrivilegedIOException(() -> readChannel.read(dst)); } @Override public boolean isOpen() { return readChannel.isOpen(); } @Override public void close() throws IOException { SocketAccess.doPrivilegedVoidIOException(readChannel::close); } }); }	i think that you can just import channels now
* @param blobSize expected size of the blob to be written */ void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException { final BlobInfo blobInfo = BlobInfo.newBuilder(bucket, blobName).build(); if (blobSize > (5 * 1024 * 1024)) { // uses "resumable upload" for files larger than 5MB, see // https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload final WriteChannel writeChannel = SocketAccess.doPrivilegedIOException(() -> storage.writer(blobInfo)); Streams.copy(inputStream, java.nio.channels.Channels.newOutputStream(new WritableByteChannel() { @Override public boolean isOpen() { return writeChannel.isOpen(); } @Override public void close() throws IOException { SocketAccess.doPrivilegedVoidIOException(writeChannel::close); } @SuppressForbidden(reason = "Channel is based of a socket not a file.") @Override public int write(ByteBuffer src) throws IOException { return SocketAccess.doPrivilegedIOException(() -> writeChannel.write(src)); } })); } else { // uses multipart upload for small files (1 request for both data and metadata, // gziped) final ByteArrayOutputStream baos = new ByteArrayOutputStream(Math.toIntExact(blobSize)); Streams.copy(inputStream, baos); SocketAccess.doPrivilegedVoidIOException(() -> storage.create(blobInfo, baos.toByteArray())); } }	can we put this in a static constant with documentation?
* @param blobSize expected size of the blob to be written */ void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException { final BlobInfo blobInfo = BlobInfo.newBuilder(bucket, blobName).build(); if (blobSize > (5 * 1024 * 1024)) { // uses "resumable upload" for files larger than 5MB, see // https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload final WriteChannel writeChannel = SocketAccess.doPrivilegedIOException(() -> storage.writer(blobInfo)); Streams.copy(inputStream, java.nio.channels.Channels.newOutputStream(new WritableByteChannel() { @Override public boolean isOpen() { return writeChannel.isOpen(); } @Override public void close() throws IOException { SocketAccess.doPrivilegedVoidIOException(writeChannel::close); } @SuppressForbidden(reason = "Channel is based of a socket not a file.") @Override public int write(ByteBuffer src) throws IOException { return SocketAccess.doPrivilegedIOException(() -> writeChannel.write(src)); } })); } else { // uses multipart upload for small files (1 request for both data and metadata, // gziped) final ByteArrayOutputStream baos = new ByteArrayOutputStream(Math.toIntExact(blobSize)); Streams.copy(inputStream, baos); SocketAccess.doPrivilegedVoidIOException(() -> storage.create(blobInfo, baos.toByteArray())); } }	can we split writeblob() into two different methods and document them?
* @param blobSize expected size of the blob to be written */ void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException { final BlobInfo blobInfo = BlobInfo.newBuilder(bucket, blobName).build(); if (blobSize > (5 * 1024 * 1024)) { // uses "resumable upload" for files larger than 5MB, see // https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload final WriteChannel writeChannel = SocketAccess.doPrivilegedIOException(() -> storage.writer(blobInfo)); Streams.copy(inputStream, java.nio.channels.Channels.newOutputStream(new WritableByteChannel() { @Override public boolean isOpen() { return writeChannel.isOpen(); } @Override public void close() throws IOException { SocketAccess.doPrivilegedVoidIOException(writeChannel::close); } @SuppressForbidden(reason = "Channel is based of a socket not a file.") @Override public int write(ByteBuffer src) throws IOException { return SocketAccess.doPrivilegedIOException(() -> writeChannel.write(src)); } })); } else { // uses multipart upload for small files (1 request for both data and metadata, // gziped) final ByteArrayOutputStream baos = new ByteArrayOutputStream(Math.toIntExact(blobSize)); Streams.copy(inputStream, baos); SocketAccess.doPrivilegedVoidIOException(() -> storage.create(blobInfo, baos.toByteArray())); } }	this is a legit optimization according to the docs, but actually it is necessary to get around the library, which [does not use the port](https://github.com/googlecloudplatform/google-cloud-java/blob/52b727aef88ae76984aa3c02b4d7067e198d34b7/google-cloud-storage/src/main/java/com/google/cloud/storage/spi/v1/httpstoragerpc.java#l720) for the new connections it opens for the "resumable" upload. this means the rest integ tests will try to upload files using the default port 80, but the fixture cannot bind this port. the multipart upload code path builds the url correctly.
*/ void deleteBlobs(Collection<String> blobNames) throws IOException { if ((blobNames == null) || blobNames.isEmpty()) { return; } if (blobNames.size() < 5) { for (final String blobName : blobNames) { deleteBlob(blobName); } return; } final List<BlobId> blobIdsToDelete = blobNames.stream().map(blobName -> BlobId.of(bucket, blobName)).collect(Collectors.toList()); final List<Boolean> deletedStatuses = SocketAccess.doPrivilegedIOException(() -> storage.delete(blobIdsToDelete)); assert blobIdsToDelete.size() == deletedStatuses.size(); boolean failed = false; for (int i = 0; i < blobIdsToDelete.size(); i++) { if (deletedStatuses.get(i) == false) { logger.error("Failed to delete blob [{}] in bucket [{}].", blobIdsToDelete.get(i).getName(), bucket); failed = true; } } if (failed) { throw new IOException("Failed to delete all [" + blobIdsToDelete.size() + "] blobs."); } }	this "optimization" is not justifiable per se, but it is required to get around another url building blunder inside the google cloud library. [batched requests don't honor the hostname + port configuration](https://github.com/googlecloudplatform/google-cloud-java/blob/52b727aef88ae76984aa3c02b4d7067e198d34b7/google-cloud-storage/src/main/java/com/google/cloud/storage/spi/v1/httpstoragerpc.java#l191). all batched request paths are hardcoded to www.googleapis.com rest integ tests don't delete more than 3 files in a single batch, so this code branch is required to make the integ tests work.
*/ void deleteBlobs(Collection<String> blobNames) throws IOException { if ((blobNames == null) || blobNames.isEmpty()) { return; } if (blobNames.size() < 5) { for (final String blobName : blobNames) { deleteBlob(blobName); } return; } final List<BlobId> blobIdsToDelete = blobNames.stream().map(blobName -> BlobId.of(bucket, blobName)).collect(Collectors.toList()); final List<Boolean> deletedStatuses = SocketAccess.doPrivilegedIOException(() -> storage.delete(blobIdsToDelete)); assert blobIdsToDelete.size() == deletedStatuses.size(); boolean failed = false; for (int i = 0; i < blobIdsToDelete.size(); i++) { if (deletedStatuses.get(i) == false) { logger.error("Failed to delete blob [{}] in bucket [{}].", blobIdsToDelete.get(i).getName(), bucket); failed = true; } } if (failed) { throw new IOException("Failed to delete all [" + blobIdsToDelete.size() + "] blobs."); } }	i'd change this for: storage.copy(request).getresult();
public Storage createClient(final String clientName) throws GeneralSecurityException, IOException { final GoogleCloudStorageClientSettings clientSettings = clientsSettings.get(clientName); if (clientSettings == null) { throw new IllegalArgumentException("Unknown client name [" + clientName + "]. Existing client configs: " + Strings.collectionToDelimitedString(clientsSettings.keySet(), ",")); } final NetHttpTransport netHttpTransport = GoogleNetHttpTransport.newTrustedTransport(); final HttpTransportOptions httpTransportOptions = HttpTransportOptions.newBuilder() .setConnectTimeout(toTimeout(clientSettings.getConnectTimeout())) .setReadTimeout(toTimeout(clientSettings.getReadTimeout())) .setHttpTransportFactory(() -> netHttpTransport) .build(); final StorageOptions.Builder storageOptionsBuilder = StorageOptions.newBuilder() .setTransportOptions(httpTransportOptions) .setHeaderProvider(() -> { final MapBuilder<String, String> mapBuilder = MapBuilder.newMapBuilder(); if (Strings.hasLength(clientSettings.getApplicationName())) { mapBuilder.put("user-agent", clientSettings.getApplicationName()); } return mapBuilder.immutableMap(); }); if (Strings.hasLength(clientSettings.getHost())) { storageOptionsBuilder.setHost(clientSettings.getHost()); } if (Strings.hasLength(clientSettings.getProjectId())) { storageOptionsBuilder.setProjectId(clientSettings.getProjectId()); } if (clientSettings.getCredential() == null) { logger.warn("\\\\"Application Default Credentials\\\\" are not supported out of the box." + " Additional file system permissions have to be granted to the plugin."); } else { final ServiceAccountCredentials serviceAccountCredentials = clientSettings.getCredential(); if (Strings.hasLength(clientSettings.getTokenUri().toString())) { storageOptionsBuilder .setCredentials(serviceAccountCredentials.toBuilder().setTokenServerUri(clientSettings.getTokenUri()).build()); } else { storageOptionsBuilder.setCredentials(serviceAccountCredentials); } } return storageOptionsBuilder.build().getService(); }	override token uri, using client setting from elasticsearch.yml, and does not override the application default credentials.
public void testMultiMatchQuery() throws Exception { createIndex("test"); indexRandom(true, client().prepareIndex("test", "type1", "1").setSource("field1", "value1", "field2", "value4", "field3", "value3"), client().prepareIndex("test", "type1", "2").setSource("field1", "value2", "field2", "value5", "field3", "value2"), client().prepareIndex("test", "type1", "3").setSource("field1", "value3", "field2", "value6", "field3", "value1") ); MultiMatchQueryBuilder builder = multiMatchQuery("value1 value2 value4", "field1", "field2"); SearchResponse searchResponse = client().prepareSearch().setQuery(builder) .addAggregation(AggregationBuilders.terms("field1").field("field1.keyword")).get(); assertHitCount(searchResponse, 2L); // this uses dismax so scores are equal and the order can be arbitrary assertSearchHits(searchResponse, "1", "2"); searchResponse = client().prepareSearch() .setQuery(builder) .get(); assertHitCount(searchResponse, 2L); assertSearchHits(searchResponse, "1", "2"); client().admin().indices().prepareRefresh("test").get(); builder = multiMatchQuery("value1", "field1", "field2") .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together. searchResponse = client().prepareSearch() .setQuery(builder) .get(); assertHitCount(searchResponse, 1L); assertFirstHit(searchResponse, hasId("1")); refresh(); builder = multiMatchQuery("value1", "field1").field("field3", 1.5f) .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together. searchResponse = client().prepareSearch().setQuery(builder).get(); assertHitCount(searchResponse, 2L); assertSearchHits(searchResponse, "3", "1"); client().admin().indices().prepareRefresh("test").get(); builder = multiMatchQuery("value1").field("field1").field("field3", 1.5f) .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together. searchResponse = client().prepareSearch().setQuery(builder).get(); assertHitCount(searchResponse, 2L); assertSearchHits(searchResponse, "3", "1"); // Test lenient client().prepareIndex("test", "type1", "3").setSource("field1", "value7", "field2", "value8", "field4", 5).get(); refresh(); builder = multiMatchQuery("value1", "field1", "field2", "field4"); //when the number for shards is randomized and we expect failures //we can either run into partial or total failures depending on the current number of shards Matcher<String> reasonMatcher = containsString("NumberFormatException: For input string: \\\\"value1\\\\""); ShardSearchFailure[] shardFailures; try { client().prepareSearch().setQuery(builder).get(); shardFailures = searchResponse.getShardFailures(); assertThat("Expected shard failures, got none", shardFailures, not(emptyArray())); } catch (SearchPhaseExecutionException e) { assertThat(e.status(), CoreMatchers.equalTo(RestStatus.BAD_REQUEST)); shardFailures = e.shardFailures(); } for (ShardSearchFailure shardSearchFailure : shardFailures) { assertThat(shardSearchFailure.status(), equalTo(RestStatus.BAD_REQUEST)); assertThat(shardSearchFailure.reason(), reasonMatcher); } builder.lenient(true); searchResponse = client().prepareSearch().setQuery(builder).get(); assertHitCount(searchResponse, 1L); assertFirstHit(searchResponse, hasId("1")); }	we should use expectthrows here
protected void masterOperation( Task task, CreateIndexRequest request, ClusterState state, ActionListener<CreateIndexResponse> finalListener ) { AtomicReference<String> indexNameRef = new AtomicReference<>(); ActionListener<AcknowledgedResponse> listener = ActionListener.wrap(response -> { String indexName = indexNameRef.get(); assert indexName != null; if (response.isAcknowledged()) { activeShardsObserver.waitForActiveShards( new String[] { indexName }, ActiveShardCount.DEFAULT, request.timeout(), shardsAcked -> finalListener.onResponse(new CreateIndexResponse(true, shardsAcked, indexName)), finalListener::onFailure ); } else { finalListener.onResponse(new CreateIndexResponse(false, false, indexName)); } }, finalListener::onFailure); CreateIndexTask clusterTask = new CreateIndexTask(request, listener, indexNameRef); clusterService.submitStateUpdateTask("auto create [" + request.index() + "]", clusterTask, clusterTask, executor, clusterTask); }	are we getting much benefit from the extends ackedclusterstateupdatetask here? i think if we dropped that and just implemented ackedclusterstatetasklistener directly we wouldn't need to construct the listener up front and could probably solve the todo about deduplicating the listeners.
protected void masterOperation( Task task, CreateIndexRequest request, ClusterState state, ActionListener<CreateIndexResponse> finalListener ) { AtomicReference<String> indexNameRef = new AtomicReference<>(); ActionListener<AcknowledgedResponse> listener = ActionListener.wrap(response -> { String indexName = indexNameRef.get(); assert indexName != null; if (response.isAcknowledged()) { activeShardsObserver.waitForActiveShards( new String[] { indexName }, ActiveShardCount.DEFAULT, request.timeout(), shardsAcked -> finalListener.onResponse(new CreateIndexResponse(true, shardsAcked, indexName)), finalListener::onFailure ); } else { finalListener.onResponse(new CreateIndexResponse(false, false, indexName)); } }, finalListener::onFailure); CreateIndexTask clusterTask = new CreateIndexTask(request, listener, indexNameRef); clusterService.submitStateUpdateTask("auto create [" + request.index() + "]", clusterTask, clusterTask, executor, clusterTask); }	does this need to be an atomicreference or could it just be a volatile field?
public void onResponse(ResyncTask resyncTask) { logger.info("primary-replica resync completed with {} operations", resyncTask.getResyncedOperations()); synchronized (mutex) { final IndexShardState prevState = changeState(IndexShardState.STARTED, "Resync is completed"); if (prevState != IndexShardState.PROMOTING) { throw new IllegalIndexShardStateException(shardId, prevState, "primary-replica resync finished but was not started"); } } }	same comment as above, let's check state first and then change it.
public void onResponse(ResyncTask resyncTask) { logger.info("primary-replica resync completed with {} operations", resyncTask.getResyncedOperations()); synchronized (mutex) { final IndexShardState prevState = changeState(IndexShardState.STARTED, "Resync is completed"); if (prevState != IndexShardState.PROMOTING) { throw new IllegalIndexShardStateException(shardId, prevState, "primary-replica resync finished but was not started"); } } }	as we have no corresponding state transition here now, does it mean that the shard can forever be stuck in promoting state, allow no relocations?
protected void doExecute( final Task task, final FollowStatsAction.StatsRequest request, final ActionListener<FollowStatsAction.StatsResponses> listener) { if (ccrLicenseChecker.isCcrAllowed() == false) { listener.onFailure(LicenseUtils.newComplianceException("ccr")); return; } final ClusterState state = clusterService.state(); Set<String> shardFollowTaskFollowerIndices = findFollowerIndicesFromShardFollowTasks(state, request.indices()); if (Strings.isAllOrWildcard(request.indices()) == false && shardFollowTaskFollowerIndices.isEmpty()) { String resources = String.join(",", request.indices()); throw new ResourceNotFoundException("No shard follow tasks for follower indices [{}]", resources); } super.doExecute(task, request, listener); }	nit: maybe only execute this the indices is not "_all"?
static Request clearRealmCache(ClearRealmCacheRequest clearRealmCacheRequest) { RequestConverters.EndpointBuilder builder = new RequestConverters.EndpointBuilder() .addPathPartAsIs("_security/realm"); if (!clearRealmCacheRequest.getRealms().isEmpty()) { builder.addCommaSeparatedPathParts(clearRealmCacheRequest.getRealms().toArray(Strings.EMPTY_ARRAY)); } else { builder.addPathPart("_all"); } final String endpoint = builder.addPathPartAsIs("_clear_cache").build(); Request request = new Request(HttpPost.METHOD_NAME, endpoint); if (!clearRealmCacheRequest.getUsernames().isEmpty()) { RequestConverters.Params params = new RequestConverters.Params(); params.putParam("usernames", Strings.collectionToCommaDelimitedString(clearRealmCacheRequest.getUsernames())); request.setParameters(params.getRequestParams()); } return request; }	can you please revert this change and all the similar ones? we tend to prefer the boolean == false over !boolean as the exclamation mark can easily be missed.
synchronized Query getRetentionQuery() { final BooleanQuery.Builder queryBuilder = new BooleanQuery.Builder(); queryBuilder.add(LongPoint.newRangeQuery(SeqNoFieldMapper.NAME, getMinRetainedSeqNo(), Long.MAX_VALUE), BooleanClause.Occur.SHOULD); if (retentionAge.seconds() > 0) { // we prefer using a docValues instead of an indexed field to avoid impact on append-only indices. // however, it's a trade-off as we have to linearly scan to find the most recent documents. final Query timestampQuery = NumericDocValuesField.newSlowRangeQuery(SeqNoFieldMapper.UPDATE_TIMESTAMP_IN_SECONDS_NAME, absoluteTimeInSeconds.getAsLong() - retentionAge.seconds(), Long.MAX_VALUE); queryBuilder.add(timestampQuery, BooleanClause.Occur.SHOULD); } return queryBuilder.build(); }	i think this the right trade off here.
public static StreamInput wrap(byte[] bytes, int offset, int length) { return new ByteBufferStreamInput(ByteBuffer.wrap(bytes, offset, length)); } /** * Reads a vint via {@link #readVInt()} and applies basic checks to ensure the read array size is sane. * This method uses {@link #ensureCanReadBytes(int)}	wow we call this method a lot! i'm curious to see what this change will translate to in terms of raw performance in general use.
public static WritePrimaryResult<BulkShardRequest, BulkShardResponse> performOnPrimary( BulkShardRequest request, IndexShard primary, UpdateHelper updateHelper, LongSupplier nowInMillisSupplier, MappingUpdatePerformer mappingUpdater) throws Exception { final IndexMetaData metaData = primary.indexSettings().getIndexMetaData(); Translog.Location location = null; for (int requestIndex = 0; requestIndex < request.items().length; requestIndex++) { if (request.items()[requestIndex].getPrimaryResponse() == null) { location = executeBulkItemRequest(metaData, primary, request, location, requestIndex, updateHelper, nowInMillisSupplier, mappingUpdater); } } BulkItemResponse[] responses = new BulkItemResponse[request.items().length]; BulkItemRequest[] items = request.items(); for (int i = 0; i < items.length; i++) { responses[i] = items[i].getPrimaryResponse(); } BulkShardResponse response = new BulkShardResponse(request.shardId(), responses); return new WritePrimaryResult<>(request, response, location, null, primary, logger); }	this is tricky - see [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/bulk/transportshardbulkaction.java#l209), which shows we may have a primary response, but still execute the item... i'm not sure it's wrong, i saying "watch out" ...
public void indicesAliases(final Request request, final Listener listener) { clusterService.submitStateUpdateTask("index-aliases", new ProcessedClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { for (AliasAction aliasAction : request.actions) { if (!currentState.metaData().hasIndex(aliasAction.index())) { listener.onFailure(new IndexMissingException(new Index(aliasAction.index()))); return currentState; } if (currentState.metaData().hasIndex(aliasAction.alias())) { listener.onFailure(new InvalidAliasNameException(new Index(aliasAction.index()), aliasAction.alias(), "an index exists with the same name as the alias")); return currentState; } if (aliasAction.indexRouting() != null && aliasAction.indexRouting().indexOf(',') != -1) { listener.onFailure(new ElasticSearchIllegalArgumentException("alias [" + aliasAction.alias() + "] has several routing values associated with it")); return currentState; } } List<String> indicesToClose = Lists.newArrayList(); Map<String, IndexService> indices = Maps.newHashMap(); try { boolean changed = false; MetaData.Builder builder = newMetaDataBuilder().metaData(currentState.metaData()); for (AliasAction aliasAction : request.actions) { IndexMetaData indexMetaData = builder.get(aliasAction.index()); if (indexMetaData == null) { throw new IndexMissingException(new Index(aliasAction.index())); } IndexMetaData.Builder indexMetaDataBuilder = newIndexMetaDataBuilder(indexMetaData); if (aliasAction.actionType() == AliasAction.Type.ADD) { String filter = aliasAction.filter(); if (Strings.hasLength(filter)) { // parse the filter, in order to validate it IndexService indexService = indices.get(indexMetaData.index()); if (indexService == null) { indexService = indicesService.indexService(indexMetaData.index()); if (indexService == null) { // temporarily create the index so we have can parse the filter indexService = indicesService.createIndex(indexMetaData.index(), indexMetaData.settings(), currentState.nodes().localNode().id()); indicesToClose.add(indexMetaData.index()); } indices.put(indexMetaData.index(), indexService); } // now, parse the filter IndexQueryParserService indexQueryParser = indexService.queryParserService(); try { XContentParser parser = XContentFactory.xContent(filter).createParser(filter); try { indexQueryParser.parseInnerFilter(parser); } finally { parser.close(); } } catch (Exception e) { listener.onFailure(new ElasticSearchIllegalArgumentException("failed to parse filter for [" + aliasAction.alias() + "]", e)); return currentState; } } AliasMetaData newAliasMd = AliasMetaData.newAliasMetaDataBuilder( aliasAction.alias()) .filter(filter) .indexRouting(aliasAction.indexRouting()) .searchRouting(aliasAction.searchRouting()) .build(); // Check if this alias already exists AliasMetaData aliasMd = indexMetaData.aliases().get(aliasAction.alias()); if (aliasMd != null && aliasMd.equals(newAliasMd)) { // It's the same alias - ignore it continue; } indexMetaDataBuilder.putAlias(newAliasMd); } else if (aliasAction.actionType() == AliasAction.Type.REMOVE) { if (!indexMetaData.aliases().containsKey(aliasAction.alias())) { // This alias doesn't exist - ignore continue; } indexMetaDataBuilder.removerAlias(aliasAction.alias()); } changed = true; builder.put(indexMetaDataBuilder); } if (changed) { ClusterState updatedState = newClusterStateBuilder().state(currentState).metaData(builder).build(); // wait for responses from other nodes if needed int responseCount = updatedState.getNodes().size(); long version = updatedState.version() + 1; logger.debug("Waiting for [{}] notifications with version [{}]", responseCount, version); aliasOperationPerformedAction.add(new CountDownListener(responseCount, listener, version), request.timeout); return updatedState; } else { // Nothing to do listener.onResponse(new Response(true)); return currentState; } } finally { for (String index : indicesToClose) { indicesService.cleanIndex(index, "created for alias processing"); } } } @Override public void clusterStateProcessed(ClusterState clusterState) { } }); }	i suggest trace logging here
@Override public RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException { boolean waitForCompletion = request.paramAsBoolean("wait_for_completion", true); // Build the internal request StartReindexJobAction.Request internal = new StartReindexJobAction.Request(setCommonOptions(request, buildRequest(request)), waitForCompletion); // Executes the request and waits for completion if (waitForCompletion) { Map<String, String> params = new HashMap<>(); params.put(BulkByScrollTask.Status.INCLUDE_CREATED, Boolean.toString(true)); params.put(BulkByScrollTask.Status.INCLUDE_UPDATED, Boolean.toString(true)); return channel -> client.execute(StartReindexJobAction.INSTANCE, internal, new ActionListener<>() { private BulkIndexByScrollResponseContentListener listener = new BulkIndexByScrollResponseContentListener(channel, params); @Override public void onResponse(StartReindexJobAction.Response response) { listener.onResponse(response.getReindexResponse()); } @Override public void onFailure(Exception e) { listener.onFailure(e); } }); } else { internal.getReindexRequest().setShouldStoreResult(true); /* * Let's try and validate before forking so the user gets some error. The * task can't totally validate until it starts but this is better than * nothing. */ ActionRequestValidationException validationException = internal.getReindexRequest().validate(); if (validationException != null) { throw validationException; } return channel -> client.execute(StartReindexJobAction.INSTANCE, internal, new RestBuilderListener<>(channel) { @Override public RestResponse buildResponse(StartReindexJobAction.Response response, XContentBuilder builder) throws Exception { builder.startObject(); // TODO: This is a different task that we returned in the past. In the past we had the // public API task ID (node id + long). This is the generated UUID for the persistent task. builder.field("task", response.getTaskId()); // TODO: Are there error conditions for the non-wait case? return new BytesRestResponse(RestStatus.OK, builder.endObject()); } }); } }	i thought you wanted to return both here for bwc reasons?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.field("node", taskId.getNodeId()); builder.field("id", taskId.getId()); builder.field("type", type); builder.field("action", action); if (status != null) { builder.field("status", status, params); } if (description != null) { builder.field("description", description); } builder.timeField("start_time_in_millis", "start_time", startTime); if (builder.humanReadable()) { builder.field("running_time", new TimeValue(runningTimeNanos, TimeUnit.NANOSECONDS).toString()); } builder.field("running_time_in_nanos", runningTimeNanos); builder.field("cancellable", cancellable); if (parentTaskId.isSet() && this.getAction().contains("reindex") == false) { builder.field("parent_task_id", parentTaskId.toString()); } builder.startObject("headers"); for(Map.Entry<String, String> attribute : headers.entrySet()) { builder.field(attribute.getKey(), attribute.getValue()); } builder.endObject(); return builder; }	what's the problem with setting this in case of reindex? is it because it's null in that case?
@Override protected void execute(Terminal terminal, OptionSet options, Environment env) throws Exception { List<PluginDescriptor> plugins = arguments.values(options) .stream() .map(id -> new PluginDescriptor(id, id)) .collect(Collectors.toList()); final boolean isBatch = options.has(batchOption); if (plugins.isEmpty()) { throw new UserException(ExitCodes.USAGE, "at least one plugin ID is required"); } InstallPluginAction action = new InstallPluginAction(terminal, env, isBatch); try { action.execute(plugins); } catch (InstallPluginException e) { int exitCode; switch (e.getProblem()) { case DUPLICATE_PLUGIN_ID: case UNKNOWN_PLUGIN: default: exitCode = ExitCodes.USAGE; break; case NO_XPACK: case RELEASE_SNAPSHOT_MISMATCH: exitCode = ExitCodes.CONFIG; break; case INVALID_CHECKSUM: case MISSING_CHECKSUM: exitCode = ExitCodes.IO_ERROR; break; case INVALID_SIGNATURE: exitCode = ExitCodes.DATA_ERROR; break; case PLUGIN_MALFORMED: exitCode = PLUGIN_MALFORMED; break; case PLUGIN_EXISTS: exitCode = PLUGIN_EXISTS; break; case INCOMPATIBLE_LICENSE: exitCode = ExitCodes.NOPERM; break; case INSTALLATION_FAILED: exitCode = 1; break; } throw new UserException(exitCode, e.getMessage(), e); } }	rather than manage this big switch statement here could we add an exitcode property to installpluginproblem, manage this mapping there, and then just return e.getproblem().getexitcode()?
@Override protected void execute(Terminal terminal, OptionSet options, Environment env) throws Exception { List<PluginDescriptor> plugins = arguments.values(options) .stream() .map(id -> new PluginDescriptor(id, id)) .collect(Collectors.toList()); final boolean isBatch = options.has(batchOption); if (plugins.isEmpty()) { throw new UserException(ExitCodes.USAGE, "at least one plugin ID is required"); } InstallPluginAction action = new InstallPluginAction(terminal, env, isBatch); try { action.execute(plugins); } catch (InstallPluginException e) { int exitCode; switch (e.getProblem()) { case DUPLICATE_PLUGIN_ID: case UNKNOWN_PLUGIN: default: exitCode = ExitCodes.USAGE; break; case NO_XPACK: case RELEASE_SNAPSHOT_MISMATCH: exitCode = ExitCodes.CONFIG; break; case INVALID_CHECKSUM: case MISSING_CHECKSUM: exitCode = ExitCodes.IO_ERROR; break; case INVALID_SIGNATURE: exitCode = ExitCodes.DATA_ERROR; break; case PLUGIN_MALFORMED: exitCode = PLUGIN_MALFORMED; break; case PLUGIN_EXISTS: exitCode = PLUGIN_EXISTS; break; case INCOMPATIBLE_LICENSE: exitCode = ExitCodes.NOPERM; break; case INSTALLATION_FAILED: exitCode = 1; break; } throw new UserException(exitCode, e.getMessage(), e); } }	if we decide my above suggestion isn't worth while can we at least place the default path at the bottom. when we move to java 17 this would be a great use case for switch expressions as the compiler would error if we failed to handle every scenario, since that's not yet the case i think the suggestion above is better as it _forces_ us to declare the right exit code for each condition.
@Override public BytesRef getPayload() throws IOException { return in.getPayload(); } } private static Bits applyRetentionQuery(CodecReader reader, Query retentionQuery) throws IOException { final Bits liveDocs = reader.getLiveDocs(); final IndexSearcher searcher = new IndexSearcher(new FilterCodecReader(reader) { @Override public CacheHelper getCoreCacheHelper() { return reader.getCoreCacheHelper(); } @Override public CacheHelper getReaderCacheHelper() { return null; // we are altering live docs } @Override public Bits getLiveDocs() { return new Bits() { @Override public boolean get(int index) { return liveDocs.get(index) == false; } @Override public int length() { return liveDocs.length(); } }; } @Override public int numDocs() { return reader.maxDoc() - reader.numDocs(); } }); searcher.setQueryCache(null); final Weight weight = searcher.createWeight(searcher.rewrite(retentionQuery), ScoreMode.COMPLETE_NO_SCORES, 1.0f); final Scorer scorer = weight.scorer(reader.getContext()); final Bits bits = Lucene.union(liveDocs, scorer == null ? DocIdSetIterator.empty() : scorer.iterator()); if (bits == liveDocs && reader.numDocs() == 0) { return new Bits.MatchNoBits(liveDocs.length()); // fully deleted segment } else { return bits; }	would it make sense to just make all docs live for this search?
public static String nodeStringValue(Object node, String defaultValue) { if (node == null) { return defaultValue; } return node.toString(); }	its probably worth adding javadoc about how this is different from objects.tostring.
static void checkForIndexDataInDefaultPathData(final NodeEnvironment nodeEnv, final Logger logger) throws IOException { if (nodeEnv.defaultNodePath() == null) { return; } final Set<String> availableIndexFolders = nodeEnv.availableIndexFoldersForPath(nodeEnv.defaultNodePath()); if (!availableIndexFolders.isEmpty()) { final String message = String.format( Locale.ROOT, "detected index data in default.path.data [%s] where there should not be any", nodeEnv.defaultNodePath().indicesPath); logger.error(message); for (final String availableIndexFolder : availableIndexFolders) { logger.info( "index folder [{}] in default.path.data [{}] must be moved to any of {}", availableIndexFolder, nodeEnv.defaultNodePath().indicesPath, Arrays.stream(nodeEnv.nodePaths()).map(np -> np.indicesPath).collect(Collectors.toList())); } throw new IllegalStateException(message); } }	how about if (availableindexfolders.size() > 0) { to avoid the double-negative?
public void testNodeConstructionWithDefaultPathDataSet() throws IOException { final Path home = createTempDir().toAbsolutePath(); final Path zero = createTempDir().toAbsolutePath(); final Path one = createTempDir().toAbsolutePath(); final Path defaultPathData = createTempDir().toAbsolutePath(); final Settings settings = Settings.builder() .put("path.home", home) .put("path.data.0", zero) .put("path.data.1", one) .put("default.path.data", defaultPathData) .put("http.enabled", false) .put("transport.type", "mock-socket-network") .build(); Files.createDirectories(defaultPathData.resolve("nodes/0")); final boolean indexExists = randomBoolean(); if (indexExists) { final int numberOfIndices = randomIntBetween(1, 3); for (final String index : IntStream.range(0, numberOfIndices).mapToObj(i -> UUIDs.randomBase64UUID()).collect(Collectors.toList())) { Files.createDirectories(defaultPathData.resolve("nodes/0/indices").resolve(index)); } } Supplier<MockNode> constructor = () -> new MockNode(settings, Collections.singletonList(MockTcpTransportPlugin.class)); if (indexExists) { final IllegalStateException e = expectThrows(IllegalStateException.class, constructor::get); final String message = String.format( Locale.ROOT, "detected index data in default.path.data [%s] where there should not be any", defaultPathData.resolve("nodes/0/indices")); assertThat(e, hasToString(containsString(message))); } else { try (Node ignored = constructor.get()) { // node construction should be okay } } }	can you randomize between .put("path.data.0", zero).put("path.data.1", one) and .putarray("path.data", zero, one)?
public void testNodeConstructionWithDefaultPathDataSet() throws IOException { final Path home = createTempDir().toAbsolutePath(); final Path zero = createTempDir().toAbsolutePath(); final Path one = createTempDir().toAbsolutePath(); final Path defaultPathData = createTempDir().toAbsolutePath(); final Settings settings = Settings.builder() .put("path.home", home) .put("path.data.0", zero) .put("path.data.1", one) .put("default.path.data", defaultPathData) .put("http.enabled", false) .put("transport.type", "mock-socket-network") .build(); Files.createDirectories(defaultPathData.resolve("nodes/0")); final boolean indexExists = randomBoolean(); if (indexExists) { final int numberOfIndices = randomIntBetween(1, 3); for (final String index : IntStream.range(0, numberOfIndices).mapToObj(i -> UUIDs.randomBase64UUID()).collect(Collectors.toList())) { Files.createDirectories(defaultPathData.resolve("nodes/0/indices").resolve(index)); } } Supplier<MockNode> constructor = () -> new MockNode(settings, Collections.singletonList(MockTcpTransportPlugin.class)); if (indexExists) { final IllegalStateException e = expectThrows(IllegalStateException.class, constructor::get); final String message = String.format( Locale.ROOT, "detected index data in default.path.data [%s] where there should not be any", defaultPathData.resolve("nodes/0/indices")); assertThat(e, hasToString(containsString(message))); } else { try (Node ignored = constructor.get()) { // node construction should be okay } } }	this seems like an overly complex way of doing java for (int i = 0; i < numberofindices; i++) { files.createdirectories(defaultpathdata.resolve("nodes/0/indices").resolve(uuids.randombase64uuid())); } what do you think?
public void testNodeConstructionWithDefaultPathDataSet() throws IOException { final Path home = createTempDir().toAbsolutePath(); final Path zero = createTempDir().toAbsolutePath(); final Path one = createTempDir().toAbsolutePath(); final Path defaultPathData = createTempDir().toAbsolutePath(); final Settings settings = Settings.builder() .put("path.home", home) .put("path.data.0", zero) .put("path.data.1", one) .put("default.path.data", defaultPathData) .put("http.enabled", false) .put("transport.type", "mock-socket-network") .build(); Files.createDirectories(defaultPathData.resolve("nodes/0")); final boolean indexExists = randomBoolean(); if (indexExists) { final int numberOfIndices = randomIntBetween(1, 3); for (final String index : IntStream.range(0, numberOfIndices).mapToObj(i -> UUIDs.randomBase64UUID()).collect(Collectors.toList())) { Files.createDirectories(defaultPathData.resolve("nodes/0/indices").resolve(index)); } } Supplier<MockNode> constructor = () -> new MockNode(settings, Collections.singletonList(MockTcpTransportPlugin.class)); if (indexExists) { final IllegalStateException e = expectThrows(IllegalStateException.class, constructor::get); final String message = String.format( Locale.ROOT, "detected index data in default.path.data [%s] where there should not be any", defaultPathData.resolve("nodes/0/indices")); assertThat(e, hasToString(containsString(message))); } else { try (Node ignored = constructor.get()) { // node construction should be okay } } }	this line could be moved above the previous indexexists if statement and then the two if (indexexists) blocks could be combined
public void testNodeConstructionWithDefaultPathDataSet() throws IOException { final Path home = createTempDir().toAbsolutePath(); final Path zero = createTempDir().toAbsolutePath(); final Path one = createTempDir().toAbsolutePath(); final Path defaultPathData = createTempDir().toAbsolutePath(); final Settings settings = Settings.builder() .put("path.home", home) .put("path.data.0", zero) .put("path.data.1", one) .put("default.path.data", defaultPathData) .put("http.enabled", false) .put("transport.type", "mock-socket-network") .build(); Files.createDirectories(defaultPathData.resolve("nodes/0")); final boolean indexExists = randomBoolean(); if (indexExists) { final int numberOfIndices = randomIntBetween(1, 3); for (final String index : IntStream.range(0, numberOfIndices).mapToObj(i -> UUIDs.randomBase64UUID()).collect(Collectors.toList())) { Files.createDirectories(defaultPathData.resolve("nodes/0/indices").resolve(index)); } } Supplier<MockNode> constructor = () -> new MockNode(settings, Collections.singletonList(MockTcpTransportPlugin.class)); if (indexExists) { final IllegalStateException e = expectThrows(IllegalStateException.class, constructor::get); final String message = String.format( Locale.ROOT, "detected index data in default.path.data [%s] where there should not be any", defaultPathData.resolve("nodes/0/indices")); assertThat(e, hasToString(containsString(message))); } else { try (Node ignored = constructor.get()) { // node construction should be okay } } }	same comment here about the streams when a normal for loop would work? i suppose here you actually save the indices, but it still seems simpler to just add them to a regular arraylist
public void testNodeConstructionWithDefaultPathDataSet() throws IOException { final Path home = createTempDir().toAbsolutePath(); final Path zero = createTempDir().toAbsolutePath(); final Path one = createTempDir().toAbsolutePath(); final Path defaultPathData = createTempDir().toAbsolutePath(); final Settings settings = Settings.builder() .put("path.home", home) .put("path.data.0", zero) .put("path.data.1", one) .put("default.path.data", defaultPathData) .put("http.enabled", false) .put("transport.type", "mock-socket-network") .build(); Files.createDirectories(defaultPathData.resolve("nodes/0")); final boolean indexExists = randomBoolean(); if (indexExists) { final int numberOfIndices = randomIntBetween(1, 3); for (final String index : IntStream.range(0, numberOfIndices).mapToObj(i -> UUIDs.randomBase64UUID()).collect(Collectors.toList())) { Files.createDirectories(defaultPathData.resolve("nodes/0/indices").resolve(index)); } } Supplier<MockNode> constructor = () -> new MockNode(settings, Collections.singletonList(MockTcpTransportPlugin.class)); if (indexExists) { final IllegalStateException e = expectThrows(IllegalStateException.class, constructor::get); final String message = String.format( Locale.ROOT, "detected index data in default.path.data [%s] where there should not be any", defaultPathData.resolve("nodes/0/indices")); assertThat(e, hasToString(containsString(message))); } else { try (Node ignored = constructor.get()) { // node construction should be okay } } }	might as well spy(logger) instead of creating a new mocked instance? at least then you'll see the logging message(s) in the test output
public void handleResponse(TransportResponse.Empty response) { pendingJoinInfo.message = PENDING_JOIN_WAITING_STATE; // only logged if state delayed pendingOutgoingJoins.remove(dedupKey); logger.debug("successfully joined {} with {}", destination, joinRequest); lastFailedJoinAttempt.set(null); }	is this needed again?
public boolean acquire(final EvictionListener listener) throws IOException { assert listener != null; ensureOpen(); boolean success = false; if (refCounter.tryIncRef()) { try { synchronized (listeners) { ensureOpen(); final boolean added = listeners.add(listener); assert added : "listener already exists " + listener; if (listeners.size() == 1) { assert channelRef == null; channelRef = new FileChannelReference(file); } } success = true; } finally { if (success == false) { refCounter.decRef(); } } } assert invariant(); return success; }	there's no need for a volatile and immutable set of listeners here. we never accessed the listeners outside of the lock so we can simply mutate them in place. i made this clearer now by using the listeners set as the mutex for all operations that apply to the set and the channel opening/closing that is a result of the set size.
public boolean release(final EvictionListener listener) { assert listener != null; boolean success = false; try { synchronized (listeners) { final boolean removed = listeners.remove(Objects.requireNonNull(listener)); assert removed : "listener does not exist " + listener; if (removed == false) { throw new IllegalStateException("Cannot remove an unknown listener"); } if (listeners.isEmpty()) { // nobody is using this file so we close the channel channelRef.decRef(); channelRef = null; } } success = true; } finally { if (success) { refCounter.decRef(); } } assert invariant(); return success; }	i found this a lot cleaner and easier to reason about than checking the value of this boolean twice with tricky locking and the cost of the atomic reference relative to the volatile boolean should be trivial.
public String groupMatch(String name, Region region, String pattern) { int number = GROK_PATTERN_REGEX.nameToBackrefNumber(name.getBytes(StandardCharsets.UTF_8), 0, name.getBytes(StandardCharsets.UTF_8).length, region); int begin = region.beg[number]; int end = region.end[number]; if (begin < 0) { // no match found return null; } return new String(pattern.getBytes(StandardCharsets.UTF_8), begin, end - begin, StandardCharsets.UTF_8); }	i think we should remove recursion here altogether, that way we can avoid stack overflows without using additional threads and context switching. as this is tail recursion it is fairly easy and straightforward: java public string toregex(string grokpattern) { stringbuilder res= new stringbuilder(); while (true) { byte[] grokpatternbytes = grokpattern.getbytes(standardcharsets.utf_8); matcher matcher = grok_pattern_regex.matcher(grokpatternbytes); int result; try { matcherwatchdog.register(matcher); result = matcher.search(0, grokpatternbytes.length, option.none); } finally { matcherwatchdog.unregister(matcher); } if (result < 0) { return res.append(grokpattern).tostring(); } region region = matcher.geteagerregion(); string namedpatternref = groupmatch(name_group, region, grokpattern); string subname = groupmatch(subname_group, region, grokpattern);// todo(tal): support definitions @suppresswarnings("unused") string definition = groupmatch(definition_group, region, grokpattern); string patternname = groupmatch(pattern_group, region, grokpattern); string pattern = patternbank.get(patternname); if (pattern == null) { throw new illegalargumentexception("unable to find pattern [" + patternname + "] in grok's pattern dictionary"); } if (pattern.contains("%{" + patternname + "}") || pattern.contains("%{" + patternname + ":")) { throw new illegalargumentexception("circular reference in pattern back [" + patternname + "]"); } string grokpart; if (namedcaptures && subname != null) { grokpart = string.format(locale.us, "(?<%s>%s)", namedpatternref, pattern); } else if (namedcaptures) { grokpart = string.format(locale.us, "(?:%s)", pattern); } else { grokpart = string.format(locale.us, "(?<%s>%s)", patternname + "_" + result, pattern); } string start = new string(grokpatternbytes, 0, result, standardcharsets.utf_8); string rest = new string(grokpatternbytes, region.end[0], grokpatternbytes.length - region.end[0], standardcharsets.utf_8); grokpattern = grokpart+rest; res.append(start); } }
public void initClient() { threadPool = new ThreadPool("test-" + getClass()); client = buildClient(HEADER_SETTINGS, ACTIONS); }	maybe use gettestname() instead?
public void testUpgradePersistentSettingsOnUpdate() { runUpgradeSettingsOnUpdateTest((settings, builder) -> builder.setPersistentSettings(settings), Metadata::persistentSettings); }	we should keep this test but handle the deprecation warning.
public void testUpdateDependentClusterSettings() { IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () -> client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder() .put("cluster.acc.test.pw", "asdf")).get()); assertEquals("missing required setting [cluster.acc.test.user] for setting [cluster.acc.test.pw]", iae.getMessage()); client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder() .put("cluster.acc.test.pw", "asdf") .put("cluster.acc.test.user", "asdf")).get(); iae = expectThrows(IllegalArgumentException.class, () -> client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder() .putNull("cluster.acc.test.user")).get()); assertEquals("missing required setting [cluster.acc.test.user] for setting [cluster.acc.test.pw]", iae.getMessage()); client().admin().cluster().prepareUpdateSettings().setPersistentSettings(Settings.builder() .putNull("cluster.acc.test.pw") .putNull("cluster.acc.test.user")).get(); }	another test we should keep but separate.
public void test33RunsIfJavaNotOnPath() throws Exception { assumeThat(distribution().hasJdk, is(true)); // we don't require java be installed for the tests, but remove it if it's there // since we don't require it for the tests, don't bother restoring it if (Files.exists(Paths.get("/usr/bin/java"))) { sh.run("sudo rm -f /usr/bin/java"); } startElasticsearch(sh, installation); runElasticsearchTests(); stopElasticsearch(sh); }	why was the previous code causing problems? if we are going to ensure /usr/bin/java doesn't exist, we should do so outside of test execution. this seems like an environmental issue, not something a test (in the middle of other tests running) should be changing. deleting and not restoring system files is a serious, unexpected side effect to a test running.
public static void logAllLogs(Path logsDir, Logger logger) { if (Files.exists(logsDir) == false) { logger.warn("Can't show logs from directory {} as it doesn't exists", logsDir); return; } logger.info("Showing contents of directory: {} ({})", logsDir, logsDir.toAbsolutePath()); try(Stream<Path> fileStream = Files.list(logsDir)) { fileStream // gc logs are verbose and not useful in this context .filter(file -> file.getFileName().toString().startsWith("gc.log") == false) .forEach(file -> { logger.info("=== Contents of `{}` ({}) ===", file, file.toAbsolutePath()); try (Stream<String> stream = Files.lines(file)) { stream.forEach(logger::info); } catch (IOException e) { logger.error("Can't show contents", e); } logger.info("=== End of contents of `{}`===", file); }); } catch (IOException e) { logger.error("Can't list log files", e); } }	nit: space after try
public ClusterState execute(ClusterState currentState) { DiscoveryNodes nodes = currentState.nodes(); SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); if (snapshots == null) { return currentState; } boolean changed = false; ArrayList<SnapshotsInProgress.Entry> entries = new ArrayList<>(); for (final SnapshotsInProgress.Entry snapshot : snapshots.entries()) { SnapshotsInProgress.Entry updatedSnapshot = snapshot; if (snapshot.state() == State.STARTED || snapshot.state() == State.ABORTED) { ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder(); boolean snapshotChanged = false; for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards()) { final ShardSnapshotStatus shardStatus = shardEntry.value; final ShardId shardId = shardEntry.key; if (!shardStatus.state().completed() && shardStatus.nodeId() != null) { if (nodes.nodeExists(shardStatus.nodeId())) { shards.put(shardId, shardStatus); } else { // TODO: Restart snapshot on another node? snapshotChanged = true; logger.warn("failing snapshot of shard [{}] on closed node [{}]", shardId, shardStatus.nodeId()); shards.put(shardId, new ShardSnapshotStatus(shardStatus.nodeId(), ShardState.FAILED, "node shutdown", shardStatus.generation())); } } else { shards.put(shardId, shardStatus); } } if (snapshotChanged) { changed = true; ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardsMap = shards.build(); if (!snapshot.state().completed() && completed(shardsMap.values())) { updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, State.SUCCESS, shardsMap); } else { updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, snapshot.state(), shardsMap); } } entries.add(updatedSnapshot); } else if (snapshot.state() == State.INIT && initializingSnapshots.contains(snapshot.snapshot()) == false) { changed = true; // Mark the snapshot as aborted as it failed to start from the previous master updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, State.ABORTED, snapshot.shards()); entries.add(updatedSnapshot); // Clean up the snapshot that failed to start from the old master deleteSnapshot(snapshot.snapshot(), new ActionListener<>() { @Override public void onResponse(Void aVoid) { logger.debug("cleaned up abandoned snapshot {} in INIT state", snapshot.snapshot()); } @Override public void onFailure(Exception e) { logger.warn("failed to clean up abandoned snapshot {} in INIT state", snapshot.snapshot()); } }, updatedSnapshot.getRepositoryStateId(), false); } assert updatedSnapshot.shards().size() == snapshot.shards().size() : "Shard count changed during snapshot status update from [" + snapshot + "] to [" + updatedSnapshot + "]"; } if (changed) { return ClusterState.builder(currentState) .putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(unmodifiableList(entries))).build(); } return currentState; }	can we assert in the constructor of snapshotsinprogress.entry that for every entry in "indices" there is at least one entry in the shards map and vice versa?
public void testRestoreSnapshot() throws IOException { Map<String, String> expectedParams = new HashMap<>(); String repository = randomIndicesNames(1, 1)[0]; String snapshot = "snapshot-" + randomAlphaOfLengthBetween(2, 5).toLowerCase(Locale.ROOT); String endpoint = String.format(Locale.ROOT, "/_snapshot/%s/%s/_restore", repository, snapshot); RestoreSnapshotRequest restoreSnapshotRequest = new RestoreSnapshotRequest(repository, snapshot); setRandomMasterTimeout(restoreSnapshotRequest, expectedParams); Boolean waitForCompletion = randomBoolean(); restoreSnapshotRequest.waitForCompletion(waitForCompletion); if (waitForCompletion) { expectedParams.put("wait_for_completion", waitForCompletion.toString()); } if (randomBoolean()) { restoreSnapshotRequest.masterNodeTimeout("120s"); expectedParams.put("master_timeout", "120s"); } Request request = RequestConverters.restoreSnapshot(restoreSnapshotRequest); assertThat(endpoint, equalTo(request.getEndpoint())); assertThat(HttpPost.METHOD_NAME, equalTo(request.getMethod())); assertThat(expectedParams, equalTo(request.getParameters())); assertToXContentBody(restoreSnapshotRequest, request.getEntity()); }	boolean instead? looks like it can't be null.
public void testRestoreSnapshot() throws IOException { String testRepository = "test"; String testSnapshot = "snapshot_1"; String testIndex = "test_index"; String restoredIndex = testIndex + "_restored"; PutRepositoryResponse putRepositoryResponse = createTestRepository(testRepository, FsRepository.TYPE, "{\\\\"location\\\\": \\\\".\\\\"}"); assertTrue(putRepositoryResponse.isAcknowledged()); createIndex(testIndex, Settings.EMPTY); CreateSnapshotRequest createSnapshotRequest = new CreateSnapshotRequest(testRepository, testSnapshot); createSnapshotRequest.indices(testIndex); createSnapshotRequest.waitForCompletion(true); CreateSnapshotResponse createSnapshotResponse = createTestSnapshot(createSnapshotRequest); assertEquals(RestStatus.OK, createSnapshotResponse.status()); RestoreSnapshotRequest request = new RestoreSnapshotRequest(testRepository, testSnapshot); request.waitForCompletion(true); request.renamePattern(testIndex); request.renameReplacement(restoredIndex); RestoreSnapshotResponse response = execute(request, highLevelClient().snapshot()::restore, highLevelClient().snapshot()::restoreAsync); RestoreInfo restoreInfo = response.getRestoreInfo(); assertThat(restoreInfo.name(), equalTo(testSnapshot)); assertThat(restoreInfo.indices(), equalTo(Collections.singletonList(restoredIndex))); assertThat(restoreInfo.successfulShards(), greaterThan(0)); assertThat(restoreInfo.failedShards(), equalTo(0)); }	delete the index and check that it came back? stick a document in it and check that it was restored? i don't want to test snapshot/restore but i do want to get a fairly real test just in case.
* @return this request */ @SuppressWarnings("unchecked") public RestoreSnapshotRequest source(Map<String, Object> source) { for (Map.Entry<String, Object> entry : source.entrySet()) { String name = entry.getKey(); if (name.equals("indices")) { if (entry.getValue() instanceof String) { indices(Strings.splitStringByCommaToArray((String) entry.getValue())); } else if (entry.getValue() instanceof ArrayList) { indices((ArrayList<String>) entry.getValue()); } else { throw new IllegalArgumentException("malformed indices section, should be an array of strings"); } } else if (name.equals("partial")) { partial(nodeBooleanValue(entry.getValue(), "partial")); } else if (name.equals("settings")) { if (!(entry.getValue() instanceof Map)) { throw new IllegalArgumentException("malformed settings section"); } settings((Map<String, Object>) entry.getValue()); } else if (name.equals("include_global_state")) { includeGlobalState = nodeBooleanValue(entry.getValue(), "include_global_state"); } else if (name.equals("include_aliases")) { includeAliases = nodeBooleanValue(entry.getValue(), "include_aliases"); } else if (name.equals("rename_pattern")) { if (entry.getValue() instanceof String) { renamePattern((String) entry.getValue()); } else if (entry.getValue() != null) { throw new IllegalArgumentException("malformed rename_pattern"); } } else if (name.equals("rename_replacement")) { if (entry.getValue() instanceof String) { renameReplacement((String) entry.getValue()); } else if (entry.getValue() != null) { throw new IllegalArgumentException("malformed rename_replacement"); } } else if (name.equals("index_settings")) { if (!(entry.getValue() instanceof Map)) { throw new IllegalArgumentException("malformed index_settings section"); } indexSettings((Map<String, Object>) entry.getValue()); } else if (name.equals("ignore_index_settings")) { if (entry.getValue() instanceof String) { ignoreIndexSettings(Strings.splitStringByCommaToArray((String) entry.getValue())); } else if (entry.getValue() instanceof List) { ignoreIndexSettings((List<String>) entry.getValue()); } else { throw new IllegalArgumentException("malformed ignore_index_settings section, should be an array of strings"); } } else { if (IndicesOptions.isIndicesOptions(name) == false) { throw new IllegalArgumentException("Unknown parameter " + name); } } } indicesOptions(IndicesOptions.fromMap(source, indicesOptions)); return this; }	should we instead skip emitting the value if it is null?
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; RestoreSnapshotRequest that = (RestoreSnapshotRequest) o; return waitForCompletion == that.waitForCompletion && includeGlobalState == that.includeGlobalState && partial == that.partial && includeAliases == that.includeAliases && Objects.equals(snapshot, that.snapshot) && Objects.equals(repository, that.repository) && Arrays.equals(indices, that.indices) && Objects.equals(indicesOptions, that.indicesOptions) && Objects.equals(renamePattern, that.renamePattern) && Objects.equals(renameReplacement, that.renameReplacement) && Objects.equals(settings, that.settings) && Objects.equals(indexSettings, that.indexSettings) && Arrays.equals(ignoreIndexSettings, that.ignoreIndexSettings); }	strings.tostring is pretty good for this. it spits it out in json.
public static RestoreSnapshotResponse fromXContent(XContentParser parser) throws IOException { RestoreSnapshotResponse response = new RestoreSnapshotResponse(); parser.nextToken(); // move to '{' if (parser.currentToken() != XContentParser.Token.START_OBJECT) { throw new IllegalArgumentException("unexpected token [" + parser.currentToken() + "], expected ['{']"); } parser.nextToken(); // move to 'snapshot' || 'accepted' if ("snapshot".equals(parser.currentName())) { response.restoreInfo = RestoreInfo.fromXContent(parser); } else if ("accepted".equals(parser.currentName())) { parser.nextToken(); // move to 'accepted' field value if (parser.booleanValue()) { // ensure accepted is a boolean value } parser.nextToken(); // move past 'true'/'false' } else { throw new IllegalArgumentException("unexpected token [" + parser.currentToken() + "] expected ['snapshot', 'accepted']"); } if (parser.currentToken() != XContentParser.Token.END_OBJECT) { throw new IllegalArgumentException("unexpected token [" + parser.currentToken() + "], expected ['}']"); } parser.nextToken(); // move past '}' return response; }	i feel like objectparser may be better here just so i don't have to think "does this thing really only ever spit out one fields? does this code properly ignore unknown fields so it is forward compatible?"
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; RestoreInfo that = (RestoreInfo) o; return totalShards == that.totalShards && successfulShards == that.successfulShards && Objects.equals(name, that.name) && Objects.equals(indices, that.indices); }	same thing about tostring as above. either way is fine with me, but we're starting to standardize on using the json.
static void checkRemoteClusterVersions(DatafeedConfig config, List<String> remoteClusters, Function<String, Version> clusterVersionSupplier) { Optional<Tuple<Version, String>> minVersionAndReason = config.minRequiredClusterVersion(); if (minVersionAndReason.isPresent() == false) { return; } final String reason = minVersionAndReason.get().v2(); final Version minVersion = minVersionAndReason.get().v1(); List<String> clustersTooOld = remoteClusters.stream() .filter(cn -> clusterVersionSupplier.apply(cn).before(minVersion)) .collect(Collectors.toList()); if (clustersTooOld.isEmpty()) { return; } throw ExceptionsHelper.badRequestException( "remote clusters {} are not at least version [{}] which is required for [{}]", clustersTooOld, minVersion.toString(), reason ); } /** Creates {@link DataExtractorFactory}	please take a look at the analogical (but different) message in sourcedestvalidator.remote_clusters_too_old and try to unify them.
public void testRemoteClusterVersionCheck() { Map<String, Version> clusterVersions = MapBuilder.<String, Version>newMapBuilder() .put("modern_cluster_1", Version.CURRENT) .put("modern_cluster_2", Version.CURRENT) .put("old_cluster_1", Version.V_7_0_0) .map(); Map<String, Object> settings = new HashMap<>(); settings.put("type", "keyword"); settings.put("script", ""); Map<String, Object> field = new HashMap<>(); field.put("runtime_field_foo", settings); DatafeedConfig config = new DatafeedConfig.Builder(DatafeedConfigTests.createRandomizedDatafeedConfig("foo")) .setRuntimeMappings(field) .build(); ElasticsearchStatusException ex = expectThrows(ElasticsearchStatusException.class, () -> TransportStartDatafeedAction.checkRemoteClusterVersions( config, Arrays.asList("old_cluster_1", "modern_cluster_2"), clusterVersions::get ) ); assertThat( ex.getMessage(), containsString( "remote clusters [old_cluster_1] are not at least version [7.11.0] which is required for [runtime_mappings]" ) ); // The rest should not throw TransportStartDatafeedAction.checkRemoteClusterVersions( config, Arrays.asList("modern_cluster_1", "modern_cluster_2"), clusterVersions::get ); DatafeedConfig configWithoutRuntimeMappings = new DatafeedConfig.Builder() .setId("foo-datafeed") .setIndices(Collections.singletonList("bar")) .setJobId("foo").build(); TransportStartDatafeedAction.checkRemoteClusterVersions( configWithoutRuntimeMappings, Arrays.asList("old_cluster_1", "modern_cluster_2"), clusterVersions::get ); }	could mapbuilder be used here too?
public void testRemoteClusterVersionCheck() { Map<String, Version> clusterVersions = MapBuilder.<String, Version>newMapBuilder() .put("modern_cluster_1", Version.CURRENT) .put("modern_cluster_2", Version.CURRENT) .put("old_cluster_1", Version.V_7_0_0) .map(); Map<String, Object> settings = new HashMap<>(); settings.put("type", "keyword"); settings.put("script", ""); Map<String, Object> field = new HashMap<>(); field.put("runtime_field_foo", settings); DatafeedConfig config = new DatafeedConfig.Builder(DatafeedConfigTests.createRandomizedDatafeedConfig("foo")) .setRuntimeMappings(field) .build(); ElasticsearchStatusException ex = expectThrows(ElasticsearchStatusException.class, () -> TransportStartDatafeedAction.checkRemoteClusterVersions( config, Arrays.asList("old_cluster_1", "modern_cluster_2"), clusterVersions::get ) ); assertThat( ex.getMessage(), containsString( "remote clusters [old_cluster_1] are not at least version [7.11.0] which is required for [runtime_mappings]" ) ); // The rest should not throw TransportStartDatafeedAction.checkRemoteClusterVersions( config, Arrays.asList("modern_cluster_1", "modern_cluster_2"), clusterVersions::get ); DatafeedConfig configWithoutRuntimeMappings = new DatafeedConfig.Builder() .setId("foo-datafeed") .setIndices(Collections.singletonList("bar")) .setJobId("foo").build(); TransportStartDatafeedAction.checkRemoteClusterVersions( configWithoutRuntimeMappings, Arrays.asList("old_cluster_1", "modern_cluster_2"), clusterVersions::get ); }	could singletonmap or mapbuilder be used here?
public void testAddNewReplicasOnFollower() throws Exception { int numberOfReplicas = between(0, 1); String leaderIndexSettings = getIndexSettings(1, numberOfReplicas, singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("leader-index").setSource(leaderIndexSettings, XContentType.JSON)); PutFollowAction.Request follow = putFollow("leader-index", "follower-index"); followerClient().execute(PutFollowAction.INSTANCE, follow).get(); getFollowerCluster().ensureAtLeastNumDataNodes(numberOfReplicas + between(2, 3)); ensureFollowerGreen("follower-index"); AtomicBoolean stopped = new AtomicBoolean(); AtomicInteger docID = new AtomicInteger(); boolean appendOnly = randomBoolean(); Thread indexingOnLeader = new Thread(() -> { while (stopped.get() == false) { try { if (appendOnly) { String id = Integer.toString(docID.incrementAndGet()); leaderClient().prepareIndex("leader-index", "doc", id).setSource("{\\\\"f\\\\":" + id + "}", XContentType.JSON).get(); } else if (frequently()) { String id = Integer.toString(frequently() ? docID.incrementAndGet() : between(0, 100)); leaderClient().prepareIndex("leader-index", "doc", id).setSource("{\\\\"f\\\\":" + id + "}", XContentType.JSON).get(); } else { String id = Integer.toString(between(0, docID.get())); leaderClient().prepareDelete("leader-index", "doc", id).get(); } } catch (Exception ex) { throw new AssertionError(ex); } } }); indexingOnLeader.start(); Thread flushingOnFollower = new Thread(() -> { while (stopped.get() == false) { try { if (rarely()) { followerClient().admin().indices().prepareFlush("follower-index").get(); } if (rarely()) { followerClient().admin().indices().prepareRefresh("follower-index").get(); } } catch (Exception ex) { throw new AssertionError(ex); } } }); flushingOnFollower.start(); awaitGlobalCheckpointAtLeast(followerClient(), new ShardId(resolveFollowerIndex("follower-index"), 0), 50); followerClient().admin().indices().prepareUpdateSettings("follower-index") .setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, numberOfReplicas + 1).build()).get(); ensureFollowerGreen("follower-index"); awaitGlobalCheckpointAtLeast(followerClient(), new ShardId(resolveFollowerIndex("follower-index"), 0), 100); stopped.set(true); flushingOnFollower.join(); indexingOnLeader.join(); assertIndexFullyReplicatedToFollower("leader-index", "follower-index"); pauseFollow("follower-index"); }	can you add a comment to each step in this test and describe why it is required? i think i understand it, but it in any way it is great to have that as documentation.
static SecureSettings loadSecureSettings(Environment initialEnv) throws BootstrapException { final KeyStoreWrapper keystore; try { keystore = KeyStoreWrapper.load(initialEnv.configFile()); } catch (IOException e) { throw new BootstrapException(e); } if (keystore == null) { return null; // no keystore } try { keystore.decrypt(new char[0] /* TODO: read password from stdin */); KeyStoreWrapper.upgrade(keystore, initialEnv.configFile()); } catch (Exception e) { throw new BootstrapException(e); } return keystore; }	i am good with the change, but i wonder if we should validate it sooner (e.g., in elasticsearch). there are other components of the system that might touch the java.io.tmpdir before this validation is performed?
static Request executeWatch(ExecuteWatchRequest executeWatchRequest) throws IOException { RequestConverters.EndpointBuilder builder = new RequestConverters.EndpointBuilder() .addPathPart("_xpack", "watcher", "watch"); if (executeWatchRequest.getId() != null) { builder.addPathPart(executeWatchRequest.getId()); } String endpoint = builder.addPathPart("_execute").build(); Request request = new Request(HttpPost.METHOD_NAME, endpoint); RequestConverters.Params params = new RequestConverters.Params(request); if (executeWatchRequest.isDebug()) { params.putParam("debug", "true"); } if (executeWatchRequest.ignoreCondition()) { params.putParam("ignore_condition", "true"); } if (executeWatchRequest.recordExecution()) { params.putParam("record_execution", "true"); } request.setEntity(RequestConverters.createEntity(executeWatchRequest, XContentType.JSON)); return request; }	these can be .addpathpartasis since they are not user supplied.
static Settings additionalSettings(final Settings settings, final boolean enabled, final boolean transportClientMode) { if (enabled && transportClientMode) { return Settings.builder() .put(SecuritySettings.addTransportSettings(settings)) .put(SecuritySettings.addUserSettings(settings)) .build(); } else { return Settings.EMPTY; } }	please also add this into the ml actions tested by reservedrolesstoretests. it should be permitted for machine_learning_admin and forbidden for machine_learning_user.
static ElasticsearchException makeCurrentlyBeingUpgradedException(Logger logger, String jobId, String explanation) { String msg = "Could not open job as relevant indices are being upgraded [" + explanation + "]"; logger.warn("[{}] {}", jobId, msg); Exception detail = new IllegalStateException(msg); return new ElasticsearchStatusException("Could not open job as indices are being upgraded", RestStatus.TOO_MANY_REQUESTS, detail); }	i don't think this needs a nested detail exception. the nested detail exception was added to other assignment failures because it contains a different reason for each node in the cluster, so can be huge. but in this case we just want to report the high level reason. so the method can be something like this: string msg = "will not open job as ml upgrade mode is enabled"; logger.warn("[{}] {}", jobid, msg); return new elasticsearchstatusexception(msg, reststatus.too_many_requests); again, i've included the exact term "upgrade mode" in the message, so that a user who's aware ml has a set_upgrade_mode endpoint may realise what's happening without having to raise a support case.
protected void masterOperation( Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener ) { final boolean enabled = XPackSettings.SECURITY_ENABLED.get(settings); final Builder builder = new Builder(enabled, usage -> listener.onResponse(new XPackUsageFeatureResponse(usage))); builder.setSslUsage(sslUsage(settings)); builder.setAuditUsage(auditUsage(settings)); builder.setIpFilterUsage(ipFilterUsage(ipFilter)); builder.setAnonymousUsage(Map.of("enabled", AnonymousUser.isAnonymousEnabled(settings))); builder.setFips140Usage(fips140Usage(settings)); builder.setOperatorPrivilegesUsage( Map.of( "available", Security.OPERATOR_PRIVILEGES_FEATURE.checkWithoutTracking(licenseState), "enabled", OperatorPrivileges.OPERATOR_PRIVILEGES_ENABLED.get(settings) ) ); if (rolesStore == null || enabled == false) { builder.setRolesStoreUsage(Map.of()); } else { rolesStore.usageStats(ActionListener.wrap(builder::setRolesStoreUsage, listener::onFailure)); } if (roleMappingStore == null || enabled == false) { builder.setRoleMappingStoreUsage(Map.of("native", Map.of())); } else { roleMappingStore.usageStats( ActionListener.wrap(usage -> builder.setRoleMappingStoreUsage(Map.of("native", usage)), listener::onFailure) ); } if (realms == null || enabled == false) { builder.setRealmsUsage(Map.of()); } else { realms.usageStats(ActionListener.wrap(builder::setRealmsUsage, listener::onFailure)); } if (tokenService == null || enabled == false) { builder.setTokenServiceUsage(Map.of()); } else { tokenService.usageStats(ActionListener.wrap(builder::setTokenServiceUsage, listener::onFailure)); } if (apiKeyService == null || enabled == false) { builder.setApiKeyServiceUsage(Map.of()); } else { apiKeyService.usageStats(ActionListener.wrap(builder::setApiKeyServiceUsage, listener::onFailure)); } }	i refactored all this countdown code into the builder at the bottom of the file. i think it's easier to read & understand, but i'm happy to try a different approach.
public BytesReference get() throws IOException { BytesReference message; try { assert messageSupplier != null; message = messageSupplier.get(); messageSupplier = null; messageSize = message.length(); TransportLogger.logOutboundMessage(channel, message); return message; } catch (Exception e) { assert false : new AssertionError("Serialization shouild never fail", e); onFailure(e); throw e; } }	i'm not convinced about this. there are some places where we throw an exception because we're trying to serialise something to a version of elasticsearch that simply cannot understand the message we want to send. for instance, this: https://github.com/elastic/elasticsearch/blob/9acd2fd1fd1130d6c8a6bfc6e19eb9af7a44621a/server/src/main/java/org/elasticsearch/common/lucene/lucene.java#l417 i think an exception is a reasonable way to handle this kind of impossible-to-represent message. i do, however, agree that this shouldn't be as harmful to the cluster as it is today.
public void generateApiKey(Authentication authentication, CreateApiKeyRequest request, ActionListener<CreateApiKeyResponse> listener) { if (authentication == null) { listener.onFailure(new ElasticsearchSecurityException("no authentication available to generate API key")); return; } apiKeyService.ensureEnabled(); final ActionListener<Set<RoleDescriptor>> roleDescriptorsListener = ActionListener.wrap(roleDescriptors -> { for (RoleDescriptor rd : roleDescriptors) { try { DLSRoleQueryValidator.validateQueryField(rd.getIndicesPrivileges(), xContentRegistry); } catch (ElasticsearchException | IllegalArgumentException e) { listener.onFailure(e); return; } } apiKeyService.createApiKey(authentication, request, roleDescriptors, listener); }, listener::onFailure); final Subject effectiveSubject = AuthenticationContext.fromAuthentication(authentication).getEffectiveSubject(); // TODO: retain current behaviour that User of an API key authentication has no roles. if (effectiveSubject.getType() == Subject.Type.API_KEY) { roleDescriptorsListener.onResponse(Set.of()); return; } rolesStore.getRoleDescriptorsList(effectiveSubject, ActionListener.wrap(roleDescriptorsList -> { assert roleDescriptorsList.size() == 1; roleDescriptorsListener.onResponse(roleDescriptorsList.iterator().next()); }, roleDescriptorsListener::onFailure)); }	can you clarify this todo? is it something to fix in this pr, or should someone fix it in the future?
public void resolveNamedRoleReference( RoleReference.NamedRoleReference namedRoleReference, ActionListener<RolesRetrievalResult> listener ) { final Set<String> roleNames = Set.copyOf(new HashSet<>(List.of(namedRoleReference.getRoleNames()))); if (roleNames.isEmpty()) { listener.onResponse(RolesRetrievalResult.EMPTY); } else if (roleNames.contains(ReservedRolesStore.SUPERUSER_ROLE_DESCRIPTOR.getName())) { listener.onResponse(RolesRetrievalResult.SUPERUSER); } else { resolveRoleNames(roleNames, listener); } }	removed the two assertions because they are not the concern of this method. this method only resolves role descriptors and does *not* build role. the role building part is not necessary for empty or superuser role and it is guaranteed elsewhere.
@Override public void connectToNode(DiscoveryNode node, ConnectionProfile connectionProfile) { connectionProfile = connectionProfile == null ? defaultConnectionProfile : connectionProfile; if (!lifecycle.started()) { throw new IllegalStateException("can't add nodes to a stopped transport"); } if (node == null) { throw new ConnectTransportException(null, "can't connect to a null node"); } globalLock.readLock().lock(); try { try (Releasable ignored = connectionLock.acquire(node.getId())) { if (!lifecycle.started()) { throw new IllegalStateException("can't add nodes to a stopped transport"); } NodeChannels nodeChannels = connectedNodes.get(node); if (nodeChannels != null) { return; } try { try { nodeChannels = openConnection(node, connectionProfile); } catch (Exception e) { logger.trace( (Supplier<?>) () -> new ParameterizedMessage( "failed to connect to [{}], cleaning dangling connections", node), e); throw e; } if (doHandshakes) { // some tests need to disable this Channel channel = nodeChannels.channel(TransportRequestOptions.Type.PING); final TimeValue connectTimeout = connectionProfile.getConnectTimeout() == null ? defaultConnectionProfile.getConnectTimeout(): connectionProfile.getConnectTimeout(); final TimeValue handshakeTimeout = connectionProfile.getHandshakeTimeout() == null ? connectTimeout: connectionProfile.getHandshakeTimeout(); Version version = executeHandshake(node, channel, handshakeTimeout); if (version != null) { // this is a BWC layer, if we talk to a pre 5.2 node then the handshake is not supported // this will go away in master once it's all ported to 5.2 but for now we keep this to make // the backport straight forward nodeChannels = new NodeChannels(nodeChannels, version); } } // we acquire a connection lock, so no way there is an existing connection connectedNodes.put(node, nodeChannels); if (logger.isDebugEnabled()) { logger.debug("connected to node [{}]", node); } transportServiceAdapter.onNodeConnected(node); } catch (ConnectTransportException e) { throw e; } catch (Exception e) { throw new ConnectTransportException(node, "general node connection failure", e); } } } finally { globalLock.readLock().unlock(); } }	nit: space after connecttimeout
public static byte setCompress(byte value) { value |= STATUS_COMPRESS; return value; }	i don't see either of these methods used...is something missing from this pr?
@Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { Transports.assertTransportThread(); if (!(msg instanceof ByteBuf)) { ctx.fireChannelRead(msg); return; } final ByteBuf buffer = (ByteBuf) msg; final int remainingMessageSize = buffer.getInt(buffer.readerIndex() - TcpHeader.MESSAGE_LENGTH_SIZE); int expectedReaderIndex = buffer.readerIndex(); try { expectedReaderIndex = buffer.readerIndex() + remainingMessageSize; InetSocketAddress remoteAddress = (InetSocketAddress) ctx.channel().remoteAddress(); // netty always copies a buffer, either in NioWorker in its read handler, where it copies to a fresh // buffer, or in the cumulation buffer, which is cleaned each time so it could be bigger than the actual size BytesReference reference = Netty4Utils.toBytesReference(buffer, remainingMessageSize); transport.messageReceived(reference, ctx.channel(), profileName, remoteAddress, remainingMessageSize); } finally { // Set the expected position of the buffer, no matter what happened buffer.readerIndex(expectedReaderIndex); } }	why can't remainingmessagesize be added to this outside the try and remove this line?
@Override public void close() throws IOException { connection.close(); } } public Transport getDelegateTransport() { Transport transport = transport(); while(transport instanceof DelegateTransport) { transport = ((DelegateTransport) transport).transport; }	nit: i think this method name is confusing, since it does *not* return a delegatetransport. perhaps getoriginaltransport() or getrealtransport() or something like that?
public void testNoopAfterRegularEngine() throws IOException { int docs = randomIntBetween(1, 10); ReplicationTracker tracker = (ReplicationTracker) engine.config().getGlobalCheckpointSupplier(); ShardRouting routing = TestShardRouting.newShardRouting("test", shardId.id(), "node", null, true, ShardRoutingState.STARTED, allocationId); IndexShardRoutingTable table = new IndexShardRoutingTable.Builder(shardId).addShard(routing).build(); tracker.updateFromMaster(1L, Collections.singleton(allocationId.getId()), table, Collections.emptySet()); tracker.activatePrimaryMode(SequenceNumbers.NO_OPS_PERFORMED); for (int i = 0; i < docs; i++) { ParsedDocument doc = testParsedDocument("" + i, null, testDocumentWithTextField(), B_1, null); engine.index(indexForDoc(doc)); tracker.updateLocalCheckpoint(allocationId.getId(), i); } engine.flush(true, true); engine.getTranslog().getDeletionPolicy().setRetentionSizeInBytes(-1); engine.getTranslog().getDeletionPolicy().setRetentionAgeInMillis(-1); engine.getTranslog().getDeletionPolicy().setMinTranslogGenerationForRecovery( engine.getTranslog().getGeneration().translogFileGeneration); engine.flush(true, true); long localCheckpoint = engine.getLocalCheckpoint(); long maxSeqNo = engine.getSeqNoStats(100L).getMaxSeqNo(); engine.close(); final NoopEngine noopEngine = new NoopEngine(noopConfig(INDEX_SETTINGS, store, primaryTranslogDir, tracker)); assertThat(noopEngine.getLocalCheckpoint(), equalTo(localCheckpoint)); assertThat(noopEngine.getSeqNoStats(100L).getMaxSeqNo(), equalTo(maxSeqNo)); try (Engine.IndexCommitRef ref = noopEngine.acquireLastIndexCommit(false)) { try (IndexReader reader = DirectoryReader.open(ref.getIndexCommit())) { assertThat(reader.numDocs(), equalTo(docs)); } } noopEngine.close(); }	why is this needed? we typically just write to the engine?
private void handleFailure(Exception e, AtomicInteger retryCounter, Runnable task) { assert e != null; if (shouldRetry(e) && isStopped() == false) { LOGGER.debug(new ParameterizedMessage("{} error during follow shard task, retrying...", params.getFollowShardId()), e); int currentRetry = retryCounter.incrementAndGet(); long delay = computeDelay(currentRetry, maxRetryDelay.getMillis()); scheduler.accept(TimeValue.timeValueMillis(delay), task); } else { markAsFailed(e); } }	we should debug log here the number of times that we have retried up to here.
private void handleFailure(Exception e, AtomicInteger retryCounter, Runnable task) { assert e != null; if (shouldRetry(e) && isStopped() == false) { LOGGER.debug(new ParameterizedMessage("{} error during follow shard task, retrying...", params.getFollowShardId()), e); int currentRetry = retryCounter.incrementAndGet(); long delay = computeDelay(currentRetry, maxRetryDelay.getMillis()); scheduler.accept(TimeValue.timeValueMillis(delay), task); } else { markAsFailed(e); } }	i think that we need to guard against overflow here!
public void testGetAutodetectParams() throws Exception { PlainActionFuture<Boolean> future = new PlainActionFuture<>(); createStateIndexAndAliasIfNecessary(client(), ClusterState.EMPTY_STATE, new IndexNameExpressionResolver(), future); future.get(); String jobId = "test_get_autodetect_params"; Job.Builder job = createJob(jobId, Arrays.asList("fruit", "tea")); String calendarId = "downtime"; Calendar calendar = new Calendar(calendarId, Collections.singletonList(jobId), null); indexCalendars(Collections.singletonList(calendar)); // index the param docs ZonedDateTime now = ZonedDateTime.now(); List<ScheduledEvent> events = new ArrayList<>(); // events in the past should be filtered out events.add(buildScheduledEvent("In the past", now.minusDays(7), now.minusDays(6), calendarId)); events.add(buildScheduledEvent("A_downtime", now.plusDays(1), now.plusDays(2), calendarId)); events.add(buildScheduledEvent("A_downtime2", now.plusDays(8), now.plusDays(9), calendarId)); indexScheduledEvents(events); List<MlFilter> filters = new ArrayList<>(); filters.add(MlFilter.builder("fruit").setItems("apple", "pear").build()); filters.add(MlFilter.builder("tea").setItems("green", "builders").build()); indexFilters(filters); DataCounts earliestCounts = DataCountsTests.createTestInstance(jobId); earliestCounts.setLatestRecordTimeStamp(new Date(1500000000000L)); indexDataCounts(earliestCounts, jobId); DataCounts latestCounts = DataCountsTests.createTestInstance(jobId); latestCounts.setLatestRecordTimeStamp(new Date(1510000000000L)); indexDataCounts(latestCounts, jobId); ModelSizeStats earliestSizeStats = new ModelSizeStats.Builder(jobId).setLogTime(new Date(1500000000000L)).build(); ModelSizeStats latestSizeStats = new ModelSizeStats.Builder(jobId).setLogTime(new Date(1510000000000L)).build(); indexModelSizeStats(earliestSizeStats); indexModelSizeStats(latestSizeStats); job.setModelSnapshotId("snap_1"); ModelSnapshot snapshot = new ModelSnapshot.Builder(jobId).setSnapshotId("snap_1").build(); indexModelSnapshot(snapshot); Quantiles quantiles = new Quantiles(jobId, new Date(), "quantile-state"); indexQuantiles(quantiles); client().admin().indices().prepareRefresh(MlMetaIndex.indexName(), AnomalyDetectorsIndex.jobStateIndexPattern(), AnomalyDetectorsIndex.jobResultsAliasedName(jobId)).get(); AutodetectParams params = getAutodetectParams(job.build(new Date())); // events assertNotNull(params.scheduledEvents()); assertEquals(3, params.scheduledEvents().size()); assertEquals(events.get(0), params.scheduledEvents().get(0)); assertEquals(events.get(1), params.scheduledEvents().get(1)); assertEquals(events.get(2), params.scheduledEvents().get(2)); // filters assertNotNull(params.filters()); assertEquals(2, params.filters().size()); assertTrue(params.filters().contains(filters.get(0))); assertTrue(params.filters().contains(filters.get(1))); // datacounts assertNotNull(params.dataCounts()); assertEquals(latestCounts, params.dataCounts()); // model size stats assertNotNull(params.modelSizeStats()); assertEquals(latestSizeStats, params.modelSizeStats()); // model snapshot assertNotNull(params.modelSnapshot()); assertEquals(snapshot, params.modelSnapshot()); // quantiles assertNotNull(params.quantiles()); assertEquals(quantiles, params.quantiles()); }	i think you should move this into the indexquantiles method, so it's closer to the reason why it's needed. without comments it's very hard to see why this test needs this and the others don't.
private static <K, V> Map<K, V> copyAndAdd(Map<K, V> source, Map.Entry<K, V> newValue) { Map.Entry<K, V>[] entries; if (source.containsKey(newValue.getKey())) { // Replace with new value entries = new Map.Entry[source.size()]; int i = 0; for (Map.Entry entry : source.entrySet()) { if (entry.getKey() == newValue.getKey()) { entries[i] = newValue; } else { entries[i] = entry; } i++; } } else { entries = source.entrySet().toArray(new Map.Entry[source.size() + 1]); entries[entries.length - 1] = newValue; } return Map.ofEntries(entries); }	will we still need this in the new system? i think we won't, right?
@Override public void delete() throws IOException { PlainActionFuture<Void> result = PlainActionFuture.newFuture(); asyncDelete(result); try { result.actionGet(); } catch (Exception e) { throw new IOException("Exception during container delete", e); } }	why recurse by level here and not list all sublevels at once?
private void deleteAndAssertEmpty(BlobPath path) throws Exception { final BlobStoreRepository repo = getRepository(); final PlainActionFuture<Void> future = PlainActionFuture.newFuture(); repo.threadPool().generic().execute(new ActionRunnable<>(future) { @Override protected void doRun() throws Exception { repo.blobStore().blobContainer(path).delete(); future.onResponse(null); } }); future.actionGet(); assertChildren(path, Collections.emptyList()); }	should also assert that the current path does not exist as child in the parent path?
public void testCircuitBreakerBreak() throws Exception { String model1 = "test-circuit-break-model-1"; String model2 = "test-circuit-break-model-2"; String model3 = "test-circuit-break-model-3"; withTrainedModel(model1, 5L); withTrainedModel(model2, 5L); withTrainedModel(model3, 12L); CircuitBreaker circuitBreaker = new CustomCircuitBreaker(11); ModelLoadingService modelLoadingService = new ModelLoadingService(trainedModelProvider, auditor, threadPool, clusterService, trainedModelStatsService, Settings.EMPTY, "test-node", circuitBreaker); // We want to be notified when the models are loaded which happens in a background thread ModelLoadedTracker loadedTracker = new ModelLoadedTracker(Arrays.asList(model1, model2)); for (String modelId : Arrays.asList(model1, model2)) { modelLoadingService.addModelLoadedListener(modelId, loadedTracker.actionListener()); } modelLoadingService.addModelLoadedListener(model3, ActionListener.wrap( r -> fail("Should not have succeeded to load model as breaker should be reached"), e -> assertThat(e, instanceOf(CircuitBreakingException.class)) )); modelLoadingService.clusterChanged(ingestChangedEvent(model1, model2, model3)); // Should have been loaded from the cluster change event but it is unknown in what order // the loading occurred or which models are currently in the cache due to evictions. // Verify that we have at least loaded all three assertBusy(() -> { verify(trainedModelProvider, times(1)).getTrainedModel(eq(model1), eq(false), any()); verify(trainedModelProvider, times(1)).getTrainedModel(eq(model2), eq(false), any()); verify(trainedModelProvider, times(1)).getTrainedModel(eq(model3), eq(false), any()); }); assertBusy(() -> { assertThat(circuitBreaker.getUsed(), equalTo(10L)); assertThat(circuitBreaker.getTrippedCount(), equalTo(1L)); }); modelLoadingService.clusterChanged(ingestChangedEvent(model1)); assertBusy(() -> { assertThat(circuitBreaker.getUsed(), equalTo(5L)); }); }	unused? the modelloadedtracker is required to tell you when a model is loaded but that should not be necessary as assertbusys are used where behaviour is asserted
public static long readLongLE(byte[] arr, int offset) { return (long) LITTLE_ENDIAN_LONG.get(arr, offset); }	shouldn't little-endian be big-endian?
@Override public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) { InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg = (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation; List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets(); List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>(); if ("expression".equals(script.getLang())) { BucketAggregateToDoubleScript.Factory factory = reduceContext.scriptService().compile(script, BucketAggregateToDoubleScript.CONTEXT); BucketAggregateToDoubleScript executableScript = factory.newInstance(); for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) { if (executableScript.execute(scriptArgs(originalAgg, bucket)) == 1.0) { newBuckets.add(bucket); } } } else { BucketAggregateToBooleanScript.Factory factory = reduceContext.scriptService().compile(script, BucketAggregateToBooleanScript.CONTEXT); BucketAggregateToBooleanScript executableScript = factory.newInstance(); for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) { if (executableScript.execute(scriptArgs(originalAgg, bucket))) { newBuckets.add(bucket); } } } return originalAgg.create(newBuckets); }	i don't think we should have any logic like this. script uses should be agnostic to the script language implementation. expressions need to handle returning a boolean value to satisfy this script's api.
@Override public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) { InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket> originalAgg = (InternalMultiBucketAggregation<InternalMultiBucketAggregation, InternalMultiBucketAggregation.InternalBucket>) aggregation; List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = originalAgg.getBuckets(); List<InternalMultiBucketAggregation.InternalBucket> newBuckets = new ArrayList<>(); if ("expression".equals(script.getLang())) { BucketAggregateToDoubleScript.Factory factory = reduceContext.scriptService().compile(script, BucketAggregateToDoubleScript.CONTEXT); BucketAggregateToDoubleScript executableScript = factory.newInstance(); for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) { if (executableScript.execute(scriptArgs(originalAgg, bucket)) == 1.0) { newBuckets.add(bucket); } } } else { BucketAggregateToBooleanScript.Factory factory = reduceContext.scriptService().compile(script, BucketAggregateToBooleanScript.CONTEXT); BucketAggregateToBooleanScript executableScript = factory.newInstance(); for (InternalMultiBucketAggregation.InternalBucket bucket : buckets) { if (executableScript.execute(scriptArgs(originalAgg, bucket))) { newBuckets.add(bucket); } } } return originalAgg.create(newBuckets); }	putting the bucket accessors inside params is how we currently do this, but i don't think it is how we should do this long term. params should be solely for user provided params (they should be read only, since they are parsed once and should be the same across all documents). can you please open an issue to discuss with the search/aggs team how these buckets should be accessible in the future? with contexts we could potentially not use buckets_path at all, and have a buckets variable available in the script. eg: current: "bucket_selector": { "buckets_path": { "foobar": "foo>bar" }, "script": "params.foobar > 200" } potential new script use: "bucket_selector": { "script": "buckets['foo']['bar'].value > 200" }
@Override public GetResult get(Get get, Function<String, Searcher> searcherFactory) throws EngineException { try (ReleasableLock ignored = readLock.acquire()) { ensureOpen(); if (get.realtime()) { VersionValue versionValue = versionMap.getUnderLock(get.uid()); if (versionValue != null) { if (versionValue.isDelete()) { return GetResult.NOT_EXISTS; } if (get.versionType().isVersionConflictForReads(versionValue.getVersion(), get.version())) { Uid uid = Uid.createUid(get.uid().text()); throw new VersionConflictEngineException(shardId, uid.type(), uid.id(), get.versionType().explainConflictForReads(versionValue.getVersion(), get.version())); } refresh("realtime_get"); } } // no version, get the version from the index, we know that we refresh on flush return getFromSearcher(get, searcherFactory); } }	the enum values make no sense in the context of the name. i wonder if we should call it operationage or something like this.
private VersionValue resolveDocVersion(final Operation op) throws IOException { assert incrementVersionLookup(); VersionValue versionValue = versionMap.getUnderLock(op.uid()); if (versionValue == null) { assert incrementIndexVersionLookup(); final long currentVersion = loadCurrentVersionFromIndex(op.uid()); if (currentVersion != Versions.NOT_FOUND) { versionValue = new VersionValue(currentVersion); } } else if (engineConfig.isEnableGcDeletes() && versionValue.isDelete() && (engineConfig.getThreadPool().relativeTimeInMillis() - versionValue.getTime()) > getGcDeletesInMillis()) { versionValue = null; } return versionValue; }	can we get the actual version in the message?
private IndexResult indexIntoLucene(Index index, long seqNo, long newVersion, boolean markDocAsCreated, boolean useLuceneUpdateDocument) throws IOException { assertSequenceNumberBeforeIndexing(index.origin(), seqNo); assert newVersion >= 0; /* Update the document's sequence number and primary term; the sequence number here is derived here from either the sequence * number service if this is on the primary, or the existing document's sequence number if this is on the replica. The * primary term here has already been set, see IndexShard#prepareIndex where the Engine$Index operation is created. */ index.parsedDoc().updateSeqID(seqNo, index.primaryTerm()); index.parsedDoc().version().setLongValue(newVersion); try { if (useLuceneUpdateDocument) { update(index.uid(), index.docs(), indexWriter); } else { // document does not exists, we can optimize for create, but double check if assertions are running assert assertDocDoesNotExist(index, canOptimizeAddDocument(index) == false); index(index.docs(), indexWriter); } versionMap.putUnderLock(index.uid().bytes(), new VersionValue(newVersion)); return new IndexResult(newVersion, seqNo, markDocAsCreated); } catch (Exception ex) { if (indexWriter.getTragicException() == null) { /* There is no tragic event recorded so this must be a document failure. * * The handling inside IW doesn't guarantee that an tragic / aborting exception * will be used as THE tragicEventException since if there are multiple exceptions causing an abort in IW * only one wins. Yet, only the one that wins will also close the IW and in turn fail the engine such that * we can potentially handle the exception before the engine is failed. * Bottom line is that we can only rely on the fact that if it's a document failure then * `indexWriter.getTragicException()` will be null otherwise we have to rethrow and treat it as fatal or rather * non-document failure */ return new IndexResult(ex, Versions.MATCH_ANY, index.seqNo()); } else { throw ex; } } }	only execute this if assertions are enabled?!
private IndexResult indexIntoLucene(Index index, long seqNo, long newVersion, boolean markDocAsCreated, boolean useLuceneUpdateDocument) throws IOException { assertSequenceNumberBeforeIndexing(index.origin(), seqNo); assert newVersion >= 0; /* Update the document's sequence number and primary term; the sequence number here is derived here from either the sequence * number service if this is on the primary, or the existing document's sequence number if this is on the replica. The * primary term here has already been set, see IndexShard#prepareIndex where the Engine$Index operation is created. */ index.parsedDoc().updateSeqID(seqNo, index.primaryTerm()); index.parsedDoc().version().setLongValue(newVersion); try { if (useLuceneUpdateDocument) { update(index.uid(), index.docs(), indexWriter); } else { // document does not exists, we can optimize for create, but double check if assertions are running assert assertDocDoesNotExist(index, canOptimizeAddDocument(index) == false); index(index.docs(), indexWriter); } versionMap.putUnderLock(index.uid().bytes(), new VersionValue(newVersion)); return new IndexResult(newVersion, seqNo, markDocAsCreated); } catch (Exception ex) { if (indexWriter.getTragicException() == null) { /* There is no tragic event recorded so this must be a document failure. * * The handling inside IW doesn't guarantee that an tragic / aborting exception * will be used as THE tragicEventException since if there are multiple exceptions causing an abort in IW * only one wins. Yet, only the one that wins will also close the IW and in turn fail the engine such that * we can potentially handle the exception before the engine is failed. * Bottom line is that we can only rely on the fact that if it's a document failure then * `indexWriter.getTragicException()` will be null otherwise we have to rethrow and treat it as fatal or rather * non-document failure */ return new IndexResult(ex, Versions.MATCH_ANY, index.seqNo()); } else { throw ex; } } }	oh why did you change this to match_any?... worth a comment?
private IndexResult indexIntoLucene(Index index, long seqNo, long newVersion, boolean markDocAsCreated, boolean useLuceneUpdateDocument) throws IOException { assertSequenceNumberBeforeIndexing(index.origin(), seqNo); assert newVersion >= 0; /* Update the document's sequence number and primary term; the sequence number here is derived here from either the sequence * number service if this is on the primary, or the existing document's sequence number if this is on the replica. The * primary term here has already been set, see IndexShard#prepareIndex where the Engine$Index operation is created. */ index.parsedDoc().updateSeqID(seqNo, index.primaryTerm()); index.parsedDoc().version().setLongValue(newVersion); try { if (useLuceneUpdateDocument) { update(index.uid(), index.docs(), indexWriter); } else { // document does not exists, we can optimize for create, but double check if assertions are running assert assertDocDoesNotExist(index, canOptimizeAddDocument(index) == false); index(index.docs(), indexWriter); } versionMap.putUnderLock(index.uid().bytes(), new VersionValue(newVersion)); return new IndexResult(newVersion, seqNo, markDocAsCreated); } catch (Exception ex) { if (indexWriter.getTragicException() == null) { /* There is no tragic event recorded so this must be a document failure. * * The handling inside IW doesn't guarantee that an tragic / aborting exception * will be used as THE tragicEventException since if there are multiple exceptions causing an abort in IW * only one wins. Yet, only the one that wins will also close the IW and in turn fail the engine such that * we can potentially handle the exception before the engine is failed. * Bottom line is that we can only rely on the fact that if it's a document failure then * `indexWriter.getTragicException()` will be null otherwise we have to rethrow and treat it as fatal or rather * non-document failure */ return new IndexResult(ex, Versions.MATCH_ANY, index.seqNo()); } else { throw ex; } } }	maybe a javadoc for this method?
private SortedMap<String, IndexAbstraction> buildIndexAbstractionsLookup() { SortedMap<String, IndexAbstraction> indexAbstractionLookup = new TreeMap<>(); Map<String, DataStream> indexToDataStreamLookup = new HashMap<>(); DataStreamMetadata dsMetadata = (DataStreamMetadata) customs.get(DataStreamMetadata.TYPE); if (dsMetadata != null) { for (DataStream ds : dsMetadata.dataStreams().values()) { IndexAbstraction ia = new IndexAbstraction() { @Override public Type getType() { return Type.DATA_STREAM; } @Override public String getName() { return ds.getName(); } @Override public List<IndexMetadata> getIndices() { List<IndexMetadata> indices = ds.getIndices() .stream() .map(i -> IndexMetadata.builder(i.getName()) .settings(Settings.builder().put(IndexMetadata.SETTING_INDEX_UUID, i.getUUID())) .build()) .collect(Collectors.toList()); return indices; } @Override public IndexMetadata getWriteIndex() { return null; } @Override public DataStream getDataStream() { return null; } @Override public boolean isHidden() { return false; } }; indexAbstractionLookup.put(ds.getName(), ia); for (Index i : ds.getIndices()) { indexToDataStreamLookup.put(i.getName(), ds); } } } for (ObjectCursor<IndexMetadata> cursor : indices.values()) { IndexMetadata indexMetadata = cursor.value; IndexAbstraction existing = indexAbstractionLookup.put( indexMetadata.getIndex().getName(), new IndexAbstraction.Index(indexMetadata, indexToDataStreamLookup.get(indexMetadata.getIndex().getName()))); assert existing == null : "duplicate for " + indexMetadata.getIndex(); for (ObjectObjectCursor<String, AliasMetadata> aliasCursor : indexMetadata.getAliases()) { AliasMetadata aliasMetadata = aliasCursor.value; indexAbstractionLookup.compute(aliasMetadata.getAlias(), (aliasName, alias) -> { if (alias == null) { return new IndexAbstraction.Alias(aliasMetadata, indexMetadata); } else { assert alias.getType() == IndexAbstraction.Type.ALIAS : alias.getClass().getName(); ((IndexAbstraction.Alias) alias).addIndex(indexMetadata); return alias; } }); } } indexAbstractionLookup.values().stream() .filter(indexAbstraction -> indexAbstraction.getType() == IndexAbstraction.Type.ALIAS) .forEach(alias -> ((IndexAbstraction.Alias) alias).computeAndValidateAliasProperties()); return indexAbstractionLookup; }	perhaps open a separate pr for the change to this method? this adds additional validation outside concrete index to data stream lookup.
@Override public Query termQuery(Object value, QueryShardContext context) { throw new UnsupportedOperationException( "Field [" + name() + "] of type [" + typeName() + "] doesn't support queries"); } } private DenseVectorFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); assert fieldType.indexOptions() == IndexOptions.NONE; } @Override protected DenseVectorFieldMapper clone() { return (DenseVectorFieldMapper) super.clone(); } @Override public DenseVectorFieldType fieldType() { return (DenseVectorFieldType) super.fieldType(); } @Override public void parse(ParseContext context) throws IOException { if (context.externalValueSet()) { throw new MapperParsingException("Field [" + name() + "] of type [" + typeName() + "] can't be used in multi-fields"); } int dims = fieldType().dims(); //number of vector dimensions // encode array of floats as array of integers and store into buf // this code is here and not int the VectorEncoderDecoder so not to create extra arrays byte[] buf = new byte[dims * INT_BYTES]; int offset = 0; int dim = 0; for (Token token = context.parser().nextToken(); token != Token.END_ARRAY; token = context.parser().nextToken()) { if (dim++ >= dims) { throw new MapperParsingException("Field [" + name() + "] of type [" + typeName() + "] of doc [" + context.sourceToParse().id() + "] has exceeded the number of dimensions [" + dims + "] defined in mapping"); } ensureExpectedToken(Token.VALUE_NUMBER, token, context.parser()::getTokenLocation); float value = context.parser().floatValue(true); int intValue = Float.floatToIntBits(value); buf[offset++] = (byte) (intValue >> 24); buf[offset++] = (byte) (intValue >> 16); buf[offset++] = (byte) (intValue >> 8); buf[offset++] = (byte) intValue; } if (dim != dims) { throw new MapperParsingException("Field [" + name() + "] of type [" + typeName() + "] of doc [" + context.sourceToParse().id() + "] has number of dimensions [" + dim + "] less than defined in the mapping [" + dims +"]"); } BinaryDocValuesField field = new BinaryDocValuesField(fieldType().name(), new BytesRef(buf, 0, offset)); if (context.doc().getByKey(fieldType().name()) != null) { throw new MapperParsingException("Field [" + name() + "] of type [" + typeName() + "] doesn't not support indexing multiple values for the same field in the same document"); } context.doc().addWithKey(fieldType().name(), field); } @Override protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException { super.doXContentBody(builder, includeDefaults, params); builder.field("dims", fieldType().dims()); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) { throw new AssertionError("parse is implemented directly"); }	it seems like illegalargumentexception would make more sense here, since we are parsing a document and not a mapping.
*/ public static float[] decodeDenseVector(BytesRef vectorBR) { if (vectorBR == null) { throw new IllegalArgumentException("A document doesn't have a value for a vector field!"); } int dimCount = vectorBR.length / INT_BYTES; float[] vector = new float[dimCount]; int offset = vectorBR.offset; for (int dim = 0; dim < dimCount; dim++) { int intValue = ((vectorBR.bytes[offset++] & 0xFF) << 24) | ((vectorBR.bytes[offset++] & 0xFF) << 16) | ((vectorBR.bytes[offset++] & 0xFF) << 8) | (vectorBR.bytes[offset++] & 0xFF); vector[dim] = Float.intBitsToFloat(intValue); } return vector; }	for my understanding, why did we change this code to use offset++?
public static double dotProduct(List<Number> queryVector, VectorScriptDocValues.DenseVectorScriptDocValues dvs){ BytesRef value = dvs.getEncodedValue(); if (value == null) return 0; float[] docVector = VectorEncoderDecoder.decodeDenseVector(value); if (queryVector.size() != docVector.length) { throw new IllegalArgumentException("Can't calculate dotProduct! The number of dimensions of the query vector [" + queryVector.size() + "] is different from the documents' vectors [" + docVector.length + "]!"); } return intDotProduct(queryVector, docVector); }	it is too bad that we need to do this check for every document, but i don't see a better way (given that we're exposing this functionality through a script function).
public static double dotProduct(List<Number> queryVector, VectorScriptDocValues.DenseVectorScriptDocValues dvs){ BytesRef value = dvs.getEncodedValue(); if (value == null) return 0; float[] docVector = VectorEncoderDecoder.decodeDenseVector(value); if (queryVector.size() != docVector.length) { throw new IllegalArgumentException("Can't calculate dotProduct! The number of dimensions of the query vector [" + queryVector.size() + "] is different from the documents' vectors [" + docVector.length + "]!"); } return intDotProduct(queryVector, docVector); }	a small comment, maybe we could remove one explanation point? :)
void loadMappings(ActionListener<List<ExpressionRoleMapping>> listener) { if (securityIndex.isIndexUpToDate() == false) { listener.onFailure(new IllegalStateException( "Security index is not on the current version - the native realm will not be operational until " + "the upgrade API is run on the security index")); return; } final QueryBuilder query = QueryBuilders.termQuery(DOC_TYPE_FIELD, DOC_TYPE_ROLE_MAPPING); final Supplier<ThreadContext.StoredContext> supplier = client.threadPool().getThreadContext().newRestorableContext(false); try (ThreadContext.StoredContext ignore = client.threadPool().getThreadContext().stashWithOrigin(SECURITY_ORIGIN)) { SearchRequest request = client.prepareSearch(SECURITY_INDEX_NAME) .setScroll(DEFAULT_KEEPALIVE_SETTING.get(settings)) .setQuery(query) .setSize(1000) .setFetchSource(true) .request(); request.indicesOptions().ignoreUnavailable(); ScrollHelper.fetchAllByEntity(client, request, new ContextPreservingActionListener<>(supplier, ActionListener.wrap((Collection<ExpressionRoleMapping> mappings) -> listener.onResponse(mappings.stream().filter(Objects::nonNull).collect(Collectors.toList())), ex -> { logger.error(new ParameterizedMessage("failed to load role mappings from index [{}] skipping all mappings.", SECURITY_INDEX_NAME), ex); listener.onResponse(Collections.emptyList()); })), doc -> buildMapping(getNameFromId(doc.getId()), doc.getSourceRef())); } }	is it related to types removal?
private Tuple<String, Settings> loadMappingAndSettingsSourceFromTemplate() { final byte[] template = TemplateUtils.loadTemplate("/" + SECURITY_TEMPLATE_NAME + ".json", Version.CURRENT.toString(), SecurityIndexManager.TEMPLATE_VERSION_PATTERN).getBytes(StandardCharsets.UTF_8); PutIndexTemplateRequest request = new PutIndexTemplateRequest(SECURITY_TEMPLATE_NAME).source(template, XContentType.JSON); final byte[] typeMappingSource = request.mappings().get(MapperService.SINGLE_MAPPING_NAME).getBytes(StandardCharsets.UTF_8); Map<String, Object> typeMappingSourceMap = XContentHelper .convertToMap(new BytesArray(typeMappingSource, 0, typeMappingSource.length), true, XContentType.JSON).v2(); XContentBuilder builder = null; try { builder = XContentFactory.jsonBuilder(); builder.value(typeMappingSourceMap.get(MapperService.SINGLE_MAPPING_NAME)); return new Tuple<>(Strings.toString(builder), request.settings()); } catch (IOException e) { throw ExceptionsHelper.convertToRuntime(e); } }	the above code looks to me as we are rewriting the type of the mappings in the template to _doc. however the template already has _doc as a type name in the template so this shouldn't be necessary? am i missing something?
private boolean tryRelocateShard(ModelNode minNode, ModelNode maxNode, String idx) { final ModelIndex index = maxNode.getIndex(idx); Decision decision = null; if (index != null) { if (logger.isTraceEnabled()) { logger.trace("Try relocating shard for index index [{}] from node [{}] to node [{}]", idx, maxNode.getNodeId(), minNode.getNodeId()); } ShardRouting candidate = null; final AllocationDeciders deciders = allocation.deciders(); final List<ShardRouting> shardRoutings = new ArrayList<>(index.numShards()); for (ShardRouting shard : index) { if (shard.started()) { // skip initializing, unassigned and relocating shards we can't relocate them anyway if (maxNode.containsShard(shard)) { shardRoutings.add(shard); } } } // look for a relocation candidate, in descending order of shard id so that the decision is deterministic shardRoutings.sort(Comparator.comparing(ShardRouting::id).reversed()); for (ShardRouting shard : shardRoutings) { Decision allocationDecision = deciders.canAllocate(shard, minNode.getRoutingNode(), allocation); Decision rebalanceDecision = deciders.canRebalance(shard, allocation); if (((allocationDecision.type() == Type.YES) || (allocationDecision.type() == Type.THROTTLE)) && ((rebalanceDecision.type() == Type.YES) || (rebalanceDecision.type() == Type.THROTTLE))) { candidate = shard; decision = new Decision.Multi().add(allocationDecision).add(rebalanceDecision); break; } } if (candidate != null) { /* allocate on the model even if not throttled */ maxNode.removeShard(candidate); long shardSize = allocation.clusterInfo().getShardSize(candidate, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE); if (decision.type() == Type.YES) { /* only allocate on the cluster if we are not throttled */ logger.debug("Relocate shard [{}] from node [{}] to node [{}]", candidate, maxNode.getNodeId(), minNode.getNodeId()); /* now allocate on the cluster */ minNode.addShard(routingNodes.relocateShard(candidate, minNode.getNodeId(), shardSize, allocation.changes()).v1()); return true; } else { assert decision.type() == Type.THROTTLE; minNode.addShard(candidate.relocate(minNode.getNodeId(), shardSize)); } } } if (logger.isTraceEnabled()) { logger.trace("Couldn't find shard to relocate from node [{}] to node [{}] allocation decision [{}]", maxNode.getNodeId(), minNode.getNodeId(), decision == null ? "NO" : decision.type().name()); } return false; }	i think this could now translate into a nice stream one liner? index.stream().filter(shardrouting::started).filter(maxnode::containsshard).sorted(comparator.comparing(shardrouting::id).reversed()) then either collect or do the final filtering and findfirst?
private void performStateRecovery(final boolean enforceRecoverAfterTime, final String reason) { if (enforceRecoverAfterTime && recoverAfterTime != null) { if (scheduledRecovery.compareAndSet(false, true)) { logger.info("delaying initial state recovery for [{}]. {}", recoverAfterTime, reason); threadPool.schedule(recoverAfterTime, ThreadPool.Names.GENERIC, () -> { if (recovered.compareAndSet(false, true)) { logger.info("recover_after_time [{}] elapsed. performing state recovery...", recoverAfterTime); recoveryRunnable.run(); } }); } } else { if (recovered.compareAndSet(false, true)) { threadPool.generic().execute(new AbstractRunnable() { @Override public void onFailure(final Exception e) { logger.warn("Recovery failed", e); // we reset `recovered` in the listener don't reset it here otherwise there might be a race // that resets it to false while a new recover is already running? GatewayService.this.onFailure("state recovery failed: " + e.getMessage()); } @Override protected void doRun() { recoveryRunnable.run(); } }); } } }	pass in clustersettings instead of clusterservice.
private void performStateRecovery(final boolean enforceRecoverAfterTime, final String reason) { if (enforceRecoverAfterTime && recoverAfterTime != null) { if (scheduledRecovery.compareAndSet(false, true)) { logger.info("delaying initial state recovery for [{}]. {}", recoverAfterTime, reason); threadPool.schedule(recoverAfterTime, ThreadPool.Names.GENERIC, () -> { if (recovered.compareAndSet(false, true)) { logger.info("recover_after_time [{}] elapsed. performing state recovery...", recoverAfterTime); recoveryRunnable.run(); } }); } } else { if (recovered.compareAndSet(false, true)) { threadPool.generic().execute(new AbstractRunnable() { @Override public void onFailure(final Exception e) { logger.warn("Recovery failed", e); // we reset `recovered` in the listener don't reset it here otherwise there might be a race // that resets it to false while a new recover is already running? GatewayService.this.onFailure("state recovery failed: " + e.getMessage()); } @Override protected void doRun() { recoveryRunnable.run(); } }); } } }	this now moves the execution of this (possibly computationally expensive) code to the cluster state update task. i'm not sure yet what kind of impact that has, and wonder whether we should do more of this for zen2 during startup instead of a post-election step. the advantage of running this process at startup is that we can fail the node if anything goes wrong here, instead of just logging. the current solution also has the disadvantage that it has potentially already published a state that is unrecoverable / cannot upgrade / archive invalid settings, possibly poisoning other nodes. can you see what it would take to move this (updaters.closebadindices + updaters.upgradeandarchiveunknownorinvalidsettings) to the place where we start (currently the place called gatewaymetastate.setlocalnode). we can pass indicesservice as constructor argument to gatewaymetastate in the node class so have all deps available to run these static methods in gatewaymetastate's setlocalnode method (which obviously will need to be renamed).
private void performStateRecovery(final boolean enforceRecoverAfterTime, final String reason) { if (enforceRecoverAfterTime && recoverAfterTime != null) { if (scheduledRecovery.compareAndSet(false, true)) { logger.info("delaying initial state recovery for [{}]. {}", recoverAfterTime, reason); threadPool.schedule(recoverAfterTime, ThreadPool.Names.GENERIC, () -> { if (recovered.compareAndSet(false, true)) { logger.info("recover_after_time [{}] elapsed. performing state recovery...", recoverAfterTime); recoveryRunnable.run(); } }); } } else { if (recovered.compareAndSet(false, true)) { threadPool.generic().execute(new AbstractRunnable() { @Override public void onFailure(final Exception e) { logger.warn("Recovery failed", e); // we reset `recovered` in the listener don't reset it here otherwise there might be a race // that resets it to false while a new recover is already running? GatewayService.this.onFailure("state recovery failed: " + e.getMessage()); } @Override protected void doRun() { recoveryRunnable.run(); } }); } } }	we log already in gatewayservice.this.onfailure. why this extra logging here? this onfailure method could possibly be called because we failed to publish the cluster state, e.g. because of another concurrent election being triggered and another node becoming master. certainly not a reason to log at error level. the error logging should probably only be for the case where closing bad indices or upgrading / archiving bad settings would horribly fail?
public List<Route> routes() { return Collections.singletonList( new Route(DELETE, MachineLearning.BASE_PATH + "_delete_expired_data/{" + Fields.JOB_ID.getPreferredName() + "}")); }	we need to make sure that we take the empty route in replacedroutes and put it here before it is removed.
@Override protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) throws IOException { DeleteExpiredDataAction.Request request; if (restRequest.hasContent()) { request = DeleteExpiredDataAction.Request.PARSER.apply(restRequest.contentParser(), null); } else { request = new DeleteExpiredDataAction.Request(); String perSecondParam = restRequest.param(DeleteExpiredDataAction.Request.REQUESTS_PER_SECOND.getPreferredName()); if (perSecondParam != null) { try { request.setRequestsPerSecond(Float.parseFloat(perSecondParam)); } catch (NumberFormatException e) { throw new IllegalArgumentException("Failed to parse float parameter [" + DeleteExpiredDataAction.Request.REQUESTS_PER_SECOND.getPreferredName() + "] with value [" + perSecondParam + "]", e); } } String timeoutParam = restRequest.param(DeleteExpiredDataAction.Request.TIMEOUT.getPreferredName()); if (timeoutParam != null) { request.setTimeout(restRequest.paramAsTime(timeoutParam, null)); } } String jobId = restRequest.param(Fields.JOB_ID.getPreferredName()); if (Strings.isNullOrEmpty(jobId) == false) { request.setJobId(jobId); } return channel -> client.execute(DeleteExpiredDataAction.INSTANCE, request, new RestToXContentListener<>(channel)); }	this should pull in the field name from the job config object. not the data frame analytics fields.
public synchronized <A,B> void addAffixUpdateConsumer(Setting.AffixSetting<A> settingA, Setting.AffixSetting<B> settingB, BiConsumer<String, Tuple<A, B>> consumer, BiConsumer<String, Tuple<A, B>> validator) { // it would be awesome to have a generic way to do that ie. a set of settings that map to an object with a builder // down the road this would be nice to have! ensureSettingIsRegistered(settingA); ensureSettingIsRegistered(settingB); SettingUpdater<Map<SettingUpdater<A>, A>> affixUpdaterA = settingA.newAffixUpdater((a,b)-> {}, logger, (a,b)-> {}); SettingUpdater<Map<SettingUpdater<B>, B>> affixUpdaterB = settingB.newAffixUpdater((a,b)-> {}, logger, (a,b)-> {}); addSettingsUpdater(new SettingUpdater<Map<String, Tuple<A, B>>>() { @Override public boolean hasChanged(Settings current, Settings previous) { return affixUpdaterA.hasChanged(current, previous) || affixUpdaterB.hasChanged(current, previous); } @Override public Map<String, Tuple<A, B>> getValue(Settings current, Settings previous) { Map<String, Tuple<A, B>> map = new HashMap<>(); BiConsumer<String, A> aConsumer = (key, value) -> { map.put(key, new Tuple<>(value, settingB.getConcreteSettingForNamespace(key).get(current))); }; BiConsumer<String, B> bConsumer = (key, value) -> { Tuple<A, B> abTuple = map.get(key); if (abTuple != null) { map.put(key, new Tuple<>(abTuple.v1(), value)); } else { map.put(key, new Tuple<>(settingA.getConcreteSettingForNamespace(key).get(current), value)); } }; SettingUpdater<Map<SettingUpdater<A>, A>> affixUpdaterA = settingA.newAffixUpdater(aConsumer, logger, (a,b) ->{}); SettingUpdater<Map<SettingUpdater<B>, B>> affixUpdaterB = settingB.newAffixUpdater(bConsumer, logger, (a,b) ->{}); affixUpdaterA.apply(current, previous); affixUpdaterB.apply(current, previous); for (Map.Entry<String, Tuple<A, B>> entry : map.entrySet()) { validator.accept(entry.getKey(), entry.getValue()); } return Collections.unmodifiableMap(map); } @Override public void apply(Map<String, Tuple<A, B>> values, Settings current, Settings previous) { for (Map.Entry<String, Tuple<A, B>> entry : values.entrySet()) { consumer.accept(entry.getKey(), entry.getValue()); } } }); }	should we assert that there is not already a key for this in the map?
public synchronized <A,B> void addAffixUpdateConsumer(Setting.AffixSetting<A> settingA, Setting.AffixSetting<B> settingB, BiConsumer<String, Tuple<A, B>> consumer, BiConsumer<String, Tuple<A, B>> validator) { // it would be awesome to have a generic way to do that ie. a set of settings that map to an object with a builder // down the road this would be nice to have! ensureSettingIsRegistered(settingA); ensureSettingIsRegistered(settingB); SettingUpdater<Map<SettingUpdater<A>, A>> affixUpdaterA = settingA.newAffixUpdater((a,b)-> {}, logger, (a,b)-> {}); SettingUpdater<Map<SettingUpdater<B>, B>> affixUpdaterB = settingB.newAffixUpdater((a,b)-> {}, logger, (a,b)-> {}); addSettingsUpdater(new SettingUpdater<Map<String, Tuple<A, B>>>() { @Override public boolean hasChanged(Settings current, Settings previous) { return affixUpdaterA.hasChanged(current, previous) || affixUpdaterB.hasChanged(current, previous); } @Override public Map<String, Tuple<A, B>> getValue(Settings current, Settings previous) { Map<String, Tuple<A, B>> map = new HashMap<>(); BiConsumer<String, A> aConsumer = (key, value) -> { map.put(key, new Tuple<>(value, settingB.getConcreteSettingForNamespace(key).get(current))); }; BiConsumer<String, B> bConsumer = (key, value) -> { Tuple<A, B> abTuple = map.get(key); if (abTuple != null) { map.put(key, new Tuple<>(abTuple.v1(), value)); } else { map.put(key, new Tuple<>(settingA.getConcreteSettingForNamespace(key).get(current), value)); } }; SettingUpdater<Map<SettingUpdater<A>, A>> affixUpdaterA = settingA.newAffixUpdater(aConsumer, logger, (a,b) ->{}); SettingUpdater<Map<SettingUpdater<B>, B>> affixUpdaterB = settingB.newAffixUpdater(bConsumer, logger, (a,b) ->{}); affixUpdaterA.apply(current, previous); affixUpdaterB.apply(current, previous); for (Map.Entry<String, Tuple<A, B>> entry : map.entrySet()) { validator.accept(entry.getKey(), entry.getValue()); } return Collections.unmodifiableMap(map); } @Override public void apply(Map<String, Tuple<A, B>> values, Settings current, Settings previous) { for (Map.Entry<String, Tuple<A, B>> entry : values.entrySet()) { consumer.accept(entry.getKey(), entry.getValue()); } } }); }	as far as i understand, this is so that it gives us the default value. can you assert that there is indeed no value defined for this setting, i.e. settinga.getconcretesettingfornamespace(key).exists(current) == false?
public void testTupleAffixUpdateConsumer() { Setting.AffixSetting<Integer> intSetting = Setting.affixKeySetting("foo.", "bar", (k) -> Setting.intSetting(k, 1, Property.Dynamic, Property.NodeScope)); Setting.AffixSetting<List<Integer>> listSetting = Setting.affixKeySetting("foo.", "list", (k) -> Setting.listSetting(k, Arrays.asList("1"), Integer::parseInt, Property.Dynamic, Property.NodeScope)); AbstractScopedSettings service = new ClusterSettings(Settings.EMPTY,new HashSet<>(Arrays.asList(intSetting, listSetting))); Map<String, Tuple<List<Integer>, Integer>> results = new HashMap<>(); BiConsumer<String, Tuple<List<Integer>, Integer>> listConsumer = results::put; service.addAffixUpdateConsumer(listSetting, intSetting, listConsumer, (s, k) -> { if (k.v1().isEmpty() && k.v2() == 2) { throw new IllegalArgumentException("boom"); } }); assertEquals(0, results.size()); service.applySettings(Settings.builder() .put("foo.test.bar", 2) .put("foo.test_1.bar", 7) .putList("foo.test.list", "16", "17") .putList("foo.test_1.list", "18", "19", "20") .build()); assertEquals(2, results.get("test").v2().intValue()); assertEquals(7, results.get("test_1").v2().intValue()); assertEquals(Arrays.asList(16, 17), results.get("test").v1()); assertEquals(Arrays.asList(18, 19, 20), results.get("test_1").v1()); assertEquals(2, results.size()); results.clear(); service.applySettings(Settings.builder() .put("foo.test.bar", 2) .put("foo.test_1.bar", 7) .putList("foo.test.list", "16", "17") .putNull("foo.test_1.list") // removed .build()); assertNull("test wasn't changed", results.get("test")); assertEquals(1, results.get("test_1").v1().size()); assertEquals(Arrays.asList(1), results.get("test_1").v1()); assertEquals(7, results.get("test_1").v2().intValue()); assertEquals(1, results.size()); results.clear(); service.applySettings(Settings.builder() .put("foo.test.bar", 2) .put("foo.test_1.bar", 7) .putList("foo.test.list", "16", "17") .putList("foo.test_2.list", "5", "6") // added .build()); assertNull("test wasn't changed", results.get("test")); assertNull("test_1 wasn't changed", results.get("test_1")); assertEquals(2, results.get("test_2").v1().size()); assertEquals(Arrays.asList(5, 6), results.get("test_2").v1()); assertEquals(1, results.get("test_2").v2().intValue()); assertEquals(1, results.size()); results.clear(); service.applySettings(Settings.builder() .put("foo.test.bar", 4) // modified .put("foo.test_1.bar", 7) .putList("foo.test.list", "16", "17") .putList("foo.test_2.list", "5", "6") .build()); assertNull("test_1 wasn't changed", results.get("test_1")); assertNull("test_2 wasn't changed", results.get("test_2")); assertEquals(2, results.get("test").v1().size()); assertEquals(Arrays.asList(16, 17), results.get("test").v1()); assertEquals(4, results.get("test").v2().intValue()); assertEquals(1, results.size()); results.clear(); IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () -> service.applySettings(Settings.builder() .put("foo.test.bar", 2) // modified to trip validator .put("foo.test_1.bar", 7) .putList("foo.test.list") // modified to trip validator .putList("foo.test_2.list", "5", "6") .build()) ); assertEquals("boom", iae.getMessage()); assertEquals(0, results.size()); }	can you randomize the order of the settings, just so to check that that does not matter?
@Override public boolean validateVersionForReads(long version) { return version >= 0L || version == Versions.MATCH_ANY; } }, /** * Warning: this version type should be used with care. Concurrent indexing may result in loss of data on replicas */ FORCE((byte) 3) { @Override public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) { if (currentVersion == Versions.NOT_SET) { return false; } if (currentVersion == Versions.NOT_FOUND) { return false; } if (expectedVersion == Versions.MATCH_ANY) { return true; } return false; } @Override public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) { return false; } @Override public long updateVersion(long currentVersion, long expectedVersion) { return expectedVersion; } @Override public boolean validateVersionForWrites(long version) { return version >= 0L; } @Override public boolean validateVersionForReads(long version) { return version >= 0L || version == Versions.MATCH_ANY; } }; private final byte value; private static final VersionType PROTOTYPE = INTERNAL; VersionType(byte value) { this.value = value; } public byte getValue() { return value; } /** * Checks whether the current version conflicts with the expected version, based on the current version type. * * @return true if versions conflict false o.w. */ public abstract boolean isVersionConflictForWrites(long currentVersion, long expectedVersion); /** * Checks whether the current version conflicts with the expected version, based on the current version type. * * @return true if versions conflict false o.w. */ public abstract boolean isVersionConflictForReads(long currentVersion, long expectedVersion); /** * Returns the new version for a document, based on its current one and the specified in the request * * @return new version */ public abstract long updateVersion(long currentVersion, long expectedVersion); /** * validate the version is a valid value for this type when writing. * * @return true if valid, false o.w */ public abstract boolean validateVersionForWrites(long version); /** * validate the version is a valid value for this type when reading. * * @return true if valid, false o.w */ public abstract boolean validateVersionForReads(long version); /** * Some version types require different semantics for primary and replicas. This version allows * the type to override the default behavior. */ public VersionType versionTypeForReplicationAndRecovery() { return this; } public static VersionType fromString(String versionType) { if ("internal".equals(versionType)) { return INTERNAL; } else if ("external".equals(versionType)) { return EXTERNAL; } else if ("external_gt".equals(versionType)) { return EXTERNAL; } else if ("external_gte".equals(versionType)) { return EXTERNAL_GTE; } else if ("force".equals(versionType)) { return FORCE; } throw new IllegalArgumentException("No version type match [" + versionType + "]"); } public static VersionType fromString(String versionType, VersionType defaultVersionType) { if (versionType == null) { return defaultVersionType; } return fromString(versionType); } public static VersionType fromValue(byte value) { if (value == 0) { return INTERNAL; } else if (value == 1) { return EXTERNAL; } else if (value == 2) { return EXTERNAL_GTE; } else if (value == 3) { return FORCE; } throw new IllegalArgumentException("No version type match [" + value + "]"); } @Override public VersionType readFrom(StreamInput in) throws IOException { return VersionType.values()[in.readVInt()]; } public static VersionType readVersionTypeFrom(StreamInput in) throws IOException { return PROTOTYPE.readFrom(in); }	can we check the value we are reading here? an assertion would be ok
@Override protected Query doToQuery(QueryShardContext context) throws IOException { MoreLikeThisQuery mltQuery = new MoreLikeThisQuery(); // set similarity mltQuery.setSimilarity(context.searchSimilarity()); // set query parameters mltQuery.setMaxQueryTerms(maxQueryTerms); mltQuery.setMinTermFrequency(minTermFreq); mltQuery.setMinDocFreq(minDocFreq); mltQuery.setMaxDocFreq(maxDocFreq); mltQuery.setMinWordLen(minWordLength); mltQuery.setMaxWordLen(maxWordLength); mltQuery.setMinimumShouldMatch(minimumShouldMatch); if (stopWords != null) { mltQuery.setStopWords(new HashSet<>(Arrays.asList(stopWords))); } // sets boost terms if (boostTerms != 0) { mltQuery.setBoostTerms(true); mltQuery.setBoostTermsFactor(boostTerms); } // set analyzer Analyzer analyzerObj = context.analysisService().analyzer(analyzer); if (analyzerObj == null) { analyzerObj = context.mapperService().searchAnalyzer(); } mltQuery.setAnalyzer(analyzerObj); // set like text fields boolean useDefaultField = (fields == null); List<String> moreLikeFields = new ArrayList<>(); if (useDefaultField) { moreLikeFields = Collections.singletonList(context.defaultField()); } else { for (String field : fields) { MappedFieldType fieldType = context.fieldMapper(field); moreLikeFields.add(fieldType == null ? field : fieldType.names().indexName()); } } // possibly remove unsupported fields removeUnsupportedFields(moreLikeFields, analyzerObj, failOnUnsupportedField); if (moreLikeFields.isEmpty()) { return null; } mltQuery.setMoreLikeFields(moreLikeFields.toArray(Strings.EMPTY_ARRAY)); // handle like texts if (!likeTexts.isEmpty()) { mltQuery.setLikeText(likeTexts); } if (!unlikeTexts.isEmpty()) { mltQuery.setUnlikeText(unlikeTexts); } // handle items if (!likeItems.isEmpty()) { return handleItems(context, mltQuery, likeItems, unlikeItems, include, moreLikeFields, useDefaultField); } else { return mltQuery; } }	can you use == false instead of !
@Override protected Expression canonicalize() { Expression canonicalChild = field().canonical(); if (canonicalChild instanceof UnaryNegateable) { return ((UnaryNegateable) canonicalChild).negate(); } return this; }	this doesn't look good as it essentially removes not over binary logical negateable not (x and y).
public void testDeprecationRouteThrottling() throws Exception { final Request deprecatedRequest = deprecatedRequest("GET", "xOpaqueId-testDeprecationRouteThrottling"); assertOK(client().performRequest(deprecatedRequest)); assertOK(client().performRequest(deprecatedRequest)); final Request postRequest = deprecatedRequest("POST", "xOpaqueId-testDeprecationRouteThrottling"); assertOK(client().performRequest(postRequest)); assertBusy(() -> { List<Map<String, Object>> documents = getIndexedDeprecations(); logger.warn(documents); assertThat(documents, hasSize(3)); assertThat( documents, containsInAnyOrder( allOf( hasEntry(KEY_FIELD_NAME, "deprecated_route_POST_/_test_cluster/deprecated_settings"), hasEntry("message", "[/_test_cluster/deprecated_settings] exists for deprecated tests") ), allOf( hasEntry(KEY_FIELD_NAME, "deprecated_route_GET_/_test_cluster/deprecated_settings"), hasEntry("message", "[/_test_cluster/deprecated_settings] exists for deprecated tests") ), allOf( hasEntry(KEY_FIELD_NAME, "deprecated_settings"), hasEntry("message", "[deprecated_settings] usage is deprecated. use [settings] instead") ) ) ); }, 30, TimeUnit.SECONDS); }	@pugnascotia addressing https://github.com/elastic/elasticsearch/pull/78319/files#r723033047 here i think it is fine (other tests log those documents too). we only store the server logs when a test failed. so this will help to debug why the test failed
static List<Node> selectHosts(NodeTuple<List<Node>> nodeTuple, Map<HttpHost, DeadHostState> blacklist, AtomicInteger lastNodeIndex, long now, NodeSelector nodeSelector) throws IOException { /* * Sort the nodes into living and dead lists. */ List<Node> livingNodes = new ArrayList<>(nodeTuple.nodes.size() - blacklist.size()); List<DeadNodeAndRevival> deadNodes = new ArrayList<>(blacklist.size()); for (Node node : nodeTuple.nodes) { DeadHostState deadness = blacklist.get(node.getHost()); if (deadness == null) { livingNodes.add(node); continue; } long nanosUntilRevival = deadness.nanosUntilRevival(now); if (nanosUntilRevival > 0) { livingNodes.add(node); continue; } deadNodes.add(new DeadNodeAndRevival(node, nanosUntilRevival)); } if (false == livingNodes.isEmpty()) { /* * Normal state: there is at least one living node. If the * selector is ok with any over the living nodes then use them * for the request. */ List<Node> selectedLivingNodes = new ArrayList<>(livingNodes); nodeSelector.select(selectedLivingNodes); if (false == selectedLivingNodes.isEmpty()) { /* * Rotate the list so subsequent requests will prefer the * nodes in a different order. */ Collections.rotate(selectedLivingNodes, lastNodeIndex.getAndIncrement()); return selectedLivingNodes; } } /* * Last resort: If there are no good nodes to use, either because * the selector rejected all the living nodes or because there aren't * any living ones. Either way, we want to revive a single dead node * that the NodeSelectors are OK with. We do this by sorting the dead * nodes by their revival time and passing them through the * NodeSelector so it can have its say in which nodes are ok and their * ordering. If the selector is ok with any of the nodes then use just * the first one in the list because we only want to revive a single * node. */ if (false == deadNodes.isEmpty()) { final List<DeadNodeAndRevival> selectedDeadNodes = new ArrayList<>(deadNodes); /* * We'd like NodeSelectors to remove items directly from deadNodes * so we can find the minimum after it is filtered without having * to compare many things. This saves us a sort on the unfiltered * list. */ nodeSelector.select(new Iterable<Node>() { @Override public Iterator<Node> iterator() { return new Adapter(selectedDeadNodes.iterator()); } }); if (false == selectedDeadNodes.isEmpty()) { return singletonList(Collections.min(selectedDeadNodes).node); } } throw new IOException("NodeSelector [" + nodeSelector + "] rejected all nodes, " + "living " + livingNodes + " and dead " + deadNodes); }	maybe silly question: how does min help compared to sorting? doesn't min iterate over all the elements?
public void testMatchNoDocsDeprecatedInterval() throws IOException { testSearchCase(new MatchNoDocsQuery(), DATASET, aggregation -> aggregation.dateHistogramInterval(DateHistogramInterval.YEAR).field(AGGREGABLE_DATE), histogram -> { assertEquals(0, histogram.getBuckets().size()); assertFalse(AggregationInspectionHelper.hasValue(histogram)); }, false ); assertWarnings("[interval] on [date_histogram] is deprecated, use [fixed_interval] or [calendar_interval] in the future."); }	this test needs a comment; it's a subtle bug, and it's not super clear how this test catches it.
public void testDefaultFieldsForDateHistograms() { final Random random = random(); DateHistogramGroupConfig dateHistogramGroupConfig = randomDateHistogramGroupConfig(random); HistogramGroupConfig histogramGroupConfig1 = randomHistogramGroupConfig(random); List<MetricConfig> metrics = new ArrayList<>(randomMetricsConfigs(random)); for (String histoField : histogramGroupConfig1.getFields()) { metrics.add(new MetricConfig(histoField, Arrays.asList("max"))); } GroupConfig groupConfig = new GroupConfig(dateHistogramGroupConfig, histogramGroupConfig1, null); RollupJobConfig rollupJobConfig = new RollupJobConfig( randomAsciiAlphanumOfLengthBetween(random, 1, 20), "indexes_*", "rollup_" + randomAsciiAlphanumOfLengthBetween(random, 1, 20), randomCron(), randomIntBetween(1, 10), groupConfig, metrics, null); Set<String> metricFields = rollupJobConfig.getMetricsConfig().stream().map(MetricConfig::getField).collect(Collectors.toSet()); assertThat(dateHistogramGroupConfig.getField(), isIn(metricFields)); List<String> histoFields = Arrays.asList(histogramGroupConfig1.getFields()); rollupJobConfig.getMetricsConfig().forEach(metricConfig -> { if (histoFields.contains(metricConfig.getField())) { // Since it is explicitly included, the defaults should not be added assertThat(metricConfig.getMetrics(), containsInAnyOrder("max")); } if (metricConfig.getField().equals(dateHistogramGroupConfig.getField())) { assertThat(metricConfig.getMetrics(), containsInAnyOrder("max", "min", "value_count")); } }); }	think this comment should be on the date_histo case below?
public QueryStringQueryBuilder splitOnWhitespace(boolean value) { if (autoGeneratePhraseQueries() && value == false) { throw new IllegalArgumentException("it is disallowed to disable split_on_whitespace " + "if auto_generate_phrase_queries is activated"); } this.splitOnWhitespace = value; return this; }	should we fail in dotoquery instead? i am wondering that the builder might be in an invalid state temporarily but in an ok state in the end depending on the order in which setters are called?
@Override public void handleException(TransportException exp) { onReplicaFailure(nodeId, exp); logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, transportReplicaAction); if (mustFailReplica(exp)) { logger.warn("{} failed to perform {} on node {}", exp, shardId, transportReplicaAction, node); shardStateAction.shardFailed(shard, indexUUID, "failed to perform " + actionName + " on replica on node " + node, exp); } }	can we assert that ignorereplicaexception is false? it will be weird to have something fail a shard but not reported.
public Extent getExtent() throws IOException { input.position(0); boolean hasExtent = input.readBoolean(); if (hasExtent) { return new Extent(input); } assert input.readVInt() == 1; ShapeType shapeType = input.readEnum(ShapeType.class); switch (shapeType) { case POLYGON: EdgeTreeReader edgeReader = new EdgeTreeReader(input); return edgeReader.getExtent(); case POINT: case MULTIPOINT: Point2DReader pointReader = new Point2DReader(input); return pointReader.getExtent(); case LINESTRING: case MULTILINESTRING: throw new UnsupportedOperationException("TODO: linestring and multilinestring"); default: throw new UnsupportedOperationException("unsupported shape type [" + shapeType + "]"); } }	this switch block is repeated twice, so it feels like it needs to be extracted into a separate method that knows how to read readers. so we probably need to have some common parent class for all readers that can provide extent() and intersects() methods. the last one will replace current containedinorcrosses and containedin methods as we discussed.
@Override public Void visit(Point point) { Point2DWriter writer = new Point2DWriter(Collections.singletonList(point)); addWriter(writer); return null; }	this can be replaced with new point2dwriter(point) as mentioned in other comments.
public Builder version(long version) { this.version = version; return this; }	this should be enforced then (set a flag, check the flag, etc.).
public void testCanRemainUsesLeastAvailableSpace() { NodeSettingsService nss = new NodeSettingsService(Settings.EMPTY); ClusterInfoService cis = EmptyClusterInfoService.INSTANCE; DiskThresholdDecider decider = new DiskThresholdDecider(Settings.EMPTY, nss, cis, null); ImmutableOpenMap.Builder<ShardRouting, String> shardRoutingMap = ImmutableOpenMap.builder(); DiscoveryNode node_0 = new DiscoveryNode("node_0", DummyTransportAddress.INSTANCE, Version.CURRENT); DiscoveryNode node_1 = new DiscoveryNode("node_1", DummyTransportAddress.INSTANCE, Version.CURRENT); ShardRouting test_0 = ShardRouting.newUnassigned("test", 0, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); ShardRoutingHelper.initialize(test_0, node_0.getId()); ShardRoutingHelper.moveToStarted(test_0); shardRoutingMap.put(test_0, "/node0/least"); ShardRouting test_1 = ShardRouting.newUnassigned("test", 1, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); ShardRoutingHelper.initialize(test_1, node_1.getId()); ShardRoutingHelper.moveToStarted(test_1); shardRoutingMap.put(test_1, "/node1/least"); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1)) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsNew(metaData.index("test")) .build(); ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build(); logger.info("--> adding two nodes"); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .put(node_0) .put(node_1) ).build(); // actual test -- after all that bloat :) ImmutableOpenMap.Builder<String, DiskUsage> leastAvailableUsages = ImmutableOpenMap.builder(); leastAvailableUsages.put("node_0", new DiskUsage("node_0", "node_0", "/node0/least", 100, 10)); // 90% used leastAvailableUsages.put("node_1", new DiskUsage("node_1", "node_1", "/node1/least", 100, 9)); // 91% used ImmutableOpenMap.Builder<String, DiskUsage> mostAvailableUsage = ImmutableOpenMap.builder(); mostAvailableUsage.put("node_0", new DiskUsage("node_0", "node_0", "/node0/most", 100, 90)); // 10% used mostAvailableUsage.put("node_1", new DiskUsage("node_1", "node_1", "/node1/most", 100, 90)); // 10% used ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder(); shardSizes.put("[test][0][p]", 10L); // 10 bytes shardSizes.put("[test][1][p]", 10L); shardSizes.put("[test][2][p]", 10L); final ClusterInfo clusterInfo = new ClusterInfo(leastAvailableUsages.build(), mostAvailableUsage.build(), shardSizes.build(), shardRoutingMap.build()); RoutingAllocation allocation = new RoutingAllocation(new AllocationDeciders(Settings.EMPTY, new AllocationDecider[]{decider}), clusterState.getRoutingNodes(), clusterState.nodes(), clusterInfo); assertEquals(Decision.YES, decider.canRemain(test_0, new RoutingNode("node_0", node_0), allocation)); assertEquals(Decision.NO, decider.canRemain(test_1, new RoutingNode("node_1", node_1), allocation)); try { decider.canRemain(test_0, new RoutingNode("node_1", node_1), allocation); fail("not allocated on this node"); } catch (IllegalArgumentException ex) { // not allocated on that node } try { decider.canRemain(test_1, new RoutingNode("node_0", node_0), allocation); fail("not allocated on this node"); } catch (IllegalArgumentException ex) { // not allocated on that node } ShardRouting test_2 = ShardRouting.newUnassigned("test", 2, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); ShardRoutingHelper.initialize(test_2, node_1.getId()); ShardRoutingHelper.moveToStarted(test_2); assertEquals("can stay since allocated on a different path with enough space", Decision.YES, decider.canRemain(test_2, new RoutingNode("node_1", node_1), allocation)); ShardRouting test_3 = ShardRouting.newUnassigned("test", 3, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); ShardRoutingHelper.initialize(test_3, node_1.getId()); ShardRoutingHelper.moveToStarted(test_3); assertEquals("can stay since we don't have information about this shard", Decision.YES, decider.canRemain(test_2, new RoutingNode("node_1", node_1), allocation)); }	is the removal (without replacement) of this intentional?
FileStructureFinder makeBestStructureFinder(List<String> explanation, String sample, String charsetName, Boolean hasByteOrderMarker, int lineMergeSizeLimit, FileStructureOverrides overrides, TimeoutChecker timeoutChecker) throws Exception { Character delimiter = overrides.getDelimiter(); Character quote = overrides.getQuote(); Boolean shouldTrimFields = overrides.getShouldTrimFields(); List<FileStructureFinderFactory> factories; double allowedFractionOfBadLines = 0.0; if (delimiter != null) { allowedFractionOfBadLines = DelimitedFileStructureFinderFactory.DEFAULT_BAD_ROWS_PERCENTAGE; // If a precise delimiter is specified, we only need one structure finder // factory, and we'll tolerate as little as one column in the input factories = Collections.singletonList(new DelimitedFileStructureFinderFactory(delimiter, (quote == null) ? '"' : quote, 1, (shouldTrimFields == null) ? (delimiter == '|') : shouldTrimFields)); } else if (quote != null || shouldTrimFields != null) { allowedFractionOfBadLines = DelimitedFileStructureFinderFactory.DEFAULT_BAD_ROWS_PERCENTAGE; // The delimiter is not specified, but some other aspect of delimited files is, // so clone our default delimited factories altering the overridden values factories = ORDERED_STRUCTURE_FACTORIES.stream().filter(factory -> factory instanceof DelimitedFileStructureFinderFactory) .map(factory -> ((DelimitedFileStructureFinderFactory) factory).makeSimilar(quote, shouldTrimFields)) .collect(Collectors.toList()); } else { // We can use the default factories, but possibly filtered down to a specific format factories = ORDERED_STRUCTURE_FACTORIES.stream() .filter(factory -> factory.canFindFormat(overrides.getFormat())).collect(Collectors.toList()); } for (FileStructureFinderFactory factory : factories) { timeoutChecker.check("high level format detection"); if (factory.canCreateFromSample(explanation, sample, allowedFractionOfBadLines)) { return factory.createFromSample(explanation, sample, charsetName, hasByteOrderMarker, lineMergeSizeLimit, overrides, timeoutChecker); } } throw new IllegalArgumentException("Input did not match " + ((overrides.getFormat() == null) ? "any known formats" : "the specified format [" + overrides.getFormat() + "]")); }	i think we should have wiggle room here as well. because the user supplying these fields implies they expect it to be delimited.
private static Query tryRewriteLongSort(SearchContext searchContext, IndexReader reader, Query query, boolean hasFilterCollector) throws IOException { if (searchContext.searchAfter() != null) return null; if (searchContext.scrollContext() != null) return null; if (searchContext.collapse() != null) return null; if (searchContext.trackScores()) return null; if (searchContext.aggregations() != null) return null; Sort sort = searchContext.sort().sort; SortField sortField = sort.getSort()[0]; if (SortField.Type.LONG.equals(IndexSortConfig.getSortFieldType(sortField)) == false) return null; // check if this is a field of type Long or Date, that is indexed and has doc values String fieldName = sortField.getField(); if (fieldName == null) return null; // happens when _score or _doc is the 1st sort field if (searchContext.mapperService() == null) return null; // mapperService can be null in tests final MappedFieldType fieldType = searchContext.mapperService().fullName(fieldName); if (fieldType == null) return null; // for unmapped fields, default behaviour depending on "unmapped_type" flag if ((fieldType.typeName().equals("long") == false) && (fieldType instanceof DateFieldType == false)) return null; if (fieldType.indexOptions() == IndexOptions.NONE) return null; //TODO: change to pointDataDimensionCount() when implemented if (fieldType.hasDocValues() == false) return null; // check that all sorts are actual document fields or _doc for (int i = 1; i < sort.getSort().length; i++) { SortField sField = sort.getSort()[i]; String sFieldName = sField.getField(); if (sFieldName == null) { if (SortField.FIELD_DOC.equals(sField) == false) return null; } else { if (searchContext.mapperService().fullName(sFieldName) == null) return null; // could be _script field that uses _score } } // check that setting of missing values allows optimization if (sortField.getMissingValue() == null) return null; Long missingValue = (Long) sortField.getMissingValue(); boolean missingValuesAccordingToSort = (sortField.getReverse() && (missingValue == Long.MIN_VALUE)) || ((sortField.getReverse() == false) && (missingValue == Long.MAX_VALUE)); if (missingValuesAccordingToSort == false) return null; // check for multiple values if (PointValues.size(reader, fieldName) != PointValues.getDocCount(reader, fieldName)) return null; //TODO: handle multiple values // check if the optimization makes sense with the track_total_hits setting if (searchContext.trackTotalHitsUpTo() == Integer.MAX_VALUE) { // with filter, we can't pre-calculate hitsCount, we need to explicitly calculate them => optimization does't make sense if (hasFilterCollector) return null; // if we can't pre-calculate hitsCount based on the query type, optimization does't make sense if (shortcutTotalHitCount(reader, query) == -1) return null; } byte[] minValueBytes = PointValues.getMinPackedValue(reader, fieldName); byte[] maxValueBytes = PointValues.getMaxPackedValue(reader, fieldName); if ((maxValueBytes == null) || (minValueBytes == null)) return null; long minValue = LongPoint.decodeDimension(minValueBytes, 0); long maxValue = LongPoint.decodeDimension(maxValueBytes, 0); Query rewrittenQuery; if (minValue == maxValue) { rewrittenQuery = new DocValuesFieldExistsQuery(fieldName); } else { long origin = (sortField.getReverse()) ? maxValue : minValue; long pivotDistance = (maxValue - minValue) >>> 1; // division by 2 on the unsigned representation to avoid overflow if (pivotDistance == 0) { // 0 if maxValue = (minValue + 1) pivotDistance = 1; } rewrittenQuery = LongPoint.newDistanceFeatureQuery(sortField.getField(), 1, origin, pivotDistance); } rewrittenQuery = new BooleanQuery.Builder() .add(query, BooleanClause.Occur.FILTER) // filter for original query .add(rewrittenQuery, BooleanClause.Occur.SHOULD) //should for rewrittenQuery .build(); return rewrittenQuery; }	the above lines seem to be indented by one more space than the lines below
@Override public Query rangeQuery(Object lowerTerm, Object upperTerm, boolean includeLower, boolean includeUpper, QueryShardContext context) { failIfNotIndexed(); return new TermRangeQuery(name(), lowerTerm == null ? null : indexedValueForSearch(lowerTerm), upperTerm == null ? null : indexedValueForSearch(upperTerm), includeLower, includeUpper); } } private final Boolean nullValue; private final boolean stored; protected BooleanFieldMapper(String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo); this.nullValue = builder.nullValue.getValue(); this.stored = builder.stored.getValue(); } @Override public BooleanFieldType fieldType() { return (BooleanFieldType) super.fieldType(); } @Override protected void parseCreateField(ParseContext context) throws IOException { if (fieldType().isSearchable() == false && stored == false && !fieldType().hasDocValues()) { return; } Boolean value = context.parseExternalValue(Boolean.class); if (value == null) { XContentParser.Token token = context.parser().currentToken(); if (token == XContentParser.Token.VALUE_NULL) { if (nullValue != null) { value = nullValue; } } else { value = context.parser().booleanValue(); } } if (value == null) { return; } if (fieldType().isSearchable()) { context.doc().add(new Field(fieldType().name(), value ? "T" : "F", Defaults.FIELD_TYPE)); } if (stored) { context.doc().add(new StoredField(fieldType().name(), value ? "T" : "F")); } if (fieldType().hasDocValues()) { context.doc().add(new SortedNumericDocValuesField(fieldType().name(), value ? 1 : 0)); } else { createFieldNamesField(context); } } @Override public ParametrizedFieldMapper.Builder getMergeBuilder() { return new Builder(simpleName()).init(this); }	seems like if the field is searchable and stored too, we end up adding two fields, while before we would only add one?
public void endRecovery() throws ElasticsearchException { store.decRef(); int left = onGoingRecoveries.decrementAndGet(); assert onGoingRecoveries.get() >= 0 : "ongoingRecoveries must be >= 0 but was: " + onGoingRecoveries.get(); if (left == 0) { try { flush(new Engine.Flush().type(Flush.Type.COMMIT_TRANSLOG)); } catch (IllegalIndexShardStateException e) { // we are being closed, or in created state, ignore } catch (FlushNotAllowedEngineException e) { // ignore this exception, we are not allowed to perform flush } catch (Throwable e) { logger.warn("failed to flush shard post recovery", e); } } }	for the 1.x and master future-port, how are you going to implement the flush? are you going to set force and waitifongoing?
public void testParseCidr() { IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> InetAddresses.parseCidr("")); assertThat(e.getMessage(), Matchers.containsString("Expected [ip/prefix] but was []")); e = expectThrows(IllegalArgumentException.class, () -> InetAddresses.parseCidr("192.168.1.42/40")); assertThat(e.getMessage(), Matchers.containsString("Illegal prefix length")); e = expectThrows(IllegalArgumentException.class, () -> InetAddresses.parseCidr("::1/130")); assertThat(e.getMessage(), Matchers.containsString("Illegal prefix length")); e = expectThrows(IllegalArgumentException.class, () -> InetAddresses.parseCidr("::ffff:0:0/96")); assertThat(e.getMessage(), Matchers.containsString("CIDR notation is not allowed with IPv6-mapped IPv4 address")); Tuple<InetAddress, Integer> cidr = InetAddresses.parseCidr("192.168.0.0/24"); assertEquals(InetAddresses.forString("192.168.0.0"), cidr.v1()); assertEquals(Integer.valueOf(24), cidr.v2()); cidr = InetAddresses.parseCidr("::fffe:0:0/95"); assertEquals(InetAddresses.forString("::fffe:0:0"), cidr.v1()); assertEquals(Integer.valueOf(95), cidr.v2()); }	can you test the edges for each, so 129, 128 and 32, 33?
@Override public long softUpdateDocuments(Term term, Iterable<? extends Iterable<? extends IndexableField>> docs, Field... softDeletes) throws IOException { assert softDeleteEnabled : "Call #softUpdateDocuments but soft-deletes is disabled"; return super.softUpdateDocuments(term, docs, softDeletes); } } /** * Returned the last local checkpoint value has been refreshed internally. */ final long lastRefreshedCheckpoint() { return lastRefreshedCheckpointListener.refreshedCheckpoint.get(); } private Object refreshIfNeededMutex = new Object(); /** * Refresh this engine **internally** iff the requesting seq_no is greater than the last refreshed checkpoint. */ protected final void refreshIfNeeded(String source, long requestingSeqNo) { if (lastRefreshedCheckpoint() < requestingSeqNo) { synchronized (refreshIfNeededMutex) { if (lastRefreshedCheckpoint() < requestingSeqNo) { refresh(source, SearcherScope.INTERNAL); } } } } private final class LastRefreshedCheckpointListener implements ReferenceManager.RefreshListener { final AtomicLong refreshedCheckpoint; private long pendingCheckpoint; LastRefreshedCheckpointListener(long initialLocalCheckpoint) { this.refreshedCheckpoint = new AtomicLong(initialLocalCheckpoint); } @Override public void beforeRefresh() { // all changes until this point should be visible after refresh pendingCheckpoint = localCheckpointTracker.getCheckpoint(); } @Override public void afterRefresh(boolean didRefresh) { if (didRefresh) { updateRefreshedCheckpoint(pendingCheckpoint); } } void updateRefreshedCheckpoint(long checkpoint) { refreshedCheckpoint.updateAndGet(curr -> Math.max(curr, checkpoint)); assert refreshedCheckpoint.get() >= checkpoint : refreshedCheckpoint.get() + " < " + checkpoint; } } @Override public final long getMaxSeenAutoIdTimestamp() { return maxSeenAutoIdTimestamp.get(); } @Override public final void updateMaxUnsafeAutoIdTimestamp(long newTimestamp) { updateAutoIdTimestamp(newTimestamp, true); } private void updateAutoIdTimestamp(long newTimestamp, boolean unsafe) { assert newTimestamp >= -1 : "invalid timestamp [" + newTimestamp + "]"; maxSeenAutoIdTimestamp.updateAndGet(curr -> Math.max(curr, newTimestamp)); if (unsafe) { maxUnsafeAutoIdTimestamp.updateAndGet(curr -> Math.max(curr, newTimestamp)); } assert maxUnsafeAutoIdTimestamp.get() <= maxSeenAutoIdTimestamp.get(); } private boolean assertMaxSeqNoOfUpdatesIsAdvanced(Term id, long seqNo, boolean allowDeleted, boolean relaxIfGapInSeqNo) { final long maxSeqNoOfUpdates = getMaxSeqNoOfUpdatesOrDeletes(); // If the primary is on an old version which does not replicate msu, we need to relax this assertion for that. if (maxSeqNoOfUpdates == SequenceNumbers.UNASSIGNED_SEQ_NO) { assert config().getIndexSettings().getIndexVersionCreated().before(Version.V_6_5_0); return true; } // We treat a delete on the tombstones on replicas as a regular document, then use updateDocument (not addDocument). if (allowDeleted) { final VersionValue versionValue = versionMap.getVersionForAssert(id.bytes()); if (versionValue != null && versionValue.isDelete()) { return true; } } // Operations can be processed on a replica in a different order than on the primary. If the order on the primary is index-1, // delete-2, index-3, and the order on a replica is index-1, index-3, delete-2, then the msu of index-3 on the replica is 2 // even though it is an update (overwrites index-1). We should relax this assertion if there is a pending gap in the seq_no. if (relaxIfGapInSeqNo && getLocalCheckpoint() < maxSeqNoOfUpdates) { return true; } assert seqNo <= maxSeqNoOfUpdates : "id=" + id + " seq_no=" + seqNo + " msu=" + maxSeqNoOfUpdates; return true; }	nit: make it final.
private static Response prepareRamIndex(Request request, CheckedBiFunction<SearchExecutionContext, LeafReaderContext, Response, IOException> handler, IndexService indexService) throws IOException { Analyzer defaultAnalyzer = indexService.getIndexAnalyzers().getDefaultIndexAnalyzer(); try (Directory directory = new ByteBuffersDirectory()) { try (IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(defaultAnalyzer))) { String index = indexService.index().getName(); BytesReference document = request.contextSetup.document; XContentType xContentType = request.contextSetup.xContentType; SourceToParse sourceToParse = new SourceToParse(index, "_id", document, xContentType); MappingLookup mappingLookup = indexService.mapperService().mappingLookup(); DocumentParser documentParser = indexService.mapperService().documentParser(); ParsedDocument parsedDocument = documentParser.parseDocument(sourceToParse, mappingLookup); indexWriter.addDocuments(parsedDocument.docs()); try (IndexReader indexReader = DirectoryReader.open(indexWriter)) { final IndexSearcher searcher = new IndexSearcher(indexReader); searcher.setQueryCache(null); final long absoluteStartMillis = System.currentTimeMillis(); SearchExecutionContext context = indexService.newSearchExecutionContext(0, 0, searcher, () -> absoluteStartMillis, null, emptyMap()); return handler.apply(context, indexReader.leaves().get(0)); } } } } } public static class RestAction extends BaseRestHandler { @Override public List<Route> routes() { return List.of( new Route(GET, "/_scripts/painless/_execute"), new Route(POST, "/_scripts/painless/_execute")); } @Override public String getName() { return "_scripts_painless_execute"; } @Override protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) throws IOException { final Request request = Request.parse(restRequest.contentOrSourceParamParser()); return channel -> client.executeLocally(INSTANCE, request, new RestToXContentListener<>(channel)); }	this is much better than before, as you don't get a null pointer, and mapping lookup is not responsible for parsing documents, but in case an index has no mappings, the mapping lookup won't hold metadata fields. maybe it would be more correct in this special case to allow to create a new mapping lookup that is parse proof, but i am not entirely sure that this is a big problem.
protected abstract LeafAgg toAgg(String id, C f); } abstract static class ExpressionTranslator<E extends Expression> { private final Class<E> typeToken = ReflectionUtils.detectSuperTypeForRuleLike(getClass()); @SuppressWarnings("unchecked") public QueryTranslation translate(Expression exp, boolean onAggs) { return (typeToken.isInstance(exp) ? asQuery((E) exp, onAggs) : null); } protected abstract QueryTranslation asQuery(E e, boolean onAggs); protected static Query generateQuery(ScalarFunction sf, Expression field, Supplier<Query> query) { if (field instanceof FieldAttribute) { return wrapIfNested(query.get(), field); } return new ScriptQuery(sf.location(), sf.asScript()); } protected static Query wrapIfNested(Query query, Expression exp) { if (exp instanceof FieldAttribute) { FieldAttribute fa = (FieldAttribute) exp; if (fa.isNested()) { return new NestedQuery(fa.location(), fa.nestedParent().name(), query); } } return query; }	maybe better suggestion for a name here?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); threads = in.readInt(); ignoreIdleThreads = in.readBoolean(); type = in.readString(); interval = TimeValue.readTimeValue(in); snapshots = in.readInt(); }	we need a version check here if this goes in 1.x
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeInt(threads); out.writeBoolean(ignoreIdleThreads); out.writeString(type); interval.writeTo(out); out.writeInt(snapshots); }	we need a version check here if this goes in 1.x
public void testLoseDedicatedMasterNode() throws Exception { internalCluster().ensureAtMostNumDataNodes(0); logger.info("Starting dedicated master node..."); internalCluster().startMasterOnlyNode(); logger.info("Starting ml and data node..."); String mlAndDataNode = internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, MachineLearning.ML_ROLE))); ensureStableClusterOnAllNodes(2); run("lose-dedicated-master-node-job", () -> { logger.info("Stopping dedicated master node"); Settings masterDataPathSettings = internalCluster().dataPathSettings(internalCluster().getMasterName()); internalCluster().stopCurrentMasterNode(); assertBusy(() -> { ClusterState state = client(mlAndDataNode).admin().cluster().prepareState() .setLocal(true).get().getState(); assertNull(state.nodes().getMasterNodeId()); }); logger.info("Restarting dedicated master node"); internalCluster().startNode(Settings.builder() .put(masterDataPathSettings) .put(masterOnlyNode()) .build()); ensureStableClusterOnAllNodes(2); }); }	the code here used to disable master, but now it is enabled. which is correct?
public void testStopAndForceStopDatafeed() throws Exception { internalCluster().ensureAtMostNumDataNodes(0); logger.info("Starting dedicated master node..."); internalCluster().startMasterOnlyNode(); logger.info("Starting ml and data node..."); internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.MASTER_ROLE))); ensureStableClusterOnAllNodes(2); // index some datafeed data client().admin().indices().prepareCreate("data") .setMapping("time", "type=date") .get(); long numDocs1 = randomIntBetween(32, 2048); long now = System.currentTimeMillis(); long weekAgo = now - 604800000; long twoWeeksAgo = weekAgo - 604800000; indexDocs(logger, "data", numDocs1, twoWeeksAgo, weekAgo); String jobId = "test-stop-and-force-stop"; String datafeedId = jobId + "-datafeed"; setupJobAndDatafeed(jobId, datafeedId, TimeValue.timeValueHours(1)); waitForDatafeed(jobId, numDocs1); GetDatafeedsStatsAction.Request datafeedStatsRequest = new GetDatafeedsStatsAction.Request(datafeedId); GetDatafeedsStatsAction.Response datafeedStatsResponse = client().execute(GetDatafeedsStatsAction.INSTANCE, datafeedStatsRequest).actionGet(); assertEquals(DatafeedState.STARTED, datafeedStatsResponse.getResponse().results().get(0).getDatafeedState()); // Stop the datafeed normally StopDatafeedAction.Request stopDatafeedRequest = new StopDatafeedAction.Request(datafeedId); ActionFuture<StopDatafeedAction.Response> normalStopActionFuture = client().execute(StopDatafeedAction.INSTANCE, stopDatafeedRequest); // Force stop the datafeed without waiting for the normal stop to return first stopDatafeedRequest = new StopDatafeedAction.Request(datafeedId); stopDatafeedRequest.setForce(true); StopDatafeedAction.Response stopDatafeedResponse = client().execute(StopDatafeedAction.INSTANCE, stopDatafeedRequest).actionGet(); assertTrue(stopDatafeedResponse.isStopped()); // Confirm that the normal stop also reports success - whichever way the datafeed // ends up getting stopped it's not an error to stop a stopped datafeed stopDatafeedResponse = normalStopActionFuture.actionGet(); assertTrue(stopDatafeedResponse.isStopped()); CloseJobAction.Request closeJobRequest = new CloseJobAction.Request(jobId); CloseJobAction.Response closeJobResponse = client().execute(CloseJobAction.INSTANCE, closeJobRequest).actionGet(); assertTrue(closeJobResponse.isClosed()); }	the code used to enable master and data here, was that a mistake in the old code?
public void testStopAndForceStopDatafeed() throws Exception { internalCluster().ensureAtMostNumDataNodes(0); logger.info("Starting dedicated master node..."); internalCluster().startMasterOnlyNode(); logger.info("Starting ml and data node..."); internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.MASTER_ROLE))); ensureStableClusterOnAllNodes(2); // index some datafeed data client().admin().indices().prepareCreate("data") .setMapping("time", "type=date") .get(); long numDocs1 = randomIntBetween(32, 2048); long now = System.currentTimeMillis(); long weekAgo = now - 604800000; long twoWeeksAgo = weekAgo - 604800000; indexDocs(logger, "data", numDocs1, twoWeeksAgo, weekAgo); String jobId = "test-stop-and-force-stop"; String datafeedId = jobId + "-datafeed"; setupJobAndDatafeed(jobId, datafeedId, TimeValue.timeValueHours(1)); waitForDatafeed(jobId, numDocs1); GetDatafeedsStatsAction.Request datafeedStatsRequest = new GetDatafeedsStatsAction.Request(datafeedId); GetDatafeedsStatsAction.Response datafeedStatsResponse = client().execute(GetDatafeedsStatsAction.INSTANCE, datafeedStatsRequest).actionGet(); assertEquals(DatafeedState.STARTED, datafeedStatsResponse.getResponse().results().get(0).getDatafeedState()); // Stop the datafeed normally StopDatafeedAction.Request stopDatafeedRequest = new StopDatafeedAction.Request(datafeedId); ActionFuture<StopDatafeedAction.Response> normalStopActionFuture = client().execute(StopDatafeedAction.INSTANCE, stopDatafeedRequest); // Force stop the datafeed without waiting for the normal stop to return first stopDatafeedRequest = new StopDatafeedAction.Request(datafeedId); stopDatafeedRequest.setForce(true); StopDatafeedAction.Response stopDatafeedResponse = client().execute(StopDatafeedAction.INSTANCE, stopDatafeedRequest).actionGet(); assertTrue(stopDatafeedResponse.isStopped()); // Confirm that the normal stop also reports success - whichever way the datafeed // ends up getting stopped it's not an error to stop a stopped datafeed stopDatafeedResponse = normalStopActionFuture.actionGet(); assertTrue(stopDatafeedResponse.isStopped()); CloseJobAction.Request closeJobRequest = new CloseJobAction.Request(jobId); CloseJobAction.Response closeJobResponse = client().execute(CloseJobAction.INSTANCE, closeJobRequest).actionGet(); assertTrue(closeJobResponse.isClosed()); }	the code here used to disable master, but now it is enabled. which is correct?
public void testExtractRawValueMixedObjects() throws IOException { { XContentBuilder builder = XContentFactory.jsonBuilder().startObject() .startObject("foo").field("cat", "meow").endObject() .field("foo.bar", "baz") .endObject(); try (XContentParser parser = createParser(JsonXContent.jsonXContent, Strings.toString(builder))) { Map<String, Object> map = parser.map(); assertThat(XContentMapValues.extractRawValues("foo.bar", map), Matchers.contains("baz")); } } { XContentBuilder builder = XContentFactory.jsonBuilder().startObject() .startObject("foo").field("bar", "meow").endObject() .field("foo.bar", "baz") .endObject(); try (XContentParser parser = createParser(JsonXContent.jsonXContent, Strings.toString(builder))) { Map<String, Object> map = parser.map(); assertThat(XContentMapValues.extractRawValues("foo.bar", map), Matchers.containsInAnyOrder("meow", "baz")); } } { XContentBuilder builder = XContentFactory.jsonBuilder().startObject() .field("foo", "bar") .field("foo.subfoo", "baz") .endObject(); try (XContentParser parser = createParser(JsonXContent.jsonXContent, Strings.toString(builder))) { assertThat(XContentMapValues.extractRawValues("foo.subfoo", parser.map()), containsInAnyOrder("baz")); } } }	were we always returning also foo's values or only when foo.subfoo did not exist?
private boolean checkEquals(Object actualValue, Object expectedValue) { if (expectedValue instanceof Map) { if (false == actualValue instanceof Map) { return false; } Map<?, ?> actualMap = (Map<?, ?>) actualValue; Map<?, ?> expectedMap = (Map<?, ?>) expectedValue; if (actualMap.size() != expectedMap.size()) { return false; } for (Map.Entry<?, ?> e : expectedMap.entrySet()) { if (false == actualMap.containsKey(e.getKey())) { return false; } Object a = actualMap.get(e.getKey()); if (e.getValue() == null) { if (a != null) { return false; } } else { if (false == checkEquals(a, e.getValue())) { return false; } } } return true; } if (expectedValue instanceof List) { if (false == actualValue instanceof List) { return false; } List<?> actualList = (List<?>) actualValue; List<?> expectedList = (List<?>) expectedValue; if (actualList.size() != expectedList.size()) { return false; } for (int i = 0; i < expectedList.size(); i++) { if (false == checkEquals(actualList.get(i), expectedList.get(i))) { return false; } } return true; } if (expectedValue instanceof Double) { Double expectedDouble = (Double) expectedValue; if (actualValue instanceof Float) { /* * We only ever parse doubles in expected values but * some xcontent types return floats in some fields. * So we compare with the precision we have. The truth * is that those types actually have higher fidelity. * The xcontent types that return doubles have rounded * real floats to doubles as part of the serialization * process. */ float actualFloat = (Float) actualValue; return actualFloat == expectedDouble.floatValue(); } if (actualValue instanceof Double) { double actualDouble = (Double) actualValue; return actualDouble == expectedDouble; } return false; } return expectedValue.equals(actualValue); }	can we do the opposite instead and promote to the type that retains more information? ie. return actualfloat == expecteddouble.doublevalue(); otherwise we might also return true for a different double whose nearest float happens to be the expected value.
private static void checkForNonCollapsableSubselects(PhysicalPlan plan, List<Failure> failures) { Holder<Boolean> hasLimit = new Holder<>(Boolean.FALSE); Holder<List<Order>> orderBy = new Holder<>(); plan.forEachUp(p -> { if (hasLimit.get() == false && p instanceof LimitExec) { hasLimit.set(Boolean.TRUE); return; } if (orderBy.get() == null && p instanceof OrderExec) { orderBy.set(((OrderExec) p).order()); return; } if (hasLimit.get() && orderBy.get() != null && p instanceof OrderExec) { if (((OrderExec) p).order().equals(orderBy.get()) == false) { failures.add(fail(p, "Cannot use ORDER BY on top of a subquery with ORDER BY and LIMIT")); } } }); }	is the error message descriptive enough, tho? that _is_ actually possible[*], just not in any (i.e. "conflicting") configuration, right? subselects are a gray area anyways, so i guess this could also remain like this, but alternatively we could add some notes in the limitations docs or somehow rephrase this message -- up to you. [*] didn't run it on the new code, but i guess this should work: select * from (select * from test_emp order by emp_no limit 10) order by emp_no limit 5
private static void checkForNonCollapsableSubselects(PhysicalPlan plan, List<Failure> failures) { Holder<Boolean> hasLimit = new Holder<>(Boolean.FALSE); Holder<List<Order>> orderBy = new Holder<>(); plan.forEachUp(p -> { if (hasLimit.get() == false && p instanceof LimitExec) { hasLimit.set(Boolean.TRUE); return; } if (orderBy.get() == null && p instanceof OrderExec) { orderBy.set(((OrderExec) p).order()); return; } if (hasLimit.get() && orderBy.get() != null && p instanceof OrderExec) { if (((OrderExec) p).order().equals(orderBy.get()) == false) { failures.add(fail(p, "Cannot use ORDER BY on top of a subquery with ORDER BY and LIMIT")); } } }); }	optional compacting. suggestion if (p instanceof limitexec) { if (haslimit.get() == false) { haslimit.set(true); // i'd use either objects or primitives for this and above statement } } else if (p instanceof orderexec) { if (orderby.get() == null) { orderby.set(((orderexec) p).order()); } else if (haslimit.get()) { if (((orderexec) p).order().equals(orderby.get()) == false) { failures.add(fail(p, "cannot use order by on top of a subquery with order by and limit")); } } }
public TokenizerFactory tokenizerFactory() { return tokenizerFactory; }	these are needed by the analysis api but not really anywhere else. they aren't a huge burden to maintain though, not like the name() member was.
public static LicensedFeature.Momentary featureFromLicenseLevel(License.OperationMode mode) { switch (mode) { case BASIC: return ML_MODEL_INFERENCE_BASIC_FEATURE; case PLATINUM: return ML_MODEL_INFERENCE_PLATINUM_FEATURE; default: assert false : "Unrecognized licensing mode for inference models [" + mode + "]"; return ML_MODEL_INFERENCE_PLATINUM_FEATURE; } }	i think this switch could be turned into an if/else. it's either platinum, or else it is basic.
@Override public boolean match(Task task) { // If we are retrieving all the s, the task description does not contain the id if (id.equals(MetaData.ALL)) { return task.getDescription().startsWith(DataFrameField.PERSISTENT_TASK_DESCRIPTION_PREFIX); } // Otherwise find the task by ID return task.getDescription().equals(DataFrameField.PERSISTENT_TASK_DESCRIPTION_PREFIX + id); }	if we are retrieving all the s
public static XContentBuilder shuffleXContent(XContentBuilder builder, Set<String> exceptFieldNames) throws IOException { BytesReference bytes = builder.bytes(); XContentParser parser = XContentFactory.xContent(bytes).createParser(bytes); // use ordered maps for reproducibility Map<String, Object> shuffledMap = shuffleMap(parser.mapOrdered(), exceptFieldNames, random()); XContentBuilder jsonBuilder = XContentFactory.jsonBuilder(); return jsonBuilder.map(shuffledMap); }	is it a good idea to always do json?
public static XContentBuilder shuffleXContent(XContentBuilder builder, Set<String> exceptFieldNames) throws IOException { BytesReference bytes = builder.bytes(); XContentParser parser = XContentFactory.xContent(bytes).createParser(bytes); // use ordered maps for reproducibility Map<String, Object> shuffledMap = shuffleMap(parser.mapOrdered(), exceptFieldNames, random()); XContentBuilder jsonBuilder = XContentFactory.jsonBuilder(); return jsonBuilder.map(shuffledMap); }	why pass random here?
public static XContentBuilder shuffleXContent(XContentBuilder builder, Set<String> exceptFieldNames) throws IOException { BytesReference bytes = builder.bytes(); XContentParser parser = XContentFactory.xContent(bytes).createParser(bytes); // use ordered maps for reproducibility Map<String, Object> shuffledMap = shuffleMap(parser.mapOrdered(), exceptFieldNames, random()); XContentBuilder jsonBuilder = XContentFactory.jsonBuilder(); return jsonBuilder.map(shuffledMap); }	i think you have to sort before shuffling, otherwise it will not be deterministic, cause the order in the list still depends on the order in the set, which depends on the jvm :)
public void test() throws Exception { prepareCreate("test").addMapping("type", "s", "type=string").execute().actionGet(); ensureGreen(); logger.info("indexing data start"); for (int i = 0; i < 10; ++i) { client().prepareIndex("test", "type", Integer.toString(i)).setSource("s", "value" + i).execute().actionGet(); } logger.info("indexing data end"); final int searchCycles = 20; refresh(); // disable field data updateFormat("disabled"); SubAggCollectionMode aggCollectionMode = randomFrom(SubAggCollectionMode.values()); SearchResponse resp = null; // try to run something that relies on field data and make sure that it fails try { for (int i = 0; i < searchCycles; i++) { resp = client().prepareSearch("test").setPreference(Integer.toString(i)).addAggregation(AggregationBuilders.terms("t").field("s") .collectMode(aggCollectionMode)).execute().actionGet(); assertFailures(resp); } } catch (SearchPhaseExecutionException e) { // expected } // enable it again updateFormat("paged_bytes"); // try to run something that relies on field data and make sure that it works for (int i = 0; i < searchCycles; i++) { resp = client().prepareSearch("test").setPreference(Integer.toString(i)).addAggregation(AggregationBuilders.terms("t").field("s") .collectMode(aggCollectionMode)).execute().actionGet(); assertNoFailures(resp); } // disable it again updateFormat("disabled"); // this time, it should work because segments are already loaded for (int i = 0; i < searchCycles; i++) { resp = client().prepareSearch("test").setPreference(Integer.toString(i)).addAggregation(AggregationBuilders.terms("t").field("s") .collectMode(aggCollectionMode)).execute().actionGet(); assertNoFailures(resp); } // but add more docs and the new segment won't be loaded client().prepareIndex("test", "type", "-1").setSource("s", "value").execute().actionGet(); refresh(); try { for (int i = 0; i < searchCycles; i++) { resp = client().prepareSearch("test").setPreference(Integer.toString(i)).addAggregation(AggregationBuilders.terms("t").field("s") .collectMode(aggCollectionMode)).execute().actionGet(); assertFailures(resp); } } catch (SearchPhaseExecutionException e) { // expected } }	something i'm unhappy with is that the test will pass if any of these requests fails while i'd like to make sure that they all fail (if a searchphaseexecutionexception is thrown)?
public void test() throws Exception { prepareCreate("test").addMapping("type", "s", "type=string").execute().actionGet(); ensureGreen(); logger.info("indexing data start"); for (int i = 0; i < 10; ++i) { client().prepareIndex("test", "type", Integer.toString(i)).setSource("s", "value" + i).execute().actionGet(); } logger.info("indexing data end"); final int searchCycles = 20; refresh(); // disable field data updateFormat("disabled"); SubAggCollectionMode aggCollectionMode = randomFrom(SubAggCollectionMode.values()); SearchResponse resp = null; // try to run something that relies on field data and make sure that it fails try { for (int i = 0; i < searchCycles; i++) { resp = client().prepareSearch("test").setPreference(Integer.toString(i)).addAggregation(AggregationBuilders.terms("t").field("s") .collectMode(aggCollectionMode)).execute().actionGet(); assertFailures(resp); } } catch (SearchPhaseExecutionException e) { // expected } // enable it again updateFormat("paged_bytes"); // try to run something that relies on field data and make sure that it works for (int i = 0; i < searchCycles; i++) { resp = client().prepareSearch("test").setPreference(Integer.toString(i)).addAggregation(AggregationBuilders.terms("t").field("s") .collectMode(aggCollectionMode)).execute().actionGet(); assertNoFailures(resp); } // disable it again updateFormat("disabled"); // this time, it should work because segments are already loaded for (int i = 0; i < searchCycles; i++) { resp = client().prepareSearch("test").setPreference(Integer.toString(i)).addAggregation(AggregationBuilders.terms("t").field("s") .collectMode(aggCollectionMode)).execute().actionGet(); assertNoFailures(resp); } // but add more docs and the new segment won't be loaded client().prepareIndex("test", "type", "-1").setSource("s", "value").execute().actionGet(); refresh(); try { for (int i = 0; i < searchCycles; i++) { resp = client().prepareSearch("test").setPreference(Integer.toString(i)).addAggregation(AggregationBuilders.terms("t").field("s") .collectMode(aggCollectionMode)).execute().actionGet(); assertFailures(resp); } } catch (SearchPhaseExecutionException e) { // expected } }	same concern about searchphaseexecutionexception
@Override public boolean match(Task task) { for(int i = 0; i < expandedJobsIds.size(); i++) { if(OpenJobAction.JobTaskMatcher.match(task, expandedJobsIds.get(i)) == true) return true; } return false; }	this can be reduced to a single line using stream().anymatch suggestion return expandedjobsids.stream().anymatch(jobid -> openjobaction.jobtaskmatcher.match(task, jobid));
public void testReproducible() throws IOException { if (ITER++ == 0) { CLUSTER_SEED = cluster().seed(); } else { assertEquals(CLUSTER_SEED, Long.valueOf(cluster().seed())); } }	i'm not sure if this is ok to remove.
@Override public ValueFetcher valueFetcher(SearchExecutionContext context, String format) { return new DocValueFetcher(docValueFormat(format, null), context.getForField(this)); } } public IndexFieldMapper() { super(IndexFieldType.INSTANCE); }	i think we could do something even simpler here since the value is constant. we could return a direct implementation of valuefetcher that always returns fullyqualifiedindexname.
private ValuesSourceRegistry registerAggregations(List<SearchPlugin> plugins) { ValuesSourceRegistry.ValuesSourceRegistryBuilder builder = new ValuesSourceRegistry.ValuesSourceRegistryBuilder(); registerAggregation(new AggregationSpec(AvgAggregationBuilder.NAME, AvgAggregationBuilder::new, AvgAggregationBuilder.PARSER) .addResultReader(InternalAvg::new) .setAggregatorRegistrar(AvgAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(WeightedAvgAggregationBuilder.NAME, WeightedAvgAggregationBuilder::new, WeightedAvgAggregationBuilder.PARSER).addResultReader(InternalWeightedAvg::new), builder); registerAggregation(new AggregationSpec(SumAggregationBuilder.NAME, SumAggregationBuilder::new, SumAggregationBuilder.PARSER) .addResultReader(InternalSum::new) .setAggregatorRegistrar(SumAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(MinAggregationBuilder.NAME, MinAggregationBuilder::new, MinAggregationBuilder.PARSER) .addResultReader(InternalMin::new) .setAggregatorRegistrar(MinAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(MaxAggregationBuilder.NAME, MaxAggregationBuilder::new, MaxAggregationBuilder.PARSER) .addResultReader(InternalMax::new) .setAggregatorRegistrar(MaxAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(StatsAggregationBuilder.NAME, StatsAggregationBuilder::new, StatsAggregationBuilder.PARSER) .addResultReader(InternalStats::new) .setAggregatorRegistrar(StatsAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(ExtendedStatsAggregationBuilder.NAME, ExtendedStatsAggregationBuilder::new, ExtendedStatsAggregationBuilder.PARSER) .addResultReader(InternalExtendedStats::new) .setAggregatorRegistrar(ExtendedStatsAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(ValueCountAggregationBuilder.NAME, ValueCountAggregationBuilder::new, ValueCountAggregationBuilder.PARSER) .addResultReader(InternalValueCount::new) .setAggregatorRegistrar(ValueCountAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(PercentilesAggregationBuilder.NAME, PercentilesAggregationBuilder::new, PercentilesAggregationBuilder.PARSER) .addResultReader(InternalTDigestPercentiles.NAME, InternalTDigestPercentiles::new) .addResultReader(InternalHDRPercentiles.NAME, InternalHDRPercentiles::new) .setAggregatorRegistrar(PercentilesAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(PercentileRanksAggregationBuilder.NAME, PercentileRanksAggregationBuilder::new, PercentileRanksAggregationBuilder.PARSER) .addResultReader(InternalTDigestPercentileRanks.NAME, InternalTDigestPercentileRanks::new) .addResultReader(InternalHDRPercentileRanks.NAME, InternalHDRPercentileRanks::new) .setAggregatorRegistrar(PercentileRanksAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(MedianAbsoluteDeviationAggregationBuilder.NAME, MedianAbsoluteDeviationAggregationBuilder::new, MedianAbsoluteDeviationAggregationBuilder.PARSER) .addResultReader(InternalMedianAbsoluteDeviation::new) .setAggregatorRegistrar(MedianAbsoluteDeviationAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(CardinalityAggregationBuilder.NAME, CardinalityAggregationBuilder::new, CardinalityAggregationBuilder.PARSER).addResultReader(InternalCardinality::new) .setAggregatorRegistrar(CardinalityAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(GlobalAggregationBuilder.NAME, GlobalAggregationBuilder::new, GlobalAggregationBuilder::parse).addResultReader(InternalGlobal::new), builder); registerAggregation(new AggregationSpec(MissingAggregationBuilder.NAME, MissingAggregationBuilder::new, MissingAggregationBuilder.PARSER) .addResultReader(InternalMissing::new) .setAggregatorRegistrar(MissingAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(FilterAggregationBuilder.NAME, FilterAggregationBuilder::new, FilterAggregationBuilder::parse).addResultReader(InternalFilter::new), builder); registerAggregation(new AggregationSpec(FiltersAggregationBuilder.NAME, FiltersAggregationBuilder::new, FiltersAggregationBuilder::parse).addResultReader(InternalFilters::new), builder); registerAggregation(new AggregationSpec(AdjacencyMatrixAggregationBuilder.NAME, AdjacencyMatrixAggregationBuilder::new, AdjacencyMatrixAggregationBuilder::parse).addResultReader(InternalAdjacencyMatrix::new), builder); registerAggregation(new AggregationSpec(SamplerAggregationBuilder.NAME, SamplerAggregationBuilder::new, SamplerAggregationBuilder::parse) .addResultReader(InternalSampler.NAME, InternalSampler::new) .addResultReader(UnmappedSampler.NAME, UnmappedSampler::new), builder); registerAggregation(new AggregationSpec(DiversifiedAggregationBuilder.NAME, DiversifiedAggregationBuilder::new, DiversifiedAggregationBuilder.PARSER) /* Reuses result readers from SamplerAggregator*/, builder); registerAggregation(new AggregationSpec(TermsAggregationBuilder.NAME, TermsAggregationBuilder::new, TermsAggregationBuilder.PARSER) .addResultReader(StringTerms.NAME, StringTerms::new) .addResultReader(UnmappedTerms.NAME, UnmappedTerms::new) .addResultReader(LongTerms.NAME, LongTerms::new) .addResultReader(DoubleTerms.NAME, DoubleTerms::new) .setAggregatorRegistrar(TermsAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(RareTermsAggregationBuilder.NAME, RareTermsAggregationBuilder::new, RareTermsAggregationBuilder.PARSER) .addResultReader(StringRareTerms.NAME, StringRareTerms::new) .addResultReader(UnmappedRareTerms.NAME, UnmappedRareTerms::new) .addResultReader(LongRareTerms.NAME, LongRareTerms::new) .setAggregatorRegistrar(RareTermsAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(SignificantTermsAggregationBuilder.NAME, SignificantTermsAggregationBuilder::new, SignificantTermsAggregationBuilder::parse) .addResultReader(SignificantStringTerms.NAME, SignificantStringTerms::new) .addResultReader(SignificantLongTerms.NAME, SignificantLongTerms::new) .addResultReader(UnmappedSignificantTerms.NAME, UnmappedSignificantTerms::new) .setAggregatorRegistrar(SignificantTermsAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(SignificantTextAggregationBuilder.NAME, SignificantTextAggregationBuilder::new, SignificantTextAggregationBuilder::parse), builder); registerAggregation(new AggregationSpec(RangeAggregationBuilder.NAME, RangeAggregationBuilder::new, RangeAggregationBuilder.PARSER) .addResultReader(InternalRange::new) .setAggregatorRegistrar(RangeAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(DateRangeAggregationBuilder.NAME, DateRangeAggregationBuilder::new, DateRangeAggregationBuilder.PARSER) .addResultReader(InternalDateRange::new) .setAggregatorRegistrar(DateRangeAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(IpRangeAggregationBuilder.NAME, IpRangeAggregationBuilder::new, IpRangeAggregationBuilder.PARSER).addResultReader(InternalBinaryRange::new), builder); registerAggregation(new AggregationSpec(HistogramAggregationBuilder.NAME, HistogramAggregationBuilder::new, HistogramAggregationBuilder.PARSER) .addResultReader(InternalHistogram::new) .setAggregatorRegistrar(HistogramAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(DateHistogramAggregationBuilder.NAME, DateHistogramAggregationBuilder::new, DateHistogramAggregationBuilder.PARSER) .addResultReader(InternalDateHistogram::new) .setAggregatorRegistrar(DateHistogramAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(AutoDateHistogramAggregationBuilder.NAME, AutoDateHistogramAggregationBuilder::new, AutoDateHistogramAggregationBuilder.PARSER).addResultReader(InternalAutoDateHistogram::new), builder); registerAggregation(new AggregationSpec(GeoDistanceAggregationBuilder.NAME, GeoDistanceAggregationBuilder::new, GeoDistanceAggregationBuilder::parse).addResultReader(InternalGeoDistance::new), builder); registerAggregation(new AggregationSpec(GeoHashGridAggregationBuilder.NAME, GeoHashGridAggregationBuilder::new, GeoHashGridAggregationBuilder.PARSER) .addResultReader(InternalGeoHashGrid::new) .setAggregatorRegistrar(GeoHashGridAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(GeoTileGridAggregationBuilder.NAME, GeoTileGridAggregationBuilder::new, GeoTileGridAggregationBuilder.PARSER) .addResultReader(InternalGeoTileGrid::new) .setAggregatorRegistrar(GeoTileGridAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(NestedAggregationBuilder.NAME, NestedAggregationBuilder::new, NestedAggregationBuilder::parse).addResultReader(InternalNested::new), builder); registerAggregation(new AggregationSpec(ReverseNestedAggregationBuilder.NAME, ReverseNestedAggregationBuilder::new, ReverseNestedAggregationBuilder::parse).addResultReader(InternalReverseNested::new), builder); registerAggregation(new AggregationSpec(TopHitsAggregationBuilder.NAME, TopHitsAggregationBuilder::new, TopHitsAggregationBuilder::parse).addResultReader(InternalTopHits::new), builder); registerAggregation(new AggregationSpec(GeoBoundsAggregationBuilder.NAME, GeoBoundsAggregationBuilder::new, GeoBoundsAggregationBuilder.PARSER) .addResultReader(InternalGeoBounds::new) .setAggregatorRegistrar(GeoBoundsAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(GeoCentroidAggregationBuilder.NAME, GeoCentroidAggregationBuilder::new, GeoCentroidAggregationBuilder.PARSER) .addResultReader(InternalGeoCentroid::new) .setAggregatorRegistrar(GeoCentroidAggregationBuilder::registerAggregators), builder); registerAggregation(new AggregationSpec(ScriptedMetricAggregationBuilder.NAME, ScriptedMetricAggregationBuilder::new, ScriptedMetricAggregationBuilder.PARSER).addResultReader(InternalScriptedMetric::new), builder); registerAggregation((new AggregationSpec(CompositeAggregationBuilder.NAME, CompositeAggregationBuilder::new, CompositeAggregationBuilder.PARSER).addResultReader(InternalComposite::new)), builder); registerFromPlugin(plugins, SearchPlugin::getAggregations, (agg) -> this.registerAggregation(agg, builder)); // after aggs have been registered, see if there are any new VSTypes that need to be linked to core fields registerFromPlugin(plugins, SearchPlugin::getBareAggregatorRegistrar, (registrar) -> this.registerBareAggregatorRegistrar(registrar, builder)); return builder.build(); }	maybe it isn't worth having the method named registerbareaggregatorregistrar now.
private static boolean isFieldMapped(QueryShardContext context, String fieldPattern) { final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType) context .getMapperService().fieldType(FieldNamesFieldMapper.NAME); if (fieldNamesFieldType == null) { // can only happen when no types exist, so no docs exist either return false; } final Collection<String> fields; if (context.getObjectMapper(fieldPattern) != null) { // the _field_names field also indexes objects, so we don't have to // do any more work to support exists queries on whole objects fields = Collections.singleton(fieldPattern); } else { fields = context.simpleMatchToIndexNames(fieldPattern); } if (fields.size() == 1) { String field = fields.iterator().next(); MappedFieldType fieldType = context.getMapperService().fieldType(field); if (fieldType == null) { // The field does not exist as a leaf but could be an object so // check for an object mapper if (context.getObjectMapper(field) != null) { return false; } return false; // no field mapped } } return true; }	it's very easy for this method to get out-of-sync with newfilter, which would likely translate into bugs. can you make this method called by newfilter? for instance i wonder that it could return a collection of fields instead of a boolean, and all above return false statements would become return collections.emptyset(); and then in dorewrite we'd check whether the return collection is empty? finally in dotoquery we could raise an exception when the returned collection is empty, as it would mean that the query wasn't rewritten first even though it is a requirement of the api.
@Override protected void doAssertLuceneQuery(ExistsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException { String fieldPattern = queryBuilder.fieldName(); Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern); Collection<String> mappedFields = fields.stream().filter((field) -> context.getObjectMapper(field) != null || context.getMapperService().fieldType(field) != null).collect(Collectors.toList()); if (fields.size() == 1 && mappedFields.size() == 0) { assertThat(query, instanceOf(MatchNoDocsQuery.class)); MatchNoDocsQuery matchNoDocsQuery = (MatchNoDocsQuery) query; assertThat(matchNoDocsQuery.toString(null), containsString("User requested \\\\"match_none\\\\" query.")); } else if (fields.size() == 1) { assertThat(query, instanceOf(ConstantScoreQuery.class)); ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query; String field = expectedFieldName(fields.iterator().next()); if (context.getObjectMapper(field) != null) { assertThat(constantScoreQuery.getQuery(), instanceOf(BooleanQuery.class)); BooleanQuery booleanQuery = (BooleanQuery) constantScoreQuery.getQuery(); List<String> childFields = new ArrayList<>(); context.getObjectMapper(field).forEach(mapper -> childFields.add(mapper.name())); assertThat(booleanQuery.clauses().size(), equalTo(childFields.size())); for (int i = 0; i < childFields.size(); i++) { BooleanClause booleanClause = booleanQuery.clauses().get(i); assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD)); } } else if (context.getMapperService().fieldType(field).hasDocValues()) { assertThat(constantScoreQuery.getQuery(), instanceOf(DocValuesFieldExistsQuery.class)); DocValuesFieldExistsQuery dvExistsQuery = (DocValuesFieldExistsQuery) constantScoreQuery.getQuery(); assertEquals(field, dvExistsQuery.getField()); } else if (context.getMapperService().fieldType(field).omitNorms() == false) { assertThat(constantScoreQuery.getQuery(), instanceOf(NormsFieldExistsQuery.class)); NormsFieldExistsQuery normsExistsQuery = (NormsFieldExistsQuery) constantScoreQuery.getQuery(); assertEquals(field, normsExistsQuery.getField()); } else { assertThat(constantScoreQuery.getQuery(), instanceOf(TermQuery.class)); TermQuery termQuery = (TermQuery) constantScoreQuery.getQuery(); assertEquals(field, termQuery.getTerm().text()); } } else { assertThat(query, instanceOf(ConstantScoreQuery.class)); ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query; assertThat(constantScoreQuery.getQuery(), instanceOf(BooleanQuery.class)); BooleanQuery booleanQuery = (BooleanQuery) constantScoreQuery.getQuery(); assertThat(booleanQuery.clauses().size(), equalTo(mappedFields.size())); for (int i = 0; i < mappedFields.size(); i++) { BooleanClause booleanClause = booleanQuery.clauses().get(i); assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD)); } } }	let's remove this assertion on the error message. i suspect that attempting to maintain this error message is not going to be worth the efforts, and there is no point in asserting on an error message that is not helpful.
@Override public void performAction(IndexMetaData indexMetaData, ClusterState clusterState, ClusterStateObserver observer, Listener listener) { final RoutingNodes routingNodes = clusterState.getRoutingNodes(); RoutingAllocation allocation = new RoutingAllocation(ALLOCATION_DECIDERS, routingNodes, clusterState, null, System.nanoTime()); List<String> validNodeIds = new ArrayList<>(); final Map<ShardId, List<ShardRouting>> routingsByShardId = clusterState.getRoutingTable() .allShards(indexMetaData.getIndex().getName()) .stream() .collect(Collectors.groupingBy(ShardRouting::shardId)); if (routingsByShardId.isEmpty() == false) { for (RoutingNode node : routingNodes) { boolean canAllocateOneCopyOfEachShard = routingsByShardId.values().stream() // For each shard .allMatch(shardRoutings -> shardRoutings.stream() // Can we allocate at least one shard copy to this node? .map(shardRouting -> ALLOCATION_DECIDERS.canAllocate(shardRouting, node, allocation).type()) .anyMatch(Decision.Type.YES::equals)); if (canAllocateOneCopyOfEachShard) { validNodeIds.add(node.node().getId()); } } // Shuffle the list of nodes so the one we pick is random Randomness.shuffle(validNodeIds); Optional<String> nodeId = validNodeIds.stream().findAny(); if (nodeId.isPresent()) { Settings settings = Settings.builder() .put(IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getKey() + "_id", nodeId.get()).build(); UpdateSettingsRequest updateSettingsRequest = new UpdateSettingsRequest(indexMetaData.getIndex().getName()) .masterNodeTimeout(getMasterTimeout(clusterState)) .settings(settings); getClient().admin().indices().updateSettings(updateSettingsRequest, ActionListener.wrap(response -> listener.onResponse(true), listener::onFailure)); } else { // No nodes currently match the allocation rules, so report this as an error and we'll retry logger.debug("could not find any nodes to allocate index [{}] onto prior to shrink"); listener.onFailure(new NoNodeAvailableException("could not find any nodes to allocate index [{}] onto prior to shrink")); } } else { // There are no shards for the index, the index might be gone. Even though this is a retryable step ILM will not retry in // this case as we're using the periodic loop to trigger the retries and that is run over *existing* indices. listener.onFailure(new IndexNotFoundException(indexMetaData.getIndex())); } }	this is missing the index name to fill in the {} in its params
public void testGroupByDateHistogram() { LogicalPlan p = plan("SELECT MAX(int) FROM test GROUP BY HISTOGRAM(int, 1000)"); assertTrue(p instanceof Aggregate); Aggregate a = (Aggregate) p; List<Expression> groupings = a.groupings(); assertEquals(1, groupings.size()); Expression exp = groupings.get(0); assertEquals(Histogram.class, exp.getClass()); Histogram h = (Histogram) exp; assertEquals(1000, h.interval().fold()); Expression field = h.field(); assertEquals(FieldAttribute.class, field.getClass()); assertEquals(DataType.INTEGER, field.dataType()); }	do we have a situation where the histogram is being grouped by very small intervals? like 1 second for a date interval.
private void cleanupStaleIndices(Map<String, BlobContainer> foundIndices, Map<String, IndexId> survivingIndices) { try { final Set<String> survivingIndexIds = survivingIndices.values().stream() .map(IndexId::getId).collect(Collectors.toSet()); for (Map.Entry<String, BlobContainer> indexEntry : foundIndices.entrySet()) { final String indexSnId = indexEntry.getKey(); try { if (survivingIndexIds.contains(indexSnId) == false) { logger.debug("[{}] Found stale index [{}]. Cleaning it up", metadata.name(), indexSnId); indexEntry.getValue().delete(); logger.debug("[{}] Cleaned up stale index [{}]", metadata.name(), indexSnId); } } catch (IOException e) { logger.warn(() -> new ParameterizedMessage( "[{}] index {} is no longer part of any snapshots in the repository, " + "but failed to clean up their index folders", metadata.name(), indexSnId), e); } } } catch (Exception e) { // TODO: We shouldn't be blanket catching and suppressing all exceptions here and instead handle them safely upstream. // Currently this catch exists as a stop gap solution to tackle unexpected runtime exceptions from implementations // bubbling up and breaking the snapshot functionality. assert false; logger.warn(new ParameterizedMessage("[{}] Exception during cleanup of stale indices", metadata.name()), e); } }	add exception to message that is shown when assertion breaks?
@SuppressForbidden(reason = "Channel is based of a socket not a file") @Override public int write(ByteBuffer src) throws IOException { return SocketAccess.doPrivilegedIOException(() -> writeChannel.write(src)); } })); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Uploads a blob using the "multipart upload" method (a single * 'multipart/related' request containing both data and metadata. The request is * gziped), see: * https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload * @param blobInfo the info for the blob to be uploaded * @param inputStream the stream containing the blob data * @param blobSize the size * @param failIfAlreadyExists whether to throw a FileAlreadyExistsException if the given blob already exists */ private void writeBlobMultipart(BlobInfo blobInfo, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) throws IOException { assert blobSize <= LARGE_BLOB_THRESHOLD_BYTE_SIZE : "large blob uploads should use the resumable upload method"; final ByteArrayOutputStream baos = new ByteArrayOutputStream(Math.toIntExact(blobSize)); Streams.copy(inputStream, baos); try { final Storage.BlobTargetOption[] targetOptions = failIfAlreadyExists ? new Storage.BlobTargetOption[] { Storage.BlobTargetOption.doesNotExist() } : new Storage.BlobTargetOption[0]; SocketAccess.doPrivilegedVoidIOException( () -> client().create(blobInfo, baos.toByteArray(), targetOptions)); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Deletes the blob from the specific bucket * * @param blobName name of the blob */ void deleteBlob(String blobName) throws IOException { final BlobId blobId = BlobId.of(bucketName, blobName); final boolean deleted = SocketAccess.doPrivilegedIOException(() -> client().delete(blobId)); if (deleted == false) { throw new NoSuchFileException("Blob [" + blobName + "] does not exist"); } } /** * Deletes multiple blobs from the specific bucket all of which have prefixed names * * @param prefix prefix of the blobs to delete */ private void deleteBlobsByPrefix(String prefix) throws IOException { deleteBlobs(listBlobsByPrefix("", prefix).keySet()); } /** * Deletes multiple blobs from the specific bucket using a batch request * * @param blobNames names of the blobs to delete */ void deleteBlobs(Collection<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } // for a single op submit a simple delete instead of a batch of size 1 if (blobNames.size() == 1) { deleteBlob(blobNames.iterator().next()); return; } final List<BlobId> blobIdsToDelete = blobNames.stream().map(blob -> BlobId.of(bucketName, blob)).collect(Collectors.toList()); final List<Boolean> deletedStatuses = new ArrayList<>(); final int deleteCount = blobIdsToDelete.size(); SocketAccess.doPrivilegedIOException(() -> { final int batches = deleteCount / MAX_BATCH_SIZE + (deleteCount % MAX_BATCH_SIZE == 0 ? 0 : 1); for (int i = 0; i < batches - 1; ++i) { deletedStatuses.addAll(client().delete(blobIdsToDelete.subList(i * MAX_BATCH_SIZE, (i + 1) * MAX_BATCH_SIZE))); } deletedStatuses.addAll(client().delete(blobIdsToDelete.subList(batches - 1, deleteCount))); return null; }); assert deleteCount == deletedStatuses.size(); boolean failed = false; for (int i = 0; i < blobIdsToDelete.size(); i++) { if (deletedStatuses.get(i) == false) { final BlobId blobId = blobIdsToDelete.get(i); try { Blob blob = SocketAccess.doPrivilegedIOException(() -> client().get(blobId)); if (blob != null) { logger.error("Failed to delete blob [{}] in bucket [{}]", blobId.getName(), bucketName); failed = true; } } catch (Exception e) { logger.error( new ParameterizedMessage( "Failed to delete and then stat blob [{}] in bucket [{}]", blobId.getName(), bucketName), e); failed = true; } } }	i find the way you do it with partitions for aws in https://github.com/elastic/elasticsearch/pull/40322 easier to read and follow, can you make these implementations consistent? we can think about refactoring this part of the code into abstractblobcontainer
@SuppressForbidden(reason = "Channel is based of a socket not a file") @Override public int write(ByteBuffer src) throws IOException { return SocketAccess.doPrivilegedIOException(() -> writeChannel.write(src)); } })); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Uploads a blob using the "multipart upload" method (a single * 'multipart/related' request containing both data and metadata. The request is * gziped), see: * https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload * @param blobInfo the info for the blob to be uploaded * @param inputStream the stream containing the blob data * @param blobSize the size * @param failIfAlreadyExists whether to throw a FileAlreadyExistsException if the given blob already exists */ private void writeBlobMultipart(BlobInfo blobInfo, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) throws IOException { assert blobSize <= LARGE_BLOB_THRESHOLD_BYTE_SIZE : "large blob uploads should use the resumable upload method"; final ByteArrayOutputStream baos = new ByteArrayOutputStream(Math.toIntExact(blobSize)); Streams.copy(inputStream, baos); try { final Storage.BlobTargetOption[] targetOptions = failIfAlreadyExists ? new Storage.BlobTargetOption[] { Storage.BlobTargetOption.doesNotExist() } : new Storage.BlobTargetOption[0]; SocketAccess.doPrivilegedVoidIOException( () -> client().create(blobInfo, baos.toByteArray(), targetOptions)); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Deletes the blob from the specific bucket * * @param blobName name of the blob */ void deleteBlob(String blobName) throws IOException { final BlobId blobId = BlobId.of(bucketName, blobName); final boolean deleted = SocketAccess.doPrivilegedIOException(() -> client().delete(blobId)); if (deleted == false) { throw new NoSuchFileException("Blob [" + blobName + "] does not exist"); } } /** * Deletes multiple blobs from the specific bucket all of which have prefixed names * * @param prefix prefix of the blobs to delete */ private void deleteBlobsByPrefix(String prefix) throws IOException { deleteBlobs(listBlobsByPrefix("", prefix).keySet()); } /** * Deletes multiple blobs from the specific bucket using a batch request * * @param blobNames names of the blobs to delete */ void deleteBlobs(Collection<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } // for a single op submit a simple delete instead of a batch of size 1 if (blobNames.size() == 1) { deleteBlob(blobNames.iterator().next()); return; } final List<BlobId> blobIdsToDelete = blobNames.stream().map(blob -> BlobId.of(bucketName, blob)).collect(Collectors.toList()); final List<Boolean> deletedStatuses = new ArrayList<>(); final int deleteCount = blobIdsToDelete.size(); SocketAccess.doPrivilegedIOException(() -> { final int batches = deleteCount / MAX_BATCH_SIZE + (deleteCount % MAX_BATCH_SIZE == 0 ? 0 : 1); for (int i = 0; i < batches - 1; ++i) { deletedStatuses.addAll(client().delete(blobIdsToDelete.subList(i * MAX_BATCH_SIZE, (i + 1) * MAX_BATCH_SIZE))); } deletedStatuses.addAll(client().delete(blobIdsToDelete.subList(batches - 1, deleteCount))); return null; }); assert deleteCount == deletedStatuses.size(); boolean failed = false; for (int i = 0; i < blobIdsToDelete.size(); i++) { if (deletedStatuses.get(i) == false) { final BlobId blobId = blobIdsToDelete.get(i); try { Blob blob = SocketAccess.doPrivilegedIOException(() -> client().get(blobId)); if (blob != null) { logger.error("Failed to delete blob [{}] in bucket [{}]", blobId.getName(), bucketName); failed = true; } } catch (Exception e) { logger.error( new ParameterizedMessage( "Failed to delete and then stat blob [{}] in bucket [{}]", blobId.getName(), bucketName), e); failed = true; } } }	i'd rather trust gcp and do not perform any additional checks.
public void accept(SecurityIndexManager.State previousState, SecurityIndexManager.State currentState) { final PrintStream out = BootstrapInfo.getOriginalStandardOut(); // Check if it has been closed, try to write something so that we trigger PrintStream#ensureOpen out.println(); if (out.checkError()) { outputOnError(new IllegalStateException("Stashed standard output stream is closed.")); return; } if (previousState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) && currentState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) == false && securityIndexManager.indexExists() == false) { final SecureString elasticPassword = new SecureString(generatePassword(20)); final SecureString kibanaSystemPassword = new SecureString(generatePassword(20)); nativeUsersStore .updateReservedUser( ElasticUser.NAME, elasticPassword.getChars(), DocWriteRequest.OpType.CREATE, WriteRequest.RefreshPolicy.IMMEDIATE, ActionListener.wrap(result -> { nativeUsersStore .updateReservedUser( KibanaSystemUser.NAME, kibanaSystemPassword.getChars(), DocWriteRequest.OpType.CREATE, WriteRequest.RefreshPolicy.IMMEDIATE, ActionListener.wrap( r -> { outputOnSuccess(elasticPassword, kibanaSystemPassword, out); }, this::outputOnError ) ); }, this::outputOnError)); securityIndexManager.removeStateListener(this); } }	is this really an illegal state? it just means that we've closed the streams, which is normal when not attached to a console.
public Settings nodeSettings(int nodeOrdinal) { return Settings.builder() .put( NodeEnvironment.MAX_LOCAL_STORAGE_NODES_SETTING.getKey(), 2 + (masterNodes ? InternalTestCluster.DEFAULT_HIGH_NUM_MASTER_NODES : 0) + maxNumDataNodes + numClientNodes) .put(NetworkModule.TRANSPORT_TYPE_KEY, getTestTransportType()) .putList(DISCOVERY_HOSTS_PROVIDER_SETTING.getKey(), "file") .build(); }	also set .putlist(settingsbasedhostsprovider.discovery_zen_ping_unicast_hosts_setting.getkey()) // empty list disables a port scan for other nodes?
public void testPrimaryContextHandoff() throws IOException { GlobalCheckpointTracker oldPrimary = new GlobalCheckpointTracker(new ShardId("test", "_na_", 0), IndexSettingsModule.newIndexSettings("test", Settings.EMPTY), UNASSIGNED_SEQ_NO); GlobalCheckpointTracker newPrimary = new GlobalCheckpointTracker(new ShardId("test", "_na_", 0), IndexSettingsModule.newIndexSettings("test", Settings.EMPTY), UNASSIGNED_SEQ_NO); FakeClusterState clusterState = initialState(); clusterState.apply(oldPrimary); clusterState.apply(newPrimary); activatePrimary(clusterState, oldPrimary); for (int i = 0; i < randomInt(10); i++) { if (rarely()) { clusterState = randomUpdateClusterState(clusterState); clusterState.apply(oldPrimary); clusterState.apply(newPrimary); } if (randomBoolean()) { randomLocalCheckpointUpdate(oldPrimary); } if (randomBoolean()) { randomMarkInSync(oldPrimary); } } GlobalCheckpointTracker.PrimaryContext primaryContext = oldPrimary.startRelocationHandOff(); if (randomBoolean()) { // cluster state update after primary context handoff if (randomBoolean()) { clusterState = randomUpdateClusterState(clusterState); clusterState.apply(oldPrimary); clusterState.apply(newPrimary); } // abort handoff, check that we can continue updates and retry handoff oldPrimary.abortRelocationHandOff(); if (rarely()) { clusterState = randomUpdateClusterState(clusterState); clusterState.apply(oldPrimary); clusterState.apply(newPrimary); } if (randomBoolean()) { randomLocalCheckpointUpdate(oldPrimary); } if (randomBoolean()) { randomMarkInSync(oldPrimary); } // do another handoff primaryContext = oldPrimary.startRelocationHandOff(); } // send primary context through the wire BytesStreamOutput output = new BytesStreamOutput(); primaryContext.writeTo(output); StreamInput streamInput = output.bytes().streamInput(); primaryContext = new GlobalCheckpointTracker.PrimaryContext(streamInput); switch (randomInt(3)) { case 0: { // apply cluster state update on old primary while primary context is being transferred clusterState = randomUpdateClusterState(clusterState); clusterState.apply(oldPrimary); // activate new primary newPrimary.activateWithPrimaryContext(primaryContext); // apply cluster state update on new primary so that the states on old and new primary are comparable clusterState.apply(newPrimary); break; } case 1: { // apply cluster state update on new primary while primary context is being transferred clusterState = randomUpdateClusterState(clusterState); clusterState.apply(newPrimary); // activate new primary newPrimary.activateWithPrimaryContext(primaryContext); // apply cluster state update on old primary so that the states on old and new primary are comparable clusterState.apply(oldPrimary); break; } case 2: { // apply cluster state update on both copies while primary context is being transferred clusterState = randomUpdateClusterState(clusterState); clusterState.apply(oldPrimary); clusterState.apply(newPrimary); newPrimary.activateWithPrimaryContext(primaryContext); break; } case 3: { // no cluster state update newPrimary.activateWithPrimaryContext(primaryContext); break; } } assertTrue(oldPrimary.primaryMode); assertTrue(newPrimary.primaryMode); assertThat(newPrimary.appliedClusterStateVersion, equalTo(oldPrimary.appliedClusterStateVersion)); assertThat(newPrimary.localCheckpoints, equalTo(oldPrimary.localCheckpoints)); assertThat(newPrimary.globalCheckpoint, equalTo(oldPrimary.globalCheckpoint)); oldPrimary.completeRelocationHandOff(); assertFalse(oldPrimary.primaryMode); }	nit: this samples the randomint again and again.. much less likely to get to 10.
void sendRequest(final DiscoveryNode node, final TcpChannel channel, final long requestId, final String action, final TransportRequest request, final TransportRequestOptions options, final Version channelVersion, final boolean compressRequest, final boolean isHandshake) throws IOException, TransportException { Version version = Version.min(this.version, channelVersion); OutboundMessage.Request message = new OutboundMessage.Request(threadPool.getThreadContext(), request, version, action, requestId, isHandshake, compressRequest); request.incRef(); ActionListener<Void> listener = ActionListener.wrap(() -> { try { messageListener.onRequestSent(node, requestId, action, request, options); } finally { request.decRef(); } }); sendMessage(channel, message, listener); } /** * Sends the response to the given channel. This method should be used to send {@link TransportResponse}	hmm, i think this bug is really in the recovery code tho, we're completing the overall listener before all the messages have gone, which is kinda lazy. it's ok in this specific case because we don't hold any other resources, but in general i worry that this change might hide different use-after-release bugs. if this call to incref() fails then imo the caller is at fault, we should fail tests that hit this. obtaining another ref to be safe here (and throwing an alreadyclosedexception if not) seems like a good backup protection in production tho, but i think we should actually be deliberately lazy in the recovery code so we don't obscure any other use-after-release bugs: diff diff --git a/server/src/main/java/org/elasticsearch/indices/recovery/remoterecoverytargethandler.java b/server/src/main/java/org/elasticsearch/indices/recovery/remoterecoverytargethandler.java index db3147c52c2..a1f89da448a 100644 --- a/server/src/main/java/org/elasticsearch/indices/recovery/remoterecoverytargethandler.java +++ b/server/src/main/java/org/elasticsearch/indices/recovery/remoterecoverytargethandler.java @@ -10,6 +10,7 @@ package org.elasticsearch.indices.recovery; import org.apache.logging.log4j.logmanager; import org.apache.logging.log4j.logger; +import org.apache.lucene.store.alreadyclosedexception; import org.apache.lucene.store.ratelimiter; import org.elasticsearch.elasticsearchexception; import org.elasticsearch.exceptionshelper; @@ -243,8 +244,19 @@ public class remoterecoverytargethandler implements recoverytargethandler { @override public void tryaction(actionlistener<t> listener) { - transportservice.sendrequest(targetnode, action, request, options, - new actionlistenerresponsehandler<>(listener, reader, threadpool.names.generic)); + if (request.tryincref()) { + transportservice.sendrequest( + targetnode, + action, + request, + options, + new actionlistenerresponsehandler<>( + actionlistener.runbefore(listener, request::decref), + reader, + threadpool.names.generic)); + } else { + listener.onfailure(new alreadyclosedexception("already closed")); + } } @override
*/ public MetricType getMetricType() { return metricType; } } private final NumberType type; private final boolean indexed; private final boolean hasDocValues; private final boolean stored; private final Explicit<Boolean> ignoreMalformed; private final Explicit<Boolean> coerce; private final Number nullValue; private final FieldValues<Number> scriptValues; private final boolean ignoreMalformedByDefault; private final boolean coerceByDefault; private final boolean dimension; private final ScriptCompiler scriptCompiler; private final Script script; private final MetricType metricType; private NumberFieldMapper(String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo, builder.script.get() != null, builder.onScriptError.getValue()); this.type = builder.type; this.indexed = builder.indexed.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); this.stored = builder.stored.getValue(); this.ignoreMalformed = builder.ignoreMalformed.getValue(); this.coerce = builder.coerce.getValue(); this.nullValue = builder.nullValue.getValue(); this.ignoreMalformedByDefault = builder.ignoreMalformed.getDefaultValue().value(); this.coerceByDefault = builder.coerce.getDefaultValue().value(); this.scriptValues = builder.scriptValues(); this.dimension = builder.dimension.getValue(); this.scriptCompiler = builder.scriptCompiler; this.script = builder.script.getValue(); this.metricType = builder.metric.getValue(); } boolean coerce() { return coerce.value(); } boolean ignoreMalformed() { return ignoreMalformed.value(); } @Override public NumberFieldType fieldType() { return (NumberFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType().type.typeName(); } @Override protected void parseCreateField(DocumentParserContext context) throws IOException { Number value; try { value = value(context.parser(), type, nullValue, coerce()); } catch (InputCoercionException | IllegalArgumentException | JsonParseException e) { if (ignoreMalformed.value() && context.parser().currentToken().isValue()) { context.addIgnoredField(mappedFieldType.name()); return; } else { throw e; } } if (value != null) { indexValue(context, value); } } /** * Read the value at the current position of the parser. * @throws InputCoercionException if xcontent couldn't convert the value in the required type, for example, integer overflow * @throws JsonParseException if there was any error parsing the json * @throws IllegalArgumentException if there was an error parsing the value from the json * @throws IOException if there was any other IO error */ private static Number value(XContentParser parser, NumberType numberType, Number nullValue, boolean coerce) throws InputCoercionException, JsonParseException, IllegalArgumentException, IOException { if (parser.currentToken() == Token.VALUE_NULL) { return nullValue; } if (coerce && parser.currentToken() == Token.VALUE_STRING && parser.textLength() == 0) { return nullValue; } return numberType.parse(parser, coerce); } private void indexValue(DocumentParserContext context, Number numericValue) { List<Field> fields = fieldType().type.createFields(fieldType().name(), numericValue, indexed, hasDocValues, stored); if (dimension && fields.isEmpty() == false) { try { // Extract the tsid part of the dimension field by using the long value. // Dimension can only be one of byte, short, int, long BytesReference bytes = TimeSeriesIdFieldMapper.extractTsidValue(numericValue.longValue()); context.doc().addDimensionBytes(fieldType().name(), bytes); } catch (IOException e) { throw new IllegalArgumentException("Dimension field [" + fieldType().name() + "] cannot be serialized.", e); } } context.doc().addAll(fields); if (hasDocValues == false && (stored || indexed)) { context.addToFieldNames(fieldType().name()); } } @Override protected void indexScriptValues( SearchLookup searchLookup, LeafReaderContext readerContext, int doc, DocumentParserContext documentParserContext ) { this.scriptValues.valuesForDoc(searchLookup, readerContext, doc, value -> indexValue(documentParserContext, value)); }	i think it might be a bit confusing to check if fields is empty but then us numericvalue. is there something up with default values here? i wonder if we're better off just checking if numericvalue != null.
@Override protected void execute(Terminal terminal, OptionSet options, Environment env) throws Exception { try (KeystoreAndPassphrase keyAndPass = KeyStoreWrapper.readOrCreate(terminal, env.configFile(), options.has(forceOption))) { if (null == keyAndPass) { return; } KeyStoreWrapper keystore = keyAndPass.getKeystore(); String setting = arguments.value(options); if (setting == null) { throw new UserException(ExitCodes.USAGE, "The setting name can not be null"); } if (keystore.getSettingNames().contains(setting) && options.has(forceOption) == false) { if (terminal.promptYesNo("Setting " + setting + " already exists. Overwrite?", false) == false) { terminal.println("Exiting without modifying keystore."); return; } } final char[] value; if (options.has(stdinOption)) { BufferedReader stdinReader = new BufferedReader(new InputStreamReader(getStdin(), StandardCharsets.UTF_8)); value = stdinReader.readLine().toCharArray(); } else { value = terminal.readSecret("Enter value for " + setting + ": "); } try { keystore.setString(setting, value); } catch (IllegalArgumentException e) { throw new UserException(ExitCodes.DATA_ERROR, "String value must contain only ASCII"); } keystore.save(env.configFile(), keyAndPass.getPassphrase()); } catch (SecurityException e) { throw new UserException(ExitCodes.DATA_ERROR, "Failed to access the keystore. Please make sure the passphrase was correct."); } }	lets not swallow this exception; we should add them to the userexception
@Override protected void execute(Terminal terminal, OptionSet options, Environment env) throws Exception { try (KeystoreAndPassphrase keyAndPass = KeyStoreWrapper.readOrCreate(terminal, env.configFile(), options.has(forceOption))) { if (null == keyAndPass) { return; } KeyStoreWrapper keystore = keyAndPass.getKeystore(); String setting = arguments.value(options); if (setting == null) { throw new UserException(ExitCodes.USAGE, "The setting name can not be null"); } if (keystore.getSettingNames().contains(setting) && options.has(forceOption) == false) { if (terminal.promptYesNo("Setting " + setting + " already exists. Overwrite?", false) == false) { terminal.println("Exiting without modifying keystore."); return; } } final char[] value; if (options.has(stdinOption)) { BufferedReader stdinReader = new BufferedReader(new InputStreamReader(getStdin(), StandardCharsets.UTF_8)); value = stdinReader.readLine().toCharArray(); } else { value = terminal.readSecret("Enter value for " + setting + ": "); } try { keystore.setString(setting, value); } catch (IllegalArgumentException e) { throw new UserException(ExitCodes.DATA_ERROR, "String value must contain only ASCII"); } keystore.save(env.configFile(), keyAndPass.getPassphrase()); } catch (SecurityException e) { throw new UserException(ExitCodes.DATA_ERROR, "Failed to access the keystore. Please make sure the passphrase was correct."); } }	lets not swallow this exception; we should add them to the userexception
protected void execute(Terminal terminal, OptionSet options, Environment env) throws Exception { KeyStoreWrapper keystore = KeyStoreWrapper.load(env.configFile()); if (keystore == null) { throw new UserException(ExitCodes.DATA_ERROR, "Elasticsearch keystore not found. Use 'create' command to create one."); } char[] passphrase; passphrase = keystore.hasPassword() ? KeyStoreWrapper.readPassphrase(terminal, false) : new char[0]; try { keystore.decrypt(passphrase); List<String> sortedEntries = new ArrayList<>(keystore.getSettingNames()); Collections.sort(sortedEntries); for (String entry : sortedEntries) { terminal.println(entry); } } catch (SecurityException e) { throw new UserException(ExitCodes.DATA_ERROR, "Failed to access the keystore. Please make sure the passphrase was correct."); } finally { Arrays.fill(passphrase, '\\\\u0000'); } }	lets not swallow this exception; we should add them to the userexception
protected void execute(Terminal terminal, OptionSet options, Environment env) throws Exception { List<String> settings = arguments.values(options); if (settings.isEmpty()) { throw new UserException(ExitCodes.USAGE, "Must supply at least one setting to remove"); } KeyStoreWrapper keystore = KeyStoreWrapper.load(env.configFile()); if (keystore == null) { throw new UserException(ExitCodes.DATA_ERROR, "Elasticsearch keystore not found. Use 'create' command to create one."); } char[] passphrase = null; try { passphrase = keystore.hasPassword() ? KeyStoreWrapper.readPassphrase(terminal, false) : new char[0]; keystore.decrypt(passphrase); for (String setting : arguments.values(options)) { if (keystore.getSettingNames().contains(setting) == false) { throw new UserException(ExitCodes.CONFIG, "Setting [" + setting + "] does not exist in the keystore."); } keystore.remove(setting); } keystore.save(env.configFile(), passphrase); } catch (SecurityException e) { throw new UserException(ExitCodes.DATA_ERROR, "Failed to access the keystore. Please make sure the passphrase was correct."); } finally { if (null != passphrase) { Arrays.fill(passphrase, '\\\\u0000'); } } }	lets not swallow this exception; we should add them to the userexception
public void testMessageListeners() throws Exception { final TransportRequestHandler<TransportRequest.Empty> requestHandler = (request, channel, task) -> { try { if (randomBoolean()) { channel.sendResponse(TransportResponse.Empty.INSTANCE); } else { channel.sendResponse(new ElasticsearchException("simulated")); } } catch (IOException e) { logger.error("Unexpected failure", e); fail(e.getMessage()); } }; final String ACTION = "internal:action"; serviceA.registerRequestHandler(ACTION, TransportRequest.Empty::new, ThreadPool.Names.GENERIC, requestHandler); serviceB.registerRequestHandler(ACTION, TransportRequest.Empty::new, ThreadPool.Names.GENERIC, requestHandler); class CountingListener implements TransportMessageListener { AtomicInteger requestsReceived = new AtomicInteger(); AtomicInteger requestsSent = new AtomicInteger(); AtomicInteger responseReceived = new AtomicInteger(); AtomicInteger responseSent = new AtomicInteger(); @Override public void onRequestReceived(long requestId, String action) { if (action.equals(ACTION)) { requestsReceived.incrementAndGet(); } } @Override public void onResponseSent(long requestId, String action, TransportResponse response) { if (action.equals(ACTION)) { responseSent.incrementAndGet(); } } @Override public void onResponseSent(long requestId, String action, Exception error) { if (action.equals(ACTION)) { responseSent.incrementAndGet(); } } @Override public void onResponseReceived(long requestId, Transport.ResponseContext context) { if (context.action().equals(ACTION)) { responseReceived.incrementAndGet(); } } @Override public void onRequestSent(DiscoveryNode node, long requestId, String action, TransportRequest request, TransportRequestOptions options) { if (action.equals(ACTION)) { requestsSent.incrementAndGet(); } } } final CountingListener tracerA = new CountingListener(); final CountingListener tracerB = new CountingListener(); serviceA.addMessageListener(tracerA); serviceB.addMessageListener(tracerB); try { serviceA.submitRequest(nodeB, ACTION, TransportRequest.Empty.INSTANCE, EmptyTransportResponseHandler.INSTANCE_SAME).get(); } catch (ExecutionException e) { assertThat(e.getCause(), instanceOf(ElasticsearchException.class)); assertThat(ExceptionsHelper.unwrapCause(e.getCause()).getMessage(), equalTo("simulated")); } // use assert busy as call backs are sometime called after the response have been sent assertBusy(() -> { assertThat(tracerA.requestsReceived.get(), equalTo(0)); assertThat(tracerA.requestsSent.get(), equalTo(1)); assertThat(tracerA.responseReceived.get(), equalTo(1)); assertThat(tracerA.responseSent.get(), equalTo(0)); assertThat(tracerB.requestsReceived.get(), equalTo(1)); assertThat(tracerB.requestsSent.get(), equalTo(0)); assertThat(tracerB.responseReceived.get(), equalTo(0)); assertThat(tracerB.responseSent.get(), equalTo(1)); }); try { serviceB.submitRequest(nodeA, ACTION, TransportRequest.Empty.INSTANCE, EmptyTransportResponseHandler.INSTANCE_SAME).get(); } catch (ExecutionException e) { assertThat(e.getCause(), instanceOf(ElasticsearchException.class)); assertThat(ExceptionsHelper.unwrapCause(e.getCause()).getMessage(), equalTo("simulated")); } // use assert busy as call backs are sometime called after the response have been sent assertBusy(() -> { assertThat(tracerA.requestsReceived.get(), equalTo(1)); assertThat(tracerA.requestsSent.get(), equalTo(1)); assertThat(tracerA.responseReceived.get(), equalTo(1)); assertThat(tracerA.responseSent.get(), equalTo(1)); assertThat(tracerB.requestsReceived.get(), equalTo(1)); assertThat(tracerB.requestsSent.get(), equalTo(1)); assertThat(tracerB.responseReceived.get(), equalTo(1)); assertThat(tracerB.responseSent.get(), equalTo(1)); }); // Test send local request try { serviceA.submitRequest(nodeA, ACTION, TransportRequest.Empty.INSTANCE, EmptyTransportResponseHandler.INSTANCE_SAME).get(); } catch (ExecutionException e) { assertThat(e.getCause(), instanceOf(ElasticsearchException.class)); assertThat(ExceptionsHelper.unwrapCause(e.getCause()).getMessage(), equalTo("simulated")); } // use assert busy as call backs are sometime called after the response have been sent assertBusy(() -> { assertThat(tracerA.requestsReceived.get(), equalTo(2)); assertThat(tracerA.requestsSent.get(), equalTo(2)); assertThat(tracerA.responseReceived.get(), equalTo(2)); assertThat(tracerA.responseSent.get(), equalTo(2)); assertThat(tracerB.requestsReceived.get(), equalTo(1)); assertThat(tracerB.requestsSent.get(), equalTo(1)); assertThat(tracerB.responseReceived.get(), equalTo(1)); assertThat(tracerB.responseSent.get(), equalTo(1)); }); }	nit: call backs -> callbacks + response -> responses? :)
public static TransformConfig randomTransformConfig() { PivotConfig pivotConfig; LatestDocConfig latestDocConfig; if (randomBoolean()) { pivotConfig = PivotConfigTests.randomPivotConfig(); latestDocConfig = null; } else { pivotConfig = null; latestDocConfig = LatestDocConfigTests.randomLatestDocConfig(); } return new TransformConfig( randomAlphaOfLengthBetween(1, 10), randomSourceConfig(), randomDestConfig(), randomBoolean() ? null : TimeValue.timeValueMillis(randomIntBetween(1000, 1000000)), randomBoolean() ? null : randomSyncConfig(), pivotConfig, latestDocConfig, randomBoolean() ? null : randomAlphaOfLengthBetween(1, 100), SettingsConfigTests.randomSettingsConfig(), randomBoolean() ? null : Instant.now(), randomBoolean() ? null : Version.CURRENT.toString() ); }	+ a test for expectthrows if both are given, to test mutual exclusiveness
private <T> T invokeParser(String name, String sql, List<SqlTypedParamValue> params, Function<SqlBaseParser, ParserRuleContext> parseFunction, BiFunction<AstBuilder, ParserRuleContext, T> visitor) { SqlBaseLexer lexer = new SqlBaseLexer(new CaseInsensitiveStream(sql)); lexer.removeErrorListeners(); lexer.addErrorListener(ERROR_LISTENER); Map<Token, SqlTypedParamValue> paramTokens = new HashMap<>(); TokenSource tokenSource = new ParametrizedTokenSource(lexer, paramTokens, params); CommonTokenStream tokenStream = new CommonTokenStream(tokenSource); SqlBaseParser parser = new SqlBaseParser(tokenStream); parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames()))); parser.removeErrorListeners(); parser.addErrorListener(ERROR_LISTENER); parser.getInterpreter().setPredictionMode(PredictionMode.SLL); if (DEBUG) { debug(parser); tokenStream.fill(); for (Token t : tokenStream.getTokens()) { String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType()); String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType()); log.info(format(Locale.ROOT, " %-15s '%s'", symbolicName == null ? literalName : symbolicName, t.getText())); }; } ParserRuleContext tree = parseFunction.apply(parser); if (DEBUG) { log.info("Parse tree {} " + tree.toStringTree()); } try { return visitor.apply(new AstBuilder(paramTokens), tree); } catch (StackOverflowError e) { throw new ParsingException("{} is too large to parse (causes stack overflow)", name); } }	would a message of the form "{} cannot be parsed" be more in line with our error messages in general? meaning, the message says right away that something is wrong, at the start of the message. also, the failure happens because the message is too large (in length) or because it has too many tokens?
public Expression createExpression(String expression, List<SqlTypedParamValue> params) { if (log.isDebugEnabled()) { log.debug("Parsing as expression: {}", expression); } return invokeParser("expression", expression, params, SqlBaseParser::singleExpression, AstBuilder::expression); }	can you name the name parameter in a more descriptive way? as i see it, it's only used for error reporting purposes, to adapt the message to what type of entity failed to be parsed. maybe querysourcetype?
public void testFromXContent() throws IOException { for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) { GeoDistanceSortBuilder testItem = randomGeoDistanceSortBuilder(); XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values())); testItem.toXContent(builder, ToXContent.EMPTY_PARAMS); XContentBuilder shuffled = shuffleXContent(builder); try (XContentParser parser = createParser(shuffled)) { parser.nextToken(); NestedSortBuilder parsedItem = NestedSortBuilder.fromXContent(parser); assertNotSame(testItem, parsedItem); assertEquals(testItem, parsedItem); assertEquals(testItem.hashCode(), parsedItem.hashCode()); } } }	i think this is not right? it does not need to be a nested sort
@Override public List<PersistentTasksExecutor<?>> getPersistentTasksExecutor(ClusterService clusterService, ThreadPool threadPool, Client client, SettingsModule settingsModule) { if (enabled == false || transportClientMode) { return emptyList(); } SchedulerEngine schedulerEngine = new SchedulerEngine(settings, Clock.systemUTC()); // the job config manager should have been created assert dataFrameJobConfigManager != null; return Collections.singletonList( new DataFrameJobPersistentTasksExecutor(client, dataFrameJobConfigManager.get(), schedulerEngine, threadPool)); }	this should call get on the setonce object dataframejobconfigmanager.get() != null
public void maybeTrack(final String request, Headers requestHeaders) { if (Regex.simpleMatch("GET /storage/v1/b/*/o/*", request)) { trackRequest("GetObject"); } else if (Regex.simpleMatch("GET /storage/v1/b/*/o*", request)) { trackRequest("ListObjGects"); } else if (Regex.simpleMatch("GET /download/storage/v1/b/*", request)) { trackRequest("GetObject"); } else if (Regex.simpleMatch("PUT /upload/storage/v1/b/*uploadType=resumable*", request) && isLastPart(requestHeaders)) { // Resumable uploads are billed as a single operation, that's the reason we're tracking // the request only when it's the last part. // See https://cloud.google.com/storage/docs/resumable-uploads#introduction trackRequest("InsertObject"); } else if (Regex.simpleMatch("POST /upload/storage/v1/b/*uploadType=multipart*", request)) { trackRequest("InsertObject"); } }	this is the wrong name? how is it passing tests?
@Override public Query toQuery(QueryParseContext parseContext) throws QueryParsingException, IOException { Query query = null; MapperService.SmartNameFieldMappers smartNameFieldMappers = parseContext.smartFieldMappers(this.fieldname); if (smartNameFieldMappers != null) { if (smartNameFieldMappers.hasMapper()) { FieldMapper mapper = smartNameFieldMappers.mapper(); if (mapper instanceof DateFieldMapper) { if ((from instanceof Number || to instanceof Number) && timeZone != null) { throw new QueryParsingException(parseContext, "[range] time_zone when using ms since epoch format as it's UTC based can not be applied to [" + this.fieldname + "]"); } DateMathParser forcedDateParser = null; if (this.format != null) { forcedDateParser = new DateMathParser(Joda.forPattern(this.format), DateFieldMapper.Defaults.TIME_UNIT); } DateTimeZone dateTimeZone = null; if (this.timeZone != null) { dateTimeZone = DateTimeZone.forID(this.timeZone); } query = ((DateFieldMapper) mapper).rangeQuery(from, to, includeLower, includeUpper, dateTimeZone, forcedDateParser, parseContext); } else { if (timeZone != null) { throw new QueryParsingException(parseContext, "[range] time_zone can not be applied to non date field [" + this.fieldname + "]"); } //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well? query = mapper.rangeQuery(from, to, includeLower, includeUpper, parseContext); } } } if (query == null) { query = new TermRangeQuery(this.fieldname, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper); } query.setBoost(boost); if (queryName != null) { parseContext.addNamedQuery(queryName, query); } return query; }	out of curiosity, did we have this check before?
protected ClusterInfoService newClusterInfoService(Settings settings, ClusterService clusterService, ThreadPool threadPool, NodeClient client, Consumer<ClusterInfo> listeners) { return new InternalClusterInfoService(settings, clusterService, threadPool, client, listeners); }	minor nit: listeners -> listener
private void handleIncomingRequest(final HttpRequest httpRequest, final HttpChannel httpChannel, final Exception exception) { Exception badRequestCause = exception; /* * We want to create a REST request from the incoming request from Netty. However, creating this request could fail if there * are incorrectly encoded parameters, or the Content-Type header is invalid. If one of these specific failures occurs, we * attempt to create a REST request again without the input that caused the exception (e.g., we remove the Content-Type header, * or skip decoding the parameters). Once we have a request in hand, we then dispatch the request as a bad request with the * underlying exception that caused us to treat the request as bad. */ final RestRequest restRequest; { RestRequest innerRestRequest; try { innerRestRequest = RestRequest.request(xContentRegistry, httpRequest, httpChannel); } catch (final RestRequest.ContentTypeHeaderException e) { badRequestCause = ExceptionsHelper.useOrSuppress(badRequestCause, e); innerRestRequest = requestWithoutContentTypeHeader(httpRequest, httpChannel, badRequestCause); } catch (final RestRequest.BadParameterException e) { badRequestCause = ExceptionsHelper.useOrSuppress(badRequestCause, e); innerRestRequest = RestRequest.requestWithoutParameters(xContentRegistry, httpRequest, httpChannel); } catch (final RestRequest.CompatibleApiHeadersCombinationException e){ badRequestCause = ExceptionsHelper.useOrSuppress(badRequestCause, e); //todo // tempting to just rethrow. removing content type is just making this less obvious throw e; // innerRestRequest = requestWithoutContentTypeHeader(httpRequest, httpChannel, badRequestCause); } restRequest = innerRestRequest; } final HttpTracer trace = tracer.maybeTraceRequest(restRequest, exception); /* * We now want to create a channel used to send the response on. However, creating this channel can fail if there are invalid * parameter values for any of the filter_path, human, or pretty parameters. We detect these specific failures via an * IllegalArgumentException from the channel constructor and then attempt to create a new channel that bypasses parsing of these * parameter values. */ final RestChannel channel; { RestChannel innerChannel; ThreadContext threadContext = threadPool.getThreadContext(); try { innerChannel = new DefaultRestChannel(httpChannel, httpRequest, restRequest, bigArrays, handlingSettings, threadContext, trace); } catch (final IllegalArgumentException e) { badRequestCause = ExceptionsHelper.useOrSuppress(badRequestCause, e); final RestRequest innerRequest = RestRequest.requestWithoutParameters(xContentRegistry, httpRequest, httpChannel); innerChannel = new DefaultRestChannel(httpChannel, httpRequest, innerRequest, bigArrays, handlingSettings, threadContext, trace); } channel = innerChannel; } dispatchRequest(restRequest, channel, badRequestCause); }	can you clean up the this todo abit ? not sure i follow.
* @param httpRequest the http request * @param httpChannel the http channel * @throws BadParameterException if the parameters can not be decoded * @throws ContentTypeHeaderException if the Content-Type header can not be parsed */ public static RestRequest request(NamedXContentRegistry xContentRegistry, HttpRequest httpRequest, HttpChannel httpChannel) { Map<String, String> params = params(httpRequest.uri()); String path = path(httpRequest.uri()); return new RestRequest(xContentRegistry, params, path, httpRequest.getHeaders(), httpRequest, httpChannel, requestIdGenerator.incrementAndGet()); }	i think this can be greatly simplified and read easier if this returns a boolean and only concerns itself with the version (not the supported media types (that can happen later if need be)). i _think_ something like this below the same level of validation. private void addcompatibleparameter() { if (isrequestingcompatibility()) { params().put(compatibleconstants.compatible_params_key, string.valueof(version.current.major - 1)); param(compatibleconstants.compatible_params_key); } } private boolean isrequestingcompatibility() { string acceptheader = header(compatibleconstants.compatible_accept_header); string aversion = xcontenttype.parseversion(acceptheader); byte acceptversion = aversion == null ? version.current.major : integer.valueof(aversion).bytevalue(); string contenttypeheader = header(compatibleconstants.compatible_content_type_header); string cversion = xcontenttype.parseversion(contenttypeheader); byte contenttypeversion = cversion == null ? version.current.major : integer.valueof(cversion).bytevalue(); if(version.current.major < acceptversion || version.current.major - acceptversion > 1 ){ throw new illegalstateexception("something about only 1 major version support"); } if (hascontent()) { if(version.current.major < contenttypeversion || version.current.major - contenttypeversion > 1 ){ throw new illegalstateexception("something about only 1 major version support"); } if (contenttypeversion != acceptversion) { throw new illegalstateexception("something about needing to match"); } } return contenttypeversion < version.current.major || acceptversion < version.current.major; }
protected boolean isDataOnlyNode(ClusterState state) { return ((isMasterEligibleNode(state) == false) && (state.nodes().localNode().dataNode() == true)); }	the == true is obsolete
* @param currentMetaData the current index state in memory. * @return iterable over all indices states that should be written to disk */ public static Iterable<GatewayMetaState.IndexMetaWriteInfo> filterStateOnDataNode(ClusterChangedEvent event, MetaData currentMetaData, MetaStateService metaStateService, ESLogger logger) { Map<String, GatewayMetaState.IndexMetaWriteInfo> indicesToWrite = new HashMap<>(); RoutingNode thisNode = event.state().getRoutingNodes().node(event.state().nodes().localNodeId()); if (thisNode == null) { // this needs some other handling return indicesToWrite.values(); } // iterate over all shards allocated on this node in the new cluster state but only write if ... for (MutableShardRouting shardRouting : thisNode) { IndexMetaData indexMetaData = event.state().metaData().index(shardRouting.index()); IndexMetaData currentIndexMetaData = maybeLoadIndexState(currentMetaData, indexMetaData, metaStateService, logger); String writeReason = null; // ... state persistence was disabled or index was newly created if (currentIndexMetaData == null) { writeReason = "freshly created"; // ... new shard is allocated on node (we could optimize here and make sure only written once and not for each shard per index -> do later) } else if (shardRouting.initializing()) { writeReason = "newly allocated on node"; // ... version changed } else if (indexMetaData.version() != currentIndexMetaData.version()) { writeReason = "version changed from [" + currentIndexMetaData.version() + "] to [" + indexMetaData.version() + "]"; } if (writeReason != null) { indicesToWrite.put(shardRouting.index(), new GatewayMetaState.IndexMetaWriteInfo(indexMetaData, currentIndexMetaData, writeReason)); } } return indicesToWrite.values(); }	not sure if we should check here if we wrote the state already.
private void assertGeoDistanceRangeQuery(String query, double lat, double lon, double distance, DistanceUnit distanceUnit) throws IOException { Query parsedQuery = parseQuery(query).toQuery(createShardContext()); assertEquals(parsedQuery.toString(), "mapped_geo_point:" + lat + "," + lon + " +/- " + distanceUnit.toMeters(distance) + " meters"); }	as far as i can see all produced queries are latlonpointqueries wrapped in a indexordocvaluesquery. would it be possible to assert that and then cast the query to latlonpointquery and then use the getters to avoid having to use "tostring"? or do you think that would be to easy to break in the future?
void run() throws InterruptedException; } public interface IOInterruptible { void run() throws IOException, InterruptedException; } public static class ExecutionCancelledException extends ElasticsearchException { public ExecutionCancelledException(String msg) { super(msg); } public ExecutionCancelledException(StreamInput in) throws IOException { super(in); } } /** * Registers a callback that will be invoked when some running operations are cancelled or {@link #checkForCancel()} is called. */ public synchronized void setOnCancel(OnCancel onCancel) { this.onCancel = onCancel; } @FunctionalInterface public interface OnCancel { /** * Called when some running operations are cancelled or {@link #checkForCancel()} is explicitly called. * If this method throws an exception, cancelling tasks will fail with that exception; otherwise they * will fail with the default exception {@link ExecutionCancelledException}	if you do that is should be using setonce<oncancel> to make sure we never override it. the other option is to call it addoncancle and use a list?
private Set<String> innerResolve(Context context, List<String> expressions, IndicesOptions options, MetaData metaData) { Set<String> result = null; boolean wildcardSeen = false; boolean isDeprecated = false; for (int i = 0; i < expressions.size(); i++) { String expression = expressions.get(i); if (aliasOrIndexExists(metaData, expression)) { if (result != null) { result.add(expression); } continue; } if (Strings.isEmpty(expression)) { throw infe(expression); } boolean add = true; if (expression.charAt(0) == '+') { // if its the first, add empty result set isDeprecated = true; if (i == 0) { result = new HashSet<>(); } expression = expression.substring(1); } else if (expression.charAt(0) == '-') { // if there is a negation without a wildcard being previously seen, add it verbatim, // otherwise return the expression if (wildcardSeen) { add = false; expression = expression.substring(1); } else { add = true; } } if (result == null) { // add all the previous ones... result = new HashSet<>(expressions.subList(0, i)); } if (!Regex.isSimpleMatchPattern(expression)) { if (!unavailableIgnoredOrExists(options, metaData, expression)) { throw infe(expression); } if (add) { result.add(expression); } else { result.remove(expression); } continue; } final IndexMetaData.State excludeState = excludeState(options); final Map<String, AliasOrIndex> matches = matches(metaData, expression); Set<String> expand = expand(context, excludeState, matches); if (add) { result.addAll(expand); } else { result.removeAll(expand); } if (!noIndicesAllowedOrMatches(options, matches)) { throw infe(expression); } if (Regex.isSimpleMatchPattern(expression)) { wildcardSeen = true; } } if(isDeprecated) { DEPRECATION_LOGGER.deprecated("use of + is deprecated in index names as it is implicit"); } return result; }	can you call it plusseen?
public void testIsPatternMatchingAllIndicesMatchingTrailingWildcardAndExclusion() throws Exception { String[] indicesOrAliases = new String[]{"index*", "-index1", "index1"}; String[] concreteIndices = new String[]{"index1", "index2", "index3"}; MetaData metaData = metaDataBuilder(concreteIndices); assertThat(indexNameExpressionResolver.isPatternMatchingAllIndices(metaData, indicesOrAliases, concreteIndices), equalTo(true)); }	i think the tests that you deleted around '+' should be added back as a separate method. you did that for wildcard expressions but not for concrete names.
private void testAllDocsWithoutStringField(String executionHint) throws IOException { try (Directory dir = newDirectory()) { try (RandomIndexWriter writer = new RandomIndexWriter(random(), dir)) { Document d = new Document(); d.add(new SortedDocValuesField("f", new BytesRef("f"))); writer.addDocument(new Document()); try (IndexReader reader = maybeWrapReaderEs(writer.getReader())) { IndexSearcher searcher = newIndexSearcher(reader); SignificantTermsAggregationBuilder request = new SignificantTermsAggregationBuilder("f").field("f") .executionHint(executionHint); SignificantStringTerms result = search(searcher, new MatchAllDocsQuery(), request, keywordField("f")); assertThat(result.getSubsetSize(), equalTo(1L)); } } } }	does d not get added to the index? why are we even creating this document?
private void testAllDocsWithoutStringField(String executionHint) throws IOException { try (Directory dir = newDirectory()) { try (RandomIndexWriter writer = new RandomIndexWriter(random(), dir)) { Document d = new Document(); d.add(new SortedDocValuesField("f", new BytesRef("f"))); writer.addDocument(new Document()); try (IndexReader reader = maybeWrapReaderEs(writer.getReader())) { IndexSearcher searcher = newIndexSearcher(reader); SignificantTermsAggregationBuilder request = new SignificantTermsAggregationBuilder("f").field("f") .executionHint(executionHint); SignificantStringTerms result = search(searcher, new MatchAllDocsQuery(), request, keywordField("f")); assertThat(result.getSubsetSize(), equalTo(1L)); } } } }	as above, why do we even need this document?
public void testRangeQuery() throws IOException { Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1).build(); QueryShardContext context = new QueryShardContext(0, new IndexSettings(IndexMetaData.builder("foo").settings(indexSettings).build(), indexSettings), null, null, null, null, null, xContentRegistry(), writableRegistry(), null, null, () -> nowInMillis, null); MappedFieldType ft = createDefaultFieldType(); ft.setName("field"); String date1 = "2015-10-12T14:10:55"; String date2 = "2016-04-28T11:33:52"; long instant1 = DateFormatters.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(date1)).toInstant().toEpochMilli(); long instant2 = DateFormatters.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(date2)).toInstant().toEpochMilli() + 999; ft.setIndexOptions(IndexOptions.DOCS); Query expected = new IndexOrDocValuesQuery( LongPoint.newRangeQuery("field", instant1, instant2), SortedNumericDocValuesField.newSlowRangeQuery("field", instant1, instant2)); assertEquals(expected, ft.rangeQuery(date1, date2, true, true, null, null, null, context).rewrite(new MultiReader())); ft.setIndexOptions(IndexOptions.NONE); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> ft.rangeQuery(date1, date2, true, true, null, null, null, context)); assertEquals("Cannot search on field [field] since it is not indexed.", e.getMessage()); }	we don't need this object, do we?
private void install(Terminal terminal, boolean isBatch, Path tmpRoot, Environment env) throws Exception { List<Path> deleteOnFailure = new ArrayList<>(); deleteOnFailure.add(tmpRoot); try { if (UberPluginInfo.isUberPlugin(tmpRoot)) { final UberPluginInfo uberInfo = UberPluginInfo.readFromProperties(tmpRoot); verifyPluginName(env.pluginsFile(), uberInfo.getName(), tmpRoot.getFileName().toString()); final Path uberPath = env.pluginsFile().resolve(uberInfo.getName()); terminal.println(VERBOSE, uberInfo.toString()); // creates the uber-plugin directory where all plugins are copied Files.createDirectory(uberPath); deleteOnFailure.add(uberPath); setFileAttributes(uberPath, PLUGIN_DIR_PERMS); // copy the uber-plugin properties file in the uber-plugin directory Path uberInfoDest = uberPath.resolve(ES_UBER_PLUGIN_PROPERTIES); Files.copy(tmpRoot.resolve(ES_UBER_PLUGIN_PROPERTIES), uberInfoDest); setFileAttributes(uberInfoDest, PLUGIN_FILES_PERMS); // install all sub-plugin for (String plugin : uberInfo.getPlugins()) { final PluginInfo info = PluginInfo.readFromProperties(uberInfo.getName(), tmpRoot.resolve(plugin)); installPlugin(terminal, isBatch, tmpRoot.resolve(plugin), info, env, deleteOnFailure); } // clean up installation IOUtils.rm(tmpRoot); terminal.println("-> Installed " + uberInfo.getName()); } else { final PluginInfo info = PluginInfo.readFromProperties(null, tmpRoot); installPlugin(terminal, isBatch, tmpRoot, info, env, deleteOnFailure); } } catch (Exception installProblem) { try { IOUtils.rm(deleteOnFailure.toArray(new Path[0])); } catch (IOException exceptionWhileRemovingFiles) { installProblem.addSuppressed(exceptionWhileRemovingFiles); } throw installProblem; } } /** * Installs the plugin from {@code tmpRoot}	why does installation need anything meta plugin specific? it should just copy all the contents of the elasticsearch directory right, just like a normal plugin right?
public void testPluginWithVerboseUberPlugins() throws Exception { buildFakeUberPlugin(env, "fake uber desc", "uber_plugin", "fake_plugin1,fake_plugin2"); buildFakePlugin(env, "uber_plugin", "fake desc 1", "fake_plugin1", "org.fake"); buildFakePlugin(env, "uber_plugin", "fake desc 2", "fake_plugin2", "org.fake2"); String[] params = { "-v" }; MockTerminal terminal = listPlugins(home, params); assertEquals( buildMultiline( "Plugins directory: " + env.pluginsFile(), "uber_plugin:fake_plugin1", "- Plugin information:", "Uber Plugin: uber_plugin", "Name: fake_plugin1", "Description: fake desc 1", "Version: 1.0", "Native Controller: false", "Requires Keystore: false", " * Classname: org.fake", "uber_plugin:fake_plugin2", "- Plugin information:", "Uber Plugin: uber_plugin", "Name: fake_plugin2", "Description: fake desc 2", "Version: 1.0", "Native Controller: false", "Requires Keystore: false", " * Classname: org.fake2"), terminal.getOutput()); }	it seems silly to print out the meta plugin this is part of, given it is already grouped under the the meta plugin.
public void testPluginWithVerboseUberPlugins() throws Exception { buildFakeUberPlugin(env, "fake uber desc", "uber_plugin", "fake_plugin1,fake_plugin2"); buildFakePlugin(env, "uber_plugin", "fake desc 1", "fake_plugin1", "org.fake"); buildFakePlugin(env, "uber_plugin", "fake desc 2", "fake_plugin2", "org.fake2"); String[] params = { "-v" }; MockTerminal terminal = listPlugins(home, params); assertEquals( buildMultiline( "Plugins directory: " + env.pluginsFile(), "uber_plugin:fake_plugin1", "- Plugin information:", "Uber Plugin: uber_plugin", "Name: fake_plugin1", "Description: fake desc 1", "Version: 1.0", "Native Controller: false", "Requires Keystore: false", " * Classname: org.fake", "uber_plugin:fake_plugin2", "- Plugin information:", "Uber Plugin: uber_plugin", "Name: fake_plugin2", "Description: fake desc 2", "Version: 1.0", "Native Controller: false", "Requires Keystore: false", " * Classname: org.fake2"), terminal.getOutput()); }	i think we should have better grouping to show these are part of the meta plugin in the output. perhaps printing the plugin info should be indented a few spaces under a meta plugin?
@SuppressWarnings("unchecked") public void testResolveTokenGroupsSID() throws Exception { Settings settings = Settings.builder() .put("path.home", createTempDir()) .put(RealmSettings.getFullSettingKey(REALM_ID, RealmSettings.ORDER_SETTING), 0) .put(buildAdSettings(REALM_ID, AD_LDAP_URL, AD_DOMAIN, "CN=Users,DC=ad,DC=test,DC=elasticsearch,DC=com", LdapSearchScope.SUB_TREE, false)) .put(ActiveDirectorySessionFactorySettings.AD_GROUP_SEARCH_BASEDN_SETTING, "DC=ad,DC=test,DC=elasticsearch,DC=com") .put(ActiveDirectorySessionFactorySettings.AD_GROUP_SEARCH_SCOPE_SETTING, LdapSearchScope.SUB_TREE) .put(getFullSettingKey(REALM_ID, LdapMetadataResolverSettings.ADDITIONAL_METADATA_SETTING), "tokenGroups") .build(); RealmConfig config = configureRealm("ad-test", LdapRealmSettings.AD_TYPE, settings); final PlainActionFuture<Map<String, Object>> future = new PlainActionFuture<>(); LdapMetadataResolver resolver = new LdapMetadataResolver(config, true); try (ActiveDirectorySessionFactory sessionFactory = getActiveDirectorySessionFactory(config, sslService, threadPool)) { String userName = "ironman"; try (LdapSession ldap = session(sessionFactory, userName, SECURED_PASSWORD)) { assertConnectionCanReconnect(ldap.getConnection()); resolver.resolve(ldap.getConnection(), BRUCE_BANNER_DN, TimeValue.timeValueSeconds(1), logger, null, future); Map<String, Object> metadata_groupSIDs = future.get(); assertThat(metadata_groupSIDs.size(), equalTo(1)); assertNotNull(metadata_groupSIDs.get("tokenGroups")); List<String> SIDs = ((List<String>) metadata_groupSIDs.get("tokenGroups")); assertThat(SIDs.size(), equalTo(7)); assertThat(SIDs, everyItem(matchesPattern("S-1-5-(?:21|32)-\\\\\\\\d+(?:-\\\\\\\\d+\\\\\\\\-\\\\\\\\d+\\\\\\\\-\\\\\\\\d+)?"))); } } }	since we're not using a bind user in the settings we should probably use an ldap session that is created for the user that we search foe and resolve its attributes, so use hulk here instead
@SuppressWarnings("unchecked") public void testResolveTokenGroupsSID() throws Exception { Settings settings = Settings.builder() .put("path.home", createTempDir()) .put(RealmSettings.getFullSettingKey(REALM_ID, RealmSettings.ORDER_SETTING), 0) .put(buildAdSettings(REALM_ID, AD_LDAP_URL, AD_DOMAIN, "CN=Users,DC=ad,DC=test,DC=elasticsearch,DC=com", LdapSearchScope.SUB_TREE, false)) .put(ActiveDirectorySessionFactorySettings.AD_GROUP_SEARCH_BASEDN_SETTING, "DC=ad,DC=test,DC=elasticsearch,DC=com") .put(ActiveDirectorySessionFactorySettings.AD_GROUP_SEARCH_SCOPE_SETTING, LdapSearchScope.SUB_TREE) .put(getFullSettingKey(REALM_ID, LdapMetadataResolverSettings.ADDITIONAL_METADATA_SETTING), "tokenGroups") .build(); RealmConfig config = configureRealm("ad-test", LdapRealmSettings.AD_TYPE, settings); final PlainActionFuture<Map<String, Object>> future = new PlainActionFuture<>(); LdapMetadataResolver resolver = new LdapMetadataResolver(config, true); try (ActiveDirectorySessionFactory sessionFactory = getActiveDirectorySessionFactory(config, sslService, threadPool)) { String userName = "ironman"; try (LdapSession ldap = session(sessionFactory, userName, SECURED_PASSWORD)) { assertConnectionCanReconnect(ldap.getConnection()); resolver.resolve(ldap.getConnection(), BRUCE_BANNER_DN, TimeValue.timeValueSeconds(1), logger, null, future); Map<String, Object> metadata_groupSIDs = future.get(); assertThat(metadata_groupSIDs.size(), equalTo(1)); assertNotNull(metadata_groupSIDs.get("tokenGroups")); List<String> SIDs = ((List<String>) metadata_groupSIDs.get("tokenGroups")); assertThat(SIDs.size(), equalTo(7)); assertThat(SIDs, everyItem(matchesPattern("S-1-5-(?:21|32)-\\\\\\\\d+(?:-\\\\\\\\d+\\\\\\\\-\\\\\\\\d+\\\\\\\\-\\\\\\\\d+)?"))); } } }	convention in our codebase is to use camel case so metadatagroupsids here
private void searchStats(String configId, ActionListener<Stats> listener) { logger.debug("[{}] Gathering stats for stopped task", configId); RetrievedStatsHolder retrievedStatsHolder = new RetrievedStatsHolder(); MultiSearchRequest multiSearchRequest = new MultiSearchRequest(); multiSearchRequest.add(buildStoredProgressSearch(configId)); multiSearchRequest.add(buildStatsDocSearch(configId, DataCounts.TYPE_VALUE)); multiSearchRequest.add(buildStatsDocSearch(configId, MemoryUsage.TYPE_VALUE)); multiSearchRequest.add(buildStatsDocSearch(configId, OutlierDetectionStats.TYPE_VALUE)); multiSearchRequest.add(buildStatsDocSearch(configId, ClassificationStats.TYPE_VALUE)); multiSearchRequest.add(buildStatsDocSearch(configId, RegressionStats.TYPE_VALUE)); executeAsyncWithOrigin(client, ML_ORIGIN, MultiSearchAction.INSTANCE, multiSearchRequest, ActionListener.wrap( multiSearchResponse -> { for (MultiSearchResponse.Item itemResponse : multiSearchResponse.getResponses()) { if (itemResponse.isFailure()) { logger.error("Item failure during multi search: " + itemResponse.getFailureMessage(), itemResponse.getFailure()); listener.onFailure(ExceptionsHelper.serverError(itemResponse.getFailureMessage(), itemResponse.getFailure())); return; } else { SearchHit[] hits = itemResponse.getResponse().getHits().getHits(); if (hits.length == 0) { // Not found } else if (hits.length == 1) { parseHit(hits[0], configId, retrievedStatsHolder); } else { throw ExceptionsHelper.serverError("Found [" + hits.length + "] hits when just one was requested"); } } } listener.onResponse(buildStats(configId, retrievedStatsHolder.progress.get(), retrievedStatsHolder.dataCounts, retrievedStatsHolder.memoryUsage, retrievedStatsHolder.analysisStats )); }, e -> listener.onFailure(ExceptionsHelper.serverError("Error searching for stats", e)) )); }	could we also add in the configid?
private void performOnReplica(final ShardRouting shard, final ReplicaRequest replicaRequest, final long globalCheckpoint, final long maxSeqNoOfUpdatesOrDeletes, final PendingReplicationActions pendingReplicationActions) { if (logger.isTraceEnabled()) { logger.trace("[{}] sending op [{}] to replica {} for request [{}]", shard.shardId(), opType, shard, replicaRequest); } totalShards.incrementAndGet(); pendingActions.incrementAndGet(); final ActionListener<ReplicaResponse> replicationListener = new ActionListener<>() { @Override public void onResponse(ReplicaResponse response) { successfulShards.incrementAndGet(); try { updateCheckPoints(shard, response::localCheckpoint, response::globalCheckpoint); } finally { decPendingAndFinishIfNeeded(); } } @Override public void onFailure(Exception replicaException) { logger.trace(() -> new ParameterizedMessage( "[{}] failure while performing [{}] on replica {}, request [{}]", shard.shardId(), opType, shard, replicaRequest), replicaException); // Only report "critical" exceptions - TODO: Reach out to the master node to get the latest shard state then report. if (TransportActions.isShardNotAvailableException(replicaException) == false) { RestStatus restStatus = ExceptionsHelper.status(replicaException); shardReplicaFailures.add(new ReplicationResponse.ShardInfo.Failure( shard.shardId(), shard.currentNodeId(), replicaException, restStatus, false)); } String message = String.format(Locale.ROOT, "failed to perform %s on replica %s", opType, shard); replicasProxy.failShardIfNeeded(shard, primaryTerm, message, replicaException, ActionListener.wrap(r -> decPendingAndFinishIfNeeded(), ReplicationOperation.this::onNoLongerPrimary)); } @Override public String toString() { return "[" + replicaRequest + "][" + shard + "]"; } }; final String allocationId = shard.allocationId().getId(); final RetryableAction<ReplicaResponse> replicationAction = new RetryableAction<>(logger, threadPool, TimeValue.timeValueMillis(50), replicaRequest.timeout(), replicationListener) { @Override public void tryAction(ActionListener<ReplicaResponse> listener) { replicasProxy.performOn(shard, replicaRequest, primaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, listener); } @Override public void onFinished() { super.onFinished(); pendingReplicationActions.removeReplicationAction(allocationId, this); } @Override public boolean shouldRetry(Exception e) { final Throwable cause = ExceptionsHelper.unwrapCause(e); return cause instanceof CircuitBreakingException || cause instanceof EsRejectedExecutionException || cause instanceof ConnectTransportException; } }; pendingReplicationActions.addPendingAction(allocationId, replicationAction); replicationAction.run(); }	let's introduce a new setting for this
@Override public void refresh() { SocketAccess.doPrivilegedVoid(credentialsProvider::refresh); } } /** * Customizes {@link com.amazonaws.auth.WebIdentityTokenCredentialsProvider} * * <ul> * <li>Reads the the location of the web identity token not from AWS_WEB_IDENTITY_TOKEN_FILE, but from a symlink * in the plugin directory, so we don't need to create a hardcoded read file permission for the plugin.</li> * <li>Supports customization of the STS endpoint via a system property, so we can test it against a test fixture.</li> * <li>Supports gracefully shutting down the provider and the STS client.</li> * </ul> */ static class CustomWebIdentityTokenCredentialsProvider implements AWSCredentialsProvider { private STSAssumeRoleWithWebIdentitySessionCredentialsProvider credentialsProvider; private AWSSecurityTokenService stsClient; @SuppressForbidden(reason = "Need to use PathUtils#get") CustomWebIdentityTokenCredentialsProvider() { // Check whether the original environment variable exists. If it doesn't, // the system doesn't support AWS web identity tokens if (System.getenv(AWS_WEB_IDENTITY_ENV_VAR) == null) { return; } String esPathConf = System.getProperty("es.path.conf"); if (esPathConf == null) { // Should be always present, but bail out in case it doesn't return; } // Make sure that a readable symlink to the token file exists in the plugin config directory Path webIdentityTokenFileSymlink = PathUtils.get(esPathConf).resolve("repository-s3/aws-web-identity-token-file"); if (Files.isReadable(webIdentityTokenFileSymlink) == false) { return; } String roleArn = System.getenv(AWS_ROLE_ARN_ENV_VAR); String roleSessionName = System.getenv(AWS_ROLE_SESSION_NAME_ENV_VAR); if (roleArn == null || roleSessionName == null) { return; } AWSSecurityTokenServiceClientBuilder stsClientBuilder = AWSSecurityTokenServiceClient.builder(); // Just for testing String customStsEndpoint = System.getProperty("com.amazonaws.sdk.stsMetadataServiceEndpointOverride"); if (customStsEndpoint != null) { stsClientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(customStsEndpoint, null)); } stsClientBuilder.withCredentials(new AWSStaticCredentialsProvider(new AnonymousAWSCredentials())); stsClient = SocketAccess.doPrivileged(stsClientBuilder::build); credentialsProvider = new STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder( roleArn, roleSessionName, webIdentityTokenFileSymlink.toString() ).withStsClient(stsClient).build(); } boolean isActive() { return credentialsProvider != null; } @Override public AWSCredentials getCredentials() { return credentialsProvider != null ? credentialsProvider.getCredentials() : new AnonymousAWSCredentials(); } @Override public void refresh() { if (credentialsProvider != null) { credentialsProvider.refresh(); } } public void shutdown() { if (credentialsProvider != null) { credentialsProvider.close(); stsClient.shutdown(); } }	we should retrieve the plugin's configuration directory from the environment instead of relying on the sysprop. maybe you can pass it down from the plugin to the s3service instance.
@Override public void refresh() { SocketAccess.doPrivilegedVoid(credentialsProvider::refresh); } } /** * Customizes {@link com.amazonaws.auth.WebIdentityTokenCredentialsProvider} * * <ul> * <li>Reads the the location of the web identity token not from AWS_WEB_IDENTITY_TOKEN_FILE, but from a symlink * in the plugin directory, so we don't need to create a hardcoded read file permission for the plugin.</li> * <li>Supports customization of the STS endpoint via a system property, so we can test it against a test fixture.</li> * <li>Supports gracefully shutting down the provider and the STS client.</li> * </ul> */ static class CustomWebIdentityTokenCredentialsProvider implements AWSCredentialsProvider { private STSAssumeRoleWithWebIdentitySessionCredentialsProvider credentialsProvider; private AWSSecurityTokenService stsClient; @SuppressForbidden(reason = "Need to use PathUtils#get") CustomWebIdentityTokenCredentialsProvider() { // Check whether the original environment variable exists. If it doesn't, // the system doesn't support AWS web identity tokens if (System.getenv(AWS_WEB_IDENTITY_ENV_VAR) == null) { return; } String esPathConf = System.getProperty("es.path.conf"); if (esPathConf == null) { // Should be always present, but bail out in case it doesn't return; } // Make sure that a readable symlink to the token file exists in the plugin config directory Path webIdentityTokenFileSymlink = PathUtils.get(esPathConf).resolve("repository-s3/aws-web-identity-token-file"); if (Files.isReadable(webIdentityTokenFileSymlink) == false) { return; } String roleArn = System.getenv(AWS_ROLE_ARN_ENV_VAR); String roleSessionName = System.getenv(AWS_ROLE_SESSION_NAME_ENV_VAR); if (roleArn == null || roleSessionName == null) { return; } AWSSecurityTokenServiceClientBuilder stsClientBuilder = AWSSecurityTokenServiceClient.builder(); // Just for testing String customStsEndpoint = System.getProperty("com.amazonaws.sdk.stsMetadataServiceEndpointOverride"); if (customStsEndpoint != null) { stsClientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(customStsEndpoint, null)); } stsClientBuilder.withCredentials(new AWSStaticCredentialsProvider(new AnonymousAWSCredentials())); stsClient = SocketAccess.doPrivileged(stsClientBuilder::build); credentialsProvider = new STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder( roleArn, roleSessionName, webIdentityTokenFileSymlink.toString() ).withStsClient(stsClient).build(); } boolean isActive() { return credentialsProvider != null; } @Override public AWSCredentials getCredentials() { return credentialsProvider != null ? credentialsProvider.getCredentials() : new AnonymousAWSCredentials(); } @Override public void refresh() { if (credentialsProvider != null) { credentialsProvider.refresh(); } } public void shutdown() { if (credentialsProvider != null) { credentialsProvider.close(); stsClient.shutdown(); } }	this looks wrong to me, we should always expect to have a configuration directory
@Override public void refresh() { SocketAccess.doPrivilegedVoid(credentialsProvider::refresh); } } /** * Customizes {@link com.amazonaws.auth.WebIdentityTokenCredentialsProvider} * * <ul> * <li>Reads the the location of the web identity token not from AWS_WEB_IDENTITY_TOKEN_FILE, but from a symlink * in the plugin directory, so we don't need to create a hardcoded read file permission for the plugin.</li> * <li>Supports customization of the STS endpoint via a system property, so we can test it against a test fixture.</li> * <li>Supports gracefully shutting down the provider and the STS client.</li> * </ul> */ static class CustomWebIdentityTokenCredentialsProvider implements AWSCredentialsProvider { private STSAssumeRoleWithWebIdentitySessionCredentialsProvider credentialsProvider; private AWSSecurityTokenService stsClient; @SuppressForbidden(reason = "Need to use PathUtils#get") CustomWebIdentityTokenCredentialsProvider() { // Check whether the original environment variable exists. If it doesn't, // the system doesn't support AWS web identity tokens if (System.getenv(AWS_WEB_IDENTITY_ENV_VAR) == null) { return; } String esPathConf = System.getProperty("es.path.conf"); if (esPathConf == null) { // Should be always present, but bail out in case it doesn't return; } // Make sure that a readable symlink to the token file exists in the plugin config directory Path webIdentityTokenFileSymlink = PathUtils.get(esPathConf).resolve("repository-s3/aws-web-identity-token-file"); if (Files.isReadable(webIdentityTokenFileSymlink) == false) { return; } String roleArn = System.getenv(AWS_ROLE_ARN_ENV_VAR); String roleSessionName = System.getenv(AWS_ROLE_SESSION_NAME_ENV_VAR); if (roleArn == null || roleSessionName == null) { return; } AWSSecurityTokenServiceClientBuilder stsClientBuilder = AWSSecurityTokenServiceClient.builder(); // Just for testing String customStsEndpoint = System.getProperty("com.amazonaws.sdk.stsMetadataServiceEndpointOverride"); if (customStsEndpoint != null) { stsClientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(customStsEndpoint, null)); } stsClientBuilder.withCredentials(new AWSStaticCredentialsProvider(new AnonymousAWSCredentials())); stsClient = SocketAccess.doPrivileged(stsClientBuilder::build); credentialsProvider = new STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder( roleArn, roleSessionName, webIdentityTokenFileSymlink.toString() ).withStsClient(stsClient).build(); } boolean isActive() { return credentialsProvider != null; } @Override public AWSCredentials getCredentials() { return credentialsProvider != null ? credentialsProvider.getCredentials() : new AnonymousAWSCredentials(); } @Override public void refresh() { if (credentialsProvider != null) { credentialsProvider.refresh(); } } public void shutdown() { if (credentialsProvider != null) { credentialsProvider.close(); stsClient.shutdown(); } }	sorry for my ignorance about sts/web identity tokens, but is it possible to have multiple tokens per node? it looks like the credentials to access a s3 repository are defined by the presence or not of system properties; does it mean that we cannot register non-sts s3 repositories on the data node?
@Override public void refresh() { SocketAccess.doPrivilegedVoid(credentialsProvider::refresh); } } /** * Customizes {@link com.amazonaws.auth.WebIdentityTokenCredentialsProvider} * * <ul> * <li>Reads the the location of the web identity token not from AWS_WEB_IDENTITY_TOKEN_FILE, but from a symlink * in the plugin directory, so we don't need to create a hardcoded read file permission for the plugin.</li> * <li>Supports customization of the STS endpoint via a system property, so we can test it against a test fixture.</li> * <li>Supports gracefully shutting down the provider and the STS client.</li> * </ul> */ static class CustomWebIdentityTokenCredentialsProvider implements AWSCredentialsProvider { private STSAssumeRoleWithWebIdentitySessionCredentialsProvider credentialsProvider; private AWSSecurityTokenService stsClient; @SuppressForbidden(reason = "Need to use PathUtils#get") CustomWebIdentityTokenCredentialsProvider() { // Check whether the original environment variable exists. If it doesn't, // the system doesn't support AWS web identity tokens if (System.getenv(AWS_WEB_IDENTITY_ENV_VAR) == null) { return; } String esPathConf = System.getProperty("es.path.conf"); if (esPathConf == null) { // Should be always present, but bail out in case it doesn't return; } // Make sure that a readable symlink to the token file exists in the plugin config directory Path webIdentityTokenFileSymlink = PathUtils.get(esPathConf).resolve("repository-s3/aws-web-identity-token-file"); if (Files.isReadable(webIdentityTokenFileSymlink) == false) { return; } String roleArn = System.getenv(AWS_ROLE_ARN_ENV_VAR); String roleSessionName = System.getenv(AWS_ROLE_SESSION_NAME_ENV_VAR); if (roleArn == null || roleSessionName == null) { return; } AWSSecurityTokenServiceClientBuilder stsClientBuilder = AWSSecurityTokenServiceClient.builder(); // Just for testing String customStsEndpoint = System.getProperty("com.amazonaws.sdk.stsMetadataServiceEndpointOverride"); if (customStsEndpoint != null) { stsClientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(customStsEndpoint, null)); } stsClientBuilder.withCredentials(new AWSStaticCredentialsProvider(new AnonymousAWSCredentials())); stsClient = SocketAccess.doPrivileged(stsClientBuilder::build); credentialsProvider = new STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder( roleArn, roleSessionName, webIdentityTokenFileSymlink.toString() ).withStsClient(stsClient).build(); } boolean isActive() { return credentialsProvider != null; } @Override public AWSCredentials getCredentials() { return credentialsProvider != null ? credentialsProvider.getCredentials() : new AnonymousAWSCredentials(); } @Override public void refresh() { if (credentialsProvider != null) { credentialsProvider.refresh(); } } public void shutdown() { if (credentialsProvider != null) { credentialsProvider.close(); stsClient.shutdown(); } }	i think we should just fail in the case a file exist but is not readable?
private void deleteState(ParentTaskAssigningClient parentTaskClient, DataFrameAnalyticsConfig config, TimeValue timeout, ActionListener<BulkByScrollResponse> listener) { ActionListener<Boolean> deleteModelStateListener = ActionListener.wrap( r -> executeDeleteByQuery( parentTaskClient, AnomalyDetectorsIndex.jobStateIndexPattern(), QueryBuilders.idsQuery().addIds(StoredProgress.documentId(config.getId())), timeout, listener ) , listener::onFailure ); deleteModelState(parentTaskClient, config, timeout, 1, deleteModelStateListener); }	does unusedstateremover class need to be changed too?
private void deleteModelState(ParentTaskAssigningClient parentTaskClient, DataFrameAnalyticsConfig config, TimeValue timeout, int docNum, ActionListener<Boolean> listener) { if (config.getAnalysis().persistsState() == false) { listener.onResponse(true); return; } IdsQueryBuilder query = QueryBuilders.idsQuery().addIds(config.getAnalysis().getStateDocIdPrefix(config.getId()) + docNum); executeDeleteByQuery( parentTaskClient, AnomalyDetectorsIndex.jobStateIndexPattern(), query, timeout, ActionListener.wrap( response -> { if (response.getDeleted() > 0) { deleteModelState(parentTaskClient, config, timeout, docNum + 1, listener); return; } listener.onResponse(true); }, listener::onFailure ) ); }	is this recursion needed? would it be possible to delete all the #1, #2, etc. in one deletebyquery request? i know we don't know the number of state docs in advance, but maybe we could apply some id pattern?
@Override public void restoreState(Client client, String stateDocIdPrefix) throws IOException { Objects.requireNonNull(stateDocIdPrefix); try (OutputStream restoreStream = processRestoreStream()) { int docNum = 0; while (true) { if (isProcessKilled()) { return; } SearchResponse stateResponse = client.prepareSearch(AnomalyDetectorsIndex.jobStateIndexPattern()) .setSize(1) .setQuery(QueryBuilders.idsQuery().addIds(stateDocIdPrefix + ++docNum)).get(); if (stateResponse.getHits().getHits().length == 0) { break; } StateToProcessWriterHelper.writeStateToStream(stateResponse.getHits().getAt(0).getSourceRef(), restoreStream); } } }	this comment is somewhat related to the one about using recursion. could we fetch all the state docs at once using some pattern-matched query? if not, and if we expect many state docs to exist, we could try to fetch them in batches (by adding ids #1, #2, ..., #10 to one ids query) to reduce the number of queries. please disregard the above if we expect the number of docs to be small so there is no need for premature optimization.
private void processNormalizerFactory( String name, AnalyzerProvider<?> normalizerFactory, Map<String, NamedAnalyzer> normalizers, TokenizerFactory tokenizerFactory, Map<String, TokenFilterFactory> tokenFilters, Map<String, CharFilterFactory> charFilters) { if (tokenizerFactory == null) { throw new IllegalStateException("keyword tokenizer factory is null, normalizers require analysis-common module"); } if (normalizerFactory instanceof CustomNormalizerProvider) { ((CustomNormalizerProvider) normalizerFactory).build(tokenizerFactory, charFilters, tokenFilters); } if (normalizers.containsKey(name)) { throw new IllegalStateException("already registered analyzer with name: " + name); } Analyzer normalizerF = normalizerFactory.get(); if (normalizerF == null) { throw new IllegalArgumentException("normalizer [" + normalizerFactory.name() + "] created null normalizer"); } NamedAnalyzer normalizer = new NamedAnalyzer(name, normalizerFactory.scope(), normalizerF); normalizers.put(name, normalizer); }	we also throw exceptions in some cases so it's not only about deprecations ?
public void testDeprecationWarnings() throws IOException { AnalyzeAction.Request req = new AnalyzeAction.Request(); req.tokenizer("standard"); req.addTokenFilter("lowercase"); req.addTokenFilter("deprecated"); req.text("test text"); AnalyzeAction.Response analyze = TransportAnalyzeAction.analyze(req, registry, mockIndexService(), maxTokenCount); assertEquals(2, analyze.getTokens().size()); assertWarnings("Using deprecated token filter [deprecated]"); // normalizer req = new AnalyzeAction.Request(); req.addTokenFilter("lowercase"); req.addTokenFilter("deprecated"); req.text("text"); analyze = TransportAnalyzeAction.analyze(req, registry, mockIndexService(), maxTokenCount); assertEquals(1, analyze.getTokens().size()); assertWarnings("Using deprecated token filter [deprecated]"); }	just a small observation: i thought this checks that the normalize method in deprecatedtokenfilterfactory, so i tried changing it but the test didn't fail, do you know why?
public TokenStream create(TokenStream tokenStream) { if (name().equals("test_filter")) { return new MockTokenFilter(tokenStream, MockTokenFilter.EMPTY_STOPSET); } return new MockTokenFilter(tokenStream, MockTokenFilter.ENGLISH_STOPSET); } } @Override public Map<String, AnalysisProvider<TokenFilterFactory>> getTokenFilters() { return singletonMap("mock", MockFactory::new); } }; IndexAnalyzers indexAnalyzers = new AnalysisModule(TestEnvironment.newEnvironment(settings), singletonList(plugin)).getAnalysisRegistry().build(idxSettings); // This shouldn't contain English stopwords try (NamedAnalyzer custom_analyser = indexAnalyzers.get("custom_analyzer_with_camel_case")) { assertNotNull(custom_analyser); TokenStream tokenStream = custom_analyser.tokenStream("foo", "has a foo"); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class); assertTrue(tokenStream.incrementToken()); assertEquals("has", charTermAttribute.toString()); assertTrue(tokenStream.incrementToken()); assertEquals("foo", charTermAttribute.toString()); assertFalse(tokenStream.incrementToken()); } // This *should* contain English stopwords try (NamedAnalyzer custom_analyser = indexAnalyzers.get("custom_analyzer_with_snake_case")) { assertNotNull(custom_analyser); TokenStream tokenStream = custom_analyser.tokenStream("foo", "has a foo"); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class); assertTrue(tokenStream.incrementToken()); assertEquals("has", charTermAttribute.toString()); assertTrue(tokenStream.incrementToken()); assertEquals("a", charTermAttribute.toString()); assertTrue(tokenStream.incrementToken()); assertEquals("foo", charTermAttribute.toString()); assertFalse(tokenStream.incrementToken()); } } public void testBuiltInAnalyzersAreCached() throws IOException { Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build(); Settings indexSettings = Settings.builder() .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build(); IndexSettings idxSettings = IndexSettingsModule.newIndexSettings("index", indexSettings); IndexAnalyzers indexAnalyzers = emptyAnalysisRegistry(settings).build(idxSettings); IndexAnalyzers otherIndexAnalyzers = emptyAnalysisRegistry(settings).build(idxSettings); final int numIters = randomIntBetween(5, 20); for (int i = 0; i < numIters; i++) { PreBuiltAnalyzers preBuiltAnalyzers = RandomPicks.randomFrom(random(), PreBuiltAnalyzers.values()); assertSame(indexAnalyzers.get(preBuiltAnalyzers.name()), otherIndexAnalyzers.get(preBuiltAnalyzers.name())); } } public void testNoTypeOrTokenizerErrorMessage() throws IOException { Version version = VersionUtils.randomVersion(random()); Settings settings = Settings .builder() .put(IndexMetaData.SETTING_VERSION_CREATED, version) .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()) .putList("index.analysis.analyzer.test_analyzer.filter", new String[] {"lowercase", "stop", "shingle"}) .putList("index.analysis.analyzer.test_analyzer.char_filter", new String[] {"html_strip"}) .build(); IndexSettings idxSettings = IndexSettingsModule.newIndexSettings("index", settings); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> emptyAnalysisRegistry(settings).build(idxSettings)); assertThat(e.getMessage(), equalTo("analyzer [test_analyzer] must specify either an analyzer type, or a tokenizer")); } public void testCloseIndexAnalyzersMultipleTimes() throws IOException { IndexAnalyzers indexAnalyzers = emptyRegistry.build(indexSettingsOfCurrentVersion(Settings.builder())); indexAnalyzers.close(); indexAnalyzers.close(); } public void testEnsureCloseInvocationProperlyDelegated() throws IOException { Settings settings = Settings.builder() .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()) .build(); PreBuiltAnalyzerProviderFactory mock = mock(PreBuiltAnalyzerProviderFactory.class); AnalysisRegistry registry = new AnalysisRegistry(TestEnvironment.newEnvironment(settings), emptyMap(), emptyMap(), emptyMap(), emptyMap(), emptyMap(), emptyMap(), emptyMap(), emptyMap(), Collections.singletonMap("key", mock)); registry.close(); verify(mock).close(); } public void testDeprecations() throws IOException { AnalysisPlugin plugin = new AnalysisPlugin() { class MockFactory extends AbstractTokenFilterFactory { MockFactory(IndexSettings indexSettings, Environment env, String name, Settings settings) { super(indexSettings, name, settings); } @Override public TokenStream create(TokenStream tokenStream) { if (indexSettings.getIndexVersionCreated().equals(Version.CURRENT)) { deprecationLogger.deprecated("Using deprecated token filter [deprecated]"); } return tokenStream; } } class UnusedMockFactory extends AbstractTokenFilterFactory { UnusedMockFactory(IndexSettings indexSettings, Environment env, String name, Settings settings) { super(indexSettings, name, settings); } @Override public TokenStream create(TokenStream tokenStream) { deprecationLogger.deprecated("Using deprecated token filter [unused]"); return tokenStream; } } class NormalizerFactory extends AbstractTokenFilterFactory implements NormalizingTokenFilterFactory { NormalizerFactory(IndexSettings indexSettings, Environment env, String name, Settings settings) { super(indexSettings, name, settings); } @Override public TokenStream create(TokenStream tokenStream) { deprecationLogger.deprecated("Using deprecated token filter [deprecated_normalizer]"); return tokenStream; } }	can you also add a test that throws an exception ?
private static List<RecoveryState> getActiveRelocations(String restoredIndex) { return client().admin() .indices() .prepareRecoveries(restoredIndex) .setDetailed(true) .setActiveOnly(true) .get() .shardRecoveryStates() .get(restoredIndex) .stream() .filter(recoveryState -> recoveryState.getSourceNode() != null) .collect(Collectors.toList()); }	maybe we could check at the end that the pre-warming phase ended?
void innerUpdatePipelines(ClusterState previousState, ClusterState state) { if (state.blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK)) { return; } IngestMetadata ingestMetadata = state.getMetaData().custom(IngestMetadata.TYPE); IngestMetadata previousIngestMetadata = previousState.getMetaData().custom(IngestMetadata.TYPE); if (Objects.equals(ingestMetadata, previousIngestMetadata)) { return; } Map<String, Pipeline> pipelines = new HashMap<>(); ArrayList<ElasticsearchParseException> exceptions = new ArrayList<>(); for (PipelineConfiguration pipeline : ingestMetadata.getPipelines().values()) { try { pipelines.put(pipeline.getId(), factory.create(pipeline.getId(), pipeline.getConfigAsMap(), processorFactories)); } catch (ElasticsearchParseException e) { pipelines.put(pipeline.getId(), Pipeline.EMPTY); exceptions.add(e); } catch (Exception e) { pipelines.put(pipeline.getId(), Pipeline.EMPTY); exceptions.add(new ElasticsearchParseException("Error updating pipeline with id [" + pipeline.getId() + "]", e)); } } this.pipelines = Collections.unmodifiableMap(pipelines); ExceptionsHelper.rethrowAndSuppress(exceptions); }	i wonder if we can create a pipeline with a single fail processor. this fail processor can then contain the reason why the pipeline in question couldn't be loaded. this way there is no need to check the logs.
@Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } BucketCountThresholds other = (BucketCountThresholds) obj; return Objects.equals(requiredSize, other.requiredSize) && Objects.equals(shardSize, other.shardSize) && Objects.equals(minDocCount, other.minDocCount) && Objects.equals(shardMinDocCount, other.shardMinDocCount); } } protected final DocValueFormat format; protected final BucketCountThresholds bucketCountThresholds; protected final BucketOrder order; protected final Comparator<InternalTerms.Bucket<?>> partiallyBuiltBucketComparator; protected final Set<Aggregator> aggsUsedForSorting = new HashSet<>(); protected final SubAggCollectionMode collectMode; public TermsAggregator(String name, AggregatorFactories factories, SearchContext context, Aggregator parent, BucketCountThresholds bucketCountThresholds, BucketOrder order, DocValueFormat format, SubAggCollectionMode collectMode, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException { super(name, factories, context, parent, pipelineAggregators, metaData); this.bucketCountThresholds = bucketCountThresholds; this.order = order; partiallyBuiltBucketComparator = order == null ? null : order.partiallyBuiltBucketComparator(b -> b.bucketOrd, this); this.format = format; if (subAggsNeedScore() && descendsFromNestedAggregator(parent)) { /** * Force the execution to depth_first because we need to access the score of * nested documents in a sub-aggregation and we are not able to generate this score * while replaying deferred documents. */ this.collectMode = SubAggCollectionMode.DEPTH_FIRST; } else { this.collectMode = collectMode; } // Don't defer any child agg if we are dependent on it for pruning results if (order instanceof Aggregation){ AggregationPath path = ((Aggregation) order).path(); aggsUsedForSorting.add(path.resolveTopmostAggregator(this)); } else if (order instanceof CompoundOrder) { CompoundOrder compoundOrder = (CompoundOrder) order; for (BucketOrder orderElement : compoundOrder.orderElements()) { if (orderElement instanceof Aggregation) { AggregationPath path = ((Aggregation) orderElement).path(); aggsUsedForSorting.add(path.resolveTopmostAggregator(this)); } } } } static boolean descendsFromNestedAggregator(Aggregator parent) { while (parent != null) { if (parent.getClass() == NestedAggregator.class) { return true; } parent = parent.parent(); } return false; } private boolean subAggsNeedScore() { for (Aggregator subAgg : subAggregators) { if (subAgg.scoreMode().needsScores()) { return true; } } return false; }	possible i'm missing something, but couldn't this npe if an order isn't set? e.g. order == null which means partiallybuiltbucketcomparator == null, and then later when we use it in a pq we pass in a null comparator and the pq chokes (because comparator.compare() npes)?
long docVersion(int segmentDocId) throws IOException { assert versionDV.docID() < segmentDocId; if (versionDV.advanceExact(segmentDocId) == false) { assert false : "DocValues for field [" + VersionFieldMapper.NAME + "] is not found"; throw new IllegalStateException("DocValues for field [" + VersionFieldMapper.NAME + "] is not found"); } return versionDV.longValue(); }	what is the point of doubling the exception with an assertion?
private void restoreVersionMapAndCheckpointTracker(DirectoryReader directoryReader) throws IOException { final IndexSearcher searcher = new IndexSearcher(directoryReader); searcher.setQueryCache(null); final Query query = new BooleanQuery.Builder() .add(LongPoint.newRangeQuery( SeqNoFieldMapper.NAME, getPersistedLocalCheckpoint() + 1, Long.MAX_VALUE), BooleanClause.Occur.MUST) .add(Queries.newNonNestedFilter(), BooleanClause.Occur.MUST) // exclude non-root nested documents .build(); final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1.0f); for (LeafReaderContext leaf : directoryReader.leaves()) { final Scorer scorer = weight.scorer(leaf); if (scorer == null) { continue; } final CombinedDocValues dv = new CombinedDocValues(leaf.reader()); final IdOnlyFieldVisitor idFieldVisitor = new IdOnlyFieldVisitor(); final DocIdSetIterator iterator = scorer.iterator(); int docId; while ((docId = iterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) { final long primaryTerm = dv.docPrimaryTerm(docId); final long seqNo = dv.docSeqNo(docId); localCheckpointTracker.markSeqNoAsProcessed(seqNo); localCheckpointTracker.markSeqNoAsPersisted(seqNo); idFieldVisitor.reset(); leaf.reader().document(docId, idFieldVisitor); if (idFieldVisitor.getId() == null) { assert dv.isTombstone(docId); continue; } final BytesRef uid = new Term(IdFieldMapper.NAME, Uid.encodeId(idFieldVisitor.getId())).bytes(); try (Releasable ignored = versionMap.acquireLock(uid)) { final VersionValue curr = versionMap.getUnderLock(uid); if (curr == null || compareOpToVersionMapOnSeqNo(idFieldVisitor.getId(), seqNo, primaryTerm, curr) == OpVsLuceneDocStatus.OP_NEWER) { if (dv.isTombstone(docId)) { // use 0L for the start time so we can prune this delete tombstone quickly // when the local checkpoint advances (i.e., after a recovery completed). final long startTime = 0L; versionMap.putDeleteUnderLock(uid, new DeleteVersionValue(dv.docVersion(docId), seqNo, primaryTerm, startTime)); } else { versionMap.putIndexUnderLock(uid, new IndexVersionValue(null, dv.docVersion(docId), seqNo, primaryTerm)); } } } } } // remove live entries in the version map refresh("restore_version_map_and_checkpoint_tracker", SearcherScope.INTERNAL, true); }	lucene requires queries to be rewritten before creating a weight. this particular boolean query is fine because it rewrites to itself, but it'd be more future-proof to call rewrite(). suggestion final query rewrittenquery = searcher.rewrite(query); final weight weight = searcher.createweight(rewrittenquery, scoremode.complete_no_scores, 1.0f);
private TopDocs searchOperations(ScoreDoc after) throws IOException { final Query rangeQuery = new BooleanQuery.Builder() .add(LongPoint.newRangeQuery(SeqNoFieldMapper.NAME, Math.max(fromSeqNo, lastSeenSeqNo), toSeqNo), BooleanClause.Occur.MUST) .add(Queries.newNonNestedFilter(), BooleanClause.Occur.MUST) // exclude non-root nested documents .build(); final Sort sortedBySeqNo = new Sort(new SortField(SeqNoFieldMapper.NAME, SortField.Type.LONG)); return indexSearcher.searchAfter(after, rangeQuery, searchBatchSize, sortedBySeqNo); }	in case you wonder: no need to explicitly rewrite here, this is implicitly done by searchafter which is a high-level api of indexsearcher unlike createweight
public IntervalsSource getSource(SearchExecutionContext context, MappedFieldType fieldType) { NamedAnalyzer analyzer = null; if (this.analyzer != null) { analyzer = context.getIndexAnalyzers().get(this.analyzer); } if (useField != null) { fieldType = context.getFieldType(useField); assert fieldType != null; } if (analyzer == null) { analyzer = fieldType.getTextSearchInfo().getSearchAnalyzer(); } // Fuzzy queries only work with unicode content so it's legal to call utf8ToString here. String normalizedTerm = analyzer.normalize(fieldType.name(), term).utf8ToString(); IntervalsSource source = fieldType.fuzzyIntervals(normalizedTerm, fuzziness.asDistance(term), prefixLength, transpositions, context); if (useField != null) { source = Intervals.fixField(useField, source); } return source; }	should we change the check to still throw a comprehensive exception when positions are not available on the field ?
private void checkJoinKeyTypes(LogicalPlan plan, Set<Failure> localFailures) { if (plan instanceof Join) { Join join = (Join) plan; List<KeyedFilter> queries = join.queries(); KeyedFilter until = join.until(); // pick first query and iterate its keys KeyedFilter first = queries.get(0); List<? extends NamedExpression> keys = first.keys(); for (int keyIndex = 0; keyIndex < keys.size(); keyIndex++) { NamedExpression currentKey = keys.get(keyIndex); for (int i = 1; i < queries.size(); i++) { KeyedFilter filter = queries.get(i); doCheckKeyTypes(join, localFailures, currentKey, filter.keys().get(keyIndex)); if (until.keys().isEmpty() == false) { doCheckKeyTypes(join, localFailures, currentKey, until.keys().get(keyIndex)); } } } } }	i would have chosen a slightly different error message, something that would made clearer that the incompatible types are to blame: key [x] type [y] has an incompatible type with key [a] type [b].
private void resolveRangeTypeEnrichMapping( EnrichPolicy policy, List<Map<String, Object>> mappings, ActionListener<XContentBuilder> resultListener ) { String matchFieldPath = "properties." + policy.getMatchField().replace(".", ".properties."); List<Map<String, String>> matchFieldMappings = mappings.stream() .map(map -> ObjectPath.<Map<String, String>>eval(matchFieldPath, map)) .filter(Objects::nonNull) .collect(Collectors.toList()); Set<String> types = matchFieldMappings.stream().map(map -> map.get("type")).collect(Collectors.toSet()); if (types.isEmpty()) { resultListener.onFailure( new ElasticsearchException( "No mapping type found for match field '{}' - indices({})", policy.getMatchField(), Strings.collectionToCommaDelimitedString(policy.getIndices()) ) ); } else if (types.size() > 1) { resultListener.onFailure( new ElasticsearchException( "Multiple distinct mapping types for match field '{}' - indices({}) types({})", policy.getMatchField(), Strings.collectionToCommaDelimitedString(policy.getIndices()), Strings.collectionToCommaDelimitedString(types) ) ); } else { String type = types.iterator().next(); switch (type) { case "integer_range": case "float_range": case "long_range": case "double_range": case "ip_range": createEnrichMappingBuilder((builder) -> builder.field("type", type).field("doc_values", false), resultListener); break; // date_range types mappings allow for the format to be specified, should be preserved in the created index case "date_range": Set<String> formatEntries = matchFieldMappings.stream().map(map -> map.get("format")).collect(Collectors.toSet()); if (formatEntries.isEmpty()) { createEnrichMappingBuilder((builder) -> builder.field("type", type).field("doc_values", false), resultListener); } else if (formatEntries.size() > 1) { resultListener.onFailure( new ElasticsearchException( "Multiple distinct date format specified for match field '{}' - indices({}) format entries({})", policy.getMatchField(), Strings.collectionToCommaDelimitedString(policy.getIndices()), Strings.collectionToCommaDelimitedString(formatEntries) ) ); } else { createEnrichMappingBuilder( (builder) -> builder.field("type", type) .field("doc_values", false) .field("format", formatEntries.iterator().next()), resultListener ); } break; default: resultListener.onFailure( new ElasticsearchException( "Field '{}' has type [{}] which doesn't appear to be a range type", policy.getMatchField(), type ) ); } } }	same here (first valid cases and then error case)?
static EnumSet<AuditLevel> parse(List<String> levels) { EnumSet<AuditLevel> enumSet = EnumSet.noneOf(AuditLevel.class); for (String level : levels) { String lowerCaseLevel = level.trim().toLowerCase(Locale.ROOT); switch (lowerCaseLevel) { case "_all": enumSet.addAll(Arrays.asList(AuditLevel.values())); break; case "anonymous_access_denied": enumSet.add(ANONYMOUS_ACCESS_DENIED); break; case "authentication_failed": enumSet.add(AUTHENTICATION_FAILED); break; case "realm_authentication_failed": enumSet.add(REALM_AUTHENTICATION_FAILED); break; case "access_granted": enumSet.add(ACCESS_GRANTED); break; case "access_denied": enumSet.add(ACCESS_DENIED); break; case "tampered_request": enumSet.add(TAMPERED_REQUEST); break; case "connection_granted": enumSet.add(CONNECTION_GRANTED); break; case "connection_denied": enumSet.add(CONNECTION_DENIED); break; case "system_access_granted": enumSet.add(SYSTEM_ACCESS_GRANTED); break; case "security_config_change": enumSet.add(SECURITY_CONFIG_CHANGE); break; case "authentication_success": enumSet.add(AUTHENTICATION_SUCCESS); break; case "run_as_granted": enumSet.add(RUN_AS_GRANTED); break; case "run_as_denied": enumSet.add(RUN_AS_DENIED); break; default: throw new IllegalArgumentException("invalid event name specified [" + level + "]"); } } return enumSet; }	the new audit event.
public void toXContentFragment(XContentBuilder builder) throws IOException { builder.field(User.Fields.USERNAME.getPreferredName(), user.principal()); builder.array(User.Fields.ROLES.getPreferredName(), user.roles()); builder.field(User.Fields.FULL_NAME.getPreferredName(), user.fullName()); builder.field(User.Fields.EMAIL.getPreferredName(), user.email()); builder.field(User.Fields.METADATA.getPreferredName(), user.metadata()); builder.field(User.Fields.ENABLED.getPreferredName(), user.enabled()); builder.startObject(User.Fields.AUTHENTICATION_REALM.getPreferredName()); builder.field(User.Fields.REALM_NAME.getPreferredName(), getAuthenticatedBy().getName()); builder.field(User.Fields.REALM_TYPE.getPreferredName(), getAuthenticatedBy().getType()); builder.endObject(); builder.startObject(User.Fields.LOOKUP_REALM.getPreferredName()); if (getLookedUpBy() != null) { builder.field(User.Fields.REALM_NAME.getPreferredName(), getLookedUpBy().getName()); builder.field(User.Fields.REALM_TYPE.getPreferredName(), getLookedUpBy().getType()); } else { builder.field(User.Fields.REALM_NAME.getPreferredName(), getAuthenticatedBy().getName()); builder.field(User.Fields.REALM_TYPE.getPreferredName(), getAuthenticatedBy().getType()); } builder.endObject(); builder.field(User.Fields.AUTHENTICATION_TYPE.getPreferredName(), getAuthenticationType().name()); }	should this be suggestion builder.field(user.fields.authentication_type.getpreferredname(), getauthenticationtype().name().tolowercase(locale.root)); ? i can't think of many places in the es apis where we use all_caps values for enums. examples: - https://github.com/elastic/elasticsearch/blob/v7.9.0/server/src/main/java/org/elasticsearch/rest/action/cat/resthealthaction.java#l90 - https://github.com/elastic/elasticsearch/blob/v7.9.0/server/src/main/java/org/elasticsearch/action/support/writerequest.java#l63-l87
private boolean deleteIfFound(Term uid, long currentVersion, boolean deleted, VersionValue versionValue) throws IOException { assert uid != null : "uid must not be null"; final boolean found; if (currentVersion == Versions.NOT_FOUND) { // doc does not exist and no prior deletes found = false; } else if (versionValue != null && deleted) { // a "delete on delete", in this case, we still increment the version, log it, and return that version found = false; } else { // we deleted a currently existing document // any exception that comes from this is a either an ACE or a fatal exception there can't be any document failures coming // from this. indexWriter.deleteDocuments(uid); found = true; } return found; }	can we assert for this?
private void maybeThrowFailure() throws IOException { if (failureToThrow.get() != null) { Exception failure = failureToThrow.get().get(); if (failure instanceof RuntimeException) { throw (RuntimeException) failure; } else if (failure instanceof IOException) { throw (IOException) failure; } else { assert false: "unsupported failure class: " + failure.getClass().getCanonicalName(); } } }	yay for stack traces
@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/pull/75759") public void testEvaluate() throws IOException { assumeFalse("https://github.com/elastic/elasticsearch/issues/75685", Constants.OS_ARCH.equals("aarch64")); createModelStoreIndex(); putTaskConfig(); putModelDefinition(); refreshModelStoreIndex(); createTrainedModel(); startDeployment(); try { for (int i = 0; i < 10; i++) { Response inference = infer("my words"); assertThat(EntityUtils.toString(inference.getEntity()), equalTo("{\\\\"inference\\\\":[[1.0,1.0]]}")); } } finally { stopDeployment(); } }	is this change intended? note that the test is currently muted.
private void cancelFailedDeployment( String modelId, Exception exception, ActionListener<CreateTrainedModelAllocationAction.Response> listener ) { trainedModelAllocationService.deleteModelAllocation(modelId, ActionListener.wrap( pTask -> listener.onFailure(exception), e -> { logger.error( new ParameterizedMessage( "[{}] Failed to delete mode allocation that had failed with the reason [{}]", modelId, exception.getMessage() ), e ); listener.onFailure(exception); } )); }	typo: mode -> model
private IndexCommit getStartingCommitPoint() throws IOException { final IndexCommit startingIndexCommit; final List<IndexCommit> existingCommits; switch (openMode) { case CREATE_INDEX_AND_TRANSLOG: startingIndexCommit = null; break; case OPEN_INDEX_CREATE_TRANSLOG: // Use the last commit existingCommits = DirectoryReader.listCommits(store.directory()); startingIndexCommit = existingCommits.get(existingCommits.size() - 1); break; case OPEN_INDEX_AND_TRANSLOG: // Use the safe commit final long lastSyncedGlobalCheckpoint = translog.getLastSyncedGlobalCheckpoint(); final long minRetainedTranslogGen = translog.getMinFileGeneration(); existingCommits = DirectoryReader.listCommits(store.directory()); // We may not have a safe commit if an index was create before v6.2; and if there is a snapshotted commit whose translog // are not retained but max_seqno is at most the global checkpoint, we may mistakenly select it as a starting commit. // To avoid this issue, we only select index commits whose translog are fully retained. if (engineConfig.getIndexSettings().getIndexVersionCreated().before(Version.V_6_2_0)) { final List<IndexCommit> recoverableCommits = new ArrayList<>(); for (IndexCommit commit : existingCommits) { if (minRetainedTranslogGen <= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) { recoverableCommits.add(commit); } } assert recoverableCommits.isEmpty() == false : "No commit point with translog found; " + "commits [" + existingCommits + "], minRetainedTranslogGen [" + minRetainedTranslogGen + "]"; startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint); } else { // TODO: Asserts the starting commit is a safe commit once peer-recovery sets global checkpoint. startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint); } break; default: throw new IllegalArgumentException("unknown mode: " + openMode); } return startingIndexCommit; }	can we assert there is only one? i believe this is the case.
private ExternalSearcherManager createSearcherManager(SearchFactory externalSearcherFactory) throws EngineException { boolean success = false; SearcherManager internalSearcherManager = null; try { try { final DirectoryReader directoryReader = ElasticsearchDirectoryReader.wrap(DirectoryReader.open(indexWriter), shardId); internalSearcherManager = new SearcherManager(directoryReader, new RamAccountingSearcherFactory(engineConfig.getCircuitBreakerService())); lastCommittedSegmentInfos = store.readLastCommittedSegmentsInfo(); ExternalSearcherManager externalSearcherManager = new ExternalSearcherManager(internalSearcherManager, externalSearcherFactory); success = true; return externalSearcherManager; } catch (IOException e) { maybeFailEngine("start", e); try { indexWriter.rollback(); } catch (IOException inner) { // iw is closed below e.addSuppressed(inner); } throw new EngineCreationFailureException(shardId, "failed to open reader on writer", e); } } finally { if (success == false) { // release everything we created on a failure IOUtils.closeWhileHandlingException(internalSearcherManager, indexWriter); } } }	i know we now trim things and this ok, but it feels risky - maybe we'll change our mind later in the future clean less heavily. it will be hard to find this place and change. maybe just keep it as is?
protected void doRun() { for (SparseFileTracker.Gap gap : gaps) { try { if (reference.tryIncRef() == false) { throw new AlreadyClosedException("Cache file channel has been released and closed"); } try { ensureOpen(); writer.fillCacheRange(reference.fileChannel, gap.start(), gap.end(), gap::onProgress); } finally { reference.decRef(); } gap.onCompletion(); } catch (Exception e) { gap.onFailure(e); } } }	related to your discussion below i believe we could add: assert false : "expected a non-closed channel reference"
protected void doRun() { for (SparseFileTracker.Gap gap : gaps) { try { if (reference.tryIncRef() == false) { throw new AlreadyClosedException("Cache file channel has been released and closed"); } try { ensureOpen(); writer.fillCacheRange(reference.fileChannel, gap.start(), gap.end(), gap::onProgress); } finally { reference.decRef(); } gap.onCompletion(); } catch (Exception e) { gap.onFailure(e); } } }	looking at this again i think that gap.oncompletion(); should be executed within the try/finally block, before the reference is decremented, as completing the gap might trigger the execution of some listeners.
Future<Integer> readIfAvailableOrPending(final Tuple<Long, Long> rangeToRead, final RangeAvailableHandler reader) { final CompletableFuture<Integer> future = new CompletableFuture<>(); Releasable decrementRef = null; try { final FileChannelReference reference = getFileChannelReference(); decrementRef = Releasables.releaseOnce(reference::decRef); if (tracker.waitForRangeIfPending(rangeToRead, rangeListener(rangeToRead, reader, future, reference, decrementRef))) { return future; } else { // complete the future to release the channel reference decrementRef.close(); return null; } } catch (Exception e) { releaseAndFail(future, decrementRef, e); return future; } }	this comment should be removed now.
@Override public void onFailure(Exception e) { gaps.forEach(gap -> gap.onFailure(e)); } }); } } catch (Exception e) { releaseAndFail(future, decrementRef, e); } return future; } /** * Notifies the {@link RangeAvailableHandler} when {@code rangeToRead} is available to read from the file. If {@code rangeToRead} is * already available then the {@link RangeAvailableHandler} is called synchronously by this method; if not, but it is pending, then the * {@link RangeAvailableHandler} is notified when the pending ranges have completed. If it contains gaps that are not currently pending * then no listeners are registered and this method returns {@code null}. * * @return a future which returns the result of the {@link RangeAvailableHandler} once it has completed, or {@code null} if the * target range is neither available nor pending. */ @Nullable Future<Integer> readIfAvailableOrPending(final Tuple<Long, Long> rangeToRead, final RangeAvailableHandler reader) { final CompletableFuture<Integer> future = new CompletableFuture<>(); Releasable decrementRef = null; try { final FileChannelReference reference = getFileChannelReference(); decrementRef = Releasables.releaseOnce(reference::decRef); if (tracker.waitForRangeIfPending(rangeToRead, rangeListener(rangeToRead, reader, future, reference, decrementRef))) { return future; } else { // complete the future to release the channel reference decrementRef.close(); return null; } } catch (Exception e) { releaseAndFail(future, decrementRef, e); return future; } } private static void releaseAndFail(CompletableFuture<Integer> future, Releasable decrementRef, Exception e) { try { Releasables.close(decrementRef); } catch (Exception ex) { e.addSuppressed(ex); } future.completeExceptionally(e); } private static ActionListener<Void> rangeListener( Tuple<Long, Long> rangeToRead, RangeAvailableHandler reader, CompletableFuture<Integer> future, FileChannelReference reference, Releasable releasable ) { return ActionListener.runAfter(ActionListener.wrap(success -> { final int read = reader.onRangeAvailable(reference.fileChannel); assert read == rangeToRead.v2() - rangeToRead.v1() : "partial read [" + read + "] does not match the range to read [" + rangeToRead.v2() + '-' + rangeToRead.v1() + ']'; future.complete(read); }	nit: maybe we should call this acquirefilechannelreference to make it excplicit that it does incref, requiring the caller to decref?
public void testConcurrentAccess() throws Exception { final Path file = createTempDir().resolve("file.cache"); final CacheFile cacheFile = new CacheFile("test", randomLongBetween(1, 100), file); final TestEvictionListener evictionListener = new TestEvictionListener(); assertTrue(cacheFile.acquire(evictionListener)); final long length = cacheFile.getLength(); final DeterministicTaskQueue deterministicTaskQueue = new DeterministicTaskQueue( builder().put(NODE_NAME_SETTING.getKey(), getTestName()).build(), random() ); final ThreadPool threadPool = deterministicTaskQueue.getThreadPool(); final Future<Integer> readFuture; if (randomBoolean()) { readFuture = cacheFile.readIfAvailableOrPending(Tuple.tuple(0L, length), channel -> Math.toIntExact(length)); } else { readFuture = cacheFile.populateAndRead( Tuple.tuple(0L, length), Tuple.tuple(0L, length), channel -> Math.toIntExact(length), (channel, from, to, progressUpdater) -> progressUpdater.accept(length), threadPool.generic() ); } final boolean evicted = randomBoolean(); if (evicted) { deterministicTaskQueue.scheduleNow(cacheFile::startEviction); } deterministicTaskQueue.scheduleNow(() -> cacheFile.release(evictionListener)); deterministicTaskQueue.runAllRunnableTasks(); if (readFuture != null) { assertTrue(readFuture.isDone()); } if (evicted) { assertFalse(Files.exists(file)); } }	it would be nice in this case to randomly first invoke populateandread too - in order to get into the more interesting part of readifavailableorpending involving the listener callback.
public void testNotWaitForQuorumCopies() throws Exception { logger.info("--> starting 3 nodes"); List<String> nodes = internalCluster().startNodes(3); logger.info("--> creating index with 1 primary and 2 replicas"); assertAcked(client().admin().indices().prepareCreate("test").setSettings(Settings.builder() .put("index.number_of_shards", randomIntBetween(1, 3)).put("index.number_of_replicas", 2)).get()); ensureGreen("test"); client().prepareIndex("test", "type1").setSource(jsonBuilder() .startObject().field("field", "value1").endObject()).get(); logger.info("--> removing 2 nodes from cluster"); internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodes.get(1))); internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodes.get(2))); internalCluster().restartRandomDataNode(); logger.info("--> checking that index still gets allocated with only 1 shard copy being available"); ensureYellow("test"); assertHitCount(client().prepareSearch().setSize(0).setQuery(matchAllQuery()).get(), 1L); }	to readd more randomization, maybe write nodes.get(1), nodes.get(2) both here and in the next line.
private Tuple<AuthenticationToken, Exception> extractAuthenticationToken(GrantType grantType, CreateTokenRequest request) { AuthenticationToken authToken = null; if (grantType == GrantType.PASSWORD) { authToken = new UsernamePasswordToken(request.getUsername(), request.getPassword()); } else if (grantType == GrantType.KERBEROS) { SecureString kerberosTicket = request.getKerberosTicket(); String base64EncodedToken = kerberosTicket.toString(); byte[] decodedKerberosTicket; try { decodedKerberosTicket = Base64.getDecoder().decode(base64EncodedToken); } catch (IllegalArgumentException iae) { return new Tuple<>(null, new UnsupportedOperationException("could not decode base64 kerberos ticket " + base64EncodedToken, iae)); } authToken = new KerberosAuthenticationToken(decodedKerberosTicket); } return new Tuple<>(authToken, null); }	++ to not having a non-void method that is also passed an actionlistener. maybe wrap exception with an optional<> ? nothing much changes but i think it captures the fact that the tuple.v2() _might_ contain an exception better?
public void testKerberosGrantTypeWillFailOnBase64DecodeError() throws Exception { final TokenService tokenService = new TokenService(SETTINGS, Clock.systemUTC(), client, license, securityContext, securityIndex, securityIndex, clusterService); Authentication authentication = new Authentication(new User("joe"), new Authentication.RealmRef("realm", "type", "node"), null); authentication.writeToContext(threadPool.getThreadContext()); final TransportCreateTokenAction action = new TransportCreateTokenAction(threadPool, mock(TransportService.class), new ActionFilters(Collections.emptySet()), tokenService, authenticationService, securityContext); final CreateTokenRequest createTokenRequest = new CreateTokenRequest(); createTokenRequest.setGrantType("_kerberos"); createTokenRequest.setKerberosTicket(new SecureString("(:I".toCharArray())); PlainActionFuture<CreateTokenResponse> tokenResponseFuture = new PlainActionFuture<>(); action.doExecute(null, createTokenRequest, tokenResponseFuture); UnsupportedOperationException e = expectThrows(UnsupportedOperationException.class, () -> tokenResponseFuture.actionGet()); assertThat(e.getMessage(), containsString("could not decode base64 kerberos ticket")); // The code flow should stop after above failure and never reach authenticationService Mockito.verifyZeroInteractions(authenticationService); }	maybe a random short string here instead of hardcoding the string that this was reported with?
public void testKerberosGrantTypeWillFailOnBase64DecodeError() throws Exception { final TokenService tokenService = new TokenService(SETTINGS, Clock.systemUTC(), client, license, securityContext, securityIndex, securityIndex, clusterService); Authentication authentication = new Authentication(new User("joe"), new Authentication.RealmRef("realm", "type", "node"), null); authentication.writeToContext(threadPool.getThreadContext()); final TransportCreateTokenAction action = new TransportCreateTokenAction(threadPool, mock(TransportService.class), new ActionFilters(Collections.emptySet()), tokenService, authenticationService, securityContext); final CreateTokenRequest createTokenRequest = new CreateTokenRequest(); createTokenRequest.setGrantType("_kerberos"); createTokenRequest.setKerberosTicket(new SecureString("(:I".toCharArray())); PlainActionFuture<CreateTokenResponse> tokenResponseFuture = new PlainActionFuture<>(); action.doExecute(null, createTokenRequest, tokenResponseFuture); UnsupportedOperationException e = expectThrows(UnsupportedOperationException.class, () -> tokenResponseFuture.actionGet()); assertThat(e.getMessage(), containsString("could not decode base64 kerberos ticket")); // The code flow should stop after above failure and never reach authenticationService Mockito.verifyZeroInteractions(authenticationService); }	we could wrap this in an [assertlistenerisonlycalledonce](https://github.com/elastic/elasticsearch/blob/51d0bcacdf0c2daf47ae1f17574e1884304a8271/x-pack/plugin/identity-provider/src/internalclustertest/java/org/elasticsearch/xpack/idp/saml/sp/samlserviceproviderindextests.java#l243)
private InetAddress[] resolveBindHostAddress(String bindHost, String defaultValue2) throws IOException { /** TODO: move this leniency out */ if (bindHost == null) { bindHost = settings.get(GLOBAL_NETWORK_BINDHOST_SETTING, settings.get(GLOBAL_NETWORK_HOST_SETTING)); } if (bindHost == null) { bindHost = defaultValue2; } return resolveInetAddress(bindHost); }	do you prefer the double-if? if not i guess you could do: java if (bindhost == null) { bindhost = settings.get(global_network_bindhost_setting, settings.get(global_network_host_setting, defaultvalue2)); } return resolveinetaddress(bindhost); instead
private InetAddress[] resolveBindHostAddress(String bindHost, String defaultValue2) throws IOException { /** TODO: move this leniency out */ if (bindHost == null) { bindHost = settings.get(GLOBAL_NETWORK_BINDHOST_SETTING, settings.get(GLOBAL_NETWORK_HOST_SETTING)); } if (bindHost == null) { bindHost = defaultValue2; } return resolveInetAddress(bindHost); }	i am not sure where we use this else but would make it sense to move _local_ to a constant? primarily i am curious what this means and if we can add some javadocs to the constant
private void bindAddress(final InetAddress hostAddress) { PortsRange portsRange = new PortsRange(port); final AtomicReference<Exception> lastException = new AtomicReference<>(); final AtomicReference<SocketAddress> boundSocket = new AtomicReference<>(); boolean success = portsRange.iterate(new PortsRange.PortCallback() { @Override public boolean onPortNumber(int portNumber) { try { synchronized (serverChannels) { Channel channel = serverBootstrap.bind(new InetSocketAddress(hostAddress, portNumber)); serverChannels.add(channel); boundSocket.set(channel.getLocalAddress()); } } catch (Exception e) { lastException.set(e); return false; } return true; } }); if (!success) { throw new BindHttpException("Failed to bind to [" + port + "]", lastException.get()); } logger.info("Bound http to address [{}]", boundSocket.get()); }	this logging is helpful, thanks for adding it!
* @param shardTarget the shard target for this failure * @param e the failure reason */ @Override public final void onShardFailure(final int shardIndex, @Nullable SearchShardTarget shardTarget, Exception e) { // we don't aggregate shard failures on non active shards (but do keep the header counts right) if (TransportActions.isShardNotAvailableException(e) == false && (requestCancelled.get() && isTaskCancelledException(e)) == false) { AtomicArray<ShardSearchFailure> shardFailures = this.shardFailures.get(); // lazily create shard failures, so we can early build the empty shard failure list in most cases (no failures) if (shardFailures == null) { // this is double checked locking but it's fine since SetOnce uses a volatile read internally synchronized (shardFailuresMutex) { shardFailures = this.shardFailures.get(); // read again otherwise somebody else has created it? if (shardFailures == null) { // still null so we are the first and create a new instance shardFailures = new AtomicArray<>(getNumShards()); this.shardFailures.set(shardFailures); } } } ShardSearchFailure failure = shardFailures.get(shardIndex); if (failure == null) { shardFailures.set(shardIndex, new ShardSearchFailure(e, shardTarget)); } else { // the failure is already present, try and not override it with an exception that is less meaningless // for example, getting illegal shard state if (TransportActions.isReadOverrideException(e)) { shardFailures.set(shardIndex, new ShardSearchFailure(e, shardTarget)); } } if (results.hasResult(shardIndex)) { assert failure == null : "shard failed before but shouldn't: " + failure; successfulOps.decrementAndGet(); // if this shard was successful before (initial phase) we have to adjust the counter } } results.consumeShardFailure(shardIndex); }	can you update the comment above to say that we don't aggregate shard failures on search cancelled internally ?
public void testCompatibleLog() throws Exception { withThreadContext(threadContext -> { threadContext.putHeader(Task.X_OPAQUE_ID, "someId"); final DeprecationLogger testLogger = DeprecationLogger.getLogger("org.elasticsearch.test"); testLogger.deprecate(DeprecationCategory.OTHER,"someKey", "deprecated message1") .compatibleApiWarning("compatibleKey","compatible API message"); final Path path = PathUtils.get( System.getProperty("es.logs.base_path"), System.getProperty("es.logs.cluster_name") + "_deprecated.json" ); try (Stream<Map<String, String>> stream = JsonLogsStream.mapStreamFrom(path)) { List<Map<String, String>> jsonLogs = stream.collect(Collectors.toList()); assertThat( jsonLogs, contains( allOf( hasEntry("log.level", "DEPRECATION"), hasEntry("event.dataset", "elasticsearch.deprecation"), hasEntry("data_stream.dataset", "elasticsearch.deprecation"), hasEntry("data_stream.type", "logs"), hasEntry("log.logger", "org.elasticsearch.deprecation.test"), hasEntry("ecs.version", DeprecatedMessage.ECS_VERSION), hasEntry("elasticsearch.cluster.name", "elasticsearch"), hasEntry("elasticsearch.node.name", "sample-name"), hasEntry("message", "deprecated message1"), hasEntry(DeprecatedMessage.KEY_FIELD_NAME, "someKey"), hasEntry(DeprecatedMessage.X_OPAQUE_ID_FIELD_NAME, "someId"), hasEntry("elasticsearch.event.category", "other") ), allOf( hasEntry("log.level", "DEPRECATION"), // event.dataset and data_stream.dataset have to be the same across the data stream hasEntry("event.dataset", "elasticsearch.deprecation"), hasEntry("data_stream.dataset", "elasticsearch.deprecation"), hasEntry("data_stream.type", "logs"), hasEntry("log.logger", "org.elasticsearch.deprecation.test"), hasEntry("ecs.version", DeprecatedMessage.ECS_VERSION), hasEntry("elasticsearch.cluster.name", "elasticsearch"), hasEntry("elasticsearch.node.name", "sample-name"), hasEntry("message", "compatible API message"), hasEntry(DeprecatedMessage.KEY_FIELD_NAME, "compatibleKey"), hasEntry(DeprecatedMessage.X_OPAQUE_ID_FIELD_NAME, "someId"), hasEntry("elasticsearch.event.category", "compatible_api") ) ) ); } assertWarnings("deprecated message1", "compatible API message"); }); }	why has this changed?
@Override public Query toQuery(QueryParseContext parseContext) { validate(); Query query = null; MapperService.SmartNameFieldMappers smartNameFieldMappers = parseContext.smartFieldMappers(this.fieldName); if (smartNameFieldMappers != null && smartNameFieldMappers.hasMapper()) { query = smartNameFieldMappers.mapper().termQuery(this.value, parseContext); } if (query == null) { query = new TermQuery(new Term(this.fieldName, BytesRefs.toBytesRef(this.value))); } query.setBoost(this.boost); if (this.queryName != null) { parseContext.addNamedQuery(queryName, query); } return query; }	are all the refactored queries going to have something like a validate method? i wonder if it makes sense to add it to the superclass for them all too, so it's available on any query.
public void testRamBytesUsed() { VersionValue versionValue = new VersionValue(randomLong()); assertEquals(RamUsageTester.sizeOf(versionValue), versionValue.ramBytesUsed()); versionValue = new VersionValue(randomLong()); assertEquals(RamUsageTester.sizeOf(versionValue), versionValue.ramBytesUsed()); }	i think you meant to create a deleteversionvalue?
@Override public void handleException(TransportException exp) { logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, transportReplicaAction); if (ignoreReplicaException(exp)) { onReplicaFailure(nodeId, exp); } else { String message = String.format(Locale.ROOT, "failed to perform %s on replica on node %s", transportReplicaAction, node); logger.warn("[{}] {}", exp, shardId, message); shardStateAction.shardFailed( shard, indexShardReference.routingEntry(), message, exp, new ShardStateAction.Listener() { @Override public void onSuccess() { onReplicaFailure(nodeId, exp); } @Override public void onFailure(Throwable shardFailedError) { if (shardFailedError instanceof ShardStateAction.NoLongerPrimaryShardException) { String message = "unknown"; try { ShardRouting primaryShard = indexShardReference.routingEntry(); message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard [%s] for [%s]", primaryShard, shard, exp); // we are no longer the primary, fail ourselves and start over indexShardReference.failShard(message, shardFailedError); } catch (Throwable t) { shardFailedError.addSuppressed(t); } forceFinishAsFailed(new RetryOnPrimaryException(shardId, message, shardFailedError)); } else { assert shardFailedError instanceof TransportException || shardFailedError instanceof NodeClosedException : shardFailedError; onReplicaFailure(nodeId, exp); } } } ); } }	can we open a follow up issue that failshard should throw an already closed exception? (this is really what the code protects against). to be clear - i think the code can stay.
@Override public void handleException(TransportException exp) { logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, transportReplicaAction); if (ignoreReplicaException(exp)) { onReplicaFailure(nodeId, exp); } else { String message = String.format(Locale.ROOT, "failed to perform %s on replica on node %s", transportReplicaAction, node); logger.warn("[{}] {}", exp, shardId, message); shardStateAction.shardFailed( shard, indexShardReference.routingEntry(), message, exp, new ShardStateAction.Listener() { @Override public void onSuccess() { onReplicaFailure(nodeId, exp); } @Override public void onFailure(Throwable shardFailedError) { if (shardFailedError instanceof ShardStateAction.NoLongerPrimaryShardException) { String message = "unknown"; try { ShardRouting primaryShard = indexShardReference.routingEntry(); message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard [%s] for [%s]", primaryShard, shard, exp); // we are no longer the primary, fail ourselves and start over indexShardReference.failShard(message, shardFailedError); } catch (Throwable t) { shardFailedError.addSuppressed(t); } forceFinishAsFailed(new RetryOnPrimaryException(shardId, message, shardFailedError)); } else { assert shardFailedError instanceof TransportException || shardFailedError instanceof NodeClosedException : shardFailedError; onReplicaFailure(nodeId, exp); } } } ); } }	add a comment about where these exceptions can come from?
@Override protected void doStop() { FutureUtils.cancel(this.reconnectToNodes); for (NotifyTimeout onGoingTimeout : onGoingTimeouts) { onGoingTimeout.cancel(); } ThreadPool.terminate(updateTasksExecutor, 10, TimeUnit.SECONDS); postAppliedListeners.stream().filter(listener -> listener instanceof TimeoutClusterStateListener) .forEach(listener -> ((TimeoutClusterStateListener) listener).onClose()); remove(localNodeMasterListeners); }	add a comment why we do it out of loop? (i.e. loop again)
void validateIncomingState(ClusterState incomingState, ClusterState lastSeenClusterState) { final ClusterName incomingClusterName = incomingState.getClusterName(); if (!incomingClusterName.equals(this.clusterName)) { logger.warn("received cluster state from [{}] which is also master but with a different cluster name [{}]", incomingState.nodes().masterNode(), incomingClusterName); throw new IllegalStateException("received state from a node that is not part of the cluster"); } final DiscoveryNodes currentNodes = nodesProvider.nodes(); if (currentNodes.localNode().equals(incomingState.nodes().localNode()) == false) { logger.warn("received a cluster state from [{}] and not part of the cluster, should not happen", incomingState.nodes().masterNode()); throw new IllegalStateException("received state from a node that is not part of the cluster"); } ZenDiscovery.validateStateIsFromCurrentMaster(logger, currentNodes, incomingState); if (lastSeenClusterState != null && lastSeenClusterState.supersedes(incomingState)) { final String message = String.format( Locale.ROOT, "received older cluster state version [%s] with uuid [%s] than last seen cluster state [%s] with uuid [%s]", incomingState.version(), incomingState.stateUUID(), lastSeenClusterState.version(), lastSeenClusterState.stateUUID() ); logger.warn(message); throw new IllegalStateException(message); } }	can we add that it came from the current master? also - thinking about this more - this can only happen in our testing right? in practice we only have 1 channel (if someone doesn't mess with some settings). maybe add a comment about it if so...
public void failShard(String reason, @Nullable Throwable e) { // fail the engine. This will cause this shard to also be removed from the node's index service. final Engine engine = getEngineOrNull(); if (engine == null) { logger.trace("ignoring request to fail the shard, we're already closed. (reason: [{}])", e, reason); } else { engine.failEngine(reason, e); } }	haha. i fixed this inline - forgot :) i think this should be a separate pr - it's not really needed for this one (we protect for it).
public void testClusterJoinDespiteOfPublishingIssues() throws Exception { List<String> nodes = startCluster(2, 1); String masterNode = internalCluster().getMasterName(); String nonMasterNode; if (masterNode.equals(nodes.get(0))) { nonMasterNode = nodes.get(1); } else { nonMasterNode = nodes.get(0); } DiscoveryNodes discoveryNodes = internalCluster().getInstance(ClusterService.class, nonMasterNode).state().nodes(); TransportService masterTranspotService = internalCluster().getInstance(TransportService.class, discoveryNodes.masterNode() .getName()); logger.info("blocking requests from non master [{}] to master [{}]", nonMasterNode, masterNode); MockTransportService nonMasterTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, nonMasterNode); nonMasterTransportService.addFailToSendNoConnectRule(masterTranspotService); assertNoMaster(nonMasterNode); logger.info("blocking cluster state publishing from master [{}] to non master [{}]", masterNode, nonMasterNode); MockTransportService masterTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, masterNode); TransportService localTransportService = internalCluster().getInstance(TransportService.class, discoveryNodes.localNode().getName ()); if (randomBoolean()) { masterTransportService.addFailToSendNoConnectRule(localTransportService, PublishClusterStateAction.SEND_ACTION_NAME); } else { masterTransportService.addFailToSendNoConnectRule(localTransportService, PublishClusterStateAction.COMMIT_ACTION_NAME); } logger.info("allowing requests from non master [{}] to master [{}], waiting for two join request", nonMasterNode, masterNode); final CountDownLatch countDownLatch = new CountDownLatch(2); nonMasterTransportService.addDelegate(masterTranspotService, new MockTransportService.DelegateTransport(nonMasterTransportService .original()) { @Override public void sendRequest(DiscoveryNode node, long requestId, String action, TransportRequest request, TransportRequestOptions options) throws IOException, TransportException { if (action.equals(MembershipAction.DISCOVERY_JOIN_ACTION_NAME)) { countDownLatch.countDown(); } super.sendRequest(node, requestId, action, request, options); } }); countDownLatch.await(); logger.info("waiting for cluster to reform"); masterTransportService.clearRule(localTransportService); nonMasterTransportService.clearRule(localTransportService); ensureStableCluster(2); // shutting down the nodes, to avoid the leakage check tripping // on the states associated with the commit requests we may have dropped internalCluster().stopRandomNonMasterNode(); }	i hate intellij's auto trimming. i have no idea who came up with that logic..
public void testOutOfOrderCommitMessages() throws Throwable { MockNode node = createMockNode("node").setAsMaster(); final CapturingTransportChannel channel = new CapturingTransportChannel(); List<ClusterState> states = new ArrayList<>(); final int numOfStates = scaledRandomIntBetween(3, 25); for (int i = 1; i <= numOfStates; i++) { states.add(ClusterState.builder(node.clusterState).version(i).stateUUID(ClusterState.UNKNOWN_UUID).build()); } final ClusterState finalState = states.get(numOfStates - 1); Collections.shuffle(states, random()); List<ClusterState> orderedSubsequence = new ArrayList<>(); long version = 0; for (ClusterState state : states) { if (state.version() >= version) { orderedSubsequence.add(state); version = state.version(); } } logger.info("--> publishing states"); for (ClusterState state : orderedSubsequence) { node.action.handleIncomingClusterStateRequest( new BytesTransportRequest(PublishClusterStateAction.serializeFullClusterState(state, Version.CURRENT), Version.CURRENT), channel); assertThat(channel.response.get(), equalTo((TransportResponse) TransportResponse.Empty.INSTANCE)); assertThat(channel.error.get(), nullValue()); channel.clear(); } logger.info("--> committing states"); Randomness.shuffle(orderedSubsequence); for (ClusterState state : orderedSubsequence) { node.action.handleCommitRequest(new PublishClusterStateAction.CommitClusterStateRequest(state.stateUUID()), channel); assertThat(channel.response.get(), equalTo((TransportResponse) TransportResponse.Empty.INSTANCE)); if (channel.error.get() != null) { throw channel.error.get(); } } channel.clear(); //now check the last state held assertSameState(node.clusterState, finalState); }	why do we need the subsequence? the original list is already ordered - we can just use it? also note that strictly speaking we do support out of order publishing (until we remove the settings that allows it, one day.. :)).
@Override public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException { XContentParser parser = parseContext.parser(); String fieldName = parser.currentName(); String rewriteMethod = null; String value = null; float boost = 1.0f; int flagsValue = DEFAULT_FLAGS_VALUE; int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES; String queryName = null; String currentFieldName = null; XContentParser.Token token; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (parseContext.isDeprecatedSetting(currentFieldName)) { // skip } else if (token == XContentParser.Token.START_OBJECT) { fieldName = currentFieldName; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else { if ("value".equals(currentFieldName)) { value = parser.textOrNull(); } else if ("boost".equals(currentFieldName)) { boost = parser.floatValue(); } else if ("rewrite".equals(currentFieldName)) { rewriteMethod = parser.textOrNull(); } else if ("flags".equals(currentFieldName)) { String flags = parser.textOrNull(); flagsValue = RegexpFlag.resolveValue(flags); } else if ("max_determinized_states".equals(currentFieldName)) { maxDeterminizedStates = parser.intValue(); } else if ("flags_value".equals(currentFieldName)) { flagsValue = parser.intValue(); } else if ("_name".equals(currentFieldName)) { queryName = parser.text(); } else { throw new QueryParsingException(parseContext, "[regexp] query does not support [" + currentFieldName + "]"); } } } } else { if ("_name".equals(currentFieldName)) { queryName = parser.text(); } else { fieldName = currentFieldName; value = parser.textOrNull(); } } } if (value == null) { throw new QueryParsingException(parseContext, "No value specified for regexp query"); } MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null); Query query = null; MappedFieldType fieldType = parseContext.fieldMapper(fieldName); if (fieldType != null) { query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, parseContext); } if (query == null) { RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates); if (method != null) { regexpQuery.setRewriteMethod(method); } query = regexpQuery; } query.setBoost(boost); if (queryName != null) { parseContext.addNamedQuery(queryName, query); } return query; }	not sure whether we want to use text() or textornull() here
@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/37206") public void testRandomGeometryIntersection() throws IOException { int testPointCount = randomIntBetween(100, 200); Point[] testPoints = new Point[testPointCount]; double extentSize = randomDoubleBetween(1, 10, true); boolean[] intersects = new boolean[testPointCount]; for (int i = 0; i < testPoints.length; i++) { testPoints[i] = randomPoint(false); } Geometry geometry = randomGeometryTreeGeometry(); GeoShapeIndexer indexer = new GeoShapeIndexer(true, "test"); geometry = indexer.prepareForIndexing(geometry); for (int i = 0; i < testPointCount; i++) { int cur = i; intersects[cur] = fold(geometry, false, (g, s) -> s || intersects(g, testPoints[cur], extentSize)); } for (int i = 0; i < testPointCount; i++) { assertEquals(intersects[i], intersects(geometry, testPoints[i], extentSize)); } }	do you think it would be ok to change this to: int xmin = geoshapecoordinateencoder.instance.encodex(math.max(p.getx() - extentsize, -179.9999)); int xmax = geoshapecoordinateencoder.instance.encodex(math.min(p.getx() + extentsize, 179.9999)); int ymin = geoshapecoordinateencoder.instance.encodey(math.max(p.gety() - extentsize, -89.9999)); int ymax = geoshapecoordinateencoder.instance.encodey(math.min(p.gety() + extentsize, 89.9999)); and also fix the other edge-tree reader bug by adding: // does rectangle's edges intersect or reside inside polygon's edge if (extent.minx() <= math.min(root.x1, root.x2) && extent.miny() <= math.min(root.y1, root.y2) && extent.maxx() >= math.max(root.x1, root.x2) && extent.maxy() >= math.max(root.y1, root.y2)) { return true; } to edgetreereader#crosses on line 204, after the // does rectangle's edges intersect or reside inside polygon's edge check? this way i think the tests would pass, and we can unmute it
public static TrainedModelDeploymentState getTrainedModelDeploymentState(PersistentTasksCustomMetadata.PersistentTask<?> task) { if (task == null) { return TrainedModelDeploymentState.STOPPED; } TrainedModelDeploymentTaskState taskState = (TrainedModelDeploymentTaskState) task.getState(); if (taskState == null) { return TrainedModelDeploymentState.STARTING; } TrainedModelDeploymentState state = taskState.getState(); if (taskState.isStatusStale(task)) { if (state == TrainedModelDeploymentState.STOPPING) { // previous executor node failed while the job was stopping - it won't // be restarted on another node, so consider it STOPPED for reassignment purposes return TrainedModelDeploymentState.STOPPED; } if (state != TrainedModelDeploymentState.FAILED) { // we are relocating at the moment return TrainedModelDeploymentState.STARTING; } } return state; }	it's probably worth adding a todo here to revisit for the new-style model allocation because, unlike persistent task, that won't have the same concept of "relocating".
@Override public String toString() { return Strings.toString(this); } } public static class TaskParams implements PersistentTaskParams, MlTaskParams { public static final Version VERSION_INTRODUCED = Version.V_8_0_0; private static final ParseField MODEL_BYTES = new ParseField("model_bytes"); /** * This has been found to be approximately 300MB on linux by manual testing. * TODO Check if it is substantially different in other platforms. */ private static final ByteSizeValue MEMORY_OVERHEAD = ByteSizeValue.ofMb(300); private final String modelId; private final String index; private final long modelBytes; public TaskParams(String modelId, String index, long modelBytes) { this.modelId = Objects.requireNonNull(modelId); this.index = Objects.requireNonNull(index); this.modelBytes = modelBytes; } public TaskParams(StreamInput in) throws IOException { this.modelId = in.readString(); this.index = in.readString(); this.modelBytes = in.readVLong(); } public String getModelId() { return modelId; } public String getIndex() { return index; } public long estimateMemoryUsageBytes() { // While loading the model in the process we need twice the model size return MEMORY_OVERHEAD.getBytes() + 2 * modelBytes; } @Override public String getWriteableName() { return MlTasks.TRAINED_MODEL_DEPLOYMENT_TASK_NAME; } @Override public Version getMinimalSupportedVersion() { return VERSION_INTRODUCED; } @Override public void writeTo(StreamOutput out) throws IOException { out.writeString(modelId); out.writeString(index); out.writeVLong(modelBytes); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(TrainedModelConfig.MODEL_ID.getPreferredName(), modelId); builder.field(IndexLocation.INDEX.getPreferredName(), index); builder.field(MODEL_BYTES.getPreferredName(), modelBytes); builder.endObject(); return builder; } @Override public int hashCode() { return Objects.hash(modelId, index, modelBytes); } @Override public boolean equals(Object o) { if (o == this) return true; if (o == null || getClass() != o.getClass()) return false; TaskParams other = (TaskParams) o; return Objects.equals(modelId, other.modelId) && Objects.equals(index, other.index) && modelBytes == other.modelBytes; } @Override public String getMlId() { return modelId; } } public interface TaskMatcher { static boolean match(Task task, String expectedId) { if (task instanceof TaskMatcher) { if (Strings.isAllOrWildcard(expectedId)) { return true; } String expectedDescription = MlTasks.TRAINED_MODEL_DEPLOYMENT_TASK_ID_PREFIX + expectedId; return expectedDescription.equals(task.getDescription()); } return false; }	machinelearning.native_executable_code_overhead (currently 30mb) will also be added on, so it might be worth deducting it from here. i imagine more than 30mb of this 300mb is from the pytorch code, so will only be incurred once per vm. it would be hard to fully model that, but at least we can treat 30mb of it as being down to shared code.
public long estimateMemoryUsageBytes() { // While loading the model in the process we need twice the model size return MEMORY_OVERHEAD.getBytes() + 2 * modelBytes; }	should we also mention that we found through experiment that the in-memory size is approximately equal to the json size? modelbytes is the json size right? while it's intuitive that in memory size = constant * json size, discovering that constant = 1 is an important piece of research and we don't want a future employee to have to redo it because we've lost track of the fact that the research has already been done. this should probably also go in a javadoc comment somewhere.
public long estimateMemoryUsageBytes() { // While loading the model in the process we need twice the model size return MEMORY_OVERHEAD.getBytes() + 2 * modelBytes; }	while i think this is a good estimate, i am not sure its going to work well with autoscaling. if you think about it, the memory a node needs is actually memory_overhead.getbytes() * nummodels + largestmodelsize + largestmodelsize + model_a +...+model_n when we load the models, we should do that serially, and always require the scaling to have at least the overhead required to load each model. always requiring 2x every model is way too much memory.
private void getModelBytes(TrainedModelConfig trainedModelConfig, ActionListener<Long> listener) { ChunkedTrainedModelRestorer restorer = new ChunkedTrainedModelRestorer(trainedModelConfig.getLocation().getModelId(), client, threadPool.executor(MachineLearning.UTILITY_THREAD_POOL_NAME), xContentRegistry); restorer.setSearchIndex(trainedModelConfig.getLocation().getResourceName()); restorer.setSearchSize(1); restorer.restoreModelDefinition( doc -> { listener.onResponse(doc.getTotalDefinitionLength()); // Return false to stop the restorer as we only need the first doc return false; }, success -> { /* nothing to do */ }, listener::onFailure ); }	to me, this is non-obvious. i fail to see how len(compress_json_object) is an adequate approximation to the neural net being loaded in memory.
*/ public static boolean isOnSharedFilesystem(Settings settings) { Version version = settings.getAsVersion(SETTING_VERSION_CREATED, Version.CURRENT); return settings.getAsBooleanLenientForPreEs6Indices(version, SETTING_SHARED_FILESYSTEM, isIndexUsingShadowReplicas(settings)); }	are you sure it is a good idea to have a default here? this shouldn't be missing in production, right?
*/ public static boolean isOnSharedFilesystem(Settings settings) { Version version = settings.getAsVersion(SETTING_VERSION_CREATED, Version.CURRENT); return settings.getAsBooleanLenientForPreEs6Indices(version, SETTING_SHARED_FILESYSTEM, isIndexUsingShadowReplicas(settings)); }	shouldn't the settings object here already have access to the version by retrieving the version inside the getasbooleanlenientforprees6indices method, so it can be skipped retrieving it outside of the method?
public static boolean nodeBooleanValue(Object node, String name, boolean defaultValue) { try { return nodeBooleanValue(node, defaultValue); } catch (IllegalArgumentException ex) { throw new IllegalArgumentException("Could not convert [" + name + "] to boolean.", ex); } }	it's super minor, but i think we usually don't include punctuation at the end of exception messages (the .)
public static boolean nodeBooleanValue(Object node, String name, boolean defaultValue) { try { return nodeBooleanValue(node, defaultValue); } catch (IllegalArgumentException ex) { throw new IllegalArgumentException("Could not convert [" + name + "] to boolean.", ex); } }	also minor, but i think i'd prefer node == null ? null : node.tostring() because it requires less negative-resolving in my brain, up to you though.
* @param provider the instance provider / factory method */ public void addIndexStore(String type, Function<IndexSettings, IndexStore> provider) { ensureNotFrozen(); if (storeTypes.containsKey(type)) { throw new IllegalArgumentException("key [" + type +"] already registered"); } storeTypes.put(type, provider); } /** * Registers the given {@link SimilarityProvider}	i think i'd prefer to have an interface for building the similarity. that way you can document what the two settings are.
public long parseToMilliseconds(Object value, boolean roundUp, @Nullable DateTimeZone zone, @Nullable DateMathParser forcedDateParser, QueryRewriteContext context) { DateMathParser dateParser = dateMathParser(); if (forcedDateParser != null) { dateParser = forcedDateParser; } String strValue; if (value instanceof BytesRef) { strValue = ((BytesRef) value).utf8ToString(); } else { strValue = value.toString(); } return dateParser.parse(strValue, context::nowInMillis, roundUp, zone); }	@colings86 i have a test on a branch of mine that reproducibly causes an npe here because context is null. the query is a multi-match query trying to construct a blended term query. it looks like the previous code handled the null context, but this code doesn't. maybe we need extra null handling here?
public void testSingleValueFieldWithExtendedBoundsTimezone() throws Exception { String index = "test12278"; prepareCreate(index) .setSettings(Settings.builder().put(indexSettings()).put("index.number_of_shards", 1).put("index.number_of_replicas", 0)) .execute().actionGet(); DateMathParser parser = new DateMathParser(Joda.getStrictStandardDateFormatter()); // we pick a random timezone offset of +12/-12 hours and insert two documents // one at 00:00 in that time zone and one at 12:00 List<IndexRequestBuilder> builders = new ArrayList<>(); int timeZoneHourOffset = randomIntBetween(-12, 12); DateTimeZone timezone = DateTimeZone.forOffsetHours(timeZoneHourOffset); DateTime timeZoneStartToday = new DateTime(parser.parse("now/d", System::currentTimeMillis, false, timezone), DateTimeZone.UTC); DateTime timeZoneNoonToday = new DateTime(parser.parse("now/d+12h", System::currentTimeMillis, false, timezone), DateTimeZone.UTC); builders.add(indexDoc(index, timeZoneStartToday, 1)); builders.add(indexDoc(index, timeZoneNoonToday, 2)); indexRandom(true, builders); ensureSearchable(index); SearchResponse response = null; // retrieve those docs with the same time zone and extended bounds response = client() .prepareSearch(index) .setQuery(QueryBuilders.rangeQuery("date").from("now/d").to("now/d").includeLower(true).includeUpper(true).timeZone(timezone.getID())) .addAggregation( dateHistogram("histo").field("date").dateHistogramInterval(DateHistogramInterval.hours(1)).timeZone(timezone).minDocCount(0) .extendedBounds(new ExtendedBounds("now/d", "now/d+23h")) ).execute().actionGet(); assertSearchResponse(response); assertThat("Expected 24 buckets for one day aggregation with hourly interval", response.getHits().totalHits(), equalTo(2L)); Histogram histo = response.getAggregations().get("histo"); assertThat(histo, notNullValue()); assertThat(histo.getName(), equalTo("histo")); List<? extends Bucket> buckets = histo.getBuckets(); assertThat(buckets.size(), equalTo(24)); for (int i = 0; i < buckets.size(); i++) { Histogram.Bucket bucket = buckets.get(i); assertThat(bucket, notNullValue()); assertThat("InternalBucket " + i + " had wrong key", (DateTime) bucket.getKey(), equalTo(new DateTime(timeZoneStartToday.getMillis() + (i * 60 * 60 * 1000), DateTimeZone.UTC))); if (i == 0 || i == 12) { assertThat(bucket.getDocCount(), equalTo(1L)); } else { assertThat(bucket.getDocCount(), equalTo(0L)); } } internalCluster().wipeIndices("test12278"); }	can we have a fake now here that is reproducible or do we need system.currenttimemillis()?
public static void validateSettings(String clusterAlias, Settings settings) { if (RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY.equals(clusterAlias)) { throw new IllegalArgumentException("remote clusters must not have the empty string as its key"); } ConnectionStrategy mode = REMOTE_CONNECTION_MODE.getConcreteSettingForNamespace(clusterAlias).get(settings); if (mode.equals(ConnectionStrategy.SNIFF)) { } else { } }	we should do this validation in the settings infrastructure, not when applying the settings.
public static void validateSettings(String clusterAlias, Settings settings) { if (RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY.equals(clusterAlias)) { throw new IllegalArgumentException("remote clusters must not have the empty string as its key"); } ConnectionStrategy mode = REMOTE_CONNECTION_MODE.getConcreteSettingForNamespace(clusterAlias).get(settings); if (mode.equals(ConnectionStrategy.SNIFF)) { } else { } }	what are these 3 lines?
public void testReconnectWhenSeedsNodesOrProxyAreUpdated() throws Exception { List<DiscoveryNode> knownNodes = new CopyOnWriteArrayList<>(); try (MockTransportService cluster_node_0 = startTransport("cluster_node_0", knownNodes, Version.CURRENT); MockTransportService cluster_node_1 = startTransport("cluster_node_1", knownNodes, Version.CURRENT)) { final DiscoveryNode node0 = cluster_node_0.getLocalDiscoNode(); final DiscoveryNode node1 = cluster_node_1.getLocalDiscoNode(); knownNodes.add(node0); knownNodes.add(node1); Collections.shuffle(knownNodes, random()); try (MockTransportService transportService = MockTransportService.createNewService(Settings.EMPTY, Version.CURRENT, threadPool, null)) { transportService.start(); transportService.acceptIncomingRequests(); final Settings.Builder builder = Settings.builder(); builder.putList("cluster.remote.cluster_test.seeds", Collections.singletonList(node0.getAddress().toString())); try (RemoteClusterService service = new RemoteClusterService(builder.build(), transportService)) { assertFalse(service.isCrossClusterSearchEnabled()); service.initializeRemoteClusters(); assertTrue(service.isCrossClusterSearchEnabled()); final RemoteClusterConnection firstRemoteClusterConnection = service.getRemoteClusterConnection("cluster_test"); assertTrue(firstRemoteClusterConnection.isNodeConnected(node0)); assertTrue(firstRemoteClusterConnection.isNodeConnected(node1)); assertEquals(2, firstRemoteClusterConnection.getNumNodesConnected()); assertFalse(firstRemoteClusterConnection.isClosed()); final CountDownLatch firstLatch = new CountDownLatch(1); service.updateRemoteCluster( "cluster_test", createSettings("cluster_test", Collections.singletonList(node0.getAddress().toString())), connectionListener(firstLatch)); firstLatch.await(); assertTrue(service.isCrossClusterSearchEnabled()); assertTrue(firstRemoteClusterConnection.isNodeConnected(node0)); assertTrue(firstRemoteClusterConnection.isNodeConnected(node1)); assertEquals(2, firstRemoteClusterConnection.getNumNodesConnected()); assertFalse(firstRemoteClusterConnection.isClosed()); assertSame(firstRemoteClusterConnection, service.getRemoteClusterConnection("cluster_test")); final List<String> newSeeds = new ArrayList<>(); newSeeds.add(node1.getAddress().toString()); if (randomBoolean()) { newSeeds.add(node0.getAddress().toString()); Collections.shuffle(newSeeds, random()); } final CountDownLatch secondLatch = new CountDownLatch(1); service.updateRemoteCluster( "cluster_test", createSettings("cluster_test", newSeeds), connectionListener(secondLatch)); secondLatch.await(); assertTrue(service.isCrossClusterSearchEnabled()); assertBusy(() -> { assertFalse(firstRemoteClusterConnection.isNodeConnected(node0)); assertFalse(firstRemoteClusterConnection.isNodeConnected(node1)); assertEquals(0, firstRemoteClusterConnection.getNumNodesConnected()); assertTrue(firstRemoteClusterConnection.isClosed()); }); final RemoteClusterConnection secondRemoteClusterConnection = service.getRemoteClusterConnection("cluster_test"); assertTrue(secondRemoteClusterConnection.isNodeConnected(node0)); assertTrue(secondRemoteClusterConnection.isNodeConnected(node1)); assertEquals(2, secondRemoteClusterConnection.getNumNodesConnected()); assertFalse(secondRemoteClusterConnection.isClosed()); // final CountDownLatch thirdLatch = new CountDownLatch(1); // service.updateRemoteCluster( // "cluster_test", // createSettings(newSeeds), node1.getAddress().toString(), // genericProfile("cluster_test"), connectionListener(thirdLatch)); // thirdLatch.await(); // // assertBusy(() -> { // assertFalse(secondRemoteClusterConnection.isNodeConnected(node0)); // assertFalse(secondRemoteClusterConnection.isNodeConnected(node1)); // assertEquals(0, secondRemoteClusterConnection.getNumNodesConnected()); // assertTrue(secondRemoteClusterConnection.isClosed()); // }); // // final RemoteClusterConnection thirdRemoteClusterConnection = service.getRemoteClusterConnection("cluster_test"); // assertTrue(thirdRemoteClusterConnection.isNodeConnected(node1)); // // Will only successfully connect to node1 because the proxy address is to node1 and // // validation will fail when attempt to connect to node0 // assertFalse(thirdRemoteClusterConnection.isNodeConnected(node0)); // assertEquals(1, thirdRemoteClusterConnection.getNumNodesConnected()); // assertFalse(thirdRemoteClusterConnection.isClosed()); } } } }	where was this test moved?
@Override protected SpanQuery analyzeGraphPhrase(TokenStream source, String field, int phraseSlop) throws IOException { source.reset(); GraphTokenStreamFiniteStrings graph = new GraphTokenStreamFiniteStrings(source); List<SpanQuery> clauses = new ArrayList<>(); int[] articulationPoints = graph.articulationPoints(); int lastState = 0; for (int i = 0; i <= articulationPoints.length; i++) { int start = lastState; int end = -1; if (i < articulationPoints.length) { end = articulationPoints[i]; } lastState = end; final SpanQuery queryPos; if (graph.hasSidePath(start)) { List<SpanQuery> queries = new ArrayList<>(); Iterator<TokenStream> it = graph.getFiniteStrings(start, end); while (it.hasNext()) { TokenStream ts = it.next(); SpanQuery q = createSpanQuery(ts, field); if (q != null) { if (queries.size() >= BooleanQuery.getMaxClauseCount()) { throw new BooleanQuery.TooManyClauses(); } queries.add(q); } } if (queries.size() > 0) { queryPos = new SpanOrQuery(queries.toArray(new SpanQuery[0])); } else { queryPos = null; } } else { Term[] terms = graph.getTerms(field, start); assert terms.length > 0; if (terms.length >= BooleanQuery.getMaxClauseCount()) { throw new BooleanQuery.TooManyClauses(); } if (terms.length == 1) { queryPos = new SpanTermQuery(terms[0]); } else { SpanTermQuery[] orClauses = new SpanTermQuery[terms.length]; for (int idx = 0; idx < terms.length; idx++) { orClauses[idx] = new SpanTermQuery(terms[idx]); } queryPos = new SpanOrQuery(orClauses); } } if (queryPos != null) { if (clauses.size() >= BooleanQuery.getMaxClauseCount()) { throw new BooleanQuery.TooManyClauses(); } clauses.add(queryPos); } } if (clauses.isEmpty()) { return null; } else if (clauses.size() == 1) { return clauses.get(0); } else { return new SpanNearQuery(clauses.toArray(new SpanQuery[0]), phraseSlop, true); } } } /** * Called when a phrase query is built with {@link QueryBuilder#analyzePhrase(String, TokenStream, int)}. * Subclass can override this function to blend this query to multiple fields. */ protected Query blendPhraseQuery(PhraseQuery query, MappedFieldType fieldType) { return query; } protected Query blendTermsQuery(Term[] terms, MappedFieldType fieldType) { return new SynonymQuery(terms); } protected Query blendTermQuery(Term term, MappedFieldType fieldType) { if (fuzziness != null) { try { Query query = fieldType.fuzzyQuery(term.text(), fuzziness, fuzzyPrefixLength, maxExpansions, transpositions); if (query instanceof FuzzyQuery) { QueryParsers.setRewriteMethod((FuzzyQuery) query, fuzzyRewriteMethod); } return query; } catch (RuntimeException e) { if (lenient) { return newLenientFieldQuery(fieldType.name(), e); } else { throw e; } } }	just for my own education, and it is certainly super minor: when reading this part i was wondering if it would make sense to get the maxclausecount limit once at the beginning of this method since its unlikely to change to avoid method calls in each iteration). maybe java does some clever optimizations to avoid this though and the effect most likely negligible.
@Override protected SpanQuery analyzeGraphPhrase(TokenStream source, String field, int phraseSlop) throws IOException { source.reset(); GraphTokenStreamFiniteStrings graph = new GraphTokenStreamFiniteStrings(source); List<SpanQuery> clauses = new ArrayList<>(); int[] articulationPoints = graph.articulationPoints(); int lastState = 0; for (int i = 0; i <= articulationPoints.length; i++) { int start = lastState; int end = -1; if (i < articulationPoints.length) { end = articulationPoints[i]; } lastState = end; final SpanQuery queryPos; if (graph.hasSidePath(start)) { List<SpanQuery> queries = new ArrayList<>(); Iterator<TokenStream> it = graph.getFiniteStrings(start, end); while (it.hasNext()) { TokenStream ts = it.next(); SpanQuery q = createSpanQuery(ts, field); if (q != null) { if (queries.size() >= BooleanQuery.getMaxClauseCount()) { throw new BooleanQuery.TooManyClauses(); } queries.add(q); } } if (queries.size() > 0) { queryPos = new SpanOrQuery(queries.toArray(new SpanQuery[0])); } else { queryPos = null; } } else { Term[] terms = graph.getTerms(field, start); assert terms.length > 0; if (terms.length >= BooleanQuery.getMaxClauseCount()) { throw new BooleanQuery.TooManyClauses(); } if (terms.length == 1) { queryPos = new SpanTermQuery(terms[0]); } else { SpanTermQuery[] orClauses = new SpanTermQuery[terms.length]; for (int idx = 0; idx < terms.length; idx++) { orClauses[idx] = new SpanTermQuery(terms[idx]); } queryPos = new SpanOrQuery(orClauses); } } if (queryPos != null) { if (clauses.size() >= BooleanQuery.getMaxClauseCount()) { throw new BooleanQuery.TooManyClauses(); } clauses.add(queryPos); } } if (clauses.isEmpty()) { return null; } else if (clauses.size() == 1) { return clauses.get(0); } else { return new SpanNearQuery(clauses.toArray(new SpanQuery[0]), phraseSlop, true); } } } /** * Called when a phrase query is built with {@link QueryBuilder#analyzePhrase(String, TokenStream, int)}. * Subclass can override this function to blend this query to multiple fields. */ protected Query blendPhraseQuery(PhraseQuery query, MappedFieldType fieldType) { return query; } protected Query blendTermsQuery(Term[] terms, MappedFieldType fieldType) { return new SynonymQuery(terms); } protected Query blendTermQuery(Term term, MappedFieldType fieldType) { if (fuzziness != null) { try { Query query = fieldType.fuzzyQuery(term.text(), fuzziness, fuzzyPrefixLength, maxExpansions, transpositions); if (query instanceof FuzzyQuery) { QueryParsers.setRewriteMethod((FuzzyQuery) query, fuzzyRewriteMethod); } return query; } catch (RuntimeException e) { if (lenient) { return newLenientFieldQuery(fieldType.name(), e); } else { throw e; } } }	it seems the former check is triggered by the test, but this code path isn't. i haven't checked all the test setup logic, but would it be difficult to include a case that also triggers this code path?
private Set<String> innerResolve(Context context, List<String> expressions, IndicesOptions options, MetaData metaData) { Set<String> result = null; boolean wildcardSeen = false; for (int i = 0; i < expressions.size(); i++) { String expression = expressions.get(i); if (Strings.isEmpty(expression)) { throw indexNotFoundException(expression); } assertValidAliasOrIndex(expression); if (aliasOrIndexExists(options, metaData, expression)) { if (result != null) { result.add(expression); } continue; } final boolean add; if (expression.charAt(0) == '-' && wildcardSeen) { add = false; expression = expression.substring(1); } else { add = true; } if (result == null) { // add all the previous ones... result = new HashSet<>(expressions.subList(0, i)); } if (!Regex.isSimpleMatchPattern(expression)) { //TODO why does wildcard resolver throw exceptions regarding non wildcarded expressions? This should not be done here. if (options.ignoreUnavailable() == false) { AliasOrIndex aliasOrIndex = metaData.getAliasAndIndexLookup().get(expression); if (aliasOrIndex == null) { throw indexNotFoundException(expression); } else if (aliasOrIndex.isAlias() && options.ignoreAliases()) { throw aliasesNotSupportedException(expression); } } if (add) { result.add(expression); } else { result.remove(expression); } continue; } final IndexMetaData.State excludeState = excludeState(options); final Map<String, AliasOrIndex> matches = matches(context, metaData, expression); Set<String> expand = expand(context, excludeState, matches); if (add) { result.addAll(expand); } else { result.removeAll(expand); } if (options.allowNoIndices() == false && matches.isEmpty()) { throw indexNotFoundException(expression); } if (Regex.isSimpleMatchPattern(expression)) { wildcardSeen = true; } } return result; }	since this is not an actual assertion (like assert), let's change the name.
public void testInvalidIndex() { MetaData.Builder mdBuilder = MetaData.builder() .put(indexBuilder("testXXX")); ClusterState state = ClusterState.builder(new ClusterName("_name")).metaData(mdBuilder).build(); IndexNameExpressionResolver.Context context = new IndexNameExpressionResolver.Context(state, IndicesOptions.lenientExpandOpen()); InvalidIndexNameException iine = expectThrows(InvalidIndexNameException.class, () -> indexNameExpressionResolver.concreteIndexNames(context, "_foo")); assertEquals("Invalid index name [_foo], must not start with '_'.", iine.getMessage()); }	it's okay to call the index "test", and to fit all of this on one line.
public DocIdAndSeqNo lookupSequenceNo(BytesRef id, Bits liveDocs, LeafReaderContext context) throws IOException { assert context.reader().getCoreCacheKey().equals(readerKey) : "context's reader is not the same as the reader class was initialized on."; int docID = getDocID(id, liveDocs); if (docID != DocIdSetIterator.NO_MORE_DOCS) { return new DocIdAndSeqNo(docID, seqNos == null ? SequenceNumbersService.UNASSIGNED_SEQ_NO : seqNos.get(docID), context); } else { return null; } }	can you add link to indexmetadata#primaryterm as it has a note about the primary term on an *operational* shard always being strictly positive?
public DocIdAndSeqNo lookupSequenceNo(BytesRef id, Bits liveDocs, LeafReaderContext context) throws IOException { assert context.reader().getCoreCacheKey().equals(readerKey) : "context's reader is not the same as the reader class was initialized on."; int docID = getDocID(id, liveDocs); if (docID != DocIdSetIterator.NO_MORE_DOCS) { return new DocIdAndSeqNo(docID, seqNos == null ? SequenceNumbersService.UNASSIGNED_SEQ_NO : seqNos.get(docID), context); } else { return null; } }	can it be package private?
private OpVsLuceneDocStatus compareOpToLuceneDocBasedOnSeqNo(final Operation op) throws IOException { assert op.seqNo() != SequenceNumbersService.UNASSIGNED_SEQ_NO : "resolving ops based seq# but no seqNo is found"; final OpVsLuceneDocStatus status; final VersionValue versionValue = versionMap.getUnderLock(op.uid()); assert incrementVersionLookup(); if (versionValue != null) { if (op.seqNo() > versionValue.seqNo || (op.seqNo() == versionValue.seqNo && op.primaryTerm() > versionValue.term)) status = OpVsLuceneDocStatus.OP_NEWER; else { status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL; } } else { // load from index assert incrementIndexVersionLookup(); try (Searcher searcher = acquireSearcher("load_seq_no")) { DocIdAndSeqNo docAndSeqNo = VersionsAndSeqNoResolver.loadDocIdAndSeqNo(searcher.reader(), op.uid()); if (docAndSeqNo == null) { status = OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND; } else if (op.seqNo() > docAndSeqNo.seqNo) { status = OpVsLuceneDocStatus.OP_NEWER; } else if (op.seqNo() == docAndSeqNo.seqNo) { // load term to tie break final long existingTerm = VersionsAndSeqNoResolver.loadPrimaryTerm(docAndSeqNo); if (op.primaryTerm() > existingTerm) { status = OpVsLuceneDocStatus.OP_NEWER; } else { status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL; } } else { status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL; } } } return status; }	it's so nice to finally see this arriving.
private OpVsLuceneDocStatus compareOpToLuceneDocBasedOnSeqNo(final Operation op) throws IOException { assert op.seqNo() != SequenceNumbersService.UNASSIGNED_SEQ_NO : "resolving ops based seq# but no seqNo is found"; final OpVsLuceneDocStatus status; final VersionValue versionValue = versionMap.getUnderLock(op.uid()); assert incrementVersionLookup(); if (versionValue != null) { if (op.seqNo() > versionValue.seqNo || (op.seqNo() == versionValue.seqNo && op.primaryTerm() > versionValue.term)) status = OpVsLuceneDocStatus.OP_NEWER; else { status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL; } } else { // load from index assert incrementIndexVersionLookup(); try (Searcher searcher = acquireSearcher("load_seq_no")) { DocIdAndSeqNo docAndSeqNo = VersionsAndSeqNoResolver.loadDocIdAndSeqNo(searcher.reader(), op.uid()); if (docAndSeqNo == null) { status = OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND; } else if (op.seqNo() > docAndSeqNo.seqNo) { status = OpVsLuceneDocStatus.OP_NEWER; } else if (op.seqNo() == docAndSeqNo.seqNo) { // load term to tie break final long existingTerm = VersionsAndSeqNoResolver.loadPrimaryTerm(docAndSeqNo); if (op.primaryTerm() > existingTerm) { status = OpVsLuceneDocStatus.OP_NEWER; } else { status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL; } } else { status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL; } } } return status; }	nit: based -> based on
@Override public boolean containsHeader(String name) { return headers().contains(name); }	sorry maybe a naive comment, is it better to name public api getrequestheaders ?
@Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { assert msg instanceof Netty4HttpResponse : "Invalid message type: " + msg.getClass(); Netty4HttpResponse response = (Netty4HttpResponse) msg; setCorsResponseHeaders(response.requestHeaders(), response, config); ctx.write(response, promise); }	do the same for niocorshandler here?
@SuppressWarnings("rawtypes") private static Object value(Object r, Mode mode) { /* * Intervals and GeoShape instances need to be serialized (as in StreamInput/Ouput serialization) as Strings * since SqlQueryResponse creation doesn't have access to GeoShape nor Interval classes to make the decision * so, we flatten them as Strings before being serialized. * CLI gets a special treatment see {@link org.elasticsearch.xpack.sql.action.SqlQueryResponse#value()} */ if (r instanceof GeoShape) { return r.toString(); } else if (r instanceof Interval) { if (mode == CLI) { return r.toString(); } else { return ((Interval) r).value(); } } return r; }	minor nit to eliminate the number of returns is to assign the new value to r and return it instead: suggestion if (r instanceof geoshape) { r = r.tostring(); } else if (r instanceof interval) { if (mode == cli) { r = r.tostring(); } else { r = ((interval) r).value(); } } return r;
public RolloverRequest addMaxIndexSizeCondition(ByteSizeValue size) { MaxSizeCondition maxSizeCondition = new MaxSizeCondition(size); if (this.conditions.containsKey(maxSizeCondition.name())) { throw new IllegalArgumentException(maxSizeCondition + " condition is already set"); } this.conditions.put(maxSizeCondition.name(), maxSizeCondition); return this; }	i think this can remove the "index" part, so it can be addmaxsingleprimarysizecondition
public RolloverRequestBuilder addMaxIndexSizeCondition(ByteSizeValue size){ this.request.addMaxIndexSizeCondition(size); return this; }	super minor nit suggestion public rolloverrequestbuilder addmaxindexsingleprimarysizecondition(bytesizevalue size) { (and same comment about dropping index)
static Map<String, Boolean> evaluateConditions(final Collection<Condition<?>> conditions, @Nullable final Condition.Stats stats) { Objects.requireNonNull(conditions, "conditions must not be null"); if (stats != null) { return conditions.stream() .map(condition -> condition.evaluate(stats)) .collect(Collectors.toMap(result -> result.condition.toString(), result -> result.matched)); } else { // no conditions matched return conditions.stream() .collect(Collectors.toMap(Condition::toString, cond -> false)); } }	we use -1 for the primary size, but 0 for a missing total size below, maybe we should use either -1 or 0 for both since they're both sizes?
@Override public synchronized void updateMetaData(final IndexMetaData metadata) { final Translog.Durability oldTranslogDurability = indexSettings.getTranslogDurability(); if (indexSettings.updateIndexMetaData(metadata)) { for (final IndexShard shard : this.shards.values()) { try { shard.onSettingsChanged(); } catch (Exception e) { logger.warn( (Supplier<?>) () -> new ParameterizedMessage( "[{}] failed to notify shard about setting change", shard.shardId().id()), e); } } if (refreshTask.getInterval().equals(indexSettings.getRefreshInterval()) == false) { rescheduleRefreshTasks(); } if (trimTranslogTask.getInterval().equals(indexSettings.getTranslogRetentionCheckInterval()) == false) { rescheduleTrimTranslogcheck(); } final Translog.Durability durability = indexSettings.getTranslogDurability(); if (durability != oldTranslogDurability) { rescheduleFsyncTask(durability); } } }	can you elaborate why you go down this route instead of using indexshard#afterwriteoperation() and maybe also do it in indexshard#flush? i'd love to rather remove tasks than adding them?! it's just way easier to reason about things if they are happening due to a user interaction form the outside
synchronized long getViewCount(long viewGen) { return translogRefCounts.getOrDefault(viewGen, Counter.newCounter(false)).get(); }	you are creating a new counter anyway even if it's there i think we can safe that object and use computeifabsent?
public synchronized Collection<RetentionLease> getRetentionLeases() { final long currentTimeMillis = currentTimeMillisSupplier.getAsLong(); final long retentionLeaseMillis = indexSettings.getRetentionLeaseMillis(); final Collection<RetentionLease> nonExpiredRetentionLeases = retentionLeases .values() .stream() .filter(retentionLease -> currentTimeMillis - retentionLease.timestamp() <= retentionLeaseMillis) .collect(Collectors.toList()); retentionLeases.clear(); retentionLeases.putAll(nonExpiredRetentionLeases.stream().collect(Collectors.toMap(RetentionLease::id, lease -> lease))); return Collections.unmodifiableCollection(new ArrayList<>(retentionLeases.values())); }	nit: maybe use nonexpiredretentionleases instead of a new arraylist?
public void testGetPrivileges() throws Exception { final RestHighLevelClient client = highLevelClient(); { final Request createPrivilegeRequest = new Request("POST", "/_xpack/security/privilege"); createPrivilegeRequest.setJsonEntity("{" + " \\\\"testapp\\\\": {" + " \\\\"read\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:read/*\\\\" ]" + " }," + " \\\\"write\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:write/*\\\\" ]," + " \\\\"metadata\\\\": { \\\\"key1\\\\": \\\\"value1\\\\" }" + " }," + " \\\\"all\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:write/*\\\\" , \\\\"manage:*\\\\"]" + " }" + " }," + " \\\\"testapp2\\\\": {" + " \\\\"read\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:read/*\\\\" ]," + " \\\\"metadata\\\\": { \\\\"key2\\\\": \\\\"value2\\\\" }" + " }," + " \\\\"write\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:write/*\\\\" ]" + " }," + " \\\\"all\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:write/*\\\\" , \\\\"manage:*\\\\"]" + " }" + " }" + "}"); final Response createPrivilegeResponse = client.getLowLevelClient().performRequest(createPrivilegeRequest); assertEquals(RestStatus.OK.getStatus(), createPrivilegeResponse.getStatusLine().getStatusCode()); } { //tag::get-privileges-request GetPrivilegesRequest request = new GetPrivilegesRequest("testapp", "write"); GetPrivilegesResponse response = client.security().getPrivileges(request, RequestOptions.DEFAULT); //end::get-roles-request assertNotNull(response); assertThat(response.getPrivileges().size(), equalTo(1)); assertThat(response.getPrivileges().get(0).getActions().size(), equalTo(2)); assertThat(response.getPrivileges().get(0).getActions(), containsInAnyOrder("action:login", "data:write/*")); assertThat(response.getPrivileges().get(0).getMetadata().isEmpty(), equalTo(false)); assertThat(response.getPrivileges().get(0).getMetadata().get("key1"), equalTo("value1")); } { //tag::get-all-application-privileges-request GetPrivilegesRequest request = new GetPrivilegesRequest("testapp", null); GetPrivilegesResponse response = client.security().getPrivileges(request, RequestOptions.DEFAULT); //end::get-all-application-privileges-request assertNotNull(response); assertThat(response.getPrivileges().size(), equalTo(3)); List<ApplicationPrivilege> privileges = response.getPrivileges(); for (ApplicationPrivilege privilege : privileges) { assertThat(privilege.getApplication(), equalTo("testapp")); if (privilege.getName().equals("read")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:read/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } else if (privilege.getName().equals("write")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(false)); assertThat(privilege.getMetadata().get("key1"), equalTo("value1")); } else if (privilege.getName().equals("all")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*", "manage:*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } } } { //tag::get-all-privileges-request GetPrivilegesRequest request = new GetPrivilegesRequest(null, null); GetPrivilegesResponse response = client.security().getPrivileges(request, RequestOptions.DEFAULT); //end::get-all-privileges-request assertNotNull(response); assertThat(response.getPrivileges().size(), equalTo(6)); List<ApplicationPrivilege> privileges = response.getPrivileges(); for (ApplicationPrivilege privilege : privileges) { if (privilege.getApplication().equals("testapp")) { if (privilege.getName().equals("read")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:read/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } else if (privilege.getName().equals("write")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(false)); assertThat(privilege.getMetadata().get("key1"), equalTo("value1")); } else if (privilege.getName().equals("all")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*", "manage:*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } } else if (privilege.getApplication().equals("testapp2")) { if (privilege.getName().equals("read")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:read/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(false)); assertThat(privilege.getMetadata().get("key2"), equalTo("value2")); } else if (privilege.getName().equals("write")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } else if (privilege.getName().equals("all")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*", "manage:*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } } } } { //tag::get-privileges-execute-listener GetPrivilegesRequest request = new GetPrivilegesRequest("testapp", "read"); ActionListener<GetPrivilegesResponse> listener = new ActionListener<GetPrivilegesResponse>() { @Override public void onResponse(GetPrivilegesResponse getPrivilegesResponse) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; //end::get-privileges-execute-listener final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); //tag::get-privileges-execute-async client.security().getPrivilegesAsync(request, RequestOptions.DEFAULT, listener); // <1> //end::get-privileges-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } }	nit: i might do this differently by creating the expected response and comparing it to verify they are equal
public void testGetPrivileges() throws Exception { final RestHighLevelClient client = highLevelClient(); { final Request createPrivilegeRequest = new Request("POST", "/_xpack/security/privilege"); createPrivilegeRequest.setJsonEntity("{" + " \\\\"testapp\\\\": {" + " \\\\"read\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:read/*\\\\" ]" + " }," + " \\\\"write\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:write/*\\\\" ]," + " \\\\"metadata\\\\": { \\\\"key1\\\\": \\\\"value1\\\\" }" + " }," + " \\\\"all\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:write/*\\\\" , \\\\"manage:*\\\\"]" + " }" + " }," + " \\\\"testapp2\\\\": {" + " \\\\"read\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:read/*\\\\" ]," + " \\\\"metadata\\\\": { \\\\"key2\\\\": \\\\"value2\\\\" }" + " }," + " \\\\"write\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:write/*\\\\" ]" + " }," + " \\\\"all\\\\": {" + " \\\\"actions\\\\": [ \\\\"action:login\\\\", \\\\"data:write/*\\\\" , \\\\"manage:*\\\\"]" + " }" + " }" + "}"); final Response createPrivilegeResponse = client.getLowLevelClient().performRequest(createPrivilegeRequest); assertEquals(RestStatus.OK.getStatus(), createPrivilegeResponse.getStatusLine().getStatusCode()); } { //tag::get-privileges-request GetPrivilegesRequest request = new GetPrivilegesRequest("testapp", "write"); GetPrivilegesResponse response = client.security().getPrivileges(request, RequestOptions.DEFAULT); //end::get-roles-request assertNotNull(response); assertThat(response.getPrivileges().size(), equalTo(1)); assertThat(response.getPrivileges().get(0).getActions().size(), equalTo(2)); assertThat(response.getPrivileges().get(0).getActions(), containsInAnyOrder("action:login", "data:write/*")); assertThat(response.getPrivileges().get(0).getMetadata().isEmpty(), equalTo(false)); assertThat(response.getPrivileges().get(0).getMetadata().get("key1"), equalTo("value1")); } { //tag::get-all-application-privileges-request GetPrivilegesRequest request = new GetPrivilegesRequest("testapp", null); GetPrivilegesResponse response = client.security().getPrivileges(request, RequestOptions.DEFAULT); //end::get-all-application-privileges-request assertNotNull(response); assertThat(response.getPrivileges().size(), equalTo(3)); List<ApplicationPrivilege> privileges = response.getPrivileges(); for (ApplicationPrivilege privilege : privileges) { assertThat(privilege.getApplication(), equalTo("testapp")); if (privilege.getName().equals("read")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:read/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } else if (privilege.getName().equals("write")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(false)); assertThat(privilege.getMetadata().get("key1"), equalTo("value1")); } else if (privilege.getName().equals("all")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*", "manage:*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } } } { //tag::get-all-privileges-request GetPrivilegesRequest request = new GetPrivilegesRequest(null, null); GetPrivilegesResponse response = client.security().getPrivileges(request, RequestOptions.DEFAULT); //end::get-all-privileges-request assertNotNull(response); assertThat(response.getPrivileges().size(), equalTo(6)); List<ApplicationPrivilege> privileges = response.getPrivileges(); for (ApplicationPrivilege privilege : privileges) { if (privilege.getApplication().equals("testapp")) { if (privilege.getName().equals("read")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:read/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } else if (privilege.getName().equals("write")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(false)); assertThat(privilege.getMetadata().get("key1"), equalTo("value1")); } else if (privilege.getName().equals("all")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*", "manage:*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } } else if (privilege.getApplication().equals("testapp2")) { if (privilege.getName().equals("read")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:read/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(false)); assertThat(privilege.getMetadata().get("key2"), equalTo("value2")); } else if (privilege.getName().equals("write")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } else if (privilege.getName().equals("all")) { assertThat(privilege.getActions(), containsInAnyOrder("action:login", "data:write/*", "manage:*")); assertThat(privilege.getMetadata().isEmpty(), equalTo(true)); } } } } { //tag::get-privileges-execute-listener GetPrivilegesRequest request = new GetPrivilegesRequest("testapp", "read"); ActionListener<GetPrivilegesResponse> listener = new ActionListener<GetPrivilegesResponse>() { @Override public void onResponse(GetPrivilegesResponse getPrivilegesResponse) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; //end::get-privileges-execute-listener final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); //tag::get-privileges-execute-async client.security().getPrivilegesAsync(request, RequestOptions.DEFAULT, listener); // <1> //end::get-privileges-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } }	i think replacing this with the future and testing the outcome would be helpful. i liked what tim did in his apis
private static FunctionDefinition[][] functions() { return new FunctionDefinition[][] { // Scalar functions // String new FunctionDefinition[] { def(Between.class, Between::new, 2, "between"), def(Substring.class, Substring::new, "substring"), }, }; }	should this be three?
public static String substring(String s, Number start, Number end) { return (String) SubstringFunctionProcessor.doProcess(s, start, end); }	can you move this one before substring (to follow the alphabetical order), please?
*/ public boolean isDimension() { return isDimension; } } private final boolean indexed; private final boolean hasDocValues; private final boolean stored; private final boolean ignoreMalformed; private final boolean dimension; private final InetAddress nullValue; private final String nullValueAsString; private final boolean ignoreMalformedByDefault; private final Version indexCreatedVersion; private final Script script; private final FieldValues<InetAddress> scriptValues; private final ScriptCompiler scriptCompiler; private IpFieldMapper(String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo, builder.script.get() != null, builder.onScriptError.get()); this.ignoreMalformedByDefault = builder.ignoreMalformedByDefault; this.indexed = builder.indexed.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); this.stored = builder.stored.getValue(); this.ignoreMalformed = builder.ignoreMalformed.getValue(); this.nullValue = builder.parseNullValue(); this.nullValueAsString = builder.nullValue.getValue(); this.indexCreatedVersion = builder.indexCreatedVersion; this.script = builder.script.get(); this.scriptValues = builder.scriptValues(); this.scriptCompiler = builder.scriptCompiler; this.dimension = builder.dimension.getValue(); } boolean ignoreMalformed() { return ignoreMalformed; } @Override public IpFieldType fieldType() { return (IpFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType().typeName(); } @Override protected void parseCreateField(DocumentParserContext context) throws IOException { InetAddress address; try { address = value(context.parser(), nullValue); } catch (IllegalArgumentException e) { if (ignoreMalformed) { context.addIgnoredField(fieldType().name()); return; } else { throw e; } } if (address != null) { indexValue(context, address); } } private static InetAddress value(XContentParser parser, InetAddress nullValue) throws IOException { String value = parser.textOrNull(); if (value == null) { return nullValue; } return InetAddresses.forString(value); } private void indexValue(DocumentParserContext context, InetAddress address) { if (dimension) { context.doc().getDimensions().addIp(fieldType().name(), address); } if (indexed) { Field field = new InetAddressPoint(fieldType().name(), address); context.doc().add(field); } if (hasDocValues) { context.doc().add(new SortedSetDocValuesField(fieldType().name(), new BytesRef(InetAddressPoint.encode(address)))); } else if (stored || indexed) { context.addToFieldNames(fieldType().name()); } if (stored) { context.doc().add(new StoredField(fieldType().name(), new BytesRef(InetAddressPoint.encode(address)))); } } @Override protected void indexScriptValues( SearchLookup searchLookup, LeafReaderContext readerContext, int doc, DocumentParserContext documentParserContext ) { this.scriptValues.valuesForDoc(searchLookup, readerContext, doc, value -> indexValue(documentParserContext, value)); }	this gets decided when the lucenedocument is built - no need to check.
@Override protected void execute(Terminal terminal, OptionSet options) throws Exception { if (subcommands.isEmpty()) { throw new IllegalStateException("No subcommands configured"); } String[] args = arguments.values(options).toArray(new String[0]); if (args.length == 0) { throw new UserException(ExitCodes.USAGE, "Missing command"); } Command subcommand = subcommands.get(args[0]); if (subcommand == null) { throw new UserException(ExitCodes.USAGE, "Unknown command [" + args[0] + "]"); } subcommand.mainWithoutErrorHandling(Arrays.copyOfRange(args, 1, args.length), terminal); }	i don't think close() is the place to be erroring if subcommands were never setup. this would already happen in execute.
@Override public void close() throws IOException { if (subcommands.isEmpty()) { throw new IllegalStateException("No subcommands configured"); } for (Command command : subcommands.values()) { if (command != null) { command.close(); } } }	please use ioutils.close() instead of iterating over the subcommands, as this will ensure all subcommands are closed, even on failures.
@Override public void close() throws IOException { if (subcommands.isEmpty()) { throw new IllegalStateException("No subcommands configured"); } for (Command command : subcommands.values()) { if (command != null) { command.close(); } } }	if a command is null there is a problem; let us not be lenient like this.
private void authorizeAction(final RequestInfo requestInfo, final String requestId, final AuthorizationInfo authzInfo, final ActionListener<Void> listener) { final Authentication authentication = requestInfo.getAuthentication(); final TransportRequest request = requestInfo.getRequest(); final String action = requestInfo.getAction(); final AuthorizationEngine authzEngine = getAuthorizationEngine(authentication); final AuditTrail auditTrail = auditTrailService.get(); if (ClusterPrivilegeResolver.isClusterAction(action)) { final ActionListener<AuthorizationResult> clusterAuthzListener = wrapPreservingContext(new AuthorizationResultListener<>(result -> { threadContext.putTransient(INDICES_PERMISSIONS_KEY, IndicesAccessControl.ALLOW_ALL); listener.onResponse(null); }, listener::onFailure, requestInfo, requestId, authzInfo), threadContext); authzEngine.authorizeClusterAction(requestInfo, authzInfo, ActionListener.wrap(result -> { if (false == result.isGranted() && QueryApiKeyAction.NAME.equals(action)) { assert request instanceof QueryApiKeyRequest : "request does not match action"; final QueryApiKeyRequest queryApiKeyRequest = (QueryApiKeyRequest) request; if (false == queryApiKeyRequest.isFilterForCurrentUser()) { queryApiKeyRequest.setFilterForCurrentUser(); authzEngine.authorizeClusterAction(requestInfo, authzInfo, clusterAuthzListener); return; } } clusterAuthzListener.onResponse(result); }, clusterAuthzListener::onFailure)); } else if (isIndexAction(action)) { final Metadata metadata = clusterService.state().metadata(); final AsyncSupplier<Set<String>> authorizedIndicesSupplier = new CachingAsyncSupplier<>(authzIndicesListener -> { LoadAuthorizedIndiciesTimeChecker timeChecker = LoadAuthorizedIndiciesTimeChecker.start(requestInfo, authzInfo); authzEngine.loadAuthorizedIndices( requestInfo, authzInfo, metadata.getIndicesLookup(), authzIndicesListener.map(authzIndices -> { timeChecker.done(authzIndices); return authzIndices; }) ); }); final AsyncSupplier<ResolvedIndices> resolvedIndicesAsyncSupplier = new CachingAsyncSupplier<>(resolvedIndicesListener -> { if (request instanceof IndicesRequest && false == indicesAndAliasesResolver.requiresWildcardExpansion((IndicesRequest) request)) { resolvedIndicesListener.onResponse( indicesAndAliasesResolver.resolveIndicesAndAliases(action, (IndicesRequest) request)); } else { authorizedIndicesSupplier.getAsync( ActionListener.wrap( authorizedIndices -> resolvedIndicesListener.onResponse( indicesAndAliasesResolver.resolve(action, request, metadata, authorizedIndices) ), e -> { auditTrail.accessDenied(requestId, authentication, action, request, authzInfo); if (e instanceof IndexNotFoundException) { listener.onFailure(e); } else { listener.onFailure(denialException(authentication, action, request, e)); } } ) ); } }); authzEngine.authorizeIndexAction(requestInfo, authzInfo, resolvedIndicesAsyncSupplier, metadata.getIndicesLookup(), wrapPreservingContext(new AuthorizationResultListener<>(result -> handleIndexActionAuthorizationResult(result, requestInfo, requestId, authzInfo, authzEngine, authorizedIndicesSupplier, resolvedIndicesAsyncSupplier, metadata, listener), listener::onFailure, requestInfo, requestId, authzInfo), threadContext)); } else { logger.warn("denying access as action [{}] is not an index or cluster action", action); auditTrail.accessDenied(requestId, authentication, action, request, authzInfo); listener.onFailure(denialException(authentication, action, request, null)); } }	breaking off the original indicesandaliasesresolver.resolve method in order to avoid authorizedindicessupplier.getasync makes sense. two minor quibbles: * can we integrate the request instanceof indicesrequest inside the indicesandaliasesresolver.requireswildcardexpansion check method. i think it would be easier to follow. * can we overload the original indicesandaliasesresolver.resolve method with the new break off method indicesandaliasesresolver.resolveindicesandaliases ? or something like: "resolvewithoutwildcards". this is just a naming preference.
@Override public void authenticationSuccess(String realm, User user, String action, TransportMessage message) { if (events.contains(AUTHENTICATION_SUCCESS)) { final Optional<String[]> indices = indices(message); if (eventFilterPolicyRegistry.ignorePredicate() .test(new AuditEventMetaInfo(Optional.of(user), Optional.of(realm), Optional.empty(), indices)) == false) { final StringMapMessage logEntry = new StringMapMessage(this.entryCommonFields.commonFields); logEntry.with(EVENT_TYPE_FIELD_NAME, TRANSPORT_ORIGIN_FIELD_VALUE) .with(EVENT_ACTION_FIELD_NAME, "authentication_success") .with(REALM_FIELD_NAME, realm) .with(ACTION_FIELD_NAME, action) .with(REQUEST_NAME_FIELD_NAME, message.getClass().getSimpleName()); principal(user, logEntry); restOrTransportOrigin(message, threadContext, logEntry); if (indices.isPresent()) { logEntry.with(INDICES_FIELD_NAME, Strings.arrayToCommaDelimitedString(indices.get())); } opaqueId(threadContext, logEntry); logger.info(logEntry); } } }	should indices be an array instead of a comma delimited string?
@Before public void init() throws Exception { includeRequestBody = randomBoolean(); settings = Settings.builder() .put("xpack.security.audit.logfile.prefix.emit_node_host_address", randomBoolean()) .put("xpack.security.audit.logfile.prefix.emit_node_host_name", randomBoolean()) .put("xpack.security.audit.logfile.prefix.emit_node_name", randomBoolean()) .put("xpack.security.audit.logfile.events.emit_request_body", includeRequestBody) .build(); localNode = mock(DiscoveryNode.class); when(localNode.getHostAddress()).thenReturn(buildNewFakeTransportAddress().toString()); clusterService = mock(ClusterService.class); when(clusterService.localNode()).thenReturn(localNode); Mockito.doAnswer((Answer) invocation -> { final LoggingAuditTrail arg0 = (LoggingAuditTrail) invocation.getArguments()[0]; arg0.updateLocalNodeInfo(localNode); return null; }).when(clusterService).addListener(Mockito.isA(LoggingAuditTrail.class)); final ClusterSettings clusterSettings = mockClusterSettings(); when(clusterService.getClusterSettings()).thenReturn(clusterSettings); commonFields = new LoggingAuditTrail.EntryCommonFields(settings, localNode).commonFields; threadContext = new ThreadContext(Settings.EMPTY); if (randomBoolean()) { threadContext.putHeader(Task.X_OPAQUE_ID, randomAlphaOfLengthBetween(1, 4)); } patternLayout = PatternLayout.newBuilder().withPattern( "{" + "\\\\"timestamp\\\\":\\\\"%d{ISO8601}\\\\"" + "%varsNotEmpty{, \\\\"node.name\\\\":\\\\"%enc{%map{node.name}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"host.name\\\\":\\\\"%enc{%map{host.name}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"host.ip\\\\":\\\\"%enc{%map{host.ip}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"event.type\\\\":\\\\"%enc{%map{event.type}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"event.action\\\\":\\\\"%enc{%map{event.action}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"user.name\\\\":\\\\"%enc{%map{user.name}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"user.run_by.name\\\\":\\\\"%enc{%map{user.run_by.name}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"user.run_as.name\\\\":\\\\"%enc{%map{user.run_as.name}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"user.realm\\\\":\\\\"%enc{%map{user.realm}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"user.run_by.realm\\\\":\\\\"%enc{%map{user.run_by.realm}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"user.run_as.realm\\\\":\\\\"%enc{%map{user.run_as.realm}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"user.roles\\\\":\\\\"%enc{%map{user.roles}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"origin.type\\\\":\\\\"%enc{%map{origin.type}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"origin.address\\\\":\\\\"%enc{%map{origin.address}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"realm\\\\":\\\\"%enc{%map{realm}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"url.path\\\\":\\\\"%enc{%map{url.path}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"url.query\\\\":\\\\"%enc{%map{url.query}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"request.body\\\\":\\\\"%enc{%map{request.body}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"action\\\\":\\\\"%enc{%map{action}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"request.name\\\\":\\\\"%enc{%map{request.name}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"indices\\\\":\\\\"%enc{%map{indices}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"opaque_id\\\\":\\\\"%enc{%map{opaque_id}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"transport.profile\\\\":\\\\"%enc{%map{transport.profile}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"rule\\\\":\\\\"%enc{%map{rule}}{JSON}\\\\"}" + "%varsNotEmpty{, \\\\"event.category\\\\":\\\\"%enc{%map{event.category}}{JSON}\\\\"}" + "}%n") .withCharset(StandardCharsets.UTF_8) .build(); logger = CapturingLogger.newCapturingLogger(Level.INFO, patternLayout); auditTrail = new LoggingAuditTrail(settings, clusterService, logger, threadContext); }	duplicate the pattern from log4j2.properties here. not strictly necessary to have the same format but a different one is unwarranted.
public void assertLogs() throws Exception { assertFalse("Previous test had an audit-related failure. All subsequent audit related assertions are bogus because we can't " + "guarantee that we fully cleaned up after the last test.", auditFailure); try { assertBusy(() -> { SecurityManager sm = System.getSecurityManager(); if (sm != null) { sm.checkPermission(new SpecialPermission()); } BufferedReader logReader = AccessController.doPrivileged((PrivilegedAction<BufferedReader>) () -> { try { return Files.newBufferedReader(AUDIT_LOG_FILE, StandardCharsets.UTF_8); } catch (IOException e) { throw new RuntimeException(e); } }); logReader.skip(auditLogWrittenBeforeTestStart); List<Map<String, Object>> logs = new ArrayList<>(); String line; while ((line = logReader.readLine()) != null) { try { final Map<String, Object> log = XContentHelper.convertToMap(JsonXContent.jsonXContent, line, false); if (false == ("access_denied".equals(log.get("event.action")) || "access_granted".equals(log.get("event.action")))) { continue; } assertThat(log.containsKey("action"), is(true)); if (false == (SQL_ACTION_NAME.equals(log.get("action")) || GetIndexAction.NAME.equals(log.get("action")))) { // TODO we may want to extend this and the assertions to SearchAction.NAME as well continue; } assertThat(log.containsKey("user.name"), is(true)); List<String> indices = new ArrayList<>(); if (log.containsKey("indices")) { indices = new ArrayList<>(Strings.tokenizeByCommaToSet((String) log.get("indices"))); if ("test_admin".equals(log.get("user.name"))) { /* * Sometimes we accidentally sneak access to the security tables. This is fine, * SQL drops them from the interface. So we might have access to them, but we * don't show them. */ indices.remove(".security"); indices.remove(".security-6"); } } // Use a sorted list for indices for consistent error reporting Collections.sort(indices); log.put("indices", indices); logs.add(log); } catch (final ElasticsearchParseException e) { throw new IllegalArgumentException("Unrecognized log: " + line, e); } } List<Map<String, Object>> allLogs = new ArrayList<>(logs); List<Integer> notMatching = new ArrayList<>(); checker: for (int c = 0; c < logCheckers.size(); c++) { Function<Map<String, Object>, Boolean> logChecker = logCheckers.get(c); for (Iterator<Map<String, Object>> logsItr = logs.iterator(); logsItr.hasNext();) { Map<String, Object> log = logsItr.next(); if (logChecker.apply(log)) { logsItr.remove(); continue checker; } } notMatching.add(c); } if (false == notMatching.isEmpty()) { fail("Some checkers " + notMatching + " didn't match any logs. All logs:" + logsMessage(allLogs) + "\\\\nRemaining logs:" + logsMessage(logs)); } if (false == logs.isEmpty()) { fail("Not all logs matched. Unmatched logs:" + logsMessage(logs)); } }); } catch (AssertionError e) { auditFailure = true; logger.warn("Failed to find an audit log. Skipping remaining tests in this class after this the missing audit" + "logs could turn up later."); throw e; } }	parse events as json but keep the filtering logic for the entries that are checked.
private void messageReceived(TcpChannel channel, InboundMessage message) throws IOException { final InetSocketAddress remoteAddress = channel.getRemoteAddress(); final Header header = message.getHeader(); assert header.needsToReadVariableHeader() == false; final long startTime = threadPool.relativeTimeInMillis(); ThreadContext threadContext = threadPool.getThreadContext(); try (ThreadContext.StoredContext existing = threadContext.stashContext()) { // Place the context with the headers from the message threadContext.setHeaders(header.getHeaders()); threadContext.putTransient("_remote_address", remoteAddress); if (header.isRequest()) { handleRequest(channel, header, message); } else { // Responses do not support short circuiting currently assert message.isShortCircuit() == false; final TransportResponseHandler<?> handler; long requestId = header.getRequestId(); if (header.isHandshake()) { handler = handshaker.removeHandlerForHandshake(requestId); } else { TransportResponseHandler<? extends TransportResponse> theHandler = responseHandlers.onResponseReceived(requestId, messageListener); if (theHandler == null && header.isError()) { handler = handshaker.removeHandlerForHandshake(requestId); } else { handler = theHandler; } } // ignore if its null, the service logs it if (handler != null) { final StreamInput streamInput; if (message.getContentLength() > 0 || header.getVersion().equals(Version.CURRENT) == false) { streamInput = namedWriteableStream(message.openOrGetStreamInput()); assertRemoteVersion(streamInput, header.getVersion()); if (header.isError()) { handlerResponseError(streamInput, handler); } else { handleResponse(remoteAddress, streamInput, handler); } // Check the entire message has been read final int nextByte = streamInput.read(); // calling read() is useful to make sure the message is fully read, even if there is an EOS marker if (nextByte != -1) { throw new IllegalStateException("Message not fully read (response) for requestId [" + requestId + "], handler [" + handler + "], error [" + header.isError() + "]; resetting"); } } else { assert header.isError() == false; handleResponse(remoteAddress, EMPTY_STREAM_INPUT, handler); } } } } finally { final long took = threadPool.relativeTimeInMillis() - startTime; final long logThreshold = slowLogThresholdMs; if (logThreshold > 0 && took > logThreshold) { logger.warn("Slow handling of transport message [{}] took [{}ms]", message, took); } } }	it'd be useful to include the phrase warn threshold as this is a useful search term for the logs when investigating general slowness/instability problems: suggestion logger.warn("handling inbound transport message [{}] took [{}ms] which is above the warn threshold of [{}ms]", message, took, logthreshold);
synchronized ClusterState updateSettings( final ClusterState currentState, final Settings transientToApply, final Settings persistentToApply, final Logger logger ) { boolean changed = false; /* * Our cluster state could have unknown or invalid settings that are known and valid in a previous version of Elasticsearch. We can * end up in this situation during a rolling upgrade where the previous version will infect the current version of Elasticsearch * with settings that the current version either no longer knows about or now considers to have invalid values. When the current * version of Elasticsearch becomes infected with a cluster state containing such settings, we need to skip validating such settings * and instead archive them. Consequently, for the current transient and persistent settings in the cluster state we do the * following: * - split existing settings instance into two with the known and valid settings in one, and the unknown or invalid in another * (note that existing archived settings are included in the known and valid settings) * - validate the incoming settings update combined with the existing known and valid settings * - merge in the archived unknown or invalid settings */ final Tuple<Settings, Settings> partitionedTransientSettings = partitionKnownAndValidSettings( currentState.metadata().transientSettings(), "transient", logger ); final Settings knownAndValidTransientSettings = partitionedTransientSettings.v1(); final Settings unknownOrInvalidTransientSettings = partitionedTransientSettings.v2(); final Settings.Builder transientSettings = Settings.builder().put(knownAndValidTransientSettings); changed |= clusterSettings.updateDynamicSettings(transientToApply, transientSettings, transientUpdates, "transient"); final Tuple<Settings, Settings> partitionedPersistentSettings = partitionKnownAndValidSettings( currentState.metadata().persistentSettings(), "persistent", logger ); final Settings knownAndValidPersistentSettings = partitionedPersistentSettings.v1(); final Settings unknownOrInvalidPersistentSettings = partitionedPersistentSettings.v2(); final Settings.Builder persistentSettings = Settings.builder().put(knownAndValidPersistentSettings); changed |= clusterSettings.updateDynamicSettings(persistentToApply, persistentSettings, persistentUpdates, "persistent"); final ClusterState clusterState; if (changed) { Settings transientFinalSettings = transientSettings.build(); Settings persistentFinalSettings = persistentSettings.build(); // both transient and persistent settings must be consistent by itself we can't allow dependencies to be // in either of them otherwise a full cluster restart will break the settings validation clusterSettings.validate(transientFinalSettings, true); clusterSettings.validate(persistentFinalSettings, true); Metadata.Builder metadata = Metadata.builder(currentState.metadata()) .transientSettings(transientFinalSettings.merge(unknownOrInvalidTransientSettings)) .persistentSettings(persistentFinalSettings.merge(unknownOrInvalidPersistentSettings)); ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks()); boolean updatedReadOnly = Metadata.SETTING_READ_ONLY_SETTING.get(metadata.persistentSettings()) || Metadata.SETTING_READ_ONLY_SETTING.get(metadata.transientSettings()); if (updatedReadOnly) { blocks.addGlobalBlock(Metadata.CLUSTER_READ_ONLY_BLOCK); } else { blocks.removeGlobalBlock(Metadata.CLUSTER_READ_ONLY_BLOCK); } boolean updatedReadOnlyAllowDelete = Metadata.SETTING_READ_ONLY_ALLOW_DELETE_SETTING.get(metadata.persistentSettings()) || Metadata.SETTING_READ_ONLY_ALLOW_DELETE_SETTING.get(metadata.transientSettings()); if (updatedReadOnlyAllowDelete) { blocks.addGlobalBlock(Metadata.CLUSTER_READ_ONLY_ALLOW_DELETE_BLOCK); } else { blocks.removeGlobalBlock(Metadata.CLUSTER_READ_ONLY_ALLOW_DELETE_BLOCK); } clusterState = builder(currentState).metadata(metadata).blocks(blocks).build(); } else { clusterState = currentState; } /* * Now we try to apply things and if they are invalid we fail. This dry run will validate, parse settings, and trigger deprecation * logging, but will not actually apply them. */ final Settings settings = clusterState.metadata().settings(); clusterSettings.validateUpdate(settings); return clusterState; }	at at least in my mind there's a bit of confusion with the new syntax if calling merge would update the 'this' object. it's an instance method, so most commonly in java classes it would mean mutation of the original object, e.g sb.append. it might be clearer if it was a static method, but i don't know if that complicates things. for example: settings.merge(transientfinalsettings, unknownorinvalidtransientsettings) it might even be possible to extend this to arbitrary length of settings parameters with the ellipsis notation?
* @return the collected mapped field types */ static Map<String, MappedFieldType> collectFieldTypes(Collection<RuntimeField> runtimeFields) { return runtimeFields.stream() .flatMap(runtimeField -> { List<String> names = runtimeField.asMappedFieldTypes().stream().map(MappedFieldType::name) .filter(name -> name.equals(runtimeField.name()) == false && (name.startsWith(runtimeField.name() + ".") == false || name.length() > runtimeField.name().length() + 1 == false)) .collect(Collectors.toList()); if (names.isEmpty() == false) { throw new IllegalArgumentException("Found sub-fields with name not belonging to the parent field they are part of " + names); } return runtimeField.asMappedFieldTypes().stream(); }) .collect(Collectors.toUnmodifiableMap(MappedFieldType::name, mappedFieldType -> mappedFieldType, (t, t2) -> { throw new IllegalArgumentException("Found two runtime fields with same name [" + t.name() + "]"); })); }	this is interesting, are we sure that we want to enforce this? we've discussed in the past wanting to have subfields that are aliased out to the root so that they don't appear in their parent object's namespace. if we *do* enforce this then that's another reason to allow runtime aliases to refer to other runtime fields.
@Override public void onFailure(Exception e) { if (e instanceof RetryOnReplicaException) { logger.trace( (org.apache.logging.log4j.util.Supplier<?>) () -> new ParameterizedMessage( "Retrying operation on replica, action [{}], request [{}]", transportReplicaAction, request), e); request.onRetry(); final Supplier<ThreadContext.StoredContext> context = threadPool.getThreadContext().newRestorableContext(false); observer.waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { try (ThreadContext.StoredContext ctx = context.get()) { // Forking a thread on local node via transport service so that custom transport service have an // opportunity to execute custom logic before the replica operation begins String extraMessage = "action [" + transportReplicaAction + "], request[" + request + "]"; TransportChannelResponseHandler<TransportResponse.Empty> handler = new TransportChannelResponseHandler<>(logger, channel, extraMessage, () -> TransportResponse.Empty.INSTANCE); transportService.sendRequest(clusterService.localNode(), transportReplicaAction, new ConcreteShardRequest<>(request, targetAllocationID), handler); } } @Override public void onClusterServiceClose() { try (ThreadContext.StoredContext ctx = context.get()) { responseWithFailure(new NodeClosedException(clusterService.localNode())); } } @Override public void onTimeout(TimeValue timeout) { throw new AssertionError("Cannot happen: there is not timeout"); } }); } else { responseWithFailure(e); } }	is this needed? i think the observer takes care of this? if not, i can work on folding this into the observer as a follow up.
@Override public void onFailure(Exception e) { if (e instanceof RetryOnReplicaException) { logger.trace( (org.apache.logging.log4j.util.Supplier<?>) () -> new ParameterizedMessage( "Retrying operation on replica, action [{}], request [{}]", transportReplicaAction, request), e); request.onRetry(); final Supplier<ThreadContext.StoredContext> context = threadPool.getThreadContext().newRestorableContext(false); observer.waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { try (ThreadContext.StoredContext ctx = context.get()) { // Forking a thread on local node via transport service so that custom transport service have an // opportunity to execute custom logic before the replica operation begins String extraMessage = "action [" + transportReplicaAction + "], request[" + request + "]"; TransportChannelResponseHandler<TransportResponse.Empty> handler = new TransportChannelResponseHandler<>(logger, channel, extraMessage, () -> TransportResponse.Empty.INSTANCE); transportService.sendRequest(clusterService.localNode(), transportReplicaAction, new ConcreteShardRequest<>(request, targetAllocationID), handler); } } @Override public void onClusterServiceClose() { try (ThreadContext.StoredContext ctx = context.get()) { responseWithFailure(new NodeClosedException(clusterService.localNode())); } } @Override public void onTimeout(TimeValue timeout) { throw new AssertionError("Cannot happen: there is not timeout"); } }); } else { responseWithFailure(e); } }	same comment about the observer.
public void testRestorableContext() { Settings build = Settings.builder().put("request.headers.default", "1").build(); ThreadContext threadContext = new ThreadContext(build); threadContext.putHeader("foo", "bar"); threadContext.putTransient("ctx.foo", 1); Supplier<ThreadContext.StoredContext> contextSupplier = threadContext.newRestorableContext(true); try (ThreadContext.StoredContext ctx = threadContext.stashContext()) { assertNull(threadContext.getHeader("foo")); assertEquals("1", threadContext.getHeader("default")); threadContext.addResponseHeader("resp.header", "boom"); try (ThreadContext.StoredContext tmp = contextSupplier.get()) { assertEquals("bar", threadContext.getHeader("foo")); assertEquals(Integer.valueOf(1), threadContext.getTransient("ctx.foo")); assertEquals("1", threadContext.getHeader("default")); assertEquals(1, threadContext.getResponseHeaders().get("resp.header").size()); assertEquals("boom", threadContext.getResponseHeaders().get("resp.header").get(0)); } assertNull(threadContext.getHeader("foo")); assertNull(threadContext.getTransient("ctx.foo")); } assertEquals("bar", threadContext.getHeader("foo")); assertEquals(Integer.valueOf(1), threadContext.getTransient("ctx.foo")); assertEquals("1", threadContext.getHeader("default")); assertNull(threadContext.getResponseHeaders().get("resp.header")); contextSupplier = threadContext.newRestorableContext(false); try (ThreadContext.StoredContext ctx = threadContext.stashContext()) { assertNull(threadContext.getHeader("foo")); assertEquals("1", threadContext.getHeader("default")); threadContext.addResponseHeader("resp.header", "boom"); try (ThreadContext.StoredContext tmp = contextSupplier.get()) { assertEquals("bar", threadContext.getHeader("foo")); assertEquals(Integer.valueOf(1), threadContext.getTransient("ctx.foo")); assertEquals("1", threadContext.getHeader("default")); assertNull(threadContext.getResponseHeaders().get("resp.header")); } assertNull(threadContext.getHeader("foo")); assertNull(threadContext.getTransient("ctx.foo")); } assertEquals("bar", threadContext.getHeader("foo")); assertEquals(Integer.valueOf(1), threadContext.getTransient("ctx.foo")); assertEquals("1", threadContext.getHeader("default")); assertNull(threadContext.getResponseHeaders().get("resp.header")); }	shall we add a duplicate response header too? i.e., one that is both in the original context and the one being replcaed?
@Override public void restoreShard(IndexShard indexShard, SnapshotId snapshotId, Version version, IndexId indexId, ShardId shardId, RecoveryState recoveryState) { // TODO: Add timeouts to network calls / the restore process. final Store store = indexShard.store(); store.incRef(); try { store.createEmpty(indexShard.indexSettings().getIndexMetaData().getCreationVersion().luceneVersion); } catch (EngineException | IOException e) { throw new IndexShardRecoveryException(shardId, "failed to create empty store", e); } finally { store.decRef(); } Store.MetadataSnapshot recoveryMetadata; try { recoveryMetadata = indexShard.snapshotStoreMetadata(); } catch (IOException e) { throw new IndexShardRecoveryException(shardId, "failed access store metadata", e); } Map<String, String> ccrMetaData = indexShard.indexSettings().getIndexMetaData().getCustomData(Ccr.CCR_CUSTOM_METADATA_KEY); String leaderUUID = ccrMetaData.get(Ccr.CCR_CUSTOM_METADATA_LEADER_INDEX_UUID_KEY); ShardId leaderShardId = new ShardId(shardId.getIndexName(), leaderUUID, shardId.getId()); Client remoteClient = client.getRemoteClusterClient(remoteClusterAlias); String sessionUUID = UUIDs.randomBase64UUID(); PutCcrRestoreSessionAction.PutCcrRestoreSessionResponse response = remoteClient.execute(PutCcrRestoreSessionAction.INSTANCE, new PutCcrRestoreSessionRequest(sessionUUID, leaderShardId, recoveryMetadata)).actionGet(); String nodeId = response.getNodeId(); // TODO: Implement file restore closeSession(remoteClient, nodeId, sessionUUID); }	do we need to get the leader index name from ccr.ccr_custom_metadata_leader_index_name_key?
public void testSettingNameWhileRewritingWhenDocumentSupplierAndSourceNotNull() { Supplier<BytesReference> supplier = () -> new BytesArray("{\\\\"test\\\\": \\\\"test\\\\"}"); String testName = "name1"; QueryShardContext shardContext = createShardContext(); PercolateQueryBuilder percolateQueryBuilder = new PercolateQueryBuilder(queryField, supplier::get); percolateQueryBuilder.setName(testName); QueryBuilder rewrittenQueryBuilder = percolateQueryBuilder.doRewrite(shardContext); assertEquals(testName, ((PercolateQueryBuilder) rewrittenQueryBuilder).getQueryName()); assertNotEquals(rewrittenQueryBuilder, percolateQueryBuilder); }	super small comment, could this just be new percolatequerybuilder(queryfield, supplier)?
private <E extends Throwable> E traceLog(String action, E exception) { if (logger.isTraceEnabled()) { if (exception instanceof ElasticsearchException) { final ElasticsearchException esEx = (ElasticsearchException) exception; final Object detail = esEx.getHeader("error_description"); if (detail != null) { logger.trace("Failure in [{}] - [{}] [{}]", action, detail, esEx.getDetailedMessage()); } else { logger.trace("Failure in [{}] - [{}]", action, esEx.getDetailedMessage()); } } else { logger.trace("Failure in [{}] - [{}]", action, exception.toString()); } } return exception; }	lets log the exception itself instead of the detailed message so we get the stacktrace
public void findActiveTokensForUser(String username, ActionListener<Collection<Tuple<UserToken, String>>> listener) { ensureEnabled(); final SecurityIndexManager frozenSecurityIndex = securityIndex.freeze(); if (Strings.isNullOrEmpty(username)) { listener.onFailure(new IllegalArgumentException("username is required")); } else if (frozenSecurityIndex.indexExists() == false) { listener.onResponse(Collections.emptyList()); } else if (frozenSecurityIndex.isAvailable() == false) { listener.onFailure(frozenSecurityIndex.getUnavailableReason()); } else { final Instant now = clock.instant(); final BoolQueryBuilder boolQuery = QueryBuilders.boolQuery() .filter(QueryBuilders.termQuery("doc_type", "token")) .filter(QueryBuilders.boolQuery() .should(QueryBuilders.boolQuery() .must(QueryBuilders.termQuery("access_token.invalidated", false)) .must(QueryBuilders.rangeQuery("access_token.user_token.expiration_time").gte(now.toEpochMilli())) ) .should(QueryBuilders.termQuery("refresh_token.invalidated", false)) ); final SearchRequest request = client.prepareSearch(SecurityIndexManager.SECURITY_INDEX_NAME) .setScroll(DEFAULT_KEEPALIVE_SETTING.get(settings)) .setQuery(boolQuery) .setVersion(false) .setSize(1000) .setFetchSource(true) .request(); securityIndex.checkIndexVersionThenExecute(listener::onFailure, () -> ScrollHelper.fetchAllByEntity(client, request, listener, (SearchHit hit) -> filterAndParseHit(hit, isOfUser(username)))); } }	maybe throw an uncheckedioexception? at the very least, we should log this at some level otherwise we lose the exception and it can make debugging harder.
@Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { final String value; if (context.externalValueSet()) { value = context.externalValue().toString(); } else { XContentParser parser = context.parser(); if (parser.currentToken() == XContentParser.Token.VALUE_NULL) { value = fieldType().nullValueAsString(); } else { value = parser.textOrNull(); } } if (value == null) { return; } RawCollationKey key = collator.getRawCollationKey(value, null); final BytesRef binaryValue = new BytesRef(key.bytes, 0, key.size); if (fieldType().indexOptions() != IndexOptions.NONE || fieldType().stored()) { Field field = new Field(fieldType().name(), binaryValue, fieldType()); fields.add(field); } if (fieldType().hasDocValues()) { if (context.mapperService().getIndexSettings().getIndexVersionCreated().onOrAfter(Version.V_5_6_0)) { fields.add(new SortedSetDocValuesField(fieldType().name(), binaryValue)); } else { fields.add(new SortedDocValuesField(fieldType().name(), binaryValue)); } } }	can you precompute this in the constructor?
public ClusterState execute(ClusterState currentState) { RoutingTable.Builder routingTableBuilder = null; Metadata.Builder metadataBuilder = Metadata.builder(currentState.metadata()); // allow to change any settings to a closed index, and only allow dynamic settings to be changed // on an open index Set<Index> openIndices = new HashSet<>(); Set<Index> closedIndices = new HashSet<>(); final String[] actualIndices = new String[request.indices().length]; for (int i = 0; i < request.indices().length; i++) { Index index = request.indices()[i]; actualIndices[i] = index.getName(); final IndexMetadata metadata = currentState.metadata().getIndexSafe(index); if (metadata.getState() == IndexMetadata.State.OPEN) { openIndices.add(index); } else { closedIndices.add(index); } } if (skippedSettings.isEmpty() == false && openIndices.isEmpty() == false) { throw new IllegalArgumentException( String.format( Locale.ROOT, "Can't update non dynamic settings [%s] for open indices %s", skippedSettings, openIndices ) ); } if (IndexMetadata.INDEX_NUMBER_OF_REPLICAS_SETTING.exists(openSettings)) { final int updatedNumberOfReplicas = IndexMetadata.INDEX_NUMBER_OF_REPLICAS_SETTING.get(openSettings); if (preserveExisting == false) { // Verify that this won't take us over the cluster shard limit. shardLimitValidator.validateShardLimitOnReplicaUpdate(currentState, request.indices(), updatedNumberOfReplicas); /* * We do not update the in-sync allocation IDs as they will be removed upon the first index operation * which makes these copies stale. * * TODO: should we update the in-sync allocation IDs once the data is deleted by the node? */ routingTableBuilder = RoutingTable.builder(currentState.routingTable()); routingTableBuilder.updateNumberOfReplicas(updatedNumberOfReplicas, actualIndices); metadataBuilder.updateNumberOfReplicas(updatedNumberOfReplicas, actualIndices); logger.info("updating number_of_replicas to [{}] for indices {}", updatedNumberOfReplicas, actualIndices); } } updateIndexSettings( openIndices, metadataBuilder, (index, indexSettings) -> indexScopedSettings.updateDynamicSettings( openSettings, indexSettings, Settings.builder(), index.getName() ), preserveExisting, indexScopedSettings ); updateIndexSettings( closedIndices, metadataBuilder, (index, indexSettings) -> indexScopedSettings.updateSettings( closedSettings, indexSettings, Settings.builder(), index.getName() ), preserveExisting, indexScopedSettings ); if (IndexSettings.INDEX_TRANSLOG_RETENTION_AGE_SETTING.exists(normalizedSettings) || IndexSettings.INDEX_TRANSLOG_RETENTION_SIZE_SETTING.exists(normalizedSettings)) { for (String index : actualIndices) { final Settings settings = metadataBuilder.get(index).getSettings(); MetadataCreateIndexService.validateTranslogRetentionSettings(settings); MetadataCreateIndexService.validateStoreTypeSetting(settings); } } boolean changed = false; // increment settings versions for (final String index : actualIndices) { if (same(currentState.metadata().index(index).getSettings(), metadataBuilder.get(index).getSettings()) == false) { changed = true; final IndexMetadata.Builder builder = IndexMetadata.builder(metadataBuilder.get(index)); builder.settingsVersion(1 + builder.settingsVersion()); metadataBuilder.put(builder); } } final ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks()); boolean changedBlocks = false; for (IndexMetadata.APIBlock block : IndexMetadata.APIBlock.values()) { changedBlocks |= maybeUpdateClusterBlock(actualIndices, blocks, block.block, block.setting, openSettings); } changed |= changedBlocks; if (changed == false) { return currentState; } ClusterState updatedState = ClusterState.builder(currentState) .metadata(metadataBuilder) .routingTable(routingTableBuilder == null ? currentState.routingTable() : routingTableBuilder.build()) .blocks(changedBlocks ? blocks.build() : currentState.blocks()) .build(); // we need to tweak auto expand replicas in order to avoid tripping assertions in // AllocationService.reroute(RoutingAllocation allocation) -- this is far from ideal updatedState = allocationService.adaptAutoExpandReplicas(updatedState); try { for (Index index : openIndices) { final IndexMetadata currentMetadata = currentState.metadata().getIndexSafe(index); final IndexMetadata updatedMetadata = updatedState.metadata().getIndexSafe(index); indicesService.verifyIndexMetadata(currentMetadata, updatedMetadata); } for (Index index : closedIndices) { final IndexMetadata currentMetadata = currentState.metadata().getIndexSafe(index); final IndexMetadata updatedMetadata = updatedState.metadata().getIndexSafe(index); // Verifies that the current index settings can be updated with the updated dynamic settings. indicesService.verifyIndexMetadata(currentMetadata, updatedMetadata); // Now check that we can create the index with the updated settings (dynamic and non-dynamic). // This step is mandatory since we allow to update non-dynamic settings on closed indices. indicesService.verifyIndexMetadata(updatedMetadata, updatedMetadata); } } catch (IOException ex) { throw ExceptionsHelper.convertToElastic(ex); } return updatedState; }	just to share the discussion we had on this: this may be something we want to fix in some form. it's somewhat weird that reroute forces an assertion on us that ensure that this was called. maybe this logic should be part of the settings update itself somehow so we don't even have to grind through all the indices here to check if any of them require an update for auto-expand-replicas.
@Override public void deleteBlobsIgnoringIfNotExists(Iterator<String> blobNames) throws IOException { IOException ioe = null; long suppressedExceptions = 0; while (blobNames.hasNext()) { try { IOUtils.rm(path.resolve(blobNames.next())); } catch (IOException e) { // track up to 10 delete exceptions and try to continue deleting on exceptions if (ioe == null) { ioe = e; } else if (ioe.getSuppressed().length < 10) { ioe.addSuppressed(e); } else { ++suppressedExceptions; } } } if (ioe != null) { if (suppressedExceptions > 0) { throw new IOException("Failed to delete files, suppressed [" + suppressedExceptions + "] failures"); } throw ioe; } }	this loses ioe altogether? maybe this: suggestion ioe.addsuppressed(new ioexception("failed to delete files, suppressed [" + suppressedexceptions + "] failures"));
private ObjectPath getWatchHistoryEntry(String watchId, String state) throws Exception { final AtomicReference<ObjectPath> objectPathReference = new AtomicReference<>(); try { assertBusy(() -> { client().performRequest(new Request("POST", "/.watcher-history-*/_refresh")); try (XContentBuilder builder = jsonBuilder()) { builder.startObject(); builder.startObject("query").startObject("bool").startArray("must"); builder.startObject().startObject("term").startObject("watch_id").field("value", watchId).endObject().endObject() .endObject(); if (Strings.isNullOrEmpty(state) == false) { builder.startObject().startObject("term").startObject("state").field("value", state).endObject().endObject() .endObject(); } builder.endArray().endObject().endObject(); builder.startArray("sort").startObject().startObject("trigger_event.triggered_time").field("order", "desc").endObject() .endObject().endArray(); builder.endObject(); Request searchRequest = new Request("POST", "/.watcher-history-*/_search"); searchRequest.addParameter(TOTAL_HITS_AS_INT_PARAM, "true"); searchRequest.setJsonEntity(Strings.toString(builder)); Response response = client().performRequest(searchRequest); ObjectPath objectPath = ObjectPath.createFromResponse(response); int totalHits = objectPath.evaluate("hits.total"); assertThat(totalHits, is(greaterThanOrEqualTo(1))); String watchid = objectPath.evaluate("hits.hits.0._source.watch_id"); assertThat(watchid, is(watchId)); objectPathReference.set(objectPath); } catch (ResponseException e) { final String err = "Failed to perform search of watcher history"; logger.info(err, e); fail(err); } }); } catch (AssertionError ae) { { Request request = new Request("GET", "/_watcher/stats"); Response response = client().performRequest(request); logger.info("watcher_stats: {}", EntityUtils.toString(response.getEntity())); } { Request request = new Request("GET", "/_cluster/state"); Response response = client().performRequest(request); logger.info("cluster_state: {}", EntityUtils.toString(response.getEntity())); } { Request request = new Request("GET", "/.watcher-history-*/_search"); Response response = client().performRequest(request); logger.info("watcher_history_snippets: {}", EntityUtils.toString(response.getEntity())); } throw ae; } return objectPathReference.get(); }	you may want add metric=_all to include current and queued watches. /_watcher/stats?metric=_all
private ObjectPath getWatchHistoryEntry(String watchId, String state) throws Exception { final AtomicReference<ObjectPath> objectPathReference = new AtomicReference<>(); try { assertBusy(() -> { client().performRequest(new Request("POST", "/.watcher-history-*/_refresh")); try (XContentBuilder builder = jsonBuilder()) { builder.startObject(); builder.startObject("query").startObject("bool").startArray("must"); builder.startObject().startObject("term").startObject("watch_id").field("value", watchId).endObject().endObject() .endObject(); if (Strings.isNullOrEmpty(state) == false) { builder.startObject().startObject("term").startObject("state").field("value", state).endObject().endObject() .endObject(); } builder.endArray().endObject().endObject(); builder.startArray("sort").startObject().startObject("trigger_event.triggered_time").field("order", "desc").endObject() .endObject().endArray(); builder.endObject(); Request searchRequest = new Request("POST", "/.watcher-history-*/_search"); searchRequest.addParameter(TOTAL_HITS_AS_INT_PARAM, "true"); searchRequest.setJsonEntity(Strings.toString(builder)); Response response = client().performRequest(searchRequest); ObjectPath objectPath = ObjectPath.createFromResponse(response); int totalHits = objectPath.evaluate("hits.total"); assertThat(totalHits, is(greaterThanOrEqualTo(1))); String watchid = objectPath.evaluate("hits.hits.0._source.watch_id"); assertThat(watchid, is(watchId)); objectPathReference.set(objectPath); } catch (ResponseException e) { final String err = "Failed to perform search of watcher history"; logger.info(err, e); fail(err); } }); } catch (AssertionError ae) { { Request request = new Request("GET", "/_watcher/stats"); Response response = client().performRequest(request); logger.info("watcher_stats: {}", EntityUtils.toString(response.getEntity())); } { Request request = new Request("GET", "/_cluster/state"); Response response = client().performRequest(request); logger.info("cluster_state: {}", EntityUtils.toString(response.getEntity())); } { Request request = new Request("GET", "/.watcher-history-*/_search"); Response response = client().performRequest(request); logger.info("watcher_history_snippets: {}", EntityUtils.toString(response.getEntity())); } throw ae; } return objectPathReference.get(); }	when this fails and is stuck for during the assertbusy, may result in alot of .watcher_history... should we increase the default size ?
@Override public Query toQuery(QueryParseContext parseContext) { validate(); Query query = null; MapperService.SmartNameFieldMappers smartNameFieldMappers = parseContext.smartFieldMappers(this.fieldName); if (smartNameFieldMappers != null && smartNameFieldMappers.hasMapper()) { query = smartNameFieldMappers.mapper().termQuery(this.value, parseContext); } if (query == null) { query = new TermQuery(new Term(this.fieldName, BytesRefs.toBytesRef(this.value))); } query.setBoost(this.boost); if (this.queryName != null) { parseContext.addNamedQuery(queryName, query); } return query; }	maybe add the @override annotation here?
public void deleteDocument(DocumentVersion version, ActionListener<DeleteResponse> listener) { final DeleteRequest request = new DeleteRequest(aliasExists ? ALIAS_NAME : INDEX_NAME) .id(version.id) .setIfSeqNo(version.seqNo) .setIfPrimaryTerm(version.primaryTerm); client.delete(request, ActionListener.wrap(response -> { logger.debug("Deleteed service provider document [{}] ({})", version.id, response.getResult()); listener.onResponse(response); }, listener::onFailure)); }	should we also handle refresh policy here ( pass the one from the request ) ?
public void testRemovePolicyWithIndexingComplete() { String indexName = randomAlphaOfLength(10); String oldPolicyName = "old_policy"; StepKey currentStep = new StepKey(randomAlphaOfLength(10), MockAction.NAME, randomAlphaOfLength(10)); LifecyclePolicy oldPolicy = createPolicy(oldPolicyName, null, currentStep); Settings.Builder indexSettingsBuilder = Settings.builder() .put(LifecycleSettings.LIFECYCLE_NAME, oldPolicyName) .put(LifecycleSettings.LIFECYCLE_INDEXING_COMPLETE, true); LifecycleExecutionState.Builder lifecycleState = LifecycleExecutionState.builder(); lifecycleState.setPhase(currentStep.getPhase()); lifecycleState.setAction(currentStep.getAction()); lifecycleState.setStep(currentStep.getName()); List<LifecyclePolicyMetadata> policyMetadatas = new ArrayList<>(); policyMetadatas.add(new LifecyclePolicyMetadata(oldPolicy, Collections.emptyMap(), randomNonNegativeLong(), randomNonNegativeLong())); ClusterState clusterState = buildClusterState(indexName, indexSettingsBuilder, lifecycleState.build(), policyMetadatas); Index index = clusterState.metaData().index(indexName).getIndex(); Index[] indices = new Index[] { index }; List<String> failedIndexes = new ArrayList<>(); ClusterState newClusterState = IndexLifecycleRunner.removePolicyForIndexes(indices, clusterState, failedIndexes); assertTrue(failedIndexes.isEmpty()); assertIndexNotManagedByILM(newClusterState, index); }	could we add a test that gets an index past rollover, runs remove policy, then sets up ilm on the index again, waits for the rollover step to error, sets indexing complete and runs the retry api? i know this sounds convoluted but i want to make sure we have a test for the scenario when a user want to re-add ilm (because maybe they made a mistake by removing it) and ensure things work as expected.
public boolean isEmpty() { return shards.isEmpty(); }	this doesn't assert that the iteration orders are equal, but that is an important thing to assert.
private boolean invariant() { // initializingShards must consistent with that in shards Collection<ShardRouting> shardRoutingsInitializing = shards.values().stream().filter(ShardRouting::initializing).collect(Collectors.toList()); assert initializingShards.size() == shardRoutingsInitializing.size(); assert initializingShards.containsAll(shardRoutingsInitializing); // relocatingShards must consistent with that in shards Collection<ShardRouting> shardRoutingsRelocating = shards.values().stream().filter(ShardRouting::relocating).collect(Collectors.toList()); assert relocatingShards.size() == shardRoutingsRelocating.size(); assert relocatingShards.containsAll(shardRoutingsRelocating); return true; }	this doesn't assert that the iteration orders are equal, but that is an important thing to assert.
public int read() throws IOException { NetworkBytesReference last = references.peekLast(); if (last == null || last.hasWriteRemaining() == false) { this.references.add(NetworkBytesReference.wrap(new BytesArray(new byte[DEFAULT_READ_LENGTH]))); } int bytesRead = channel.read(references.getLast()); if (bytesRead == -1) { return bytesRead; } rawBytesCount += bytesRead; BytesReference message; while ((message = frameDecoder.decode(createCompositeBuffer(), rawBytesCount)) != null) { int messageLengthWithHeader = message.length(); NetworkBytesReference.vectorizedIncrementReadIndexes(references, messageLengthWithHeader); trimDecodedMessages(messageLengthWithHeader); rawBytesCount -= messageLengthWithHeader; try { BytesReference messageWithoutHeader = message.slice(6, message.length() - 6); // A message length of 6 bytes it is just a ping. Ignore for now. if (messageLengthWithHeader != 6) { handler.handleMessage(messageWithoutHeader, channel, channel.getProfile(), messageWithoutHeader.length()); } } catch (Exception e) { handler.handleException(channel, e); } } return bytesRead; }	right, should we check if the header is correct and if not close the connection?
* @param responses atomic array to hold the responses corresponding to each search request slot * @param responseCounter incremented on each response * @param listener the listener attached to the multi-search request */ private void executeSearch( final Queue<SearchRequestSlot> requests, final AtomicArray<MultiSearchResponse.Item> responses, final AtomicInteger responseCounter, final ActionListener<MultiSearchResponse> listener) { SearchRequestSlot request = requests.poll(); if (request == null) { /* * The number of times that we poll an item from the queue here is the minimum of the number of requests and the maximum number * of concurrent requests. At first glance, it appears that we should never poll from the queue and not obtain a request given * that we only poll here no more times than the number of requests. However, this is not the only consumer of this queue as * earlier requests that have already completed will poll from the queue too and they could complete before later polls are * invoked here. Thus, it can be the case that we poll here and and the queue was empty. */ return; } /* * With a request in hand, we are now prepared to execute the search request. There are two possibilities, either we go asynchronous * or we do not (this can happen if the request does not resolve to any shards). If we do not go asynchronous, we are going to come * back on the same thread that attempted to execute the search request. At this point, or any other point where we come back on the * same thread as when the request was submitted, we should not recurse lest we might descend into a stack overflow. To avoid this, * when we handle the response rather than going recursive, we set ourselves up to loop around and submit a new request. Otherwise * we recurse. */ final AtomicReference<SearchRequestSlot> next = new AtomicReference<>(); final Thread thread = Thread.currentThread(); do { final SearchRequestSlot current; if (next.get() != null) { current = next.get(); next.set(null); } else { current = request; } searchAction.execute(current.request, new ActionListener<SearchResponse>() { @Override public void onResponse(final SearchResponse searchResponse) { handleResponse(current.responseSlot, new MultiSearchResponse.Item(searchResponse, null)); } @Override public void onFailure(final Exception e) { handleResponse(current.responseSlot, new MultiSearchResponse.Item(null, e)); } private void handleResponse(final int responseSlot, final MultiSearchResponse.Item item) { responses.set(responseSlot, item); if (responseCounter.decrementAndGet() == 0) { assert requests.isEmpty(); finish(); } else { if (thread == Thread.currentThread()) { // we are on the same thread, let's set ourselves up to peel off another request and loop. next.set(requests.poll()); } else { // we are on a different thread (we went asynchronous), it's safe to recurse executeSearch(requests, responses, responseCounter, listener); } } } private void finish() { listener.onResponse(new MultiSearchResponse(responses.toArray(new MultiSearchResponse.Item[responses.length()]))); } }); } while (next.get() != null); }	mayb initialize next to the initial request? i think this will mean that we can remove this if and make do with current = next.getandset(null)
* @param responses atomic array to hold the responses corresponding to each search request slot * @param responseCounter incremented on each response * @param listener the listener attached to the multi-search request */ private void executeSearch( final Queue<SearchRequestSlot> requests, final AtomicArray<MultiSearchResponse.Item> responses, final AtomicInteger responseCounter, final ActionListener<MultiSearchResponse> listener) { SearchRequestSlot request = requests.poll(); if (request == null) { /* * The number of times that we poll an item from the queue here is the minimum of the number of requests and the maximum number * of concurrent requests. At first glance, it appears that we should never poll from the queue and not obtain a request given * that we only poll here no more times than the number of requests. However, this is not the only consumer of this queue as * earlier requests that have already completed will poll from the queue too and they could complete before later polls are * invoked here. Thus, it can be the case that we poll here and and the queue was empty. */ return; } /* * With a request in hand, we are now prepared to execute the search request. There are two possibilities, either we go asynchronous * or we do not (this can happen if the request does not resolve to any shards). If we do not go asynchronous, we are going to come * back on the same thread that attempted to execute the search request. At this point, or any other point where we come back on the * same thread as when the request was submitted, we should not recurse lest we might descend into a stack overflow. To avoid this, * when we handle the response rather than going recursive, we set ourselves up to loop around and submit a new request. Otherwise * we recurse. */ final AtomicReference<SearchRequestSlot> next = new AtomicReference<>(); final Thread thread = Thread.currentThread(); do { final SearchRequestSlot current; if (next.get() != null) { current = next.get(); next.set(null); } else { current = request; } searchAction.execute(current.request, new ActionListener<SearchResponse>() { @Override public void onResponse(final SearchResponse searchResponse) { handleResponse(current.responseSlot, new MultiSearchResponse.Item(searchResponse, null)); } @Override public void onFailure(final Exception e) { handleResponse(current.responseSlot, new MultiSearchResponse.Item(null, e)); } private void handleResponse(final int responseSlot, final MultiSearchResponse.Item item) { responses.set(responseSlot, item); if (responseCounter.decrementAndGet() == 0) { assert requests.isEmpty(); finish(); } else { if (thread == Thread.currentThread()) { // we are on the same thread, let's set ourselves up to peel off another request and loop. next.set(requests.poll()); } else { // we are on a different thread (we went asynchronous), it's safe to recurse executeSearch(requests, responses, responseCounter, listener); } } } private void finish() { listener.onResponse(new MultiSearchResponse(responses.toArray(new MultiSearchResponse.Item[responses.length()]))); } }); } while (next.get() != null); }	can we instead always do: next.set(requests.poll());? i think this would make the code simpler?
* @param responses atomic array to hold the responses corresponding to each search request slot * @param responseCounter incremented on each response * @param listener the listener attached to the multi-search request */ private void executeSearch( final Queue<SearchRequestSlot> requests, final AtomicArray<MultiSearchResponse.Item> responses, final AtomicInteger responseCounter, final ActionListener<MultiSearchResponse> listener) { SearchRequestSlot request = requests.poll(); if (request == null) { /* * The number of times that we poll an item from the queue here is the minimum of the number of requests and the maximum number * of concurrent requests. At first glance, it appears that we should never poll from the queue and not obtain a request given * that we only poll here no more times than the number of requests. However, this is not the only consumer of this queue as * earlier requests that have already completed will poll from the queue too and they could complete before later polls are * invoked here. Thus, it can be the case that we poll here and and the queue was empty. */ return; } /* * With a request in hand, we are now prepared to execute the search request. There are two possibilities, either we go asynchronous * or we do not (this can happen if the request does not resolve to any shards). If we do not go asynchronous, we are going to come * back on the same thread that attempted to execute the search request. At this point, or any other point where we come back on the * same thread as when the request was submitted, we should not recurse lest we might descend into a stack overflow. To avoid this, * when we handle the response rather than going recursive, we set ourselves up to loop around and submit a new request. Otherwise * we recurse. */ final AtomicReference<SearchRequestSlot> next = new AtomicReference<>(); final Thread thread = Thread.currentThread(); do { final SearchRequestSlot current; if (next.get() != null) { current = next.get(); next.set(null); } else { current = request; } searchAction.execute(current.request, new ActionListener<SearchResponse>() { @Override public void onResponse(final SearchResponse searchResponse) { handleResponse(current.responseSlot, new MultiSearchResponse.Item(searchResponse, null)); } @Override public void onFailure(final Exception e) { handleResponse(current.responseSlot, new MultiSearchResponse.Item(null, e)); } private void handleResponse(final int responseSlot, final MultiSearchResponse.Item item) { responses.set(responseSlot, item); if (responseCounter.decrementAndGet() == 0) { assert requests.isEmpty(); finish(); } else { if (thread == Thread.currentThread()) { // we are on the same thread, let's set ourselves up to peel off another request and loop. next.set(requests.poll()); } else { // we are on a different thread (we went asynchronous), it's safe to recurse executeSearch(requests, responses, responseCounter, listener); } } } private void finish() { listener.onResponse(new MultiSearchResponse(responses.toArray(new MultiSearchResponse.Item[responses.length()]))); } }); } while (next.get() != null); }	can we assert this is null?
* @return the routing factor for and shrunk index with the given number of target shards. * @throws IllegalArgumentException if the number of source shards is less than the number of target shards or if the source shards * are not divisible by the number of target shards. */ public static int getRoutingFactor(int sourceNumberOfShards, int targetNumberOfShards) { if (sourceNumberOfShards < targetNumberOfShards) { int spare = sourceNumberOfShards; sourceNumberOfShards = targetNumberOfShards; targetNumberOfShards = spare; } int factor = sourceNumberOfShards / targetNumberOfShards; if (factor * targetNumberOfShards != sourceNumberOfShards || factor <= 1) { if (sourceNumberOfShards < targetNumberOfShards) { throw new IllegalArgumentException("the number of source shards [" + sourceNumberOfShards + "] must be a must be a " + "factor of [" + targetNumberOfShards + "]"); } throw new IllegalArgumentException("the number of source shards [" + sourceNumberOfShards + "] must be a must be a " + "multiple of [" + targetNumberOfShards + "]"); } return factor; }	i'm not too happy with this leniency and swap. can we agree on semantics? i think it's worth it even if we have to have too methods. now it leads to small mistakes like in the line below (which checks for an impossible situation) if (sourcenumberofshards < targetnumberofshards) {
static void validateSplitIndex(ClusterState state, String sourceIndex, Set<String> targetIndexMappingsTypes, String targetIndexName, Settings targetIndexSettings) { IndexMetaData sourceMetaData = validateResize(state, sourceIndex, targetIndexMappingsTypes, targetIndexName, targetIndexSettings); if (IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.exists(targetIndexSettings)) { IndexMetaData.selectSplitShard(0, sourceMetaData, IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.get(targetIndexSettings)); } if (sourceMetaData.getCreationVersion().before(Version.V_6_0_0_alpha1)) { // ensure we have a single type. throw new IllegalStateException("source index created version is too old to apply a split operation"); } }	i think that by the time we get here, this must be set.
static void validateSplitIndex(ClusterState state, String sourceIndex, Set<String> targetIndexMappingsTypes, String targetIndexName, Settings targetIndexSettings) { IndexMetaData sourceMetaData = validateResize(state, sourceIndex, targetIndexMappingsTypes, targetIndexName, targetIndexSettings); if (IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.exists(targetIndexSettings)) { IndexMetaData.selectSplitShard(0, sourceMetaData, IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.get(targetIndexSettings)); } if (sourceMetaData.getCreationVersion().before(Version.V_6_0_0_alpha1)) { // ensure we have a single type. throw new IllegalStateException("source index created version is too old to apply a split operation"); } }	same comment - i think this will be set by the time we get here.
static void validateSplitIndex(ClusterState state, String sourceIndex, Set<String> targetIndexMappingsTypes, String targetIndexName, Settings targetIndexSettings) { IndexMetaData sourceMetaData = validateResize(state, sourceIndex, targetIndexMappingsTypes, targetIndexName, targetIndexSettings); if (IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.exists(targetIndexSettings)) { IndexMetaData.selectSplitShard(0, sourceMetaData, IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.get(targetIndexSettings)); } if (sourceMetaData.getCreationVersion().before(Version.V_6_0_0_alpha1)) { // ensure we have a single type. throw new IllegalStateException("source index created version is too old to apply a split operation"); } }	why do we have the requirement for a single type?
static void validateSplitIndex(ClusterState state, String sourceIndex, Set<String> targetIndexMappingsTypes, String targetIndexName, Settings targetIndexSettings) { IndexMetaData sourceMetaData = validateResize(state, sourceIndex, targetIndexMappingsTypes, targetIndexName, targetIndexSettings); if (IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.exists(targetIndexSettings)) { IndexMetaData.selectSplitShard(0, sourceMetaData, IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.get(targetIndexSettings)); } if (sourceMetaData.getCreationVersion().before(Version.V_6_0_0_alpha1)) { // ensure we have a single type. throw new IllegalStateException("source index created version is too old to apply a split operation"); } }	what's the plan around bwc? i presume we can't split indices while there are 6.0 nodes in the cluster. also when we do shrinking we should use the old setting if we're allocating to old nodes. will this come later in the backport?
public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, final LeafBucketCollector sub) throws IOException { if (valuesSource == null) { return LeafBucketCollector.NO_OP_COLLECTOR; } final BigArrays bigArrays = context.bigArrays(); if (valuesSource instanceof ValuesSource.Numeric) { final SortedNumericDocValues values = ((Numeric)valuesSource).longValues(ctx); return new LeafBucketCollectorBase(sub, values) { @Override public void collect(int doc, long bucket) throws IOException { counts = bigArrays.grow(counts, bucket + 1); if (values.advanceExact(doc)) { counts.increment(bucket, values.docValueCount()); } } }; } if (valuesSource instanceof ValuesSource.Bytes.GeoPoint) { MultiGeoPointValues values = ((GeoPoint)valuesSource).geoPointValues(ctx); return new LeafBucketCollectorBase(sub, null) { @Override public void collect(int doc, long bucket) throws IOException { counts = bigArrays.grow(counts, bucket + 1); if (values.advanceExact(doc)) { counts.increment(bucket, values.docValueCount()); } } }; } // The following is default collector. Including the keyword FieldType final SortedBinaryDocValues values = valuesSource.bytesValues(ctx); return new LeafBucketCollectorBase(sub, values) { @Override public void collect(int doc, long bucket) throws IOException { counts = bigArrays.grow(counts, bucket + 1); if (values.advanceExact(doc)) { counts.increment(bucket, values.docValueCount()); } } }; }	i think it'd be nice to have a "value counting" member in valuessource but i'd be happy to merge this as is and open up a follow up myself to do that. or you can, if you want @xjtushilei.
@Override void performDuringNoSnapshot(IndexMetadata indexMetadata, ClusterState currentClusterState, Listener listener) { String indexName = indexMetadata.getIndex().getName(); LifecycleExecutionState lifecycleState = fromIndexMetadata(indexMetadata); String policyName = indexMetadata.getSettings().get(LifecycleSettings.LIFECYCLE_NAME); final String snapshotRepository = lifecycleState.getSnapshotRepository(); if (Strings.hasText(snapshotRepository) == false) { listener.onFailure(new IllegalStateException("snapshot repository is not present for policy [" + policyName + "] and index [" + indexName + "]")); return; } final String snapshotName = lifecycleState.getSnapshotName(); if (Strings.hasText(snapshotName) == false) { listener.onFailure( new IllegalStateException("snapshot name was not generated for policy [" + policyName + "] and index [" + indexName + "]")); return; } String mountedIndexName = restoredIndexPrefix + indexName; if (currentClusterState.metadata().index(mountedIndexName) != null) { logger.debug("mounted index [{}] for policy [{}] and index [{}] already exists. will not attempt to mount the index again", mountedIndexName, policyName, indexName); listener.onResponse(true); return; } final String snapshotIndexName = lifecycleState.getSnapshotIndexName(); if (snapshotIndexName == null) { // This index had its searchable snapshot created prior to a version where we captured // the original index name, so make our best guess at the name indexName = bestEffortIndexNameResolution(indexName); logger.debug("index [{}] using policy [{}] does not have a stored snapshot index name, " + "using our best effort guess of [{}] for the original snapshotted index name", indexMetadata.getIndex().getName(), policyName, indexName); } else { // Use the name of the snapshot as specified in the metadata, because the current index // name not might not reflect the name of the index actually in the snapshot logger.debug("index [{}] using policy [{}] has a different name [{}] within the snapshot to be restored, " + "using the snapshot index name from generated metadata for mounting", indexName, policyName, snapshotIndexName); indexName = snapshotIndexName; } final MountSearchableSnapshotRequest mountSearchableSnapshotRequest = new MountSearchableSnapshotRequest(mountedIndexName, snapshotRepository, snapshotName, indexName, Settings.builder() .put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), Boolean.FALSE.toString()) .build(), // we captured the index metadata when we took the snapshot. the index likely had the ILM execution state in the metadata. // if we were to restore the lifecycle.name setting, the restored index would be captured by the ILM runner and, // depending on what ILM execution state was captured at snapshot time, make it's way forward from _that_ step forward in // the ILM policy. // we'll re-set this setting on the restored index at a later step once we restored a deterministic execution state new String[]{LifecycleSettings.LIFECYCLE_NAME}, // we'll not wait for the snapshot to complete in this step as the async steps are executed from threads that shouldn't // perform expensive operations (ie. clusterStateProcessed) false, storageType); getClient().execute(MountSearchableSnapshotAction.INSTANCE, mountSearchableSnapshotRequest, ActionListener.wrap(response -> { if (response.status() != RestStatus.OK && response.status() != RestStatus.ACCEPTED) { logger.debug("mount snapshot response failed to complete"); throw new ElasticsearchException("mount snapshot response failed to complete, got response " + response.status()); } listener.onResponse(true); }, listener::onFailure)); }	should we log on warn here?
String getRestoredIndexPrefix(StepKey currentKey) { if (storageType == null) { if (currentKey.getPhase().equals(TimeseriesLifecycleType.FROZEN_PHASE)) { return PARTIAL_RESTORED_INDEX_PREFIX; } else { return FULL_RESTORED_INDEX_PREFIX; } } switch (storageType) { case FULL_COPY: return FULL_RESTORED_INDEX_PREFIX; case SHARED_CACHE: return PARTIAL_RESTORED_INDEX_PREFIX; default: throw new IllegalArgumentException("unexpected storage type: " + storageType); } }	this could be static
public static Storage fromString(String type) { if ("full_copy".equals(type)) { return FULL_COPY; } else if ("shared_cache".equals(type)) { return SHARED_CACHE; } else { throw new IllegalArgumentException("unknown searchable snapshot storage type [" + type + "], valid types are: " + Strings.arrayToCommaDelimitedString(Storage.values())); } }	should we equalsignorecase in these branches? or is case important?
public void testResponseStatusHandling() { String indexName = randomAlphaOfLength(10); String policyName = "test-ilm-policy"; Map<String, String> ilmCustom = new HashMap<>(); String snapshotName = indexName + "-" + policyName; ilmCustom.put("snapshot_name", snapshotName); String repository = "repository"; ilmCustom.put("snapshot_repository", repository); IndexMetadata.Builder indexMetadataBuilder = IndexMetadata.builder(indexName).settings(settings(Version.CURRENT).put(LifecycleSettings.LIFECYCLE_NAME, policyName)) .putCustom(LifecycleExecutionState.ILM_CUSTOM_METADATA_KEY, ilmCustom) .numberOfShards(randomIntBetween(1, 5)).numberOfReplicas(randomIntBetween(0, 5)); IndexMetadata indexMetadata = indexMetadataBuilder.build(); ClusterState clusterState = ClusterState.builder(emptyClusterState()).metadata(Metadata.builder().put(indexMetadata, true).build()).build(); { RestoreSnapshotResponse responseWithOKStatus = new RestoreSnapshotResponse(new RestoreInfo("test", List.of(), 1, 1)); try (NoOpClient clientPropagatingOKResponse = getClientTriggeringResponse(responseWithOKStatus)) { MountSnapshotStep step = new MountSnapshotStep(randomStepKey(), randomStepKey(), clientPropagatingOKResponse, RESTORED_INDEX_PREFIX, randomStorageType()); step.performAction(indexMetadata, clusterState, null, new AsyncActionStep.Listener() { @Override public void onResponse(boolean complete) { assertThat(complete, is(true)); } @Override public void onFailure(Exception e) { fail("expecting successful response but got: [" + e.getMessage() + "]"); } }); } } { RestoreSnapshotResponse responseWithACCEPTEDStatus = new RestoreSnapshotResponse((RestoreInfo) null); try (NoOpClient clientPropagatingACCEPTEDResponse = getClientTriggeringResponse(responseWithACCEPTEDStatus)) { MountSnapshotStep step = new MountSnapshotStep(randomStepKey(), randomStepKey(), clientPropagatingACCEPTEDResponse, RESTORED_INDEX_PREFIX, randomStorageType()); step.performAction(indexMetadata, clusterState, null, new AsyncActionStep.Listener() { @Override public void onResponse(boolean complete) { assertThat(complete, is(true)); } @Override public void onFailure(Exception e) { fail("expecting successful response but got: [" + e.getMessage() + "]"); } }); } } }	shall we add test cases for my-partial-restored-potato, restored-partial-potato, and my-restored-partial-potato ?
protected void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception { final FullHttpRequest request; final HttpPipelinedRequest pipelinedRequest; if (this.httpPipeliningEnabled && msg instanceof HttpPipelinedRequest) { pipelinedRequest = (HttpPipelinedRequest) msg; request = (FullHttpRequest) pipelinedRequest.last(); } else { pipelinedRequest = null; request = (FullHttpRequest) msg; } final FullHttpRequest copy = new DefaultFullHttpRequest( request.protocolVersion(), request.method(), request.uri(), Unpooled.copiedBuffer(request.content()), request.headers(), request.trailingHeaders()); final Netty4HttpRequest httpRequest; try { httpRequest = new Netty4HttpRequest(serverTransport.xContentRegistry, copy, ctx.channel()); } catch (Exception ex) { request.release(); throw ex; } final Netty4HttpChannel channel = new Netty4HttpChannel(serverTransport, httpRequest, pipelinedRequest, detailedErrorsEnabled, threadContext); if (request.decoderResult().isSuccess()) { serverTransport.dispatchRequest(httpRequest, channel); } else { assert request.decoderResult().isFailure(); serverTransport.dispatchBadRequest(httpRequest, channel, request.decoderResult().cause()); } }	i don't think this is right, i think this should only be released here if we added a retain which only happens if msg is an instance of httppipelinedrequest.
private static List<JdbcColumnInfo> columnInfo(String tableName, Object... cols) throws JdbcSQLException { List<JdbcColumnInfo> columns = new ArrayList<>(); for (int i = 0; i < cols.length; i++) { Object obj = cols[i]; if (obj instanceof String) { String name = obj.toString(); DataType type = DataType.KEYWORD; if (i + 1 < cols.length) { Object next = cols[i + 1]; // check if the next item it's a type if (next instanceof DataType || next instanceof JDBCType) { try { type = TypeUtils.of((JDBCType) next); i++; } catch (SQLException ex) { throw new JdbcSQLException(ex, "Invalid metadata schema definition"); } } // it's not, use the default and move on } columns.add(new JdbcColumnInfo(name, type, tableName, "INFORMATION_SCHEMA", "", "", 0)); } else { throw new JdbcSQLException("Invalid metadata schema definition"); } } return columns; }	org.elasticsearch.xpack.sql.client.stringutils.empty instead of "", for consistency. this is being used in another place with this constructor, as well (jdbchttpclient.tojdbccolumninfo).
private List<JdbcColumnInfo> toJdbcColumnInfo(List<org.elasticsearch.xpack.sql.proto.ColumnInfo> columns) throws SQLException { List<JdbcColumnInfo> cols = new ArrayList<>(columns.size()); for (ColumnInfo info : columns) { cols.add(new JdbcColumnInfo(info.name(), TypeUtils.of(info.esType()), EMPTY, EMPTY, EMPTY, EMPTY, info.displaySize())); } return cols; }	i don't think the fqcn is needed here.
public void testThrownExceptionsWhenSettingByteTypeValues() throws SQLException { JdbcPreparedStatement jps = createJdbcPreparedStatement(); SQLException sqle = expectThrows(SQLException.class, () -> jps.setObject(1, (byte) 6, Types.TIMESTAMP)); assertEquals("Unable to convert value [6] of type [BYTE] to [TIMESTAMP]", sqle.getMessage()); }	here the error message mentions byte (es specific type) and timestamp (java.sql type)...
public void testSettingStringValues() throws SQLException { JdbcPreparedStatement jps = createJdbcPreparedStatement(); jps.setString(1, "foo bar"); assertEquals("foo bar", value(jps)); assertEquals(KEYWORD, jdbcType(jps)); jps.setObject(1, "foo bar"); assertEquals("foo bar", value(jps)); assertEquals(KEYWORD, jdbcType(jps)); jps.setObject(1, "foo bar", Types.VARCHAR); assertEquals("foo bar", value(jps)); assertEquals(KEYWORD, jdbcType(jps)); assertTrue(value(jps) instanceof String); }	kind of surprising/strange-looking to see this change: the method name is jdbctype() expecting to return a jdbc type, but it's returning our own definition of a data type (specific to es). same goes for the error messages in this same class which will mention our own data type and not the java.sql specific ones.
public static void main(String[] args) throws Exception { try (RestClient client = RestClient.builder(new HttpHost("localhost", 9200)).build()) { //loadEmpDatasetIntoEs(client); loadDocsDatasetIntoEs(client); LogManager.getLogger(DataLoader.class).info("Data loaded"); } }	just a test leftover?
private static boolean areTypesCompatible(DataType left, DataType right) { if (left == right) { return true; } else { return (left == DataType.NULL || right == DataType.NULL) || (left.isString() && right.isString()) || (left.isNumeric() && right.isNumeric()); } }	&& instead of ||.
public void testCreateKeyManagerFromPKCS12ContainingCA() throws Exception { assumeFalse("Can't run in a FIPS JVM", inFipsJvm()); final Settings settings = Settings.builder().put("path.home", createTempDir()).build(); final Path path = getDataPath("/org/elasticsearch/xpack/security/transport/ssl/certs/simple/httpCa.p12"); final SecureString keyStorePassword = new SecureString("password".toCharArray()); final StoreKeyConfig keyConfig = new StoreKeyConfig(path.toString(), "PKCS12", keyStorePassword, keyStorePassword, KeyManagerFactory.getDefaultAlgorithm(), TrustManagerFactory.getDefaultAlgorithm()); KeyStore keyStore = KeyStore.getInstance("PKCS12"); try (InputStream in = Files.newInputStream(path)) { keyStore.load(in, keyStorePassword.getChars()); } List<String> aliases = new ArrayList<>(); for (String s : Collections.list(keyStore.aliases())) { if (keyStore.isKeyEntry(s)) { aliases.add(s); } } assertThat(aliases.size(), equalTo(2)); final X509ExtendedKeyManager keyManager = keyConfig.createKeyManager(TestEnvironment.newEnvironment(settings)); for (String alias : aliases) { PrivateKey key = keyManager.getPrivateKey(alias); assertTrue(key == null || alias.equals("http")); } final String[] new_aliases = keyManager.getServerAliases("RSA", null); final X509Certificate[] certificates = keyManager.getCertificateChain("http"); assertThat(new_aliases.length, equalTo(1)); assertThat(certificates.length, equalTo(2)); }	we should not be filtering here. we should check that the aliases of all entries are 3 so that we can verify below that we do not remove non privatekeyentry entries
* @param source the source settings object to diff * @param defaultSettings the default settings object to diff against */ public void diff(Settings.Builder builder, Settings source, Settings defaultSettings) { if (exists(source) == false && exists(defaultSettings) == false) { // If the setting is in neither source nor default, get the value from source as it may depend on the value of other settings builder.put(getKey(), getRaw(source)); } else if (exists(source) == false && exists(defaultSettings)) { // If the setting is only in the defaults, use the value from the defaults builder.put(getKey(), getRaw(defaultSettings)); } }	i think this might read better if the outer "doesn't exist in source" stays the same and then have two inner conditions for "if (exists in defaults) x else y"
protected void retryRecovery(final RecoveryRef recoveryRef, final Throwable reason, TimeValue retryAfter, TimeValue retryCleanupTimeout, final StartRecoveryRequest currentRequest) { logger.trace( (Supplier<?>) () -> new ParameterizedMessage( "will retry recovery with id [{}] in [{}]", recoveryRef.status().recoveryId(), retryAfter), reason); retryRecovery(recoveryRef, retryAfter, retryCleanupTimeout, currentRequest); }	shoulds we change the status() field to target()?
* @return newly created RecoveryTarget */ public RecoveryTarget resetRecovery(final RecoveryRef recoveryRef, TimeValue retryCleanupTimeout, StartRecoveryRequest currentRequest) { long id = recoveryRef.status().recoveryId(); ShardId shardId = recoveryRef.status().shardId(); RecoveryTarget oldRecoveryTarget = null; RecoveryTarget newRecoveryTarget = null; try { synchronized (onGoingRecoveries) { // swap recovery targets in a synchronized block to ensure that the newly added recovery target is picked up by // cancelRecoveriesForShard whenever the old recovery target is picked up oldRecoveryTarget = onGoingRecoveries.remove(id); if (oldRecoveryTarget == null) { return null; } assert recoveryRef.status() == oldRecoveryTarget : "recovery target not the one we were expecting"; assert oldRecoveryTarget.shardId().equals(shardId) : "shard id mismatch, expected " + shardId + " but was " + oldRecoveryTarget.shardId(); newRecoveryTarget = oldRecoveryTarget.retryCopy(); RecoveryTarget existingTarget = onGoingRecoveries.putIfAbsent(newRecoveryTarget.recoveryId(), newRecoveryTarget); assert existingTarget == null : "found two RecoveryTarget instances with the same id"; } recoveryRef.close(); // close the reference held by RecoveryRunner oldRecoveryTarget.resetRecovery(retryCleanupTimeout); // Closes the current recovery target logger.trace("{} restarted recovery from {}, id [{}]", shardId, oldRecoveryTarget.sourceNode(), newRecoveryTarget.recoveryId()); return newRecoveryTarget; } catch (Exception e) { // fail shard to be safe oldRecoveryTarget.notifyListener(new RecoveryFailedException(currentRequest, "failed to retry recovery", e), true); return null; } } /** * gets the {@link RecoveryTarget } for a given id. The RecoveryStatus returned has it's ref count already incremented * to make sure it's safe to use. However, you must call {@link RecoveryTarget#decRef()}	we need to have a new monitor here..
* @return newly created RecoveryTarget */ public RecoveryTarget resetRecovery(final RecoveryRef recoveryRef, TimeValue retryCleanupTimeout, StartRecoveryRequest currentRequest) { long id = recoveryRef.status().recoveryId(); ShardId shardId = recoveryRef.status().shardId(); RecoveryTarget oldRecoveryTarget = null; RecoveryTarget newRecoveryTarget = null; try { synchronized (onGoingRecoveries) { // swap recovery targets in a synchronized block to ensure that the newly added recovery target is picked up by // cancelRecoveriesForShard whenever the old recovery target is picked up oldRecoveryTarget = onGoingRecoveries.remove(id); if (oldRecoveryTarget == null) { return null; } assert recoveryRef.status() == oldRecoveryTarget : "recovery target not the one we were expecting"; assert oldRecoveryTarget.shardId().equals(shardId) : "shard id mismatch, expected " + shardId + " but was " + oldRecoveryTarget.shardId(); newRecoveryTarget = oldRecoveryTarget.retryCopy(); RecoveryTarget existingTarget = onGoingRecoveries.putIfAbsent(newRecoveryTarget.recoveryId(), newRecoveryTarget); assert existingTarget == null : "found two RecoveryTarget instances with the same id"; } recoveryRef.close(); // close the reference held by RecoveryRunner oldRecoveryTarget.resetRecovery(retryCleanupTimeout); // Closes the current recovery target logger.trace("{} restarted recovery from {}, id [{}]", shardId, oldRecoveryTarget.sourceNode(), newRecoveryTarget.recoveryId()); return newRecoveryTarget; } catch (Exception e) { // fail shard to be safe oldRecoveryTarget.notifyListener(new RecoveryFailedException(currentRequest, "failed to retry recovery", e), true); return null; } } /** * gets the {@link RecoveryTarget } for a given id. The RecoveryStatus returned has it's ref count already incremented * to make sure it's safe to use. However, you must call {@link RecoveryTarget#decRef()}	we discussed an alternative to not have to close this reference here, but make dorecovery use a recoveryid
* @return newly created RecoveryTarget */ public RecoveryTarget resetRecovery(final RecoveryRef recoveryRef, TimeValue retryCleanupTimeout, StartRecoveryRequest currentRequest) { long id = recoveryRef.status().recoveryId(); ShardId shardId = recoveryRef.status().shardId(); RecoveryTarget oldRecoveryTarget = null; RecoveryTarget newRecoveryTarget = null; try { synchronized (onGoingRecoveries) { // swap recovery targets in a synchronized block to ensure that the newly added recovery target is picked up by // cancelRecoveriesForShard whenever the old recovery target is picked up oldRecoveryTarget = onGoingRecoveries.remove(id); if (oldRecoveryTarget == null) { return null; } assert recoveryRef.status() == oldRecoveryTarget : "recovery target not the one we were expecting"; assert oldRecoveryTarget.shardId().equals(shardId) : "shard id mismatch, expected " + shardId + " but was " + oldRecoveryTarget.shardId(); newRecoveryTarget = oldRecoveryTarget.retryCopy(); RecoveryTarget existingTarget = onGoingRecoveries.putIfAbsent(newRecoveryTarget.recoveryId(), newRecoveryTarget); assert existingTarget == null : "found two RecoveryTarget instances with the same id"; } recoveryRef.close(); // close the reference held by RecoveryRunner oldRecoveryTarget.resetRecovery(retryCleanupTimeout); // Closes the current recovery target logger.trace("{} restarted recovery from {}, id [{}]", shardId, oldRecoveryTarget.sourceNode(), newRecoveryTarget.recoveryId()); return newRecoveryTarget; } catch (Exception e) { // fail shard to be safe oldRecoveryTarget.notifyListener(new RecoveryFailedException(currentRequest, "failed to retry recovery", e), true); return null; } } /** * gets the {@link RecoveryTarget } for a given id. The RecoveryStatus returned has it's ref count already incremented * to make sure it's safe to use. However, you must call {@link RecoveryTarget#decRef()}	we talked about an option to run this under the cancellable threads of the new target, so we rely on it's monitor to cancel this if it takes too long. this will save us retrycleanuptimeout setting.
* @param listener snapshot creation listener */ public void createSnapshot(final CreateSnapshotRequest request, final CreateSnapshotListener listener) { final String repositoryName = request.repository(); final String snapshotName = indexNameExpressionResolver.resolveDateMathExpression(request.snapshot()); validate(repositoryName, snapshotName); final SnapshotId snapshotId = new SnapshotId(snapshotName, UUIDs.randomBase64UUID()); // new UUID for the snapshot final RepositoryData repositoryData = repositoriesService.repository(repositoryName).getRepositoryData(); clusterService.submitStateUpdateTask("create_snapshot [" + snapshotName + ']', new ClusterStateUpdateTask() { private SnapshotsInProgress.Entry newSnapshot = null; @Override public ClusterState execute(ClusterState currentState) { validate(repositoryName, snapshotName, currentState); SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE); if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) { throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, "cannot snapshot while a snapshot deletion is in-progress"); } SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); if (snapshots == null || snapshots.entries().isEmpty()) { // Store newSnapshot here to be processed in clusterStateProcessed List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndexNames(currentState, request.indicesOptions(), request.indices())); logger.trace("[{}][{}] creating snapshot for indices [{}]", repositoryName, snapshotName, indices); List<IndexId> snapshotIndices = repositoryData.resolveNewIndices(indices); newSnapshot = new SnapshotsInProgress.Entry(new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), request.partial(), State.INIT, snapshotIndices, System.currentTimeMillis(), repositoryData.getGenId(), null); snapshots = new SnapshotsInProgress(newSnapshot); } else { throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, " a snapshot is already running"); } return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, snapshots).build(); } @Override public void onFailure(String source, Exception e) { logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to create snapshot", repositoryName, snapshotName), e); newSnapshot = null; listener.onFailure(e); } @Override public void clusterStateProcessed(String source, ClusterState oldState, final ClusterState newState) { if (newSnapshot != null) { beginSnapshot(newState, newSnapshot, request.partial(), listener); } } @Override public TimeValue timeout() { return request.masterNodeTimeout(); } }); }	we resolve this twice now, and if you have stuff like now in it, it might resolve to two different values. perhaps we can wrap the original request?
public String render(TextTemplate textTemplate, Map<String, Object> model) { if (textTemplate == null) { return null; } String template = textTemplate.getTemplate(); String mediaType = compileParams(detectContentType(template)); template = trimContentType(textTemplate); int indexStartMustacheExpression = template.indexOf("{{"); int indexEndMustacheExpression = template.indexOf("}}"); if (indexStartMustacheExpression == -1 || indexStartMustacheExpression > indexEndMustacheExpression) { return template; } Map<String, Object> mergedModel = new HashMap<>(); if (textTemplate.getParams() != null) { mergedModel.putAll(textTemplate.getParams()); } mergedModel.putAll(model); Map<String, String> options = null; if (textTemplate.getType() == ScriptType.INLINE) { options = new HashMap<>(); if (textTemplate.getScript() != null && textTemplate.getScript().getOptions() != null) { options.putAll(textTemplate.getScript().getOptions()); } options.put(Script.CONTENT_TYPE_OPTION, mediaType); } Script script = new Script(textTemplate.getType(), textTemplate.getType() == ScriptType.STORED ? null : "mustache", template, options, mergedModel); TemplateScript.Factory compiledTemplate = service.compile(script, Watcher.SCRIPT_TEMPLATE_CONTEXT); return compiledTemplate.newInstance(model).execute(); }	why do we need to check for end brackets at all? isn't }}{{ and invalid mustache template?
public void testDontInvokeScriptServiceOnNonMustacheText() { String input = rarely() ? "}}{{" : "this is my text"; String output = engine.render(new TextTemplate(input), Collections.emptyMap()); assertThat(input, is(output)); verifyZeroInteractions(service); }	if we are going to have this special case, why can't we test it every time, instead of relying on randomization?
public void process(AnalyticsProcess<AnalyticsResult> process) { long totalRows = process.getConfig().rows(); // TODO When java 9 features can be used, we will not need the local variable here try (DataFrameRowsJoiner resultsJoiner = dataFrameRowsJoiner) { Iterator<AnalyticsResult> iterator = process.readAnalyticsResults(); while (iterator.hasNext()) { processResult(iterator.next(), resultsJoiner, totalRows); } } catch (Exception e) { if (isCancelled) { // No need to log error as it's due to stopping } else { setAndReportFailure(e); } } finally { if (isCancelled == false && failure == null) { completeResultsProgress(); } completionLatch.countDown(); } }	here is the offending code piece. latch down was counted down before we close output stream.
default boolean allowSystemIndexAccessByDefault() { return false; }	just for my own learning, was nobody using this interface method before? do we need to still report somewhere that the default supported media type is media_type_registry?
@Override public synchronized void close() { if (chars != null) { Arrays.fill(chars, '\\\\0'); chars = null; } } /** * Returns a new copy of this object that is backed by its own char array. Closing the new instance has no effect on the instance it * was created from. This is useful for APIs which accept a char array and you want to be safe about the API potentially modifying the * char array. For example: * * <pre> * try (SecureString copy = secureString.newCopy()) { * // execute code that uses the char[] one time and no longer needs it such as the keystore API * KeyStore store = KeyStore.getInstance("jks"); * store.load(inputStream, copy.getChars()); * ... * }	i wonder if this new method/accessing the char[] is something we should just deprecate immediately? long term i don't think eg we should not be using the keystore to store a password for another keystore; the contents of the other keystore should be moved into the elasticsearch keystore.
@Override public synchronized void close() { if (chars != null) { Arrays.fill(chars, '\\\\0'); chars = null; } } /** * Returns a new copy of this object that is backed by its own char array. Closing the new instance has no effect on the instance it * was created from. This is useful for APIs which accept a char array and you want to be safe about the API potentially modifying the * char array. For example: * * <pre> * try (SecureString copy = secureString.newCopy()) { * // execute code that uses the char[] one time and no longer needs it such as the keystore API * KeyStore store = KeyStore.getInstance("jks"); * store.load(inputStream, copy.getChars()); * ... * }	why wouldn't we just override clone()?
public void testAttributeMapWithSameAliasesCanResolveAttributes() { Alias param1 = createIntParameterAlias(1, 100); Alias param2 = createIntParameterAlias(2, 100); assertTrue(param1.equals(param2)); assertTrue(param1.semanticEquals(param2)); // equality on literals assertTrue(param1.child().equals(param2.child())); assertTrue(param1.child().semanticEquals(param2.child())); assertTrue(param1.toAttribute().equals(param2.toAttribute())); assertFalse(param1.toAttribute().semanticEquals(param2.toAttribute())); Map<Attribute, Expression> collectRefs = new LinkedHashMap<>(); for (Alias a : List.of(param1, param2)) { collectRefs.put(a.toAttribute(), a.child()); } // we can look up the same item by both attributes assertNotNull(collectRefs.get(param1.toAttribute())); assertNotNull(collectRefs.get(param2.toAttribute())); AttributeMap<Expression> attributeMap = new AttributeMap<>(collectRefs); // validate that all Alias can be e assertTrue(attributeMap.containsKey(param1.toAttribute())); assertFalse(attributeMap.containsKey(param2.toAttribute())); // results in unknown attribute exception AttributeMap.Builder<Expression> mapBuilder = AttributeMap.builder(); for (Alias a : List.of(param1, param2)) { mapBuilder.put(a.toAttribute(), a.child()); } AttributeMap<Expression> newAttributeMap = mapBuilder.build(); assertTrue(newAttributeMap.containsKey(param1.toAttribute())); assertTrue(newAttributeMap.containsKey(param2.toAttribute())); // no more unknown attribute exception }	nit: "~ wrapper collided" or some other comment correction (can be the next pr, obv.)
public void testAttributeMapWithSameAliasesCanResolveAttributes() { Alias param1 = createIntParameterAlias(1, 100); Alias param2 = createIntParameterAlias(2, 100); assertTrue(param1.equals(param2)); assertTrue(param1.semanticEquals(param2)); // equality on literals assertTrue(param1.child().equals(param2.child())); assertTrue(param1.child().semanticEquals(param2.child())); assertTrue(param1.toAttribute().equals(param2.toAttribute())); assertFalse(param1.toAttribute().semanticEquals(param2.toAttribute())); Map<Attribute, Expression> collectRefs = new LinkedHashMap<>(); for (Alias a : List.of(param1, param2)) { collectRefs.put(a.toAttribute(), a.child()); } // we can look up the same item by both attributes assertNotNull(collectRefs.get(param1.toAttribute())); assertNotNull(collectRefs.get(param2.toAttribute())); AttributeMap<Expression> attributeMap = new AttributeMap<>(collectRefs); // validate that all Alias can be e assertTrue(attributeMap.containsKey(param1.toAttribute())); assertFalse(attributeMap.containsKey(param2.toAttribute())); // results in unknown attribute exception AttributeMap.Builder<Expression> mapBuilder = AttributeMap.builder(); for (Alias a : List.of(param1, param2)) { mapBuilder.put(a.toAttribute(), a.child()); } AttributeMap<Expression> newAttributeMap = mapBuilder.build(); assertTrue(newAttributeMap.containsKey(param1.toAttribute())); assertTrue(newAttributeMap.containsKey(param2.toAttribute())); // no more unknown attribute exception }	maybe also check the returned values with get() from the map, can be done in the next pr though.
public void writeTo(StreamOutput out) throws IOException { out.writeStringArray(indices); out.writeStringArray(aliases); this.aliasAction.writeTo(out); } } /** * Adds an alias to the index. * @param alias The alias * @param indices The indices */ public IndicesAliasesRequest addAlias(String alias, String... indices) { addAliasAction(new AliasActions(AliasAction.Type.ADD, indices, alias)); return this; } public void addAliasAction(AliasActions aliasAction) { allAliasActions.add(aliasAction); } public IndicesAliasesRequest addAliasAction(AliasAction action) { addAliasAction(new AliasActions(action)); return this; } /** * Adds an alias to the index. * @param alias The alias * @param filter The filter * @param indices The indices */ public IndicesAliasesRequest addAlias(String alias, Map<String, Object> filter, String... indices) { addAliasAction(new AliasActions(AliasAction.Type.ADD, indices, alias).filter(filter)); return this; } /** * Adds an alias to the index. * @param alias The alias * @param filterBuilder The filter * @param indices The indices */ public IndicesAliasesRequest addAlias(String alias, QueryBuilder filterBuilder, String... indices) { addAliasAction(new AliasActions(AliasAction.Type.ADD, indices, alias).filter(filterBuilder)); return this; } /** * Removes an alias to the index. * * @param indices The indices * @param aliases The aliases */ public IndicesAliasesRequest removeAlias(String[] indices, String... aliases) { addAliasAction(new AliasActions(AliasAction.Type.REMOVE, indices, aliases)); return this; } /** * Removes an alias to the index. * * @param index The index * @param aliases The aliases */ public IndicesAliasesRequest removeAlias(String index, String... aliases) { addAliasAction(new AliasActions(AliasAction.Type.REMOVE, index, aliases)); return this; } List<AliasActions> aliasActions() { return this.allAliasActions; } public List<AliasActions> getAliasActions() { return aliasActions(); } @Override public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (allAliasActions.isEmpty()) { return addValidationError("Must specify at least one alias action", validationException); } for (AliasActions aliasAction : allAliasActions) { if (aliasAction.aliases.length == 0) { validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH) + "]: aliases may not be empty", validationException); } for (String alias : aliasAction.aliases) { if (!Strings.hasText(alias)) { validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH) + "]: [alias] may not be empty string", validationException); } } if (CollectionUtils.isEmpty(aliasAction.indices)) { validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH) + "]: Property [index] was either missing or null", validationException); } else { for (String index : aliasAction.indices) { if (!Strings.hasText(index)) { validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH) + "]: [index] may not be empty string", validationException); } } } } return validationException; } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); int size = in.readVInt(); for (int i = 0; i < size; i++) { allAliasActions.add(readAliasActions(in)); } readTimeout(in); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeVInt(allAliasActions.size()); for (AliasActions aliasAction : allAliasActions) { aliasAction.writeTo(out); } writeTimeout(out); } public IndicesOptions indicesOptions() { return INDICES_OPTIONS; } private static AliasActions readAliasActions(StreamInput in) throws IOException { AliasActions actions = new AliasActions(); return actions.readFrom(in); }	this should be aliasaction.aliases == null || aliasaction.aliases.length == 0
public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws Exception { IndicesAliasesRequest indicesAliasesRequest = new IndicesAliasesRequest(); indicesAliasesRequest.masterNodeTimeout(request.paramAsTime("master_timeout", indicesAliasesRequest.masterNodeTimeout())); try (XContentParser parser = XContentFactory.xContent(request.content()).createParser(request.content())) { // { // actions : [ // { add : { index : "test1", alias : "alias1", filter : {"user" : "kimchy"} } } // { remove : { index : "test1", alias : "alias1" } } // ] // } indicesAliasesRequest.timeout(request.paramAsTime("timeout", indicesAliasesRequest.timeout())); XContentParser.Token token = parser.nextToken(); if (token == null) { throw new IllegalArgumentException("No action is specified"); } while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.START_ARRAY) { while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token == XContentParser.Token.FIELD_NAME) { String action = parser.currentName(); AliasAction.Type type; if ("add".equals(action)) { type = AliasAction.Type.ADD; } else if ("remove".equals(action)) { type = AliasAction.Type.REMOVE; } else { throw new IllegalArgumentException("Alias action [" + action + "] not supported"); } String[] indices = null; String[] aliases = null; Map<String, Object> filter = null; String routing = null; boolean routingSet = false; String indexRouting = null; boolean indexRoutingSet = false; String searchRouting = null; boolean searchRoutingSet = false; String currentFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token.isValue()) { if ("index".equals(currentFieldName)) { indices = new String[] { parser.text() }; } else if ("alias".equals(currentFieldName)) { aliases = new String[] { parser.text() }; } else if ("routing".equals(currentFieldName)) { routing = parser.textOrNull(); routingSet = true; } else if ("indexRouting".equals(currentFieldName) || "index-routing".equals(currentFieldName) || "index_routing".equals(currentFieldName)) { indexRouting = parser.textOrNull(); indexRoutingSet = true; } else if ("searchRouting".equals(currentFieldName) || "search-routing".equals(currentFieldName) || "search_routing".equals(currentFieldName)) { searchRouting = parser.textOrNull(); searchRoutingSet = true; } } else if (token == XContentParser.Token.START_ARRAY) { if ("indices".equals(currentFieldName)) { List<String> indexNames = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { String index = parser.text(); indexNames.add(index); } indices = indexNames.toArray(new String[indexNames.size()]); } if ("aliases".equals(currentFieldName)) { List<String> aliasNames = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { String alias = parser.text(); aliasNames.add(alias); } aliases = aliasNames.toArray(new String[aliasNames.size()]); } } else if (token == XContentParser.Token.START_OBJECT) { if ("filter".equals(currentFieldName)) { filter = parser.mapOrdered(); } } } if (type == AliasAction.Type.ADD) { AliasActions aliasActions = new AliasActions(type, indices, aliases); if (routingSet) { aliasActions.routing(routing); } if (indexRoutingSet) { aliasActions.indexRouting(indexRouting); } if (searchRoutingSet) { aliasActions.searchRouting(searchRouting); } indicesAliasesRequest.addAliasAction(aliasActions); } else if (type == AliasAction.Type.REMOVE) { indicesAliasesRequest.removeAlias(indices, aliases); } } } } } } client.admin().indices().aliases(indicesAliasesRequest, new AcknowledgedRestListener<IndicesAliasesResponse>(channel)); }	the bug is here.
private void checkExecutionException(Consumer<Runnable> runner, boolean expectException, Function<Runnable, String> logMessageFunction) throws InterruptedException { final Runnable runnable; final boolean willThrow; if (randomBoolean()) { logger.info("checking direct exception for {}", runner); runnable = () -> { throw new IllegalStateException("future exception"); }; willThrow = expectException; } else { logger.info("checking abstract runnable exception for {}", runner); runnable = new AbstractRunnable() { @Override public void onFailure(Exception e) { } @Override protected void doRun() { throw new IllegalStateException("future exception"); } }; willThrow = false; } runExecutionTest( runner, runnable, willThrow, o -> { assertEquals(willThrow, o.isPresent()); if (willThrow) { if (o.get() instanceof Error) throw (Error) o.get(); assertThat(o.get(), instanceOf(IllegalStateException.class)); assertThat(o.get(), hasToString(containsString("future exception"))); } }, logMessageFunction); }	why was this change necessary? we assert in the next line that it's an illegalstateexception, we should throw an assertionerror if not the case?
this.executor = executor; this.scheduler = scheduler; this.rejectionConsumer = rejectionConsumer; this.failureConsumer = failureConsumer; scheduler.schedule(this, interval, executor); }	nit: make this final
public void cancel() { assert responseHandlers.contains(requestId) == false : "cancel must be called after the requestId [" + requestId + "] has been removed from clientHandlers"; if (cancellable != null) { cancellable.cancel(); } }	package-visible instead of public?
static Request listTasks(ListTasksRequest request) { if (request.getTaskId() != null && request.getTaskId().isSet()) { throw new IllegalArgumentException("TaskId cannot be used for list tasks request"); } Params params = Params.builder() .withTimeout(request.getTimeout()) .withDetailed(request.getDetailed()) .withWaitForCompletion(request.getWaitForCompletion()) .withParentTaskId(request.getParentTaskId()) .withNodes(request.getNodes()) .withActions(request.getActions()) .putParam("group_by", "none"); return new Request(HttpGet.METHOD_NAME, "/_tasks", params.getParams(), null); }	for my understanding: is "group_by" fixed here because the current implementation of listtasksrequests doesn't prrovide it (i had a quick look and couldn't find anything)? if so, should we support it? when i look at the docs it seems like group_by=parents is supported.
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ListTasksResponse that = (ListTasksResponse) o; return Objects.equals(tasks, that.tasks) && Objects.equals(getTaskFailures(), that.getTaskFailures()); }	i assume the taskfailures are left out here in purpose? in case we keep equals/hashcode here, should they be added?
public static <T extends ListTasksResponse> ActionListener<T> listTasksResponseListener( Supplier<DiscoveryNodes> nodesInCluster, String groupBy, final RestChannel channel) { if ("nodes".equals(groupBy)) { return new RestBuilderListener<T>(channel) { @Override public RestResponse buildResponse(T response, XContentBuilder builder) throws Exception { builder.startObject(); response.toXContentGroupedByNode(builder, channel.request(), nodesInCluster.get()); builder.endObject(); return new BytesRestResponse(RestStatus.OK, builder); } }; } else if ("parents".equals(groupBy)) { return new RestBuilderListener<T>(channel) { @Override public RestResponse buildResponse(T response, XContentBuilder builder) throws Exception { builder.startObject(); response.toXContentGroupedByParents(builder, channel.request()); builder.endObject(); return new BytesRestResponse(RestStatus.OK, builder); } }; } else if ("none".equals(groupBy)) { return new RestToXContentListener<>(channel); } else { throw new IllegalArgumentException("[group_by] must be one of [nodes], [parents] or [none] but was [" + groupBy + "]"); } }	why the switch here? i'm probably missing some context.
@Override public HealthIndicatorResult calculate(boolean includeDetails) { var snapshotMetadata = clusterService.state().metadata().custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY); if (snapshotMetadata.repositories().isEmpty()) { return createIndicator( GREEN, "No repositories configured.", HealthIndicatorDetails.EMPTY, Collections.emptyList(), Collections.emptyList() ); } var corrupted = snapshotMetadata.repositories() .stream() .filter(repository -> repository.generation() == RepositoryData.CORRUPTED_REPO_GEN) .map(RepositoryMetadata::name) .toList(); var totalRepositories = snapshotMetadata.repositories().size(); var corruptedRepositories = corrupted.size(); if (corrupted.isEmpty()) { return createIndicator( GREEN, "No corrupted repositories.", includeDetails ? new SimpleHealthIndicatorDetails(Map.of("total_repositories", totalRepositories)) : HealthIndicatorDetails.EMPTY, Collections.emptyList(), Collections.emptyList() ); } List<HealthIndicatorImpact> impacts = Collections.singletonList( new HealthIndicatorImpact( 2, "Snapshots in corrupted repositories cannot be restored. Data loss is possible.", List.of(ImpactArea.SEARCH) ) ); return createIndicator( RED, createCorruptedRepositorySummary(corrupted), includeDetails ? new SimpleHealthIndicatorDetails( Map.of( "total_repositories", totalRepositories, "corrupted_repositories", corruptedRepositories, "corrupted", limitSize(corrupted, 10) ) ) : HealthIndicatorDetails.EMPTY, impacts, Collections.emptyList() ); }	should we add a backup impact area? maybe the corrupted repository should be severity 1? i'm also wondering if we should include the repository names in the impact (similarly to how we do it in the shards_availability indicator) https://github.com/elastic/elasticsearch/blob/1cdb03ddb1c97b4869de33515305b7daee6cebb7/server/src/main/java/org/elasticsearch/cluster/routing/allocation/shardsavailabilityhealthindicatorservice.java#l762
@Override public HealthIndicatorResult calculate(boolean includeDetails) { var ilmMetadata = clusterService.state().metadata().custom(IndexLifecycleMetadata.TYPE, IndexLifecycleMetadata.EMPTY); if (ilmMetadata.getPolicyMetadatas().isEmpty()) { return createIndicator( GREEN, "No policies configured", createDetails(includeDetails, ilmMetadata), Collections.emptyList(), Collections.emptyList() ); } else if (ilmMetadata.getOperationMode() != OperationMode.RUNNING) { List<HealthIndicatorImpact> impacts = Collections.singletonList( new HealthIndicatorImpact( 3, "Indices are not being rolled over, which could lead to future instability.", List.of(ImpactArea.SEARCH) ) ); return createIndicator( YELLOW, "ILM is not running", createDetails(includeDetails, ilmMetadata), impacts, Collections.emptyList() ); } else { return createIndicator( GREEN, "ILM is running", createDetails(includeDetails, ilmMetadata), Collections.emptyList(), Collections.emptyList() ); } }	is there anything more specific that could be said about "future instability"? as in, could you get more specific around potential for running out of resources or data loss or something that might be coming?
@Override public HealthIndicatorResult calculate(boolean includeDetails) { var ilmMetadata = clusterService.state().metadata().custom(IndexLifecycleMetadata.TYPE, IndexLifecycleMetadata.EMPTY); if (ilmMetadata.getPolicyMetadatas().isEmpty()) { return createIndicator( GREEN, "No policies configured", createDetails(includeDetails, ilmMetadata), Collections.emptyList(), Collections.emptyList() ); } else if (ilmMetadata.getOperationMode() != OperationMode.RUNNING) { List<HealthIndicatorImpact> impacts = Collections.singletonList( new HealthIndicatorImpact( 3, "Indices are not being rolled over, which could lead to future instability.", List.of(ImpactArea.SEARCH) ) ); return createIndicator( YELLOW, "ILM is not running", createDetails(includeDetails, ilmMetadata), impacts, Collections.emptyList() ); } else { return createIndicator( GREEN, "ILM is running", createDetails(includeDetails, ilmMetadata), Collections.emptyList(), Collections.emptyList() ); } }	should we add a deployment management impact area? i wonder about the impact message. rollover is an implementation detail. maybe we shouldn't mention it in this case. how about? automatic index lifecycle and data retention management is disabled. the performance and stability of your system could be impacted.
public void testSignIntegerType() { assertEquals(-1, MathOperation.SIGN.apply((byte) -42)); assertEquals( -1, MathOperation.SIGN.apply((short) -42)); assertEquals(-1, MathOperation.SIGN.apply(-42)); assertEquals( -1, MathOperation.SIGN.apply((long) -42)); assertEquals(-1, MathOperation.SIGN.apply(-42.0f)); assertEquals(-1, MathOperation.SIGN.apply(-42.0d)); assertEquals( 1, MathOperation.SIGN.apply((byte) 42)); assertEquals( 1, MathOperation.SIGN.apply((short) 42)); assertEquals(1, MathOperation.SIGN.apply(42)); assertEquals( 1, MathOperation.SIGN.apply((long) 42)); assertEquals(1, MathOperation.SIGN.apply(42.0f)); assertEquals(1, MathOperation.SIGN.apply(42.0d)); assertEquals(0, MathOperation.SIGN.apply((byte) 0)); assertEquals( 0, MathOperation.SIGN.apply((short) 0)); assertEquals(0, MathOperation.SIGN.apply(0)); assertEquals( 0, MathOperation.SIGN.apply((long) 0)); assertEquals(0, MathOperation.SIGN.apply(-0.0f)); assertEquals(0, MathOperation.SIGN.apply(-0.0d)); }	since the assertion values are the same, please put the values in an array and iterate over them: java list<number> negative = aslist(-42, long.value(-42), etc...) for (number number : negative) { number result = mathoperations.sign.apply(number); assertequals(integer.class, result.getclass()); assertequals(-1, result); } ...
private <T, C extends Collection<? super T>> C readCollection(Writeable.Reader<T> reader, IntFunction<C> constructor, C empty) throws IOException { int count = readArraySize(); if (count == 0) { return empty; } C builder = constructor.apply(count); for (int i=0; i<count; i++) { builder.add(reader.read(this)); } return builder; } /** * Reads a list of {@link NamedWriteable}	instead of passing in an empty already constructed object (even if it is empty, it would still create a ref right?), i think we could pass in supplier<c>, then use eg collections::emptyset above?
@Override public void collect(int doc, long owningBucketOrdinal) throws IOException { aggregators = bigArrays.grow(aggregators, owningBucketOrdinal + 1); Aggregator aggregator = aggregators.get(owningBucketOrdinal); if (aggregator == null) { aggregator = create(parent.context(), factory, parent, estimatedBucketsCount); aggregator.setNextReader(currentReader()); aggregators.set(owningBucketOrdinal, aggregator); } aggregator.collect(doc, 0); }	maybe use a foreach loop here?
void replay(BucketCollector collector) throws IOException { lastDocId = 0; collector.setNextReader(this.topReaderContext); collector.setNextReader(readerContext); collector.setScorer(unavailableScorer); if (!hasItems()) { return; } if (buckets == null) { final AppendingDeltaPackedLongBuffer.Iterator docsIter = docs.iterator(); while (docsIter.hasNext()) { lastDocId += (int) docsIter.next(); collector.collect(lastDocId, 0); } } else { assert docs.size() == buckets.size(); final AppendingDeltaPackedLongBuffer.Iterator docsIter = docs.iterator(); final AppendingDeltaPackedLongBuffer.Iterator bucketsIter = buckets.iterator(); while (docsIter.hasNext()) { lastDocId += (int) docsIter.next(); collector.collect(lastDocId, bucketsIter.next()); } } }	hmm we do this twice? this looks odd
public void test73CustomJvmOptionsDirectoryFilesWithoutOptionsExtensionIgnored() throws Exception { final Path jvmOptionsIgnored = installation.config(Paths.get("jvm.options.d", "jvm.options.ignored")); try { append(jvmOptionsIgnored, "-Xms512\\\\n-Xmx512m\\\\n"); startElasticsearch(); final String nodesResponse = makeRequest(Request.Get("http://localhost:9200/_nodes")); assertThat(nodesResponse, not(containsString("\\\\"heap_init_in_bytes\\\\":536870912"))); stopElasticsearch(); } finally { rm(jvmOptionsIgnored); } }	this changes makes me uncomfortable. is it not possible that we might happen to calculate the heap to be this size, causing the test to fail?
protected static void deleteAllDataStreams() { final DeleteDataStreamAction.Request deleteDataStreamRequest = new DeleteDataStreamAction.Request(new String[]{"*"}); deleteDataStreamRequest.indicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN_HIDDEN); AcknowledgedResponse response = client().execute( DeleteDataStreamAction.INSTANCE, deleteDataStreamRequest ).actionGet(); assertAcked(response); }	this is problematic, because if we want to remove indices on cleanup we have to remember to disable deprecation log indexing. otherwise it might create a deprecation and recreate datastream and its indices.
public final SegmentsStats segmentsStats(boolean includeSegmentFileSizes) { ensureOpen(); Set<String> segmentName = new HashSet<>(); SegmentsStats stats = new SegmentsStats(); try (Searcher searcher = acquireSearcher("segments_stats", SearcherScope.INTERNAL)) { List<SegmentReader> internalSegmentReaders = searcher.reader().getContext() .leaves().stream().map(l -> Lucene.segmentReader(l.reader())).collect(Collectors.toList()); internalSegmentReaders.forEach(r -> segmentName.add(r.getSegmentName())); fillSegmentStats(includeSegmentFileSizes, internalSegmentReaders, stats); } try (Searcher searcher = acquireSearcher("segments_stats", SearcherScope.EXTERNAL)) { Set<SegmentReader> externalSegmentReaders = searcher.reader().getContext() .leaves().stream().map(l -> Lucene.segmentReader(l.reader())) .filter(r -> segmentName.contains(r.getSegmentName()) == false) .collect(Collectors.toSet()); fillSegmentStats(includeSegmentFileSizes, externalSegmentReaders, stats); } writerSegmentStats(stats); return stats; }	i would find it easier to read without streams i think, by making fillsegmentstats take a single segmentreader and then doing regular for loops over the segments of each index?
protected Segment[] getSegmentInfo(SegmentInfos lastCommittedSegmentInfos, boolean verbose) { ensureOpen(); Map<String, Segment> segments = new HashMap<>(); // first, go over and compute the search ones... try (Searcher searcher = acquireSearcher("segments", SearcherScope.EXTERNAL)){ List<SegmentReader> externalSegmentReaders = searcher.reader().getContext() .leaves().stream().map(l -> Lucene.segmentReader(l.reader())).collect(Collectors.toList()); fillSegmentInfo(verbose, true, segments, externalSegmentReaders); } try (Searcher searcher = acquireSearcher("segments", SearcherScope.INTERNAL)){ List<SegmentReader> internalSegmentReaders = searcher.reader().getContext() .leaves().stream().map(l -> Lucene.segmentReader(l.reader())) .filter(r -> segments.containsKey(r.getSegmentName()) == false) .collect(Collectors.toList()); fillSegmentInfo(verbose, false, segments, internalSegmentReaders); } // now, correlate or add the committed ones... if (lastCommittedSegmentInfos != null) { SegmentInfos infos = lastCommittedSegmentInfos; for (SegmentCommitInfo info : infos) { Segment segment = segments.get(info.info.name); if (segment == null) { segment = new Segment(info.info.name); segment.search = false; segment.committed = true; segment.docCount = info.info.maxDoc(); segment.delDocCount = info.getDelCount(); segment.version = info.info.getVersion(); segment.compound = info.info.getUseCompoundFile(); try { segment.sizeInBytes = info.sizeInBytes(); } catch (IOException e) { logger.trace((Supplier<?>) () -> new ParameterizedMessage("failed to get size for [{}]", info.info.name), e); } segments.put(info.info.name, segment); } else { segment.committed = true; } } } Segment[] segmentsArr = segments.values().toArray(new Segment[segments.values().size()]); Arrays.sort(segmentsArr, (o1, o2) -> (int) (o1.getGeneration() - o2.getGeneration())); return segmentsArr; }	similarly here i would like it better with a regular for loop and by making fillsegmentinfo take a single segment at a time
public void testNoMasterActionsWriteMasterBlock() throws Exception { Settings settings = Settings.builder() .put(AutoCreateIndex.AUTO_CREATE_INDEX_SETTING.getKey(), false) .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), Integer.MAX_VALUE) .put(GatewayService.RECOVER_AFTER_MASTER_NODES_SETTING.getKey(), 3) .put(DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), "write") .put(ClusterBootstrapService.INITIAL_MASTER_NODE_COUNT_SETTING.getKey(), 3) .build(); internalCluster().startNodes(3, settings); prepareCreate("test1").setSettings( Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2)).get(); prepareCreate("test2").setSettings( Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 3).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)).get(); client().admin().cluster().prepareHealth("_all").setWaitForGreenStatus().get(); client().prepareIndex("test1", "type1", "1").setSource("field", "value1").get(); client().prepareIndex("test2", "type1", "1").setSource("field", "value1").get(); refresh(); ensureSearchable("test1", "test2"); ClusterStateResponse clusterState = client().admin().cluster().prepareState().get(); logger.info("Cluster state:\\\\n{}", clusterState.getState()); internalCluster().stopRandomDataNode(); internalCluster().restartRandomDataNode(new RestartCallback() { @Override public Settings onNodeStopped(String nodeName) throws Exception { final Client remainingClient = client(Arrays.stream( internalCluster().getNodeNames()).filter(n -> n.equals(nodeName) == false).findAny().get()); assertTrue(awaitBusy(() -> { ClusterState state = remainingClient.admin().cluster().prepareState().setLocal(true).get().getState(); return state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID); } )); GetResponse getResponse = remainingClient.prepareGet("test1", "type1", "1").get(); assertExists(getResponse); SearchResponse countResponse = remainingClient.prepareSearch("test1").setAllowPartialSearchResults(true).setSize(0).get(); assertHitCount(countResponse, 1L); logger.info("--> here 3"); SearchResponse searchResponse = remainingClient.prepareSearch("test1").setAllowPartialSearchResults(true).get(); assertHitCount(searchResponse, 1L); countResponse = remainingClient.prepareSearch("test2").setAllowPartialSearchResults(true).setSize(0).get(); assertThat(countResponse.getTotalShards(), equalTo(3)); assertThat(countResponse.getSuccessfulShards(), equalTo(1)); TimeValue timeout = TimeValue.timeValueMillis(200); long now = System.currentTimeMillis(); try { remainingClient.prepareUpdate("test1", "type1", "1") .setDoc(Requests.INDEX_CONTENT_TYPE, "field", "value2").setTimeout(timeout).get(); fail("Expected ClusterBlockException"); } catch (ClusterBlockException e) { assertThat(System.currentTimeMillis() - now, greaterThan(timeout.millis() - 50)); assertThat(e.status(), equalTo(RestStatus.SERVICE_UNAVAILABLE)); } catch (Exception e) { logger.info("unexpected", e); throw e; } try { remainingClient.prepareIndex("test1", "type1", "1") .setSource(XContentFactory.jsonBuilder().startObject().endObject()).setTimeout(timeout).get(); fail("Expected ClusterBlockException"); } catch (ClusterBlockException e) { assertThat(e.status(), equalTo(RestStatus.SERVICE_UNAVAILABLE)); } logger.info("finished assertions, restarting node [{}]", nodeName); return Settings.EMPTY; } }); internalCluster().startNode(settings); client().admin().cluster().prepareHealth().setWaitForGreenStatus().setWaitForNodes("3").get(); }	same here. why is this needed?
public void testNoMasterActions() throws Exception { Settings settings = Settings.builder() .put(AutoCreateIndex.AUTO_CREATE_INDEX_SETTING.getKey(), true) .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), Integer.MAX_VALUE) .put(GatewayService.RECOVER_AFTER_MASTER_NODES_SETTING.getKey(), 3) .put(DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), "all") .put(ClusterBootstrapService.INITIAL_MASTER_NODE_COUNT_SETTING.getKey(), 3) .build(); final TimeValue timeout = TimeValue.timeValueMillis(10); internalCluster().startNodes(3, settings); createIndex("test"); client().admin().cluster().prepareHealth("test").setWaitForGreenStatus().execute().actionGet(); internalCluster().stopRandomDataNode(); internalCluster().restartRandomDataNode(new RestartCallback() { @Override public Settings onNodeStopped(String nodeName) throws Exception { final Client remainingClient = client(Arrays.stream( internalCluster().getNodeNames()).filter(n -> n.equals(nodeName) == false).findAny().get()); assertBusy(() -> { ClusterState state = remainingClient.admin().cluster().prepareState().setLocal(true).execute().actionGet().getState(); assertTrue(state.blocks().hasGlobalBlock(DiscoverySettings.NO_MASTER_BLOCK_ID)); }); assertThrows(remainingClient.prepareGet("test", "type1", "1"), ClusterBlockException.class, RestStatus.SERVICE_UNAVAILABLE ); assertThrows(remainingClient.prepareGet("no_index", "type1", "1"), ClusterBlockException.class, RestStatus.SERVICE_UNAVAILABLE ); assertThrows(remainingClient.prepareMultiGet().add("test", "type1", "1"), ClusterBlockException.class, RestStatus.SERVICE_UNAVAILABLE ); assertThrows(remainingClient.prepareMultiGet().add("no_index", "type1", "1"), ClusterBlockException.class, RestStatus.SERVICE_UNAVAILABLE ); assertThrows(remainingClient.admin().indices().prepareAnalyze("test", "this is a test"), ClusterBlockException.class, RestStatus.SERVICE_UNAVAILABLE ); assertThrows(remainingClient.admin().indices().prepareAnalyze("no_index", "this is a test"), ClusterBlockException.class, RestStatus.SERVICE_UNAVAILABLE ); assertThrows(remainingClient.prepareSearch("test").setSize(0), ClusterBlockException.class, RestStatus.SERVICE_UNAVAILABLE ); assertThrows(remainingClient.prepareSearch("no_index").setSize(0), ClusterBlockException.class, RestStatus.SERVICE_UNAVAILABLE ); checkUpdateAction(false, timeout, remainingClient.prepareUpdate("test", "type1", "1") .setScript(new Script( ScriptType.INLINE, Script.DEFAULT_SCRIPT_LANG, "test script", Collections.emptyMap())).setTimeout(timeout)); checkUpdateAction(true, timeout, remainingClient.prepareUpdate("no_index", "type1", "1") .setScript(new Script( ScriptType.INLINE, Script.DEFAULT_SCRIPT_LANG, "test script", Collections.emptyMap())).setTimeout(timeout)); checkWriteAction(remainingClient.prepareIndex("test", "type1", "1") .setSource(XContentFactory.jsonBuilder().startObject().endObject()).setTimeout(timeout)); checkWriteAction(remainingClient.prepareIndex("no_index", "type1", "1") .setSource(XContentFactory.jsonBuilder().startObject().endObject()).setTimeout(timeout)); BulkRequestBuilder bulkRequestBuilder = remainingClient.prepareBulk(); bulkRequestBuilder.add(remainingClient.prepareIndex("test", "type1", "1") .setSource(XContentFactory.jsonBuilder().startObject().endObject())); bulkRequestBuilder.add(remainingClient.prepareIndex("test", "type1", "2") .setSource(XContentFactory.jsonBuilder().startObject().endObject())); bulkRequestBuilder.setTimeout(timeout); checkWriteAction(bulkRequestBuilder); bulkRequestBuilder = remainingClient.prepareBulk(); bulkRequestBuilder.add(remainingClient.prepareIndex("no_index", "type1", "1") .setSource(XContentFactory.jsonBuilder().startObject().endObject())); bulkRequestBuilder.add(remainingClient.prepareIndex("no_index", "type1", "2") .setSource(XContentFactory.jsonBuilder().startObject().endObject())); bulkRequestBuilder.setTimeout(timeout); checkWriteAction(bulkRequestBuilder); return Settings.EMPTY; } }); internalCluster().startNode(settings); client().admin().cluster().prepareHealth().setWaitForGreenStatus().setWaitForNodes("3").execute().actionGet(); }	why add this setting? this should not be needed?
public void testStopOutlierDetectionWithEnoughDocumentsToScroll() { String sourceIndex = "test-outlier-detection-with-enough-docs-to-scroll"; client().admin().indices().prepareCreate(sourceIndex) .addMapping("_doc", "numeric_1", "type=double", "numeric_2", "type=float", "categorical_1", "type=keyword") .get(); BulkRequestBuilder bulkRequestBuilder = client().prepareBulk(); bulkRequestBuilder.setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); int docCount = randomIntBetween(1024, 2048); for (int i = 0; i < docCount; i++) { IndexRequest indexRequest = new IndexRequest(sourceIndex); indexRequest.source("numeric_1", randomDouble(), "numeric_2", randomFloat(), "categorical_1", randomAlphaOfLength(10)); bulkRequestBuilder.add(indexRequest); } BulkResponse bulkResponse = bulkRequestBuilder.get(); if (bulkResponse.hasFailures()) { fail("Failed to index data: " + bulkResponse.buildFailureMessage()); } String id = "test_outlier_detection_with_enough_docs_to_scroll"; DataFrameAnalyticsConfig config = buildOutlierDetectionAnalytics(id, sourceIndex, "custom_ml"); registerAnalytics(config); putAnalytics(config); assertState(id, DataFrameAnalyticsState.STOPPED); startAnalytics(id); assertState(id, DataFrameAnalyticsState.STARTED); assertThat(stopAnalytics(id).isStopped(), is(true)); assertState(id, DataFrameAnalyticsState.STOPPED); SearchResponse searchResponse = client().prepareSearch(config.getDest().getIndex()).setTrackTotalHits(true).get(); if (searchResponse.getHits().getTotalHits().value == docCount) { searchResponse = client().prepareSearch(config.getDest().getIndex()) .setTrackTotalHits(true) .setQuery(QueryBuilders.existsQuery("custom_ml.outlier_score")).get(); logger.debug("We stopped during analysis: [{}] < [{}]", searchResponse.getHits().getTotalHits().value, docCount); assertThat(searchResponse.getHits().getTotalHits().value, lessThan((long) docCount)); } else { logger.debug("We stopped during reindexing: [{}] < [{}]", searchResponse.getHits().getTotalHits().value, docCount); } }	there is the possibility that the config.getdest().getindex() has not been created yet.
public void testRestartIndexCreationAfterFullClusterRestart() throws Exception { client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put("cluster.routing.allocation.enable", "none")).get(); client().admin().indices().prepareCreate("test").setSettings(indexSettings()).get(); internalCluster().fullRestart(); ensureGreen("test"); }	i think we can test this without creating an index - maybe a simple unittest in analysisservicetests would do? to trigger the problem i think all you need to do is to build an analysisservice like this: java version version = versionutils.randomversion(random()); settings settings = settings .builder() .put(indexmetadata.setting_version_created, version) .put(environment.path_home_setting.getkey(), createtempdir().tostring()) .putarray("analysis.analyzer.test_analyzer.filter", new string[] {"lowercase", "stop", "shingle"}); .putarray("analysis.analyzer.test_analyzer.char_filter", new string[] {"html_strip"}) .build(); indexsettings idxsettings = indexsettingsmodule.newindexsettings("index", settings); analysisservice analysisservice = new analysisregistry(null, new environment(settings)).build(idxsettings);
public void testNoTypeOrTokenizerErrorMessage() throws Exception { Settings.Builder stBldr = Settings.builder(); stBldr.putArray("analysis.analyzer.test_analyzer.filter", new String[] {"lowercase", "stop", "shingle"}); stBldr.putArray("analysis.analyzer.test_analyzer.char_filter", new String[] {"html_strip"}).build(); CreateIndexRequestBuilder b = prepareCreate("test", 1, stBldr); try { b.get(); fail("IllegalArgumentException should have been thrown"); } catch (Exception e) { assertThat(e, instanceOf(IllegalArgumentException.class)); assertThat(e.getMessage(), equalTo("analyzer [test_analyzer] must specify either an analyzer type, or a tokenizer")); } }	can you use java assertequals(expectthrows(illegalargumentexception.class, () -> b.get()).getmessage(), "analyzer [test_analyzer] must specify either an analyzer type, or a tokenizer");
@Override public void merge(Mapper mergeWith, MergeContext mergeContext) throws MergeMappingException { if (!this.getClass().equals(mergeWith.getClass())) { String mergedType = mergeWith.getClass().getSimpleName(); if (mergeWith instanceof AbstractFieldMapper) { mergedType = ((AbstractFieldMapper) mergeWith).contentType(); } mergeContext.addConflict("mapper [" + names.fullName() + "] of different type, current_type [" + contentType() + "], merged_type [" + mergedType + "]"); // different types, return return; } AbstractFieldMapper fieldMergeWith = (AbstractFieldMapper) mergeWith; if (this.fieldType().indexed() != fieldMergeWith.fieldType().indexed() || this.fieldType().tokenized() != fieldMergeWith.fieldType().tokenized()) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different index values"); } if (this.fieldType().stored() != fieldMergeWith.fieldType().stored()) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different store values"); } if (!this.hasDocValues() && fieldMergeWith.hasDocValues()) { // don't add conflict if this mapper has doc values while the mapper to merge doesn't since doc values are implicitely set // when the doc_values field data format is configured mergeContext.addConflict("mapper [" + names.fullName() + "] has different " + TypeParsers.DOC_VALUES + " values"); } if (this.fieldType().omitNorms() && !fieldMergeWith.fieldType.omitNorms()) { mergeContext.addConflict("mapper [" + names.fullName() + "] cannot enable norms (`norms.enabled`)"); } if (this.fieldType().tokenized() != fieldMergeWith.fieldType().tokenized()) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different tokenize values"); } if (this.fieldType().storeTermVectors() != fieldMergeWith.fieldType().storeTermVectors()) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different store_term_vector values"); } if (this.fieldType().storeTermVectorOffsets() != fieldMergeWith.fieldType().storeTermVectorOffsets()) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different store_term_vector_offsets values"); } if (this.fieldType().storeTermVectorPositions() != fieldMergeWith.fieldType().storeTermVectorPositions()) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different store_term_vector_positions values"); } if (this.fieldType().storeTermVectorPayloads() != fieldMergeWith.fieldType().storeTermVectorPayloads()) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different store_term_vector_payloads values"); } if (this.indexAnalyzer == null) { if (fieldMergeWith.indexAnalyzer != null) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different index_analyzer"); } } else if (fieldMergeWith.indexAnalyzer == null) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different index_analyzer"); } else if (!this.indexAnalyzer.name().equals(fieldMergeWith.indexAnalyzer.name())) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different index_analyzer"); } if (this.similarity == null) { if (fieldMergeWith.similarity() != null) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different similarity"); } } else if (fieldMergeWith.similarity() == null) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different similarity"); } else if (!this.similarity().equals(fieldMergeWith.similarity())) { mergeContext.addConflict("mapper [" + names.fullName() + "] has different similarity"); } multiFields.merge(mergeWith, mergeContext); if (!mergeContext.mergeFlags().simulate()) { // apply changeable values this.fieldType = fieldMergeWith.fieldType; this.boost = fieldMergeWith.boost; this.normsLoading = fieldMergeWith.normsLoading; this.copyTo = fieldMergeWith.copyTo; if (fieldMergeWith.postingsFormat != null) { this.postingsFormat = fieldMergeWith.postingsFormat; } if (fieldMergeWith.docValuesFormat != null) { this.docValuesFormat = fieldMergeWith.docValuesFormat; } if (fieldMergeWith.searchAnalyzer != null) { this.searchAnalyzer = fieldMergeWith.searchAnalyzer; } if (fieldMergeWith.customFieldDataSettings != null) { if (!Objects.equal(fieldMergeWith.customFieldDataSettings, this.customFieldDataSettings)) { this.customFieldDataSettings = fieldMergeWith.customFieldDataSettings; this.fieldDataType = new FieldDataType(defaultFieldDataType().getType(), ImmutableSettings.builder().put(defaultFieldDataType().getSettings()).put(this.customFieldDataSettings) ); } } } }	i'm worried that by copying over the field type completely we get more stuff then what we tested for collisions. for example, indexoptions is not tested. maybe there are other stuff.
public void testWithDeletes() throws Exception { String indexName = "xyz"; assertAcked( prepareCreate(indexName).setMapping( addFieldMappings(buildParentJoinFieldMappingFromSimplifiedDef("join_field", true, "parent", "child"), "name", "keyword") ) ); List<IndexRequestBuilder> requests = new ArrayList<>(); requests.add(createIndexRequest(indexName, "parent", "1", null)); requests.add(createIndexRequest(indexName, "child", "2", "1", "count", 1)); requests.add(createIndexRequest(indexName, "child", "3", "1", "count", 1)); requests.add(createIndexRequest(indexName, "child", "4", "1", "count", 1)); requests.add(createIndexRequest(indexName, "child", "5", "1", "count", 1)); indexRandom(true, requests); for (int i = 0; i < 10; i++) { SearchResponse searchResponse = client().prepareSearch(indexName) .addAggregation(children("children", "child").subAggregation(sum("counts").field("count"))) .get(); assertNoFailures(searchResponse); Children children = searchResponse.getAggregations().get("children"); assertThat(children.getDocCount(), equalTo(4L)); Sum count = children.getAggregations().get("counts"); assertThat(count.value(), equalTo(4.)); String idToUpdate = Integer.toString(2 + randomInt(3)); /* * The whole point of this test is to test these things with deleted * docs in the index so we turn off detect_noop to make sure that * the updates cause that. */ UpdateResponse updateResponse; updateResponse = client().prepareUpdate(indexName, idToUpdate) .setRouting("1") .setDoc(Requests.INDEX_CONTENT_TYPE, "count", 1) .setDetectNoop(false) .get(); assertThat(updateResponse.getVersion(), greaterThan(1L)); refresh(); } }	the getvalue method was on the sum interface and duplicated the value method on the internal classes.
@Override public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (text == null || text.length == 0) { validationException = addValidationError("text is missing", validationException); } if (tokenFilters == null) { validationException = addValidationError("token filters must not be null", validationException); } if (charFilters == null) { validationException = addValidationError("char filters must not be null", validationException); } if (attributes == null) { validationException = addValidationError("attributes must not be null", validationException); } return validationException; }	given that attributes has a sensible default, can we not check on the setter method that the parameter is non null rather than checking here, so that way we can fail faster?
public static void buildFromContent(BytesReference content, AnalyzeRequest analyzeRequest) { try (XContentParser parser = XContentHelper.createParser(content)) { if (parser.nextToken() != XContentParser.Token.START_OBJECT) { throw new IllegalArgumentException("Malforrmed content, must start with an object"); } else { XContentParser.Token token; String currentFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if ("text".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) { analyzeRequest.text(parser.text()); } else if ("text".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY) { List<String> texts = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token.isValue() == false) { throw new IllegalArgumentException(currentFieldName + " array element should only contain text"); } texts.add(parser.text()); } analyzeRequest.text(texts.toArray(new String[texts.size()])); } else if ("analyzer".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) { analyzeRequest.analyzer(parser.text()); } else if ("field".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) { analyzeRequest.field(parser.text()); } else if ("tokenizer".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) { analyzeRequest.tokenizer(parser.text()); } else if (("token_filters".equals(currentFieldName) || "filters".equals(currentFieldName)) && token == XContentParser.Token.START_ARRAY) { List<String> filters = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token.isValue() == false) { throw new IllegalArgumentException(currentFieldName + " array element should only contain token filter's name"); } filters.add(parser.text()); } analyzeRequest.tokenFilters(filters.toArray(new String[filters.size()])); } else if ("char_filters".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY) { List<String> charFilters = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token.isValue() == false) { throw new IllegalArgumentException(currentFieldName + " array element should only contain char filter's name"); } charFilters.add(parser.text()); } analyzeRequest.charFilters(charFilters.toArray(new String[charFilters.size()])); } else if ("detail".equals(currentFieldName) && token == XContentParser.Token.VALUE_BOOLEAN) { analyzeRequest.detail(parser.booleanValue()); } else if ("attributes".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY){ List<String> attributes = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token.isValue() == false) { throw new IllegalArgumentException(currentFieldName + " array element should only contain attribute name"); } attributes.add(parser.text()); } analyzeRequest.attributes(attributes.toArray(new String[attributes.size()])); } else { throw new IllegalArgumentException("Unknown parameter [" + currentFieldName + "] in request body or parameter is of the wrong type[" + token + "] "); } } } } catch (IOException e) { throw new IllegalArgumentException("Failed to parse request body", e); } }	can we use the parsefieldmatcher and parsefield objects here since we should be moving all parsers over to that anyway?
public void testRetryPolicy() throws Exception { RestHighLevelClient client = highLevelClient(); // setup policy to immediately fail on index { Map<String, Phase> phases = new HashMap<>(); Map<String, LifecycleAction> warmActions = new HashMap<>(); warmActions.put(ShrinkAction.NAME, new ShrinkAction(1)); phases.put("warm", new Phase("warm", TimeValue.ZERO, warmActions)); Map<String, LifecycleAction> deleteActions = Collections.singletonMap(DeleteAction.NAME, new DeleteAction()); phases.put("delete", new Phase("delete", new TimeValue(90, TimeUnit.DAYS), deleteActions)); LifecyclePolicy policy = new LifecyclePolicy("my_policy", phases); PutLifecyclePolicyRequest putRequest = new PutLifecyclePolicyRequest(policy); client.indexLifecycle().putLifecyclePolicy(putRequest, RequestOptions.DEFAULT); CreateIndexRequest createIndexRequest = new CreateIndexRequest("my_index", Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put("index.lifecycle.name", "my_policy") .build()); client.indices().create(createIndexRequest, RequestOptions.DEFAULT); assertBusy(() -> assertNotNull(client.indexLifecycle() .explainLifecycle(new ExplainLifecycleRequest().indices("my_index"), RequestOptions.DEFAULT) .getIndexResponses().get("my_index").getFailedStep())); } // tag::ilm-retry-lifecycle-policy-request RetryLifecyclePolicyRequest request = new RetryLifecyclePolicyRequest("my_index"); // <1> // end::ilm-retry-lifecycle-policy-request // tag::ilm-retry-lifecycle-policy-execute AcknowledgedResponse response = client.indexLifecycle() .retryLifecyclePolicy(request, RequestOptions.DEFAULT); // end::ilm-retry-lifecycle-policy-execute // tag::ilm-retry-lifecycle-policy-response boolean acknowledged = response.isAcknowledged(); // <1> // end::ilm-retry-lifecycle-policy-response assertTrue(acknowledged); // tag::ilm-retry-lifecycle-policy-execute-listener ActionListener<AcknowledgedResponse> listener = new ActionListener<AcknowledgedResponse>() { @Override public void onResponse(AcknowledgedResponse response) { boolean acknowledged = response.isAcknowledged(); // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::ilm-retry-lifecycle-policy-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::ilm-retry-lifecycle-policy-execute-async client.indexLifecycle().retryLifecyclePolicyAsync(request, RequestOptions.DEFAULT, listener); // <1> // end::ilm-retry-lifecycle-policy-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); }	what's the reasoning behind having a delete phase here?
public void testSLMRetentionAfterRestore() throws Exception { final String indexName = "test"; final String policyName = "test-policy"; int docCount = 20; for (int i = 0; i < docCount; i++) { index(indexName, i + "", Collections.singletonMap("foo", "bar")); } // Create a snapshot repo initializeRepo(REPO); logger.info("--> creating policy {}", policyName); createSnapshotPolicy(policyName, "snap", NEVER_EXECUTE_CRON_SCHEDULE, REPO, indexName, true, false, new SnapshotRetentionConfiguration(TimeValue.ZERO, null, null)); logger.info("--> executing snapshot lifecycle"); final String snapshotName = executePolicy(policyName); // Check that the executed snapshot shows up in the SLM output assertBusy(() -> { GetSnapshotLifecycleAction.Response getResp = client().execute(GetSnapshotLifecycleAction.INSTANCE, new GetSnapshotLifecycleAction.Request(policyName)).get(); logger.info("--> checking for in progress snapshot..."); assertThat(getResp.getPolicies().size(), greaterThan(0)); SnapshotLifecyclePolicyItem item = getResp.getPolicies().get(0); SnapshotInvocationRecord lastSuccess = item.getLastSuccess(); assertNotNull(lastSuccess); assertThat(lastSuccess.getSnapshotName(), equalTo(snapshotName)); }); logger.info("--> restoring index"); RestoreSnapshotRequest restoreReq = new RestoreSnapshotRequest(REPO, snapshotName); restoreReq.indices(indexName); restoreReq.renamePattern("(.+)"); restoreReq.renameReplacement("restored_$1"); restoreReq.waitForCompletion(true); RestoreSnapshotResponse resp = client().execute(RestoreSnapshotAction.INSTANCE, restoreReq).get(); assertThat(resp.status(), equalTo(RestStatus.OK)); logger.info("--> executing SLM retention"); assertAcked(client().execute(ExecuteSnapshotRetentionAction.INSTANCE, new ExecuteSnapshotRetentionAction.Request()).get()); logger.info("--> waiting for {} snapshot to be deleted", snapshotName); assertBusy(() -> { try { try { GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster() .prepareGetSnapshots(REPO).setSnapshots(snapshotName).get(); assertThat(snapshotsStatusResponse.getSnapshots(REPO), empty()); } catch (SnapshotMissingException e) { // This is what we want to happen } logger.info("--> snapshot [{}] has been deleted", snapshotName); } catch (RepositoryException re) { // Concurrent status calls and write operations may lead to failures in determining the current repository generation // TODO: Remove this hack once tracking the current repository generation has been made consistent throw new AssertionError(re); } }); // Cancel/delete the snapshot try { client().admin().cluster().prepareDeleteSnapshot(REPO, snapshotName).get(); } catch (SnapshotMissingException e) { // ignore } }	don't need to block this pr on it, but we should check if this hack is still necessary.
private void setTranslogFlushThresholdSize(ByteSizeValue byteSizeValue) { this.flushThresholdSize = byteSizeValue; }	i think this could be logged several times without the user changing anything. for instance transportnodeslistshardstoremetadata sometimes creates a new indexsettings object, which would retrigger logging this (i think). i imagine that it is quite common to have custom translog settings, if not this might not be as important. i wonder if it was better to just add a new check in deprecationchecks.index_settings_checks?
public void testTranslogStats() { final String indexName = "test"; IndexService indexService = createIndex(indexName, Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .build()); final int nbDocs = randomIntBetween(0, 50); int uncommittedOps = 0; for (long i = 0; i < nbDocs; i++) { final IndexResponse indexResponse = client().prepareIndex(indexName, "_doc", Long.toString(i)).setSource("field", i).get(); assertThat(indexResponse.status(), is(RestStatus.CREATED)); if (rarely()) { client().admin().indices().prepareFlush(indexName).get(); uncommittedOps = 0; } else { uncommittedOps += 1; } } IndicesStatsResponse stats = client().admin().indices().prepareStats(indexName).clear().setTranslog(true).get(); assertThat(stats.getIndex(indexName), notNullValue()); assertThat(stats.getIndex(indexName).getPrimaries().getTranslog().estimatedNumberOfOperations(), equalTo( indexService.getIndexSettings().isSoftDeleteEnabled() ? uncommittedOps : nbDocs)); assertThat(stats.getIndex(indexName).getPrimaries().getTranslog().getUncommittedOperations(), equalTo(uncommittedOps)); assertAcked(client().execute(FreezeIndexAction.INSTANCE, new FreezeRequest(indexName)).actionGet()); assertIndexFrozen(indexName); IndicesOptions indicesOptions = IndicesOptions.STRICT_EXPAND_OPEN; stats = client().admin().indices().prepareStats(indexName).setIndicesOptions(indicesOptions).clear().setTranslog(true).get(); assertThat(stats.getIndex(indexName), notNullValue()); assertThat(stats.getIndex(indexName).getPrimaries().getTranslog().estimatedNumberOfOperations(), equalTo(indexService.getIndexSettings().isSoftDeleteEnabled() ? 0 : nbDocs)); assertThat(stats.getIndex(indexName).getPrimaries().getTranslog().getUncommittedOperations(), equalTo(0)); }	should we maybe randomly set soft deletes true/false?
@Test public void testRetryWithAutogeneratedIdWorksAndNoDuplicateDocs() throws IOException { ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, false); boolean canHaveDuplicates = false; boolean autoGeneratedId = true; Engine.Create index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime(), canHaveDuplicates, autoGeneratedId); engine.create(index); assertThat(index.version(), equalTo(1l)); index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, REPLICA, System.nanoTime(), canHaveDuplicates, autoGeneratedId); replicaEngine.create(index); assertThat(index.version(), equalTo(1l)); canHaveDuplicates = true; index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime(), canHaveDuplicates, autoGeneratedId); engine.create(index); assertThat(index.version(), equalTo(1l)); engine.refresh("test", true); Engine.Searcher searcher = engine.acquireSearcher("test"); TopDocs topDocs = searcher.searcher().search(new MatchAllDocsQuery(), 10); assertThat(topDocs.totalHits, equalTo(1)); index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, REPLICA, System.nanoTime(), canHaveDuplicates, autoGeneratedId); replicaEngine.create(index); assertThat(index.version(), equalTo(1l)); replicaEngine.refresh("test", true); Engine.Searcher replicaSearcher = replicaEngine.acquireSearcher("test"); topDocs = replicaSearcher.searcher().search(new MatchAllDocsQuery(), 10); assertThat(topDocs.totalHits, equalTo(1)); searcher.close(); replicaSearcher.close(); }	this shouldn't have any effect any more, right? do we want to randomize it with a comment it's being removed?
@Test public void testRetryWithAutogeneratedIdWorksAndNoDuplicateDocs() throws IOException { ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, false); boolean canHaveDuplicates = false; boolean autoGeneratedId = true; Engine.Create index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime(), canHaveDuplicates, autoGeneratedId); engine.create(index); assertThat(index.version(), equalTo(1l)); index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, REPLICA, System.nanoTime(), canHaveDuplicates, autoGeneratedId); replicaEngine.create(index); assertThat(index.version(), equalTo(1l)); canHaveDuplicates = true; index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime(), canHaveDuplicates, autoGeneratedId); engine.create(index); assertThat(index.version(), equalTo(1l)); engine.refresh("test", true); Engine.Searcher searcher = engine.acquireSearcher("test"); TopDocs topDocs = searcher.searcher().search(new MatchAllDocsQuery(), 10); assertThat(topDocs.totalHits, equalTo(1)); index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, REPLICA, System.nanoTime(), canHaveDuplicates, autoGeneratedId); replicaEngine.create(index); assertThat(index.version(), equalTo(1l)); replicaEngine.refresh("test", true); Engine.Searcher replicaSearcher = replicaEngine.acquireSearcher("test"); topDocs = replicaSearcher.searcher().search(new MatchAllDocsQuery(), 10); assertThat(topDocs.totalHits, equalTo(1)); searcher.close(); replicaSearcher.close(); }	same comment here. randomize?
@Test public void testRetryWithAutogeneratedIdWorksAndNoDuplicateDocs() throws IOException { ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, false); boolean canHaveDuplicates = false; boolean autoGeneratedId = true; Engine.Create index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime(), canHaveDuplicates, autoGeneratedId); engine.create(index); assertThat(index.version(), equalTo(1l)); index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, REPLICA, System.nanoTime(), canHaveDuplicates, autoGeneratedId); replicaEngine.create(index); assertThat(index.version(), equalTo(1l)); canHaveDuplicates = true; index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime(), canHaveDuplicates, autoGeneratedId); engine.create(index); assertThat(index.version(), equalTo(1l)); engine.refresh("test", true); Engine.Searcher searcher = engine.acquireSearcher("test"); TopDocs topDocs = searcher.searcher().search(new MatchAllDocsQuery(), 10); assertThat(topDocs.totalHits, equalTo(1)); index = new Engine.Create(null, analyzer, newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, REPLICA, System.nanoTime(), canHaveDuplicates, autoGeneratedId); replicaEngine.create(index); assertThat(index.version(), equalTo(1l)); replicaEngine.refresh("test", true); Engine.Searcher replicaSearcher = replicaEngine.acquireSearcher("test"); topDocs = replicaSearcher.searcher().search(new MatchAllDocsQuery(), 10); assertThat(topDocs.totalHits, equalTo(1)); searcher.close(); replicaSearcher.close(); }	to simulate replication we need to update the version type . see https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/index/transportindexaction.java#l216
public static void writeMappingMetadata(StreamOutput out, ImmutableOpenMap<String, MappingMetadata> mappings) throws IOException { out.writeMap(mappings, StreamOutput::writeString, out.getVersion().before(Version.V_8_0_0) ? (o, v) -> { o.writeVInt(v == MappingMetadata.EMPTY_MAPPINGS ? 0 : 1); if (v != MappingMetadata.EMPTY_MAPPINGS) { o.writeString(MapperService.SINGLE_MAPPING_NAME); v.writeTo(o); } } : (o, v) -> { o.writeBoolean(v != MappingMetadata.EMPTY_MAPPINGS); if (v != MappingMetadata.EMPTY_MAPPINGS) { v.writeTo(o); } } ); }	maybe this should exist in mappingmetadata, rather than a random response class?
* @param rootBlobs Blobs at the repository root * @return RepositoryData */ private RepositoryData safeRepositoryData(long repositoryStateId, Map<String, BlobMetaData> rootBlobs) { final long generation = latestGeneration(rootBlobs.keySet()); final long genToLoad = latestKnownRepoGen.updateAndGet(known -> Math.max(known, generation)); if (genToLoad != generation) { // It's always a possibility to not see the latest index-N in the listing here on an eventually consistent blob store, just // debug log it. Any blobs leaked as a result of an inconsistent listing here will be cleaned up in a subsequent cleanup or // snapshot delete run anyway. logger.debug("Determined repository's generation from its contents to [" + generation + "] but " + "current generation is at least [" + genToLoad + "]"); } if (genToLoad != repositoryStateId) { throw new IllegalStateException("Determined latest repository generation to be [" + genToLoad + "] but this operation " + "assumes generation [" + repositoryStateId + "]"); } return getRepositoryData(genToLoad); } /** * After updating the {@link RepositoryData}	should we write gentoload > generation here to make it clearer that this is the case where we want to log this message?
* @param rootBlobs Blobs at the repository root * @return RepositoryData */ private RepositoryData safeRepositoryData(long repositoryStateId, Map<String, BlobMetaData> rootBlobs) { final long generation = latestGeneration(rootBlobs.keySet()); final long genToLoad = latestKnownRepoGen.updateAndGet(known -> Math.max(known, generation)); if (genToLoad != generation) { // It's always a possibility to not see the latest index-N in the listing here on an eventually consistent blob store, just // debug log it. Any blobs leaked as a result of an inconsistent listing here will be cleaned up in a subsequent cleanup or // snapshot delete run anyway. logger.debug("Determined repository's generation from its contents to [" + generation + "] but " + "current generation is at least [" + genToLoad + "]"); } if (genToLoad != repositoryStateId) { throw new IllegalStateException("Determined latest repository generation to be [" + genToLoad + "] but this operation " + "assumes generation [" + repositoryStateId + "]"); } return getRepositoryData(genToLoad); } /** * After updating the {@link RepositoryData}	this may be a little controversial: by tracking the latest gen in the field, we can now identify out of sync listings that we would have previous missed and that would just have failed in a subsequent step where the repo gen is compared. with this change, if we miss to list the latest index-n, we can still complete a delte or cleanup just fine (assuming the value in latestknownrepogen is correct). i think it's better user experience to not do a perfect cleanup in this edge case but proceed with the delete/cleanup as if nothing happened. on an eventually consistent repo, the fact that we list out the correct index-n does not guarantee that we didn't miss any other root blobs in the listing anyway. also, apart from maybe missing some stale blobs, the delete will work out perfectly fine otherwise.
* @param rootBlobs Blobs at the repository root * @return RepositoryData */ private RepositoryData safeRepositoryData(long repositoryStateId, Map<String, BlobMetaData> rootBlobs) { final long generation = latestGeneration(rootBlobs.keySet()); final long genToLoad = latestKnownRepoGen.updateAndGet(known -> Math.max(known, generation)); if (genToLoad != generation) { // It's always a possibility to not see the latest index-N in the listing here on an eventually consistent blob store, just // debug log it. Any blobs leaked as a result of an inconsistent listing here will be cleaned up in a subsequent cleanup or // snapshot delete run anyway. logger.debug("Determined repository's generation from its contents to [" + generation + "] but " + "current generation is at least [" + genToLoad + "]"); } if (genToLoad != repositoryStateId) { throw new IllegalStateException("Determined latest repository generation to be [" + genToLoad + "] but this operation " + "assumes generation [" + repositoryStateId + "]"); } return getRepositoryData(genToLoad); } /** * After updating the {@link RepositoryData}	should we latestknownrepogen.updateandget(known -> math.max(known, repositorystateid)) on entry to this method? whenever we learn about repositorystateid we can assume that such a file must have existed? think e.g. about a master failover where the new master now executes finalizesnapshotdeletionfrompreviousmaster. if it were to list out an older generation, this makes us now get the latest correct one.
* @param rootBlobs Blobs at the repository root * @return RepositoryData */ private RepositoryData safeRepositoryData(long repositoryStateId, Map<String, BlobMetaData> rootBlobs) { final long generation = latestGeneration(rootBlobs.keySet()); final long genToLoad = latestKnownRepoGen.updateAndGet(known -> Math.max(known, generation)); if (genToLoad != generation) { // It's always a possibility to not see the latest index-N in the listing here on an eventually consistent blob store, just // debug log it. Any blobs leaked as a result of an inconsistent listing here will be cleaned up in a subsequent cleanup or // snapshot delete run anyway. logger.debug("Determined repository's generation from its contents to [" + generation + "] but " + "current generation is at least [" + genToLoad + "]"); } if (genToLoad != repositoryStateId) { throw new IllegalStateException("Determined latest repository generation to be [" + genToLoad + "] but this operation " + "assumes generation [" + repositoryStateId + "]"); } return getRepositoryData(genToLoad); } /** * After updating the {@link RepositoryData}	this message is confusing to a user. perhaps this should be expressed in terms of a concurrent modification?
private RepositoryData getRepositoryData(long indexGen) { if (indexGen == RepositoryData.EMPTY_REPO_GEN) { return RepositoryData.EMPTY; } try { final String snapshotsIndexBlobName = INDEX_FILE_PREFIX + Long.toString(indexGen); // EMPTY is safe here because RepositoryData#fromXContent calls namedObject try (InputStream blob = blobContainer().readBlob(snapshotsIndexBlobName); XContentParser parser = XContentType.JSON.xContent().createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, blob)) { return RepositoryData.snapshotsFromXContent(parser, indexGen); } } catch (IOException ioe) { // If we fail to load the generation we tracked in latestKnownRepoGen we reset it. // This is done as a fail-safe in case a user manually deletes the contents of the repository in which case subsequent // operations must start from the EMPTY_REPO_GEN again if (RepositoryData.EMPTY_REPO_GEN == latestKnownRepoGen.updateAndGet(known -> known == indexGen ? RepositoryData.EMPTY_REPO_GEN : known)) { logger.warn("Resetting repository generation tracker because we failed to read generation [" + indexGen + "]"); } throw new RepositoryException(metadata.name(), "could not read repository data from index blob", ioe); } }	just use if (latestknownrepogen.compareandset(indexgen, repositorydata.empty_repo_gen)) here?
private RepositoryData getRepositoryData(long indexGen) { if (indexGen == RepositoryData.EMPTY_REPO_GEN) { return RepositoryData.EMPTY; } try { final String snapshotsIndexBlobName = INDEX_FILE_PREFIX + Long.toString(indexGen); // EMPTY is safe here because RepositoryData#fromXContent calls namedObject try (InputStream blob = blobContainer().readBlob(snapshotsIndexBlobName); XContentParser parser = XContentType.JSON.xContent().createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, blob)) { return RepositoryData.snapshotsFromXContent(parser, indexGen); } } catch (IOException ioe) { // If we fail to load the generation we tracked in latestKnownRepoGen we reset it. // This is done as a fail-safe in case a user manually deletes the contents of the repository in which case subsequent // operations must start from the EMPTY_REPO_GEN again if (RepositoryData.EMPTY_REPO_GEN == latestKnownRepoGen.updateAndGet(known -> known == indexGen ? RepositoryData.EMPTY_REPO_GEN : known)) { logger.warn("Resetting repository generation tracker because we failed to read generation [" + indexGen + "]"); } throw new RepositoryException(metadata.name(), "could not read repository data from index blob", ioe); } }	can you also add exception here?
@Override public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) { return maybeMissLatestIndexN( Maps.ofEntries(listBlobs().entrySet().stream().filter(entry -> entry.getKey().startsWith(blobNamePrefix)) .collect(Collectors.toList()))); }	i am aware that this does not fully cover all possible inconsistent listing scenarios, but only the scenario of missing a known (in the latestknownrepogen field) index-n, but correctly handling this scenario is the only thing fixed here for now. it's the most likely scenario in practice though in my opinion (inconsistent listing after back-to-back operations without master failover).
StreamOutput streamOutput(StreamOutput out) throws IOException; /** * Creates a new stream output that compresses the contents and writes to the provided stream * output. Closing the returned {@link StreamOutput} will not close the provided stream output * as closing would also release the bytes of the {@link ReleasableBytesStreamOutput}	should this be a default impl and delegate to the more generic one?
public void testAddConsumerAffix() { Setting.AffixSetting<Integer> testSetting = Setting.affixKeySetting("foo.", "bar", (k) -> Setting.intSetting(k, 1, Property.Dynamic, Property.NodeScope)); Setting.AffixSetting<List<Integer>> testSetting2 = Setting.affixKeySetting("foo.", "list", (k) -> Setting.listSetting(k, Arrays.asList("1"), Integer::parseInt, Property.Dynamic, Property.NodeScope)); AbstractScopedSettings service = new ClusterSettings(Settings.EMPTY,new HashSet<>(Arrays.asList(testSetting, testSetting2))); Map<String, List<Integer>> listResults = new HashMap<>(); Map<String, Integer> intResults = new HashMap<>(); BiConsumer<String, List<Integer>> listConsumer = listResults::put; BiConsumer<String, Integer> intConsumer = intResults::put; service.addAffixUpdateConsumer(testSetting2, listConsumer, (s, k) -> {}); service.addAffixUpdateConsumer(testSetting, intConsumer, (s, k) -> {}); assertEquals(0, listResults.size()); assertEquals(0, intResults.size()); service.applySettings(Settings.builder() .put("foo.test.bar", 2) .put("foo.test_1.bar", 7) .putArray("foo.test_list.list", "16", "17") .putArray("foo.test_list_1.list", "18", "19", "20") .build()); assertEquals(2, intResults.get("test").intValue()); assertEquals(7, intResults.get("test_1").intValue()); assertEquals(Arrays.asList(16, 17), listResults.get("test_list")); assertEquals(Arrays.asList(18, 19, 20), listResults.get("test_list_1")); assertEquals(2, listResults.size()); assertEquals(2, intResults.size()); listResults.clear(); intResults.clear(); service.applySettings(Settings.builder() .put("foo.test.bar", 2) .put("foo.test_1.bar", 8) .putArray("foo.test_list.list", "16", "17") .putNull("foo.test_list_1.list") .build()); assertNull("test wasn't changed", intResults.get("test")); assertEquals(8, intResults.get("test_1").intValue()); assertNull("test_list wasn't changed", listResults.get("test_list")); assertEquals(Arrays.asList(1), listResults.get("test_list_1")); // reset to default assertEquals(1, listResults.size()); assertEquals(1, intResults.size()); }	i'd switch the order of these so it matches the declaration order.
private void buildResponse(final ClusterStateRequest request, final ClusterState currentState, final ActionListener<ClusterStateResponse> listener) throws IOException { logger.trace("Serving cluster state request using version {}", currentState.version()); ClusterState.Builder builder = ClusterState.builder(currentState.getClusterName()); builder.version(currentState.version()); builder.stateUUID(currentState.stateUUID()); if (request.nodes()) { builder.nodes(currentState.nodes()); } if (request.routingTable()) { if (request.indices().length > 0) { RoutingTable.Builder routingTableBuilder = RoutingTable.builder(); String[] indices = indexNameExpressionResolver.concreteIndexNames(currentState, request); for (String filteredIndex : indices) { if (currentState.routingTable().getIndicesRouting().containsKey(filteredIndex)) { routingTableBuilder.add(currentState.routingTable().getIndicesRouting().get(filteredIndex)); } } builder.routingTable(routingTableBuilder.build()); } else { builder.routingTable(currentState.routingTable()); } } if (request.blocks()) { builder.blocks(currentState.blocks()); } MetaData.Builder mdBuilder = MetaData.builder(); mdBuilder.clusterUUID(currentState.metaData().clusterUUID()); mdBuilder.coordinationMetaData(currentState.coordinationMetaData()); if (request.metaData()) { if (request.indices().length > 0) { mdBuilder.version(currentState.metaData().version()); String[] indices = indexNameExpressionResolver.concreteIndexNames(currentState, request); for (String filteredIndex : indices) { IndexMetaData indexMetaData = currentState.metaData().index(filteredIndex); if (indexMetaData != null) { mdBuilder.put(indexMetaData, false); } } } else { mdBuilder = MetaData.builder(currentState.metaData()); } // filter out metadata that shouldn't be returned by the API for (ObjectObjectCursor<String, Custom> custom : currentState.metaData().customs()) { if (custom.value.context().contains(MetaData.XContentContext.API) == false) { mdBuilder.removeCustom(custom.key); } } } builder.metaData(mdBuilder); if (request.customs()) { for (ObjectObjectCursor<String, ClusterState.Custom> custom : currentState.customs()) { if (custom.value.isPrivate() == false) { builder.putCustom(custom.key, custom.value); } } } clusterStateSizeByVersionCache.getOrComputeCachedSize(currentState.version(), () -> PublicationTransportHandler.serializeFullClusterState(currentState, Version.CURRENT).length(), ActionListener.wrap(size -> listener.onResponse(new ClusterStateResponse(currentState.getClusterName(), builder.build(), size, false)), listener::onFailure)); }	nit: you could use the new actionlistener#map here to make this a little nicer :)
@SuppressWarnings("unchecked") @Override public Map<String, Object> getExplicitlyMappedFields(Map<String, Object> mappingsProperties, String resultsFieldName) { Map<String, Object> additionalProperties = new HashMap<>(); additionalProperties.put(resultsFieldName + ".feature_importance", FEATURE_IMPORTANCE_MAPPING); Object dependentVariableMapping = extractMapping(dependentVariable, mappingsProperties); if ((dependentVariableMapping instanceof Map) == false) { return additionalProperties; } Map<String, Object> dependentVariableMappingAsMap = (Map) dependentVariableMapping; // If the source field is an alias, fetch the concrete field that the alias points to. if (FieldAliasMapper.CONTENT_TYPE.equals(dependentVariableMappingAsMap.get("type"))) { String path = (String) dependentVariableMappingAsMap.get(FieldAliasMapper.Names.PATH); dependentVariableMapping = extractMapping(path, mappingsProperties); } // We may have updated the value of {@code dependentVariableMapping} in the "if" block above. // Hence, we need to check the "instanceof" condition again. if ((dependentVariableMapping instanceof Map) == false) { return additionalProperties; } additionalProperties.put(resultsFieldName + "." + predictionFieldName, dependentVariableMapping); additionalProperties.put( resultsFieldName + ".top_classes", Map.of( "type", ObjectMapper.NESTED_CONTENT_TYPE, "properties", Map.of("class_name", dependentVariableMapping))); return additionalProperties; }	let's not use map.of as it doesn't backport well to 7.x
default <T extends EvaluationMetric> List<T> initMetrics(@Nullable List<T> parsedMetrics, Supplier<List<T>> defaultMetricsSupplier) { List<T> metrics = parsedMetrics == null ? defaultMetricsSupplier.get() : new ArrayList<>(parsedMetrics); if (metrics.isEmpty()) { throw ExceptionsHelper.badRequestException("[{}] must have one or more metrics", getName()); } Collections.sort(metrics, Comparator.comparing(EvaluationMetric::getName)); for (Tuple<String, String> requiredField : getFields().listAll()) { String fieldDescriptor = requiredField.v1(); String field = requiredField.v2(); if (field == null) { String metricNamesString = metrics.stream() .filter(m -> m.getRequiredFields().contains(fieldDescriptor)) .map(EvaluationMetric::getName) .collect(joining(", ")); if (metricNamesString.isEmpty() == false) { throw ExceptionsHelper.badRequestException( "[{}] must define [{}] as required by the following metrics [{}]", getName(), fieldDescriptor, metricNamesString); } } } return metrics; }	extract in a method checkrequiredfieldsareset or similar?
*/ default SearchSourceBuilder buildSearch(EvaluationParameters parameters, QueryBuilder userProvidedQueryBuilder) { Objects.requireNonNull(userProvidedQueryBuilder); BoolQueryBuilder boolQuery = QueryBuilders.boolQuery() // Verify existence of the actual field (which is always required) .filter(QueryBuilders.existsQuery(getFields().getActualField())); if (getFields().getPredictedField() != null) { // Verify existence of the predicted field if required for this evaluation boolQuery.filter(QueryBuilders.existsQuery(getFields().getPredictedField())); } if (getFields().getResultsNestedField() != null) { // Verify existence of the results nested field if required for this evaluation QueryBuilder resultsNestedFieldExistsQuery = QueryBuilders.existsQuery(getFields().getResultsNestedField()); boolQuery.filter( QueryBuilders.nestedQuery(getFields().getResultsNestedField(), resultsNestedFieldExistsQuery, ScoreMode.None) .ignoreUnmapped(true)); } if (getFields().getPredictedClassNameField() != null) { assert getFields().getResultsNestedField() != null; // Verify existence of the predicted class name field if required for this evaluation QueryBuilder classNameFieldExistsQuery = QueryBuilders.existsQuery(getFields().getPredictedClassNameField()); boolQuery.filter( QueryBuilders.nestedQuery(getFields().getResultsNestedField(), classNameFieldExistsQuery, ScoreMode.None) .ignoreUnmapped(true)); } if (getFields().getPredictedProbabilityField() != null) { // Verify existence of the predicted probability field if required for this evaluation QueryBuilder probabilityFieldExistsQuery = QueryBuilders.existsQuery(getFields().getPredictedProbabilityField()); // predicted probability field may be either nested (just like in case of classification evaluation) or non-nested (just like // in case of outlier detection evaluation). Here we support both modes. if (getFields().getResultsNestedField() != null) { boolQuery.filter( QueryBuilders.nestedQuery(getFields().getResultsNestedField(), probabilityFieldExistsQuery, ScoreMode.None) .ignoreUnmapped(true)); } else { boolQuery.filter(probabilityFieldExistsQuery); } } // Apply user-provided query boolQuery.filter(userProvidedQueryBuilder); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().size(0).query(boolQuery); for (EvaluationMetric metric : getMetrics()) { // Fetch aggregations requested by individual metrics Tuple<List<AggregationBuilder>, List<PipelineAggregationBuilder>> aggs = metric.aggs(parameters, getFields()); aggs.v1().forEach(searchSourceBuilder::aggregation); aggs.v2().forEach(searchSourceBuilder::aggregation); } return searchSourceBuilder; } /** * Processes {@link SearchResponse}	could we have a method list<querybuilder> evaluationfields.buildexistsqueries()? would that allow us to delete the explicit getters from evaluationfields?
* @param seqNo the sequence number associated with the operation * @return the location the bytes were written to * @throws IOException if writing to the translog resulted in an I/O exception */ public Translog.Location add(final ReleasableBytesReference data, final long seqNo) throws IOException { final Translog.Location location; final long bytesBufferedAfterAdd; synchronized (this) { ensureOpen(); final long offset = totalOffset; totalOffset += data.length(); bufferedBytes += data.length(); bufferedOps.add(new Operation(seqNo, data.retain())); assert minSeqNo != SequenceNumbers.NO_OPS_PERFORMED || operationCounter == 0; assert maxSeqNo != SequenceNumbers.NO_OPS_PERFORMED || operationCounter == 0; minSeqNo = SequenceNumbers.min(minSeqNo, seqNo); maxSeqNo = SequenceNumbers.max(maxSeqNo, seqNo); operationCounter++; assert assertNoSeqNumberConflict(seqNo, data); location = new Translog.Location(generation, offset, data.length()); bytesBufferedAfterAdd = bufferedBytes; } if (bytesBufferedAfterAdd >= forceWriteThreshold) { writeBufferedOps(Long.MAX_VALUE, false); } return location; }	i would like to limit the max buffer size (also if writing the buffer is slow), perhaps something like this: suggestion writebufferedops(long.max_value, bytesbufferedafteradd >= forcewritethreshold * 2);
*/ public TranslogReader closeIntoReader() throws IOException { // make sure to acquire the sync lock first, to prevent dead locks with threads calling // syncUpTo() , where the sync lock is acquired first, following by the synchronize(this) // After the sync lock we acquire the write lock to avoid deadlocks with threads writing where // the write lock is acquired first followed by synchronize(this). // // Note: While this is not strictly needed as this method is called while blocking all ops on the translog, // we do this to for correctness and preventing future issues. synchronized (syncLock) { try (ReleasableLock toClose = writeLock.acquire()) { synchronized (this) { try { sync(); // sync before we close.. } catch (final Exception ex) { closeWithTragicEvent(ex); throw ex; } // If we reached this point, all of the buffered ops should have been flushed successfully. assert bufferedOps.size() == 0; if (closed.compareAndSet(false, true)) { try { checkpointChannel.close(); } catch (final Exception ex) { closeWithTragicEvent(ex); throw ex; } return new TranslogReader(getLastSyncedCheckpoint(), channel, path, header); } else { throw new AlreadyClosedException("translog [" + getGeneration() + "] is already closed (path [" + path + "]", tragedy.get()); } } } } }	can we also assert that totalops == getwrittenoffset()?
@Override public TranslogSnapshot newSnapshot() { // make sure to acquire the sync lock first, to prevent dead locks with threads calling // syncUpTo() , where the sync lock is acquired first, following by the synchronize(this) // After the sync lock we acquire the write lock to avoid deadlocks with threads writing where // the write lock is acquired first followed by synchronize(this). synchronized (syncLock) { try (ReleasableLock toClose = writeLock.acquire()) { synchronized (this) { ensureOpen(); try { sync(); } catch (IOException e) { throw new TranslogException(shardId, "exception while syncing before creating a snapshot", e); } return super.newSnapshot(); } } } }	i think the assert on bufferedops size and totalops above can also go here?
@Override protected void executeChunkRequest(FileChunk request, ActionListener<Void> listener) { remoteClient.execute(GetCcrRestoreFileChunkAction.INSTANCE, new GetCcrRestoreFileChunkRequest(node, sessionUUID, request.md.name(), request.bytesRequested), ListenerTimeouts.wrapWithTimeout(threadPool, new ActionListener<>() { @Override public void onResponse( GetCcrRestoreFileChunkAction.GetCcrRestoreFileChunkResponse getCcrRestoreFileChunkResponse) { getCcrRestoreFileChunkResponse.incRef(); threadPool.generic().execute(new ActionRunnable<>(listener) { @Override protected void doRun() throws Exception { writeFileChunk(request.md, getCcrRestoreFileChunkResponse); listener.onResponse(null); } @Override public void onAfter() { getCcrRestoreFileChunkResponse.decRef(); } }); } @Override public void onFailure(Exception e) { threadPool.generic().execute(() -> { try { listener.onFailure(e); } catch (Exception ex) { e.addSuppressed(ex); logger.warn(() -> new ParameterizedMessage("failed to execute failure callback for chunk request"), e); } }); } }, ccrSettings.getRecoveryActionTimeout(), ThreadPool.Names.GENERIC, GetCcrRestoreFileChunkAction.NAME)); }	this whole thing is kind of verbose but since we need to fork-off after incrementing the ref there wasn't a nice way of doing this via threadedactionlistener and this recreates the old behavior of that class pretty much exactly and with a more meaningful log message. (we definitely have to catch here, the callback does blocking transport requests that will fail at some rate)
public void testCheckpointOnDiskFull() throws IOException { final Checkpoint checkpoint = randomCheckpoint(); Path tempDir = createTempDir(); Checkpoint.write(FileChannel::open, tempDir.resolve("foo.cpk"), checkpoint, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW); final Checkpoint checkpoint2 = randomCheckpoint(); try { Checkpoint.write((p, o) -> { if (randomBoolean()) { throw new MockDirectoryWrapper.FakeIOException(); } FileChannel open = FileChannel.open(p, o); FailSwitch failSwitch = new FailSwitch(); failSwitch.failNever(); // don't fail in the ctor ThrowingFileChannel channel = new ThrowingFileChannel(failSwitch, false, false, open); failSwitch.failAlways(); return channel; }, tempDir.resolve("foo.cpk"), checkpoint2, StandardOpenOption.WRITE); fail("should have failed earlier"); } catch (MockDirectoryWrapper.FakeIOException ex) { //fine } Checkpoint read = Checkpoint.read(tempDir.resolve("foo.cpk")); assertEquals(read, checkpoint); }	:+1: although #44217 means this now throws a different exception.
private void processData(DataFrameAnalyticsTask task, ProcessContext processContext, BytesReference state) { DataFrameAnalyticsConfig config = processContext.config; DataFrameDataExtractor dataExtractor = processContext.dataExtractor.get(); AnalyticsProcess<AnalyticsResult> process = processContext.process.get(); AnalyticsResultProcessor resultProcessor = processContext.resultProcessor.get(); try { writeHeaderRecord(dataExtractor, process); writeDataRows(dataExtractor, process, config.getAnalysis(), task.getProgressTracker()); process.writeEndOfDataMessage(); process.flushStream(); restoreState(task, config, state, process); LOGGER.info("[{}] Waiting for result processor to complete", config.getId()); resultProcessor.awaitForCompletion(); processContext.setFailureReason(resultProcessor.getFailure()); refreshDest(config); LOGGER.info("[{}] Result processor has completed", config.getId()); } catch (Exception e) { String errorMsg = new ParameterizedMessage("[{}] Error while processing data [{}]", config.getId(), e.getMessage()).getFormattedMessage(); if (task.isStopping()) { // Errors during task stopping are expected but we still want to log them just in case. LOGGER.debug(errorMsg, e); } else { LOGGER.error(errorMsg, e); processContext.setFailureReason(errorMsg); } } finally { closeProcess(task); processContextByAllocation.remove(task.getAllocationId()); LOGGER.debug("Removed process context for task [{}]; [{}] processes still running", config.getId(), processContextByAllocation.size()); if (processContext.getFailureReason() == null) { // This results in marking the persistent task as complete LOGGER.info("[{}] Marking task completed", config.getId()); auditor.info(config.getId(), Messages.DATA_FRAME_ANALYTICS_AUDIT_FINISHED_ANALYSIS); task.markAsCompleted(); } else { LOGGER.error("[{}] Marking task failed; {}", config.getId(), processContext.getFailureReason()); task.updateState(DataFrameAnalyticsState.FAILED, processContext.getFailureReason()); // Note: We are not marking the task as failed here as we want the user to be able to inspect the failure reason. } } }	it will also be useful to log we're stopping in this case.
private static void executeDockerRun(Distribution distribution, Map<Path, Path> volumes, Map<String, String> envVars) { removeContainer(); final List<String> args = new ArrayList<>(); args.add("docker run"); // Run the container in the background args.add("--detach"); if (envVars != null) { envVars.forEach((key, value) -> args.add("--env " + key + "=\\\\"" + value + "\\\\"")); } // The container won't run without configuring discovery args.add("--env discovery.type=single-node"); // Map ports in the container to the host, so that we can send requests args.add("--publish 9200:9200"); args.add("--publish 9300:9300"); // Bind-mount any volumes if (volumes != null) { volumes.forEach((localPath, containerPath) -> { assertTrue(localPath + " doesn't exist", Files.exists(localPath)); // The process in the Docker container doesn't run as root, so we need to ensure that // it is able to read the bind-mounted files. sh.run("chown -R 1000:0 " + localPath); args.add("--volume \\\\"" + localPath + ":" + containerPath + "\\\\""); }); } // Image name args.add(distribution.flavor.name + ":test"); final String command = String.join(" ", args); logger.info("Running command: " + command); containerId = sh.run(command).stdout.trim(); }	@pugnascotia doesn't this require elevated privileges on the localhost?
static ClusterState moveClusterStateToStep(String indexName, ClusterState currentState, StepKey currentStepKey, StepKey nextStepKey, LongSupplier nowSupplier, PolicyStepsRegistry stepRegistry) { IndexMetaData idxMeta = currentState.getMetaData().index(indexName); Settings indexSettings = idxMeta.getSettings(); String indexPolicySetting = LifecycleSettings.LIFECYCLE_NAME_SETTING.get(indexSettings); // policy could be updated in-between execution if (Strings.isNullOrEmpty(indexPolicySetting)) { throw new IllegalArgumentException("index [" + indexName + "] is not associated with an Index Lifecycle Policy"); } if (currentStepKey.equals(IndexLifecycleRunner.getCurrentStepKey(indexSettings)) == false) { throw new IllegalArgumentException("index [" + indexName + "] is not on current step [" + currentStepKey + "]"); } Step nextStep = stepRegistry.getStep(indexPolicySetting, nextStepKey); if (nextStep == null) { // stepRegistry may not be up-to-date with latest policies/steps in cluster-state return currentState; } return IndexLifecycleRunner.moveClusterStateToNextStep(idxMeta.getIndex(), currentState, currentStepKey, nextStepKey, nowSupplier); }	sorry, i had not looked closely enough at what this method is used for. it seems that its only used for the "move to step" and retry apis so actually its probably ok for this method to throw an exception directly. i had thought before that we use this method for moving between steps normally. i think we should throw directly here as the alternative of returning the same cluster state and then using that as an indication that the step is not recognised is a bit trappy. to ensure this method is kept only for api calls and not for normal execution maybe we can add a javadoc comment?
private void validateMappings(final GetIndexResponse getIndexResponse) { String[] sourceIndices = getIndexResponse.getIndices(); logger.debug("Policy [{}]: Validating [{}] source mappings", policyName, sourceIndices); for (String sourceIndex : sourceIndices) { Map<String, Object> mapping = getMappings(getIndexResponse, sourceIndex); // First ensure mapping is set if (mapping.get("properties") == null) { throw new ElasticsearchException( "Enrich policy execution for [{}] failed. Could not read mapping for source [{}] included by pattern [{}]", policyName, sourceIndex, policy.getIndices()); } // Validate the key and values try { validateField(mapping, policy.getEnrichKey(), true); for (String valueFieldName : policy.getEnrichValues()) { validateField(mapping, valueFieldName, false); } } catch (ElasticsearchException e) { throw new ElasticsearchException( "Enrich policy execution for [{}] failed while validating field mappings for index [{}]", e, policyName, sourceIndex); } } }	assert strings.isempty(fieldname) == false: "..";
private DocumentMapper mergeAndApplyMappings(String mappingType, CompressedXContent mappingSource, MergeReason reason) { final DocumentMapper currentMapper = this.mapper; if (currentMapper != null && currentMapper.mappingSource().equals(mappingSource)) { return currentMapper; } synchronized (this) { Mapping incomingMapping = parseMapping(mappingType, mappingSource); Mapping mapping = mergeMappings(this.mapper, incomingMapping, reason); DocumentMapper newMapper = newDocumentMapper(mapping, reason); if (reason == MergeReason.MAPPING_UPDATE_PREFLIGHT) { return newMapper; } this.mapper = newMapper; assert assertSerialization(newMapper); return newMapper; } }	this can probably be outside the synchronized block as well?
@Override public boolean equals(Object o) { if (this == o) return true; if (!(o instanceof NamedAnalyzer)) return false; NamedAnalyzer that = (NamedAnalyzer) o; return Objects.equals(name, that.name); }	i don't have a better idea, but this makes me slightly nervous since nothing in the api prevents you from doing java analyzer a1 = new namedanalyzer("foo", someanalyzer); analyzer a2 = new namedanalyzer("foo", otheranalyzer);
public static FieldMaskingSpanQueryBuilder fromXContent(XContentParser parser) throws IOException { float boost = AbstractQueryBuilder.DEFAULT_BOOST; SpanQueryBuilder inner = null; String field = null; String queryName = null; String currentFieldName = null; XContentParser.Token token; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_OBJECT) { if (QUERY_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { QueryBuilder query = parseInnerQueryBuilder(parser); if (query instanceof SpanQueryBuilder == false) { throw new ParsingException(parser.getTokenLocation(), "[field_masking_span] query must be of type span query"); } inner = (SpanQueryBuilder) query; checkNoBoost(NAME.getPreferredName(), currentFieldName, parser, inner); } else { throw new ParsingException(parser.getTokenLocation(), "[field_masking_span] query does not support [" + currentFieldName + "]"); } } else { if (AbstractQueryBuilder.BOOST_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { boost = parser.floatValue(); } else if (FIELD_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { field = parser.text(); } else if (AbstractQueryBuilder.NAME_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { queryName = parser.text(); } else { throw new ParsingException(parser.getTokenLocation(), "[field_masking_span] query does not support [" + currentFieldName + "]"); } } } if (inner == null) { throw new ParsingException(parser.getTokenLocation(), "field_masking_span must have [query] span query clause"); } if (field == null) { throw new ParsingException(parser.getTokenLocation(), "field_masking_span must have [field] set for it"); } FieldMaskingSpanQueryBuilder queryBuilder = new FieldMaskingSpanQueryBuilder(inner, field); queryBuilder.boost(boost); queryBuilder.queryName(queryName); return queryBuilder; }	can we replace the name in these error messages as well?
public void testFromJson() throws IOException { String json = "{\\\\n" + " \\\\"" + NAME.getPreferredName() + "\\\\" : {\\\\n" + " \\\\"query\\\\" : {\\\\n" + " \\\\"span_term\\\\" : {\\\\n" + " \\\\"value\\\\" : {\\\\n" + " \\\\"value\\\\" : 0.5,\\\\n" + " \\\\"boost\\\\" : 0.23\\\\n" + " }\\\\n" + " }\\\\n" + " },\\\\n" + " \\\\"field\\\\" : \\\\"mapped_geo_shape\\\\",\\\\n" + " \\\\"boost\\\\" : 42.0,\\\\n" + " \\\\"_name\\\\" : \\\\"KPI\\\\"\\\\n" + " }\\\\n" + "}"; Exception exception = expectThrows(ParsingException.class, () -> parseQuery(json)); assertThat(exception.getMessage(), equalTo(NAME.getPreferredName() + " [query] as a nested span clause can't have non-default boost value [0.23]")); }	can we also add a test that uses the old deprecated name, and checks for a deprecation warning? there's an assertwarnings() helper method that will check for specific deprecation strings.
* @param failure exception if snapshot failed * @param listener listener to notify when snapshot information is removed from the cluster state */ private void removeSnapshotFromClusterState(final Snapshot snapshot, final SnapshotInfo snapshotInfo, final Exception failure, @Nullable CleanupAfterErrorListener listener) { clusterService.submitStateUpdateTask("remove snapshot metadata", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); if (snapshots != null) { boolean changed = false; ArrayList<SnapshotsInProgress.Entry> entries = new ArrayList<>(); for (SnapshotsInProgress.Entry entry : snapshots.entries()) { if (entry.snapshot().equals(snapshot)) { changed = true; } else { entries.add(entry); } } if (changed) { snapshots = new SnapshotsInProgress(entries.toArray(new SnapshotsInProgress.Entry[entries.size()])); return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, snapshots).build(); } } return currentState; } @Override public void onFailure(String source, Exception e) { logger.warn((Supplier<?>) () -> new ParameterizedMessage("[{}] failed to remove snapshot metadata", snapshot), e); if (listener != null) { listener.onFailure(e); } } @Override public void onNoLongerMaster(String source) { listener.onNoLongerMaster(source); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { for (SnapshotCompletionListener listener : snapshotCompletionListeners) { try { if (snapshotInfo != null) { listener.onSnapshotCompletion(snapshot, snapshotInfo); } else { listener.onSnapshotFailure(snapshot, failure); } } catch (Exception t) { logger.warn((Supplier<?>) () -> new ParameterizedMessage("failed to notify listener [{}]", listener), t); } } if (listener != null) { listener.onResponse(snapshotInfo); } } }); } /** * Deletes a snapshot from the repository, looking up the {@link Snapshot}	can listener be null here? it is marked as nullable above?
@Override public SignificantStringTerms buildAggregation(long owningBucketOrdinal) { assert owningBucketOrdinal == 0; final int size = (int) Math.min(bucketOrds.size(), shardSize); ContextIndexSearcher searcher = context.searchContext().searcher(); IndexReader topReader = searcher.getIndexReader(); long supersetSize = topReader.numDocs(); long subsetSize = numCollectedDocs; BucketSignificancePriorityQueue ordered = new BucketSignificancePriorityQueue(size); SignificantStringTerms.Bucket spare = null; for (int i = 0; i < bucketOrds.size(); i++) { if (spare == null) { spare = new SignificantStringTerms.Bucket(new BytesRef(), 0, 0, 0, 0, null); termsAggFactory.buildTermsEnum(context); } bucketOrds.get(i, spare.termBytes); spare.subsetDf = bucketDocCount(i); spare.subsetSize = subsetSize; spare.supersetDf = termsAggFactory.getBackgroundFrequency(spare.termBytes); spare.supersetSize = supersetSize; assert spare.subsetDf <= spare.supersetDf; // During shard-local down-selection we use subset/superset stats // that are for this shard only // Back at the central reducer these properties will be updated with // global stats spare.updateScore(); spare.bucketOrd = i; spare = (SignificantStringTerms.Bucket) ordered.insertWithOverflow(spare); } final InternalSignificantTerms.Bucket[] list = new InternalSignificantTerms.Bucket[ordered.size()]; for (int i = ordered.size() - 1; i >= 0; i--) { final SignificantStringTerms.Bucket bucket = (SignificantStringTerms.Bucket) ordered.pop(); // the terms are owned by the BytesRefHash, we need to pull a copy since the BytesRef hash data may be recycled at some point bucket.termBytes = BytesRef.deepCopyOf(bucket.termBytes); bucket.aggregations = bucketAggregations(bucket.bucketOrd); list[i] = bucket; } return new SignificantStringTerms(subsetSize, supersetSize, name, requiredSize, minDocCount, Arrays.asList(list)); }	should it be called in front of the for-loop instead?
public void testDefaultPipelineBeforeCSRecovered() throws Exception { internalCluster().startNode(); BytesReference pipeline = new BytesArray("{\\\\n" + " \\\\"processors\\\\" : [\\\\n" + " {\\\\n" + " \\\\"remove\\\\": {\\\\n" + " \\\\"field\\\\": \\\\"_type\\\\"\\\\n" + " }\\\\n" + " }" + " ]\\\\n" + "}"); client().admin().cluster().preparePutPipeline("test_pipeline", pipeline, XContentType.JSON).get(); client().admin().indices().preparePutTemplate("pipeline_template") .setPatterns(Collections.singletonList("*")) .setSettings( "{\\\\n" + " \\\\"index\\\\" : {\\\\n" + " \\\\"default_pipeline\\\\" : \\\\"test_pipeline\\\\"" + " }\\\\n" + "}\\\\n", XContentType.JSON).get(); internalCluster().fullRestart(new InternalTestCluster.RestartCallback() { @Override public Settings onNodeStopped(String nodeName) { return Settings.builder().put(GatewayService.RECOVER_AFTER_NODES_SETTING.getKey(), "2").build(); } @Override public boolean validateClusterForming() { return false; } }); CountDownLatch latch = new CountDownLatch(1); // this one should fail assertThat(expectThrows(ClusterBlockException.class, () -> client().prepareIndex("index", "foo", "fails") .setSource("x", 1) .setTimeout(TimeValue.timeValueMillis(100)) // 100ms, to fail quickly .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) .get()).getMessage(), equalTo("blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];")); // but this one should pass since it has a longer timeout client().prepareIndex("index", "foo", "passes1") .setSource("x", 2) .setTimeout(TimeValue.timeValueSeconds(60)) // wait for second node to start .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) .execute(new ActionListener<IndexResponse>() { @Override public void onResponse(IndexResponse indexResponse) { assertThat(indexResponse.status(), equalTo(RestStatus.CREATED)); assertThat(indexResponse.getResult(), equalTo(DocWriteResponse.Result.CREATED)); latch.countDown(); } @Override public void onFailure(Exception e) { fail("Should not have failed with exception: " + e.getMessage()); } }); // so the cluster state can be recovered internalCluster() .startNode(Settings.builder().put(GatewayService.RECOVER_AFTER_NODES_SETTING.getKey(), "1")); ensureYellow("index"); assertTrue(latch.await(5, TimeUnit.SECONDS)); client().prepareIndex("index", "bar", "passes2") .setSource("x", 3) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) .get(); client().admin().indices().prepareRefresh("index").get(); // note that the types are _doc not `foo` or `bar` Map<String, Object> source = client().prepareGet("index", "_doc", "passes1").get().getSource(); assertThat(source.get("x"), equalTo(2)); source = client().prepareGet("index", "_doc", "passes2").get().getSource(); assertThat(source.get("x"), equalTo(3)); // and make sure this failed doc didn't get through source = client().prepareGet("index", "foo", "fails").get().getSource(); assertNull(source); source = client().prepareGet("index", "_doc", "fails").get().getSource(); assertNull(source); }	i think this can just be .startnode() - the other node is the master, so this node doesn't do any state recovery.
public void testSourceFilteringWithJsonField() { prepareCreate("test").addMapping("_doc", "headers", "type=json").get(); ensureGreen(); Map<String, Object> headers = new HashMap<>(); headers.put("content-type", "application/json"); headers.put("origin", "https://www.elastic.co"); Map<String, Object> source = Collections.singletonMap("headers", headers); client().prepareIndex("test", "_doc", "1").setSource(source).get(); refresh(); SearchResponse response = client().prepareSearch("test").setFetchSource(true).get(); assertThat(response.getHits().getAt(0).getSourceAsMap(), equalTo(source)); response = client().prepareSearch("test").setFetchSource("headers", null).get(); assertThat(response.getHits().getAt(0).getSourceAsMap(), equalTo(source)); response = client().prepareSearch("test").setFetchSource("headers.content-type", null).get(); Map<String, Object> filteredSource = Collections.singletonMap("headers", Collections.singletonMap("content-type", "application/json")); assertThat(response.getHits().getAt(0).getSourceAsMap(), equalTo(filteredSource)); response = client().prepareSearch("test").setFetchSource(null, "headers.content-type").get(); filteredSource = Collections.singletonMap("headers", Collections.singletonMap("origin", "https://www.elastic.co")); assertThat(response.getHits().getAt(0).getSourceAsMap(), equalTo(filteredSource)); }	nit: could you add a comment to this block and the block above explaining that this is testing the exclude filtering and the one above is testing the include filtering?
@Override protected NodeGatewayStartedShards nodeOperation(NodeRequest request) { try { final ShardId shardId = request.getShardId(); final String indexUUID = request.getIndexUUID(); logger.trace("{} loading local shard state info", shardId); ShardStateMetaData shardStateMetaData = ShardStateMetaData.FORMAT.loadLatestState(logger, nodeEnv.availableShardPaths(request.shardId)); if (shardStateMetaData != null) { final IndexMetaData metaData = clusterService.state().metaData().index(shardId.index().name()); // it's a mystery why this is sometimes null if (metaData != null) { ShardPath shardPath = null; try { shardPath = ShardPath.loadShardPath(logger, nodeEnv, shardId, metaData.settings()); if (shardPath == null) { throw new IllegalStateException(shardId + " no shard state found"); } Store.tryOpenIndex(shardPath.resolveIndex()); } catch (Exception exception) { logger.trace("{} can't open index for shard [{}] in path [{}]", exception, shardId, shardStateMetaData, (shardPath != null) ? shardPath.resolveIndex() : ""); return new NodeGatewayStartedShards(clusterService.localNode(), -1, exception); } } // old shard metadata doesn't have the actual index UUID so we need to check if the actual uuid in the metadata // is equal to IndexMetaData.INDEX_UUID_NA_VALUE otherwise this shard doesn't belong to the requested index. if (indexUUID.equals(shardStateMetaData.indexUUID) == false && IndexMetaData.INDEX_UUID_NA_VALUE.equals(shardStateMetaData.indexUUID) == false) { logger.warn("{} shard state info found but indexUUID didn't match expected [{}] actual [{}]", shardId, indexUUID, shardStateMetaData.indexUUID); } else { logger.debug("{} shard state info found: [{}]", shardId, shardStateMetaData); return new NodeGatewayStartedShards(clusterService.localNode(), shardStateMetaData.version); } } logger.trace("{} no local shard info found", shardId); return new NodeGatewayStartedShards(clusterService.localNode(), -1); } catch (Exception e) { throw new ElasticsearchException("failed to load started shards", e); } }	is this message correct? it should be no shard path found , right?
@Override protected NodeGatewayStartedShards nodeOperation(NodeRequest request) { try { final ShardId shardId = request.getShardId(); final String indexUUID = request.getIndexUUID(); logger.trace("{} loading local shard state info", shardId); ShardStateMetaData shardStateMetaData = ShardStateMetaData.FORMAT.loadLatestState(logger, nodeEnv.availableShardPaths(request.shardId)); if (shardStateMetaData != null) { final IndexMetaData metaData = clusterService.state().metaData().index(shardId.index().name()); // it's a mystery why this is sometimes null if (metaData != null) { ShardPath shardPath = null; try { shardPath = ShardPath.loadShardPath(logger, nodeEnv, shardId, metaData.settings()); if (shardPath == null) { throw new IllegalStateException(shardId + " no shard state found"); } Store.tryOpenIndex(shardPath.resolveIndex()); } catch (Exception exception) { logger.trace("{} can't open index for shard [{}] in path [{}]", exception, shardId, shardStateMetaData, (shardPath != null) ? shardPath.resolveIndex() : ""); return new NodeGatewayStartedShards(clusterService.localNode(), -1, exception); } } // old shard metadata doesn't have the actual index UUID so we need to check if the actual uuid in the metadata // is equal to IndexMetaData.INDEX_UUID_NA_VALUE otherwise this shard doesn't belong to the requested index. if (indexUUID.equals(shardStateMetaData.indexUUID) == false && IndexMetaData.INDEX_UUID_NA_VALUE.equals(shardStateMetaData.indexUUID) == false) { logger.warn("{} shard state info found but indexUUID didn't match expected [{}] actual [{}]", shardId, indexUUID, shardStateMetaData.indexUUID); } else { logger.debug("{} shard state info found: [{}]", shardId, shardStateMetaData); return new NodeGatewayStartedShards(clusterService.localNode(), shardStateMetaData.version); } } logger.trace("{} no local shard info found", shardId); return new NodeGatewayStartedShards(clusterService.localNode(), -1); } catch (Exception e) { throw new ElasticsearchException("failed to load started shards", e); } }	can we report the right version we found ? note that we would probably need to change the the logic in the gateway allocator to check for both -1 version and exception (now -1 means both).
public void addConnections(int numConnections, TransportRequestOptions.Type... types) { if (types == null || types.length == 0) { throw new IllegalArgumentException("types must not be null"); } for (TransportRequestOptions.Type type : types) { if (addedTypes.contains(type)) { throw new IllegalArgumentException("type [" + type + "] is already registered"); } } addedTypes.addAll(Arrays.asList(types)); handles.add(new ConnectionTypeHandle(offset, numConnections, types)); offset += numConnections; } /** * Creates a new {@link ConnectionProfile} based on the added connections. * @throws IllegalArgumentException if any of the {@link org.elasticsearch.transport.TransportRequestOptions.Type}	this javadoc is incorrect; the method throws an illegalstateexception (which in this case i think is correct). :smile:
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (source != null) { builder.rawValue(new BytesArray(source), XContentType.JSON); } else { builder.startObject().endObject(); } return builder; }	is it ok here to only end the object? should we not start it too? and above, does rawvalue print out a valid complete object?
public void testToXContent() throws IOException { PutMappingRequest request = new PutMappingRequest("foo"); request.type("my_type"); XContentBuilder mapping = JsonXContent.contentBuilder().startObject(); mapping.startObject("properties"); mapping.startObject("email"); mapping.field("type", "text"); mapping.endObject(); mapping.endObject(); mapping.endObject(); request.source(mapping); String actualRequestBody = Strings.toString(request); String expectedRequestBody = "{\\\\"properties\\\\":{\\\\"email\\\\":{\\\\"type\\\\":\\\\"text\\\\"}}}"; assertEquals(expectedRequestBody, actualRequestBody); }	do we need to test the case where source is not set?
StorageOptions createStorageOptions(final GoogleCloudStorageClientSettings clientSettings, final HttpTransportOptions httpTransportOptions) { StorageOptions options = super.createStorageOptions(clientSettings, httpTransportOptions); return options.toBuilder() .setHost(options.getHost()) .setCredentials(options.getCredentials()) .setRetrySettings(RetrySettings.newBuilder() .setTotalTimeout(options.getRetrySettings().getTotalTimeout()) .setInitialRetryDelay(Duration.ofMillis(10L)) .setRetryDelayMultiplier(options.getRetrySettings().getRetryDelayMultiplier()) .setMaxRetryDelay(Duration.ofSeconds(1L)) .setMaxAttempts(0) .setJittered(false) .setInitialRpcTimeout(options.getRetrySettings().getInitialRpcTimeout()) .setRpcTimeoutMultiplier(options.getRetrySettings().getRpcTimeoutMultiplier()) .setMaxRpcTimeout(options.getRetrySettings().getMaxRpcTimeout()) .build()) .build(); }	for some reason the tobuilder keeps track of all settings but these two in the latest version so we have to manually set these again.
@Override public Map<String, Repository.Factory> getRepositories(Environment env, NamedXContentRegistry registry, ClusterService clusterService) { return Collections.singletonMap(GoogleCloudStorageRepository.TYPE, metadata -> new GoogleCloudStorageRepository(metadata, registry, this.storageService, clusterService) { @Override protected GoogleCloudStorageBlobStore createBlobStore() { return new GoogleCloudStorageBlobStore("bucket", "test", storageService) { @Override long getLargeBlobThresholdInBytes() { return ByteSizeUnit.MB.toBytes(1); } }; } }); } } @SuppressForbidden(reason = "this test uses a HttpHandler to emulate a Google Cloud Storage endpoint") private static class GoogleCloudStorageBlobStoreHttpHandler extends GoogleCloudStorageHttpHandler implements BlobStoreHttpHandler { GoogleCloudStorageBlobStoreHttpHandler(final String bucket) { super(bucket); } } /** * HTTP handler that injects random Google Cloud Storage service errors * * Note: it is not a good idea to allow this handler to simulate too many errors as it would * slow down the test suite. */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate a Google Cloud Storage endpoint") private static class GoogleErroneousHttpHandler extends ErroneousHttpHandler { GoogleErroneousHttpHandler(final HttpHandler delegate, final int maxErrorsPerRequest) { super(delegate, maxErrorsPerRequest); } @Override protected String requestUniqueId(HttpExchange exchange) { if ("/token".equals(exchange.getRequestURI().getPath())) { try { // token content is unique per node (not per request) return Streams.readFully(Streams.noCloseStream(exchange.getRequestBody())).utf8ToString(); } catch (IOException e) { throw new AssertionError("Unable to read token request body", e); } } final String range = exchange.getRequestHeaders().getFirst("Content-Range"); return exchange.getRemoteAddress().getHostString() + " " + exchange.getRequestMethod() + " " + exchange.getRequestURI() + (range != null ? " " + range : ""); } @Override protected boolean canFailRequest(final HttpExchange exchange) { // Batch requests are not retried so we don't want to fail them // The batched request are supposed to be retried (not tested here) return exchange.getRequestURI().toString().startsWith("/batch/") == false; }	there seems to be a behavior change with retries here. the retries are now executed across multiple different tcp connections instead of reusing the same connection. this means we can't key them by the full address including the port any longer because that would allow for more retries than we assume.
* @since 10 */ public static <T> java.util.Set<T> copyOf(Collection<? extends T> coll) { switch (coll.size()) { case 0: return Set.of(); case 1: return Set.of(coll.iterator().next()); default: return Collections.unmodifiableSet(new HashSet<>(coll)); } }	this one looks especially bad, because we allocate a hashset just to call toarray which exists for collection.
void innerExecute(int index, List<?> values, List<Object> newValues, IngestDocument document, BiConsumer<IngestDocument, Exception> handler) { if (index == values.size()) { document.setFieldValue(field, new ArrayList<>(newValues)); handler.accept(document, null); return; } Object value = values.get(index); Object previousValue = document.getIngestMetadata().put("_value", value); final Thread thread = Thread.currentThread(); processor.execute(document, (result, e) -> { if (e != null) { newValues.add(document.getIngestMetadata().put("_value", previousValue)); handler.accept(null, e); } else if (result == null) { handler.accept(null, null); } else { newValues.add(document.getIngestMetadata().put("_value", previousValue)); if (thread == Thread.currentThread() && (index + 1) % MAX_RECURSE_PER_THREAD == 0) { // we are on the same thread and we need to fork to another thread to avoid recursive stack overflow on a single thread // only fork after 10 recursive calls, then fork every 10 to keep the number of threads down threadPool.generic().execute(() -> innerExecute(index + 1, values, newValues, document, handler)); } else { // we are on a different thread (we went asynchronous), it's safe to recurse innerExecute(index + 1, values, newValues, document, handler); } } }); }	amend comment: or we have not yet recursed 10 times with the same thread.
public void testCancellationDuringAggregation() throws Exception { List<ScriptedBlockPlugin> plugins = initBlockFactory(); indexTestData(); logger.info("Executing search"); ActionFuture<SearchResponse> searchResponse = client() .prepareSearch("test") .setQuery(matchAllQuery()) .addAggregation( new TermsAggregationBuilder("test_agg") .script(new Script( ScriptType.INLINE, "mockscript", ScriptedBlockPlugin.TERM_SCRIPT_NAME, Collections.emptyMap() )) .subAggregation( new ScriptedMetricAggregationBuilder("sub_agg") .initScript( new Script( ScriptType.INLINE, "mockscript", ScriptedBlockPlugin.INIT_SCRIPT_NAME, Collections.emptyMap() ) ) .mapScript( new Script( ScriptType.INLINE, "mockscript", ScriptedBlockPlugin.MAP_SCRIPT_NAME, Collections.emptyMap() ) ) .combineScript( new Script( ScriptType.INLINE, "mockscript", ScriptedBlockPlugin.COMBINE_SCRIPT_NAME, Collections.emptyMap() ) ) .reduceScript( new Script( ScriptType.INLINE, "mockscript", ScriptedBlockPlugin.REDUCE_SCRIPT_NAME, Collections.emptyMap() ) ) ) ) .execute(); awaitForBlock(plugins); cancelSearch(SearchAction.NAME); disableBlocks(plugins); assertNull(ensureSearchWasCancelled(searchResponse)); }	could you use a regular terms agg? if not probably worth a comment why. i spent a little while trying to figure out if the terms script created a second block or something before i went and read it.
public static String blockMasterFromFinalizingSnapshotOnIndexFile(final String repositoryName) { final String masterName = internalCluster().getMasterName(); ((MockRepository)internalCluster().getInstance(RepositoriesService.class, masterName) .repository(repositoryName)).setBlockOnWriteIndexFile(true); return masterName; }	@dakrone i had to make this change as the test i added uses the return value. the new implementation of this method is the same as it is in master, in 7.x the implementation of this method was identical to blockalldatanodes - i'm not sure exactly what was up with that. just a heads up.
public static ShardPath selectNewPathForShard(NodeEnvironment env, ShardId shardId, IndexSettings indexSettings, long avgShardSizeInBytes, Map<Path,Integer> dataPathToShardCount) throws IOException { final Path dataPath; final Path statePath; if (indexSettings.hasCustomDataPath()) { dataPath = env.resolveCustomLocation(indexSettings, shardId); statePath = env.nodePaths()[0].resolve(shardId); } else { BigInteger totFreeSpace = BigInteger.ZERO; for (NodeEnvironment.NodePath nodePath : env.nodePaths()) { totFreeSpace = totFreeSpace.add(BigInteger.valueOf(nodePath.fileStore.getUsableSpace())); } // TODO: this is a hack!! We should instead keep track of incoming (relocated) shards since we know // how large they will be once they're done copying, instead of a silly guess for such cases: // Very rough heuristic of how much disk space we expect the shard will use over its lifetime, the max of current average // shard size across the cluster and 5% of the total available free space on this node: BigInteger estShardSizeInBytes = BigInteger.valueOf(avgShardSizeInBytes).max(totFreeSpace.divide(BigInteger.valueOf(20))); // TODO - do we need something more extensible? Yet, this does the job for now... final NodeEnvironment.NodePath[] paths = env.nodePaths(); NodeEnvironment.NodePath bestPath = null; if (paths.length == 1) { // There's only one option, it's the best! bestPath = paths[0]; } else { int shardCount = indexSettings.getNumberOfShards(); // Maximum number of shards that a path should have for a particular index assuming // all the shards were assigned to this node. For example, with a node with 4 data // paths and an index with 9 primary shards, the maximum number of shards per path // would be 3. int maxShardsPerPath = Math.floorDiv(shardCount, paths.length) + ((shardCount % paths.length) == 0 ? 0 : 1); ObjectLongHashMap<NodeEnvironment.NodePath> pathToShardCount = env.shardCountPerPath(shardId.getIndex()); BigInteger maxUsableBytes = BigInteger.valueOf(Long.MIN_VALUE); for (NodeEnvironment.NodePath nodePath : paths) { FileStore fileStore = nodePath.fileStore; BigInteger usableBytes = BigInteger.valueOf(fileStore.getUsableSpace()); assert usableBytes.compareTo(BigInteger.ZERO) >= 0; // Deduct estimated reserved bytes from usable space: Integer count = dataPathToShardCount.get(nodePath.path); if (count != null) { usableBytes = usableBytes.subtract(estShardSizeInBytes.multiply(BigInteger.valueOf(count))); } if (pathToShardCount.get(nodePath) >= maxShardsPerPath) { // Too many shards for this index on this path, skip this path continue; } else if (bestPath == null || usableBytes.compareTo(maxUsableBytes) > 0) { // This path has been determined to be "better" based on the usable bytes maxUsableBytes = usableBytes; bestPath = nodePath; } } } statePath = bestPath.resolve(shardId); dataPath = statePath; } return new ShardPath(indexSettings.hasCustomDataPath(), dataPath, statePath, shardId); }	can you please put a message in here?
public static ShardPath selectNewPathForShard(NodeEnvironment env, ShardId shardId, IndexSettings indexSettings, long avgShardSizeInBytes, Map<Path,Integer> dataPathToShardCount) throws IOException { final Path dataPath; final Path statePath; if (indexSettings.hasCustomDataPath()) { dataPath = env.resolveCustomLocation(indexSettings, shardId); statePath = env.nodePaths()[0].resolve(shardId); } else { BigInteger totFreeSpace = BigInteger.ZERO; for (NodeEnvironment.NodePath nodePath : env.nodePaths()) { totFreeSpace = totFreeSpace.add(BigInteger.valueOf(nodePath.fileStore.getUsableSpace())); } // TODO: this is a hack!! We should instead keep track of incoming (relocated) shards since we know // how large they will be once they're done copying, instead of a silly guess for such cases: // Very rough heuristic of how much disk space we expect the shard will use over its lifetime, the max of current average // shard size across the cluster and 5% of the total available free space on this node: BigInteger estShardSizeInBytes = BigInteger.valueOf(avgShardSizeInBytes).max(totFreeSpace.divide(BigInteger.valueOf(20))); // TODO - do we need something more extensible? Yet, this does the job for now... final NodeEnvironment.NodePath[] paths = env.nodePaths(); NodeEnvironment.NodePath bestPath = null; if (paths.length == 1) { // There's only one option, it's the best! bestPath = paths[0]; } else { int shardCount = indexSettings.getNumberOfShards(); // Maximum number of shards that a path should have for a particular index assuming // all the shards were assigned to this node. For example, with a node with 4 data // paths and an index with 9 primary shards, the maximum number of shards per path // would be 3. int maxShardsPerPath = Math.floorDiv(shardCount, paths.length) + ((shardCount % paths.length) == 0 ? 0 : 1); ObjectLongHashMap<NodeEnvironment.NodePath> pathToShardCount = env.shardCountPerPath(shardId.getIndex()); BigInteger maxUsableBytes = BigInteger.valueOf(Long.MIN_VALUE); for (NodeEnvironment.NodePath nodePath : paths) { FileStore fileStore = nodePath.fileStore; BigInteger usableBytes = BigInteger.valueOf(fileStore.getUsableSpace()); assert usableBytes.compareTo(BigInteger.ZERO) >= 0; // Deduct estimated reserved bytes from usable space: Integer count = dataPathToShardCount.get(nodePath.path); if (count != null) { usableBytes = usableBytes.subtract(estShardSizeInBytes.multiply(BigInteger.valueOf(count))); } if (pathToShardCount.get(nodePath) >= maxShardsPerPath) { // Too many shards for this index on this path, skip this path continue; } else if (bestPath == null || usableBytes.compareTo(maxUsableBytes) > 0) { // This path has been determined to be "better" based on the usable bytes maxUsableBytes = usableBytes; bestPath = nodePath; } } } statePath = bestPath.resolve(shardId); dataPath = statePath; } return new ShardPath(indexSettings.hasCustomDataPath(), dataPath, statePath, shardId); }	i don't understand this. if we already have count shards allocated it should be already taken into account in usable space? why would we multiply it here again?
public static ShardPath selectNewPathForShard(NodeEnvironment env, ShardId shardId, IndexSettings indexSettings, long avgShardSizeInBytes, Map<Path,Integer> dataPathToShardCount) throws IOException { final Path dataPath; final Path statePath; if (indexSettings.hasCustomDataPath()) { dataPath = env.resolveCustomLocation(indexSettings, shardId); statePath = env.nodePaths()[0].resolve(shardId); } else { BigInteger totFreeSpace = BigInteger.ZERO; for (NodeEnvironment.NodePath nodePath : env.nodePaths()) { totFreeSpace = totFreeSpace.add(BigInteger.valueOf(nodePath.fileStore.getUsableSpace())); } // TODO: this is a hack!! We should instead keep track of incoming (relocated) shards since we know // how large they will be once they're done copying, instead of a silly guess for such cases: // Very rough heuristic of how much disk space we expect the shard will use over its lifetime, the max of current average // shard size across the cluster and 5% of the total available free space on this node: BigInteger estShardSizeInBytes = BigInteger.valueOf(avgShardSizeInBytes).max(totFreeSpace.divide(BigInteger.valueOf(20))); // TODO - do we need something more extensible? Yet, this does the job for now... final NodeEnvironment.NodePath[] paths = env.nodePaths(); NodeEnvironment.NodePath bestPath = null; if (paths.length == 1) { // There's only one option, it's the best! bestPath = paths[0]; } else { int shardCount = indexSettings.getNumberOfShards(); // Maximum number of shards that a path should have for a particular index assuming // all the shards were assigned to this node. For example, with a node with 4 data // paths and an index with 9 primary shards, the maximum number of shards per path // would be 3. int maxShardsPerPath = Math.floorDiv(shardCount, paths.length) + ((shardCount % paths.length) == 0 ? 0 : 1); ObjectLongHashMap<NodeEnvironment.NodePath> pathToShardCount = env.shardCountPerPath(shardId.getIndex()); BigInteger maxUsableBytes = BigInteger.valueOf(Long.MIN_VALUE); for (NodeEnvironment.NodePath nodePath : paths) { FileStore fileStore = nodePath.fileStore; BigInteger usableBytes = BigInteger.valueOf(fileStore.getUsableSpace()); assert usableBytes.compareTo(BigInteger.ZERO) >= 0; // Deduct estimated reserved bytes from usable space: Integer count = dataPathToShardCount.get(nodePath.path); if (count != null) { usableBytes = usableBytes.subtract(estShardSizeInBytes.multiply(BigInteger.valueOf(count))); } if (pathToShardCount.get(nodePath) >= maxShardsPerPath) { // Too many shards for this index on this path, skip this path continue; } else if (bestPath == null || usableBytes.compareTo(maxUsableBytes) > 0) { // This path has been determined to be "better" based on the usable bytes maxUsableBytes = usableBytes; bestPath = nodePath; } } } statePath = bestPath.resolve(shardId); dataPath = statePath; } return new ShardPath(indexSettings.hasCustomDataPath(), dataPath, statePath, shardId); }	i think we should take the path with the most usable space and assign it to bestpath before we go into this optmization to be sure we take the biggest available and don't rely on iteration order.
private boolean isValidParameter(String typeWithSubtype, String parameterName, String parameterValue) { if(parametersMap.containsKey(typeWithSubtype)){ Map<String, String> parameters = parametersMap.get(typeWithSubtype); if(parameters.containsKey(parameterName)){ String regex = parameters.get(parameterName); return parameterValue.matches(regex);//todo pg should we precompile regex? } } return false; }	we should compile the regex into a pattern to avoid doing it everytime
@Override public XContent xContent() { return YamlXContent.yamlXContent; } }, /** * A CBOR based content type. */ CBOR(3) { @Override public String mediaTypeWithoutParameters() { return "application/cbor"; } @Override public String subtype() { return "cbor"; } @Override public XContent xContent() { return CborXContent.cborXContent; } }; /** * A regexp to allow parsing media types. It covers two use cases. * 1. Media type with a version - requires a custom vnd.elasticsearch subtype and a compatible-with parameter * i.e. application/vnd.elasticsearch+json;compatible-with * 2. Media type without a version - for users not using compatible API i.e. application/json */ private static final MediaTypeParser<XContentType> mediaTypeParser = new MediaTypeParser.Builder<XContentType>() .withMediaTypeAndParams("application/smile", SMILE, Collections.emptyMap()) .withMediaTypeAndParams("application/cbor", CBOR, Collections.emptyMap()) .withMediaTypeAndParams("application/json", JSON, Map.of("charset", "UTF-8")) .withMediaTypeAndParams("application/yaml", YAML, Map.of("charset", "UTF-8")) .withMediaTypeAndParams("application/*", JSON, Map.of("charset", "UTF-8")) .withMediaTypeAndParams("application/x-ndjson", JSON, Map.of("charset", "UTF-8")) .withMediaTypeAndParams("application/vnd.elasticsearch+json", JSON, Map.of("compatible-with", "\\\\\\\\d+","charset", "UTF-8")) .withMediaTypeAndParams("application/vnd.elasticsearch+smile", SMILE, Map.of("compatible-with", "\\\\\\\\d+","charset", "UTF-8")) .withMediaTypeAndParams("application/vnd.elasticsearch+yaml", YAML, Map.of("compatible-with", "\\\\\\\\d+","charset", "UTF-8")) .withMediaTypeAndParams("application/vnd.elasticsearch+cbor", CBOR, Map.of("compatible-with", "\\\\\\\\d+","charset", "UTF-8")) .withMediaTypeAndParams("application/vnd.elasticsearch+x-ndjson", JSON, Map.of("compatible-with", "\\\\\\\\d+","charset", "UTF-8")) .build(); /** * Accepts a format string, which is most of the time is equivalent to {@link XContentType#subtype()} * and attempts to match the value to an {@link XContentType}. * The comparisons are done in lower case format. * This method will return {@code null} if no match is found */ public static XContentType fromFormat(String mediaType) { return mediaTypeParser.fromFormat(mediaType); } /** * Attempts to match the given media type with the known {@link XContentType} values. This match is done in a case-insensitive manner. * The provided media type can optionally has parameters. * This method is suitable for parsing of the {@code Content-Type} and {@code Accept} HTTP headers. * This method will return {@code null} if no match is found */ public static XContentType fromMediaType(String mediaTypeHeaderValue) { return mediaTypeParser.fromMediaType(mediaTypeHeaderValue); } private int index; XContentType(int index) { this.index = index; } public static Byte parseVersion(String mediaType) { MediaTypeParser<XContentType>.ParsedMediaType parsedMediaType = mediaTypeParser.parseMediaType(mediaType); if(parsedMediaType != null) { String version = parsedMediaType .getParameters() .get("compatible-with"); return version != null ? Byte.parseByte(version) : null; } return null; } public int index() { return index; } public String mediaType() { return mediaTypeWithoutParameters(); } public abstract XContent xContent(); public abstract String mediaTypeWithoutParameters(); @Override public String type() { return "application"; }	for the discussion: defining versioned media types up front, means that we won't throw an exception if someone specifies it with oss licence if he uses oss on v8 server for api that was removed and provides application/vnd.elasticsearch+json;compatible-with=7 he will get a 404 is there an easy way to make xcontenttype plugin aware? or licence aware? or are we ok with allowing to use versioned media types with oss?
private static BiConsumer<SearchRequest, BiConsumer<SearchResponse, Exception>> createSearchRunner( Client client, EnrichCache enrichCache ) { Client originClient = new OriginSettingClient(client, ENRICH_ORIGIN); return (req, handler) -> { try { CompletableFuture<SearchResponse> cacheEntry = enrichCache.computeIfAbsent(req, request -> { CompletableFuture<SearchResponse> completableFuture = new CompletableFuture<>(); originClient.execute( EnrichCoordinatorProxyAction.INSTANCE, request, ActionListener.wrap(completableFuture::complete, completableFuture::completeExceptionally) ); return completableFuture; }); cacheEntry.whenComplete((response, throwable) -> { Exception exception = null; if (throwable != null) { enrichCache.invalidate(req, cacheEntry); if (throwable instanceof Exception) { exception = (Exception) throwable; } else { exception = new Exception("Unexpected error", throwable); } } handler.accept(response, exception); }); } catch (ExecutionException e) { handler.accept(null, e); } }; }	in case throwable is an instance of error then we should throw it (throw (error) throwable;). so that the elasticsearchuncaughtexceptionhandler can catch it.
private void createWorkingDir(Path distroExtractDir) throws IOException { syncWithLinks(distroExtractDir, workingDir); Files.createDirectories(configFile.getParent()); Files.createDirectories(confPathRepo); Files.createDirectories(confPathData); Files.createDirectories(confPathLogs); Files.createDirectories(tmpDir); }	i don't think we should have special windows logic that is always run. linux should not need this leniency right? additionally, when would we be in a situation where a single build is reusing the same destination dir? any test clusters should each have their own unique dirs, so i don't see the situation that gets us needing to wait/retry on the destination being removed async.
public final RestResponse buildResponse(Response response) throws Exception { try (XContentBuilder builder = channel.newBuilder()) { final RestResponse restResponse = buildResponse(response, builder); assert assertBuilderClosed(builder) : "callers should ensure the XContentBuilder is closed themselves"; return restResponse; } } /** * Builds a response to send back over the channel. Implementors should ensure that they close the provided {@link XContentBuilder} * using the {@link XContentBuilder#close()}	i usually do this: assert xcontentbuilder.generator().isclosed(); return true;
@SuppressWarnings("unchecked") private ClusterState executeTask() throws Exception { setupState(); setupRequest(); final MetaDataCreateIndexService.IndexCreationTask task = new MetaDataCreateIndexService.IndexCreationTask( logger, allocationService, request, listener, indicesService, aliasValidator, xContentRegistry, clusterStateSettings.build(), validator, IndexScopedSettings.DEFAULT_SCOPED_SETTINGS) { @Override protected void checkShardLimit(final Settings settings, final ClusterState clusterState) { } }; return task.execute(state); }	could we add a short comment here explaining why this is here?
@Override public Settings additionalSettings() { return Settings.builder().put("netty.assert.buglevel", true) .build(); } } @Override public void close() { super.close(); if (!NetworkModule.TRANSPORT_TYPE_SETTING.exists(settings) || NetworkModule.TRANSPORT_TYPE_SETTING.get(settings).equals(Netty4Plugin.NETTY_TRANSPORT_NAME)) { try { GlobalEventExecutor.INSTANCE.awaitInactivity(5, TimeUnit.SECONDS); ThreadDeathWatcher.awaitInactivity(5, TimeUnit.SECONDS); } catch (InterruptedException e) { throw new RuntimeException(e); } }	is it ok to directly throw an exception in case of an interrupted exception in the first call without trying to close the threaddeathwatcher?
* @param consumer a {@link Runnable} that is executed after operations are blocked * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation * @throws InterruptedException if blocking operations is interrupted */ public void relocated(final Consumer<ReplicationTracker.PrimaryContext> consumer) throws IllegalIndexShardStateException, InterruptedException { assert shardRouting.primary() : "only primaries can be marked as relocated: " + shardRouting; refreshListeners.disallowAdd(); try { if (refreshListeners.refreshNeeded()) { refresh("relocated"); } indexShardOperationPermits.blockOperations(30, TimeUnit.MINUTES, () -> { // no shard operation permits are being held here, move state from started to relocated assert indexShardOperationPermits.getActiveOperationsCount() == 0 : "in-flight operations in progress while moving shard state to relocated"; /* * We should not invoke the runnable under the mutex as the expected implementation is to handoff the primary context via a * network operation. Doing this under the mutex can implicitly block the cluster state update thread on network operations. */ verifyRelocatingState(); final ReplicationTracker.PrimaryContext primaryContext = replicationTracker.startRelocationHandoff(); try { consumer.accept(primaryContext); synchronized (mutex) { verifyRelocatingState(); replicationTracker.completeRelocationHandoff(); // make changes to primaryMode and relocated flag only under mutex } } catch (final Exception e) { try { replicationTracker.abortRelocationHandoff(); } catch (final Exception inner) { e.addSuppressed(inner); } throw e; } }); } catch (TimeoutException e) { logger.warn("timed out waiting for relocation hand-off to complete"); // This is really bad as ongoing replication operations are preventing this shard from completing relocation hand-off. // Fail primary relocation source and target shards. failShard("timed out waiting for relocation hand-off to complete", null); throw new IndexShardClosedException(shardId(), "timed out waiting for relocation hand-off to complete"); } finally { refreshListeners.allowAdd(); } }	could this be concurrently called? what should the semantics then be? i think we'll need a refreshforcers counter in refreshlisteners, a simple boolean might not suffice
* @param consumer a {@link Runnable} that is executed after operations are blocked * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation * @throws InterruptedException if blocking operations is interrupted */ public void relocated(final Consumer<ReplicationTracker.PrimaryContext> consumer) throws IllegalIndexShardStateException, InterruptedException { assert shardRouting.primary() : "only primaries can be marked as relocated: " + shardRouting; refreshListeners.disallowAdd(); try { if (refreshListeners.refreshNeeded()) { refresh("relocated"); } indexShardOperationPermits.blockOperations(30, TimeUnit.MINUTES, () -> { // no shard operation permits are being held here, move state from started to relocated assert indexShardOperationPermits.getActiveOperationsCount() == 0 : "in-flight operations in progress while moving shard state to relocated"; /* * We should not invoke the runnable under the mutex as the expected implementation is to handoff the primary context via a * network operation. Doing this under the mutex can implicitly block the cluster state update thread on network operations. */ verifyRelocatingState(); final ReplicationTracker.PrimaryContext primaryContext = replicationTracker.startRelocationHandoff(); try { consumer.accept(primaryContext); synchronized (mutex) { verifyRelocatingState(); replicationTracker.completeRelocationHandoff(); // make changes to primaryMode and relocated flag only under mutex } } catch (final Exception e) { try { replicationTracker.abortRelocationHandoff(); } catch (final Exception inner) { e.addSuppressed(inner); } throw e; } }); } catch (TimeoutException e) { logger.warn("timed out waiting for relocation hand-off to complete"); // This is really bad as ongoing replication operations are preventing this shard from completing relocation hand-off. // Fail primary relocation source and target shards. failShard("timed out waiting for relocation hand-off to complete", null); throw new IndexShardClosedException(shardId(), "timed out waiting for relocation hand-off to complete"); } finally { refreshListeners.allowAdd(); } }	as we always want to do the refresh after calling disallowadd, i wonder if we should combine both into one method
public void testStopAndRestartCompositeDatafeed() throws Exception { client().admin() .cluster() .prepareUpdateSettings() .setTransientSettings(Settings.builder() .put("logger.org.elasticsearch.xpack.ml.datafeed", "TRACE") .build()).get(); String indexName = "stop-restart-data"; client().admin().indices().prepareCreate("stop-restart-data") .setMapping("time", "type=date") .get(); long numDocs = randomIntBetween(32, 2048); long now = Intervals.alignToFloor(System.currentTimeMillis(), TimeValue.timeValueHours(1).millis()) + 1; long oneWeekAgo = now - 604800000; long twoWeeksAgo = oneWeekAgo - 604800000; indexDocs(logger, indexName, numDocs, twoWeeksAgo, oneWeekAgo); long numDocs2 = randomIntBetween(32, 2048); indexDocs(logger, indexName, numDocs2, oneWeekAgo, now); client().admin().cluster().prepareHealth(indexName).setWaitForYellowStatus().get(); String scrollJobId = "stop-restart-scroll"; Job.Builder scrollJob = createScheduledJob(scrollJobId); putJob(scrollJob); openJob(scrollJobId); assertBusy(() -> assertEquals(getJobStats(scrollJobId).get(0).getState(), JobState.OPENED)); DatafeedConfig datafeedConfig = createDatafeedBuilder(scrollJobId+ "-datafeed", scrollJobId, Collections.singletonList(indexName)) .setChunkingConfig(ChunkingConfig.newManual(new TimeValue(1, TimeUnit.SECONDS))) .build(); putDatafeed(datafeedConfig); startDatafeed(datafeedConfig.getId(), 0L, null); // Wait until we have processed data assertBusy(() -> assertThat(getDataCounts(scrollJobId).getProcessedRecordCount(), greaterThan(0L))); stopDatafeed(datafeedConfig.getId()); assertBusy(() -> assertThat(getJobStats(scrollJobId).get(0).getState(), is(oneOf(JobState.CLOSED, JobState.OPENED)))); // If we are not OPENED, then we are closed and shouldn't restart as the datafeed finished running through the data if (getJobStats(scrollJobId).get(0).getState().equals(JobState.OPENED)) { updateDatafeed(new DatafeedUpdate.Builder().setId(datafeedConfig.getId()).setChunkingConfig(ChunkingConfig.newAuto()).build()); startDatafeed( datafeedConfig.getId(), randomLongBetween(0, getDataCounts(scrollJobId).getLatestRecordTimeStamp().getTime()), now ); waitUntilJobIsClosed(scrollJobId); } assertBusy(() -> { DataCounts dataCounts = getDataCounts(scrollJobId); assertThat(dataCounts.getProcessedRecordCount(), equalTo(numDocs + numDocs2)); assertThat(dataCounts.getOutOfOrderTimeStampCount(), equalTo(0L)); }, 60, TimeUnit.SECONDS); String compositeJobId = "stop-restart-composite"; Job.Builder compositeJob = createScheduledJob(compositeJobId); compositeJob.setAnalysisConfig( new AnalysisConfig.Builder(compositeJob.getAnalysisConfig()).setSummaryCountFieldName("doc_count") ); putJob(compositeJob); openJob(compositeJobId); assertBusy(() -> assertEquals(getJobStats(compositeJobId).get(0).getState(), JobState.OPENED)); AggregatorFactories.Builder aggs = new AggregatorFactories.Builder(); aggs.addAggregator( AggregationBuilders.composite( "buckets", Collections.singletonList( new DateHistogramValuesSourceBuilder("timebucket") .fixedInterval(new DateHistogramInterval("1h")) .field("time") ) // Set size to 1 so that start stop actually doesn't page through all the results too quickly ).subAggregation(AggregationBuilders.max("time").field("time")).size(1) ); DatafeedConfig compositeDatafeedConfig = createDatafeedBuilder( compositeJobId + "-datafeed", compositeJobId, Collections.singletonList(indexName)) .setParsedAggregations(aggs) .setFrequency(TimeValue.timeValueHours(1)) .build(); putDatafeed(compositeDatafeedConfig); startDatafeed(compositeDatafeedConfig.getId(), 0L, null); // Wait until we have processed data assertBusy(() -> assertThat(getDataCounts(compositeJobId).getProcessedRecordCount(), greaterThan(0L))); stopDatafeed(compositeDatafeedConfig.getId()); assertBusy(() -> assertThat(getJobStats(compositeJobId).get(0).getState(), is(oneOf(JobState.CLOSED, JobState.OPENED))) ); // If we are not OPENED, then we are closed and shouldn't restart as the datafeed finished running through the data if (getJobStats(compositeJobId).get(0).getState().equals(JobState.OPENED)) { aggs = new AggregatorFactories.Builder(); aggs.addAggregator( AggregationBuilders.composite( "buckets", Collections.singletonList( new DateHistogramValuesSourceBuilder("timebucket") .fixedInterval(new DateHistogramInterval("1h")) .field("time") ) ).subAggregation(AggregationBuilders.max("time").field("time")).size(100) ); updateDatafeed(new DatafeedUpdate.Builder() .setId(compositeDatafeedConfig.getId()) .setParsedAggregations(aggs) .build()); startDatafeed( compositeDatafeedConfig.getId(), randomLongBetween(0, getDataCounts(compositeJobId).getLatestRecordTimeStamp().getTime()), now ); waitUntilJobIsClosed(compositeJobId); } List<Bucket> scrollBuckets = getBuckets(scrollJobId); List<Bucket> compositeBuckets = getBuckets(compositeJobId); assertThat( "scroll bucket size " + scrollBuckets + " does not equal composite bucket size" + compositeBuckets, compositeBuckets.size(), equalTo(scrollBuckets.size()) ); for (int i = 0; i < scrollBuckets.size(); i++) { Bucket scrollBucket = scrollBuckets.get(i); Bucket compositeBucket = compositeBuckets.get(i); try { assertThat("scroll buckets " + scrollBuckets + " composite buckets " + compositeBuckets, compositeBucket.getTimestamp(), equalTo(scrollBucket.getTimestamp()) ); assertThat( "composite bucket [" + compositeBucket.getTimestamp() + "] [" + compositeBucket.getEventCount() + "] does not equal" + " scroll bucket [" + scrollBucket.getTimestamp() + "] [" + scrollBucket.getEventCount() + "]", compositeBucket.getEventCount(), equalTo(scrollBucket.getEventCount()) ); } catch (AssertionError ae) { String originalMessage = ae.getMessage(); try { SearchSourceBuilder builder = new SearchSourceBuilder().query(QueryBuilders.rangeQuery("time") .gte(scrollBucket.getTimestamp().getTime()) .lte(scrollBucket.getTimestamp().getTime() + TimeValue.timeValueHours(1).getMillis())) .size(10_000); SearchHits hits = client().search(new SearchRequest() .indices(indexName) .source(builder)).actionGet().getHits(); fail("Hits: " + Strings.arrayToDelimitedString(hits.getHits(), "\\\\n") + " \\\\n failure: " + originalMessage); } catch (ElasticsearchException ee) { fail("could not search indices for better info. Original failure: " + originalMessage); } } } }	this is still causing a failure :(((. trying to figure out how to make this deterministic.
@Override public InternalAggregation buildAggregation(long owningBucketOrdinal) { Object aggregation; if (combineScript != null) { aggregation = combineScript.execute(); CollectionUtils.ensureNoSelfReferences(aggregation, "Scripted metric aggs combine script"); } else { aggregation = aggState; } StreamOutput.checkWriteable(aggregation); return new InternalScriptedMetric(name, aggregation, reduceScript, metadata()); }	i wonder if we could/should combine this with org.elasticsearch.common.util.collectionutils#ensurenoselfreferences(java.lang.object, java.lang.string) somehow? it seems like technically we would want to use this in all the spots that we currently also use that method to validate script returns (maybe i'm missing some detail and we wouldn't need to do this kind of validation in other script return spots and only need the no-self-references there for some reason?). that way we'd save iterating through the object to validate twice?
public boolean skip(Version currentVersion) { if (isEmpty()) { return false; } boolean skip = false; for (VersionRange versionRange : versionRanges) { skip |= versionRange.contain(currentVersion); } skip |= Features.areAllSupported(features) == false; return skip; }	what about: java boolean skip = versionranges.stream().anymatch(range -> range.contain(currentversion)); return skip || features.areallsupported(features) == false;
static List<VersionRange> parseVersionRanges(String rawRanges) { if (rawRanges == null) { return Collections.singletonList(new VersionRange(null, null)); } if (rawRanges.trim().equals("all")) { return Collections.singletonList(new VersionRange(VersionUtils.getFirstVersion(), Version.CURRENT)); } String[] ranges = rawRanges.split(","); List<VersionRange> versionRanges = new ArrayList<>(); for (String rawRange : ranges) { String[] skipVersions = wrapWithSpaces(rawRange).split("-"); if (skipVersions.length > 2) { throw new IllegalArgumentException("version range malformed: " + rawRanges); } String lower = skipVersions[0].trim(); String upper = skipVersions[1].trim(); VersionRange versionRange = new VersionRange( lower.isEmpty() ? VersionUtils.getFirstVersion() : Version.fromString(lower), upper.isEmpty() ? Version.CURRENT : Version.fromString(upper) ); versionRanges.add(versionRange); } return versionRanges; }	i don't understand why wrapwithspaces is necessary here?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); if (docAsUpsert) { builder.field("doc_as_upsert", docAsUpsert); } if (doc != null) { XContentType xContentType = doc.getContentType(); try (XContentParser parser = XContentHelper.createParser(NamedXContentRegistry.EMPTY, doc.source(), xContentType)) { builder.field("doc"); builder.copyCurrentStructure(parser); } } if (script != null) { builder.field("script", script); } if (upsertRequest != null) { XContentType xContentType = upsertRequest.getContentType(); try (XContentParser parser = XContentHelper.createParser(NamedXContentRegistry.EMPTY, upsertRequest.source(), xContentType)) { builder.field("upsert"); builder.copyCurrentStructure(parser); } } if (scriptedUpsert) { builder.field("scripted_upsert", scriptedUpsert); } if (detectNoop == false) { builder.field("detect_noop", detectNoop); } if (fetchSourceContext != null) { builder.field("_source", fetchSourceContext); } builder.endObject(); return builder; }	shouldn't we print out fields as well?
@Test public void testRequiredRoutingWithPathNumericType() throws Exception { client().admin().indices().prepareCreate("test") .addMapping("type1", XContentFactory.jsonBuilder().startObject().startObject("type1") .startObject("_routing").field("required", true).field("path", "routing_field").endObject() .startObject("properties") .startObject("routing_field") .field("type", "long") .field("doc_values", false) // TODO this test fails with doc values https://github.com/elasticsearch/elasticsearch/pull/5858 .endObject() .endObject() .endObject().endObject()) .execute().actionGet(); ensureGreen(); logger.info("--> indexing with id [1], and routing [0]"); client().prepareIndex("test", "type1", "1").setSource("field", "value1", "routing_field", 0).execute().actionGet(); client().admin().indices().prepareRefresh().execute().actionGet(); logger.info("--> verifying get with no routing, should fail"); for (int i = 0; i < 5; i++) { try { client().prepareGet("test", "type1", "1").execute().actionGet().isExists(); fail(); } catch (RoutingMissingException e) { assertThat(e.status(), equalTo(RestStatus.BAD_REQUEST)); assertThat(e.getMessage(), equalTo("routing is required for [test]/[type1]/[1]")); } } logger.info("--> verifying get with routing, should find"); for (int i = 0; i < 5; i++) { assertThat(client().prepareGet("test", "type1", "1").setRouting("0").execute().actionGet().isExists(), equalTo(true)); } }	maybe an @awaitsfix here instead of todo? but i see that the test fails only with doc values and you are probably going to fix it right after pushig this pr.
private void randomIndexTemplate() throws IOException { // TODO move settings for random directory etc here into the index based randomized settings. if (immutableCluster().size() > 0) { ImmutableSettings.Builder randomSettingsBuilder = setRandomSettings(getRandom(), ImmutableSettings.builder()) .put(SETTING_INDEX_SEED, getRandom().nextLong()); if (randomizeNumberOfShardsAndReplicas()) { randomSettingsBuilder.put(SETTING_NUMBER_OF_SHARDS, between(DEFAULT_MIN_NUM_SHARDS, DEFAULT_MAX_NUM_SHARDS)) //use either 0 or 1 replica, yet a higher amount when possible, but only rarely .put(SETTING_NUMBER_OF_REPLICAS, between(0, getRandom().nextInt(10) > 0 ? 1 : immutableCluster().numDataNodes() - 1)); } XContentBuilder mappings = null; if (frequently() && randomDynamicTemplates()) { mappings = XContentFactory.jsonBuilder().startObject().startObject("_default_"); if (randomBoolean()) { mappings.startObject(IdFieldMapper.NAME) .field("index", randomFrom("not_analyzed", "no")) .endObject(); } mappings.startArray("dynamic_templates") .startObject() .startObject("template-strings") .field("match", "*") .field("match_mapping_type", "string") .startObject("mapping") .startObject("fielddata") .field(FieldDataType.FORMAT_KEY, randomFrom("paged_bytes", "fst")) // unfortunately doc values only work on not_analyzed fields .field(Loading.KEY, randomFrom(Loading.values())) .endObject() .endObject() .endObject() .endObject() .startObject() .startObject("template-longs") .field("match", "*") .field("match_mapping_type", "long") .startObject("mapping") .startObject("fielddata") .field(FieldDataType.FORMAT_KEY, randomFrom("array", "doc_values")) .field(Loading.KEY, randomFrom(Loading.LAZY, Loading.EAGER)) .endObject() .endObject() .endObject() .endObject() .startObject() .startObject("template-doubles") .field("match", "*") .field("match_mapping_type", "double") .startObject("mapping") .startObject("fielddata") .field(FieldDataType.FORMAT_KEY, randomFrom("array", "doc_values")) .field(Loading.KEY, randomFrom(Loading.LAZY, Loading.EAGER)) .endObject() .endObject() .endObject() .endObject() .startObject() .startObject("template-geo_points") .field("match", "*") .field("match_mapping_type", "geo_point") .startObject("mapping") .startObject("fielddata") .field(FieldDataType.FORMAT_KEY, randomFrom("array", "doc_values")) .field(Loading.KEY, randomFrom(Loading.LAZY, Loading.EAGER)) .endObject() .endObject() .endObject() .endObject() .endArray(); mappings.endObject().endObject(); } PutIndexTemplateRequestBuilder putTemplate = client().admin().indices() .preparePutTemplate("random_index_template") .setTemplate("*") .setOrder(0) .setSettings(randomSettingsBuilder); if (mappings != null) { putTemplate.addMapping("_default_", mappings); } assertAcked(putTemplate.execute().actionGet()); } }	would be great to find a way to use doc values for not_analyzed strings as well... cannot come up with any idea for now though as the default is analyzed :)
public static void main(String[] args) throws IOException { Class<?> testClass = null; Class<?> integTestClass = null; String rootPathList = null; boolean skipIntegTestsInDisguise = false; boolean selfTest = false; boolean checkMainClasses = false; for (int i = 0; i < args.length; i++) { String arg = args[i]; switch (arg) { case "--test-class": testClass = loadClassWithoutInitializing(args[++i]); break; case "--integ-test-class": integTestClass = loadClassWithoutInitializing(args[++i]); break; case "--skip-integ-tests-in-disguise": skipIntegTestsInDisguise = true; break; case "--self-test": selfTest = true; break; case "--main": checkMainClasses = true; break; case "--": rootPathList = args[++i]; break; default: fail("unsupported argument '" + arg + "'"); } } NamingConventionsCheck check = new NamingConventionsCheck(testClass, integTestClass); for (String rootDir : rootPathList.split(Pattern.quote(File.pathSeparator))) { Path rootPath = Paths.get(rootDir); if (checkMainClasses) { check.checkMain(rootPath); } else { check.checkTests(rootPath, skipIntegTestsInDisguise); } } if (selfTest) { if (checkMainClasses) { assertViolation(NamingConventionsCheckInMainTests.class.getName(), check.testsInMain); assertViolation(NamingConventionsCheckInMainIT.class.getName(), check.testsInMain); } else { assertViolation("WrongName", check.missingSuffix); assertViolation("WrongNameTheSecond", check.missingSuffix); assertViolation("DummyAbstractTests", check.notRunnable); assertViolation("DummyInterfaceTests", check.notRunnable); assertViolation("InnerTests", check.innerClasses); assertViolation("NotImplementingTests", check.notImplementing); assertViolation("PlainUnit", check.pureUnitTest); } } // Now we should have no violations assertNoViolations( "Not all subclasses of " + check.testClass.getSimpleName() + " match the naming convention. Concrete classes must end with [Tests]", check.missingSuffix); assertNoViolations("Classes ending with [Tests] are abstract or interfaces", check.notRunnable); assertNoViolations("Found inner classes that are tests, which are excluded from the test runner", check.innerClasses); assertNoViolations("Pure Unit-Test found must subclass [" + check.testClass.getSimpleName() + "]", check.pureUnitTest); assertNoViolations("Classes ending with [Tests] must subclass [" + check.testClass.getSimpleName() + "]", check.notImplementing); assertNoViolations( "Classes ending with [Tests] or [IT] or extending [" + check.testClass.getSimpleName() + "] must be in src/test/java", check.testsInMain); if (skipIntegTestsInDisguise == false) { assertNoViolations( "Subclasses of " + check.integTestClass.getSimpleName() + " should end with IT as they are integration tests", check.integTestsInDisguise); } }	this seems inconsistent with the gradle code. there, a single dir is currently passed in. but here it looks like the path of that dir is split into its elements but then each read as a dir...
public void resolve(LDAPInterface connection, String userDn, TimeValue timeout, Logger logger, Collection<Attribute> attributes, ActionListener<List<String>> listener) { buildGroupQuery(connection, userDn, timeout, ignoreReferralErrors, ActionListener.wrap((filter) -> { if (filter == null) { listener.onResponse(List.of()); } else { logger.debug("group SID to DN [{}] search filter: [{}]", userDn, filter); search(connection, baseDn, scope.scope(), filter, Math.toIntExact(timeout.seconds()), ignoreReferralErrors, ActionListener.wrap((results) -> { List<String> groups = results.stream() .map(SearchResultEntry::getDN) .collect(Collectors.toUnmodifiableList()); listener.onResponse(groups); }, listener::onFailure), SearchRequest.NO_ATTRIBUTES); } }, listener::onFailure)); }	per my comment above, i think we need to decide whether we want to switch these across the board. i don't see any reason to do so, but i'm not specifically opposed to it.
static ClaimParser forSetting(Logger logger, OpenIdConnectRealmSettings.ClaimSetting setting, RealmConfig realmConfig, boolean required) { if (realmConfig.hasSetting(setting.getClaim())) { String claimName = realmConfig.getSetting(setting.getClaim()); if (realmConfig.hasSetting(setting.getPattern())) { Pattern regex = Pattern.compile(realmConfig.getSetting(setting.getPattern())); return new ClaimParser( "OpenID Connect Claim [" + claimName + "] with pattern [" + regex.pattern() + "] for [" + setting.name(realmConfig) + "]", claims -> { Object claimValueObject = claims.getClaim(claimName); List<String> values; if (claimValueObject == null) { values = List.of(); } else if (claimValueObject instanceof String) { values = List.of((String) claimValueObject); } else if (claimValueObject instanceof List) { values = (List<String>) claimValueObject; } else { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + " expects a claim with String or a String List value but found a " + claimValueObject.getClass().getName()); } return values.stream().map(s -> { if (s == null) { logger.debug("OpenID Connect Claim [{}] is null", claimName); return null; } final Matcher matcher = regex.matcher(s); if (matcher.find() == false) { logger.debug("OpenID Connect Claim [{}] is [{}], which does not match [{}]", claimName, s, regex.pattern()); return null; } final String value = matcher.group(1); if (Strings.isNullOrEmpty(value)) { logger.debug("OpenID Connect Claim [{}] is [{}], which does match [{}] but group(1) is empty", claimName, s, regex.pattern()); return null; } return value; }).filter(Objects::nonNull).collect(Collectors.toUnmodifiableList()); }); } else { return new ClaimParser( "OpenID Connect Claim [" + claimName + "] for [" + setting.name(realmConfig) + "]", claims -> { Object claimValueObject = claims.getClaim(claimName); if (claimValueObject == null) { return List.of(); } else if (claimValueObject instanceof String) { return List.of((String) claimValueObject); } else if (claimValueObject instanceof List == false) { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + " expects a claim with String or a String List value but found a " + claimValueObject.getClass().getName()); } return ((List<String>) claimValueObject).stream() .filter(Objects::nonNull) .collect(Collectors.toUnmodifiableList()); }); } } else if (required) { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + "] is required"); } else if (realmConfig.hasSetting(setting.getPattern())) { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getPattern()) + "] cannot be set unless [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + "] is also set"); } else { return new ClaimParser("No OpenID Connect Claim for [" + setting.name(realmConfig) + "]", attributes -> List.of()); } }	i'll defer to @jkakavas on this, but i think the original message was correct. this is refering to claims in the oidc json, and the json term for a multi-valued element is an array, not a list.
static ClaimParser forSetting(Logger logger, OpenIdConnectRealmSettings.ClaimSetting setting, RealmConfig realmConfig, boolean required) { if (realmConfig.hasSetting(setting.getClaim())) { String claimName = realmConfig.getSetting(setting.getClaim()); if (realmConfig.hasSetting(setting.getPattern())) { Pattern regex = Pattern.compile(realmConfig.getSetting(setting.getPattern())); return new ClaimParser( "OpenID Connect Claim [" + claimName + "] with pattern [" + regex.pattern() + "] for [" + setting.name(realmConfig) + "]", claims -> { Object claimValueObject = claims.getClaim(claimName); List<String> values; if (claimValueObject == null) { values = List.of(); } else if (claimValueObject instanceof String) { values = List.of((String) claimValueObject); } else if (claimValueObject instanceof List) { values = (List<String>) claimValueObject; } else { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + " expects a claim with String or a String List value but found a " + claimValueObject.getClass().getName()); } return values.stream().map(s -> { if (s == null) { logger.debug("OpenID Connect Claim [{}] is null", claimName); return null; } final Matcher matcher = regex.matcher(s); if (matcher.find() == false) { logger.debug("OpenID Connect Claim [{}] is [{}], which does not match [{}]", claimName, s, regex.pattern()); return null; } final String value = matcher.group(1); if (Strings.isNullOrEmpty(value)) { logger.debug("OpenID Connect Claim [{}] is [{}], which does match [{}] but group(1) is empty", claimName, s, regex.pattern()); return null; } return value; }).filter(Objects::nonNull).collect(Collectors.toUnmodifiableList()); }); } else { return new ClaimParser( "OpenID Connect Claim [" + claimName + "] for [" + setting.name(realmConfig) + "]", claims -> { Object claimValueObject = claims.getClaim(claimName); if (claimValueObject == null) { return List.of(); } else if (claimValueObject instanceof String) { return List.of((String) claimValueObject); } else if (claimValueObject instanceof List == false) { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + " expects a claim with String or a String List value but found a " + claimValueObject.getClass().getName()); } return ((List<String>) claimValueObject).stream() .filter(Objects::nonNull) .collect(Collectors.toUnmodifiableList()); }); } } else if (required) { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + "] is required"); } else if (realmConfig.hasSetting(setting.getPattern())) { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getPattern()) + "] cannot be set unless [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + "] is also set"); } else { return new ClaimParser("No OpenID Connect Claim for [" + setting.name(realmConfig) + "]", attributes -> List.of()); } }	as above (though i wonder why we have duplicated code here)
private void buildUser(X509AuthenticationToken token, ActionListener<AuthenticationResult> listener) { final Map<String, Object> metadata = Map.of("pki_dn", token.dn()); final UserRoleMapper.UserData userData = new UserRoleMapper.UserData(token.principal(), token.dn(), Set.of(), metadata, config); roleMapper.resolveRoles(userData, ActionListener.wrap(roles -> { final User computedUser = new User(token.principal(), roles.toArray(new String[roles.size()]), null, null, metadata, true); listener.onResponse(AuthenticationResult.success(computedUser)); }, listener::onFailure)); }	similar to my comments on empty collections, i think we need to reach a team-wide decision on singletonxxx vs of
String name() { return name; } /** * Returns the {@link MappedFieldType}	there is a duplicated the in the comment.
private static XContentBuilder getProperties(XContentBuilder builder, RollupActionConfig config) throws IOException { builder.startObject("properties"); RollupActionGroupConfig groupConfig = config.getGroupConfig(); RollupActionDateHistogramGroupConfig dateHistogramConfig = groupConfig.getDateHistogram(); String dateField = dateHistogramConfig.getField(); String dateIntervalType = dateHistogramConfig.getIntervalTypeName(); String dateInterval = dateHistogramConfig.getInterval().toString(); String tz = dateHistogramConfig.getTimeZone() != null ? dateHistogramConfig.getTimeZone() : RollupActionDateHistogramGroupConfig.DEFAULT_TIMEZONE; builder.startObject(dateField).field("type", DateFieldMapper.CONTENT_TYPE) .startObject("meta") .field(dateIntervalType, dateInterval) .field(RollupActionDateHistogramGroupConfig.CalendarInterval.TIME_ZONE, tz) .endObject() .endObject(); HistogramGroupConfig histogramGroupConfig = groupConfig.getHistogram(); if (histogramGroupConfig != null) { for (String field : histogramGroupConfig.getFields()) { builder.startObject(field).field("type", NumberFieldMapper.NumberType.DOUBLE.typeName()) .startObject("meta") .field(HistogramGroupConfig.INTERVAL, String.valueOf(histogramGroupConfig.getInterval())) .endObject() .endObject(); } } List<MetricConfig> metricConfigs = config.getMetricsConfig(); for (MetricConfig metricConfig : metricConfigs) { List<String> metrics = FieldMetricsProducer.normalizeMetrics(metricConfig.getMetrics()); String defaultMetric = metrics.contains("value_count") ? "value_count" : metrics.get(0); builder.startObject(metricConfig.getField()) .field("type", AggregateDoubleMetricFieldMapper.CONTENT_TYPE) .array(AggregateDoubleMetricFieldMapper.Names.METRICS, metrics.toArray()) .field(AggregateDoubleMetricFieldMapper.Names.DEFAULT_METRIC, defaultMetric) .endObject(); } return builder.endObject(); }	i am not sure this needs to be in an integration test, or a unit, but i think we should assert the expected mapping with the respective metadata in our tests. i may be mistaken, but i do not see this.
private void markNodeDataDirsAsPendingForWipe(Node node) { assert Thread.holdsLock(this); NodeEnvironment nodeEnv = getInstanceFromNode(NodeEnvironment.class, node); if (nodeEnv.hasNodeFile()) { dataDirToClean.addAll(Arrays.asList(nodeEnv.nodeDataPaths())); } }	while we are at it maybe add a node.getnodeenvironment() method that doesn't go through guice?
* @param properties properties for this setting like scope, filtering... */ public Setting(String key, Function<Settings, String> defaultValue, Function<String, T> parser, Property... properties) { this(new SimpleKey(key), defaultValue, parser, properties); } /** * Creates a new Setting instance * @param key the settings key for this setting. * @param fallbackSetting a setting who's value to fallback on if this setting is not defined * @param parser a parser that parses the string rep into a complex datatype. * @param properties properties for this setting like scope, filtering... */ public Setting(Key key, Setting<T> fallbackSetting, Function<String, T> parser, Property... properties) { this(key, fallbackSetting, fallbackSetting::getRaw, parser, (v, m) -> {}, properties); } /** * Creates a new Setting instance * @param key the settings key for this setting. * @param fallBackSetting a setting to fall back to if the current setting is not set. * @param parser a parser that parses the string rep into a complex datatype. * @param properties properties for this setting like scope, filtering... */ public Setting(String key, Setting<T> fallBackSetting, Function<String, T> parser, Property... properties) { this(new SimpleKey(key), fallBackSetting, parser, properties); } /** * Returns the settings key or a prefix if this setting is a group setting. * <b>Note: this method should not be used to retrieve a value from a {@link Settings} object. * Use {@link #get(Settings)} instead</b> * * @see #isGroupSetting() */ public final String getKey() { return key.toString(); }	remove empty return in javadocs
* @param hash the char array with the hash against which the string is verified * @return true if the hash corresponds to the data, false otherwise */ public static boolean verifyHash(SecureString data, char[] hash) { try { final Hasher hasher = resolveFromHash(hash); return hasher.verify(data, hash); } catch (IllegalArgumentException e) { // The password hash format is invalid, we're unable to verify password return false; } }	lets set these to null. we never use these arrays. we can have null checks in the finally block.
private static byte[] generateSalt(int length) { Random random = new SecureRandom(); byte[] salt = new byte[length]; random.nextBytes(salt); return salt; }	can we store the value? securerandom objects can be reused and we don't need to keep creating them
public void testRecoverWhileRelocating() throws Exception { final int numShards = between(2, 5); final int numReplicas = 0; logger.info("--> creating test index ..."); int allowNodes = 2; assertAcked(prepareCreate("test", 3, Settings.builder() .put(SETTING_NUMBER_OF_SHARDS, numShards) .put(SETTING_NUMBER_OF_REPLICAS, numReplicas) .put(IndexSettings.INDEX_TRANSLOG_DURABILITY_SETTING.getKey(), Translog.Durability.ASYNC) .put(IndexService.RETENTION_LEASE_SYNC_INTERVAL_SETTING.getKey(), randomFrom("100ms")))); final int numDocs = scaledRandomIntBetween(200, 9999); try (BackgroundIndexer indexer = new BackgroundIndexer("test", "type", client(), numDocs)) { for (int i = 0; i < numDocs; i += scaledRandomIntBetween(100, Math.min(1000, numDocs))) { indexer.assertNoFailures(); logger.info("--> waiting for {} docs to be indexed ...", i); waitForDocs(i, indexer); logger.info("--> {} docs indexed", i); allowNodes = 2 / allowNodes; allowNodes("test", allowNodes); logger.info("--> waiting for GREEN health status ..."); ensureGreen(TimeValue.timeValueMinutes(5)); } logger.info("--> marking and waiting for indexing threads to stop ..."); indexer.stop(); logger.info("--> indexing threads stopped"); logger.info("--> bump up number of replicas to 1 and allow all nodes to hold the index"); allowNodes("test", 3); assertAcked(client().admin().indices().prepareUpdateSettings("test") .setSettings(Settings.builder().put("number_of_replicas", 1)).get()); ensureGreen(TimeValue.timeValueMinutes(5)); logger.info("--> refreshing the index"); refreshAndAssert(); logger.info("--> verifying indexed content"); iterateAssertCount(numShards, 10, indexer.getIds()); } }	there isn't any randomization here? inadvertently left over from testing, you meant to also include the default?
public void testUserWithNoRolesOpenPointInTimeWithRemoteIndices() { final boolean hasLocalIndices = randomBoolean(); final String[] indices = new String[]{ hasLocalIndices ? randomAlphaOfLength(5) : "other_cluster:" + randomAlphaOfLength(5), "other_cluster:" + randomAlphaOfLength(5) }; final OpenPointInTimeRequest openPointInTimeRequest = new OpenPointInTimeRequest( indices, OpenPointInTimeRequest.DEFAULT_INDICES_OPTIONS, TimeValue.timeValueMinutes(randomLongBetween(1, 10)), randomAlphaOfLength(5), randomAlphaOfLength(5) ); final Authentication authentication = createAuthentication(new User("test user")); mockEmptyMetadata(); final String requestId = AuditUtil.getOrGenerateRequestId(threadContext); if (hasLocalIndices) { assertThrowsAuthorizationException( () -> authorize(authentication, OpenPointInTimeAction.NAME, openPointInTimeRequest), "indices:data/read/open_point_in_time", "test user" ); verify(auditTrail).accessDenied(eq(requestId), eq(authentication), eq("indices:data/read/open_point_in_time"), eq(openPointInTimeRequest), authzInfoRoles(Role.EMPTY.names())); } else { authorize(authentication, OpenPointInTimeAction.NAME, openPointInTimeRequest); verify(auditTrail).accessGranted(eq(requestId), eq(authentication), eq(OpenPointInTimeAction.NAME), eq(openPointInTimeRequest), authzInfoRoles(Role.EMPTY.names())); } verifyNoMoreInteractions(auditTrail); }	set this to both values in a for-each.
public void testUserWithNoRolesOpenPointInTimeWithRemoteIndices() { final boolean hasLocalIndices = randomBoolean(); final String[] indices = new String[]{ hasLocalIndices ? randomAlphaOfLength(5) : "other_cluster:" + randomAlphaOfLength(5), "other_cluster:" + randomAlphaOfLength(5) }; final OpenPointInTimeRequest openPointInTimeRequest = new OpenPointInTimeRequest( indices, OpenPointInTimeRequest.DEFAULT_INDICES_OPTIONS, TimeValue.timeValueMinutes(randomLongBetween(1, 10)), randomAlphaOfLength(5), randomAlphaOfLength(5) ); final Authentication authentication = createAuthentication(new User("test user")); mockEmptyMetadata(); final String requestId = AuditUtil.getOrGenerateRequestId(threadContext); if (hasLocalIndices) { assertThrowsAuthorizationException( () -> authorize(authentication, OpenPointInTimeAction.NAME, openPointInTimeRequest), "indices:data/read/open_point_in_time", "test user" ); verify(auditTrail).accessDenied(eq(requestId), eq(authentication), eq("indices:data/read/open_point_in_time"), eq(openPointInTimeRequest), authzInfoRoles(Role.EMPTY.names())); } else { authorize(authentication, OpenPointInTimeAction.NAME, openPointInTimeRequest); verify(auditTrail).accessGranted(eq(requestId), eq(authentication), eq(OpenPointInTimeAction.NAME), eq(openPointInTimeRequest), authzInfoRoles(Role.EMPTY.names())); } verifyNoMoreInteractions(auditTrail); }	i would also exercise the condition that remote indices are wildcards (it could be a random)
@TaskAction public void validate(InputChanges inputChanges) throws IOException { final File jsonSchemaOnDisk = getJsonSchema(); final JsonSchema jsonSchema = buildSchemaObject(jsonSchemaOnDisk); final Map<File, Set<String>> errors = new LinkedHashMap<>(); final ObjectMapper mapper = this.getMapper(); // incrementally evaluate input files // validate all files and hold on to errors for a complete report if there are failures StreamSupport.stream(inputChanges.getFileChanges(getInputFiles()).spliterator(), false) .filter(f -> f.getChangeType() != ChangeType.REMOVED) .map(FileChange::getFile) .filter(file -> file.isDirectory() == false) .forEach(file -> { getLogger().debug("Validating {} [{}]", getFileType(), file.getName()); try { Set<ValidationMessage> validationMessages = jsonSchema.validate(mapper.readTree(file)); maybeLogAndCollectError(validationMessages, errors, file); } catch (IOException e) { throw new UncheckedIOException(e); } }); if (errors.isEmpty()) { Files.writeString(getReport().toPath(), "Success! No validation errors found.", StandardOpenOption.CREATE); } else { try (PrintWriter printWriter = new PrintWriter(getReport())) { printWriter.printf("Schema: %s%n", jsonSchemaOnDisk); printWriter.println("----------Validation Errors-----------"); errors.values().stream().flatMap(Collection::stream).forEach(printWriter::println); } StringBuilder sb = new StringBuilder(); sb.append("Error validating JSON. See the report at: "); sb.append(getReport().toURI().toASCIIString()); sb.append(System.lineSeparator()); sb.append( String.format("Verification failed: %d files contained %d violations", errors.keySet().size(), errors.values().size()) ); throw new JsonSchemaException(sb.toString()); } }	fwiw, i find debug level logging mostly useless. if you pass --debug to a gradle build you encounter an absolute avalanche of logging. if we think this will _actually_ be helpful let's drop it down to info or remove it entirely.
public void testLabels() { Version v = Version.fromString("1.2.3"); assertThat(v.getLabels(), empty()); v = Version.fromString("1.2.3-rc1"); assertThat(v.getLabels(), contains("rc1")); v = Version.fromString("1.2.3-rc1-SNAPSHOT"); assertThat(v.getLabels(), contains("rc1", "SNAPSHOT")); v = Version.fromString("1.2.3-SNAPSHOT"); assertThat(v.getLabels(), contains("SNAPSHOT")); }	is this a thing? why would we have multiple labels here? wouldn't 1.2.3-rc1-snapshot be identical to 1.2.3-snapshot? in what circumstance would such a version ever exist?
private void addSortField(SearchContext context, List<SortField> sortFields, String fieldName, boolean reverse, String ignoreUnmapped, @Nullable final String missing, MultiValueMode sortMode, String nestedPath, Filter nestedFilter) { if (SCORE_FIELD_NAME.equals(fieldName)) { if (reverse) { sortFields.add(SORT_SCORE_REVERSE); } else { sortFields.add(SORT_SCORE); } } else if (DOC_FIELD_NAME.equals(fieldName)) { if (reverse) { sortFields.add(SORT_DOC_REVERSE); } else { sortFields.add(SORT_DOC); } } else { FieldMapper<?> fieldMapper = context.smartNameFieldMapper(fieldName); if (fieldMapper == null) { if (ignoreUnmapped != null) { // We build a fake field that will have empty values since it doesn't exist final Mapper.TypeParser.ParserContext parserContext = context.mapperService().documentMapperParser().parserContext(); Mapper.TypeParser typeParser = parserContext.typeParser(ignoreUnmapped); if (typeParser == null) { throw new SearchParseException(context, "No mapper found for type [" + ignoreUnmapped + "]"); } Mapper.Builder<?, ?> builder = typeParser.parse(fieldName, ImmutableMap.<String, Object>of(), parserContext); Settings indexSettings = context.indexShard().indexService().settingsService().getSettings(); BuilderContext builderContext = new BuilderContext(indexSettings, new ContentPath(1)); fieldMapper = (FieldMapper<?>) builder.build(builderContext); } else { throw new SearchParseException(context, "No mapping found for [" + fieldName + "] in order to sort on"); } } if (!fieldMapper.isSortable()) { throw new SearchParseException(context, "Sorting not supported for field[" + fieldName + "]"); } // Enable when we also know how to detect fields that do tokenize, but only emit one token /*if (fieldMapper instanceof StringFieldMapper) { StringFieldMapper stringFieldMapper = (StringFieldMapper) fieldMapper; if (stringFieldMapper.fieldType().tokenized()) { // Fail early throw new SearchParseException(context, "Can't sort on tokenized string field[" + fieldName + "]"); } }*/ // We only support AVG and SUM on number based fields if (!(fieldMapper instanceof NumberFieldMapper) && (sortMode == MultiValueMode.SUM || sortMode == MultiValueMode.AVG)) { sortMode = null; } if (sortMode == null) { sortMode = resolveDefaultSortMode(reverse); } ObjectMapper objectMapper; if (nestedPath != null) { ObjectMappers objectMappers = context.mapperService().objectMapper(nestedPath); if (objectMappers == null) { throw new ElasticsearchIllegalArgumentException("failed to find nested object mapping for explicit nested path [" + nestedPath + "]"); } objectMapper = objectMappers.mapper(); if (!objectMapper.nested().isNested()) { throw new ElasticsearchIllegalArgumentException("mapping for explicit nested path is not mapped as nested: [" + nestedPath + "]"); } } else { objectMapper = context.mapperService().resolveClosestNestedObjectMapper(fieldName); } final Nested nested; if (objectMapper != null && objectMapper.nested().isNested()) { Filter rootDocumentsFilter = context.filterCache().cache(NonNestedDocsFilter.INSTANCE); Filter innerDocumentsFilter; if (nestedFilter != null) { innerDocumentsFilter = context.filterCache().cache(nestedFilter); } else { innerDocumentsFilter = context.filterCache().cache(objectMapper.nestedTypeFilter()); } nested = new Nested(rootDocumentsFilter, innerDocumentsFilter); } else { nested = null; } IndexFieldData.XFieldComparatorSource fieldComparatorSource = context.fieldData().getForField(fieldMapper) .comparatorSource(missing, sortMode, nested); sortFields.add(new SortField(fieldMapper.names().indexName(), fieldComparatorSource, reverse)); } }	can we maybe share these on an index level, instead of doing the whole build process over and over? just a set of static (index level) common types?
public void testCompletionWithContextAndSubCompletion() throws Exception { DocumentMapper defaultMapper = createDocumentMapper(fieldMapping(b -> { b.field("type", "completion"); b.startArray("contexts"); { b.startObject(); b.field("name", "place_type"); b.field("type", "category"); b.field("path", "cat"); b.endObject(); } b.endArray(); b.startObject("fields"); { b.startObject("subsuggest"); { b.field("type", "completion"); b.startArray("contexts"); { b.startObject(); b.field("name", "place_type"); b.field("type", "category"); b.field("path", "cat"); b.endObject(); } b.endArray(); } b.endObject(); } b.endObject(); })); { ParsedDocument parsedDocument = defaultMapper.parse(source(b -> { b.startObject("field"); { b.array("input", "timmy", "starbucks"); b.startObject("contexts").array("place_type", "cafe", "food").endObject(); b.field("weight", 3); } b.endObject(); })); ParseContext.Document indexableFields = parsedDocument.rootDoc(); assertThat(indexableFields.getFields("field"), arrayContainingInAnyOrder( contextSuggestField("timmy"), contextSuggestField("starbucks") )); assertThat(indexableFields.getFields("field.subsuggest"), arrayContainingInAnyOrder( contextSuggestField("timmy"), contextSuggestField("starbucks") )); try (TokenStream ts = indexableFields.getFields("field.subsuggest")[0].tokenStream(Lucene.WHITESPACE_ANALYZER, null)) { ts.reset(); while (ts.incrementToken()) {} ts.end(); } //unable to assert about context, covered in a REST test } { ParsedDocument parsedDocument = defaultMapper.parse(source(b -> { b.array("field", "timmy", "starbucks"); b.array("cat", "cafe", "food"); })); ParseContext.Document indexableFields = parsedDocument.rootDoc(); assertThat(indexableFields.getFields("field"), arrayContainingInAnyOrder( contextSuggestField("timmy"), contextSuggestField("starbucks") )); assertThat(indexableFields.getFields("field.subsuggest"), arrayContainingInAnyOrder( contextSuggestField("timmy"), contextSuggestField("starbucks") )); //unable to assert about context, covered in a REST test } }	could you explain what this tests?
public float weight(Balancer balancer, ModelNode node, String index) { return weight(balancer, node, index, 0, 0.0f); }	hmm can you put a comment here why this is 0.0f? i'd have expected it to be the default.
@Override public void start() { if (nodes.size() > globalSemaphore.availablePermits()) { throw new TestClustersException("Cluster " + this + " is too large, requires " + nodes.size() + " nodes, but this system only supports " + globalSemaphore.availablePermits() ); } LOGGER.info( "Will acquire {} permits for {} on {}", nodes.size(), this, Thread.currentThread().getName() ); long startedAt = System.currentTimeMillis(); globalSemaphore.acquireUninterruptibly(nodes.size()); LOGGER.info( "Acquired {} permits for {} took {} seconds", nodes.size(), this, (startedAt - System.currentTimeMillis())/1000 ); String nodeNames = nodes.stream().map(ElasticsearchNode::getName).collect(Collectors.joining(",")); for (ElasticsearchNode node : nodes) { if (Version.fromString(node.getVersion()).getMajor() >= 7) { node.defaultConfig.put("cluster.initial_master_nodes", "[" + nodeNames + "]"); node.defaultConfig.put("discovery.seed_providers", "file"); } node.start(); } }	let's use instants here so then we can more easily turn this into a duration which has nice benefits like having a much more human readable output from tostring(). it's also a much more pleasant api to work with.
public void testSnapshotDeleteWithRepositoryOutage() throws Exception { disableRepoConsistencyCheck("This test expects residual files in repository"); Client client = client(); Path repo = randomRepoPath(); logger.info("--> creating repository at {}", repo.toAbsolutePath()); assertAcked(client.admin().cluster().preparePutRepository("test-repo") .setType("mock").setSettings(Settings.builder() .put("location", repo) .put("compress", false) .put("chunk_size", randomIntBetween(100, 1000), ByteSizeUnit.BYTES))); logger.info("--> creating index-1 and ingest data"); createIndex("test-idx-1"); ensureGreen(); for (int j = 0; j < 10; j++) { indexDoc("test-idx-1", Integer.toString( 10 + j), "foo", "bar" + 10 + j); } refresh(); // Make repository to throw exception when trying to delete stale indices String masterNode = internalCluster().getMasterName(); ((MockRepository)internalCluster().getInstance(RepositoriesService.class, masterNode).repository("test-repo")) .setThrowExceptionWhileDelete(true); logger.info("--> creating snapshot"); CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap-1") .setWaitForCompletion(true).get(); assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0)); assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards())); logger.info("--> delete the snapshot"); client.admin().cluster().prepareDeleteSnapshot("test-repo", "test-snap-1").get(); logger.info("--> done"); }	we have org.elasticsearch.snapshots.abstractsnapshotintegtestcase#createfullsnapshot now which does all of the snapshot creating + logging and assertions here in one call which you can use everywhere instead of this pattern :)
public void testSnapshotDeleteWithRepositoryOutage() throws Exception { disableRepoConsistencyCheck("This test expects residual files in repository"); Client client = client(); Path repo = randomRepoPath(); logger.info("--> creating repository at {}", repo.toAbsolutePath()); assertAcked(client.admin().cluster().preparePutRepository("test-repo") .setType("mock").setSettings(Settings.builder() .put("location", repo) .put("compress", false) .put("chunk_size", randomIntBetween(100, 1000), ByteSizeUnit.BYTES))); logger.info("--> creating index-1 and ingest data"); createIndex("test-idx-1"); ensureGreen(); for (int j = 0; j < 10; j++) { indexDoc("test-idx-1", Integer.toString( 10 + j), "foo", "bar" + 10 + j); } refresh(); // Make repository to throw exception when trying to delete stale indices String masterNode = internalCluster().getMasterName(); ((MockRepository)internalCluster().getInstance(RepositoriesService.class, masterNode).repository("test-repo")) .setThrowExceptionWhileDelete(true); logger.info("--> creating snapshot"); CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap-1") .setWaitForCompletion(true).get(); assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0)); assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards())); logger.info("--> delete the snapshot"); client.admin().cluster().prepareDeleteSnapshot("test-repo", "test-snap-1").get(); logger.info("--> done"); }	we have a shortcut here now as well org.elasticsearch.snapshots.abstractsnapshotintegtestcase#createrepository(java.lang.string, java.lang.string, java.nio.file.path) :)
private void cleanupStaleIndices(Map<String, BlobContainer> foundIndices, Set<String> survivingIndexIds, Executor executor, GroupedActionListener<DeleteResult> listener) { final GroupedActionListener<DeleteResult> groupedListener = new GroupedActionListener<>(ActionListener.wrap(deleteResults -> { DeleteResult deleteResult = DeleteResult.ZERO; for (DeleteResult result : deleteResults) { deleteResult = deleteResult.add(result); } listener.onResponse(deleteResult); }, listener::onFailure), foundIndices.size() - survivingIndexIds.size()); try { final BlockingQueue<Map.Entry<String, BlobContainer>> staleIndicesToDelete = new LinkedBlockingQueue<>(); for (Map.Entry<String, BlobContainer> indexEntry : foundIndices.entrySet()) { staleIndicesToDelete.put(indexEntry); } // Start as many workers as fit into the snapshot pool at once at the most final int workers = Math.min(threadPool.info(ThreadPool.Names.SNAPSHOT).getMax(), foundIndices.size() - survivingIndexIds.size()); for (int i = 0; i < workers; ++i) { executeOneStaeIndexDelete( staleIndicesToDelete, executor, groupedListener); } } catch (Exception e) { // TODO: We shouldn't be blanket catching and suppressing all exceptions here and instead handle them safely upstream. // Currently this catch exists as a stop gap solution to tackle unexpected runtime exceptions from implementations // bubbling up and breaking the snapshot functionality. assert false : e; logger.warn(new ParameterizedMessage("[{}] Exception during cleanup of stale indices", metadata.name()), e); } }	typo: executeonestaeindexdelete -> executeonestaleindexdelete also there's an extra space in the arguments here
private void executeOneStaeIndexDelete(BlockingQueue<Map.Entry<String, BlobContainer>> staleIndicesToDelete, Executor executor, GroupedActionListener<DeleteResult> listener) throws InterruptedException { Map.Entry<String, BlobContainer> indexEntry = staleIndicesToDelete.poll(0L, TimeUnit.MILLISECONDS); if (indexEntry == null) { listener.onResponse(DeleteResult.ZERO); } else { final String indexSnId = indexEntry.getKey(); executor.execute(ActionRunnable.supply(listener, () -> { try { DeleteResult staleIndexDeleteResult = indexEntry.getValue().delete(); logger.debug("[{}] Cleaned up stale index [{}]", metadata.name(), indexSnId); executeOneStaeIndexDelete( staleIndicesToDelete, executor, listener); return staleIndexDeleteResult; } catch (IOException e) { logger.warn(() -> new ParameterizedMessage( "[{}] index {} is no longer part of any snapshots in the repository, " + "but failed to clean up their index folders", metadata.name(), indexSnId), e); return DeleteResult.ZERO; } })); } }	i think we can just use threadpool.executor(threadpool.names.snapshot) here instead of passing the executor as a parameter down here.
private void executeOneStaeIndexDelete(BlockingQueue<Map.Entry<String, BlobContainer>> staleIndicesToDelete, Executor executor, GroupedActionListener<DeleteResult> listener) throws InterruptedException { Map.Entry<String, BlobContainer> indexEntry = staleIndicesToDelete.poll(0L, TimeUnit.MILLISECONDS); if (indexEntry == null) { listener.onResponse(DeleteResult.ZERO); } else { final String indexSnId = indexEntry.getKey(); executor.execute(ActionRunnable.supply(listener, () -> { try { DeleteResult staleIndexDeleteResult = indexEntry.getValue().delete(); logger.debug("[{}] Cleaned up stale index [{}]", metadata.name(), indexSnId); executeOneStaeIndexDelete( staleIndicesToDelete, executor, listener); return staleIndexDeleteResult; } catch (IOException e) { logger.warn(() -> new ParameterizedMessage( "[{}] index {} is no longer part of any snapshots in the repository, " + "but failed to clean up their index folders", metadata.name(), indexSnId), e); return DeleteResult.ZERO; } })); } }	i think we need to add another catch here now that effectively does what this block used to do (+ return the zero delete result from it as well): https://github.com/elastic/elasticsearch/pull/64513/files#diff-b9c15e79024348d03f0af119486a84c301b54135cc309c638834c8dfd9c101b0r1052 since it's not synchronous any longer.
static Request getDeprecationInfo(DeprecationInfoRequest deprecationInfoRequest) { String endpoint = new RequestConverters.EndpointBuilder() .addCommaSeparatedPathParts(deprecationInfoRequest.getIndices()) .addPathPartAsIs("_xpack", "migration", "deprecations") .build(); return new Request(HttpGet.METHOD_NAME, endpoint); }	in 7.0 this should use _migration/deprecations as of https://github.com/elastic/elasticsearch/pull/35976, although in 6.x it will have to use _xpack.
public void testGetDeprecationInfo() throws IOException { createIndex("test", Settings.EMPTY); DeprecationInfoRequest request = new DeprecationInfoRequest(Collections.singletonList("test")); DeprecationInfoResponse response = highLevelClient().migration().getDeprecationInfo(request, RequestOptions.DEFAULT); // a test like this cannot test actual deprecations assertThat(response.getClusterSettingsIssues().size(), equalTo(0)); assertThat(response.getIndexSettingsIssues().size(), equalTo(0)); assertThat(response.getNodeSettingsIssues().size(), equalTo(0)); }	this would be pretty easy to test in 6.x, but not so much in master. it might be worth adding a test case to 6.x, but i'm not sure.
private Decision underCapacity(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation, boolean moveToNode) { if (awarenessAttributes.length == 0) { return allocation.decision(Decision.YES, NAME, "allocation awareness is not enabled, set [%s] to enable it", CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING.getKey()); } IndexMetaData indexMetaData = allocation.metaData().getIndexSafe(shardRouting.index()); int shardCount = indexMetaData.getNumberOfReplicas() + 1; // 1 for primary for (String awarenessAttribute : awarenessAttributes) { // the node the shard exists on must be associated with an awareness attribute if (!node.node().getAttributes().containsKey(awarenessAttribute)) { return allocation.decision(Decision.NO, NAME, "node does not contain the awareness attribute [%s]; required attributes [%s=%s]", awarenessAttribute, CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING.getKey(), Strings.arrayToCommaDelimitedString(awarenessAttributes)); } // build attr_value -> nodes map ObjectIntHashMap<String> nodesPerAttribute = allocation.routingNodes().nodesPerAttributesCounts(awarenessAttribute); // build the count of shards per attribute value ObjectIntHashMap<String> shardPerAttribute = new ObjectIntHashMap<>(); for (ShardRouting assignedShard : allocation.routingNodes().assignedShards(shardRouting.shardId())) { if (assignedShard.started() || assignedShard.initializing()) { // Note: this also counts relocation targets as that will be the new location of the shard. // Relocation sources should not be counted as the shard is moving away RoutingNode routingNode = allocation.routingNodes().node(assignedShard.currentNodeId()); shardPerAttribute.addTo(routingNode.node().getAttributes().get(awarenessAttribute), 1); } } if (moveToNode) { if (shardRouting.assignedToNode()) { String nodeId = shardRouting.relocating() ? shardRouting.relocatingNodeId() : shardRouting.currentNodeId(); if (!node.nodeId().equals(nodeId)) { // we work on different nodes, move counts around shardPerAttribute.putOrAdd(allocation.routingNodes().node(nodeId).node().getAttributes().get(awarenessAttribute), 0, -1); shardPerAttribute.addTo(node.node().getAttributes().get(awarenessAttribute), 1); } } else { shardPerAttribute.addTo(node.node().getAttributes().get(awarenessAttribute), 1); } } int numberOfAttributes = nodesPerAttribute.size(); String[] fullValues = forcedAwarenessAttributes.get(awarenessAttribute); if (fullValues != null) { for (String fullValue : fullValues) { if (!shardPerAttribute.containsKey(fullValue)) { numberOfAttributes++; } } } // TODO should we remove ones that are not part of full list? int averagePerAttribute = shardCount / numberOfAttributes; int totalLeftover = shardCount % numberOfAttributes; int requiredCountPerAttribute; if (averagePerAttribute == 0) { // if we have more attributes values than shard count, no leftover totalLeftover = 0; requiredCountPerAttribute = 1; } else { requiredCountPerAttribute = averagePerAttribute; } int leftoverPerAttribute = totalLeftover == 0 ? 0 : 1; int currentNodeCount = shardPerAttribute.get(node.node().getAttributes().get(awarenessAttribute)); // if we are above with leftover, then we know we are not good, even with mod if (currentNodeCount > (requiredCountPerAttribute + leftoverPerAttribute)) { return allocation.decision(Decision.NO, NAME, "there are too many copies of the shard allocated to nodes with attribute [%s], there are [%d] total shards " + "for the index and [%d] total attributes values, expected the allocated shard count per attribute [%d] to be " + "less than or equal to the upper bound of the required number of shards per attribute [%d]", awarenessAttribute, shardCount, numberOfAttributes, currentNodeCount, requiredCountPerAttribute + leftoverPerAttribute); } // all is well, we are below or same as average if (currentNodeCount <= requiredCountPerAttribute) { continue; } } return allocation.decision(Decision.YES, NAME, "node meets all awareness attribute requirements"); }	i worry about performance here as this is evaluated eagerly even if we don't need it.
@Override public Decision canRebalance(RoutingAllocation allocation) { if (type == ClusterRebalanceType.INDICES_PRIMARIES_ACTIVE) { // check if there are unassigned primaries. if ( allocation.routingNodes().hasUnassignedPrimaries() ) { return allocation.decision(Decision.NO, NAME, "the cluster has unassigned primary shards and [%s] is set to [%s]", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), type); } // check if there are initializing primaries that don't have a relocatingNodeId entry. if ( allocation.routingNodes().hasInactivePrimaries() ) { return allocation.decision(Decision.NO, NAME, "the cluster has inactive primary shards and [%s] is set to [%s]", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), type); } return allocation.decision(Decision.YES, NAME, "all primary shards are active"); } if (type == ClusterRebalanceType.INDICES_ALL_ACTIVE) { // check if there are unassigned shards. if (allocation.routingNodes().hasUnassignedShards() ) { return allocation.decision(Decision.NO, NAME, "the cluster has unassigned shards and [%s] is set to [%s]", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), type); } // in case all indices are assigned, are there initializing shards which // are not relocating? if ( allocation.routingNodes().hasInactiveShards() ) { return allocation.decision(Decision.NO, NAME, "the cluster has inactive shards and [%s] is set to [%s]", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), type); } } // type == Type.ALWAYS return allocation.decision(Decision.YES, NAME, "all shards are active"); }	can you make an extra string constant for this and not call getkey() everywhere?
@Override public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { if (allocation.ignoreDisable()) { return allocation.decision(Decision.YES, NAME, "explicitly ignoring any disabling of allocation due to manual _reroute call"); } final IndexMetaData indexMetaData = allocation.metaData().getIndexSafe(shardRouting.index()); final Allocation enable; final boolean usedIndexSetting; if (INDEX_ROUTING_ALLOCATION_ENABLE_SETTING.exists(indexMetaData.getSettings())) { enable = INDEX_ROUTING_ALLOCATION_ENABLE_SETTING.get(indexMetaData.getSettings()); usedIndexSetting = true; } else { enable = this.enableAllocation; usedIndexSetting = false; } switch (enable) { case ALL: return allocation.decision(Decision.YES, NAME, "all allocations are allowed"); case NONE: return allocation.decision(Decision.NO, NAME, "no allocations are allowed due to {}", setting(enable, usedIndexSetting)); case NEW_PRIMARIES: if (shardRouting.primary() && shardRouting.active() == false && shardRouting.recoverySource().getType() != RecoverySource.Type.EXISTING_STORE) { return allocation.decision(Decision.YES, NAME, "new primary allocations are allowed"); } else { return allocation.decision(Decision.NO, NAME, "non-new primary allocations are forbidden due to {}", setting(enable, usedIndexSetting)); } case PRIMARIES: if (shardRouting.primary()) { return allocation.decision(Decision.YES, NAME, "primary allocations are allowed"); } else { return allocation.decision(Decision.NO, NAME, "replica allocations are forbidden due to {}", setting(enable, usedIndexSetting)); } default: throw new IllegalStateException("Unknown allocation option"); } }	it's not the full _reroute call that ignores the allocation disables but the manual allocation commands (allocate_replica/allocate_empty_primary etc.) that ignore this decider.
private Decision shouldIndexFilter(IndexMetaData indexMd, RoutingNode node, RoutingAllocation allocation) { if (indexMd.requireFilters() != null) { if (!indexMd.requireFilters().match(node.node())) { return allocation.decision(Decision.NO, NAME, "node does not match [%s] filters [%s]", IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_PREFIX, indexMd.requireFilters()); } } if (indexMd.includeFilters() != null) { if (!indexMd.includeFilters().match(node.node())) { return allocation.decision(Decision.NO, NAME, "node does not match [%s] filters [%s]", IndexMetaData.INDEX_ROUTING_INCLUDE_GROUP_PREFIX, indexMd.includeFilters()); } } if (indexMd.excludeFilters() != null) { if (indexMd.excludeFilters().match(node.node())) { return allocation.decision(Decision.NO, NAME, "node matches [%s] filters [%s]", IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey(), indexMd.excludeFilters()); } } return null; }	this comment is more related to the other pr on refactoring the explain api, but this shows why the full discoverynode is helpful to understand why the node does not match (i.e. incl. it's attributes).
public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { if (shardRouting.unassigned()) { // only for unassigned - we filter allocation right after the index creation ie. for shard shrinking etc. to ensure // that once it has been allocated post API the replicas can be allocated elsewhere without user interaction // this is a setting that can only be set within the system! IndexMetaData indexMd = allocation.metaData().getIndexSafe(shardRouting.index()); DiscoveryNodeFilters initialRecoveryFilters = indexMd.getInitialRecoveryFilters(); if (initialRecoveryFilters != null && RecoverySource.isInitialRecovery(shardRouting.recoverySource().getType()) && initialRecoveryFilters.match(node.node()) == false) { return allocation.decision(Decision.NO, NAME, "initial allocation of the shrunken index is only allowed on nodes [%s] that hold a copy of every shard in the index", indexMd.includeFilters()); } } return shouldFilter(shardRouting, node, allocation); }	the initialrecoveryfilters are currently only used for index shrinking but also apply to any primaries that are allocated "for the first time" (i.e. where there never was an active primary before). i wonder if the message is too specific here.
public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { Iterable<ShardRouting> assignedShards = allocation.routingNodes().assignedShards(shardRouting.shardId()); for (ShardRouting assignedShard : assignedShards) { if (node.nodeId().equals(assignedShard.currentNodeId())) { return allocation.decision(Decision.NO, NAME, "the shard cannot be allocated to the same node id [%s] on which a copy of the shard already exists", node.nodeId()); } } if (sameHost) { if (node.node() != null) { for (RoutingNode checkNode : allocation.routingNodes()) { if (checkNode.node() == null) { continue; } // check if its on the same host as the one we want to allocate to boolean checkNodeOnSameHost = false; if (Strings.hasLength(checkNode.node().getHostAddress()) && Strings.hasLength(node.node().getHostAddress())) { if (checkNode.node().getHostAddress().equals(node.node().getHostAddress())) { checkNodeOnSameHost = true; } } else if (Strings.hasLength(checkNode.node().getHostName()) && Strings.hasLength(node.node().getHostName())) { if (checkNode.node().getHostName().equals(node.node().getHostName())) { checkNodeOnSameHost = true; } } if (checkNodeOnSameHost) { for (ShardRouting assignedShard : assignedShards) { if (checkNode.nodeId().equals(assignedShard.currentNodeId())) { return allocation.decision(Decision.NO, NAME, "the shard cannot be allocated on host [%s], where it already exists on node [%s]; " + "set [%s] to false to allow multiple nodes on the same host to hold the same shard copies", node.node().getHostName(), node.nodeId(), CLUSTER_ROUTING_ALLOCATION_SAME_HOST_SETTING.getKey()); } } } } } } return allocation.decision(Decision.YES, NAME, "the shard does not exist on the same node or host"); }	instead of saying "allocated to the same node *id*" it should just say "to the same *node*". i wonder if we should provide more info here as well, maybe we could put the whole shard routing object instead of the node id. i also wonder whether it makes to distinguish between the same shardrouting object and a different one (i.e. are we trying to allocate to the node on which we are already allocated or are trying to allocate to a node where another copy of the shard is already allocated)
public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { Iterable<ShardRouting> assignedShards = allocation.routingNodes().assignedShards(shardRouting.shardId()); for (ShardRouting assignedShard : assignedShards) { if (node.nodeId().equals(assignedShard.currentNodeId())) { return allocation.decision(Decision.NO, NAME, "the shard cannot be allocated to the same node id [%s] on which a copy of the shard already exists", node.nodeId()); } } if (sameHost) { if (node.node() != null) { for (RoutingNode checkNode : allocation.routingNodes()) { if (checkNode.node() == null) { continue; } // check if its on the same host as the one we want to allocate to boolean checkNodeOnSameHost = false; if (Strings.hasLength(checkNode.node().getHostAddress()) && Strings.hasLength(node.node().getHostAddress())) { if (checkNode.node().getHostAddress().equals(node.node().getHostAddress())) { checkNodeOnSameHost = true; } } else if (Strings.hasLength(checkNode.node().getHostName()) && Strings.hasLength(node.node().getHostName())) { if (checkNode.node().getHostName().equals(node.node().getHostName())) { checkNodeOnSameHost = true; } } if (checkNodeOnSameHost) { for (ShardRouting assignedShard : assignedShards) { if (checkNode.nodeId().equals(assignedShard.currentNodeId())) { return allocation.decision(Decision.NO, NAME, "the shard cannot be allocated on host [%s], where it already exists on node [%s]; " + "set [%s] to false to allow multiple nodes on the same host to hold the same shard copies", node.node().getHostName(), node.nodeId(), CLUSTER_ROUTING_ALLOCATION_SAME_HOST_SETTING.getKey()); } } } } } } return allocation.decision(Decision.YES, NAME, "the shard does not exist on the same node or host"); }	it's not only the hostname, it's also the hostaddress that comes into play (if hostname or hostaddress match, it is considered same host)
public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { Iterable<ShardRouting> assignedShards = allocation.routingNodes().assignedShards(shardRouting.shardId()); for (ShardRouting assignedShard : assignedShards) { if (node.nodeId().equals(assignedShard.currentNodeId())) { return allocation.decision(Decision.NO, NAME, "the shard cannot be allocated to the same node id [%s] on which a copy of the shard already exists", node.nodeId()); } } if (sameHost) { if (node.node() != null) { for (RoutingNode checkNode : allocation.routingNodes()) { if (checkNode.node() == null) { continue; } // check if its on the same host as the one we want to allocate to boolean checkNodeOnSameHost = false; if (Strings.hasLength(checkNode.node().getHostAddress()) && Strings.hasLength(node.node().getHostAddress())) { if (checkNode.node().getHostAddress().equals(node.node().getHostAddress())) { checkNodeOnSameHost = true; } } else if (Strings.hasLength(checkNode.node().getHostName()) && Strings.hasLength(node.node().getHostName())) { if (checkNode.node().getHostName().equals(node.node().getHostName())) { checkNodeOnSameHost = true; } } if (checkNodeOnSameHost) { for (ShardRouting assignedShard : assignedShards) { if (checkNode.nodeId().equals(assignedShard.currentNodeId())) { return allocation.decision(Decision.NO, NAME, "the shard cannot be allocated on host [%s], where it already exists on node [%s]; " + "set [%s] to false to allow multiple nodes on the same host to hold the same shard copies", node.node().getHostName(), node.nodeId(), CLUSTER_ROUTING_ALLOCATION_SAME_HOST_SETTING.getKey()); } } } } } } return allocation.decision(Decision.YES, NAME, "the shard does not exist on the same node or host"); }	let's have two different messages depending on whether samehost is enabled or not.
@Override public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { IndexMetaData indexMd = allocation.metaData().getIndexSafe(shardRouting.index()); final int indexShardLimit = INDEX_TOTAL_SHARDS_PER_NODE_SETTING.get(indexMd.getSettings(), settings); // Capture the limit here in case it changes during this method's // execution final int clusterShardLimit = this.clusterShardLimit; if (indexShardLimit <= 0 && clusterShardLimit <= 0) { return allocation.decision(Decision.YES, NAME, "total shard limits are disabled: [index: %d, cluster: %d] <= 0", indexShardLimit, clusterShardLimit); } int indexShardCount = 0; int nodeShardCount = 0; for (ShardRouting nodeShard : node) { // don't count relocating shards... if (nodeShard.relocating()) { continue; } nodeShardCount++; if (nodeShard.index().equals(shardRouting.index())) { indexShardCount++; } } if (clusterShardLimit > 0 && nodeShardCount >= clusterShardLimit) { return allocation.decision(Decision.NO, NAME, "too many shards [%d] are already on this node, [%s=%d]", nodeShardCount, CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING.getKey(), clusterShardLimit); } if (indexShardLimit > 0 && indexShardCount >= indexShardLimit) { return allocation.decision(Decision.NO, NAME, "too many shards [%d] are already on node for index [%s], [%s=%d]", indexShardCount, shardRouting.index(), INDEX_TOTAL_SHARDS_PER_NODE_SETTING.getKey(), indexShardLimit); } return allocation.decision(Decision.YES, NAME, "the shard count [%d] for this node is under the index limit [%d] and cluster level node limit [%d]", nodeShardCount, indexShardLimit, clusterShardLimit); }	too many shards [%d] already allocated to this node, [%s=%d]
@Override public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { IndexMetaData indexMd = allocation.metaData().getIndexSafe(shardRouting.index()); final int indexShardLimit = INDEX_TOTAL_SHARDS_PER_NODE_SETTING.get(indexMd.getSettings(), settings); // Capture the limit here in case it changes during this method's // execution final int clusterShardLimit = this.clusterShardLimit; if (indexShardLimit <= 0 && clusterShardLimit <= 0) { return allocation.decision(Decision.YES, NAME, "total shard limits are disabled: [index: %d, cluster: %d] <= 0", indexShardLimit, clusterShardLimit); } int indexShardCount = 0; int nodeShardCount = 0; for (ShardRouting nodeShard : node) { // don't count relocating shards... if (nodeShard.relocating()) { continue; } nodeShardCount++; if (nodeShard.index().equals(shardRouting.index())) { indexShardCount++; } } if (clusterShardLimit > 0 && nodeShardCount >= clusterShardLimit) { return allocation.decision(Decision.NO, NAME, "too many shards [%d] are already on this node, [%s=%d]", nodeShardCount, CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING.getKey(), clusterShardLimit); } if (indexShardLimit > 0 && indexShardCount >= indexShardLimit) { return allocation.decision(Decision.NO, NAME, "too many shards [%d] are already on node for index [%s], [%s=%d]", indexShardCount, shardRouting.index(), INDEX_TOTAL_SHARDS_PER_NODE_SETTING.getKey(), indexShardLimit); } return allocation.decision(Decision.YES, NAME, "the shard count [%d] for this node is under the index limit [%d] and cluster level node limit [%d]", nodeShardCount, indexShardLimit, clusterShardLimit); }	too many shards already allocated to this node for index ...
private Decision canMove(ShardRouting shardRouting, RoutingAllocation allocation) { if (shardRouting.primary()) { // Only primary shards are snapshotted SnapshotsInProgress snapshotsInProgress = allocation.custom(SnapshotsInProgress.TYPE); if (snapshotsInProgress == null) { // Snapshots are not running return allocation.decision(Decision.YES, NAME, "no snapshots are currently running"); } for (SnapshotsInProgress.Entry snapshot : snapshotsInProgress.entries()) { SnapshotsInProgress.ShardSnapshotStatus shardSnapshotStatus = snapshot.shards().get(shardRouting.shardId()); if (shardSnapshotStatus != null && !shardSnapshotStatus.state().completed() && shardSnapshotStatus.nodeId() != null && shardSnapshotStatus.nodeId().equals(shardRouting.currentNodeId())) { if (logger.isTraceEnabled()) { logger.trace("Preventing snapshotted shard [{}] from being moved away from node [{}]", shardRouting.shardId(), shardSnapshotStatus.nodeId()); } return allocation.decision(Decision.NO, NAME, "snapshot for shard [%s] is currently running on node [%s]", shardRouting.shardId(), shardSnapshotStatus.nodeId()); } } } return allocation.decision(Decision.YES, NAME, "the shard is not being snapshotted"); }	i wonder if this should be throttle as well. also wonder if we should extend the message to say " waiting for snapshotting process to complete for this shard"
@Override public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { if (shardRouting.primary() && shardRouting.unassigned()) { assert initializingShard(shardRouting, node.nodeId()).recoverySource().getType() != RecoverySource.Type.PEER; // primary is unassigned, means we are going to do recovery from store, snapshot or local shards // count *just the primaries* currently doing recovery on the node and check against primariesInitialRecoveries int primariesInRecovery = 0; for (ShardRouting shard : node) { // when a primary shard is INITIALIZING, it can be because of *initial recovery* or *relocation from another node* // we only count initial recoveries here, so we need to make sure that relocating node is null if (shard.initializing() && shard.primary() && shard.relocatingNodeId() == null) { primariesInRecovery++; } } if (primariesInRecovery >= primariesInitialRecoveries) { // TODO: Should index creation not be throttled for primary shards? return allocation.decision(THROTTLE, NAME, "reached the limit of ongoing initial primary recoveries [%d], [%s=%d]", primariesInRecovery, CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING.getKey(), primariesInitialRecoveries); } else { return allocation.decision(YES, NAME, "below primary recovery limit of [%d]", primariesInitialRecoveries); } } else { // Peer recovery assert initializingShard(shardRouting, node.nodeId()).recoverySource().getType() == RecoverySource.Type.PEER; // Allocating a shard to this node will increase the incoming recoveries int currentInRecoveries = allocation.routingNodes().getIncomingRecoveries(node.nodeId()); if (currentInRecoveries >= concurrentIncomingRecoveries) { return allocation.decision(THROTTLE, NAME, "reached the limit of incoming shard recoveries [%d], [%s=%d], [%s=%d]", currentInRecoveries, CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_INCOMING_RECOVERIES_SETTING.getKey(), concurrentIncomingRecoveries, CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.get(settings)); } else { // search for corresponding recovery source (= primary shard) and check number of outgoing recoveries on that node ShardRouting primaryShard = allocation.routingNodes().activePrimary(shardRouting.shardId()); if (primaryShard == null) { return allocation.decision(Decision.NO, NAME, "primary shard for this replica is not yet active"); } int primaryNodeOutRecoveries = allocation.routingNodes().getOutgoingRecoveries(primaryShard.currentNodeId()); if (primaryNodeOutRecoveries >= concurrentOutgoingRecoveries) { return allocation.decision(THROTTLE, NAME, "reached the limit of outgoing shard recoveries [%d], [%s=%d], [%s=%d]", primaryNodeOutRecoveries, CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_OUTGOING_RECOVERIES_SETTING.getKey(), concurrentOutgoingRecoveries, CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.get(settings)); } else { return allocation.decision(YES, NAME, "below shard recovery limit of outgoing: [%d < %d] incoming: [%d < %d]", primaryNodeOutRecoveries, concurrentOutgoingRecoveries, currentInRecoveries, concurrentIncomingRecoveries); } } } } /** * The shard routing passed to {@link #canAllocate(ShardRouting, RoutingNode, RoutingAllocation)}	it would be useful to know for which node this limit was reached. in case of incoming recoveries, it is the node where the shard is to be allocated to. in case of outgoing recoveries, however, the node is the one that has the primary. let's add that node id here.
@Override public void nextPage(SqlConfiguration cfg, Client client, NamedWriteableRegistry registry, ActionListener<Page> listener) { SearchSourceBuilder q; try { q = deserializeQuery(registry, nextQuery); } catch (Exception ex) { listener.onFailure(ex); return; } SearchSourceBuilder query = q; if (log.isTraceEnabled()) { log.trace("About to execute composite query {} on {}", StringUtils.toString(query), indices); } SearchRequest request = Querier.prepareRequest(query, cfg.requestTimeout(), includeFrozen, indices); client.search(request, new ActionListener.Delegating<>(listener) { @Override public void onResponse(SearchResponse response) { handle(response, request.source(), makeRowSet(response), makeCursor(), () -> client.search(request, this), delegate, Schema.EMPTY); } }); }	wouldn't this change make pagination through aggs results require a different use of the api compared to paginating through [search hits](https://github.com/elastic/elasticsearch/pull/79360/files#diff-3f0641a79abfbc1680ac9183b1c2e04cafe9f39526546f197c1043554a928427r99): the former would use the request_timeout, while the latter page_timeout.
@Override protected Aggregator create(ValuesSource valuesSource, long expectedBucketsCount, AggregationContext aggregationContext, Aggregator parent) { if (termsEnum == null) { SearchContext searchContext = aggregationContext.searchContext(); ContextIndexSearcher searcher = searchContext.searcher(); try { Terms terms = MultiFields.getTerms(searcher.getIndexReader(), indexedFieldName); if (terms != null) { termsEnum = terms.iterator(null); if (parent != null) { // potentially >1 Signif terms agg looking up the same // terms - create a caching wrapper for termsEnum termsEnum = new FrequencyCachingTermsEnumWrapper(termsEnum, searchContext.bigArrays(), true, false); } } } catch (IOException e) { throw new ElasticsearchException("IOException loading background document frequency info", e); } } long estimatedBucketCount = valuesSource.metaData().maxAtomicUniqueValuesCount(); if (estimatedBucketCount < 0) { // there isn't an estimation available.. 50 should be a good start estimatedBucketCount = 50; } // adding an upper bound on the estimation as some atomic field data in the future (binary doc values) and not // going to know their exact cardinality and will return upper bounds in AtomicFieldData.getNumberUniqueValues() // that may be largely over-estimated.. the value chosen here is arbitrary just to play nice with typical CPU cache // // Another reason is that it may be faster to resize upon growth than to start directly with the appropriate size. // And that all values are not necessarily visited by the matches. estimatedBucketCount = Math.min(estimatedBucketCount, 512); if (valuesSource instanceof BytesValuesSource) { if (executionHint != null && !executionHint.equals(EXECUTION_HINT_VALUE_MAP) && !executionHint.equals(EXECUTION_HINT_VALUE_ORDINALS)) { throw new ElasticsearchIllegalArgumentException("execution_hint can only be '" + EXECUTION_HINT_VALUE_MAP + "' or '" + EXECUTION_HINT_VALUE_ORDINALS + "', not " + executionHint); } String execution = executionHint; if (!(valuesSource instanceof BytesValuesSource.WithOrdinals)) { execution = EXECUTION_HINT_VALUE_MAP; } else if (includeExclude != null) { execution = EXECUTION_HINT_VALUE_MAP; } if (execution == null) { if ((valuesSource instanceof BytesValuesSource.WithOrdinals) && !hasParentBucketAggregator(parent)) { execution = EXECUTION_HINT_VALUE_ORDINALS; } else { execution = EXECUTION_HINT_VALUE_MAP; } } assert execution != null; if (execution.equals(EXECUTION_HINT_VALUE_ORDINALS)) { assert includeExclude == null; return new SignificantStringTermsAggregator.WithOrdinals(name, factories, (BytesValuesSource.WithOrdinals) valuesSource, estimatedBucketCount, requiredSize, shardSize, minDocCount, aggregationContext, parent, this); } return new SignificantStringTermsAggregator(name, factories, valuesSource, estimatedBucketCount, requiredSize, shardSize, minDocCount, includeExclude, aggregationContext, parent, this); } if (includeExclude != null) { throw new AggregationExecutionException("Aggregation [" + name + "] cannot support the include/exclude " + "settings as it can only be applied to string values"); } if (valuesSource instanceof NumericValuesSource) { if (((NumericValuesSource) valuesSource).isFloatingPoint()) { throw new UnsupportedOperationException("No support for examining floating point numerics"); } return new SignificantLongTermsAggregator(name, factories, (NumericValuesSource) valuesSource, estimatedBucketCount, requiredSize, shardSize, minDocCount, aggregationContext, parent, this); } throw new AggregationExecutionException("sigfnificant_terms aggregation cannot be applied to field [" + valuesSourceConfig.fieldContext().field() + "]. It can only be applied to numeric or string fields."); }	i think we should have something to characterize aggregators that create buckets (just checking parent != null is not enough as the parent might just be a filter agg). for example, metrics aggregators have metricsaggregator.singlevalue and metricsaggregator.multivalue, maybe buckets aggregators could have bucketsaggregator.singlebucket (singlebucketaggregator today) and bucketsaggregator.multibucket (doesn't exist today, terms/histogram/... aggs extend aggregator directly) so that checking if an aggregator creates buckets would just be parent instanceof bucketsaggregator.multibucket?
@Override public String toString() { return delegate + " offset by " + offset; } } public static Rounding read(StreamInput in) throws IOException { byte id = in.readByte(); switch (id) { case TimeUnitRounding.ID: return new TimeUnitRounding(in); case TimeIntervalRounding.ID: return new TimeIntervalRounding(in); case OffsetRounding.ID: return new OffsetRounding(in); default: throw new ElasticsearchException("unknown rounding id [" + id + "]"); } } /** * Attempt to build a {@link Prepared} implementation that relies on pre-calcuated * "round down" points. If there would be more than {@code max} points then return * the original implementation, otherwise return the new, faster implementation. */ static Prepared maybeUseArray(Prepared orig, long minUtcMillis, long maxUtcMillis, int max) { long[] values = new long[1]; long rounded = orig.round(minUtcMillis); int i = 0; values[i++] = rounded; while ((rounded = orig.nextRoundingValue(rounded)) <= maxUtcMillis) { if (i >= max) { return orig; } assert values[i - 1] == orig.round(rounded - 1); values = ArrayUtil.grow(values, i + 1); values[i++]= rounded; } return new ArrayRounding(values, i, orig); } /** * Implementation of {@link Prepared} using pre-calculated "round down" points. */ private static class ArrayRounding implements Prepared { private final long[] values; private final int max; private final Prepared delegate; private ArrayRounding(long[] values, int max, Prepared delegate) { this.values = values; this.max = max; this.delegate = delegate; } @Override public long round(long utcMillis) { assert values[0] <= utcMillis : "utcMillis must be after " + values[0]; int idx = Arrays.binarySearch(values, 0, max, utcMillis); assert idx != -1 : "The insertion point is before the array! This should have tripped the assertion above."; assert -1 - idx <= values.length : "This insertion point is after the end of the array."; if (idx < 0) { idx = -2 - idx; } return values[idx]; } @Override public long nextRoundingValue(long utcMillis) { return delegate.nextRoundingValue(utcMillis); }	this will need to implement new methods added in #61369.
private void warnAboutSlowTaskIfNeeded(long executionTime, String source) { if (executionTime >= slowTaskLoggingThreshold.getMillis()) { logger.warn("cluster state update task [{}] took too long: {}ms", source, executionTime); } }	can we log what the limit here? took x above the warn threshold of y? also, i wonder if this should be warn compared to info, don't have strong feelings, but it might be ok that it takes more than 10 seconds sometimes, depending on the cluster state task (like the initial gateway reach to discover which shards are around)?
@Override public void usage(ActionListener<XPackFeatureSet.Usage> listener) { if (enabled == false) { listener.onResponse(new DataFrameFeatureSetUsage(available(), enabled(), Collections.emptyMap(), DataFrameIndexerTransformStats.withNullTransformId())); return; } PersistentTasksCustomMetaData taskMetadata = PersistentTasksCustomMetaData.getPersistentTasksCustomMetaData(clusterService.state()); Collection<PersistentTasksCustomMetaData.PersistentTask<?>> dataFrameTasks = taskMetadata == null ? Collections.emptyList() : taskMetadata.findTasks(DataFrameTransform.NAME, (t) -> true); final int taskCount = dataFrameTasks.size(); final Map<String, Long> transformsCountByState = new HashMap<>(); for(PersistentTasksCustomMetaData.PersistentTask<?> dataFrameTask : dataFrameTasks) { DataFrameTransformState state = (DataFrameTransformState)dataFrameTask.getState(); transformsCountByState.merge(state.getTaskState().value(), 1L, Long::sum); } ActionListener<DataFrameIndexerTransformStats> totalStatsListener = ActionListener.wrap( statSummations -> listener.onResponse(new DataFrameFeatureSetUsage(available(), enabled(), transformsCountByState, statSummations)), listener::onFailure ); ActionListener<SearchResponse> totalTransformCountListener = ActionListener.wrap( transformCountSuccess -> { long totalTransforms = transformCountSuccess.getHits().getTotalHits().value; if (totalTransforms == 0) { listener.onResponse(new DataFrameFeatureSetUsage(available(), enabled(), transformsCountByState, DataFrameIndexerTransformStats.withNullTransformId())); return; } transformsCountByState.merge(DataFrameTransformTaskState.STOPPED.value(), totalTransforms - taskCount, Long::sum); getStatisticSummations(client, totalStatsListener); }, transformCountFailure -> { if (transformCountFailure instanceof ResourceNotFoundException) { getStatisticSummations(client, totalStatsListener); } else { listener.onFailure(transformCountFailure); } } ); SearchRequest totalTransformCount = client.prepareSearch(DataFrameInternalIndex.INDEX_NAME) .setTrackTotalHits(true) .setQuery(QueryBuilders.constantScoreQuery(QueryBuilders.boolQuery() .filter(QueryBuilders.termQuery(DataFrameField.INDEX_DOC_TYPE.getPreferredName(), DataFrameTransformConfig.NAME)))) .request(); ClientHelper.executeAsyncWithOrigin(client.threadPool().getThreadContext(), ClientHelper.DATA_FRAME_ORIGIN, totalTransformCount, totalTransformCountListener, client::search); }	unlikely to happen but i think it is good practice to check getshardfailures
@Override protected void doExecute(Task task, Request request, ActionListener<Response> finalListener) { final ClusterState state = clusterService.state(); final DiscoveryNodes nodes = state.nodes(); if (nodes.isLocalNodeElectedMaster()) { dataFrameTransformsConfigManager.expandTransformIds(request.getId(), request.getPageParams(), ActionListener.wrap( ids -> { request.setExpandedIds(ids); super.doExecute(task, request, ActionListener.wrap( response -> collectStatsForTransformsWithoutTasks(request, response, finalListener), finalListener::onFailure )); }, e -> { // If the index to search, or the individual config is not there, just return empty if (e instanceof ResourceNotFoundException) { finalListener.onResponse(new Response(Collections.emptyList())); } else { finalListener.onFailure(e); } } )); } else { // Delegates GetTransforms to elected master node, so it becomes the coordinating node. // Non-master nodes may have a stale cluster state that shows transforms which are cancelled // on the master, which makes testing difficult. if (nodes.getMasterNode() == null) { finalListener.onFailure(new MasterNotDiscoveredException("no known master nodes")); } else { transportService.sendRequest(nodes.getMasterNode(), actionName, request, new ActionListenerResponseHandler<>(finalListener, Response::new)); } } }	would be good to check for shard failures
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { String repository = request.param("repository"); String[] snapshots = request.paramAsStringArray("snapshot", Strings.EMPTY_ARRAY); final GetSnapshotsRequest getSnapshotsRequest = getSnapshotsRequest(repository).snapshots(snapshots); getSnapshotsRequest.ignoreUnavailable(request.paramAsBoolean("ignore_unavailable", getSnapshotsRequest.ignoreUnavailable())); getSnapshotsRequest.verbose(request.paramAsBoolean("verbose", getSnapshotsRequest.verbose())); getSnapshotsRequest.masterNodeTimeout(request.paramAsTime("master_timeout", getSnapshotsRequest.masterNodeTimeout())); return channel -> client.admin().cluster().getSnapshots(getSnapshotsRequest, new RestBuilderListener<GetSnapshotsResponse>(channel) { @Override public RestResponse buildResponse(GetSnapshotsResponse response, XContentBuilder builder) throws Exception { Map<String, String> params = Collections.singletonMap("verbose", Boolean.toString(getSnapshotsRequest.verbose())); ToXContent.MapParams xContentParams = new ToXContent.MapParams(params); response.toXContent(builder, xContentParams); return new BytesRestResponse(OK, builder); } }); }	i don't think is necessary. request is passed asparams. so, "verbose" is already in params if it was specified, you just need to be careful resolving defaults. you would also miss other parameters here such as human and pretty.
private void commonNodeConfig() { final String nodeNames; if (nodes.stream().map(ElasticsearchNode::getName).anyMatch(name -> name == null)) { nodeNames = null; } else { nodeNames = nodes.stream().map(ElasticsearchNode::getName).map(this::safeName).collect(Collectors.joining(",")); } ElasticsearchNode firstNode = null; for (ElasticsearchNode node : nodes) { if (node.getVersion().onOrAfter("8.0.0") && node.getTestDistribution().equals(TestDistribution.INTEG_TEST)) { node.defaultConfig.put("xpack.security.enabled", "false"); } // Can only configure master nodes if we have node names defined if (nodeNames != null) { if (node.getVersion().onOrAfter("7.0.0")) { node.defaultConfig.keySet() .stream() .filter(name -> name.startsWith("discovery.zen.")) .collect(Collectors.toList()) .forEach(node.defaultConfig::remove); node.defaultConfig.put("cluster.initial_master_nodes", "[" + nodeNames + "]"); node.defaultConfig.put("discovery.seed_providers", "file"); node.defaultConfig.put("discovery.seed_hosts", "[]"); } else { node.defaultConfig.put("discovery.zen.master_election.wait_for_joins_timeout", "5s"); if (nodes.size() > 1) { node.defaultConfig.put("discovery.zen.minimum_master_nodes", Integer.toString(nodes.size() / 2 + 1)); } if (node.getVersion().onOrAfter("6.5.0")) { node.defaultConfig.put("discovery.zen.hosts_provider", "file"); node.defaultConfig.put("discovery.zen.ping.unicast.hosts", "[]"); } else { if (firstNode == null) { node.defaultConfig.put("discovery.zen.ping.unicast.hosts", "[]"); } else { firstNode.waitForAllConditions(); node.defaultConfig.put("discovery.zen.ping.unicast.hosts", "[\\\\"" + firstNode.getTransportPortURI() + "\\\\"]"); } } } } if (firstNode == null) { firstNode = node; } } }	changes are superseded by https://github.com/elastic/elasticsearch/pull/77632
private ICUTokenizerConfig getIcuConfig(Environment env, Settings settings) { Map<Integer, String> tailored = new HashMap<>(); try { List<String> ruleFiles = settings.getAsList(RULE_FILES); for (String scriptAndResourcePath : ruleFiles) { int colonPos = scriptAndResourcePath.indexOf(":"); if (colonPos == -1 || colonPos == scriptAndResourcePath.length() - 1) { throw new IllegalArgumentException(RULE_FILES + " should contain comma-separated \\\\"code:rulefile\\\\" pairs"); } String scriptCode = scriptAndResourcePath.substring(0, colonPos).trim(); String resourcePath = scriptAndResourcePath.substring(colonPos + 1).trim(); tailored.put(UCharacter.getPropertyValueEnum(UProperty.SCRIPT, scriptCode), resourcePath); } if (tailored.isEmpty()) { return null; } else { final RuleBasedBreakIterator breakers[] = new RuleBasedBreakIterator[UScript.CODE_LIMIT]; for (Map.Entry<Integer, String> entry : tailored.entrySet()) { int code = entry.getKey(); String resourcePath = entry.getValue(); breakers[code] = parseRules(resourcePath, env); } // cjkAsWords nor myanmarAsWords are not configurable yet. ICUTokenizerConfig config = new DefaultICUTokenizerConfig(true, true) { @Override public RuleBasedBreakIterator getBreakIterator(int script) { if (breakers[script] != null) { return (RuleBasedBreakIterator) breakers[script].clone(); } else { return super.getBreakIterator(script); } } }; return config; } } catch (Exception e) { throw new ElasticsearchException("failed to load ICU rule files", e); } }	this change is already in master
public MatchQuery matchQuery(QueryShardContext context, String analyzer, int slop) { throw new QueryShardException(context, "Can only use match queries on keyword and text fields - not on [" + name + "] which is of type [" + typeName() + "]"); }	can this be changed to analyzephrase(queryshardcontext context, string text, analyzer analyzer, int slop) ? it seems to much to allow this override so i'd prefer something more focus on the real purpose.
@Override public Query rangeQuery(String field, boolean hasDocValues, Object lowerTerm, Object upperTerm, boolean includeLower, boolean includeUpper, ShapeRelation relation, @Nullable DateTimeZone timeZone, @Nullable JodaDateMathParser parser, QueryShardContext context) { DateTimeZone zone = (timeZone == null) ? DateTimeZone.UTC : timeZone; ZoneId zoneId = DateFormatters.timeZoneToZoneId(zone); JodaDateMathParser dateMathParser = (parser == null) ? new JodaDateMathParser(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER) : parser; Long low = lowerTerm == null ? Long.MIN_VALUE : dateMathParser.parse(lowerTerm instanceof BytesRef ? ((BytesRef) lowerTerm).utf8ToString() : lowerTerm.toString(), context::nowInMillis, false, zoneId); Long high = upperTerm == null ? Long.MAX_VALUE : dateMathParser.parse(upperTerm instanceof BytesRef ? ((BytesRef) upperTerm).utf8ToString() : upperTerm.toString(), context::nowInMillis, false, zoneId); return super.rangeQuery(field, hasDocValues, low, high, includeLower, includeUpper, relation, zone, dateMathParser, context); }	i may be misinterpreting this: shouldnt this be the datemathparser as argument in the fieldmappers, so that the underlying implementation can be replaced?
public void testScoreExecutionContext() throws IOException { ScriptService scriptService = getInstanceFromNode(ScriptService.class); IndexService indexService = createIndex("index", Settings.EMPTY, "doc", "rank", "type=long", "text", "type=text"); Request.ContextSetup contextSetup = new Request.ContextSetup("index", new BytesArray("{\\\\"rank\\\\": 4.0, \\\\"text\\\\": \\\\"quick brown fox\\\\"}"), new MatchQueryBuilder("text", "fox")); contextSetup.setXContentType(XContentType.JSON); Request request = new Request(new Script(ScriptType.INLINE, "painless", "Math.round((_score + (doc['rank'].value / params.max_rank)) * 100.0) / 100.0", singletonMap("max_rank", 5.0)), "score", contextSetup); Response response = innerShardOperation(request, scriptService, indexService); assertThat(response.getResult(), equalTo(0.93D)); }	here as well we use the updated bm25similarity instead of the legacy one then?
@Override public void process(IndexRouting indexRouting) { indexRouting.process(this); }	this is a bit spooky. i think it might be cleaner if we moved id generation into indexrouting with clear assignment. and generated timestamp in separate method called accordingly.
public static void main(String[] args) throws IOException { // Need to setup the log configuration properly to avoid messages when creating a new RestClient PluginManager.addPackage(LogConfigurator.class.getPackage().getName()); final CredentialsProvider credentialsProvider = new BasicCredentialsProvider(); credentialsProvider.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials("admin", "admin-password")); try ( RestClient client = RestClient.builder(new HttpHost("localhost", 9200)) .setHttpClientConfigCallback(new HttpClientConfigCallback() { @Override public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder.setDefaultCredentialsProvider(credentialsProvider); } }) .setRequestConfigCallback( requestConfigBuilder -> requestConfigBuilder.setConnectTimeout(30000000) .setConnectionRequestTimeout(30000000) .setSocketTimeout(30000000) ) .setStrictDeprecationMode(true) .build() ) { Properties configuration = loadConfiguration(); restoreSnapshot(new RestHighLevelClient(client, ignore -> {}, List.of()) { }, configuration); } }	i think it makes it generally easier with testing, so good to have it, but curious if this was added for a specific reason.
@Override void analyze(Locals program) { for (SFunction function : functions) { Locals functionLocals = Locals.newFunctionScope(program, function.rtnType, function.parameters, function.reserved.getMaxLoopCounter()); function.analyze(functionLocals); } if (statements == null || statements.isEmpty()) { throw createError(new IllegalArgumentException("Cannot generate an empty script.")); } mainMethod = Locals.newMainMethodScope(scriptClassInfo, program, reserved.getMaxLoopCounter()); // process get methods to add variables to the main method locally getMethods = new ArrayList<>(scriptClassInfo.getGetMethods()); List<Definition.Type> getReturns = new ArrayList<>(scriptClassInfo.getGetReturns()); for (int get = 0; get < getMethods.size();) { String name = getMethods.get(get).getName().substring(3); name = Character.toLowerCase(name.charAt(0)) + name.substring(1); Definition.Type rtn = getReturns.get(get); if (reserved.getUsedVariables().contains(name)) { // found use of the get variable so we add it to the variable list mainMethod.addVariable(new Location("getter [" + name + "]", 0), rtn, name, true); ++get; } else { // the get variable is not used in this script so remove it from the list getMethods.remove(get); getReturns.remove(get); } } AStatement last = statements.get(statements.size() - 1); for (AStatement statement : statements) { // Note that we do not need to check after the last statement because // there is no statement that can be unreachable after the last. if (allEscape) { throw createError(new IllegalArgumentException("Unreachable statement.")); } statement.lastSource = statement == last; statement.analyze(mainMethod); methodEscape = statement.methodEscape; allEscape = statement.allEscape; } }	can we just not remove, but only add the methods/returns we want?
* @param parserContext the parser context * @param fieldNode the root node of the map of mappings for this field */ public final void parse(String name, TypeParser.ParserContext parserContext, Map<String, Object> fieldNode) { Map<String, Parameter<?>> paramsMap = new HashMap<>(); for (Parameter<?> param : getParameters()) { paramsMap.put(param.name, param); } String type = (String) fieldNode.remove("type"); for (Iterator<Map.Entry<String, Object>> iterator = fieldNode.entrySet().iterator(); iterator.hasNext();) { Map.Entry<String, Object> entry = iterator.next(); final String propName = entry.getKey(); final Object propNode = entry.getValue(); if (Objects.equals("fields", propName)) { TypeParsers.parseMultiField(multiFieldsBuilder::add, name, parserContext, propName, propNode); iterator.remove(); continue; } if (Objects.equals("copy_to", propName)) { TypeParsers.parseCopyFields(propNode).forEach(copyTo::add); iterator.remove(); continue; } Parameter<?> parameter = paramsMap.get(propName); if (parameter == null) { if (checkForDeprecatedParameter(propName, parserContext.indexVersionCreated())) { deprecationLogger.deprecate(propName, "Parameter [{}] is unused on type [{}] and will be removed in future", propName, type); iterator.remove(); continue; } throw new MapperParsingException("unknown parameter [" + propName + "] on mapper [" + name + "] of type [" + type + "]"); } if (propNode == null && parameter.acceptsNull == false) { throw new MapperParsingException("[" + propName + "] on mapper [" + name + "] of type [" + type + "] must not have a [null] value"); } parameter.parse(name, propNode); iterator.remove(); } }	maybe saying "has no effect" would be clearer to users?
public static String nodeFilter(DiscoveryNode node, Job job) { String jobId = job.getId(); if (TransportOpenJobAction.nodeSupportsModelSnapshotVersion(node, job) == false) { return "Not opening job [" + jobId + "] on node [" + JobNodeSelector.nodeNameAndVersion(node) + "], because the job's model snapshot requires a node of version [" + job.getModelSnapshotMinVersion() + "] or higher"; } if (Job.getCompatibleJobTypes(node.getVersion()).contains(job.getJobType()) == false) { return "Not opening job [" + jobId + "] on node [" + JobNodeSelector.nodeNameAndVersion(node) + "], because this node does not support jobs of type [" + job.getJobType() + "]"; } if (TransportOpenJobAction.jobHasRules(job) && node.getVersion().before(DetectionRule.VERSION_INTRODUCED)) { return "Not opening job [" + jobId + "] on node [" + JobNodeSelector.nodeNameAndVersion(node) + "], because jobs using custom_rules require a node of version [" + DetectionRule.VERSION_INTRODUCED + "] or higher"; } return null; }	i think we can get rid of this validation as we're now in 7.x and the detectionrule.version_introduced is 6.4. i'm also ok if you'd rather leave that for a future cleanup pr.
@Override public PersistentTasksCustomMetaData.Assignment getAssignment(StartDataFrameAnalyticsAction.TaskParams params, ClusterState clusterState) { // If we are waiting for an upgrade to complete, we should not assign to a node if (MlMetadata.getMlMetadata(clusterState).isUpgradeMode()) { return AWAITING_UPGRADE; } String id = params.getId(); boolean isMemoryTrackerRecentlyRefreshed = memoryTracker.isRecentlyRefreshed(); if (isMemoryTrackerRecentlyRefreshed == false) { boolean scheduledRefresh = memoryTracker.asyncRefresh(); if (scheduledRefresh) { String reason = "Not opening job [" + id + "] because job memory requirements are stale - refresh requested"; LOGGER.debug(reason); return new PersistentTasksCustomMetaData.Assignment(null, reason); } } JobNodeSelector jobNodeSelector = new JobNodeSelector(clusterState, id, MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME, memoryTracker, node -> nodeFilter(node, id)); // Pass an effectively infinite value for max concurrent opening jobs, because data frame analytics jobs do // not have an "opening" state so would never be rejected for causing too many jobs in the "opening" state PersistentTasksCustomMetaData.Assignment assignment = jobNodeSelector.selectNode( maxOpenJobs, Integer.MAX_VALUE, maxMachineMemoryPercent, isMemoryTrackerRecentlyRefreshed); if (assignment.getExecutorNode() == null) { int numMlNodes = 0; for (DiscoveryNode node : clusterState.getNodes()) { if (MachineLearning.isMlNode(node)) { numMlNodes++; } } if (numMlNodes < maxLazyMLNodes) { // Means we have lazy nodes left to allocate assignment = TransportOpenJobAction.AWAITING_LAZY_ASSIGNMENT; } } return assignment; }	the code that checks for lazy ml nodes seems to also be common between anomaly detection jobs and df-analytics. could we move that in the jobnodeselector too? it would also avoid using transportopenjobaction.awaiting_lazy_assignment in here, as awaiting_lazy_assignment is now a common concept.
public static void tryVirtualLock() { Kernel32Library kernel = Kernel32Library.getInstance(); Pointer process = null; try { process = kernel.GetCurrentProcess(); // By default, Windows limits the number of pages that can be locked. // Thus, we need to first increase the working set size of the JVM by // the amount of memory we wish to lock (Xmx), plus a small overhead (1MB). SizeT size = new SizeT(JvmInfo.jvmInfo().getMem().getHeapInit().getBytes() + (1024 * 1024)); if (!kernel.SetProcessWorkingSetSize(process, size, size)) { logger.warn("Unable to lock JVM memory. Failed to set working set size. Error code " + Native.getLastError()); } else { Kernel32Library.MemoryBasicInformation memInfo = new Kernel32Library.MemoryBasicInformation(); long address = 0; while (kernel.VirtualQueryEx(process, new Pointer(address), memInfo, memInfo.size()) != 0) { boolean lockable = memInfo.State.longValue() == Kernel32Library.MEM_COMMIT && (memInfo.Protect.longValue() & Kernel32Library.PAGE_NOACCESS) != Kernel32Library.PAGE_NOACCESS && (memInfo.Protect.longValue() & Kernel32Library.PAGE_GUARD) != Kernel32Library.PAGE_GUARD; if (lockable) { kernel.VirtualLock(memInfo.BaseAddress, new SizeT(memInfo.RegionSize.longValue())); } // Move to the next region address += memInfo.RegionSize.longValue(); } LOCAL_MLOCKALL = true; } } catch (UnsatisfiedLinkError e) { // this will have already been logged by Kernel32Library, no need to repeat it } finally { if (process != null) { kernel.CloseHandle(process); } } }	do we want to use xmx here? getheapinit is xms, i mean, typically, this will be the same, but by default, they are different in our config
@Override public void nextPage(Configuration cfg, Client client, NamedWriteableRegistry registry, ActionListener<Page> listener) { log.trace("About to execute scroll query {}", scrollId); SearchScrollRequest request = new SearchScrollRequest(scrollId).scroll(cfg.pageTimeout()); client.searchScroll(request, wrap(response -> { handle(response, () -> new SearchHitRowSet(extractors, mask, limit, response), p -> listener.onResponse(p), p -> clear(cfg, client, wrap(success -> listener.onResponse(p), listener::onFailure)), Schema.EMPTY); }, listener::onFailure)); }	shouldn't this log be conditioned by log.istraceenabled()?
@Override public int setValues(CellIdSource.CellValues values, MultiGeoValues.GeoValue geoValue, int precision) { if (precision == 0) { values.resizeCell(1); values.add(0, Geohash.longEncode("")); return 1; } MultiGeoValues.BoundingBox bounds = geoValue.boundingBox(); assert bounds.minX() <= bounds.maxX(); long numLonCells = (long) ((bounds.maxX() - bounds.minX()) / Geohash.lonWidthInDegrees(precision)); long numLatCells = (long) ((bounds.maxY() - bounds.minY()) / Geohash.latHeightInDegrees(precision)); long count = (numLonCells + 1) * (numLatCells + 1); if (count == 1) { String hash = Geohash.stringEncode(bounds.minX(), bounds.minY(), precision); values.resizeCell(1); values.add(0, Geohash.longEncode(hash)); return 1; } else if (count <= precision) { return setValuesByBruteForceScan(values, geoValue, precision, bounds); } else { return setValuesByRasterization("", values, 0, precision, geoValue); } }	should we be more coarse here? the way this code works, wouldn't we re-allocate and copy the long[] potentially 10,000 times?
public void testGeoHash() throws Exception { double x = randomDouble(); double y = randomDouble(); int precision = randomIntBetween(0, Geohash.PRECISION); assertThat(GEOHASH.encode(x, y, precision), equalTo(Geohash.longEncode(x, y, precision))); Rectangle tile = Geohash.toBoundingBox(Geohash.stringEncode(x, y, 5)); GeometryTreeWriter writer = new GeometryTreeWriter( new Rectangle(tile.getMinX() + 0.00001, tile.getMaxX() - 0.00001, tile.getMaxY() - 0.00001, tile.getMinY() + 0.00001), GeoShapeCoordinateEncoder.INSTANCE); BytesStreamOutput output = new BytesStreamOutput(); writer.writeTo(output); output.close(); GeometryTreeReader reader = new GeometryTreeReader(output.bytes().toBytesRef(), GeoShapeCoordinateEncoder.INSTANCE); MultiGeoValues.GeoShapeValue value = new MultiGeoValues.GeoShapeValue(reader); // test shape within tile bounds { CellIdSource.CellValues values = new CellIdSource.CellValues(null, precision, GEOTILE); int count = GEOHASH.setValues(values, value, 5); assertThat(count, equalTo(1)); } { CellIdSource.CellValues values = new CellIdSource.CellValues(null, precision, GEOTILE); int count = GEOHASH.setValues(values, value, 6); assertThat(count, equalTo(32)); } { CellIdSource.CellValues values = new CellIdSource.CellValues(null, precision, GEOTILE); int count = GEOHASH.setValues(values, value, 7); assertThat(count, equalTo(1024)); } }	igor's multipolygon support has been merged, so this can be removed now!
private InferenceStats handleMultiNodeStatsResponse(SearchResponse response, String modelId) { if (response.getAggregations() == null) { logger.trace(() -> new ParameterizedMessage("[{}] no previously stored stats found", modelId)); return null; } Sum failures = response.getAggregations().get(InferenceStats.FAILURE_COUNT.getPreferredName()); Sum missing = response.getAggregations().get(InferenceStats.MISSING_ALL_FIELDS_COUNT.getPreferredName()); Sum count = response.getAggregations().get(InferenceStats.INFERENCE_COUNT.getPreferredName()); Max timeStamp = response.getAggregations().get(InferenceStats.TIMESTAMP.getPreferredName()); return new InferenceStats( missing == null ? 0L : Double.valueOf(missing.getValue()).longValue(), count == null ? 0L : Double.valueOf(count.getValue()).longValue(), failures == null ? 0L : Double.valueOf(failures.getValue()).longValue(), modelId, null, timeStamp == null || (Numbers.isValidDouble(timeStamp.getValue()) == false) ? Instant.now() : Instant.ofEpochMilli(Double.valueOf(timeStamp.getValue()).longValue()) ); }	is it possible for timestamp to be an invalid number here?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); timestamp = in.readVLong(); if (in.readBoolean()) { indices = NodeIndicesStats.readIndicesStats(in); } os = in.readOptionalWriteable(OsStats::new); process = in.readOptionalWriteable(ProcessStats::new); jvm = in.readOptionalWriteable(JvmStats::new); threadPool = in.readOptionalWriteable(ThreadPoolStats::new); fs = in.readOptionalWriteable(FsInfo::new); transport = in.readOptionalWriteable(TransportStats::new); http = in.readOptionalWriteable(HttpStats::new); breaker = in.readOptionalWriteable(AllCircuitBreakerStats::new); scriptStats = in.readOptionalWriteable(ScriptStats::new); discoveryStats = in.readOptionalWriteable(DiscoveryStats::new); ingestStats = in.readOptionalWriteable(IngestStats::new); if (in.getVersion().onOrAfter(Version.V_6_1_0)) { adaptiveSelectionStats = in.readOptionalWriteable(AdaptiveSelectionStats::new); } else { adaptiveSelectionStats = null; } if (in.getVersion().onOrAfter(Version.V_6_5_0) && in.readBoolean()) { shardsStats = in.readList(ShardStats::readShardStats); } else { shardsStats = null; } }	more for my own education: why do we need the version check here and in other places in this pr?
protected LogicalPlan rule(LogicalPlan plan) { if (plan instanceof Project) { Project p = (Project) plan; return new Project(p.source(), p.child(), cleanChildrenAliases(p.projections())); } if (plan instanceof Aggregate) { Aggregate a = (Aggregate) plan; // aliases inside GROUP BY are irelevant so remove all of them // however aggregations are important (ultimately a projection) return new Aggregate(a.source(), a.child(), cleanAllAliases(a.groupings()), cleanChildrenAliases(a.aggregates())); } if (plan instanceof Pivot) { Pivot p = (Pivot) plan; return new Pivot(p.source(), p.child(), trimAliases(p.column()), cleanChildrenAliases(p.values()), cleanChildrenAliases(p.aggregates())); } return plan.transformExpressionsOnly(e -> { if (e instanceof Alias) { return ((Alias) e).child(); } return e; }); }	i think both versions are wrong, it should be irrelevant.
@Override public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null || getClass() != obj.getClass()) { return false; } Literal other = (Literal) obj; // add shortcut for null since the dataType become irrelevant return (value == null && other.value == null) || (Objects.equals(value, other.value) && Objects.equals(dataType, other.dataType)); }	this means that a null of type integer is equal to a null of type varchar. not 100% if this is right or not.
@Override public Object fold() { return value; } // @Override // public ScriptTemplate asScript() { // return new ScriptTemplate(String.valueOf(value), Params.EMPTY, dataType); // }	remove if not used anymore?
private AttributeSet valuesOutput() { if (valueOutput == null) { List<Attribute> out = new ArrayList<>(aggregates.size() * values.size()); if (aggregates.size() == 1) { NamedExpression agg = aggregates.get(0); for (NamedExpression value : values) { out.add(value.toAttribute().withDataType(agg.dataType())); } } // for multiple args, concat the function and the value else { for (NamedExpression agg : aggregates) { String name = agg.name(); if (agg instanceof Alias) { Alias a = (Alias) agg; if (a.child() instanceof Function) { name = ((Function) a.child()).functionName(); } } //FIXME: the value attributes are reused and thus will clash - new ids need to be created for (NamedExpression value : values) { out.add(value.toAttribute().withName(value.name() + "_" + name).withDataType(agg.dataType())); } } } valueOutput = new AttributeSet(out); } return valueOutput; }	is this currently concerning or could be fixed later?
private void toString(StringBuilder sb, Object obj) { if (obj instanceof Iterable) { sb.append("["); for (Iterator<?> it = ((Iterable<?>) obj).iterator(); it.hasNext();) { Object o = it.next(); toString(sb, o); if (it.hasNext() == true) { sb.append(',').append(' '); } } sb.append("]"); } else if (obj instanceof Node<?>) { sb.append(((Node<?>) obj).nodeString()); } else { sb.append(Objects.toString(obj)); } }	very minor: you could directly append ", ".
public void testFoldingOfPercentileSecondArgument() { PhysicalPlan p = plan("SELECT PERCENTILE(int, 1 + 2) FROM test"); assertEquals(EsQueryExec.class, p.getClass()); EsQueryExec ee = (EsQueryExec) p; assertEquals(1, ee.output().size()); assertEquals(ReferenceAttribute.class, ee.output().get(0).getClass()); }	can we still test somehow the 1 + 2 = 3 ?
* @param config The config for the values source metric. */ public static Function<byte[], Number> getPointReaderOrNull(SearchContext context, Aggregator parent, ValuesSourceConfig<ValuesSource.Numeric> config) { if (context.query() != null && context.query().getClass() != MatchAllDocsQuery.class) { return null; } if (parent != null) { return null; } if (config.fieldContext() != null && config.script() == null) { MappedFieldType fieldType = config.fieldContext().fieldType(); if (fieldType == null || fieldType.indexOptions() == IndexOptions.NONE) { return null; } Function<byte[], Number> converter = null; if (fieldType instanceof NumberFieldMapper.NumberFieldType) { converter = ((NumberFieldMapper.NumberFieldType) fieldType)::parsePoint; } else if (fieldType.getClass() == DateFieldMapper.DateFieldType.class) { converter = (in) -> LongPoint.decodeDimension(in, 0); } return converter; } return null; } /** * Returns the minimum value indexed in the <code>fieldName</code> field or <code>null</code> * if the value cannot be inferred from the indexed {@link PointValues}	i'd rather use a similar mechanism as max and avoid throwing an exception here.
public static AnalysisConfig.Builder createRandomized() { boolean isCategorization = randomBoolean(); List<Detector> detectors = new ArrayList<>(); int numDetectors = randomIntBetween(1, 10); for (int i = 0; i < numDetectors; i++) { Detector.Builder builder = new Detector.Builder("count", null); builder.setPartitionFieldName(isCategorization ? "mlcategory" : "part"); detectors.add(builder.build()); } AnalysisConfig.Builder builder = new AnalysisConfig.Builder(detectors); if (randomBoolean()) { TimeValue bucketSpan = TimeValue.timeValueSeconds(randomIntBetween(1, 1_000_000)); builder.setBucketSpan(bucketSpan); } if (isCategorization) { builder.setCategorizationFieldName(randomAlphaOfLength(10)); if (randomBoolean()) { builder.setCategorizationFilters(Arrays.asList(generateRandomStringArray(10, 10, false))); } else { CategorizationAnalyzerConfig.Builder analyzerBuilder = new CategorizationAnalyzerConfig.Builder(); if (rarely()) { analyzerBuilder.setAnalyzer(randomAlphaOfLength(10)); } else { if (randomBoolean()) { for (String pattern : generateRandomStringArray(3, 40, false)) { Map<String, Object> charFilter = new HashMap<>(); charFilter.put("type", "pattern_replace"); charFilter.put("pattern", pattern); analyzerBuilder.addCharFilter(charFilter); } } Map<String, Object> tokenizer = new HashMap<>(); tokenizer.put("type", "pattern"); tokenizer.put("pattern", randomAlphaOfLength(10)); analyzerBuilder.setTokenizer(tokenizer); if (randomBoolean()) { for (String pattern : generateRandomStringArray(4, 40, false)) { Map<String, Object> tokenFilter = new HashMap<>(); tokenFilter.put("type", "pattern_replace"); tokenFilter.put("pattern", pattern); analyzerBuilder.addTokenFilter(tokenFilter); } } } builder.setCategorizationAnalyzerConfig(analyzerBuilder.build()); } } if (randomBoolean()) { builder.setLatency(TimeValue.timeValueSeconds(randomIntBetween(1, 1_000_000))); } if (randomBoolean()) { builder.setMultivariateByFields(randomBoolean()); } if (randomBoolean()) { builder.setOverlappingBuckets(randomBoolean()); } if (randomBoolean()) { builder.setResultFinalizationWindow(randomNonNegativeLong()); } boolean usePerPartitionNormalisation = randomBoolean(); builder.setUsePerPartitionNormalization(usePerPartitionNormalisation); if (!usePerPartitionNormalisation) { // influencers can't be used with per partition normalisation builder.setInfluencers(Arrays.asList(generateRandomStringArray(10, 10, false))); } return builder; }	this was testing the complex validation in the builder that's been removed, so it's not worth having in the client.
@Override public ClusterState execute(ClusterState currentState) throws Exception { XPackPlugin.checkReadyForXPackCustomMetadata(currentState); final Version oldestNodeVersion = currentState.nodes().getSmallestNonClientNodeVersion(); if (licenseIsCompatible(newLicense, oldestNodeVersion) == false) { throw new IllegalStateException("The provided license is not compatible with node version [" + oldestNodeVersion + "]"); } MetaData currentMetadata = currentState.metaData(); LicensesMetaData licensesMetaData = currentMetadata.custom(LicensesMetaData.TYPE); Version trialVersion = null; if (licensesMetaData != null) { trialVersion = licensesMetaData.getMostRecentTrialVersion(); } MetaData.Builder mdBuilder = MetaData.builder(currentMetadata); mdBuilder.putCustom(LicensesMetaData.TYPE, new LicensesMetaData(newLicense, trialVersion)); return ClusterState.builder(currentState).metaData(mdBuilder).build(); }	iirc the suggested approach is to use the targeted version here and set bwc_enabled to false in build.gradle until the backporting pr is merged
void startTrialLicense(PostStartTrialRequest request, final ActionListener<PostStartTrialResponse> listener) { License.LicenseType requestedType = License.LicenseType.parse(request.getType()); if (VALID_TRIAL_TYPES.contains(requestedType) == false) { throw new IllegalArgumentException("Cannot start trial of type [" + request.getType() + "]. Valid trial types are " + VALID_TRIAL_TYPES + "."); } StartTrialClusterTask task = new StartTrialClusterTask(logger, clusterService.getClusterName().value(), clock, request, listener); clusterService.submitStateUpdateTask("started trial license", task); }	request.gettype() to requestedtype.gettypename() ?
@Override public void finalizeRecovery(final long globalCheckpoint, final long startingSeqNo, ActionListener<Void> listener) { ActionListener.completeWith(listener, () -> { indexShard.updateGlobalCheckpointOnReplica(globalCheckpoint, "finalizing recovery"); // Persist the global checkpoint. indexShard.sync(); indexShard.persistRetentionLeases(); if (hasUncommittedOperations()) { indexShard.flush(new FlushRequest().force(true).waitIfOngoing(true)); } if (startingSeqNo != SequenceNumbers.UNASSIGNED_SEQ_NO) { // We should erase all translog operations >= startingSeqNo as we have received either the same or a newer copy // from the recovery source in phase2. Rolling a new translog generation is not strictly required here for we won't // trim the current generation. It's merely to satisfy the assumption that the current generation does not have any // operation that would be trimmed (see TranslogWriter#assertNoSeqAbove). This assumption does not hold for peer // recovery because we could have received operations above startingSeqNo from the previous primary terms. indexShard.rollTranslogGeneration(); indexShard.trimOperationOfPreviousPrimaryTerms(startingSeqNo - 1); } indexShard.finalizeRecovery(); return null; }); }	this relies on the fact that for peer recovery, operations that already exist on the shard (i.e. hasprocessed() == true) are still added to the translog (i.e. origin.type() == peer_recovery & version conflict). this is quite subtle, and i could easily see us in the future removing that (i.e. avoid writing op to translog if we already know it's there). we should have an explicit test for that, with a long explanation why we do this.
public void testWriteBlobWithRetries() throws Exception { final int maxRetries = randomInt(5); final CountDown countDown = new CountDown(maxRetries + 1); final byte[] bytes = randomByteArrayOfLength(randomIntBetween(1, frequently() ? 512 : 1 << 20)); // rarely up to 1mb final long expectedChecksum = checksum(new ByteArrayInputStream(bytes), bytes.length); httpServer.createContext("/bucket/write_blob_max_retries", exchange -> { if ("PUT".equals(exchange.getRequestMethod()) && exchange.getRequestURI().getQuery() == null) { if (countDown.countDown()) { int contentLength = Integer.parseInt(exchange.getRequestHeaders().getFirst("Content-Length")); long checksum = checksum(exchange.getRequestBody(), contentLength); if (checksum == expectedChecksum) { exchange.sendResponseHeaders(HttpStatus.SC_OK, -1); } else { exchange.sendResponseHeaders(HttpStatus.SC_BAD_REQUEST, -1); } exchange.close(); return; } if (randomBoolean()) { if (randomBoolean()) { Streams.readFully(exchange.getRequestBody(), new byte[randomIntBetween(1, bytes.length - 1)]); } else { Streams.readFully(exchange.getRequestBody()); exchange.sendResponseHeaders(randomFrom(HttpStatus.SC_INTERNAL_SERVER_ERROR, HttpStatus.SC_BAD_GATEWAY, HttpStatus.SC_SERVICE_UNAVAILABLE, HttpStatus.SC_GATEWAY_TIMEOUT), -1); } } exchange.close(); } }); final BlobContainer blobContainer = createBlobContainer(maxRetries, null, true, null); try (InputStream stream = new InputStreamIndexInput(new ByteArrayIndexInput("desc", bytes), bytes.length)) { blobContainer.writeBlob("write_blob_max_retries", stream, bytes.length, false); } assertThat(countDown.isCountedDown(), is(true)); }	why do we need the checksum? :d i'm not sure i understand why it's better here it just seems more confusing :)
public void markShardAsEvictedInCache(SnapshotId snapshotId, IndexId indexId, ShardId shardId) { final ShardEviction shardEviction = new ShardEviction(snapshotId, indexId, shardId); if (evictedShards.add(shardEviction)) { threadPool.generic().submit(new AbstractRunnable() { @Override protected void doRun() { runIfShardMarkedAsEvictedInCache(shardEviction, () -> { assert shardsEvictionLock.isHeldByCurrentThread(shardEviction); final Map<CacheKey, CacheFile> cacheFilesToEvict = new HashMap<>(); cache.forEach((cacheKey, cacheFile) -> { if (shardEviction.matches(cacheKey)) { cacheFilesToEvict.put(cacheKey, cacheFile); } }); if (cacheFilesToEvict.isEmpty() == false) { for (Map.Entry<CacheKey, CacheFile> cacheFile : cacheFilesToEvict.entrySet()) { try { cache.invalidate(cacheFile.getKey(), cacheFile.getValue()); } catch (Exception e) { assert e instanceof IOException : e; logger.warn(() -> new ParameterizedMessage("failed to evict cache file {}", cacheFile.getKey()), e); } } } }); } @Override public void onFailure(Exception e) { logger.warn( () -> new ParameterizedMessage("failed to evict cache files associated with evicted shard {}", shardEviction), e ); } }); } } /** * Allows to run the specified {@link Runnable} if the shard represented by the triplet ({@link SnapshotId}, {@link IndexId}, * {@link SnapshotId}	nit: this check seems superfluous?
public void markShardAsEvictedInCache(SnapshotId snapshotId, IndexId indexId, ShardId shardId) { final ShardEviction shardEviction = new ShardEviction(snapshotId, indexId, shardId); if (evictedShards.add(shardEviction)) { threadPool.generic().submit(new AbstractRunnable() { @Override protected void doRun() { runIfShardMarkedAsEvictedInCache(shardEviction, () -> { assert shardsEvictionLock.isHeldByCurrentThread(shardEviction); final Map<CacheKey, CacheFile> cacheFilesToEvict = new HashMap<>(); cache.forEach((cacheKey, cacheFile) -> { if (shardEviction.matches(cacheKey)) { cacheFilesToEvict.put(cacheKey, cacheFile); } }); if (cacheFilesToEvict.isEmpty() == false) { for (Map.Entry<CacheKey, CacheFile> cacheFile : cacheFilesToEvict.entrySet()) { try { cache.invalidate(cacheFile.getKey(), cacheFile.getValue()); } catch (Exception e) { assert e instanceof IOException : e; logger.warn(() -> new ParameterizedMessage("failed to evict cache file {}", cacheFile.getKey()), e); } } } }); } @Override public void onFailure(Exception e) { logger.warn( () -> new ParameterizedMessage("failed to evict cache files associated with evicted shard {}", shardEviction), e ); } }); } } /** * Allows to run the specified {@link Runnable} if the shard represented by the triplet ({@link SnapshotId}, {@link IndexId}, * {@link SnapshotId}	can we just catch runtimeexception instead, since invalidate declares to not throw checked?
public void markShardAsEvictedInCache(SnapshotId snapshotId, IndexId indexId, ShardId shardId) { final ShardEviction shardEviction = new ShardEviction(snapshotId, indexId, shardId); if (evictedShards.add(shardEviction)) { threadPool.generic().submit(new AbstractRunnable() { @Override protected void doRun() { runIfShardMarkedAsEvictedInCache(shardEviction, () -> { assert shardsEvictionLock.isHeldByCurrentThread(shardEviction); final Map<CacheKey, CacheFile> cacheFilesToEvict = new HashMap<>(); cache.forEach((cacheKey, cacheFile) -> { if (shardEviction.matches(cacheKey)) { cacheFilesToEvict.put(cacheKey, cacheFile); } }); if (cacheFilesToEvict.isEmpty() == false) { for (Map.Entry<CacheKey, CacheFile> cacheFile : cacheFilesToEvict.entrySet()) { try { cache.invalidate(cacheFile.getKey(), cacheFile.getValue()); } catch (Exception e) { assert e instanceof IOException : e; logger.warn(() -> new ParameterizedMessage("failed to evict cache file {}", cacheFile.getKey()), e); } } } }); } @Override public void onFailure(Exception e) { logger.warn( () -> new ParameterizedMessage("failed to evict cache files associated with evicted shard {}", shardEviction), e ); } }); } } /** * Allows to run the specified {@link Runnable} if the shard represented by the triplet ({@link SnapshotId}, {@link IndexId}, * {@link SnapshotId}	looking closer at this, i suppose this should never happen since we catch io exceptions in oncachefileremoval, so we could just assert false : e here instead?
public void markShardAsEvictedInCache(SnapshotId snapshotId, IndexId indexId, ShardId shardId) { final ShardEviction shardEviction = new ShardEviction(snapshotId, indexId, shardId); if (evictedShards.add(shardEviction)) { threadPool.generic().submit(new AbstractRunnable() { @Override protected void doRun() { runIfShardMarkedAsEvictedInCache(shardEviction, () -> { assert shardsEvictionLock.isHeldByCurrentThread(shardEviction); final Map<CacheKey, CacheFile> cacheFilesToEvict = new HashMap<>(); cache.forEach((cacheKey, cacheFile) -> { if (shardEviction.matches(cacheKey)) { cacheFilesToEvict.put(cacheKey, cacheFile); } }); if (cacheFilesToEvict.isEmpty() == false) { for (Map.Entry<CacheKey, CacheFile> cacheFile : cacheFilesToEvict.entrySet()) { try { cache.invalidate(cacheFile.getKey(), cacheFile.getValue()); } catch (Exception e) { assert e instanceof IOException : e; logger.warn(() -> new ParameterizedMessage("failed to evict cache file {}", cacheFile.getKey()), e); } } } }); } @Override public void onFailure(Exception e) { logger.warn( () -> new ParameterizedMessage("failed to evict cache files associated with evicted shard {}", shardEviction), e ); } }); } } /** * Allows to run the specified {@link Runnable} if the shard represented by the triplet ({@link SnapshotId}, {@link IndexId}, * {@link SnapshotId}	i think we could also assert false : e here?
private static SortedDocValuesField createJoinField(String parentType, String id) { return new SortedDocValuesField("join_field#" + parentType, new BytesRef(id)); }	replaced with withjoinfields which allows us to use a non-mocked version.
public QueryShardContext newQueryShardContext( int shardId, IndexSearcher searcher, LongSupplier nowInMillis, String clusterAlias, Map<String, Object> runtimeMappings ) { final SearchIndexNameMatcher indexNameMatcher = new SearchIndexNameMatcher(index().getName(), clusterAlias, clusterService, expressionResolver); return new QueryShardContext( shardId, indexSettings, bigArrays, indexCache.bitsetFilterCache(), indexFieldData::getForField, mapperService(), mapperService().lookup(), similarityService(), scriptService, xContentRegistry, namedWriteableRegistry, client, searcher, nowInMillis, clusterAlias, indexNameMatcher, allowExpensiveQueries, valuesSourceRegistry, runtimeMappings ); } /** * The {@link ThreadPool}	i *could* pass just the mapperservice here but then we'd often have to mock the mapper service just to return the lookup.
public boolean hasNested() { return lookup().hasNested(); }	instead of two volatile reads this method now does one. i think that is nice.
* directly associated index-time analyzer */ public NamedAnalyzer indexAnalyzer(String field, Function<String, NamedAnalyzer> unindexedFieldAnalyzer) { return lookup().indexAnalyzer(field, unindexedFieldAnalyzer); }	i remembered that search analyzers can be [reloaded](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-reload-analyzers.html) which doesn't cause a mapping update but changes how fields are analyzed! this is also an important way in which mappinglookup is not immutable.
public void loadIntoContext(ShardSearchRequest request, SearchContext context, QueryPhase queryPhase) throws Exception { assert canCache(request, context); final DirectoryReader directoryReader = context.searcher().getDirectoryReader(); boolean[] loadedFromCache = new boolean[] { true }; BytesReference cacheKey = request.cacheKey(context.getQueryShardContext().mappingKey()); BytesReference bytesReference = cacheShardLevelResult(context.indexShard(), directoryReader, cacheKey, out -> { queryPhase.execute(context); context.queryResult().writeToNoId(out); loadedFromCache[0] = false; }); if (loadedFromCache[0]) { // restore the cached query result into the context final QuerySearchResult result = context.queryResult(); StreamInput in = new NamedWriteableAwareStreamInput(bytesReference.streamInput(), namedWriteableRegistry); result.readFromWithId(context.id(), in); result.setSearchShardTarget(context.shardTarget()); } else if (context.queryResult().searchTimedOut()) { // we have to invalidate the cache entry if we cached a query result form a request that timed out. // we can't really throw exceptions in the loading part to signal a timed out search to the outside world since if there are // multiple requests that wait for the cache entry to be calculated they'd fail all with the same exception. // instead we all caching such a result for the time being, return the timed out result for all other searches with that cache // key invalidate the result in the thread that caused the timeout. This will end up to be simpler and eventually correct since // running a search that times out concurrently will likely timeout again if it's run while we have this `stale` result in the // cache. One other option is to not cache requests with a timeout at all... indicesRequestCache.invalidate(new IndexShardCacheEntity(context.indexShard()), directoryReader, cacheKey); if (logger.isTraceEnabled()) { logger.trace("Query timed out, invalidating cache entry for request on shard [{}]:\\\\n {}", request.shardId(), request.source()); } } }	i could be missing an important consideration, but could we key on some java object related to mappinglookup instead of appending the serialized source? this would be consistent with what we do for directoryreader, where we use the object reader.getreadercachehelper().getkey().
BulkRequest internalAdd(IndexRequest request) { Objects.requireNonNull(request, "'request' must not be null"); applyGlobalMandatoryParameters(request); request.routing(valueOrDefault(request.routing(), globalRouting)); request.setPipeline(valueOrDefault(request.getPipeline(), globalPipeline)); request.setRequireAlias(valueOrDefault(request.getRequireAlias(), globalRequireAlias)); requests.add(request); // lack of source is validated in validate() method sizeInBytes += (request.source() != null ? request.source().length() : 0) + REQUEST_OVERHEAD; indices.add(request.index()); return this; } /** * Adds an {@link UpdateRequest}	it might be safer to not apply the default routing option. routing is exceptionally tricky and difficult to debug. might be better to create an issue and remove it from this pr.
public void testEqualsDefAndPrimitive() { /* This test needs an Integer that isn't cached by Integer.valueOf so we draw one randomly. We can't use any fixed integer because * we can never be sure that the JVM hasn't configured itself to cache that Integer. It is sneaky like that. */ int uncachedAutoboxedInt = randomValueOtherThanMany(i -> Integer.valueOf(i) == Integer.valueOf(i), ESTestCase::randomInt); assertEquals(true, exec("def x = params.i; int y = params.i; return x == y;", singletonMap("i", uncachedAutoboxedInt), true)); assertEquals(false, exec("def x = params.i; int y = params.i; return x === y;", singletonMap("i", uncachedAutoboxedInt), true)); assertEquals(true, exec("def x = params.i; int y = params.i; return y == x;", singletonMap("i", uncachedAutoboxedInt), true)); assertEquals(false, exec("def x = params.i; int y = params.i; return y === x;", singletonMap("i", uncachedAutoboxedInt), true)); /* Now check that we use valueOf with the boxing used for comparing primitives to def. For this we need an * integer that is cached by Integer.valueOf. The JLS says 0 should always be cached. */ int cachedAutoboxedInt = 0; assertEquals(Integer.valueOf(cachedAutoboxedInt), Integer.valueOf(cachedAutoboxedInt)); assertEquals(true, exec("def x = params.i; int y = params.i; return x == y;", singletonMap("i", cachedAutoboxedInt), true)); assertEquals(true, exec("def x = params.i; int y = params.i; return x === y;", singletonMap("i", cachedAutoboxedInt), true)); assertEquals(true, exec("def x = params.i; int y = params.i; return y == x;", singletonMap("i", cachedAutoboxedInt), true)); assertEquals(true, exec("def x = params.i; int y = params.i; return y === x;", singletonMap("i", cachedAutoboxedInt), true)); }	i took a look because it looked fun, sorry if i'm getting things wrong. if i understand this test right, then these two values should be referentialy equal, so shouldn't this be assertsame?
private String formatUsingMappings(TemporalAccessor temporalAccessor, String pattern) { for (DateTimeFormatMapping mapping : JAVA_TIME_TO_CHAR_MAPPINGS) { if (pattern.contains(mapping.getPattern())) { var middle = mapping .getAdditionalMapper() .apply(format(mapping.getJavaPattern(), temporalAccessor)); var parts = pattern.split(mapping.getPattern(), 2); return formatUsingMappings(temporalAccessor, parts[0]) + middle + formatUsingMappings(temporalAccessor, parts[1]); } } return format(pattern, temporalAccessor); }	also, although this implementation is more concise, my preference would be not to drive the recursion directly by user input. if the splitting would not be limited, the java_time_to_char_mappings would also only need to be walked once (and not restarted on each function call).
private String formatUsingMappings(TemporalAccessor temporalAccessor, String pattern) { for (DateTimeFormatMapping mapping : JAVA_TIME_TO_CHAR_MAPPINGS) { if (pattern.contains(mapping.getPattern())) { var middle = mapping .getAdditionalMapper() .apply(format(mapping.getJavaPattern(), temporalAccessor)); var parts = pattern.split(mapping.getPattern(), 2); return formatUsingMappings(temporalAccessor, parts[0]) + middle + formatUsingMappings(temporalAccessor, parts[1]); } } return format(pattern, temporalAccessor); }	can it be made static?
public static String toRome(int month) { switch (month) { case 1: return "I"; case 2: return "II"; case 3: return "III"; case 4: return "IV"; case 5: return "V"; case 6: return "VI"; case 7: return "VII"; case 8: return "VIII"; case 9: return "IX"; case 10: return "X"; case 11: return "XI"; case 12: return "XII"; default: return ""; } }	your use is safe, but as a "util", i'd be more comfortable with an exception here.
public static void setupTestDependenciesDefaults(Project project, SourceSet sourceSet) { project.getDependencies().add(sourceSet.getImplementationConfigurationName(), project.project(":test:rest-runner")); }	this is also used for java rest tests so i'm not sure this is correct. it "works" since the test framework is a transitive dependency but we don't actually _need_ any of the yaml testing stuff for those.
@Override public DocValuesField<?> getScriptField(String name) { try { if (indexed) { VectorValues values = reader.getVectorValues(field); if (values == null || values == VectorValues.EMPTY) { // There's no way for KnnDenseVectorDocValuesField to reliably differentiate between VectorValues.EMPTY and // values that can be iterated through. Since VectorValues.EMPTY throws on docID(), pass a null instead. values = null; } return new KnnDenseVectorDocValuesField(values, name, dims); } else { BinaryDocValues values = DocValues.getBinary(reader, field); return new BinaryDenseVectorDocValuesField(values, name, dims, indexVersion); } } catch (IOException e) { throw new IllegalStateException("Cannot load doc values for vector field!", e); } }	you don't need the null check here anymore.
@Override public DocValuesField<?> getScriptField(String name) { try { if (indexed) { VectorValues values = reader.getVectorValues(field); if (values == null || values == VectorValues.EMPTY) { // There's no way for KnnDenseVectorDocValuesField to reliably differentiate between VectorValues.EMPTY and // values that can be iterated through. Since VectorValues.EMPTY throws on docID(), pass a null instead. values = null; } return new KnnDenseVectorDocValuesField(values, name, dims); } else { BinaryDocValues values = DocValues.getBinary(reader, field); return new BinaryDenseVectorDocValuesField(values, name, dims, indexVersion); } } catch (IOException e) { throw new IllegalStateException("Cannot load doc values for vector field!", e); } }	i find this behavior to be a bit strange in that vectorvalues.empty will throw an exception, but null does not.
public boolean isDataStreamTimestampFieldEnabled() { DataStreamTimestampFieldMapper dtfm = mapping.getMetadataMapperByClass(DataStreamTimestampFieldMapper.class); return dtfm != null && dtfm.isEnabled(); } /** * Returns if this mapping contains a timestamp field that is of type date and indexed * @return {@code true} if contains an indexed timestamp field of type date, {@code false}	should we also check if doc values are enabled ?
private SqlParser.SqlParameter param(ParserRuleContext ctx) { if (!ctx.getStart().equals(ctx.getStop())) { throw new ParsingException(source(ctx), "Single PARAM literal expected"); } if (params.containsKey(ctx.getStart()) == false) { throw new ParsingException(source(ctx), "Unexpected parameter"); } return params.get(ctx.getStart()); }	incorrect style (use == false instead of !). i'm not sure what this check tries to prevent...
private static Object makeMap(Class<? extends Node<?>> toBuildClass, ParameterizedType pt) throws Exception { Map<Object, Object> map = new HashMap<>(); int size = randomSizeForCollection(toBuildClass); while (map.size() < size) { Object key = makeArg(toBuildClass, pt.getActualTypeArguments()[0]); Object value = makeArg(toBuildClass, pt.getActualTypeArguments()[1]); map.put(key, value); } return map; }	instead of this long if, maybe a final list and use .stream().anymatch()?
void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete, TimeValue maximumTime) { int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum(); if (count == 0) { logger.debug("no snapshots are eligible for deletion"); return; } logger.info("starting snapshot retention deletion for [{}] snapshots", count); long startTime = nowNanoSupplier.getAsLong(); int deleted = 0; int failed = 0; for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) { String repo = entry.getKey(); List<SnapshotInfo> snapshots = entry.getValue(); for (SnapshotInfo info : snapshots) { Optional<SnapshotHistoryItem> result = deleteSnapshot(repo, info); if (result.isPresent()) { if (result.get().isSuccess()) { deleted++; } historyStore.putAsync(result.get()); } else { failed++; } // Check whether we have exceeded the maximum time allowed to spend deleting // snapshots, if we have, short-circuit the rest of the deletions TimeValue elapsedDeletionTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime); logger.trace("elapsed time for deletion of [{}] snapshot: {}", info.snapshotId(), elapsedDeletionTime); if (elapsedDeletionTime.compareTo(maximumTime) > 0) { logger.info("maximum snapshot retention deletion time reached, time spent: [{}]," + " maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]", elapsedDeletionTime, maximumTime, deleted, count, failed); return; } } } }	hmm.. i think a listener-based approach might be better here, maybe something where the deletesnapshot call is passed a function to execute on success and one for failure, these could then increment the deleted and failed counts (they'd have to be atomicintegers then) and it could index the history information also
void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete, TimeValue maximumTime) { int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum(); if (count == 0) { logger.debug("no snapshots are eligible for deletion"); return; } logger.info("starting snapshot retention deletion for [{}] snapshots", count); long startTime = nowNanoSupplier.getAsLong(); int deleted = 0; int failed = 0; for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) { String repo = entry.getKey(); List<SnapshotInfo> snapshots = entry.getValue(); for (SnapshotInfo info : snapshots) { Optional<SnapshotHistoryItem> result = deleteSnapshot(repo, info); if (result.isPresent()) { if (result.get().isSuccess()) { deleted++; } historyStore.putAsync(result.get()); } else { failed++; } // Check whether we have exceeded the maximum time allowed to spend deleting // snapshots, if we have, short-circuit the rest of the deletions TimeValue elapsedDeletionTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime); logger.trace("elapsed time for deletion of [{}] snapshot: {}", info.snapshotId(), elapsedDeletionTime); if (elapsedDeletionTime.compareTo(maximumTime) > 0) { logger.info("maximum snapshot retention deletion time reached, time spent: [{}]," + " maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]", elapsedDeletionTime, maximumTime, deleted, count, failed); return; } } } }	i think this misses the failed path, where the delete returned a response (so ispresent() is true), but the response is a failure. it should increment failed in that case?
* @param info The snapshot metadata * @return If present, a SnapshotHistoryItem containing the results of the deletion. Empty if no response or interrupted. */ Optional<SnapshotHistoryItem> deleteSnapshot(String repo, SnapshotInfo info) { logger.info("[{}] snapshot retention deleting snapshot [{}]", repo, info.snapshotId()); CountDownLatch latch = new CountDownLatch(1); String policyId = (String) info.userMetadata().get(POLICY_ID_METADATA_FIELD); // TODO: use getPolicyID once #45362 is merged AtomicReference<SnapshotHistoryItem> result = new AtomicReference<>(); client.admin().cluster().prepareDeleteSnapshot(repo, info.snapshotId().getName()) .execute(new LatchedActionListener<>(new ActionListener<>() { @Override public void onResponse(AcknowledgedResponse acknowledgedResponse) { if (acknowledgedResponse.isAcknowledged()) { logger.debug("[{}] snapshot [{}] deleted successfully", repo, info.snapshotId()); result.set(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(), info.snapshotId().getName(), policyId, repo)); } } @Override public void onFailure(Exception e) { logger.warn(new ParameterizedMessage("[{}] failed to delete snapshot [{}] for retention", repo, info.snapshotId()), e); try { result.set(SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(), info.snapshotId().getName(), policyId, repo, e)); } catch (IOException ex) { // This shouldn't happen unless there's an issue with serializing the original exception logger.error(new ParameterizedMessage( "failed to record snapshot creation failure for snapshot lifecycle policy [{}]", policyId), e); } } }, latch)); try { // Deletes cannot occur simultaneously, so wait for this // deletion to complete before attempting the next one latch.await(); } catch (InterruptedException e) { logger.error(new ParameterizedMessage("[{}] deletion of snapshot [{}] interrupted", repo, info.snapshotId()), e); } return Optional.ofNullable(result.get()); }	i think this todo can be resolved now :)
* @param info The snapshot metadata * @return If present, a SnapshotHistoryItem containing the results of the deletion. Empty if no response or interrupted. */ Optional<SnapshotHistoryItem> deleteSnapshot(String repo, SnapshotInfo info) { logger.info("[{}] snapshot retention deleting snapshot [{}]", repo, info.snapshotId()); CountDownLatch latch = new CountDownLatch(1); String policyId = (String) info.userMetadata().get(POLICY_ID_METADATA_FIELD); // TODO: use getPolicyID once #45362 is merged AtomicReference<SnapshotHistoryItem> result = new AtomicReference<>(); client.admin().cluster().prepareDeleteSnapshot(repo, info.snapshotId().getName()) .execute(new LatchedActionListener<>(new ActionListener<>() { @Override public void onResponse(AcknowledgedResponse acknowledgedResponse) { if (acknowledgedResponse.isAcknowledged()) { logger.debug("[{}] snapshot [{}] deleted successfully", repo, info.snapshotId()); result.set(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(), info.snapshotId().getName(), policyId, repo)); } } @Override public void onFailure(Exception e) { logger.warn(new ParameterizedMessage("[{}] failed to delete snapshot [{}] for retention", repo, info.snapshotId()), e); try { result.set(SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(), info.snapshotId().getName(), policyId, repo, e)); } catch (IOException ex) { // This shouldn't happen unless there's an issue with serializing the original exception logger.error(new ParameterizedMessage( "failed to record snapshot creation failure for snapshot lifecycle policy [{}]", policyId), e); } } }, latch)); try { // Deletes cannot occur simultaneously, so wait for this // deletion to complete before attempting the next one latch.await(); } catch (InterruptedException e) { logger.error(new ParameterizedMessage("[{}] deletion of snapshot [{}] interrupted", repo, info.snapshotId()), e); } return Optional.ofNullable(result.get()); }	i think the listener/chained model would be better than putting a result in the atomicreference, what do you think?
@Override protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) throws IOException { GetFiltersAction.Request getListRequest = new GetFiltersAction.Request(); String filterId = restRequest.param(MlFilter.ID.getPreferredName()); if (!Strings.isNullOrEmpty(filterId)) { getListRequest.setFilterId(filterId); } if (restRequest.hasParam(PageParams.FROM.getPreferredName()) || restRequest.hasParam(PageParams.SIZE.getPreferredName())) { getListRequest.setPageParams( new PageParams(restRequest.paramAsInt(PageParams.FROM.getPreferredName(), PageParams.DEFAULT_FROM), restRequest.paramAsInt(PageParams.SIZE.getPreferredName(), PageParams.DEFAULT_SIZE))); } else { getListRequest.setPageParams(null); } getListRequest.setAllowNoResources(restRequest.paramAsBoolean(ALLOW_NO_RESOURCES, true)); return channel -> client.execute(GetFiltersAction.INSTANCE, getListRequest, new RestStatusToXContentListener<>(channel)); }	this looks like it has moved beyond refactoring to adding new functionality. i would say that this pr should just hardcode true here, as that's what the behaviour was before. (if we were to have a parameter to say finding no filters was unacceptable then maybe we'd want to call it allow_no_filters.)
private void updateRetentionPolicy() throws IOException { assert Thread.holdsLock(this); logger.debug("Safe commit [{}], last commit [{}]", commitDescription(safeCommit), commitDescription(lastCommit)); assert safeCommit.isDeleted() == false : "The safe commit must not be deleted"; final long minRequiredGen = Long.parseLong(safeCommit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY)); assert lastCommit.isDeleted() == false : "The last commit must not be deleted"; final long lastGen = Long.parseLong(lastCommit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY)); assert minRequiredGen <= lastGen : "minRequiredGen must not be greater than lastGen"; translogDeletionPolicy.setTranslogGenerationOfLastCommit(lastGen); translogDeletionPolicy.setMinTranslogGenerationForRecovery(minRequiredGen); final long localCheckpointOfSafeCommit = Long.parseLong(safeCommit.getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY)); softDeletesPolicy.setLocalCheckpointOfSafeCommit(localCheckpointOfSafeCommit); final long docCountOfSafeCommit = getDocCountOfSafeCommit(); minimumReasonableRetainedSeqNo = localCheckpointOfSafeCommit + 1 - Math.round(Math.ceil(docCountOfSafeCommit * reasonableOperationsBasedRecoveryProportion)); logger.trace("updating minimum reasonable retained seqno: " + "generation={}, localCheckpointOfSafeCommit={}, docCountOfSafeCommit={}, minimumReasonableRetainedSeqNo={}", safeCommit.getGeneration(), localCheckpointOfSafeCommit, docCountOfSafeCommit, minimumReasonableRetainedSeqNo); }	drive by comment: i think this will do io while holding the monitor. this would then prevent concurrent acquireindexcommit and releaseindexcommit calls. a quick scrape of usages did not find any where this would hurt, but i would anyway suggest to do the calculation of minimumreasonableretainedseqno outside the monitor instead.
private void onFinalResponse(AsyncSearchTask searchTask, AsyncSearchResponse response, Runnable nextAction) { store.updateResponse(searchTask.getExecutionId().getDocId(), threadContext.getResponseHeaders(),response, ActionListener.wrap(resp -> unregisterTaskAndMoveOn(searchTask, nextAction), exc -> { Throwable cause = ExceptionsHelper.unwrapCause(exc); if (cause instanceof DocumentMissingException == false && cause instanceof VersionConflictEngineException == false) { logger.error(() -> new ParameterizedMessage("failed to store async-search [{}]", searchTask.getExecutionId().getEncoded()), exc); updateStoredResponseWithFailure(searchTask.getExecutionId().getDocId(), response, exc); } unregisterTaskAndMoveOn(searchTask, nextAction); })); }	i think we should only call unregistertaskandmoveon after updatestoredresponsewithfailure has completed.
default Settings getAdditionalIndexSettings( String indexName, String dataStreamName, Metadata metadata, long resolvedAt, Settings templateAndRequestSettings ) { return Settings.EMPTY; }	i think this will conflict with #81986, so maybe that one should be merged first?
public Settings getAdditionalIndexSettings( String indexName, String dataStreamName, Metadata metadata, long resolvedAt, Settings templateAndRequestSettings ) { if (dataStreamName != null) { IndexMode indexMode = Optional.ofNullable(templateAndRequestSettings.get(IndexSettings.MODE.getKey())) .map(value -> IndexMode.valueOf(value.toUpperCase(Locale.ROOT))) .orElse(IndexMode.STANDARD); if (indexMode == IndexMode.TIME_SERIES) { TimeValue lookAheadTime = templateAndRequestSettings.getAsTime( IndexSettings.LOOK_AHEAD_TIME.getKey(), IndexSettings.LOOK_AHEAD_TIME.getDefault(templateAndRequestSettings) ); Settings.Builder builder = Settings.builder(); DataStream dataStream = metadata.dataStreams().get(dataStreamName); Instant start; if (dataStream == null) { start = Instant.ofEpochMilli(resolvedAt).minusMillis(lookAheadTime.getMillis()); } else { IndexMetadata currentLatestBackingIndex = metadata.index(dataStream.getWriteIndex()); if (currentLatestBackingIndex.getSettings().hasValue(IndexSettings.TIME_SERIES_END_TIME.getKey()) == false) { throw new IllegalStateException( String.format( Locale.ROOT, "backing index [%s] in tsdb mode doesn't have the [%s] index setting", currentLatestBackingIndex.getIndex().getName(), IndexSettings.TIME_SERIES_START_TIME.getKey() ) ); } start = IndexSettings.TIME_SERIES_END_TIME.get(currentLatestBackingIndex.getSettings()); } builder.put(IndexSettings.TIME_SERIES_START_TIME.getKey(), FORMATTER.format(start)); Instant end = Instant.ofEpochMilli(resolvedAt).plusMillis(lookAheadTime.getMillis()); builder.put(IndexSettings.TIME_SERIES_END_TIME.getKey(), FORMATTER.format(end)); return builder.build(); } } return Settings.EMPTY; }	doesn't this mean that you cannot switch a data stream from regular mode (such as before tsdb is released) to the timeseries mode, or else rollover won't work due to this exception? we'll likely need a way to allow switching a data stream into this mode without having to recreated the entire data stream, right? is this something we'll implement or relax in later work, or should we handle it here?
private static void setupDownloadServiceRepo(Project project) { if (project.getRepositories().findByName(DOWNLOAD_REPO_NAME) != null) { return; } addIvyRepo(project, DOWNLOAD_REPO_NAME, "https://artifacts.elastic.co", FAKE_IVY_GROUP); if (ClasspathUtils.isElasticsearchProject(project) == false) { // external, so add snapshot repo as well addIvyRepo(project, SNAPSHOT_REPO_NAME, "https://snapshots.elastic.co", FAKE_SNAPSHOT_IVY_GROUP); } project.getRepositories().all(repo -> { if (repo.getName().equals(DOWNLOAD_REPO_NAME) == false) { // all other repos should ignore the special group name repo.content(content -> content.excludeGroup(FAKE_IVY_GROUP)); } if (repo.getName().equals(SNAPSHOT_REPO_NAME) == false) { // all other repos should ignore the special group name repo.content(content -> content.excludeGroup(FAKE_SNAPSHOT_IVY_GROUP)); } }); } /** * Returns a dependency object representing the given distribution. * * The returned object is suitable to be passed to {@link DependencyHandler}. * The concrete type of the object will either be a project {@link Dependency} or * a set of maven coordinates as a {@link String}	it might make sense to add the registration of this ignore rule into the addivyrepo() method itself so we don't have to hard-code these for every repo we add.
private Object dependencyNotation(Project project, ElasticsearchDistribution distribution) { if (ClasspathUtils.isElasticsearchProject(project)) { // non-external project, so depend on local build if (VersionProperties.getElasticsearch().equals(distribution.getVersion())) { return projectDependency(project, distributionProjectPath(distribution), "default"); // TODO: snapshot dep when not in ES repo } BwcVersions.UnreleasedVersionInfo unreleasedInfo = bwcVersions.unreleasedInfo(Version.fromString(distribution.getVersion())); if (unreleasedInfo != null) { assert distribution.getBundledJdk(); return projectDependency(project, unreleasedInfo.gradleProjectPath, distributionProjectName(distribution)); } } if (distribution.getType() == Type.INTEG_TEST_ZIP) { return "org.elasticsearch.distribution.integ-test-zip:elasticsearch:" + distribution.getVersion(); } Version distroVersion = Version.fromString(distribution.getVersion()); String extension = distribution.getType().toString(); String classifier = ":x86_64"; if (distribution.getType() == Type.ARCHIVE) { extension = distribution.getPlatform() == Platform.WINDOWS ? "zip" : "tar.gz"; if (distroVersion.onOrAfter("7.0.0")) { classifier = ":" + distribution.getPlatform() + "-x86_64"; } else { classifier = ""; } } else if (distribution.getType() == Type.DEB) { classifier = ":amd64"; } String flavor = ""; if (distribution.getFlavor() == Flavor.OSS && distroVersion.onOrAfter("6.3.0")) { flavor = "-oss"; } String group = distribution.getVersion().endsWith("-SNAPSHOT") ? FAKE_SNAPSHOT_IVY_GROUP : FAKE_IVY_GROUP; return group + ":elasticsearch" + flavor + ":" + distribution.getVersion() + classifier + "@" + extension; }	what was the motivation of changing elasticsearchdistribution to store the version as a string if that then forces us to re-parse it into a version in places. was calling tostring() too cumbersome?
@Override protected PrimaryResult shardOperationOnPrimary(ShardFlushRequest shardRequest, IndexShard primary) { primary.flush(shardRequest.getRequest()); logger.trace("{} flush request executed on primary", primary.shardId()); return new PrimaryResult(shardRequest, new ReplicationResponse(), null); }	can we have constructor overloads for this? i.e., primaryresult(request, response) and primaryresult(request, failure)?
private Translog.Location executeBulkItemRequest(IndexMetaData metaData, IndexShard primary, BulkShardRequest request, long[] preVersions, VersionType[] preVersionTypes, Translog.Location location, int requestIndex) throws Exception { final DocWriteRequest itemRequest = request.items()[requestIndex].request(); preVersions[requestIndex] = itemRequest.version(); preVersionTypes[requestIndex] = itemRequest.versionType(); DocWriteRequest.OpType opType = itemRequest.opType(); try { // execute item request final Engine.Result operationResult; final DocWriteResponse response; switch (itemRequest.opType()) { case CREATE: case INDEX: final IndexRequest indexRequest = (IndexRequest) itemRequest; operationResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdatedAction); response = operationResult.hasFailure() ? null : new IndexResponse(primary.shardId(), indexRequest.type(), indexRequest.id(), operationResult.getVersion(), ((Engine.IndexResult) operationResult).isCreated()); break; case UPDATE: UpdateResultHolder updateResultHolder = executeUpdateRequest(((UpdateRequest) itemRequest), primary, metaData, request, requestIndex); operationResult = updateResultHolder.operationResult; response = updateResultHolder.response; break; case DELETE: final DeleteRequest deleteRequest = (DeleteRequest) itemRequest; operationResult = executeDeleteRequestOnPrimary(deleteRequest, primary); response = operationResult.hasFailure() ? null : new DeleteResponse(request.shardId(), deleteRequest.type(), deleteRequest.id(), operationResult.getVersion(), ((Engine.DeleteResult) operationResult).isFound()); break; default: throw new IllegalStateException("unexpected opType [" + itemRequest.opType() + "] found"); } // update the bulk item request because update request execution can mutate the bulk item request BulkItemRequest item = request.items()[requestIndex]; if (operationResult == null // in case of a noop update operation || operationResult.hasFailure() == false) { if (operationResult != null) { location = locationToSync(location, operationResult.getLocation()); } else { assert response.getResult() == DocWriteResponse.Result.NOOP : "only noop update can have null operation"; } // set update response item.setPrimaryResponse(new BulkItemResponse(item.id(), opType, response)); } else { DocWriteRequest docWriteRequest = item.request(); Exception failure = operationResult.getFailure(); if (isConflictException(failure)) { logger.trace((Supplier<?>) () -> new ParameterizedMessage("{} failed to execute bulk item ({}) {}", request.shardId(), docWriteRequest.opType().getLowercase(), request), failure); } else { logger.debug((Supplier<?>) () -> new ParameterizedMessage("{} failed to execute bulk item ({}) {}", request.shardId(), docWriteRequest.opType().getLowercase(), request), failure); } // if its a conflict failure, and we already executed the request on a primary (and we execute it // again, due to primary relocation and only processing up to N bulk items when the shard gets closed) // then just use the response we got from the successful execution if (item.getPrimaryResponse() == null || isConflictException(failure) == false) { item.setPrimaryResponse(new BulkItemResponse(item.id(), docWriteRequest.opType(), new BulkItemResponse.Failure(request.index(), docWriteRequest.type(), docWriteRequest.id(), failure))); } } assert item.getPrimaryResponse() != null; assert preVersionTypes[requestIndex] != null; if (item.getPrimaryResponse().isFailed() || item.getPrimaryResponse().getResponse().getResult() == DocWriteResponse.Result.NOOP) { item.setIgnoreOnReplica(); } else { // set the ShardInfo to 0 so we can safely send it to the replicas. We won't use it in the real response though. item.getPrimaryResponse().getResponse().setShardInfo(new ShardInfo()); } } catch (Exception e) { // rethrow the failure if we are going to retry on primary and let parent failure to handle it if (retryPrimaryException(e)) { // restore updated versions... for (int j = 0; j < requestIndex; j++) { DocWriteRequest docWriteRequest = request.items()[j].request(); docWriteRequest.version(preVersions[j]); docWriteRequest.versionType(preVersionTypes[j]); } throw e; } // TODO: maybe this assert is too strict, we can still get environment failures while executing write operations assert false : "unexpected exception: " + e.getMessage() + " class:" + e.getClass().getSimpleName(); } return location; }	can we separate this nested ifs into: if (operation == null) { assert noop item.setprimaryresponse(new bulkitemresponse(item.id(), optype, response)) } else (operationresult.hasfailure() == false) { success! item.setprimaryresponse(new bulkitemresponse(item.id(), optype, response)) } else { // failure } i think it will be easier to follow and well worth the price of the on line duplicate
private Translog.Location executeBulkItemRequest(IndexMetaData metaData, IndexShard primary, BulkShardRequest request, long[] preVersions, VersionType[] preVersionTypes, Translog.Location location, int requestIndex) throws Exception { final DocWriteRequest itemRequest = request.items()[requestIndex].request(); preVersions[requestIndex] = itemRequest.version(); preVersionTypes[requestIndex] = itemRequest.versionType(); DocWriteRequest.OpType opType = itemRequest.opType(); try { // execute item request final Engine.Result operationResult; final DocWriteResponse response; switch (itemRequest.opType()) { case CREATE: case INDEX: final IndexRequest indexRequest = (IndexRequest) itemRequest; operationResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdatedAction); response = operationResult.hasFailure() ? null : new IndexResponse(primary.shardId(), indexRequest.type(), indexRequest.id(), operationResult.getVersion(), ((Engine.IndexResult) operationResult).isCreated()); break; case UPDATE: UpdateResultHolder updateResultHolder = executeUpdateRequest(((UpdateRequest) itemRequest), primary, metaData, request, requestIndex); operationResult = updateResultHolder.operationResult; response = updateResultHolder.response; break; case DELETE: final DeleteRequest deleteRequest = (DeleteRequest) itemRequest; operationResult = executeDeleteRequestOnPrimary(deleteRequest, primary); response = operationResult.hasFailure() ? null : new DeleteResponse(request.shardId(), deleteRequest.type(), deleteRequest.id(), operationResult.getVersion(), ((Engine.DeleteResult) operationResult).isFound()); break; default: throw new IllegalStateException("unexpected opType [" + itemRequest.opType() + "] found"); } // update the bulk item request because update request execution can mutate the bulk item request BulkItemRequest item = request.items()[requestIndex]; if (operationResult == null // in case of a noop update operation || operationResult.hasFailure() == false) { if (operationResult != null) { location = locationToSync(location, operationResult.getLocation()); } else { assert response.getResult() == DocWriteResponse.Result.NOOP : "only noop update can have null operation"; } // set update response item.setPrimaryResponse(new BulkItemResponse(item.id(), opType, response)); } else { DocWriteRequest docWriteRequest = item.request(); Exception failure = operationResult.getFailure(); if (isConflictException(failure)) { logger.trace((Supplier<?>) () -> new ParameterizedMessage("{} failed to execute bulk item ({}) {}", request.shardId(), docWriteRequest.opType().getLowercase(), request), failure); } else { logger.debug((Supplier<?>) () -> new ParameterizedMessage("{} failed to execute bulk item ({}) {}", request.shardId(), docWriteRequest.opType().getLowercase(), request), failure); } // if its a conflict failure, and we already executed the request on a primary (and we execute it // again, due to primary relocation and only processing up to N bulk items when the shard gets closed) // then just use the response we got from the successful execution if (item.getPrimaryResponse() == null || isConflictException(failure) == false) { item.setPrimaryResponse(new BulkItemResponse(item.id(), docWriteRequest.opType(), new BulkItemResponse.Failure(request.index(), docWriteRequest.type(), docWriteRequest.id(), failure))); } } assert item.getPrimaryResponse() != null; assert preVersionTypes[requestIndex] != null; if (item.getPrimaryResponse().isFailed() || item.getPrimaryResponse().getResponse().getResult() == DocWriteResponse.Result.NOOP) { item.setIgnoreOnReplica(); } else { // set the ShardInfo to 0 so we can safely send it to the replicas. We won't use it in the real response though. item.getPrimaryResponse().getResponse().setShardInfo(new ShardInfo()); } } catch (Exception e) { // rethrow the failure if we are going to retry on primary and let parent failure to handle it if (retryPrimaryException(e)) { // restore updated versions... for (int j = 0; j < requestIndex; j++) { DocWriteRequest docWriteRequest = request.items()[j].request(); docWriteRequest.version(preVersions[j]); docWriteRequest.versionType(preVersionTypes[j]); } throw e; } // TODO: maybe this assert is too strict, we can still get environment failures while executing write operations assert false : "unexpected exception: " + e.getMessage() + " class:" + e.getClass().getSimpleName(); } return location; }	can we inline this in the proper place in the above if clauses?
private Translog.Location executeBulkItemRequest(IndexMetaData metaData, IndexShard primary, BulkShardRequest request, long[] preVersions, VersionType[] preVersionTypes, Translog.Location location, int requestIndex) throws Exception { final DocWriteRequest itemRequest = request.items()[requestIndex].request(); preVersions[requestIndex] = itemRequest.version(); preVersionTypes[requestIndex] = itemRequest.versionType(); DocWriteRequest.OpType opType = itemRequest.opType(); try { // execute item request final Engine.Result operationResult; final DocWriteResponse response; switch (itemRequest.opType()) { case CREATE: case INDEX: final IndexRequest indexRequest = (IndexRequest) itemRequest; operationResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdatedAction); response = operationResult.hasFailure() ? null : new IndexResponse(primary.shardId(), indexRequest.type(), indexRequest.id(), operationResult.getVersion(), ((Engine.IndexResult) operationResult).isCreated()); break; case UPDATE: UpdateResultHolder updateResultHolder = executeUpdateRequest(((UpdateRequest) itemRequest), primary, metaData, request, requestIndex); operationResult = updateResultHolder.operationResult; response = updateResultHolder.response; break; case DELETE: final DeleteRequest deleteRequest = (DeleteRequest) itemRequest; operationResult = executeDeleteRequestOnPrimary(deleteRequest, primary); response = operationResult.hasFailure() ? null : new DeleteResponse(request.shardId(), deleteRequest.type(), deleteRequest.id(), operationResult.getVersion(), ((Engine.DeleteResult) operationResult).isFound()); break; default: throw new IllegalStateException("unexpected opType [" + itemRequest.opType() + "] found"); } // update the bulk item request because update request execution can mutate the bulk item request BulkItemRequest item = request.items()[requestIndex]; if (operationResult == null // in case of a noop update operation || operationResult.hasFailure() == false) { if (operationResult != null) { location = locationToSync(location, operationResult.getLocation()); } else { assert response.getResult() == DocWriteResponse.Result.NOOP : "only noop update can have null operation"; } // set update response item.setPrimaryResponse(new BulkItemResponse(item.id(), opType, response)); } else { DocWriteRequest docWriteRequest = item.request(); Exception failure = operationResult.getFailure(); if (isConflictException(failure)) { logger.trace((Supplier<?>) () -> new ParameterizedMessage("{} failed to execute bulk item ({}) {}", request.shardId(), docWriteRequest.opType().getLowercase(), request), failure); } else { logger.debug((Supplier<?>) () -> new ParameterizedMessage("{} failed to execute bulk item ({}) {}", request.shardId(), docWriteRequest.opType().getLowercase(), request), failure); } // if its a conflict failure, and we already executed the request on a primary (and we execute it // again, due to primary relocation and only processing up to N bulk items when the shard gets closed) // then just use the response we got from the successful execution if (item.getPrimaryResponse() == null || isConflictException(failure) == false) { item.setPrimaryResponse(new BulkItemResponse(item.id(), docWriteRequest.opType(), new BulkItemResponse.Failure(request.index(), docWriteRequest.type(), docWriteRequest.id(), failure))); } } assert item.getPrimaryResponse() != null; assert preVersionTypes[requestIndex] != null; if (item.getPrimaryResponse().isFailed() || item.getPrimaryResponse().getResponse().getResult() == DocWriteResponse.Result.NOOP) { item.setIgnoreOnReplica(); } else { // set the ShardInfo to 0 so we can safely send it to the replicas. We won't use it in the real response though. item.getPrimaryResponse().getResponse().setShardInfo(new ShardInfo()); } } catch (Exception e) { // rethrow the failure if we are going to retry on primary and let parent failure to handle it if (retryPrimaryException(e)) { // restore updated versions... for (int j = 0; j < requestIndex; j++) { DocWriteRequest docWriteRequest = request.items()[j].request(); docWriteRequest.version(preVersions[j]); docWriteRequest.versionType(preVersionTypes[j]); } throw e; } // TODO: maybe this assert is too strict, we can still get environment failures while executing write operations assert false : "unexpected exception: " + e.getMessage() + " class:" + e.getClass().getSimpleName(); } return location; }	i think this can be operationfailedengineexception ? i have some issues with that one but will write that in the engine part.
* */ private UpdateResultHolder executeUpdateRequest(UpdateRequest updateRequest, IndexShard primary, IndexMetaData metaData, BulkShardRequest request, int requestIndex) throws Exception { Engine.Result updateOperationResult = null; UpdateResponse updateResponse = null; int maxAttempts = updateRequest.retryOnConflict(); for (int attemptCount = 0; attemptCount <= maxAttempts; attemptCount++) { final UpdateHelper.Result translate; // translate update request try { translate = updateHelper.prepare(updateRequest, primary, threadPool::estimatedTimeInMillis); } catch (Exception failure) { // we may fail translating a update to index or delete operation // we use index result to communicate failure while translating update request updateOperationResult = new Engine.IndexResult(failure, updateRequest.version(), 0); break; // out of retry loop } // execute translated update request switch (translate.getResponseResult()) { case CREATED: case UPDATED: IndexRequest indexRequest = translate.action(); MappingMetaData mappingMd = metaData.mappingOrDefault(indexRequest.type()); indexRequest.process(mappingMd, allowIdGeneration, request.index()); updateOperationResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdatedAction); break; case DELETED: updateOperationResult = executeDeleteRequestOnPrimary(translate.action(), primary); break; case NOOP: primary.noopUpdate(updateRequest.type()); break; default: throw new IllegalStateException("Illegal update operation " + translate.getResponseResult()); } if (updateOperationResult == null) { // this is a noop operation updateResponse = translate.action(); } else { if (updateOperationResult.hasFailure() == false) { // enrich update response and // set translated update (index/delete) request for replica execution in bulk items switch (updateOperationResult.getOperationType()) { case INDEX: IndexRequest updateIndexRequest = translate.action(); final IndexResponse indexResponse = new IndexResponse(primary.shardId(), updateIndexRequest.type(), updateIndexRequest.id(), updateOperationResult.getVersion(), ((Engine.IndexResult) updateOperationResult).isCreated()); BytesReference indexSourceAsBytes = updateIndexRequest.source(); updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getShardId(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.getResult()); if ((updateRequest.fetchSource() != null && updateRequest.fetchSource().fetchSource()) || (updateRequest.fields() != null && updateRequest.fields().length > 0)) { Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(indexSourceAsBytes, true); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes)); } // replace the update request to the translated index request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateIndexRequest); break; case DELETE: DeleteRequest updateDeleteRequest = translate.action(); DeleteResponse deleteResponse = new DeleteResponse(primary.shardId(), updateDeleteRequest.type(), updateDeleteRequest.id(), updateOperationResult.getVersion(), ((Engine.DeleteResult) updateOperationResult).isFound()); updateResponse = new UpdateResponse(deleteResponse.getShardInfo(), deleteResponse.getShardId(), deleteResponse.getType(), deleteResponse.getId(), deleteResponse.getVersion(), deleteResponse.getResult()); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), deleteResponse.getVersion(), translate.updatedSourceAsMap(), translate.updateSourceContentType(), null)); // replace the update request to the translated delete request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateDeleteRequest); break; } } else { // version conflict exception, retry if (updateOperationResult.getFailure() instanceof VersionConflictEngineException) { continue; } } } break; // out of retry loop } return new UpdateResultHolder(updateOperationResult, updateResponse); }	fold this into else if?
* */ private UpdateResultHolder executeUpdateRequest(UpdateRequest updateRequest, IndexShard primary, IndexMetaData metaData, BulkShardRequest request, int requestIndex) throws Exception { Engine.Result updateOperationResult = null; UpdateResponse updateResponse = null; int maxAttempts = updateRequest.retryOnConflict(); for (int attemptCount = 0; attemptCount <= maxAttempts; attemptCount++) { final UpdateHelper.Result translate; // translate update request try { translate = updateHelper.prepare(updateRequest, primary, threadPool::estimatedTimeInMillis); } catch (Exception failure) { // we may fail translating a update to index or delete operation // we use index result to communicate failure while translating update request updateOperationResult = new Engine.IndexResult(failure, updateRequest.version(), 0); break; // out of retry loop } // execute translated update request switch (translate.getResponseResult()) { case CREATED: case UPDATED: IndexRequest indexRequest = translate.action(); MappingMetaData mappingMd = metaData.mappingOrDefault(indexRequest.type()); indexRequest.process(mappingMd, allowIdGeneration, request.index()); updateOperationResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdatedAction); break; case DELETED: updateOperationResult = executeDeleteRequestOnPrimary(translate.action(), primary); break; case NOOP: primary.noopUpdate(updateRequest.type()); break; default: throw new IllegalStateException("Illegal update operation " + translate.getResponseResult()); } if (updateOperationResult == null) { // this is a noop operation updateResponse = translate.action(); } else { if (updateOperationResult.hasFailure() == false) { // enrich update response and // set translated update (index/delete) request for replica execution in bulk items switch (updateOperationResult.getOperationType()) { case INDEX: IndexRequest updateIndexRequest = translate.action(); final IndexResponse indexResponse = new IndexResponse(primary.shardId(), updateIndexRequest.type(), updateIndexRequest.id(), updateOperationResult.getVersion(), ((Engine.IndexResult) updateOperationResult).isCreated()); BytesReference indexSourceAsBytes = updateIndexRequest.source(); updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getShardId(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.getResult()); if ((updateRequest.fetchSource() != null && updateRequest.fetchSource().fetchSource()) || (updateRequest.fields() != null && updateRequest.fields().length > 0)) { Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(indexSourceAsBytes, true); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes)); } // replace the update request to the translated index request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateIndexRequest); break; case DELETE: DeleteRequest updateDeleteRequest = translate.action(); DeleteResponse deleteResponse = new DeleteResponse(primary.shardId(), updateDeleteRequest.type(), updateDeleteRequest.id(), updateOperationResult.getVersion(), ((Engine.DeleteResult) updateOperationResult).isFound()); updateResponse = new UpdateResponse(deleteResponse.getShardInfo(), deleteResponse.getShardId(), deleteResponse.getType(), deleteResponse.getId(), deleteResponse.getVersion(), deleteResponse.getResult()); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), deleteResponse.getVersion(), translate.updatedSourceAsMap(), translate.updateSourceContentType(), null)); // replace the update request to the translated delete request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateDeleteRequest); break; } } else { // version conflict exception, retry if (updateOperationResult.getFailure() instanceof VersionConflictEngineException) { continue; } } } break; // out of retry loop } return new UpdateResultHolder(updateOperationResult, updateResponse); }	we should really reuse the transportupdateaction logic here... but let's do that as a follow up :)
* */ private UpdateResultHolder executeUpdateRequest(UpdateRequest updateRequest, IndexShard primary, IndexMetaData metaData, BulkShardRequest request, int requestIndex) throws Exception { Engine.Result updateOperationResult = null; UpdateResponse updateResponse = null; int maxAttempts = updateRequest.retryOnConflict(); for (int attemptCount = 0; attemptCount <= maxAttempts; attemptCount++) { final UpdateHelper.Result translate; // translate update request try { translate = updateHelper.prepare(updateRequest, primary, threadPool::estimatedTimeInMillis); } catch (Exception failure) { // we may fail translating a update to index or delete operation // we use index result to communicate failure while translating update request updateOperationResult = new Engine.IndexResult(failure, updateRequest.version(), 0); break; // out of retry loop } // execute translated update request switch (translate.getResponseResult()) { case CREATED: case UPDATED: IndexRequest indexRequest = translate.action(); MappingMetaData mappingMd = metaData.mappingOrDefault(indexRequest.type()); indexRequest.process(mappingMd, allowIdGeneration, request.index()); updateOperationResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdatedAction); break; case DELETED: updateOperationResult = executeDeleteRequestOnPrimary(translate.action(), primary); break; case NOOP: primary.noopUpdate(updateRequest.type()); break; default: throw new IllegalStateException("Illegal update operation " + translate.getResponseResult()); } if (updateOperationResult == null) { // this is a noop operation updateResponse = translate.action(); } else { if (updateOperationResult.hasFailure() == false) { // enrich update response and // set translated update (index/delete) request for replica execution in bulk items switch (updateOperationResult.getOperationType()) { case INDEX: IndexRequest updateIndexRequest = translate.action(); final IndexResponse indexResponse = new IndexResponse(primary.shardId(), updateIndexRequest.type(), updateIndexRequest.id(), updateOperationResult.getVersion(), ((Engine.IndexResult) updateOperationResult).isCreated()); BytesReference indexSourceAsBytes = updateIndexRequest.source(); updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getShardId(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.getResult()); if ((updateRequest.fetchSource() != null && updateRequest.fetchSource().fetchSource()) || (updateRequest.fields() != null && updateRequest.fields().length > 0)) { Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(indexSourceAsBytes, true); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes)); } // replace the update request to the translated index request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateIndexRequest); break; case DELETE: DeleteRequest updateDeleteRequest = translate.action(); DeleteResponse deleteResponse = new DeleteResponse(primary.shardId(), updateDeleteRequest.type(), updateDeleteRequest.id(), updateOperationResult.getVersion(), ((Engine.DeleteResult) updateOperationResult).isFound()); updateResponse = new UpdateResponse(deleteResponse.getShardInfo(), deleteResponse.getShardId(), deleteResponse.getType(), deleteResponse.getId(), deleteResponse.getVersion(), deleteResponse.getResult()); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), deleteResponse.getVersion(), translate.updatedSourceAsMap(), translate.updateSourceContentType(), null)); // replace the update request to the translated delete request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateDeleteRequest); break; } } else { // version conflict exception, retry if (updateOperationResult.getFailure() instanceof VersionConflictEngineException) { continue; } } } break; // out of retry loop } return new UpdateResultHolder(updateOperationResult, updateResponse); }	can we make this a variable name replicarequest and return it as part of updateresultsholder (like updateresponse ) and process this in the calling function? there is enough craft here.
* */ private UpdateResultHolder executeUpdateRequest(UpdateRequest updateRequest, IndexShard primary, IndexMetaData metaData, BulkShardRequest request, int requestIndex) throws Exception { Engine.Result updateOperationResult = null; UpdateResponse updateResponse = null; int maxAttempts = updateRequest.retryOnConflict(); for (int attemptCount = 0; attemptCount <= maxAttempts; attemptCount++) { final UpdateHelper.Result translate; // translate update request try { translate = updateHelper.prepare(updateRequest, primary, threadPool::estimatedTimeInMillis); } catch (Exception failure) { // we may fail translating a update to index or delete operation // we use index result to communicate failure while translating update request updateOperationResult = new Engine.IndexResult(failure, updateRequest.version(), 0); break; // out of retry loop } // execute translated update request switch (translate.getResponseResult()) { case CREATED: case UPDATED: IndexRequest indexRequest = translate.action(); MappingMetaData mappingMd = metaData.mappingOrDefault(indexRequest.type()); indexRequest.process(mappingMd, allowIdGeneration, request.index()); updateOperationResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdatedAction); break; case DELETED: updateOperationResult = executeDeleteRequestOnPrimary(translate.action(), primary); break; case NOOP: primary.noopUpdate(updateRequest.type()); break; default: throw new IllegalStateException("Illegal update operation " + translate.getResponseResult()); } if (updateOperationResult == null) { // this is a noop operation updateResponse = translate.action(); } else { if (updateOperationResult.hasFailure() == false) { // enrich update response and // set translated update (index/delete) request for replica execution in bulk items switch (updateOperationResult.getOperationType()) { case INDEX: IndexRequest updateIndexRequest = translate.action(); final IndexResponse indexResponse = new IndexResponse(primary.shardId(), updateIndexRequest.type(), updateIndexRequest.id(), updateOperationResult.getVersion(), ((Engine.IndexResult) updateOperationResult).isCreated()); BytesReference indexSourceAsBytes = updateIndexRequest.source(); updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getShardId(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.getResult()); if ((updateRequest.fetchSource() != null && updateRequest.fetchSource().fetchSource()) || (updateRequest.fields() != null && updateRequest.fields().length > 0)) { Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(indexSourceAsBytes, true); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes)); } // replace the update request to the translated index request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateIndexRequest); break; case DELETE: DeleteRequest updateDeleteRequest = translate.action(); DeleteResponse deleteResponse = new DeleteResponse(primary.shardId(), updateDeleteRequest.type(), updateDeleteRequest.id(), updateOperationResult.getVersion(), ((Engine.DeleteResult) updateOperationResult).isFound()); updateResponse = new UpdateResponse(deleteResponse.getShardInfo(), deleteResponse.getShardId(), deleteResponse.getType(), deleteResponse.getId(), deleteResponse.getVersion(), deleteResponse.getResult()); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), deleteResponse.getVersion(), translate.updatedSourceAsMap(), translate.updateSourceContentType(), null)); // replace the update request to the translated delete request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateDeleteRequest); break; } } else { // version conflict exception, retry if (updateOperationResult.getFailure() instanceof VersionConflictEngineException) { continue; } } } break; // out of retry loop } return new UpdateResultHolder(updateOperationResult, updateResponse); }	can we flip this around and use the natural loop flow? i.e., check if it's not an versionconflictengineexception and break?
* */ private UpdateResultHolder executeUpdateRequest(UpdateRequest updateRequest, IndexShard primary, IndexMetaData metaData, BulkShardRequest request, int requestIndex) throws Exception { Engine.Result updateOperationResult = null; UpdateResponse updateResponse = null; int maxAttempts = updateRequest.retryOnConflict(); for (int attemptCount = 0; attemptCount <= maxAttempts; attemptCount++) { final UpdateHelper.Result translate; // translate update request try { translate = updateHelper.prepare(updateRequest, primary, threadPool::estimatedTimeInMillis); } catch (Exception failure) { // we may fail translating a update to index or delete operation // we use index result to communicate failure while translating update request updateOperationResult = new Engine.IndexResult(failure, updateRequest.version(), 0); break; // out of retry loop } // execute translated update request switch (translate.getResponseResult()) { case CREATED: case UPDATED: IndexRequest indexRequest = translate.action(); MappingMetaData mappingMd = metaData.mappingOrDefault(indexRequest.type()); indexRequest.process(mappingMd, allowIdGeneration, request.index()); updateOperationResult = executeIndexRequestOnPrimary(indexRequest, primary, mappingUpdatedAction); break; case DELETED: updateOperationResult = executeDeleteRequestOnPrimary(translate.action(), primary); break; case NOOP: primary.noopUpdate(updateRequest.type()); break; default: throw new IllegalStateException("Illegal update operation " + translate.getResponseResult()); } if (updateOperationResult == null) { // this is a noop operation updateResponse = translate.action(); } else { if (updateOperationResult.hasFailure() == false) { // enrich update response and // set translated update (index/delete) request for replica execution in bulk items switch (updateOperationResult.getOperationType()) { case INDEX: IndexRequest updateIndexRequest = translate.action(); final IndexResponse indexResponse = new IndexResponse(primary.shardId(), updateIndexRequest.type(), updateIndexRequest.id(), updateOperationResult.getVersion(), ((Engine.IndexResult) updateOperationResult).isCreated()); BytesReference indexSourceAsBytes = updateIndexRequest.source(); updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getShardId(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.getResult()); if ((updateRequest.fetchSource() != null && updateRequest.fetchSource().fetchSource()) || (updateRequest.fields() != null && updateRequest.fields().length > 0)) { Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(indexSourceAsBytes, true); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes)); } // replace the update request to the translated index request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateIndexRequest); break; case DELETE: DeleteRequest updateDeleteRequest = translate.action(); DeleteResponse deleteResponse = new DeleteResponse(primary.shardId(), updateDeleteRequest.type(), updateDeleteRequest.id(), updateOperationResult.getVersion(), ((Engine.DeleteResult) updateOperationResult).isFound()); updateResponse = new UpdateResponse(deleteResponse.getShardInfo(), deleteResponse.getShardId(), deleteResponse.getType(), deleteResponse.getId(), deleteResponse.getVersion(), deleteResponse.getResult()); updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), deleteResponse.getVersion(), translate.updatedSourceAsMap(), translate.updateSourceContentType(), null)); // replace the update request to the translated delete request to execute on the replica. request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), updateDeleteRequest); break; } } else { // version conflict exception, retry if (updateOperationResult.getFailure() instanceof VersionConflictEngineException) { continue; } } } break; // out of retry loop } return new UpdateResultHolder(updateOperationResult, updateResponse); }	can we add an assertion here? i don't see what doc level failure should end up here.
protected boolean retryPrimaryException(final Throwable e) { boolean retry = e.getClass() == ReplicationOperation.RetryOnPrimaryException.class || TransportActions.isShardNotAvailableException(e); if (retry) { assert e instanceof ElasticsearchException : "expected all retry on primary exception to be ElasticsearchException instances, found: " + e.getClass(); } return retry; }	out of curiosity - what made you do this?
@Override public IndexResult index(Index index) { IndexResult result; try (ReleasableLock lock = readLock.acquire()) { ensureOpen(); if (index.origin().isRecovery()) { // Don't throttle recovery operations result = innerIndex(index); } else { try (Releasable r = throttle.acquireThrottle()) { result = innerIndex(index); } } } catch (Exception e) { Exception documentFailure = extractDocumentFailure(index, e); result = new IndexResult(documentFailure, index.version(), index.estimatedSizeInBytes()); } return result; }	nit: call this checkifdocumentfailureorthrow?
@Override public IndexResult index(Index index) { IndexResult result; try (ReleasableLock lock = readLock.acquire()) { ensureOpen(); if (index.origin().isRecovery()) { // Don't throttle recovery operations result = innerIndex(index); } else { try (Releasable r = throttle.acquireThrottle()) { result = innerIndex(index); } } } catch (Exception e) { Exception documentFailure = extractDocumentFailure(index, e); result = new IndexResult(documentFailure, index.version(), index.estimatedSizeInBytes()); } return result; }	i do wonder why we need this extra exception (i know we had it before). if we get so far the failure has nothing to do with the incoming operation but more with the engine and the shard, so i'm not sure have the operation type (clear by the stack trace) and the specific doc id helps. how about just throwing an elasticsearchexception with the shard header set?
private DeleteResult innerDelete(Delete delete) throws IOException { final Translog.Location location; final long updatedVersion; final boolean found; try (Releasable ignored = acquireLock(delete.uid())) { lastWriteNanos = delete.startTime(); final long currentVersion; final boolean deleted; final VersionValue versionValue = versionMap.getUnderLock(delete.uid()); assert incrementVersionLookup(); if (versionValue == null) { currentVersion = loadCurrentVersionFromIndex(delete.uid()); deleted = currentVersion == Versions.NOT_FOUND; } else { currentVersion = checkDeletedAndGCed(versionValue); deleted = versionValue.delete(); } final long expectedVersion = delete.version(); if (checkVersionConflict(delete, currentVersion, expectedVersion, deleted)) { // skip executing delete because of version conflict on recovery return new DeleteResult(expectedVersion, true, delete.estimatedSizeInBytes()); } else { updatedVersion = delete.versionType().updateVersion(currentVersion, expectedVersion); found = deleteIfFound(delete.uid(), currentVersion, deleted, versionValue); DeleteResult deleteResult = new DeleteResult(updatedVersion, found, delete.estimatedSizeInBytes()); if (delete.origin() != Operation.Origin.LOCAL_TRANSLOG_RECOVERY) { location = translog.add(new Translog.Delete(delete, deleteResult)); } else { location = null; } versionMap.putUnderLock(delete.uid().bytes(), new DeleteVersionValue(updatedVersion, engineConfig.getThreadPool().estimatedTimeInMillis())); deleteResult.setLocation(location); deleteResult.setTook(System.nanoTime() - delete.startTime()); deleteResult.freeze(); return deleteResult; } } }	can we have one return statement in the end of the method and a final result variable, declared in the beginning and must be set?
private DeleteResult innerDelete(Delete delete) throws IOException { final Translog.Location location; final long updatedVersion; final boolean found; try (Releasable ignored = acquireLock(delete.uid())) { lastWriteNanos = delete.startTime(); final long currentVersion; final boolean deleted; final VersionValue versionValue = versionMap.getUnderLock(delete.uid()); assert incrementVersionLookup(); if (versionValue == null) { currentVersion = loadCurrentVersionFromIndex(delete.uid()); deleted = currentVersion == Versions.NOT_FOUND; } else { currentVersion = checkDeletedAndGCed(versionValue); deleted = versionValue.delete(); } final long expectedVersion = delete.version(); if (checkVersionConflict(delete, currentVersion, expectedVersion, deleted)) { // skip executing delete because of version conflict on recovery return new DeleteResult(expectedVersion, true, delete.estimatedSizeInBytes()); } else { updatedVersion = delete.versionType().updateVersion(currentVersion, expectedVersion); found = deleteIfFound(delete.uid(), currentVersion, deleted, versionValue); DeleteResult deleteResult = new DeleteResult(updatedVersion, found, delete.estimatedSizeInBytes()); if (delete.origin() != Operation.Origin.LOCAL_TRANSLOG_RECOVERY) { location = translog.add(new Translog.Delete(delete, deleteResult)); } else { location = null; } versionMap.putUnderLock(delete.uid().bytes(), new DeleteVersionValue(updatedVersion, engineConfig.getThreadPool().estimatedTimeInMillis())); deleteResult.setLocation(location); deleteResult.setTook(System.nanoTime() - delete.startTime()); deleteResult.freeze(); return deleteResult; } } }	weird line wrap + the same request as before - one return statement (with a freeze)
private void verifyPrimary() { if (shardRouting.primary() == false) { // TODO throw a more appropriate exception throw new ShardNotFoundException(shardRouting.shardId(), "shard is not a primary anymore"); } }	why did you changed it? i think it's good to keep it consistent with verifyreplicationtarget and not use an exception that is covered by isshardnotavailableexception. these days, with aid checks and all, end up here by mistake is something we shouldn't swallow.
private Engine.IndexResult index(final Engine engine, final Engine.Index index) { active.set(true); final Engine.IndexResult result; indexingOperationListeners.preIndex(index); try { if (logger.isTraceEnabled()) { logger.trace("index [{}][{}]{}", index.type(), index.id(), index.docs()); } result = engine.index(index); } catch (Exception e) { indexingOperationListeners.postIndex(index, e); throw e; } if (result.hasFailure()) { indexingOperationListeners.postIndex(index, result.getFailure()); } else { indexingOperationListeners.postIndex(index, result); } return result; }	i'm wondering if should make the distinction here and not just call postindex(index,result) where the result is a failure
protected ReplicaResult shardOperationOnReplica(Request request, IndexShard replica) { assertIndexShardCounter(1); assertPhase(task, "replica"); if (throwException) { throw new ElasticsearchException("simulated"); } return new ReplicaResult(null); }	i expected to see some test that shows we respond with a failure if the (primary|replica)result has a failure but i can't find it. is there one?
*/ public BytesReference copyBytes() { final byte[] keyBytes = new byte[count]; int offset = 0; final BytesRefIterator iterator = bytes().iterator(); BytesRef slice; try { while ((slice = iterator.next()) != null) { System.arraycopy(slice.bytes, slice.offset, keyBytes, offset, slice.length); offset += slice.length; } } catch (IOException e) { throw new AssertionError(e); } return new BytesArray(keyBytes);// do a deep copy } /** * Returns the number of bytes used by the underlying {@link org.elasticsearch.common.util.ByteArray}	is this comment supposed to be here?
public static void close(Iterable<? extends Releasable> releasables) { close(releasables, false); } /** Release the provided {@link Releasable}	same optimization already added to ioutils for a single releasable since we now use that heavily in closing the releasable stream.
public void testNodeReplacementOnlyAllowsShardsFromReplacedNode() throws Exception { String nodeA = internalCluster().startNode(Settings.builder().put("node.name", "node-a")); Settings.Builder nodeASettings = Settings.builder().put("index.number_of_shards", 3).put("index.number_of_replicas", 1); createIndex("myindex", nodeASettings.build()); final String nodeAId = getNodeId(nodeA); final String nodeB = "node_t1"; // TODO: fix this to so it's actually overrideable // Mark the nodeA as being replaced PutShutdownNodeAction.Request putShutdownRequest = new PutShutdownNodeAction.Request( nodeAId, SingleNodeShutdownMetadata.Type.REPLACE, this.getTestName(), null, nodeB ); AcknowledgedResponse putShutdownResponse = client().execute(PutShutdownNodeAction.INSTANCE, putShutdownRequest).get(); assertTrue(putShutdownResponse.isAcknowledged()); GetShutdownStatusAction.Response getResp = client().execute( GetShutdownStatusAction.INSTANCE, new GetShutdownStatusAction.Request(nodeAId) ).get(); assertThat(getResp.getShutdownStatuses().get(0).migrationStatus().getStatus(), equalTo(STALLED)); internalCluster().startNode(Settings.builder().put("node.name", nodeB)); final String nodeBId = getNodeId(nodeB); logger.info("--> NodeA: {} -- {}", nodeA, nodeAId); logger.info("--> NodeB: {} -- {}", nodeB, nodeBId); assertBusy(() -> { ClusterState state = client().admin().cluster().prepareState().clear().setRoutingTable(true).get().getState(); int active = 0; for (ShardRouting sr : state.routingTable().allShards("myindex")) { if (sr.active()) { active++; assertThat( "expected shard on nodeB (" + nodeBId + ") but it was on a different node", sr.currentNodeId(), equalTo(nodeBId) ); } } assertThat("expected all 3 of the primary shards to be allocated", active, equalTo(3)); }); assertBusy(() -> { GetShutdownStatusAction.Response shutdownStatus = client().execute( GetShutdownStatusAction.INSTANCE, new GetShutdownStatusAction.Request(nodeAId) ).get(); assertThat(shutdownStatus.getShutdownStatuses().get(0).migrationStatus().getStatus(), equalTo(COMPLETE)); }); final String nodeC = internalCluster().startNode(); createIndex("other", Settings.builder().put("index.number_of_shards", 1).put("index.number_of_replicas", 1).build()); ensureYellow("other"); // Explain the replica for the "other" index ClusterAllocationExplainResponse explainResponse = client().admin() .cluster() .prepareAllocationExplain() .setIndex("other") .setShard(0) .setPrimary(false) .get(); // Validate that the replica cannot be allocated to nodeB because it's the target of a node replacement explainResponse.getExplanation() .getShardAllocationDecision() .getAllocateDecision() .getNodeDecisions() .stream() .filter(nodeDecision -> nodeDecision.getNode().getId().equals(nodeBId)) .findFirst() .ifPresentOrElse(nodeAllocationResult -> { assertThat(nodeAllocationResult.getCanAllocateDecision().type(), equalTo(Decision.Type.NO)); assertTrue( "expected decisions to mention node replacement: " + nodeAllocationResult.getCanAllocateDecision() .getDecisions() .stream() .map(Decision::getExplanation) .collect(Collectors.joining(",")), nodeAllocationResult.getCanAllocateDecision() .getDecisions() .stream() .anyMatch( decision -> decision.getExplanation().contains("is replacing the vacating node") && decision.getExplanation().contains("may be allocated to it until the replacement is complete") ) ); }, () -> fail("expected a 'NO' decision for nodeB but there was no explanation for that node")); }	can we update this comment, looks like a copy-paste error, since we do not pin the index here?
public void testNodeReplacementOnlyToTarget() throws Exception { String nodeA = internalCluster().startNode( Settings.builder().put("node.name", "node-a").put("cluster.routing.rebalance.enable", "none") ); // Create an index and pin it to nodeA, when we replace it with nodeB, // it'll move the data, overridding the `_name` allocation filter Settings.Builder nodeASettings = Settings.builder().put("index.number_of_shards", 4).put("index.number_of_replicas", 0); createIndex("myindex", nodeASettings.build()); final String nodeAId = getNodeId(nodeA); final String nodeB = "node_t1"; // TODO: fix this to so it's actually overrideable final String nodeC = "node_t2"; // TODO: fix this to so it's actually overrideable // Mark the nodeA as being replaced PutShutdownNodeAction.Request putShutdownRequest = new PutShutdownNodeAction.Request( nodeAId, SingleNodeShutdownMetadata.Type.REPLACE, this.getTestName(), null, nodeB ); AcknowledgedResponse putShutdownResponse = client().execute(PutShutdownNodeAction.INSTANCE, putShutdownRequest).get(); assertTrue(putShutdownResponse.isAcknowledged()); GetShutdownStatusAction.Response getResp = client().execute( GetShutdownStatusAction.INSTANCE, new GetShutdownStatusAction.Request(nodeAId) ).get(); assertThat(getResp.getShutdownStatuses().get(0).migrationStatus().getStatus(), equalTo(STALLED)); internalCluster().startNode(Settings.builder().put("node.name", nodeB)); internalCluster().startNode(Settings.builder().put("node.name", nodeC)); final String nodeBId = getNodeId(nodeB); final String nodeCId = getNodeId(nodeC); logger.info("--> NodeA: {} -- {}", nodeA, nodeAId); logger.info("--> NodeB: {} -- {}", nodeB, nodeBId); logger.info("--> NodeC: {} -- {}", nodeC, nodeCId); assertBusy(() -> { ClusterState state = client().admin().cluster().prepareState().clear().setRoutingTable(true).get().getState(); for (ShardRouting sr : state.routingTable().allShards("myindex")) { assertThat( "expected all shards for index to be on node B (" + nodeBId + ") but " + sr.toString() + " is on " + sr.currentNodeId(), sr.currentNodeId(), equalTo(nodeBId) ); } }); assertBusy(() -> { GetShutdownStatusAction.Response shutdownStatus = client().execute( GetShutdownStatusAction.INSTANCE, new GetShutdownStatusAction.Request(nodeAId) ).get(); assertThat(shutdownStatus.getShutdownStatuses().get(0).migrationStatus().getStatus(), equalTo(COMPLETE)); }); }	i wonder if we can swap the order of starting nodes? suppose we allowed allocating to both nodes (a bug). but we then risk the shards moving over to b (they only have to start the relocation) before node c have started?
public void getLatestSuccessfulSnapshotForShard( String repositoryName, ShardId shardId, ActionListener<Optional<ShardSnapshotInfo>> originalListener ) { final ActionListener<Optional<ShardSnapshotInfo>> listener = originalListener.delegateResponse( (delegate, err) -> { delegate.onFailure( new RepositoryException(repositoryName, "Unable to find the latest snapshot for shard [" + shardId + "]", err) ); } ); final Repository repository = getRepository(repositoryName); if (repository == null) { listener.onFailure(new RepositoryMissingException(repositoryName)); return; } final String indexName = shardId.getIndexName(); StepListener<RepositoryData> repositoryDataStepListener = new StepListener<>(); StepListener<FetchShardSnapshotContext> snapshotInfoStepListener = new StepListener<>(); repositoryDataStepListener.whenComplete(repositoryData -> { if (repositoryData.hasIndex(indexName) == false) { listener.onResponse(Optional.empty()); return; } final IndexId indexId = repositoryData.resolveIndexId(indexName); final List<SnapshotId> indexSnapshots = repositoryData.getSnapshots(indexId); final Optional<SnapshotId> latestSnapshotId = indexSnapshots.stream() .map(snapshotId -> Tuple.tuple(snapshotId, repositoryData.getSnapshotDetails(snapshotId))) .filter(s -> s.v2().getSnapshotState() != null && s.v2().getSnapshotState() == SnapshotState.SUCCESS) .filter(s -> s.v2().getStartTimeMillis() != -1 && s.v2().getEndTimeMillis() != -1) .max(START_TIME_COMPARATOR) .map(Tuple::v1); if (latestSnapshotId.isEmpty()) { // It's possible that some of the backups were taken before 7.14 and they were successful backups, but they don't // have the start/end date populated in RepositoryData. We could fetch all the backups and find out if there is // a valid candidate, but for simplicity we just consider that we couldn't find any valid snapshot. Existing // snapshots start/end timestamps should appear in the RepositoryData eventually. listener.onResponse(Optional.empty()); return; } final SnapshotId snapshotId = latestSnapshotId.get(); repository.getSnapshotInfo( snapshotId, snapshotInfoStepListener.map( snapshotInfo -> new FetchShardSnapshotContext(repository, repositoryData, indexId, shardId, snapshotInfo) ) ); }, listener::onFailure); snapshotInfoStepListener.whenComplete(fetchSnapshotContext -> { final SnapshotInfo snapshotInfo = fetchSnapshotContext.snapshotInfo; if (snapshotInfo == null || snapshotInfo.state() != SnapshotState.SUCCESS) { // We couldn't find a valid candidate listener.onResponse(Optional.empty()); return; } // TODO: This is executed in SNAPSHOT_META thread pool and we perform a few blocking operations here, should we execute this // step somewhere else? // We fetch BlobStoreIndexShardSnapshots instead of BlobStoreIndexShardSnapshot in order to get the shardStateId that // allows us to tell whether or not this shard had in-flight operations while the snapshot was taken. final BlobStoreIndexShardSnapshots blobStoreIndexShardSnapshots = fetchSnapshotContext.getBlobStoreIndexShardSnapshots(); final String indexMetadataId = fetchSnapshotContext.getIndexMetadataId(); final Optional<ShardSnapshotInfo> indexShardSnapshotInfo = blobStoreIndexShardSnapshots.snapshots() .stream() .filter(snapshotFiles -> snapshotFiles.snapshot().equals(snapshotInfo.snapshotId().getName())) .findFirst() .map(snapshotFiles -> fetchSnapshotContext.createIndexShardSnapshotInfo(indexMetadataId, snapshotFiles)); listener.onResponse(indexShardSnapshotInfo); }, listener::onFailure); repository.getRepositoryData(repositoryDataStepListener); }	reading a private field here, would prefer it to be package-private or to explicitly use a method.
public void getLatestSuccessfulSnapshotForShard( String repositoryName, ShardId shardId, ActionListener<Optional<ShardSnapshotInfo>> originalListener ) { final ActionListener<Optional<ShardSnapshotInfo>> listener = originalListener.delegateResponse( (delegate, err) -> { delegate.onFailure( new RepositoryException(repositoryName, "Unable to find the latest snapshot for shard [" + shardId + "]", err) ); } ); final Repository repository = getRepository(repositoryName); if (repository == null) { listener.onFailure(new RepositoryMissingException(repositoryName)); return; } final String indexName = shardId.getIndexName(); StepListener<RepositoryData> repositoryDataStepListener = new StepListener<>(); StepListener<FetchShardSnapshotContext> snapshotInfoStepListener = new StepListener<>(); repositoryDataStepListener.whenComplete(repositoryData -> { if (repositoryData.hasIndex(indexName) == false) { listener.onResponse(Optional.empty()); return; } final IndexId indexId = repositoryData.resolveIndexId(indexName); final List<SnapshotId> indexSnapshots = repositoryData.getSnapshots(indexId); final Optional<SnapshotId> latestSnapshotId = indexSnapshots.stream() .map(snapshotId -> Tuple.tuple(snapshotId, repositoryData.getSnapshotDetails(snapshotId))) .filter(s -> s.v2().getSnapshotState() != null && s.v2().getSnapshotState() == SnapshotState.SUCCESS) .filter(s -> s.v2().getStartTimeMillis() != -1 && s.v2().getEndTimeMillis() != -1) .max(START_TIME_COMPARATOR) .map(Tuple::v1); if (latestSnapshotId.isEmpty()) { // It's possible that some of the backups were taken before 7.14 and they were successful backups, but they don't // have the start/end date populated in RepositoryData. We could fetch all the backups and find out if there is // a valid candidate, but for simplicity we just consider that we couldn't find any valid snapshot. Existing // snapshots start/end timestamps should appear in the RepositoryData eventually. listener.onResponse(Optional.empty()); return; } final SnapshotId snapshotId = latestSnapshotId.get(); repository.getSnapshotInfo( snapshotId, snapshotInfoStepListener.map( snapshotInfo -> new FetchShardSnapshotContext(repository, repositoryData, indexId, shardId, snapshotInfo) ) ); }, listener::onFailure); snapshotInfoStepListener.whenComplete(fetchSnapshotContext -> { final SnapshotInfo snapshotInfo = fetchSnapshotContext.snapshotInfo; if (snapshotInfo == null || snapshotInfo.state() != SnapshotState.SUCCESS) { // We couldn't find a valid candidate listener.onResponse(Optional.empty()); return; } // TODO: This is executed in SNAPSHOT_META thread pool and we perform a few blocking operations here, should we execute this // step somewhere else? // We fetch BlobStoreIndexShardSnapshots instead of BlobStoreIndexShardSnapshot in order to get the shardStateId that // allows us to tell whether or not this shard had in-flight operations while the snapshot was taken. final BlobStoreIndexShardSnapshots blobStoreIndexShardSnapshots = fetchSnapshotContext.getBlobStoreIndexShardSnapshots(); final String indexMetadataId = fetchSnapshotContext.getIndexMetadataId(); final Optional<ShardSnapshotInfo> indexShardSnapshotInfo = blobStoreIndexShardSnapshots.snapshots() .stream() .filter(snapshotFiles -> snapshotFiles.snapshot().equals(snapshotInfo.snapshotId().getName())) .findFirst() .map(snapshotFiles -> fetchSnapshotContext.createIndexShardSnapshotInfo(indexMetadataId, snapshotFiles)); listener.onResponse(indexShardSnapshotInfo); }, listener::onFailure); repository.getRepositoryData(repositoryDataStepListener); }	i think this is ok, the snapshot_meta threadpool is an appropriate place to block on a read of some snapshot metadata. wouldn't hurt to document with an assertion that we're on this threadpool tho.
public void getLatestSuccessfulSnapshotForShard( String repositoryName, ShardId shardId, ActionListener<Optional<ShardSnapshotInfo>> originalListener ) { final ActionListener<Optional<ShardSnapshotInfo>> listener = originalListener.delegateResponse( (delegate, err) -> { delegate.onFailure( new RepositoryException(repositoryName, "Unable to find the latest snapshot for shard [" + shardId + "]", err) ); } ); final Repository repository = getRepository(repositoryName); if (repository == null) { listener.onFailure(new RepositoryMissingException(repositoryName)); return; } final String indexName = shardId.getIndexName(); StepListener<RepositoryData> repositoryDataStepListener = new StepListener<>(); StepListener<FetchShardSnapshotContext> snapshotInfoStepListener = new StepListener<>(); repositoryDataStepListener.whenComplete(repositoryData -> { if (repositoryData.hasIndex(indexName) == false) { listener.onResponse(Optional.empty()); return; } final IndexId indexId = repositoryData.resolveIndexId(indexName); final List<SnapshotId> indexSnapshots = repositoryData.getSnapshots(indexId); final Optional<SnapshotId> latestSnapshotId = indexSnapshots.stream() .map(snapshotId -> Tuple.tuple(snapshotId, repositoryData.getSnapshotDetails(snapshotId))) .filter(s -> s.v2().getSnapshotState() != null && s.v2().getSnapshotState() == SnapshotState.SUCCESS) .filter(s -> s.v2().getStartTimeMillis() != -1 && s.v2().getEndTimeMillis() != -1) .max(START_TIME_COMPARATOR) .map(Tuple::v1); if (latestSnapshotId.isEmpty()) { // It's possible that some of the backups were taken before 7.14 and they were successful backups, but they don't // have the start/end date populated in RepositoryData. We could fetch all the backups and find out if there is // a valid candidate, but for simplicity we just consider that we couldn't find any valid snapshot. Existing // snapshots start/end timestamps should appear in the RepositoryData eventually. listener.onResponse(Optional.empty()); return; } final SnapshotId snapshotId = latestSnapshotId.get(); repository.getSnapshotInfo( snapshotId, snapshotInfoStepListener.map( snapshotInfo -> new FetchShardSnapshotContext(repository, repositoryData, indexId, shardId, snapshotInfo) ) ); }, listener::onFailure); snapshotInfoStepListener.whenComplete(fetchSnapshotContext -> { final SnapshotInfo snapshotInfo = fetchSnapshotContext.snapshotInfo; if (snapshotInfo == null || snapshotInfo.state() != SnapshotState.SUCCESS) { // We couldn't find a valid candidate listener.onResponse(Optional.empty()); return; } // TODO: This is executed in SNAPSHOT_META thread pool and we perform a few blocking operations here, should we execute this // step somewhere else? // We fetch BlobStoreIndexShardSnapshots instead of BlobStoreIndexShardSnapshot in order to get the shardStateId that // allows us to tell whether or not this shard had in-flight operations while the snapshot was taken. final BlobStoreIndexShardSnapshots blobStoreIndexShardSnapshots = fetchSnapshotContext.getBlobStoreIndexShardSnapshots(); final String indexMetadataId = fetchSnapshotContext.getIndexMetadataId(); final Optional<ShardSnapshotInfo> indexShardSnapshotInfo = blobStoreIndexShardSnapshots.snapshots() .stream() .filter(snapshotFiles -> snapshotFiles.snapshot().equals(snapshotInfo.snapshotId().getName())) .findFirst() .map(snapshotFiles -> fetchSnapshotContext.createIndexShardSnapshotInfo(indexMetadataId, snapshotFiles)); listener.onResponse(indexShardSnapshotInfo); }, listener::onFailure); repository.getRepositoryData(repositoryDataStepListener); }	do any of the tests cover this fallback?
@Override public void parse(ParseContext context) throws IOException, MapperParsingException { if (context.sourceToParse().ttl() < 0) { // no ttl has been provided externally long ttl; if (context.parser().currentToken() == XContentParser.Token.VALUE_STRING) { ttl = TimeValue.parseTimeValue(context.parser().text(), null, "ttl").millis(); } else { ttl = context.parser().longValue(coerce.value()); } if (ttl <= 0) { throw new MapperParsingException("TTL value must be > 0. Illegal value provided [" + ttl + "]"); } context.sourceToParse().ttl(ttl); } }	it feels inconsistent to require a unit when the ttl is specified as a string and not when it is specified as a long?
@Override protected XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException { builder = super.innerToXContent(builder, params); builder.field(Fields.FREQ.getPreferredName(), freq); return builder; }	nit: can we change the name to termsuggestionoptionparser to reflect the class name?
public void testRandomValueOtherThan() { // "normal" way of calling where the value is not null int bad = randomInt(); assertNotEquals(bad, (int) randomValueOtherThan(bad, ESTestCase::randomInt)); /* * "funny" way of calling where the value is null. This once * had a unique behavior but at this point `null` acts just * like any other value. */ Supplier<Object> usuallyNull = () -> usually() ? null : randomInt(); assertNotNull(null, randomValueOtherThan(null, usuallyNull)); }	why pass a null message here, i think assertnotnull(randomvalueotherthan(null, usuallynull)); is fine here, the expectation will fail saying we got null?
public void persistQuantiles(Quantiles quantiles, Supplier<Boolean> shouldRetry) { String quantilesDocId = Quantiles.documentId(quantiles.getJobId()); SearchRequest searchRequest = new SearchRequest(AnomalyDetectorsIndex.jobStateIndexPattern()) .source(new SearchSourceBuilder().size(1).query(new IdsQueryBuilder().addIds(quantilesDocId))); SearchResponse searchResponse = client.search(searchRequest).actionGet(); String indexOrAlias = AnomalyDetectorsIndex.jobStateIndexWriteAlias(); if (searchResponse.getHits().getHits().length > 0) { indexOrAlias = searchResponse.getHits().getHits()[0].getIndex(); } Persistable persistable = new Persistable(indexOrAlias, quantiles.getJobId(), quantiles, quantilesDocId); persistable.persist(shouldRetry); }	i am not 100% sure about this. this means that if we fail to search for the id, the job will be flagged as failed immediately and stop running. i wonder if we should do retries on the synchronous search as we do for the persistence?
*/ public Version esVersion() { return clientYamlTestClient.getEsVersion(); } /** * @return the version that skipping tests is based on, usually the same as {@link #esVersion()}	i would not call this skipversion, but rather getesversion like in the test client. for the multi_cluster tests, the es version will take into account the remote cluster.
protected ClientYamlTestExecutionContext createRestTestExecutionContext( ClientYamlTestCandidate clientYamlTestCandidate, ClientYamlTestClient clientYamlTestClient, boolean randomizeContentType ) { return new ClientYamlTestExecutionContext(clientYamlTestCandidate, clientYamlTestClient, randomizeContentType()); }	you are ignoring the randomizecontenttype argument i think?
private Repository createRepository(RepositoryMetaData repositoryMetaData, Map<String, Repository.Factory> factories) { logger.debug("creating repository [{}][{}]", repositoryMetaData.type(), repositoryMetaData.name()); Repository.Factory factory = factories.get(repositoryMetaData.type()); if (factory == null) { throw new RepositoryException(repositoryMetaData.name(), "repository type [" + repositoryMetaData.type() + "] does not exist"); } boolean success = false; Repository repository = null; try { repository = factory.create(repositoryMetaData, factories::get); repository.start(); success = true; return repository; } catch (Exception e) { logger.warn(new ParameterizedMessage("failed to create repository [{}][{}]", repositoryMetaData.type(), repositoryMetaData.name()), e); throw new RepositoryException(repositoryMetaData.name(), "failed to create repository", e); } finally { if (success == false) { IOUtils.closeWhileHandlingException(repository); } } }	why not just put the following into the catch block: ioutils.closewhilehandlingexception(repository);. no need for success flag and/or finally block
@Override public void readFrom(StreamInput in) throws IOException { clusterName = readClusterName(in); node = readNode(in); if (in.readBoolean()) { master = readNode(in); } this.hasJoinedOnce = in.readBoolean(); this.id = in.readLong(); }	i assume this version check remains in 1.x and 1.4, right?
@Override public String toString() { return "ping_response{node [" + node + "], id[" + id + "], master [" + master + "], hasJoinedOnce [" + hasJoinedOnce + "], cluster_name[" + clusterName.value() + "]}"; } } /** * a utility collection of pings where only the most recent ping is stored per node */ public static class PingCollection { Map<DiscoveryNode, PingResponse> pings; public PingCollection() { pings = new HashMap<>(); } /** * adds a ping if newer than previous pings from the same node * * @return true if added, false o.w. */ public synchronized boolean addPing(PingResponse ping) { PingResponse existingResponse = pings.get(ping.node()); // in case both existing and new ping have the same id (probably because they come // from nodes from version <1.4.0) we prefer to use the last added one. if (existingResponse == null || existingResponse.id() <= ping.id()) { pings.put(ping.node(), ping); return true; } return false; } /** adds multiple pings if newer than previous pings from the same node */ public synchronized void addPings(PingResponse[] pings) { for (PingResponse ping : pings) { addPing(ping); } } /** serialize current pings to an array */ public synchronized PingResponse[] toArray() { return pings.values().toArray(new PingResponse[pings.size()]); }	+1 this really cleans up code in several places
@Override public String toString() { return "ping_response{node [" + node + "], id[" + id + "], master [" + master + "], hasJoinedOnce [" + hasJoinedOnce + "], cluster_name[" + clusterName.value() + "]}"; } } /** * a utility collection of pings where only the most recent ping is stored per node */ public static class PingCollection { Map<DiscoveryNode, PingResponse> pings; public PingCollection() { pings = new HashMap<>(); } /** * adds a ping if newer than previous pings from the same node * * @return true if added, false o.w. */ public synchronized boolean addPing(PingResponse ping) { PingResponse existingResponse = pings.get(ping.node()); // in case both existing and new ping have the same id (probably because they come // from nodes from version <1.4.0) we prefer to use the last added one. if (existingResponse == null || existingResponse.id() <= ping.id()) { pings.put(ping.node(), ping); return true; } return false; } /** adds multiple pings if newer than previous pings from the same node */ public synchronized void addPings(PingResponse[] pings) { for (PingResponse ping : pings) { addPing(ping); } } /** serialize current pings to an array */ public synchronized PingResponse[] toArray() { return pings.values().toArray(new PingResponse[pings.size()]); }	maybe use concurrenthashmap? if we can then we don't the synchronized methods.
public void testConnectAndDisconnect() throws Exception { final NodeConnectionsService service = new NodeConnectionsService(Settings.EMPTY, threadPool, transportService); final AtomicBoolean stopReconnecting = new AtomicBoolean(); final Thread reconnectionThread = new Thread(() -> { while (stopReconnecting.get() == false) { final PlainActionFuture<Void> future = new PlainActionFuture<>(); service.ensureConnections(() -> future.onResponse(null)); future.actionGet(); } }, "reconnection thread"); reconnectionThread.start(); try { final List<DiscoveryNode> allNodes = generateNodes(); for (int iteration = 0; iteration < 3; iteration++) { final boolean isDisrupting = randomBoolean(); if (isDisrupting == false) { // if the previous iteration was a disrupting one then there could still be some pending disconnections which would // prevent us from asserting that all nodes are connected in this iteration without this call. ensureConnections(service); } final AtomicBoolean stopDisrupting = new AtomicBoolean(); final Thread disruptionThread = new Thread(() -> { while (isDisrupting && stopDisrupting.get() == false) { transportService.disconnectFromNode(randomFrom(allNodes)); } }, "disruption thread " + iteration); disruptionThread.start(); final DiscoveryNodes nodes = discoveryNodesFromList(randomSubsetOf(allNodes)); final PlainActionFuture<Void> future = new PlainActionFuture<>(); service.connectToNodes(nodes, () -> future.onResponse(null)); future.actionGet(); if (isDisrupting == false) { assertConnected(nodes); } service.disconnectFromNodesExcept(nodes); assertTrue(stopDisrupting.compareAndSet(false, true)); disruptionThread.join(); if (randomBoolean()) { // sometimes do not wait for the disconnections to complete before starting the next connections if (usually()) { ensureConnections(service); assertConnectedExactlyToNodes(nodes); } else { assertBusy(() -> assertConnectedExactlyToNodes(nodes)); } } } } finally { assertTrue(stopReconnecting.compareAndSet(false, true)); reconnectionThread.join(); ensureConnections(service); } }	i would prefer to put ensureconnections outside the finally block to ensure that if the test fails, we see the original error rather than potentially an error from ensureconnections.
* @param predicate the persistent task predicate to evaluate * @param timeout a timeout for waiting * @param listener the callback listener */ public void waitForPersistentTask(final String taskId, final Predicate<PersistentTask<?>> predicate, final @Nullable TimeValue timeout, final WaitForPersistentTaskListener<?> listener) { final Predicate<ClusterState> clusterStatePredicate = clusterState -> predicate.test(PersistentTasksCustomMetaData.getTaskWithId(clusterState, taskId)); final ClusterStateObserver observer = new ClusterStateObserver(clusterService, timeout, logger, threadPool.getThreadContext()); final ClusterState clusterState = observer.setAndGetObservedState(); if (clusterStatePredicate.test(clusterState)) { listener.onResponse(PersistentTasksCustomMetaData.getTaskWithId(clusterState, taskId)); } else { observer.waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { listener.onResponse(PersistentTasksCustomMetaData.getTaskWithId(state, taskId)); } @Override public void onClusterServiceClose() { listener.onFailure(new NodeClosedException(clusterService.localNode())); } @Override public void onTimeout(TimeValue timeout) { listener.onTimeout(timeout); } }, clusterStatePredicate); } }	same - wdyt about a condition suffix/
@Override public RestChannelConsumer doCatRequest(final RestRequest request, final NodeClient client) { final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); final IndicesOptions indicesOptions = IndicesOptions.strictExpand(); final boolean local = request.paramAsBoolean("local", false); final TimeValue masterNodeTimeout = request.paramAsTime("master_timeout", DEFAULT_MASTER_NODE_TIMEOUT); final boolean includeUnloadedSegments = request.paramAsBoolean("include_unloaded_segments", false); return channel -> { final ActionListener<Table> listener = ActionListener.notifyOnce(new RestResponseListener<>(channel) { @Override public RestResponse buildResponse(final Table table) throws Exception { return RestTable.buildResponse(table, channel); } }); sendGetSettingsRequest(indices, indicesOptions, local, masterNodeTimeout, client, new ActionListener<>() { @Override public void onResponse(final GetSettingsResponse getSettingsResponse) { final String[] concreteIndices = new String[getSettingsResponse.getIndexToSettings().size()]; int i = 0; for(ObjectObjectCursor<String, Settings> cursor : getSettingsResponse.getIndexToSettings()) { concreteIndices[i++] = cursor.key; } final GroupedActionListener<ActionResponse> groupedListener = createGroupedListener(request, 4, listener); groupedListener.onResponse(getSettingsResponse); sendIndicesStatsRequest(concreteIndices, indicesOptions, includeUnloadedSegments, client, groupedListener); sendClusterStateRequest(concreteIndices, indicesOptions, local, masterNodeTimeout, client, groupedListener); sendClusterHealthRequest(concreteIndices, indicesOptions, local, masterNodeTimeout, client, groupedListener); } @Override public void onFailure(final Exception e) { listener.onFailure(e); } }); }; }	why do we use the concreteindices rather than indices with indicesoptions. there is a comment in the original code: ... // this behavior can be ensured by letting the cluster health and indices // stats requests re-resolve the index names with the same indices options // that we used for the initial cluster state request (strictexpand). that i believe still applies (if wildcards expand to indices that are subsequently deleted, we should not error).
@Override public RestChannelConsumer doCatRequest(final RestRequest request, final NodeClient client) { final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); final IndicesOptions indicesOptions = IndicesOptions.strictExpand(); final boolean local = request.paramAsBoolean("local", false); final TimeValue masterNodeTimeout = request.paramAsTime("master_timeout", DEFAULT_MASTER_NODE_TIMEOUT); final boolean includeUnloadedSegments = request.paramAsBoolean("include_unloaded_segments", false); return channel -> { final ActionListener<Table> listener = ActionListener.notifyOnce(new RestResponseListener<>(channel) { @Override public RestResponse buildResponse(final Table table) throws Exception { return RestTable.buildResponse(table, channel); } }); sendGetSettingsRequest(indices, indicesOptions, local, masterNodeTimeout, client, new ActionListener<>() { @Override public void onResponse(final GetSettingsResponse getSettingsResponse) { final String[] concreteIndices = new String[getSettingsResponse.getIndexToSettings().size()]; int i = 0; for(ObjectObjectCursor<String, Settings> cursor : getSettingsResponse.getIndexToSettings()) { concreteIndices[i++] = cursor.key; } final GroupedActionListener<ActionResponse> groupedListener = createGroupedListener(request, 4, listener); groupedListener.onResponse(getSettingsResponse); sendIndicesStatsRequest(concreteIndices, indicesOptions, includeUnloadedSegments, client, groupedListener); sendClusterStateRequest(concreteIndices, indicesOptions, local, masterNodeTimeout, client, groupedListener); sendClusterHealthRequest(concreteIndices, indicesOptions, local, masterNodeTimeout, client, groupedListener); } @Override public void onFailure(final Exception e) { listener.onFailure(e); } }); }; }	this can be simplified.
@Override public RestChannelConsumer doCatRequest(final RestRequest request, final NodeClient client) { final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); final IndicesOptions indicesOptions = IndicesOptions.strictExpand(); final boolean local = request.paramAsBoolean("local", false); final TimeValue masterNodeTimeout = request.paramAsTime("master_timeout", DEFAULT_MASTER_NODE_TIMEOUT); final boolean includeUnloadedSegments = request.paramAsBoolean("include_unloaded_segments", false); return channel -> { final ActionListener<Table> listener = ActionListener.notifyOnce(new RestResponseListener<>(channel) { @Override public RestResponse buildResponse(final Table table) throws Exception { return RestTable.buildResponse(table, channel); } }); sendGetSettingsRequest(indices, indicesOptions, local, masterNodeTimeout, client, new ActionListener<>() { @Override public void onResponse(final GetSettingsResponse getSettingsResponse) { final String[] concreteIndices = new String[getSettingsResponse.getIndexToSettings().size()]; int i = 0; for(ObjectObjectCursor<String, Settings> cursor : getSettingsResponse.getIndexToSettings()) { concreteIndices[i++] = cursor.key; } final GroupedActionListener<ActionResponse> groupedListener = createGroupedListener(request, 4, listener); groupedListener.onResponse(getSettingsResponse); sendIndicesStatsRequest(concreteIndices, indicesOptions, includeUnloadedSegments, client, groupedListener); sendClusterStateRequest(concreteIndices, indicesOptions, local, masterNodeTimeout, client, groupedListener); sendClusterHealthRequest(concreteIndices, indicesOptions, local, masterNodeTimeout, client, groupedListener); } @Override public void onFailure(final Exception e) { listener.onFailure(e); } }); }; }	this can be simplified to client.admin().cluster().health(request, listener) there are 3 occurrences of this.
public void testHardBoundsOnDouble() throws Exception { try (Directory dir = newDirectory(); RandomIndexWriter w = new RandomIndexWriter(random(), dir)) { for (double value : new double[] { 3.2, -5, -4.5, 4.3 }) { Document doc = new Document(); doc.add(new SortedNumericDocValuesField("field", NumericUtils.doubleToSortableLong(value))); w.addDocument(doc); } HistogramAggregationBuilder aggBuilder = new HistogramAggregationBuilder("my_agg").field("field") .interval(5) .hardBounds(new DoubleBounds(0.0, 10.0)); MappedFieldType fieldType = new NumberFieldMapper.NumberFieldType("field", NumberFieldMapper.NumberType.DOUBLE); try (IndexReader reader = w.getReader()) { IndexSearcher searcher = new IndexSearcher(reader); InternalHistogram histogram = searchAndReduce(searcher, new MatchAllDocsQuery(), aggBuilder, fieldType); assertEquals(1, histogram.getBuckets().size()); assertEquals(0d, histogram.getBuckets().get(0).getKey()); assertEquals(2, histogram.getBuckets().get(0).getDocCount()); assertTrue(AggregationInspectionHelper.hasValue(histogram)); } } }	i'm confused, is this a different test than the one above it? if so, i'm missing what's different.
@Override protected int advanceValue(org.elasticsearch.common.geo.GeoPoint target, int valuesIdx) { values[valuesIdx] = Geohash.longEncode(target.getLon(), target.getLat(), precision); return valuesIdx + 1; } } private static class BoundedCellValues extends CellValues { private final GeoHashBoundedPredicate predicate; private final GeoBoundingBox bbox; BoundedCellValues(MultiGeoPointValues geoValues, int precision, GeoBoundingBox bbox) { super(geoValues, precision); this.predicate = new GeoHashBoundedPredicate(precision, bbox); this.bbox = bbox; } @Override protected int advanceValue(org.elasticsearch.common.geo.GeoPoint target, int valuesIdx) { final String hash = Geohash.stringEncode(target.getLon(), target.getLat(), precision); if (validPoint(target.getLon(), target.getLat()) || predicate.validHash(hash)) { values[valuesIdx] = Geohash.longEncode(hash); return valuesIdx + 1; } return valuesIdx; } private boolean validPoint(double x, double y) { if (bbox.top() > y && bbox.bottom() < y) { boolean crossesDateline = bbox.left() > bbox.right(); if (crossesDateline) { return bbox.left() < x || bbox.right() > x; } else { return bbox.left() < x && bbox.right() > x; } } return false; }	it feels like this could be moved to geohashboundedpredicate, just as the similar method from geotilecellidsource was moved.
public void testFieldNameLengthLimit() throws Throwable { int maxFieldNameLength = randomIntBetween(25, 30); String testString = new String(new char[maxFieldNameLength + 1]).replace("\\\\0", "a"); Settings settings = Settings.builder().put(MapperService.INDEX_MAPPING_FIELD_NAME_LENGTH_LIMIT_SETTING.getKey(), maxFieldNameLength) .build(); MapperService mapperService = createMapperService(settings, fieldMapping(b -> b.field("type", "text"))); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> merge(mapperService, mapping(b -> b.startObject(testString).field("type", "text").endObject()))); assertEquals("Field name [" + testString + "] is longer than the limit of [" + maxFieldNameLength + "] characters", e.getMessage()); }	could you explain these? i imagine they are useful but i don't know why right now.
public NodeMetadata build() { final Version nodeVersion; final Version oldestIndexVersion; if (this.nodeVersion == null) { assert Version.CURRENT.major <= Version.V_7_0_0.major + 1 : "version is required in the node metadata from v9 onwards"; nodeVersion = Version.V_EMPTY; } else { nodeVersion = this.nodeVersion; } if (this.previousNodeVersion == null) { previousNodeVersion = nodeVersion; } if (this.oldestIndexVersion == null) { oldestIndexVersion = Version.V_EMPTY; } else { oldestIndexVersion = this.oldestIndexVersion; } return new NodeMetadata(nodeId, nodeVersion, previousNodeVersion, oldestIndexVersion); }	this is only used in the legacy format which we shouldn't really be reading here, but i think this means that the index compatibility check will fail before the node version check happens, so the error will say that there are some ancient indices rather than that the previous node version is too old. i wonder if it would be better to check the index version in nodemetadata#upgradetocurrentversion and just call that in both places so we have consistent checks.
*/ @Nullable public static NodeMetadata nodeMetadata(Path... dataPaths) throws IOException { String nodeId = null; Version version = null; Version oldestIndexVersion = Version.V_EMPTY; for (final Path dataPath : dataPaths) { final Path indexPath = dataPath.resolve(METADATA_DIRECTORY_NAME); if (Files.exists(indexPath)) { try (DirectoryReader reader = DirectoryReader.open(new NIOFSDirectory(dataPath.resolve(METADATA_DIRECTORY_NAME)))) { final Map<String, String> userData = reader.getIndexCommit().getUserData(); assert userData.get(NODE_VERSION_KEY) != null; final String thisNodeId = userData.get(NODE_ID_KEY); assert thisNodeId != null; if (nodeId != null && nodeId.equals(thisNodeId) == false) { throw new CorruptStateException( "unexpected node ID in metadata, found [" + thisNodeId + "] in [" + dataPath + "] but expected [" + nodeId + "]" ); } else if (nodeId == null) { nodeId = thisNodeId; version = Version.fromId(Integer.parseInt(userData.get(NODE_VERSION_KEY))); if (userData.containsKey(OLDEST_INDEX_VERSION_KEY)) { oldestIndexVersion = Version.fromId(Integer.parseInt(userData.get(OLDEST_INDEX_VERSION_KEY))); } } } catch (IndexNotFoundException e) { logger.debug(new ParameterizedMessage("no on-disk state at {}", indexPath), e); } } } if (nodeId == null) { return null; } return new NodeMetadata(nodeId, version, oldestIndexVersion); }	should we set oldestindexversion back to version.v_empty here if the key is missing? otherwise we end up returning a nodemetadata that never really existed. super-rare case i know...
public void writeIncrementalTermUpdateAndCommit(long currentTerm, long lastAcceptedVersion, int oldestIndexVersion) throws IOException { ensureOpen(); ensureFullStateWritten(); commit(currentTerm, lastAcceptedVersion, oldestIndexVersion); }	similarly here: suggestion public void writeincrementaltermupdateandcommit(long currentterm, long lastacceptedversion, version oldestindexversion)
public void testIndexCompatibilityChecks() throws IOException { final Settings settings = buildEnvSettings(Settings.EMPTY); try (NodeEnvironment env = newNodeEnvironment(settings)) { try ( PersistedClusterStateService.Writer writer = new PersistedClusterStateService( env.nodeDataPaths(), env.nodeId(), xContentRegistry(), new ClusterSettings(settings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), () -> 0L ).createWriter() ) { writer.writeFullStateAndCommit( 1L, ClusterState.builder(ClusterName.DEFAULT) .metadata( Metadata.builder() .persistentSettings(Settings.builder().put(Metadata.SETTING_READ_ONLY_SETTING.getKey(), true).build()) .build() ) .build() ); } Version oldIndexVersion = Version.fromId(between(1, Version.CURRENT.minimumIndexCompatibilityVersion().id - 1)); overrideOldestIndexVersion(oldIndexVersion, env.nodeDataPaths()); IllegalStateException ex = expectThrows( IllegalStateException.class, "Must fail the check on index that's too old", () -> checkForIndexCompatibility(logger, env.nodePaths()) ); assertThat(ex.getMessage(), containsString("[" + oldIndexVersion + "] exist")); assertThat(ex.getMessage(), startsWith("cannot upgrade node because incompatible indices created with version")); // This should work overrideOldestIndexVersion(Version.CURRENT.minimumIndexCompatibilityVersion(), env.nodeDataPaths()); checkForIndexCompatibility(logger, env.nodePaths()); // Trying to boot with newer version should pass this check overrideOldestIndexVersion(NodeMetadataTests.tooNewVersion(), env.nodeDataPaths()); checkForIndexCompatibility(logger, env.nodePaths()); // Simulate empty old index version, attempting to upgrade before 7.17 removeOldestIndexVersion(env.nodeDataPaths()); ex = expectThrows( IllegalStateException.class, "Must fail the check on index that's too old", () -> checkForIndexCompatibility(logger, env.nodePaths()) ); assertThat(ex.getMessage(), startsWith("cannot upgrade node because incompatible indices created with version")); assertThat(ex.getMessage(), containsString("[" + Version.V_EMPTY + "] exist")); } }	aha yeah this is what i meant in a previous comment - we should report this as the node version being too old rather than mentioning v_empty.
private static void overrideOldestIndexVersion(Version oldVersion, Path... dataPaths) throws IOException { for (final Path dataPath : dataPaths) { final Path indexPath = dataPath.resolve(METADATA_DIRECTORY_NAME); if (Files.exists(indexPath)) { try (DirectoryReader reader = DirectoryReader.open(new NIOFSDirectory(dataPath.resolve(METADATA_DIRECTORY_NAME)))) { final Map<String, String> userData = reader.getIndexCommit().getUserData(); final IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new KeywordAnalyzer()); indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.APPEND); indexWriterConfig.setCommitOnClose(false); indexWriterConfig.setRAMBufferSizeMB(1.0); indexWriterConfig.setMergeScheduler(new SerialMergeScheduler()); try ( IndexWriter indexWriter = new IndexWriter( new NIOFSDirectory(dataPath.resolve(METADATA_DIRECTORY_NAME)), indexWriterConfig ) ) { final Map<String, String> commitData = new HashMap<>(userData); commitData.put(OLDEST_INDEX_VERSION_KEY, Integer.toString(oldVersion.id)); indexWriter.setLiveCommitData(commitData.entrySet()); indexWriter.commit(); } } } } }	i think the defaults are fine for our purposes here, no need to adjust them like this.
public void testEqualsHashcodeSerialization() { final Path tempDir = createTempDir(); EqualsHashCodeTestUtils.checkEqualsAndHashCode( new NodeMetadata(randomAlphaOfLength(10), randomVersion(), randomVersion()), nodeMetadata -> { final long generation = NodeMetadata.FORMAT.writeAndCleanup(nodeMetadata, tempDir); final Tuple<NodeMetadata, Long> nodeMetadataLongTuple = NodeMetadata.FORMAT.loadLatestStateWithGeneration( logger, xContentRegistry(), tempDir ); assertThat(nodeMetadataLongTuple.v2(), equalTo(generation)); return nodeMetadataLongTuple.v1(); }, nodeMetadata -> { if (randomBoolean()) { return new NodeMetadata( randomAlphaOfLength(21 - nodeMetadata.nodeId().length()), nodeMetadata.nodeVersion(), Version.CURRENT ); } else { return new NodeMetadata( nodeMetadata.nodeId(), randomValueOtherThan(nodeMetadata.nodeVersion(), this::randomVersion), Version.CURRENT ); } } ); }	could we have a third branch which mutates the new field and leaves the other two fields alone, to verify that equals() does compare the new field too?
private void processAfter(IndexRequest request, IndexShard indexShard, Translog.Location location) { if (request.refresh()) { try { indexShard.refresh("refresh_flag_index"); } catch (Throwable e) { // ignore } } if (indexShard.getTranslogDurability() == Translog.Durabilty.REQUEST && location != null) { try { indexShard.sync(location); } catch (EngineClosedException e) { // ignore, the engine is already closed and we do not want the // operation to be retried, because it has been modified } } }	i think this is the wrong fix since we call this in many places ie. during delete and bulk. i think we should just rename this method into trysync and don't sync if the engine is closed. so we handle it correctly everywhere?
public void testGetFieldFilterSecurityEnabledLicenseNoFLS() throws Exception { createComponents(Settings.EMPTY); Function<String, Predicate<String>> fieldFilter = security.getFieldFilter(); assertNotSame(MapperPlugin.NOOP_FIELD_FILTER, fieldFilter); licenseState.update( randomFrom(License.OperationMode.BASIC, License.OperationMode.STANDARD, License.OperationMode.GOLD), true, null); assertNotSame(MapperPlugin.NOOP_FIELD_FILTER, fieldFilter); assertSame(MapperPlugin.NOOP_FIELD_PREDICATE, fieldFilter.apply(randomAlphaOfLengthBetween(3, 6))); }	why not put this in defaultauthenticationfailurehandlertests and then it doesn't need such a long name? like we can just have testsortswwwauthenticateheadervalues in that class
static Request cleanupRepository(CleanupRepositoryRequest cleanupRepositoryRequest) { String endpoint = new RequestConverters.EndpointBuilder().addPathPartAsIs("_snapshot") .addPathPart(cleanupRepositoryRequest.repository()) .addPathPartAsIs("cleanup") .build(); Request request = new Request(HttpPost.METHOD_NAME, endpoint); RequestConverters.Params parameters = new RequestConverters.Params(); parameters.withMasterTimeout(cleanupRepositoryRequest.masterNodeTimeout()); parameters.withTimeout(cleanupRepositoryRequest.timeout()); request.addParameters(parameters.asMap()); return request; }	should we use _cleanup instead of cleanup (like we use _verify)?
@SuppressForbidden(reason = "Channel is based of a socket not a file") @Override public int write(ByteBuffer src) throws IOException { return SocketAccess.doPrivilegedIOException(() -> writeChannel.write(src)); } })); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Uploads a blob using the "multipart upload" method (a single * 'multipart/related' request containing both data and metadata. The request is * gziped), see: * https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload * @param blobInfo the info for the blob to be uploaded * @param inputStream the stream containing the blob data * @param blobSize the size * @param failIfAlreadyExists whether to throw a FileAlreadyExistsException if the given blob already exists */ private void writeBlobMultipart(BlobInfo blobInfo, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) throws IOException { assert blobSize <= LARGE_BLOB_THRESHOLD_BYTE_SIZE : "large blob uploads should use the resumable upload method"; final ByteArrayOutputStream baos = new ByteArrayOutputStream(Math.toIntExact(blobSize)); Streams.copy(inputStream, baos); try { final Storage.BlobTargetOption[] targetOptions = failIfAlreadyExists ? new Storage.BlobTargetOption[] { Storage.BlobTargetOption.doesNotExist() } : new Storage.BlobTargetOption[0]; SocketAccess.doPrivilegedVoidIOException( () -> client().create(blobInfo, baos.toByteArray(), targetOptions)); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Deletes the blob from the specific bucket * * @param blobName name of the blob */ void deleteBlob(String blobName) throws IOException { final BlobId blobId = BlobId.of(bucketName, blobName); final boolean deleted = SocketAccess.doPrivilegedIOException(() -> client().delete(blobId)); if (deleted == false) { throw new NoSuchFileException("Blob [" + blobName + "] does not exist"); } } /** * Deletes the given path and all its children. * * @param pathStr Name of path to delete */ void deleteDirectory(String pathStr, LongConsumer resultConsumer) throws IOException { SocketAccess.doPrivilegedVoidIOException(() -> { Page<Blob> page = client().get(bucketName).list(BlobListOption.prefix(pathStr)); do { final Collection<String> blobsToDelete = new ArrayList<>(); page.getValues().forEach(b -> { blobsToDelete.add(b.getName()); resultConsumer.accept(b.getSize()); }); deleteBlobsIgnoringIfNotExists(blobsToDelete); page = page.getNextPage(); } while (page != null); }); } /** * Deletes multiple blobs from the specific bucket using a batch request * * @param blobNames names of the blobs to delete */ void deleteBlobsIgnoringIfNotExists(Collection<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } final List<BlobId> blobIdsToDelete = blobNames.stream().map(blob -> BlobId.of(bucketName, blob)).collect(Collectors.toList()); final List<BlobId> failedBlobs = Collections.synchronizedList(new ArrayList<>()); final StorageException e = SocketAccess.doPrivilegedIOException(() -> { final AtomicReference<StorageException> ioe = new AtomicReference<>(); final StorageBatch batch = client().batch(); for (BlobId blob : blobIdsToDelete) { batch.delete(blob).notify( new BatchResult.Callback<>() { @Override public void success(Boolean result) { } @Override public void error(StorageException exception) { if (exception.getCode() != HTTP_NOT_FOUND) { failedBlobs.add(blob); if (ioe.compareAndSet(null, exception) == false) { ioe.get().addSuppressed(exception); } } } }); } batch.submit(); return ioe.get(); }	we should consume the size only if the deletion executes correctly
@Override public void delete(LongConsumer resultConsumer) throws IOException { try (AmazonS3Reference clientReference = blobStore.clientReference()) { ObjectListing prevListing = null; while (true) { ObjectListing list; if (prevListing != null) { final ObjectListing finalPrevListing = prevListing; list = SocketAccess.doPrivileged(() -> clientReference.client().listNextBatchOfObjects(finalPrevListing)); } else { final ListObjectsRequest listObjectsRequest = new ListObjectsRequest(); listObjectsRequest.setBucketName(blobStore.bucket()); listObjectsRequest.setPrefix(keyPath); list = SocketAccess.doPrivileged(() -> clientReference.client().listObjects(listObjectsRequest)); } final List<String> blobsToDelete = new ArrayList<>(); list.getObjectSummaries().forEach(s3ObjectSummary -> { resultConsumer.accept(s3ObjectSummary.getSize()); blobsToDelete.add(s3ObjectSummary.getKey()); }); if (list.isTruncated()) { doDeleteBlobs(blobsToDelete, false); prevListing = list; } else { final List<String> lastBlobsToDelete = new ArrayList<>(blobsToDelete); lastBlobsToDelete.add(keyPath); doDeleteBlobs(lastBlobsToDelete, false); break; } } } catch (final AmazonClientException e) { throw new IOException("Exception when deleting blob container [" + keyPath + "]", e); } }	we should consume the size only if the deletion executes correctly
@Override public void delete(LongConsumer resultConsumer) throws IOException { Files.walkFileTree(path, new SimpleFileVisitor<>() { @Override public FileVisitResult postVisitDirectory(Path dir, IOException impossible) throws IOException { assert impossible == null; Files.delete(dir); return FileVisitResult.CONTINUE; } @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { Files.delete(file); resultConsumer.accept(attrs.size()); return FileVisitResult.CONTINUE; } }); }	i think we should continue to delete other files/dirs even if deleting a specific file/dir throws an exception.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(MAX_NUM_SEGMENTS_FIELD.getPreferredName(), maxNumSegments); builder.field(CODEC.getPreferredName(), codec); builder.endObject(); return builder; }	when this is made @nullable this should be inside an java if (codec != null) { builder.field(...) }
@Override public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) { Settings readOnlySettings = Settings.builder().put(IndexMetaData.SETTING_BLOCKS_WRITE, true).build(); Settings bestCompressionSettings = Settings.builder() .put(EngineConfig.INDEX_CODEC_SETTING.getKey(), CodecService.BEST_COMPRESSION_CODEC).build(); StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME); StepKey forceMergeKey = new StepKey(phase, NAME, ForceMergeStep.NAME); StepKey countKey = new StepKey(phase, NAME, SegmentCountStep.NAME); if (codec.getName().equals(CodecService.BEST_COMPRESSION_CODEC)) { StepKey closeKey = new StepKey(phase, NAME, CloseIndexStep.NAME); StepKey openKey = new StepKey(phase, NAME, OpenIndexStep.NAME); StepKey waitForGreenIndexKey = new StepKey(phase, NAME, WaitForIndexColorStep.NAME); StepKey updateCompressionKey = new StepKey(phase, NAME, UpdateSettingsStep.NAME); CloseIndexStep closeIndexStep = new CloseIndexStep(closeKey, updateCompressionKey, client); UpdateSettingsStep updateBestCompressionSettings = new UpdateSettingsStep(updateCompressionKey, openKey, client, bestCompressionSettings); OpenIndexStep openIndexStep = new OpenIndexStep(openKey, waitForGreenIndexKey, client); WaitForIndexColorStep waitForIndexGreenStep = new WaitForIndexColorStep(waitForGreenIndexKey, forceMergeKey, ClusterHealthStatus.GREEN); ForceMergeStep forceMergeStep = new ForceMergeStep(forceMergeKey, nextStepKey, client, maxNumSegments); return Arrays.asList(closeIndexStep, updateBestCompressionSettings, openIndexStep, waitForIndexGreenStep, forceMergeStep); } UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, forceMergeKey, client, readOnlySettings); ForceMergeStep forceMergeStep = new ForceMergeStep(forceMergeKey, countKey, client, maxNumSegments); SegmentCountStep segmentCountStep = new SegmentCountStep(countKey, nextStepKey, client, maxNumSegments); return Arrays.asList(readOnlyStep, forceMergeStep, segmentCountStep); }	can you add an else clause here that should throw an error if any codec is specified that is *not* best_compression
private void assertBestCompression(ForceMergeAction instance) { String phase = randomAlphaOfLength(5); StepKey nextStepKey = new StepKey(randomAlphaOfLength(10), randomAlphaOfLength(10), randomAlphaOfLength(10)); List<Step> steps = instance.toSteps(null, phase, nextStepKey); assertNotNull(steps); assertEquals(5, steps.size()); CloseIndexStep firstStep = (CloseIndexStep) steps.get(0); UpdateSettingsStep secondStep = (UpdateSettingsStep) steps.get(1); OpenIndexStep thirdStep = (OpenIndexStep) steps.get(2); WaitForIndexColorStep fourthStep = (WaitForIndexColorStep) steps.get(3); ForceMergeStep fifthStep = (ForceMergeStep) steps.get(4); assertThat(firstStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, CloseIndexStep.NAME))); assertThat(firstStep.getNextStepKey(), equalTo(secondStep.getKey())); assertThat(secondStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, UpdateSettingsStep.NAME))); assertThat(secondStep.getSettings().get(EngineConfig.INDEX_CODEC_SETTING.getKey()), equalTo(CodecService.BEST_COMPRESSION_CODEC)); assertThat(secondStep.getNextStepKey(), equalTo(thirdStep.getKey())); assertThat(thirdStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, OpenIndexStep.NAME))); assertThat(thirdStep.getNextStepKey(), equalTo(fourthStep)); assertThat(fourthStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, WaitForIndexColorStep.NAME))); assertThat(fourthStep.getNextStepKey(), equalTo(fifthStep)); assertThat(fifthStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, ForceMergeStep.NAME))); assertThat(fifthStep.getNextStepKey(), equalTo(nextStepKey)); }	this test is missing the segment count step check
private void assertBestCompression(ForceMergeAction instance) { String phase = randomAlphaOfLength(5); StepKey nextStepKey = new StepKey(randomAlphaOfLength(10), randomAlphaOfLength(10), randomAlphaOfLength(10)); List<Step> steps = instance.toSteps(null, phase, nextStepKey); assertNotNull(steps); assertEquals(5, steps.size()); CloseIndexStep firstStep = (CloseIndexStep) steps.get(0); UpdateSettingsStep secondStep = (UpdateSettingsStep) steps.get(1); OpenIndexStep thirdStep = (OpenIndexStep) steps.get(2); WaitForIndexColorStep fourthStep = (WaitForIndexColorStep) steps.get(3); ForceMergeStep fifthStep = (ForceMergeStep) steps.get(4); assertThat(firstStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, CloseIndexStep.NAME))); assertThat(firstStep.getNextStepKey(), equalTo(secondStep.getKey())); assertThat(secondStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, UpdateSettingsStep.NAME))); assertThat(secondStep.getSettings().get(EngineConfig.INDEX_CODEC_SETTING.getKey()), equalTo(CodecService.BEST_COMPRESSION_CODEC)); assertThat(secondStep.getNextStepKey(), equalTo(thirdStep.getKey())); assertThat(thirdStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, OpenIndexStep.NAME))); assertThat(thirdStep.getNextStepKey(), equalTo(fourthStep)); assertThat(fourthStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, WaitForIndexColorStep.NAME))); assertThat(fourthStep.getNextStepKey(), equalTo(fifthStep)); assertThat(fifthStep.getKey(), equalTo(new StepKey(phase, ForceMergeAction.NAME, ForceMergeStep.NAME))); assertThat(fifthStep.getNextStepKey(), equalTo(nextStepKey)); }	we need a test for an invalid codec setting too (one where the setting is not best_compression)
private ConcurrentMap<String, LifecycleAction> convertActionNamesToActions(String... availableActionNames) { return Arrays.asList(availableActionNames).stream().map(n -> { switch (n) { case AllocateAction.NAME: return new AllocateAction(null, Collections.singletonMap("foo", "bar"), Collections.emptyMap(), Collections.emptyMap()); case DeleteAction.NAME: return new DeleteAction(); case ForceMergeAction.NAME: return new ForceMergeAction(1, Codec.getDefault()); case ReadOnlyAction.NAME: return new ReadOnlyAction(); case RolloverAction.NAME: return new RolloverAction(ByteSizeValue.parseBytesSizeValue("0b", "test"), TimeValue.ZERO, 1L); case ShrinkAction.NAME: return new ShrinkAction(1); case FreezeAction.NAME: return new FreezeAction(); case SetPriorityAction.NAME: return new SetPriorityAction(0); case UnfollowAction.NAME: return new UnfollowAction(); } return new DeleteAction(); }).collect(Collectors.toConcurrentMap(LifecycleAction::getWriteableName, Function.identity())); }	these should pass null (or have a second constructor so there's no need to specify the codec)
@Override public void onFailure(final Exception e) { onFailure.accept(unknownLicense.apply(e)); } }); } /** * Fetches the history UUIDs for leader index on per shard basis using the specified leaderClient. * * @param leaderClient the leader client * @param leaderIndexMetaData the leader index metadata * @param onFailure the failure consumer * @param historyUUIDConsumer the leader index history uuid and consumer */ // NOTE: Placed this method here; in order to avoid duplication of logic for fetching history UUIDs // in case of following a local or a remote cluster. public void fetchLeaderHistoryUUIDs( final Client leaderClient, final IndexMetaData leaderIndexMetaData, final Consumer<Exception> onFailure, final Consumer<String[]> historyUUIDConsumer) { String leaderIndex = leaderIndexMetaData.getIndex().getName(); CheckedConsumer<IndicesStatsResponse, Exception> indicesStatsHandler = indicesStatsResponse -> { IndexStats indexStats = indicesStatsResponse.getIndices().get(leaderIndex); String[] historyUUIDs = new String[leaderIndexMetaData.getNumberOfShards()]; for (IndexShardStats indexShardStats : indexStats) { for (ShardStats shardStats : indexShardStats) { // Ignore replica shards as they may not have yet started and // we just end up overwriting slots in historyUUIDs if (shardStats.getShardRouting().primary() == false) { continue; } CommitStats commitStats = shardStats.getCommitStats(); if (commitStats == null) { onFailure.accept(new IllegalArgumentException("leader index's commit stats are missing")); return; } String historyUUID = commitStats.getUserData().get(Engine.HISTORY_UUID_KEY); if (historyUUID == null) { onFailure.accept(new IllegalArgumentException("leader index does not have an history uuid")); return; } ShardId shardId = shardStats.getShardRouting().shardId(); assert historyUUIDs[shardId.id()] == null; historyUUIDs[shardId.id()] = historyUUID; } } assert new HashSet<>(Arrays.asList(historyUUIDs)).size() == leaderIndexMetaData.getNumberOfShards(); historyUUIDConsumer.accept(historyUUIDs); }; IndicesStatsRequest request = new IndicesStatsRequest(); request.clear(); request.indices(leaderIndex); leaderClient.admin().indices().stats(request, ActionListener.wrap(indicesStatsHandler, onFailure)); } private static ElasticsearchStatusException indexMetadataNonCompliantRemoteLicense( final String leaderIndex, final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) { final String clusterAlias = licenseCheck.remoteClusterLicenseInfo().clusterAlias(); final String message = String.format( Locale.ROOT, "can not fetch remote index [%s:%s] metadata as the remote cluster [%s] is not licensed for [ccr]; %s", clusterAlias, leaderIndex, clusterAlias, RemoteClusterLicenseChecker.buildErrorMessage( "ccr", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)); return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST); } private static ElasticsearchStatusException clusterStateNonCompliantRemoteLicense( final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) { final String clusterAlias = licenseCheck.remoteClusterLicenseInfo().clusterAlias(); final String message = String.format( Locale.ROOT, "can not fetch remote cluster state as the remote cluster [%s] is not licensed for [ccr]; %s", clusterAlias, RemoteClusterLicenseChecker.buildErrorMessage( "ccr", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)); return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST); }	martijn, i am sorry. i should have been clearer here: - if a commit stats is not null, it should have a valid history uuid. we can remove the null check historyuuid == null. - if a primary is unassigned, the index_shard_stats of that primary is not returned in the response; thus we won't have a history uuid for that shardid in the array. i think we should check that every entry in the historyuuids array is not null; otherwise, we should fail the request. wdyt? please note the assertion assert new hashset<>(arrays.aslist(historyuuids)).size() == leaderindexmetadata.getnumberofshards(); does not guarantee that every entry is non-null.
public ClusterState performAction(Index index, ClusterState clusterState) { IndexMetaData indexMetaData = clusterState.getMetaData().index(index); if (indexMetaData == null) { logger.debug("[{}] lifecycle action for index [{}] executed but index no longer exists", getKey().getAction(), index.getName()); // Index must have been since deleted, ignore it return clusterState; } LifecycleExecutionState lifecycleState = LifecycleExecutionState .fromIndexMetadata(indexMetaData); if (shouldParseIndexName(indexMetaData.getSettings())) { long parsedOriginationDate = parseIndexNameAndExtractDate(index.getName()); ClusterState.Builder newClusterStateBuilder = ClusterState.builder(clusterState); LifecycleExecutionState.Builder newCustomData = LifecycleExecutionState.builder(lifecycleState); newCustomData.setIndexCreationDate(indexMetaData.getCreationDate()); newClusterStateBuilder.metaData(MetaData.builder(clusterState.getMetaData()).put(IndexMetaData .builder(indexMetaData) .settingsVersion(indexMetaData.getSettingsVersion() + 1) .settings(Settings.builder() .put(indexMetaData.getSettings()) .put(LifecycleSettings.LIFECYCLE_ORIGINATION_DATE, parsedOriginationDate) .build() ) .putCustom(ILM_CUSTOM_METADATA_KEY, newCustomData.build().asMap()))); return newClusterStateBuilder.build(); } if (lifecycleState.getLifecycleDate() != null) { return clusterState; } ClusterState.Builder newClusterStateBuilder = ClusterState.builder(clusterState); LifecycleExecutionState.Builder newCustomData = LifecycleExecutionState.builder(lifecycleState); newCustomData.setIndexCreationDate(indexMetaData.getCreationDate()); newClusterStateBuilder.metaData(MetaData.builder(clusterState.getMetaData()).put(IndexMetaData .builder(indexMetaData) .putCustom(ILM_CUSTOM_METADATA_KEY, newCustomData.build().asMap()))); return newClusterStateBuilder.build(); }	i think this can be refactored a bit not to have multiple return paths for the cluster state, instead using a single clusterstate.builder and only adding updated settings iff shouldparseindexname is true
@Override public void onIndexModule(IndexModule indexModule) { if (ilmEnabled) { indexModule.addIndexEventListener(indexLifecycleInitialisationService.get()); } }	can you add an assert indexlifecycleinitialisationservice.get() != null before this line? i want to make sure we don't add any npes in the future
void start() { final ClusterState clusterState = followerClusterStateSupplier.get(); final AutoFollowMetadata autoFollowMetadata = clusterState.metaData().custom(AutoFollowMetadata.TYPE); if (autoFollowMetadata == null) { LOGGER.info("AutoFollower for cluster [{}] has stopped, because there is no autofollow metadata", remoteCluster); return; } final List<String> patterns = autoFollowMetadata.getPatterns().entrySet().stream() .filter(entry -> entry.getValue().getRemoteCluster().equals(remoteCluster)) .map(Map.Entry::getKey) .collect(Collectors.toList()); if (patterns.isEmpty()) { LOGGER.info("AutoFollower for cluster [{}] has stopped, because there are no more patterns", remoteCluster); return; } this.autoFollowPatternsCountDown = new CountDown(patterns.size()); this.autoFollowResults = new AtomicArray<>(patterns.size()); getLeaderClusterState(remoteCluster, (remoteClusterState, remoteError) -> { if (remoteClusterState != null) { assert remoteError == null; autoFollowIndices(autoFollowMetadata, clusterState, remoteClusterState, patterns); } else { for (int i = 0; i < patterns.size(); i++) { String autoFollowPatternName = patterns.get(i); finalise(i, new AutoFollowResult(autoFollowPatternName, remoteError)); } } }); }	i guess this should be renamed to getremoteclusterstate now.
void start() { final ClusterState clusterState = followerClusterStateSupplier.get(); final AutoFollowMetadata autoFollowMetadata = clusterState.metaData().custom(AutoFollowMetadata.TYPE); if (autoFollowMetadata == null) { LOGGER.info("AutoFollower for cluster [{}] has stopped, because there is no autofollow metadata", remoteCluster); return; } final List<String> patterns = autoFollowMetadata.getPatterns().entrySet().stream() .filter(entry -> entry.getValue().getRemoteCluster().equals(remoteCluster)) .map(Map.Entry::getKey) .collect(Collectors.toList()); if (patterns.isEmpty()) { LOGGER.info("AutoFollower for cluster [{}] has stopped, because there are no more patterns", remoteCluster); return; } this.autoFollowPatternsCountDown = new CountDown(patterns.size()); this.autoFollowResults = new AtomicArray<>(patterns.size()); getLeaderClusterState(remoteCluster, (remoteClusterState, remoteError) -> { if (remoteClusterState != null) { assert remoteError == null; autoFollowIndices(autoFollowMetadata, clusterState, remoteClusterState, patterns); } else { for (int i = 0; i < patterns.size(); i++) { String autoFollowPatternName = patterns.get(i); finalise(i, new AutoFollowResult(autoFollowPatternName, remoteError)); } } }); }	maybe an assertion that remoteerror is not null?
void cleanFollowedLeaderIndices(final ClusterState remoteClusterState, final List<String> patterns) { updateAutoFollowMetadata(cleanFollowedLeaderIndices(remoteClusterState.metaData(), patterns), e -> { if (e != null) { LOGGER.warn("Error occured while cleaning followed leader indices", e); } }); }	how about diff diff --git a/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/autofollowcoordinator.java b/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/autofollowcoordinator.java index b3e437bc381..8fe89b3ae35 100644 --- a/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/autofollowcoordinator.java +++ b/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/autofollowcoordinator.java @@ -476,8 +476,8 @@ public class autofollowcoordinator implements clusterstatelistener { }); } - static function<clusterstate, clusterstate> cleanfollowedleaderindices(final metadata remotemetadata, - final list<string> autofollowpatternnames) { + static function<clusterstate, clusterstate> cleanfollowedleaderindices( + final metadata remotemetadata, final list<string> autofollowpatternnames) { return currentstate -> { autofollowmetadata currentautofollowmetadata = currentstate.metadata().custom(autofollowmetadata.type); map<string, list<string>> newfollowedindexuuids = new hashmap<>(currentautofollowmetadata.getfollowedleaderindexuuids());
@BeforeClass public static void beforeClass() { nodeSettings = Settings.builder() .put("node.name", AbstractQueryTestCase.class.toString()) .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) .build(); index = new Index(randomAlphaOfLengthBetween(1, 10), "_na_"); if (rarely()) { currentTypes = new String[0]; // no types } else { currentTypes = new String[] { "_doc" }; } randomTypes = getRandomTypes(); }	if we only have one allowed type on master now, we could also look into making currenttypes a string, not an array. also changing it to "currenttype" might be better then.
private static String[] getRandomTypes() { int rand = random().nextInt(3); switch (rand) { case 0: return new String[0]; case 1: return currentTypes; case 2: return new String[] {MetaData.ALL}; default: throw new AssertionError("invalid random number " + rand); } }	if we change currenttypes to be a string constant, we could also use this here.
public BulkRequest add(byte[] data, int from, int length, XContentType xContentType) throws IOException { return add(data, from, length, null, null, xContentType); } /** * Adds a framed data in binary format * @deprecated use {@link #add(byte[], int, int, String, XContentType)}	could the other methods using "type" be deprecated as well?
public BulkRequestBuilder add(byte[] data, int from, int length, XContentType xContentType) throws Exception { request.add(data, from, length, null, xContentType); return this; } /** * Adds a framed data in binary format * @deprecated use {@link #add(byte[], int, int, String, XContentType)}	could the other methods using "type" be deprecated as well?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); // A 7.x request allows null types but if deserialized in a 6.x node will cause nullpointer exceptions. // So we use the type accessor method here to make the type non-null (will default it to "_doc"). out.writeOptionalString(type()); out.writeOptionalString(id); out.writeOptionalString(routing); if (out.getVersion().before(Version.V_7_0_0)) { out.writeOptionalString(null); // _parent } if (out.getVersion().before(Version.V_6_0_0_alpha1)) { // Serialize a fake timestamp. 5.x expect this value to be set by the #process method so we can't use null. // On the other hand, indices created on 5.x do not index the timestamp field. Therefore passing a 0 (or any value) for // the transport layer OK as it will be ignored. out.writeOptionalString("0"); out.writeOptionalWriteable(null); } out.writeBytesReference(source); out.writeByte(opType.getId()); out.writeLong(version); out.writeByte(versionType.getValue()); out.writeOptionalString(pipeline); out.writeBoolean(isRetry); out.writeLong(autoGeneratedTimestamp); if (contentType != null) { out.writeBoolean(true); out.writeEnum(contentType); } else { out.writeBoolean(false); } if (out.getVersion().onOrAfter(Version.V_6_6_0)) { out.writeZLong(ifSeqNo); out.writeVLong(ifPrimaryTerm); } else if (ifSeqNo != UNASSIGNED_SEQ_NO || ifPrimaryTerm != UNASSIGNED_PRIMARY_TERM) { assert false : "setIfMatch [" + ifSeqNo + "], currentDocTem [" + ifPrimaryTerm + "]"; throw new IllegalStateException( "sequence number based compare and write is not supported until all nodes are on version 7.0 or higher. " + "Stream version [" + out.getVersion() + "]"); } }	maybe add the same comment to the writeto() above in deleterequest too. it helps.
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); waitForActiveShards.writeTo(out); out.writeString(type()); out.writeString(id); out.writeOptionalString(routing); if (out.getVersion().before(Version.V_7_0_0)) { out.writeOptionalString(null); // _parent } boolean hasScript = script != null; out.writeBoolean(hasScript); if (hasScript) { script.writeTo(out); } out.writeVInt(retryOnConflict); refreshPolicy.writeTo(out); if (doc == null) { out.writeBoolean(false); } else { out.writeBoolean(true); // make sure the basics are set doc.index(index); doc.type(type); doc.id(id); doc.writeTo(out); } if (out.getVersion().before(Version.V_7_0_0)) { out.writeOptionalStringArray(null); } out.writeOptionalWriteable(fetchSourceContext); if (upsertRequest == null) { out.writeBoolean(false); } else { out.writeBoolean(true); // make sure the basics are set upsertRequest.index(index); upsertRequest.type(type); upsertRequest.id(id); upsertRequest.writeTo(out); } out.writeBoolean(docAsUpsert); out.writeLong(version); out.writeByte(versionType.getValue()); out.writeBoolean(detectNoop); out.writeBoolean(scriptedUpsert); }	same here, would be nice to transfer the comment from the other writeto() method.
*/ private static int addFields(Random random, XContentBuilder builder, int minNumFields, int currentDepth, int currentFields) throws IOException { int maxTotalFields = 200; int maxFields = Math.max(minNumFields, 10 - (currentFields * 10)/maxTotalFields); // Map to range 0-10 int numFields = randomIntBetween(random, minNumFields, maxFields); int maxDepth = 5 - (currentFields * 5)/maxTotalFields; // Map to range 0-5 currentFields += numFields; for (int i = 0; i < numFields; i++) { if (currentDepth < maxDepth && random.nextBoolean()) { if (random.nextBoolean()) { builder.startObject(RandomStrings.randomAsciiOfLengthBetween(random, 6, 10)); currentFields = addFields(random, builder, minNumFields, currentDepth + 1, currentFields); builder.endObject(); } else { builder.startArray(RandomStrings.randomAsciiOfLengthBetween(random, 6, 10)); int numElements = randomIntBetween(random, 1, 5); boolean object = random.nextBoolean(); int dataType = -1; if (object == false) { dataType = randomDataType(random); } for (int j = 0; j < numElements; j++) { if (object) { builder.startObject(); currentFields = addFields(random, builder, minNumFields, 5, currentFields); builder.endObject(); } else { builder.value(randomFieldValue(random, dataType)); } } builder.endArray(); } } else { builder.field(RandomStrings.randomAsciiOfLengthBetween(random, 6, 10), randomFieldValue(random, randomDataType(random))); } } return currentFields; }	i'm not sure what value is good for this. from my results, 200 seemed to give decently big documents without going beyond 100kb.
*/ private static int addFields(Random random, XContentBuilder builder, int minNumFields, int currentDepth, int currentFields) throws IOException { int maxTotalFields = 200; int maxFields = Math.max(minNumFields, 10 - (currentFields * 10)/maxTotalFields); // Map to range 0-10 int numFields = randomIntBetween(random, minNumFields, maxFields); int maxDepth = 5 - (currentFields * 5)/maxTotalFields; // Map to range 0-5 currentFields += numFields; for (int i = 0; i < numFields; i++) { if (currentDepth < maxDepth && random.nextBoolean()) { if (random.nextBoolean()) { builder.startObject(RandomStrings.randomAsciiOfLengthBetween(random, 6, 10)); currentFields = addFields(random, builder, minNumFields, currentDepth + 1, currentFields); builder.endObject(); } else { builder.startArray(RandomStrings.randomAsciiOfLengthBetween(random, 6, 10)); int numElements = randomIntBetween(random, 1, 5); boolean object = random.nextBoolean(); int dataType = -1; if (object == false) { dataType = randomDataType(random); } for (int j = 0; j < numElements; j++) { if (object) { builder.startObject(); currentFields = addFields(random, builder, minNumFields, 5, currentFields); builder.endObject(); } else { builder.value(randomFieldValue(random, dataType)); } } builder.endArray(); } } else { builder.field(RandomStrings.randomAsciiOfLengthBetween(random, 6, 10), randomFieldValue(random, randomDataType(random))); } } return currentFields; }	with the goal of trying to keep things simple, i wonder if decreasing the max number of fields e.g. to 5 would be enough. what do you think?
private static String sslContextAlgorithm(List<String> supportedProtocols) { if (supportedProtocols.isEmpty()) { throw new IllegalArgumentException("no SSL/TLS protocols have been configured"); } for (Entry<String, String> entry : ORDERED_PROTOCOL_ALGORITHM_MAP.entrySet()) { if (supportedProtocols.contains(entry.getKey())) { return entry.getValue(); } } throw new IllegalArgumentException("no supported SSL/TLS protocol was found in the configured supported protocols: " + supportedProtocols); }	if (xpacksettings.fips_mode_enabled.get(settings) && diagnose_trust_exceptions_setting.exists(settings) == false ) { logger.info("diagnostic messages for ssl/tls trust failures are not enabled in fips 140 mode."); return false; } else { return diagnose_trust_exceptions_setting.get(settings); }
static void validateForFips(Settings settings) { final List<String> validationErrors = new ArrayList<>(); Settings keystoreTypeSettings = settings.filter(k -> k.endsWith("keystore.type")) .filter(k -> settings.get(k).equalsIgnoreCase("jks")); if (keystoreTypeSettings.isEmpty() == false) { validationErrors.add("JKS Keystores cannot be used in a FIPS 140 compliant JVM. Please " + "revisit [" + keystoreTypeSettings.toDelimitedString(',') + "] settings"); } Settings keystorePathSettings = settings.filter(k -> k.endsWith("keystore.path")) .filter(k -> settings.hasValue(k.replace(".path", ".type")) == false); if (keystorePathSettings.isEmpty() == false && SSLConfigurationSettings.inferKeyStoreType(null).equals("jks")) { validationErrors.add("JKS Keystores cannot be used in a FIPS 140 compliant JVM. Please " + "revisit [" + keystorePathSettings.toDelimitedString(',') + "] settings"); } final String selectedAlgorithm = XPackSettings.PASSWORD_HASHING_ALGORITHM.get(settings); if (selectedAlgorithm.toLowerCase(Locale.ROOT).startsWith("pbkdf2") == false) { validationErrors.add("Only PBKDF2 is allowed for password hashing in a FIPS 140 JVM. Please set the " + "appropriate value for [ " + XPackSettings.PASSWORD_HASHING_ALGORITHM.getKey() + " ] setting."); } if (XPackSettings.DIAGNOSE_TRUST_EXCEPTIONS_SETTING.exists(settings) && XPackSettings.DIAGNOSE_TRUST_EXCEPTIONS_SETTING.get(settings)) { validationErrors.add("SSL/TLS diagnostic messages cannot be enabled in a FIPS 140 JVM. Please set [ " + XPackSettings.DIAGNOSE_TRUST_EXCEPTIONS_SETTING.getKey() + " ] to false or remove this setting from your configuration. "); } if (validationErrors.isEmpty() == false) { final StringBuilder sb = new StringBuilder(); sb.append("Validation for FIPS 140 mode failed: \\\\n"); int index = 0; for (String error : validationErrors) { sb.append(++index).append(": ").append(error).append(";\\\\n"); } throw new IllegalArgumentException(sb.toString()); } }	i think we can just drop this.
* @param consumer which feature is requesting the model * @param modelActionListener the listener to alert when the model has been retrieved. */ private void getModel(String modelId, Consumer consumer, ActionListener<LocalModel> modelActionListener) { ModelAndConsumer cachedModel = localModelCache.get(modelId); if (cachedModel != null) { cachedModel.consumers.add(consumer); cachedModel.model.acquire(); modelActionListener.onResponse(cachedModel.model); logger.trace(() -> new ParameterizedMessage("[{}] loaded from cache", modelId)); return; } if (loadModelIfNecessary(modelId, consumer, modelActionListener)) { logger.trace(() -> new ParameterizedMessage("[{}] is loading or loaded, added new listener to queue", modelId)); } }	this might have a race condition (will think more) - model is grabbed from cache in 203. model has a ref count of 1 - model is evicted from cache due to size or something - ref count is now 0 - model is attempted to acquire again but ref count 0 and the bytes are already removed from the breaker
@Override protected void masterOperation(Task task, StartDataFrameAnalyticsAction.Request request, ClusterState state, ActionListener<AcknowledgedResponse> listener) { if (licenseState.isMachineLearningAllowed() == false) { listener.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)); return; } // Wait for analytics to be started ActionListener<PersistentTasksCustomMetaData.PersistentTask<StartDataFrameAnalyticsAction.TaskParams>> waitForAnalyticsToStart = new ActionListener<PersistentTasksCustomMetaData.PersistentTask<StartDataFrameAnalyticsAction.TaskParams>>() { @Override public void onResponse(PersistentTasksCustomMetaData.PersistentTask<StartDataFrameAnalyticsAction.TaskParams> task) { waitForAnalyticsStarted(task, request.getTimeout(), listener); } @Override public void onFailure(Exception e) { if (e instanceof ResourceAlreadyExistsException) { e = new ElasticsearchStatusException("Cannot open data frame analytics [" + request.getId() + "] because it has already been opened", RestStatus.CONFLICT, e); } listener.onFailure(e); } }; AtomicReference<DataFrameAnalyticsConfig> configHolder = new AtomicReference<>(); // Start persistent task ActionListener<Void> memoryRequirementRefreshListener = ActionListener.wrap( aVoid -> { StartDataFrameAnalyticsAction.TaskParams taskParams = new StartDataFrameAnalyticsAction.TaskParams( request.getId(), configHolder.get().getVersion()); persistentTasksService.sendStartRequest(MlTasks.dataFrameAnalyticsTaskId(request.getId()), MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME, taskParams, waitForAnalyticsToStart); }, listener::onFailure ); // Tell the job tracker to refresh the memory requirement for this job and all other jobs that have persistent tasks ActionListener<EstimateMemoryUsageAction.Response> estimateMemoryUsageListener = ActionListener.wrap( estimateMemoryUsageResponse -> { // Validate that model memory limit is sufficient to run the analysis if (configHolder.get().getModelMemoryLimit() .compareTo(estimateMemoryUsageResponse.getExpectedMemoryUsageWithOnePartition()) < 0) { ElasticsearchStatusException e = ExceptionsHelper.badRequestException( "Cannot start because the configured model memory limit [{}] is lower than the expected memory usage [{}].", configHolder.get().getModelMemoryLimit(), estimateMemoryUsageResponse.getExpectedMemoryUsageWithOnePartition()); listener.onFailure(e); return; } // Refresh memory requirement for jobs memoryTracker.addDataFrameAnalyticsJobMemoryAndRefreshAllOthers( request.getId(), configHolder.get().getModelMemoryLimit().getBytes(), memoryRequirementRefreshListener); }, listener::onFailure ); // Perform memory usage estimation for this config ActionListener<DataFrameAnalyticsConfig> configListener = ActionListener.wrap( config -> { configHolder.set(config); EstimateMemoryUsageAction.Request estimateMemoryUsageRequest = new EstimateMemoryUsageAction.Request(config); ClientHelper.executeAsyncWithOrigin( client, ClientHelper.ML_ORIGIN, EstimateMemoryUsageAction.INSTANCE, estimateMemoryUsageRequest, estimateMemoryUsageListener); }, listener::onFailure ); // Get config getConfigAndValidate(request.getId(), configListener); }	nit: remove dot at the end of the message
@Override public Query termQuery(Object value, @Nullable QueryParseContext context) { return new TermQuery(new Term(names.indexName(), indexedValueForSearch(value))); }	could we move this up to numberfieldmapper (replacing the range query there now) to get the benefit with the other types @rmuir mentions?
List<String> getIndexSettingsValidationErrors(Settings settings) { String customPath = IndexMetaData.INDEX_DATA_PATH_SETTING.get(settings); List<String> validationErrors = new ArrayList<>(); if (Strings.isEmpty(customPath) == false && env.sharedDataFile() == null) { validationErrors.add("path.shared_data must be set in order to use custom data paths"); } else if (Strings.isEmpty(customPath) == false) { Path resolvedPath = PathUtils.get(new Path[]{env.sharedDataFile()}, customPath); if (resolvedPath == null) { validationErrors.add("custom path [" + customPath + "] is not a sub-path of path.shared_data [" + env.sharedDataFile() + "]"); } } //norelease - this can be removed? Integer number_of_primaries = settings.getAsInt(IndexMetaData.SETTING_NUMBER_OF_SHARDS, null); Integer number_of_replicas = settings.getAsInt(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, null); if (number_of_primaries != null && number_of_primaries <= 0) { validationErrors.add("index must have 1 or more primary shards"); } if (number_of_replicas != null && number_of_replicas < 0) { validationErrors.add("index must have 0 or more replica shards"); } return validationErrors; }	not yet! those are still just strings but i suspect this note is to make sure that they grow their validation at the setting level.
@Override public ClusterState execute(ClusterState currentState) { String[] actualIndices = indexNameExpressionResolver.concreteIndices(currentState, IndicesOptions.strictExpand(), request.indices()); RoutingTable.Builder routingTableBuilder = RoutingTable.builder(currentState.routingTable()); MetaData.Builder metaDataBuilder = MetaData.builder(currentState.metaData()); // allow to change any settings to a close index, and only allow dynamic settings to be changed // on an open index Set<String> openIndices = new HashSet<>(); Set<String> closeIndices = new HashSet<>(); for (String index : actualIndices) { if (currentState.metaData().index(index).getState() == IndexMetaData.State.OPEN) { openIndices.add(index); } else { closeIndices.add(index); } } if (closeIndices.size() > 0 && closedSettings.get(IndexMetaData.SETTING_NUMBER_OF_REPLICAS) != null) { throw new IllegalArgumentException(String.format(Locale.ROOT, "Can't update [%s] on closed indices [%s] - can leave index in an unopenable state", IndexMetaData.SETTING_NUMBER_OF_REPLICAS, closeIndices )); } if (!skippedSettigns.getAsMap().isEmpty() && !openIndices.isEmpty()) { throw new IllegalArgumentException(String.format(Locale.ROOT, "Can't update non dynamic settings[%s] for open indices [%s]", skippedSettigns.getAsMap().keySet(), openIndices )); } int updatedNumberOfReplicas = openSettings.getAsInt(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, -1); if (updatedNumberOfReplicas != -1) { routingTableBuilder.updateNumberOfReplicas(updatedNumberOfReplicas, actualIndices); metaDataBuilder.updateNumberOfReplicas(updatedNumberOfReplicas, actualIndices); logger.info("updating number_of_replicas to [{}] for indices {}", updatedNumberOfReplicas, actualIndices); } ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks()); final boolean updatedReadOnly = IndexMetaData.INDEX_READ_ONLY_SETTING.get(openSettings); for (String index : actualIndices) { if (updatedReadOnly) { blocks.addIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_BLOCK); } else { blocks.removeIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_BLOCK); } } final boolean updateMetaDataBlock = IndexMetaData.INDEX_BLOCKS_METADATA_SETTING.get(openSettings); for (String index : actualIndices) { if (updateMetaDataBlock) { blocks.addIndexBlock(index, IndexMetaData.INDEX_METADATA_BLOCK); } else { blocks.removeIndexBlock(index, IndexMetaData.INDEX_METADATA_BLOCK); } } final boolean updateWriteBlock = IndexMetaData.INDEX_BLOCKS_WRITE_SETTING.get(openSettings); for (String index : actualIndices) { if (updateWriteBlock) { blocks.addIndexBlock(index, IndexMetaData.INDEX_WRITE_BLOCK); } else { blocks.removeIndexBlock(index, IndexMetaData.INDEX_WRITE_BLOCK); } } final boolean updateReadBlock = IndexMetaData.INDEX_BLOCKS_READ_SETTING.get(openSettings); for (String index : actualIndices) { if (updateReadBlock) { blocks.addIndexBlock(index, IndexMetaData.INDEX_READ_BLOCK); } else { blocks.removeIndexBlock(index, IndexMetaData.INDEX_READ_BLOCK); } } if (!openIndices.isEmpty()) { for (String index : openIndices) { IndexMetaData indexMetaData = metaDataBuilder.get(index); if (indexMetaData == null) { throw new IndexNotFoundException(index); } Settings.Builder updates = Settings.builder(); Settings.Builder indexSettings = Settings.builder().put(indexMetaData.getSettings()); if (indexScopeSettings.updateSettings(openSettings, indexSettings, updates, index, false)) { metaDataBuilder.put(IndexMetaData.builder(indexMetaData).settings(indexSettings)); } } } if (!closeIndices.isEmpty()) { for (String index : closeIndices) { IndexMetaData indexMetaData = metaDataBuilder.get(index); if (indexMetaData == null) { throw new IndexNotFoundException(index); } Settings.Builder updates = Settings.builder(); Settings.Builder indexSettings = Settings.builder().put(indexMetaData.getSettings()); if (indexScopeSettings.updateSettings(closedSettings, indexSettings, updates, index, true)) { metaDataBuilder.put(IndexMetaData.builder(indexMetaData).settings(indexSettings)); } } } ClusterState updatedState = ClusterState.builder(currentState).metaData(metaDataBuilder).routingTable(routingTableBuilder.build()).blocks(blocks).build(); // now, reroute in case things change that require it (like number of replicas) RoutingAllocation.Result routingResult = allocationService.reroute(updatedState, "settings update"); updatedState = ClusterState.builder(updatedState).routingResult(routingResult).build(); for (String index : openIndices) { indexScopeSettings.dryRun(updatedState.metaData().index(index).getSettings()); } for (String index : closeIndices) { indexScopeSettings.dryRun(updatedState.metaData().index(index).getSettings()); } return updatedState; }	if looks like if you didn't specify the value it used to leave it unchanged on all the indexes but now it'll remove all the blocks i think.
* @param target the target settings builder that the updates are applied to. All keys that have explicit null value in toApply will be removed from this builder * @param updates a settings builder that holds all updates applied to target * @param type a free text string to allow better exceptions messages * @param all if <code>true</code> all settings are updated otherwise only dynamic settings are updated. if set to <code>false</code> and a non-dynamic setting is updated an exception is thrown * @return <code>true</code> if the target has changed otherwise <code>false</code> */ public boolean updateSettings(Settings toApply, Settings.Builder target, Settings.Builder updates, String type, boolean all) { boolean changed = false; final Set<String> toRemove = new HashSet<>(); Settings.Builder settingsBuilder = Settings.settingsBuilder(); for (Map.Entry<String, String> entry : toApply.getAsMap().entrySet()) { if (entry.getValue() == null) { toRemove.add(entry.getKey()); } else if ((all && get(entry.getKey()) != null) || hasDynamicSetting(entry.getKey())) { validate(entry.getKey(), entry.getValue()); settingsBuilder.put(entry.getKey(), entry.getValue()); updates.put(entry.getKey(), entry.getValue()); changed = true; } else { throw new IllegalArgumentException(type + " setting [" + entry.getKey() + "], not dynamically updateable"); } } changed |= applyDeletes(toRemove, target); target.put(settingsBuilder.build()); return changed; }	maybe have dynamiconly instead? it feels more precise.
protected QueryBuilder doRewrite(QueryRewriteContext queryShardContext) { if (queryShardContext.convertToShardContext() != null) { throw new IllegalStateException("Fail on rewrite phase"); } return this; }	maybe rename queryshardcontext to queryrewritecontext then to make it less confusing
@SuppressWarnings("unchecked") static Script parseScript(String name, Object scriptObject) { if (scriptObject instanceof Map) { Map<String, ?> scriptMap = (Map<String, ?>) scriptObject; Object sourceObject = scriptMap.remove("source"); if (sourceObject == null) { throw new IllegalArgumentException("script source must be specified for script field [" + name + "]"); } Object langObject = scriptMap.remove("lang"); if (langObject != null && langObject.toString().equals(PainlessScriptEngine.NAME) == false) { throw new IllegalArgumentException("script lang [" + langObject.toString() + "] not supported for script field [" + name + "]"); } Map<String, Object> params; Object paramsObject = scriptMap.remove("params"); if (paramsObject != null) { if (paramsObject instanceof Map == false) { throw new IllegalArgumentException("unable to parse params for script field [" + name + "]"); } params = (Map<String, Object>) paramsObject; } else { params = Collections.emptyMap(); } Map<String, String> options; Object optionsObject = scriptMap.remove("options"); if (optionsObject != null) { if (optionsObject instanceof Map == false) { throw new IllegalArgumentException("unable to parse options for script field [" + name + "]"); } options = (Map<String, String>) optionsObject; } else { options = Collections.emptyMap(); } if (scriptMap.size() > 0) { throw new IllegalArgumentException("unsupported parameters specified for script field [" + name + "]: " + scriptMap.keySet()); } return new Script(ScriptType.INLINE, PainlessScriptEngine.NAME, sourceObject.toString(), options, params); } else if (scriptObject instanceof String) { return new Script((String) scriptObject); } else { throw new IllegalArgumentException("unable to parse script for script field [" + name + "]"); } } } public static class TypeParser implements Mapper.TypeParser { // TODO this is quite ugly and it's static which makes it even worse private static final SetOnce<ScriptService> SCRIPT_SERVICE = new SetOnce<>(); public void setScriptService(ScriptService scriptService) { SCRIPT_SERVICE.set(scriptService); } @Override public ScriptFieldMapper.Builder parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { ScriptFieldMapper.Builder builder = new ScriptFieldMapper.Builder(name, SCRIPT_SERVICE.get()); builder.parse(name, parserContext, node); return builder; }	is there no way of getting it via the parsercontext?
public static Boolean eq(Object l, Object r) { Integer i = compare(l, r); return i == null ? null : i.intValue() == 0; }	return i == null? null : i.intvalue() != 0; just add a whitespace: return i == null ? null : i.intvalue() != 0;. if the pr is held back by this picky comment, you can ignore it and merge.
public void testToString() { ScriptSortBuilder ssb = prepareSSB(); String ssbPrint = ssb.toString(); assertThat(ssbPrint, containsString("testtype")); }	same here, simple assertquals with string comparison would be easier to read i think.
@Override protected Query doToQuery(QueryShardContext context) throws IOException { MappedFieldType fieldType = context.fieldMapper(field); if (fieldType == null) { // Be lenient with unmapped fields so that cross-index search will work nicely return new MatchNoDocsQuery(); } if (fieldType.tokenized() == false || fieldType.indexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) { throw new IllegalArgumentException("Cannot create IntervalQuery over field [" + field + "] with no indexed positions"); } Set<String> maskedFields = new HashSet<>(); sourceProvider.extractFields(maskedFields); for (String maskedField : maskedFields) { MappedFieldType ft = context.fieldMapper(maskedField); if (ft == null) { return new MatchNoDocsQuery(); } if (ft.tokenized() == false || ft.indexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) { throw new IllegalArgumentException("Cannot create IntervalQuery over field [" + maskedField + "] with no indexed positions"); } } return new IntervalQuery(field, sourceProvider.getSource(context, fieldType)); }	is this really needed ? couldn't we check these options in mappedfieldtype#intervals since use_field is only applicable on the match rule ?
public void testSeqNoBasedRecoveryIsUsedAfterPrimaryFailOver() throws Exception { List<String> dataNodes = internalCluster().startDataOnlyNodes(3); String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); createIndex(indexName, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false) .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), "1s") .put("index.routing.allocation.include._name", String.join(",", dataNodes)) .build() ); int numDocs = randomIntBetween(300, 1000); indexDocs(indexName, 0, numDocs); // Flush to ensure that index_commit_seq_nos(replica) == index_commit_seq_nos(primary), // since the primary flushes the index before taking the snapshot. flush(indexName); String repoType = randomFrom(TestRepositoryPlugin.FAULTY_TYPE, TestRepositoryPlugin.INSTRUMENTED_TYPE, "fs"); String repoName = "repo"; createRepo(repoName, repoType); createSnapshot(repoName, "snap", Collections.singletonList(indexName)); ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); String primaryNodeId = clusterState.routingTable().index(indexName).shard(0).primaryShard().currentNodeId(); String primaryNodeName = clusterState.nodes().resolveNode(primaryNodeId).getName(); assertThat(internalCluster().stopNode(primaryNodeName), is(equalTo(true))); ensureGreen(indexName); ClusterState clusterStateAfterPrimaryFailOver = client().admin().cluster().prepareState().get().getState(); IndexShardRoutingTable shardRoutingTableAfterFailOver = clusterStateAfterPrimaryFailOver.routingTable().index(indexName).shard(0); String primaryNodeIdAfterFailOver = shardRoutingTableAfterFailOver.primaryShard().currentNodeId(); String primaryNodeNameAfterFailOver = clusterStateAfterPrimaryFailOver.nodes().resolveNode(primaryNodeIdAfterFailOver).getName(); String replicaNodeIdAfterFailOver = shardRoutingTableAfterFailOver.replicaShards().get(0).currentNodeId(); String replicaNodeNameAfterFailOver = clusterStateAfterPrimaryFailOver.nodes().resolveNode(replicaNodeIdAfterFailOver).getName(); RecoveryState recoveryState = getLatestPeerRecoveryStateForShard(indexName, 0); assertPeerRecoveryWasSuccessful(recoveryState, primaryNodeNameAfterFailOver, replicaNodeNameAfterFailOver); assertDocumentsAreEqual(indexName, numDocs); if (repoType.equals(TestRepositoryPlugin.FAULTY_TYPE) == false) { for (RecoveryState.FileDetail fileDetail : recoveryState.getIndex().fileDetails()) { assertThat(fileDetail.recoveredFromSnapshot(), is(equalTo(fileDetail.length()))); } ShardId shardId = shardRoutingTableAfterFailOver.shardId(); assertRecoveryWasBasedOnSeqNo(shardId, primaryNodeNameAfterFailOver); } }	this seems a little backward, as if we validate the test more than the recovery. i wonder if we could instead capture the files from primary right after snapshot and then compare to the replica, expecting some identical files?
public void testSeqNoBasedRecoveryIsUsedAfterPrimaryFailOver() throws Exception { List<String> dataNodes = internalCluster().startDataOnlyNodes(3); String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); createIndex(indexName, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false) .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), "1s") .put("index.routing.allocation.include._name", String.join(",", dataNodes)) .build() ); int numDocs = randomIntBetween(300, 1000); indexDocs(indexName, 0, numDocs); // Flush to ensure that index_commit_seq_nos(replica) == index_commit_seq_nos(primary), // since the primary flushes the index before taking the snapshot. flush(indexName); String repoType = randomFrom(TestRepositoryPlugin.FAULTY_TYPE, TestRepositoryPlugin.INSTRUMENTED_TYPE, "fs"); String repoName = "repo"; createRepo(repoName, repoType); createSnapshot(repoName, "snap", Collections.singletonList(indexName)); ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); String primaryNodeId = clusterState.routingTable().index(indexName).shard(0).primaryShard().currentNodeId(); String primaryNodeName = clusterState.nodes().resolveNode(primaryNodeId).getName(); assertThat(internalCluster().stopNode(primaryNodeName), is(equalTo(true))); ensureGreen(indexName); ClusterState clusterStateAfterPrimaryFailOver = client().admin().cluster().prepareState().get().getState(); IndexShardRoutingTable shardRoutingTableAfterFailOver = clusterStateAfterPrimaryFailOver.routingTable().index(indexName).shard(0); String primaryNodeIdAfterFailOver = shardRoutingTableAfterFailOver.primaryShard().currentNodeId(); String primaryNodeNameAfterFailOver = clusterStateAfterPrimaryFailOver.nodes().resolveNode(primaryNodeIdAfterFailOver).getName(); String replicaNodeIdAfterFailOver = shardRoutingTableAfterFailOver.replicaShards().get(0).currentNodeId(); String replicaNodeNameAfterFailOver = clusterStateAfterPrimaryFailOver.nodes().resolveNode(replicaNodeIdAfterFailOver).getName(); RecoveryState recoveryState = getLatestPeerRecoveryStateForShard(indexName, 0); assertPeerRecoveryWasSuccessful(recoveryState, primaryNodeNameAfterFailOver, replicaNodeNameAfterFailOver); assertDocumentsAreEqual(indexName, numDocs); if (repoType.equals(TestRepositoryPlugin.FAULTY_TYPE) == false) { for (RecoveryState.FileDetail fileDetail : recoveryState.getIndex().fileDetails()) { assertThat(fileDetail.recoveredFromSnapshot(), is(equalTo(fileDetail.length()))); } ShardId shardId = shardRoutingTableAfterFailOver.shardId(); assertRecoveryWasBasedOnSeqNo(shardId, primaryNodeNameAfterFailOver); } }	i wonder if we can collapse this with testrecoveryiscancelledafterdeletingtheindex by randomly choosing either the primary or the replica to stop in this variant of the test? if we stop the replica, it will do a file-based snapshot download and if we stop the primary it will do a seqno based snapshot download.
private boolean assertSequenceNumbersInCommit() throws IOException { final SegmentInfos segmentCommitInfos = SegmentInfos.readLatestCommit(store.directory()); final Map<String, String> userData = segmentCommitInfos.getUserData(); assert userData.containsKey(SequenceNumbers.LOCAL_CHECKPOINT_KEY) : "commit point doesn't contains a local checkpoint"; assert userData.containsKey(SequenceNumbers.MAX_SEQ_NO) : "commit point doesn't contains a maximum sequence number"; assert userData.containsKey(Engine.HISTORY_UUID_KEY) : "commit point doesn't contains a history uuid"; assert userData.get(Engine.HISTORY_UUID_KEY).equals(getHistoryUUID()) : "commit point history uuid [" + userData.get(Engine.HISTORY_UUID_KEY) + "] is different than engine [" + getHistoryUUID() + "]"; assert userData.containsKey(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID) : "opening index which was created post 5.5.0 but " + Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID + " is not found in commit"; final org.apache.lucene.util.Version commitLuceneVersion = segmentCommitInfos.getCommitLuceneVersion(); assert commitLuceneVersion.onOrAfter(RecoverySettings.SEQ_NO_SNAPSHOT_RECOVERIES_SUPPORTED_VERSION.luceneVersion) == false || userData.containsKey(Engine.ES_VERSION) && Version.fromString(userData.get(Engine.ES_VERSION)).onOrBefore(Version.CURRENT) : "commit point has an invalid ES_VERSION value. commit point lucene version [" + commitLuceneVersion + "]," + " ES_VERSION [" + userData.get(Engine.ES_VERSION) + "]"; return true; }	let us add a comment here that this relies on previous minor having another lucene version (which it does now, but that could be reverted or updated in a previous minor patch).
public void testSeqNoBasedRecoveryCleanSnapshotFilesAndRecoversFromFallbackPlanAfterAFailure() throws Exception { try (Store store = newStore(createTempDir("source"), false)) { IndexShard shard = mock(IndexShard.class); when(shard.store()).thenReturn(store); when(shard.state()).thenReturn(IndexShardState.STARTED); ShardRecoveryPlan shardRecoveryPlan = createShardRecoveryPlanWithFallback(store, randomIntBetween(10, 20), randomIntBetween(10, 20)); final ShardRecoveryPlan fallbackPlan = shardRecoveryPlan.getFallbackPlan(); List<StoreFileMetadata> sourceFilesToRecover = fallbackPlan.getSourceFilesToRecover(); List<StoreFileMetadata> snapshotFilesToRecover = shardRecoveryPlan.getSnapshotFilesToRecover() .getSnapshotFiles() .stream() .map(BlobStoreIndexShardSnapshot.FileInfo::metadata) .collect(Collectors.toList()); int maxConcurrentSnapshotFileDownloads = randomIntBetween(2, 4); CountDownLatch downloadSnapshotFileReceived = new CountDownLatch(maxConcurrentSnapshotFileDownloads); List<RecoverSnapshotFileResponse> inFlightRecoverSnapshotFileRequests = new CopyOnWriteArrayList<>(); AtomicBoolean retryingUsingFallbackPlan = new AtomicBoolean(); AtomicBoolean snapshotFileRecoveryFailed = new AtomicBoolean(); Set<StoreFileMetadata> filesRecoveredFromSource = ConcurrentCollections.newConcurrentSet(); TestRecoveryTargetHandler recoveryTarget = new Phase1RecoveryTargetHandler() { @Override public void receiveFileInfo(List<String> phase1FileNames, List<Long> phase1FileSizes, List<String> phase1ExistingFileNames, List<Long> phase1ExistingFileSizes, int totalTranslogOps, ActionListener<Void> listener) { assert retryingUsingFallbackPlan.get() == false; final List<StoreFileMetadata> filesToRecover; if (snapshotFileRecoveryFailed.get()) { filesToRecover = fallbackPlan.getSourceFilesToRecover(); retryingUsingFallbackPlan.set(true); } else { filesToRecover = shardRecoveryPlan.getSnapshotFilesToRecover() .getSnapshotFiles() .stream() .map(BlobStoreIndexShardSnapshot.FileInfo::metadata) .collect(Collectors.toList()); } for (int i = 0; i < phase1FileNames.size(); i++) { String fileName = phase1FileNames.get(i); long fileSize = phase1FileSizes.get(i); assertThat(containsFile(filesToRecover, fileName, fileSize), is(equalTo(true))); } listener.onResponse(null); } @Override public void restoreFileFromSnapshot(String repository, IndexId indexId, BlobStoreIndexShardSnapshot.FileInfo snapshotFile, ActionListener<Void> listener) { assert retryingUsingFallbackPlan.get() == false; inFlightRecoverSnapshotFileRequests.add(new RecoverSnapshotFileResponse(snapshotFile, listener)); downloadSnapshotFileReceived.countDown(); } @Override public void writeFileChunk(StoreFileMetadata fileMetadata, long position, ReleasableBytesReference content, boolean lastChunk, int totalTranslogOps, ActionListener<Void> listener) { assert retryingUsingFallbackPlan.get(); assertThat(containsFile(sourceFilesToRecover, fileMetadata), is(equalTo(true))); assertThat(containsFile(snapshotFilesToRecover, fileMetadata), is(equalTo(false))); filesRecoveredFromSource.add(fileMetadata); listener.onResponse(null); } @Override public void cleanFiles(int totalTranslogOps, long globalCheckpoint, Store.MetadataSnapshot sourceMetadata, ActionListener<Void> listener) { assert retryingUsingFallbackPlan.get(); assertThat(sourceMetadata, is(equalTo(fallbackPlan.getSourceMetadataSnapshot()))); listener.onResponse(null); } }; RecoverySourceHandler handler = new RecoverySourceHandler( shard, new AsyncRecoveryTarget(recoveryTarget, threadPool.generic()), threadPool, getStartRecoveryRequest(), between(1, 16), between(1, 4), between(1, 4), maxConcurrentSnapshotFileDownloads, true, null) { @Override void createRetentionLease(long startingSeqNo, ActionListener<RetentionLease> listener) { listener.onResponse(new RetentionLease("id", startingSeqNo, 0, "test")); } }; PlainActionFuture<RecoverySourceHandler.SendFileResult> future = PlainActionFuture.newFuture(); handler.recoverFilesFromSourceAndSnapshot(shardRecoveryPlan, store, mock(StopWatch.class), future); downloadSnapshotFileReceived.await(); assertThat(inFlightRecoverSnapshotFileRequests.size(), is(equalTo(maxConcurrentSnapshotFileDownloads))); assertThat(future.isDone(), is(equalTo(false))); List<RecoverSnapshotFileResponse> firstRoundOfResponses = new ArrayList<>(inFlightRecoverSnapshotFileRequests); inFlightRecoverSnapshotFileRequests.clear(); int inFlightResponsesBeforeFailure = 0; // fail at least once RecoverSnapshotFileResponse failingDownloadRequest = randomFrom(firstRoundOfResponses); for (final RecoverSnapshotFileResponse inFlightRequest : firstRoundOfResponses) { if (randomBoolean() || inFlightRequest == failingDownloadRequest) { inFlightRequest.listener.onFailure(new IOException("i/o failure")); snapshotFileRecoveryFailed.set(true); } else { inFlightRequest.listener.onResponse(null); if (snapshotFileRecoveryFailed.get() == false) { inFlightResponsesBeforeFailure++; } } } // wait until all the expected outgoing request have been received int expectedInFlightResponsesBeforeFailure = inFlightResponsesBeforeFailure; assertBusy(() -> assertThat(inFlightRecoverSnapshotFileRequests.size(), equalTo(expectedInFlightResponsesBeforeFailure))); if (inFlightRecoverSnapshotFileRequests.isEmpty() == false) { assertThat(retryingUsingFallbackPlan.get(), is(equalTo(false))); } for (RecoverSnapshotFileResponse inFlightRecoverSnapshotRequest : inFlightRecoverSnapshotFileRequests) { if (randomBoolean()) { inFlightRecoverSnapshotRequest.listener.onFailure(new RuntimeException("boom")); } else { inFlightRecoverSnapshotRequest.listener.onResponse(null); } } future.get(); assertThat(filesRecoveredFromSource.size(), is(equalTo(fallbackPlan.getSourceFilesToRecover().size()))); for (StoreFileMetadata fileRecoveredFromSource : filesRecoveredFromSource) { assertThat(containsFile(fallbackPlan.getSourceFilesToRecover(), fileRecoveredFromSource), is(equalTo(true))); } } }	it would be nice to verify that we get here exactly, once without snapshotfilerecoveryfailed and once with (the latter is verified already).
public void testSeqNoBasedRecoveryCleanSnapshotFilesAndRecoversFromFallbackPlanAfterAFailure() throws Exception { try (Store store = newStore(createTempDir("source"), false)) { IndexShard shard = mock(IndexShard.class); when(shard.store()).thenReturn(store); when(shard.state()).thenReturn(IndexShardState.STARTED); ShardRecoveryPlan shardRecoveryPlan = createShardRecoveryPlanWithFallback(store, randomIntBetween(10, 20), randomIntBetween(10, 20)); final ShardRecoveryPlan fallbackPlan = shardRecoveryPlan.getFallbackPlan(); List<StoreFileMetadata> sourceFilesToRecover = fallbackPlan.getSourceFilesToRecover(); List<StoreFileMetadata> snapshotFilesToRecover = shardRecoveryPlan.getSnapshotFilesToRecover() .getSnapshotFiles() .stream() .map(BlobStoreIndexShardSnapshot.FileInfo::metadata) .collect(Collectors.toList()); int maxConcurrentSnapshotFileDownloads = randomIntBetween(2, 4); CountDownLatch downloadSnapshotFileReceived = new CountDownLatch(maxConcurrentSnapshotFileDownloads); List<RecoverSnapshotFileResponse> inFlightRecoverSnapshotFileRequests = new CopyOnWriteArrayList<>(); AtomicBoolean retryingUsingFallbackPlan = new AtomicBoolean(); AtomicBoolean snapshotFileRecoveryFailed = new AtomicBoolean(); Set<StoreFileMetadata> filesRecoveredFromSource = ConcurrentCollections.newConcurrentSet(); TestRecoveryTargetHandler recoveryTarget = new Phase1RecoveryTargetHandler() { @Override public void receiveFileInfo(List<String> phase1FileNames, List<Long> phase1FileSizes, List<String> phase1ExistingFileNames, List<Long> phase1ExistingFileSizes, int totalTranslogOps, ActionListener<Void> listener) { assert retryingUsingFallbackPlan.get() == false; final List<StoreFileMetadata> filesToRecover; if (snapshotFileRecoveryFailed.get()) { filesToRecover = fallbackPlan.getSourceFilesToRecover(); retryingUsingFallbackPlan.set(true); } else { filesToRecover = shardRecoveryPlan.getSnapshotFilesToRecover() .getSnapshotFiles() .stream() .map(BlobStoreIndexShardSnapshot.FileInfo::metadata) .collect(Collectors.toList()); } for (int i = 0; i < phase1FileNames.size(); i++) { String fileName = phase1FileNames.get(i); long fileSize = phase1FileSizes.get(i); assertThat(containsFile(filesToRecover, fileName, fileSize), is(equalTo(true))); } listener.onResponse(null); } @Override public void restoreFileFromSnapshot(String repository, IndexId indexId, BlobStoreIndexShardSnapshot.FileInfo snapshotFile, ActionListener<Void> listener) { assert retryingUsingFallbackPlan.get() == false; inFlightRecoverSnapshotFileRequests.add(new RecoverSnapshotFileResponse(snapshotFile, listener)); downloadSnapshotFileReceived.countDown(); } @Override public void writeFileChunk(StoreFileMetadata fileMetadata, long position, ReleasableBytesReference content, boolean lastChunk, int totalTranslogOps, ActionListener<Void> listener) { assert retryingUsingFallbackPlan.get(); assertThat(containsFile(sourceFilesToRecover, fileMetadata), is(equalTo(true))); assertThat(containsFile(snapshotFilesToRecover, fileMetadata), is(equalTo(false))); filesRecoveredFromSource.add(fileMetadata); listener.onResponse(null); } @Override public void cleanFiles(int totalTranslogOps, long globalCheckpoint, Store.MetadataSnapshot sourceMetadata, ActionListener<Void> listener) { assert retryingUsingFallbackPlan.get(); assertThat(sourceMetadata, is(equalTo(fallbackPlan.getSourceMetadataSnapshot()))); listener.onResponse(null); } }; RecoverySourceHandler handler = new RecoverySourceHandler( shard, new AsyncRecoveryTarget(recoveryTarget, threadPool.generic()), threadPool, getStartRecoveryRequest(), between(1, 16), between(1, 4), between(1, 4), maxConcurrentSnapshotFileDownloads, true, null) { @Override void createRetentionLease(long startingSeqNo, ActionListener<RetentionLease> listener) { listener.onResponse(new RetentionLease("id", startingSeqNo, 0, "test")); } }; PlainActionFuture<RecoverySourceHandler.SendFileResult> future = PlainActionFuture.newFuture(); handler.recoverFilesFromSourceAndSnapshot(shardRecoveryPlan, store, mock(StopWatch.class), future); downloadSnapshotFileReceived.await(); assertThat(inFlightRecoverSnapshotFileRequests.size(), is(equalTo(maxConcurrentSnapshotFileDownloads))); assertThat(future.isDone(), is(equalTo(false))); List<RecoverSnapshotFileResponse> firstRoundOfResponses = new ArrayList<>(inFlightRecoverSnapshotFileRequests); inFlightRecoverSnapshotFileRequests.clear(); int inFlightResponsesBeforeFailure = 0; // fail at least once RecoverSnapshotFileResponse failingDownloadRequest = randomFrom(firstRoundOfResponses); for (final RecoverSnapshotFileResponse inFlightRequest : firstRoundOfResponses) { if (randomBoolean() || inFlightRequest == failingDownloadRequest) { inFlightRequest.listener.onFailure(new IOException("i/o failure")); snapshotFileRecoveryFailed.set(true); } else { inFlightRequest.listener.onResponse(null); if (snapshotFileRecoveryFailed.get() == false) { inFlightResponsesBeforeFailure++; } } } // wait until all the expected outgoing request have been received int expectedInFlightResponsesBeforeFailure = inFlightResponsesBeforeFailure; assertBusy(() -> assertThat(inFlightRecoverSnapshotFileRequests.size(), equalTo(expectedInFlightResponsesBeforeFailure))); if (inFlightRecoverSnapshotFileRequests.isEmpty() == false) { assertThat(retryingUsingFallbackPlan.get(), is(equalTo(false))); } for (RecoverSnapshotFileResponse inFlightRecoverSnapshotRequest : inFlightRecoverSnapshotFileRequests) { if (randomBoolean()) { inFlightRecoverSnapshotRequest.listener.onFailure(new RuntimeException("boom")); } else { inFlightRecoverSnapshotRequest.listener.onResponse(null); } } future.get(); assertThat(filesRecoveredFromSource.size(), is(equalTo(fallbackPlan.getSourceFilesToRecover().size()))); for (StoreFileMetadata fileRecoveredFromSource : filesRecoveredFromSource) { assertThat(containsFile(fallbackPlan.getSourceFilesToRecover(), fileRecoveredFromSource), is(equalTo(true))); } } }	can we assert that retryingusingfallbackplan.get(), is(true)?
public void testLogicallyEquivalentSnapshotIsUsedEvenIfFilesAreDifferent() throws Exception { createStore(store -> { boolean shareFilesWithSource = randomBoolean(); Store.MetadataSnapshot targetSourceMetadata = generateRandomTargetState(store, shareFilesWithSource); writeRandomDocs(store, randomIntBetween(10, 100)); Store.MetadataSnapshot sourceMetadata = store.getMetadata(null); boolean compatibleVersion = randomBoolean(); final Version snapshotVersion; final org.apache.lucene.util.Version luceneVersion; if (compatibleVersion) { snapshotVersion = randomBoolean() ? null : randomCompatibleVersion(random(), Version.CURRENT); luceneVersion = randomCompatibleVersion(random(), Version.CURRENT).luceneVersion; } else { snapshotVersion = randomBoolean() ? null : Version.fromId(Integer.MAX_VALUE); luceneVersion = org.apache.lucene.util.Version.parse("255.255.255"); } // The snapshot shardStateIdentifier is the same as the source, but the files are different. // This can happen after a primary fail-over. ShardSnapshot latestSnapshot = createShardSnapshotThatDoNotShareSegmentFiles("repo", snapshotVersion, luceneVersion); String shardStateIdentifier = latestSnapshot.getShardStateIdentifier(); long startingSeqNo = randomNonNegativeLong(); int translogOps = randomIntBetween(1, 100); ShardRecoveryPlan shardRecoveryPlan = computeShardRecoveryPlan( shardStateIdentifier, sourceMetadata, targetSourceMetadata, startingSeqNo, translogOps, new ShardSnapshotsService(null, null, null, null) { @Override public void fetchLatestSnapshotsForShard(ShardId shardId, ActionListener<Optional<ShardSnapshot>> listener) { listener.onResponse(Optional.of(latestSnapshot)); } }, true ); if (shareFilesWithSource || compatibleVersion == false) { assertPlanIsValid(shardRecoveryPlan, sourceMetadata); assertAllSourceFilesAreAvailableInSource(shardRecoveryPlan, sourceMetadata); assertAllIdenticalFilesAreAvailableInTarget(shardRecoveryPlan, targetSourceMetadata); assertThat(shardRecoveryPlan.getSnapshotFilesToRecover(), is(equalTo(ShardRecoveryPlan.SnapshotFilesToRecover.EMPTY))); } else { assertPlanIsValid(shardRecoveryPlan, latestSnapshot.getMetadataSnapshot()); assertUsesExpectedSnapshot(shardRecoveryPlan, latestSnapshot); assertThat(shardRecoveryPlan.getSourceFilesToRecover(), is(empty())); assertAllIdenticalFilesAreAvailableInTarget(shardRecoveryPlan, targetSourceMetadata); assertThat(shardRecoveryPlan.getStartingSeqNo(), equalTo(startingSeqNo)); assertThat(shardRecoveryPlan.getTranslogOps(), equalTo(translogOps)); assertThat(shardRecoveryPlan.canRecoverSnapshotFilesFromSourceNode(), is(equalTo(false))); ShardRecoveryPlan fallbackPlan = shardRecoveryPlan.getFallbackPlan(); assertThat(fallbackPlan, is(notNullValue())); assertPlanIsValid(fallbackPlan, sourceMetadata); assertAllSourceFilesAreAvailableInSource(fallbackPlan, sourceMetadata); assertAllIdenticalFilesAreAvailableInTarget(fallbackPlan, targetSourceMetadata); assertThat(fallbackPlan.getSnapshotFilesToRecover(), is(equalTo(ShardRecoveryPlan.SnapshotFilesToRecover.EMPTY))); } }); }	in the case where snapshotversion == null we need to use a luceneversion that is before the lucene version of seq_no_snapshot_recoveries_supported_version
@Override public ValueFetcher valueFetcher(SearchExecutionContext context, String format) { return new DocValueFetcher(docValueFormat(format, null), context.getForField(this)); }	i had to implement this method so fetching version works, the default implementation in mappedfieldtype throws an error. i'm not sure if this has undesired effects elsewhere though.
public void testFetchFieldValue() throws IOException { MapperService mapperService = createMapperService( fieldMapping(b -> b.field("type", "keyword")) ); String index = randomAlphaOfLength(12); withLuceneIndex(mapperService, iw -> { XContentBuilder builder = JsonXContent.contentBuilder().startObject(); builder.field("field", "value"); builder.endObject(); SourceToParse source = new SourceToParse(index, "id", BytesReference.bytes(builder), XContentType.JSON, "", Map.of()); iw.addDocument(mapperService.documentMapper().parse(source).rootDoc()); }, iw -> { IndexFieldMapper.IndexFieldType ft = (IndexFieldMapper.IndexFieldType) mapperService.fieldType("_index"); SearchLookup lookup = new SearchLookup(mapperService::fieldType, fieldDataLookup()); SearchExecutionContext searchExecutionContext = createSearchExecutionContext(mapperService); when(searchExecutionContext.getForField(ft)).thenReturn( ft.fielddataBuilder(index, () -> lookup).build(new IndexFieldDataCache.None(), new NoneCircuitBreakerService()) ); ValueFetcher valueFetcher = ft.valueFetcher(searchExecutionContext, null); IndexSearcher searcher = newSearcher(iw); LeafReaderContext context = searcher.getIndexReader().leaves().get(0); lookup.source().setSegmentAndDocument(context, 0); valueFetcher.setNextReader(context); assertEquals(List.of(index), valueFetcher.fetchValues(lookup.source())); }); }	nit: use source() here as well?
private void handleClusterAlias( int clusterAliasSlot, String clusterAlias, AutoFollowPattern autoFollowPattern, List<String> followedIndexUUIDs, ClusterState leaderClusterState ) { final List<Index> leaderIndicesToFollow = getLeaderIndicesToFollow(autoFollowPattern, leaderClusterState, followerClusterState, followedIndexUUIDs); if (leaderIndicesToFollow.isEmpty()) { finalise(clusterAliasSlot, new AutoFollowResult(clusterAlias)); } else { final CountDown leaderIndicesCountDown = new CountDown(leaderIndicesToFollow.size()); final AtomicArray<Tuple<Index, Exception>> results = new AtomicArray<>(leaderIndicesToFollow.size()); for (int i = 0; i < leaderIndicesToFollow.size(); i++) { final int slot = i; final Index indexToFollow = leaderIndicesToFollow.get(i); final String leaderIndexName = indexToFollow.getName(); final String followIndexName = getFollowerIndexName(autoFollowPattern, leaderIndexName); String leaderIndexNameWithClusterAliasPrefix = clusterAlias.equals("_local_") ? leaderIndexName : clusterAlias + ":" + leaderIndexName; FollowIndexAction.Request followRequest = new FollowIndexAction.Request(leaderIndexNameWithClusterAliasPrefix, followIndexName, autoFollowPattern.getMaxBatchOperationCount(), autoFollowPattern.getMaxConcurrentReadBatches(), autoFollowPattern.getMaxOperationSizeInBytes(), autoFollowPattern.getMaxConcurrentWriteBatches(), autoFollowPattern.getMaxWriteBufferSize(), autoFollowPattern.getMaxRetryDelay(), autoFollowPattern.getIdleShardRetryDelay()); // Execute if the create and follow api call succeeds: Runnable successHandler = () -> { LOGGER.info("Auto followed leader index [{}] as follow index [{}]", leaderIndexName, followIndexName); // This function updates the auto follow metadata in the cluster to record that the leader index has been followed: // (so that we do not try to follow it in subsequent auto follow runs) Function<ClusterState, ClusterState> function = recordLeaderIndexAsFollowFunction(clusterAlias, indexToFollow); // The coordinator always runs on the elected master node, so we can update cluster state here: updateAutoFollowMetadata(function, updateError -> { if (updateError != null) { LOGGER.error("Failed to mark leader index [" + leaderIndexName + "] as auto followed", updateError); results.set(slot, new Tuple<>(indexToFollow, updateError)); } else { results.set(slot, new Tuple<>(indexToFollow, null)); LOGGER.debug("Successfully marked leader index [{}] as auto followed", leaderIndexName); } if (leaderIndicesCountDown.countDown()) { finalise(clusterAliasSlot, new AutoFollowResult(clusterAlias, results.asList())); } }); }; // Execute if the create and follow apu call fails: Consumer<Exception> failureHandler = followError -> { assert followError != null; LOGGER.warn("Failed to auto follow leader index [" + leaderIndexName + "]", followError); results.set(slot, new Tuple<>(indexToFollow, followError)); if (leaderIndicesCountDown.countDown()) { finalise(clusterAliasSlot, new AutoFollowResult(clusterAlias, results.asList())); } }; createAndFollow(followRequest, successHandler, failureHandler); } } }	can we assert that every slot is assigned?
private void handleClusterAlias( int clusterAliasSlot, String clusterAlias, AutoFollowPattern autoFollowPattern, List<String> followedIndexUUIDs, ClusterState leaderClusterState ) { final List<Index> leaderIndicesToFollow = getLeaderIndicesToFollow(autoFollowPattern, leaderClusterState, followerClusterState, followedIndexUUIDs); if (leaderIndicesToFollow.isEmpty()) { finalise(clusterAliasSlot, new AutoFollowResult(clusterAlias)); } else { final CountDown leaderIndicesCountDown = new CountDown(leaderIndicesToFollow.size()); final AtomicArray<Tuple<Index, Exception>> results = new AtomicArray<>(leaderIndicesToFollow.size()); for (int i = 0; i < leaderIndicesToFollow.size(); i++) { final int slot = i; final Index indexToFollow = leaderIndicesToFollow.get(i); final String leaderIndexName = indexToFollow.getName(); final String followIndexName = getFollowerIndexName(autoFollowPattern, leaderIndexName); String leaderIndexNameWithClusterAliasPrefix = clusterAlias.equals("_local_") ? leaderIndexName : clusterAlias + ":" + leaderIndexName; FollowIndexAction.Request followRequest = new FollowIndexAction.Request(leaderIndexNameWithClusterAliasPrefix, followIndexName, autoFollowPattern.getMaxBatchOperationCount(), autoFollowPattern.getMaxConcurrentReadBatches(), autoFollowPattern.getMaxOperationSizeInBytes(), autoFollowPattern.getMaxConcurrentWriteBatches(), autoFollowPattern.getMaxWriteBufferSize(), autoFollowPattern.getMaxRetryDelay(), autoFollowPattern.getIdleShardRetryDelay()); // Execute if the create and follow api call succeeds: Runnable successHandler = () -> { LOGGER.info("Auto followed leader index [{}] as follow index [{}]", leaderIndexName, followIndexName); // This function updates the auto follow metadata in the cluster to record that the leader index has been followed: // (so that we do not try to follow it in subsequent auto follow runs) Function<ClusterState, ClusterState> function = recordLeaderIndexAsFollowFunction(clusterAlias, indexToFollow); // The coordinator always runs on the elected master node, so we can update cluster state here: updateAutoFollowMetadata(function, updateError -> { if (updateError != null) { LOGGER.error("Failed to mark leader index [" + leaderIndexName + "] as auto followed", updateError); results.set(slot, new Tuple<>(indexToFollow, updateError)); } else { results.set(slot, new Tuple<>(indexToFollow, null)); LOGGER.debug("Successfully marked leader index [{}] as auto followed", leaderIndexName); } if (leaderIndicesCountDown.countDown()) { finalise(clusterAliasSlot, new AutoFollowResult(clusterAlias, results.asList())); } }); }; // Execute if the create and follow apu call fails: Consumer<Exception> failureHandler = followError -> { assert followError != null; LOGGER.warn("Failed to auto follow leader index [" + leaderIndexName + "]", followError); results.set(slot, new Tuple<>(indexToFollow, followError)); if (leaderIndicesCountDown.countDown()) { finalise(clusterAliasSlot, new AutoFollowResult(clusterAlias, results.asList())); } }; createAndFollow(followRequest, successHandler, failureHandler); } } }	can we assert that every slot is assigned?
private void doAutoFollow() { if (localNodeMaster == false) { return; } ClusterState followerClusterState = clusterService.state(); AutoFollowMetadata autoFollowMetadata = followerClusterState.getMetaData().custom(AutoFollowMetadata.TYPE); if (autoFollowMetadata == null) { threadPool.schedule(pollInterval, ThreadPool.Names.SAME, this::doAutoFollow); return; } if (autoFollowMetadata.getPatterns().isEmpty()) { threadPool.schedule(pollInterval, ThreadPool.Names.SAME, this::doAutoFollow); return; } if (ccrLicenseChecker.isCcrAllowed() == false) { // TODO: set non-compliant status on auto-follow coordination that can be viewed via a stats API LOGGER.warn("skipping auto-follower coordination", LicenseUtils.newComplianceException("ccr")); threadPool.schedule(pollInterval, ThreadPool.Names.SAME, this::doAutoFollow); return; } Consumer<List<AutoFollowResult>> handler = results -> { for (AutoFollowResult result : results) { if (result.clusterStateFetchException != null) { LOGGER.warn(new ParameterizedMessage("failure occurred while fetching cluster state in leader cluster [{}]", result.clusterAlias), result.clusterStateFetchException); } for (Map.Entry<Index, Exception> entry : result.autoFollowExecutionResults.entrySet()) { if (entry.getValue() != null) { LOGGER.warn(new ParameterizedMessage("failure occurred while auto following index [{}] in leader cluster [{}]", entry.getKey(), result.clusterAlias), entry.getValue()); } } } updateStats(results); threadPool.schedule(pollInterval, ThreadPool.Names.SAME, this::doAutoFollow); }; AutoFollower operation = new AutoFollower(handler, followerClusterState) { @Override void getLeaderClusterState(final String leaderClusterAlias, final BiConsumer<ClusterState, Exception> handler) { final ClusterStateRequest request = new ClusterStateRequest(); request.clear(); request.metaData(true); if ("_local_".equals(leaderClusterAlias)) { client.admin().cluster().state( request, ActionListener.wrap(r -> handler.accept(r.getState(), null), e -> handler.accept(null, e))); } else { final Client leaderClient = client.getRemoteClusterClient(leaderClusterAlias); // TODO: set non-compliant status on auto-follow coordination that can be viewed via a stats API ccrLicenseChecker.checkRemoteClusterLicenseAndFetchClusterState( leaderClient, leaderClusterAlias, request, e -> handler.accept(null, e), leaderClusterState -> handler.accept(leaderClusterState, null)); } } @Override void createAndFollow(FollowIndexAction.Request followRequest, Runnable successHandler, Consumer<Exception> failureHandler) { client.execute(CreateAndFollowIndexAction.INSTANCE, new CreateAndFollowIndexAction.Request(followRequest), ActionListener.wrap(r -> successHandler.run(), failureHandler)); } @Override void updateAutoFollowMetadata(Function<ClusterState, ClusterState> updateFunction, Consumer<Exception> handler) { clusterService.submitStateUpdateTask("update_auto_follow_metadata", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) throws Exception { return updateFunction.apply(currentState); } @Override public void onFailure(String source, Exception e) { handler.accept(e); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { handler.accept(null); } }); } }; operation.autoFollowIndices(); }	this flow is quite similar to updatestats. should we combine these to a single method?
public Version minimumIndexCompatibilityVersion() { final int bwcMajor; if (major == 5) { bwcMajor = 2; // we jumped from 2 to 5 } else if (major == 7) { bwcMajor = 6; } else if (major == 8) { bwcMajor = 7; } else { bwcMajor = major - 1; } final int bwcMinor = 0; return Version.min(this, fromId(bwcMajor * 1000000 + bwcMinor * 10000 + 99)); }	neither this nor the major == 7 branch should be necessary. the else branch does major - 1.
public void testCreateSplitIndex() { internalCluster().ensureAtLeastNumDataNodes(2); Version version = VersionUtils.randomVersionBetween(random(), Version.V_7_0_0, Version.CURRENT); prepareCreate("source").setSettings(Settings.builder().put(indexSettings()) .put("number_of_shards", 1) .put("index.version.created", version) ).get(); final int docs = randomIntBetween(0, 128); for (int i = 0; i < docs; i++) { client().prepareIndex("source", "type") .setSource("{\\\\"foo\\\\" : \\\\"bar\\\\", \\\\"i\\\\" : " + i + "}", XContentType.JSON).get(); } ImmutableOpenMap<String, DiscoveryNode> dataNodes = client().admin().cluster().prepareState().get().getState().nodes().getDataNodes(); assertTrue("at least 2 nodes but was: " + dataNodes.size(), dataNodes.size() >= 2); // ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node // if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due // to the require._name below. ensureGreen(); // relocate all shards to one node such that we can merge it. client().admin().indices().prepareUpdateSettings("source") .setSettings(Settings.builder() .put("index.blocks.write", true)).get(); ensureGreen(); final IndicesStatsResponse sourceStats = client().admin().indices().prepareStats("source").setSegments(true).get(); // disable rebalancing to be able to capture the right stats. balancing can move the target primary // making it hard to pin point the source shards. client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put( EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none" )).get(); try { final boolean createWithReplicas = randomBoolean(); assertAcked(client().admin().indices().prepareResizeIndex("source", "target") .setResizeType(ResizeType.SPLIT) .setSettings(Settings.builder() .put("index.number_of_replicas", createWithReplicas ? 1 : 0) .put("index.number_of_shards", 2) .putNull("index.blocks.write") .build()).get()); ensureGreen(); final ClusterState state = client().admin().cluster().prepareState().get().getState(); DiscoveryNode mergeNode = state.nodes().get(state.getRoutingTable().index("target").shard(0).primaryShard().currentNodeId()); logger.info("split node {}", mergeNode); final long maxSeqNo = Arrays.stream(sourceStats.getShards()) .filter(shard -> shard.getShardRouting().currentNodeId().equals(mergeNode.getId())) .map(ShardStats::getSeqNoStats).mapToLong(SeqNoStats::getMaxSeqNo).max().getAsLong(); final long maxUnsafeAutoIdTimestamp = Arrays.stream(sourceStats.getShards()) .filter(shard -> shard.getShardRouting().currentNodeId().equals(mergeNode.getId())) .map(ShardStats::getStats) .map(CommonStats::getSegments) .mapToLong(SegmentsStats::getMaxUnsafeAutoIdTimestamp) .max() .getAsLong(); final IndicesStatsResponse targetStats = client().admin().indices().prepareStats("target").get(); for (final ShardStats shardStats : targetStats.getShards()) { final SeqNoStats seqNoStats = shardStats.getSeqNoStats(); final ShardRouting shardRouting = shardStats.getShardRouting(); assertThat("failed on " + shardRouting, seqNoStats.getMaxSeqNo(), equalTo(maxSeqNo)); assertThat("failed on " + shardRouting, seqNoStats.getLocalCheckpoint(), equalTo(maxSeqNo)); assertThat("failed on " + shardRouting, shardStats.getStats().getSegments().getMaxUnsafeAutoIdTimestamp(), equalTo(maxUnsafeAutoIdTimestamp)); } final int size = docs > 0 ? 2 * docs : 1; assertHitCount(client().prepareSearch("target").setSize(size).setQuery(new TermsQueryBuilder("foo", "bar")).get(), docs); if (createWithReplicas == false) { // bump replicas client().admin().indices().prepareUpdateSettings("target") .setSettings(Settings.builder() .put("index.number_of_replicas", 1)).get(); ensureGreen(); assertHitCount(client().prepareSearch("target").setSize(size).setQuery(new TermsQueryBuilder("foo", "bar")).get(), docs); } for (int i = docs; i < 2 * docs; i++) { client().prepareIndex("target", "type") .setSource("{\\\\"foo\\\\" : \\\\"bar\\\\", \\\\"i\\\\" : " + i + "}", XContentType.JSON).get(); } flushAndRefresh(); assertHitCount(client().prepareSearch("target").setSize(2 * size).setQuery(new TermsQueryBuilder("foo", "bar")).get(), 2 * docs); assertHitCount(client().prepareSearch("source").setSize(size).setQuery(new TermsQueryBuilder("foo", "bar")).get(), docs); GetSettingsResponse target = client().admin().indices().prepareGetSettings("target").get(); assertEquals(version, target.getIndexToSettings().get("target").getAsVersion("index.version.created", null)); } finally { // clean up client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put( EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), (String)null )).get(); } }	this is the kind of test i think should be updated to not use a hardcoded version. please look at versionutil.randomindexcompatibleversion
public void testCalculateNumRoutingShards() { assertEquals(1024, MetaDataCreateIndexService.calculateNumRoutingShards(1, Version.CURRENT)); assertEquals(1024, MetaDataCreateIndexService.calculateNumRoutingShards(2, Version.CURRENT)); assertEquals(768, MetaDataCreateIndexService.calculateNumRoutingShards(3, Version.CURRENT)); assertEquals(576, MetaDataCreateIndexService.calculateNumRoutingShards(9, Version.CURRENT)); assertEquals(1024, MetaDataCreateIndexService.calculateNumRoutingShards(512, Version.CURRENT)); assertEquals(2048, MetaDataCreateIndexService.calculateNumRoutingShards(1024, Version.CURRENT)); assertEquals(4096, MetaDataCreateIndexService.calculateNumRoutingShards(2048, Version.CURRENT)); Version latestV6 = VersionUtils.getPreviousVersion(Version.V_7_0_0); int numShards = randomIntBetween(1, 1000); assertEquals(numShards, MetaDataCreateIndexService.calculateNumRoutingShards(numShards, latestV6)); assertEquals(numShards, MetaDataCreateIndexService.calculateNumRoutingShards(numShards, VersionUtils.randomVersionBetween(random(), VersionUtils.getFirstVersion(), latestV6))); for (int i = 0; i < 1000; i++) { int randomNumShards = randomIntBetween(1, 10000); int numRoutingShards = MetaDataCreateIndexService.calculateNumRoutingShards(randomNumShards, Version.CURRENT); if (numRoutingShards <= 1024) { assertTrue("numShards: " + randomNumShards, randomNumShards < 513); assertTrue("numRoutingShards: " + numRoutingShards, numRoutingShards > 512); } else { assertEquals("numShards: " + randomNumShards, numRoutingShards / 2, randomNumShards); } double ratio = numRoutingShards / randomNumShards; int intRatio = (int) ratio; assertEquals(ratio, (intRatio), 0.0d); assertTrue(1 < ratio); assertTrue(ratio <= 1024); assertEquals(0, intRatio % 2); assertEquals("ratio is not a power of two", intRatio, Integer.highestOneBit(intRatio)); } }	nit: parenthesis can be removed
public boolean isLogstashAllowed() { Status localStatus = status; if (localStatus.active == false) { return false; } switch (localStatus.mode) { case TRIAL: case GOLD: case PLATINUM: case STANDARD: return true; default: return false; } } /** * Beats is allowed as long as there is an active license of type TRIAL, STANDARD, GOLD or PLATINUM * @return {@code true}	given that the ingest team owns, i wonder if this should instead be isingestallowed and simply replace islogstashallowed? they share the same license requirements, so it seems reasonable to me that they shouldn't dupe their code too.
public void execute() throws Exception { final String activeShardCountFailure = checkActiveShardCount(); final ShardRouting primaryRouting = primary.routingEntry(); final ShardId primaryId = primaryRouting.shardId(); if (activeShardCountFailure != null) { finishAsFailed(new UnavailableShardsException(primaryId, "{} Timeout: [{}], request: [{}]", activeShardCountFailure, request.timeout(), request)); return; } totalShards.incrementAndGet(); pendingActions.incrementAndGet(); // increase by 1 until we finish all primary coordination primary.perform(request, ActionListener.wrap( primaryResult -> handlePrimaryResult(primaryRouting, primaryId, primaryResult), resultListener::onFailure)); }	no need to pass primaryrouting and primaryid, you can get those from the shard again (shardid stays the same, and the allocation id, which we are interested in, stays the same as well).
* @throws IOException if an I/O exception occurs reading * {@code /proc/self/cgroup} */ private Map<String, String> getControlGroups() throws IOException { final List<String> lines = readProcSelfCgroup(); if (lines.isEmpty()) { return Collections.emptyMap(); } else { final Map<String, String> controllerMap = new HashMap<>(); for (final String line : lines) { final Matcher matcher = CONTROL_GROUP_PATTERN.matcher(line); // note that Matcher#matches must be invoked as // matching is lazy; this can not happen in an assert // as assertions might not be enabled final boolean matches = matcher.matches(); assert matches : line; // at this point we have captured the subsystems and the // control group final String[] controllers = matcher.group(1).split(","); for (final String controller : controllers) { controllerMap.put(controller, matcher.group(2)); } } return controllerMap; } } /** * The lines from {@code /proc/self/cgroup}. This file represents * the control groups to which the Elasticsearch process belongs. * Each line in this file represents a control group hierarchy of * the form * <p> * {@code \\\\d+:([^:,]+(?:,[^:,]+)?):(/.*)} * <p> * with the first field representing the hierarchy ID, the second * field representing a comma-separated list of the subsystems * bound to the hierarchy, and the last field representing the * control group. * * @return the lines from {@code /proc/self/cgroup} * @throws IOException if an I/O exception occurs reading * {@code /proc/self/cgroup}	isn't this obsolete now? we don't return empty form this method?
@Override public void writeTo(final StreamOutput out) throws IOException { out.writeOptionalString(cpuAcctControlGroup); out.writeLong(cpuAcctUsageNanos); out.writeString(cpuControlGroup); out.writeLong(cpuCfsPeriodMicros); out.writeLong(cpuCfsQuotaMicros); cpuStat.writeTo(out); }	you require it non-null above but here it's optional?
static Request updateByQuery(UpdateByQueryRequest updateByQueryRequest) throws IOException { String endpoint = updateByQueryRequest.isNoTypeRequest() ? endpoint(updateByQueryRequest.indices(), "_update_by_query") : endpoint(updateByQueryRequest.indices(), updateByQueryRequest.getDocTypes(), "_update_by_query"); Request request = new Request(HttpPost.METHOD_NAME, endpoint); Params params = new Params(request) .withRouting(updateByQueryRequest.getRouting()) .withPipeline(updateByQueryRequest.getPipeline()) .withRefresh(updateByQueryRequest.isRefresh()) .withTimeout(updateByQueryRequest.getTimeout()) .withWaitForActiveShards(updateByQueryRequest.getWaitForActiveShards()) .withRequestsPerSecond(updateByQueryRequest.getRequestsPerSecond()) .withIndicesOptions(updateByQueryRequest.indicesOptions()); if (updateByQueryRequest.isAbortOnVersionConflict() == false) { params.putParam("conflicts", "proceed"); } if (updateByQueryRequest.getBatchSize() != AbstractBulkByScrollRequest.DEFAULT_SCROLL_SIZE) { params.putParam("scroll_size", Integer.toString(updateByQueryRequest.getBatchSize())); } if (updateByQueryRequest.getScrollTime() != AbstractBulkByScrollRequest.DEFAULT_SCROLL_TIMEOUT) { params.putParam("scroll", updateByQueryRequest.getScrollTime()); } if (updateByQueryRequest.getSize() > 0) { params.putParam("size", Integer.toString(updateByQueryRequest.getSize())); } request.setEntity(createEntity(updateByQueryRequest, REQUEST_BODY_CONTENT_TYPE)); return request; }	i don't think we need a special check for no types here, because if the types array is empty, the logic in endpoint will just skip that path component. this is nice, because it lets us avoid introducing the additional isnotyperequest methods.
private String readComplexJsonElement(XContentParser parser) throws IOException { int tokenDepth = 10; XContentParser.Token closingToken = null; if (parser.currentToken() == XContentParser.Token.START_ARRAY) { closingToken = XContentParser.Token.END_ARRAY; } else if (parser.currentToken() == XContentParser.Token.START_OBJECT) { closingToken = XContentParser.Token.END_OBJECT; } else return null; StringBuilder builder = new StringBuilder(); for (int i = 0; i < tokenDepth && parser.currentToken() != null && parser.currentToken() != closingToken; i++) { builder.append(parser.text()); } return builder.toString(); } /** * Parse the field value using the provided {@link ParseContext}	we may have problem here. parser.text() does not behave nicely in case of start/end of objects/arrays https://github.com/elastic/elasticsearch/blob/master/libs/x-content/src/main/java/org/elasticsearch/common/xcontent/json/jsonxcontentparser.java#l81-l86 i did not find how to read the string value for delimiters in a content type in this api https://github.com/elastic/elasticsearch/blob/92b2e1a2091ce30d6267029b7b7b19d4b6690d64/libs/x-content/src/main/java/org/elasticsearch/common/xcontent/xcontentparser.java we can hardcode the json values for those delimiters, but that would not work with other types of contents, which may be a problem. a dirty alternative would be using xcontentparser.skipchildren() to get an offset in a parser buffer of beginning and ending of the serialized object/array, but we will have to handle the case of buffer refresh in case objects/array is big enough, in which case we cannot just take interval from one index in buffer to another. we can try to go around it by reading until certain max number of characters, but then we also risk hitting the right edge of buffer when currently cached buffer ends before the last char of the object/array. not really sure what should we prioritize here - support of multiple xcontents or simple and reliable parsing in case of json
public void acquireReplicaOperationPermit(final long opPrimaryTerm, final long globalCheckpoint, final ActionListener<Releasable> onPermitAcquired, final String executorOnDelay, final Object debugInfo) { verifyNotClosed(); verifyReplicationTarget(); if (opPrimaryTerm > pendingPrimaryTerm) { synchronized (mutex) { if (opPrimaryTerm > pendingPrimaryTerm) { IndexShardState shardState = state(); // only roll translog and update primary term if shard has made it past recovery // Having a new primary term here means that the old primary failed and that there is a new primary, which again // means that the master will fail this shard as all initializing shards are failed when a primary is selected // We abort early here to prevent an ongoing recovery from the failed primary to mess with the global / local checkpoint if (shardState != IndexShardState.POST_RECOVERY && shardState != IndexShardState.STARTED) { throw new IndexShardNotStartedException(shardId, shardState); } if (opPrimaryTerm > pendingPrimaryTerm) { bumpPrimaryTerm(opPrimaryTerm, () -> { // a primary promotion, or another primary term transition, might have been triggered concurrently to this // recheck under the operation permit if we can skip doing this work if (opPrimaryTerm == pendingPrimaryTerm) { assert operationPrimaryTerm < pendingPrimaryTerm; operationPrimaryTerm = pendingPrimaryTerm; updateGlobalCheckpointOnReplica(globalCheckpoint, "primary term transition"); final long currentGlobalCheckpoint = getGlobalCheckpoint(); final long localCheckpoint; if (currentGlobalCheckpoint == SequenceNumbers.UNASSIGNED_SEQ_NO) { localCheckpoint = SequenceNumbers.NO_OPS_PERFORMED; } else { localCheckpoint = currentGlobalCheckpoint; } logger.trace( "detected new primary with primary term [{}], resetting local checkpoint from [{}] to [{}]", opPrimaryTerm, getLocalCheckpoint(), localCheckpoint); getEngine().resetLocalCheckpoint(localCheckpoint); getEngine().rollTranslogGeneration(); } else { logger.trace("a primary promotion or concurrent primary term transition has made this reset obsolete"); } }); } } } } assert opPrimaryTerm <= pendingPrimaryTerm : "operation primary term [" + opPrimaryTerm + "] should be at most [" + pendingPrimaryTerm + "]"; indexShardOperationPermits.acquire( new ActionListener<Releasable>() { @Override public void onResponse(final Releasable releasable) { if (opPrimaryTerm < operationPrimaryTerm) { releasable.close(); final String message = String.format( Locale.ROOT, "%s operation primary term [%d] is too old (current [%d])", shardId, opPrimaryTerm, operationPrimaryTerm); onPermitAcquired.onFailure(new IllegalStateException(message)); } else { try { updateGlobalCheckpointOnReplica(globalCheckpoint, "operation"); } catch (Exception e) { releasable.close(); onPermitAcquired.onFailure(e); return; } onPermitAcquired.onResponse(releasable); } } @Override public void onFailure(final Exception e) { onPermitAcquired.onFailure(e); } }, executorOnDelay, true, debugInfo); }	how is that possible? shouldn't the pending primary term update under mutex prevent this?
* ... * } * } */ public static ScriptMetadata fromXContent(XContentParser parser) throws IOException { Map<String, StoredScriptSource> scripts = new HashMap<>(); String id = null; Token token = parser.currentToken(); if (token == null) { token = parser.nextToken(); } if (token != Token.START_OBJECT) { throw new ParsingException(parser.getTokenLocation(), "unexpected token [" + token + "], expected [{]"); } token = parser.nextToken(); while (token != Token.END_OBJECT) { switch (token) { case FIELD_NAME: id = parser.currentName(); break; case START_OBJECT: if (id == null) { throw new ParsingException(parser.getTokenLocation(), "unexpected token [" + token + "], expected [<id>, <code>, {]"); } StoredScriptSource source = StoredScriptSource.fromXContent(parser, true); // as of 8.0 we drop scripts/templates with an empty source // this check should be removed for the next upgradable version after 8.0 // since there is a guarantee no more empty scripts will exist if (source.getSource().isEmpty()) { if (Script.DEFAULT_TEMPLATE_LANG.equals(source.getLang())) { deprecationLogger.critical(DeprecationCategory.TEMPLATES, "empty_templates", "empty template found and dropped"); } else { deprecationLogger.critical(DeprecationCategory.TEMPLATES, "empty_scripts", "empty script found and dropped"); } } else { scripts.put(id, source); } id = null; break; default: throw new ParsingException(parser.getTokenLocation(), "unexpected token [" + token + "], expected [<id>, <code>, {]"); } token = parser.nextToken(); }	i think these should be warning logs, not deprecations. the deprecation already happened, this is a warning that we are dropping something meaningless.
protected void doExecute(PutWatchRequest request, ActionListener<PutWatchResponse> listener) { try { DateTime now = new DateTime(clock.millis(), UTC); boolean isUpdate = request.getVersion() > 0 || request.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO; Watch watch = parser.parseWithSecrets(request.getId(), false, request.getSource(), now, request.xContentType(), isUpdate, request.getIfSeqNo(), request.getIfPrimaryTerm()); watch.setState(request.isActive(), now); // ensure we only filter for the allowed headers Map<String, String> filteredHeaders = threadPool.getThreadContext().getHeaders().entrySet().stream() .filter(e -> ClientHelper.SECURITY_HEADER_FILTERS.contains(e.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); watch.status().setHeaders(filteredHeaders); try (XContentBuilder builder = jsonBuilder()) { watch.toXContent(builder, DEFAULT_PARAMS); if (isUpdate) { UpdateRequest updateRequest = new UpdateRequest(Watch.INDEX, Watch.DOC_TYPE, request.getId()); if (request.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO) { updateRequest.setIfSeqNo(request.getIfSeqNo()); updateRequest.setIfPrimaryTerm(request.getIfPrimaryTerm()); } else { updateRequest.version(request.getVersion()); } updateRequest.setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); updateRequest.doc(builder); executeAsyncWithOrigin(client.threadPool().getThreadContext(), WATCHER_ORIGIN, updateRequest, ActionListener.<UpdateResponse>wrap(response -> { boolean created = response.getResult() == DocWriteResponse.Result.CREATED; listener.onResponse(new PutWatchResponse(response.getId(), response.getVersion(), response.getSeqNo(), response.getPrimaryTerm(), created)); }, listener::onFailure), client::update); } else { IndexRequest indexRequest = new IndexRequest(Watch.INDEX, Watch.DOC_TYPE, request.getId()); indexRequest.source(builder); indexRequest.setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); executeAsyncWithOrigin(client.threadPool().getThreadContext(), WATCHER_ORIGIN, indexRequest, ActionListener.<IndexResponse>wrap(response -> { boolean created = response.getResult() == DocWriteResponse.Result.CREATED; listener.onResponse(new PutWatchResponse(response.getId(), response.getVersion(), response.getSeqNo(), response.getPrimaryTerm(), created)); }, listener::onFailure), client::index); } } } catch (Exception e) { listener.onFailure(e); } }	why is this one not guarded by if (clusterservice.state().nodes().getminnodeversion().onorafter(version.v_7_0_0)) { but the one in transportackwatchaction is?
public Engine newReadWriteEngine(EngineConfig config) { return new InternalEngine(config); }	can you take a look at how we handle the following engine in ccr and see if that extension point makes sense instead of adding this here? it feels conceptually wrong that the internal engine factory returns something other than an internal engine.
public static IndexTemplateMetaData readFrom(StreamInput in) throws IOException { Builder builder = new Builder(in.readString()); builder.order(in.readInt()); if (in.getVersion().onOrAfter(Version.V_6_0_0_alpha1)) { List<String> indexPatterns = in.readList(StreamInput::readString); if (in.getVersion().onOrAfter(Version.V_7_0_0_alpha1)) { builder.autoCreateIndex(Boolean.parseBoolean(indexPatterns.remove(indexPatterns.size() - 1))); } builder.patterns(indexPatterns); } else { builder.patterns(Collections.singletonList(in.readString())); } builder.settings(Settings.readSettingsFromStream(in)); int mappingsSize = in.readVInt(); for (int i = 0; i < mappingsSize; i++) { builder.putMapping(in.readString(), CompressedXContent.readCompressedString(in)); } int aliasesSize = in.readVInt(); for (int i = 0; i < aliasesSize; i++) { AliasMetaData aliasMd = new AliasMetaData(in); builder.putAlias(aliasMd); } int customSize = in.readVInt(); for (int i = 0; i < customSize; i++) { String type = in.readString(); IndexMetaData.Custom customIndexMetaData = IndexMetaData.lookupPrototypeSafe(type).readFrom(in); builder.putCustom(type, customIndexMetaData); } builder.version(in.readOptionalVInt()); return builder.build(); }	adding a serialized boolean to the end of this list feels really dirty, but i couldn't get the bwc tests to pass when adding a new byte to the message (even when wrapping writing that boolean in a if (in.getversion().onorafter(version.v_7_0_0_alpha1)) {). maybe i'm making a mistake here? java if (in.getversion().onorafter(version.v_7_0_0_alpha1)) { autocreateindex = in.readboolean(); } did not work for me in bwc at least.
private static List<DistributionProject> resolveArchiveProjects(File checkoutDir, Version bwcVersion) { List<String> projects = new ArrayList<>(); if (bwcVersion.onOrAfter("7.13.0")) { projects.addAll(asList("deb", "rpm")); projects.addAll(asList("windows-zip", "darwin-tar", "linux-tar")); projects.addAll(asList("darwin-aarch64-tar", "linux-aarch64-tar")); } else { projects.addAll(asList("deb", "rpm", "oss-deb", "oss-rpm")); if (bwcVersion.onOrAfter("7.0.0")) { // starting with 7.0 we bundle a jdk which means we have platform-specific archives projects.addAll(asList("oss-windows-zip", "windows-zip", "oss-darwin-tar", "darwin-tar", "oss-linux-tar", "linux-tar")); // We support aarch64 for linux and mac starting from 7.12 if (bwcVersion.onOrAfter("7.12.0")) { projects.addAll(asList("oss-darwin-aarch64-tar", "oss-linux-aarch64-tar", "darwin-aarch64-tar", "linux-aarch64-tar")); } } else { // prior to 7.0 we published only a single zip and tar archives for oss and default distributions projects.addAll(asList("oss-zip", "zip", "tar", "oss-tar")); } } return projects.stream().map(name -> { String baseDir = "distribution" + (name.endsWith("zip") || name.endsWith("tar") ? "/archives" : "/packages"); String classifier = ""; String extension = name; if (bwcVersion.onOrAfter("7.0.0")) { if (name.contains("zip") || name.contains("tar")) { int index = name.lastIndexOf('-'); String baseName = name.startsWith("oss-") ? name.substring(4, index) : name.substring(0, index); classifier = "-" + baseName + (name.contains("aarch64") ? "-aarch64" : "-x86_64"); extension = name.substring(index + 1); if (extension.equals("tar")) { extension += ".gz"; } } else if (name.contains("deb")) { classifier = "-amd64"; } else if (name.contains("rpm")) { classifier = "-x86_64"; } } return new DistributionProject(name, baseDir, bwcVersion, classifier, extension, checkoutDir); }).collect(Collectors.toList()); }	i'd only ask that we include a comment explaining that this is due to oss distro removal.
void innerPerformAction(String followerIndex, Listener listener) { UnfollowAction.Request request = new UnfollowAction.Request(followerIndex); getClient().execute(UnfollowAction.INSTANCE, request, ActionListener.wrap( r -> { assert r.isAcknowledged() : "unfollow response is not acknowledged"; listener.onResponse(true); }, exception -> { if (exception instanceof ElasticsearchException && ((ElasticsearchException) exception).getMetadata("es.failed_to_remove_retention_leases") != null) { List<String> leasesNotRemoved = ((ElasticsearchException) exception) .getMetadata("es.failed_to_remove_retention_leases"); logger.debug("Failed to remove leader retention lease(s) {} while unfollowing index [{}], " + "continuing with lifecycle execution", leasesNotRemoved, followerIndex); listener.onResponse(true); } else { listener.onFailure(exception); } } )); }	super minor but we usually don't capitalize log messages
public void testFailureToReleaseRetentionLeases() { IndexMetaData indexMetadata = IndexMetaData.builder("follower-index") .settings(settings(Version.CURRENT).put(LifecycleSettings.LIFECYCLE_INDEXING_COMPLETE, "true")) .putCustom(CCR_METADATA_KEY, Collections.emptyMap()) .numberOfShards(1) .numberOfReplicas(0) .build(); Client client = Mockito.mock(Client.class); AdminClient adminClient = Mockito.mock(AdminClient.class); Mockito.when(client.admin()).thenReturn(adminClient); IndicesAdminClient indicesClient = Mockito.mock(IndicesAdminClient.class); Mockito.when(adminClient.indices()).thenReturn(indicesClient); // Mock unfollow api call: ElasticsearchException error = new ElasticsearchException("text exception"); error.addMetadata("es.failed_to_remove_retention_leases", randomAlphaOfLength(10)); Mockito.doAnswer(invocation -> { UnfollowAction.Request request = (UnfollowAction.Request) invocation.getArguments()[1]; assertThat(request.getFollowerIndex(), equalTo("follower-index")); ActionListener listener = (ActionListener) invocation.getArguments()[2]; listener.onFailure(error); return null; }).when(client).execute(Mockito.same(UnfollowAction.INSTANCE), Mockito.any(), Mockito.any()); Boolean[] completed = new Boolean[1]; Exception[] failure = new Exception[1]; UnfollowFollowIndexStep step = new UnfollowFollowIndexStep(randomStepKey(), randomStepKey(), client); step.performAction(indexMetadata, null, null, new AsyncActionStep.Listener() { @Override public void onResponse(boolean complete) { completed[0] = complete; } @Override public void onFailure(Exception e) { failure[0] = e; } }); assertThat(completed[0], equalTo(true)); assertThat(failure[0], nullValue()); }	i understand we want to be able to mutate these things here, but i think i'd rather see atomicboolean and atomicreference for these, since it's cleaner what's going on and reduces the chance of hitting an arrayindexoutofboundsexception if this test gets refactored in the future.
@Override public SortField sortField(@Nullable Object missingValue, MultiValueMode sortMode, Nested nested, boolean reverse) { final XFieldComparatorSource source = new DoubleValuesComparatorSource(this, missingValue, sortMode, nested); return new SortField(getFieldName(), source, reverse); }	i haven't written tests for plugging in these abstractions other than the tests for top_metrics itself. i'd love some advice on what is worth it. this one probably isn't, but doublevaluescomparatorsource probably is. same for the sort implementations, i guess. thoughts?
protected static void declareAggregationFields(AbstractObjectParser<? extends ParsedAggregation, ?> objectParser) { objectParser.declareObject((parsedAgg, metadata) -> parsedAgg.metadata = Collections.unmodifiableMap(metadata), (parser, context) -> parser.map(), InternalAggregation.CommonFields.META); }	i'm using a constructingobjectparser to build parsedtophits, thus the change here.
@Override public SortFieldAndFormat build(QueryShardContext context) throws IOException { if (DOC_FIELD_NAME.equals(fieldName)) { return order == SortOrder.DESC ? SORT_DOC_REVERSE : SORT_DOC; } MappedFieldType fieldType = context.fieldMapper(fieldName); Nested nested = nested(context, fieldType); if (fieldType == null) { if (unmappedType != null) { fieldType = context.getMapperService().unmappedFieldType(unmappedType); } else { throw new QueryShardException(context, "No mapping found for [" + fieldName + "] in order to sort on"); } } boolean reverse = order == SortOrder.DESC; IndexFieldData<?> fieldData = context.getForField(fieldType); if (fieldData instanceof IndexNumericFieldData == false && (sortMode == SortMode.SUM || sortMode == SortMode.AVG || sortMode == SortMode.MEDIAN)) { throw new QueryShardException(context, "we only support AVG, MEDIAN and SUM on number based fields"); } final SortField field; if (numericType != null) { if (fieldData instanceof IndexNumericFieldData == false) { throw new QueryShardException(context, "[numeric_type] option cannot be set on a non-numeric field, got " + fieldType.typeName()); } SortedNumericDVIndexFieldData numericFieldData = (SortedNumericDVIndexFieldData) fieldData; NumericType resolvedType = resolveNumericType(numericType); field = numericFieldData.sortField(resolvedType, missing, localSortMode(), nested, reverse); } else { field = fieldData.sortField(missing, localSortMode(), nested, reverse); } return new SortFieldAndFormat(field, fieldType.docValueFormat(null, null)); }	hmmm - this looks like it should have been a nocommit for me. i don't believe you can get here now but i need to double check on it.
@Override public SortFieldAndFormat build(QueryShardContext context) throws IOException { GeoPoint[] localPoints = localPoints(); boolean reverse = order == SortOrder.DESC; MultiValueMode localSortMode = localSortMode(); IndexGeoPointFieldData geoIndexFieldData = fieldData(context); Nested nested = nested(context); if (geoIndexFieldData.getClass() == LatLonPointDVIndexFieldData.class // only works with 5.x geo_point && nested == null && localSortMode == MultiValueMode.MIN // LatLonDocValuesField internally picks the closest point && unit == DistanceUnit.METERS && reverse == false && localPoints.length == 1) { return new SortFieldAndFormat( LatLonDocValuesField.newDistanceSort(fieldName, localPoints[0].lat(), localPoints[0].lon()), DocValueFormat.RAW); } return new SortFieldAndFormat( new SortField(fieldName, comparatorSource(localPoints, localSortMode, geoIndexFieldData, nested), reverse), DocValueFormat.RAW); }	these are all extracted into private methods to make building the bucketedsort simpler.
@Override public SortFieldAndFormat build(QueryShardContext context) throws IOException { GeoPoint[] localPoints = localPoints(); boolean reverse = order == SortOrder.DESC; MultiValueMode localSortMode = localSortMode(); IndexGeoPointFieldData geoIndexFieldData = fieldData(context); Nested nested = nested(context); if (geoIndexFieldData.getClass() == LatLonPointDVIndexFieldData.class // only works with 5.x geo_point && nested == null && localSortMode == MultiValueMode.MIN // LatLonDocValuesField internally picks the closest point && unit == DistanceUnit.METERS && reverse == false && localPoints.length == 1) { return new SortFieldAndFormat( LatLonDocValuesField.newDistanceSort(fieldName, localPoints[0].lat(), localPoints[0].lon()), DocValueFormat.RAW); } return new SortFieldAndFormat( new SortField(fieldName, comparatorSource(localPoints, localSortMode, geoIndexFieldData, nested), reverse), DocValueFormat.RAW); }	it looks like we actually have three ways to do geo distance - the optimization above and a single point to single point optimization inside geoutils.distancevalues. it isn't clear to me that it is worth doing a bunch of work to mimic the optimization above without benchmarks. which i will write eventually.
@SuppressWarnings("unchecked") public void testUuidOnRootStatsIndices() throws IOException { String uuid = createIndex("test").indexUUID(); IndicesStatsResponse rsp = client().admin().indices().prepareStats().get(); try (XContentParser parser = createParser(JsonXContent.jsonXContent, rsp.toString())) { assertEquals( uuid, ((Map<String, Object>)((Map<String,Object>) parser.map().get("indices")).get("test")).get("uuid") ); } }	instead of parsing json, could you use rsp.getindex("test") like the other tests here do and then get the uuid from the indexstats object returned?
@Override public RestResponse buildResponse(FlushResponse flushResponse, XContentBuilder builder) throws Exception { final RestStatus restStatus = flushResponse.getFailedShards() > 0 ? RestStatus.OK : RestStatus.CONFLICT; final SyncedFlushResponse syncedFlushResponse = new SyncedFlushResponse(flushResponse); syncedFlushResponse.toXContent(builder, channel.request()); return new BytesRestResponse(restStatus, builder); } } static final class SyncedFlushResponse implements ToXContent { private final FlushResponse flushResponse; SyncedFlushResponse(FlushResponse flushResponse) { this.flushResponse = flushResponse; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject("_shards"); builder.field("warning", "Synced flush was removed and a normal flush was performed instead. " + "This transition will be removed a future Elasticsearch version."); builder.field("total", flushResponse.getTotalShards()); builder.field("successful", flushResponse.getSuccessfulShards()); builder.field("failed", flushResponse.getFailedShards()); // We can't serialize the detail of each index as we don't have the shard count per index. builder.endObject(); return builder; }	will be removed in a future
public void testCancelNewShardRecoveryAndUsesExistingShardCopy() throws Exception { logger.info("--> start node A"); final String nodeA = internalCluster().startNode(); logger.info("--> create index on node: {}", nodeA); createAndPopulateIndex(INDEX_NAME, 1, SHARD_COUNT, REPLICA_COUNT) .getShards()[0].getStats().getStore().size(); logger.info("--> start node B"); // force a shard recovery from nodeA to nodeB final String nodeB = internalCluster().startNode(); logger.info("--> add replica for {} on node: {}", INDEX_NAME, nodeB); assertAcked(client().admin().indices().prepareUpdateSettings(INDEX_NAME) .setSettings(Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1) .put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), 0))); ensureGreen(INDEX_NAME); logger.info("--> start node C"); final String nodeC = internalCluster().startNode(); // TODO: Sync retention leases // hold peer recovery on phase 2 after nodeB down CountDownLatch phase1ReadyBlocked = new CountDownLatch(1); CountDownLatch allowToCompletePhase1Latch = new CountDownLatch(1); MockTransportService transportService = (MockTransportService) internalCluster().getInstance(TransportService.class, nodeA); transportService.addSendBehavior((connection, requestId, action, request, options) -> { if (PeerRecoveryTargetService.Actions.CLEAN_FILES.equals(action)) { phase1ReadyBlocked.countDown(); try { allowToCompletePhase1Latch.await(); } catch (InterruptedException e) { throw new AssertionError(e); } } connection.sendRequest(requestId, action, request, options); }); logger.info("--> restart node B"); internalCluster().restartNode(nodeB, new InternalTestCluster.RestartCallback() { @Override public Settings onNodeStopped(String nodeName) throws Exception { phase1ReadyBlocked.await(); // nodeB stopped, peer recovery from nodeA to nodeC, it will be cancelled after nodeB get started. RecoveryResponse response = client().admin().indices().prepareRecoveries(INDEX_NAME).execute().actionGet(); List<RecoveryState> recoveryStates = response.shardRecoveryStates().get(INDEX_NAME); List<RecoveryState> nodeCRecoveryStates = findRecoveriesForTargetNode(nodeC, recoveryStates); assertThat(nodeCRecoveryStates.size(), equalTo(1)); assertOnGoingRecoveryState(nodeCRecoveryStates.get(0), 0, PeerRecoverySource.INSTANCE, false, nodeA, nodeC); validateIndexRecoveryState(nodeCRecoveryStates.get(0).getIndex()); return super.onNodeStopped(nodeName); } }); // wait for peer recovery from nodeA to nodeB which is a no-op recovery so it skips the CLEAN_FILES stage and hence is not blocked ensureGreen(); allowToCompletePhase1Latch.countDown(); transportService.clearAllRules(); // make sure nodeA has primary and nodeB has replica ClusterState state = client().admin().cluster().prepareState().get().getState(); List<ShardRouting> startedShards = state.routingTable().shardsWithState(ShardRoutingState.STARTED); assertThat(startedShards.size(), equalTo(2)); for (ShardRouting shardRouting : startedShards) { if (shardRouting.primary()) { assertThat(state.nodes().get(shardRouting.currentNodeId()).getName(), equalTo(nodeA)); } else { assertThat(state.nodes().get(shardRouting.currentNodeId()).getName(), equalTo(nodeB)); } } }	what about this todo?
@Override public void writeTo(StreamOutput out) throws IOException { Version clientVersion = out.getVersion(); int ord = this.ordinal(); if (clientVersion.before(Version.V_5_3_0)) { switch (ord) { case 0: out.write(0); // write PLANE ordinal return; case 1: out.write(2); // write bwc ARC ordinal return; default: throw new IOException("Unknown GeoDistance ordinal [" + ord + "]"); } } else if (clientVersion.before(Version.V_5_3_3)) { switch (ord) { case 0: out.write(0); // write PLANE ordinal return; case 1: out.write(1); // write bwc ARC ordinal return; default: throw new IOException("Unknown GeoDistance ordinal [" + ord + "]"); } } else { out.writeVInt(this.ordinal()); } } /** * Get a {@link GeoDistance} according to a given name. Valid values are * * <ul> * <li><b>plane</b> for <code>GeoDistance.PLANE</code></li> * <li><b>arc</b> for <code>GeoDistance.ARC</code></li> * </ul> * * @param name name of the {@link GeoDistance} * @return a {@link GeoDistance}	sorry, i am a bit confused here, but shouldn't this logic also apply to readfrom? wouldn't we incorrectly read arc as plane while talking to v5.3.1 nodes?
public void testGetApiKeysForApiKeyName() throws InterruptedException, ExecutionException { List<CreateApiKeyResponse> responses = createApiKeys(3, null); Client client = client().filterWithHeader(Collections.singletonMap("Authorization", UsernamePasswordToken .basicAuthHeaderValue(SecuritySettingsSource.TEST_SUPERUSER, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING))); PlainActionFuture<GetApiKeyResponse> listener = new PlainActionFuture<>(); client.execute(GetApiKeyAction.INSTANCE, GetApiKeyRequest.usingApiKeyName(responses.get(0).getName(), false), listener); GetApiKeyResponse response = listener.get(); verifyGetResponse(1, responses, response, Collections.singleton(responses.get(0).getId()), null); PlainActionFuture<GetApiKeyResponse> listener2 = new PlainActionFuture<>(); client.execute(GetApiKeyAction.INSTANCE, GetApiKeyRequest.usingApiKeyName("test-key*", false), listener2); verifyGetResponse(3, responses, listener2.get(), responses.stream().map(o -> o.getId()).collect(Collectors.toSet()), null); PlainActionFuture<GetApiKeyResponse> listener3 = new PlainActionFuture<>(); client.execute(GetApiKeyAction.INSTANCE, GetApiKeyRequest.usingApiKeyName("*", false), listener2); verifyGetResponse(3, responses, listener2.get(), responses.stream().map(o -> o.getId()).collect(Collectors.toSet()), null); }	can we have a non-match wildcard as well? plainactionfuture<getapikeyresponse> listener4 = new plainactionfuture<>(); client.execute(getapikeyaction.instance, getapikeyrequest.usingapikeyname("not-test-key*", false), listener4); assertthat(listener4.get().getapikeyinfos(), arraywithsize(0)); ideally a subset match (find 2 out of the 3 keys) would be nice, but the test infrastructure in this class doesn't make that easy.
@Override public ScriptTemplate asScript() { StringJoiner sj = new StringJoiner(" || "); ScriptTemplate leftScript = asScript(value); List<Params> rightParams = new ArrayList<>(); String scriptPrefix = leftScript + "=="; LinkedHashSet<Object> values = list.stream().map(Expression::fold).collect(Collectors.toCollection(LinkedHashSet::new)); for (Object valueFromList : values) { if (valueFromList instanceof Expression) { ScriptTemplate rightScript = asScript((Expression) valueFromList); sj.add(scriptPrefix + rightScript.template()); rightParams.add(rightScript.params()); } else { if (valueFromList instanceof String) { sj.add(scriptPrefix + '"' + valueFromList + '"'); } else { sj.add(scriptPrefix + (valueFromList == null ? "null" : valueFromList.toString())); } } } ParamsBuilder paramsBuilder = paramsBuilder().script(leftScript.params()); for (Params p : rightParams) { paramsBuilder = paramsBuilder.script(p); } return new ScriptTemplate(format(Locale.ROOT, "%s", sj.toString()), paramsBuilder.build(), dataType()); }	i don't think this block ever executing considering the values is a list of _folded_ expressions and thus values.
@Override public ScriptTemplate asScript() { StringJoiner sj = new StringJoiner(" || "); ScriptTemplate leftScript = asScript(value); List<Params> rightParams = new ArrayList<>(); String scriptPrefix = leftScript + "=="; LinkedHashSet<Object> values = list.stream().map(Expression::fold).collect(Collectors.toCollection(LinkedHashSet::new)); for (Object valueFromList : values) { if (valueFromList instanceof Expression) { ScriptTemplate rightScript = asScript((Expression) valueFromList); sj.add(scriptPrefix + rightScript.template()); rightParams.add(rightScript.params()); } else { if (valueFromList instanceof String) { sj.add(scriptPrefix + '"' + valueFromList + '"'); } else { sj.add(scriptPrefix + (valueFromList == null ? "null" : valueFromList.toString())); } } } ParamsBuilder paramsBuilder = paramsBuilder().script(leftScript.params()); for (Params p : rightParams) { paramsBuilder = paramsBuilder.script(p); } return new ScriptTemplate(format(Locale.ROOT, "%s", sj.toString()), paramsBuilder.build(), dataType()); }	the script should just delegate to internalsqlscriptsutils.in not duplicate the code inside the script.
public void testTranslateInExpression_HavingClause_PainlessAndNullHandling() { LogicalPlan p = plan("SELECT keyword, max(int) FROM test GROUP BY keyword HAVING max(int) IN (10, null, 20, 30, null, 30 - 10)"); assertTrue(p instanceof Project); assertTrue(p.children().get(0) instanceof Filter); Expression condition = ((Filter) p.children().get(0)).condition(); assertFalse(condition.foldable()); QueryTranslation translation = QueryTranslator.toQuery(condition, true); assertNull(translation.query); AggFilter aggFilter = translation.aggFilter; assertEquals("InternalSqlScriptUtils.nullSafeFilter(" + "params.a0==10 || params.a0==null || params.a0==20 || params.a0==30)", aggFilter.scriptTemplate().toString()); assertThat(aggFilter.scriptTemplate().params().toString(), startsWith("[{a=MAX(int){a->")); }	this should be nullsafefilter(internalsqlscriptutils.in(params.a0, params) with the constants being sent as a params to the script
private void innerJoinCluster() { boolean retry = true; while (retry) { if (lifecycle.stoppedOrClosed()) { return; } retry = false; final DiscoveryNode masterNode = findMaster(); if (masterNode == null) { logger.trace("no masterNode returned"); retry = true; continue; } if (localNode.equals(masterNode)) { clusterService.submitStateUpdateTask("zen-disco-join (elected_as_master)", Priority.IMMEDIATE, new ProcessedClusterStateNonMasterUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { if (currentState.nodes().masterNode() != null) { logger.debug("New cluster state has {} as master, but we were about to become master, rejoin"); return rejoin(currentState, "rejoin_due_to_master_switch_after_local_was_picked_as_master"); } master = true; nodesFD.start(); // start the nodes FD // Take into account the previous known nodes, if they happen not to be available // then fault detection will remove these nodes. DiscoveryNodes.Builder builder = new DiscoveryNodes.Builder(latestDiscoNodes) .localNodeId(localNode.id()) .masterNodeId(localNode.id()) // put our local node .put(localNode); // update the fact that we are the master... latestDiscoNodes = builder.build(); ClusterBlocks clusterBlocks = ClusterBlocks.builder().blocks(currentState.blocks()).removeGlobalBlock(discoverySettings.getNoMasterBlock()).build(); currentState = ClusterState.builder(currentState).nodes(latestDiscoNodes).blocks(clusterBlocks).build(); // eagerly run reroute to remove dead nodes from routing table RoutingAllocation.Result result = allocationService.reroute(currentState); return ClusterState.builder(currentState).routingResult(result).build(); } @Override public void onFailure(String source, Throwable t) { logger.error("unexpected failure during [{}]", t, source); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { sendInitialStateEventIfNeeded(); long count = clusterJoinsCounter.incrementAndGet(); logger.trace("cluster joins counter set to [{}] (elected as master)", count); } }); } else { retry = !joinElectedMaster(masterNode); if (retry) { continue; } clusterService.submitStateUpdateTask("join_master", Priority.IMMEDIATE, new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) throws Exception { if (!masterNode.equals(currentState.nodes().masterNode())) { logger.debug("Master node has switched on us, rejoining..."); return rejoin(currentState, "rejoin_due_to_master_switch"); } // the joinElectedMaster should create a full circle and publish a state that includes "us" // in it from the master node, whereby handleNewState will place the latest disco nodes // with the new master node in it // TODO in theory, there is no need to even start the masterFD, since it will be started in handleNewState if (latestDiscoNodes.masterNode() == null) { logger.debug("no master node is set, despite of join request completing, rejoining..."); return rejoin(currentState, "rejoin_because_no_master_node_set"); } masterFD.start(masterNode, "initial_join"); long count = clusterJoinsCounter.incrementAndGet(); logger.trace("cluster joins counter set to [{}] (joined master)", count); return currentState; } @Override public void onFailure(String source, final Throwable t1) { clusterService.submitStateUpdateTask("rejoin_on_join_master", Priority.IMMEDIATE, new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) throws Exception { logger.debug("Rejoining rejoin failed", t1); return rejoin(currentState, "rejoin_on_join_master_failure"); } @Override public void onFailure(String source, Throwable t2) { logger.error("Couldn't rejoin after original rejoin failed. Original error {}", t2, t1); } }); } }); } } }	i think the above test effectively has the same test as below, the currentstate will by definition need to be updated to reflect the fact that the joining node will get the published state with it "in it" and the master node set. i think that we can remove the below check, yet still have the mentioned comment above, and mention that the master might get switched on us _or_ that we haven't completed a full circle and we will retry again
public boolean equals(long bucket, HyperLogLogPlusPlus other) { return Objects.equals(p, other.p) && Objects.equals(algorithm.get(bucket), other.algorithm.get(bucket)) && Objects.equals(getComparableData(bucket), other.getComparableData(bucket)); }	could you do these one per line?
public void testCompleteLonRange() throws Exception { Version version = VersionUtils.randomVersionBetween(random(), Version.V_6_0_0, Version.CURRENT); Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, version).build(); XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().startObject().startObject("type1") .startObject("properties").startObject("location").field("type", "geo_point"); xContentBuilder.endObject().endObject().endObject().endObject(); assertAcked(prepareCreate("test").setSettings(settings).addMapping("type1", xContentBuilder)); ensureGreen(); client().prepareIndex("test", "type1", "1").setSource(jsonBuilder().startObject() .field("userid", 880) .field("title", "Place in Stockholm") .startObject("location").field("lat", 59.328355000000002).field("lon", 18.036842).endObject() .endObject()) .setRefreshPolicy(IMMEDIATE) .get(); client().prepareIndex("test", "type1", "2").setSource(jsonBuilder().startObject() .field("userid", 534) .field("title", "Place in Montreal") .startObject("location").field("lat", 45.509526999999999).field("lon", -73.570986000000005).endObject() .endObject()) .setRefreshPolicy(IMMEDIATE) .get(); SearchResponse searchResponse = client().prepareSearch() .setQuery( geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(50, -180, -50, 180) ).execute().actionGet(); assertThat(searchResponse.getHits().getTotalHits(), equalTo(1L)); searchResponse = client().prepareSearch() .setQuery( geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(50, -180, -50, 180) .type("indexed") ).execute().actionGet(); assertThat(searchResponse.getHits().getTotalHits(), equalTo(1L)); searchResponse = client().prepareSearch() .setQuery( geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(90, -180, -90, 180) ).execute().actionGet(); assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L)); searchResponse = client().prepareSearch() .setQuery( geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(90, -180, -90, 180) .type("indexed") ).execute().actionGet(); assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L)); searchResponse = client().prepareSearch() .setQuery( geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(50, 0, -50, 360) ).execute().actionGet(); assertThat(searchResponse.getHits().getTotalHits(), equalTo(1L)); searchResponse = client().prepareSearch() .setQuery( geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(50, 0, -50, 360) .type("indexed") ).execute().actionGet(); assertThat(searchResponse.getHits().getTotalHits(), equalTo(1L)); searchResponse = client().prepareSearch() .setQuery( geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(90, 0, -90, 360) ).execute().actionGet(); assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L)); searchResponse = client().prepareSearch() .setQuery( geoBoundingBoxQuery("location").setValidationMethod(GeoValidationMethod.COERCE).setCorners(90, 0, -90, 360) .type("indexed") ).execute().actionGet(); assertThat(searchResponse.getHits().getTotalHits(), equalTo(2L)); }	having this line on the same indention of as the one that builds the object that it is poking doesn't quite feel right to me.
public void testSimpleScrollQueryThenFetchClearAllScrollIds() throws Exception { client().admin().indices().prepareCreate("test").setSettings(Settings.builder().put("index.number_of_shards", 3)).execute() .actionGet(); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().execute().actionGet(); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().execute().actionGet(); for (int i = 0; i < 100; i++) { client().prepareIndex("test", "type1", Integer.toString(i)).setSource(jsonBuilder().startObject().field("field", i).endObject()) .execute().actionGet(); } client().admin().indices().prepareRefresh().execute().actionGet(); SearchResponse searchResponse1 = client().prepareSearch() .setQuery(matchAllQuery()) .setSize(35) .setScroll(TimeValue.timeValueMinutes(2)) .setSearchType(SearchType.QUERY_THEN_FETCH) .addSort("field", SortOrder.ASC) .execute().actionGet(); SearchResponse searchResponse2 = client().prepareSearch() .setQuery(matchAllQuery()) .setSize(35) .setScroll(TimeValue.timeValueMinutes(2)) .setSearchType(SearchType.QUERY_THEN_FETCH) .addSort("field", SortOrder.ASC) .execute().actionGet(); long counter1 = 0; long counter2 = 0; assertThat(searchResponse1.getHits().getTotalHits(), equalTo(100L)); assertThat(searchResponse1.getHits().getHits().length, equalTo(35)); for (SearchHit hit : searchResponse1.getHits()) { assertThat(((Number) hit.getSortValues()[0]).longValue(), equalTo(counter1++)); } assertThat(searchResponse2.getHits().getTotalHits(), equalTo(100L)); assertThat(searchResponse2.getHits().getHits().length, equalTo(35)); for (SearchHit hit : searchResponse2.getHits()) { assertThat(((Number) hit.getSortValues()[0]).longValue(), equalTo(counter2++)); } searchResponse1 = client().prepareSearchScroll(searchResponse1.getScrollId()) .setScroll(TimeValue.timeValueMinutes(2)) .execute().actionGet(); searchResponse2 = client().prepareSearchScroll(searchResponse2.getScrollId()) .setScroll(TimeValue.timeValueMinutes(2)) .execute().actionGet(); assertThat(searchResponse1.getHits().getTotalHits(), equalTo(100L)); assertThat(searchResponse1.getHits().getHits().length, equalTo(35)); for (SearchHit hit : searchResponse1.getHits()) { assertThat(((Number) hit.getSortValues()[0]).longValue(), equalTo(counter1++)); } assertThat(searchResponse2.getHits().getTotalHits(), equalTo(100L)); assertThat(searchResponse2.getHits().getHits().length, equalTo(35)); for (SearchHit hit : searchResponse2.getHits()) { assertThat(((Number) hit.getSortValues()[0]).longValue(), equalTo(counter2++)); } ClearScrollResponse clearResponse = client().prepareClearScroll().addScrollId("_all") .execute().actionGet(); assertThat(clearResponse.isSucceeded(), is(true)); assertThat(clearResponse.getNumFreed(), greaterThan(0)); assertThat(clearResponse.status(), equalTo(RestStatus.OK)); assertToXContentResponse(clearResponse, true, clearResponse.getNumFreed()); assertThrows(internalCluster().transportClient().prepareSearchScroll(searchResponse1.getScrollId()) .setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND); assertThrows(internalCluster().transportClient().prepareSearchScroll(searchResponse2.getScrollId()) .setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND); }	this one doesn't look right.
@Override public final boolean isAggregatable() { return true; }	i pulled this function into the superclass because it the "for query" version of it is tricky. we could probably avoid the type parameter if we really wanted to by being a bit more tricky, but i'm not sure it is worth it.
@Override public Query termQuery(Object value, QueryShardContext context) { checkAllowExpensiveQueries(context); if (value instanceof InetAddress) { return inetAddressQuery((InetAddress) value, context); } String term = BytesRefs.toString(value); if (term.contains("/")) { return cidrQuery(term, context); } InetAddress address = InetAddresses.forString(term); return inetAddressQuery(address, context); }	this was a bug!
@Override public Query regexpQuery( String value, int syntaxFlags, int matchFlags, int maxDeterminizedStates, RewriteMethod method, QueryShardContext context ) { checkAllowExpensiveQueries(context); if (matchFlags != 0) { throw new UnsupportedOperationException("we'll need to add case insensitive match support before release...."); } return new StringScriptFieldRegexpQuery(script, leafFactory(context), name(), value, syntaxFlags, maxDeterminizedStates); }	i'd like to leave this for later.
public Engine.Index prepareIndexOnReplica(SourceToParse source, long seqNo, long primaryTerm, long version, VersionType versionType, long autoGeneratedIdTimestamp, boolean isRetry) { try { verifyReplicationTarget(); assert primaryTerm == this.primaryTerm : "op term [ " + primaryTerm + " ] != shard term [" + this.primaryTerm + "]"; return prepareIndex(docMapper(source.type()), source, seqNo, primaryTerm, version, versionType, Engine.Operation.Origin.REPLICA, autoGeneratedIdTimestamp, isRetry); } catch (Exception e) { verifyNotClosed(e); throw e; } }	should this be a hard exception? something is bloody wrong here if this happens? maybe illegalstateexception?
@SuppressWarnings("unchecked") Object extractFromSource(Map<String, Object> map) { Object value = null; // Used to avoid recursive method calls // Holds the sub-maps in the document hierarchy that are pending to be inspected. LinkedList<Map<String, Object>> queue = new LinkedList<>(); // along with the current index of the `path`. LinkedList<Integer> idxQueue = new LinkedList<>(); queue.add(map); idxQueue.add(-1); while (!queue.isEmpty()) { int idx = idxQueue.removeLast(); Map<String, Object> subMap = queue.removeLast(); // Find all possible entries by examining all combinations under the current level ("idx") of the "path" // e.g.: If the path == "a.b.c.d" and the idx == 0, we need to check the current subMap against the keys: // "b", "b.c" and "b.c.d" StringJoiner sj = new StringJoiner("."); for (int i = idx + 1; i < path.length; i++) { sj.add(path[i]); Object node = subMap.get(sj.toString()); if (node instanceof Map) { // Add the sub-map to the queue along with the current path index queue.add((Map<String, Object>) node); idxQueue.add(i); } else if (node != null) { if (i < path.length - 1) { // If we reach a concrete value without exhausting the full path, something is wrong with the mapping // e.g.: map is {"a" : { "b" : "value }} and we are looking for a path: "a.b.c.d" throw new SqlIllegalArgumentException("Cannot extract value [{}] from source", fieldName); } if (value != null) { // A value has already been found so this means that there are more than one // values in the document for the same path but different hierarchy. // e.g.: {"a" : {"b" : {"c" : "value"}}}, {"a.b" : {"c" : "value"}}, ... throw new SqlIllegalArgumentException("Multiple values (returned by [{}]) are not supported", fieldName); } value = node; } } } return unwrapMultiValue(value); }	minor nitpick - use the deque instead of linkedlist.
@SuppressWarnings("unchecked") Object extractFromSource(Map<String, Object> map) { Object value = null; // Used to avoid recursive method calls // Holds the sub-maps in the document hierarchy that are pending to be inspected. LinkedList<Map<String, Object>> queue = new LinkedList<>(); // along with the current index of the `path`. LinkedList<Integer> idxQueue = new LinkedList<>(); queue.add(map); idxQueue.add(-1); while (!queue.isEmpty()) { int idx = idxQueue.removeLast(); Map<String, Object> subMap = queue.removeLast(); // Find all possible entries by examining all combinations under the current level ("idx") of the "path" // e.g.: If the path == "a.b.c.d" and the idx == 0, we need to check the current subMap against the keys: // "b", "b.c" and "b.c.d" StringJoiner sj = new StringJoiner("."); for (int i = idx + 1; i < path.length; i++) { sj.add(path[i]); Object node = subMap.get(sj.toString()); if (node instanceof Map) { // Add the sub-map to the queue along with the current path index queue.add((Map<String, Object>) node); idxQueue.add(i); } else if (node != null) { if (i < path.length - 1) { // If we reach a concrete value without exhausting the full path, something is wrong with the mapping // e.g.: map is {"a" : { "b" : "value }} and we are looking for a path: "a.b.c.d" throw new SqlIllegalArgumentException("Cannot extract value [{}] from source", fieldName); } if (value != null) { // A value has already been found so this means that there are more than one // values in the document for the same path but different hierarchy. // e.g.: {"a" : {"b" : {"c" : "value"}}}, {"a.b" : {"c" : "value"}}, ... throw new SqlIllegalArgumentException("Multiple values (returned by [{}]) are not supported", fieldName); } value = node; } } } return unwrapMultiValue(value); }	why not add i+1 to the queue since the next loop should start from the next position?
protected static LineStringBuilder parseLineString(CoordinateNode coordinates) { /** * Per GeoJSON spec (http://geojson.org/geojson-spec.html#linestring) * "coordinates" member must be an array of two or more positions * LineStringBuilder should throw a graceful exception if < 2 coordinates/points are provided */ if (coordinates.children.size() < 2) { throw new ElasticsearchParseException("Invalid number of points in LineString (found " + coordinates.children.size() + " - must be 0 or >= 2)"); } LineStringBuilder line = newLineString(); for (CoordinateNode node : coordinates.children) { line.point(node.coordinate); } return line; }	this statement and the message below don't seem to correlate. should we not also check if coordinates.children.size() == 0 here? also from the linked spec it looks like 0 points would not be valid anyway?
protected static LineStringBuilder parseLinearRing(CoordinateNode coordinates) { /** * Per GeoJSON spec (http://geojson.org/geojson-spec.html#linestring) * A LinearRing is closed LineString with 4 or more positions. The first and last positions * are equivalent (they represent equivalent points). Though a LinearRing is not explicitly * represented as a GeoJSON geometry type, it is referred to in the Polygon geometry type definition. */ if (coordinates.children.size() < 4) { throw new ElasticsearchParseException("Invalid number of points in LinearRing (found " + coordinates.children.size() + " - must be 0 or >= 4)"); } else if (coordinates.children.size() != 0 && !coordinates.children.get(0).coordinate.equals( coordinates.children.get(coordinates.children.size() - 1).coordinate)) { throw new ElasticsearchParseException("Invalid LinearRing found (coordinates are not closed)"); } return parseLineString(coordinates); }	i think the if and else if should be swapped here as, if there are no children, it looks like we would throw an exception. again, should a linearring with no points be valid?
private static double distance(double lat1, double lon1, double lat2, double lon2) { return GeoDistance.ARC.calculate(lat1, lon1, lat2, lon2, DistanceUnit.DEFAULT); }	would it be better to use the assert.fail(string) method or throw an assertionerror here? that way the test will fail correctly in the test framework
@Override public void remove() { itr.remove(); } } private class InternalRequest { private final Request request; private final Map<String, String> params; private final Set<Integer> ignoreErrorCodes; private final HttpRequestBase httpRequest; private final WarningsHandler warningsHandler; InternalRequest(Request request) { this.request = request; this.params = new HashMap<>(request.getParameters()); //ignore is a special parameter supported by the clients, shouldn't be sent to es String ignoreString = params.remove("ignore"); this.ignoreErrorCodes = getIgnoreErrorCodes(ignoreString, request.getMethod()); URI uri = buildUri(pathPrefix, request.getEndpoint(), params); this.httpRequest = createHttpRequest(request.getMethod(), uri, request.getEntity()); setHeaders(httpRequest, request.getOptions().getHeaders()); this.warningsHandler = request.getOptions().getWarningsHandler() == null ? RestClient.this.warningsHandler : request.getOptions().getWarningsHandler(); } private void setHeaders(HttpRequest httpRequest, Collection<Header> requestHeaders) { // request headers override default headers, so we don't add default headers if they exist as request headers final Set<String> requestNames = new HashSet<>(requestHeaders.size()); for (Header requestHeader : requestHeaders) { httpRequest.addHeader(requestHeader); requestNames.add(requestHeader.getName()); } for (Header defaultHeader : defaultHeaders) { if (requestNames.contains(defaultHeader.getName()) == false) { httpRequest.addHeader(defaultHeader); } } } RequestContext createContextForNextAttempt(Node node, AuthCache authCache) { this.httpRequest.reset(); return new RequestContext(this, node, authCache); } } private static class RequestContext { private final Node node; private final HttpAsyncRequestProducer requestProducer; private final HttpAsyncResponseConsumer<HttpResponse> asyncResponseConsumer; private final HttpClientContext context; RequestContext(InternalRequest request, Node node, AuthCache authCache) { this.node = node; //we stream the request body if the entity allows for it this.requestProducer = HttpAsyncMethods.create(node.getHost(), request.httpRequest); this.asyncResponseConsumer = request.request.getOptions().getHttpAsyncResponseConsumerFactory().createHttpAsyncResponseConsumer(); this.context = HttpClientContext.create(); context.setAuthCache(authCache); } } private static Set<Integer> getIgnoreErrorCodes(String ignoreString, String requestMethod) { Set<Integer> ignoreErrorCodes; if (ignoreString == null) { if (HttpHead.METHOD_NAME.equals(requestMethod)) { //404 never causes error if returned for a HEAD request ignoreErrorCodes = Collections.singleton(404); } else { ignoreErrorCodes = Collections.emptySet(); } } else { String[] ignoresArray = ignoreString.split(","); ignoreErrorCodes = new HashSet<>(); if (HttpHead.METHOD_NAME.equals(requestMethod)) { //404 never causes error if returned for a HEAD request ignoreErrorCodes.add(404); } for (String ignoreCode : ignoresArray) { try { ignoreErrorCodes.add(Integer.valueOf(ignoreCode)); } catch (NumberFormatException e) { throw new IllegalArgumentException("ignore value should be a number, found [" + ignoreString + "] instead", e); } } } return ignoreErrorCodes; } private static class InternalResponse { private final Response response; private final ResponseException responseException; InternalResponse(Response response) { this.response = Objects.requireNonNull(response); this.responseException = null; } InternalResponse(ResponseException responseException) { this.responseException = Objects.requireNonNull(responseException); this.response = null; }	should this be inside of internalrequest? i don't feel super strongly about that, but it looks like it is only used there.
@Override public void remove() { itr.remove(); } } private class InternalRequest { private final Request request; private final Map<String, String> params; private final Set<Integer> ignoreErrorCodes; private final HttpRequestBase httpRequest; private final WarningsHandler warningsHandler; InternalRequest(Request request) { this.request = request; this.params = new HashMap<>(request.getParameters()); //ignore is a special parameter supported by the clients, shouldn't be sent to es String ignoreString = params.remove("ignore"); this.ignoreErrorCodes = getIgnoreErrorCodes(ignoreString, request.getMethod()); URI uri = buildUri(pathPrefix, request.getEndpoint(), params); this.httpRequest = createHttpRequest(request.getMethod(), uri, request.getEntity()); setHeaders(httpRequest, request.getOptions().getHeaders()); this.warningsHandler = request.getOptions().getWarningsHandler() == null ? RestClient.this.warningsHandler : request.getOptions().getWarningsHandler(); } private void setHeaders(HttpRequest httpRequest, Collection<Header> requestHeaders) { // request headers override default headers, so we don't add default headers if they exist as request headers final Set<String> requestNames = new HashSet<>(requestHeaders.size()); for (Header requestHeader : requestHeaders) { httpRequest.addHeader(requestHeader); requestNames.add(requestHeader.getName()); } for (Header defaultHeader : defaultHeaders) { if (requestNames.contains(defaultHeader.getName()) == false) { httpRequest.addHeader(defaultHeader); } } } RequestContext createContextForNextAttempt(Node node, AuthCache authCache) { this.httpRequest.reset(); return new RequestContext(this, node, authCache); } } private static class RequestContext { private final Node node; private final HttpAsyncRequestProducer requestProducer; private final HttpAsyncResponseConsumer<HttpResponse> asyncResponseConsumer; private final HttpClientContext context; RequestContext(InternalRequest request, Node node, AuthCache authCache) { this.node = node; //we stream the request body if the entity allows for it this.requestProducer = HttpAsyncMethods.create(node.getHost(), request.httpRequest); this.asyncResponseConsumer = request.request.getOptions().getHttpAsyncResponseConsumerFactory().createHttpAsyncResponseConsumer(); this.context = HttpClientContext.create(); context.setAuthCache(authCache); } } private static Set<Integer> getIgnoreErrorCodes(String ignoreString, String requestMethod) { Set<Integer> ignoreErrorCodes; if (ignoreString == null) { if (HttpHead.METHOD_NAME.equals(requestMethod)) { //404 never causes error if returned for a HEAD request ignoreErrorCodes = Collections.singleton(404); } else { ignoreErrorCodes = Collections.emptySet(); } } else { String[] ignoresArray = ignoreString.split(","); ignoreErrorCodes = new HashSet<>(); if (HttpHead.METHOD_NAME.equals(requestMethod)) { //404 never causes error if returned for a HEAD request ignoreErrorCodes.add(404); } for (String ignoreCode : ignoresArray) { try { ignoreErrorCodes.add(Integer.valueOf(ignoreCode)); } catch (NumberFormatException e) { throw new IllegalArgumentException("ignore value should be a number, found [" + ignoreString + "] instead", e); } } } return ignoreErrorCodes; } private static class InternalResponse { private final Response response; private final ResponseException responseException; InternalResponse(Response response) { this.response = Objects.requireNonNull(response); this.responseException = null; } InternalResponse(ResponseException responseException) { this.responseException = Objects.requireNonNull(responseException); this.response = null; }	i think it'd be a little clearer to call this responseorexception so it is obvious it is "just" a union type.
@Override public void setCircuitBreaker(CircuitBreaker circuitBreaker) { assertThat(circuitBreaker.getName(), equalTo("test_breaker")); myCircuitBreaker.set(circuitBreaker); } } public static class TestRestCompatibility1 extends Plugin implements RestCompatibilityPlugin { @Override public Version getCompatibleVersion(ParsedMediaType acceptHeader, ParsedMediaType contentTypeHeader, boolean hasContent) { return Version.CURRENT.previousMajor(); } } public static class TestRestCompatibility2 extends Plugin implements RestCompatibilityPlugin { @Override public Version getCompatibleVersion(ParsedMediaType acceptHeader, ParsedMediaType contentTypeHeader, boolean hasContent) { return null; } } public void testLoadingMultipleRestCompatibilityPlugins() throws IOException { Settings.Builder settings = baseSettings(); // throw an exception when two plugins are registered List<Class<? extends Plugin>> plugins = basePlugins(); plugins.add(TestRestCompatibility1.class); plugins.add(TestRestCompatibility2.class); expectThrows(IllegalStateException.class, () -> new MockNode(settings.build(), plugins)); } public void testCorrectUsageOfRestCompatibilityPlugin() throws IOException { Settings.Builder settings = baseSettings(); // the correct usage expects one plugin List<Class<? extends Plugin>> plugins = basePlugins(); plugins.add(TestRestCompatibility1.class); try (Node node = new MockNode(settings.build(), plugins)) { CompatibleVersion restCompatibleFunction = node.getRestCompatibleFunction(); assertThat(restCompatibleFunction.get(null, null, false), equalTo(Version.CURRENT.previousMajor())); } } public void testDefaultingRestCompatibilityPlugin() throws IOException { Settings.Builder settings = baseSettings(); // default to CompatibleVersion.CURRENT_VERSION when no plugins provided List<Class<? extends Plugin>> plugins = basePlugins(); try (Node node = new MockNode(settings.build(), plugins)) { CompatibleVersion restCompatibleFunction = node.getRestCompatibleFunction(); assertThat(restCompatibleFunction.get(null, null, false), equalTo(Version.CURRENT)); }	can you check the message? we've had tests like this before and didn't check the message and there was some other cause of the exception rather than the true cause.
public void transformTest(ObjectNode doNodeParent) { ObjectNode doNodeValue = (ObjectNode) doNodeParent.get(getKeyToFind()); if(isCatOperation(doNodeValue)){ return; } ObjectNode headersNode = (ObjectNode) doNodeValue.get("headers"); if (headersNode == null) { headersNode = new ObjectNode(jsonNodeFactory); } for (Map.Entry<String, String> entry : headers.entrySet()) { headersNode.set(entry.getKey(), TextNode.valueOf(entry.getValue())); } doNodeValue.set("headers", headersNode); }	rather than making this class aware of "cat." names, can you introduce conditional headers to inject ? so something like : public injectheaders(map<string, string> headers, map<function<objectnode, boolean>, map<string, string>> conditionalheaders) then if the function key evaluates to true, then insert the associated headers.
@Override protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) throws IOException { String jobId = restRequest.param(Job.ID.getPreferredName()); final Request request; if (restRequest.hasContentOrSourceParam()) { XContentParser parser = restRequest.contentOrSourceParamParser(); request = Request.parseRequest(jobId, parser); } else { request = new Request(jobId); request.setTopN(restRequest.paramAsInt(Request.TOP_N.getPreferredName(), request.getTopN())); if (restRequest.hasParam(Request.BUCKET_SPAN.getPreferredName())) { request.setBucketSpan(restRequest.param(Request.BUCKET_SPAN.getPreferredName())); } request.setOverallScore(Double.parseDouble(restRequest.param(Request.OVERALL_SCORE.getPreferredName(), "0.0"))); request.setExcludeInterim(restRequest.paramAsBoolean(Request.EXCLUDE_INTERIM.getPreferredName(), request.isExcludeInterim())); if (restRequest.hasParam(Request.START.getPreferredName())) { request.setStart(restRequest.param(Request.START.getPreferredName())); } if (restRequest.hasParam(Request.END.getPreferredName())) { request.setEnd(restRequest.param(Request.END.getPreferredName())); } request.setAllowNoMatch( restRequest.paramAsBoolean( Request.ALLOW_NO_MATCH.getPreferredName(), restRequest.paramAsBoolean(Request.ALLOW_NO_JOBS, request.allowNoMatch()))); } return channel -> client.execute(GetOverallBucketsAction.INSTANCE, request, new RestToXContentListener<>(channel)); }	it seems to me that the default way to use these parameters via an api is through the request parameter (not in the body). does this add a deprecation warning if the user provides allow_no_jobs as a url param?
@Override public boolean equals(Object other) { if (other == this) { return true; } if (other == null || other.getClass() != getClass()) { return false; } SettingsConfig that = (SettingsConfig) other; return Objects.equals(maxPageSearchSize, that.maxPageSearchSize) && Objects.equals(docsPerSecond, that.docsPerSecond) && Objects.equals(datesAsEpochMillis, that.datesAsEpochMillis); }	do you mean it to stay or was it just for debug?
public Object value(Object key, String type) { if (isNumericType(type) && key instanceof Double) { return dropFloatingPointComponentIfTypeRequiresIt(type, (Double) key); } else if ((DateFieldMapper.CONTENT_TYPE.equals(type) || DateFieldMapper.DATE_NANOS_CONTENT_TYPE.equals(type)) && key instanceof Long) { return DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.formatMillis((Long) key); } return key; } } static class DateAsEpochBucketKeyExtractor implements BucketKeyExtractor { @Override public Object value(Object key, String type) { if (isNumericType(type) && key instanceof Double) { return dropFloatingPointComponentIfTypeRequiresIt(type, (Double) key); } return key; }	you use formatmillis here for formatting but in the line above you check that the type is datefieldmapper.date_nanos_content_type. please double-check it works as expected in case of nanos.
private void ensureValidPosition(long position) { if (position < 0L || position > fileInfo.length()) { throw new IllegalArgumentException("Position [" + position + "] is invalid"); } }	i think it'd be better to assert this (inline) in the constructor.
@Override public Terms terms(String field) throws IOException { return wrapTerms(super.terms(field), field); } } private Terms wrapTerms(Terms terms, String field) throws IOException { if (!hasField(field)) { return null; } else if (FieldNamesFieldMapper.NAME.equals(field)) { // for the _field_names field, fields for the document // are encoded as postings, where term is the field. // so we hide terms for fields we filter out. return fieldNamesFilterTerms; } else { return terms; } } /** * Terms impl for _field_names (used by exists filter) that filters out terms * representing fields that should not be visible in this reader. */ class FieldNamesTerms extends FilterTerms { long size = 0; long sumDocFreq; int docCount; FieldNamesTerms(Terms in) throws IOException { super(in); assert in.hasFreqs() == false; // re-compute the stats for the field to take // into account the filtered terms. computeFilteredStats(); } @Override public TermsEnum iterator() throws IOException { return new FieldNamesTermsEnum(in.iterator()); } private void computeFilteredStats() throws IOException { TermsEnum e = iterator(); while (e.next() != null) { size ++; sumDocFreq += e.docFreq(); docCount = Math.max(e.docFreq(), docCount); } } @Override public long size() throws IOException { return size; } @Override public long getSumDocFreq() throws IOException { return sumDocFreq; } @Override public int getDocCount() throws IOException { return docCount; } } /** * TermsEnum impl for _field_names (used by exists filter) that filters out terms * representing fields that should not be visible in this reader. */ class FieldNamesTermsEnum extends FilterTermsEnum { FieldNamesTermsEnum(TermsEnum in) { super(in); } /** Return true if term is accepted (matches a field name in this reader). */ boolean accept(BytesRef term) { return hasField(term.utf8ToString()); } @Override public boolean seekExact(BytesRef term) throws IOException { return accept(term) && in.seekExact(term); } @Override public SeekStatus seekCeil(BytesRef term) throws IOException { SeekStatus status = in.seekCeil(term); if (status == SeekStatus.END || accept(term())) { return status; } return next() == null ? SeekStatus.END : SeekStatus.NOT_FOUND; } @Override public BytesRef next() throws IOException { BytesRef next; while ((next = in.next()) != null) { if (accept(next)) { break; } } return next; } // we don't support ordinals, but _field_names is not used in this way @Override public void seekExact(long ord) throws IOException { throw new UnsupportedOperationException(); } @Override public long ord() throws IOException { throw new UnsupportedOperationException(); } } @Override public PointValues getPointValues(String fieldName) throws IOException { if (hasField(fieldName)) { return super.getPointValues(fieldName); } else { return null; }	i don't think this is correct... maybe we should assume doccount = maxdoc.
@Override public int read(final byte[] b, final int bOffset, final int len) throws IOException { if (len == 0) { return 0; } if (pos >= offset + length) { return -1; } // we need to stop at the end int todo = Math.min(len, length); // current offset into the underlying ByteArray long bytearrayOffset = offset + pos; // bytes already copied int written = 0; while (written < todo) { long pagefragment = PAGE_SIZE - (bytearrayOffset % PAGE_SIZE); // how much can we read until hitting N*PAGE_SIZE? int bulksize = (int)Math.min(pagefragment, todo - written); // we cannot copy more than a page fragment boolean copied = bytearray.get(bytearrayOffset, bulksize, ref); // get the fragment assert (copied == false); // we should never ever get back a materialized byte[] System.arraycopy(ref.bytes, ref.offset, b, bOffset + written, bulksize); // copy fragment contents written += bulksize; // count how much we copied bytearrayOffset += bulksize; // advance ByteArray index } pos += written; // finally advance our stream position return written; }	this still confuses me why do we return that boolean if it is always expected to be false?
protected void doExecute(final MultiGetRequest request, final ActionListener<MultiGetResponse> listener) { ClusterState clusterState = clusterService.state(); clusterState.blocks().globalBlockedRaiseException(ClusterBlockLevel.READ); final AtomicArray<MultiGetItemResponse> responses = new AtomicArray<>(request.items.size()); final Map<ShardId, MultiGetShardRequest> shardRequests = new HashMap<>(); for (int i = 0; i < request.items.size(); i++) { MultiGetRequest.Item item = request.items.get(i); String concreteSingleIndex; try { concreteSingleIndex = indexNameExpressionResolver.concreteSingleIndex(clusterState, item).getName(); } catch (Exception e) { responses.set(i, newItemFailure(item.index(), item.type(), item.id(), e)); continue; } item.routing(clusterState.metaData().resolveIndexRouting(item.parent(), item.routing(), concreteSingleIndex)); if ((item.routing() == null) && (clusterState.getMetaData().routingRequired(concreteSingleIndex, item.type()))) { String message = "routing is required for [" + concreteSingleIndex + "]/[" + item.type() + "]/[" + item.id() + "]"; responses.set(i, newItemFailure(concreteSingleIndex, item.type(), item.id(), new IllegalArgumentException(message))); continue; } ShardId shardId = clusterService.operationRouting() .getShards(clusterState, concreteSingleIndex, item.id(), item.routing(), null) .shardId(); MultiGetShardRequest shardRequest = shardRequests.get(shardId); if (shardRequest == null) { shardRequest = new MultiGetShardRequest(request, shardId.getIndexName(), shardId.getId()); shardRequests.put(shardId, shardRequest); } shardRequest.add(i, item); } if (shardRequests.isEmpty()) { // only failures.. listener.onResponse(new MultiGetResponse(responses.toArray(new MultiGetItemResponse[responses.length()]))); } final AtomicInteger counter = new AtomicInteger(shardRequests.size()); for (final MultiGetShardRequest shardRequest : shardRequests.values()) { shardAction.execute(shardRequest, new ActionListener<MultiGetShardResponse>() { @Override public void onResponse(MultiGetShardResponse response) { for (int i = 0; i < response.locations.size(); i++) { MultiGetItemResponse itemResponse = new MultiGetItemResponse(response.responses.get(i), response.failures.get(i)); responses.set(response.locations.get(i), itemResponse); } if (counter.decrementAndGet() == 0) { finishHim(); } } @Override public void onFailure(Exception e) { // create failures for all relevant requests for (int i = 0; i < shardRequest.locations.size(); i++) { MultiGetRequest.Item item = shardRequest.items.get(i); responses.set(shardRequest.locations.get(i), newItemFailure(shardRequest.index(), item.type(), item.id(), e)); } if (counter.decrementAndGet() == 0) { finishHim(); } } private void finishHim() { listener.onResponse(new MultiGetResponse(responses.toArray(new MultiGetItemResponse[responses.length()]))); } }); } }	can you move this block back within the try catch please? resolveindexrouting may throw exceptions too, and we don't want those to fail the whole request. maybe we can then just throw exception from within the if. see https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/metadata/metadata.java#l414 . maybe we should write a test for this too :) (if you are up for it).
public void testThatMgetShouldWorkWithMultiIndexAlias() throws IOException { assertAcked(prepareCreate("test").addAlias(new Alias("multiIndexAlias"))); assertAcked(prepareCreate("test2").addAlias(new Alias("multiIndexAlias"))); client().prepareIndex("test", "test", "1").setSource(jsonBuilder().startObject().field("foo", "bar").endObject()) .setRefreshPolicy(IMMEDIATE).get(); MultiGetResponse mgetResponse = client().prepareMultiGet() .add(new MultiGetRequest.Item("test", "test", "1")) .add(new MultiGetRequest.Item("multiIndexAlias", "test", "1")) .execute().actionGet(); assertThat(mgetResponse.getResponses().length, is(2)); assertThat(mgetResponse.getResponses()[0].getIndex(), is("test")); assertThat(mgetResponse.getResponses()[0].isFailed(), is(false)); assertThat(mgetResponse.getResponses()[1].getIndex(), is("multiIndexAlias")); assertThat(mgetResponse.getResponses()[1].isFailed(), is(true)); assertThat(mgetResponse.getResponses()[1].getFailure().getMessage(), containsString("more than one indices")); mgetResponse = client().prepareMultiGet() .add(new MultiGetRequest.Item("multiIndexAlias", "test", "1")) .execute().actionGet(); assertThat(mgetResponse.getResponses().length, is(1)); assertThat(mgetResponse.getResponses()[0].getIndex(), is("multiIndexAlias")); assertThat(mgetResponse.getResponses()[0].isFailed(), is(true)); assertThat(mgetResponse.getResponses()[0].getFailure().getMessage(), containsString("more than one indices")); }	can you use .get() instead of .execute().actionget();?
public String source() { return source; }	if we can move to a concrete reader then we can make this a hard cast to elasticsearchdirectoryreader
static Map<String, UserAgentParser> createUserAgentParsers(Path userAgentConfigDirectory, UserAgentCache cache) throws IOException { Map<String, UserAgentParser> userAgentParsers = new HashMap<>(); InputStream deviceTypeRegexStream = IngestUserAgentPlugin.class.getResourceAsStream("/device_type_regexes.yml"); if (Files.exists(PathUtils.get(String.valueOf(userAgentConfigDirectory), "/device_type_regexes.yml")) == false) { deviceTypeRegexStream = null; } UserAgentParser defaultParser = new UserAgentParser(DEFAULT_PARSER_NAME, IngestUserAgentPlugin.class.getResourceAsStream("/regexes.yml"), deviceTypeRegexStream, cache); userAgentParsers.put(DEFAULT_PARSER_NAME, defaultParser); if (Files.exists(userAgentConfigDirectory) && Files.isDirectory(userAgentConfigDirectory)) { PathMatcher pathMatcher = userAgentConfigDirectory.getFileSystem().getPathMatcher("glob:**.yml"); try (Stream<Path> regexFiles = Files.find(userAgentConfigDirectory, 1, (path, attr) -> attr.isRegularFile() && pathMatcher.matches(path))) { Iterable<Path> iterable = regexFiles::iterator; for (Path path : iterable) { String parserName = path.getFileName().toString(); try (InputStream regexStream = Files.newInputStream(path, StandardOpenOption.READ)) { userAgentParsers.put(parserName, new UserAgentParser(parserName, regexStream, deviceTypeRegexStream, cache)); } } } } return Collections.unmodifiableMap(userAgentParsers); }	is this check necessary if device_type_regexes.yml is a resource within the jar file?
protected PassageFormatter getPassageFormatter(SearchContextHighlight.Field field, Encoder encoder) { CustomPassageFormatter passageFormatter = new CustomPassageFormatter(field.fieldOptions().preTags()[0], field.fieldOptions().postTags()[0], encoder); return passageFormatter; }	can we merge wrap() into getanalyzer()? it feels wrong to have both methods
AddVotingConfigExclusionsRequest resolveVotingConfigExclusionsRequest(final RestRequest request) { String deprecatedNodeDescription = null; String nodeIds = null; String nodeNames = null; if (request.hasParam("node_name")) { DEPRECATION_LOGGER.deprecatedAndMaybeLog("add_voting_config_exclusion", "Using [node_name] for adding voting config exclustion will be removed in a future version. " + "Please use [node_ids] or [node_names] instead"); deprecatedNodeDescription = request.param("node_name"); } if (request.hasParam("node_ids")){ nodeIds = request.param("node_ids"); } if (request.hasParam("node_names")){ nodeNames = request.param("node_names"); } if(!oneAndonlyOneIsSet(deprecatedNodeDescription, nodeIds, nodeNames)) { throw new IllegalArgumentException("Please set node identifiers correctly. " + "One and only one of [node_name], [node_names] and [node_ids] has to be set"); } return new AddVotingConfigExclusionsRequest( Strings.splitStringByCommaToArray(deprecatedNodeDescription), Strings.splitStringByCommaToArray(nodeIds), Strings.splitStringByCommaToArray(nodeNames), TimeValue.parseTimeValue(request.param("timeout"), DEFAULT_TIMEOUT, getClass().getSimpleName() + ".timeout") ); }	this isn't needed if using controller.registerasdeprecatedhandler.
AddVotingConfigExclusionsRequest resolveVotingConfigExclusionsRequest(final RestRequest request) { String deprecatedNodeDescription = null; String nodeIds = null; String nodeNames = null; if (request.hasParam("node_name")) { DEPRECATION_LOGGER.deprecatedAndMaybeLog("add_voting_config_exclusion", "Using [node_name] for adding voting config exclustion will be removed in a future version. " + "Please use [node_ids] or [node_names] instead"); deprecatedNodeDescription = request.param("node_name"); } if (request.hasParam("node_ids")){ nodeIds = request.param("node_ids"); } if (request.hasParam("node_names")){ nodeNames = request.param("node_names"); } if(!oneAndonlyOneIsSet(deprecatedNodeDescription, nodeIds, nodeNames)) { throw new IllegalArgumentException("Please set node identifiers correctly. " + "One and only one of [node_name], [node_names] and [node_ids] has to be set"); } return new AddVotingConfigExclusionsRequest( Strings.splitStringByCommaToArray(deprecatedNodeDescription), Strings.splitStringByCommaToArray(nodeIds), Strings.splitStringByCommaToArray(nodeNames), TimeValue.parseTimeValue(request.param("timeout"), DEFAULT_TIMEOUT, getClass().getSimpleName() + ".timeout") ); }	the house style is to use == false since unary ! is too easy to miss: suggestion if (oneandonlyoneisset(deprecatednodedescription, nodeids, nodenames) == false) { however in this case maybe we can invert the sense of this method instead: suggestion if (morethanoneisset(deprecatednodedescription, nodeids, nodenames)) { also, as requested earlier, we can move this validation to the constructor of addvotingconfigexclusionsrequest.
@Override public String toString() { StringBuilder builder = new StringBuilder("SnapshotsInProgress["); for (int i = 0; i < entries.size(); i++) { builder.append(entries.get(i).snapshot().getSnapshotId().getName()); if (i + 1 < entries.size()) { builder.append(","); } } return builder.append("]").toString(); } /** * Creates the initial {@link Entry}	perhaps mention that the state can also be success here already?
@Override public void onMaster() { this.isMaster = true; maybeScheduleJob(); ClusterState clusterState = clusterService.state(); IndexLifecycleMetadata currentMetadata = clusterState.metaData().custom(IndexLifecycleMetadata.TYPE); if (currentMetadata != null) { OperationMode currentMode = currentMetadata.getOperationMode(); if (OperationMode.STOPPED.equals(currentMode)) { return; } // If we just became master, we need to kick off any async actions that // may have not been run due to master rollover for (ObjectCursor<IndexMetaData> cursor : clusterState.metaData().indices().values()) { IndexMetaData idxMeta = cursor.value; String policyName = LifecycleSettings.LIFECYCLE_NAME_SETTING.get(idxMeta.getSettings()); if (Strings.isNullOrEmpty(policyName) == false) { StepKey stepKey = IndexLifecycleRunner.getCurrentStepKey(LifecycleExecutionState.fromIndexMetadata(idxMeta)); if (OperationMode.STOPPING == currentMode && stepKey != null && IGNORE_ACTIONS_MAINTENANCE_REQUESTED.contains(stepKey.getAction()) == false) { logger.info("skipping policy [{}] for index [{}]. stopping Index Lifecycle execution", policyName, idxMeta.getIndex().getName()); continue; } lifecycleRunner.maybeRunAsyncAction(clusterState, idxMeta, policyName, stepKey); } } } }	i think we have lost the bit that sets the currentmode to stopped if no indices are in the ignore actions list?
* @param credentials the credentials provided by the user * @param listener the listener to notify after verification */ static void validateApiKeyCredentials(Map<String, Object> source, ApiKeyCredentials credentials, Clock clock, ActionListener<AuthenticationResult> listener) { final Boolean invalidated = (Boolean) source.get("api_key_invalidated"); if (invalidated == null) { listener.onResponse(AuthenticationResult.terminate("api key document is missing invalidated field", null)); } else if (invalidated) { listener.onResponse(AuthenticationResult.terminate("api key has been invalidated", null)); } final String apiKeyHash = (String) source.get("api_key_hash"); if (apiKeyHash == null) { throw new IllegalStateException("api key hash is missing"); } final boolean verified = verifyKeyAgainstHash(apiKeyHash, credentials); if (verified) { final Long expirationEpochMilli = (Long) source.get("expiration_time"); if (expirationEpochMilli == null || Instant.ofEpochMilli(expirationEpochMilli).isAfter(clock.instant())) { final Map<String, Object> creator = Objects.requireNonNull((Map<String, Object>) source.get("creator")); final String principal = Objects.requireNonNull((String) creator.get("principal")); final Map<String, Object> metadata = (Map<String, Object>) creator.get("metadata"); final Map<String, Object> roleDescriptors = (Map<String, Object>) source.get("role_descriptors"); final String[] roleNames = roleDescriptors.keySet().toArray(Strings.EMPTY_ARRAY); final User apiKeyUser = new User(principal, roleNames, null, null, metadata, true); final Map<String, Object> authResultMetadata = new HashMap<>(); authResultMetadata.put(API_KEY_ROLE_DESCRIPTORS_KEY, roleDescriptors); authResultMetadata.put(API_KEY_ID_KEY, credentials.getId()); listener.onResponse(AuthenticationResult.success(apiKeyUser, authResultMetadata)); } else { listener.onResponse(AuthenticationResult.terminate("api key is expired", null)); } } else { listener.onResponse(AuthenticationResult.unsuccessful("invalid credentials", null)); } }	you need an else here. without it the listener could get called with the result from the rest of the method. @jkakavas knows about this :)
private static void verifyOssInstallation(Installation es) { dockerShell.run("id elasticsearch"); dockerShell.run("getent group elasticsearch"); final Shell.Result passwdResult = dockerShell.run("getent passwd elasticsearch"); final String homeDir = passwdResult.stdout.trim().split(":")[5]; assertThat(homeDir, equalTo("/usr/share/elasticsearch")); Stream.of(es.home, es.data, es.logs, es.config, es.plugins).forEach(dir -> assertPermissionsAndOwnership(dir, p775)); Stream.of(es.modules).forEach(dir -> assertPermissionsAndOwnership(dir, p755)); Stream.of("elasticsearch.yml", "jvm.options", "log4j2.properties") .forEach(configFile -> assertPermissionsAndOwnership(es.config(configFile), p664)); assertThat(dockerShell.run(es.bin("elasticsearch-keystore") + " list").stdout, containsString("keystore.seed")); Stream.of(es.bin, es.lib).forEach(dir -> assertPermissionsAndOwnership(dir, p755)); Stream.of( "elasticsearch", "elasticsearch-cli", "elasticsearch-env", "elasticsearch-keystore", "elasticsearch-node", "elasticsearch-plugin", "elasticsearch-shard" ).forEach(executable -> assertPermissionsAndOwnership(es.bin(executable), p755)); Stream.of("LICENSE.txt", "NOTICE.txt", "README.asciidoc").forEach(doc -> assertPermissionsAndOwnership(es.home.resolve(doc), p644)); // nc is useful for checking network issues // zip/unzip are installed to help users who are working with certificates. Stream.of("nc", "unzip", "zip") .forEach( cliBinary -> assertTrue( cliBinary + " ought to be available.", dockerShell.runIgnoreExitCode("bash -c 'hash " + cliBinary + "'").isSuccess() ) ); }	to be honest, i'm not sure why this change is necessary. something about the default shell might be different? other command work just fine but for some reason if i just do hash foo via docker exec it complains that it can't find a builtin by that name. why it's look for a builtin command i have no clue. in the end this change doesn't affect the purpose of the assertion, which is to validate these tools are available in the container.
public <K, V> Map<K, V> readMap(Writeable.Reader<K> keyReader, Writeable.Reader<V> valueReader) throws IOException { int size = readArraySize(); if (size == 0) { return Collections.emptyMap(); } Map<K, V> map = new HashMap<>(size); for (int i = 0; i < size; i++) { K key = keyReader.read(this); V value = valueReader.read(this); map.put(key, value); } return map; } /** * Read a {@link Map} of {@code K}-type keys to {@code V}-type {@link List}s. * <pre><code> * Map&lt;String, List&lt;String&gt;&gt; map = in.readMapOfLists(StreamInput::readString, StreamInput::readString); * </code></pre> * If the map or a list in it contains any elements it will be mutable, otherwise either the empty map or empty lists it contains * might be immutable. * * @param keyReader The key reader * @param valueReader The value reader * @return Never {@code null}	can we add unit tests for this?
* @param valueWriter The value writer */ public final <K, V> void writeMap(final ImmutableOpenMap<K, V> map, final Writer<K> keyWriter, final Writer<V> valueWriter) throws IOException { writeVInt(map.size()); for (final ObjectObjectCursor<K, V> entry : map) { keyWriter.write(this, entry.key); valueWriter.write(this, entry.value); } } /** * Write a {@link ImmutableOpenMap} of {@code K}-type keys to {@code V}	can we add unit tests for this?
@Override public void infer(Map<String, Object> fields, InferenceConfig config, ActionListener<InferenceResults> listener) { try { statsAccumulator.incInference(); currentInferenceCount.increment(); Model.mapFieldsIfNecessary(fields, defaultFieldMap); long startTimeInNanos = nanoTimeSupplier.get(); boolean shouldPersistStats = (TimeUnit.NANOSECONDS.toMillis(startTimeInNanos - lastStatsQueue) > MIN_PERSISTENCE_INTERVAL) || ((currentInferenceCount.sum() + 1) % persistenceQuotient == 0); if (fieldNames.stream().allMatch(f -> MapHelper.dig(f, fields) == null)) { statsAccumulator.incMissingFields(); if (shouldPersistStats) { persistStats(); } listener.onResponse(new WarningInferenceResults(Messages.getMessage(INFERENCE_WARNING_ALL_FIELDS_MISSING, modelId))); return; } InferenceResults inferenceResults = trainedModelDefinition.infer(fields, config); statsAccumulator.timeSpent(nanoTimeSupplier.get() - startTimeInNanos); if (shouldPersistStats) { persistStats(); } listener.onResponse(inferenceResults); } catch (Exception e) { statsAccumulator.incFailure(); listener.onFailure(e); } }	this is a tricky one. when the model is done with how do we ensure it persists the latest stats. could this implement closable and persist on close when the model is evicted from the cache?
public void getModel(String modelId, ActionListener<Model> modelActionListener) { LocalModel cachedModel = localModelCache.get(modelId); if (cachedModel != null) { modelActionListener.onResponse(cachedModel); logger.trace(() -> new ParameterizedMessage("[{}] loaded from cache", modelId)); return; } if (loadModelIfNecessary(modelId, modelActionListener) == false) { // If we the model is not loaded and we did not kick off a new loading attempt, this means that we may be getting called // by a simulated pipeline logger.trace(() -> new ParameterizedMessage("[{}] not actively loading, eager loading without cache", modelId)); provider.getTrainedModel(modelId, true, ActionListener.wrap( trainedModelConfig -> modelActionListener.onResponse(new LocalModel( trainedModelConfig.getModelId(), trainedModelConfig.ensureParsedDefinition(namedXContentRegistry).getModelDefinition(), trainedModelConfig.getInput(), trainedModelConfig.getDefaultFieldMap(), InferenceStats.emptyStats(modelId, localNode), modelStatsService)), modelActionListener::onFailure )); } else { logger.trace(() -> new ParameterizedMessage("[{}] is loading or loaded, added new listener to queue", modelId)); } }	i thought the string replacement in the message parameter only occurred if trace is enabled
public void getInferenceStats(String[] modelIds, ActionListener<List<InferenceStats>> listener) { MultiSearchRequest multiSearchRequest = new MultiSearchRequest(); Arrays.stream(modelIds).map(this::buildStatsSearchRequest).forEach(multiSearchRequest::add); if (multiSearchRequest.requests().isEmpty()) { listener.onResponse(Collections.emptyList()); return; } executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, multiSearchRequest, ActionListener.<MultiSearchResponse>wrap( responses -> { List<InferenceStats> allStats = new ArrayList<>(modelIds.length); int modelIndex = 0; assert responses.getResponses().length == modelIds.length : "mismatch between search response size and models requested"; for (MultiSearchResponse.Item response : responses.getResponses()) { if (response.isFailure()) { if (ExceptionsHelper.unwrapCause(response.getFailure()) instanceof ResourceNotFoundException) { continue; } logger.error(new ParameterizedMessage("search failed for models [{}]", Strings.arrayToCommaDelimitedString(modelIds)), response.getFailure()); listener.onFailure(ExceptionsHelper.serverError("Searching for stats for models [{}] failed", response.getFailure(), Strings.arrayToCommaDelimitedString(modelIds))); return; } try { InferenceStats inferenceStats = handleMultiNodeStatsResponse(response.getResponse(), modelIds[modelIndex++]); if (inferenceStats != null) { allStats.add(inferenceStats); } } catch (Exception e) { listener.onFailure(e); return; } } listener.onResponse(allStats); }, e -> { Throwable unwrapped = ExceptionsHelper.unwrapCause(e); if (unwrapped instanceof ResourceNotFoundException) { listener.onResponse(Collections.emptyList()); return; } listener.onFailure((Exception)unwrapped); } ), client::multiSearch); }	search failed for model stats maybe construct the string and use the same one for the logger and exception message
public void getInferenceStats(String[] modelIds, ActionListener<List<InferenceStats>> listener) { MultiSearchRequest multiSearchRequest = new MultiSearchRequest(); Arrays.stream(modelIds).map(this::buildStatsSearchRequest).forEach(multiSearchRequest::add); if (multiSearchRequest.requests().isEmpty()) { listener.onResponse(Collections.emptyList()); return; } executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, multiSearchRequest, ActionListener.<MultiSearchResponse>wrap( responses -> { List<InferenceStats> allStats = new ArrayList<>(modelIds.length); int modelIndex = 0; assert responses.getResponses().length == modelIds.length : "mismatch between search response size and models requested"; for (MultiSearchResponse.Item response : responses.getResponses()) { if (response.isFailure()) { if (ExceptionsHelper.unwrapCause(response.getFailure()) instanceof ResourceNotFoundException) { continue; } logger.error(new ParameterizedMessage("search failed for models [{}]", Strings.arrayToCommaDelimitedString(modelIds)), response.getFailure()); listener.onFailure(ExceptionsHelper.serverError("Searching for stats for models [{}] failed", response.getFailure(), Strings.arrayToCommaDelimitedString(modelIds))); return; } try { InferenceStats inferenceStats = handleMultiNodeStatsResponse(response.getResponse(), modelIds[modelIndex++]); if (inferenceStats != null) { allStats.add(inferenceStats); } } catch (Exception e) { listener.onFailure(e); return; } } listener.onResponse(allStats); }, e -> { Throwable unwrapped = ExceptionsHelper.unwrapCause(e); if (unwrapped instanceof ResourceNotFoundException) { listener.onResponse(Collections.emptyList()); return; } listener.onFailure((Exception)unwrapped); } ), client::multiSearch); }	in theory the number of stats docs returned is the number of distinct count of nodes the model was ever open on? and there will be one document for every unique node/model id pair
@Override protected void doRun() throws Exception { int bytesRequested = request.getSize(); ByteArray array = bigArrays.newByteArray(bytesRequested, false); String fileName = request.getFileName(); String sessionUUID = request.getSessionUUID(); // This is currently safe to do because calling `onResponse` will serialize the bytes to the network layer data // structure on the same thread. So the bytes will be copied before the reference is released. try (ReleasablePagedBytesReference reference = new ReleasablePagedBytesReference(array, bytesRequested, array)) { try (CcrRestoreSourceService.Reader reader = restoreSourceService.getSessionReader(sessionUUID, fileName)) { int bytesRead = reader.readFileBytes(reference); if (bytesRead < bytesRequested) { listener.onFailure(new IOException("[" + bytesRead + "] bytes requested. Only [" + bytesRead + "] could be read from file.")); } else { listener.onResponse(new GetCcrRestoreFileChunkResponse(reference)); } } } } }); } } public static class GetCcrRestoreFileChunkResponse extends ActionResponse { private final BytesReference chunk; GetCcrRestoreFileChunkResponse(StreamInput streamInput) throws IOException { super(streamInput); chunk = streamInput.readBytesReference(); } GetCcrRestoreFileChunkResponse(BytesReference chunk) { this.chunk = chunk; }	doesn't this mean we need to exactly know how much is left to read otherwise bytesrequested will be bigger than the remaining bytes in the case we are approaching the end of the file? can't we simply treat this as an upperbound? seems odd to fail here.
@Override protected void doRun() throws Exception { int bytesRequested = request.getSize(); ByteArray array = bigArrays.newByteArray(bytesRequested, false); String fileName = request.getFileName(); String sessionUUID = request.getSessionUUID(); // This is currently safe to do because calling `onResponse` will serialize the bytes to the network layer data // structure on the same thread. So the bytes will be copied before the reference is released. try (ReleasablePagedBytesReference reference = new ReleasablePagedBytesReference(array, bytesRequested, array)) { try (CcrRestoreSourceService.Reader reader = restoreSourceService.getSessionReader(sessionUUID, fileName)) { int bytesRead = reader.readFileBytes(reference); if (bytesRead < bytesRequested) { listener.onFailure(new IOException("[" + bytesRead + "] bytes requested. Only [" + bytesRead + "] could be read from file.")); } else { listener.onResponse(new GetCcrRestoreFileChunkResponse(reference)); } } } } }); } } public static class GetCcrRestoreFileChunkResponse extends ActionResponse { private final BytesReference chunk; GetCcrRestoreFileChunkResponse(StreamInput streamInput) throws IOException { super(streamInput); chunk = streamInput.readBytesReference(); } GetCcrRestoreFileChunkResponse(BytesReference chunk) { this.chunk = chunk; }	it would be nice to get some kind of progress indication on the follower side ie. a chunk ordinal or so that i can ensure i read all chucks in order. can we sent back the offset we read from in the response and assert that on the follower end?
public synchronized Reader getSessionReader(String sessionUUID, String fileName) throws IOException { RestoreContext restore = onGoingRestores.get(sessionUUID); if (restore == null) { logger.info("could not get session [{}] because session not found", sessionUUID); throw new IllegalArgumentException("session [" + sessionUUID + "] not found"); } return restore.getFileReader(fileName); }	do we need to log at info level here? throwing the exception will fail the recovery and bubble up the failure already
public synchronized Reader getSessionReader(String sessionUUID, String fileName) throws IOException { RestoreContext restore = onGoingRestores.get(sessionUUID); if (restore == null) { logger.info("could not get session [{}] because session not found", sessionUUID); throw new IllegalArgumentException("session [" + sessionUUID + "] not found"); } return restore.getFileReader(fileName); }	this means that opening the index input is under a global lock. if there are multiple shards recovering from remote, this might become a performance bottleneck.
private void removeSessionForShard(String sessionUUID, IndexShard indexShard) { logger.debug("closing session [{}] for shard [{}]", sessionUUID, indexShard.shardId()); HashSet<String> sessions = sessionsForShard.get(indexShard); if (sessions != null) { sessions.remove(sessionUUID); if (sessions.isEmpty()) { sessionsForShard.remove(indexShard); } } } } private static class RefCountedCloseable<T extends Closeable> extends AbstractRefCounted { private final String name; private final T object; private RefCountedCloseable(String name, T object) { super(name); this.name = name; this.object = object; } @Override protected void closeInternal() { IOUtils.closeWhileHandlingException(object); } } public static class Reader implements Closeable { private final RefCountedCloseable<Engine.IndexCommitRef> session; private final RefCountedCloseable<IndexInput> input; private final Releasable lockRelease; private Reader(RefCountedCloseable<Engine.IndexCommitRef> session, RefCountedCloseable<IndexInput> input, Releasable lockRelease) { this.session = session; this.input = input; this.lockRelease = lockRelease; boolean sessionRefIncremented = false; boolean inputRefIncremented = false; try { session.incRef(); sessionRefIncremented = true; input.incRef(); inputRefIncremented = true; } finally { if (sessionRefIncremented == false) { IOUtils.closeWhileHandlingException(lockRelease); } else if (inputRefIncremented == false) { IOUtils.closeWhileHandlingException(session::decRef, lockRelease); } } } public int readFileBytes(BytesReference reference) throws IOException { IndexInput in = input.object; BytesRefIterator refIterator = reference.iterator(); BytesRef ref; int bytesWritten = 0; while ((ref = refIterator.next()) != null) { byte[] refBytes = ref.bytes; in.readBytes(refBytes, 0, refBytes.length); bytesWritten += ref.length; } return bytesWritten; } @Override public void close() { IOUtils.closeWhileHandlingException(input::decRef, session::decRef, lockRelease); }	why store the name here if it's already accessible in the superclass?
private void removeSessionForShard(String sessionUUID, IndexShard indexShard) { logger.debug("closing session [{}] for shard [{}]", sessionUUID, indexShard.shardId()); HashSet<String> sessions = sessionsForShard.get(indexShard); if (sessions != null) { sessions.remove(sessionUUID); if (sessions.isEmpty()) { sessionsForShard.remove(indexShard); } } } } private static class RefCountedCloseable<T extends Closeable> extends AbstractRefCounted { private final String name; private final T object; private RefCountedCloseable(String name, T object) { super(name); this.name = name; this.object = object; } @Override protected void closeInternal() { IOUtils.closeWhileHandlingException(object); } } public static class Reader implements Closeable { private final RefCountedCloseable<Engine.IndexCommitRef> session; private final RefCountedCloseable<IndexInput> input; private final Releasable lockRelease; private Reader(RefCountedCloseable<Engine.IndexCommitRef> session, RefCountedCloseable<IndexInput> input, Releasable lockRelease) { this.session = session; this.input = input; this.lockRelease = lockRelease; boolean sessionRefIncremented = false; boolean inputRefIncremented = false; try { session.incRef(); sessionRefIncremented = true; input.incRef(); inputRefIncremented = true; } finally { if (sessionRefIncremented == false) { IOUtils.closeWhileHandlingException(lockRelease); } else if (inputRefIncremented == false) { IOUtils.closeWhileHandlingException(session::decRef, lockRelease); } } } public int readFileBytes(BytesReference reference) throws IOException { IndexInput in = input.object; BytesRefIterator refIterator = reference.iterator(); BytesRef ref; int bytesWritten = 0; while ((ref = refIterator.next()) != null) { byte[] refBytes = ref.bytes; in.readBytes(refBytes, 0, refBytes.length); bytesWritten += ref.length; } return bytesWritten; } @Override public void close() { IOUtils.closeWhileHandlingException(input::decRef, session::decRef, lockRelease); }	should this be called bytesread?
protected void closeInternal() { IOUtils.closeWhileHandlingException(object); } } public static class Reader implements Closeable { private final RefCountedCloseable<Engine.IndexCommitRef> session; private final RefCountedCloseable<IndexInput> input; private final Releasable lockRelease; private Reader(RefCountedCloseable<Engine.IndexCommitRef> session, RefCountedCloseable<IndexInput> input, Releasable lockRelease) { this.session = session; this.input = input; this.lockRelease = lockRelease; boolean sessionRefIncremented = false; boolean inputRefIncremented = false; try { session.incRef(); sessionRefIncremented = true; input.incRef(); inputRefIncremented = true; } finally { if (sessionRefIncremented == false) { IOUtils.closeWhileHandlingException(lockRelease); } else if (inputRefIncremented == false) { IOUtils.closeWhileHandlingException(session::decRef, lockRelease); } } } public int readFileBytes(BytesReference reference) throws IOException { IndexInput in = input.object; BytesRefIterator refIterator = reference.iterator(); BytesRef ref; int bytesWritten = 0; while ((ref = refIterator.next()) != null) { byte[] refBytes = ref.bytes; in.readBytes(refBytes, 0, refBytes.length); bytesWritten += ref.length; } return bytesWritten; } @Override public void close() { IOUtils.closeWhileHandlingException(input::decRef, session::decRef, lockRelease); }	maybe you wanna use something like this: java list<closeable> torelease = new arraylist<>(); torelease.add(lockrelease); try { session.incref() torelease.add(session); input.incref(); torelease.clear(); // all done no need to release anything } finally { ioutils.closewhilehandlingexception(torelease); }
private static List<SnapshotId> matchingSnapshotIds(RepositoryData repositoryData, Collection<String> snapshotsOrPatterns, String repositoryName) { final Map<String, SnapshotId> allSnapshotIds = repositoryData.getSnapshotIds().stream().collect( Collectors.toMap(SnapshotId::getName, Function.identity())); final List<SnapshotId> foundSnapshots = new ArrayList<>(); for (String snapshotOrPattern : snapshotsOrPatterns) { if (Regex.isSimpleMatchPattern(snapshotOrPattern) == false) { final SnapshotId foundId = allSnapshotIds.get(snapshotOrPattern); if (foundId == null) { throw new SnapshotMissingException(repositoryName, snapshotOrPattern); } else { foundSnapshots.add(allSnapshotIds.get(snapshotOrPattern)); } } else { for (Map.Entry<String, SnapshotId> entry : allSnapshotIds.entrySet()) { if (Regex.simpleMatch(snapshotOrPattern, entry.getKey())) { foundSnapshots.add(entry.getValue()); } } } } return foundSnapshots; }	do we need a set? isn't there a risk that we're adding the same item multiple times if there are overlapping wildcards?
public ClusterState execute(ClusterState currentState) { if (snapshotNames.size() > 1 && currentState.nodes().getMinNodeVersion().before(MULTI_DELETE_VERSION)) { throw new IllegalArgumentException("Deleting multiple snapshots in a single request is only supported in version [ " + MULTI_DELETE_VERSION + "] but cluster contained node of version [" + currentState.nodes().getMinNodeVersion() + "]"); } final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); final SnapshotsInProgress.Entry snapshotEntry; if (snapshotNames.size() == 1) { final String snapshotName = snapshotNames.iterator().next(); if (Regex.isSimpleMatchPattern(snapshotName)) { snapshotEntry = null; } else { snapshotEntry = findInProgressSnapshot(snapshots, snapshotName, repositoryName); } } else { snapshotEntry = null; } if (snapshotEntry == null) { return currentState; } runningSnapshot = snapshotEntry.snapshot(); final ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards; final State state = snapshotEntry.state(); final String failure; if (state == State.INIT) { // snapshot is still initializing, mark it as aborted shards = snapshotEntry.shards(); assert shards.isEmpty(); failure = "Snapshot was aborted during initialization"; abortedDuringInit = true; } else if (state == State.STARTED) { // snapshot is started - mark every non completed shard as aborted final ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shardsBuilder = ImmutableOpenMap.builder(); for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshotEntry.shards()) { ShardSnapshotStatus status = shardEntry.value; if (status.state().completed() == false) { status = new ShardSnapshotStatus( status.nodeId(), ShardState.ABORTED, "aborted by snapshot deletion", status.generation()); } shardsBuilder.put(shardEntry.key, status); } shards = shardsBuilder.build(); failure = "Snapshot was aborted by deletion"; } else { boolean hasUncompletedShards = false; // Cleanup in case a node gone missing and snapshot wasn't updated for some reason for (ObjectCursor<ShardSnapshotStatus> shardStatus : snapshotEntry.shards().values()) { // Check if we still have shard running on existing nodes if (shardStatus.value.state().completed() == false && shardStatus.value.nodeId() != null && currentState.nodes().get(shardStatus.value.nodeId()) != null) { hasUncompletedShards = true; break; } } if (hasUncompletedShards) { // snapshot is being finalized - wait for shards to complete finalization process logger.debug("trying to delete completed snapshot - should wait for shards to finalize on all nodes"); return currentState; } else { // no shards to wait for but a node is gone - this is the only case // where we force to finish the snapshot logger.debug("trying to delete completed snapshot with no finalizing shards - can delete immediately"); shards = snapshotEntry.shards(); } failure = snapshotEntry.failure(); } return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(snapshots.entries().stream().map(existing -> { if (existing.equals(snapshotEntry)) { return new SnapshotsInProgress.Entry(snapshotEntry, State.ABORTED, shards, failure); } return existing; }).collect(Collectors.toUnmodifiableList()))).build(); }	do we need to throw a concurrent...exception in case where there is snapshotsinprogress.entry but we did not look at aborting it?
void deleteSnapshots(Map<String, List<SnapshotInfo>> snapshotsToDelete, TimeValue maximumTime, SnapshotLifecycleStats slmStats) { int count = snapshotsToDelete.values().stream().mapToInt(List::size).sum(); logger.info("starting snapshot retention deletion for [{}] snapshots", count); long startTime = nowNanoSupplier.getAsLong(); final AtomicInteger deleted = new AtomicInteger(0); final AtomicInteger failed = new AtomicInteger(0); for (Map.Entry<String, List<SnapshotInfo>> entry : snapshotsToDelete.entrySet()) { String repo = entry.getKey(); List<SnapshotInfo> snapshots = entry.getValue(); for (SnapshotInfo info : snapshots) { final String policyId = getPolicyId(info); final long deleteStartTime = nowNanoSupplier.getAsLong(); // TODO: Use snapshot multi-delete instead of this loop deleteSnapshot(policyId, repo, info.snapshotId(), slmStats, ActionListener.wrap(acknowledgedResponse -> { deleted.incrementAndGet(); if (acknowledgedResponse.isAcknowledged()) { historyStore.putAsync(SnapshotHistoryItem.deletionSuccessRecord(Instant.now().toEpochMilli(), info.snapshotId().getName(), policyId, repo)); } else { SnapshotHistoryItem.deletionPossibleSuccessRecord(Instant.now().toEpochMilli(), info.snapshotId().getName(), policyId, repo, "deletion request issued successfully, no acknowledgement received"); } }, e -> { failed.incrementAndGet(); try { final SnapshotHistoryItem result = SnapshotHistoryItem.deletionFailureRecord(Instant.now().toEpochMilli(), info.snapshotId().getName(), policyId, repo, e); historyStore.putAsync(result); } catch (IOException ex) { // This shouldn't happen unless there's an issue with serializing the original exception logger.error(new ParameterizedMessage( "failed to record snapshot deletion failure for snapshot lifecycle policy [{}]", policyId), ex); } })); // Check whether we have exceeded the maximum time allowed to spend deleting // snapshots, if we have, short-circuit the rest of the deletions long finishTime = nowNanoSupplier.getAsLong(); TimeValue deletionTime = TimeValue.timeValueNanos(finishTime - deleteStartTime); logger.debug("elapsed time for deletion of [{}] snapshot: {}", info.snapshotId(), deletionTime); TimeValue totalDeletionTime = TimeValue.timeValueNanos(finishTime - startTime); if (totalDeletionTime.compareTo(maximumTime) > 0) { logger.info("maximum snapshot retention deletion time reached, time spent: [{}]," + " maximum allowed time: [{}], deleted [{}] out of [{}] snapshots scheduled for deletion, failed to delete [{}]", totalDeletionTime, maximumTime, deleted, count, failed); slmStats.deletionTime(totalDeletionTime); slmStats.retentionTimedOut(); return; } } } TimeValue totalElapsedTime = TimeValue.timeValueNanos(nowNanoSupplier.getAsLong() - startTime); logger.debug("total elapsed time for deletion of [{}] snapshots: {}", deleted, totalElapsedTime); slmStats.deletionTime(totalElapsedTime); } /** * Delete the given snapshot from the repository in blocking manner * * @param repo The repository the snapshot is in * @param snapshot The snapshot metadata * @param listener {@link ActionListener#onResponse(Object)} is called if a {@link SnapshotHistoryItem} can be created representing a * successful or failed deletion call. {@link ActionListener#onFailure(Exception)}	add a note that this should only be done once all nodes are on >= minversion.
public void testDiskThresholdWithSnapshotShardSizes() { final long shardSizeInBytes = randomBoolean() ? 10L : 50L; logger.info("--> using shard size [{}]", shardSizeInBytes); final Settings diskSettings = Settings.builder() .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true) .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "90%") .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "95%") .build(); final ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder(); usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 21)); // 79% used usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 1)); // 99% used final ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build(); final ClusterInfoService clusterInfoService = () -> new DevNullClusterInfo(usages, usages, ImmutableOpenMap.of()); final AllocationDeciders deciders = new AllocationDeciders( new HashSet<>(Arrays.asList( new RestoreInProgressAllocationDecider(), new SameShardAllocationDecider( Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS) ), makeDecider(diskSettings)))); final Snapshot snapshot = new Snapshot("_repository", new SnapshotId("_snapshot_uuid", "_snapshot_name")); final IndexId indexId = new IndexId("_indexid_uuid", "_indexid_name"); final ShardId shardId = new ShardId(new Index("test", IndexMetadata.INDEX_UUID_NA_VALUE), 0); final AtomicReference<SnapshotShardSizeInfo> snapshotShardSizeInfoRef = new AtomicReference<>(SnapshotShardSizeInfo.EMPTY); final SnapshotsInfoService snapshotsInfoService = snapshotShardSizeInfoRef::get; final AllocationService strategy = new AllocationService(deciders, new TestGatewayAllocator(), new BalancedShardsAllocator(Settings.EMPTY), clusterInfoService, snapshotsInfoService); final Metadata metadata = Metadata.builder() .put(IndexMetadata.builder("test") .settings(settings(Version.CURRENT)) .numberOfShards(1) .numberOfReplicas(0) .putInSyncAllocationIds(0, Set.of(AllocationId.newInitializing().getId())) ).build(); final RoutingTable routingTable = RoutingTable.builder() .addAsNewRestore( metadata.index("test"), new RecoverySource.SnapshotRecoverySource("_restore_uuid", snapshot, Version.CURRENT, indexId), new IntHashSet() ).build(); final ImmutableOpenMap.Builder<ShardId, RestoreInProgress.ShardRestoreStatus> shards = ImmutableOpenMap.builder(); shards.put(shardId, new RestoreInProgress.ShardRestoreStatus("node1")); final RestoreInProgress.Builder restores = new RestoreInProgress.Builder() .add(new RestoreInProgress.Entry("_restore_uuid", snapshot, RestoreInProgress.State.INIT, List.of("test"), shards.build())); ClusterState clusterState = ClusterState.builder(new ClusterName(getTestName())) .metadata(metadata) .routingTable(routingTable) .putCustom(RestoreInProgress.TYPE, restores.build()) .nodes(DiscoveryNodes.builder() .add(newNode("node1")) .add(newNode("node2")) // node2 is added because DiskThresholdDecider automatically ignore single-node clusters ).build(); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).stream().map(ShardRouting::unassignedInfo) .allMatch(unassignedInfo -> Reason.NEW_INDEX_RESTORED.equals(unassignedInfo.getReason())), is(true)); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).stream().map(ShardRouting::unassignedInfo) .allMatch(unassignedInfo -> AllocationStatus.NO_ATTEMPT.equals(unassignedInfo.getLastAllocationStatus())), is(true)); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1)); // reroute triggers snapshot shard size fetching clusterState = strategy.reroute(clusterState, "reroute"); logShardStates(clusterState); // shard cannot be assigned yet as the snapshot shard size is unknown assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).stream().map(ShardRouting::unassignedInfo) .allMatch(unassignedInfo -> AllocationStatus.FETCHING_SHARD_DATA.equals(unassignedInfo.getLastAllocationStatus())), is(true)); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1)); final SnapshotShard snapshotShard = new SnapshotShard(snapshot, indexId, shardId); final ImmutableOpenMap.Builder<SnapshotShard, Long> snapshotShardSizes = ImmutableOpenMap.builder(); final boolean shouldAllocate; if (randomBoolean()) { logger.info("--> simulating snapshot shards size retrieval success"); snapshotShardSizes.put(snapshotShard, shardSizeInBytes); logger.info("--> shard allocation depends on its size"); shouldAllocate = shardSizeInBytes < usages.get("node1").getFreeBytes(); } else { logger.info("--> simulating snapshot shards size retrieval failure"); snapshotShardSizes.put(snapshotShard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE); logger.info("--> shard is always allocated when its size could not be retrieved"); shouldAllocate = true; } snapshotShardSizeInfoRef.set(new SnapshotShardSizeInfo(snapshotShardSizes.build())); // reroute uses the previous snapshot shard size clusterState = startInitializingShardsAndReroute(strategy, clusterState); logShardStates(clusterState); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(shouldAllocate ? 0 : 1)); assertThat(clusterState.getRoutingNodes().shardsWithState("test", INITIALIZING, STARTED).size(), equalTo(shouldAllocate ? 1 : 0)); }	nit: no functional impact here, but the first argument is the snapshot name and the second the uuid :) i always just use final snapshot snapshot = new snapshot("_repository", new snapshotid("_snapshot_name", uuids.randombase64uuid(random())));
public void testDiskThresholdWithSnapshotShardSizes() { final long shardSizeInBytes = randomBoolean() ? 10L : 50L; logger.info("--> using shard size [{}]", shardSizeInBytes); final Settings diskSettings = Settings.builder() .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true) .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "90%") .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "95%") .build(); final ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder(); usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 21)); // 79% used usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 1)); // 99% used final ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build(); final ClusterInfoService clusterInfoService = () -> new DevNullClusterInfo(usages, usages, ImmutableOpenMap.of()); final AllocationDeciders deciders = new AllocationDeciders( new HashSet<>(Arrays.asList( new RestoreInProgressAllocationDecider(), new SameShardAllocationDecider( Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS) ), makeDecider(diskSettings)))); final Snapshot snapshot = new Snapshot("_repository", new SnapshotId("_snapshot_uuid", "_snapshot_name")); final IndexId indexId = new IndexId("_indexid_uuid", "_indexid_name"); final ShardId shardId = new ShardId(new Index("test", IndexMetadata.INDEX_UUID_NA_VALUE), 0); final AtomicReference<SnapshotShardSizeInfo> snapshotShardSizeInfoRef = new AtomicReference<>(SnapshotShardSizeInfo.EMPTY); final SnapshotsInfoService snapshotsInfoService = snapshotShardSizeInfoRef::get; final AllocationService strategy = new AllocationService(deciders, new TestGatewayAllocator(), new BalancedShardsAllocator(Settings.EMPTY), clusterInfoService, snapshotsInfoService); final Metadata metadata = Metadata.builder() .put(IndexMetadata.builder("test") .settings(settings(Version.CURRENT)) .numberOfShards(1) .numberOfReplicas(0) .putInSyncAllocationIds(0, Set.of(AllocationId.newInitializing().getId())) ).build(); final RoutingTable routingTable = RoutingTable.builder() .addAsNewRestore( metadata.index("test"), new RecoverySource.SnapshotRecoverySource("_restore_uuid", snapshot, Version.CURRENT, indexId), new IntHashSet() ).build(); final ImmutableOpenMap.Builder<ShardId, RestoreInProgress.ShardRestoreStatus> shards = ImmutableOpenMap.builder(); shards.put(shardId, new RestoreInProgress.ShardRestoreStatus("node1")); final RestoreInProgress.Builder restores = new RestoreInProgress.Builder() .add(new RestoreInProgress.Entry("_restore_uuid", snapshot, RestoreInProgress.State.INIT, List.of("test"), shards.build())); ClusterState clusterState = ClusterState.builder(new ClusterName(getTestName())) .metadata(metadata) .routingTable(routingTable) .putCustom(RestoreInProgress.TYPE, restores.build()) .nodes(DiscoveryNodes.builder() .add(newNode("node1")) .add(newNode("node2")) // node2 is added because DiskThresholdDecider automatically ignore single-node clusters ).build(); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).stream().map(ShardRouting::unassignedInfo) .allMatch(unassignedInfo -> Reason.NEW_INDEX_RESTORED.equals(unassignedInfo.getReason())), is(true)); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).stream().map(ShardRouting::unassignedInfo) .allMatch(unassignedInfo -> AllocationStatus.NO_ATTEMPT.equals(unassignedInfo.getLastAllocationStatus())), is(true)); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1)); // reroute triggers snapshot shard size fetching clusterState = strategy.reroute(clusterState, "reroute"); logShardStates(clusterState); // shard cannot be assigned yet as the snapshot shard size is unknown assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).stream().map(ShardRouting::unassignedInfo) .allMatch(unassignedInfo -> AllocationStatus.FETCHING_SHARD_DATA.equals(unassignedInfo.getLastAllocationStatus())), is(true)); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1)); final SnapshotShard snapshotShard = new SnapshotShard(snapshot, indexId, shardId); final ImmutableOpenMap.Builder<SnapshotShard, Long> snapshotShardSizes = ImmutableOpenMap.builder(); final boolean shouldAllocate; if (randomBoolean()) { logger.info("--> simulating snapshot shards size retrieval success"); snapshotShardSizes.put(snapshotShard, shardSizeInBytes); logger.info("--> shard allocation depends on its size"); shouldAllocate = shardSizeInBytes < usages.get("node1").getFreeBytes(); } else { logger.info("--> simulating snapshot shards size retrieval failure"); snapshotShardSizes.put(snapshotShard, ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE); logger.info("--> shard is always allocated when its size could not be retrieved"); shouldAllocate = true; } snapshotShardSizeInfoRef.set(new SnapshotShardSizeInfo(snapshotShardSizes.build())); // reroute uses the previous snapshot shard size clusterState = startInitializingShardsAndReroute(strategy, clusterState); logShardStates(clusterState); assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(shouldAllocate ? 0 : 1)); assertThat(clusterState.getRoutingNodes().shardsWithState("test", INITIALIZING, STARTED).size(), equalTo(shouldAllocate ? 1 : 0)); }	nit: i think i would have had an easier time following this test if we had moved this to where it's actually first used after the assertions on the cs we construct below to like: java final atomicreference<snapshotshardsizeinfo> snapshotshardsizeinforef = new atomicreference<>(snapshotshardsizeinfo.empty); final allocationservice strategy = new allocationservice(deciders, new testgatewayallocator(), new balancedshardsallocator(settings.empty), clusterinfoservice, snapshotshardsizeinforef::get); // reroute triggers snapshot shard size fetching clusterstate = strategy.reroute(clusterstate, "reroute"); logshardstates(clusterstate);
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (customs.isEmpty() == false) { throw new IllegalArgumentException("Custom data type is no longer supported in index template [" + customs + "]"); } builder.field("index_patterns", indexPatterns); builder.field("order", order); if (version != null) { builder.field("version", version); } builder.startObject("settings"); settings.toXContent(builder, params); builder.endObject(); builder.startObject("mappings"); for (Map.Entry<String, String> entry : mappings.entrySet()) { Map<String, Object> mapping = XContentHelper.convertToMap(new BytesArray(entry.getValue()), false).v2(); builder.field(entry.getKey(), mapping); } builder.endObject(); builder.startObject("aliases"); for (Alias alias : aliases) { alias.toXContent(builder, params); } builder.endObject(); return builder; }	why throw exception when calling toxcontent? if customs are not supported anymore why can users set them but not serialize them to json?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (customs.isEmpty() == false) { throw new IllegalArgumentException("Custom data type is no longer supported in index template [" + customs + "]"); } builder.field("index_patterns", indexPatterns); builder.field("order", order); if (version != null) { builder.field("version", version); } builder.startObject("settings"); settings.toXContent(builder, params); builder.endObject(); builder.startObject("mappings"); for (Map.Entry<String, String> entry : mappings.entrySet()) { Map<String, Object> mapping = XContentHelper.convertToMap(new BytesArray(entry.getValue()), false).v2(); builder.field(entry.getKey(), mapping); } builder.endObject(); builder.startObject("aliases"); for (Alias alias : aliases) { alias.toXContent(builder, params); } builder.endObject(); return builder; }	could of things here: we should not use a deprecated method, i think it is guaranteed that the request holds json (we convert it on the setter methods), hence we should not need content-type auto-detection. furthermore, we don't need to parse stuff back into a map to the write it back to the builder. i would try and do something like this: builder.field(entry.getkey()); xcontentparser parser = jsonxcontent.jsonxcontent.createparser(namedxcontentregistry.empty, deprecationhandler .throw_unsupported_operation, entry.getvalue()); builder.copycurrentstructure(parser);
private PutIndexTemplateRequest randomPutIndexTemplateRequest() throws IOException { PutIndexTemplateRequest request = new PutIndexTemplateRequest(); request.name("test"); if (randomBoolean()){ request.version(randomInt()); } if (randomBoolean()){ request.order(randomInt()); } request.patterns(Arrays.asList(generateRandomStringArray(20, 100, false, false))); int numAlias = between(0, 5); for (int i = 0; i < numAlias; i++) { Alias alias = new Alias(randomRealisticUnicodeOfLengthBetween(1, 10)); if (randomBoolean()) { alias.indexRouting(randomRealisticUnicodeOfLengthBetween(1, 10)); } if (randomBoolean()) { alias.searchRouting(randomRealisticUnicodeOfLengthBetween(1, 10)); } request.alias(alias); } if (randomBoolean()) { request.mapping("doc", XContentFactory.jsonBuilder().startObject() .startObject("doc").startObject("properties") .startObject("field-" + randomInt()).field("type", randomFrom("keyword", "text")).endObject() .endObject().endObject().endObject()); } if (randomBoolean()){ request.settings(Settings.builder().put("setting1", randomLong()).put("setting2", randomTimeValue()).build()); } return request; }	i would make this class extend abstractxcontenttestcase, then your randomputindextemplaterequest would become createtestinstance, and @override protected putindextemplaterequest doparseinstance(xcontentparser parser) throws ioexception { return new putindextemplaterequest().source(parser.map()); } @override protected boolean supportsunknownfields() { return false; } @override protected void assertequalinstances(putindextemplaterequest expected, putindextemplaterequest parsed) { assertnotsame(expected, parsed); assertthat(parsed.version(), equalto(expected.version())); assertthat(parsed.order(), equalto(expected.order())); assertthat(parsed.patterns(), equalto(expected.patterns())); assertthat(parsed.aliases(), equalto(expected.aliases())); assertthat(parsed.mappings(), equalto(expected.mappings())); assertthat(parsed.settings(), equalto(expected.settings())); }
public void testBooleanStrictQuery() throws Exception { Exception e = expectThrows(Exception.class, () -> client().prepareSearch("test").setQuery(queryStringQuery("foo").field("f_bool")).get()); System.out.println(e); assertThat(ExceptionsHelper.unwrap(e, IllegalArgumentException.class).getMessage(), containsString("Can't parse boolean value [foo], expected [true] or [false]")); }	is there any reason we are using system.out.println here? or did you use for debugging?
@Override public double parseDouble(String value, boolean roundUp, LongSupplier now) { Number n; try { n = format.parse(value); } catch (ParseException e) { throw new RuntimeException("Cannot parse the value [" + value + "] using the pattern [" + pattern + "]"); } return n.doubleValue(); }	maybe add , e just so we don't lose it?
private void monitorFSHealth() { Set<Path> currentUnhealthyPaths = null; brokenLock = false; Path[] paths = null; try { paths = nodeEnv.nodeDataPaths(); } catch (IllegalStateException e) { logger.error("Lock assertions failed due to", e); brokenLock = true; return; } for (Path path : paths) { long executionStartTime = currentTimeMillisSupplier.getAsLong(); try { if (Files.exists(path)) { Path tempDataPath = path.resolve(TEMP_FILE_NAME); Files.deleteIfExists(tempDataPath); try (OutputStream os = Files.newOutputStream(tempDataPath, StandardOpenOption.CREATE_NEW)) { os.write(byteToWrite); IOUtils.fsync(tempDataPath, false); } Files.delete(tempDataPath); final long elapsedTime = currentTimeMillisSupplier.getAsLong() - executionStartTime; if (elapsedTime > slowPathLoggingThreshold.millis()) { logger.warn("health check of [{}] took [{}ms] which is above the warn threshold of [{}]", path, elapsedTime, slowPathLoggingThreshold); } } } catch (Exception ex) { logger.error(new ParameterizedMessage("health check of [{}] failed", path), ex); if (currentUnhealthyPaths == null) { currentUnhealthyPaths = new HashSet<>(1); } currentUnhealthyPaths.add(path); } } unhealthyPaths = currentUnhealthyPaths; }	clearing this flag here may mean the node reports itself as healthy even though it hasn't actually passed this health check yet. i think we should clear this flag only after setting unhealthypaths at the very bottom of this method.
private void monitorFSHealth() { Set<Path> currentUnhealthyPaths = null; brokenLock = false; Path[] paths = null; try { paths = nodeEnv.nodeDataPaths(); } catch (IllegalStateException e) { logger.error("Lock assertions failed due to", e); brokenLock = true; return; } for (Path path : paths) { long executionStartTime = currentTimeMillisSupplier.getAsLong(); try { if (Files.exists(path)) { Path tempDataPath = path.resolve(TEMP_FILE_NAME); Files.deleteIfExists(tempDataPath); try (OutputStream os = Files.newOutputStream(tempDataPath, StandardOpenOption.CREATE_NEW)) { os.write(byteToWrite); IOUtils.fsync(tempDataPath, false); } Files.delete(tempDataPath); final long elapsedTime = currentTimeMillisSupplier.getAsLong() - executionStartTime; if (elapsedTime > slowPathLoggingThreshold.millis()) { logger.warn("health check of [{}] took [{}ms] which is above the warn threshold of [{}]", path, elapsedTime, slowPathLoggingThreshold); } } } catch (Exception ex) { logger.error(new ParameterizedMessage("health check of [{}] failed", path), ex); if (currentUnhealthyPaths == null) { currentUnhealthyPaths = new HashSet<>(1); } currentUnhealthyPaths.add(path); } } unhealthyPaths = currentUnhealthyPaths; }	minor wording nit: suggestion logger.error("health check failed", e);
public void updateBufferSize(ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) { Engine engine = engineUnsafe(); if (engine == null) { logger.debug("updateBufferSize: engine is closed; skipping"); return; } final EngineConfig config = engineConfig; final ByteSizeValue preValue = config.getIndexingBufferSize(); config.setIndexingBufferSize(shardIndexingBufferSize); // update engine if it is already started. if (preValue.bytes() != shardIndexingBufferSize.bytes()) { if (shardIndexingBufferSize == EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER) { // it's inactive: make sure we do a refresh / full IW flush in this case, since the memory // changes only after a "data" change has happened to the writer // the index writer lazily allocates memory and a refresh will clean it all up. logger.debug("updating index_buffer_size from [{}] to (inactive) [{}]", preValue, shardIndexingBufferSize); try { refresh("update index buffer"); } catch (Throwable e) { logger.warn("failed to refresh after setting shard to inactive", e); } } else { logger.debug("updating index_buffer_size from [{}] to [{}]", preValue, shardIndexingBufferSize); } // so we push changes these changes down to IndexWriter: engine.onSettingsChanged(); } engine.getTranslog().updateBuffer(shardTranslogBufferSize); }	double checking this is ok - the previous code first did the settings update on the iw, then hit refresh... also, refresh is optional (mayberefreshblocking) - what happens if there is a large buffer but no data to refresh?
*/ private EnumSet<ShardStatusChangeType> purgeDeletedAndClosedShards() { EnumSet<ShardStatusChangeType> changes = EnumSet.noneOf(ShardStatusChangeType.class); Iterator<ShardId> statusShardIdIterator = shardsIndicesStatus.keySet().iterator(); while (statusShardIdIterator.hasNext()) { ShardId statusShardId = statusShardIdIterator.next(); IndexService indexService = indicesService.indexService(statusShardId.getIndex()); boolean remove; if (indexService == null) { remove = true; } else { IndexShard indexShard = indexService.shard(statusShardId.id()); if (indexShard == null) { remove = true; } else { remove = !CAN_UPDATE_INDEX_BUFFER_STATES.contains(indexShard.state()); } } if (remove) { changes.add(ShardStatusChangeType.DELETED); statusShardIdIterator.remove(); } } return changes; }	trace? this runs very frequently..
public void testSegments() throws Exception { try (Store store = createStore(); Engine engine = createEngine(defaultSettings, store, createTempDir(), new MergeSchedulerConfig(defaultSettings), NoMergePolicy.INSTANCE)) { List<Segment> segments = engine.segments(false); assertThat(segments.isEmpty(), equalTo(true)); assertThat(engine.segmentsStats().getCount(), equalTo(0l)); assertThat(engine.segmentsStats().getMemoryInBytes(), equalTo(0l)); final boolean defaultCompound = defaultSettings.getAsBoolean(EngineConfig.INDEX_COMPOUND_ON_FLUSH, true); // create a doc and refresh ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null); engine.create(new Engine.Create(newUid("1"), doc)); ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null); engine.create(new Engine.Create(newUid("2"), doc2)); engine.refresh("test"); segments = engine.segments(false); assertThat(segments.size(), equalTo(1)); SegmentsStats stats = engine.segmentsStats(); assertThat(stats.getCount(), equalTo(1l)); assertThat(stats.getTermsMemoryInBytes(), greaterThan(0l)); assertThat(stats.getStoredFieldsMemoryInBytes(), greaterThan(0l)); assertThat(stats.getTermVectorsMemoryInBytes(), equalTo(0l)); assertThat(stats.getNormsMemoryInBytes(), greaterThan(0l)); assertThat(stats.getDocValuesMemoryInBytes(), greaterThan(0l)); assertThat(segments.get(0).isCommitted(), equalTo(false)); assertThat(segments.get(0).isSearch(), equalTo(true)); assertThat(segments.get(0).getNumDocs(), equalTo(2)); assertThat(segments.get(0).getDeletedDocs(), equalTo(0)); assertThat(segments.get(0).isCompound(), equalTo(defaultCompound)); assertThat(segments.get(0).ramTree, nullValue()); engine.flush(); segments = engine.segments(false); assertThat(segments.size(), equalTo(1)); assertThat(engine.segmentsStats().getCount(), equalTo(1l)); assertThat(segments.get(0).isCommitted(), equalTo(true)); assertThat(segments.get(0).isSearch(), equalTo(true)); assertThat(segments.get(0).getNumDocs(), equalTo(2)); assertThat(segments.get(0).getDeletedDocs(), equalTo(0)); assertThat(segments.get(0).isCompound(), equalTo(defaultCompound)); engine.config().setCompoundOnFlush(false); engine.onSettingsChanged(); ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null); engine.create(new Engine.Create(newUid("3"), doc3)); engine.refresh("test"); segments = engine.segments(false); assertThat(segments.size(), equalTo(2)); assertThat(engine.segmentsStats().getCount(), equalTo(2l)); assertThat(engine.segmentsStats().getTermsMemoryInBytes(), greaterThan(stats.getTermsMemoryInBytes())); assertThat(engine.segmentsStats().getStoredFieldsMemoryInBytes(), greaterThan(stats.getStoredFieldsMemoryInBytes())); assertThat(engine.segmentsStats().getTermVectorsMemoryInBytes(), equalTo(0l)); assertThat(engine.segmentsStats().getNormsMemoryInBytes(), greaterThan(stats.getNormsMemoryInBytes())); assertThat(engine.segmentsStats().getDocValuesMemoryInBytes(), greaterThan(stats.getDocValuesMemoryInBytes())); assertThat(segments.get(0).getGeneration() < segments.get(1).getGeneration(), equalTo(true)); assertThat(segments.get(0).isCommitted(), equalTo(true)); assertThat(segments.get(0).isSearch(), equalTo(true)); assertThat(segments.get(0).getNumDocs(), equalTo(2)); assertThat(segments.get(0).getDeletedDocs(), equalTo(0)); assertThat(segments.get(0).isCompound(), equalTo(defaultCompound)); assertThat(segments.get(1).isCommitted(), equalTo(false)); assertThat(segments.get(1).isSearch(), equalTo(true)); assertThat(segments.get(1).getNumDocs(), equalTo(1)); assertThat(segments.get(1).getDeletedDocs(), equalTo(0)); assertThat(segments.get(1).isCompound(), equalTo(false)); engine.delete(new Engine.Delete("test", "1", newUid("1"))); engine.refresh("test"); segments = engine.segments(false); assertThat(segments.size(), equalTo(2)); assertThat(engine.segmentsStats().getCount(), equalTo(2l)); assertThat(segments.get(0).getGeneration() < segments.get(1).getGeneration(), equalTo(true)); assertThat(segments.get(0).isCommitted(), equalTo(true)); assertThat(segments.get(0).isSearch(), equalTo(true)); assertThat(segments.get(0).getNumDocs(), equalTo(1)); assertThat(segments.get(0).getDeletedDocs(), equalTo(1)); assertThat(segments.get(0).isCompound(), equalTo(defaultCompound)); assertThat(segments.get(1).isCommitted(), equalTo(false)); assertThat(segments.get(1).isSearch(), equalTo(true)); assertThat(segments.get(1).getNumDocs(), equalTo(1)); assertThat(segments.get(1).getDeletedDocs(), equalTo(0)); assertThat(segments.get(1).isCompound(), equalTo(false)); engine.config().setCompoundOnFlush(true); engine.onSettingsChanged(); ParsedDocument doc4 = testParsedDocument("4", "4", "test", null, -1, -1, testDocumentWithTextField(), B_3, null); engine.create(new Engine.Create(newUid("4"), doc4)); engine.refresh("test"); segments = engine.segments(false); assertThat(segments.size(), equalTo(3)); assertThat(engine.segmentsStats().getCount(), equalTo(3l)); assertThat(segments.get(0).getGeneration() < segments.get(1).getGeneration(), equalTo(true)); assertThat(segments.get(0).isCommitted(), equalTo(true)); assertThat(segments.get(0).isSearch(), equalTo(true)); assertThat(segments.get(0).getNumDocs(), equalTo(1)); assertThat(segments.get(0).getDeletedDocs(), equalTo(1)); assertThat(segments.get(0).isCompound(), equalTo(defaultCompound)); assertThat(segments.get(1).isCommitted(), equalTo(false)); assertThat(segments.get(1).isSearch(), equalTo(true)); assertThat(segments.get(1).getNumDocs(), equalTo(1)); assertThat(segments.get(1).getDeletedDocs(), equalTo(0)); assertThat(segments.get(1).isCompound(), equalTo(false)); assertThat(segments.get(2).isCommitted(), equalTo(false)); assertThat(segments.get(2).isSearch(), equalTo(true)); assertThat(segments.get(2).getNumDocs(), equalTo(1)); assertThat(segments.get(2).getDeletedDocs(), equalTo(0)); assertThat(segments.get(2).isCompound(), equalTo(true)); } }	wondering - why is this needed now?
public void accept(TcpChannel channel, Exception e) { if (!lifecycle.started()) { // just close and ignore - we are already stopped and just need to make sure we release all resources CloseableChannel.closeChannel(channel); } else if (SSLExceptionHelper.isNotSslRecordException(e)) { logger.warn("received plaintext traffic on an encrypted channel, closing connection {}", channel); CloseableChannel.closeChannel(channel); } else if (SSLExceptionHelper.isCloseDuringHandshakeException(e)) { if (logger.isTraceEnabled()) { logger.trace(new ParameterizedMessage("connection {} closed during ssl handshake", channel), e); } else { logger.debug("connection {} closed during handshake", channel); } CloseableChannel.closeChannel(channel); } else if (SSLExceptionHelper.isReceivedCertificateUnknownException(e)) { if (logger.isTraceEnabled()) { logger.trace(new ParameterizedMessage("client did not trust server's certificate, closing connection {}", channel), e); } else { logger.warn("client did not trust this server's certificate, closing connection {}", channel); } CloseableChannel.closeChannel(channel); } else { fallback.accept(channel, e); } }	is it okay to change this? the @elastic/es-security team when they added this seemed to prefer the trace then fallback to warn style. it seems odd that this pr modifies this just for this specific log message?
public void testRegisterPipelineAggregation() { SearchModule module = new SearchModule(Settings.EMPTY, false, singletonList(new SearchPlugin() { @Override public List<PipelineAggregationSpec> getPipelineAggregations() { return singletonList(new PipelineAggregationSpec("test", TestPipelineAggregationBuilder::new, TestPipelineAggregator::new, TestPipelineAggregationBuilder::fromXContent)); } })); assertThat( module.getNamedXContents().stream() .filter(entry -> entry.categoryClass.equals(BaseAggregationBuilder.class) && entry.name.match("test", LoggingDeprecationHandler.INSTANCE)) .collect(toList()), hasSize(1)); }	these might be better if they just fail rather than go to the depercationlogger. this is fine though.
@Override public long repositoryStateId() { return repositoryStateId; } } public enum State { WAITING((byte) 0), META_DATA((byte) 1); private final byte value; State(byte value) { this.value = value; } public static State fromValue(byte value) { switch (value) { case 0: return WAITING; case 1: return META_DATA; default: throw new IllegalArgumentException("No snapshot delete state for value [" + value + "]"); } }	i would prefer a static readfrom method and implements writeable on this class, so that we have the serialization covered here directly
@Override public long repositoryStateId() { return repositoryStateId; } } public enum State { WAITING((byte) 0), META_DATA((byte) 1); private final byte value; State(byte value) { this.value = value; } public static State fromValue(byte value) { switch (value) { case 0: return WAITING; case 1: return META_DATA; default: throw new IllegalArgumentException("No snapshot delete state for value [" + value + "]"); } }	can you add some docs here as to what these states mean?
public SnapshotDeletionsInProgress withRemovedEntry(String deleteUUID) { List<Entry> updatedEntries = new ArrayList<>(entries.size() - 1); boolean removed = false; for (Entry entry : entries) { if (entry.uuid().equals(deleteUUID)) { removed = true; } else { updatedEntries.add(entry); } } return removed ? new SnapshotDeletionsInProgress(updatedEntries) : this; }	if two or more deletes share overlapping snapshots but couldn't be batched into one we need to "clean up" deleted snapshots from existing deletes so that the remaining snapshots don't fail trying to delete already deleted snapshots.
public SnapshotDeletionsInProgress withRemovedSnapshotIds(String repository, Collection<SnapshotId> snapshotIds) { boolean changed = false; List<Entry> updatedEntries = new ArrayList<>(entries.size()); for (Entry entry : entries) { if (entry.repository().equals(repository)) { final List<SnapshotId> updatedSnapshotIds = new ArrayList<>(entry.getSnapshots()); if (updatedSnapshotIds.removeAll(snapshotIds)) { changed = true; updatedEntries.add(entry.withSnapshots(updatedSnapshotIds)); } else { updatedEntries.add(entry); } } else { updatedEntries.add(entry); } } return changed ? new SnapshotDeletionsInProgress(updatedEntries) : this; }	if we fail loading repository data for a repository but have queued up deletes we just drop all the deletes for the repo on failure.
private static boolean assertConsistentEntries(List<Entry> entries) { final Map<String, Set<ShardId>> startedShardsByRepo = new HashMap<>(); for (Entry entry : entries) { for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shard : entry.shards()) { final ShardState shardState = shard.value.state(); if (shardState == ShardState.INIT || shardState == ShardState.ABORTED) { assert startedShardsByRepo.computeIfAbsent(entry.repository(), k -> new HashSet<>()).add(shard.key) : "Found duplicate shard assignments in " + entries; } } } return true; }	this got me confused a bit, because we do not have a started state on shardstate and because this talks about startedshardsbyrepo. i wonder if we should add a new method ispossiblyactivelyrunning to shardstate, which only returns true for init and aborted, and use that here (with similar naming for the hashmap)
* @param inFlightIds name to index mapping for currently in-flight snapshots not yet in the repository data to fall back to */ public List<IndexId> resolveNewIndices(List<String> indicesToResolve, Map<String, IndexId> inFlightIds) { List<IndexId> snapshotIndices = new ArrayList<>(); for (String index : indicesToResolve) { final IndexId indexId; if (indices.containsKey(index)) { indexId = indices.get(index); } else { if (inFlightIds.containsKey(index)) { indexId = inFlightIds.get(index); } else { indexId = new IndexId(index, UUIDs.randomBase64UUID()); } } snapshotIndices.add(indexId); } return snapshotIndices; }	instead of doing the lookup twice, perhaps just get the value and check if null
private ClusterState updateRepositoryGenerations(ClusterState state, long oldGen, long newGen) { final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE); final String repoName = metadata.name(); final List<SnapshotsInProgress.Entry> snapshotEntries; if (snapshotsInProgress == null) { snapshotEntries = List.of(); } else { snapshotEntries = new ArrayList<>(); for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) { if (entry.repository().equals(repoName) && entry.repositoryStateId() == oldGen) { snapshotEntries.add(entry.withRepoGen(newGen)); } else { snapshotEntries.add(entry); } } } final SnapshotDeletionsInProgress snapshotDeletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE); final List<SnapshotDeletionsInProgress.Entry> deletionEntries; if (snapshotDeletionsInProgress == null) { deletionEntries = List.of(); } else { deletionEntries = new ArrayList<>(); for (SnapshotDeletionsInProgress.Entry entry : snapshotDeletionsInProgress.getEntries()) { if (entry.repositoryStateId() == oldGen) { deletionEntries.add(entry.withRepoGen(newGen)); } else { deletionEntries.add(entry); } } } return ClusterState.builder(state).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(List.copyOf(snapshotEntries))) .putCustom(SnapshotDeletionsInProgress.TYPE, new SnapshotDeletionsInProgress(List.copyOf(deletionEntries))) .build(); }	what are the situations where we expect entry.repositorystateid() != oldgen? is there something we can assert here?
private long bestGeneration(Collection<? extends RepositoryOperation> operations) { final String repoName = metadata.name(); return operations.stream().filter(e -> e.repository().equals(repoName)).mapToLong(RepositoryOperation::repositoryStateId) .max().orElse(RepositoryData.EMPTY_REPO_GEN); }	how do we expect the repositorystateid to compare across the operations? will it always be the same?
@Override public ClusterState execute(ClusterState currentState) { SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE); final Version minNodeVersion = currentState.nodes().getMinNodeVersion(); if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)) { if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) { throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)), "cannot delete - another snapshot is currently being deleted in [" + deletionsInProgress + "]"); } } final RepositoryCleanupInProgress repositoryCleanupInProgress = currentState.custom(RepositoryCleanupInProgress.TYPE); if (repositoryCleanupInProgress != null && repositoryCleanupInProgress.hasCleanupInProgress()) { throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)), "cannot delete snapshots while a repository cleanup is in-progress in [" + repositoryCleanupInProgress + "]"); } RestoreInProgress restoreInProgress = currentState.custom(RestoreInProgress.TYPE); if (restoreInProgress != null) { // don't allow snapshot deletions while a restore is taking place, // otherwise we could end up deleting a snapshot that is being restored // and the files the restore depends on would all be gone for (RestoreInProgress.Entry entry : restoreInProgress) { if (repoName.equals(entry.snapshot().getRepository()) && snapshotIds.contains(entry.snapshot().getSnapshotId())) { throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)), "cannot delete snapshot during a restore in progress in [" + restoreInProgress + "]"); } } } final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); final SnapshotsInProgress updatedSnapshots; if (snapshots == null) { updatedSnapshots = new SnapshotsInProgress(); } else if (minNodeVersion.onOrAfter(FULL_CONCURRENCY_VERSION)) { updatedSnapshots = new SnapshotsInProgress(snapshots.entries().stream() .map(existing -> { // snapshot is started - mark every non completed shard as aborted if (existing.state() == State.STARTED && snapshotIds.contains(existing.snapshot().getSnapshotId())) { final ImmutableOpenMap<ShardId, ShardSnapshotStatus> abortedShards = abortEntry(existing); final boolean isCompleted = completed(abortedShards.values()); final SnapshotsInProgress.Entry abortedEntry = new SnapshotsInProgress.Entry( existing, isCompleted ? State.SUCCESS : State.ABORTED, abortedShards, "Snapshot was aborted by deletion"); if (isCompleted) { completedSnapshots.add(abortedEntry); } return abortedEntry; } return existing; }).collect(Collectors.toUnmodifiableList())); } else { if (snapshots.entries().isEmpty() == false) { // However other snapshots are running - cannot continue throw new ConcurrentSnapshotExecutionException( repoName, snapshotIds.toString(), "another snapshot is currently running cannot delete"); } updatedSnapshots = snapshots; } SnapshotDeletionsInProgress.Entry replacedEntry = null; // add the snapshot deletion to the cluster state if (deletionsInProgress != null) { replacedEntry = deletionsInProgress.getEntries().stream().filter(entry -> entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.WAITING) .findFirst().orElse(null); if (replacedEntry == null) { final Optional<SnapshotDeletionsInProgress.Entry> foundDuplicate = deletionsInProgress.getEntries().stream().filter(entry -> entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.META_DATA && entry.getSnapshots().containsAll(snapshotIds)).findFirst(); if (foundDuplicate.isPresent()) { newDelete = foundDuplicate.get(); return currentState; } } } if (replacedEntry == null) { newDelete = new SnapshotDeletionsInProgress.Entry( snapshotIds, repoName, threadPool.absoluteTimeInMillis(), repositoryStateId, updatedSnapshots.entries().stream().filter(entry -> repoName.equals(entry.repository())) .noneMatch(SnapshotsService::isWritingToRepository) ? SnapshotDeletionsInProgress.State.META_DATA : SnapshotDeletionsInProgress.State.WAITING ); } else { newDelete = replacedEntry.withAddedSnapshots(snapshotIds); } if (deletionsInProgress != null) { if (replacedEntry != null) { deletionsInProgress = deletionsInProgress.withRemovedEntry(replacedEntry.uuid()); } deletionsInProgress = deletionsInProgress.withAddedEntry(newDelete); } else { deletionsInProgress = SnapshotDeletionsInProgress.newInstance(newDelete); } return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletionsInProgress) .putCustom(SnapshotsInProgress.TYPE, updatedSnapshots).build(); }	this pattern is repeated quite often. i wonder if we should provide an overloaded method clusterstate.custom(string name, supplier<custom> defaultsupplier)
@Override public ClusterState execute(ClusterState currentState) { SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE); final Version minNodeVersion = currentState.nodes().getMinNodeVersion(); if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)) { if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) { throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)), "cannot delete - another snapshot is currently being deleted in [" + deletionsInProgress + "]"); } } final RepositoryCleanupInProgress repositoryCleanupInProgress = currentState.custom(RepositoryCleanupInProgress.TYPE); if (repositoryCleanupInProgress != null && repositoryCleanupInProgress.hasCleanupInProgress()) { throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)), "cannot delete snapshots while a repository cleanup is in-progress in [" + repositoryCleanupInProgress + "]"); } RestoreInProgress restoreInProgress = currentState.custom(RestoreInProgress.TYPE); if (restoreInProgress != null) { // don't allow snapshot deletions while a restore is taking place, // otherwise we could end up deleting a snapshot that is being restored // and the files the restore depends on would all be gone for (RestoreInProgress.Entry entry : restoreInProgress) { if (repoName.equals(entry.snapshot().getRepository()) && snapshotIds.contains(entry.snapshot().getSnapshotId())) { throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)), "cannot delete snapshot during a restore in progress in [" + restoreInProgress + "]"); } } } final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); final SnapshotsInProgress updatedSnapshots; if (snapshots == null) { updatedSnapshots = new SnapshotsInProgress(); } else if (minNodeVersion.onOrAfter(FULL_CONCURRENCY_VERSION)) { updatedSnapshots = new SnapshotsInProgress(snapshots.entries().stream() .map(existing -> { // snapshot is started - mark every non completed shard as aborted if (existing.state() == State.STARTED && snapshotIds.contains(existing.snapshot().getSnapshotId())) { final ImmutableOpenMap<ShardId, ShardSnapshotStatus> abortedShards = abortEntry(existing); final boolean isCompleted = completed(abortedShards.values()); final SnapshotsInProgress.Entry abortedEntry = new SnapshotsInProgress.Entry( existing, isCompleted ? State.SUCCESS : State.ABORTED, abortedShards, "Snapshot was aborted by deletion"); if (isCompleted) { completedSnapshots.add(abortedEntry); } return abortedEntry; } return existing; }).collect(Collectors.toUnmodifiableList())); } else { if (snapshots.entries().isEmpty() == false) { // However other snapshots are running - cannot continue throw new ConcurrentSnapshotExecutionException( repoName, snapshotIds.toString(), "another snapshot is currently running cannot delete"); } updatedSnapshots = snapshots; } SnapshotDeletionsInProgress.Entry replacedEntry = null; // add the snapshot deletion to the cluster state if (deletionsInProgress != null) { replacedEntry = deletionsInProgress.getEntries().stream().filter(entry -> entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.WAITING) .findFirst().orElse(null); if (replacedEntry == null) { final Optional<SnapshotDeletionsInProgress.Entry> foundDuplicate = deletionsInProgress.getEntries().stream().filter(entry -> entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.META_DATA && entry.getSnapshots().containsAll(snapshotIds)).findFirst(); if (foundDuplicate.isPresent()) { newDelete = foundDuplicate.get(); return currentState; } } } if (replacedEntry == null) { newDelete = new SnapshotDeletionsInProgress.Entry( snapshotIds, repoName, threadPool.absoluteTimeInMillis(), repositoryStateId, updatedSnapshots.entries().stream().filter(entry -> repoName.equals(entry.repository())) .noneMatch(SnapshotsService::isWritingToRepository) ? SnapshotDeletionsInProgress.State.META_DATA : SnapshotDeletionsInProgress.State.WAITING ); } else { newDelete = replacedEntry.withAddedSnapshots(snapshotIds); } if (deletionsInProgress != null) { if (replacedEntry != null) { deletionsInProgress = deletionsInProgress.withRemovedEntry(replacedEntry.uuid()); } deletionsInProgress = deletionsInProgress.withAddedEntry(newDelete); } else { deletionsInProgress = SnapshotDeletionsInProgress.newInstance(newDelete); } return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletionsInProgress) .putCustom(SnapshotsInProgress.TYPE, updatedSnapshots).build(); }	this is somewhat stupid, but i made this work the same way shard state updates are processed right now (setting success if all the shards completed). a follow-up to simplify and speed things up would be to simply remove snapshots that haven't done any work yet here right away instead of going through the redundant cycle of finalizing them and then deleting them again right away.
@Override public ClusterTasksResult<UpdateIndexShardSnapshotStatusRequest> execute(ClusterState currentState, List<UpdateIndexShardSnapshotStatusRequest> tasks) { final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); if (snapshots != null) { int changedCount = 0; final List<SnapshotsInProgress.Entry> entries = new ArrayList<>(); final Map<String, Set<ShardId>> reusedShardIdsByRepo = new HashMap<>(); for (SnapshotsInProgress.Entry entry : snapshots.entries()) { ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder(); boolean updated = false; for (UpdateIndexShardSnapshotStatusRequest updateSnapshotState : tasks) { final ShardId finishedShardId = updateSnapshotState.shardId(); if (entry.snapshot().equals(updateSnapshotState.snapshot())) { logger.trace("[{}] Updating shard [{}] with status [{}]", updateSnapshotState.snapshot(), finishedShardId, updateSnapshotState.status().state()); if (updated == false) { shards.putAll(entry.shards()); updated = true; } shards.put(finishedShardId, updateSnapshotState.status()); changedCount++; } else { final Set<ShardId> reusedShardIds = reusedShardIdsByRepo.computeIfAbsent(entry.repository(), k -> new HashSet<>()); if (entry.state().completed() == false && reusedShardIds.contains(finishedShardId) == false && entry.shards().keys().contains(finishedShardId)) { final ShardSnapshotStatus existingStatus = entry.shards().get(finishedShardId); if (existingStatus.state() != ShardState.WAITING) { continue; } if (updated == false) { shards.putAll(entry.shards()); updated = true; } final ShardSnapshotStatus finishedStatus = updateSnapshotState.status(); logger.trace("Starting [{}] on [{}] with generation [{}]", finishedShardId, finishedStatus.nodeId(), finishedStatus.generation()); shards.put(finishedShardId, new ShardSnapshotStatus(finishedStatus.nodeId(), finishedStatus.generation())); reusedShardIds.add(finishedShardId); } } } if (updated) { if (completed(shards.values()) == false) { entries.add(new SnapshotsInProgress.Entry(entry, shards.build())); } else { // Snapshot is finished - mark it as done // TODO: Add PARTIAL_SUCCESS status? SnapshotsInProgress.Entry updatedEntry = new SnapshotsInProgress.Entry(entry, State.SUCCESS, shards.build()); entries.add(updatedEntry); } } else { entries.add(entry); } } if (changedCount > 0) { logger.trace("changed cluster state triggered by {} snapshot state updates", changedCount); return ClusterTasksResult.<UpdateIndexShardSnapshotStatusRequest>builder().successes(tasks) .build(ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(unmodifiableList(entries))).build()); } } return ClusterTasksResult.<UpdateIndexShardSnapshotStatusRequest>builder().successes(tasks).build(currentState); } } /** * Updates the shard status on master node * * @param request update shard status request */ private void innerUpdateSnapshotState(final UpdateIndexShardSnapshotStatusRequest request, ActionListener<UpdateIndexShardSnapshotStatusResponse> listener) { logger.trace("received updated snapshot restore state [{}]", request); clusterService.submitStateUpdateTask( "update snapshot state", request, ClusterStateTaskConfig.build(Priority.NORMAL), snapshotStateExecutor, new ClusterStateTaskListener() { @Override public void onFailure(String source, Exception e) { listener.onFailure(e); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { try { listener.onResponse(new UpdateIndexShardSnapshotStatusResponse()); } finally { // Maybe this state update completed the snapshot. If we are not already ending it because of a concurrent // state update we check if its state is completed and end it if it is. if (endingSnapshots.contains(request.snapshot()) == false) { final SnapshotsInProgress snapshotsInProgress = newState.custom(SnapshotsInProgress.TYPE); final SnapshotsInProgress.Entry updatedEntry = snapshotsInProgress.snapshot(request.snapshot()); // If the entry is still in the cluster state and is completed, try finalizing the snapshot in the repo if (updatedEntry != null && updatedEntry.state().completed()) { endSnapshot(updatedEntry, newState.metadata()); } } } } }); } private class UpdateSnapshotStatusAction extends TransportMasterNodeAction<UpdateIndexShardSnapshotStatusRequest, UpdateIndexShardSnapshotStatusResponse> { UpdateSnapshotStatusAction(TransportService transportService, ClusterService clusterService, ThreadPool threadPool, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) { super(UPDATE_SNAPSHOT_STATUS_ACTION_NAME, false, transportService, clusterService, threadPool, actionFilters, UpdateIndexShardSnapshotStatusRequest::new, indexNameExpressionResolver ); } @Override protected String executor() { return ThreadPool.Names.SAME; } @Override protected UpdateIndexShardSnapshotStatusResponse read(StreamInput in) throws IOException { return new UpdateIndexShardSnapshotStatusResponse(in); } @Override protected void masterOperation(Task task, UpdateIndexShardSnapshotStatusRequest request, ClusterState state, ActionListener<UpdateIndexShardSnapshotStatusResponse> listener) { innerUpdateSnapshotState(request, listener); } @Override protected ClusterBlockException checkBlock(UpdateIndexShardSnapshotStatusRequest request, ClusterState state) { return null; } } private static final class SnapshotFinalization { private final SnapshotsInProgress.Entry entry; private final Metadata metadata; SnapshotFinalization(SnapshotsInProgress.Entry entry, Metadata metadata) { this.entry = entry; this.metadata = metadata; }	does this logic still make sure that a snapshot that's taken contains a set of docs that existed sometime between start and end of snapshot command? or will this batch up snapshots that had been running prior to the command?
public ClusterState execute(ClusterState currentState) { // check if the snapshot name already exists in the repository SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots == null ? List.of() : snapshots.entries(); if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName)) || runningSnapshots.stream().anyMatch(s -> { final Snapshot running = s.snapshot(); return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName); })) { throw new InvalidSnapshotNameException( repository.getMetadata().name(), snapshotName, "snapshot with the same name already exists"); } validate(repositoryName, snapshotName, currentState); final Version minNodeVersion = currentState.nodes().getMinNodeVersion(); SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE); boolean readyToExecute = true; if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) { if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)) { throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, "cannot snapshot while a snapshot deletion is in-progress in [" + deletionsInProgress + "]"); } else { readyToExecute = deletionsInProgress.getEntries().stream().noneMatch(entry -> entry.repository().equals(repositoryName) && entry.state() == SnapshotDeletionsInProgress.State.META_DATA); } } final RepositoryCleanupInProgress repositoryCleanupInProgress = currentState.custom(RepositoryCleanupInProgress.TYPE); if (repositoryCleanupInProgress != null && repositoryCleanupInProgress.hasCleanupInProgress()) { throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, "cannot snapshot while a repository cleanup is in-progress in [" + repositoryCleanupInProgress + "]"); } // Fail if there are any concurrently running snapshots. The only exception to this being a snapshot in INIT state from a // previous master that we can simply ignore and remove from the cluster state because we would clean it up from the // cluster state anyway in #applyClusterState. if (minNodeVersion.before(FULL_CONCURRENCY_VERSION) && snapshots != null && runningSnapshots.stream().anyMatch(entry -> entry.state() != State.INIT)) { throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, " a snapshot is already running"); } // Store newSnapshot here to be processed in clusterStateProcessed List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndexNames(currentState, request.indicesOptions(), request.indices())); logger.trace("[{}][{}] creating snapshot for indices [{}]", repositoryName, snapshotName, indices); final List<IndexId> indexIds = repositoryData.resolveNewIndices( indices, runningSnapshots.stream().filter(entry -> entry.repository().equals(repositoryName)) .flatMap(entry -> entry.indices().stream()).distinct() .collect(Collectors.toMap(IndexId::getName, Function.identity()))); final Version version = minCompatibleVersion(currentState.nodes().getMinNodeVersion(), repositoryData, null); ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards = shards(currentState, indexIds, useShardGenerations(version), repositoryData, repositoryName, readyToExecute); if (request.partial() == false) { Set<String> missing = new HashSet<>(); for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards) { if (entry.value.state() == ShardState.MISSING) { missing.add(entry.key.getIndex().getName()); } } if (missing.isEmpty() == false) { // TODO: We should just throw here instead of creating a FAILED and hence useless snapshot in the repository newEntry = new SnapshotsInProgress.Entry( new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), false, State.FAILED, indexIds, threadPool.absoluteTimeInMillis(), repositoryData.getGenId(), shards, "Indices don't have primary shards " + missing, userMeta, version); } } if (newEntry == null) { newEntry = new SnapshotsInProgress.Entry( new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), request.partial(), State.STARTED, indexIds, threadPool.absoluteTimeInMillis(), repositoryData.getGenId(), shards, null, userMeta, version); } final List<SnapshotsInProgress.Entry> newEntries = new ArrayList<>(runningSnapshots); newEntries.add(newEntry); return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(List.copyOf(newEntries))).build(); }	do we have a test somewhere that ensures that we're not resolving to new indices for concurrent snaps? (which would always work, but give us less incrementality)
*/ private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata) { if (endingSnapshots.add(entry.snapshot()) == false) { return; } final Snapshot snapshot = entry.snapshot(); if (entry.repositoryStateId() == RepositoryData.UNKNOWN_REPO_GEN) { // BwC logic to handle master fail-over from an older version that still used unknown repo generation snapshot entries logger.debug("[{}] was aborted before starting", snapshot); removeSnapshotFromClusterState(snapshot, new SnapshotException(snapshot, "Aborted on initialization"), new ActionListener<>() { @Override public void onResponse(Void aVoid) { logger.debug("Removed [{}] from cluster state", snapshot); } @Override public void onFailure(Exception e) { logger.debug(() -> new ParameterizedMessage("Failed to remove [{}] from cluster state", snapshot), e); } }); return; } synchronized (currentlyFinalizing) { if (currentlyFinalizing.add(entry.repository())) { finalizeSnapshotEntry(entry, metadata, entry.repositoryStateId()); } else { snapshotsToFinalize.computeIfAbsent(entry.repository(), k -> new LinkedList<>()) .add(new SnapshotFinalization(entry, metadata)); } } }	should we disallow concurrent operations in this case where there are still entries with unknown_repo_gen? just eliminates one more odd configuration to handle?
public ClusterState execute(ClusterState currentState) throws Exception { final Version minNodeVersion = currentState.nodes().getMinNodeVersion(); if (snapshotNames.length > 1 && minNodeVersion.before(MULTI_DELETE_VERSION)) { throw new IllegalArgumentException("Deleting multiple snapshots in a single request is only supported in version [ " + MULTI_DELETE_VERSION + "] but cluster contained node of version [" + minNodeVersion + "]"); } final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); final List<SnapshotsInProgress.Entry> snapshotEntries = findInProgressSnapshots(snapshots, snapshotNames, repositoryName); final List<SnapshotId> snapshotIds = matchingSnapshotIds( snapshotEntries.stream().map(e -> e.snapshot().getSnapshotId()).collect(Collectors.toList()), repositoryData, snapshotNames, repositoryName); if (snapshotEntries.isEmpty() || minNodeVersion.onOrAfter(SnapshotsService.FULL_CONCURRENCY_VERSION)) { deleteFromRepoTask = createDeleteStateUpdate(snapshotIds, repositoryName, repositoryData.getGenId(), Priority.NORMAL, listener); return deleteFromRepoTask.execute(currentState); } assert snapshotEntries.size() == 1 : "Expected just a single running snapshot but saw " + snapshotEntries; final SnapshotsInProgress.Entry snapshotEntry = snapshotEntries.get(0); runningSnapshot = snapshotEntry.snapshot(); final ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards; final State state = snapshotEntry.state(); final String failure; outstandingDeletes = new ArrayList<>(snapshotIds); if (state != State.INIT) { // INIT state snapshots won't ever be physically written to the repository but all other states will end up in the repo outstandingDeletes.add(runningSnapshot.getSnapshotId()); } if (state == State.INIT) { // snapshot was created by an older version of ES and never moved past INIT, remove it from the cluster state shards = snapshotEntry.shards(); assert shards.isEmpty(); failure = null; abortedDuringInit = true; } else if (state == State.STARTED) { // snapshot is started - mark every non completed shard as aborted shards = abortEntry(snapshotEntry); failure = "Snapshot was aborted by deletion"; } else { boolean hasUncompletedShards = false; // Cleanup in case a node gone missing and snapshot wasn't updated for some reason for (ObjectCursor<ShardSnapshotStatus> shardStatus : snapshotEntry.shards().values()) { // Check if we still have shard running on existing nodes if (shardStatus.value.state().completed() == false && shardStatus.value.nodeId() != null && currentState.nodes().get(shardStatus.value.nodeId()) != null) { hasUncompletedShards = true; break; } } if (hasUncompletedShards) { // snapshot is being finalized - wait for shards to complete finalization process logger.debug("trying to delete completed snapshot - should wait for shards to finalize on all nodes"); return currentState; } else { // no shards to wait for but a node is gone - this is the only case // where we force to finish the snapshot logger.debug("trying to delete completed snapshot with no finalizing shards - can delete immediately"); shards = snapshotEntry.shards(); } failure = snapshotEntry.failure(); } return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(snapshots.entries().stream() // remove init state snapshot we found from a previous master if there was one .filter(existing -> abortedDuringInit == false || existing.equals(snapshotEntry) == false) .map(existing -> { if (existing.equals(snapshotEntry)) { return new SnapshotsInProgress.Entry(snapshotEntry, State.ABORTED, shards, failure); } return existing; }).collect(Collectors.toUnmodifiableList()))).build(); }	why is there only at most one?
@Override public void executeConsistentStateUpdate(Function<RepositoryData, ClusterStateUpdateTask> createUpdateTask, Consumer<Exception> onFailure) { threadPool.generic().execute(new AbstractRunnable() { @Override protected void doRun() { final RepositoryMetadata repositoryMetadataStart = metadata; getRepositoryData(ActionListener.wrap(repositoryData -> clusterService.submitStateUpdateTask("consistent state update", new ClusterStateUpdateTask() { private ClusterStateUpdateTask updateTask; @Override public ClusterState execute(ClusterState currentState) throws Exception { // Comparing the full metadata here on purpose instead of simply comparing the safe generation. // If the safe generation has changed, then we have to reload repository data and start over. // If the pending generation has changed we are in the midst of a write operation and might pick up the // updated repository data and state on the retry. We don't want to wait for the write to finish though // because it could fail for any number of reasons so we just retry instead of waiting on the cluster state // to change in any form. if (repositoryMetadataStart.equals(getRepoMetadata(currentState))) { updateTask = createUpdateTask.apply(repositoryData); return updateTask.execute(currentState); } return currentState; } @Override public void onFailure(String source, Exception e) { if (updateTask == null) { onFailure.accept(e); } else { updateTask.onFailure(source, e); } } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { if (updateTask == null) { executeConsistentStateUpdate(createUpdateTask, onFailure); } else { updateTask.clusterStateProcessed(source, oldState, newState); } } }), onFailure)); } @Override public void onFailure(Exception e) { onFailure.accept(e); } }); }	consistent state update. can you pass in a more descriptive state update message?
@Override public Query wildcardQuery(String value, MultiTermQuery.RewriteMethod method, QueryShardContext context) { Query termQuery = termQuery(value, context); if (termQuery instanceof MatchNoDocsQuery || termQuery instanceof MatchAllDocsQuery) { return termQuery; } if (context.allowExpensiveQueries() == false) { throw new ElasticsearchException("[wildcard] queries cannot be executed when '" + ALLOW_EXPENSIVE_QUERIES.getKey() + "' is set to false."); } Term term = MappedFieldType.extractTerm(termQuery); WildcardQuery query = new WildcardQuery(term); QueryParsers.setRewriteMethod(query, method); return query; }	we don't want to create a wildcard query for the _type field, this is not needed. it should be possible to extend the new constantfieldtype now that _type can have a single value (or none) ? currently typefieldtype#termquery can return queries.newnonnestedfilter() and that's not handled in the code below. although, i don't think this is needed since we [exclude nested docs](https://github.com/elastic/elasticsearch/blob/b2ea32959103b93c03cc487ce475adde792f21e3/server/src/main/java/org/elasticsearch/search/defaultsearchcontext.java#l268) for the entire query at the end of the query parsing so it should be safe to remove. imo the _type field should solely return matchnodocsquery and matchalldocsquery, that's exactly what the constantfieldtype provides :). @romseygeek what do you think ?
public static Stream<Map<String, Object>> extractCompositeAggregationResults(CompositeAggregation agg, GroupConfig groups, Collection<AggregationBuilder> aggregationBuilders, Collection<PipelineAggregationBuilder> pipelineAggs, Map<String, String> fieldTypeMap, DataFrameIndexerTransformStats stats) { return agg.getBuckets().stream().map(bucket -> { stats.incrementNumDocuments(bucket.getDocCount()); Map<String, Object> document = new HashMap<>(); // generator to create unique but deterministic document ids, so we // - do not create duplicates if we re-run after failure // - update documents IDGenerator idGen = new IDGenerator(); groups.getGroups().keySet().forEach(destinationFieldName -> { Object value = bucket.getKey().get(destinationFieldName); idGen.add(destinationFieldName, value); updateDocument(document, destinationFieldName, value); }); List<String> aggNames = aggregationBuilders.stream().map(AggregationBuilder::getName).collect(Collectors.toList()); aggNames.addAll(pipelineAggs.stream().map(PipelineAggregationBuilder::getName).collect(Collectors.toList())); for (String aggName: aggNames) { Aggregation aggResult = bucket.getAggregations().get(aggName); // This indicates not that the value contained in the `aggResult` is null, but that the `aggResult` is not // present at all in the `bucket.getAggregations`. This could occur in the case of a `bucket_selector` agg, which // does not calculate a value, but instead manipulates other results. if (aggResult != null) { final String fieldType = fieldTypeMap.get(aggName); AggValueExtractor extractor = getExtractor(aggResult); updateDocument(document, aggName, extractor.value(aggResult, fieldType)); } } document.put(DataFrameField.DOCUMENT_ID_FIELD, idGen.getID()); return document; }); }	the only valid case of this occurring is in bucket_selector aggregations. i cannot think of any code paths where the aggregation result will be null for when the async indexer is actually running. the only two cases i can think of are where * the search request is some how mutated between being created by the pivotconfig and being executed * the search response is corrupted, or mutated somehow before being processed both of these scenarios are undefined and i am not sure they are possible.
Optional<Integer> getDesiredNumberOfReplicas(int numDataNodes) { if (enabled) { final int min = getMinReplicas(); final int max = getMaxReplicas(numDataNodes); int numberOfReplicas = numDataNodes - 1; if (numberOfReplicas < min) { numberOfReplicas = min; } else if (numberOfReplicas > max) { numberOfReplicas = max; } if (numberOfReplicas >= min && numberOfReplicas <= max) { return Optional.of(numberOfReplicas); } } return Optional.empty(); }	i'm confused by this . if min<= max, we always match this clause?
@Override public Object getProperty(List<String> path) { if (path.size() == 2 && "std_deviation_bounds".equals(path.get(0))) { String bound = path.get(1); if ("lower".equals(bound)) { return getStdDeviationBound(Bounds.LOWER); } if ("upper".equals(bound)) { return getStdDeviationBound(Bounds.UPPER); } } return super.getProperty(path); }	the disadvantage of doing the change here is that we will have two ways of getting the upper and lower bounds. in buckets path syntax for pipeline aggregations we will have std_deviation_bounds.upper and std_deviation_bounds.lower and for the aggregationpath syntax used by the terms aggregation order parameter we will still use std_upper and std_lower. i now wonder if we should instead change the name in the value(string name) method so that it is standard_deviation_bounds.upper and standard_deviation_bounds.lower? then it would be consistent all the places it is used. we could make the change in 6.0 and have a change in 5.x which accepts both versions but logs to the deprecation logger if the std_upper or std_lower version is used. @clintongormley @jpountz what do you think?
static Predicate<DiscoveryNode> getNodePredicate(Settings settings) { if (REMOTE_NODE_ATTRIBUTE.exists(settings)) { // nodes can be tagged with node.attr.remote_gateway: true to allow a node to be a gateway node for cross cluster search String attribute = REMOTE_NODE_ATTRIBUTE.get(settings); return DEFAULT_NODE_PREDICATE.and((node) -> Booleans.parseBoolean(node.getAttributes().getOrDefault(attribute, "false"))); } return DEFAULT_NODE_PREDICATE; }	question: when node attributes are configured, should we still apply the selection based on node roles? i am doing so, but i want to double check. with this change there is no way to ever have a dedicated master act as a gateway, even if node attributes are configured to do so. i think that's a good thing though. i am not sure whether we should have some protection for cases where we connect to a cluster that has only dedicated masters on compatible versions. that should be already in place as it could happen before too that no nodes are left after the selection.
private SearchContext findContext(long id) throws SearchContextMissingException { SearchContext context = activeContexts.get(id); if (context == null) { throw new SearchContextMissingException(id); } SearchOperationListener operationListener = context.indexShard().getSearchOperationListener(); try { operationListener.validateSearchContext(context); return context; } catch (Exception e) { processFailure(context, e); throw ExceptionsHelper.convertToRuntime(e); } }	this can only be a runtime exception you can just bubble it up no need to use converttoruntime
void onScrollResponse(ScrollableHitSource.AsyncResponse asyncResponse) { // lastBatchStartTime is essentially unused (see WorkerBulkByScrollTaskState.throttleWaitTime. Leaving it for now, since it seems // like a bug? onScrollResponse(System.nanoTime(), this.lastBatchSize, asyncResponse); }	this was interesting anyway since this constructor timevalue(long) is intended to be passed a value that represents milliseconds, which system#nanotime does not.
@Override public Query geoShapeQuery(Geometry shape, String fieldName, ShapeRelation relation, QueryShardContext context) { return queryProcessor.geoShapeQuery(shape, fieldName, relation, context); } } @SuppressWarnings("deprecation") public static Mapper.TypeParser PARSER = (name, node, parserContext) -> { ParametrizedFieldMapper.Builder builder; boolean ignoreMalformedByDefault = IGNORE_MALFORMED_SETTING.get(parserContext.getSettings()); boolean coerceByDefault = COERCE_SETTING.get(parserContext.getSettings()); if (LegacyGeoShapeFieldMapper.containsDeprecatedParameter(node.keySet())) { builder = new LegacyGeoShapeFieldMapper.Builder( name, parserContext.indexVersionCreated(), ignoreMalformedByDefault, coerceByDefault); } else { builder = new GeoShapeWithDocValuesFieldMapper.Builder( name, parserContext.indexVersionCreated(), ignoreMalformedByDefault, coerceByDefault); } builder.parse(name, parserContext, node); return builder; }; private final Builder builder; public GeoShapeWithDocValuesFieldMapper(String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, GeoShapeIndexer indexer, GeoShapeParser parser, Builder builder) { super(simpleName, mappedFieldType, builder.ignoreMalformed.get(), builder.coerce.get(), builder.ignoreZValue.get(), builder.orientation.get(), multiFields, copyTo, indexer, parser); this.builder = builder; } @Override protected String contentType() { return CONTENT_TYPE; } @Override public ParametrizedFieldMapper.Builder getMergeBuilder() { return new Builder( simpleName(), builder.version, builder.ignoreMalformed.getDefaultValue().value(), builder.coerce.getDefaultValue().value() ).init(this); } @Override public GeoShapeWithDocValuesFieldType fieldType() { return (GeoShapeWithDocValuesFieldType) super.fieldType(); } @Override protected void addStoredFields(ParseContext context, Geometry geometry) { }	it looks like the old code had a comment that this was intentionally a noop. i think it is worth keeping that.
@Override public boolean equals(Object other) { double oX; double oY; if (other instanceof CartesianPoint) { CartesianPoint o = (CartesianPoint)other; oX = o.getX(); oY = o.getY(); } else { return false; } if (Double.compare(oX, x) != 0) return false; if (Double.compare(oY, y) != 0) return false; return true; }	we don't need to be equal to parsedcartesianpoints?
protected static NodeShardsResult buildAllocationIdBasedNodeShardsResult(ShardRouting shard, boolean matchAnyShard, Set<String> ignoreNodes, Set<String> inSyncAllocationIds, FetchResult<NodeGatewayStartedShards> shardState, Logger logger) { LinkedList<NodeGatewayStartedShards> matchingNodeShardStates = new LinkedList<>(); LinkedList<NodeGatewayStartedShards> nonMatchingNodeShardStates = new LinkedList<>(); int numberOfAllocationsFound = 0; for (NodeGatewayStartedShards nodeShardState : shardState.getData().values()) { DiscoveryNode node = nodeShardState.getNode(); String allocationId = nodeShardState.allocationId(); if (ignoreNodes.contains(node.getId())) { continue; } if (nodeShardState.storeException() == null) { if (allocationId == null && nodeShardState.legacyVersion() == ShardStateMetaData.NO_VERSION) { logger.trace("[{}] on node [{}] has no shard state information", shard, nodeShardState.getNode()); } else if (allocationId != null) { assert nodeShardState.legacyVersion() == ShardStateMetaData.NO_VERSION : "Allocation id and legacy version cannot be both present"; logger.trace("[{}] on node [{}] has allocation id [{}]", shard, nodeShardState.getNode(), allocationId); } else { logger.trace("[{}] on node [{}] has no allocation id, out-dated shard (shard state version: [{}])", shard, nodeShardState.getNode(), nodeShardState.legacyVersion()); } } else { final String finalAllocationId = allocationId; if (nodeShardState.storeException() instanceof ShardLockObtainFailedException) { logger.trace((Supplier<?>) () -> new ParameterizedMessage("[{}] on node [{}] has allocation id [{}] but the store can not be opened as it's locked, treating as valid shard", shard, nodeShardState.getNode(), finalAllocationId), nodeShardState.storeException()); } else { logger.trace((Supplier<?>) () -> new ParameterizedMessage("[{}] on node [{}] has allocation id [{}] but the store can not be opened, treating as no allocation id", shard, nodeShardState.getNode(), finalAllocationId), nodeShardState.storeException()); allocationId = null; } } if (allocationId != null) { numberOfAllocationsFound++; if (inSyncAllocationIds.contains(allocationId)) { // put shards that were primary before and that didn't throw a ShardLockObtainFailedException first if (nodeShardState.primary() && nodeShardState.storeException() == null) { matchingNodeShardStates.addFirst(nodeShardState); } else { matchingNodeShardStates.addLast(nodeShardState); } } else if (matchAnyShard) { // put shards that were primary before and that didn't throw a ShardLockObtainFailedException first if (nodeShardState.primary() && nodeShardState.storeException() == null) { nonMatchingNodeShardStates.addFirst(nodeShardState); } else { nonMatchingNodeShardStates.addLast(nodeShardState); } } } } List<NodeGatewayStartedShards> nodeShardStates = new ArrayList<>(); nodeShardStates.addAll(matchingNodeShardStates); nodeShardStates.addAll(nonMatchingNodeShardStates); if (logger.isTraceEnabled()) { logger.trace("{} candidates for allocation: {}", shard, nodeShardStates.stream().map(s -> s.getNode().getName()).collect(Collectors.joining(", "))); } return new NodeShardsResult(nodeShardStates, numberOfAllocationsFound); }	can we assert that the store exception is what we expect it to be?
protected static NodeShardsResult buildAllocationIdBasedNodeShardsResult(ShardRouting shard, boolean matchAnyShard, Set<String> ignoreNodes, Set<String> inSyncAllocationIds, FetchResult<NodeGatewayStartedShards> shardState, Logger logger) { LinkedList<NodeGatewayStartedShards> matchingNodeShardStates = new LinkedList<>(); LinkedList<NodeGatewayStartedShards> nonMatchingNodeShardStates = new LinkedList<>(); int numberOfAllocationsFound = 0; for (NodeGatewayStartedShards nodeShardState : shardState.getData().values()) { DiscoveryNode node = nodeShardState.getNode(); String allocationId = nodeShardState.allocationId(); if (ignoreNodes.contains(node.getId())) { continue; } if (nodeShardState.storeException() == null) { if (allocationId == null && nodeShardState.legacyVersion() == ShardStateMetaData.NO_VERSION) { logger.trace("[{}] on node [{}] has no shard state information", shard, nodeShardState.getNode()); } else if (allocationId != null) { assert nodeShardState.legacyVersion() == ShardStateMetaData.NO_VERSION : "Allocation id and legacy version cannot be both present"; logger.trace("[{}] on node [{}] has allocation id [{}]", shard, nodeShardState.getNode(), allocationId); } else { logger.trace("[{}] on node [{}] has no allocation id, out-dated shard (shard state version: [{}])", shard, nodeShardState.getNode(), nodeShardState.legacyVersion()); } } else { final String finalAllocationId = allocationId; if (nodeShardState.storeException() instanceof ShardLockObtainFailedException) { logger.trace((Supplier<?>) () -> new ParameterizedMessage("[{}] on node [{}] has allocation id [{}] but the store can not be opened as it's locked, treating as valid shard", shard, nodeShardState.getNode(), finalAllocationId), nodeShardState.storeException()); } else { logger.trace((Supplier<?>) () -> new ParameterizedMessage("[{}] on node [{}] has allocation id [{}] but the store can not be opened, treating as no allocation id", shard, nodeShardState.getNode(), finalAllocationId), nodeShardState.storeException()); allocationId = null; } } if (allocationId != null) { numberOfAllocationsFound++; if (inSyncAllocationIds.contains(allocationId)) { // put shards that were primary before and that didn't throw a ShardLockObtainFailedException first if (nodeShardState.primary() && nodeShardState.storeException() == null) { matchingNodeShardStates.addFirst(nodeShardState); } else { matchingNodeShardStates.addLast(nodeShardState); } } else if (matchAnyShard) { // put shards that were primary before and that didn't throw a ShardLockObtainFailedException first if (nodeShardState.primary() && nodeShardState.storeException() == null) { nonMatchingNodeShardStates.addFirst(nodeShardState); } else { nonMatchingNodeShardStates.addLast(nodeShardState); } } } } List<NodeGatewayStartedShards> nodeShardStates = new ArrayList<>(); nodeShardStates.addAll(matchingNodeShardStates); nodeShardStates.addAll(nonMatchingNodeShardStates); if (logger.isTraceEnabled()) { logger.trace("{} candidates for allocation: {}", shard, nodeShardStates.stream().map(s -> s.getNode().getName()).collect(Collectors.joining(", "))); } return new NodeShardsResult(nodeShardStates, numberOfAllocationsFound); }	same request for assertion.
static NodeShardsResult buildVersionBasedNodeShardsResult(ShardRouting shard, boolean matchAnyShard, Set<String> ignoreNodes, FetchResult<NodeGatewayStartedShards> shardState, Logger logger) { final List<NodeGatewayStartedShards> allocationCandidates = new ArrayList<>(); int numberOfAllocationsFound = 0; long highestVersion = ShardStateMetaData.NO_VERSION; for (NodeGatewayStartedShards nodeShardState : shardState.getData().values()) { long version = nodeShardState.legacyVersion(); DiscoveryNode node = nodeShardState.getNode(); if (ignoreNodes.contains(node.getId())) { continue; } if (nodeShardState.storeException() == null) { if (version == ShardStateMetaData.NO_VERSION && nodeShardState.allocationId() == null) { logger.trace("[{}] on node [{}] has no shard state information", shard, nodeShardState.getNode()); } else if (version != ShardStateMetaData.NO_VERSION) { assert nodeShardState.allocationId() == null : "Allocation id and legacy version cannot be both present"; logger.trace("[{}] on node [{}] has version [{}] of shard", shard, nodeShardState.getNode(), version); } else { // shard was already selected in a 5.x cluster as primary for recovery, was initialized (and wrote a new state file) but // did not make it to STARTED state before the cluster crashed (otherwise list of active allocation ids would be // non-empty and allocation id - based allocation mode would be chosen). // Prefer this shard copy again. version = Long.MAX_VALUE; logger.trace("[{}] on node [{}] has allocation id [{}]", shard, nodeShardState.getNode(), nodeShardState.allocationId()); } } else { final long finalVersion = version; if (nodeShardState.storeException() instanceof ShardLockObtainFailedException) { logger.trace((Supplier<?>) () -> new ParameterizedMessage("[{}] on node [{}] has version [{}] but the store can not be opened as it's locked, treating as valid shard", shard, nodeShardState.getNode(), finalVersion), nodeShardState.storeException()); if (nodeShardState.allocationId() != null) { version = Long.MAX_VALUE; // shard was already selected in a 5.x cluster as primary, prefer this shard copy again. } else { version = 0L; // treat as lowest version so that this shard is the least likely to be selected as primary } } else { // disregard the reported version and assign it as no version (same as shard does not exist) logger.trace((Supplier<?>) () -> new ParameterizedMessage("[{}] on node [{}] has version [{}] but the store can not be opened, treating no version", shard, nodeShardState.getNode(), finalVersion), nodeShardState.storeException()); version = ShardStateMetaData.NO_VERSION; } } if (version != ShardStateMetaData.NO_VERSION) { numberOfAllocationsFound++; // If we've found a new "best" candidate, clear the // current candidates and add it if (version > highestVersion) { highestVersion = version; if (matchAnyShard == false) { allocationCandidates.clear(); } allocationCandidates.add(nodeShardState); } else if (version == highestVersion) { // If the candidate is the same, add it to the // list, but keep the current candidate allocationCandidates.add(nodeShardState); } } } // sort array so the node with the highest version is at the beginning CollectionUtil.timSort(allocationCandidates, Comparator.comparing(NodeGatewayStartedShards::legacyVersion).reversed()); if (logger.isTraceEnabled()) { StringBuilder sb = new StringBuilder("["); for (NodeGatewayStartedShards n : allocationCandidates) { sb.append("[").append(n.getNode().getName()).append("]").append(" -> ").append(n.legacyVersion()).append(", "); } sb.append("]"); logger.trace("{} candidates for allocation: {}", shard, sb.toString()); } return new NodeShardsResult(Collections.unmodifiableList(allocationCandidates), numberOfAllocationsFound); } /** * Return {@code true}	did you run into this being a problem? how can we open an index that was created before 5.0.0 and never had insync replicas but does have allocationid? the only thing i can think of is a node network issue during shard initialization. i'm wondering if we need to optimize for this and no keep this code simple (i.e., demote shards with a lock exception)
@Override public void forceMerge(boolean flush, int maxNumSegments, boolean onlyExpungeDeletes, boolean upgrade, boolean upgradeOnlyAncientSegments, String forceMergeUUID) { if (maxNumSegments < lastCommittedSegmentInfos.size()) { throw new UnsupportedOperationException("force merge is not supported on a read-only engine, " + "target max number of segments[" + maxNumSegments + "], " + "current number of segments[" + lastCommittedSegmentInfos.size() + "]."); } else { logger.debug("current number of segments[{}] is not greater than target max number of segments[{}].", lastCommittedSegmentInfos.size(), maxNumSegments); } }	this rejects force-merge requests that do not specify the maxnumsegments parameter at all, since that comes through to here as maxnumsegments == forcemergesegments.defaults.max_num_segments == -1. we should accept these requests too, it's ok for them to do nothing.
public void onResponse(CreateSnapshotResponse createSnapshotResponse) { logger.debug( "snapshot response for [{}]: {}", policyMetadata.getPolicy().getId(), Strings.toString(createSnapshotResponse) ); final SnapshotInfo snapInfo = createSnapshotResponse.getSnapshotInfo(); // Check that there are no failed shards, since the request may not entirely // fail, but may still have failures (such as in the case of an aborted snapshot) if (snapInfo.failedShards() == 0) { long snapshotStartTime = snapInfo.startTime(); final long timestamp = Instant.now().toEpochMilli(); clusterService.submitStateUpdateTask( "slm-record-success-" + policyMetadata.getPolicy().getId(), WriteJobStatus.success(policyMetadata.getPolicy().getId(), request.snapshot(), snapshotStartTime, timestamp) ); historyStore.putAsync( SnapshotHistoryItem.creationSuccessRecord(timestamp, policyMetadata.getPolicy(), request.snapshot()) ); } else { int failures = snapInfo.failedShards(); int total = snapInfo.totalShards(); final SnapshotException e = new SnapshotException( request.repository(), request.snapshot(), "failed to create snapshot successfully, " + failures + " out of " + total + " total shards failed" ); // Add each failed shard's exception as suppressed, the exception contains // information about which shard failed // TODO: this seems wrong, investigate whether we actually need all the shard level exception here given that we // could be dealing with tens of thousands of them at a time snapInfo.shardFailures().forEach(e::addSuppressed); // Call the failure handler to register this as a failure and persist it onFailure(e); } }	:+1: since we're no longer adding an inner indexshardsnapshotfailedexception that duplicates the info in the wrapper.
public void testDateTimes() throws IOException { assertQuery("SELECT CAST('2019-01-14T12:29:25.000Z' AS DATETIME)", "CAST('2019-01-14T12:29:25.000Z' AS DATETIME)", "datetime", "2019-01-14T12:29:25.000Z", 35); assertQuery("SELECT CAST(-26853765751000 AS DATETIME)", "CAST(-26853765751000 AS DATETIME)", "datetime", "1119-01-15T12:37:29.000Z", 35); assertQuery("SELECT CAST(CAST('-26853765751000' AS BIGINT) AS DATETIME)", "CAST(CAST('-26853765751000' AS BIGINT) AS DATETIME)", "datetime", "1119-01-15T12:37:29.000Z", 35); assertQuery("SELECT CAST('2019-01-14' AS DATE)", "CAST('2019-01-14' AS DATE)", "date", "2019-01-14T00:00:00.000Z", 29); assertQuery("SELECT CAST(-26853765751000 AS DATE)", "CAST(-26853765751000 AS DATE)", "date", "1119-01-15T00:00:00.000Z", 29); }	since the precision changed, please change the tests' values or add new tests (preferred) to reflect the increased precision.
public static SortValue from(BytesRef bytes) { return new BytesSortValue(bytes); } /** * Get a {@linkplain SortValue}	we should probably add a few things to sortvaluetests too - just to quickly catch stuff like serialization errors.
private void addSupplier(String name, Map<String, FileSupplier> collector, String key, File actualValue) { requireNonNull(actualValue, name + " value was null when configuring test cluster `" + this + "`"); if (actualValue.exists()) { throw new TestClustersException("Found a non existent file " + actualValue + " while configuring " + this); } addSupplier(name, collector, key, () -> actualValue); }	why all these extra levels of indirection? i think this addsupplier, and the other one added here could just be implemented inside keystore that takes a filesupplier, and the file variant calls that method instead of this indirection.
@Override public synchronized void start() { LOGGER.info("Starting `{}`", this); Path distroArtifact = artifactsExtractDir .resolve(distribution.getGroup()) .resolve(distribution.getArtifactName() + "-" + getVersion()); if (Files.exists(distroArtifact) == false) { throw new TestClustersException("Can not start " + this + ", missing: " + distroArtifact); } if (Files.isDirectory(distroArtifact) == false) { throw new TestClustersException("Can not start " + this + ", is not a directory: " + distroArtifact); } try { createWorkingDir(distroArtifact); } catch (IOException e) { throw new UncheckedIOException("Failed to create working directory for " + this, e); } createConfiguration(); plugins.forEach(plugin -> runElaticsearchBinScript( "elasticsearch-plugin", "install", "--batch", plugin.toString()) ); if (keystoreSettings.isEmpty() == false || keystoreFiles.isEmpty() == false) { runElaticsearchBinScript("elasticsearch-keystore", "create"); checkSuppliers("Keystore", keystoreSettings); keystoreSettings.forEach((key, value) -> runElaticsearchBinScriptWithInput(value.get().toString(), "elasticsearch-keystore", "add", "-x", key) ); keystoreFiles.values().stream() .map(each -> each.get()) .filter(each -> each == null || each.exists() == false) .forEach(each -> { requireNonNull("supplied keystoreFile was null when configuring test cluster `" + this + "`"); throw new TestClustersException("supplied keystore file " + each + " does not exist, require for " + this); }); keystoreFiles.forEach((key, value) -> runElaticsearchBinScript("elasticsearch-keystore", "add-file", key, value.get().toString()) ); } installModules(); copyExtraConfigFiles(); startElasticsearchProcess(); }	sorry but these uses of foreach are getting out of hand. can you please just use a simple loop? this foreach and the one below can simply be: for (var entry : keystorefiles.entryset()) { file file = entry.getvalue().get(); if (file == null) { throw new illegalargumentexception("supplied keystorefile was null when configuring test cluster " + this + "") } if (file.exists() == false) { throw new testclustersexception("supplied keystore file " + each + " does not exist, require for " + this); } runelaticsearchbinscript("elasticsearch-keystore", "add-file", entry.getkey(), file.tostring()) } it is the same number of lines, yet does not require thinking about the special way that the first foreach actually works (which gives a different error if null was found instead of a non existent file).
private GetResult getFromTranslog(Get get, Translog.Index index, MappingLookup mappingLookup, DocumentParser documentParser, Function<Searcher, Searcher> searcherWrapper) throws IOException { assert get.isReadFromTranslog(); final SingleDocDirectoryReader inMemoryReader = new SingleDocDirectoryReader(shardId, index, mappingLookup, documentParser, config().getAnalyzer()); final Engine.Searcher searcher = new Engine.Searcher("realtime_get", ElasticsearchDirectoryReader.wrap(inMemoryReader, shardId), config().getSimilarity(), config().getQueryCache(), config().getQueryCachingPolicy(), inMemoryReader); final Searcher wrappedSearcher = searcherWrapper.apply(searcher); // TODO: we always wrap with field usage tracking, so we need to find another way to determine if we can safely do this if (wrappedSearcher == searcher || (wrappedSearcher.getIndexReader() instanceof FieldUsageTrackingDirectoryReader && ((FieldUsageTrackingDirectoryReader) wrappedSearcher.getIndexReader()).getDelegate() == searcher.getIndexReader())) { wrappedSearcher.close(); assert inMemoryReader.assertMemorySegmentStatus(false); final TranslogLeafReader translogLeafReader = new TranslogLeafReader(index); return new GetResult(new Engine.Searcher("realtime_get", translogLeafReader, IndexSearcher.getDefaultSimilarity(), null, IndexSearcher.getDefaultQueryCachingPolicy(), translogLeafReader), new VersionsAndSeqNoResolver.DocIdAndVersion( 0, index.version(), index.seqNo(), index.primaryTerm(), translogLeafReader, 0), true); } else { assert inMemoryReader.assertMemorySegmentStatus(true); return getFromSearcher(get, wrappedSearcher); } }	@dnhatn would love to have your thoughts on this ^^ i'm wondering if we could have translogleafreader somehow embedded in singledocdirectoryreader so that we always apply the wrapper, but then lazily load the inmemoryleafreader when there is access to more than what the translogleafreader can offer.
@Override public void indexTranslogOperations( final List<Translog.Operation> operations, final int totalTranslogOps, final long maxSeenAutoIdTimestampOnPrimary, final long maxSeqNoOfDeletesOrUpdatesOnPrimary, final RetentionLeases retentionLeases, final long mappingVersionOnPrimary, final ActionListener<Long> listener) { final String action = PeerRecoveryTargetService.Actions.TRANSLOG_OPS; final RecoveryTranslogOperationsRequest request = new RecoveryTranslogOperationsRequest( recoveryId, shardId, operations, totalTranslogOps, maxSeenAutoIdTimestampOnPrimary, maxSeqNoOfDeletesOrUpdatesOnPrimary, retentionLeases, mappingVersionOnPrimary); final Writeable.Reader<RecoveryTranslogOperationsResponse> reader = RecoveryTranslogOperationsResponse::new; final ActionListener<RecoveryTranslogOperationsResponse> responseListener = ActionListener.map(listener, r -> r.localCheckpoint); executeRetryableAction(action, request, translogOpsRequestOptions, responseListener, reader); }	nit: revert indent change?
@Override public SortFieldAndFormat build(QueryShardContext context) throws IOException { if (DOC_FIELD_NAME.equals(fieldName)) { if (order == SortOrder.DESC) { return SORT_DOC_REVERSE; } else { return SORT_DOC; } } else { MappedFieldType fieldType = context.fieldMapper(fieldName); if (fieldType == null) { if (unmappedType != null) { fieldType = context.getMapperService().unmappedFieldType(unmappedType); } else { throw new QueryShardException(context, "No mapping found for [" + fieldName + "] in order to sort on"); } } MultiValueMode localSortMode = null; if (sortMode != null) { localSortMode = MultiValueMode.fromString(sortMode.toString()); } boolean reverse = (order == SortOrder.DESC); if (localSortMode == null) { localSortMode = reverse ? MultiValueMode.MAX : MultiValueMode.MIN; } final Nested nested; if (nestedSort != null) { if (context.indexVersionCreated().before(Version.V_6_5_0) && nestedSort.getMaxChildren() != Integer.MAX_VALUE) { throw new QueryShardException(context, "max_children is only supported on v6.5.0 or higher"); } if (nestedSort.getNestedSort() != null && nestedSort.getMaxChildren() != Integer.MAX_VALUE) { throw new QueryShardException(context, "max_children is only supported on last level of nested sort"); } // new nested sorts takes priority nested = resolveNested(context, nestedSort); } else { nested = resolveNested(context, nestedPath, nestedFilter); } IndexFieldData<?> fieldData = context.getForField(fieldType); if (fieldData instanceof IndexNumericFieldData == false && (sortMode == SortMode.SUM || sortMode == SortMode.AVG || sortMode == SortMode.MEDIAN)) { throw new QueryShardException(context, "we only support AVG, MEDIAN and SUM on number based fields"); } final SortField field; if (numericType != null) { if (fieldData instanceof SortedNumericDVIndexFieldData == false) { throw new QueryShardException(context, "[numeric_type] option cannot be set on a non-numeric field, got " + fieldType.typeName()); } SortedNumericDVIndexFieldData numericFieldData = (SortedNumericDVIndexFieldData) fieldData; NumericType resolvedType = NumericType.valueOf(numericType); field = numericFieldData.sortField(resolvedType, missing, localSortMode, nested, reverse); } else { field = fieldData.sortField(missing, localSortMode, nested, reverse); } return new SortFieldAndFormat(field, fieldType.docValueFormat(null, null)); } }	why do you check against this class rather than indexnumericfielddata?
public void testCollectToUnmodifiableSortedMap() { SortedMap<String, String> canadianProvinces = Stream.of( new Tuple<>("ON", "Ontario"), new Tuple<>("QC", "Quebec"), new Tuple<>("NS", "Nova Scotia"), new Tuple<>("NB", "New Brunswick"), new Tuple<>("MB", "Manitoba")) .collect(Maps.toUnmodifiableSortedMap(Tuple::v1, Tuple::v2)); assertThat(canadianProvinces, equalTo(new TreeMap<>(Maps.ofEntries(List.of( entry("ON", "Ontario"), entry("QC", "Quebec"), entry("NS", "Nova Scotia"), entry("NB", "New Brunswick"), entry("MB", "Manitoba")) )))); expectThrows(UnsupportedOperationException.class, () -> canadianProvinces.put("BC", "British Columbia")); }	unfortunately, using distinct() here is not enough to guarantee unique keys, it only guarantees that the (key, value) pairs are unique. you would have to generate the keys as distinct and then add random values, something like: randomlist(0, 100, () -> randomalphaoflength(10)).stream().distint().map(key -> tuple.of(key, randomalphasoflength(10)). collect(collectors.tolist())
public void testCollectToUnmodifiableSortedMap() { SortedMap<String, String> canadianProvinces = Stream.of( new Tuple<>("ON", "Ontario"), new Tuple<>("QC", "Quebec"), new Tuple<>("NS", "Nova Scotia"), new Tuple<>("NB", "New Brunswick"), new Tuple<>("MB", "Manitoba")) .collect(Maps.toUnmodifiableSortedMap(Tuple::v1, Tuple::v2)); assertThat(canadianProvinces, equalTo(new TreeMap<>(Maps.ofEntries(List.of( entry("ON", "Ontario"), entry("QC", "Quebec"), entry("NS", "Nova Scotia"), entry("NB", "New Brunswick"), entry("MB", "Manitoba")) )))); expectThrows(UnsupportedOperationException.class, () -> canadianProvinces.put("BC", "British Columbia")); }	slightly shorter variant: suggestion randomness.shuffle(tuples);
public void testSimulate() throws IOException { String source = "{\\\\n" + " \\\\"pipeline\\\\": {\\\\n" + " \\\\"processors\\\\": [\\\\n" + " {\\\\n" + " \\\\"inference\\\\": {\\\\n" + " \\\\"target_field\\\\": \\\\"ml.classification\\\\",\\\\n" + " \\\\"inference_config\\\\": {\\\\"classification\\\\": " + " {\\\\"num_top_classes\\\\":2, \\\\"top_classes_results_field\\\\": \\\\"result_class_prob\\\\"}},\\\\n" + " \\\\"model_id\\\\": \\\\"test_classification\\\\",\\\\n" + " \\\\"field_mappings\\\\": {\\\\n" + " \\\\"col1\\\\": \\\\"col1\\\\",\\\\n" + " \\\\"col2\\\\": \\\\"col2\\\\",\\\\n" + " \\\\"col3\\\\": \\\\"col3\\\\",\\\\n" + " \\\\"col4\\\\": \\\\"col4\\\\"\\\\n" + " }\\\\n" + " }\\\\n" + " },\\\\n" + " {\\\\n" + " \\\\"inference\\\\": {\\\\n" + " \\\\"target_field\\\\": \\\\"ml.regression\\\\",\\\\n" + " \\\\"model_id\\\\": \\\\"test_regression\\\\",\\\\n" + " \\\\"inference_config\\\\": {\\\\"regression\\\\":{}},\\\\n" + " \\\\"field_mappings\\\\": {\\\\n" + " \\\\"col1\\\\": \\\\"col1\\\\",\\\\n" + " \\\\"col2\\\\": \\\\"col2\\\\",\\\\n" + " \\\\"col3\\\\": \\\\"col3\\\\",\\\\n" + " \\\\"col4\\\\": \\\\"col4\\\\"\\\\n" + " }\\\\n" + " }\\\\n" + " }\\\\n" + " ]\\\\n" + " },\\\\n" + " \\\\"docs\\\\": [\\\\n" + " {\\\\"_source\\\\": {\\\\n" + " \\\\"col1\\\\": \\\\"female\\\\",\\\\n" + " \\\\"col2\\\\": \\\\"M\\\\",\\\\n" + " \\\\"col3\\\\": \\\\"none\\\\",\\\\n" + " \\\\"col4\\\\": 10\\\\n" + " }}]\\\\n" + "}"; Response response = client().performRequest(simulateRequest(source)); String responseString = EntityUtils.toString(response.getEntity()); assertThat(responseString, containsString("\\\\"predicted_value\\\\":\\\\"second\\\\"")); assertThat(responseString, containsString("\\\\"predicted_value\\\\":1.0")); String sourceWithMissingModel = "{\\\\n" + " \\\\"pipeline\\\\": {\\\\n" + " \\\\"processors\\\\": [\\\\n" + " {\\\\n" + " \\\\"inference\\\\": {\\\\n" + " \\\\"model_id\\\\": \\\\"test_classification_missing\\\\",\\\\n" + " \\\\"inference_config\\\\": {\\\\"classification\\\\":{}},\\\\n" + " \\\\"field_mappings\\\\": {\\\\n" + " \\\\"col1\\\\": \\\\"col1\\\\",\\\\n" + " \\\\"col2\\\\": \\\\"col2\\\\",\\\\n" + " \\\\"col3\\\\": \\\\"col3\\\\",\\\\n" + " \\\\"col4\\\\": \\\\"col4\\\\"\\\\n" + " }\\\\n" + " }\\\\n" + " }\\\\n" + " ]\\\\n" + " },\\\\n" + " \\\\"docs\\\\": [\\\\n" + " {\\\\"_source\\\\": {\\\\n" + " \\\\"col1\\\\": \\\\"female\\\\",\\\\n" + " \\\\"col2\\\\": \\\\"M\\\\",\\\\n" + " \\\\"col3\\\\": \\\\"none\\\\",\\\\n" + " \\\\"col4\\\\": 10\\\\n" + " }}]\\\\n" + "}"; response = client().performRequest(simulateRequest(sourceWithMissingModel)); responseString = EntityUtils.toString(response.getEntity()); assertThat(responseString, containsString("Could not find trained model [test_classification_missing]")); }	is this a standard practice in rest tests to grep for substrings rather than parsing the response into json?
* @param snapshotIds snapshots for which to fetch snapshot information * @param ignoreUnavailable if true, snapshots that could not be read will only be logged with a warning, * if false, they will throw an error */ private void snapshots(SnapshotsInProgress snapshotsInProgress, String repositoryName, Collection<SnapshotId> snapshotIds, boolean ignoreUnavailable, CancellableTask task, ActionListener<List<SnapshotInfo>> listener) { if (task.isCancelled()) { listener.onFailure(new TaskCancelledException("task cancelled")); return; } final Set<SnapshotInfo> snapshotSet = new HashSet<>(); final Set<SnapshotId> snapshotIdsToIterate = new HashSet<>(snapshotIds); // first, look at the snapshots in progress final List<SnapshotsInProgress.Entry> entries = SnapshotsService.currentSnapshots( snapshotsInProgress, repositoryName, snapshotIdsToIterate.stream().map(SnapshotId::getName).collect(Collectors.toList())); for (SnapshotsInProgress.Entry entry : entries) { if (snapshotIdsToIterate.remove(entry.snapshot().getSnapshotId())) { snapshotSet.add(new SnapshotInfo(entry)); } } // then, look in the repository if there's any matching snapshots left final List<SnapshotInfo> snapshotInfos; if (snapshotIdsToIterate.isEmpty()) { snapshotInfos = Collections.emptyList(); } else { snapshotInfos = Collections.synchronizedList(new ArrayList<>()); } final ActionListener<Collection<Void>> allDoneListener = listener.delegateFailure((l, v) -> { final ArrayList<SnapshotInfo> snapshotList = new ArrayList<>(snapshotInfos); snapshotList.addAll(snapshotSet); CollectionUtil.timSort(snapshotList); listener.onResponse(unmodifiableList(snapshotList)); }); if (snapshotIdsToIterate.isEmpty()) { allDoneListener.onResponse(Collections.emptyList()); return; } // put snapshot info downloads into a task queue instead of pushing them all into the queue to not completely monopolize the // snapshot meta pool for a single request final int workers = Math.min(threadPool.info(ThreadPool.Names.SNAPSHOT_META).getMax(), snapshotIdsToIterate.size()); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT_META); final BlockingQueue<SnapshotId> queue = new LinkedBlockingQueue<>(snapshotIdsToIterate); final ActionListener<Void> workerDoneListener = new GroupedActionListener<>(allDoneListener, workers).delegateResponse((l, e) -> { queue.clear(); // Stop fetching the remaining snapshots once we've failed fetching one since the response is an error response // anyway in this case l.onFailure(e); }); final Repository repository = repositoriesService.repository(repositoryName); for (int i = 0; i < workers; i++) { getOneSnapshotInfo( ignoreUnavailable, repository, queue, snapshotInfos, task, executor, workerDoneListener ); } }	nit: can we add a bit of javadoc?
* @param snapshotIds snapshots for which to fetch snapshot information * @param ignoreUnavailable if true, snapshots that could not be read will only be logged with a warning, * if false, they will throw an error */ private void snapshots(SnapshotsInProgress snapshotsInProgress, String repositoryName, Collection<SnapshotId> snapshotIds, boolean ignoreUnavailable, CancellableTask task, ActionListener<List<SnapshotInfo>> listener) { if (task.isCancelled()) { listener.onFailure(new TaskCancelledException("task cancelled")); return; } final Set<SnapshotInfo> snapshotSet = new HashSet<>(); final Set<SnapshotId> snapshotIdsToIterate = new HashSet<>(snapshotIds); // first, look at the snapshots in progress final List<SnapshotsInProgress.Entry> entries = SnapshotsService.currentSnapshots( snapshotsInProgress, repositoryName, snapshotIdsToIterate.stream().map(SnapshotId::getName).collect(Collectors.toList())); for (SnapshotsInProgress.Entry entry : entries) { if (snapshotIdsToIterate.remove(entry.snapshot().getSnapshotId())) { snapshotSet.add(new SnapshotInfo(entry)); } } // then, look in the repository if there's any matching snapshots left final List<SnapshotInfo> snapshotInfos; if (snapshotIdsToIterate.isEmpty()) { snapshotInfos = Collections.emptyList(); } else { snapshotInfos = Collections.synchronizedList(new ArrayList<>()); } final ActionListener<Collection<Void>> allDoneListener = listener.delegateFailure((l, v) -> { final ArrayList<SnapshotInfo> snapshotList = new ArrayList<>(snapshotInfos); snapshotList.addAll(snapshotSet); CollectionUtil.timSort(snapshotList); listener.onResponse(unmodifiableList(snapshotList)); }); if (snapshotIdsToIterate.isEmpty()) { allDoneListener.onResponse(Collections.emptyList()); return; } // put snapshot info downloads into a task queue instead of pushing them all into the queue to not completely monopolize the // snapshot meta pool for a single request final int workers = Math.min(threadPool.info(ThreadPool.Names.SNAPSHOT_META).getMax(), snapshotIdsToIterate.size()); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT_META); final BlockingQueue<SnapshotId> queue = new LinkedBlockingQueue<>(snapshotIdsToIterate); final ActionListener<Void> workerDoneListener = new GroupedActionListener<>(allDoneListener, workers).delegateResponse((l, e) -> { queue.clear(); // Stop fetching the remaining snapshots once we've failed fetching one since the response is an error response // anyway in this case l.onFailure(e); }); final Repository repository = repositoriesService.repository(repositoryName); for (int i = 0; i < workers; i++) { getOneSnapshotInfo( ignoreUnavailable, repository, queue, snapshotInfos, task, executor, workerDoneListener ); } }	i wonder if we should retrieve the repository from the the repositoriesservice for each snapshotinfo to load, so that if the repository is gone the repositorymissing is easier to propagate through listeners (and grouped listener which clears the queue etc). otherwise a repositorymissing might be thrown i think and will be caught at a higher level but we keep fetching snapshot info here.
private int expectedSize(final String threadPoolName, final int numberOfProcessors) { final Map<String, Function<Integer, Integer>> sizes = new HashMap<>(); sizes.put(ThreadPool.Names.GENERIC, n -> ThreadPool.boundedBy(4 * n, 128, 512)); sizes.put(ThreadPool.Names.MANAGEMENT, n -> ThreadPool.boundedBy(n, 1, 5)); sizes.put(ThreadPool.Names.FLUSH, ThreadPool::halfAllocatedProcessorsMaxFive); sizes.put(ThreadPool.Names.REFRESH, ThreadPool::halfAllocatedProcessorsMaxTen); sizes.put(ThreadPool.Names.WARMER, ThreadPool::halfAllocatedProcessorsMaxFive); sizes.put(ThreadPool.Names.SNAPSHOT, ThreadPool::halfAllocatedProcessorsMaxFive); sizes.put(ThreadPool.Names.SNAPSHOT_META, ThreadPool::twiceAllocatedProcessors); sizes.put(ThreadPool.Names.FETCH_SHARD_STARTED, ThreadPool::twiceAllocatedProcessors); sizes.put(ThreadPool.Names.FETCH_SHARD_STORE, ThreadPool::twiceAllocatedProcessors); return sizes.get(threadPoolName).apply(numberOfProcessors); }	can we document this thread pool?
public boolean hasGlobalOrdinals() { return valuesSource.hasGlobalOrdinals(); }	it's probably worth some javadoc on this one too.
protected LogicalPlan rule(UnaryPlan plan) { if (plan.child() instanceof SubQueryAlias) { SubQueryAlias a = (SubQueryAlias) plan.child(); return plan.transformExpressionsDown(FieldAttribute.class, f -> { if (f.qualifier() != null && f.qualifier().equals(a.alias())) { // Find the underlying concrete relation (EsIndex) and its name as the new qualifier String newQualifier = null; List<LogicalPlan> children = a.collectFirstChildren(p -> p instanceof EsRelation); if (children.isEmpty() == false) { newQualifier = ((EsRelation) children.get(0)).index().name(); } return f.withQualifier(newQualifier); } else { return f; } }); } return plan; }	shouldn't an else just return f here? otherwise it seems to strip the qualifier. not sure if it can happen (maybe when ref'ing an unresolvedrelation?), but might be safer.
public void putAsync(String repository, String name, String path, long offset, BytesReference content, ActionListener<Void> listener) { try { final CachedBlob cachedBlob = new CachedBlob( Instant.ofEpochMilli(threadPool.absoluteTimeInMillis()), Version.CURRENT, repository, name, path, content, offset ); final IndexRequest request = new IndexRequest(index).id(cachedBlob.generatedId()); try (XContentBuilder builder = jsonBuilder()) { request.source(cachedBlob.toXContent(builder, ToXContent.EMPTY_PARAMS)); } client.index(request, new ActionListener<IndexResponse>() { @Override public void onResponse(IndexResponse indexResponse) { logger.trace("cache fill ({}): [{}]", indexResponse.status(), request.id()); listener.onResponse(null); } @Override public void onFailure(Exception e) { logger.debug(new ParameterizedMessage("failure in cache fill: [{}]", request.id()), e); listener.onFailure(e); } }); } catch (Exception e) { logger.warn(new ParameterizedMessage("cache fill failure: [{}]", CachedBlob.generateId(repository, name, path, offset)), e); listener.onFailure(e); } }	we can use debug(supplier<?> msgsupplier, throwable t)
public void putAsync(String repository, String name, String path, long offset, BytesReference content, ActionListener<Void> listener) { try { final CachedBlob cachedBlob = new CachedBlob( Instant.ofEpochMilli(threadPool.absoluteTimeInMillis()), Version.CURRENT, repository, name, path, content, offset ); final IndexRequest request = new IndexRequest(index).id(cachedBlob.generatedId()); try (XContentBuilder builder = jsonBuilder()) { request.source(cachedBlob.toXContent(builder, ToXContent.EMPTY_PARAMS)); } client.index(request, new ActionListener<IndexResponse>() { @Override public void onResponse(IndexResponse indexResponse) { logger.trace("cache fill ({}): [{}]", indexResponse.status(), request.id()); listener.onResponse(null); } @Override public void onFailure(Exception e) { logger.debug(new ParameterizedMessage("failure in cache fill: [{}]", request.id()), e); listener.onFailure(e); } }); } catch (Exception e) { logger.warn(new ParameterizedMessage("cache fill failure: [{}]", CachedBlob.generateId(repository, name, path, offset)), e); listener.onFailure(e); } }	we should use warn(supplier<?> msgsupplier, throwable t)
public static <T> Boolean multiValueDocValues(Map<String, ScriptDocValues<T>> doc, String fieldName, Predicate<T> script) { if (doc.containsKey(fieldName)) { ScriptDocValues<T> docValues = doc.get(fieldName); if (docValues.isEmpty() == false) { for (int i = 0; i < docValues.size(); i++) { T value = docValues.get(i); if (script.test(value)) { return true; } } return false; } else { if (script.test(null)) { return true; } } } else { if (script.test(null)) { return true; } } return false; }	no need to check for the key, simply retrieve it and check if it's null. java docvalues = doc.get(fieldname); if (docvalues != null && docvalues.isempty() > 0) { for... }
public static <T> Boolean multiValueDocValues(Map<String, ScriptDocValues<T>> doc, String fieldName, Predicate<T> script) { if (doc.containsKey(fieldName)) { ScriptDocValues<T> docValues = doc.get(fieldName); if (docValues.isEmpty() == false) { for (int i = 0; i < docValues.size(); i++) { T value = docValues.get(i); if (script.test(value)) { return true; } } return false; } else { if (script.test(null)) { return true; } } } else { if (script.test(null)) { return true; } } return false; }	why not iterate directly: java for (t value : docvalues) { if (..) }
public static <T> Boolean multiValueDocValues(Map<String, ScriptDocValues<T>> doc, String fieldName, Predicate<T> script) { if (doc.containsKey(fieldName)) { ScriptDocValues<T> docValues = doc.get(fieldName); if (docValues.isEmpty() == false) { for (int i = 0; i < docValues.size(); i++) { T value = docValues.get(i); if (script.test(value)) { return true; } } return false; } else { if (script.test(null)) { return true; } } } else { if (script.test(null)) { return true; } } return false; }	this can be simplified by letting the else fall through to avoid repeating the script.test: java if (doc.containskey()... if (docvalues.is) ... } } return script.test(null);
public static Query toQuery(Expression e, TranslatorHandler handler) { Query translation = null; int i = 0; while (translation == null && i < QUERY_TRANSLATORS.size()) { translation = QUERY_TRANSLATORS.get(i).translate(e, handler); i++; } if (translation != null) { if (translation instanceof ScriptQuery) { // check the operators and the expressions involved in these operations so that all can be used // in a doc-values multi-valued context boolean multiValuedIncompatible = e.anyMatch(exp -> { if (exp instanceof Literal || exp instanceof FieldAttribute || exp instanceof Function) { return false; } else { return true; }}); if (multiValuedIncompatible == false) { ScriptQuery query = (ScriptQuery) translation; return new NoNullSafetyScriptQuery(query.source(), Scripts.multiValueDocValuesRewrite(query.script())); } } return translation; } throw new QlIllegalArgumentException("Don't know how to translate {} {}", e.nodeName(), e); }	why move from the iterable to the index based approach? i see the later as a downgrade.
public static String classPackageAsPrefix(Class<?> function) { String prefix = function.getPackageName().substring(PKG_LENGTH); int index = prefix.indexOf('.'); Check.isTrue(index > 0, "invalid package {}", prefix); return "{" + prefix.substring(0, index) + "}"; } /** * This method replaces any .docValue(doc,params.%s) call with a variable. * Each variable is then used in a {@code java.util.function.Predicate} to iterate over the doc_values in a Painless script. * Multiple .docValue(doc,params.%s) calls for the same field will use, in the end, only one .docValue call, meaning * the same value of the field will be used in all usages in the script. * * For example, a query of the form fieldA - fieldB > 0 that gets translated into the following Painless script * {@code InternalQlScriptUtils.nullSafeFilter(InternalQlScriptUtils.gt(InternalQlScriptUtils.sub( * InternalQlScriptUtils.docValue(doc,params.v0),InternalQlScriptUtils.docValue(doc,params.v1)),params.v2))} * will become, after this method rewrite * {@code InternalEqlScriptUtils.multiValueDocValues(doc,params.v0,X1 -> InternalEqlScriptUtils.multiValueDocValues(doc,params.v1, * X2 -> InternalQlScriptUtils.nullSafeFilter(InternalQlScriptUtils.gt(InternalQlScriptUtils.sub(X1,X2),params.v2))))}	this method is never used. is it (and the usesamevalueinscript parameter) still needed?
@Override public void deleteOldIndices(ActionListener<Boolean> listener) { DeleteIndexRequest deleteRequest = new DeleteIndexRequest( TransformInternalIndexConstants.INDEX_NAME_PATTERN, TransformInternalIndexConstants.INDEX_NAME_PATTERN_DEPRECATED, "-" + TransformInternalIndexConstants.LATEST_INDEX_VERSIONED_NAME ).indicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN); executeAsyncWithOrigin(client, TRANSFORM_ORIGIN, DeleteIndexAction.INSTANCE, deleteRequest, ActionListener.wrap(response -> { if (response.isAcknowledged() == false) { listener.onFailure(new ElasticsearchStatusException("Failed to delete internal indices", RestStatus.INTERNAL_SERVER_ERROR)); return; } listener.onResponse(true); }, listener::onFailure)); }	does the minus ("-") sign mean to exclude an index from being deleted?
@Override public void deleteOldCheckpoints(String transformId, long deleteCheckpointsBelow, long deleteOlderThan, ActionListener<Long> listener) { List<TransformCheckpoint> checkpointsById = checkpoints.get(transformId); int sizeBeforeDelete = checkpointsById.size(); if (checkpointsById != null) { checkpointsById.removeIf(cp -> { return cp.getCheckpoint() < deleteCheckpointsBelow && cp.getTimestamp() < deleteOlderThan; }); } long deletedDocs = sizeBeforeDelete - checkpointsById.size(); checkpointsById = oldCheckpoints.get(transformId); sizeBeforeDelete = checkpointsById.size(); if (checkpointsById != null) { checkpointsById.removeIf(cp -> { return cp.getCheckpoint() < deleteCheckpointsBelow && cp.getTimestamp() < deleteOlderThan; }); } deletedDocs += sizeBeforeDelete - checkpointsById.size(); listener.onResponse(deletedDocs); }	don't we sum up the same entries twice? in line 118 deleteddocs is initialized as: long deleteddocs = sizebeforedelete - checkpointsbyid.size();
protected void doExecute(Task task, BulkRequest bulkRequest, ActionListener<BulkResponse> listener) { boolean hasIndexRequestsWithPipelines = false; ImmutableOpenMap<String, IndexMetaData> indicesMetaData = clusterService.state().getMetaData().indices(); for (DocWriteRequest<?> actionRequest : bulkRequest.requests) { if (actionRequest instanceof IndexRequest) { IndexRequest indexRequest = (IndexRequest) actionRequest; String pipeline = indexRequest.getPipeline(); if (pipeline == null) { IndexMetaData indexMetaData = indicesMetaData.get(indexRequest.index()); if (indexMetaData != null) { String defaultPipeline = IndexSettings.DEFAULT_PIPELINE.get(indexMetaData.getSettings()); if (!defaultPipeline.isEmpty()) { indexRequest.setPipeline(defaultPipeline); hasIndexRequestsWithPipelines = true; } } } else if (!pipeline.isEmpty()) { hasIndexRequestsWithPipelines = true; } } } if (hasIndexRequestsWithPipelines) { if (clusterService.localNode().isIngestNode()) { processBulkIndexIngestRequest(task, bulkRequest, listener); } else { ingestForwarder.forwardIngestRequest(BulkAction.INSTANCE, bulkRequest, listener); } return; } final long startTime = relativeTime(); final AtomicArray<BulkItemResponse> responses = new AtomicArray<>(bulkRequest.requests.size()); if (needToCheck()) { // Attempt to create all the indices that we're going to need during the bulk before we start. // Step 1: collect all the indices in the request final Set<String> indices = bulkRequest.requests.stream() // delete requests should not attempt to create the index (if the index does not // exists), unless an external versioning is used .filter(request -> request.opType() != DocWriteRequest.OpType.DELETE || request.versionType() == VersionType.EXTERNAL || request.versionType() == VersionType.EXTERNAL_GTE) .map(DocWriteRequest::index) .collect(Collectors.toSet()); /* Step 2: filter that to indices that don't exist and we can create. At the same time build a map of indices we can't create * that we'll use when we try to run the requests. */ final Map<String, IndexNotFoundException> indicesThatCannotBeCreated = new HashMap<>(); Set<String> autoCreateIndices = new HashSet<>(); ClusterState state = clusterService.state(); for (String index : indices) { boolean shouldAutoCreate; try { shouldAutoCreate = shouldAutoCreate(index, state); } catch (IndexNotFoundException e) { shouldAutoCreate = false; indicesThatCannotBeCreated.put(index, e); } if (shouldAutoCreate) { autoCreateIndices.add(index); } } // Step 3: create all the indices that are missing, if there are any missing. start the bulk after all the creates come back. if (autoCreateIndices.isEmpty()) { executeBulk(task, bulkRequest, startTime, listener, responses, indicesThatCannotBeCreated); } else { final AtomicInteger counter = new AtomicInteger(autoCreateIndices.size()); for (String index : autoCreateIndices) { createIndex(index, bulkRequest.timeout(), new ActionListener<CreateIndexResponse>() { @Override public void onResponse(CreateIndexResponse result) { if (counter.decrementAndGet() == 0) { executeBulk(task, bulkRequest, startTime, listener, responses, indicesThatCannotBeCreated); } } @Override public void onFailure(Exception e) { if (!(ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException)) { // fail all requests involving this index, if create didn't work for (int i = 0; i < bulkRequest.requests.size(); i++) { DocWriteRequest<?> request = bulkRequest.requests.get(i); if (request != null && setResponseFailureIfIndexMatches(responses, i, request, index, e)) { bulkRequest.requests.set(i, null); } } } if (counter.decrementAndGet() == 0) { executeBulk(task, bulkRequest, startTime, ActionListener.wrap(listener::onResponse, inner -> { inner.addSuppressed(e); listener.onFailure(inner); }), responses, indicesThatCannotBeCreated); } } }); } } } else { executeBulk(task, bulkRequest, startTime, listener, responses, emptyMap()); } }	we prefer == false instead of ! as the latter is susceptible to being missed when reading through code.
public void executeBulkRequest(Iterable<DocWriteRequest<?>> actionRequests, BiConsumer<IndexRequest, Exception> itemFailureHandler, Consumer<Exception> completionHandler) { threadPool.executor(ThreadPool.Names.WRITE).execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { completionHandler.accept(e); } @Override protected void doRun() throws Exception { for (DocWriteRequest<?> actionRequest : actionRequests) { IndexRequest indexRequest = null; if (actionRequest instanceof IndexRequest) { indexRequest = (IndexRequest) actionRequest; } else if (actionRequest instanceof UpdateRequest) { UpdateRequest updateRequest = (UpdateRequest) actionRequest; indexRequest = updateRequest.docAsUpsert() ? updateRequest.doc() : updateRequest.upsertRequest(); } if (indexRequest != null && Strings.hasText(indexRequest.getPipeline())) { try { innerExecute(indexRequest, getPipeline(indexRequest.getPipeline())); //this shouldn't be needed here but we do it for consistency with index api // which requires it to prevent double execution indexRequest.setPipeline(""); } catch (Exception e) { itemFailureHandler.accept(indexRequest, e); } } } completionHandler.accept(null); } }); }	had to change this since null now triggers an empty loop because it causes the default pipeline to be set over and over.
@Override public UnaryOperator<Map<String, IndexTemplateMetadata>> getIndexTemplateMetadataUpgrader() { return map -> { List<IndexTemplateMetadata> monitoringTemplates = createMonitoringTemplates(getMissingMonitoringTemplateIds(map)); for (IndexTemplateMetadata newTemplate : monitoringTemplates) { map.put(newTemplate.getName(), newTemplate); } map.entrySet().removeIf(Monitoring::isTypedAPMTemplate); // this template was not migrated to typeless due to the possibility of the old /_monitoring/bulk API being used // see {@link org.elasticsearch.xpack.core.monitoring.exporter.MonitoringTemplateUtils#OLD_TEMPLATE_VERSION} // however the bulk API is not typed (the type field is for the docs, a field inside the docs) so it's safe to remove this // old template and rely on the updated, typeless, .monitoring-alerts-7 template map.remove(".monitoring-alerts"); return map; }; } /** * Returns a list of template IDs (as defined by {@link MonitoringTemplateUtils#TEMPLATE_IDS}) that are not present in the provided * map or don't have at least {@link MonitoringTemplateUtils#LAST_UPDATED_VERSION}	nit: s/created/creating (unless this matches other messages) ideally the log message would happen after the actual action was performed.
@Override public UnaryOperator<Map<String, IndexTemplateMetadata>> getIndexTemplateMetadataUpgrader() { return map -> { List<IndexTemplateMetadata> monitoringTemplates = createMonitoringTemplates(getMissingMonitoringTemplateIds(map)); for (IndexTemplateMetadata newTemplate : monitoringTemplates) { map.put(newTemplate.getName(), newTemplate); } map.entrySet().removeIf(Monitoring::isTypedAPMTemplate); // this template was not migrated to typeless due to the possibility of the old /_monitoring/bulk API being used // see {@link org.elasticsearch.xpack.core.monitoring.exporter.MonitoringTemplateUtils#OLD_TEMPLATE_VERSION} // however the bulk API is not typed (the type field is for the docs, a field inside the docs) so it's safe to remove this // old template and rely on the updated, typeless, .monitoring-alerts-7 template map.remove(".monitoring-alerts"); return map; }; } /** * Returns a list of template IDs (as defined by {@link MonitoringTemplateUtils#TEMPLATE_IDS}) that are not present in the provided * map or don't have at least {@link MonitoringTemplateUtils#LAST_UPDATED_VERSION}	is this catch necessary ?
@Override public UnaryOperator<Map<String, IndexTemplateMetadata>> getIndexTemplateMetadataUpgrader() { return map -> { List<IndexTemplateMetadata> monitoringTemplates = createMonitoringTemplates(getMissingMonitoringTemplateIds(map)); for (IndexTemplateMetadata newTemplate : monitoringTemplates) { map.put(newTemplate.getName(), newTemplate); } map.entrySet().removeIf(Monitoring::isTypedAPMTemplate); // this template was not migrated to typeless due to the possibility of the old /_monitoring/bulk API being used // see {@link org.elasticsearch.xpack.core.monitoring.exporter.MonitoringTemplateUtils#OLD_TEMPLATE_VERSION} // however the bulk API is not typed (the type field is for the docs, a field inside the docs) so it's safe to remove this // old template and rely on the updated, typeless, .monitoring-alerts-7 template map.remove(".monitoring-alerts"); return map; }; } /** * Returns a list of template IDs (as defined by {@link MonitoringTemplateUtils#TEMPLATE_IDS}) that are not present in the provided * map or don't have at least {@link MonitoringTemplateUtils#LAST_UPDATED_VERSION}	the apm template(s) are separate from the monitoring templates. can you move this out of the monitoring code (likely a different pr) ?
@Override public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, final LeafBucketCollector sub) throws IOException { if (valuesSource == null) { return LeafBucketCollector.NO_OP_COLLECTOR; } final BigArrays bigArrays = context.bigArrays(); if (valuesSource instanceof ValuesSource.Histogram) { final HistogramValues values = ((ValuesSource.Histogram)valuesSource).getHistogramValues(ctx); return collectHistogramValues(values, bigArrays, sub); } else { final SortedNumericDoubleValues values = ((ValuesSource.Numeric)valuesSource).doubleValues(ctx); return collectNumeric(values, bigArrays, sub); } }	is it a good idea to be doing an instanceof operation for every document here? maybe the performance of instanceof is negligible these days but if not, given this is run in a very tight loop, maybe we should be choosing a "mode" in the constructor and then checking that enum/boolean each time instead?
public static final Function<Aggregation, InternalAggregation> FUNCTION = new Function<Aggregation, InternalAggregation>() { @Override public InternalAggregation apply(Aggregation input) { return (InternalAggregation) input; } }	not in this pr, but in this file... can you change parsefield("bucketspath") to parsefield("buckets_path") please don't use camelcasing
public void aggregations(InternalAggregations aggregations) { this.aggregations = aggregations; }	can it really only work for siblings or is it just that we only have siblings for now?
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { if (request.getRestApiVersion() == RestApiVersion.V_7 && request.hasParam(INCLUDE_TYPE_NAME_PARAMETER)) { request.param(INCLUDE_TYPE_NAME_PARAMETER); deprecationLogger.compatibleApiWarning("get_mapping_with_types", TYPES_DEPRECATION_MESSAGE); } final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); final GetMappingsRequest getMappingsRequest = new GetMappingsRequest(); getMappingsRequest.indices(indices); getMappingsRequest.indicesOptions(IndicesOptions.fromRequest(request, getMappingsRequest.indicesOptions())); final TimeValue timeout = request.paramAsTime("master_timeout", getMappingsRequest.masterNodeTimeout()); getMappingsRequest.masterNodeTimeout(timeout); getMappingsRequest.local(request.paramAsBoolean("local", getMappingsRequest.local())); final HttpChannel httpChannel = request.getHttpChannel(); return channel -> new RestCancellableNodeClient(client, httpChannel).admin().indices().getMappings(getMappingsRequest, new RestActionListener<>(channel) { @Override protected void processResponse(GetMappingsResponse getMappingsResponse) { final long startTimeMs = threadPool.relativeTimeInMillis(); // Process serialization on MANAGEMENT pool since the serialization of the raw mappings to XContent can be too slow to // execute on an IO thread threadPool.executor(ThreadPool.Names.MANAGEMENT).execute( ActionRunnable.wrap(this, l -> new RestBuilderListener<GetMappingsResponse>(channel) { @Override public RestResponse buildResponse(final GetMappingsResponse response, final XContentBuilder builder) throws Exception { if (threadPool.relativeTimeInMillis() - startTimeMs > timeout.millis()) { throw new ElasticsearchTimeoutException("Timed out getting mappings"); } builder.startObject(); response.toXContent(builder, request); builder.endObject(); return new BytesRestResponse(RestStatus.OK, builder); } }.onResponse(getMappingsResponse))); } }); }	this is quite similar to dispatchingresttoxcontentlistener, with deficiencies on both sides: - here we have a timeout but dispatchingresttoxcontentlistener doesn't - dispatchingresttoxcontentlistener checks for cancellation after dispatch but here we don't - here we don't have a proper toxcontentobject, we call startobject() and endobject() ourselves, but dispatchingresttoxcontentlistener requires a toxcontentobject. what do you think about moving to dispatchingresttoxcontentlistener here? i think it'd just need a little adapter object to implement the timeout and the outer xcontent object wrapper, see e.g. restclusterstateresponse.
public ClusterState execute(ClusterState currentState) { NodesShutdownMetadata currentShutdownMetadata = currentState.metadata().custom(NodesShutdownMetadata.TYPE); if (currentShutdownMetadata == null) { currentShutdownMetadata = new NodesShutdownMetadata(new HashMap<>()); } // Verify that there's not already a shutdown metadata for this node SingleNodeShutdownMetadata existingRecord = currentShutdownMetadata.getAllNodeMetadataMap().get(request.getNodeId()); if (existingRecord != null) { logger.trace( "replacing existing shutdown record for node [{}] of type [{}] with reason [{}] with new type [{}] and reason [{}]", existingRecord.getNodeId(), existingRecord.getType(), existingRecord.getReason(), request.getType(), request.getReason() ); } SingleNodeShutdownMetadata newNodeMetadata = SingleNodeShutdownMetadata.builder() .setNodeId(request.getNodeId()) .setType(request.getType()) .setReason(request.getReason()) .setStartedAtMillis(System.currentTimeMillis()) .build(); return ClusterState.builder(currentState) .metadata( Metadata.builder(currentState.metadata()) .putCustom(NodesShutdownMetadata.TYPE, currentShutdownMetadata.putSingleNodeMetadata(newNodeMetadata)) ) .build(); }	i think this is a rare enough occurrence, and useful enough, to update to this: suggestion logger.info( "updating existing shutdown record for node [{}] of type [{}] with reason [{}] with new type [{}] and reason [{}]",
public ClusterState execute(ClusterState currentState) { DiscoveryNodes nodes = currentState.nodes(); SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); if (snapshots == null) { return currentState; } boolean changed = false; ArrayList<SnapshotsInProgress.Entry> entries = new ArrayList<>(); for (final SnapshotsInProgress.Entry snapshot : snapshots.entries()) { SnapshotsInProgress.Entry updatedSnapshot = snapshot; if (snapshot.state() == State.STARTED || snapshot.state() == State.ABORTED) { ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder(); boolean snapshotChanged = false; for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards()) { final ShardSnapshotStatus shardStatus = shardEntry.value; final ShardId shardId = shardEntry.key; if (!shardStatus.state().completed() && shardStatus.nodeId() != null) { if (nodes.nodeExists(shardStatus.nodeId())) { shards.put(shardId, shardStatus); } else { // TODO: Restart snapshot on another node? snapshotChanged = true; logger.warn("failing snapshot of shard [{}] on closed node [{}]", shardId, shardStatus.nodeId()); shards.put(shardId, new ShardSnapshotStatus(shardStatus.nodeId(), ShardState.FAILED, "node shutdown", shardStatus.generation())); } } else { shards.put(shardId, shardStatus); } } if (snapshotChanged) { changed = true; ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardsMap = shards.build(); if (!snapshot.state().completed() && completed(shardsMap.values())) { updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, State.SUCCESS, shardsMap); } else { updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, snapshot.state(), shardsMap); } } entries.add(updatedSnapshot); } else if (snapshot.state() == State.INIT && initializingSnapshots.contains(snapshot.snapshot()) == false) { changed = true; // A snapshot in INIT state hasn't yet written anything to the repository so we simply remove it // from the cluster state without any further cleanup continue; } assert updatedSnapshot.shards().size() == snapshot.shards().size() : "Shard count changed during snapshot status update from [" + snapshot + "] to [" + updatedSnapshot + "]"; } if (changed) { return ClusterState.builder(currentState) .putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(unmodifiableList(entries))).build(); } return currentState; }	no need to call continue here?
@Override protected void createCheckpoint(ActionListener<DataFrameTransformCheckpoint> listener) { checkpointProvider.createNextCheckpoint(getLastCheckpoint(), ActionListener.wrap( checkpoint -> transformsConfigManager.putTransformCheckpoint(checkpoint, ActionListener.wrap( putCheckPointResponse -> listener.onResponse(checkpoint), createCheckpointException -> { logger.warn(new ParameterizedMessage("[{}] failed to create checkpoint.", transformId), createCheckpointException); listener.onFailure( new RuntimeException("Failed to create checkpoint due to " + createCheckpointException.getMessage(), createCheckpointException)); } )), getCheckPointException -> { logger.warn(new ParameterizedMessage("[{}] failed to retrieve checkpoint.", transformId), getCheckPointException); listener.onFailure( new RuntimeException("Failed to retrieve checkpoint due to " + getCheckPointException.getMessage(), getCheckPointException)); } )); }	won't the exception message be repeated here if the exception also the cause?
synchronized void handleFailure(Exception e) { logger.warn(new ParameterizedMessage("[{}] data frame transform encountered an exception: ", transformTask.getTransformId()), e); if (handleCircuitBreakingException(e)) { return; } if (isIrrecoverableFailure(e) || failureCount.incrementAndGet() > transformTask.getNumFailureRetries()) { String failureMessage = isIrrecoverableFailure(e) ? "task encountered irrecoverable failure: " + e.getMessage() : "task encountered more than " + transformTask.getNumFailureRetries() + " failures; latest failure: " + e.getMessage(); failIndexer(failureMessage); } else { // Since our schedule fires again very quickly after failures it is possible to run into the same failure numerous // times in a row, very quickly. We do not want to spam the audit log with repeated failures, so only record the first one if (e.getMessage().equals(lastAuditedExceptionMessage) == false) { auditor.warning(transformTask.getTransformId(), "Data frame transform encountered an exception: " + e.getMessage() + " Will attempt again at next scheduled trigger."); lastAuditedExceptionMessage = e.getMessage(); } } }	nit: do we need a dot after the message? it seems like 2 different sentences.
@SuppressWarnings("unchecked") static void updateMapping(RollupJob job, ActionListener<AcknowledgedResponse> listener, PersistentTasksService persistentTasksService, Client client, Logger logger) { final String indexName = job.getConfig().getRollupIndex(); CheckedConsumer<GetMappingsResponse, Exception> getMappingResponseHandler = getMappingResponse -> { MappingMetaData mappings = getMappingResponse.getMappings().get(indexName).get(RollupField.TYPE_NAME); Object m = mappings.getSourceAsMap().get("_meta"); if (m == null) { String msg = "Rollup indices cannot co-mingle with non-rollup data (expected to find _meta key in " + "mapping of rollup index [" + indexName + "] but not found)."; logger.error(msg); listener.onFailure(new RuntimeException(msg)); return; } Map<String, Object> metadata = (Map<String, Object>) m; if (metadata.get(RollupField.ROLLUP_META) == null) { String msg = "Rollup indices cannot co-mingle with non-rollup data (expected to find " + "rollup meta key [" + RollupField.ROLLUP_META + "] in mapping of rollup index [" + indexName + "] but not found)."; logger.error(msg); listener.onFailure(new RuntimeException(msg)); return; } Map<String, Object> rollupMeta = (Map<String, Object>)((Map<String, Object>) m).get(RollupField.ROLLUP_META); String stringVersion = (String)((Map<String, Object>) m).get(Rollup.ROLLUP_TEMPLATE_VERSION_FIELD); if (stringVersion == null) { listener.onFailure(new IllegalStateException("Could not determine version of existing rollup metadata for index [" + indexName + "]")); return; } if (rollupMeta.get(job.getConfig().getId()) != null) { String msg = "Cannot create rollup job [" + job.getConfig().getId() + "] because job was previously created (existing metadata)."; logger.error(msg); listener.onFailure(new ElasticsearchStatusException(msg, RestStatus.CONFLICT)); return; } rollupMeta.put(job.getConfig().getId(), job.getConfig()); metadata.put(RollupField.ROLLUP_META, rollupMeta); Map<String, Object> newMapping = mappings.getSourceAsMap(); newMapping.put("_meta", metadata); PutMappingRequest request = new PutMappingRequest(indexName); request.type(RollupField.TYPE_NAME); request.source(newMapping); client.execute(PutMappingAction.INSTANCE, request, ActionListener.wrap(putMappingResponse -> startPersistentTask(job, listener, persistentTasksService), listener::onFailure)); }; GetMappingsRequest request = new GetMappingsRequest(); client.execute(GetMappingsAction.INSTANCE, request, ActionListener.wrap(getMappingResponseHandler, e -> { String msg = "Could not update mappings for rollup job [" + job.getConfig().getId() + "]"; logger.error(msg); listener.onFailure(new RuntimeException(msg, e)); })); }	i think co-exist might be better than co-mingle here? also this message might still be a little confusing because its comparing a rollup indices with non-rollup data, should we instead say either called both indices or both data?
@SuppressWarnings("unchecked") static void updateMapping(RollupJob job, ActionListener<AcknowledgedResponse> listener, PersistentTasksService persistentTasksService, Client client, Logger logger) { final String indexName = job.getConfig().getRollupIndex(); CheckedConsumer<GetMappingsResponse, Exception> getMappingResponseHandler = getMappingResponse -> { MappingMetaData mappings = getMappingResponse.getMappings().get(indexName).get(RollupField.TYPE_NAME); Object m = mappings.getSourceAsMap().get("_meta"); if (m == null) { String msg = "Rollup indices cannot co-mingle with non-rollup data (expected to find _meta key in " + "mapping of rollup index [" + indexName + "] but not found)."; logger.error(msg); listener.onFailure(new RuntimeException(msg)); return; } Map<String, Object> metadata = (Map<String, Object>) m; if (metadata.get(RollupField.ROLLUP_META) == null) { String msg = "Rollup indices cannot co-mingle with non-rollup data (expected to find " + "rollup meta key [" + RollupField.ROLLUP_META + "] in mapping of rollup index [" + indexName + "] but not found)."; logger.error(msg); listener.onFailure(new RuntimeException(msg)); return; } Map<String, Object> rollupMeta = (Map<String, Object>)((Map<String, Object>) m).get(RollupField.ROLLUP_META); String stringVersion = (String)((Map<String, Object>) m).get(Rollup.ROLLUP_TEMPLATE_VERSION_FIELD); if (stringVersion == null) { listener.onFailure(new IllegalStateException("Could not determine version of existing rollup metadata for index [" + indexName + "]")); return; } if (rollupMeta.get(job.getConfig().getId()) != null) { String msg = "Cannot create rollup job [" + job.getConfig().getId() + "] because job was previously created (existing metadata)."; logger.error(msg); listener.onFailure(new ElasticsearchStatusException(msg, RestStatus.CONFLICT)); return; } rollupMeta.put(job.getConfig().getId(), job.getConfig()); metadata.put(RollupField.ROLLUP_META, rollupMeta); Map<String, Object> newMapping = mappings.getSourceAsMap(); newMapping.put("_meta", metadata); PutMappingRequest request = new PutMappingRequest(indexName); request.type(RollupField.TYPE_NAME); request.source(newMapping); client.execute(PutMappingAction.INSTANCE, request, ActionListener.wrap(putMappingResponse -> startPersistentTask(job, listener, persistentTasksService), listener::onFailure)); }; GetMappingsRequest request = new GetMappingsRequest(); client.execute(GetMappingsAction.INSTANCE, request, ActionListener.wrap(getMappingResponseHandler, e -> { String msg = "Could not update mappings for rollup job [" + job.getConfig().getId() + "]"; logger.error(msg); listener.onFailure(new RuntimeException(msg, e)); })); }	same comment as above
@Override protected Query newTermQuery(Term term, float boost) { return blendTerm(context, term.bytes(), tieBreaker, lenient, blendedFields); }	a couple notes: * although the cross_fields query doesn't allow phrases, the analyzephrase method must be implemented to support multi-word synonyms with auto_generate_synonyms_phrase_query: true. * i think we can remove analyzemultiphrase. this method supports phrase queries that contain a term with synonyms, which should never occur with cross_fields. i opted not to remove it since the logic is tricky and i didn't want to risk a regression when we aim to deprecate cross_fields anyways.
@Override protected void execute(Terminal terminal, OptionSet options) throws Exception { try { super.execute(terminal, options); } catch (OptionException e) { if (e.options().size() == 1 && e.options().contains("keep-ca-key")) { throw new UserException(ExitCodes.USAGE, "Generating certificates without providing a CA is no longer supported.\\\\n" + "Please first generate a CA with the 'ca' sub-command and provide the ca file \\\\n" + "with either --ca or --ca-cert/--ca-key to generate certificates.\\\\n" + "Alternatively, you can also generate self-signed certificates with --self-signed."); } else { throw e; } } }	i would drop the sentence about --self-signed.
private static Map<String, RoleDescriptor> initializeReservedRoles() { return MapBuilder.<String, RoleDescriptor>newMapBuilder() .put("superuser", SUPERUSER_ROLE_DESCRIPTOR) .put("transport_client", new RoleDescriptor("transport_client", new String[] { "transport_client" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_user", new RoleDescriptor("kibana_user", null, null, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("all").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("monitoring_user", new RoleDescriptor("monitoring_user", new String[] { "cluster:monitor/main", "cluster:monitor/xpack/info" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_agent", new RoleDescriptor("remote_monitoring_agent", new String[] { "manage_index_templates", "manage_ingest_pipelines", "monitor", "cluster:monitor/xpack/watcher/watch/get", "cluster:admin/xpack/watcher/watch/put", "cluster:admin/xpack/watcher/watch/delete", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices("metricbeat-*").privileges("index", "create_index").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_collector", new RoleDescriptor( "remote_monitoring_collector", new String[] { "monitor" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices("*").privileges("monitor").allowRestrictedIndices(true).build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*").privileges("read").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null )) .put("ingest_admin", new RoleDescriptor("ingest_admin", new String[] { "manage_index_templates", "manage_pipeline" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) // reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role .put("reporting_user", new RoleDescriptor("reporting_user", null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_dashboard_only_user", new RoleDescriptor( "kibana_dashboard_only_user", null, null, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put(KibanaUser.ROLE_NAME, new RoleDescriptor(KibanaUser.ROLE_NAME, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", "manage_token" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*", ".reporting-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".code-*").privileges("all").build(), }, null, new ConditionalClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("logstash_system", new RoleDescriptor("logstash_system", new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("beats_admin", new RoleDescriptor("beats_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.BEATS_ROLE, new RoleDescriptor(UsernamesField.BEATS_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.APM_ROLE, new RoleDescriptor(UsernamesField.APM_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("apm_user", new RoleDescriptor("apm_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("apm-*") .privileges("read", "view_index_metadata").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*") .privileges("view_index_metadata", "read").build(), }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_user", new RoleDescriptor("machine_learning_user", new String[] { "monitor_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-notifications*") .privileges("view_index_metadata", "read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*") .privileges("view_index_metadata", "read", "write").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_admin", new RoleDescriptor("machine_learning_admin", new String[] { "manage_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".ml-anomalies*", ".ml-notifications*", ".ml-state*", ".ml-meta*") .privileges("view_index_metadata", "read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*") .privileges("view_index_metadata", "read", "write").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("data_frame_transforms_admin", new RoleDescriptor("data_frame_transforms_admin", new String[] { "manage_data_frame_transforms" }, new RoleDescriptor.IndicesPrivileges[]{ RoleDescriptor.IndicesPrivileges.builder() .indices(".data-frame-notifications*") .privileges("view_index_metadata", "read").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("data_frame_transforms_user", new RoleDescriptor("data_frame_transforms_user", new String[] { "monitor_data_frame_transforms" }, new RoleDescriptor.IndicesPrivileges[]{ RoleDescriptor.IndicesPrivileges.builder() .indices(".data-frame-notifications*") .privileges("view_index_metadata", "read").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("watcher_admin", new RoleDescriptor("watcher_admin", new String[] { "manage_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX, TriggeredWatchStoreField.INDEX_NAME, HistoryStoreField.INDEX_PREFIX + "*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_user", new RoleDescriptor("watcher_user", new String[] { "monitor_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX) .privileges("read") .build(), RoleDescriptor.IndicesPrivileges.builder().indices(HistoryStoreField.INDEX_PREFIX + "*") .privileges("read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("logstash_admin", new RoleDescriptor("logstash_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".logstash*") .privileges("create", "delete", "index", "manage", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_user", new RoleDescriptor("rollup_user", new String[] { "monitor_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_admin", new RoleDescriptor("rollup_admin", new String[] { "manage_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("code_admin", new RoleDescriptor("code_admin", new String[] {}, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".code-*").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("code_user", new RoleDescriptor("code_user", new String[] {}, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".code-*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("snapshot_user", new RoleDescriptor("snapshot_user", new String[] { "create_snapshot", GetRepositoriesAction.NAME }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices("*") .privileges("view_index_metadata") .allowRestrictedIndices(true) .build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .immutableMap(); }	please open a separate pr on the stack docs similar to elastic/stack-docs#246 that updates the docs for the definition of these two roles. it causes problems for people who create their own custom roles if we don't accurately state which privileges our built in roles provide.
* @param username the username to supply, or null * @param password the password to supply, or null * @return the response from the server * @throws IOException if an error occurs */ private static HttpResponse execute(Request request, String username, String password) throws IOException { final Executor executor = Executor.newInstance(); if (username != null && password != null) { executor.auth(username, password); executor.authPreemptive(new HttpHost("localhost", 9200)); } return executor.execute(request).returnResponse(); }	this is just a preference, but i think it is easier to read like this: java try (socket s = new socket(inetaddress.getloopbackaddress(), 9200)) { return; } catch (ioexception e) { // ignore, only want to establish a connection } try { thread.sleep(1000); } catch (interruptedexception e) { thread.currentthread().interrupt(); return; }
@Override public RestChannelConsumer doCatRequest(final RestRequest request, final NodeClient client) { final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); final IndicesOptions indicesOptions = IndicesOptions.fromRequest(request, IndicesOptions.strictExpand()); // needed only in v8 to catch breaking usages and may be removed in v9 if (request.hasParam("local") && Version.CURRENT.major == Version.V_7_0_0.major + 1) { throw new IllegalArgumentException("parameter [local] is not supported"); } final TimeValue masterNodeTimeout = request.paramAsTime("master_timeout", DEFAULT_MASTER_NODE_TIMEOUT); final boolean includeUnloadedSegments = request.paramAsBoolean("include_unloaded_segments", false); return channel -> { final ActionListener<Table> listener = ActionListener.notifyOnce(new RestResponseListener<>(channel) { @Override public RestResponse buildResponse(final Table table) throws Exception { return RestTable.buildResponse(table, channel); } }); sendGetSettingsRequest(indices, indicesOptions, masterNodeTimeout, client, new ActionListener<>() { @Override public void onResponse(final GetSettingsResponse getSettingsResponse) { final GroupedActionListener<ActionResponse> groupedListener = createGroupedListener(request, 4, listener); groupedListener.onResponse(getSettingsResponse); // The list of indices that will be returned is determined by the indices returned from the Get Settings call. // All the other requests just provide additional detail, and wildcards may be resolved differently depending on the // type of request in the presence of security plugins (looking at you, ClusterHealthRequest), so // force the IndicesOptions for all the sub-requests to be as inclusive as possible. final IndicesOptions subRequestIndicesOptions = IndicesOptions.lenientExpandHidden(); // Indices that were successfully resolved during the get settings request might be deleted when the subsequent cluster // state, cluster health and indices stats requests execute. We have to distinguish two cases: // 1) the deleted index was explicitly passed as parameter to the /_cat/indices request. In this case we want the // subsequent requests to fail. // 2) the deleted index was resolved as part of a wildcard or _all. In this case, we want the subsequent requests not to // fail on the deleted index (as we want to ignore wildcards that cannot be resolved). // This behavior can be ensured by letting the cluster state, cluster health and indices stats requests re-resolve the // index names with the same indices options that we used for the initial cluster state request (strictExpand). sendIndicesStatsRequest(indices, subRequestIndicesOptions, includeUnloadedSegments, client, ActionListener.wrap(groupedListener::onResponse, groupedListener::onFailure)); sendClusterStateRequest(indices, subRequestIndicesOptions, masterNodeTimeout, client, ActionListener.wrap(groupedListener::onResponse, groupedListener::onFailure)); sendClusterHealthRequest(indices, subRequestIndicesOptions, masterNodeTimeout, client, ActionListener.wrap(groupedListener::onResponse, groupedListener::onFailure)); } @Override public void onFailure(final Exception e) { listener.onFailure(e); } }); }; }	this is unnecessary, as mentioned in https://github.com/elastic/elasticsearch/pull/64867#discussion_r520703765
private String runForbiddenAPIsCli() throws IOException { ByteArrayOutputStream errorOut = new ByteArrayOutputStream(); ExecResult result = getProject().javaexec(spec -> { if (javaHome != null) { spec.setExecutable(javaHome + "/bin/java"); } spec.classpath( getForbiddenAPIsConfiguration(), getRuntimeConfiguration(), getProject().getConfigurations().getByName("compileOnly") ); spec.setMain("de.thetaphi.forbiddenapis.cli.CliMain"); spec.args( "-f", getSignatureFile().getAbsolutePath(), "-d", getJarExpandDir(), "--allowmissingclasses" ); spec.setErrorOutput(errorOut); if (getLogger().isInfoEnabled() == false) { spec.setStandardOutput(new NullOutputStream()); } spec.setIgnoreExitValue(true); }); if (OS.current().equals(OS.LINUX) && result.getExitValue() == SIG_KILL_EXIT_VALUE) { throw new IllegalStateException( "Third party audit was killed buy SIGKILL, could be a victim of the Linux OOM killer" ); } final String forbiddenApisOutput; try (ByteArrayOutputStream outputStream = errorOut) { forbiddenApisOutput = outputStream.toString(StandardCharsets.UTF_8.name()); } return forbiddenApisOutput; }	does the process return a stable exit code on a "normal" failure? if so, could we just check to see if the exit code was anything but the expected one and return an error? also, i don't think we need to filter this to just linux. if this process is killed or terminates awkwardly for some reason on any other os that's worth reporting even if it's unlikely to happen.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(HIGHLIGHT_ELEMENT_NAME); innerXContent(builder); builder.endObject(); return builder; } /** * Creates a new {@link HighlightBuilder} from the highlighter held by the {@link QueryParseContext} * in {@link org.elasticsearch.common.xcontent.XContent} format * * @param parseContext * the input parse context. The state on the parser contained in * this context will be changed as a side effect of this method * call * @return the new {@link HighlightBuilder}	can we not make the abstracthighlighterbuilder define a fromxcontent() method so it has the code for parsing the common attributes and then defer to a dofromxcontent() to read the implementation specific attributes in highlighterbuilder and field? i am worried that if we duplicate this code in each implementation we will end up with parsing inconsistencies over time
public static HighlightBuilder fromXContent(QueryParseContext parseContext) throws IOException { XContentParser parser = parseContext.parser(); XContentParser.Token token; String topLevelFieldName = null; HighlightBuilder highlightBuilder = new HighlightBuilder(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { topLevelFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_ARRAY) { if (parseContext.parseFieldMatcher().match(topLevelFieldName, PRE_TAGS_FIELD)) { List<String> preTagsList = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { preTagsList.add(parser.text()); } highlightBuilder.preTags(preTagsList.toArray(new String[preTagsList.size()])); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, POST_TAGS_FIELD)) { List<String> postTagsList = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { postTagsList.add(parser.text()); } highlightBuilder.postTags(postTagsList.toArray(new String[postTagsList.size()])); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, FIELDS_FIELD)) { highlightBuilder.useExplicitFieldOrder(true); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token == XContentParser.Token.START_OBJECT) { String highlightFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { if (highlightFieldName != null) { throw new ParsingException(parser.getTokenLocation(), "If highlighter fields is an array it must contain objects containing a single field"); } highlightFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_OBJECT) { highlightBuilder.field(Field.fromXContent(highlightFieldName, parseContext)); } } } else { throw new ParsingException(parser.getTokenLocation(), "If highlighter fields is an array it must contain objects containing a single field"); } } } else { throw new ParsingException(parser.getTokenLocation(), "cannot parse array with name [{}]", topLevelFieldName); } } else if (token.isValue()) { if (parseContext.parseFieldMatcher().match(topLevelFieldName, ORDER_FIELD)) { highlightBuilder.order(parser.text()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, TAGS_SCHEMA_FIELD)) { highlightBuilder.tagsSchema(parser.text()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, HIGHLIGHT_FILTER_FIELD)) { highlightBuilder.highlightFilter(parser.booleanValue()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, FRAGMENT_SIZE_FIELD)) { highlightBuilder.fragmentSize(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, NUMBER_OF_FRAGMENTS_FIELD)) { highlightBuilder.numOfFragments(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, ENCODER_FIELD)) { highlightBuilder.encoder(parser.text()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, REQUIRE_FIELD_MATCH_FIELD)) { highlightBuilder.requireFieldMatch(parser.booleanValue()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, BOUNDARY_MAX_SCAN_FIELD)) { highlightBuilder.boundaryMaxScan(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, BOUNDARY_CHARS_FIELD)) { highlightBuilder.boundaryChars(parser.text().toCharArray()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, TYPE_FIELD)) { highlightBuilder.highlighterType(parser.text()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, FRAGMENTER_FIELD)) { highlightBuilder.fragmenter(parser.text()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, NO_MATCH_SIZE_FIELD)) { highlightBuilder.noMatchSize(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, FORCE_SOURCE_FIELD)) { highlightBuilder.forceSource(parser.booleanValue()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, PHRASE_LIMIT_FIELD)) { highlightBuilder.phraseLimit(parser.intValue()); } else { throw new ParsingException(parser.getTokenLocation(), "unexpected fieldname [{}]", topLevelFieldName); } } else if (token == XContentParser.Token.START_OBJECT && topLevelFieldName != null) { if (parseContext.parseFieldMatcher().match(topLevelFieldName, OPTIONS_FIELD)) { highlightBuilder.options(parser.map()); } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, FIELDS_FIELD)) { String highlightFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { highlightFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_OBJECT) { highlightBuilder.field(Field.fromXContent(highlightFieldName, parseContext)); } } } else if (parseContext.parseFieldMatcher().match(topLevelFieldName, HIGHLIGHT_QUERY_FIELD)) { highlightBuilder.highlightQuery(parseContext.parseInnerQueryBuilder()); } else { throw new ParsingException(parser.getTokenLocation(), "cannot parse object with name [{}]", topLevelFieldName); } } } if (highlightBuilder.preTags() != null && highlightBuilder.postTags() == null) { throw new ParsingException(parser.getTokenLocation(), "Highlighter global preTags are set, but global postTags are not set"); } return highlightBuilder; }	we should add an else block here too so we throw an error if we get a token of a type we are not expecting
private static HighlightBuilder.Field fromXContent(String fieldname, QueryParseContext parseContext) throws IOException { XContentParser parser = parseContext.parser(); XContentParser.Token token; final HighlightBuilder.Field field = new HighlightBuilder.Field(fieldname); String currentFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_ARRAY) { if (parseContext.parseFieldMatcher().match(currentFieldName, PRE_TAGS_FIELD)) { List<String> preTagsList = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { preTagsList.add(parser.text()); } field.preTags(preTagsList.toArray(new String[preTagsList.size()])); } else if (parseContext.parseFieldMatcher().match(currentFieldName, POST_TAGS_FIELD)) { List<String> postTagsList = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { postTagsList.add(parser.text()); } field.postTags(postTagsList.toArray(new String[postTagsList.size()])); } else if (parseContext.parseFieldMatcher().match(currentFieldName, MATCHED_FIELDS_FIELD)) { List<String> matchedFields = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { matchedFields.add(parser.text()); } field.matchedFields(matchedFields.toArray(new String[matchedFields.size()])); } else { throw new ParsingException(parser.getTokenLocation(), "cannot parse array with name [{}]", currentFieldName); } } else if (token.isValue()) { if (parseContext.parseFieldMatcher().match(currentFieldName, FRAGMENT_SIZE_FIELD)) { field.fragmentSize(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, NUMBER_OF_FRAGMENTS_FIELD)) { field.numOfFragments(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, FRAGMENT_OFFSET_FIELD)) { field.fragmentOffset(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, HIGHLIGHT_FILTER_FIELD)) { field.highlightFilter(parser.booleanValue()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, ORDER_FIELD)) { field.order(parser.text()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, REQUIRE_FIELD_MATCH_FIELD)) { field.requireFieldMatch(parser.booleanValue()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, BOUNDARY_MAX_SCAN_FIELD)) { field.boundaryMaxScan(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, BOUNDARY_CHARS_FIELD)) { field.boundaryChars(parser.text().toCharArray()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, TYPE_FIELD)) { field.highlighterType(parser.text()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, FRAGMENTER_FIELD)) { field.fragmenter(parser.text()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, NO_MATCH_SIZE_FIELD)) { field.noMatchSize(parser.intValue()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, FORCE_SOURCE_FIELD)) { field.forceSource(parser.booleanValue()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, PHRASE_LIMIT_FIELD)) { field.phraseLimit(parser.intValue()); } else { throw new ParsingException(parser.getTokenLocation(), "unexpected fieldname [{}]", currentFieldName); } } else if (token == XContentParser.Token.START_OBJECT && currentFieldName != null) { if (parseContext.parseFieldMatcher().match(currentFieldName, HIGHLIGHT_QUERY_FIELD)) { field.highlightQuery(parseContext.parseInnerQueryBuilder()); } else if (parseContext.parseFieldMatcher().match(currentFieldName, OPTIONS_FIELD)) { field.options(parser.map()); } else { throw new ParsingException(parser.getTokenLocation(), "cannot parse object with name [{}]", currentFieldName); } } } return field; }	we should add an else block here too so we throw an error if we get a token of a type we are not expecting
public static void init() { namedWriteableRegistry = new NamedWriteableRegistry(); @SuppressWarnings("rawtypes") Set<QueryParser> injectedQueryParsers = new HashSet<>(); injectedQueryParsers.add(new MatchAllQueryParser()); injectedQueryParsers.add(new IdsQueryParser()); injectedQueryParsers.add(new TermQueryParser()); indicesQueriesRegistry = new IndicesQueriesRegistry(Settings.settingsBuilder().build(), injectedQueryParsers, namedWriteableRegistry); }	can this not use the injector approach we have used in the abstractquerytestcase and the searchsourcebuildertests? that way we won't have the hassle of registering each query builder/parser when we add a test that uses a new one.
public static boolean parseMultiField(FieldMapper.Builder builder, String name, Mapper.TypeParser.ParserContext parserContext, String propName, Object propNode) { if (propName.equals("fields")) { if (parserContext.isWithinMultiField()) { deprecationLogger.deprecatedAndMaybeLog("multifield_within_multifield", "The multi-field " + "named [" + name + "] contains its own [fields] entry. Defining multi-fields within a " + "multi-field is deprecated and will no longer be supported in 8.0."); } parserContext = parserContext.createMultiFieldContext(parserContext); final Map<String, Object> multiFieldsPropNodes; if (propNode instanceof List && ((List<?>) propNode).isEmpty()) { multiFieldsPropNodes = Collections.emptyMap(); } else if (propNode instanceof Map) { multiFieldsPropNodes = (Map<String, Object>) propNode; } else { throw new MapperParsingException("expected map for property [fields] on field [" + propNode + "] or " + "[" + propName + "] but got a " + propNode.getClass()); } for (Map.Entry<String, Object> multiFieldEntry : multiFieldsPropNodes.entrySet()) { String multiFieldName = multiFieldEntry.getKey(); if (multiFieldName.contains(".")) { throw new MapperParsingException("Field name [" + multiFieldName + "] which is a multi field of [" + name + "] cannot" + " contain '.'"); } if (!(multiFieldEntry.getValue() instanceof Map)) { throw new MapperParsingException("illegal field [" + multiFieldName + "], only fields can be specified inside fields"); } @SuppressWarnings("unchecked") Map<String, Object> multiFieldNodes = (Map<String, Object>) multiFieldEntry.getValue(); String type; Object typeNode = multiFieldNodes.get("type"); if (typeNode != null) { type = typeNode.toString(); } else { throw new MapperParsingException("no type specified for property [" + multiFieldName + "]"); } if (type.equals(ObjectMapper.CONTENT_TYPE) || type.equals(ObjectMapper.NESTED_CONTENT_TYPE) || type.equals(FieldAliasMapper.CONTENT_TYPE)) { throw new MapperParsingException("Type [" + type + "] cannot be used in multi field"); } Mapper.TypeParser typeParser = parserContext.typeParser(type); if (typeParser == null) { throw new MapperParsingException("no handler for type [" + type + "] declared on field [" + multiFieldName + "]"); } builder.addMultiField(typeParser.parse(multiFieldName, multiFieldNodes, parserContext)); multiFieldNodes.remove("type"); DocumentMapperParser.checkNoRemainingFields(propName, multiFieldNodes, parserContext.indexVersionCreated()); } return true; } return false; }	kind of a shame we are specific about a name for the first occurrence and then stay quiet for the rest of the offenses. should we add a recommendation to the user to check *all* their other mappings/templates as well?
*/ void getFinishedTaskFromIndex(Task thisTask, GetTaskRequest request, ActionListener<GetTaskResponse> listener) { GetRequest get = new GetRequest(TaskPersistenceService.TASK_INDEX, TaskPersistenceService.TASK_TYPE, request.getTaskId().toString()); get.setParentTask(clusterService.localNode().getId(), thisTask.getId()); client.get(get, new ActionListener<GetResponse>() { @Override @SwallowsExceptions(reason = "?") public void onResponse(GetResponse getResponse) { try { onGetFinishedTaskFromIndex(getResponse, listener); } catch (Exception e) { listener.onFailure(e); } } @Override public void onFailure(Exception e) { if (ExceptionsHelper.unwrap(e, IndexNotFoundException.class) != null) { // We haven't yet created the index for the task results so it can't be found. listener.onFailure(new ResourceNotFoundException("task [{}] isn't running or persisted", e, request.getTaskId())); } else { listener.onFailure(e); } } }); } /** * Called with the {@linkplain GetResponse}	these are expected, even required when implementing actionlistener. either we should create an abstractactionlistener that does this for us the same way we have a abstractrunnable or we should add a permanent hack to allow this specific construct in actionlistener subclasses. the former seems like a better choice but it'd mean more work before we can get this merged.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(CONFIG.getPreferredName()); job.toXContent(builder, params); builder.field(STATUS.getPreferredName(), status); // special BWC handling for rollup builder.startObject(STATS.getPreferredName()); stats.toUnwrappedXContent(builder, true); builder.endObject(); builder.endObject(); return builder; }	the problem i see here is that there is no way for users of rollup to migrate to the new response api. i think we need a parameter passed in through the params argument to this method which the user can control to determine whether they ask for the new response format or the old one (with it defaulting to the old one. in fact, its possibly better for indexerjobstats.toxcontent() to get this parameter itself so the bwc is fully contained within that class?
private static boolean verifyBcryptHash(SecureString text, char[] hash) { String hashStr = new String(hash); if (hashStr.startsWith(BCRYPT_PREFIX) == false) { return false; } return BCrypt.checkpw(text, hashStr); } /** * Returns a list of lower case String identifiers for the Hashing algorithm and parameter * combinations that can be used for password hashing. The identifiers can be used to get * an instance of the appropriate {@link Hasher} by using {@link #resolve(String) resolve()}	nit: suggestion private static char[] sha512hash(securestring text) { (hashing doesn't inherently include a salt, so it feels strange calling it out in the method name )
@Override public MappedFieldType getFieldType(String path) { return context.getFieldType(path); }	you may also remove getmapper from this class , one less usage of getmapperservice ;)
*/ public static int parsePrecision(XContentParser parser) throws IOException, ElasticsearchParseException { XContentParser.Token token = parser.currentToken(); if (token.equals(XContentParser.Token.VALUE_NUMBER)) { return XContentMapValues.nodeIntegerValue(parser.intValue()); } else { String precision = parser.text(); try { // we want to treat simple integer strings as precision levels, not distances return XContentMapValues.nodeIntegerValue(Integer.parseInt(precision)); } catch (NumberFormatException e) { // try to parse as a distance value try { return checkPrecisionRange(GeoUtils.geoHashLevelsForPrecision(precision)); } catch (NumberFormatException e2) { // can happen when distance unit is unknown, in this case we simply want to know the reason throw e2; } catch (IllegalArgumentException e3) { // this happens when distance too small, so precision > 12. We'd like to see the original string throw new IllegalArgumentException("precision too high [" + precision + "]", e3); } } } }	can you add a short javadoc?
@Override protected void assertFromXContent(InternalStats aggregation, ParsedAggregation parsedAggregation) { assertTrue(parsedAggregation instanceof ParsedStats); ParsedStats parsed = (ParsedStats) parsedAggregation; commonStatsAssertions(aggregation, parsed); }	can you rename this method, i'd like method names to contain a verb which indicates what they do e.g. assertstats ?
public static void init() throws Exception { assumeFalse("Can't run in a FIPS JVM, there is no DOM XMLSignature Factory so we can't sign XML documents", inFipsJvm()); assumeFalse("https://github.com/elastic/elasticsearch/issues/49742",System.getProperty("java.vendor", "").contains("Azul")); // TODO: Refactor the signing to use org.opensaml.xmlsec.signature.support.Signer so that we can run the tests SamlUtils.initialize(LogManager.getLogger(SamlAuthenticatorTests.class)); // Initialise Apache XML security so that the signDoc methods work correctly. Init.init(); }	string is azul systems, inc.
@Override public Boolean fold() { if (value.dataType() == DataType.NULL || list.size() == 1 && list.get(0).dataType() == DataType.NULL) { return false; } Object foldedLeftValue = value.fold(); Boolean result = false; for (Expression rightValue : list) { Boolean compResult = Comparisons.eq(foldedLeftValue, rightValue.fold()); if (compResult == null) { result = null; } else if (compResult) { return true; } } return result; }	better readability if additional brackets are used? if (value.datatype() == datatype.null || (list.size() == 1 && list.get(0).datatype() == datatype.null))
void cancelTaskAndDescendants(CancellableTask task, String reason, boolean waitForCompletion, ActionListener<Void> listener) { if (task.shouldCancelChildrenOnCancellation()) { StepListener<Void> completedListener = new StepListener<>(); GroupedActionListener<Void> groupedListener = new GroupedActionListener<>(ActionListener.map(completedListener, r -> null), 3); Collection<DiscoveryNode> childrenNodes = taskManager.startBanOnChildrenNodes(task.getId(), () -> groupedListener.onResponse(null)); taskManager.cancel(task, reason, () -> groupedListener.onResponse(null)); StepListener<Void> banOnNodesListener = new StepListener<>(); setBanOnNodes(reason, waitForCompletion, task, childrenNodes, banOnNodesListener); banOnNodesListener.whenComplete(groupedListener::onResponse, groupedListener::onFailure); final Runnable removeBansRunnable = transportService.getThreadPool().getThreadContext() .preserveContext(() -> removeBanOnNodes(task, childrenNodes)); // We remove bans after all child tasks are completed although in theory we can do it on a per-node basis. completedListener.whenComplete(r -> removeBansRunnable.run(), e -> removeBansRunnable.run()); // if wait_for_completion is true, then only return when (1) bans are placed on child nodes, (2) child tasks are // completed or failed, (3) the main task is cancelled. Otherwise, return after bans are placed on child nodes. if (waitForCompletion) { completedListener.whenComplete(r -> listener.onResponse(null), listener::onFailure); } else { banOnNodesListener.whenComplete(r -> listener.onResponse(null), listener::onFailure); } } else { logger.trace("task {} doesn't have any children that should be cancelled", task.getId()); if (waitForCompletion) { taskManager.cancel(task, reason, () -> listener.onResponse(null)); } else { taskManager.cancel(task, reason, () -> {}); listener.onResponse(null); } } }	can you add a comment saying why this is necessary?
public void collect(int doc, long bucket) throws IOException { assert bucket == 0; if(values.advanceExact(doc)){ final int valuesCount = values.docValueCount(); double prevVal = Double.NEGATIVE_INFINITY; for (int i = 0; i < valuesCount; ++i) { double val = values.nextValue(); assert val >= prevVal; if (val == prevVal){ continue; } collectValue(sub, doc, val); } } }	we *tend* to prefer: if (numcachedocs != cachelimit) { return; } it so much wider but it makes the return just stand out a bit better. probably just my legacy as a java developer for so many years kicking in.
public static List<CompressedXContent> collectMappings(final ClusterState state, final String templateName, final String indexName, final NamedXContentRegistry xContentRegistry) throws Exception { final ComposableIndexTemplate template = state.metadata().templatesV2().get(templateName); assert template != null : "attempted to resolve mappings for a template [" + templateName + "] that did not exist in the cluster state"; if (template == null) { return List.of(); } final Map<String, ComponentTemplate> componentTemplates = state.metadata().componentTemplates(); List<CompressedXContent> mappings = template.composedOf().stream() .map(componentTemplates::get) .filter(Objects::nonNull) .map(ComponentTemplate::template) .map(Template::mappings) .filter(Objects::nonNull) .collect(Collectors.toCollection(LinkedList::new)); // Add the actual index template's mappings, since it takes the highest precedence Optional.ofNullable(template.template()) .map(Template::mappings) .ifPresent(mappings::add); // add a default mapping for the `@timestamp` field, at the lowest precedence, to make bootstrapping data streams more // straightforward mappings.add(0, new CompressedXContent(wrapMappingsIfNecessary(DEFAULT_TIMESTAMP_MAPPING, xContentRegistry))); // Only include _timestamp mapping snippet if creating backing index. if (indexName.startsWith(DataStream.BACKING_INDEX_PREFIX)) { // Only if template has data stream definition this should be added and // adding this template last, since _timestamp field should have highest precedence: Optional.ofNullable(template.getDataStreamTemplate()) .map(ComposableIndexTemplate.DataStreamTemplate::getDataSteamMappingSnippet) .map(mapping -> { try (XContentBuilder builder = XContentBuilder.builder(XContentType.JSON.xContent())) { builder.value(mapping); return new CompressedXContent(BytesReference.bytes(builder)); } catch (IOException e) { throw new UncheckedIOException(e); } }) .ifPresent(mappings::add); } return Collections.unmodifiableList(mappings); }	shouldn't this be inside an if to only add this if the template has a data stream component defined?
public int getTotalNumberOfShards() { return totalNumberOfShards(); }	do we really need both the getter style and the bare style?
public void testIntegerInvalidString() throws IOException { DocumentMapper docMapper = createDocumentMapper("b", mapping(b -> { b.startObject("a").field("type", "integer").field("time_series_dimension", true).endObject(); b.startObject("b").field("type", "keyword").field("time_series_dimension", true).endObject(); })); Exception e = expectThrows(MapperParsingException.class, () -> parseDocument(docMapper, b -> b.field("a", "not_an_int"))); assertThat( e.getMessage(), equalTo("failed to parse field [a] of type [integer] in document with id 'null'. Preview of field's value: 'not_an_int'") ); }	i think this is a user-facing error message. i wonder if we should special handle the null to be more user-friendly, like say "auto-generated id"? i could imagine someone opening a bug-report due to the null showing through.
private Engine.Index getIndex(String id) { final LuceneDocument document = new LuceneDocument(); document.add(new TextField("test", "test", Field.Store.YES)); final Field idField = IdFieldMapper.standardIdField(id); // TODO tsdbid field could be different. final Field versionField = new NumericDocValuesField("_version", Versions.MATCH_ANY); final SeqNoFieldMapper.SequenceIDFields seqID = SeqNoFieldMapper.SequenceIDFields.emptySeqID(); document.add(idField); document.add(versionField); document.add(seqID.seqNo); document.add(seqID.seqNoDocValue); document.add(seqID.primaryTerm); final BytesReference source = new BytesArray(new byte[] { 1 }); final ParsedDocument doc = new ParsedDocument( versionField, seqID, id, null, Arrays.asList(document), source, XContentType.JSON, null ); return new Engine.Index(new Term("_id", Uid.encodeId(doc.id())), randomNonNegativeLong(), doc); }	will you tackle this todo here or in a follow-up? seems relevant to have recovery tests run with the tsdb id field mapper.
*/ public void updateGlobalCheckpointOnReplica(final long globalCheckpoint) { verifyReplicationTarget(); final SequenceNumbersService seqNoService = getEngine().seqNoService(); final long localCheckpoint = seqNoService.getLocalCheckpoint(); if (globalCheckpoint <= localCheckpoint) { seqNoService.updateGlobalCheckpointOnReplica(globalCheckpoint); } else { assert state() == IndexShardState.RECOVERING; } } /** * Notifies the service of the current allocation IDs in the cluster state. See * {@link GlobalCheckpointTracker#updateAllocationIdsFromMaster(Set, Set)}	can you add a message here explaining when we would expect this? also i wonder if we can make the assertion stronger by adding that recoverystate.getstage() is neither finalize nor done.
public void testFailFlush() throws IOException { Path tempDir = createTempDir(); final AtomicBoolean failWrite = new AtomicBoolean(); final AtomicBoolean simulateDiskFull = new AtomicBoolean(); TranslogConfig config = getTranslogConfig(tempDir); Translog translog = new Translog(config) { @Override TranslogWriter.ChannelFactory getChannelFactory() { final TranslogWriter.ChannelFactory factory = super.getChannelFactory(); return new TranslogWriter.ChannelFactory() { @Override public FileChannel open(Path file) throws IOException { FileChannel channel = factory.open(file); return new FilterFileChannel(channel) { @Override public int write(ByteBuffer src) throws IOException { if (failWrite.get()) { throw new IOException("boom"); } if (simulateDiskFull.get()) { if (src.limit() > 1) { final int pos = src.position(); final int limit = src.limit(); src.limit(limit / 2); super.write(src); src.position(pos); src.limit(limit); throw new IOException("no space left on device"); } } return super.write(src); } }; } }; } }; List<Translog.Location> locations = new ArrayList<>(); int opsSynced = 0; int opsAdded = 0; boolean failed = false; while(failed == false) { try { locations.add(translog.add(new Translog.Index("test", "" + opsSynced, Integer.toString(opsSynced).getBytes(Charset.forName("UTF-8"))))); opsAdded++; translog.sync(); opsSynced++; } catch (IOException ex) { failed = true; assertEquals("no space left on device", ex.getMessage()); } catch (TranslogException ex) { // we catch IOExceptions in Translog#add -- that's how we got here failed = true; assertTrue(ex.toString(), ex.getMessage().startsWith("Failed to write operation")); } simulateDiskFull.set(randomBoolean()); } simulateDiskFull.set(false); if (randomBoolean()) { try { locations.add(translog.add(new Translog.Index("test", "" + opsSynced, Integer.toString(opsSynced).getBytes(Charset.forName("UTF-8"))))); fail("we are already closed"); } catch (AlreadyClosedException ex) { assertNotNull(ex.getCause()); assertEquals(ex.getCause().getMessage(), "no space left on device"); } } Translog.TranslogGeneration translogGeneration = translog.getGeneration(); try { translog.newSnapshot(); fail("already closed"); } catch (AlreadyClosedException ex) { // all is well } try { translog.close(); if (opsAdded != opsSynced) { fail("already closed"); } } catch (AlreadyClosedException ex) { assertNotNull(ex.getCause()); } config.setTranslogGeneration(translogGeneration); try (Translog tlog = new Translog(config)){ assertEquals("lastCommitted must be 1 less than current", translogGeneration.translogFileGeneration + 1, tlog.currentFileGeneration()); assertFalse(tlog.syncNeeded()); try (Translog.Snapshot snapshot = tlog.newSnapshot()) { assertEquals(opsSynced, snapshot.estimatedTotalOperations()); for (int i = 0; i < opsSynced; i++) { assertEquals("expected operation" + i + " to be in the previous translog but wasn't", tlog.currentFileGeneration() - 1, locations.get(i).generation); Translog.Operation next = snapshot.next(); assertNotNull("operation " + i + " must be non-null", next); assertEquals(i, Integer.parseInt(next.getSource().source.toUtf8())); } } } }	in that case can look at the cause and it should have no space left on device, right?
*/ public long recoverLocallyUpToGlobalCheckpoint() { assert Thread.holdsLock(mutex) == false : "recover locally under mutex"; if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } assert recoveryState.getStage() == RecoveryState.Stage.INDEX : "unexpected recovery stage [" + recoveryState.getStage() + "]"; assert routingEntry().recoverySource().getType() == RecoverySource.Type.PEER : "not a peer recovery [" + routingEntry() + "]"; final Optional<SequenceNumbers.CommitInfo> safeCommit; final long globalCheckpoint; try { final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); safeCommit = store.findSafeIndexCommit(globalCheckpoint); } catch (org.apache.lucene.index.IndexNotFoundException e) { logger.trace("skip local recovery as no index commit found"); return UNASSIGNED_SEQ_NO; } catch (Exception e) { logger.debug("skip local recovery as failed to find the safe commit", e); return UNASSIGNED_SEQ_NO; } try { maybeCheckIndex(); // check index here and won't do it again if ops-based recovery occurs recoveryState.setStage(RecoveryState.Stage.TRANSLOG); if (safeCommit.isPresent() == false) { assert globalCheckpoint == UNASSIGNED_SEQ_NO || indexSettings.getIndexVersionCreated().before(Version.V_6_2_0) : "global checkpoint [" + globalCheckpoint + "] [ created version [" + indexSettings.getIndexVersionCreated() + "]"; logger.trace("skip local recovery as no safe commit found"); return UNASSIGNED_SEQ_NO; } assert safeCommit.get().localCheckpoint <= globalCheckpoint : safeCommit.get().localCheckpoint + " > " + globalCheckpoint; if (safeCommit.get().localCheckpoint == globalCheckpoint) { logger.trace("skip local recovery as the safe commit is up to date; safe commit {} global checkpoint {}", safeCommit.get(), globalCheckpoint); recoveryState.getTranslog().totalLocal(0); return globalCheckpoint + 1; } if (indexSettings.getIndexMetaData().getState() == IndexMetaData.State.CLOSE || IndexMetaData.INDEX_BLOCKS_WRITE_SETTING.get(indexSettings.getSettings())) { logger.trace("skip local recovery as the index was closed or not allowed to write; safe commit {} global checkpoint {}", safeCommit.get(), globalCheckpoint); recoveryState.getTranslog().totalLocal(0); return safeCommit.get().localCheckpoint + 1; } try { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (engine, snapshot) -> { recoveryState.getTranslog().totalLocal(snapshot.totalOperations()); final int recoveredOps = runTranslogRecovery(engine, snapshot, Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY, recoveryState.getTranslog()::incrementRecoveredOperations); recoveryState.getTranslog().totalLocal(recoveredOps); // adjust the total local to reflect the actual count return recoveredOps; }; innerOpenEngineAndTranslog(() -> globalCheckpoint); getEngine().recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); logger.trace("shard locally recovered up to {}", getEngine().getSeqNoStats(globalCheckpoint)); } finally { synchronized (engineMutex) { IOUtils.close(currentEngineReference.getAndSet(null)); } } } catch (Exception e) { logger.debug(new ParameterizedMessage("failed to recover shard locally up to global checkpoint {}", globalCheckpoint), e); return UNASSIGNED_SEQ_NO; } try { // we need to find the safe commit again as we should have created a new one during the local recovery final Optional<SequenceNumbers.CommitInfo> newSafeCommit = store.findSafeIndexCommit(globalCheckpoint); assert newSafeCommit.isPresent() : "no safe commit found after local recovery"; return newSafeCommit.get().localCheckpoint + 1; } catch (Exception e) { logger.debug(new ParameterizedMessage( "failed to find the safe commit after recovering shard locally up to global checkpoint {}", globalCheckpoint), e); return UNASSIGNED_SEQ_NO; } }	the new test found this issue where the index is synced flush, but the global checkpoint is still unassigned.
@Override public IngestDocument execute(IngestDocument ingestDocument) throws Exception { List<?> values = ingestDocument.getFieldValue(field, List.class, ignoreMissing); if (values == null) { if (ignoreMissing) { return ingestDocument; } throw new IllegalArgumentException("field [" + field + "] is null, cannot loop over its elements."); } List<Object> newValues = new ArrayList<>(values.size()); for (Object value : values) { Object previousValue = ingestDocument.getIngestMetadata().put("_value", value); try { if (processor.execute(ingestDocument) == null) { return null; } } finally { newValues.add(ingestDocument.getIngestMetadata().put("_value", previousValue)); } } ingestDocument.setFieldValue(field, newValues); return ingestDocument; }	technically shouldn't the ingestdocument be chained for each successive execute call?
public void writeException(Throwable throwable) throws IOException { if (throwable == null) { writeBoolean(false); } else { writeBoolean(true); boolean writeCause = true; boolean writeMessage = true; if (throwable instanceof CorruptIndexException) { writeVInt(1); writeOptionalString(((CorruptIndexException)throwable).getOriginalMessage()); writeOptionalString(((CorruptIndexException)throwable).getResourceDescription()); writeMessage = false; } else if (throwable instanceof IndexFormatTooNewException) { writeVInt(2); writeOptionalString(((IndexFormatTooNewException)throwable).getResourceDescription()); writeInt(((IndexFormatTooNewException)throwable).getVersion()); writeInt(((IndexFormatTooNewException)throwable).getMinVersion()); writeInt(((IndexFormatTooNewException)throwable).getMaxVersion()); writeMessage = false; writeCause = false; } else if (throwable instanceof IndexFormatTooOldException) { writeVInt(3); IndexFormatTooOldException t = (IndexFormatTooOldException) throwable; writeOptionalString(t.getResourceDescription()); if (t.getVersion() == null) { writeBoolean(false); writeOptionalString(t.getReason()); } else { writeBoolean(true); writeInt(t.getVersion()); writeInt(t.getMinVersion()); writeInt(t.getMaxVersion()); } writeMessage = false; writeCause = false; } else if (throwable instanceof NullPointerException) { writeVInt(4); writeCause = false; } else if (throwable instanceof NumberFormatException) { writeVInt(5); writeCause = false; } else if (throwable instanceof IllegalArgumentException) { writeVInt(6); } else if (throwable instanceof AlreadyClosedException) { writeVInt(7); } else if (throwable instanceof EOFException) { writeVInt(8); writeCause = false; } else if (throwable instanceof SecurityException) { writeVInt(9); } else if (throwable instanceof StringIndexOutOfBoundsException) { writeVInt(10); writeCause = false; } else if (throwable instanceof ArrayIndexOutOfBoundsException) { writeVInt(11); writeCause = false; } else if (throwable instanceof FileNotFoundException) { writeVInt(12); writeCause = false; } else if (throwable instanceof FileSystemException) { writeVInt(13); if (throwable instanceof NoSuchFileException) { writeVInt(0); } else if (throwable instanceof NotDirectoryException) { writeVInt(1); } else if (throwable instanceof DirectoryNotEmptyException) { writeVInt(2); } else if (throwable instanceof AtomicMoveNotSupportedException) { writeVInt(3); } else if (throwable instanceof FileAlreadyExistsException) { writeVInt(4); } else if (throwable instanceof AccessDeniedException) { writeVInt(5); } else if (throwable instanceof FileSystemLoopException) { writeVInt(6); } else { writeVInt(7); } writeOptionalString(((FileSystemException) throwable).getFile()); writeOptionalString(((FileSystemException) throwable).getOtherFile()); writeOptionalString(((FileSystemException) throwable).getReason()); writeCause = false; } else if (throwable instanceof IllegalStateException) { writeVInt(14); } else if (throwable instanceof LockObtainFailedException) { writeVInt(15); } else if (throwable instanceof InterruptedException) { writeVInt(16); writeCause = false; } else if (throwable instanceof IOException) { writeVInt(17); } else if (throwable instanceof EsRejectedExecutionException) { // TODO: remove the if branch when master is bumped to 8.0.0 assert Version.CURRENT.major < 8; if (version.before(Version.V_7_0_0_alpha1)) { /* * This is a backwards compatibility layer when speaking to nodes that still treated EsRejectedExceutionException as an * instance of ElasticsearchException. As such, we serialize this in a way that the receiving node would read this as an * EsRejectedExecutionException. */ final ElasticsearchException ex = new ElasticsearchException(throwable.getMessage()); writeVInt(0); writeVInt(59); ex.writeTo(this); writeBoolean(((EsRejectedExecutionException) throwable).isExecutorShutdown()); return; } else { writeVInt(18); writeBoolean(((EsRejectedExecutionException) throwable).isExecutorShutdown()); writeCause = false; } } else { final ElasticsearchException ex; if (throwable instanceof ElasticsearchException && ElasticsearchException.isRegistered(throwable.getClass(), version)) { ex = (ElasticsearchException) throwable; } else { ex = new NotSerializableExceptionWrapper(throwable); } writeVInt(0); writeVInt(ElasticsearchException.getId(ex.getClass())); ex.writeTo(this); return; } if (writeMessage) { writeOptionalString(throwable.getMessage()); } if (writeCause) { writeException(throwable.getCause()); } ElasticsearchException.writeStackTraces(throwable, this); } } /** * Writes a {@link NamedWriteable}	it seems like you're not planning to back port this any more. can you push a comment in the 6.x branch to note that if you change the wire level there, you need to look at 7.x code too? i don't think we will so a comment seems enough (a test is tricky)
public void testGetDataFrameTransformStats() { GetDataFrameTransformStatsRequest getStatsRequest = new GetDataFrameTransformStatsRequest("foo"); Request request = DataFrameRequestConverters.getDataFrameTransformStats(getStatsRequest); assertEquals(HttpGet.METHOD_NAME, request.getMethod()); assertThat(request.getEndpoint(), equalTo("/_data_frame/transforms/foo/_stats")); assertFalse(request.getParameters().containsKey("from")); assertFalse(request.getParameters().containsKey("size")); getStatsRequest.setPageParams(new PageParams(0, null)); request = DataFrameRequestConverters.getDataFrameTransformStats(getStatsRequest); assertEquals("0", request.getParameters().get("from")); assertEquals(null, request.getParameters().get("size")); getStatsRequest.setPageParams(new PageParams(null, 50)); request = DataFrameRequestConverters.getDataFrameTransformStats(getStatsRequest); assertEquals(null, request.getParameters().get("from")); assertEquals("50", request.getParameters().get("size")); getStatsRequest.setPageParams(new PageParams(0, 10)); request = DataFrameRequestConverters.getDataFrameTransformStats(getStatsRequest); assertEquals("0", request.getParameters().get("from")); assertEquals("10", request.getParameters().get("size")); }	i find the following syntax more readable: assertthat(request.getparameters(), hasentry("from", "0")); wdyt?
public void testGetDataFrameTransformStats() { GetDataFrameTransformStatsRequest getStatsRequest = new GetDataFrameTransformStatsRequest("foo"); Request request = DataFrameRequestConverters.getDataFrameTransformStats(getStatsRequest); assertEquals(HttpGet.METHOD_NAME, request.getMethod()); assertThat(request.getEndpoint(), equalTo("/_data_frame/transforms/foo/_stats")); assertFalse(request.getParameters().containsKey("from")); assertFalse(request.getParameters().containsKey("size")); getStatsRequest.setPageParams(new PageParams(0, null)); request = DataFrameRequestConverters.getDataFrameTransformStats(getStatsRequest); assertEquals("0", request.getParameters().get("from")); assertEquals(null, request.getParameters().get("size")); getStatsRequest.setPageParams(new PageParams(null, 50)); request = DataFrameRequestConverters.getDataFrameTransformStats(getStatsRequest); assertEquals(null, request.getParameters().get("from")); assertEquals("50", request.getParameters().get("size")); getStatsRequest.setPageParams(new PageParams(0, 10)); request = DataFrameRequestConverters.getDataFrameTransformStats(getStatsRequest); assertEquals("0", request.getParameters().get("from")); assertEquals("10", request.getParameters().get("size")); }	i find the following syntax more readable: assertthat(request.getparameters(), allof(hasentry("from", "0"), hasentry("size", "10"))); wdyt?
public void testGetDataFrameTransform() { GetDataFrameTransformRequest getRequest = new GetDataFrameTransformRequest("bar"); Request request = DataFrameRequestConverters.getDataFrameTransform(getRequest); assertEquals(HttpGet.METHOD_NAME, request.getMethod()); assertThat(request.getEndpoint(), equalTo("/_data_frame/transforms/bar")); assertFalse(request.getParameters().containsKey("from")); assertFalse(request.getParameters().containsKey("size")); getRequest.setPageParams(new PageParams(0, null)); request = DataFrameRequestConverters.getDataFrameTransform(getRequest); assertEquals("0", request.getParameters().get("from")); assertEquals(null, request.getParameters().get("size")); getRequest.setPageParams(new PageParams(null, 50)); request = DataFrameRequestConverters.getDataFrameTransform(getRequest); assertEquals(null, request.getParameters().get("from")); assertEquals("50", request.getParameters().get("size")); getRequest.setPageParams(new PageParams(0, 10)); request = DataFrameRequestConverters.getDataFrameTransform(getRequest); assertEquals("0", request.getParameters().get("from")); assertEquals("10", request.getParameters().get("size")); }	i find the following syntax more readable: assertthat(request.getparameters(), allof(hasentry("from", "0"), not(haskey("size")))); wdyt?
@Override protected ShardValidateQueryResponse shardOperation(ShardValidateQueryRequest request) throws ElasticsearchException { IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex()); IndexQueryParserService queryParserService = indexService.queryParserService(); IndexShard indexShard = indexService.shardSafe(request.shardId().id()); boolean valid; String explanation = null; String error = null; Engine.Searcher searcher = indexShard.acquireSearcher("validate_query"); DefaultSearchContext searchContext = new DefaultSearchContext(0, new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()), null, searcher, indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter() ); SearchContext.setCurrent(searchContext); try { if (request.source() != null && request.source().length() > 0) { searchContext.parsedQuery(queryParserService.parseQuery(request.source())); } searchContext.preProcess(); valid = true; if (request.explain()) { explanation = searchContext.query().rewrite(searcher.reader()).toString(); } } catch (QueryParsingException e) { valid = false; error = e.getDetailedMessage(); } catch (AssertionError e) { valid = false; error = e.getMessage(); } catch (IOException e) { valid = false; error = e.getMessage(); } finally { SearchContext.current().close(); SearchContext.removeCurrent(); } return new ShardValidateQueryResponse(request.shardId(), valid, explanation, error); }	not very expert on this aspect but i wonder if and how rewriting affects other queries explain output? also, tests would help here just to validate what we print out.
public static ShardsSyncedFlushResult attemptSyncedFlush(Logger logger, InternalTestCluster cluster, ShardId shardId) throws Exception { /* * When the last indexing operation is completed, we will fire a global checkpoint sync. * Since a global checkpoint sync request is a replication request, it will acquire an index * shard permit on the primary when executing. If this happens at the same time while we are * issuing the synced-flush, the synced-flush request will fail as it thinks there are * in-flight operations. We can avoid such situation by continuing issuing another synced-flush * if the synced-flush failed due to the ongoing operations on the primary. */ SyncedFlushService service = cluster.getInstance(SyncedFlushService.class); AtomicReference<LatchedListener<ShardsSyncedFlushResult>> listenerHolder = new AtomicReference<>(); assertBusy(() -> { LatchedListener<ShardsSyncedFlushResult> listener = new LatchedListener<>(); listenerHolder.set(listener); service.attemptSyncedFlush(shardId, listener); listener.latch.await(); if (listener.error != null) { return; // stop here so that we can preserve the error } if (listener.result.failed()) { // only retry if request failed due to ongoing operations on primary assertThat(listener.result.failureReason(), not(containsString("ongoing operations on primary"))); } }); if (listenerHolder.get().error != null) { throw ExceptionsHelper.convertToElastic(listenerHolder.get().error); } return listenerHolder.get().result; }	nit: just fold this into an if else?
public static ShardsSyncedFlushResult attemptSyncedFlush(Logger logger, InternalTestCluster cluster, ShardId shardId) throws Exception { /* * When the last indexing operation is completed, we will fire a global checkpoint sync. * Since a global checkpoint sync request is a replication request, it will acquire an index * shard permit on the primary when executing. If this happens at the same time while we are * issuing the synced-flush, the synced-flush request will fail as it thinks there are * in-flight operations. We can avoid such situation by continuing issuing another synced-flush * if the synced-flush failed due to the ongoing operations on the primary. */ SyncedFlushService service = cluster.getInstance(SyncedFlushService.class); AtomicReference<LatchedListener<ShardsSyncedFlushResult>> listenerHolder = new AtomicReference<>(); assertBusy(() -> { LatchedListener<ShardsSyncedFlushResult> listener = new LatchedListener<>(); listenerHolder.set(listener); service.attemptSyncedFlush(shardId, listener); listener.latch.await(); if (listener.error != null) { return; // stop here so that we can preserve the error } if (listener.result.failed()) { // only retry if request failed due to ongoing operations on primary assertThat(listener.result.failureReason(), not(containsString("ongoing operations on primary"))); } }); if (listenerHolder.get().error != null) { throw ExceptionsHelper.convertToElastic(listenerHolder.get().error); } return listenerHolder.get().result; }	i personally favor something like: if (listener.result != null && listener. result.failurereason != null && listener. result.failurereason.contains("ongoing operations on primary") { throw new asserionerror(listener.reasult.failurereason); // cause the assert busy to retry } instead of these two ifs.
* @throws TimeoutException if timed out waiting for in-flight operations to finish * @throws IndexShardClosedException if operation permit has been closed */ <E extends Exception> void blockOperations( final long timeout, final TimeUnit timeUnit, final CheckedRunnable<E> onBlocked) throws InterruptedException, TimeoutException, E { delayOperations(); try (Releasable ignored = acquireAll(timeout, timeUnit)) { onBlocked.run(); } finally { releaseDelayedOperations(); } } /** * Immediately delays operations and on another thread waits for in-flight operations to finish and then executes {@code onBlocked} * under the guarantee that no new operations are started. Delayed operations are run after {@code onBlocked} has executed. After * operations are delayed and the blocking is forked to another thread, returns to the caller. If a failure occurs while blocking * operations or executing {@code onBlocked} then the {@code onFailure} handler will be invoked. * * @param timeout the maximum time to wait for the in-flight operations block * @param timeUnit the time unit of the {@code timeout} argument * @param onBlocked the action to run once the block has been acquired * @param onFailure the action to run if a failure occurs while blocking operations * @param <E> the type of checked exception thrown by {@code onBlocked}	since we only have one user of this api, i wonder wether we should just remove this syntactic sugar method and put the single user to the other api
static void getStatisticSummations(Client client, ActionListener<DataFrameIndexerTransformStats> statsListener) { QueryBuilder queryBuilder = QueryBuilders.constantScoreQuery(QueryBuilders.boolQuery() .filter(QueryBuilders.termQuery(DataFrameField.INDEX_DOC_TYPE.getPreferredName(), DataFrameTransformStoredDoc.NAME))); SearchRequestBuilder requestBuilder = client.prepareSearch(DataFrameInternalIndex.LATEST_INDEX_NAME) .setSize(0) .setQuery(queryBuilder); final String path = DataFrameField.STATS_FIELD.getPreferredName() + "."; for(String statName : PROVIDED_STATS) { requestBuilder.addAggregation(AggregationBuilders.sum(statName).field(path + statName)); } ActionListener<SearchResponse> getStatisticSummationsListener = ActionListener.wrap( searchResponse -> { if (searchResponse.getShardFailures().length > 0) { logger.error("statistics summations search returned shard failures: {}", Arrays.toString(searchResponse.getShardFailures())); } statsListener.onResponse(parseSearchAggs(searchResponse)); }, failure -> { if (failure instanceof ResourceNotFoundException) { statsListener.onResponse(new DataFrameIndexerTransformStats()); } else { statsListener.onFailure(failure); } } ); ClientHelper.executeAsyncWithOrigin(client.threadPool().getThreadContext(), ClientHelper.DATA_FRAME_ORIGIN, requestBuilder.request(), getStatisticSummationsListener, client::search); }	this will not provide accurate data. stats documents for various transforms could exist between various index versions. since we use aggregations and a transform could have multiple stored docs between index versions, i am not sure we can even provide accurate stats. @droberts195 what do you think?
* index * @param tokensIndex the manager for the index where the tokens are stored * @param listener The listener to call upon completion with a {@link Tuple} containing the * serialized access token and serialized refresh token as these will be returned to the client */ void decryptAndReturnSupersedingTokens(String refreshToken, RefreshTokenStatus refreshTokenStatus, SecurityIndexManager tokensIndex, ActionListener<Tuple<String, String>> listener) { final byte[] iv = Base64.getDecoder().decode(refreshTokenStatus.getIv()); final byte[] salt = Base64.getDecoder().decode(refreshTokenStatus.getSalt()); final byte[] encryptedSupersedingTokens = Base64.getDecoder().decode(refreshTokenStatus.getSupersedingTokens()); try { Cipher cipher = getDecryptionCipher(iv, refreshToken, salt); final String supersedingTokens = new String(cipher.doFinal(encryptedSupersedingTokens), StandardCharsets.UTF_8); final String[] decryptedTokens = supersedingTokens.split("\\\\\\\\|"); if (decryptedTokens.length != 2) { logger.warn("Decrypted tokens string is not correctly formatted"); listener.onFailure(invalidGrantException("could not refresh the requested token")); } else { // We expect this to protect against race conditions that manifest within few ms final Iterator<TimeValue> backoff = BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(10), 8).iterator(); final String tokenDocId = getTokenDocumentId(hashTokenString(decryptedTokens[0])); final Consumer<Exception> onFailure = ex -> listener.onFailure(traceLog("decrypt and get superseding token", tokenDocId, ex)); getTokenDocAsync(tokenDocId, tokensIndex, new ActionListener<>() { @Override public void onResponse(GetResponse response) { if (response.isExists()) { try { listener.onResponse( new Tuple<>(prependVersionAndEncodeAccessToken(refreshTokenStatus.getVersion(), decryptedTokens[0]), prependVersionAndEncodeRefreshToken(refreshTokenStatus.getVersion(), decryptedTokens[1]))); } catch (GeneralSecurityException | IOException e) { logger.warn("Could not format stored superseding token values", e); onFailure.accept(invalidGrantException("could not refresh the requested token")); } } else { if (backoff.hasNext()) { logger.trace("could not get token document [{}] that should have been created, retrying", tokenDocId); client.threadPool().schedule( () -> getTokenDocAsync(tokenDocId, tokensIndex, this), backoff.next(), GENERIC); } else { logger.warn("could not get token document [{}] that should have been created after all retries", tokenDocId); onFailure.accept(invalidGrantException("could not refresh the requested token")); } } } @Override public void onFailure(Exception e) { if (isShardNotAvailableException(e)) { if (backoff.hasNext()) { logger.info("could not get token document [{}] that should have been created, retrying", tokenDocId); client.threadPool().schedule(() -> getTokenDocAsync(tokenDocId, tokensIndex, this), backoff.next(), GENERIC); } else { logger.warn("could not get token document [{}] that should have been created after all retries", tokenDocId); onFailure.accept(invalidGrantException("could not refresh the requested token")); } } else { onFailure.accept(e); } } }); } } catch (GeneralSecurityException e) { logger.warn("Could not get stored superseding token values", e); listener.onFailure(invalidGrantException("could not refresh the requested token")); } }	maybe extract this logic into a consumer? it can be reused for the onreponse part as well.
* @param connectionListener a listener invoked once every configured cluster has been connected to */ private synchronized void updateRemoteClusters(Map<String, Tuple<String, List<Tuple<String, Supplier<DiscoveryNode>>>>> seeds, ActionListener<Void> connectionListener) { if (seeds.containsKey(LOCAL_CLUSTER_GROUP_KEY)) { throw new IllegalArgumentException("remote clusters must not have the empty string as its key"); } Map<String, RemoteClusterConnection> remoteClusters = new HashMap<>(); if (seeds.isEmpty()) { connectionListener.onResponse(null); } else { CountDown countDown = new CountDown(seeds.size()); remoteClusters.putAll(this.remoteClusters); for (Map.Entry<String, Tuple<String, List<Tuple<String, Supplier<DiscoveryNode>>>>> entry : seeds.entrySet()) { List<Tuple<String, Supplier<DiscoveryNode>>> seedList = entry.getValue().v2(); String proxyAddress = entry.getValue().v1(); String clusterAlias = entry.getKey(); RemoteClusterConnection remote = this.remoteClusters.get(clusterAlias); ConnectionProfile connectionProfile = this.remoteClusterConnectionProfiles.get(clusterAlias); if (seedList.isEmpty()) { // with no seed nodes we just remove the connection try { IOUtils.close(remote); } catch (IOException e) { logger.warn("failed to close remote cluster connections for cluster: " + clusterAlias, e); } remoteClusters.remove(clusterAlias); continue; } if (remote == null) { // this is a new cluster we have to add a new representation remote = new RemoteClusterConnection(settings, clusterAlias, seedList, transportService, numRemoteConnections, getNodePredicate(settings), proxyAddress); remoteClusters.put(clusterAlias, remote); } else if (connectionProfileChanged(remote.getConnectionManager().getConnectionProfile(), connectionProfile)) { // New ConnectionProfile. Must tear down existing connection try { IOUtils.close(remote); } catch (IOException e) { logger.warn("failed to close remote cluster connections for cluster: " + clusterAlias, e); } remoteClusters.remove(clusterAlias); remote = new RemoteClusterConnection(settings, clusterAlias, seedList, transportService, numRemoteConnections, getNodePredicate(settings), proxyAddress); remoteClusters.put(clusterAlias, remote); } // now update the seed nodes no matter if it's new or already existing RemoteClusterConnection finalRemote = remote; remote.updateSeedNodes(proxyAddress, seedList, ActionListener.wrap( response -> { if (countDown.countDown()) { connectionListener.onResponse(response); } }, exception -> { if (countDown.fastForward()) { connectionListener.onFailure(exception); } if (finalRemote.isClosed() == false) { logger.warn("failed to update seed list for cluster: " + clusterAlias, exception); } })); } } this.remoteClusters = Collections.unmodifiableMap(remoteClusters); }	if i understand this correctly we will cause existing connections to fail and further, if concurrently somebody is using is it will not be able to retry since it's close and there will be intermediate failures in such a case. i think in this case we have to somehow reconfigure instead of closing? i know i would like this to be simple but the sideeffects of a close of a connection while some consumer is "in the middle of something" might cause a lot of headaches since it's a rare event, hard to debug and to test. wdyt?
public static Map<Node<?>, String> verifyFailures(LogicalPlan plan) { Collection<Failure> failures = Verifier.verify(plan, null); return failures.stream().collect(toMap(Failure::source, Failure::message)); }	why is counter required to be passed in to the verifier? a rule in the verifier can simply verify whether it's enabled or not (if that's needed) and based on that increase the counter accordingly. the verifier as a whole doesn't need the metric.
private static Failure fail(Node<?> source, String message, Object... args) { return new Failure(source, format(Locale.ROOT, message, args)); }	same here, the counter/map shouldn't be passed in to the verify method.
private Translog openTranslog(EngineConfig engineConfig, TranslogDeletionPolicy translogDeletionPolicy, LongSupplier globalCheckpointSupplier) throws IOException { assert openMode != null; final TranslogConfig translogConfig = engineConfig.getTranslogConfig(); String translogUUID = null; if (openMode == EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG) { translogUUID = loadTranslogUUIDFromLastCommit(); // We expect that this shard already exists, so it must already have an existing translog else something is badly wrong! if (translogUUID == null) { throw new IndexFormatTooOldException("translog", "translog has no generation nor a UUID - this might be an index from a previous version consider upgrading to N-1 first"); } } return new Translog(translogConfig, translogUUID, translogDeletionPolicy, globalCheckpointSupplier); }	assertion message reads funny
void start() { try { while (running) { Runnable runnable = queue.poll(500, TimeUnit.MILLISECONDS); if (runnable != null) { try { runnable.run(); } catch (Exception e) { logger.error("error handling job operation", e); } EsExecutors.rethrowErrors(contextHolder.unwrap(runnable)); } } // if shutdown with tasks pending notify the handlers if (queue.isEmpty() == false) { List<Runnable> notExecuted = new ArrayList<>(); queue.drainTo(notExecuted); for (Runnable runnable : notExecuted) { if (runnable instanceof AbstractRunnable) { ((AbstractRunnable) runnable).onRejection( new EsRejectedExecutionException("unable to process as autodetect worker service has shutdown", true)); } } } } catch (InterruptedException e) { Thread.currentThread().interrupt(); } finally { awaitTermination.countDown(); } }	i think there's still a race condition, although probably less likely than it was before. consider this scenario: 1. thread 1 is running start(), and is preempted on line 841. 2. thread 2 is calling execute(), and is preempted on line 828 while running == true. 3. thread 3 calls shutdown() and sets running = false. 4. thread 1 goes back on cpu and progresses to line 860, i.e. all the way through the // if shutdown with tasks pending notify the handlers block. 5. thread 2 goes back on cpu and adds its command to the queue. step 5 happens after the queue is drained, so the problem this pr is trying to fix will still occur - its command is never run nor rejected. one way to fix this race would be to make the whole of execute() synchronized, plus the queue draining part of start(), i.e. lines 849-860.
protected void metadataToXContent(XContentBuilder builder, Params params) throws IOException { } /** * Static toXContent helper method that renders {@link org.elasticsearch.ElasticsearchException} or {@link Throwable} instances * as XContent, delegating the rendering to {@link #toXContent(XContentBuilder, Params)} * or {@link #innerToXContent(XContentBuilder, Params, Throwable, String, String, Map, Throwable)}. * * This method is usually used when the {@link Throwable}	isn't the name implicit by the parameter list? what's wrong with toxcontent?
@Override public boolean supportsVersion(Version indexCreatedVersion) { return true; } } public static class Builder extends Mapper.Builder { private String name; private String path; protected Builder(String name) { super(name); this.name = name; } public String name() { return this.name; } public Builder path(String path) { this.path = path; return this; } @Override public FieldAliasMapper build(MapperBuilderContext context) { String fullName = context.buildFullName(name); return new FieldAliasMapper(name, fullName, path); }	this i find a bit surprising especially given that field aliases were added with 6.4 . i wonder if this is never called, in which case maybe we should throw exception instead, or if it could rely on the default impl instead.
private static boolean isDeprecatedParameter(String propName, Version indexCreatedVersion) { if (indexCreatedVersion.onOrAfter(Version.V_8_0_0)) { return false; } return DEPRECATED_PARAMS.contains(propName); } } public static BiConsumer<String, MappingParserContext> notInMultiFields(String type) { return (n, c) -> { if (c.isWithinMultiField()) { throw new MapperParsingException("Field [" + n + "] of type [" + type + "] can't be used in multifields"); } }; } /** * TypeParser implementation that automatically handles parsing */ public static final class TypeParser implements Mapper.TypeParser { private final BiFunction<String, MappingParserContext, Builder> builderFunction; private final BiConsumer<String, MappingParserContext> contextValidator; private final Version minimumCompatibilityVersion; // see Mapper.TypeParser#supportsVersion() /** * Creates a new TypeParser * @param builderFunction a function that produces a Builder from a name and parsercontext */ public TypeParser(BiFunction<String, MappingParserContext, Builder> builderFunction) { this(builderFunction, (n, c) -> {}, Version.CURRENT.minimumIndexCompatibilityVersion()); } public TypeParser(BiFunction<String, MappingParserContext, Builder> builderFunction, Version minimumCompatibilityVersion) { this(builderFunction, (n, c) -> {}, minimumCompatibilityVersion); } public TypeParser( BiFunction<String, MappingParserContext, Builder> builderFunction, BiConsumer<String, MappingParserContext> contextValidator ) { this(builderFunction, contextValidator, Version.CURRENT.minimumIndexCompatibilityVersion()); } public TypeParser( BiFunction<String, MappingParserContext, Builder> builderFunction, BiConsumer<String, MappingParserContext> contextValidator, Version minimumCompatibilityVersion ) { this.builderFunction = builderFunction; this.contextValidator = contextValidator; this.minimumCompatibilityVersion = minimumCompatibilityVersion; } @Override public Builder parse(String name, Map<String, Object> node, MappingParserContext parserContext) throws MapperParsingException { contextValidator.accept(name, parserContext); Builder builder = builderFunction.apply(name, parserContext); builder.parse(name, parserContext, node); return builder; } @Override public boolean supportsVersion(Version indexCreatedVersion) { return indexCreatedVersion.onOrAfter(minimumCompatibilityVersion); }	would it help to add javadocs here to clarify when to use which constructor? as far as i understand, the constructor that takes the version is the one called by all the mappers that are supported on legacy indices, all the rest remains the same?
private static boolean isDeprecatedParameter(String propName, Version indexCreatedVersion) { if (indexCreatedVersion.onOrAfter(Version.V_8_0_0)) { return false; } return DEPRECATED_PARAMS.contains(propName); } } public static BiConsumer<String, MappingParserContext> notInMultiFields(String type) { return (n, c) -> { if (c.isWithinMultiField()) { throw new MapperParsingException("Field [" + n + "] of type [" + type + "] can't be used in multifields"); } }; } /** * TypeParser implementation that automatically handles parsing */ public static final class TypeParser implements Mapper.TypeParser { private final BiFunction<String, MappingParserContext, Builder> builderFunction; private final BiConsumer<String, MappingParserContext> contextValidator; private final Version minimumCompatibilityVersion; // see Mapper.TypeParser#supportsVersion() /** * Creates a new TypeParser * @param builderFunction a function that produces a Builder from a name and parsercontext */ public TypeParser(BiFunction<String, MappingParserContext, Builder> builderFunction) { this(builderFunction, (n, c) -> {}, Version.CURRENT.minimumIndexCompatibilityVersion()); } public TypeParser(BiFunction<String, MappingParserContext, Builder> builderFunction, Version minimumCompatibilityVersion) { this(builderFunction, (n, c) -> {}, minimumCompatibilityVersion); } public TypeParser( BiFunction<String, MappingParserContext, Builder> builderFunction, BiConsumer<String, MappingParserContext> contextValidator ) { this(builderFunction, contextValidator, Version.CURRENT.minimumIndexCompatibilityVersion()); } public TypeParser( BiFunction<String, MappingParserContext, Builder> builderFunction, BiConsumer<String, MappingParserContext> contextValidator, Version minimumCompatibilityVersion ) { this.builderFunction = builderFunction; this.contextValidator = contextValidator; this.minimumCompatibilityVersion = minimumCompatibilityVersion; } @Override public Builder parse(String name, Map<String, Object> node, MappingParserContext parserContext) throws MapperParsingException { contextValidator.accept(name, parserContext); Builder builder = builderFunction.apply(name, parserContext); builder.parse(name, parserContext, node); return builder; } @Override public boolean supportsVersion(Version indexCreatedVersion) { return indexCreatedVersion.onOrAfter(minimumCompatibilityVersion); }	i believe this one could be made private
@Override public boolean supportsVersion(Version indexCreatedVersion) { return true; } } public static class ConfigurableTypeParser implements TypeParser { final Function<MappingParserContext, MetadataFieldMapper> defaultMapperParser; final Function<MappingParserContext, Builder> builderFunction; public ConfigurableTypeParser( Function<MappingParserContext, MetadataFieldMapper> defaultMapperParser, Function<MappingParserContext, Builder> builderFunction ) { this.defaultMapperParser = defaultMapperParser; this.builderFunction = builderFunction; } @Override public Builder parse(String name, Map<String, Object> node, MappingParserContext parserContext) throws MapperParsingException { Builder builder = builderFunction.apply(parserContext); builder.parse(name, parserContext, node); return builder; } @Override public MetadataFieldMapper getDefault(MappingParserContext parserContext) { return defaultMapperParser.apply(parserContext); } @Override public boolean supportsVersion(Version indexCreatedVersion) { return true; } } public abstract static class Builder extends FieldMapper.Builder { protected Builder(String name) { super(name); } boolean isConfigured() { for (Parameter<?> param : getParameters()) { if (param.isConfigured()) { return true; } } return false; } @Override public final MetadataFieldMapper build(MapperBuilderContext context) { return build(); } public abstract MetadataFieldMapper build(); } protected MetadataFieldMapper(MappedFieldType mappedFieldType) { super(mappedFieldType.name(), mappedFieldType, MultiFields.empty(), CopyTo.empty()); } protected MetadataFieldMapper(MappedFieldType mappedFieldType, NamedAnalyzer indexAnalyzer) { super(mappedFieldType.name(), mappedFieldType, indexAnalyzer, MultiFields.empty(), CopyTo.empty()); } @Override public FieldMapper.Builder getMergeBuilder() { return null; // by default, things can't be configured so we have no builder } @Override public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { MetadataFieldMapper.Builder mergeBuilder = (MetadataFieldMapper.Builder) getMergeBuilder(); if (mergeBuilder == null || mergeBuilder.isConfigured() == false) { return builder; } builder.startObject(simpleName()); getMergeBuilder().toXContent(builder, params); return builder.endObject(); } @Override protected void parseCreateField(DocumentParserContext context) throws IOException { throw new MapperParsingException( "Field [" + name() + "] is a metadata field and cannot be added inside" + " a document. Use the index API request parameters." ); } /** * Called before {@link FieldMapper#parse(DocumentParserContext)} on the {@link RootObjectMapper}. */ public void preParse(DocumentParserContext context) throws IOException { // do nothing } /** * Called after {@link FieldMapper#parse(DocumentParserContext)} on the {@link RootObjectMapper}	i wonder if overriding these two supportsversion is needed. maybe it does because metadata fields are always supported and they don't take a version in like fieldmapper does?
public org.apache.lucene.util.Version minimumCompatibleVersion() { org.apache.lucene.util.Version luceneVersion = Version.indexCreated(indexSettings).luceneVersion; for(Segment segment : engine().segments()) { if (segment.getVersion().onOrAfter(luceneVersion)) { luceneVersion = segment.getVersion(); } } return luceneVersion; }	what happens if the index is empty and just a commit point?
public void testUpgradeMixed_0_20_6_and_0_90_6() throws Exception { String indexName = "index-0.20.6-and-0.90.6"; loadIndex(indexName + ".zip", indexName); // Has ancient segments?: assertTrue(UpgradeTest.hasAncientSegments(client(), indexName)); // Also has "merely old" segments?: assertTrue(UpgradeTest.hasOldButNotAncientSegments(client(), indexName)); // Needs upgrading? UpgradeTest.assertNotUpgraded(client(), indexName); // Now upgrade only the ancient ones: assertNoFailures(client().admin().indices().prepareUpgrade(indexName).setUpgradeOnlyAncientSegments(true).get()); // No more ancient ones? assertFalse(UpgradeTest.hasAncientSegments(client(), indexName)); // We succeeded in upgrading only the ancient segments but leaving the "merely old" ones untouched: assertTrue(UpgradeTest.hasOldButNotAncientSegments(client(), indexName)); assertEquals(Version.CURRENT.luceneVersion.toString(), client().admin().indices().prepareGetSettings(indexName).get().getSetting(indexName, IndexMetaData.SETTING_VERSION_MINIMUM_COMPATIBLE)); }	i'm confused about this assert. especially given the comments above it. if this minimum version is a lucene version, should it not be the oldest lucene version? in this case the ancient segments will be upgraded, but the "merely old" ones will still be there, so i can't see it being current.
@Override public void setCurrentTerm(long currentTerm) { Manifest manifest = new Manifest(currentTerm, previousManifest.getClusterStateVersion(), previousManifest.getGlobalGeneration(), new HashMap<>(previousManifest.getIndexGenerations())); try { metaStateService.writeManifestAndCleanup("current term changed", manifest); previousManifest = manifest; } catch (WriteStateException e) { logger.error("Failed to set current term to {}", currentTerm); e.rethrowAsErrorOrUncheckedException("setCurrentTerm failed"); } }	could we log the exception and stack trace too? logger.error(new parameterizedmessage("failed to set current term to {}", currentterm), e);
@Override public void setLastAcceptedState(ClusterState clusterState) { assert clusterState.blocks().disableStatePersistence() == false; try { incrementalWrite = previousClusterState.term() == clusterState.term(); updateClusterState(clusterState, previousClusterState); } catch (WriteStateException e) { logger.error("Failed to set last accepted state with version {}", clusterState.version()); e.rethrowAsErrorOrUncheckedException("setLastAcceptedState failed"); } } /** * This class is used to write changed global {@link MetaData}, {@link IndexMetaData} and {@link Manifest} to disk. * This class delegates <code>write*</code> calls to corresponding write calls in {@link MetaStateService}	could we log the exception and stack trace too?
public void rethrowAsErrorOrUncheckedException(String msg) { if (isDirty()) { // IOError is the best thing we have in java library to indicate that serious IO problem has occurred. // IOError will be caught by ElasticsearchUncaughtExceptionHandler and JVM will be halted. // Sadly, it has no constructor that accepts error message, so we first wrap WriteStateException with IOException. throw new IOError(new IOException(msg + ", storage is dirty", this)); } else { throw new UncheckedIOException(msg, this); } }	i think we do not need the message here: the message indicates the calling method but this is also in the stack trace (and the previous log line) and we would know that the storage was dirty if we have an ioerror caused by a writestateexception.
public void rethrowAsErrorOrUncheckedException(String msg) { if (isDirty()) { // IOError is the best thing we have in java library to indicate that serious IO problem has occurred. // IOError will be caught by ElasticsearchUncaughtExceptionHandler and JVM will be halted. // Sadly, it has no constructor that accepts error message, so we first wrap WriteStateException with IOException. throw new IOError(new IOException(msg + ", storage is dirty", this)); } else { throw new UncheckedIOException(msg, this); } }	similarly i think we don't need to add this message.
@Override public int hashCode() { return name.hashCode(); } } public static class Response extends ActionResponse implements ToXContentObject { private final List<DataStream> dataStreams; public Response(List<DataStream> dataStreams) { this.dataStreams = dataStreams; } public Response(StreamInput in) throws IOException { this(in.readList(DataStream::new)); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeList(dataStreams); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startArray(); for (DataStream dataStream : dataStreams) { dataStream.toXContent(builder, params); } builder.endArray(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Response response = (Response) o; return dataStreams.equals(response.dataStreams); } @Override public int hashCode() { return Objects.hash(dataStreams); } } public static class TransportAction extends TransportMasterNodeReadAction<Request, Response> { @Inject public TransportAction(TransportService transportService, ClusterService clusterService, ThreadPool threadPool, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) { super(NAME, transportService, clusterService, threadPool, actionFilters, Request::new, indexNameExpressionResolver); } @Override protected String executor() { return ThreadPool.Names.SAME; } @Override protected Response read(StreamInput in) throws IOException { return new Response(in); } @Override protected void masterOperation(Task task, Request request, ClusterState state, ActionListener<Response> listener) throws Exception { listener.onResponse(new Response(getDataStreams(state, request))); } static List<DataStream> getDataStreams(ClusterState clusterState, Request request) { Map<String, DataStream> dataStreams = clusterState.metadata().dataStreams(); // return all data streams if no name was specified final String requestedName = request.name == null ? "*" : request.name; final List<DataStream> results = new ArrayList<>(); if (Regex.isSimpleMatchPattern(requestedName)) { for (Map.Entry<String, DataStream> entry : dataStreams.entrySet()) { if (Regex.simpleMatch(requestedName, entry.getKey())) { results.add(entry.getValue()); } } } else if (dataStreams.containsKey(request.name)) { results.add(dataStreams.get(request.name)); } else { throw new ResourceNotFoundException("data_stream matching [" + request.name + "] not found"); } results.sort(Comparator.comparing(DataStream::getName)); return results; } @Override protected ClusterBlockException checkBlock(Request request, ClusterState state) { return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE); }	this should be writeoptionalstring() instead of writestring()
@Override public int hashCode() { return name.hashCode(); } } public static class Response extends ActionResponse implements ToXContentObject { private final List<DataStream> dataStreams; public Response(List<DataStream> dataStreams) { this.dataStreams = dataStreams; } public Response(StreamInput in) throws IOException { this(in.readList(DataStream::new)); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeList(dataStreams); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startArray(); for (DataStream dataStream : dataStreams) { dataStream.toXContent(builder, params); } builder.endArray(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Response response = (Response) o; return dataStreams.equals(response.dataStreams); } @Override public int hashCode() { return Objects.hash(dataStreams); } } public static class TransportAction extends TransportMasterNodeReadAction<Request, Response> { @Inject public TransportAction(TransportService transportService, ClusterService clusterService, ThreadPool threadPool, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) { super(NAME, transportService, clusterService, threadPool, actionFilters, Request::new, indexNameExpressionResolver); } @Override protected String executor() { return ThreadPool.Names.SAME; } @Override protected Response read(StreamInput in) throws IOException { return new Response(in); } @Override protected void masterOperation(Task task, Request request, ClusterState state, ActionListener<Response> listener) throws Exception { listener.onResponse(new Response(getDataStreams(state, request))); } static List<DataStream> getDataStreams(ClusterState clusterState, Request request) { Map<String, DataStream> dataStreams = clusterState.metadata().dataStreams(); // return all data streams if no name was specified final String requestedName = request.name == null ? "*" : request.name; final List<DataStream> results = new ArrayList<>(); if (Regex.isSimpleMatchPattern(requestedName)) { for (Map.Entry<String, DataStream> entry : dataStreams.entrySet()) { if (Regex.simpleMatch(requestedName, entry.getKey())) { results.add(entry.getValue()); } } } else if (dataStreams.containsKey(request.name)) { results.add(dataStreams.get(request.name)); } else { throw new ResourceNotFoundException("data_stream matching [" + request.name + "] not found"); } results.sort(Comparator.comparing(DataStream::getName)); return results; } @Override protected ClusterBlockException checkBlock(Request request, ClusterState state) { return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE); }	objects.equals(...) should be used here.
@Override public int hashCode() { return name.hashCode(); } } public static class Response extends ActionResponse implements ToXContentObject { private final List<DataStream> dataStreams; public Response(List<DataStream> dataStreams) { this.dataStreams = dataStreams; } public Response(StreamInput in) throws IOException { this(in.readList(DataStream::new)); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeList(dataStreams); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startArray(); for (DataStream dataStream : dataStreams) { dataStream.toXContent(builder, params); } builder.endArray(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Response response = (Response) o; return dataStreams.equals(response.dataStreams); } @Override public int hashCode() { return Objects.hash(dataStreams); } } public static class TransportAction extends TransportMasterNodeReadAction<Request, Response> { @Inject public TransportAction(TransportService transportService, ClusterService clusterService, ThreadPool threadPool, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) { super(NAME, transportService, clusterService, threadPool, actionFilters, Request::new, indexNameExpressionResolver); } @Override protected String executor() { return ThreadPool.Names.SAME; } @Override protected Response read(StreamInput in) throws IOException { return new Response(in); } @Override protected void masterOperation(Task task, Request request, ClusterState state, ActionListener<Response> listener) throws Exception { listener.onResponse(new Response(getDataStreams(state, request))); } static List<DataStream> getDataStreams(ClusterState clusterState, Request request) { Map<String, DataStream> dataStreams = clusterState.metadata().dataStreams(); // return all data streams if no name was specified final String requestedName = request.name == null ? "*" : request.name; final List<DataStream> results = new ArrayList<>(); if (Regex.isSimpleMatchPattern(requestedName)) { for (Map.Entry<String, DataStream> entry : dataStreams.entrySet()) { if (Regex.simpleMatch(requestedName, entry.getKey())) { results.add(entry.getValue()); } } } else if (dataStreams.containsKey(request.name)) { results.add(dataStreams.get(request.name)); } else { throw new ResourceNotFoundException("data_stream matching [" + request.name + "] not found"); } results.sort(Comparator.comparing(DataStream::getName)); return results; } @Override protected ClusterBlockException checkBlock(Request request, ClusterState state) { return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE); }	objects.hashcode(...) should be used here.
@Override protected Request createTestInstance() { return new Request(randomAlphaOfLength(8) + (randomBoolean() ? "*" : "")); }	maybe also specify null as a value here?
@Override protected Request createTestInstance() { return new Request(randomAlphaOfLength(8) + (randomBoolean() ? "*" : "")); }	nit: suggestion public void testgetdatastream() {
public static TypeResolution isIP(Expression e, String operationName, ParamOrdinal paramOrd) { return isType(e, dt -> dt == IP, operationName, paramOrd, "ip"); }	unless this method is used in multiple places, i would not promote it to typeresolution. the name also looks incorrect, isenum but there's no enum in the args - if i'm guessing the intent correctly, pass an enum along side a translating function so the object/string mapping is generalized: <e extends enum<e>> isstringinenum(expression e, function<enum<e>, string> value, enum<e> e, string operationname, paramordinal paramord) this however seems too complicated for a narrow case, just checking a string inside a list of strings is much more common but that should be reflected in the name.
public static TypeResolution isEnum(Expression e, String operationName, ParamOrdinal paramOrd, Set<String> acceptedValues) { TypeResolution resolution = isFoldable(e, operationName, paramOrd); if (resolution.unresolved()) { return resolution; } resolution = isString(e, operationName, paramOrd); if (resolution.unresolved()) { return resolution; } String value = (String)e.fold(); if (acceptedValues.contains(value) == false) { return new TypeResolution(format(null, "{}argument of [{}] must be a string, one of {}, received [{}]", paramOrd == null || paramOrd == ParamOrdinal.DEFAULT ? "" : paramOrd.name().toLowerCase(Locale.ROOT) + " ", operationName, acceptedValues, value)); } return TypeResolution.TYPE_RESOLVED; }	yet again an issue with formatting - please fix it.
public static TypeResolution isEnum(Expression e, String operationName, ParamOrdinal paramOrd, Set<String> acceptedValues) { TypeResolution resolution = isFoldable(e, operationName, paramOrd); if (resolution.unresolved()) { return resolution; } resolution = isString(e, operationName, paramOrd); if (resolution.unresolved()) { return resolution; } String value = (String)e.fold(); if (acceptedValues.contains(value) == false) { return new TypeResolution(format(null, "{}argument of [{}] must be a string, one of {}, received [{}]", paramOrd == null || paramOrd == ParamOrdinal.DEFAULT ? "" : paramOrd.name().toLowerCase(Locale.ROOT) + " ", operationName, acceptedValues, value)); } return TypeResolution.TYPE_RESOLVED; }	the error message is a bit messed up. this is how it looks with actual values in it: third argument of [percentile(int, 50, null)] must be a string, one of [tdigest, hdr], received [null]. how about third argument of [percentile(int, 50, null)] must be one of [tdigest, hdr], received [null]? to me it seems more clear and concise.
public static <T extends Function> FunctionDefinition def(Class<T> function, FourParametersFunctionBuilder<T> ctorRef, String... names) { FunctionBuilder builder = (source, children, distinct, cfg) -> { if (TwoOptionalArguments.class.isAssignableFrom(function)) { if (children.size() > 4 || children.size() < 2) { throw new QlIllegalArgumentException("expects minimum two, maximum four arguments"); } } else if (OptionalArgument.class.isAssignableFrom(function)) { if (children.size() > 4 || children.size() < 3) { throw new QlIllegalArgumentException("expects three or four arguments"); } } else if (children.size() != 4) { throw new QlIllegalArgumentException("expects exactly four arguments"); } if (distinct) { throw new QlIllegalArgumentException("does not support DISTINCT yet it was specified"); } return ctorRef.build(source, children.get(0), children.get(1), children.size() > 2 ? children.get(2) : null, children.size() > 3 ? children.get(3) : null); }; return def(function, builder, false, names); }	the initial order is more readable - start with optionalargument, second twooptionalarguments
public static <T extends Function> FunctionDefinition def(Class<T> function, FourParametersFunctionBuilder<T> ctorRef, String... names) { FunctionBuilder builder = (source, children, distinct, cfg) -> { if (TwoOptionalArguments.class.isAssignableFrom(function)) { if (children.size() > 4 || children.size() < 2) { throw new QlIllegalArgumentException("expects minimum two, maximum four arguments"); } } else if (OptionalArgument.class.isAssignableFrom(function)) { if (children.size() > 4 || children.size() < 3) { throw new QlIllegalArgumentException("expects three or four arguments"); } } else if (children.size() != 4) { throw new QlIllegalArgumentException("expects exactly four arguments"); } if (distinct) { throw new QlIllegalArgumentException("does not support DISTINCT yet it was specified"); } return ctorRef.build(source, children.get(0), children.get(1), children.size() > 2 ? children.get(2) : null, children.size() > 3 ? children.get(3) : null); }; return def(function, builder, false, names); }	extract the children extraction the ifs above - it's easier to understand what branch did what.
public static void parseField(FieldMapper.Builder builder, String name, Map<String, Object> fieldNode, Mapper.TypeParser.ParserContext parserContext) { NamedAnalyzer indexAnalyzer = builder.fieldType().indexAnalyzer(); NamedAnalyzer searchAnalyzer = builder.fieldType().searchAnalyzer(); for (Iterator<Map.Entry<String, Object>> iterator = fieldNode.entrySet().iterator(); iterator.hasNext();) { Map.Entry<String, Object> entry = iterator.next(); final String propName = Strings.toUnderscoreCase(entry.getKey()); final Object propNode = entry.getValue(); if (propName.equals("index_name") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) { builder.indexName(propNode.toString()); iterator.remove(); } else if (propName.equals("store")) { builder.store(parseStore(name, propNode.toString())); iterator.remove(); } else if (propName.equals("index")) { parseIndex(name, propNode.toString(), builder); iterator.remove(); } else if (propName.equals(DOC_VALUES)) { builder.docValues(nodeBooleanValue(propNode)); iterator.remove(); } else if (propName.equals("term_vector")) { parseTermVector(name, propNode.toString(), builder); iterator.remove(); } else if (propName.equals("boost")) { builder.boost(nodeFloatValue(propNode)); iterator.remove(); } else if (propName.equals("store_term_vectors")) { builder.storeTermVectors(nodeBooleanValue(propNode)); iterator.remove(); } else if (propName.equals("store_term_vector_offsets")) { builder.storeTermVectorOffsets(nodeBooleanValue(propNode)); iterator.remove(); } else if (propName.equals("store_term_vector_positions")) { builder.storeTermVectorPositions(nodeBooleanValue(propNode)); iterator.remove(); } else if (propName.equals("store_term_vector_payloads")) { builder.storeTermVectorPayloads(nodeBooleanValue(propNode)); iterator.remove(); } else if (propName.equals("omit_norms")) { builder.omitNorms(nodeBooleanValue(propNode)); iterator.remove(); } else if (propName.equals("norms")) { final Map<String, Object> properties = nodeMapValue(propNode, "norms"); for (Iterator<Entry<String, Object>> propsIterator = properties.entrySet().iterator(); propsIterator.hasNext();) { Entry<String, Object> entry2 = propsIterator.next(); final String propName2 = Strings.toUnderscoreCase(entry2.getKey()); final Object propNode2 = entry2.getValue(); if (propName2.equals("enabled")) { builder.omitNorms(!nodeBooleanValue(propNode2)); propsIterator.remove(); } else if (propName2.equals(Loading.KEY)) { builder.normsLoading(Loading.parse(nodeStringValue(propNode2, null), null)); propsIterator.remove(); } } DocumentMapperParser.checkNoRemainingFields(propName, properties, parserContext.indexVersionCreated()); iterator.remove(); } else if (propName.equals("omit_term_freq_and_positions")) { final IndexOptions op = nodeBooleanValue(propNode) ? IndexOptions.DOCS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS; if (parserContext.indexVersionCreated().onOrAfter(Version.V_1_0_0_RC2)) { throw new ElasticsearchParseException("'omit_term_freq_and_positions' is not supported anymore - use ['index_options' : 'docs'] instead"); } // deprecated option for BW compat builder.indexOptions(op); iterator.remove(); } else if (propName.equals("index_options")) { builder.indexOptions(nodeIndexOptionValue(propNode)); iterator.remove(); } else if (propName.equals("analyzer") || // for backcompat, reading old indexes, remove for v3.0 propName.equals("index_analyzer") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) { NamedAnalyzer analyzer = parserContext.analysisService().analyzer(propNode.toString()); if (analyzer == null) { throw new MapperParsingException("analyzer [" + propNode.toString() + "] not found for field [" + name + "]"); } indexAnalyzer = analyzer; iterator.remove(); } else if (propName.equals("search_analyzer")) { NamedAnalyzer analyzer = parserContext.analysisService().analyzer(propNode.toString()); if (analyzer == null) { throw new MapperParsingException("analyzer [" + propNode.toString() + "] not found for field [" + name + "]"); } searchAnalyzer = analyzer; iterator.remove(); } else if (propName.equals("include_in_all")) { builder.includeInAll(nodeBooleanValue(propNode)); iterator.remove(); } else if (propName.equals("postings_format") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) { // ignore for old indexes iterator.remove(); } else if (propName.equals("doc_values_format") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) { // ignore for old indexes iterator.remove(); } else if (propName.equals("similarity")) { builder.similarity(parserContext.getSimilarity(propNode.toString())); iterator.remove(); } else if (propName.equals("fielddata")) { final Settings settings = Settings.builder().put(SettingsLoader.Helper.loadNestedFromMap(nodeMapValue(propNode, "fielddata"))).build(); builder.fieldDataSettings(settings); iterator.remove(); } else if (propName.equals("copy_to")) { parseCopyFields(propNode, builder); iterator.remove(); } } if (indexAnalyzer == null) { if (searchAnalyzer != null) { throw new MapperParsingException("analyzer on field [" + name + "] must be set when search_analyzer is set"); } } else if (searchAnalyzer == null) { searchAnalyzer = indexAnalyzer; } builder.indexAnalyzer(indexAnalyzer); builder.searchAnalyzer(searchAnalyzer); }	do you want to remove it from the builder too?
@Override public TimeValue defaultRefreshInterval() { return new TimeValue(1, TimeUnit.SECONDS); }	if it is a test only setting please make it pkg private and move the test if necesary. if not we should add javadocs and pull it up to the interface
@Override public void afterIndexShardClosed(ShardId shardId) { synchronized (mutex) { shardsIndicesStatus.remove(shardId); shardsRecoveredOrDeleted.set(true); } } } private void calcAndSetShardBuffers(String reason) { int shardsCount = countShards(); if (shardsCount == 0) { return; } ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / shardsCount); if (shardIndexingBufferSize.bytes() < minShardIndexBufferSize.bytes()) { shardIndexingBufferSize = minShardIndexBufferSize; } if (shardIndexingBufferSize.bytes() > maxShardIndexBufferSize.bytes()) { shardIndexingBufferSize = maxShardIndexBufferSize; } ByteSizeValue shardTranslogBufferSize = new ByteSizeValue(translogBuffer.bytes() / shardsCount); if (shardTranslogBufferSize.bytes() < minShardTranslogBufferSize.bytes()) { shardTranslogBufferSize = minShardTranslogBufferSize; } if (shardTranslogBufferSize.bytes() > maxShardTranslogBufferSize.bytes()) { shardTranslogBufferSize = maxShardTranslogBufferSize; } logger.debug("recalculating shard indexing buffer (reason={}), total is [{}] with [{}] active shards, each shard set to indexing=[{}], translog=[{}]", reason, indexingBuffer, shardsCount, shardIndexingBufferSize, shardTranslogBufferSize); for (IndexService indexService : indicesService) { for (IndexShard indexShard : indexService) { IndexShardState state = indexShard.state(); if (state != IndexShardState.POST_RECOVERY && state != IndexShardState.STARTED && state != IndexShardState.RELOCATED) { logger.trace("shard [{}] is not yet ready for index buffer update. index shard state: [{}]", indexShard.shardId(), state); continue; } ShardIndexingStatus status = shardsIndicesStatus.get(indexShard.shardId()); if (status == null || !status.inactiveIndexing) { try { ((InternalIndexShard) indexShard).engine().updateIndexingBufferSize(shardIndexingBufferSize); ((InternalIndexShard) indexShard).translog().updateBuffer(shardTranslogBufferSize); } catch (EngineClosedException e) { // ignore continue; } catch (FlushNotAllowedEngineException e) { // ignore continue; } catch (Exception e) { logger.warn("failed to set shard {} index buffer to [{}]", indexShard.shardId(), shardIndexingBufferSize); } } } } } private int countShards() { int shardsCount = 0; for (IndexService indexService : indicesService) { for (IndexShard indexShard : indexService) { ShardIndexingStatus status = shardsIndicesStatus.get(indexShard.shardId()); if (status == null || !status.inactiveIndexing) { shardsCount++; } } } return shardsCount; }	i'd really like us to start using enumsets for this kind of stuff and then do java private static final enumset<indexshardstate> can_update_index_buffer_states = enumset.of(....); if (can_update_index_buffer_states.contains(state) == false) { continue; }
* @return this request */ public PutRepositoryRequest settings(Settings.Builder settings) { this.settings = settings.build(); return this; } /** * Sets the repository settings. * * @param source repository settings in json, yaml or properties format * @return this request * @deprecated use {@link #settings(String, XContentType)}	content type auto-detection ?
* @return this request */ public CreateSnapshotRequest settings(Settings.Builder settings) { this.settings = settings.build(); return this; } /** * Sets repository-specific snapshot settings in JSON or YAML format * <p> * See repository documentation for more information. * * @param source repository-specific snapshot settings * @return this request * @deprecated use {@link #settings(String, XContentType)}	thanks for cleaning these comments up!
* @return this request */ public RestoreSnapshotRequest settings(Settings.Builder settings) { this.settings = settings.build(); return this; } /** * Sets repository-specific restore settings in JSON, YAML or properties format * <p> * See repository documentation for more information. * * @param source repository-specific snapshot settings * @return this request * @deprecated use {@link #settings(String, XContentType)}	properties should go away here too?
* @return this builder */ public RestoreSnapshotRequestBuilder setSettings(Settings.Builder settings) { request.settings(settings); return this; } /** * Sets repository-specific restore settings in JSON, YAML or properties format * <p> * See repository documentation for more information. * * @param source repository-specific snapshot settings * @return this builder * @deprecated use {@link #setSettings(String, XContentType)}	properties should go away?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeString(scriptLang); out.writeOptionalString(id); out.writeBytesReference(script); if (out.getVersion().onOrAfter(Version.V_5_3_0_UNRELEASED)) { xContentType.writeTo(out); } }	can you fix the indentation here?
@Deprecated public CreateIndexRequest mapping(String type, String source) { if (mappings.containsKey(type)) { throw new IllegalStateException("mappings for type \\\\"" + type + "\\\\" were already defined"); } XContentType xContentType = Objects.requireNonNull(XContentFactory.xContentType(source)); mappings.put(type, new Tuple<>(xContentType, new BytesArray(source.getBytes(StandardCharsets.UTF_8)))); return this; }	have this call the other non deprecated setter?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); cause = in.readString(); index = in.readString(); settings = readSettingsFromStream(in); readTimeout(in); int size = in.readVInt(); for (int i = 0; i < size; i++) { if (in.getVersion().onOrAfter(Version.V_5_3_0_UNRELEASED)) { final String type = in.readString(); final BytesReference bytesReference = in.readBytesReference(); final XContentType xContentType = XContentType.readFrom(in); mappings.put(type, new Tuple<>(xContentType, bytesReference)); } else { final String type = in.readString(); final BytesReference bytesReference = new BytesArray(in.readString().getBytes(StandardCharsets.UTF_8)); final XContentType xContentType = XContentFactory.xContentType(bytesReference); mappings.put(type, new Tuple<>(xContentType, bytesReference)); } } int customSize = in.readVInt(); for (int i = 0; i < customSize; i++) { String type = in.readString(); IndexMetaData.Custom customIndexMetaData = IndexMetaData.lookupPrototypeSafe(type).readFrom(in); customs.put(type, customIndexMetaData); } int aliasesSize = in.readVInt(); for (int i = 0; i < aliasesSize; i++) { aliases.add(Alias.read(in)); } updateAllTypes = in.readBoolean(); waitForActiveShards = ActiveShardCount.readFrom(in); }	out of curiosity why repeating type and source de-serialization when the only added bit is the content type?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeString(cause); out.writeString(index); writeSettingsToStream(settings, out); writeTimeout(out); out.writeVInt(mappings.size()); for (Map.Entry<String, Tuple<XContentType, BytesReference>> entry : mappings.entrySet()) { out.writeString(entry.getKey()); final Tuple<XContentType, BytesReference> value = entry.getValue(); if (out.getVersion().onOrAfter(Version.V_5_3_0_UNRELEASED)) { out.writeBytesReference(value.v2()); value.v1().writeTo(out); } else if (value.v1().hasStringRepresentation()) { out.writeString(value.v2().utf8ToString()); } else { throw new IllegalStateException("cannot send [" + value.v1() + "] to an older node"); } } out.writeVInt(customs.size()); for (Map.Entry<String, IndexMetaData.Custom> entry : customs.entrySet()) { out.writeString(entry.getKey()); entry.getValue().writeTo(out); } out.writeVInt(aliases.size()); for (Alias alias : aliases) { alias.writeTo(out); } out.writeBoolean(updateAllTypes); waitForActiveShards.writeTo(out); }	how could this happen? i think that we support any format, but we always transform to json? so this really has to be json?
public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (type == null) { validationException = addValidationError("mapping type is missing", validationException); }else if (type.isEmpty()) { validationException = addValidationError("mapping type is empty", validationException); } if (source == null) { validationException = addValidationError("mapping source is missing", validationException); } else if (source == null || source.length() == 0) { validationException = addValidationError("mapping source is empty", validationException); } if (concreteIndex != null && (indices != null && indices.length > 0)) { validationException = addValidationError("either concrete index or unresolved indices can be set, concrete index: [" + concreteIndex + "] and indices: " + Arrays.asList(indices) , validationException); } return validationException; }	i don't get this change (the additional null check), why is it needed?
public PutIndexTemplateRequestBuilder setSettings(Map<String, Object> source) { request.settings(source); return this; } /** * Adds mapping that will be added when the index template gets created. * * @param type The mapping type * @param source The mapping source * @deprecated use {@link #addMapping(String, BytesReference, XContentType)}	this should go away
public Builder putMapping(String mappingType, BytesReference mappingSource) throws IOException { mappings.put(mappingType, new CompressedXContent(mappingSource)); return this; }	i think these changes are not needed anymore.
@Override public boolean hasStringRepresentation() { return true; } }, /** * A CBOR based content type. */ CBOR(3) { @Override public String mediaTypeWithoutParameters() { return "application/cbor"; } @Override public String shortName() { return "cbor"; } @Override public XContent xContent() { return CborXContent.cborXContent; } @Override public boolean hasStringRepresentation() { return false; } }; /** * Accepts either a format string, which is equivalent to {@link XContentType#shortName()} or a media type that optionally has * parameters and attempts to match the value to an {@link XContentType}. The comparisons are done in lower case format and this method * also supports a wildcard accept for {@code application/*}. This method can be used to parse the {@code Accept} HTTP header or a * format query parameter. This method will return {@code null} if no match is found */ public static XContentType fromMediaTypeOrFormat(String mediaType) { if (mediaType == null) { return null; } for (XContentType type : values()) { if (isSameMediaTypeOrFormatAs(mediaType, type)) { return type; } } if(mediaType.toLowerCase(Locale.ROOT).startsWith("application/*")) { return JSON; } return null; } /** * Attempts to match the given media type with the known {@link XContentType} values. This match is done in a case-insensitive manner. * The provided media type should not included any parameters. This method is suitable for parsing part of the {@code Content-Type} * HTTP header. This method will return {@code null} if no match is found */ public static XContentType fromMediaTypeStrict(String mediaType) { final String lowercaseMediaType = mediaType.toLowerCase(Locale.ROOT); for (XContentType type : values()) { if (type.mediaTypeWithoutParameters().equals(lowercaseMediaType)) { return type; } } return null; } private static boolean isSameMediaTypeOrFormatAs(String stringType, XContentType type) { return type.mediaTypeWithoutParameters().equalsIgnoreCase(stringType) || stringType.toLowerCase(Locale.ROOT).startsWith(type.mediaTypeWithoutParameters().toLowerCase(Locale.ROOT) + ";") || type.shortName().equalsIgnoreCase(stringType); } private int index; XContentType(int index) { this.index = index; } public int index() { return index; } public String mediaType() { return mediaTypeWithoutParameters(); } public abstract String shortName(); public abstract XContent xContent(); public abstract String mediaTypeWithoutParameters(); /** * Returns {@code true} if this {@link XContentType} can be represented as a String and is not a binary only format */ public abstract boolean hasStringRepresentation(); public static XContentType readFrom(StreamInput in) throws IOException { int index = in.readVInt(); for (XContentType contentType : values()) { if (index == contentType.index) { return contentType; } } throw new IllegalStateException("Unknown XContentType with index [" + index + "]"); }	the naming change has to do with the fact that we also accept the query_string format parameter right? shall we add that bit to the javadocs?
*/ public static XContentType fromMediaTypeOrFormat(String mediaType) { if (mediaType == null) { return null; } for (XContentType type : values()) { if (isSameMediaTypeOrFormatAs(mediaType, type)) { return type; } } if(mediaType.toLowerCase(Locale.ROOT).startsWith("application/*")) { return JSON; } return null; } /** * Attempts to match the given media type with the known {@link XContentType} values. This match is done in a case-insensitive manner. * The provided media type should not included any parameters. This method is suitable for parsing part of the {@code Content-Type} * HTTP header. This method will return {@code null}	s/included/include ? what does the strict part refer to again? is it that it doesn't accept null? for some reason when i read strict i would expect it to throw exception rather than returning null. maybe we should do objects.requirenonnull with a nicer message in case it's null?
*/ public static XContentType fromMediaTypeOrFormat(String mediaType) { if (mediaType == null) { return null; } for (XContentType type : values()) { if (isSameMediaTypeOrFormatAs(mediaType, type)) { return type; } } if(mediaType.toLowerCase(Locale.ROOT).startsWith("application/*")) { return JSON; } return null; } /** * Attempts to match the given media type with the known {@link XContentType} values. This match is done in a case-insensitive manner. * The provided media type should not included any parameters. This method is suitable for parsing part of the {@code Content-Type} * HTTP header. This method will return {@code null}	wondering, should remotescrollablehitsource be using this new method rather than the other one? not sure...
public CompressedXContent mappingSource() { return this.mappingSource; }	is a new method needed for this purpose?
public DocumentMapper parse(@Nullable String type, CompressedXContent source, String defaultSource, XContentType xContentType) throws MapperParsingException { Map<String, Object> mapping = null; if (source != null) { Map<String, Object> root; if (xContentType != null) { root = XContentHelper.convertToMap(source.compressedReference(), true, xContentType).v2(); } else { root = XContentHelper.convertToMap(source.compressedReference(), true).v2(); } Tuple<String, Map<String, Object>> t = extractMapping(type, root); type = t.v1(); mapping = t.v2(); } if (mapping == null) { mapping = new HashMap<>(); } return parse(type, mapping, defaultSource); } @SuppressWarnings({"unchecked"}	here we should just assume json as we said above.
public int completedOperations() { return completedOperations; } } private void maybeAddMappingUpdate(String type, Mapping update, String docId, boolean allowMappingUpdates) { if (update == null) { return; } if (allowMappingUpdates == false) { throw new MapperException("mapping updates are not allowed (type: [" + type + "], id: [" + docId + "])"); } Mapping currentUpdate = recoveredTypes.get(type); if (currentUpdate == null) { recoveredTypes.put(type, update); } else { currentUpdate = currentUpdate.merge(update, false); } } /** * Performs a single recovery operation. * * @param allowMappingUpdates true if mapping update should be accepted (but collected). Setting it to false will * cause a {@link MapperException} to be thrown if an update * is encountered. */ private void performRecoveryOperation(Engine engine, Translog.Operation operation, boolean allowMappingUpdates, Engine.Operation.Origin origin) throws IOException { try { switch (operation.opType()) { case INDEX: Translog.Index index = (Translog.Index) operation; // we set canHaveDuplicates to true all the time such that we de-optimze the translog case and ensure that all // autoGeneratedID docs that are coming from the primary are updated correctly. Engine.Index engineIndex = IndexShard.prepareIndex(docMapper(index.type()), source(shardId.getIndexName(), index.type(), index.id(), index.source(), XContentFactory.xContentType(index.source())) .routing(index.routing()).parent(index.parent()), index.seqNo(), index.primaryTerm(), index.version(), index.versionType().versionTypeForReplicationAndRecovery(), origin, index.getAutoGeneratedIdTimestamp(), true); maybeAddMappingUpdate(engineIndex.type(), engineIndex.parsedDoc().dynamicMappingsUpdate(), engineIndex.id(), allowMappingUpdates); logger.trace("[translog] recover [index] op [({}, {})] of [{}][{}]", index.seqNo(), index.primaryTerm(), index.type(), index.id()); index(engine, engineIndex); break; case DELETE: Translog.Delete delete = (Translog.Delete) operation; Uid uid = Uid.createUid(delete.uid().text()); logger.trace("[translog] recover [delete] op [({}, {})] of [{}][{}]", delete.seqNo(), delete.primaryTerm(), uid.type(), uid.id()); final Engine.Delete engineDelete = new Engine.Delete(uid.type(), uid.id(), delete.uid(), delete.seqNo(), delete.primaryTerm(), delete.version(), delete.versionType().versionTypeForReplicationAndRecovery(), origin, System.nanoTime()); delete(engine, engineDelete); break; case NO_OP: final Translog.NoOp noOp = (Translog.NoOp) operation; final long seqNo = noOp.seqNo(); final long primaryTerm = noOp.primaryTerm(); final String reason = noOp.reason(); logger.trace("[translog] recover [no_op] op [({}, {})] of [{}]", seqNo, primaryTerm, reason); final Engine.NoOp engineNoOp = new Engine.NoOp(null, seqNo, primaryTerm, 0, VersionType.INTERNAL, origin, System.nanoTime(), reason); noOp(engine, engineNoOp); break; default: throw new IllegalStateException("No operation defined for [" + operation + "]"); } } catch (ElasticsearchException e) { boolean hasIgnoreOnRecoveryException = false; ElasticsearchException current = e; while (true) { if (current instanceof IgnoreOnRecoveryEngineException) { hasIgnoreOnRecoveryException = true; break; } if (current.getCause() instanceof ElasticsearchException) { current = (ElasticsearchException) current.getCause(); } else { break; } } if (!hasIgnoreOnRecoveryException) { throw e; } } operationProcessed(); } protected void index(Engine engine, Engine.Index engineIndex) throws IOException { engine.index(engineIndex); } protected void delete(Engine engine, Engine.Delete engineDelete) throws IOException { engine.delete(engineDelete); } protected void noOp(Engine engine, Engine.NoOp engineNoOp) { engine.noOp(engineNoOp); } /** * Called once for every processed operation by this recovery performer. * This can be used to get progress information on the translog execution. */ protected void operationProcessed() { // noop }	we still do auto-detection here? that's because we don't want to modify translog format in minor versions?
@Override public void sendErrorResponse(RestStatus restStatus, String errorMessage) throws IOException { sendResponse(new BytesRestResponse(restStatus, newErrorBuilder().startObject() .field("error", errorMessage) .endObject())); }	can we rather build the output in bytesrestresponse, somehow reusing what we have in the build method. i do realize that we don't have an exception here, i am just afraid of creating error messages in different places, i would like that to be centralized as we also need to parse it back in the high level rest client.
public void dispatchRequest(RestRequest request, RestChannel channel, ThreadContext threadContext) { if (request.rawPath().equals("/favicon.ico")) { handleFavicon(request, channel); return; } RestChannel responseChannel = channel; try { final int contentLength = request.hasContent() ? request.content().length() : 0; final RestHandler handler = getHandler(request); if (checkForContentType(request, responseChannel, contentLength, handler)) { if (canTripCircuitBreaker(request)) { inFlightRequestsBreaker(circuitBreakerService).addEstimateBytesAndMaybeBreak(contentLength, "<http_request>"); } else { inFlightRequestsBreaker(circuitBreakerService).addWithoutBreaking(contentLength); } // iff we could reserve bytes for the request we need to send the response also over this channel responseChannel = new ResourceHandlingHttpChannel(channel, circuitBreakerService, contentLength); dispatchRequest(request, responseChannel, client, threadContext, handler); } } catch (Exception e) { try { responseChannel.sendResponse(new BytesRestResponse(channel, e)); } catch (Exception inner) { inner.addSuppressed(e); logger.error((Supplier<?>) () -> new ParameterizedMessage("failed to send failure response for uri [{}]", request.uri()), inner); } } }	would it be that bad if checkforcontenttype threw an exception rather than returning a response directly? i find it a bit delicate that the method returns either true or false, and false means that a response has already been sent back. i can imagine changes made to that code in the future that may easily break that contract? or maybe i am being paranoid.
void dispatchRequest(final RestRequest request, final RestChannel channel, final NodeClient client, ThreadContext threadContext, final RestHandler handler) throws Exception { if (!checkRequestParameters(request, channel)) { return; } try (ThreadContext.StoredContext ignored = threadContext.stashContext()) { for (String key : headersToCopy) { String httpHeader = request.header(key); if (httpHeader != null) { threadContext.putHeader(key, httpHeader); } } if (handler == null) { if (request.method() == RestRequest.Method.OPTIONS) { // when we have OPTIONS request, simply send OK by default (with the Access Control Origin header which gets automatically added) channel.sendResponse(new BytesRestResponse(OK, BytesRestResponse.TEXT_CONTENT_TYPE, BytesArray.EMPTY)); } else { final String msg = "No handler found for uri [" + request.uri() + "] and method [" + request.method() + "]"; channel.sendResponse(new BytesRestResponse(BAD_REQUEST, msg)); } } else { final RestHandler wrappedHandler = Objects.requireNonNull(handlerWrapper.apply(handler)); wrappedHandler.handleRequest(request, channel, client); } } } /** * If a request contains content, this method will return {@code true} if the {@code Content-Type} header is present, matches an * {@link XContentType}	do we even have to call it if the contentlength is 0? seems like we wouldn't even need this argument then.
boolean checkForContentType(final RestRequest restRequest, final RestChannel channel, final int contentLength, final RestHandler restHandler) { if (contentLength > 0) { if (restRequest.getXContentType() == null) { if (restHandler != null && restHandler.supportsPlainText()) { deprecationLogger.deprecated("Plain text request bodies are deprecated. Use request parameters or body " + "in a supported format."); } else if (isContentTypeRequired) { sendContentTypeErrorMessage(restRequest, channel); return false; } else { deprecationLogger.deprecated("Content type detection for rest requests is deprecated. Specify the content type using" + "the [Content-Type] header."); XContentType xContentType = XContentFactory.xContentType(restRequest.content()); if (xContentType == null) { if (restHandler != null) { if (restHandler.supportsPlainText()) { deprecationLogger.deprecated("Plain text request bodies are deprecated. Use request parameters or body " + "in a supported format."); } else { try { channel.sendErrorResponse(NOT_ACCEPTABLE, "Could not determine content type from request body."); } catch (IOException e) { logger.warn("Failed to send response", e); } return false; } } } else { restRequest.setxContentType(xContentType); } } } } return true; }	what's the contract here for cases when we go ahead with the request although the content-type doesn't get set to the restrequest object?
public abstract BytesReference content(); /** * Get the value of the header or {@code null} if not found. This method only retrieves the first header value if multiple values are * sent. Use of {@link #getAllHeaderValues(String)}	can you explain this move? these were previously implemented in the netty request class right?
public final XContentParser contentParser() throws IOException { BytesReference content = content(); if (content.length() == 0) { throw new ElasticsearchParseException("Body required"); } else if (xContentType.get() == null) { throw new IllegalStateException("Content-Type must be provided"); } return xContentType.get().xContent().createParser(xContentRegistry, content); } /** * If there is any content then call {@code applyParser}	this is a bug right? cause we already auto-detected if we didn't get the header? but then the error message is misleading i think
public final Tuple<XContentType, BytesReference> contentOrSourceParam() { if (hasContent()) { if (xContentType.get() == null) { throw new IllegalStateException("Content-Type must be provided"); } return new Tuple<>(xContentType.get(), content()); } String source = param("source"); if (source != null) { BytesArray bytes = new BytesArray(source); String typeParam = param("source_content_type"); final XContentType xContentType; if (typeParam != null) { xContentType = parseContentType(Collections.singletonList(typeParam)); } else { DEPRECATION_LOGGER.deprecated("Deprecated use of the [source] parameter without the [source_content_type] parameter. Use " + "the [source_content_type] parameter to specify the content type of the source such as [application/json]"); xContentType = XContentFactory.xContentType(bytes); } if (xContentType == null) { throw new IllegalStateException("could not determine source content type"); } return new Tuple<>(xContentType, bytes); } return new Tuple<>(XContentType.JSON, BytesArray.EMPTY); } /** * Call a consumer with the parser for the contents of this request if it has contents, otherwise with a parser for the {@code source} * parameter if there is one, otherwise with {@code null}. Use {@link #contentOrSourceParamParser()}	can we document this whenever source is documented (hopefully it is)
public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { String scrollId = request.param("scroll_id"); SearchScrollRequest searchScrollRequest = new SearchScrollRequest(); searchScrollRequest.scrollId(scrollId); String scroll = request.param("scroll"); if (scroll != null) { searchScrollRequest.scroll(new Scroll(parseTimeValue(scroll, null, "scroll"))); } request.withContentOrSourceParamParserOrNull(xContentParser -> { if (xContentParser == null) { if (request.hasContent()) { // TODO: why do we accept this plain text value? maybe we can just use the scroll params? BytesReference body = request.getContentOrSourceParamOnly(); if (scrollId == null) { String bodyScrollId = body.utf8ToString(); searchScrollRequest.scrollId(bodyScrollId); } } } else { // NOTE: if rest request with xcontent body has request parameters, these parameters override xcontent values try { buildFromContent(xContentParser, searchScrollRequest); } catch (IOException e) { throw new IllegalArgumentException("Failed to parse request body", e); } } }); return channel -> client.searchScroll(searchScrollRequest, new RestStatusToXContentListener<>(channel)); }	in this plain/text case does one have to provide the header? not yet in 5.x but they will need to in master?
public static String parseStoredScript(BytesReference scriptAsBytes, XContentType xContentType) { // Scripts can be stored via API in several ways: // 1) wrapped into a 'script' json object or field // 2) wrapped into a 'template' json object or field // 3) just as is // In order to fetch the actual script in consistent manner this parsing logic is needed: // EMPTY is ok here because we never call namedObject, we're just copying structure. try (XContentParser parser = xContentType == null ? XContentHelper.createParser(NamedXContentRegistry.EMPTY, scriptAsBytes) : xContentType.xContent().createParser(NamedXContentRegistry.EMPTY, scriptAsBytes); XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON)) { parser.nextToken(); parser.nextToken(); if (parser.currentToken() == Token.END_OBJECT) { throw new IllegalArgumentException("Empty script"); } switch (parser.currentName()) { case "script": case "template": if (parser.nextToken() == Token.VALUE_STRING) { return parser.text(); } else { builder.copyCurrentStructure(parser); } break; default: // There is no enclosing 'script' or 'template' object so we just need to return the script as is... // because the parsers current location is already beyond the beginning we need to add a START_OBJECT: builder.startObject(); builder.copyCurrentStructure(parser); builder.endObject(); break; } return builder.string(); } catch (IOException e) { throw new RuntimeException(e); } }	can we add a comment here about null content types? that is for bw comp given that we may have scripts already stored in different content types? and this has to stay in master too?
static HttpEntity clearScrollEntity(String scroll) { try (XContentBuilder entity = JsonXContent.contentBuilder()) { return new StringEntity(entity.startObject() .array("scroll_id", scroll) .endObject().string(), ContentType.APPLICATION_JSON); } catch (IOException e) { throw new ElasticsearchException("failed to build clear scroll entity", e); } }	aren't these two exactly the same?
@Override public final void writeTo(StreamOutput out) throws IOException { out.writeByte(type); out.writeLong(maxDoc); out.writeLong(docCount); out.writeLong(sumDocFreq); out.writeLong(sumTotalTermFreq); out.writeBoolean(isSearchable); out.writeBoolean(isAggregatable); if (out.getVersion().onOrAfter(Version.V_5_2_0_UNRELEASED)) { out.writeBoolean(hasMinMax); if (hasMinMax) { writeMinMax(out); } } else { if (hasMinMax == false) { throw new IllegalArgumentException("cannot serialize null min/max fieldstats in a mixed-cluster " + "with pre-" + Version.V_5_2_0_UNRELEASED + " nodes, node version [" + out.getVersion() + "]"); } writeMinMax(out); } }	this only happens if we mess something up right? i wonder if we should make it an assertion instead?
public void testPrefixLoggerMarkersCanBeCollected() throws IOException, UserException { setupLogging("prefix"); for (int i = 0; i < 1 << 20; i++) { // this has the side effect of caching a marker with this prefix Loggers.getLogger("prefix" + i, "prefix" + i); } // this will free the weakly referenced keys in the marker cache System.gc(); assertThat(PrefixLogger.markersSize() < 1 << 20, equalTo(true)); }	maybe move the limit out to a local, with a comment on the reason for the size (i presume to ensure system.gc() actually collects something below).
@Override protected ShardStatus shardOperation(IndexShardStatusRequest request) throws ElasticsearchException { IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex()); IndexShard indexShard = indexService.shardSafe(request.shardId().id()); ShardStatus shardStatus = new ShardStatus(indexShard.routingEntry()); shardStatus.state = indexShard.state(); final Store store = indexShard.store(); store.incRef(); try { shardStatus.storeSize = new ByteSizeValue(Directories.estimateSize(store.directory())); } catch (IOException e) { // failure to get the store size... } finally { store.decRef(); } if (indexShard.state() == IndexShardState.STARTED) { // shardStatus.estimatedFlushableMemorySize = indexShard.estimateFlushableMemorySize(); shardStatus.translogId = indexShard.translog().currentId(); shardStatus.translogOperations = indexShard.translog().estimatedNumberOfOperations(); Engine.Searcher searcher = indexShard.acquireSearcher("indices_status"); try { shardStatus.docs = new DocsStatus(); shardStatus.docs.numDocs = searcher.reader().numDocs(); shardStatus.docs.maxDoc = searcher.reader().maxDoc(); shardStatus.docs.deletedDocs = searcher.reader().numDeletedDocs(); } finally { searcher.close(); } shardStatus.mergeStats = indexShard.mergeScheduler().stats(); shardStatus.refreshStats = indexShard.refreshStats(); shardStatus.flushStats = indexShard.flushStats(); } if (request.recovery) { // check on going recovery (from peer or gateway) RecoveryState recoveryState = indexShard.recoveryState(); if (recoveryState == null) { } else if (recoveryState.getType() == RecoveryState.Type.REPLICA || recoveryState.getType() == RecoveryState.Type.REPLICA) { PeerRecoveryStatus.Stage stage; switch (recoveryState.getStage()) { case INIT: stage = PeerRecoveryStatus.Stage.INIT; break; case INDEX: stage = PeerRecoveryStatus.Stage.INDEX; break; case TRANSLOG: stage = PeerRecoveryStatus.Stage.TRANSLOG; break; case FINALIZE: stage = PeerRecoveryStatus.Stage.FINALIZE; break; case DONE: stage = PeerRecoveryStatus.Stage.DONE; break; default: stage = PeerRecoveryStatus.Stage.INIT; } shardStatus.peerRecoveryStatus = new PeerRecoveryStatus(stage, recoveryState.getTimer().startTime(), recoveryState.getTimer().time(), recoveryState.getIndex().totalBytes(), recoveryState.getIndex().reusedBytes(), recoveryState.getIndex().recoveredBytes(), recoveryState.getTranslog().currentTranslogOperations()); } else if (recoveryState.getType() == RecoveryState.Type.GATEWAY) { GatewayRecoveryStatus.Stage stage; switch (recoveryState.getStage()) { case INIT: stage = GatewayRecoveryStatus.Stage.INIT; break; case INDEX: stage = GatewayRecoveryStatus.Stage.INDEX; break; case TRANSLOG: stage = GatewayRecoveryStatus.Stage.TRANSLOG; break; case DONE: stage = GatewayRecoveryStatus.Stage.DONE; break; default: stage = GatewayRecoveryStatus.Stage.INIT; } shardStatus.gatewayRecoveryStatus = new GatewayRecoveryStatus(stage, recoveryState.getTimer().startTime(), recoveryState.getTimer().time(), recoveryState.getIndex().totalBytes(), recoveryState.getIndex().reusedBytes(), recoveryState.getIndex().recoveredBytes(), recoveryState.getTranslog().currentTranslogOperations()); } } return shardStatus; }	weird... this if has an empty body?
@Override protected ShardStatus shardOperation(IndexShardStatusRequest request) throws ElasticsearchException { IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex()); IndexShard indexShard = indexService.shardSafe(request.shardId().id()); ShardStatus shardStatus = new ShardStatus(indexShard.routingEntry()); shardStatus.state = indexShard.state(); final Store store = indexShard.store(); store.incRef(); try { shardStatus.storeSize = new ByteSizeValue(Directories.estimateSize(store.directory())); } catch (IOException e) { // failure to get the store size... } finally { store.decRef(); } if (indexShard.state() == IndexShardState.STARTED) { // shardStatus.estimatedFlushableMemorySize = indexShard.estimateFlushableMemorySize(); shardStatus.translogId = indexShard.translog().currentId(); shardStatus.translogOperations = indexShard.translog().estimatedNumberOfOperations(); Engine.Searcher searcher = indexShard.acquireSearcher("indices_status"); try { shardStatus.docs = new DocsStatus(); shardStatus.docs.numDocs = searcher.reader().numDocs(); shardStatus.docs.maxDoc = searcher.reader().maxDoc(); shardStatus.docs.deletedDocs = searcher.reader().numDeletedDocs(); } finally { searcher.close(); } shardStatus.mergeStats = indexShard.mergeScheduler().stats(); shardStatus.refreshStats = indexShard.refreshStats(); shardStatus.flushStats = indexShard.flushStats(); } if (request.recovery) { // check on going recovery (from peer or gateway) RecoveryState recoveryState = indexShard.recoveryState(); if (recoveryState == null) { } else if (recoveryState.getType() == RecoveryState.Type.REPLICA || recoveryState.getType() == RecoveryState.Type.REPLICA) { PeerRecoveryStatus.Stage stage; switch (recoveryState.getStage()) { case INIT: stage = PeerRecoveryStatus.Stage.INIT; break; case INDEX: stage = PeerRecoveryStatus.Stage.INDEX; break; case TRANSLOG: stage = PeerRecoveryStatus.Stage.TRANSLOG; break; case FINALIZE: stage = PeerRecoveryStatus.Stage.FINALIZE; break; case DONE: stage = PeerRecoveryStatus.Stage.DONE; break; default: stage = PeerRecoveryStatus.Stage.INIT; } shardStatus.peerRecoveryStatus = new PeerRecoveryStatus(stage, recoveryState.getTimer().startTime(), recoveryState.getTimer().time(), recoveryState.getIndex().totalBytes(), recoveryState.getIndex().reusedBytes(), recoveryState.getIndex().recoveredBytes(), recoveryState.getTranslog().currentTranslogOperations()); } else if (recoveryState.getType() == RecoveryState.Type.GATEWAY) { GatewayRecoveryStatus.Stage stage; switch (recoveryState.getStage()) { case INIT: stage = GatewayRecoveryStatus.Stage.INIT; break; case INDEX: stage = GatewayRecoveryStatus.Stage.INDEX; break; case TRANSLOG: stage = GatewayRecoveryStatus.Stage.TRANSLOG; break; case DONE: stage = GatewayRecoveryStatus.Stage.DONE; break; default: stage = GatewayRecoveryStatus.Stage.INIT; } shardStatus.gatewayRecoveryStatus = new GatewayRecoveryStatus(stage, recoveryState.getTimer().startTime(), recoveryState.getTimer().time(), recoveryState.getIndex().totalBytes(), recoveryState.getIndex().reusedBytes(), recoveryState.getIndex().recoveredBytes(), recoveryState.getTranslog().currentTranslogOperations()); } } return shardStatus; }	instead of calling recoverystate.getindex() all time can we assign it to a local var above?
@Override protected ShardStatus shardOperation(IndexShardStatusRequest request) throws ElasticsearchException { IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex()); IndexShard indexShard = indexService.shardSafe(request.shardId().id()); ShardStatus shardStatus = new ShardStatus(indexShard.routingEntry()); shardStatus.state = indexShard.state(); final Store store = indexShard.store(); store.incRef(); try { shardStatus.storeSize = new ByteSizeValue(Directories.estimateSize(store.directory())); } catch (IOException e) { // failure to get the store size... } finally { store.decRef(); } if (indexShard.state() == IndexShardState.STARTED) { // shardStatus.estimatedFlushableMemorySize = indexShard.estimateFlushableMemorySize(); shardStatus.translogId = indexShard.translog().currentId(); shardStatus.translogOperations = indexShard.translog().estimatedNumberOfOperations(); Engine.Searcher searcher = indexShard.acquireSearcher("indices_status"); try { shardStatus.docs = new DocsStatus(); shardStatus.docs.numDocs = searcher.reader().numDocs(); shardStatus.docs.maxDoc = searcher.reader().maxDoc(); shardStatus.docs.deletedDocs = searcher.reader().numDeletedDocs(); } finally { searcher.close(); } shardStatus.mergeStats = indexShard.mergeScheduler().stats(); shardStatus.refreshStats = indexShard.refreshStats(); shardStatus.flushStats = indexShard.flushStats(); } if (request.recovery) { // check on going recovery (from peer or gateway) RecoveryState recoveryState = indexShard.recoveryState(); if (recoveryState == null) { } else if (recoveryState.getType() == RecoveryState.Type.REPLICA || recoveryState.getType() == RecoveryState.Type.REPLICA) { PeerRecoveryStatus.Stage stage; switch (recoveryState.getStage()) { case INIT: stage = PeerRecoveryStatus.Stage.INIT; break; case INDEX: stage = PeerRecoveryStatus.Stage.INDEX; break; case TRANSLOG: stage = PeerRecoveryStatus.Stage.TRANSLOG; break; case FINALIZE: stage = PeerRecoveryStatus.Stage.FINALIZE; break; case DONE: stage = PeerRecoveryStatus.Stage.DONE; break; default: stage = PeerRecoveryStatus.Stage.INIT; } shardStatus.peerRecoveryStatus = new PeerRecoveryStatus(stage, recoveryState.getTimer().startTime(), recoveryState.getTimer().time(), recoveryState.getIndex().totalBytes(), recoveryState.getIndex().reusedBytes(), recoveryState.getIndex().recoveredBytes(), recoveryState.getTranslog().currentTranslogOperations()); } else if (recoveryState.getType() == RecoveryState.Type.GATEWAY) { GatewayRecoveryStatus.Stage stage; switch (recoveryState.getStage()) { case INIT: stage = GatewayRecoveryStatus.Stage.INIT; break; case INDEX: stage = GatewayRecoveryStatus.Stage.INDEX; break; case TRANSLOG: stage = GatewayRecoveryStatus.Stage.TRANSLOG; break; case DONE: stage = GatewayRecoveryStatus.Stage.DONE; break; default: stage = GatewayRecoveryStatus.Stage.INIT; } shardStatus.gatewayRecoveryStatus = new GatewayRecoveryStatus(stage, recoveryState.getTimer().startTime(), recoveryState.getTimer().time(), recoveryState.getIndex().totalBytes(), recoveryState.getIndex().reusedBytes(), recoveryState.getIndex().recoveredBytes(), recoveryState.getTranslog().currentTranslogOperations()); } } return shardStatus; }	instead of calling recoverystate.getindex() all time can we assign it to a local var above?
protected void doUpload(S3BlobStore blobStore, String bucketName, String blobName, InputStream is, int length, boolean serverSideEncryption) throws AmazonS3Exception { ObjectMetadata md = new ObjectMetadata(); md.setContentLength(length); PutObjectRequest putRequest = new PutObjectRequest(bucketName, blobName, is, md) .withStorageClass(blobStore.getStorageClass()) .withCannedAcl(blobStore.getCannedACL()); if (serverSideEncryption) { if (blobStore.serverSideEncryptionKey().isEmpty()) { md.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION); } else { putRequest.setSSEAwsKeyManagementParams(new SSEAwsKeyManagementParams(blobStore.serverSideEncryptionKey())); } } blobStore.client().putObject(putRequest); }	why do we need to reach into blobstore to get the encryption key, if this defaults3outputstream was instantiated with a serversideencryptionkey?
protected void doUpload(S3BlobStore blobStore, String bucketName, String blobName, InputStream is, int length, boolean serverSideEncryption) throws AmazonS3Exception { ObjectMetadata md = new ObjectMetadata(); md.setContentLength(length); PutObjectRequest putRequest = new PutObjectRequest(bucketName, blobName, is, md) .withStorageClass(blobStore.getStorageClass()) .withCannedAcl(blobStore.getCannedACL()); if (serverSideEncryption) { if (blobStore.serverSideEncryptionKey().isEmpty()) { md.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION); } else { putRequest.setSSEAwsKeyManagementParams(new SSEAwsKeyManagementParams(blobStore.serverSideEncryptionKey())); } } blobStore.client().putObject(putRequest); }	unconventional newline between new and the class that's being constructed makes this hard to read; should probably just use s3blobstore#getsseawskey().
protected String doInitialize(S3BlobStore blobStore, String bucketName, String blobName, boolean serverSideEncryption) { InitiateMultipartUploadRequest request = new InitiateMultipartUploadRequest(bucketName, blobName) .withCannedACL(blobStore.getCannedACL()) .withStorageClass(blobStore.getStorageClass()); if (serverSideEncryption) { ObjectMetadata md = new ObjectMetadata(); if (blobStore.serverSideEncryptionKey().isEmpty()) { md.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION); } else { request.setSSEAwsKeyManagementParams(new SSEAwsKeyManagementParams(blobStore.serverSideEncryptionKey())); } request.setObjectMetadata(md); } return blobStore.client().initiateMultipartUpload(request).getUploadId(); }	our blobstore already has a method that will return us an instance of sseawskeymanagementparams; we should probably use it instead of newing up our own here.
private OutputStream createOutput(final String blobName) throws IOException { // UploadS3OutputStream does buffering & retry logic internally return new DefaultS3OutputStream(blobStore, blobStore.bucket(), buildKey(blobName), blobStore.bufferSizeInBytes(), blobStore.serverSideEncryption(), blobStore.serverSideEncryptionKey()); }	unconventional indentation here makes this hard to read; additionally please see my other comment, in which i indicate that the signature of defaults3outputstream's constructor doesn't need to be changed because we're passing it the s3blobstore and it doesn't need the key separately.
public CannedAccessControlList getCannedACL() { return cannedACL; }	unrelated changes should be picked around (git add --patch is your friend here); while i agree that one-line method definitions are not good, these non-related changes create a lot of noise that makes it hard to review your _intended_ changes.
private void setup(boolean addShutdownHook, Environment environment) throws BootstrapException { Settings settings = environment.settings(); try { spawner.spawnNativePluginControllers(environment); } catch (IOException e) { throw new BootstrapException(e); } initializeNatives( environment.tmpFile(), BootstrapSettings.MEMORY_LOCK_SETTING.get(settings), BootstrapSettings.SYSTEM_CALL_FILTER_SETTING.get(settings), BootstrapSettings.CTRLHANDLER_SETTING.get(settings)); // initialize probes before the security manager is installed initializeProbes(); if (addShutdownHook) { Runtime.getRuntime().addShutdownHook(new Thread() { @Override public void run() { try (Spawner spawnerToClose = spawner) { IOUtils.close(node); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); } catch (IOException ex) { throw new ElasticsearchException("failed to stop node", ex); } } }); } try { // look for jar hell JarHell.checkJarHell(); } catch (IOException | URISyntaxException e) { throw new BootstrapException(e); } // Log ifconfig output before SecurityManager is installed IfConfig.logIfNecessary(); // install SM after natives, shutdown hooks, etc. try { Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings)); } catch (IOException | NoSuchAlgorithmException e) { throw new BootstrapException(e); } node = new Node(environment) { @Override protected void validateNodeBeforeAcceptingRequests( final Settings settings, final BoundTransportAddress boundTransportAddress, List<BootstrapCheck> checks) throws NodeValidationException { BootstrapChecks.check(settings, boundTransportAddress, checks); } }; }	we should keep logging as the last component to shutdown, maybe we should do this: diff diff --git a/core/src/main/java/org/elasticsearch/bootstrap/bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/bootstrap.java index 3bfc2ac3b5..1e53faa9ef 100644 --- a/core/src/main/java/org/elasticsearch/bootstrap/bootstrap.java +++ b/core/src/main/java/org/elasticsearch/bootstrap/bootstrap.java @@ -180,8 +180,8 @@ final class bootstrap { runtime.getruntime().addshutdownhook(new thread() { @override public void run() { - try (spawner spawnertoclose = spawner) { - ioutils.close(node); + try { + ioutils.close(node, spawner); loggercontext context = (loggercontext) logmanager.getcontext(false); configurator.shutdown(context); } catch (ioexception ex) { @@ -258,8 +258,8 @@ final class bootstrap { } static void stop() throws ioexception { - try (spawner spawnertoclose = instance.spawner) { - ioutils.close(instance.node); + try { + ioutils.close(instance.node, instance.spawner); } finally { instance.keepalivelatch.countdown(); } also, we are probably missing a logging shutdown in the stop method, but let's worry about that separately.
AmazonS3 buildClient(final S3ClientSettings clientSettings) { final AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard(); builder.withCredentials(buildCredentials(logger, clientSettings)); builder.withClientConfiguration(buildConfiguration(clientSettings)); final String endpoint = Strings.hasLength(clientSettings.endpoint) ? clientSettings.endpoint : Constants.S3_HOSTNAME; logger.debug("using endpoint [{}]", endpoint); // If the endpoint configuration isn't set on the builder then the default behaviour is to try // and work out what region we are in and use an appropriate endpoint - see AwsClientBuilder#setRegion. // In contrast, directly-constructed clients use s3.amazonaws.com unless otherwise instructed. We currently // use a directly-constructed client, and need to keep the existing behaviour to avoid a breaking change, // so to move to using the builder we must set it explicitly to keep the existing behaviour. // // We do this because directly constructing the client is deprecated (was already deprecated in 1.1.223 too) // so this change removes that usage of a deprecated API. builder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endpoint, null)) .withPathStyleAccessEnabled(true); return builder.build(); }	nit: i think the sdk provides a enablepathstyleaccess() method?
private static boolean isAllowedTypeForClaim(Object o) { return (o instanceof String || o instanceof Boolean || o instanceof Number || (o instanceof Collection && ((Collection<?>) o).stream().allMatch(c -> c instanceof String || c instanceof Boolean || c instanceof Number))); }	moved claimparser to separate class so jwtrealmsettingstests can reuse it.
public void testAliasAndGroupByResolution(){ LogicalPlan p = plan("SELECT COUNT(*) AS c FROM test WHERE ABS(int) > 0 GROUP BY int"); assertTrue(p instanceof Aggregate); var pc = ((Aggregate) p).child(); assertTrue(pc instanceof Filter); Expression condition = ((Filter) pc).condition(); assertEquals(((GreaterThan) condition).functionName(), "GREATERTHAN"); List<Expression> groupings = ((Aggregate) p).groupings(); assertTrue(groupings.get(0).resolved()); var agg = ((Aggregate) p).aggregates(); assertEquals((agg.get(0)).name(), "c"); assertEquals(((Count) ((Alias) agg.get(0)).child()).functionName(), "COUNT"); }	can you please assert all groupings and aggregates and first their size()? please also test with more than 1 literal in the select clause.
public static Instant parseTimeFieldToInstant(XContentParser parser, String fieldName) throws IOException { if (parser.currentToken() == XContentParser.Token.VALUE_NUMBER) { return Instant.ofEpochMilli(parser.longValue()); } else if (parser.currentToken() == XContentParser.Token.VALUE_STRING) { return DateFormatters.from(DateTimeFormatter.ISO_INSTANT.parse(parser.text())).toInstant(); } throw new IllegalArgumentException( "unexpected token [" + parser.currentToken() + "] for [" + fieldName + "]"); }	could org.elasticsearch.client.transform.transforms.util.timeutil class be moved to org.elasticsearch.client.common and reused here?
public void addPermissionsIfNotPresent(IndicesAccessControl other) { final Map<String, IndexAccessControl> map = new HashMap<>(this.indexPermissions); for (Map.Entry<String, IndexAccessControl> entry : other.indexPermissions.entrySet()) { String indexName = entry.getKey(); IndexAccessControl existingControl = map.get(indexName); IndexAccessControl newControl = entry.getValue(); if (existingControl == null) { map.put(indexName, newControl); } else if (newControl.equals(existingControl) == false) { logger.debug("Already have index access control [{}] for [{}], not replacing with [{}]", existingControl, indexName, newControl); } } this.indexPermissions = Map.copyOf(map); }	is it necessary to check granted before proceeding to merge?
private static List<RestTestCandidate> collectTestCandidates(int id, int count) throws RestTestParseException, IOException { List<RestTestCandidate> testCandidates = Lists.newArrayList(); FileSystem fileSystem = getFileSystem(); try { String[] paths = resolvePathsProperty(REST_TESTS_SUITE, DEFAULT_TESTS_PATH); Map<String, Set<Path>> yamlSuites = FileUtils.findYamlSuites(fileSystem, DEFAULT_TESTS_PATH, paths); RestTestSuiteParser restTestSuiteParser = new RestTestSuiteParser(); //yaml suites are grouped by directory (effectively by api) for (String api : yamlSuites.keySet()) { List<Path> yamlFiles = Lists.newArrayList(yamlSuites.get(api)); for (Path yamlFile : yamlFiles) { String key = api + yamlFile.getFileName().toString(); if (mustExecute(key, id, count)) { RestTestSuite restTestSuite = restTestSuiteParser.parse(api, yamlFile); for (TestSection testSection : restTestSuite.getTestSections()) { testCandidates.add(new RestTestCandidate(restTestSuite, testSection)); } } } } } finally { IOUtils.close(fileSystem); } //sort the candidates so they will always be in the same order before being shuffled, for repeatability Collections.sort(testCandidates, new Comparator<RestTestCandidate>() { @Override public int compare(RestTestCandidate o1, RestTestCandidate o2) { return o1.getTestPath().compareTo(o2.getTestPath()); } }); return testCandidates; }	can you use try/with here?
public void openIndex(final OpenIndexClusterStateUpdateRequest request, final ActionListener<ClusterStateUpdateResponse> listener) { if (request.indices() == null || request.indices().length == 0) { throw new IllegalArgumentException("Index name is required"); } final String indicesAsString = Arrays.toString(request.indices()); clusterService.submitStateUpdateTask("open-indices " + indicesAsString, new AckedClusterStateUpdateTask<ClusterStateUpdateResponse>(Priority.URGENT, request, listener) { @Override protected ClusterStateUpdateResponse newResponse(boolean acknowledged) { return new ClusterStateUpdateResponse(acknowledged); } @Override public ClusterState execute(ClusterState currentState) { List<IndexMetaData> indicesToOpen = new ArrayList<>(); for (Index index : request.indices()) { final IndexMetaData indexMetaData = currentState.metaData().getIndexSafe(index); if (indexMetaData.getState() != IndexMetaData.State.OPEN) { indicesToOpen.add(indexMetaData); } } if (indicesToOpen.isEmpty()) { return currentState; } logger.info("opening indices [{}]", indicesAsString); MetaData.Builder mdBuilder = MetaData.builder(currentState.metaData()); ClusterBlocks.Builder blocksBuilder = ClusterBlocks.builder() .blocks(currentState.blocks()); Version minIndexCompatibilityVersion = currentState.getNodes().getSmallestNonClientNodeVersion() .minimumIndexCompatibilityVersion(); for (IndexMetaData closedMetaData : indicesToOpen) { final String indexName = closedMetaData.getIndex().getName(); IndexMetaData indexMetaData = IndexMetaData.builder(closedMetaData).state(IndexMetaData.State.OPEN).build(); // The index might be closed because we couldn't import it due to old incompatible version // We need to check that this index can be upgraded to the current version indexMetaData = metaDataIndexUpgradeService.upgradeIndexMetaData(indexMetaData, minIndexCompatibilityVersion); try { indicesService.verifyIndexMetadata(indexMetaData, indexMetaData); } catch (Exception e) { throw new ElasticsearchException("Failed to verify index " + indexMetaData.getIndex(), e); } mdBuilder.put(indexMetaData, true); blocksBuilder.removeIndexBlock(indexName, INDEX_CLOSED_BLOCK); } ClusterState updatedState = ClusterState.builder(currentState).metaData(mdBuilder).blocks(blocksBuilder).build(); RoutingTable.Builder rtBuilder = RoutingTable.builder(updatedState.routingTable()); for (IndexMetaData index : indicesToOpen) { rtBuilder.addAsFromCloseToOpen(updatedState.metaData().getIndexSafe(index.getIndex())); } //no explicit wait for other nodes needed as we use AckedClusterStateUpdateTask return allocationService.reroute( ClusterState.builder(updatedState).routingTable(rtBuilder.build()).build(), "indices opened [" + indicesAsString + "]"); } }); }	hmm... i'm wondering if we should exclude the non-data non-master nodes here. they do all the coordination and might rely on some metadata they can't digest? we're still have to keep the transport client around, but i don't think we should exclude clientnode (now called coordinating nodes) ?
private void checkSupportedVersion(IndexMetaData indexMetaData, Version minimumIndexCompatibilityVersion) { if (indexMetaData.getState() == IndexMetaData.State.OPEN && isSupportedVersion(indexMetaData, minimumIndexCompatibilityVersion) == false) { throw new IllegalStateException("The index [" + indexMetaData.getIndex() + "] was created before v5.0.0.beta1." + " It should be reindexed in Elasticsearch 5.x before upgrading to " + Version.CURRENT + "."); } }	any way we could bake the minimumindexcompatibilityversion into the message ? i'm afraid we'll forget in the future and the error message will be wrong.
public void testRestoreOldSnapshots() throws Exception { String repo = "test_repo"; String snapshot = "test_1"; List<String> repoVersions = repoVersions(); assertThat(repoVersions.size(), greaterThan(0)); for (String version : repoVersions) { createRepo("repo", version, repo); testOldSnapshot(version, repo, snapshot); } SortedSet<String> expectedVersions = new TreeSet<>(); for (Version v : VersionUtils.allReleasedVersions()) { if (VersionUtils.isSnapshot(v)) continue; // snapshots are unreleased, so there is no backcompat yet if (v.isRelease() == false) continue; // no guarantees for prereleases if (v.onOrBefore(Version.V_5_0_0_beta1)) continue; // we can only test back one major lucene version if (v.equals(Version.CURRENT)) continue; // the current version is always compatible with itself expectedVersions.add(v.toString()); } for (String repoVersion : repoVersions) { if (expectedVersions.remove(repoVersion) == false) { logger.warn("Old repositories tests contain extra repo: {}", repoVersion); } } if (expectedVersions.isEmpty() == false) { StringBuilder msg = new StringBuilder("Old repositories tests are missing versions:"); for (String expected : expectedVersions) { msg.append("\\\\n" + expected); } fail(msg.toString()); } }	wondering - we didn't you go with version.current.minimumindexcompatibilityversion() here?
public void testShardInfoToXContent() throws IOException { ReplicationResponse.ShardInfo shardInfo = new ReplicationResponse.ShardInfo(5, 3); final XContent xContent = randomFrom(XContentType.values()).xContent(); try (XContentBuilder builder = XContentBuilder.builder(xContent)) { builder.startObject(); shardInfo.toXContent(builder, ToXContent.EMPTY_PARAMS); builder.endObject(); // Expected JSON is {"_shards":{"total":5,"successful":3,"failed":0}} try (XContentParser parser = xContent.createParser(builder.bytes())) { assertEquals(XContentParser.Token.START_OBJECT, parser.nextToken()); assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("_shards", parser.currentName()); assertEquals(XContentParser.Token.START_OBJECT, parser.nextToken()); assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("total", parser.currentName()); assertEquals(XContentParser.Token.VALUE_NUMBER, parser.nextToken()); assertEquals(shardInfo.getTotal(), parser.intValue()); assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("successful", parser.currentName()); assertEquals(XContentParser.Token.VALUE_NUMBER, parser.nextToken()); assertEquals(shardInfo.getSuccessful(), parser.intValue()); assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("failed", parser.currentName()); assertEquals(XContentParser.Token.VALUE_NUMBER, parser.nextToken()); assertEquals(shardInfo.getFailed(), parser.intValue()); assertEquals(XContentParser.Token.END_OBJECT, parser.nextToken()); assertEquals(XContentParser.Token.END_OBJECT, parser.nextToken()); assertNull(parser.nextToken()); } } }	i am adding a util method for this in xcontenthelper, maybe worth replacing this later (should not block this pr)
public void testRandomShardInfoToXContent() throws IOException { final ReplicationResponse.ShardInfo shardInfo = randomShardInfo(); final XContent xContent = randomFrom(XContentType.values()).xContent(); try (XContentBuilder builder = XContentBuilder.builder(xContent)) { builder.startObject(); shardInfo.toXContent(builder, ToXContent.EMPTY_PARAMS); builder.endObject(); try (XContentParser parser = xContent.createParser(builder.bytes())) { assertEquals(XContentParser.Token.START_OBJECT, parser.nextToken()); assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("_shards", parser.currentName()); assertEquals(XContentParser.Token.START_OBJECT, parser.nextToken()); assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("total", parser.currentName()); assertEquals(XContentParser.Token.VALUE_NUMBER, parser.nextToken()); assertEquals(shardInfo.getTotal(), parser.intValue()); assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("successful", parser.currentName()); assertEquals(XContentParser.Token.VALUE_NUMBER, parser.nextToken()); assertEquals(shardInfo.getSuccessful(), parser.intValue()); assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("failed", parser.currentName()); assertEquals(XContentParser.Token.VALUE_NUMBER, parser.nextToken()); assertEquals(shardInfo.getFailed(), parser.intValue()); if (shardInfo.getFailures() != null && shardInfo.getFailures().length > 0) { assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken()); assertEquals("failures", parser.currentName()); assertEquals(XContentParser.Token.START_ARRAY, parser.nextToken()); for (int i = 0; i < shardInfo.getFailures().length; i++) { assertFailure(parser, shardInfo.getFailures()[i]); } assertEquals(XContentParser.Token.END_ARRAY, parser.nextToken()); } assertEquals(XContentParser.Token.END_OBJECT, parser.nextToken()); assertEquals(XContentParser.Token.END_OBJECT, parser.nextToken()); assertNull(parser.nextToken()); } } }	not sure, but maybe next time you can save yourself all this manual work by parsing both into a map of maps and comparing those two?
@Override protected void masterOperation(Task task, final RolloverRequest rolloverRequest, final ClusterState state, final ActionListener<RolloverResponse> listener) { final MetaData metaData = state.metaData(); validate(metaData, rolloverRequest); final AliasOrIndex.Alias alias = (AliasOrIndex.Alias) metaData.getAliasAndIndexLookup().get(rolloverRequest.getAlias()); final IndexMetaData indexMetaData = alias.getWriteIndex(); final boolean explicitWriteIndex = Boolean.TRUE.equals(indexMetaData.getAliases().get(alias.getAliasName()).writeIndex()); final String sourceProvidedName = indexMetaData.getSettings().get(IndexMetaData.SETTING_INDEX_PROVIDED_NAME, indexMetaData.getIndex().getName()); final String sourceIndexName = indexMetaData.getIndex().getName(); final String unresolvedName = (rolloverRequest.getNewIndexName() != null) ? rolloverRequest.getNewIndexName() : generateRolloverIndexName(sourceProvidedName, indexNameExpressionResolver); final String rolloverIndexName = indexNameExpressionResolver.resolveDateMathExpression(unresolvedName); MetaDataCreateIndexService.validateIndexName(rolloverIndexName, state, Collections.emptySet()); // fails if the index already exists checkNoDuplicatedAliasInIndexTemplate(metaData, rolloverIndexName, rolloverRequest.getAlias()); IndicesStatsRequest statsRequest = new IndicesStatsRequest().indices(rolloverRequest.getAlias()) .clear() .indicesOptions(IndicesOptions.fromOptions(true, false, true, true)) .docs(true); statsRequest.setParentTask(clusterService.localNode().getId(), task.getId()); client.execute(IndicesStatsAction.INSTANCE, statsRequest, new ActionListener<IndicesStatsResponse>() { @Override public void onResponse(IndicesStatsResponse statsResponse) { final Map<String, Boolean> conditionResults = evaluateConditions(rolloverRequest.getConditions().values(), metaData.index(sourceIndexName), statsResponse); if (rolloverRequest.isDryRun()) { listener.onResponse( new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, true, false, false, false)); return; } List<Condition<?>> metConditions = rolloverRequest.getConditions().values().stream() .filter(condition -> conditionResults.get(condition.toString())).collect(Collectors.toList()); if (conditionResults.size() == 0 || metConditions.size() > 0) { CreateIndexClusterStateUpdateRequest updateRequest = prepareCreateIndexRequest(unresolvedName, rolloverIndexName, rolloverRequest); createIndexService.createIndex(updateRequest, ActionListener.wrap(createIndexClusterStateUpdateResponse -> { final IndicesAliasesClusterStateUpdateRequest aliasesUpdateRequest; if (explicitWriteIndex) { aliasesUpdateRequest = prepareRolloverAliasesWriteIndexUpdateRequest(sourceIndexName, rolloverIndexName, rolloverRequest); } else { aliasesUpdateRequest = prepareRolloverAliasesUpdateRequest(sourceIndexName, rolloverIndexName, rolloverRequest); } indexAliasesService.indicesAliases(aliasesUpdateRequest, ActionListener.wrap(aliasClusterStateUpdateResponse -> { if (aliasClusterStateUpdateResponse.isAcknowledged()) { clusterService.submitStateUpdateTask("update_rollover_info", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { RolloverInfo rolloverInfo = new RolloverInfo(rolloverRequest.getAlias(), metConditions, threadPool.absoluteTimeInMillis()); return ClusterState.builder(currentState) .metaData(MetaData.builder(currentState.metaData()) .put(IndexMetaData.builder(currentState.metaData().index(sourceIndexName)) .putRolloverInfo(rolloverInfo))).build(); } @Override public void onFailure(String source, Exception e) { listener.onFailure(e); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { activeShardsObserver.waitForActiveShards(new String[]{rolloverIndexName}, rolloverRequest.getCreateIndexRequest().waitForActiveShards(), rolloverRequest.masterNodeTimeout(), isShardsAcknowledged -> listener.onResponse(new RolloverResponse( sourceIndexName, rolloverIndexName, conditionResults, false, true, true, isShardsAcknowledged)), listener::onFailure); } }); } else { listener.onResponse(new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, false, true, false, false)); } }, listener::onFailure)); }, listener::onFailure)); } else { // conditions not met listener.onResponse( new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, false, false, false, false) ); } } @Override public void onFailure(Exception e) { listener.onFailure(e); } } ); }	like my other comment, this should probably have the allowed set; i don't think we have system indices that use rollover but i'd rather be safe since we already have the instance of the metadatacreateindexservice in this transport action. maybe we can just make the method non-static and remove the need to pass in the set for name validation
* @param reason the reason the global checkpoint was updated */ public synchronized void updateGlobalCheckpointOnReplica(final long globalCheckpoint, final String reason) { assert invariant(); assert primaryMode == false; /* * The global checkpoint here is a local knowledge which is updated under the mandate of the primary. It can happen that the primary * information is lagging compared to a replica (e.g., if a replica is promoted to primary but has stale info relative to other * replica shards). In these cases, the local knowledge of the global checkpoint could be higher than the sync from the lagging * primary. */ updateGlobalCheckpoint( shardAllocationId, globalCheckpoint, current -> logger.trace("updating global checkpoint from [{}] to [{}] due to [{}]", current, globalCheckpoint, reason)); assert invariant(); }	also add trace logging when updating the global checkpoint knowledge of another shard copy?
@Override public Object getProperty(List<String> path) { if (path.isEmpty()) { return this; } if (path.size() != 1) { throw new IllegalArgumentException("path not supported for [" + getName() + "]: " + path); } int index = metricNames.indexOf(path.get(0)); if (index < 0) { throw new IllegalArgumentException("path not supported for [" + getName() + "]: " + path); } if (topMetrics.isEmpty()) { // Unmapped. return null; } assert topMetrics.size() == 1 : "property paths should only resolve against top metrics with size == 1."; MetricValue metric = topMetrics.get(0).metricValues.get(index); if (metric == null) { return null; } return metric.getValue().getKey(); }	this'll return a bytesref if you fetch from a keyword or ip or bytes field. would it be more useful to you if i formatted it? or do you want the raw bytes?
public void testReadNonexistentBlobThrowsNoSuchFileException() { final BlobContainer blobContainer = createBlobContainer(between(1, 5), null, null, null); Exception exception = expectThrows(NoSuchFileException.class, () -> blobContainer.readBlob("read_nonexistent_blob")); assertThat(exception.getMessage().toLowerCase(Locale.ROOT), containsString("blob object [read_nonexistent_blob] not found")); final long position = randomLongBetween(0, Long.MAX_VALUE - 1); final int length = randomIntBetween(0, Math.toIntExact(Math.min(Integer.MAX_VALUE, Long.MAX_VALUE - 1 - position))); assertThat(expectThrows(NoSuchFileException.class, () -> blobContainer.readBlob("read_nonexistent_blob", position, length)) .getMessage().toLowerCase(Locale.ROOT), containsString("blob object [read_nonexistent_blob] not found")); }	maybe just randomize the call to range/non-range read instead of doing both?
public void testReadBlobWithRetries() throws Exception { final int maxRetries = randomInt(5); final CountDown countDown = new CountDown(maxRetries + 1); final byte[] bytes = randomBlobContent(); httpServer.createContext("/bucket/read_blob_max_retries", exchange -> { Streams.readFully(exchange.getRequestBody()); if (countDown.countDown()) { final int rangeStart = getRangeStart(exchange); assertThat(rangeStart, lessThan(bytes.length)); assertEquals(Optional.empty(), getRangeEnd(exchange)); exchange.getResponseHeaders().add("Content-Type", "text/plain; charset=utf-8"); exchange.sendResponseHeaders(HttpStatus.SC_OK, bytes.length - rangeStart); exchange.getResponseBody().write(bytes, rangeStart, bytes.length - rangeStart); exchange.close(); return; } if (randomBoolean()) { exchange.sendResponseHeaders(randomFrom(HttpStatus.SC_INTERNAL_SERVER_ERROR, HttpStatus.SC_BAD_GATEWAY, HttpStatus.SC_SERVICE_UNAVAILABLE, HttpStatus.SC_GATEWAY_TIMEOUT), -1); } else if (randomBoolean()) { sendIncompleteContent(exchange, bytes); } if (randomBoolean()) { exchange.close(); } }); final TimeValue readTimeout = TimeValue.timeValueMillis(between(100, 500)); final BlobContainer blobContainer = createBlobContainer(maxRetries, readTimeout, null, null); try (InputStream inputStream = blobContainer.readBlob("read_blob_max_retries")) { assertArrayEquals(bytes, BytesReference.toBytes(Streams.readFully(inputStream))); assertThat(countDown.isCountedDown(), is(true)); } }	maybe rename read_blob_max_retries -> read_range_blob_max_retries so that each test uses a dedicated http context (this is no mandatory but can avoid some confusion maybe)?
public void testReadBlobWithPrematureConnectionClose() { final int maxRetries = randomInt(20); final BlobContainer blobContainer = createBlobContainer(maxRetries, null, null, null); // HTTP server sends a partial response final byte[] bytes = randomBlobContent(); httpServer.createContext("/bucket/read_blob_incomplete", exchange -> { sendIncompleteContent(exchange, bytes); exchange.close(); }); final Exception exception = expectThrows(ConnectionClosedException.class, () -> { try (InputStream stream = randomBoolean() ? blobContainer.readBlob("read_blob_incomplete") : blobContainer.readBlob("read_blob_no_response", 0, 1)) { Streams.readFully(stream); } }); assertThat(exception.getMessage().toLowerCase(Locale.ROOT), containsString("premature end of content-length delimited message body")); assertThat(exception.getSuppressed().length, equalTo(Math.min(S3RetryingInputStream.MAX_SUPPRESSED_EXCEPTIONS, maxRetries))); }	it should be read_blob_incomplete
@Override protected InternalCardinality mutateInstance(InternalCardinality instance) { String name = instance.getName(); AbstractHyperLogLogPlusPlus state = instance.getState(); Map<String, Object> metadata = instance.getMetadata(); switch (between(0, 2)) { case 0: name += randomAlphaOfLength(5); break; case 1: HyperLogLogPlusPlus newState = new HyperLogLogPlusPlus(state.precision(), new MockBigArrays(new MockPageCacheRecycler(Settings.EMPTY), new NoneCircuitBreakerService()), 0); for (int i = 0; i < 10; i++) { newState.collect(0, BitMixer.mix64(randomIntBetween(500, 10000))); } algos.add(newState); state = newState; break; case 2: if (metadata == null) { metadata = new HashMap<>(1); } else { metadata = new HashMap<>(instance.getMetadata()); } metadata.put(randomAlphaOfLength(15), randomInt()); break; default: throw new AssertionError("Illegal randomisation branch"); } return new InternalCardinality(name, state, metadata); }	haven't looked closely at the test, but is there a way to ensure we add new values that weren't previously added? maybe by picking from a different range of random values or something? or keeping a set? maybe it's a non-issue and random enough to not matter... might be a problem for a different day :)
static Request executeWatch(ExecuteWatchRequest executeWatchRequest) throws IOException { RequestConverters.EndpointBuilder builder = new RequestConverters.EndpointBuilder() .addPathPartAsIs("_xpack", "watcher", "watch"); builder.addPathPart(executeWatchRequest.getId()); // will ignore if ID is null String endpoint = builder.addPathPart("_execute").build(); Request request = new Request(HttpPost.METHOD_NAME, endpoint); RequestConverters.Params params = new RequestConverters.Params(request); if (executeWatchRequest.isDebug()) { params.putParam("debug", "true"); } if (executeWatchRequest.ignoreCondition()) { params.putParam("ignore_condition", "true"); } if (executeWatchRequest.recordExecution()) { params.putParam("record_execution", "true"); } request.setEntity(RequestConverters.createEntity(executeWatchRequest, XContentType.JSON)); return request; }	string endpoint = new requestconverters.endpointbuilder() .addpathpartasis("_xpack", "watcher", "watch") .addpathpart(...getid()) .addpathpartasis("_execute").build()
public static ParsedDocument noopTombstone(String index, String reason) { SourceToParse source = new SourceToParse(index, "", new BytesArray("{}"), XContentType.JSON); ParsedDocument doc = tombstone(source, VersionFieldMapper.INSTANCE, SeqNoFieldMapper.INSTANCE); // Store the reason of a noop as a raw string in the _source field final BytesRef byteRef = new BytesRef(reason); doc.rootDoc().add(new StoredField(SourceFieldMapper.NAME, byteRef.bytes, byteRef.offset, byteRef.length)); return doc; }	are we good using the hardcoded false here for the fielddata? i wonder if somehow it needs to be configurable, but i admit i am not familiar with what is needed in this context.
@Override public BytesRef getPayload() throws IOException { return in.getPayload(); } } private static Tuple<Bits, Boolean> applyRetentionQuery(CodecReader reader, Query retentionQuery) throws IOException { final IndexSearcher searcher = new IndexSearcher(new FilterCodecReader(reader) { private final Bits liveDocs = reader.getLiveDocs(); @Override public CacheHelper getCoreCacheHelper() { return reader.getCoreCacheHelper(); } @Override public CacheHelper getReaderCacheHelper() { return null; // we are altering live docs } @Override public Bits getLiveDocs() { return new Bits() { @Override public boolean get(int index) { return liveDocs.get(index) == false; } @Override public int length() { return liveDocs.length(); } }; } @Override public int numDocs() { return reader.maxDoc() - reader.numDocs(); } }); searcher.setQueryCache(null); final Weight weight = searcher.createWeight(searcher.rewrite(retentionQuery), ScoreMode.COMPLETE_NO_SCORES, 1.0f); final Scorer scorer = weight.scorer(reader.getContext()); if (scorer == null) { return Tuple.tuple(reader.getLiveDocs(), reader.numDocs() == 0); } else { final FixedBitSet liveDocs = FixedBitSet.copyOf(reader.getLiveDocs()); final DocIdSetIterator iterator = scorer.iterator(); int numDocs = reader.numDocs(); while (iterator.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) { if (liveDocs.getAndSet(iterator.docID()) == false) { numDocs++; } } return Tuple.tuple(liveDocs, numDocs == 0); }	i think we should do this differently: 1. lets return only bits and if it's fully deleted return bits.matchnobits then you can just do an instanceof check 2. execute the query (it's likely it doesn't match) and if it doesn't match just return the incoming live-docs? 3. i consider the set of docs this query matches low so lets build a different bits instance that uses less memory from it and intersect it with the original bits instead of rebuilding it entirely by wrapping the two sets?
public static List<CompressedXContent> collectMappings(final ClusterState state, final String templateName, final String indexName) { final ComposableIndexTemplate template = state.metadata().templatesV2().get(templateName); assert template != null : "attempted to resolve mappings for a template [" + templateName + "] that did not exist in the cluster state"; if (template == null) { return List.of(); } final Map<String, ComponentTemplate> componentTemplates = state.metadata().componentTemplates(); List<CompressedXContent> mappings = template.composedOf().stream() .map(componentTemplates::get) .filter(Objects::nonNull) .map(ComponentTemplate::template) .map(Template::mappings) .filter(Objects::nonNull) .collect(Collectors.toList()); // Add the actual index template's mappings, since it takes the highest precedence Optional.ofNullable(template.template()) .map(Template::mappings) .ifPresent(mappings::add); // Only include _timestamp mapping snippet if creating backing index. if (indexName.startsWith(DataStream.BACKING_INDEX_PREFIX)) { // Only if template has data stream definition this should be added and // adding this template last, since _timestamp field should have highest precedence: Optional.ofNullable(template.getDataStreamTemplate()) .map(ComposableIndexTemplate.DataStreamTemplate::getDataSteamMappingSnippet) .map(mapping -> { try (XContentBuilder builder = XContentBuilder.builder(XContentType.JSON.xContent())) { builder.value(mapping); return new CompressedXContent(BytesReference.bytes(builder)); } catch (IOException e) { throw new UncheckedIOException(e); } }) .ifPresent(mappings::add); } return Collections.unmodifiableList(mappings); }	@dakrone i had to add this if statement otherwise the meta field mapper was going to be applied on each create index api call. in a docs test, the new logs-*-* composable index template was triggered by a regular create index api call and then the test failed, because the document being indexed had no timestamp field. i think only applying the meta field automatically makes sense for backing indices only and not when a user creates a new index via create index api and the index composable template matches? see commits: a23d2e484594b5cbe788f3ef4bfcd40f97136f67, which then was superseded by: c78733204c12cf44b53026052aff9865d99ced03
public void invalidateAccessToken(String accessToken, ActionListener<TokensInvalidationResult> listener) { ensureEnabled(); if (Strings.isNullOrEmpty(accessToken)) { listener.onFailure(traceLog("invalidate access token", new IllegalArgumentException("access token must be provided"))); } else { maybeStartTokenRemover(); final Iterator<TimeValue> backoff = DEFAULT_BACKOFF.iterator(); decodeToken(accessToken, ActionListener.wrap(userToken -> { if (userToken == null) { // The chances of a random token string decoding to something that we can read is minimal, so // we assume that this was a token we have created but is now expired/revoked and deleted logger.trace("The access token [{}] is expired and already deleted", accessToken); listener.onResponse(TokensInvalidationResult.emptyResult(RestStatus.NOT_FOUND)); } else { indexInvalidation(Collections.singleton(userToken), backoff, "access_token", null, listener); } }, e -> { if (e instanceof IndexNotFoundException || e instanceof IndexClosedException) { listener.onFailure(new ElasticsearchSecurityException("failed to invalidate token", RestStatus.BAD_REQUEST)); } else { listener.onFailure(unableToPerformAction(e)); } })); } }	i'd prefer to have the new logic encapsulated inside unabletoperformaction(e). otherwise it looks good.
public void intercept(ResizeRequest request, Authentication authentication, Role userPermissions, String action) { final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState(); if (frozenLicenseState.isAuthAllowed()) { if (frozenLicenseState.isDocumentAndFieldLevelSecurityAllowed()) { IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY); IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(request.getSourceIndex()); if (indexAccessControl != null) { final boolean fls = indexAccessControl.getFieldPermissions().hasFieldLevelSecurity(); final boolean dls = indexAccessControl.getQueries() != null; if (fls || dls) { throw new ElasticsearchSecurityException("Resize requests are not allowed for users when " + "field or document level security is enabled on the source index", RestStatus.BAD_REQUEST); } } } // ensure that the user would have the same level of access OR less on the target index final Automaton sourceIndexPermissions = userPermissions.indices().allowedActionsMatcher(request.getSourceIndex()); final Automaton targetIndexPermissions = userPermissions.indices().allowedActionsMatcher(request.getTargetIndexRequest().index()); if (Operations.subsetOf(targetIndexPermissions, sourceIndexPermissions) == false) { // TODO we've already audited a access granted event so this is going to look ugly auditTrailService.accessDenied(null, authentication, action, request, userPermissions.names()); throw Exceptions.authorizationError("Resizing an index is not allowed when the target index " + "has more permissions than the source index"); } } }	idem: i prefer us to be explicit here: threadcontext.gettransient(audit_request_id). it is a case of tamper request , if audit request id is missing.
LogEntryBuilder withRequestId(String requestId, ThreadContext threadContext) { if (Strings.isNullOrEmpty(requestId)) { requestId = AuditUtil.extractRequestId(threadContext); } if (requestId != null) { logEntry.with(REQUEST_ID_FIELD_NAME, requestId); } return this; }	this is again a place where i think we should not generate if missing. missing is an indication that we don't know how this request came to be which is a problem in itself.
public static Translog.Location performOnReplica(BulkShardRequest request, IndexShard replica) throws Exception { Translog.Location location = null; for (int i = 0; i < request.items().length; i++) { BulkItemRequest item = request.items()[i]; final Engine.Result operationResult; DocWriteRequest<?> docWriteRequest = item.request(); final BulkItemResponse response = item.getPrimaryResponse(); final BulkItemResponse.Failure failure = response.getFailure(); final DocWriteResponse writeResponse = response.getResponse(); final long seqNum = failure == null ? writeResponse.getSeqNo() : failure.getSeqNo(); if (seqNum == SequenceNumbers.UNASSIGNED_SEQ_NO) { continue; } if (failure == null) { operationResult = performOpOnReplica(writeResponse, docWriteRequest, replica); } else { operationResult = replica.markSeqNoAsNoop(seqNum, failure.getMessage()); } assert operationResult != null : "operation result must never be null when primary response has no failure"; location = syncOperationResultOrThrow(operationResult, location); } return location; }	can we assert failure is not null here?
*/ public RestResponse getSource(GetRequest getRequest, RequestOptions options) throws IOException { return performRequest(getRequest, RequestConverters::getSource, options, RestHighLevelClient::convertBytesResponse, emptySet()); } /** * Asynchronously retrieves the source field only of a document using GetSource API. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html#_source">Get Source API * on elastic.co</a> * @param getRequest the request * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	i think this api needs to use its own request and response class. although the get source api support almost all request parameters that the get api support, the get source api doesn't support stored_fields, version and version_type. the latter two may be supported in the future, but i don't see stored_fields ever be supported (that retrieves something that isn't part of the _source). also i see that the get api may support doc_value_fields option to and this is unrelated to the _source like stored_fields is. therefor i think that this api should have its own high level client side request class. i also think that this api should have a dedicated response class, that just includes a map<string, object> field for the source instead of the generic restresponse class.
public Aggregator createInternal(Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException { if (collectsFromSingleBucket == false) { return asMultiBucketAggregator(this, context, parent); } Map<String, Object> aggParams = this.aggParams; if (aggParams != null) { aggParams = deepCopyParams(aggParams, context); } else { aggParams = new HashMap<>(); } // Add _agg to params map for backwards compatibility (redundant with context variables on the scripts created below). // When this is removed, aggState (as passed to ScriptedMetricAggregator) can be changed to Map<String, Object>, since // it won't be possible to completely replace it with another type as is possible when it's an entry in params. Object aggState = new HashMap<String, Object>(); if (ScriptedMetricAggContexts.deprecatedAggParamEnabled()) { if (!aggParams.containsKey("_agg")) { // Add _agg if it wasn't added manually aggParams.put("_agg", aggState); } else { // If it was added manually, also use it for the agg context variable to reduce the likelihood of // weird behavior due to multiple different variables. aggState = aggParams.get("_agg"); } } final ScriptedMetricAggContexts.InitScript initScript = this.initScript.newInstance( mergeParams(aggParams, initScriptParams), aggState); final ScriptedMetricAggContexts.MapScript.LeafFactory mapScript = this.mapScript.newFactory( mergeParams(aggParams, mapScriptParams), aggState, lookup); final ScriptedMetricAggContexts.CombineScript combineScript = this.combineScript.newInstance( mergeParams(aggParams, combineScriptParams), aggState); final Script reduceScript = deepCopyScript(this.reduceScript, context); if (initScript != null) { initScript.execute(); CollectionUtils.ensureNoSelfReferences(aggState, "Scripted metric aggs init script"); } return new ScriptedMetricAggregator(name, mapScript, combineScript, reduceScript, aggState, context, parent, pipelineAggregators, metaData); }	nit: we tend to prefer aggparams.containskey("_agg") == false as its more obvious to read since ! can easily be missed
* * @return a map of grouped remote and local indices */ Map<String, List<String>> groupClusterIndices(String[] requestIndices, Predicate<String> indexExists) { Map<String, List<String>> perClusterIndices = new HashMap<>(); Set<String> remoteClusterNames = this.remoteClusters.keySet(); for (String index : requestIndices) { int i = index.indexOf(REMOTE_CLUSTER_INDEX_SEPARATOR); if (i >= 0) { String remoteClusterName = index.substring(0, i); List<String> clusters = clusterNameResolver.resolveClusterNames(remoteClusterNames, remoteClusterName); if (clusters != null) { if (indexExists.test(index)) { // we use : as a separator for remote clusters. might conflict if there is an index that is actually named // remote_cluster_alias:index_name - for this case we fail the request. the user can easily change the cluster alias // if that happens throw new IllegalArgumentException("Can not filter indices; index " + index + " exists but there is also a remote cluster named: " + remoteClusterName); } String indexName = index.substring(i + 1); for (String clusterName : clusters) { perClusterIndices.computeIfAbsent(clusterName, k -> new ArrayList<>()).add(indexName); } } else { perClusterIndices.computeIfAbsent(LOCAL_CLUSTER_GROUP_KEY, k -> new ArrayList<>()).add(index); } } else { perClusterIndices.computeIfAbsent(LOCAL_CLUSTER_GROUP_KEY, k -> new ArrayList<>()).add(index); } } return perClusterIndices; }	can we maybe return an emtpy list instead of null, i think we don't necessarily need the null invariant?
@Before public void initClient() throws IOException { if (client == null) { assert adminClient == null; assert clusterHosts == null; assert hasXPack == null; assert nodeVersions == null; String cluster = System.getProperty("tests.rest.cluster"); if (cluster == null) { throw new RuntimeException("Must specify [tests.rest.cluster] system property with a comma delimited list of [host:port] " + "to which to send REST requests"); } String[] stringUrls = cluster.split(","); List<HttpHost> hosts = new ArrayList<>(stringUrls.length); for (String stringUrl : stringUrls) { int portSeparator = stringUrl.lastIndexOf(':'); if (portSeparator < 0) { throw new IllegalArgumentException("Illegal cluster url [" + stringUrl + "]"); } String host = stringUrl.substring(0, portSeparator); int port = Integer.valueOf(stringUrl.substring(portSeparator + 1)); hosts.add(buildHttpHost(host, port)); } clusterHosts = unmodifiableList(hosts); logger.info("initializing REST clients against {}", clusterHosts); client = buildClient(restClientSettings(), clusterHosts.toArray(new HttpHost[clusterHosts.size()])); adminClient = buildClient(restAdminSettings(), clusterHosts.toArray(new HttpHost[clusterHosts.size()])); hasXPack = false; nodeVersions = new TreeSet<>(); Map<?, ?> response = entityAsMap(adminClient.performRequest(new Request("GET", "_nodes/plugins"))); Map<?, ?> nodes = (Map<?, ?>) response.get("nodes"); for (Map.Entry<?, ?> node : nodes.entrySet()) { Map<?, ?> nodeInfo = (Map<?, ?>) node.getValue(); nodeVersions.add(Version.fromString(nodeInfo.get("version").toString())); for (Object module: (List<?>) nodeInfo.get("modules")) { Map<?, ?> moduleInfo = (Map<?, ?>) module; if (moduleInfo.get("name").toString().startsWith("x-pack-")) { hasXPack = true; } } } } assert client != null; assert adminClient != null; assert clusterHosts != null; assert hasXPack != null; assert nodeVersions != null; }	are there concurrency concerns here that closeclients is called while we are initializing? i see that both hasxpack and nodeversions are explicitly assigned to non-null values. anyways, i guess assertions do not hurt!
static DateHistogramGroupConfig randomDateHistogramGroupConfig() { final String field = randomAlphaOfLength(randomIntBetween(3, 10)); final DateHistogramInterval delay = randomBoolean() ? new DateHistogramInterval(randomPositiveTimeValue()) : null; final String timezone = randomBoolean() ? randomDateTimeZone().toString() : null; int i = randomIntBetween(0,2); if (i == 0) { final DateHistogramInterval interval = new DateHistogramInterval(randomPositiveTimeValue()); return new DateHistogramGroupConfig.FixedInterval(field, interval, delay, timezone); } else if (i == 1) { final DateHistogramInterval interval = new DateHistogramInterval(randomTimeValue(1,1, "m", "h", "d", "w")); return new DateHistogramGroupConfig.CalendarInterval(field, interval, delay, timezone); } else { final DateHistogramInterval interval = new DateHistogramInterval(randomPositiveTimeValue()); return new DateHistogramGroupConfig(field, interval, delay, timezone); } }	nit: could we make this a switch statement? imo this is cleaner to read as there is less boilerplate
private static Map<String, RollupFieldCaps> createRollupFieldCaps(final RollupJobConfig rollupJobConfig) { final Map<String, List<Map<String, Object>>> tempFieldCaps = new HashMap<>(); final GroupConfig groupConfig = rollupJobConfig.getGroupConfig(); if (groupConfig != null) { // Create RollupFieldCaps for the date histogram final DateHistogramGroupConfig dateHistogram = groupConfig.getDateHistogram(); final Map<String, Object> dateHistogramAggCap = new HashMap<>(); dateHistogramAggCap.put("agg", DateHistogramAggregationBuilder.NAME); if (dateHistogram.getClass().equals(DateHistogramGroupConfig.CalendarInterval.class)) { dateHistogramAggCap.put(DateHistogramGroupConfig.CALENDAR_INTERVAL, dateHistogram.getInterval().toString()); } else if (dateHistogram.getClass().equals(DateHistogramGroupConfig.FixedInterval.class)) { dateHistogramAggCap.put(DateHistogramGroupConfig.FIXED_INTERVAL, dateHistogram.getInterval().toString()); } else { dateHistogramAggCap.put(DateHistogramGroupConfig.INTERVAL, dateHistogram.getInterval().toString()); } if (dateHistogram.getDelay() != null) { dateHistogramAggCap.put(DateHistogramGroupConfig.DELAY, dateHistogram.getDelay().toString()); } dateHistogramAggCap.put(DateHistogramGroupConfig.TIME_ZONE, dateHistogram.getTimeZone()); List<Map<String, Object>> dateAggCaps = tempFieldCaps.getOrDefault(dateHistogram.getField(), new ArrayList<>()); dateAggCaps.add(dateHistogramAggCap); tempFieldCaps.put(dateHistogram.getField(), dateAggCaps); // Create RollupFieldCaps for the histogram final HistogramGroupConfig histogram = groupConfig.getHistogram(); if (histogram != null) { final Map<String, Object> histogramAggCap = new HashMap<>(); histogramAggCap.put("agg", HistogramAggregationBuilder.NAME); histogramAggCap.put(HistogramGroupConfig.INTERVAL, histogram.getInterval()); Arrays.stream(rollupJobConfig.getGroupConfig().getHistogram().getFields()).forEach(field -> { List<Map<String, Object>> caps = tempFieldCaps.getOrDefault(field, new ArrayList<>()); caps.add(histogramAggCap); tempFieldCaps.put(field, caps); }); } // Create RollupFieldCaps for the term final TermsGroupConfig terms = groupConfig.getTerms(); if (terms != null) { final Map<String, Object> termsAggCap = singletonMap("agg", TermsAggregationBuilder.NAME); Arrays.stream(rollupJobConfig.getGroupConfig().getTerms().getFields()).forEach(field -> { List<Map<String, Object>> caps = tempFieldCaps.getOrDefault(field, new ArrayList<>()); caps.add(termsAggCap); tempFieldCaps.put(field, caps); }); } } // Create RollupFieldCaps for the metrics final List<MetricConfig> metricsConfig = rollupJobConfig.getMetricsConfig(); if (metricsConfig.size() > 0) { rollupJobConfig.getMetricsConfig().forEach(metricConfig -> { final List<Map<String, Object>> metrics = metricConfig.getMetrics().stream() .map(metric -> singletonMap("agg", (Object) metric)) .collect(Collectors.toList()); metrics.forEach(m -> { List<Map<String, Object>> caps = tempFieldCaps .getOrDefault(metricConfig.getField(), new ArrayList<>()); caps.add(m); tempFieldCaps.put(metricConfig.getField(), caps); }); }); } return Collections.unmodifiableMap(tempFieldCaps.entrySet() .stream() .collect(Collectors.toMap(Map.Entry::getKey, e -> new RollupFieldCaps(e.getValue())))); }	minor: to avoid the class checks here we _could_ add a gettypename() method to datehistogramgroupconfig and just call it to get the key for the map?
@Override public XContentBuilder toXContent(final XContentBuilder builder, final Params params) throws IOException { builder.startObject(); { if (this.getClass().equals(CalendarInterval.class)) { builder.field(CALENDAR_INTERVAL, interval.toString()); } else if (this.getClass().equals(FixedInterval.class)) { builder.field(FIXED_INTERVAL, interval.toString()); } else { builder.field(INTERVAL, interval.toString()); } builder.field(FIELD, field); if (delay != null) { builder.field(DELAY, delay.toString()); } builder.field(TIME_ZONE, timeZone); } return builder.endObject(); }	as above, maybe a gettypename() method would help this avoid the class comparisons?
@Override public boolean equals(final Object other) { if (this == other) { return true; } if (other == null || other instanceof DateHistogramGroupConfig == false) { return false; } final DateHistogramGroupConfig that = (DateHistogramGroupConfig) other; return Objects.equals(interval, that.interval) && Objects.equals(field, that.field) && Objects.equals(delay, that.delay) && Objects.equals(timeZone, that.timeZone); }	i wonder if this change is correct? this suggests that a calendarinterval could in some cases be equal to a fixedinterval and i don't think we want that to be true?
*/ public final void fieldCapsAsync(FieldCapabilitiesRequest fieldCapabilitiesRequest, RequestOptions options, ActionListener<FieldCapabilitiesResponse> listener) { performRequestAsyncAndParseEntity(fieldCapabilitiesRequest, RequestConverters::fieldCaps, options, FieldCapabilitiesResponse::fromXContent, listener, emptySet()); } /** * @deprecated If creating a new HLRC ReST API call, consider creating new actions instead of reusing server actions. The Validation * layer has been added to the ReST client, and requests should extend {@link Validatable} instead of {@link ActionRequest}	one of the side-effects of these methods being public is that they will be suggested to our users by ides although they will never be what users need to call, also they become part of our public api while we considered them internal until now. i wish there was a way to not make them public. maybe we should consider keeping the clients all on the same package? i think that having the other clients subclass this class is a no-go given that the main client needs to have knowledge of all of the clients.
key -> Setting.simpleString( key, Exporter.INDEX_FORMAT, new Setting.Validator<String>() { @Override public void validate(String value) { try { if (value != null) { DateFormatter.forPattern(value).withZone(ZoneOffset.UTC); } } catch (RuntimeException e) { throw new SettingsException("[" + INDEX_NAME_TIME_FORMAT_SETTING.getKey() + "] invalid index name time format: [" + value + "]", e); } } }	is the generic type for this affixsetting wrong? it seems like if we are going to parse into a dateformatter, we should be doing that once, not duplicating here and in exporter.datetimeformatter (where it also looks like we have default value logic?).
static void registerAggregators(ValuesSourceRegistry valuesSourceRegistry) { valuesSourceRegistry.register(HistogramAggregationBuilder.NAME, CoreValuesSourceType.RANGE, new HistogramAggregatorSupplier() { @Override public Aggregator build(String name, AggregatorFactories factories, double interval, double offset, BucketOrder order, boolean keyed, long minDocCount, double minBound, double maxBound, ValuesSource valuesSource, DocValueFormat formatter, SearchContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException { ValuesSource.Range rangeValueSource = (ValuesSource.Range) valuesSource; if (rangeValueSource.rangeType().isNumeric() == false) { throw new IllegalArgumentException("Expected numeric range type but found non-numeric range [" + rangeValueSource.rangeType().name + "]"); } return new RangeHistogramAggregator(name, factories, interval, offset, order, keyed, minDocCount, minBound, maxBound, rangeValueSource, formatter, context, parent, pipelineAggregators, metaData); } } ); valuesSourceRegistry.register(HistogramAggregationBuilder.NAME, List.of(CoreValuesSourceType.NUMERIC, CoreValuesSourceType.DATE), new HistogramAggregatorSupplier() { @Override public Aggregator build(String name, AggregatorFactories factories, double interval, double offset, BucketOrder order, boolean keyed, long minDocCount, double minBound, double maxBound, ValuesSource valuesSource, DocValueFormat formatter, SearchContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException { return new NumericHistogramAggregator(name, factories, interval, offset, order, keyed, minDocCount, minBound, maxBound, (ValuesSource.Numeric) valuesSource, formatter, context, parent, pipelineAggregators, metaData); } } ); }	oops, i forgot to include booleans. i'll push an update shortly.
public void testGetNonExistentFieldMapping() { GetFieldMappingsResponse response = client().admin().indices().prepareGetFieldMappings("index1").setFields("non-existent").get(); Map<String, Map<String, GetFieldMappingsResponse.FieldMappingMetaData>> mappings = response.mappings(); assertEquals(1, mappings.size()); Map<String, GetFieldMappingsResponse.FieldMappingMetaData> fieldmapping = mappings.get("index1"); assertEquals(1, fieldmapping.size()); assertEquals(GetFieldMappingsResponse.FieldMappingMetaData.NULL, fieldmapping.get("non-existent")); }	small comment, fieldmapping -> fieldmapping.
@SuppressWarnings("unchecked") private static Map<String, Object> extractMetadata(Map<String, Object> snapshotResponseMap, String snapshotPrefix) { List<Map<String, Object>> snapResponse = ((List<Map<String, Object>>) snapshotResponseMap.get("responses")).stream() .findFirst() .map(m -> (List<Map<String, Object>>) m.get("snapshots")) .orElseThrow(() -> new AssertionError("failed to find snapshot response in " + snapshotResponseMap)); return snapResponse.stream() .filter(snapshot -> ((String) snapshot.get("snapshot")).startsWith(snapshotPrefix)) .map(snapshot -> (Map<String, Object>) snapshot.get("metadata")) .findFirst() .orElse(null); }	it's very hard for me to understand this, mix of streams, optional and nested checks with streams doesn't work well for me. i'd rather split it into separate calls which is more explicit and it makes this code easier to digest: java map<string, object> snapresponse; try { map<string, object> responsemap = ((list<map<string, object>>) snapshotresponsemap.get("responses")).get(0); list<map<string, object>> snapshots = (list<map<string, object>>) responsemap.get("snapshots"); asserttrue(snapshots.stream().anymatch(s -> s.containskey("snapshot") && s.get("snapshot").tostring().startswith("snap-"))); snapresponse = snapshots.get(0); } catch (exception e){ throw new assertionerror("failed to find snapshot response in " + snapshotresponsemap, e); } assertthat(snapresponse.get("indices"), equalto(collections.singletonlist(indexname))); map<string, object> metadata = (map<string, object>) snapresponse.get("metadata"); if you go with above, please check if i got it right, as i said it's hard for me to follow.
@Override protected BlobStore newBlobStore(BlobStoreRepository blobStoreRepository) { EncryptedRepository.EncryptedBlobStore blobStore = (EncryptedRepository.EncryptedBlobStore) super.newBlobStore(blobStoreRepository); if (false == blobStoreRepository.isReadOnly()) { PlainActionFuture.get( l -> threadPool.generic().execute(ActionRunnable.run(l, () -> blobStore.maybeInitializePasswordGeneration())) ); } return blobStore; }	some integration tests directly access the blob store for writes. but this bypasses the repository initialization (ie creating the initial password generation) which kicks in when a snapshot is started, which regularly happens before a blob write. this artificially introduces the same initialization so the blobstore can be used for writes.
static void prepopulateSecurityCaller() { try { Class<?> c = Class.forName("sun.misc.Unsafe"); MethodHandles.Lookup lookup = MethodHandles.privateLookupIn(c, MethodHandles.lookup()); VarHandle handle = lookup.findStaticVarHandle(c, "theUnsafe", c); Object theUnsafe = handle.get(); Field f = getDeclaredField(Class.forName("java.lang.System$CallersHolder", true, null), "callers"); MethodHandle mh = lookup.findVirtual(c, "staticFieldBase", methodType(Object.class, Field.class)); mh = mh.asType(mh.type().changeParameterType(0, Object.class)); Object base = mh.invokeExact(theUnsafe, f); mh = lookup.findVirtual(c, "staticFieldOffset", methodType(long.class, Field.class)); mh = mh.asType(mh.type().changeParameterType(0, Object.class)); long offset = (long) mh.invokeExact(theUnsafe, f); mh = lookup.findVirtual(c, "getObject", methodType(Object.class, Object.class, long.class)); mh = mh.asType(mh.type().changeParameterType(0, Object.class)); @SuppressWarnings("unchecked") Map<Class<?>, Boolean> callers = (Map<Class<?>, Boolean>) (Object) mh.invokeExact(theUnsafe, base, offset); callers.put(org.elasticsearch.bootstrap.Security.class, true); } catch (NoSuchFieldException ignore) {} catch (Throwable t) { throw new AssertionError(t); } }	i just had one minor question for my own curiosity, does this work exactly the same for all jdk versions? does it make sense for us to do this only for jdk17 where the warning is shown? i'm thinking more along the lines of 7.16 and the various supported jdk versions we run/test with.
public static FieldAttribute checkIsFieldAttribute(Expression e) { Check.isTrue(e instanceof FieldAttribute, "Expected a FieldAttributed but received [{}]", e); return (FieldAttribute) e; }	typo suggestion check.istrue(e instanceof fieldattribute, "expected a fieldattribute but received [{}]", e);
private void checkTotalFieldsLimit(long totalMappers) { long allowedTotalFields = indexSettings.getValue(INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING); if (allowedTotalFields > 0 && allowedTotalFields < totalMappers) { throw new IllegalArgumentException("Limit of total fields [" + allowedTotalFields + "] in index [" + index().getName() + "] has been exceeded"); } }	like simon suggested, i would just do: if (allowedtotalfields < totalmappers) {. if someone needs to have many fields anyway and understands the issue, (s)he can still set index.mapping.total_fields.limit=1000000 for instance.
@Override public Query geoShapeQuery(Geometry shape, String fieldName, ShapeRelation relation, SearchExecutionContext context) { final LatLonGeometry[] luceneGeometries = GeoShapeUtils.toLuceneGeometry(fieldName, context, shape, relation); if (luceneGeometries.length == 0) { return new MatchNoDocsQuery(); } // For point queries and intersects, lucene does not match points that are encoded to Integer.MAX_VALUE. Use contains // instead that returns the expected results. ShapeField.QueryRelation luceneRelation = relation == ShapeRelation.INTERSECTS && shape.type() == ShapeType.POINT ? ShapeField.QueryRelation.CONTAINS : relation.getLuceneRelation(); Query query = LatLonPoint.newGeometryQuery(fieldName, luceneRelation, luceneGeometries); if (hasDocValues()) { Query dvQuery = LatLonDocValuesField.newSlowGeometryQuery(fieldName, luceneRelation, luceneGeometries); query = new IndexOrDocValuesQuery(query, dvQuery); } return query; }	i think in this case explicit if statement might make it much easier to comprehend. there is just too much going on with the comment and expression otherwise. wdyt? final shapefield.queryrelation lucenerelation; if (shape.type() == shapetype.point && relation == shaperelation.intersects) { // for point queries and intersects, lucene does not match points that are encoded to integer.max_value. use contains instead lucenerelation = shapefield.queryrelation.contains; } else { lucenerelation = relation.getlucenerelation(); }
public Automaton allowedActionsMatcher(String index) { List<Automaton> automatonList = new ArrayList<>(); for (Group group : groups) { if (group.indexNameMatcher.test(index)) { automatonList.add(group.privilege.getAutomaton()); } } return automatonList.isEmpty() ? Automatons.EMPTY : Automatons.unionAndMinimize(automatonList); }	please add javadocs explaining what this is doing including why we cache groups to automatons
private Automaton indexMatcherAutomaton(String... indices) { final List<String> exactMatch = new ArrayList<>(); final List<String> patternMatch = new ArrayList<>(); for (String indexPattern : indices) { if (isIndexPattern(indexPattern)) { patternMatch.add(indexPattern); } else { exactMatch.add(indexPattern); } } try { final Automaton exactMatchAutomaton = Automatons.patterns(exactMatch); final Automaton indexPatternAutomaton = Automatons.patterns(patternMatch); return Automatons.unionAndMinimize( Arrays.asList(exactMatchAutomaton, Automatons.minusAndMinimize(indexPatternAutomaton, systemIndicesAutomaton))); } catch (TooComplexToDeterminizeException e) { logger.debug("Index pattern automaton [{}] is too complex", Strings.arrayToCommaDelimitedString(indices)); String description = Strings.arrayToCommaDelimitedString(indices); if (description.length() > 80) { description = Strings.cleanTruncate(description, 80) + "..."; } throw new ElasticsearchSecurityException("The set of permitted index patterns [{}] is too complex to evaluate", e, description); } }	this drops the exact match optimization. why do we do it this way?
public void testTopHitsAggregationWithOneArg() { { PhysicalPlan p = optimizeAndPlan("SELECT FIRST(keyword) FROM test"); assertEquals(EsQueryExec.class, p.getClass()); EsQueryExec eqe = (EsQueryExec) p; assertEquals(1, eqe.output().size()); assertEquals("FIRST(keyword)", eqe.output().get(0).qualifiedName()); assertEquals(KEYWORD, eqe.output().get(0).dataType()); assertThat(eqe.queryContainer().aggs().asAggBuilder().toString().replaceAll("\\\\\\\\s+", ""), endsWith("\\\\"top_hits\\\\":{\\\\"from\\\\":0,\\\\"size\\\\":1,\\\\"version\\\\":false,\\\\"seq_no_primary_term\\\\":false," + "\\\\"explain\\\\":false,\\\\"docvalue_fields\\\\":[{\\\\"field\\\\":\\\\"keyword\\\\"}]," + "\\\\"sort\\\\":[{\\\\"keyword\\\\":{\\\\"order\\\\":\\\\"asc\\\\",\\\\"missing\\\\":\\\\"_last\\\\",\\\\"unmapped_type\\\\":\\\\"keyword\\\\"}}]}}}}}")); } { PhysicalPlan p = optimizeAndPlan("SELECT MIN(keyword) FROM test"); assertEquals(EsQueryExec.class, p.getClass()); EsQueryExec eqe = (EsQueryExec) p; assertEquals(1, eqe.output().size()); assertEquals("MIN(keyword)", eqe.output().get(0).qualifiedName()); assertEquals(KEYWORD, eqe.output().get(0).dataType()); assertThat(eqe.queryContainer().aggs().asAggBuilder().toString().replaceAll("\\\\\\\\s+", ""), endsWith("\\\\"top_hits\\\\":{\\\\"from\\\\":0,\\\\"size\\\\":1,\\\\"version\\\\":false,\\\\"seq_no_primary_term\\\\":false," + "\\\\"explain\\\\":false,\\\\"docvalue_fields\\\\":[{\\\\"field\\\\":\\\\"keyword\\\\"}]," + "\\\\"sort\\\\":[{\\\\"keyword\\\\":{\\\\"order\\\\":\\\\"asc\\\\",\\\\"missing\\\\":\\\\"_last\\\\",\\\\"unmapped_type\\\\":\\\\"keyword\\\\"}}]}}}}}")); } { PhysicalPlan p = optimizeAndPlan("SELECT LAST(date) FROM test"); assertEquals(EsQueryExec.class, p.getClass()); EsQueryExec eqe = (EsQueryExec) p; assertEquals(1, eqe.output().size()); assertEquals("LAST(date)", eqe.output().get(0).qualifiedName()); assertEquals(DATETIME, eqe.output().get(0).dataType()); assertThat(eqe.queryContainer().aggs().asAggBuilder().toString().replaceAll("\\\\\\\\s+", ""), endsWith("\\\\"top_hits\\\\":{\\\\"from\\\\":0,\\\\"size\\\\":1,\\\\"version\\\\":false,\\\\"seq_no_primary_term\\\\":false," + "\\\\"explain\\\\":false,\\\\"docvalue_fields\\\\":[{\\\\"field\\\\":\\\\"date\\\\",\\\\"format\\\\":\\\\"epoch_millis\\\\"}]," + "\\\\"sort\\\\":[{\\\\"date\\\\":{\\\\"order\\\\":\\\\"desc\\\\",\\\\"missing\\\\":\\\\"_last\\\\",\\\\"unmapped_type\\\\":\\\\"date\\\\"}}	this is not correct, it should be \\\\"calendar_interval\\\\":\\\\"3w\\\\".
private void onNoLongerPrimary(Exception failure) { final String message; if (failure instanceof ShardStateAction.NoLongerPrimaryShardException) { // we are no longer the primary, fail ourselves and start over message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard", primary.routingEntry()); primary.failShard(message, failure); } else { // these can occur if the node is shutting down and are okay any other exception here is not expected and merits investigation. assert failure instanceof NodeClosedException || failure instanceof TransportException : failure; message = String.format(Locale.ROOT, "primary node [%s] is shutting down while failing replica shard", primary.routingEntry()); } finishAsFailed(new RetryOnPrimaryException(primary.routingEntry().shardId(), message, failure)); }	@bleskes mentioned that the transportexception here should be coming from https://github.com/elastic/elasticsearch/blob/022726011ce1d9e4748fd39293a3aeae6954b3ac/server/src/main/java/org/elasticsearch/transport/transportservice.java#l645-l649 we should verify this here with an assertion and (i think in a follow-up) look into throwing a more appropriate exception in transportservice. possible options are alreadyclosedexception or a custom subclass of transportexception.
public void testRestartPrimaryNodeWhileIndexing() throws Exception { startCluster(3); String index = "failover_index"; assertAcked(client().admin().indices().prepareCreate(index).setSettings(Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, between(1, 2)))); AtomicBoolean stopped = new AtomicBoolean(); Thread[] threads = new Thread[between(1, 4)]; AtomicInteger docID = new AtomicInteger(); Set<String> ackedDocs = ConcurrentCollections.newConcurrentSet(); for (int i = 0; i < threads.length; i++) { threads[i] = new Thread(() -> { while (stopped.get() == false && docID.get() < 5000) { int docId = frequently() ? docID.getAndIncrement() : between(0, docID.getAndIncrement()); try { IndexResponse response = client().prepareIndex(index, "_doc", Integer.toString(docId)) .setSource("{\\\\"f\\\\":" + docId + "}", XContentType.JSON).get(); assertThat(response.getResult(), isOneOf(CREATED, UPDATED)); logger.info("--> index id={} seq_no={}", response.getId(), response.getSeqNo()); ackedDocs.add(response.getId()); } catch (ElasticsearchException ignore) { logger.info("--> fail to index id={}", docId); } } }); threads[i].start(); } ensureGreen(index); assertBusy(() -> assertThat(docID.get(), greaterThanOrEqualTo(100))); ClusterState clusterState = internalCluster().clusterService().state(); for (ShardRouting shardRouting : clusterState.routingTable().allShards(index)) { if (shardRouting.primary()) { String nodeName = clusterState.nodes().get(shardRouting.currentNodeId()).getName(); internalCluster().restartNode(nodeName, new InternalTestCluster.RestartCallback()); break; } } ensureGreen(index); assertBusy(() -> assertThat(docID.get(), greaterThanOrEqualTo(200))); stopped.set(true); for (Thread thread : threads) { thread.join(); } clusterState = internalCluster().clusterService().state(); for (ShardRouting shardRouting : clusterState.routingTable().allShards(index)) { String nodeName = clusterState.nodes().get(shardRouting.currentNodeId()).getName(); IndicesService indicesService = internalCluster().getInstance(IndicesService.class, nodeName); IndexShard indexShard = indicesService.getShardOrNull(shardRouting.shardId()); assertThat(IndexShardTestCase.getShardDocUIDs(indexShard), equalTo(ackedDocs)); } }	perhaps just restart a random node instead of explicitly the one with the primary?
* @return true if the index has a custom data path */ public static boolean hasCustomDataPath(@IndexSettings Settings indexSettings) { return indexSettings.get(IndexMetaData.SETTING_DATA_PATH) != null; } /** * Resolve the custom path for a index's shard. * Uses the {@code IndexMetaData.SETTING_DATA_PATH}	hmm can we put this back once you address the nocommit about getting the right settings passed down?
protected void possiblySetElasticPassword(SecurityIndexManager.State previousState, SecurityIndexManager.State currentState) { if (previousState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) && currentState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) == false && securityIndex.get().indexExists() == false && elasticPasswordHash.get() != null) { final ChangePasswordRequest request = new ChangePasswordRequest(); request.username("elastic"); request.passwordHash(elasticPasswordHash.get().getChars()); nativeUsersStore.get().changePassword(request, ActionListener.wrap( r -> {}, e -> logger.warn("failed to set the elastic user password from the value of [" + ELASTIC_PASSWORD_HASH.getKey() + "]"))); elasticPasswordHash.get().close(); } }	wait until the state of the security index has been recovered from either the disk or a cluster state update.
protected void possiblySetElasticPassword(SecurityIndexManager.State previousState, SecurityIndexManager.State currentState) { if (previousState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) && currentState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) == false && securityIndex.get().indexExists() == false && elasticPasswordHash.get() != null) { final ChangePasswordRequest request = new ChangePasswordRequest(); request.username("elastic"); request.passwordHash(elasticPasswordHash.get().getChars()); nativeUsersStore.get().changePassword(request, ActionListener.wrap( r -> {}, e -> logger.warn("failed to set the elastic user password from the value of [" + ELASTIC_PASSWORD_HASH.getKey() + "]"))); elasticPasswordHash.get().close(); } }	what should happen if security index gets deleted ? would bootstrapping this again to set value be valuable/surprising/wrong?
protected void possiblySetElasticPassword(SecurityIndexManager.State previousState, SecurityIndexManager.State currentState) { if (previousState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) && currentState.equals(SecurityIndexManager.State.UNRECOVERED_STATE) == false && securityIndex.get().indexExists() == false && elasticPasswordHash.get() != null) { final ChangePasswordRequest request = new ChangePasswordRequest(); request.username("elastic"); request.passwordHash(elasticPasswordHash.get().getChars()); nativeUsersStore.get().changePassword(request, ActionListener.wrap( r -> {}, e -> logger.warn("failed to set the elastic user password from the value of [" + ELASTIC_PASSWORD_HASH.getKey() + "]"))); elasticPasswordHash.get().close(); } }	if this fails the first time we're in trouble, the "promised" password is never going to be set.
private void applyChanges(UpdateTask task, ClusterState previousClusterState, ClusterState newClusterState) { ClusterChangedEvent clusterChangedEvent = new ClusterChangedEvent(task.source, newClusterState, previousClusterState); // new cluster state, notify all listeners final DiscoveryNodes.Delta nodesDelta = clusterChangedEvent.nodesDelta(); if (nodesDelta.hasChanges() && logger.isInfoEnabled()) { String summary = nodesDelta.shortSummary(); if (summary.length() > 0) { logger.info("{}, term: {}, version: {}, reason: {}", summary, newClusterState.term(), newClusterState.version(), task.source); } } nodeConnectionsService.connectToNodes(newClusterState.nodes()); logger.debug("applying cluster state version {}", newClusterState.version()); try { // nothing to do until we actually recover from the gateway or any other block indicates we need to disable persistency if (clusterChangedEvent.state().blocks().disableStatePersistence() == false && clusterChangedEvent.metaDataChanged()) { final Settings incomingSettings = clusterChangedEvent.state().metaData().settings(); clusterSettings.applySettings(incomingSettings); } } catch (Exception ex) { logger.warn("failed to apply cluster settings", ex); } logger.debug("apply cluster state with version {}", newClusterState.version()); callClusterStateAppliers(clusterChangedEvent); nodeConnectionsService.disconnectFromNodesExcept(newClusterState.nodes()); logger.debug("set locally applied cluster state to version {}", newClusterState.version()); state.set(newClusterState); callClusterStateListeners(clusterChangedEvent); task.listener.onSuccess(task.source); }	should we update the same message in masterservice?
private BulkRequest processBulkIfNeeded(BulkRequest bulkRequest, boolean force) { if ((force && bulkRequest.numberOfActions() > 0) || bulkRequest.numberOfActions() >= bulkSize) { try { bulkAction.executeBulk(bulkRequest, new ActionListener<BulkResponse>() { @Override public void onResponse(BulkResponse bulkResponse) { if (bulkResponse.hasFailures()) { int failedItems = 0; for (BulkItemResponse response : bulkResponse) { if (response.isFailed()) failedItems++; } if (logger.isTraceEnabled()) { logger.trace("Bulk deletion failures for [{}]/[{}] items, failure message: [{}]", failedItems, bulkResponse.getItems().length, bulkResponse.buildFailureMessage()); } else { logger.error("Bulk deletion failures for [{}]/[{}] items", failedItems, bulkResponse.getItems().length); } } else { logger.trace("Bulk deletion took " + bulkResponse.getTookInMillis() + "ms"); } } @Override public void onFailure(Throwable e) { if (logger.isTraceEnabled()) { logger.trace("failed to execute bulk", e); } else { logger.warn("failed to execute bulk: [{}]", e.getMessage()); } } }); } catch (Exception e) { logger.warn("failed to process bulk", e); } bulkRequest = new BulkRequest(); } return bulkRequest; }	can we s/bulk/bulk here? i think we try to stay with lowercase in logs right?
@Override public SortedBinaryDocValues bytesValues(LeafReaderContext context) throws IOException { return new Bytes.WithScript.BytesValues(delegate.bytesValues(context), script.newInstance(context)); }	@polyfractal as per above comment, this is one of the rough edges. we're in a numeric subclass, but we have this bytesvalues method which calls into a different subclass now. i'm just guessing, but i suspect this is why withscript wasn't a bytes subclass originally.
private static BytesReference replaceVariables(BytesReference input, String version, String versionProperty, Map<String, String> variables) { String template = replaceVariable(input.utf8ToString(), versionProperty, version); for (Map.Entry<String, String> variable : variables.entrySet()) { template = replaceVariable(template, variable.getKey(), variable.getValue()); } return new BytesArray(template); }	it looks unsafe to inject a string into a regex pattern like this, it would be good to use pattern.quote(variable) or disallow special characters
public static IndexStorePlugin.DirectoryFactory newDirectoryFactory(final Supplier<RepositoriesService> repositoriesService, final Supplier<CacheService> cacheService, final LongSupplier currentTimeNanosSupplier) { return (indexSettings, shardPath) -> { final RepositoriesService repositories = repositoriesService.get(); assert repositories != null; final CacheService cache = cacheService.get(); assert cache != null; final Repository repository = repositories.repository(SNAPSHOT_REPOSITORY_SETTING.get(indexSettings.getSettings())); // TODO: is this okay? // if (repository instanceof SearchableSnapshotRepository == false) { // throw new IllegalArgumentException("Repository [" + repository + "] is not searchable"); // } SearchableSnapshotRepository searchableRepo = new SearchableSnapshotRepository(repository); return searchableRepo.makeDirectory(indexSettings, shardPath, cache, currentTimeNanosSupplier); }; }	this is "disrupting" the usual lifecycle of repository. on the other hand, searchablesnapshotrepository was useful when the searchable repository was explicitly registered as a delegating repository. i don't think it's needed anymore: the directoryfactory logic can be moved back to the plugin class, and instead of passing directly a blobcontainer instance to the searchablesnapshotdirectory it could use the repositoriesservice to retrieve it on demande. this way we get rid of a class and we take care of reloading the blobstorerepository instance that could have changed. wdyt?
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ContextSetup that = (ContextSetup) o; return Objects.equals(index, that.index) && Objects.equals(document, that.document) && Objects.equals(query, that.query) && Objects.equals(xContentType, that.xContentType); }	this line fixes the bug reported in #36050. the rest of this change is to improve testing for the painless execute api request serialization.
static DeprecationIssue checkReservedPrefixedRealmNames(final Settings settings, final PluginsAndModules pluginsAndModules) { final Map<RealmConfig.RealmIdentifier, Settings> realmSettings = RealmSettings.getRealmSettings(settings); if (realmSettings.isEmpty()) { return null; } List<RealmConfig.RealmIdentifier> reservedPrefixedRealmIdentifiers = new ArrayList<>(); for (RealmConfig.RealmIdentifier realmIdentifier: realmSettings.keySet()) { if (realmIdentifier.getName().startsWith(RESERVED_REALM_NAME_PREFIX)) { reservedPrefixedRealmIdentifiers.add(realmIdentifier); } } if (reservedPrefixedRealmIdentifiers.isEmpty()) { return null; } else { return new DeprecationIssue( DeprecationIssue.Level.CRITICAL, "Realm names cannot start with [" + RESERVED_REALM_NAME_PREFIX + "] in next major release.", "https://www.elastic.co/guide/en/elasticsearch/reference/7.14/deprecated-7.14.html#reserved-prefixed-realm-names", String.format(Locale.ROOT, "Found realm " + (reservedPrefixedRealmIdentifiers.size() == 1 ? "name" : "names") + " with reserved prefix [%s]: [%s]. " + "In next major release, node will fail to start if any realm names start with reserved prefix.", RESERVED_REALM_NAME_PREFIX, reservedPrefixedRealmIdentifiers.stream() .map(rid -> RealmSettings.PREFIX + rid.getType() + "." + rid.getName()) .sorted() .collect(Collectors.joining("; "))) ); } }	per above comments, we need to decide on our plans for 8.0 in order to work out what message to include here.
private String convertIntToOrdinal(int i) { String[] suffixes = new String[] { "th", "st", "nd", "rd", "th", "th", "th", "th", "th", "th" }; switch (i % 100) { case 11: case 12: case 13: return i + "th"; default: return i + suffixes[i % 10]; } }	i'm not sure we can just lift code from https://stackoverflow.com/questions/6810336/is-there-a-way-in-java-to-convert-an-integer-to-its-ordinal-name/6810409#6810409 - it's cc-by-sa and we're dual sspl/elastic v2. i don't know.
public void testResolvePath() throws Exception { final int numOfNodes = randomIntBetween(1, 5); final List<String> nodeNames = internalCluster().startNodes(numOfNodes, Settings.EMPTY); final String indexName = "test" + randomInt(100); assertAcked(prepareCreate(indexName).setSettings(Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, numOfNodes - 1) )); flush(indexName); ensureGreen(indexName); final Map<String, String> nodeNameToNodeId = new HashMap<>(); final ClusterState state = client().admin().cluster().prepareState().get().getState(); final DiscoveryNodes nodes = state.nodes(); for (ObjectObjectCursor<String, DiscoveryNode> cursor : nodes.getNodes()) { nodeNameToNodeId.put(cursor.value.getName(), cursor.key); } final GroupShardsIterator shardIterators = state.getRoutingTable().activePrimaryShardsGrouped(new String[]{indexName}, false); final List<ShardIterator> iterators = iterableAsArrayList(shardIterators); final ShardRouting shardRouting = iterators.iterator().next().nextOrNull(); assertThat(shardRouting, notNullValue()); final ShardId shardId = shardRouting.shardId(); final RemoveCorruptedShardDataCommand command = new RemoveCorruptedShardDataCommand(); final OptionParser parser = command.getParser(); final Map<String, Path> indexPathByNodeName = new HashMap<>(); final Map<String, Environment> environmentByNodeName = new HashMap<>(); for (String nodeName : nodeNames) { final String nodeId = nodeNameToNodeId.get(nodeName); indexPathByNodeName.put(nodeName, getPathToShardData(nodeId, shardId, ShardPath.INDEX_FOLDER_NAME)); final Environment environment = TestEnvironment.newEnvironment( Settings.builder().put(internalCluster().getDefaultSettings()).put(internalCluster().dataPathSettings(nodeName)).build()); environmentByNodeName.put(nodeName, environment); internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodeName)); logger.info(" -- stopped {}", nodeName); } for (String nodeName : nodeNames) { final Path indexPath = indexPathByNodeName.get(nodeName); final OptionSet options = parser.parse("--dir", indexPath.toAbsolutePath().toString()); command.findAndProcessShardPath(options, environmentByNodeName.get(nodeName), new Path[] { environmentByNodeName.get(nodeName).dataFile() }, state, shardPath -> assertThat(shardPath.resolveIndex(), equalTo(indexPath))); } }	is there potential for a follow up here to make this method parameter be single-valued?
*/ public boolean isOnlyExact() { return onlyExact; } /** * A simple matcher with one purpose to test whether an id * matches a expression that may contain wildcards. * Use the {@link #idMatches(String)} function to * test if the given id is matched by any of the matchers. * * Unlike {@link ExpandedIdsMatcher}	why do we need this? can't we use expandedidsmatcher instead?
public ModelStats getStats(TrainedModelDeploymentTask task) { ProcessContext processContext = processContextByAllocation.get(task.getAllocationId()); Long modelSizeBytes = processContext.getModelSizeBytes() < 0 ? null : (long) processContext.getModelSizeBytes(); return new ModelStats(processContext.resultProcessor.getTimingStats(), processContext.resultProcessor.getLastUsed(), modelSizeBytes); }	should we check the process context exists and throw an error if not?
public void testScroll() throws IOException { RestHighLevelClient client = highLevelClient(); { BulkRequest request = new BulkRequest(); request.add(new IndexRequest("posts", "doc", "1") .source(XContentType.JSON, "title", "In which order are my Elasticsearch queries executed?")); request.add(new IndexRequest("posts", "doc", "2") .source(XContentType.JSON, "title", "Current status and upcoming changes in Elasticsearch")); request.add(new IndexRequest("posts", "doc", "3") .source(XContentType.JSON, "title", "The Future of Federated Search in Elasticsearch")); request.setRefreshPolicy(WriteRequest.RefreshPolicy.WAIT_UNTIL); BulkResponse bulkResponse = client.bulk(request); assertSame(bulkResponse.status(), RestStatus.OK); assertFalse(bulkResponse.hasFailures()); } String lastScrollId = null; { // tag::search-scroll-request SearchRequest searchRequest = new SearchRequest("posts"); // <1> searchRequest.scroll(TimeValue.timeValueMinutes(1L)); // <2> // end::search-scroll-request searchRequest.source(new SearchSourceBuilder().size(1)); // tag::search-response-scroll-id SearchResponse searchResponse = client.search(searchRequest); // <1> SearchHits searchHits = searchResponse.getHits(); // <2> String scrollId = searchResponse.getScrollId(); // <3> // end::search-response-scroll-id assertEquals(0, searchResponse.getFailedShards()); assertEquals(3L, searchResponse.getHits().getTotalHits()); assertEquals(1L, searchHits.getHits().length); assertNotNull(scrollId); lastScrollId = scrollId; } { String scrollId = lastScrollId; // tag::search-scroll-execute while (true) { SearchScrollRequest scrollRequest = new SearchScrollRequest() // <1> .scroll("60s") // <2> .scrollId(scrollId); // <3> SearchResponse searchResponse = client.searchScroll(scrollRequest); // <4> scrollId = searchResponse.getScrollId(); // <5> SearchHit[] searchHits = searchResponse.getHits().getHits(); // <6> if (searchHits != null && searchHits.length > 0) { // <7> } else { // <8> break; } } // end::search-scroll-execute assertNotNull(scrollId); lastScrollId = scrollId; } { SearchScrollRequest scrollRequest = new SearchScrollRequest(); scrollRequest.scrollId(lastScrollId); // tag::scroll-request-scroll scrollRequest.scroll(TimeValue.timeValueSeconds(60L)); // <1> scrollRequest.scroll("60s"); // <2> // end::scroll-request-scroll // tag::search-scroll-execute-sync SearchResponse searchResponse = client.searchScroll(scrollRequest); // end::search-scroll-execute-sync assertEquals(0, searchResponse.getFailedShards()); assertEquals(3L, searchResponse.getHits().getTotalHits()); // tag::search-scroll-execute-async client.searchScrollAsync(scrollRequest, new ActionListener<SearchResponse>() { @Override public void onResponse(SearchResponse searchResponse) { // <1> } @Override public void onFailure(Exception e) { // <2> } }); // end::search-scroll-execute-async } { String scrollId = lastScrollId; // tag::clear-scroll-request ClearScrollRequest request = new ClearScrollRequest(); // <1> request.addScrollId(scrollId); // <2> // end::clear-scroll-request // tag::clear-scroll-add-scroll-id request.addScrollId(scrollId); // end::clear-scroll-add-scroll-id List<String> scrollIds = Arrays.asList(scrollId); // tag::clear-scroll-add-scroll-ids request.setScrollIds(scrollIds); // end::clear-scroll-add-scroll-ids // tag::clear-scroll-execute ClearScrollResponse response = client.clearScroll(request); // end::clear-scroll-execute // tag::clear-scroll-response boolean success = response.isSucceeded(); // <1> int released = response.getNumFreed(); // <2> // end::clear-scroll-response assertTrue(success); assertThat(released, greaterThan(0)); // tag::clear-scroll-execute-async client.clearScrollAsync(request, new ActionListener<ClearScrollResponse>() { @Override public void onResponse(ClearScrollResponse clearScrollResponse) { // <1> } @Override public void onFailure(Exception e) { // <2> } }); // end::clear-scroll-execute-async } }	how about having a complete example with the initial search response too? maybe leave a separate search scroll snippet, then move to the complete example where you reuse the searchhits from the initial searchresponse and you could do while(searchhits != null && searchhits.length > 0) instead of while(true)? would that work or would it be too complicated?
public void testPerformAction() { IndexMetaData.Builder indexMetaDataBuilder = IndexMetaData.builder(randomAlphaOfLength(10)).settings(settings(Version.CURRENT)) .numberOfShards(randomIntBetween(1, 5)).numberOfReplicas(randomIntBetween(0, 5)); AliasMetaData.Builder aliasBuilder = AliasMetaData.builder(randomAlphaOfLengthBetween(3, 10)); if (randomBoolean()) { aliasBuilder.routing(randomAlphaOfLengthBetween(3, 10)); } if (randomBoolean()) { aliasBuilder.searchRouting(randomAlphaOfLengthBetween(3, 10)); } if (randomBoolean()) { aliasBuilder.indexRouting(randomAlphaOfLengthBetween(3, 10)); } String aliasMetaDataFilter = randomBoolean() ? null : "{\\\\"term\\\\":{\\\\"year\\\\":2016}}"; aliasBuilder.filter(aliasMetaDataFilter); aliasBuilder.writeIndex(randomBoolean()); AliasMetaData aliasMetaData = aliasBuilder.build(); IndexMetaData indexMetaData = indexMetaDataBuilder.putAlias(aliasMetaData).build(); ShrinkSetAliasStep step = createRandomInstance(); String sourceIndex = indexMetaData.getIndex().getName(); String shrunkenIndex = step.getShrunkIndexPrefix() + sourceIndex; List<AliasActions> expectedAliasActions = Arrays.asList( IndicesAliasesRequest.AliasActions.removeIndex().index(sourceIndex), IndicesAliasesRequest.AliasActions.add().index(shrunkenIndex).alias(sourceIndex), IndicesAliasesRequest.AliasActions.add().index(shrunkenIndex).alias(aliasMetaData.alias()) .searchRouting(aliasMetaData.searchRouting()).indexRouting(aliasMetaData.indexRouting()) .filter(aliasMetaDataFilter).writeIndex(null)); AdminClient adminClient = Mockito.mock(AdminClient.class); IndicesAdminClient indicesClient = Mockito.mock(IndicesAdminClient.class); Mockito.when(client.admin()).thenReturn(adminClient); Mockito.when(adminClient.indices()).thenReturn(indicesClient); Mockito.doAnswer(new Answer<Void>() { @Override public Void answer(InvocationOnMock invocation) throws Throwable { IndicesAliasesRequest request = (IndicesAliasesRequest) invocation.getArguments()[0]; assertThat(request.getAliasActions(), equalTo(expectedAliasActions)); @SuppressWarnings("unchecked") ActionListener<AcknowledgedResponse> listener = (ActionListener<AcknowledgedResponse>) invocation.getArguments()[1]; listener.onResponse(new AcknowledgedResponse(true)); return null; } }).when(indicesClient).aliases(Mockito.any(), Mockito.any()); SetOnce<Boolean> actionCompleted = new SetOnce<>(); step.performAction(indexMetaData, null, new Listener() { @Override public void onResponse(boolean complete) { actionCompleted.set(complete); } @Override public void onFailure(Exception e) { throw new AssertionError("Unexpected method call", e); } }); assertTrue(actionCompleted.get()); Mockito.verify(client, Mockito.only()).admin(); Mockito.verify(adminClient, Mockito.only()).indices(); Mockito.verify(indicesClient, Mockito.only()).aliases(Mockito.any(), Mockito.any()); }	can we randomly set is_write_index as well to make sure that is not copied?
static void addSearchRequestParams(Params params, SearchRequest searchRequest) { params.putParam(RestSearchAction.TYPED_KEYS_PARAM, "true"); params.withRouting(searchRequest.routing()); params.withPreference(searchRequest.preference()); params.withIndicesOptions(searchRequest.indicesOptions()); params.putParam("search_type", searchRequest.searchType().name().toLowerCase(Locale.ROOT)); params.putParam("ccs_minimize_roundtrips", Boolean.toString(searchRequest.isCcsMinimizeRoundtrips())); params.putParam("pre_filter_shard_size", Integer.toString(searchRequest.getPreFilterShardSize())); if (searchRequest.requestCache() != null) { params.putParam("request_cache", Boolean.toString(searchRequest.requestCache())); } if (searchRequest.allowPartialSearchResults() != null) { params.putParam("allow_partial_search_results", Boolean.toString(searchRequest.allowPartialSearchResults())); } params.putParam("batched_reduce_size", Integer.toString(searchRequest.getBatchedReduceSize())); if (searchRequest.scroll() != null) { params.putParam("scroll", searchRequest.scroll().keepAlive()); } }	good catch, looks like search was not supporting this parameter.
@Override protected void masterOperation(Task task, final CreateIndexRequest request, final ClusterState state, final ActionListener<CreateIndexResponse> listener) { String cause = request.cause(); if (cause.length() == 0) { cause = "api"; } final String indexName = indexNameExpressionResolver.resolveDateMathExpression(request.index()); String mappings = request.mappings(); Settings settings = request.settings(); Set<Alias> aliases = request.aliases(); String concreteIndexName = indexName; boolean isSystemIndex = false; SystemIndexDescriptor descriptor = systemIndices.findMatchingDescriptor(indexName); if (descriptor != null && descriptor.isAutomaticallyManaged()) { isSystemIndex = true; // System indices define their own settings and mappings, which cannot be overridden. mappings = descriptor.getMappings(); settings = descriptor.getSettings(); if (descriptor.getAliasName() == null) { aliases = Set.of(); } else { concreteIndexName = descriptor.getIndexPattern(); aliases = Set.of(new Alias(descriptor.getAliasName())); } } final CreateIndexClusterStateUpdateRequest updateRequest = new CreateIndexClusterStateUpdateRequest(cause, concreteIndexName, request.index()) .ackTimeout(request.timeout()).masterNodeTimeout(request.masterNodeTimeout()) .aliases(aliases) .waitForActiveShards(request.waitForActiveShards()); if (isSystemIndex) { updateRequest.waitForActiveShards(ActiveShardCount.ALL); } if (mappings != null) { updateRequest.mappings(mappings); } if (settings != null) { updateRequest.settings(settings); } createIndexService.createIndex(updateRequest, ActionListener.map(listener, response -> new CreateIndexResponse(response.isAcknowledged(), response.isShardsAcknowledged(), indexName))); }	i think we might need a method to get the current concrete index name? maybe someone defines a pattern with a * to cover a variety of system indices but we should know what the concrete value to create is (ie security_6 vs security_7).
public void add(QueryCacheStats stats) { ramBytesUsed += stats.ramBytesUsed; hitCount += stats.hitCount; missCount += stats.missCount; cacheCount += stats.cacheCount; cacheSize += stats.cacheSize; if (ramBytesUsed < -1 && (ramBytesUsed + stats.ramBytesUsed >= 0)) { logger.debug(() -> new ParameterizedMessage( "negative query cache size [{}] on thread [{}] with stats [{}] and stack trace:\\\\n{}", ramBytesUsed, Thread.currentThread().getName(), stats.ramBytesUsed, ExceptionsHelper.formatStackTrace(Thread.currentThread().getStackTrace()))); } }	can you add a comment to why such an odd if condition ? specifically why && (rambytesused + stats.rambytesused >= 0) is part of the if condition. alternatively, since this is an edge case at debug level, can the if condition be simplified to just the first part ? if (rambytesused < -1 ){
default void markLastAcceptedConfigAsCommitted() { final ClusterState lastAcceptedState = getLastAcceptedState(); MetaData.Builder metaDataBuilder = null; if (lastAcceptedState.getLastAcceptedConfiguration().equals(lastAcceptedState.getLastCommittedConfiguration()) == false) { final CoordinationMetaData coordinationMetaData = CoordinationMetaData.builder(lastAcceptedState.coordinationMetaData()) .lastCommittedConfiguration(lastAcceptedState.getLastAcceptedConfiguration()) .build(); if (metaDataBuilder == null) { metaDataBuilder = MetaData.builder(lastAcceptedState.metaData()); } metaDataBuilder.coordinationMetaData(coordinationMetaData); } if (lastAcceptedState.metaData().clusterUUID().equals(MetaData.UNKNOWN_CLUSTER_UUID) == false && lastAcceptedState.metaData().clusterUUIDCommitted() == false) { if (metaDataBuilder == null) { metaDataBuilder = MetaData.builder(lastAcceptedState.metaData()); } metaDataBuilder.clusterUUIDCommitted(true); } if (metaDataBuilder != null) { setLastAcceptedState(ClusterState.builder(lastAcceptedState).metaData(metaDataBuilder).build()); } }	how metadatabuilder can be not null at this point?
public void testCannotJoinClusterWithDifferentUUID() throws IllegalAccessException { final Cluster cluster1 = new Cluster(randomIntBetween(1, 3)); cluster1.runRandomly(); cluster1.stabilise(); final Cluster cluster2 = new Cluster(3); cluster2.runRandomly(); cluster2.stabilise(); final ClusterNode shiftedNode = randomFrom(cluster2.clusterNodes).restartedNode(); final ClusterNode newNode = cluster1.new ClusterNode(nextNodeIndex.getAndIncrement(), shiftedNode.getLocalNode(), n -> shiftedNode.persistedState); cluster1.clusterNodes.add(newNode); MockLogAppender mockAppender = new MockLogAppender(); mockAppender.start(); mockAppender.addExpectation( new MockLogAppender.SeenEventExpectation( "test1", JoinHelper.class.getCanonicalName(), Level.INFO, "*failed to join*")); Logger joinLogger = LogManager.getLogger(JoinHelper.class); Loggers.addAppender(joinLogger, mockAppender); cluster1.runFor(10000, "failing join validation"); try { mockAppender.assertAllExpectationsMatched(); } finally { Loggers.removeAppender(joinLogger, mockAppender); mockAppender.stop(); } assertTrue(newNode.getLastAppliedClusterState().version() == 0); // reset clusterUUIDCommitted (and node / cluster state term) to let node join again final ClusterNode detachedNode = newNode.restartedNode( metaData -> MetaData.builder(metaData) .clusterUUIDCommitted(false) .coordinationMetaData(CoordinationMetaData.builder(metaData.coordinationMetaData()) .term(0L).build()) .build(), term -> 0L); cluster1.clusterNodes.replaceAll(cn -> cn == newNode ? detachedNode : cn); cluster1.stabilise(); }	it's a pity that we don't have a better way of understanding that join validation has failed, other than analyzing log output
public void testCannotJoinClusterWithDifferentUUID() throws IllegalAccessException { final Cluster cluster1 = new Cluster(randomIntBetween(1, 3)); cluster1.runRandomly(); cluster1.stabilise(); final Cluster cluster2 = new Cluster(3); cluster2.runRandomly(); cluster2.stabilise(); final ClusterNode shiftedNode = randomFrom(cluster2.clusterNodes).restartedNode(); final ClusterNode newNode = cluster1.new ClusterNode(nextNodeIndex.getAndIncrement(), shiftedNode.getLocalNode(), n -> shiftedNode.persistedState); cluster1.clusterNodes.add(newNode); MockLogAppender mockAppender = new MockLogAppender(); mockAppender.start(); mockAppender.addExpectation( new MockLogAppender.SeenEventExpectation( "test1", JoinHelper.class.getCanonicalName(), Level.INFO, "*failed to join*")); Logger joinLogger = LogManager.getLogger(JoinHelper.class); Loggers.addAppender(joinLogger, mockAppender); cluster1.runFor(10000, "failing join validation"); try { mockAppender.assertAllExpectationsMatched(); } finally { Loggers.removeAppender(joinLogger, mockAppender); mockAppender.stop(); } assertTrue(newNode.getLastAppliedClusterState().version() == 0); // reset clusterUUIDCommitted (and node / cluster state term) to let node join again final ClusterNode detachedNode = newNode.restartedNode( metaData -> MetaData.builder(metaData) .clusterUUIDCommitted(false) .coordinationMetaData(CoordinationMetaData.builder(metaData.coordinationMetaData()) .term(0L).build()) .build(), term -> 0L); cluster1.clusterNodes.replaceAll(cn -> cn == newNode ? detachedNode : cn); cluster1.stabilise(); }	seems that resetting term and currentterm to 0 was required? shall we do the same in elasticsearch-node tool? what about version?
public void testNextPageWithDatetimeAndTimezoneParam() throws IOException { Request request = new Request("PUT", "/test_date_timezone"); XContentBuilder createIndex = JsonXContent.contentBuilder().startObject(); createIndex.startObject("mappings"); { createIndex.startObject("properties"); { createIndex.startObject("date").field("type", "date").field("format", "epoch_millis"); createIndex.endObject(); } createIndex.endObject(); } createIndex.endObject().endObject(); request.setJsonEntity(Strings.toString(createIndex)); client().performRequest(request); request = new Request("PUT", "/test_date_timezone/_bulk"); request.addParameter("refresh", "true"); StringBuilder bulk = new StringBuilder(); long[] datetimes = new long[] { 1_000, 10_000, 100_000, 1_000_000, 10_000_000 }; for (long datetime : datetimes) { bulk.append("{\\\\"index\\\\":{}}\\\\n"); bulk.append("{\\\\"date\\\\":").append(datetime).append("}\\\\n"); } request.setJsonEntity(bulk.toString()); assertEquals(200, client().performRequest(request).getStatusLine().getStatusCode()); ZoneId zoneId = randomZone(); String mode = randomMode(); String sqlRequest = "{\\\\"query\\\\":\\\\"SELECT DATE_PART('TZOFFSET', date) AS tz FROM test_date_timezone ORDER BY date\\\\"," + "\\\\"time_zone\\\\":\\\\"" + zoneId.getId() + "\\\\", " + "\\\\"mode\\\\":\\\\"" + mode + "\\\\", " + "\\\\"fetch_size\\\\":2}"; String cursor = null; for (int i = 0; i <= datetimes.length; i += 2) { Map<String, Object> expected = new HashMap<>(); Map<String, Object> response; if (i == 0) { expected.put("columns", singletonList(columnInfo(mode, "tz", "integer", JDBCType.INTEGER, 11))); response = runSql(new StringEntity(sqlRequest, ContentType.APPLICATION_JSON), "", mode); } else { response = runSql(new StringEntity("{\\\\"cursor\\\\":\\\\"" + cursor + "\\\\"" + mode(mode) + "}", ContentType.APPLICATION_JSON), StringUtils.EMPTY, mode); } List<Object> values = new ArrayList<>(2); for (int j = 0; j < (i < datetimes.length - 1 ? 2 : 1); j++) { values.add(singletonList(ZonedDateTime.ofInstant(Instant.ofEpochMilli(datetimes[i + j]), zoneId) .getOffset().getTotalSeconds() / 60)); } expected.put("rows", values); cursor = (String) response.remove("cursor"); assertResponse(expected, response); assertNotNull(cursor); } Map<String, Object> expected = new HashMap<>(); expected.put("rows", emptyList()); assertResponse(expected, runSql(new StringEntity("{ \\\\"cursor\\\\":\\\\"" + cursor + "\\\\"" + mode(mode) + "}", ContentType.APPLICATION_JSON), StringUtils.EMPTY, mode)); }	i think it's perfectly fine as is, but was curious about the reason - if any - for choosing a fetch_size of 2, vs. 1, which would simplify the test just a bit.
@Override protected void addCustomFields(XContentBuilder builder, Params params) throws IOException { builder.startObject("state"); state.toXContent(builder, params); builder.endObject(); if (params.paramAsBoolean("explain", false)) { explanations.toXContent(builder, ToXContent.EMPTY_PARAMS); } }	okay, i see this was previously done in the restclusterrerouteaction.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { ClusterRerouteRequest clusterRerouteRequest = createRequest(request); settingsFilter.addFilterSettingParams(request); if (clusterRerouteRequest.explain()) { request.params().put("explain", Boolean.TRUE.toString()); } // by default, return everything but metadata final String metric = request.param("metric"); if (metric == null) { request.params().put("metric", DEFAULT_METRICS); } return channel -> client.admin().cluster().reroute(clusterRerouteRequest, new RestToXContentListener<>(channel)); }	i see that applying the settingsfilter moved up in this method, but how do we know that the request (as params) is what is used then rendering the clusterstate now in the response? i was following the code but couldn't connect the dots yet, maybe you know?
public void testRemoteMonitoringCollectorRole() { final TransportRequest request = mock(TransportRequest.class); final Authentication authentication = mock(Authentication.class); RoleDescriptor roleDescriptor = new ReservedRolesStore().roleDescriptor("remote_monitoring_collector"); assertNotNull(roleDescriptor); assertThat(roleDescriptor.getMetadata(), hasEntry("_reserved", true)); Role remoteMonitoringCollectorRole = Role.builder(roleDescriptor, null).build(); assertThat(remoteMonitoringCollectorRole.cluster().check(ClusterHealthAction.NAME, request, authentication), is(true)); assertThat(remoteMonitoringCollectorRole.cluster().check(ClusterStateAction.NAME, request, authentication), is(true)); assertThat(remoteMonitoringCollectorRole.cluster().check(ClusterStatsAction.NAME, request, authentication), is(true)); assertThat(remoteMonitoringCollectorRole.cluster().check(GetIndexTemplatesAction.NAME, request, authentication), is(false)); assertThat(remoteMonitoringCollectorRole.cluster().check(PutIndexTemplateAction.NAME, request, authentication), is(false)); assertThat(remoteMonitoringCollectorRole.cluster().check(DeleteIndexTemplateAction.NAME, request, authentication), is(false)); assertThat(remoteMonitoringCollectorRole.cluster().check(ClusterRerouteAction.NAME, request, authentication), is(false)); assertThat(remoteMonitoringCollectorRole.cluster().check(ClusterUpdateSettingsAction.NAME, request, authentication), is(false)); assertThat(remoteMonitoringCollectorRole.cluster().check(MonitoringBulkAction.NAME, request, authentication), is(false)); assertThat(remoteMonitoringCollectorRole.cluster().check(DelegatePkiAuthenticationAction.NAME, request, authentication), is(false)); assertThat(remoteMonitoringCollectorRole.runAs().check(randomAlphaOfLengthBetween(1, 12)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(RecoveryAction.NAME) .test(mockIndexAbstraction("foo")), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(SearchAction.NAME) .test(mockIndexAbstraction("foo")), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(SearchAction.NAME) .test(mockIndexAbstraction(".reporting")), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(SearchAction.NAME) .test(mockIndexAbstraction(".kibana")), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetAction.NAME) .test(mockIndexAbstraction(".kibana")), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher("indices:foo") .test(mockIndexAbstraction(randomAlphaOfLengthBetween(8, 24))), is(false)); Arrays.asList( ".monitoring-" + randomAlphaOfLength(randomIntBetween(0, 13)), "metricbeat-" + randomAlphaOfLength(randomIntBetween(0, 13)) ).forEach((index) -> { logger.info("index name [{}]", index); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher("indices:foo") .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher("indices:bar") .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(DeleteIndexAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(CreateIndexAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndexAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetIndexAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetAliasesAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndicesSegmentsAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(RemoveIndexLifecyclePolicyAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(DeleteAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(UpdateSettingsAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(SearchAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetAction.NAME) .test(mockIndexAbstraction(index)), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetIndexAction.NAME) .test(mockIndexAbstraction(index)), is(false)); }); // These tests might need to change if we add new non-security restricted indices that the monitoring user isn't supposed to see // (but ideally, the monitoring user should see all indices). assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetSettingsAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetSettingsAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndicesShardStoresAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndicesShardStoresAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(UpgradeStatusAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(UpgradeStatusAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(RecoveryAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(RecoveryAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndicesStatsAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndicesStatsAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndicesSegmentsAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndicesSegmentsAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(true)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(SearchAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(SearchAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(GetAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(DeleteAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(DeleteAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndexAction.NAME) .test(mockIndexAbstraction(randomFrom(RestrictedIndicesNames.RESTRICTED_NAMES))), is(false)); assertThat(remoteMonitoringCollectorRole.indices().allowedIndicesMatcher(IndexAction.NAME) .test(mockIndexAbstraction(RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2))), is(false)); assertMonitoringOnRestrictedIndices(remoteMonitoringCollectorRole); assertNoAccessAllowed(remoteMonitoringCollectorRole, RestrictedIndicesNames.RESTRICTED_NAMES); assertNoAccessAllowed(remoteMonitoringCollectorRole, RestrictedIndicesNames.ASYNC_SEARCH_PREFIX + randomAlphaOfLengthBetween(0, 2)); }	why did you change it ti remotemonitoringcollectorrole while the changes are added to remote_monitoring_agent?
@Override public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException { ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalHistogram.TYPE, context) .targetValueType(ValueType.NUMERIC) .formattable(true) .build(); boolean keyed = false; long minDocCount = 1; InternalOrder order = (InternalOrder) InternalOrder.KEY_ASC; long interval = -1; ExtendedBounds extendedBounds = null; long offset = 0; XContentParser.Token token; String currentFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (vsParser.token(currentFieldName, token, parser)) { continue; } else if (token.isValue()) { if ("interval".equals(currentFieldName)) { interval = parser.longValue(); } else if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) { minDocCount = parser.longValue(); } else if ("keyed".equals(currentFieldName)) { keyed = parser.booleanValue(); } else if ("offset".equals(currentFieldName) || "offset".equals(currentFieldName)) { offset = parser.longValue(); } else { throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "]."); } } else if (token == XContentParser.Token.START_OBJECT) { if ("order".equals(currentFieldName)) { while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.VALUE_STRING) { String dir = parser.text(); boolean asc = "asc".equals(dir); if (!asc && !"desc".equals(dir)) { throw new SearchParseException(context, "Unknown order direction [" + dir + "] in aggregation [" + aggregationName + "]. Should be either [asc] or [desc]"); } order = resolveOrder(currentFieldName, asc); } } } else if (EXTENDED_BOUNDS.match(currentFieldName)) { extendedBounds = new ExtendedBounds(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token.isValue()) { if ("min".equals(currentFieldName)) { extendedBounds.min = parser.longValue(true); } else if ("max".equals(currentFieldName)) { extendedBounds.max = parser.longValue(true); } else { throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "]."); } } } } else { throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "]."); } } else { throw new SearchParseException(context, "Unexpected token " + token + " in aggregation [" + aggregationName + "]."); } } if (interval < 0) { throw new SearchParseException(context, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]"); } Rounding rounding = new Rounding.Interval(interval); if (offset != 0) { rounding = new Rounding.PrePostRounding((Rounding.Interval) rounding, -offset, offset); } if (extendedBounds != null) { // with numeric histogram, we can process here and fail fast if the bounds are invalid extendedBounds.processAndValidate(aggregationName, context, ValueParser.RAW); } return new HistogramAggregator.Factory(aggregationName, vsParser.config(), rounding, order, keyed, minDocCount, extendedBounds, new InternalHistogram.Factory()); }	since it is the same in camel and underscore case, you can only check it once?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); coordinatorStats.writeTo(out); } } public static class TransportAction extends TransportNodesAction<Request, Response, NodeRequest, NodeResponse> { private final EnrichCoordinatorProxyAction.Coordinator coordinator; @Inject public TransportAction( ThreadPool threadPool, ClusterService clusterService, TransportService transportService, ActionFilters actionFilters, EnrichCoordinatorProxyAction.Coordinator coordinator ) { super( NAME, threadPool, clusterService, transportService, actionFilters, Request::new, NodeRequest::new, ThreadPool.Names.SAME, NodeResponse.class ); this.coordinator = coordinator; transportService.registerRequestHandler( EnrichCoordinatorStatsAction.BWC_NAME + "[n]", ThreadPool.Names.SAME, NodeRequest::new, (NodeRequest request, TransportChannel channel, Task task) -> channel.sendResponse(nodeOperation(request, task)) ); transportService.registerRequestHandler( EnrichCoordinatorStatsAction.BWC_NAME, ThreadPool.Names.SAME, false, true, Request::new, (final Request request, final TransportChannel channel, Task task) -> execute( task, request, new ChannelActionListener<>(channel, EnrichCoordinatorStatsAction.BWC_NAME, request) ) ); } @Override protected void resolveRequest(Request request, ClusterState clusterState) { DiscoveryNode[] ingestNodes = clusterState.getNodes().getIngestNodes().values().toArray(DiscoveryNode.class); request.setConcreteNodes(ingestNodes); } @Override protected Response newResponse(Request request, List<NodeResponse> nodeResponses, List<FailedNodeException> failures) { return new Response(clusterService.getClusterName(), nodeResponses, failures); } @Override protected NodeRequest newNodeRequest(Request request) { return new NodeRequest(); } @Override protected NodeResponse newNodeResponse(StreamInput in) throws IOException { return new NodeResponse(in); } @Override protected NodeResponse nodeOperation(NodeRequest request) { DiscoveryNode node = clusterService.localNode(); return new NodeResponse(node, coordinator.getStats(node.getId())); } @Override protected String getBwcTransportNodeAction(DiscoveryNode node) { return node.getVersion().before(Version.V_7_7_0) ? BWC_NAME + "[n]" : transportNodeAction; }	this needs to update now that it's slipped.
public void apply(Project project) { if (project != project.getRootProject()) { throw new IllegalStateException(this.getClass().getName() + " can only be applied to the root project."); } GlobalInfoExtension extension = project.getExtensions().create(GLOBAL_INFO_EXTENSION_NAME, GlobalInfoExtension.class); JavaVersion minimumCompilerVersion = JavaVersion.toVersion(getResourceContents("/minimumCompilerVersion")); JavaVersion minimumRuntimeVersion = JavaVersion.toVersion(getResourceContents("/minimumRuntimeVersion")); File compilerJavaHome = findCompilerJavaHome(); File runtimeJavaHome = findRuntimeJavaHome(compilerJavaHome); String testSeedProperty = System.getProperty("tests.seed"); final String testSeed; if (testSeedProperty == null) { long seed = new Random(System.currentTimeMillis()).nextLong(); testSeed = Long.toUnsignedString(seed, 16).toUpperCase(Locale.ROOT); } else { testSeed = testSeedProperty; } final List<JavaHome> javaVersions = new ArrayList<>(); for (int version = 8; version <= Integer.parseInt(minimumCompilerVersion.getMajorVersion()); version++) { if (System.getenv(getJavaHomeEnvVarName(Integer.toString(version))) != null) { javaVersions.add(JavaHome.of(version, new File(findJavaHome(Integer.toString(version))))); } } GenerateGlobalBuildInfoTask generateTask = project.getTasks().create("generateGlobalBuildInfo", GenerateGlobalBuildInfoTask.class, task -> { task.setJavaVersions(javaVersions); task.setMinimumCompilerVersion(minimumCompilerVersion); task.setMinimumRuntimeVersion(minimumRuntimeVersion); task.setCompilerJavaHome(compilerJavaHome); task.setRuntimeJavaHome(runtimeJavaHome); task.getOutputFile().set(new File(project.getBuildDir(), "global-build-info")); task.getCompilerVersionFile().set(new File(project.getBuildDir(), "java-compiler-version")); task.getRuntimeVersionFile().set(new File(project.getBuildDir(), "java-runtime-version")); task.getFipsJvmFile().set(new File(project.getBuildDir(), "in-fips-jvm")); }); PrintGlobalBuildInfoTask printTask = project.getTasks().create("printGlobalBuildInfo", PrintGlobalBuildInfoTask.class, task -> { task.getBuildInfoFile().set(generateTask.getOutputFile()); task.getCompilerVersionFile().set(generateTask.getCompilerVersionFile()); task.getRuntimeVersionFile().set(generateTask.getRuntimeVersionFile()); task.getFipsJvmFile().set(generateTask.getFipsJvmFile()); task.setGlobalInfoListeners(extension.listeners); }); project.getExtensions().getByType(ExtraPropertiesExtension.class).set("defaultParallel", findDefaultParallel(project)); project.allprojects(p -> { // Make sure than any task execution generates and prints build info p.getTasks().configureEach(task -> { if (task != generateTask && task != printTask) { task.dependsOn(printTask); } }); ExtraPropertiesExtension ext = p.getExtensions().getByType(ExtraPropertiesExtension.class); ext.set("compilerJavaHome", compilerJavaHome); ext.set("runtimeJavaHome", runtimeJavaHome); ext.set("isRuntimeJavaHomeSet", compilerJavaHome.equals(runtimeJavaHome) == false); ext.set("javaVersions", javaVersions); ext.set("minimumCompilerVersion", minimumCompilerVersion); ext.set("minimumRuntimeVersion", minimumRuntimeVersion); ext.set("gradleJavaVersion", Jvm.current().getJavaVersion()); ext.set("gitRevision", gitRevision(project.getRootProject().getRootDir())); ext.set("buildDate", ZonedDateTime.now(ZoneOffset.UTC)); ext.set("testSeed", testSeed); ext.set("isCi", System.getenv("JENKINS_URL") != null); }); }	i feel like this could be simplified to something like this which seems easier to grok: string testseed = system.getproperty("tests.seed", generateseed()); private static string generateseed() { double seed = new random().nextlong(); return long.tounsignedstring(seed, 16).touppercase(locale.root); }
@Override public synchronized void onCommit(List<? extends IndexCommit> commits) throws IOException { final int keptPosition = indexOfKeptCommits(commits, globalCheckpointSupplier.getAsLong()); lastCommit = commits.get(commits.size() - 1); safeCommit = commits.get(keptPosition); for (int i = 0; i < keptPosition; i++) { if (snapshottedCommits.containsKey(commits.get(i)) == false) { commits.get(i).delete(); } } updateTranslogDeletionPolicy(); }	can we assert that the translog gen is in this commit is lower than all the ones in higher commits?
@Override public TermsEnum getTerms(boolean caseInsensitive, String string, SearchExecutionContext queryShardContext) throws IOException { IndexReader reader = queryShardContext.searcher().getTopReaderContext().reader(); String searchString = FlattenedFieldParser.createKeyedValue(key, string); Terms terms = MultiTerms.getTerms(reader, name()); if (terms == null) { // Field does not exist on this shard. return null; } Automaton a = caseInsensitive ? AutomatonQueries.caseInsensitivePrefix(searchString) : Automata.makeString(searchString); a = Operations.concatenate(a, Automata.makeAnyString()); a = MinimizationOperations.minimize(a, Integer.MAX_VALUE); CompiledAutomaton automaton = new CompiledAutomaton(a); // Wrap result in a class that strips field names from discovered terms return new TranslatingTermsEnum(automaton.getTermsEnum(terms)); }	i don't think that this is correct as this will do case-insensitive search on the field name too. e.g. if the flattened object contains {"foo": "bar", "foo": "quux"} we should only consider the right foo/foo even when case insensitivity is enabled.
static BytesRef extractKey(BytesRef keyedValue) { int length; for (length = 0; length < keyedValue.length; length++){ if (keyedValue.bytes[keyedValue.offset + length] == SEPARATOR_BYTE) { break; } } return new BytesRef(keyedValue.bytes, keyedValue.offset, length); }	can you add a newline between the two functions?
public void testRandomIndexFusion() throws Exception { String fieldName = "foo"; Map<String, Integer> globalTermCounts = new HashMap<>(); int numShards = randomIntBetween(2, 15); ArrayList<Closeable> closeables = new ArrayList<>(); ArrayList<DirectoryReader> readers = new ArrayList<>(); try { for (int s = 0; s < numShards; s++) { Directory directory = new ByteBuffersDirectory(); IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random()))); int numDocs = randomIntBetween(10,200); for (int i = 0; i < numDocs; i++) { Document document = new Document(); String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT); document.add(new StringField(fieldName, term, Field.Store.YES)); writer.addDocument(document); int count = 0; if (globalTermCounts.containsKey(term)) { count = globalTermCounts.get(term); } count++; globalTermCounts.put(term, count); } DirectoryReader reader = DirectoryReader.open(writer); readers.add(reader); writer.close(); closeables.add(reader); closeables.add(directory); } int numSearches = 100; for (int q = 0; q < numSearches; q++) { String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT); Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix); a = Operations.concatenate(a, Automata.makeAnyString()); a = MinimizationOperations.minimize(a, Integer.MAX_VALUE); CompiledAutomaton automaton = new CompiledAutomaton(a); ArrayList<TermsEnum> termsEnums = new ArrayList<>(); for (DirectoryReader reader : readers) { Terms terms = MultiTerms.getTerms(reader, fieldName); TermsEnum te = automaton.getTermsEnum(terms); if(randomBoolean()) { // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results // rather than the raw TermsEnum from Lucene. ArrayList<TermCount> termCounts = new ArrayList<>(); while(te.next()!=null) { termCounts.add(new TermCount(te.term().utf8ToString(), te.docFreq())); } SimpleTermCountEnum simpleEnum = new SimpleTermCountEnum(termCounts.toArray(new TermCount[0])); termsEnums.add(simpleEnum); } else { termsEnums.add(te); } } MultiShardTermsEnum mte = new MultiShardTermsEnum(termsEnums.toArray(new TermsEnum[0])); HashMap<String, Integer> expecteds = new HashMap<>(); for (String term : globalTermCounts.keySet()) { if(term.startsWith(searchPrefix)) { expecteds.put(term, globalTermCounts.get(term)); } } while (mte.next() != null) { String teString = mte.term().utf8ToString(); long actual = mte.docFreq(); assertTrue(expecteds.containsKey(teString)); long expected = expecteds.get(teString); expecteds.remove(teString); assertEquals(mte.term().utf8ToString() + " string count wrong", expected, actual); } assertEquals("Expected results not found", 0, expecteds.size()); } } finally { IOUtils.close(closeables.toArray(new Closeable[0])); } }	nit: can you iterate over entries rathen than keys since you need values too?
@Override public TermsEnum getTerms(boolean caseInsensitive, String string, SearchExecutionContext queryShardContext) throws IOException { boolean matches = caseInsensitive ? value.toLowerCase(Locale.ROOT).startsWith(string.toLowerCase(Locale.ROOT)) : value.startsWith(string); if (matches == false) { return null; } int docCount = queryShardContext.searcher().getIndexReader().numDocs(); return new SimpleTermCountEnum(new TermCount(value, docCount)); }	should we use maxdoc() to be consistent with keyword fields, which don't ignore deletes?
public static List<Object[]> asArray(List<EqlSpec> specs) { AtomicInteger counter = new AtomicInteger(); return specs.stream().map(spec -> { String name = spec.description(); if (Strings.isNullOrEmpty(name)) { name = spec.note(); } if (Strings.isNullOrEmpty(name)) { name = "" + (counter.get() + 1); } return new Object[] { counter.incrementAndGet(), name, spec }; }).collect(toList()); }	changed this one since eclipse cannot run it (it's an invalid method name according to the java conventions).
@Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } EqlSearchRequest that = (EqlSearchRequest) o; return fetchSize == that.fetchSize && Arrays.equals(indices, that.indices) && Objects.equals(indicesOptions, that.indicesOptions) && Objects.equals(filter, that.filter) && Objects.equals(timestampField, that.timestampField) && Objects.equals(tieBreakerField, that.tieBreakerField) && Objects.equals(eventCategoryField, that.eventCategoryField) && Objects.equals(implicitJoinKeyField, that.implicitJoinKeyField) && Objects.equals(searchAfterBuilder, that.searchAfterBuilder) && Objects.equals(query, that.query) && Objects.equals(isCaseSensitive, that.isCaseSensitive); }	this needs to be cleaned-up since we don't use it - future work if anybody wants to pick it.
private Query parseQuery(String type, XContentParser parser) { String[] previousTypes = null; if (type != null) { QueryParseContext.setTypesWithPrevious(new String[]{type}); } QueryParseContext context = cache.get(); try { context.reset(parser); // This means that fields in the query need to exist in the mapping prior to registering this query // The reason that this is required, is that if a field doesn't exist then the query assumes defaults, which may be undesired. // // Even worse when fields mentioned in percolator queries do go added to map after the queries have been registered // then the percolator queries don't work as expected any more. // // Query parsing can't introduce new fields in mappings (which happens when registering a percolator query), // because field type can't be inferred from queries (like document do) so the best option here is to disallow // the usage of unmapped fields in percolator queries to avoid unexpected behaviour // // For backward compatibility, query can contain unmapped fields only if index.percolator.map_unmapped_fields_as_string // is set to true context.setAllowUnmappedFields(false); context.setMapUnmappedFieldAsString(mapUnmappedFieldsAsString ? true : false); return queryParserService.parseInnerQuery(context); } catch (IOException e) { throw new QueryParsingException(queryParserService.index(), "Failed to parse", e); } finally { if (type != null) { QueryParseContext.setTypes(previousTypes); } context.reset(null); } }	i think this bwc comment can be removed? the index.percolator.map_unmapped_fields_as_string setting is there just to facilitate in the case a field is unmapped and then automatically creating a default string field.
private MapperService.SmartNameFieldMappers failIfFieldMappingNotFound(String name, MapperService.SmartNameFieldMappers fieldMapping) { if (allowUnmappedFields) { return fieldMapping; } else if (mapUnmappedFieldAsString){ StringFieldMapper.Builder builder = MapperBuilders.stringField(name); StringFieldMapper stringFieldMapper = builder.build(new Mapper.BuilderContext(ImmutableSettings.EMPTY,new ContentPath(1))); return new MapperService.SmartNameFieldMappers(mapperService(), new FieldMappers(stringFieldMapper), null, false); } else { Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion(); if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) { throw new QueryParsingException(index, "Strict field resolution and no field mapping can be found for the field with name [" + name + "]"); } else { return fieldMapping; } } }	maybe add a space between the two arguments to buildercontext constructor?
@Override protected void assertEqualInstances(CloseIndexResponse expected, CloseIndexResponse actual) { assertNotSame(expected, actual); assertThat(actual.isAcknowledged(), equalTo(expected.isAcknowledged())); assertThat(actual.isShardsAcknowledged(), equalTo(expected.isShardsAcknowledged())); for (int i = 0; i < expected.getIndices().size(); i++) { CloseIndexResponse.IndexResult expectedIndexResult = expected.getIndices().get(i); CloseIndexResponse.IndexResult actualIndexResult = actual.getIndices().get(i); assertNotSame(expectedIndexResult, actualIndexResult); assertThat(actualIndexResult.getIndex(), equalTo(expectedIndexResult.getIndex())); assertThat(actualIndexResult.hasFailures(), equalTo(expectedIndexResult.hasFailures())); if (expectedIndexResult.hasFailures() == false) { assertThat(actualIndexResult.getException(), nullValue()); if (actualIndexResult.getShards() != null) { assertThat(Arrays.stream(actualIndexResult.getShards()) .allMatch(shardResult -> shardResult.hasFailures() == false), is(true)); } } if (expectedIndexResult.getException() != null) { assertThat(actualIndexResult.getShards(), nullValue()); assertThat(actualIndexResult.getException(), notNullValue()); assertThat(actualIndexResult.getException().getMessage(), equalTo(expectedIndexResult.getException().getMessage())); assertThat(actualIndexResult.getException().getClass(), equalTo(expectedIndexResult.getException().getClass())); assertArrayEquals(actualIndexResult.getException().getStackTrace(), expectedIndexResult.getException().getStackTrace()); } else { assertThat(actualIndexResult.getException(), nullValue()); } if (expectedIndexResult.getShards() != null) { assertThat(actualIndexResult.getShards().length, equalTo(expectedIndexResult.getShards().length)); for (int j = 0; j < expectedIndexResult.getShards().length; j++) { CloseIndexResponse.ShardResult expectedShardResult = expectedIndexResult.getShards()[j]; CloseIndexResponse.ShardResult actualShardResult = actualIndexResult.getShards()[j]; assertThat(actualShardResult.getId(), equalTo(expectedShardResult.getId())); assertThat(actualShardResult.hasFailures(), equalTo(expectedShardResult.hasFailures())); if (expectedShardResult.hasFailures()) { assertThat(actualShardResult.getFailures().length, equalTo(expectedShardResult.getFailures().length)); for (int k = 0; k < expectedShardResult.getFailures().length; k++) { CloseIndexResponse.ShardResult.Failure expectedFailure = expectedShardResult.getFailures()[k]; CloseIndexResponse.ShardResult.Failure actualFailure = actualShardResult.getFailures()[k]; assertThat(actualFailure.getNodeId(), equalTo(expectedFailure.getNodeId())); assertThat(actualFailure.index(), equalTo(expectedFailure.index())); assertThat(actualFailure.shardId(), equalTo(expectedFailure.shardId())); // Serialising and deserialising an exception seems to remove the "java.base/" part from the stack trace, // so these string replacements account for this. assertThat(actualFailure.reason().replace("java.base/", ""), equalTo(expectedFailure.reason().replace("java.base/", ""))); assertThat(actualFailure.getCause().getMessage(), equalTo(expectedFailure.getCause().getMessage())); assertThat(actualFailure.getCause().getClass(), equalTo(expectedFailure.getCause().getClass())); assertArrayEquals(actualFailure.getCause().getStackTrace(), expectedFailure.getCause().getStackTrace()); } } else { assertThat(actualShardResult.getFailures(), nullValue()); } } } else { assertThat(actualIndexResult.getShards(), nullValue()); } } }	i made this change (and again in 1 other class) to get the test to pass, but i don't like it. i'd appreciate other suggestions.
public static Integer dateDiff(String dateField, Object dateTime1, Object dateTime2, String tzId) { return (Integer) DateDiffProcessor.process(dateField, asDateTime(dateTime1), asDateTime(dateTime2) , ZoneId.of(tzId)); }	why did you add true here, it shouldn't be needed.
protected static <T extends CreateIndexResponse> void declareFields(ConstructingObjectParser<T, Void> objectParser) { declareAcknowledgedAndShardsAcknowledgedFields(objectParser); objectParser.declareField(constructorArg(), (parser, context) -> parser.textOrNull(), INDEX, ObjectParser.ValueType.STRING); }	xcontent parsing is currently only used for testing.
public void testAnalyzeRequest() throws Exception { AnalyzeRequest indexAnalyzeRequest = new AnalyzeRequest() .text("Here is some text") .index("test_index") .analyzer("test_analyzer"); Request request = RequestConverters.analyze(indexAnalyzeRequest); assertThat(request.getEndpoint(), equalTo("/test_index/_analyze")); assertThat(request.getEntity(), notNullValue()); AnalyzeRequest analyzeRequest = new AnalyzeRequest() .text("more text") .analyzer("test_analyzer"); assertThat(RequestConverters.analyze(analyzeRequest).getEndpoint(), equalTo("/_analyze")); }	i think other folks are using asserttoxcontentbody(validatequeryrequest, request.getentity());. might be worth leaving a comment about why you aren't using it so no one gets confused.
public void testAnalyze() throws IOException, InterruptedException { RestHighLevelClient client = highLevelClient(); { // tag::analyze-builtin-request AnalyzeRequest request = new AnalyzeRequest(); request.text("Some text to analyze", "Some more text to analyze"); // <1> request.analyzer("english"); // <2> // end::analyze-builtin-request } { // tag::analyze-custom-request AnalyzeRequest request = new AnalyzeRequest(); request.text("<b>Some text to analyze</b>"); request.addCharFilter("html_strip"); // <1> request.tokenizer("standard"); // <2> request.addTokenFilter("lowercase"); // <3> Map<String, Object> stopFilter = new HashMap<>(); stopFilter.put("type", "stop"); stopFilter.put("stopwords", new String[]{ "to" }); // <4> request.addTokenFilter(stopFilter); // <5> // end::analyze-custom-request } { // tag::analyze-custom-normalizer-request AnalyzeRequest request = new AnalyzeRequest(); request.text("<b>BaR</b>"); request.addCharFilter("html_strip"); request.addTokenFilter("lowercase"); // end::analyze-custom-normalizer-request // tag::analyze-request-explain request.explain(true); request.attributes("keyword", "type"); // end::analyze-request-explain // tag::analyze-request-sync AnalyzeResponse response = client.indices().analyze(request, RequestOptions.DEFAULT); // end::analyze-request-sync // tag::analyze-response List<AnalyzeResponse.AnalyzeToken> tokens = response.getTokens(); // <1> DetailAnalyzeResponse detail = response.detail(); // <2> // end::analyze-response assertEquals(tokens.size(), 1); assertEquals(tokens.get(0).getTerm(), "bar"); assertNotNull(detail.tokenizer()); } CreateIndexRequest req = new CreateIndexRequest("my_index"); CreateIndexResponse resp = client.indices().create(req, RequestOptions.DEFAULT); assertTrue(resp.isAcknowledged()); PutMappingRequest pmReq = new PutMappingRequest() .indices("my_index") .source("my_field", "type=text,analyzer=english"); PutMappingResponse pmResp = client.indices().putMapping(pmReq, RequestOptions.DEFAULT); assertTrue(pmResp.isAcknowledged()); { // tag::analyze-index-request AnalyzeRequest request = new AnalyzeRequest(); request.index("my_index"); // <1> request.analyzer("my_analyzer"); // <2> request.text("some text to analyze"); // end::analyze-index-request // tag::analyze-execute-listener ActionListener<AnalyzeResponse> listener = new ActionListener<AnalyzeResponse>() { @Override public void onResponse(AnalyzeResponse analyzeTokens) { } @Override public void onFailure(Exception e) { } }; // end::analyze-execute-listener // Use a blocking listener in the test final CountDownLatch latch = new CountDownLatch(1); final ActionListener<AnalyzeResponse> blockingListener = new LatchedActionListener<>(listener, latch); listener = ActionListener.wrap(r -> { assertThat(r.getTokens(), hasSize(4)); }, e-> { blockingListener.onFailure(e); fail("should not fail"); }); // tag::analyze-request-async client.indices().analyzeAsync(request, RequestOptions.DEFAULT, listener); // end::analyze-request-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } { // tag::analyze-index-normalizer-request AnalyzeRequest request = new AnalyzeRequest(); request.index("my_index"); // <1> request.normalizer("my_normalizer"); // <2> request.text("some text to analyze"); // end::analyze-index-normalizer-request } { // tag::analyze-field-request AnalyzeRequest request = new AnalyzeRequest(); request.index("my_index"); request.field("my_field"); request.text("some text to analyze"); // end::analyze-field-request } }	make them line up?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (Strings.isNullOrEmpty(name) == false) { return builder.value(name); } return definition.toXContent(builder, params); }	this looks like it is missing the name.
public static AnalyzeToken readAnalyzeToken(XContentParser parser) throws IOException { ensureExpectedToken(XContentParser.Token.START_OBJECT, parser.currentToken(), parser::getTokenLocation); String field = null; String term = ""; int position = -1; int startOffset = -1; int endOffset = -1; int positionLength = 1; String type = ""; Map<String, Object> attributes = new HashMap<>(); for (XContentParser.Token t = parser.nextToken(); t != XContentParser.Token.END_OBJECT; t = parser.nextToken()) { if (t == XContentParser.Token.FIELD_NAME) { field = parser.currentName(); continue; } if (Fields.TOKEN.equals(field)) { term = parser.text(); } else if (Fields.POSITION.equals(field)) { position = parser.intValue(); } else if (Fields.START_OFFSET.equals(field)) { startOffset = parser.intValue(); } else if (Fields.END_OFFSET.equals(field)) { endOffset = parser.intValue(); } else if (Fields.POSITION_LENGTH.equals(field)) { positionLength = parser.intValue(); } else if (Fields.TYPE.equals(field)) { type = parser.text(); } else { attributes.put(field, parser.text()); } } return new AnalyzeToken(term, position, startOffset, endOffset, positionLength, type, attributes); }	i think constructingobjectparser or objectparser would be better here because they make it easier to be sure that the error handling is good.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(term); out.writeInt(startOffset); out.writeInt(endOffset); out.writeVInt(position); if (out.getVersion().onOrAfter(Version.V_5_2_0)) { out.writeOptionalVInt(positionLength > 1 ? positionLength : null); } out.writeOptionalString(type); out.writeGenericValue(attributes); } } private DetailAnalyzeResponse detail; private List<AnalyzeToken> tokens; AnalyzeResponse() { } public AnalyzeResponse(List<AnalyzeToken> tokens, DetailAnalyzeResponse detail) { this.tokens = tokens; this.detail = detail; } public List<AnalyzeToken> getTokens() { return this.tokens; } public DetailAnalyzeResponse detail() { return this.detail; } @Override public Iterator<AnalyzeToken> iterator() { return tokens.iterator(); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); if (tokens != null) { builder.startArray(Fields.TOKENS); for (AnalyzeToken token : tokens) { token.toXContent(builder, params); } builder.endArray(); } if (detail != null) { builder.startObject(Fields.DETAIL); detail.toXContent(builder, params); builder.endObject(); } builder.endObject(); return builder; } public static AnalyzeResponse fromXContent(XContentParser parser) throws IOException { if (parser.currentToken() == null) { parser.nextToken(); } ensureExpectedToken(XContentParser.Token.START_OBJECT, parser.currentToken(), parser::getTokenLocation); String fieldName = null; List<AnalyzeToken> tokens = null; DetailAnalyzeResponse dar = null; for (XContentParser.Token token = parser.nextToken(); token != XContentParser.Token.END_OBJECT; token = parser.nextToken()) { if (token == XContentParser.Token.FIELD_NAME) { fieldName = parser.currentName(); if (Fields.TOKENS.equals(fieldName)) { tokens = readTokenList(parser); } else if (Fields.DETAIL.equals(fieldName)) { dar = DetailAnalyzeResponse.fromXContent(parser); } else { throw new ParsingException(parser.getTokenLocation(), "Unexpected field name [" + fieldName + "] in AnalyzeResponse"); } } } return new AnalyzeResponse(tokens, dar); } private static List<AnalyzeToken> readTokenList(XContentParser parser) throws IOException { ensureExpectedToken(XContentParser.Token.START_ARRAY, parser.nextToken(), parser::getTokenLocation); List<AnalyzeToken> tokens = new ArrayList<>(); while (parser.nextToken() != XContentParser.Token.END_ARRAY) { tokens.add(AnalyzeToken.readAnalyzeToken(parser)); } return tokens; } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); int size = in.readVInt(); tokens = new ArrayList<>(size); for (int i = 0; i < size; i++) { tokens.add(AnalyzeToken.readAnalyzeToken(in)); } detail = in.readOptionalStreamable(DetailAnalyzeResponse::new); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); if (tokens != null) { out.writeVInt(tokens.size()); for (AnalyzeToken token : tokens) { token.writeTo(out); } } else { out.writeVInt(0); } out.writeOptionalStreamable(detail); } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; AnalyzeResponse that = (AnalyzeResponse) o; return Objects.equals(detail, that.detail) && Objects.equals(tokens, that.tokens); } @Override public int hashCode() { return Objects.hash(detail, tokens); }	i think the same thing is true here too.
@Override public ANode visitCalllocal(CalllocalContext ctx) { String name = ctx.ID() == null ? ctx.DOLLAR().getText() : ctx.ID().getText(); List<AExpression> arguments = collectArguments(ctx.arguments()); return new ECallLocal(nextIdentifier(), location(ctx), name, arguments); }	will name be $ in this case?
@Override public void visitCallLocal(ECallLocal userCallLocalNode, SemanticScope semanticScope) { if ("$".equals(userCallLocalNode.getMethodName())) { ScriptScope scriptScope = semanticScope.getScriptScope(); PainlessMethod thisMethod = scriptScope.getPainlessLookup() .lookupPainlessMethod(scriptScope.getScriptClassInfo().getBaseClass(), false, "field", 1); if (thisMethod == null) { throw new IllegalArgumentException("invalid shortcut [$] for [field]; ensure [field] exists in this context"); } semanticScope.setUsesInstanceMethod(); semanticScope.putDecoration(userCallLocalNode, new ThisPainlessMethod(thisMethod)); scriptScope.markNonDeterministic(thisMethod.annotations.containsKey(NonDeterministicAnnotation.class)); Class<?>[] typeParameters = new Class<?>[] { String.class, def.class }; List<AExpression> userArgumentNodes = userCallLocalNode.getArgumentNodes(); int userArgumentsSize = userArgumentNodes.size(); if (userArgumentsSize != 2) { throw new IllegalArgumentException( "invalid number of arguments for [$] shortcut for [field]; expected <field name> and <default value>" ); } for (int argument = 0; argument < userArgumentsSize; ++argument) { AExpression userArgumentNode = userArgumentNodes.get(argument); semanticScope.setCondition(userArgumentNode, Read.class); semanticScope.putDecoration(userArgumentNode, new TargetType(typeParameters[argument])); semanticScope.setCondition(userArgumentNode, Internal.class); checkedVisit(userArgumentNode, semanticScope); decorateWithCast(userArgumentNode, semanticScope); } semanticScope.putDecoration(userCallLocalNode, new ValueType(def.class)); } else { super.visitCallLocal(userCallLocalNode, semanticScope); } }	why mark this nondeterministic?
@Override public void visitCallLocal(ECallLocal userCallLocalNode, SemanticScope semanticScope) { if ("$".equals(userCallLocalNode.getMethodName())) { ScriptScope scriptScope = semanticScope.getScriptScope(); PainlessMethod thisMethod = scriptScope.getPainlessLookup() .lookupPainlessMethod(scriptScope.getScriptClassInfo().getBaseClass(), false, "field", 1); if (thisMethod == null) { throw new IllegalArgumentException("invalid shortcut [$] for [field]; ensure [field] exists in this context"); } semanticScope.setUsesInstanceMethod(); semanticScope.putDecoration(userCallLocalNode, new ThisPainlessMethod(thisMethod)); scriptScope.markNonDeterministic(thisMethod.annotations.containsKey(NonDeterministicAnnotation.class)); Class<?>[] typeParameters = new Class<?>[] { String.class, def.class }; List<AExpression> userArgumentNodes = userCallLocalNode.getArgumentNodes(); int userArgumentsSize = userArgumentNodes.size(); if (userArgumentsSize != 2) { throw new IllegalArgumentException( "invalid number of arguments for [$] shortcut for [field]; expected <field name> and <default value>" ); } for (int argument = 0; argument < userArgumentsSize; ++argument) { AExpression userArgumentNode = userArgumentNodes.get(argument); semanticScope.setCondition(userArgumentNode, Read.class); semanticScope.putDecoration(userArgumentNode, new TargetType(typeParameters[argument])); semanticScope.setCondition(userArgumentNode, Internal.class); checkedVisit(userArgumentNode, semanticScope); decorateWithCast(userArgumentNode, semanticScope); } semanticScope.putDecoration(userCallLocalNode, new ValueType(def.class)); } else { super.visitCallLocal(userCallLocalNode, semanticScope); } }	can you add a note that we're using def here as the type of the default argument due to duck typing?
@Override public void visitCallLocal(ECallLocal userCallLocalNode, ScriptScope scriptScope) { if ("$".equals(userCallLocalNode.getMethodName())) { PainlessMethod thisMethod = scriptScope.getDecoration(userCallLocalNode, ThisPainlessMethod.class).getThisPainlessMethod(); InvokeCallMemberNode irInvokeCallMemberNode = new InvokeCallMemberNode(userCallLocalNode.getLocation()); irInvokeCallMemberNode.attachDecoration(new IRDThisMethod(thisMethod)); irInvokeCallMemberNode.addArgumentNode(injectCast(userCallLocalNode.getArgumentNodes().get(0), scriptScope)); irInvokeCallMemberNode.attachDecoration(new IRDExpressionType(def.class)); InvokeCallDefNode irCallSubDefNode = new InvokeCallDefNode(userCallLocalNode.getLocation()); irCallSubDefNode.addArgumentNode(injectCast(userCallLocalNode.getArgumentNodes().get(1), scriptScope)); irCallSubDefNode.attachDecoration(new IRDExpressionType(def.class)); irCallSubDefNode.attachDecoration(new IRDName("get")); BinaryImplNode irBinaryImplNode = new BinaryImplNode(userCallLocalNode.getLocation()); irBinaryImplNode.setLeftNode(irInvokeCallMemberNode); irBinaryImplNode.setRightNode(irCallSubDefNode); irBinaryImplNode.attachDecoration(new IRDExpressionType(def.class)); scriptScope.putDecoration(userCallLocalNode, new IRNodeDecoration(irBinaryImplNode)); } else { super.visitCallLocal(userCallLocalNode, scriptScope); } }	is there a way to arrange this so we can have defaultusertreetoirtreephase.visitcalllocal do this?
protected ScriptTemplate scriptWithAggregate(AggregateFunction aggregate) { String template = "{}"; ParamsBuilder paramsBuilder = paramsBuilder().agg(aggregate); DataType nullSafeCastDataType = null; DataType dataType = aggregate.dataType(); if (dataType.name().equals("DATE") || dataType == DATETIME || // Aggregations on date_nanos are returned as string aggregate.field().dataType() == DATETIME) { template = "{sql}.asDateTime({})"; } else if (dataType.isInteger()) { // MAX, MIN need to retain field's data type, so that possible operations on integral types (like division) work // correctly -> perform a cast in the aggs filtering script, the bucket selector for HAVING. // SQL function classes not available in QL: filter by name String fn = aggregate.functionName(); if ("MAX".equals(fn) || "MIN".equals(fn)) { nullSafeCastDataType = dataType; } else if ("SUM".equals(fn)) { // SUM(integral_type) requires returning a LONG value nullSafeCastDataType = LONG; } } if (nullSafeCastDataType != null) { template = "{ql}.nullSafeCastNumeric({},{})"; paramsBuilder.variable(nullSafeCastDataType.name()); } return new ScriptTemplate(processScript(template), paramsBuilder.build(), dataType()); }	i've eventually removed this function since: * the invocation from scriptwithgrouping() never exercise more than basic {} template initialisation (if even that, given the comment) * most of its logic can be moved to scriptwithaggregate(). * its fixme seems to have been addressed?
public void testSearchAfterWithPointInTime() throws Exception { RestHighLevelClient client = highLevelClient(); int numDocs = between(50, 100); BulkRequest request = new BulkRequest(); for (int i = 0; i < numDocs; i++) { request.add(new IndexRequest("posts").id(Integer.toString(i)).source(XContentType.JSON, "field", i)); } request.setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); BulkResponse bulkResponse = client.bulk(request, RequestOptions.DEFAULT); assertSame(RestStatus.OK, bulkResponse.status()); assertFalse(bulkResponse.hasFailures()); OpenPointInTimeRequest openRequest = new OpenPointInTimeRequest("posts"); openRequest.keepAlive(TimeValue.timeValueMinutes(20)); String pitId = client.openPointInTime(openRequest, RequestOptions.DEFAULT).getPointInTimeId(); assertNotNull(pitId); SearchResponse searchResponse = null; int totalHits = 0; do { SearchRequest searchRequest = new SearchRequest().source(new SearchSourceBuilder().sort("field").size(5)); if (searchResponse != null) { final SearchHit[] lastHits = searchResponse.getHits().getHits(); searchRequest.source().searchAfter(lastHits[lastHits.length - 1].getSortValues()); } searchRequest.source().pointInTimeBuilder(new PointInTimeBuilder(pitId)); searchResponse = client.search(searchRequest, RequestOptions.DEFAULT); assertThat(searchResponse.pointInTimeId(), equalTo(pitId)); totalHits += searchResponse.getHits().getHits().length; } while (searchResponse.getHits().getHits().length > 0); assertThat(totalHits, equalTo(numDocs)); ClearScrollResponse closeResponse = client.closePointInTime(new ClosePointInTimeRequest(pitId), RequestOptions.DEFAULT); // <5> assertTrue(closeResponse.isSucceeded()); }	should we remove also "// <5> " comment on the line 819?
@Override public Closeable acquireRetentionLock() { if (softDeleteEnabled) { final Closeable softDeletesRetentionLock = softDeletesPolicy.acquireRetentionLock(); final Closeable translogRetentionLock = translog.acquireRetentionLock(); return () -> { softDeletesRetentionLock.close(); translogRetentionLock.close(); }; } else { return translog.acquireRetentionLock(); } }	i think we should release softdeletesretentionlock if we hit an exception while acquiring translog lock.
@Override public Closeable acquireRetentionLock() { if (softDeleteEnabled) { final Closeable softDeletesRetentionLock = softDeletesPolicy.acquireRetentionLock(); final Closeable translogRetentionLock = translog.acquireRetentionLock(); return () -> { softDeletesRetentionLock.close(); translogRetentionLock.close(); }; } else { return translog.acquireRetentionLock(); } }	maybe use () -> ioutils.close(softdeletesretentionlock, translogretentionlock) so we always close both of them.
*/ public void setHosts(Iterable<HttpHost> hosts, HostMetadataResolver metaResolver) { if (hosts == null) { throw new IllegalArgumentException("hosts must not be null"); } if (metaResolver == null) { throw new IllegalArgumentException("metaResolver must not be null"); } Set<HttpHost> newHosts = new HashSet<>(); AuthCache authCache = new BasicAuthCache(); for (HttpHost host : hosts) { Objects.requireNonNull(host, "host cannot be null"); newHosts.add(host); authCache.put(host, new BasicScheme()); } if (newHosts.isEmpty()) { throw new IllegalArgumentException("hosts must not be empty"); } this.hostTuple = new HostTuple<>(Collections.unmodifiableSet(newHosts), authCache, metaResolver); this.blacklist.clear(); }	all of these javadocs are now on the restclientactions interface.
private Expression collectResolvedAndReplace(Expression e, Map<String, List<Function>> seen) { if (e instanceof Function && e.resolved()) { Function f = (Function) e; String fName = f.functionName(); // the function is resolved and its name normalized already List<Function> list = getList(seen, fName); for (Function seenFunction : list) { if (seenFunction != f && f.arguments().equals(seenFunction.arguments())) { // Special check for COUNT: an already seen COUNT function will be returned only if its DISTINCT property // matches the one from the unresolved function to be checked. if (seenFunction instanceof Count) { if (((Count) seenFunction).distinct() == ((Count) f).distinct()) { return seenFunction; } } else { return seenFunction; } } } list.add(f); } return e; }	why is this check different than here: https://github.com/elastic/elasticsearch/pull/37254/files#diff-bb55908282831d2f432f4a4650d55521r825 ? is it safe to cast (count) f without checking it with instanceof ?
protected LogicalPlan resolve(LogicalPlan plan, Map<String, List<Function>> seen) { return plan.transformExpressionsUp(e -> { if (e instanceof UnresolvedFunction) { UnresolvedFunction uf = (UnresolvedFunction) e; if (uf.analyzed()) { return uf; } String name = uf.name(); if (hasStar(uf.arguments())) { uf = uf.preprocessStar(); if (uf.analyzed()) { return uf; } } if (!uf.childrenResolved()) { return uf; } String functionName = functionRegistry.resolveAlias(name); List<Function> list = getList(seen, functionName); // first try to resolve from seen functions if (!list.isEmpty()) { for (Function seenFunction : list) { if (uf.arguments().equals(seenFunction.arguments())) { // Special check for COUNT: an already seen COUNT function will be returned only if its DISTINCT property // matches the one from the unresolved function to be checked. if (seenFunction instanceof Count) { if (uf.resolutionType() == DISTINCT && ((Count) seenFunction).distinct() || uf.resolutionType() == STANDARD && ((Count) seenFunction).distinct() == false) { return seenFunction; } } else { return seenFunction; } } } } // not seen before, use the registry if (!functionRegistry.functionExists(functionName)) { return uf.missing(functionName, functionRegistry.listFunctions()); } // TODO: look into Generator for significant terms, etc.. FunctionDefinition def = functionRegistry.resolveFunction(functionName); Function f = uf.buildResolved(configuration, def); list.add(f); return f; } return e; }); }	this potentially can be encapsulated into unresolvedfunction without exposing the resolutiontype: uf.sameas(seenfunction) to move the resolution type and the distinct property inside resolutiontype which is already count aware.
@Override public String functionId() { String functionId = id().toString(); // if count works against a given expression, use its id (to identify the group) // in case of COUNT DISTINCT don't use the expression id to avoid possible duplicate IDs when COUNT and COUNT DISTINCT is used // in the same query if (!distinct() && field() instanceof NamedExpression) { functionId = ((NamedExpression) field()).id().toString(); } return functionId; }	just a reminder that this might not be needed after this: https://github.com/elastic/elasticsearch/pull/37161
public static <T extends ToXContent> void testFromXContent(int numberOfTestRuns, Supplier<T> instanceSupplier, boolean supportsUnknownFields, String[] shuffleFieldsExceptions, Predicate<String> randomFieldsExcludeFilter, CheckedBiFunction<XContent, BytesReference, XContentParser, IOException> createParserFunction, CheckedFunction<XContentParser, T, IOException> parseFunction, BiConsumer<T, T> assertEqualsConsumer, boolean assertToXContentEquivalence, ToXContent.Params toXContentParams) throws IOException { for (int runs = 0; runs < numberOfTestRuns; runs++) { T testInstance = instanceSupplier.get(); XContentType xContentType = randomFrom(XContentType.values()); BytesReference xContent = XContentHelper.toXContent(testInstance, xContentType, ToXContent.EMPTY_PARAMS, false); BytesReference withRandomFields; if (supportsUnknownFields) { // we add a few random fields to check that parser is lenient on new fields withRandomFields = XContentTestUtils.insertRandomFields(xContentType, xContent, randomFieldsExcludeFilter, random()); } else { withRandomFields = xContent; } XContentParser parserWithRandonFields = createParserFunction.apply(XContentFactory.xContent(xContentType), withRandomFields); BytesReference shuffledContent = BytesReference.bytes(shuffleXContent(parserWithRandonFields, false, shuffleFieldsExceptions)); XContentParser parser = createParserFunction.apply(XContentFactory.xContent(xContentType), shuffledContent); T parsed = parseFunction.apply(parser); assertEqualsConsumer.accept(testInstance, parsed); if (assertToXContentEquivalence) { assertToXContentEquivalent(xContent, XContentHelper.toXContent(parsed, xContentType, false), xContentType); } } }	the error mentioned is most likely cause by not using the "toxcontentparams" here as before mit using the empty_params. i think this needs to be changed.
public static <T extends ToXContent> void testFromXContent(int numberOfTestRuns, Supplier<T> instanceSupplier, boolean supportsUnknownFields, String[] shuffleFieldsExceptions, Predicate<String> randomFieldsExcludeFilter, CheckedBiFunction<XContent, BytesReference, XContentParser, IOException> createParserFunction, CheckedFunction<XContentParser, T, IOException> parseFunction, BiConsumer<T, T> assertEqualsConsumer, boolean assertToXContentEquivalence, ToXContent.Params toXContentParams) throws IOException { for (int runs = 0; runs < numberOfTestRuns; runs++) { T testInstance = instanceSupplier.get(); XContentType xContentType = randomFrom(XContentType.values()); BytesReference xContent = XContentHelper.toXContent(testInstance, xContentType, ToXContent.EMPTY_PARAMS, false); BytesReference withRandomFields; if (supportsUnknownFields) { // we add a few random fields to check that parser is lenient on new fields withRandomFields = XContentTestUtils.insertRandomFields(xContentType, xContent, randomFieldsExcludeFilter, random()); } else { withRandomFields = xContent; } XContentParser parserWithRandonFields = createParserFunction.apply(XContentFactory.xContent(xContentType), withRandomFields); BytesReference shuffledContent = BytesReference.bytes(shuffleXContent(parserWithRandonFields, false, shuffleFieldsExceptions)); XContentParser parser = createParserFunction.apply(XContentFactory.xContent(xContentType), shuffledContent); T parsed = parseFunction.apply(parser); assertEqualsConsumer.accept(testInstance, parsed); if (assertToXContentEquivalence) { assertToXContentEquivalent(xContent, XContentHelper.toXContent(parsed, xContentType, false), xContentType); } } }	i think the toxcontentparams also need to be re-added here.
@Override protected void doMerge(Mapper mergeWith) { super.doMerge(mergeWith); SearchAsYouTypeFieldMapper mw = (SearchAsYouTypeFieldMapper) mergeWith; if (mw.maxShingleSize != maxShingleSize) { throw new IllegalArgumentException("mapper [" + name() + "] has different [max_shingle_size] setting, current [" + this.maxShingleSize + "], merged [" + mw.maxShingleSize + "]"); } if (prefixField.equals(mw.prefixField) == false) { this.prefixField = (PrefixFieldMapper) this.prefixField.merge(mw.prefixField); } ShingleFieldMapper[] shingleFieldMappers = new ShingleFieldMapper[mw.shingleFields.length]; for (int i = 0; i < shingleFieldMappers.length; i++) { this.shingleFields[i] = (ShingleFieldMapper) this.shingleFields[i].merge(mw.shingleFields[i]); } }	should we use this same logic for the shingle fields below (i.e. only merge if they're unequal)?
@Override public Iterator<Mapper> iterator() { List<Mapper> subIterators = new ArrayList<>(); subIterators.add(prefixField); subIterators.addAll(Arrays.asList(shingleFields)); return Iterators.concat(super.iterator(), subIterators.iterator()); } /** * An analyzer wrapper to add a shingle token filter, an edge ngram token filter or both to its wrapped analyzer. When adding an edge * ngrams token filter, it also adds a {@link TrailingShingleTokenFilter}	it looks like we fail compilation on the warning from the call to iterators.concat here
public void invariant() { synchronized (mutex) { final Optional<DiscoveryNode> peerFinderLeader = peerFinder.getLeader(); assert peerFinder.getCurrentTerm() == getCurrentTerm(); assert followersChecker.getFastResponseState().term == getCurrentTerm() : followersChecker.getFastResponseState(); assert followersChecker.getFastResponseState().mode == getMode() : followersChecker.getFastResponseState(); assert (applierState.nodes().getMasterNodeId() == null) == applierState.blocks().hasGlobalBlock(NO_MASTER_BLOCK_WRITES.id()); assert preVoteCollector.getPreVoteResponse().equals(getPreVoteResponse()) : preVoteCollector + " vs " + getPreVoteResponse(); { final Set<DiscoveryNode> lagDetectorTrackedNodes = new HashSet<>(lagDetector.getTrackedNodes()); assert lagDetectorTrackedNodes.isEmpty() || lagDetectorTrackedNodes.remove(getLocalNode()); assert followersChecker.getKnownFollowers().equals(lagDetectorTrackedNodes); } if (mode == Mode.LEADER) { final boolean becomingMaster = getStateForMasterService().term() != getCurrentTerm(); assert coordinationState.get().electionWon(); assert lastKnownLeader.isPresent() && lastKnownLeader.get().equals(getLocalNode()); assert joinAccumulator instanceof JoinHelper.LeaderJoinAccumulator; assert peerFinderLeader.equals(lastKnownLeader) : peerFinderLeader; assert electionScheduler == null : electionScheduler; assert prevotingRound == null : prevotingRound; assert becomingMaster || getStateForMasterService().nodes().getMasterNodeId() != null : getStateForMasterService(); assert leaderChecker.leader() == null : leaderChecker.leader(); assert applierState.nodes().getMasterNodeId() == null || getLocalNode().equals(applierState.nodes().getMasterNode()); assert preVoteCollector.getLeader() == getLocalNode() : preVoteCollector; final boolean activePublication = currentPublication.map(CoordinatorPublication::isActiveForCurrentLeader).orElse(false); if (becomingMaster && activePublication == false) { // cluster state update task to become master is submitted to MasterService, but publication has not started yet assert followersChecker.getKnownFollowers().isEmpty() : followersChecker.getKnownFollowers(); } else { final ClusterState lastPublishedState; if (activePublication) { // active publication in progress: followersChecker is up-to-date with nodes that we're actively publishing to lastPublishedState = currentPublication.get().publishedState(); } else { // no active publication: followersChecker is up-to-date with the nodes of the latest publication lastPublishedState = coordinationState.get().getLastAcceptedState(); } final Set<DiscoveryNode> lastPublishedNodes = new HashSet<>(); lastPublishedState.nodes().forEach(lastPublishedNodes::add); assert lastPublishedNodes.remove(getLocalNode()); // followersChecker excludes local node assert lastPublishedNodes.equals(followersChecker.getKnownFollowers()) : lastPublishedNodes + " != " + followersChecker.getKnownFollowers(); } assert becomingMaster || activePublication || coordinationState.get().getLastAcceptedConfiguration().equals(coordinationState.get().getLastCommittedConfiguration()) : coordinationState.get().getLastAcceptedConfiguration() + " != " + coordinationState.get().getLastCommittedConfiguration(); } else if (mode == Mode.FOLLOWER) { assert coordinationState.get().electionWon() == false : getLocalNode() + " is FOLLOWER so electionWon() should be false"; assert lastKnownLeader.isPresent() && (lastKnownLeader.get().equals(getLocalNode()) == false); assert joinAccumulator instanceof JoinHelper.FollowerJoinAccumulator; assert peerFinderLeader.equals(lastKnownLeader) : peerFinderLeader; assert electionScheduler == null : electionScheduler; assert prevotingRound == null : prevotingRound; assert getStateForMasterService().nodes().getMasterNodeId() == null : getStateForMasterService(); assert leaderChecker.currentNodeIsMaster() == false; assert lastKnownLeader.equals(Optional.of(leaderChecker.leader())); assert followersChecker.getKnownFollowers().isEmpty(); assert currentPublication.map(Publication::isCommitted).orElse(true); assert preVoteCollector.getLeader().equals(lastKnownLeader.get()) : preVoteCollector; } else { assert mode == Mode.CANDIDATE; assert joinAccumulator instanceof JoinHelper.CandidateJoinAccumulator; assert peerFinderLeader.isPresent() == false : peerFinderLeader; assert prevotingRound == null || electionScheduler != null; assert getStateForMasterService().nodes().getMasterNodeId() == null : getStateForMasterService(); assert leaderChecker.currentNodeIsMaster() == false; assert leaderChecker.leader() == null : leaderChecker.leader(); assert followersChecker.getKnownFollowers().isEmpty(); assert applierState.nodes().getMasterNodeId() == null; assert currentPublication.map(Publication::isCommitted).orElse(true); assert preVoteCollector.getLeader() == null : preVoteCollector; } } }	let's avoid the extra brackets here.
static Request delete(DeleteRequest deleteRequest) { String endpoint = endpoint(deleteRequest.index(), deleteRequest.type(), deleteRequest.id()); Params parameters = Params.builder(); parameters.withRouting(deleteRequest.routing()); parameters.withParent(deleteRequest.parent()); parameters.withTimeout(deleteRequest.timeout()); parameters.withVersion(deleteRequest.version()); parameters.withVersionType(deleteRequest.versionType()); parameters.withRefreshPolicy(deleteRequest.getRefreshPolicy()); parameters.withWaitForActiveShards(deleteRequest.waitForActiveShards()); return new Request(HttpDelete.METHOD_NAME, endpoint, parameters.getParams(), null); }	nit: can you remove this additional empty line? one is enough between two methods.
static Request deleteIndex(DeleteIndexRequest deleteIndexRequest) { String endpoint = endpoint(deleteIndexRequest.indices(), ""); Params parameters = Params.builder(); parameters.withTimeout(deleteIndexRequest.timeout()); parameters.withIndicesOptions(deleteIndexRequest.indicesOptions()); return new Request(HttpDelete.METHOD_NAME, endpoint, parameters.getParams(), null); }	i think that master_timeout is ignored, can you read and set that one too?
static String endpoint(String[] indices, String[] types, String endpoint) { return endpoint(String.join(",", indices), String.join(",", types), endpoint); }	nit: can you remove this additional empty line? one is enough between two methods.
static String endpoint(String[] indices, String[] types, String endpoint) { return endpoint(String.join(",", indices), String.join(",", types), endpoint); }	can you remove the endpoint argument here, as it is not needed. we will add it later if/when we need it for other apis.
static String endpoint(String[] indices, String[] types, String endpoint) { return endpoint(String.join(",", indices), String.join(",", types), endpoint); }	can we add a unit test method to requesttests for the new deleteindex method?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); writeAcknowledged(out); }	can you remove this unnecessary change?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); writeAcknowledged(out); }	we need to add unit tests for this parsing method, similar to what we did in indexresponsetests, getresponsetests etc.
protected void masterOperation(Task task, RevertModelSnapshotAction.Request request, ClusterState state, ActionListener<RevertModelSnapshotAction.Response> listener) { final String jobId = request.getJobId(); if (migrationEligibilityCheck.jobIsEligibleForMigration(jobId, state)) { listener.onFailure(ExceptionsHelper.configHasNotBeenMigrated("revert model snapshot", jobId)); return; } logger.debug("Received request to revert to snapshot id '{}' for job '{}', deleting intervening results: {}", request.getSnapshotId(), jobId, request.getDeleteInterveningResults()); // 5. Revert the state ActionListener<Boolean> annotationsIndexUpdateListener = ActionListener.wrap( r -> { ActionListener<Job> jobListener = ActionListener.wrap( job -> { PersistentTasksCustomMetadata tasks = state.getMetadata().custom(PersistentTasksCustomMetadata.TYPE); JobState jobState = MlTasks.getJobState(job.getId(), tasks); if (request.isForce() == false && jobState.equals(JobState.CLOSED) == false) { listener.onFailure(ExceptionsHelper.conflictStatusException( Messages.getMessage(Messages.REST_JOB_NOT_CLOSED_REVERT))); return; } if (MlTasks.getSnapshotUpgraderTask(jobId, request.getSnapshotId(), tasks) != null) { listener.onFailure(ExceptionsHelper.conflictStatusException( "Cannot revert job [{}] to snapshot [{}] as it is being upgraded", jobId, request.getSnapshotId() )); return; } if (job.getBlockReason() != null && job.getBlockReason() != BlockReason.REVERT) { listener.onFailure(ExceptionsHelper.conflictStatusException( "cannot revert job [{}] to snapshot [{}] while it is blocked with [{}]", jobId, request.getSnapshotId(), job.getBlockReason()) ); return; } jobManager.updateJobBlockReason(jobId, BlockReason.REVERT, ActionListener.wrap( aBoolean -> revertSnapshot(jobId, request, listener), listener::onFailure )); }, listener::onFailure ); jobManager.getJob(jobId, jobListener); }, listener::onFailure ); // 4. Ensure the annotations index mappings are up to date ActionListener<Boolean> configMappingUpdateListener = ActionListener.wrap( r -> AnnotationIndex.createAnnotationsIndexIfNecessaryAndWaitForYellow(client, state, request.masterNodeTimeout(), annotationsIndexUpdateListener), listener::onFailure ); // 3. Ensure the config index mappings are up to date ActionListener<Boolean> jobExistsListener = ActionListener.wrap( r -> ElasticsearchMappings.addDocMappingIfMissing(MlConfigIndex.indexName(), MlConfigIndex::mapping, client, state, request.masterNodeTimeout(), configMappingUpdateListener), listener::onFailure ); // 2. Verify the job exists ActionListener<Boolean> createStateIndexListener = ActionListener.wrap( r -> jobManager.jobExists(jobId, jobExistsListener), listener::onFailure ); // 1. Verify/Create the state index and its alias exists AnomalyDetectorsIndex.createStateIndexAndAliasIfNecessary(client, state, indexNameExpressionResolver, request.masterNodeTimeout(), createStateIndexListener); }	given the way revert works i don't think you can safely revert to a different snapshot while an existing revert request for another snapshot is in progress.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { CreateSnapshotRequest createSnapshotRequest = createSnapshotRequest(request.param("repository"), request.param("snapshot")); request.applyContentParser(p -> createSnapshotRequest.source(p.mapOrdered())); createSnapshotRequest.masterNodeTimeout(request.paramAsTime("master_timeout", createSnapshotRequest.masterNodeTimeout())); createSnapshotRequest.waitForCompletion(request.paramAsBoolean("wait_for_completion", false)); createSnapshotRequest.indicesOptions(IndicesOptions.fromRequest(request, createSnapshotRequest.indicesOptions())); return channel -> client.admin().cluster().createSnapshot(createSnapshotRequest, new RestToXContentListener<>(channel)); }	i am missing why we need to add support for these params as request parameters. aren't they part of the request body for this api? are they still supported in the body, meaning do we parse them back when reading the request body?
public void testResumeFollow() { if (runningAgainstLeaderCluster == false) { final Request request = new Request("POST", "/follower/_ccr/resume_follow"); request.setJsonEntity("{\\\\"leader_cluster_alias\\\\": \\\\"leader_cluster\\\\", \\\\"leader_index\\\\": \\\\"leader\\\\"}"); assertNonCompliantLicense(request); } }	let us drop _alias from this field so that it is leader_cluster.
@Override protected void masterOperation( final PutFollowAction.Request request, final ClusterState state, final ActionListener<PutFollowAction.Response> listener) throws Exception { if (ccrLicenseChecker.isCcrAllowed() == false) { listener.onFailure(LicenseUtils.newComplianceException("ccr")); return; } String clusterAlias = request.getFollowRequest().getLeaderClusterAlias(); if (clusterAlias == null) { createFollowerIndexAndFollowLocalIndex(request, state, listener); } else { // This will when clusterAlias has not been configured: client.getRemoteClusterClient(clusterAlias); String leaderIndex = request.getFollowRequest().getLeaderIndex(); createFollowerIndexAndFollowRemoteIndex(request, clusterAlias, leaderIndex, listener); } }	i think this comment is missing a word?
protected void doExecute(final Task task, final ResumeFollowAction.Request request, final ActionListener<AcknowledgedResponse> listener) { if (ccrLicenseChecker.isCcrAllowed() == false) { listener.onFailure(LicenseUtils.newComplianceException("ccr")); return; } final String clusterAlias = request.getLeaderClusterAlias(); if (clusterAlias == null) { followLocalIndex(request, listener); } else { // This will when clusterAlias has not been configured: client.getRemoteClusterClient(clusterAlias); final String leaderIndex = request.getLeaderIndex(); followRemoteIndex(request, clusterAlias, leaderIndex, listener); } }	i think this comment is missing a word?
public void onNewInfo(ClusterInfo info) { ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages(); if (usages != null) { boolean reroute = false; String explanation = ""; // Garbage collect nodes that have been removed from the cluster // from the map that tracks watermark crossing ObjectLookupContainer<String> nodes = usages.keys(); for (String node : nodeHasPassedWatermark) { if (nodes.contains(node) == false) { nodeHasPassedWatermark.remove(node); } } ClusterState state = clusterStateSupplier.get(); Set<String> indicesToMarkReadOnly = new HashSet<>(); RoutingNodes routingNodes = state.getRoutingNodes(); Map<String, Boolean> indexAutoReleaseEligibility = new HashMap<>(); // Ensure we release indices on nodes that have a usage response from node stats markNodesMissingUsageIneligibleForRelease(routingNodes, usages, indexAutoReleaseEligibility); for (ObjectObjectCursor<String, DiskUsage> entry : usages) { String node = entry.key; DiskUsage usage = entry.value; warnAboutDiskIfNeeded(usage); RoutingNode routingNode = state.getRoutingNodes().node(node); // Only unblock index if all nodes that contain shards of it are below the high disk watermark if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, false); } else { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, true); } if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) { if (routingNode != null) { // this might happen if we haven't got the full cluster-state yet?! for (ShardRouting routing : routingNode) { indicesToMarkReadOnly.add(routing.index().getName()); } } } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "high disk watermark exceeded on one or more nodes"; } else { logger.debug("high disk watermark exceeded on {} but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } nodeHasPassedWatermark.add(node); } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) { nodeHasPassedWatermark.add(node); } else { if (nodeHasPassedWatermark.contains(node)) { // The node has previously been over the high or // low watermark, but is no longer, so we should // reroute so any unassigned shards can be allocated // if they are able to be if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "one or more nodes has gone under the high or low watermark"; nodeHasPassedWatermark.remove(node); } else { logger.debug("{} has gone below a disk threshold, but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } } } } if (reroute) { logger.info("rerouting shards: [{}]", explanation); reroute(); } // Get set of indices that are eligible to be automatically unblocked // Only collect indices that are currently blocked Set<String> indicesToAutoRelease = indexAutoReleaseEligibility.entrySet().stream() .filter(Map.Entry::getValue) .map(Map.Entry::getKey) .filter(index -> state.getBlocks().hasIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_ALLOW_DELETE_BLOCK)) .collect(Collectors.toCollection(HashSet::new)); if (indicesToAutoRelease.isEmpty() == false) { if (diskThresholdSettings.isAutoReleaseIndexEnabled()) { logger.info("Releasing read-only allow delete block on indices: [{}]", indicesToAutoRelease); updateIndicesReadOnly(indicesToAutoRelease, false); } else { deprecationLogger.deprecated("es.disk.auto_release_flood_stage_block will be removed in 8.0.0"); } } indicesToMarkReadOnly.removeIf(index -> state.getBlocks().indexBlocked(ClusterBlockLevel.WRITE, index)); if (indicesToMarkReadOnly.isEmpty() == false) { updateIndicesReadOnly(indicesToMarkReadOnly, true); } } }	this is only used in one place, i'd say you should inline it.
public void onNewInfo(ClusterInfo info) { ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages(); if (usages != null) { boolean reroute = false; String explanation = ""; // Garbage collect nodes that have been removed from the cluster // from the map that tracks watermark crossing ObjectLookupContainer<String> nodes = usages.keys(); for (String node : nodeHasPassedWatermark) { if (nodes.contains(node) == false) { nodeHasPassedWatermark.remove(node); } } ClusterState state = clusterStateSupplier.get(); Set<String> indicesToMarkReadOnly = new HashSet<>(); RoutingNodes routingNodes = state.getRoutingNodes(); Map<String, Boolean> indexAutoReleaseEligibility = new HashMap<>(); // Ensure we release indices on nodes that have a usage response from node stats markNodesMissingUsageIneligibleForRelease(routingNodes, usages, indexAutoReleaseEligibility); for (ObjectObjectCursor<String, DiskUsage> entry : usages) { String node = entry.key; DiskUsage usage = entry.value; warnAboutDiskIfNeeded(usage); RoutingNode routingNode = state.getRoutingNodes().node(node); // Only unblock index if all nodes that contain shards of it are below the high disk watermark if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, false); } else { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, true); } if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) { if (routingNode != null) { // this might happen if we haven't got the full cluster-state yet?! for (ShardRouting routing : routingNode) { indicesToMarkReadOnly.add(routing.index().getName()); } } } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "high disk watermark exceeded on one or more nodes"; } else { logger.debug("high disk watermark exceeded on {} but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } nodeHasPassedWatermark.add(node); } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) { nodeHasPassedWatermark.add(node); } else { if (nodeHasPassedWatermark.contains(node)) { // The node has previously been over the high or // low watermark, but is no longer, so we should // reroute so any unassigned shards can be allocated // if they are able to be if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "one or more nodes has gone under the high or low watermark"; nodeHasPassedWatermark.remove(node); } else { logger.debug("{} has gone below a disk threshold, but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } } } } if (reroute) { logger.info("rerouting shards: [{}]", explanation); reroute(); } // Get set of indices that are eligible to be automatically unblocked // Only collect indices that are currently blocked Set<String> indicesToAutoRelease = indexAutoReleaseEligibility.entrySet().stream() .filter(Map.Entry::getValue) .map(Map.Entry::getKey) .filter(index -> state.getBlocks().hasIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_ALLOW_DELETE_BLOCK)) .collect(Collectors.toCollection(HashSet::new)); if (indicesToAutoRelease.isEmpty() == false) { if (diskThresholdSettings.isAutoReleaseIndexEnabled()) { logger.info("Releasing read-only allow delete block on indices: [{}]", indicesToAutoRelease); updateIndicesReadOnly(indicesToAutoRelease, false); } else { deprecationLogger.deprecated("es.disk.auto_release_flood_stage_block will be removed in 8.0.0"); } } indicesToMarkReadOnly.removeIf(index -> state.getBlocks().indexBlocked(ClusterBlockLevel.WRITE, index)); if (indicesToMarkReadOnly.isEmpty() == false) { updateIndicesReadOnly(indicesToMarkReadOnly, true); } } }	why not just track the set of ineligible indices? i don't see the difference between true and missing in this map.
public void onNewInfo(ClusterInfo info) { ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages(); if (usages != null) { boolean reroute = false; String explanation = ""; // Garbage collect nodes that have been removed from the cluster // from the map that tracks watermark crossing ObjectLookupContainer<String> nodes = usages.keys(); for (String node : nodeHasPassedWatermark) { if (nodes.contains(node) == false) { nodeHasPassedWatermark.remove(node); } } ClusterState state = clusterStateSupplier.get(); Set<String> indicesToMarkReadOnly = new HashSet<>(); RoutingNodes routingNodes = state.getRoutingNodes(); Map<String, Boolean> indexAutoReleaseEligibility = new HashMap<>(); // Ensure we release indices on nodes that have a usage response from node stats markNodesMissingUsageIneligibleForRelease(routingNodes, usages, indexAutoReleaseEligibility); for (ObjectObjectCursor<String, DiskUsage> entry : usages) { String node = entry.key; DiskUsage usage = entry.value; warnAboutDiskIfNeeded(usage); RoutingNode routingNode = state.getRoutingNodes().node(node); // Only unblock index if all nodes that contain shards of it are below the high disk watermark if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, false); } else { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, true); } if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) { if (routingNode != null) { // this might happen if we haven't got the full cluster-state yet?! for (ShardRouting routing : routingNode) { indicesToMarkReadOnly.add(routing.index().getName()); } } } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "high disk watermark exceeded on one or more nodes"; } else { logger.debug("high disk watermark exceeded on {} but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } nodeHasPassedWatermark.add(node); } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) { nodeHasPassedWatermark.add(node); } else { if (nodeHasPassedWatermark.contains(node)) { // The node has previously been over the high or // low watermark, but is no longer, so we should // reroute so any unassigned shards can be allocated // if they are able to be if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "one or more nodes has gone under the high or low watermark"; nodeHasPassedWatermark.remove(node); } else { logger.debug("{} has gone below a disk threshold, but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } } } } if (reroute) { logger.info("rerouting shards: [{}]", explanation); reroute(); } // Get set of indices that are eligible to be automatically unblocked // Only collect indices that are currently blocked Set<String> indicesToAutoRelease = indexAutoReleaseEligibility.entrySet().stream() .filter(Map.Entry::getValue) .map(Map.Entry::getKey) .filter(index -> state.getBlocks().hasIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_ALLOW_DELETE_BLOCK)) .collect(Collectors.toCollection(HashSet::new)); if (indicesToAutoRelease.isEmpty() == false) { if (diskThresholdSettings.isAutoReleaseIndexEnabled()) { logger.info("Releasing read-only allow delete block on indices: [{}]", indicesToAutoRelease); updateIndicesReadOnly(indicesToAutoRelease, false); } else { deprecationLogger.deprecated("es.disk.auto_release_flood_stage_block will be removed in 8.0.0"); } } indicesToMarkReadOnly.removeIf(index -> state.getBlocks().indexBlocked(ClusterBlockLevel.WRITE, index)); if (indicesToMarkReadOnly.isEmpty() == false) { updateIndicesReadOnly(indicesToMarkReadOnly, true); } } }	this also needs to address [this comment](https://github.com/elastic/elasticsearch/issues/39334#issuecomment-467891057): > we think the block should be removed only when the disk usage is at least 5% below the flood_stage watermark and below the high watermark. as it is, if you set the high and flood-stage watermarks to the same value then a node that is just on the edge could be continually adding and removing the block, i.e. it will be flapping.
public void onNewInfo(ClusterInfo info) { ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages(); if (usages != null) { boolean reroute = false; String explanation = ""; // Garbage collect nodes that have been removed from the cluster // from the map that tracks watermark crossing ObjectLookupContainer<String> nodes = usages.keys(); for (String node : nodeHasPassedWatermark) { if (nodes.contains(node) == false) { nodeHasPassedWatermark.remove(node); } } ClusterState state = clusterStateSupplier.get(); Set<String> indicesToMarkReadOnly = new HashSet<>(); RoutingNodes routingNodes = state.getRoutingNodes(); Map<String, Boolean> indexAutoReleaseEligibility = new HashMap<>(); // Ensure we release indices on nodes that have a usage response from node stats markNodesMissingUsageIneligibleForRelease(routingNodes, usages, indexAutoReleaseEligibility); for (ObjectObjectCursor<String, DiskUsage> entry : usages) { String node = entry.key; DiskUsage usage = entry.value; warnAboutDiskIfNeeded(usage); RoutingNode routingNode = state.getRoutingNodes().node(node); // Only unblock index if all nodes that contain shards of it are below the high disk watermark if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, false); } else { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, true); } if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) { if (routingNode != null) { // this might happen if we haven't got the full cluster-state yet?! for (ShardRouting routing : routingNode) { indicesToMarkReadOnly.add(routing.index().getName()); } } } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "high disk watermark exceeded on one or more nodes"; } else { logger.debug("high disk watermark exceeded on {} but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } nodeHasPassedWatermark.add(node); } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) { nodeHasPassedWatermark.add(node); } else { if (nodeHasPassedWatermark.contains(node)) { // The node has previously been over the high or // low watermark, but is no longer, so we should // reroute so any unassigned shards can be allocated // if they are able to be if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "one or more nodes has gone under the high or low watermark"; nodeHasPassedWatermark.remove(node); } else { logger.debug("{} has gone below a disk threshold, but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } } } } if (reroute) { logger.info("rerouting shards: [{}]", explanation); reroute(); } // Get set of indices that are eligible to be automatically unblocked // Only collect indices that are currently blocked Set<String> indicesToAutoRelease = indexAutoReleaseEligibility.entrySet().stream() .filter(Map.Entry::getValue) .map(Map.Entry::getKey) .filter(index -> state.getBlocks().hasIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_ALLOW_DELETE_BLOCK)) .collect(Collectors.toCollection(HashSet::new)); if (indicesToAutoRelease.isEmpty() == false) { if (diskThresholdSettings.isAutoReleaseIndexEnabled()) { logger.info("Releasing read-only allow delete block on indices: [{}]", indicesToAutoRelease); updateIndicesReadOnly(indicesToAutoRelease, false); } else { deprecationLogger.deprecated("es.disk.auto_release_flood_stage_block will be removed in 8.0.0"); } } indicesToMarkReadOnly.removeIf(index -> state.getBlocks().indexBlocked(ClusterBlockLevel.WRITE, index)); if (indicesToMarkReadOnly.isEmpty() == false) { updateIndicesReadOnly(indicesToMarkReadOnly, true); } } }	could you use a string constant here to make sure we log the actual name of the property?
public void onNewInfo(ClusterInfo info) { ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages(); if (usages != null) { boolean reroute = false; String explanation = ""; // Garbage collect nodes that have been removed from the cluster // from the map that tracks watermark crossing ObjectLookupContainer<String> nodes = usages.keys(); for (String node : nodeHasPassedWatermark) { if (nodes.contains(node) == false) { nodeHasPassedWatermark.remove(node); } } ClusterState state = clusterStateSupplier.get(); Set<String> indicesToMarkReadOnly = new HashSet<>(); RoutingNodes routingNodes = state.getRoutingNodes(); Map<String, Boolean> indexAutoReleaseEligibility = new HashMap<>(); // Ensure we release indices on nodes that have a usage response from node stats markNodesMissingUsageIneligibleForRelease(routingNodes, usages, indexAutoReleaseEligibility); for (ObjectObjectCursor<String, DiskUsage> entry : usages) { String node = entry.key; DiskUsage usage = entry.value; warnAboutDiskIfNeeded(usage); RoutingNode routingNode = state.getRoutingNodes().node(node); // Only unblock index if all nodes that contain shards of it are below the high disk watermark if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, false); } else { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, true); } if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) { if (routingNode != null) { // this might happen if we haven't got the full cluster-state yet?! for (ShardRouting routing : routingNode) { indicesToMarkReadOnly.add(routing.index().getName()); } } } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "high disk watermark exceeded on one or more nodes"; } else { logger.debug("high disk watermark exceeded on {} but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } nodeHasPassedWatermark.add(node); } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) { nodeHasPassedWatermark.add(node); } else { if (nodeHasPassedWatermark.contains(node)) { // The node has previously been over the high or // low watermark, but is no longer, so we should // reroute so any unassigned shards can be allocated // if they are able to be if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "one or more nodes has gone under the high or low watermark"; nodeHasPassedWatermark.remove(node); } else { logger.debug("{} has gone below a disk threshold, but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } } } } if (reroute) { logger.info("rerouting shards: [{}]", explanation); reroute(); } // Get set of indices that are eligible to be automatically unblocked // Only collect indices that are currently blocked Set<String> indicesToAutoRelease = indexAutoReleaseEligibility.entrySet().stream() .filter(Map.Entry::getValue) .map(Map.Entry::getKey) .filter(index -> state.getBlocks().hasIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_ALLOW_DELETE_BLOCK)) .collect(Collectors.toCollection(HashSet::new)); if (indicesToAutoRelease.isEmpty() == false) { if (diskThresholdSettings.isAutoReleaseIndexEnabled()) { logger.info("Releasing read-only allow delete block on indices: [{}]", indicesToAutoRelease); updateIndicesReadOnly(indicesToAutoRelease, false); } else { deprecationLogger.deprecated("es.disk.auto_release_flood_stage_block will be removed in 8.0.0"); } } indicesToMarkReadOnly.removeIf(index -> state.getBlocks().indexBlocked(ClusterBlockLevel.WRITE, index)); if (indicesToMarkReadOnly.isEmpty() == false) { updateIndicesReadOnly(indicesToMarkReadOnly, true); } } }	this seems wrong. if they're the same size then there's still no guarantee they contain the same keys.
public void onNewInfo(ClusterInfo info) { ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages(); if (usages != null) { boolean reroute = false; String explanation = ""; // Garbage collect nodes that have been removed from the cluster // from the map that tracks watermark crossing ObjectLookupContainer<String> nodes = usages.keys(); for (String node : nodeHasPassedWatermark) { if (nodes.contains(node) == false) { nodeHasPassedWatermark.remove(node); } } ClusterState state = clusterStateSupplier.get(); Set<String> indicesToMarkReadOnly = new HashSet<>(); RoutingNodes routingNodes = state.getRoutingNodes(); Map<String, Boolean> indexAutoReleaseEligibility = new HashMap<>(); // Ensure we release indices on nodes that have a usage response from node stats markNodesMissingUsageIneligibleForRelease(routingNodes, usages, indexAutoReleaseEligibility); for (ObjectObjectCursor<String, DiskUsage> entry : usages) { String node = entry.key; DiskUsage usage = entry.value; warnAboutDiskIfNeeded(usage); RoutingNode routingNode = state.getRoutingNodes().node(node); // Only unblock index if all nodes that contain shards of it are below the high disk watermark if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, false); } else { markEligiblityForAutoRelease(routingNode, indexAutoReleaseEligibility, true); } if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) { if (routingNode != null) { // this might happen if we haven't got the full cluster-state yet?! for (ShardRouting routing : routingNode) { indicesToMarkReadOnly.add(routing.index().getName()); } } } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "high disk watermark exceeded on one or more nodes"; } else { logger.debug("high disk watermark exceeded on {} but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } nodeHasPassedWatermark.add(node); } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) { nodeHasPassedWatermark.add(node); } else { if (nodeHasPassedWatermark.contains(node)) { // The node has previously been over the high or // low watermark, but is no longer, so we should // reroute so any unassigned shards can be allocated // if they are able to be if ((System.nanoTime() - lastRunNS) > diskThresholdSettings.getRerouteInterval().nanos()) { lastRunNS = System.nanoTime(); reroute = true; explanation = "one or more nodes has gone under the high or low watermark"; nodeHasPassedWatermark.remove(node); } else { logger.debug("{} has gone below a disk threshold, but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } } } } if (reroute) { logger.info("rerouting shards: [{}]", explanation); reroute(); } // Get set of indices that are eligible to be automatically unblocked // Only collect indices that are currently blocked Set<String> indicesToAutoRelease = indexAutoReleaseEligibility.entrySet().stream() .filter(Map.Entry::getValue) .map(Map.Entry::getKey) .filter(index -> state.getBlocks().hasIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_ALLOW_DELETE_BLOCK)) .collect(Collectors.toCollection(HashSet::new)); if (indicesToAutoRelease.isEmpty() == false) { if (diskThresholdSettings.isAutoReleaseIndexEnabled()) { logger.info("Releasing read-only allow delete block on indices: [{}]", indicesToAutoRelease); updateIndicesReadOnly(indicesToAutoRelease, false); } else { deprecationLogger.deprecated("es.disk.auto_release_flood_stage_block will be removed in 8.0.0"); } } indicesToMarkReadOnly.removeIf(index -> state.getBlocks().indexBlocked(ClusterBlockLevel.WRITE, index)); if (indicesToMarkReadOnly.isEmpty() == false) { updateIndicesReadOnly(indicesToMarkReadOnly, true); } } }	why not containskey()? also, please use == false rather than a unary !.
public void accept(ShardSearchRequest request, StreamOutput out) throws IOException { var licenseChecker = new MemoizedSupplier<>(() -> DOCUMENT_LEVEL_SECURITY_FEATURE.checkWithoutTracking(licenseState)); final SecurityContext securityContext = securityContextHolder.get(); final IndicesAccessControl indicesAccessControl = securityContext.getThreadContext().getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY); final String indexName = request.shardId().getIndexName(); IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(indexName); if (indexAccessControl != null) { final boolean flsEnabled = indexAccessControl.getFieldPermissions().hasFieldLevelSecurity(); final boolean dlsEnabled = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions(); if ((flsEnabled || dlsEnabled) && licenseChecker.get()) { logger.debug("index [{}] with field level access controls [{}] " + "document level access controls [{}]. Differentiating request cache key", indexName, flsEnabled, dlsEnabled); indexAccessControl.buildCacheKey( out, SecurityQueryTemplateEvaluator.wrap(securityContext.getUser(), scriptServiceReference.get())); } } }	per my earlier comment, i think we can drop the supplier if we're not tracking.
public void intercept( AuthorizationEngine.RequestInfo requestInfo, AuthorizationEngine authorizationEngine, AuthorizationInfo authorizationInfo, ActionListener<Void> listener) { if (requestInfo.getRequest() instanceof IndicesRequest && false == TransportActionProxy.isProxyAction(requestInfo.getAction())) { final Role role = RBACEngine.maybeGetRBACEngineRole(threadContext.getTransient(AUTHORIZATION_INFO_KEY)); if (role == null || role.hasFieldOrDocumentLevelSecurity()) { logger.trace("Role has DLS or FLS. Checking for whether the request touches any indices that have DLS or FLS configured"); final IndicesAccessControl indicesAccessControl = threadContext.getTransient(INDICES_PERMISSIONS_KEY); if (indicesAccessControl != null) { final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState(); final IndicesAccessControl.DlsFlsUsage dlsFlsUsage = indicesAccessControl.getFieldAndDocumentLevelSecurityUsage(); boolean incompatibleLicense = false; if (dlsFlsUsage == IndicesAccessControl.DlsFlsUsage.FLS || dlsFlsUsage == IndicesAccessControl.DlsFlsUsage.BOTH) { if (false == FIELD_LEVEL_SECURITY_FEATURE.check(frozenLicenseState)) { incompatibleLicense = true; } } if (dlsFlsUsage == IndicesAccessControl.DlsFlsUsage.DLS || dlsFlsUsage == IndicesAccessControl.DlsFlsUsage.BOTH) { if (false == DOCUMENT_LEVEL_SECURITY_FEATURE.check(frozenLicenseState)) { incompatibleLicense = true; } } if (incompatibleLicense) { final ElasticsearchSecurityException licenseException = LicenseUtils.newComplianceException("field and document level security"); licenseException.addMetadata( "es.indices_with_dls_or_fls", indicesAccessControl.getIndicesWithFieldOrDocumentLevelSecurity()); listener.onFailure(licenseException); return; } } } } listener.onResponse(null); }	why does a null role get treated as using dls/fls ? that seems wrong.
public void intercept( AuthorizationEngine.RequestInfo requestInfo, AuthorizationEngine authorizationEngine, AuthorizationInfo authorizationInfo, ActionListener<Void> listener) { if (requestInfo.getRequest() instanceof IndicesRequest && false == TransportActionProxy.isProxyAction(requestInfo.getAction())) { final Role role = RBACEngine.maybeGetRBACEngineRole(threadContext.getTransient(AUTHORIZATION_INFO_KEY)); if (role == null || role.hasFieldOrDocumentLevelSecurity()) { logger.trace("Role has DLS or FLS. Checking for whether the request touches any indices that have DLS or FLS configured"); final IndicesAccessControl indicesAccessControl = threadContext.getTransient(INDICES_PERMISSIONS_KEY); if (indicesAccessControl != null) { final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState(); final IndicesAccessControl.DlsFlsUsage dlsFlsUsage = indicesAccessControl.getFieldAndDocumentLevelSecurityUsage(); boolean incompatibleLicense = false; if (dlsFlsUsage == IndicesAccessControl.DlsFlsUsage.FLS || dlsFlsUsage == IndicesAccessControl.DlsFlsUsage.BOTH) { if (false == FIELD_LEVEL_SECURITY_FEATURE.check(frozenLicenseState)) { incompatibleLicense = true; } } if (dlsFlsUsage == IndicesAccessControl.DlsFlsUsage.DLS || dlsFlsUsage == IndicesAccessControl.DlsFlsUsage.BOTH) { if (false == DOCUMENT_LEVEL_SECURITY_FEATURE.check(frozenLicenseState)) { incompatibleLicense = true; } } if (incompatibleLicense) { final ElasticsearchSecurityException licenseException = LicenseUtils.newComplianceException("field and document level security"); licenseException.addMetadata( "es.indices_with_dls_or_fls", indicesAccessControl.getIndicesWithFieldOrDocumentLevelSecurity()); listener.onFailure(licenseException); return; } } } } listener.onResponse(null); }	instead of using an enum, how about an int flags? 0 == none, 1 == fls, 2 == dls, 3 == both, so you can just check the bit.
public Builder setCompressionEnabled(Compression.Enabled compressionEnabled) { this.compressionEnabled = compressionEnabled; return this; }	wrong comment here, should probably be: suggestion * sets compression scheme for this connection profile
public static byte unSetCompress(byte value) { value |= STATUS_COMPRESS; return value; }	this looks wrong (identical to setcompress). but the method is also unused so i suggest to simply remove it.
public void testSendRequest() throws IOException { ThreadContext threadContext = threadPool.getThreadContext(); Version version = randomFrom(Version.CURRENT, Version.CURRENT.minimumCompatibilityVersion()); String action = "handshake"; long requestId = randomLongBetween(0, 300); boolean isHandshake = randomBoolean(); boolean compress; if (compressionScheme == Compression.Scheme.LZ4 && version.before(Compression.Scheme.LZ4_VERSION)) { compress = false; } else { compress = randomBoolean(); } String value = "message"; threadContext.putHeader("header", "header_value"); TestRequest request = new TestRequest(value); AtomicReference<DiscoveryNode> nodeRef = new AtomicReference<>(); AtomicLong requestIdRef = new AtomicLong(); AtomicReference<String> actionRef = new AtomicReference<>(); AtomicReference<TransportRequest> requestRef = new AtomicReference<>(); handler.setMessageListener(new TransportMessageListener() { @Override public void onRequestSent(DiscoveryNode node, long requestId, String action, TransportRequest request, TransportRequestOptions options) { nodeRef.set(node); requestIdRef.set(requestId); actionRef.set(action); requestRef.set(request); } }); handler.sendRequest(node, channel, requestId, action, request, options, version, compress, isHandshake); BytesReference reference = channel.getMessageCaptor().get(); ActionListener<Void> sendListener = channel.getListenerCaptor().get(); if (randomBoolean()) { sendListener.onResponse(null); } else { sendListener.onFailure(new IOException("failed")); } assertEquals(node, nodeRef.get()); assertEquals(requestId, requestIdRef.get()); assertEquals(action, actionRef.get()); assertEquals(request, requestRef.get()); pipeline.handleBytes(channel, new ReleasableBytesReference(reference, () -> { })); final Tuple<Header, BytesReference> tuple = message.get(); final Header header = tuple.v1(); final TestRequest message = new TestRequest(tuple.v2().streamInput()); assertEquals(version, header.getVersion()); assertEquals(requestId, header.getRequestId()); assertTrue(header.isRequest()); assertFalse(header.isResponse()); if (isHandshake) { assertTrue(header.isHandshake()); } else { assertFalse(header.isHandshake()); } if (compress) { assertTrue(header.isCompressed()); } else { assertFalse(header.isCompressed()); } assertEquals(value, message.value); assertEquals("header_value", header.getHeaders().v1().get("header")); }	this compress boolean is used both in handler.sendrequest and for validation afterwards. i think we should split it into two so that we can verify the fallback to not compress against old versions.
public void testSendResponse() throws IOException { ThreadContext threadContext = threadPool.getThreadContext(); Version version = randomFrom(Version.CURRENT, Version.CURRENT.minimumCompatibilityVersion()); String action = "handshake"; long requestId = randomLongBetween(0, 300); boolean isHandshake = randomBoolean(); boolean compress; if (compressionScheme == Compression.Scheme.LZ4 && version.before(Compression.Scheme.LZ4_VERSION)) { compress = false; } else { compress = randomBoolean(); } String value = "message"; threadContext.putHeader("header", "header_value"); TestResponse response = new TestResponse(value); AtomicLong requestIdRef = new AtomicLong(); AtomicReference<String> actionRef = new AtomicReference<>(); AtomicReference<TransportResponse> responseRef = new AtomicReference<>(); handler.setMessageListener(new TransportMessageListener() { @Override public void onResponseSent(long requestId, String action, TransportResponse response) { requestIdRef.set(requestId); actionRef.set(action); responseRef.set(response); } }); handler.sendResponse(version, channel, requestId, action, response, compress, isHandshake); BytesReference reference = channel.getMessageCaptor().get(); ActionListener<Void> sendListener = channel.getListenerCaptor().get(); if (randomBoolean()) { sendListener.onResponse(null); } else { sendListener.onFailure(new IOException("failed")); } assertEquals(requestId, requestIdRef.get()); assertEquals(action, actionRef.get()); assertEquals(response, responseRef.get()); pipeline.handleBytes(channel, new ReleasableBytesReference(reference, () -> { })); final Tuple<Header, BytesReference> tuple = message.get(); final Header header = tuple.v1(); final TestResponse message = new TestResponse(tuple.v2().streamInput()); assertEquals(version, header.getVersion()); assertEquals(requestId, header.getRequestId()); assertFalse(header.isRequest()); assertTrue(header.isResponse()); if (isHandshake) { assertTrue(header.isHandshake()); } else { assertFalse(header.isHandshake()); } if (compress) { assertTrue(header.isCompressed()); } else { assertFalse(header.isCompressed()); } assertFalse(header.isError()); assertEquals(value, message.value); assertEquals("header_value", header.getHeaders().v1().get("header")); }	same comment as above.
public void testHelloWorldCompressed() throws Exception { try (MockTransportService serviceC = buildService("TS_C", CURRENT_VERSION, Settings.EMPTY)) { serviceA.registerRequestHandler("internal:sayHello", ThreadPool.Names.GENERIC, StringMessageRequest::new, (request, channel, task) -> { assertThat("moshe", equalTo(request.message)); try { channel.sendResponse(new StringMessageResponse("hello " + request.message)); } catch (IOException e) { logger.error("Unexpected failure", e); fail(e.getMessage()); } }); Settings settingsWithCompress = Settings.builder() .put(TransportSettings.TRANSPORT_COMPRESS.getKey(), Compression.Enabled.TRUE) .build(); ConnectionProfile connectionProfile = ConnectionProfile.buildDefaultConnectionProfile(settingsWithCompress); connectToNode(serviceC, serviceA.getLocalDiscoNode(), connectionProfile); Future<StringMessageResponse> res = submitRequest(serviceC, nodeA, "internal:sayHello", new StringMessageRequest("moshe"), new TransportResponseHandler<>() { @Override public StringMessageResponse read(StreamInput in) throws IOException { return new StringMessageResponse(in); } @Override public String executor() { return ThreadPool.Names.GENERIC; } @Override public void handleResponse(StringMessageResponse response) { assertThat("hello moshe", equalTo(response.message)); } @Override public void handleException(TransportException exp) { logger.error("Unexpected failure", exp); fail("got exception instead of a response: " + exp.getMessage()); } }); StringMessageResponse message = res.get(); assertThat("hello moshe", equalTo(message.message)); } }	also add random compresion scheme here?
@Override public void prepareForTranslogOperations(boolean deleteLocalTranslog, int totalTranslogOps) throws IOException { state().getTranslog().totalOperations(totalTranslogOps); if (deleteLocalTranslog) { indexShard().openIndexAndCreateTranslog(false, SequenceNumbers.UNASSIGNED_SEQ_NO); } else { indexShard().openIndexAndSkipTranslogRecovery(); } }	the todo is still relevant no?
private void internalClose(boolean waitForPendingConnections) { assert Transports.assertNotTransportThread("Closing ConnectionManager"); if (closing.compareAndSet(false, true)) { connectingRefCounter.decRef(); if (waitForPendingConnections) { try { closeLatch.await(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new IllegalStateException(e); } } } }	not sure if this is actually safe. if the problem is that we're not safely handling all the pending connections here because of some race with the thread-pool then we're leaking fds and now have no visibility on that? can't we just do: diff diff --git a/server/src/main/java/org/elasticsearch/transport/connectionmanager.java b/server/src/main/java/org/elasticsearch/transport/connectionmanager.java index ed26d0b07cd..d87aca31a13 100644 --- a/server/src/main/java/org/elasticsearch/transport/connectionmanager.java +++ b/server/src/main/java/org/elasticsearch/transport/connectionmanager.java @@ -41,6 +41,7 @@ import java.util.set; import java.util.concurrent.concurrentmap; import java.util.concurrent.copyonwritearraylist; import java.util.concurrent.countdownlatch; +import java.util.concurrent.timeunit; import java.util.concurrent.atomic.atomicboolean; /** @@ -255,8 +256,17 @@ public class connectionmanager implements closeable { try { closelatch.await(); } catch (interruptedexception e) { - thread.currentthread().interrupt(); - throw new illegalstateexception(e); + logger.warn("interrupted while waiting for in-progress connections to finish."); + try { + if (closelatch.await(30l, timeunit.seconds) == false) { + thread.currentthread().interrupt(); + throw new illegalstateexception(e); + } + } catch (interruptedexception ie) { + thread.currentthread().interrupt(); + e.addsuppressed(ie); + throw new illegalstateexception(e); + } } iterator<map.entry<discoverynode, transport.connection>> iterator = connectednodes.entryset().iterator(); while (iterator.hasnext()) { something like this? if we in fact are safely shutting down those connections eventually even if the threadpool is shut down too early this should still finish properly. if not, we get proper visibility and know we have to fix the shutdown sequence to not leak connection attempts here?
SocketChannel openNioChannel(InetSocketAddress remoteAddress) throws IOException { SocketChannel socketChannel = SocketChannel.open(); configureSocketChannel(socketChannel); try { PrivilegedSocketAccess.connect(socketChannel, remoteAddress); } catch (IOException e) { closeRawChannel(socketChannel, e); throw e; } return socketChannel; }	should we move the configuresocketchannel call into the try block as well here? seems cleaner
ServerSocketChannel openNioServerSocketChannel(InetSocketAddress address) throws IOException { ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); ServerSocket socket = serverSocketChannel.socket(); socket.setReuseAddress(tcpReusedAddress); try { serverSocketChannel.bind(address); } catch (IOException e) { closeRawChannel(serverSocketChannel, e); throw e; } return serverSocketChannel; }	same commetn as above move the socket.setreuseaddress(tcpreusedaddress); into the try block?
public final T read(Path file) throws IOException { try (Directory dir = newReadOnlyDirectory(file.getParent())) { try (final IndexInput indexInput = dir.openInput(file.getFileName().toString(), IOContext.DEFAULT)) { // We checksum the entire file before we even go and parse it. If it's corrupted we barf right here. CodecUtil.checksumEntireFile(indexInput); final int fileVersion = CodecUtil.checkHeader(indexInput, STATE_FILE_CODEC, MIN_COMPATIBLE_STATE_FILE_VERSION, STATE_FILE_VERSION); final XContentType xContentType = XContentType.values()[indexInput.readInt()]; if (fileVersion == STATE_FILE_VERSION_ES_2X_AND_BELOW) { // format version 0, wrote a version that always came from the content state file and was never used indexInput.readLong(); // version currently unused } long filePointer = indexInput.getFilePointer(); long contentSize = indexInput.length() - CodecUtil.footerLength() - filePointer; try (IndexInput slice = indexInput.slice("state_xcontent", filePointer, contentSize)) { try (XContentParser parser = XContentFactory.xContent(xContentType).createParser(new InputStreamIndexInput(slice, contentSize))) { return fromXContent(parser); } } } catch(CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) { // we trick this into a dedicated exception with the original stacktrace throw new CorruptStateException(ex); } } }	can't we just check if the directory exists before we open that directory and if it doesn't exist we don't open it? would that be the simplest fix?
public void testNormalizerWithIndex() throws IOException { AnalyzeAction.Request request = new AnalyzeAction.Request("index"); request.normalizer("my_normalizer"); request.text("Wi-fi"); AnalyzeAction.Response analyze = TransportAnalyzeAction.analyze(request, registry, mockIndexService(), maxTokenCount); List<AnalyzeAction.AnalyzeToken> tokens = analyze.getTokens(); assertEquals(1, tokens.size()); assertEquals("wi-fi", tokens.get(0).getTerm()); }	this is great and absolutely correct. it took me a while reading the code though to understand though why this fails without the additions you made in transportanalyzeaction. can you add something along the lines of the following comment to explain that we expect this normalizer to use the "keyword" tokenizer and "lowercase" filter. the standard analyzer thats picked by error now also seems to lowercase which is why the test passed with the current test text. lets make the assumptions around the tokenizer also very clear here. suggestion // this should be tokenized with the "keyword" tokenizer and then lowercased request.text("wi-fi");
@Override public double next(Collection<Double> values) { double avg = 0; for (Double v : values) { avg += v; } return avg / values.size(); }	can we call this sum as its not actually the avg?
@Override protected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException { SqlClearCursorRequest sqlRequest; try (XContentParser parser = request.contentParser()) { sqlRequest = SqlClearCursorRequest.fromXContent(parser); } return channel -> client.executeLocally( SqlClearCursorAction.INSTANCE, sqlRequest, new SqlClearCursorResponseListener(channel, request, sqlRequest) ); }	please use an anonymous class for sending the response: return channel -> client.executelocally(sqlclearcursoraction.instance, sqlrequest, new resttoxcontentlistener<>(channel) { @override public restresponse buildresponse(sqlclearcursorresponse response) throws exception { // no need for filtering boolean binaryrequest = response.binaryrequest; xcontenttype type = boolean.true.equals(binaryrequest) || (binaryrequest == null && mode.isdriver(sqlrequest.mode())) ? cbor : json; xcontentbuilder builder = channel.newbuilder(request.getxcontenttype(), responsetype, false); response.toxcontent(builder, request); return new bytesrestresponse(reststatus.ok, builder); } }
@Override protected void doXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(IdsQueryParser.NAME); if (types != null) { if (types.length == 1) { builder.field("type", types[0]); } else { builder.array("types", types); } } builder.startArray("values"); for (String value : ids) { builder.value(value); } builder.endArray(); if (boost != 1.0f) { builder.field("boost", boost); } if (queryName != null) { builder.field("_name", queryName); } builder.endObject(); }	can you double check @cbuescher that this was a bug introduced while migrating this query?
@Override public IdsQueryBuilder readFrom(StreamInput in) throws IOException { IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readStringArray()); idsQueryBuilder.addIds(in.readStringArray()); idsQueryBuilder.queryName = in.readOptionalString(); idsQueryBuilder.boost = in.readFloat(); return idsQueryBuilder; }	i think at this point i will also remove the readstringlist and writestringlist that we previously added to master if you don't mind @cbuescher :)
public void onResponse(ClusterStateResponse clusterStateResponse) { logger.trace("remote cluster state is [{}] [{}]", clusterStateResponse.getClusterName(), clusterStateResponse.getState()); if (canStart(clusterStateResponse.getState())) { updateCurrentIndexMappingsIfNecessary(clusterStateResponse.getState()); } else if (TemplateUtils.checkTemplateExistsAndVersionMatches(INDEX_TEMPLATE_NAME, SECURITY_VERSION_STRING, clusterStateResponse.getState(), logger, Version.CURRENT::onOrBefore) == false) { putTemplate(customAuditIndexSettings(settings, logger), e -> { logger.error("failed to put audit trail template", e); transitionStartingToInitialized(); }); } else { // for some reason we can't start up since the remote cluster is not fully setup. in this case // we try to wait for yellow status (all primaries started up) this will also wait for // state recovery etc. String indexName = getIndexName(); // if this index doesn't exists the call will fail with a not_found exception... client.admin().cluster().prepareHealth().setIndices(indexName).setWaitForYellowStatus().execute( ActionListener.wrap( (x) -> { logger.debug("have yellow status on remote index [{}] ", indexName); transitionStartingToInitialized(); start(); }, (e) -> { logger.error("failed to get wait for yellow status on remote index [" + indexName + "]", e); transitionStartingToInitialized(); })); } }	follow-up of #35988 , but does not affect the failures #33867 #37062 .
void updateCurrentIndexMappingsIfNecessary(ClusterState state) { final String nextIndex = getNextIndexName(); final String index = getIndexName(); AliasOrIndex aliasOrIndex = state.getMetaData().getAliasAndIndexLookup().get(index); if (aliasOrIndex != null) { // check mappings final List<IndexMetaData> indices = aliasOrIndex.getIndices(); if (aliasOrIndex.isAlias() && indices.size() > 1) { throw new IllegalStateException("Alias [" + index + "] points to more than one index: " + indices.stream().map(imd -> imd.getIndex().getName()).collect(Collectors.toList())); } IndexMetaData indexMetaData = indices.get(0); MappingMetaData docMapping = indexMetaData.mapping("doc"); if (docMapping == null) { if (indexToRemoteCluster || state.nodes().isLocalNodeElectedMaster() || hasStaleMessage()) { putAuditIndexMappingsAndStart(index, nextIndex); } else { logger.debug("audit index [{}] is missing mapping for type [{}]", index, DOC_TYPE); transitionStartingToInitialized(); } } else { @SuppressWarnings("unchecked") Map<String, Object> meta = (Map<String, Object>) docMapping.sourceAsMap().get("_meta"); if (meta == null) { logger.warn("Missing _meta field in mapping [{}] of index [{}]", docMapping.type(), index); if (indexToRemoteCluster || state.nodes().isLocalNodeElectedMaster() || hasStaleMessage()) { putAuditIndexMappingsAndStart(index, nextIndex); } else { logger.debug("audit index [{}] is missing _meta for type [{}]", index, DOC_TYPE); transitionStartingToInitialized(); } } else { final String versionString = (String) meta.get(SECURITY_VERSION_STRING); if (versionString != null && Version.fromString(versionString).onOrAfter(Version.CURRENT)) { innerStart(); } else { if (indexToRemoteCluster || state.nodes().isLocalNodeElectedMaster() || hasStaleMessage()) { putAuditIndexMappingsAndStart(index, nextIndex); } else if (versionString == null) { logger.debug("audit index [{}] mapping is missing meta field [{}]", index, SECURITY_VERSION_STRING); transitionStartingToInitialized(); } else { logger.debug("audit index [{}] has the incorrect version [{}]", index, versionString); transitionStartingToInitialized(); } } } } } else { innerStart(); } }	fixes https://github.com/elastic/elasticsearch/issues/37062#issuecomment-459293100 . a non-master node detects an un-updated audit index and bails. instead it should hold off, and retry. the index is un-updated because the master had updated the mapping for the index before it the rollover timeline ("the race" - the template upgrade happend after the rollover edge, but audit events on the master came before that).
void updateCurrentIndexMappingsIfNecessary(ClusterState state) { final String nextIndex = getNextIndexName(); final String index = getIndexName(); AliasOrIndex aliasOrIndex = state.getMetaData().getAliasAndIndexLookup().get(index); if (aliasOrIndex != null) { // check mappings final List<IndexMetaData> indices = aliasOrIndex.getIndices(); if (aliasOrIndex.isAlias() && indices.size() > 1) { throw new IllegalStateException("Alias [" + index + "] points to more than one index: " + indices.stream().map(imd -> imd.getIndex().getName()).collect(Collectors.toList())); } IndexMetaData indexMetaData = indices.get(0); MappingMetaData docMapping = indexMetaData.mapping("doc"); if (docMapping == null) { if (indexToRemoteCluster || state.nodes().isLocalNodeElectedMaster() || hasStaleMessage()) { putAuditIndexMappingsAndStart(index, nextIndex); } else { logger.debug("audit index [{}] is missing mapping for type [{}]", index, DOC_TYPE); transitionStartingToInitialized(); } } else { @SuppressWarnings("unchecked") Map<String, Object> meta = (Map<String, Object>) docMapping.sourceAsMap().get("_meta"); if (meta == null) { logger.warn("Missing _meta field in mapping [{}] of index [{}]", docMapping.type(), index); if (indexToRemoteCluster || state.nodes().isLocalNodeElectedMaster() || hasStaleMessage()) { putAuditIndexMappingsAndStart(index, nextIndex); } else { logger.debug("audit index [{}] is missing _meta for type [{}]", index, DOC_TYPE); transitionStartingToInitialized(); } } else { final String versionString = (String) meta.get(SECURITY_VERSION_STRING); if (versionString != null && Version.fromString(versionString).onOrAfter(Version.CURRENT)) { innerStart(); } else { if (indexToRemoteCluster || state.nodes().isLocalNodeElectedMaster() || hasStaleMessage()) { putAuditIndexMappingsAndStart(index, nextIndex); } else if (versionString == null) { logger.debug("audit index [{}] mapping is missing meta field [{}]", index, SECURITY_VERSION_STRING); transitionStartingToInitialized(); } else { logger.debug("audit index [{}] has the incorrect version [{}]", index, versionString); transitionStartingToInitialized(); } } } } } else { innerStart(); } }	master tries to update the mapping for the next rollover index, just in case....
public void testAuditLogs() throws Exception { assertBusy(() -> { assertAuditDocsExist(); assertNumUniqueNodeNameBuckets(expectedNumUniqueNodeNameBuckets()); }, 30, TimeUnit.SECONDS); }	allows some slack for creating and allocating a new audit index by the old nodes while the master is down for upgrade.
private Annotation createModelSnapshotAnnotation(ModelSnapshot modelSnapshot) { assert modelSnapshot != null; Date currentTime = new Date(clock.millis()); return new Annotation( Messages.JOB_AUDIT_SNAPSHOT_STORED, currentTime, XPackUser.NAME, modelSnapshot.getTimestamp(), modelSnapshot.getLatestRecordTimeStamp(), jobId, currentTime, XPackUser.NAME, "annotation"); }	can we also include the model snapshot id, so that if somebody wants to revert to the snapshot they can get the piece of information they need in order to do this from the annotation?
private Annotation createModelSnapshotAnnotation(ModelSnapshot modelSnapshot) { assert modelSnapshot != null; Date currentTime = new Date(clock.millis()); return new Annotation( Messages.JOB_AUDIT_SNAPSHOT_STORED, currentTime, XPackUser.NAME, modelSnapshot.getTimestamp(), modelSnapshot.getLatestRecordTimeStamp(), jobId, currentTime, XPackUser.NAME, "annotation"); }	i think it could be confusing if the annotation spans a time range. is it possible to set both the start and end time to be the model snapshot's timestamp? or does it break the ui if the start and end timestamps are identical?
@Override protected void processInput(DesiredBalanceInput desiredBalanceInput) { if (desiredBalanceService.updateDesiredBalanceAndReroute(desiredBalanceInput, this::isFresh)) { pendingReroute = true; boolean isFreshInput = isFresh(desiredBalanceInput); var listener = new ActionListener<ClusterState>() { @Override public void onResponse(ClusterState clusterState) { if (isFreshInput) { pendingReroute = false; triggerProcessedListeners(desiredBalanceInput.index()); } } @Override public void onFailure(Exception e) { //TODO check exception type? triggerAllListenersFailure(e); } }; rerouteServiceSupplier.get().reroute("desired balance changed", Priority.NORMAL, listener); } }	we should clear this flag whether we're triggering listeners or not i think?
@Override protected void processInput(DesiredBalanceInput desiredBalanceInput) { if (desiredBalanceService.updateDesiredBalanceAndReroute(desiredBalanceInput, this::isFresh)) { pendingReroute = true; boolean isFreshInput = isFresh(desiredBalanceInput); var listener = new ActionListener<ClusterState>() { @Override public void onResponse(ClusterState clusterState) { if (isFreshInput) { pendingReroute = false; triggerProcessedListeners(desiredBalanceInput.index()); } } @Override public void onFailure(Exception e) { //TODO check exception type? triggerAllListenersFailure(e); } }; rerouteServiceSupplier.get().reroute("desired balance changed", Priority.NORMAL, listener); } }	i would rather complete the listener outside of the synchronized block. i think it's ok to acquire and release the lock on each iteration instead.
@Override protected void processInput(DesiredBalanceInput desiredBalanceInput) { if (desiredBalanceService.updateDesiredBalanceAndReroute(desiredBalanceInput, this::isFresh)) { pendingReroute = true; boolean isFreshInput = isFresh(desiredBalanceInput); var listener = new ActionListener<ClusterState>() { @Override public void onResponse(ClusterState clusterState) { if (isFreshInput) { pendingReroute = false; triggerProcessedListeners(desiredBalanceInput.index()); } } @Override public void onFailure(Exception e) { //TODO check exception type? triggerAllListenersFailure(e); } }; rerouteServiceSupplier.get().reroute("desired balance changed", Priority.NORMAL, listener); } }	why complete them in reverse order here? i think we can use a regular queue<> rather than a deque<> if we don't need this.
@Override protected void processInput(DesiredBalanceInput desiredBalanceInput) { if (desiredBalanceService.updateDesiredBalanceAndReroute(desiredBalanceInput, this::isFresh)) { pendingReroute = true; boolean isFreshInput = isFresh(desiredBalanceInput); var listener = new ActionListener<ClusterState>() { @Override public void onResponse(ClusterState clusterState) { if (isFreshInput) { pendingReroute = false; triggerProcessedListeners(desiredBalanceInput.index()); } } @Override public void onFailure(Exception e) { //TODO check exception type? triggerAllListenersFailure(e); } }; rerouteServiceSupplier.get().reroute("desired balance changed", Priority.NORMAL, listener); } }	similarly here - i would rather complete the listener outside of the synchronized block.
@Override public void allocate(RoutingAllocation allocation, ActionListener<Void> listener) { assert MasterService.isMasterUpdateThread() || Thread.currentThread().getName().startsWith("TEST-") : Thread.currentThread().getName(); // assert allocation.debugDecision() == false; set to true when called via the reroute API assert allocation.ignoreDisable() == false; // TODO must also capture any shards that the existing-shards allocators have allocated this pass, not just the ignored ones var index = indexGenerator.incrementAndGet(); synchronized (pendingListeners) { pendingListeners.addLast(new DesiredBalancesListener(index, listener)); } desiredBalanceComputation.onNewInput( new DesiredBalanceInput(index, allocation.immutableClone(), new ArrayList<>(allocation.routingNodes().unassigned().ignored())) ); // TODO possibly add a bounded wait for the computation to complete? // Otherwise we will have to do a second cluster state update straight away. new DesiredBalanceReconciler(getCurrentDesiredBalance(), allocation).run(); }	do we need to preserve the thread context of this listener? that's generally a problem with collecting listeners and then completing them all later. but i guess we're in system context here and we also complete them all in system context so maybe not. we should be able to assert that we're in system context tho (with some test adjustments because the tests won't be running this in system context i think)
public void testBwcSerialization() throws Exception { { final CloseIndexRequest request = randomRequest(); try (BytesStreamOutput out = new BytesStreamOutput()) { out.setVersion(VersionUtils.randomCompatibleVersion(random(), Version.CURRENT)); request.writeTo(out); try (StreamInput in = out.bytes().streamInput()) { assertEquals(request.getParentTask(), TaskId.readFromStream(in)); assertEquals(request.masterNodeTimeout(), in.readTimeValue()); assertEquals(request.timeout(), in.readTimeValue()); assertArrayEquals(request.indices(), in.readStringArray()); assertEquals(request.indicesOptions(), IndicesOptions.readIndicesOptions(in)); } } } { final CloseIndexRequest sample = randomRequest(); try (BytesStreamOutput out = new BytesStreamOutput()) { sample.getParentTask().writeTo(out); out.writeTimeValue(sample.masterNodeTimeout()); out.writeTimeValue(sample.timeout()); out.writeStringArray(sample.indices()); sample.indicesOptions().writeIndicesOptions(out); final CloseIndexRequest deserializedRequest = new CloseIndexRequest(); try (StreamInput in = out.bytes().streamInput()) { in.setVersion(VersionUtils.randomVersionBetween(random(), VersionUtils.getFirstVersion(), Version.V_7_2_0)); deserializedRequest.readFrom(in); } assertEquals(sample.getParentTask(), deserializedRequest.getParentTask()); assertEquals(sample.masterNodeTimeout(), deserializedRequest.masterNodeTimeout()); assertEquals(sample.timeout(), deserializedRequest.timeout()); assertArrayEquals(sample.indices(), deserializedRequest.indices()); assertEquals(sample.indicesOptions(), deserializedRequest.indicesOptions()); assertEquals(ActiveShardCount.NONE, deserializedRequest.waitForActiveShards()); } } }	should this be random version on or before 7.2? seems by doing compatible version we have increased the possible versions it could be
public void testPreventJoinClusterWithUnsupportedNodeVersions() { DiscoveryNodes.Builder builder = DiscoveryNodes.builder(); final Version version = randomVersion(random()); builder.add(new DiscoveryNode(UUIDs.base64UUID(), buildNewFakeTransportAddress(), version)); builder.add(new DiscoveryNode(UUIDs.base64UUID(), buildNewFakeTransportAddress(), randomCompatibleVersion(random(), version))); DiscoveryNodes nodes = builder.build(); final Version maxNodeVersion = nodes.getMaxNodeVersion(); final Version minNodeVersion = nodes.getMinNodeVersion(); if (maxNodeVersion.onOrAfter(Version.V_7_0_0)) { final Version tooLow = getPreviousVersion(maxNodeVersion.minimumCompatibilityVersion()); expectThrows(IllegalStateException.class, () -> { if (randomBoolean()) { JoinTaskExecutor.ensureNodesCompatibility(tooLow, nodes); } else { JoinTaskExecutor.ensureNodesCompatibility(tooLow, minNodeVersion, maxNodeVersion); } }); } if (minNodeVersion.onOrAfter(Version.V_8_0_0)) { Version oldMajor = Version.V_7_2_0.minimumCompatibilityVersion(); expectThrows(IllegalStateException.class, () -> JoinTaskExecutor.ensureMajorVersionBarrier(oldMajor, minNodeVersion)); } final Version minGoodVersion = maxNodeVersion.major == minNodeVersion.major ? // we have to stick with the same major minNodeVersion : maxNodeVersion.minimumCompatibilityVersion(); final Version justGood = randomVersionBetween(random(), minGoodVersion, maxCompatibleVersion(minNodeVersion)); if (randomBoolean()) { JoinTaskExecutor.ensureNodesCompatibility(justGood, nodes); } else { JoinTaskExecutor.ensureNodesCompatibility(justGood, minNodeVersion, maxNodeVersion); } }	why is this changed to a hardcoded 7.2 compat version?
public static boolean isKeyDesc(BucketOrder order) { return isOrder(order, KEY_DESC); } /** * Return the primary {@link BucketOrder} if the provided <code>order</code> * is a {@link CompoundOrder}	toplevel? maybe even make this a method one bucketorder instead of this instanceof stuff?
@Override StringTerms buildResult(long owningBucketOrd, long otherDocCount, StringTerms.Bucket[] topBuckets) { final BucketOrder reduceOrder; if (isKeyOrder(order) == false) { reduceOrder = isKeyOrder(order) ? InternalOrder.unwrap(order) : InternalOrder.key(true); Arrays.sort(topBuckets, reduceOrder.comparator()); } else { reduceOrder = order; } return new StringTerms(name, reduceOrder, order, bucketCountThresholds.getRequiredSize(), bucketCountThresholds.getMinDocCount(), metadata(), format, bucketCountThresholds.getShardSize(), showTermDocCountError, otherDocCount, Arrays.asList(topBuckets), 0); }	iskeyorder is false here, right?
protected final void doWriteTo(StreamOutput out) throws IOException { if (out.getVersion().onOrAfter(Version.V_8_0_0)) { reduceOrder.writeTo(out); } order.writeTo(out); writeSize(requiredSize, out); out.writeVLong(minDocCount); writeTermTypeInfoTo(out); }	fine for now, but i feel like we have four or five of these and maybe it'd be worth merging their implementations.
public Result isConditionMet(Index index, ClusterState clusterState) { IndexMetaData idxMeta = clusterState.metaData().index(index); if (idxMeta == null) { // Index must have been since deleted, ignore it logger.debug("[{}] lifecycle action for index [{}] executed but index no longer exists", getKey().getAction(), index.getName()); return new Result(false, null); } if (ActiveShardCount.ALL.enoughShardsActive(clusterState, index.getName()) == false) { logger.debug("[{}] lifecycle action for index [{}] cannot make progress because not all shards are active", getKey().getAction(), index.getName()); return new Result(false, new Info(idxMeta.getNumberOfReplicas(), -1, false)); } // All the allocation attributes are already set so just need to check // if the allocation has happened RoutingAllocation allocation = new RoutingAllocation(ALLOCATION_DECIDERS, clusterState.getRoutingNodes(), clusterState, null, System.nanoTime()); int allocationPendingAllShards = 0; ImmutableOpenIntMap<IndexShardRoutingTable> allShards = clusterState.getRoutingTable().index(index).getShards(); for (ObjectCursor<IndexShardRoutingTable> shardRoutingTable : allShards.values()) { for (ShardRouting shardRouting : shardRoutingTable.value.shards()) { String currentNodeId = shardRouting.currentNodeId(); boolean canRemainOnCurrentNode = ALLOCATION_DECIDERS .canRemain(shardRouting, clusterState.getRoutingNodes().node(currentNodeId), allocation) .type() == Decision.Type.YES; if (canRemainOnCurrentNode == false) { allocationPendingAllShards++; } } } if (allocationPendingAllShards > 0) { logger.debug("{} lifecycle action [{}] waiting for [{}] shards to be allocated to nodes matching the given filters", index, getKey().getAction(), allocationPendingAllShards); return new Result(false, new Info(idxMeta.getNumberOfReplicas(), allocationPendingAllShards, true)); } else { logger.debug("{} lifecycle action for [{}] complete", index, getKey().getAction()); return new Result(true, null); } }	we need to add a check for the shard being started here too otherwise we will end up with the same issue on the allocate action but it will be harder to diagnose
@Override public void onFileInit(Path file) { Tuple<String, String> scriptNameExt = getScriptNameExt(file); if (scriptNameExt == null) { logger.debug("Skipped script with invalid extension : [{}]", file); return; } if (logger.isTraceEnabled()) { logger.trace("Loading script file : [{}]", file); } ScriptEngineService engineService = getScriptEngineServiceForFileExt(scriptNameExt.v2()); if (engineService == null) { logger.warn("No script engine found for [{}]", scriptNameExt.v2()); } else { try { //we don't know yet what the script will be used for, but if all of the operations for this lang // with file scripts are disabled, it makes no sense to even compile it and cache it. if (isAnyScriptContextEnabled(engineService.getType(), ScriptType.FILE)) { logger.info("compiling script file [{}]", file.toAbsolutePath()); try (InputStreamReader reader = new InputStreamReader(Files.newInputStream(file), StandardCharsets.UTF_8)) { String script = Streams.copyToString(reader); String name = scriptNameExt.v1(); CacheKey cacheKey = new CacheKey(engineService, name, null, Collections.emptyMap()); // pass the actual file name to the compiler (for script engines that care about this) Object executable = engineService.compile(file.getFileName().toString(), script, Collections.emptyMap()); CompiledScript compiledScript = new CompiledScript(ScriptType.FILE, name, engineService.getType(), executable); staticCache.put(cacheKey, compiledScript); scriptMetrics.onCompilation(); } } else { logger.warn("skipping compile of script file [{}] as all scripted operations are disabled for file scripts", file.toAbsolutePath()); } } catch (Exception e) { try (XContentBuilder builder = JsonXContent.contentBuilder()) { builder.prettyPrint(); builder.startObject(); ElasticsearchException.toXContent(builder, ToXContent.EMPTY_PARAMS, e); builder.endObject(); logger.warn("failed to load/compile script [{}]: {}", scriptNameExt.v1(), builder.string()); } catch (IOException ioe) { ioe.addSuppressed(e); logger.warn((Supplier<?>) () -> new ParameterizedMessage( "failed to log an appropriate warning after failing to load/compile script [{}]", scriptNameExt.v1()), ioe); } /* Log at the whole exception at the debug level as well just in case the stack trace is important. That way you can * turn on the stack trace if you need it. */ logger.debug((Supplier<?>) () -> new ParameterizedMessage("failed to load/compile script [{}]", scriptNameExt.v1()), e); } } }	can you only do toxcontent for scriptexception, but still log the entire stack trace for other exceptions? eg, if we have an npe because of a bug in painless, i think we should still see the full stack trace.
@Override public void onFileInit(Path file) { Tuple<String, String> scriptNameExt = getScriptNameExt(file); if (scriptNameExt == null) { logger.debug("Skipped script with invalid extension : [{}]", file); return; } if (logger.isTraceEnabled()) { logger.trace("Loading script file : [{}]", file); } ScriptEngineService engineService = getScriptEngineServiceForFileExt(scriptNameExt.v2()); if (engineService == null) { logger.warn("No script engine found for [{}]", scriptNameExt.v2()); } else { try { //we don't know yet what the script will be used for, but if all of the operations for this lang // with file scripts are disabled, it makes no sense to even compile it and cache it. if (isAnyScriptContextEnabled(engineService.getType(), ScriptType.FILE)) { logger.info("compiling script file [{}]", file.toAbsolutePath()); try (InputStreamReader reader = new InputStreamReader(Files.newInputStream(file), StandardCharsets.UTF_8)) { String script = Streams.copyToString(reader); String name = scriptNameExt.v1(); CacheKey cacheKey = new CacheKey(engineService, name, null, Collections.emptyMap()); // pass the actual file name to the compiler (for script engines that care about this) Object executable = engineService.compile(file.getFileName().toString(), script, Collections.emptyMap()); CompiledScript compiledScript = new CompiledScript(ScriptType.FILE, name, engineService.getType(), executable); staticCache.put(cacheKey, compiledScript); scriptMetrics.onCompilation(); } } else { logger.warn("skipping compile of script file [{}] as all scripted operations are disabled for file scripts", file.toAbsolutePath()); } } catch (Exception e) { try (XContentBuilder builder = JsonXContent.contentBuilder()) { builder.prettyPrint(); builder.startObject(); ElasticsearchException.toXContent(builder, ToXContent.EMPTY_PARAMS, e); builder.endObject(); logger.warn("failed to load/compile script [{}]: {}", scriptNameExt.v1(), builder.string()); } catch (IOException ioe) { ioe.addSuppressed(e); logger.warn((Supplier<?>) () -> new ParameterizedMessage( "failed to log an appropriate warning after failing to load/compile script [{}]", scriptNameExt.v1()), ioe); } /* Log at the whole exception at the debug level as well just in case the stack trace is important. That way you can * turn on the stack trace if you need it. */ logger.debug((Supplier<?>) () -> new ParameterizedMessage("failed to load/compile script [{}]", scriptNameExt.v1()), e); } } }	i think the message here should be different? otherwise it is confusing to see the same message, with different endings (one being xcontent, and the other being a java stack trace).
R doApply(GeoShape s); } public enum GeoOperation { ASWKT(GeoShape::toString), X(GeoShape::getX), Y(GeoShape::getY), GEOMETRY_TYPE(GeoShape::getGeometryType); private final Function<Object, Object> apply; GeoOperation(GeoShapeFunction<Object> apply) { this.apply = l -> l == null ? null : apply.apply(l); } public final Object apply(Object l) { return apply.apply(l); } } public static final String NAME = "geo"; private final GeoOperation processor; public GeoProcessor(GeoOperation processor) { this.processor = processor; } public GeoProcessor(StreamInput in) throws IOException { processor = in.readEnum(GeoOperation.class); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeEnum(processor); } @Override public String getWriteableName() { return NAME; } @Override public Object process(Object input) { return processor.apply(input); } GeoOperation processor() { return processor; } @Override public boolean equals(Object obj) { if (obj == null || obj.getClass() != getClass()) { return false; } GeoProcessor other = (GeoProcessor) obj; return processor == other.processor; } @Override public int hashCode() { return processor.hashCode(); }	can you put this one second in line, please? (considering an alphabetic ordering)
public Point firstPoint() { return shapeBuilder.buildGeometry().visit(new GeometryVisitor<Point>() { @Override public Point visit(Circle circle) { return new Point(circle.getLat(), circle.getLon(), circle.hasAlt() ? circle.getAlt() : Double.NaN); } @Override public Point visit(GeometryCollection<?> collection) { if (collection.size() > 0) { return collection.get(0).visit(this); } return null; } @Override public Point visit(Line line) { if (line.length() > 0) { return new Point(line.getLat(0), line.getLon(0), line.hasAlt() ? line.getAlt(0) : Double.NaN); } return null; } @Override public Point visit(LinearRing ring) { return visit((Line) ring); } @Override public Point visit(MultiLine multiLine) { return visit((GeometryCollection<?>) multiLine); } @Override public Point visit(MultiPoint multiPoint) { return visit((GeometryCollection<?>) multiPoint); } @Override public Point visit(MultiPolygon multiPolygon) { return visit((GeometryCollection<?>) multiPolygon); } @Override public Point visit(Point point) { return point; } @Override public Point visit(Polygon polygon) { return visit(polygon.getPolygon()); } @Override public Point visit(Rectangle rectangle) { return new Point(rectangle.getMinLat(), rectangle.getMinLon(), rectangle.getMinAlt()); } }); }	i think here and below the cast is unnecessary?
public Point firstPoint() { return shapeBuilder.buildGeometry().visit(new GeometryVisitor<Point>() { @Override public Point visit(Circle circle) { return new Point(circle.getLat(), circle.getLon(), circle.hasAlt() ? circle.getAlt() : Double.NaN); } @Override public Point visit(GeometryCollection<?> collection) { if (collection.size() > 0) { return collection.get(0).visit(this); } return null; } @Override public Point visit(Line line) { if (line.length() > 0) { return new Point(line.getLat(0), line.getLon(0), line.hasAlt() ? line.getAlt(0) : Double.NaN); } return null; } @Override public Point visit(LinearRing ring) { return visit((Line) ring); } @Override public Point visit(MultiLine multiLine) { return visit((GeometryCollection<?>) multiLine); } @Override public Point visit(MultiPoint multiPoint) { return visit((GeometryCollection<?>) multiPoint); } @Override public Point visit(MultiPolygon multiPolygon) { return visit((GeometryCollection<?>) multiPolygon); } @Override public Point visit(Point point) { return point; } @Override public Point visit(Polygon polygon) { return visit(polygon.getPolygon()); } @Override public Point visit(Rectangle rectangle) { return new Point(rectangle.getMinLat(), rectangle.getMinLon(), rectangle.getMinAlt()); } }); }	all this logic could be re-written in a more compact way as return firstpoint != null ? firstpoint.getlon() : null;
public Point firstPoint() { return shapeBuilder.buildGeometry().visit(new GeometryVisitor<Point>() { @Override public Point visit(Circle circle) { return new Point(circle.getLat(), circle.getLon(), circle.hasAlt() ? circle.getAlt() : Double.NaN); } @Override public Point visit(GeometryCollection<?> collection) { if (collection.size() > 0) { return collection.get(0).visit(this); } return null; } @Override public Point visit(Line line) { if (line.length() > 0) { return new Point(line.getLat(0), line.getLon(0), line.hasAlt() ? line.getAlt(0) : Double.NaN); } return null; } @Override public Point visit(LinearRing ring) { return visit((Line) ring); } @Override public Point visit(MultiLine multiLine) { return visit((GeometryCollection<?>) multiLine); } @Override public Point visit(MultiPoint multiPoint) { return visit((GeometryCollection<?>) multiPoint); } @Override public Point visit(MultiPolygon multiPolygon) { return visit((GeometryCollection<?>) multiPolygon); } @Override public Point visit(Point point) { return point; } @Override public Point visit(Polygon polygon) { return visit(polygon.getPolygon()); } @Override public Point visit(Rectangle rectangle) { return new Point(rectangle.getMinLat(), rectangle.getMinLon(), rectangle.getMinAlt()); } }); }	same here about a more compact way of returning a value.
private void writeDocument(MonitoringDoc doc, StreamOutput out) throws IOException { final XContentType xContentType = XContentType.JSON; final XContent xContent = xContentType.xContent(); final String index = MonitoringTemplateUtils.indexName(formatter, doc.getSystem(), doc.getTimestamp()); final String id = doc.getId(); try (XContentBuilder builder = new XContentBuilder(xContent, out)) { // Builds the bulk action metadata line builder.startObject(); { builder.startObject("index"); { builder.field("_index", index); if (id != null) { builder.field("_id", id); } } builder.endObject(); } builder.endObject(); } // Adds action metadata line bulk separator out.write(xContent.streamSeparator()); // Adds the source of the monitoring document try (XContentBuilder builder = new XContentBuilder(xContent, out)) { doc.toXContent(builder, ToXContent.EMPTY_PARAMS); } // Adds final bulk separator out.write(xContent.streamSeparator()); logger.trace( "http exporter [{}] - added index request [index={}, id={}, monitoring data type={}]", name, index, id, doc.getType() ); }	this is hard to model with streaming io, but to me catch and appending an empty byte array seems bogus here and not worth retaining (there's no test covering this case and we should simply make sure out documents that we have full control over serialise shouldn't we?).
public void doAdd(Collection<MonitoringDoc> docs) throws ExportException { try { if (docs != null && docs.isEmpty() == false) { try (BytesStreamOutput payload = new BytesStreamOutput()) { for (MonitoringDoc monitoringDoc : docs) { writeDocument(monitoringDoc, payload); } // store the payload until we flush this.payload = payload.bytes(); } } } catch (Exception e) { throw new ExportException("failed to add documents to export bulk [{}]", e, name); } }	now that we are not copying bytes, is this still thread safe ? e.g. if one thread is calling doadd, but another thread is calling doflush , couldn't this line swap out the reference that flush is using ? (i think resulting in missing documents)
@Override public void doFlush(ActionListener<Void> listener) throws ExportException { if (payload == null) { listener.onFailure(new ExportException("unable to send documents because none were loaded for export bulk [{}]", name)); } else if (payload.length() != 0) { final Request request = new Request("POST", "/_bulk"); for (Map.Entry<String, String> param : params.entrySet()) { request.addParameter(param.getKey(), param.getValue()); } try { request.setEntity(new InputStreamEntity(payload.streamInput(), payload.length(), ContentType.APPLICATION_JSON)); } catch (IOException e) { throw new AssertionError("No actual IO happens here", e); } // null out serialized docs to make things easier on the GC payload = null; client.performRequestAsync(request, new ResponseListener() { @Override public void onSuccess(Response response) { try { HttpExportBulkResponseListener.INSTANCE.onSuccess(response); } finally { listener.onResponse(null); } } @Override public void onFailure(Exception exception) { try { HttpExportBulkResponseListener.INSTANCE.onFailure(exception); } finally { listener.onFailure(exception); } } }); } }	can we use an //do nothing comment + assert here instead of throwing an error ?
@Override public void doFlush(ActionListener<Void> listener) throws ExportException { if (payload == null) { listener.onFailure(new ExportException("unable to send documents because none were loaded for export bulk [{}]", name)); } else if (payload.length() != 0) { final Request request = new Request("POST", "/_bulk"); for (Map.Entry<String, String> param : params.entrySet()) { request.addParameter(param.getKey(), param.getValue()); } try { request.setEntity(new InputStreamEntity(payload.streamInput(), payload.length(), ContentType.APPLICATION_JSON)); } catch (IOException e) { throw new AssertionError("No actual IO happens here", e); } // null out serialized docs to make things easier on the GC payload = null; client.performRequestAsync(request, new ResponseListener() { @Override public void onSuccess(Response response) { try { HttpExportBulkResponseListener.INSTANCE.onSuccess(response); } finally { listener.onResponse(null); } } @Override public void onFailure(Exception exception) { try { HttpExportBulkResponseListener.INSTANCE.onFailure(exception); } finally { listener.onFailure(exception); } } }); } }	does this really help ? it would seem that payload.streaminput() creates an object that also has a reference the same bytesreference (this). i think this line may just cause a little bit of confusion.
public void testSnapshotVerifyRepository() throws IOException { RestHighLevelClient client = highLevelClient(); createTestRepositories(); // tag::verify-repository-request VerifyRepositoryRequest request = new VerifyRepositoryRequest(repositoryName); // end::verify-repository-request // tag::verify-repository-request-masterTimeout request.masterNodeTimeout(TimeValue.timeValueMinutes(1)); // <1> request.masterNodeTimeout("1m"); // <2> // end::verify-repository-request-masterTimeout // tag::verify-repository-request-timeout request.timeout(TimeValue.timeValueMinutes(1)); // <1> request.timeout("1m"); // <2> // end::verify-repository-request-timeout // tag::verify-repository-execute VerifyRepositoryRestResponse response = client.snapshot().verifyRepository(request); // end::verify-repository-execute // tag::verify-repository-response List<VerifyRepositoryRestResponse.NodeView> repositoryMetaDataResponse = response.nodes(); // end::verify-repository-response assertThat(1, equalTo(repositoryMetaDataResponse.size())); assertThat("node-0", equalTo(repositoryMetaDataResponse.get(0).getName())); }	could we show a bit more about what to do with the response?
public void testDeactivateAndActivate() throws Exception { PutWatchResponse putWatchResponse = watcherClient().preparePutWatch() .setId("_id") .setSource(watchBuilder() .trigger(schedule(interval("1s"))) .input(simpleInput("foo", "bar")) .addAction("_a1", indexAction("actions")) .defaultThrottlePeriod(new TimeValue(0, TimeUnit.SECONDS))) .get(); assertThat(putWatchResponse.isCreated(), is(true)); GetWatchResponse getWatchResponse = watcherClient().prepareGetWatch("_id").get(); assertThat(getWatchResponse, notNullValue()); assertThat(getWatchResponse.getStatus().state().isActive(), is(true)); logger.info("Waiting for watch to be executed at least once"); assertWatchWithMinimumActionsCount("_id", ExecutionState.EXECUTED, 1); // we now know the watch is executing... lets deactivate it ActivateWatchResponse activateWatchResponse = watcherClient().prepareActivateWatch("_id", false).get(); assertThat(activateWatchResponse, notNullValue()); assertThat(activateWatchResponse.getStatus().state().isActive(), is(false)); getWatchResponse = watcherClient().prepareGetWatch("_id").get(); assertThat(getWatchResponse, notNullValue()); assertThat(getWatchResponse.getStatus().state().isActive(), is(false)); // wait until no watch is executing assertBusy(() -> { WatcherStatsResponse statsResponse = watcherClient().prepareWatcherStats().setIncludeCurrentWatches(true).get(); int sum = statsResponse.getNodes().stream().map(WatcherStatsResponse.Node::getSnapshots).mapToInt(List::size).sum(); assertThat(sum, is(0)); }); logger.info("Ensured no more watches are being executed"); refresh(); long count1 = docCount(".watcher-history*", matchAllQuery()); refresh(); //ensure no new watch history awaitBusy(() -> count1 != docCount(".watcher-history*", matchAllQuery()), 5, TimeUnit.SECONDS); // lets activate it again logger.info("Activating watch again"); activateWatchResponse = watcherClient().prepareActivateWatch("_id", true).get(); assertThat(activateWatchResponse, notNullValue()); assertThat(activateWatchResponse.getStatus().state().isActive(), is(true)); getWatchResponse = watcherClient().prepareGetWatch("_id").get(); assertThat(getWatchResponse, notNullValue()); assertThat(getWatchResponse.getStatus().state().isActive(), is(true)); refresh(); assertBusy(() -> { long count2 = docCount(".watcher-history*", matchAllQuery()); assertThat(count2, greaterThan(count1)); }); }	i noticed you added the 5s timeout to the first awaitbusy(), any reason it was not added here?
public void registerAggregationUsage(String aggregationName, String valuesSourceType) { Map<String, LongAdder> subAgg = aggs.computeIfAbsent(aggregationName, k -> new HashMap<>()); if ( subAgg.put(valuesSourceType, new LongAdder()) != null) { throw new IllegalArgumentException("stats for aggregation [" + aggregationName + "][" + valuesSourceType + "] already registered"); } }	nit: javadoc. especially since this looks like it's used concurrently, we should document the expected concurrency behavior. also, am i missing something or is this not called right now? i only saw a few uses in tests.
@Override public IndexOutput createOutput(final String name, IOContext context) throws IOException { final IndexOutput output = super.createOutput(name, context); fileLengthCache.remove(name); // protection against multiple writes return new IndexOutput(output.toString()) { @Override public void close() throws IOException { output.close(); fileLengthCache.put(name, getFilePointer()); } @Override public long getFilePointer() { return output.getFilePointer(); } @Override public long getChecksum() throws IOException { return output.getChecksum(); } @Override public void writeByte(byte b) throws IOException { output.writeByte(b); } @Override public void writeBytes(byte[] b, int offset, int length) throws IOException { output.writeBytes(b, offset, length); } }; }	should we call remove before put? (was just trying to think about what would happen if source == dest)
private static SnapshotShardFailure constructSnapshotShardFailure(Object[] args) { String index = (String) args[0]; String indexUuid = (String) args[1]; String nodeId = (String) args[2]; String reason = (String) args[3]; Integer intShardId = (Integer) args[4]; String status = (String) args[5]; if (index == null) { throw new ElasticsearchParseException("index name was not set"); } if (intShardId == null) { throw new ElasticsearchParseException("index shard was not set"); } ShardId shardId = new ShardId(index, indexUuid != null ? indexUuid : IndexMetaData.INDEX_UUID_NA_VALUE, intShardId); RestStatus restStatus; if (status != null) { restStatus = RestStatus.valueOf(status); } else { restStatus = RestStatus.INTERNAL_SERVER_ERROR; } return new SnapshotShardFailure(nodeId, shardId, reason, restStatus); }	this was for bwc with 5.x, it's irrelevant in 8.0 now
public void testSyncedFlush() throws IOException { try (Store store = createStore(); Engine engine = new InternalEngine(config(defaultSettings, store, createTempDir(), new LogByteSizeMergePolicy(), null))) { final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20); ParsedDocument doc = testParsedDocument("1", null, testDocumentWithTextField(), B_1, null); engine.index(indexForDoc(doc)); Engine.CommitId commitID = engine.flush(); assertThat(commitID, equalTo(new Engine.CommitId(store.readLastCommittedSegmentsInfo().getId()))); byte[] wrongBytes = Base64.getDecoder().decode(commitID.toString()); wrongBytes[0] = (byte) ~wrongBytes[0]; Engine.CommitId wrongId = new Engine.CommitId(wrongBytes); assertEquals("should fail to sync flush with wrong id (but no docs)", engine.syncFlush(syncId + "1", wrongId), Engine.SyncedFlushResult.COMMIT_MISMATCH); engine.index(indexForDoc(doc)); assertEquals("should fail to sync flush with right id but pending doc", engine.syncFlush(syncId + "2", commitID), Engine.SyncedFlushResult.PENDING_OPERATIONS); commitID = engine.flush(); assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID), Engine.SyncedFlushResult.SUCCESS); assertEquals(store.readLastCommittedSegmentsInfo().getUserData().get(Engine.SYNC_COMMIT_ID), syncId); assertEquals(engine.getLastCommittedSegmentInfos().getUserData().get(Engine.SYNC_COMMIT_ID), syncId); } }	extra space is extra.
public void testSyncedFlush() throws IOException { try (Store store = createStore(); Engine engine = new InternalEngine(config(defaultSettings, store, createTempDir(), new LogByteSizeMergePolicy(), null))) { final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20); ParsedDocument doc = testParsedDocument("1", null, testDocumentWithTextField(), B_1, null); engine.index(indexForDoc(doc)); Engine.CommitId commitID = engine.flush(); assertThat(commitID, equalTo(new Engine.CommitId(store.readLastCommittedSegmentsInfo().getId()))); byte[] wrongBytes = Base64.getDecoder().decode(commitID.toString()); wrongBytes[0] = (byte) ~wrongBytes[0]; Engine.CommitId wrongId = new Engine.CommitId(wrongBytes); assertEquals("should fail to sync flush with wrong id (but no docs)", engine.syncFlush(syncId + "1", wrongId), Engine.SyncedFlushResult.COMMIT_MISMATCH); engine.index(indexForDoc(doc)); assertEquals("should fail to sync flush with right id but pending doc", engine.syncFlush(syncId + "2", commitID), Engine.SyncedFlushResult.PENDING_OPERATIONS); commitID = engine.flush(); assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID), Engine.SyncedFlushResult.SUCCESS); assertEquals(store.readLastCommittedSegmentsInfo().getUserData().get(Engine.SYNC_COMMIT_ID), syncId); assertEquals(engine.getLastCommittedSegmentInfos().getUserData().get(Engine.SYNC_COMMIT_ID), syncId); } }	extra space is extra.
private Path writeTranslog( final ShardId shardId, final String translogUUID, final long generation, final long globalCheckpoint ) throws IOException { final Path tempDir = createTempDir(); final Path resolve = tempDir.resolve(Translog.getFilename(generation)); Files.createFile(tempDir.resolve(Translog.CHECKPOINT_FILE_NAME)); try (TranslogWriter ignored = TranslogWriter.create( shardId, translogUUID, generation, resolve, FileChannel::open, TranslogConfig.DEFAULT_BUFFER_SIZE, () -> globalCheckpoint, () -> generation)) { } return tempDir; }	it looks like it was unnecessary to touch this file?
* @param taskId the id of the current task. This is added to the thread name for easier tracking * @param threadCollector a list in which we collect all the threads created by the client */ static RestClient buildRestClient(RemoteInfo remoteInfo, long taskId, List<Thread> threadCollector) { Header[] clientHeaders = new Header[remoteInfo.getHeaders().size()]; int i = 0; for (Map.Entry<String, String> header : remoteInfo.getHeaders().entrySet()) { clientHeaders[i++] = new BasicHeader(header.getKey(), header.getValue()); } return RestClient.builder(new HttpHost(remoteInfo.getHost(), remoteInfo.getPort(), remoteInfo.getScheme())) .setDefaultHeaders(clientHeaders) .setRequestConfigCallback(c -> { c.setConnectTimeout(Math.toIntExact(remoteInfo.getConnectTimeout().millis())); c.setSocketTimeout(Math.toIntExact(remoteInfo.getSocketTimeout().millis())); return c; }) .setHttpClientConfigCallback(c -> { // Enable basic auth if it is configured if (remoteInfo.getUsername() != null) { UsernamePasswordCredentials creds = new UsernamePasswordCredentials(remoteInfo.getUsername(), remoteInfo.getPassword()); CredentialsProvider credentialsProvider = new BasicCredentialsProvider(); credentialsProvider.setCredentials(AuthScope.ANY, creds); c.setDefaultCredentialsProvider(credentialsProvider); } // Stick the task id in the thread name so we can track down tasks from stack traces AtomicInteger threads = new AtomicInteger(); c.setThreadFactory(r -> { String name = "es-client-" + taskId + "-" + threads.getAndIncrement(); Thread t = new Thread(r, name); threadCollector.add(t); return t; }); // Limit ourselves to one reactor thread because for now the search process is single threaded. c.setDefaultIOReactorConfig(IOReactorConfig.custom().setIoThreadCount(1).build()); return c; }).build(); }	yeah, that looks like the right fix....
private Object unstash(String key) throws IOException { Object stashedValue = stashObjectPath.evaluate(key, this); if (stashedValue == null) { throw new IllegalArgumentException("stashed value not found for key [" + key + "]"); } return stashedValue; }	i will extract this into a separate pr with a corresponding unit test in stashtests.
private CheckedRunnable<Exception> assertTask(final int numberOfPrimaryShards, final Map<ShardId, Long> numDocsPerShard) { return () -> { final ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); final PersistentTasksCustomMetaData taskMetadata = clusterState.getMetaData().custom(PersistentTasksCustomMetaData.TYPE); ListTasksRequest listTasksRequest = new ListTasksRequest(); listTasksRequest.setDetailed(true); listTasksRequest.setActions(ShardFollowTask.NAME + "[c]"); ListTasksResponse listTasksResponse = client().admin().cluster().listTasks(listTasksRequest).actionGet(); assertThat(listTasksResponse.getNodeFailures().size(), equalTo(0)); assertThat(listTasksResponse.getTaskFailures().size(), equalTo(0)); List<TaskInfo> taskInfos = listTasksResponse.getTasks(); assertThat(taskInfos.size(), equalTo(numberOfPrimaryShards)); Collection<PersistentTasksCustomMetaData.PersistentTask<?>> shardFollowTasks = taskMetadata.findTasks(ShardFollowTask.NAME, Objects::nonNull); for (PersistentTasksCustomMetaData.PersistentTask<?> shardFollowTask : shardFollowTasks) { final ShardFollowTask shardFollowTaskParams = (ShardFollowTask) shardFollowTask.getParams(); TaskInfo taskInfo = null; String expectedId = "id=" + shardFollowTask.getId(); for (TaskInfo info : taskInfos) { if (expectedId.equals(info.getDescription())) { taskInfo = info; break; } } assertThat(taskInfo, notNullValue()); ShardFollowNodeTask.Status status = (ShardFollowNodeTask.Status) taskInfo.getStatus(); assertThat(status, notNullValue()); assertThat("incorrect global checkpoint " + shardFollowTaskParams, status.followerGlobalCheckpoint(), equalTo(numDocsPerShard.get(shardFollowTaskParams.getLeaderShardId()))); } }; }	just wondering (i don't care much) - why did you move from the get* method naming?
public static HealthComponentResult createClusterCoordinationComponent( final DiscoveryNode coordinatingNode, final ClusterState clusterState ) { final DiscoveryNodes nodes = clusterState.nodes(); final DiscoveryNode masterNode = nodes.getMasterNode(); HealthStatus instanceHasMasterStatus = masterNode == null ? HealthStatus.RED : HealthStatus.GREEN; String instanceHasMasterSummary = masterNode == null ? INSTANCE_HAS_MASTER_RED_SUMMARY : INSTANCE_HAS_MASTER_GREEN_SUMMARY; HealthIndicatorResult instanceHasMaster = new HealthIndicatorResult( INSTANCE_HAS_MASTER_NAME, NAME, instanceHasMasterStatus, instanceHasMasterSummary, (builder, params) -> { builder.startObject(); builder.object("coordinating_node", xContentBuilder -> { builder.field("node_id", coordinatingNode.getId()); builder.field("name", coordinatingNode.getName()); }); builder.object("master_node", xContentBuilder -> { if (masterNode != null) { builder.field("node_id", masterNode.getId()); builder.field("name", masterNode.getName()); } else { builder.nullField("node_id"); builder.nullField("name"); } }); return builder.endObject(); } ); var noEligibleMasterNodes = noEligibleMasterNodes(clusterState); var lackOfQuorum = lackOfQuorum(clusterState); final HealthStatus status = HealthStatus.merge( Stream.of(instanceHasMaster, noEligibleMasterNodes, lackOfQuorum).map(HealthIndicatorResult::status) ); return new HealthComponentResult( NAME, status, Map.of(INSTANCE_HAS_MASTER_NAME, instanceHasMaster, NO_ELIGIBLE_MASTERS, noEligibleMasterNodes, LACK_OF_QUORUM, lackOfQuorum) ); }	i guess we should be consistent here. the indicator i committed has a period as i feel like a summary could potentially include multiple sentences and we should use periods here. so i guess my preference would be each summary is a grammatically correct sentence with a period. although, i am open to other thoughts.
private Table buildTable(RestRequest req, GetSnapshotsResponse getSnapshotsResponse) { Table table = getTableWithHeader(req); if (getSnapshotsResponse.isFailed()) { throw new ElasticsearchException( "Repositories [" + Strings.collectionToCommaDelimitedString(getSnapshotsResponse.getFailedResponses().keySet()) + "] failed to retrieve snapshots"); } for (Map.Entry<String, List<SnapshotInfo>> response : getSnapshotsResponse.getSuccessfulResponses().entrySet()) { String repository = response.getKey(); for (SnapshotInfo snapshotStatus : response.getValue()) { table.startRow(); table.addCell(snapshotStatus.snapshotId().getName()); table.addCell(repository); table.addCell(snapshotStatus.state()); table.addCell(TimeUnit.SECONDS.convert(snapshotStatus.startTime(), TimeUnit.MILLISECONDS)); table.addCell(FORMATTER.format(Instant.ofEpochMilli(snapshotStatus.startTime()))); table.addCell(TimeUnit.SECONDS.convert(snapshotStatus.endTime(), TimeUnit.MILLISECONDS)); table.addCell(FORMATTER.format(Instant.ofEpochMilli(snapshotStatus.endTime()))); final long durationMillis; if (snapshotStatus.state() == SnapshotState.IN_PROGRESS) { durationMillis = System.currentTimeMillis() - snapshotStatus.startTime(); } else { durationMillis = snapshotStatus.endTime() - snapshotStatus.startTime(); } table.addCell(TimeValue.timeValueMillis(durationMillis)); table.addCell(snapshotStatus.indices().size()); table.addCell(snapshotStatus.successfulShards()); table.addCell(snapshotStatus.failedShards()); table.addCell(snapshotStatus.totalShards()); table.addCell(snapshotStatus.reason()); table.endRow(); } } return table; }	this hides the original exception?
@Override public void optimize(Optimize optimize) throws EngineException { if (optimize.flush()) { flush(new Flush().force(true).waitIfOngoing(true)); } final ElasticsearchMergePolicy elasticsearchMergePolicy; if (indexWriter.getConfig().getMergePolicy() instanceof ElasticsearchMergePolicy) { elasticsearchMergePolicy = (ElasticsearchMergePolicy) indexWriter.getConfig().getMergePolicy(); } else { elasticsearchMergePolicy = null; } if (optimize.force() && elasticsearchMergePolicy == null) { throw new ElasticsearchIllegalStateException("The `force` flag can only be used if the merge policy is an instance of " + ElasticsearchMergePolicy.class.getSimpleName() + ", got [" + indexWriter.getConfig().getMergePolicy().getClass().getName() + "]"); } if (optimizeMutex.compareAndSet(false, true)) { rwl.readLock().lock(); try { ensureOpen(); /* * The way we implement "forced forced merges" is a bit hackish in the sense that we set an instance variable and that this * setting will thus apply to all forced merges that will be run until `force` is set back to false. However, since * InternalEngine.optimize is the only place in code where we call forceMerge and since calls are protected with * `optimizeMutex`, this has the expected behavior. */ if (optimize.force()) { elasticsearchMergePolicy.setForce(true); } if (optimize.onlyExpungeDeletes()) { indexWriter.forceMergeDeletes(false); } else if (optimize.maxNumSegments() <= 0) { indexWriter.maybeMerge(); possibleMergeNeeded = false; } else { indexWriter.forceMerge(optimize.maxNumSegments(), false); } } catch (OutOfMemoryError e) { failEngine(e); throw new OptimizeFailedEngineException(shardId, e); } catch (IllegalStateException e) { if (e.getMessage().contains("OutOfMemoryError")) { failEngine(e); } throw new OptimizeFailedEngineException(shardId, e); } catch (Throwable e) { throw new OptimizeFailedEngineException(shardId, e); } finally { if (elasticsearchMergePolicy != null) { elasticsearchMergePolicy.setForce(false); } rwl.readLock().unlock(); optimizeMutex.set(false); } } // wait for the merges outside of the read lock if (optimize.waitForMerge()) { indexWriter.waitForMerges(); } if (optimize.flush()) { flush(new Flush().force(true).waitIfOngoing(true)); } }	is there a chance that the mp is changed while this runs so we better do it under the lock?
@Override public void optimize(Optimize optimize) throws EngineException { if (optimize.flush()) { flush(new Flush().force(true).waitIfOngoing(true)); } final ElasticsearchMergePolicy elasticsearchMergePolicy; if (indexWriter.getConfig().getMergePolicy() instanceof ElasticsearchMergePolicy) { elasticsearchMergePolicy = (ElasticsearchMergePolicy) indexWriter.getConfig().getMergePolicy(); } else { elasticsearchMergePolicy = null; } if (optimize.force() && elasticsearchMergePolicy == null) { throw new ElasticsearchIllegalStateException("The `force` flag can only be used if the merge policy is an instance of " + ElasticsearchMergePolicy.class.getSimpleName() + ", got [" + indexWriter.getConfig().getMergePolicy().getClass().getName() + "]"); } if (optimizeMutex.compareAndSet(false, true)) { rwl.readLock().lock(); try { ensureOpen(); /* * The way we implement "forced forced merges" is a bit hackish in the sense that we set an instance variable and that this * setting will thus apply to all forced merges that will be run until `force` is set back to false. However, since * InternalEngine.optimize is the only place in code where we call forceMerge and since calls are protected with * `optimizeMutex`, this has the expected behavior. */ if (optimize.force()) { elasticsearchMergePolicy.setForce(true); } if (optimize.onlyExpungeDeletes()) { indexWriter.forceMergeDeletes(false); } else if (optimize.maxNumSegments() <= 0) { indexWriter.maybeMerge(); possibleMergeNeeded = false; } else { indexWriter.forceMerge(optimize.maxNumSegments(), false); } } catch (OutOfMemoryError e) { failEngine(e); throw new OptimizeFailedEngineException(shardId, e); } catch (IllegalStateException e) { if (e.getMessage().contains("OutOfMemoryError")) { failEngine(e); } throw new OptimizeFailedEngineException(shardId, e); } catch (Throwable e) { throw new OptimizeFailedEngineException(shardId, e); } finally { if (elasticsearchMergePolicy != null) { elasticsearchMergePolicy.setForce(false); } rwl.readLock().unlock(); optimizeMutex.set(false); } } // wait for the merges outside of the read lock if (optimize.waitForMerge()) { indexWriter.waitForMerges(); } if (optimize.flush()) { flush(new Flush().force(true).waitIfOngoing(true)); } }	ok i can see why this sucks.... my suggestion is to add a custom mergetrigger to lucene that we can use and then we can just call private final void maybemerge(mergetrigger trigger, int maxnumsegments) instead?
public static IndexTemplateMetaData fromXContent(XContentParser parser, String templateName) throws IOException { Builder builder = new Builder(templateName); String currentFieldName = skipTemplateName(parser); XContentParser.Token token; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_OBJECT) { if ("settings".equals(currentFieldName)) { Settings.Builder templateSettingsBuilder = Settings.builder(); templateSettingsBuilder.put( SettingsLoader.Helper.loadNestedFromMap(parser.mapOrdered())) .normalizePrefix(IndexMetaData.INDEX_SETTING_PREFIX); builder.settings(templateSettingsBuilder.build()); } else if ("mappings".equals(currentFieldName)) { while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_OBJECT) { String mappingType = currentFieldName; Map<String, Object> mappingSource = MapBuilder.<String, Object>newMapBuilder().put(mappingType, parser.mapOrdered()).map(); builder.putMapping(mappingType, XContentFactory.jsonBuilder().map(mappingSource).string()); } } } else if ("aliases".equals(currentFieldName)) { while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { builder.putAlias(AliasMetaData.Builder.fromXContent(parser)); } } else { // check if its a custom index metadata IndexMetaData.Custom proto = IndexMetaData.lookupPrototype(currentFieldName); if (proto == null) { //TODO warn parser.skipChildren(); } else { IndexMetaData.Custom custom = proto.fromXContent(parser); builder.putCustom(custom.type(), custom); } } } else if (token == XContentParser.Token.START_ARRAY) { if ("mappings".equals(currentFieldName)) { while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { Map<String, Object> mapping = parser.mapOrdered(); if (mapping.size() == 1) { String mappingType = mapping.keySet().iterator().next(); String mappingSource = XContentFactory.jsonBuilder().map(mapping).string(); if (mappingSource == null) { // crap, no mapping source, warn? } else { builder.putMapping(mappingType, mappingSource); } } } } else if ("index_patterns".equals(currentFieldName)) { List<String> index_patterns = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { index_patterns.add(parser.text()); } builder.patterns(index_patterns); } } else if (token.isValue()) { // This is for roll-forward bwc with (#21009) if ("template".equals(currentFieldName)) { builder.patterns(Collections.singletonList(parser.text())); } else if ("order".equals(currentFieldName)) { builder.order(parser.intValue()); } else if ("version".equals(currentFieldName)) { builder.version(parser.intValue()); } } } return builder.build(); }	i'd change the comment to something like "prior to 5.1 elasticsearch only supported a single index pattern and called it template."
private static ByteBufAllocator createByteBufAllocator() { int nHeapArena = PooledByteBufAllocator.defaultNumHeapArena(); int pageSize = PooledByteBufAllocator.defaultPageSize(); int maxOrder = PooledByteBufAllocator.defaultMaxOrder(); int tinyCacheSize = PooledByteBufAllocator.defaultTinyCacheSize(); int smallCacheSize = PooledByteBufAllocator.defaultSmallCacheSize(); int normalCacheSize = PooledByteBufAllocator.defaultNormalCacheSize(); return new PooledByteBufAllocator(false, nHeapArena, 0, pageSize, maxOrder, tinyCacheSize, smallCacheSize, normalCacheSize, false); }	i think that we can reduce those too, wdyt @original-brownbear?
@Override public List<PreConfiguredTokenizer> getPreConfiguredTokenizer() { List<PreConfiguredTokenizer> tokenizers = new ArrayList<>(); tokenizers.add(PreConfiguredTokenizer.singleton("lowercase", LowerCaseTokenizer::new, () -> new TokenFilterFactory() { @Override public String name() { return "lowercase"; } @Override public TokenStream create(TokenStream tokenStream) { return new LowerCaseFilter(tokenStream); } })); return tokenizers; }	i wonder if (as a followup) name() could be removed from tokenfilterfactory? it seems to only have 3 uses, one of which is a test. then tokenfilterfactory could be a functional interface and this would look much cleaner: tokenizers.add(preconfiguredtokenizer.singleton("lowercase", lowercasetokenizer::new, lowercasetokenfilter::new));
@SuppressWarnings("unchecked") private void getPreview(Pivot pivot, SourceConfig source, String pipeline, String dest, ActionListener<PreviewDataFrameTransformAction.Response> listener) { final PreviewDataFrameTransformAction.Response previewResponse = new PreviewDataFrameTransformAction.Response(); ActionListener<SimulatePipelineResponse> pipelineResponseActionListener = ActionListener.wrap( simulatePipelineResponse -> { List<Map<String, Object>> response = new ArrayList<>(simulatePipelineResponse.getResults().size()); for(var simulateDocumentResult : simulatePipelineResponse.getResults()) { try(XContentBuilder xContentBuilder = XContentFactory.jsonBuilder()) { XContentBuilder content = simulateDocumentResult.toXContent(xContentBuilder, ToXContent.EMPTY_PARAMS); Map<String, Object> tempMap = XContentHelper.convertToMap(BytesReference.bytes(content), true, XContentType.JSON).v2(); response.add((Map<String, Object>)XContentMapValues.extractValue("doc._source", tempMap)); } } previewResponse.setDocs(response); listener.onResponse(previewResponse); }, listener::onFailure ); pivot.deduceMappings(client, source, ActionListener.wrap( deducedMappings -> { previewResponse.setMappingsFromStringMap(deducedMappings); ClientHelper.executeWithHeadersAsync(threadPool.getThreadContext().getHeaders(), ClientHelper.DATA_FRAME_ORIGIN, client, SearchAction.INSTANCE, pivot.buildSearchRequest(source, null, NUMBER_OF_PREVIEW_BUCKETS), ActionListener.wrap( r -> { try { final Aggregations aggregations = r.getAggregations(); if (aggregations == null) { listener.onFailure( new ElasticsearchStatusException("Underlying source indices have gone away", RestStatus.BAD_REQUEST) ); return; } final CompositeAggregation agg = aggregations.get(COMPOSITE_AGGREGATION_NAME); DataFrameIndexerTransformStats stats = new DataFrameIndexerTransformStats(); // remove all internal fields if (pipeline == null) { List<Map<String, Object>> results = pivot.extractResults(agg, deducedMappings, stats) .peek(doc -> doc.keySet().removeIf(k -> k.startsWith("_"))) .collect(Collectors.toList()); previewResponse.setDocs(results); listener.onResponse(previewResponse); } else { List<Map<String, Object>> results = pivot.extractResults(agg, deducedMappings, stats) .map(doc -> { Map<String, Object> src = new HashMap<>(); String id = (String) doc.get(DataFrameField.DOCUMENT_ID_FIELD); doc.keySet().removeIf(k -> k.startsWith("_")); src.put("_source", doc); src.put("_id", id); src.put("_index", dest); return src; }).collect(Collectors.toList()); try (XContentBuilder builder = jsonBuilder()) { builder.startObject(); builder.field("docs", results); builder.endObject(); var pipelineRequest = new SimulatePipelineRequest(BytesReference.bytes(builder), XContentType.JSON); pipelineRequest.setId(pipeline); ClientHelper.executeAsyncWithOrigin(client, ClientHelper.DATA_FRAME_ORIGIN, SimulatePipelineAction.INSTANCE, pipelineRequest, pipelineResponseActionListener); } } } catch (AggregationResultUtils.AggregationExtractionException extractionException) { listener.onFailure( new ElasticsearchStatusException(extractionException.getMessage(), RestStatus.BAD_REQUEST)); } }, listener::onFailure )); }, listener::onFailure )); }	how about source indices have been deleted or closed?
@Override protected IterationResult<DataFrameIndexerPosition> doProcess(SearchResponse searchResponse) { final Aggregations aggregations = searchResponse.getAggregations(); // Treat this as a "we reached the end". // This should only happen when all underlying indices have gone away. Consequently, there is no more data to read. if (aggregations == null) { logger.info("[" + getJobId() + "] unexpected null aggregations in search response. " + "Concrete indices may have disappeared or have been closed"); auditor.info(getJobId(), "Concrete source indices seem to have gone away. " + "Please verify that these indices exist and are open [" + Strings.arrayToCommaDelimitedString(getConfig().getSource().getIndex()) + "]."); return new IterationResult<>(Collections.emptyList(), null, true); } final CompositeAggregation agg = aggregations.get(COMPOSITE_AGGREGATION_NAME); switch (runState) { case FULL_RUN: return processBuckets(agg); case PARTIAL_RUN_APPLY_CHANGES: return processPartialBucketUpdates(agg); case PARTIAL_RUN_IDENTIFY_CHANGES: return processChangedBuckets(agg); default: // Any other state is a bug, should not happen logger.warn("Encountered unexpected run state [" + runState + "]"); throw new IllegalStateException("DataFrame indexer job encountered an illegal state [" + runState + "]"); } }	again, source indices have been deleted or closed for the second sentence
@Override protected IterationResult<DataFrameIndexerPosition> doProcess(SearchResponse searchResponse) { final Aggregations aggregations = searchResponse.getAggregations(); // Treat this as a "we reached the end". // This should only happen when all underlying indices have gone away. Consequently, there is no more data to read. if (aggregations == null) { logger.info("[" + getJobId() + "] unexpected null aggregations in search response. " + "Concrete indices may have disappeared or have been closed"); auditor.info(getJobId(), "Concrete source indices seem to have gone away. " + "Please verify that these indices exist and are open [" + Strings.arrayToCommaDelimitedString(getConfig().getSource().getIndex()) + "]."); return new IterationResult<>(Collections.emptyList(), null, true); } final CompositeAggregation agg = aggregations.get(COMPOSITE_AGGREGATION_NAME); switch (runState) { case FULL_RUN: return processBuckets(agg); case PARTIAL_RUN_APPLY_CHANGES: return processPartialBucketUpdates(agg); case PARTIAL_RUN_IDENTIFY_CHANGES: return processChangedBuckets(agg); default: // Any other state is a bug, should not happen logger.warn("Encountered unexpected run state [" + runState + "]"); throw new IllegalStateException("DataFrame indexer job encountered an illegal state [" + runState + "]"); } }	source indices have been deleted or closed for the first sentence
static <Response> ActionListener<Response> runAfter(ActionListener<Response> delegate, Runnable runAfter) { return new ActionListener<Response>() { @Override public void onResponse(Response response) { try { delegate.onResponse(response); } finally { runAfter.run(); } } @Override public void onFailure(Exception e) { try { delegate.onFailure(e); } finally { runAfter.run(); } } }; } /** * Wraps a given listener and returns a new listener which executes the provided {@code runBefore} * callback before the listener is notified via either {@code #onResponse} or {@code #onFailure}. * If the callback throws an exception then it will be passed to the listener's {@code #onFailure} and its {@code #onResponse}	maybe word this a little differently, this reads somewhat confusing (imo): the principle is that the mapped listener will handle exceptions from the mapping function {@code fn} but it is the responsibility of {@code delegate} to handle its own exceptions inside onresponse and onfailure. ?
public void testWriteLargeBlobStreaming() throws Exception { final boolean useTimeout = rarely(); final TimeValue readTimeout = useTimeout ? TimeValue.timeValueMillis(randomIntBetween(100, 500)) : null; final ByteSizeValue bufferSize = new ByteSizeValue(5, ByteSizeUnit.MB); final BlobContainer blobContainer = createBlobContainer(null, readTimeout, true, bufferSize); final int parts = randomIntBetween(1, 5); final long lastPartSize = randomLongBetween(10, 512); final long blobSize = (parts * bufferSize.getBytes()) + lastPartSize; final int nbErrors = 2; // we want all requests to fail at least once final CountDown countDownInitiate = new CountDown(nbErrors); final AtomicInteger counterUploads = new AtomicInteger(0); final AtomicLong bytesReceived = new AtomicLong(0L); final CountDown countDownComplete = new CountDown(nbErrors); httpServer.createContext("/bucket/write_large_blob", exchange -> { final long contentLength = Long.parseLong(exchange.getRequestHeaders().getFirst("Content-Length")); if ("POST".equals(exchange.getRequestMethod()) && exchange.getRequestURI().getQuery().equals("uploads")) { // initiate multipart upload request if (countDownInitiate.countDown()) { byte[] response = ("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>\\\\n" + "<InitiateMultipartUploadResult>\\\\n" + " <Bucket>bucket</Bucket>\\\\n" + " <Key>write_large_blob_streaming</Key>\\\\n" + " <UploadId>TEST</UploadId>\\\\n" + "</InitiateMultipartUploadResult>").getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(HttpStatus.SC_OK, response.length); exchange.getResponseBody().write(response); exchange.close(); return; } } else if ("PUT".equals(exchange.getRequestMethod()) && exchange.getRequestURI().getQuery().contains("uploadId=TEST") && exchange.getRequestURI().getQuery().contains("partNumber=")) { // upload part request MD5DigestCalculatingInputStream md5 = new MD5DigestCalculatingInputStream(exchange.getRequestBody()); BytesReference bytes = Streams.readFully(md5); if (counterUploads.incrementAndGet() % 2 == 0) { bytesReceived.addAndGet(bytes.length()); exchange.getResponseHeaders().add("ETag", Base16.encodeAsString(md5.getMd5Digest())); exchange.sendResponseHeaders(HttpStatus.SC_OK, -1); exchange.close(); return; } } else if ("POST".equals(exchange.getRequestMethod()) && exchange.getRequestURI().getQuery().equals("uploadId=TEST")) { // complete multipart upload request if (countDownComplete.countDown()) { Streams.readFully(exchange.getRequestBody()); byte[] response = ("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>\\\\n" + "<CompleteMultipartUploadResult>\\\\n" + " <Bucket>bucket</Bucket>\\\\n" + " <Key>write_large_blob_streaming</Key>\\\\n" + "</CompleteMultipartUploadResult>").getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(HttpStatus.SC_OK, response.length); exchange.getResponseBody().write(response); exchange.close(); return; } } // sends an error back or let the request time out if (useTimeout == false) { if (randomBoolean() && contentLength > 0) { Streams.readFully(exchange.getRequestBody(), new byte[randomIntBetween(1, Math.toIntExact(contentLength - 1))]); } else { Streams.readFully(exchange.getRequestBody()); exchange.sendResponseHeaders(randomFrom(HttpStatus.SC_INTERNAL_SERVER_ERROR, HttpStatus.SC_BAD_GATEWAY, HttpStatus.SC_SERVICE_UNAVAILABLE, HttpStatus.SC_GATEWAY_TIMEOUT), -1); } exchange.close(); } }); blobContainer.writeBlob("write_large_blob_streaming", false, randomBoolean(), out -> { final byte[] buffer = new byte[16 * 1024]; long outstanding = blobSize; while (outstanding > 0) { if (randomBoolean()) { int toWrite = Math.toIntExact(Math.min(randomIntBetween(64, buffer.length), outstanding)); out.write(buffer, 0, toWrite); outstanding -= toWrite; } else { out.write(0); outstanding--; } } }); assertEquals(blobSize, bytesReceived.get()); }	we should use a different http context path here
private void handleLoadSuccess(String modelId, Consumer consumer, TrainedModelConfig trainedModelConfig, InferenceDefinition inferenceDefinition) { Queue<ActionListener<LocalModel>> listeners; InferenceConfig inferenceConfig = trainedModelConfig.getInferenceConfig() == null ? inferenceConfigFromTargetType(inferenceDefinition.getTargetType()) : trainedModelConfig.getInferenceConfig(); LocalModel loadedModel = new LocalModel( trainedModelConfig.getModelId(), localNode, inferenceDefinition, trainedModelConfig.getInput(), trainedModelConfig.getDefaultFieldMap(), inferenceConfig, trainedModelConfig.getLicenseLevel(), modelStatsService, trainedModelCircuitBreaker); final ModelAndConsumerLoader modelAndConsumerLoader = new ModelAndConsumerLoader(new ModelAndConsumer(loadedModel, consumer)); synchronized (loadingListeners) { populateNewModelAlias(modelId); // If the model is referenced, that means it is currently in a pipeline somewhere // Also, if the consume is a search consumer, we should always cache it if (referencedModels.contains(modelId) || Sets.haveNonEmptyIntersection(modelIdToModelAliases.getOrDefault(modelId, new HashSet<>()), referencedModels) || consumer.equals(Consumer.SEARCH)) { try { // The local model may already be in cache. If it is, we don't bother adding it to cache. // If it isn't, we flip an `isLoaded` flag, and increment the model counter to make sure if it is evicted // between now and when the listeners access it, the circuit breaker reflects actual usage. localModelCache.computeIfAbsent(modelId, modelAndConsumerLoader); } catch (ExecutionException ee) { logger.warn(() -> new ParameterizedMessage("[{}] threw when attempting add to cache", modelId), ee); } shouldNotAudit.remove(modelId); } listeners = loadingListeners.remove(modelId); // if there are no listeners, we should just exit if (listeners == null) { // If we newly added it into cache, release the model so that the circuit breaker can still accurately keep track // of memory if(modelAndConsumerLoader.isLoaded()) { loadedModel.release(); } return; } } // synchronized (loadingListeners) for (ActionListener<LocalModel> listener = listeners.poll(); listener != null; listener = listeners.poll()) { loadedModel.acquire(); listener.onResponse(loadedModel); } // account for the acquire in the synchronized block above if the model was loaded into the cache if (modelAndConsumerLoader.isLoaded()) { loadedModel.release(); } }	@davidkyle this was a bit tricky. it is possible that we don't have any listeners, but still load something into cache. this would usually be due to loading a new model via a name change and not a new reference and during that time, no callers are requesting that model. also, i noticed in testing that we were unnecessarily evicting models that were accidentally cached twice (the second cache evicts the first model cached). this causes weird logging and is ultimately unnecessary if we use the computeifabsent logic in the model cache. i ran this a bunch locally and it is all checking out ok. ci and should time agree.
public void fetchLeaderHistoryUUIDs( final Client leaderClient, final IndexMetaData leaderIndexMetaData, final Consumer<Exception> onFailure, final Consumer<String[]> historyUUIDConsumer) { String leaderIndex = leaderIndexMetaData.getIndex().getName(); CheckedConsumer<IndicesStatsResponse, Exception> indicesStatsHandler = indicesStatsResponse -> { IndexStats indexStats = indicesStatsResponse.getIndices().get(leaderIndex); if (indexStats == null) { onFailure.accept(new IllegalArgumentException("no index shards available, is the leader index red?")); return; } String[] historyUUIDs = new String[leaderIndexMetaData.getNumberOfShards()]; for (IndexShardStats indexShardStats : indexStats) { for (ShardStats shardStats : indexShardStats) { // Ignore replica shards as they may not have yet started and // we just end up overwriting slots in historyUUIDs if (shardStats.getShardRouting().primary() == false) { continue; } CommitStats commitStats = shardStats.getCommitStats(); if (commitStats == null) { onFailure.accept(new IllegalArgumentException("leader index's commit stats are missing")); return; } String historyUUID = commitStats.getUserData().get(Engine.HISTORY_UUID_KEY); ShardId shardId = shardStats.getShardRouting().shardId(); historyUUIDs[shardId.id()] = historyUUID; } } for (int i = 0; i < historyUUIDs.length; i++) { if (historyUUIDs[i] == null) { onFailure.accept(new IllegalArgumentException("no history uuid for [" + leaderIndex + "][" + i + "]")); return; } } historyUUIDConsumer.accept(historyUUIDs); }; IndicesStatsRequest request = new IndicesStatsRequest(); request.clear(); request.indices(leaderIndex); leaderClient.admin().indices().stats(request, ActionListener.wrap(indicesStatsHandler, onFailure)); }	how about just saying - no index stats available for the leader index ? the leader index can be red when missing just one primary and you'd still have stats. also maybe something else caused this and we're just misleading people? i prefer to keep this simple.
* @param allocationId allocation id of the shard to fail * @param primaryTerm the primary term associated with the primary shard that is failing the shard. Must be strictly positive. * @param markAsStale whether or not to mark a failing shard as stale (eg. removing from in-sync set) when failing the shard. * @param message the reason for the failure * @param failure the underlying cause of the failure * @param listener callback upon completion of the request */ public void remoteShardFailed( final ShardId shardId, String allocationId, long primaryTerm, boolean markAsStale, final String message, @Nullable final Exception failure, ActionListener<Void> listener ) { assert primaryTerm > 0L : "primary term should be strictly positive"; remoteShardStateUpdateDeduplicator.executeOnce( new FailedShardEntry(shardId, allocationId, primaryTerm, message, failure, markAsStale), listener, (req, reqListener) -> sendShardAction(SHARD_FAILED_ACTION_NAME, clusterService.state(), req, reqListener) ); }	since we call this from multiple threads, there is a bit of best-effort over this method, i think that is worth documenting. for instance, this may clear out a remote shard failed request deduplication to the new master in edge cases. this does no real harm, since we still protect the master.
public void testHandleExceptionOnSendFiles() throws Throwable { final RecoverySettings recoverySettings = new RecoverySettings(Settings.EMPTY, service); final StartRecoveryRequest request = getStartRecoveryRequest(); Path tempDir = createTempDir(); Store store = newStore(tempDir, false); AtomicBoolean failedEngine = new AtomicBoolean(false); Directory dir = store.directory(); RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig()); int numDocs = randomIntBetween(10, 100); for (int i = 0; i < numDocs; i++) { Document document = new Document(); document.add(new StringField("id", Integer.toString(i), Field.Store.YES)); document.add(newField("field", randomUnicodeOfCodepointLengthBetween(1, 10), TextField.TYPE_STORED)); writer.addDocument(document); } writer.commit(); writer.close(); Store.MetadataSnapshot metadata = store.getMetadata(null); List<StoreFileMetaData> metas = new ArrayList<>(); for (StoreFileMetaData md : metadata) { metas.add(md); } final boolean throwCorruptedIndexException = randomBoolean(); RecoveryTargetHandler target = new TestRecoveryTargetHandler() { @Override public void writeFileChunk(StoreFileMetaData md, long position, BytesReference content, boolean lastChunk, int totalTranslogOps, ActionListener<Void> listener) { if (throwCorruptedIndexException) { listener.onFailure(new RuntimeException(new CorruptIndexException("foo", "bar"))); } else { listener.onFailure(new RuntimeException("boom")); } } }; RecoverySourceHandler handler = new RecoverySourceHandler(null, target, request, Math.toIntExact(recoverySettings.getChunkSize().getBytes()), between(1, 10)) { @Override protected void failEngine(IOException cause) { assertFalse(failedEngine.get()); failedEngine.set(true); } }; try { handler.sendFiles(store, metas.toArray(new StoreFileMetaData[0]), () -> 0); fail("exception index"); } catch (RuntimeException ex) { assertNotNull(ExceptionsHelper.unwrapCorruption(ex)); if (throwCorruptedIndexException) { assertEquals(ex.getMessage(), "[File corruption occurred on recovery but checksums are ok]"); } else { assertEquals(ex.getMessage(), "boom"); } } catch (CorruptIndexException ex) { fail("not expected here"); } assertFalse(failedEngine.get()); IOUtils.close(store); }	this now finds the underlying corruption exception that is supressed on the runtimeexception
public void onResponse(Map<String, List<SnapshotInfo>> allSnapshots) { if (logger.isTraceEnabled()) { logger.trace("retrieved snapshots: [{}]", formatSnapshots(allSnapshots)); } // Find all the snapshots that are past their retention date final Map<String, List<Tuple<SnapshotId, String>>> snapshotsToBeDeleted = allSnapshots.entrySet().stream() .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().stream() .filter(snapshot -> snapshotEligibleForDeletion(snapshot, allSnapshots, policiesWithRetention)) .map(snapshotInfo -> Tuple.tuple(snapshotInfo.snapshotId(), getPolicyId(snapshotInfo))) .collect(Collectors.toList()))); if (logger.isTraceEnabled()) { logger.trace("snapshots eligible for deletion: [{}]", snapshotsToBeDeleted); } // Finally, delete the snapshots that need to be deleted deleteSnapshots(snapshotsToBeDeleted, slmStats, ActionListener.wrap(() -> { updateStateWithStats(slmStats); logger.info("SLM retention snapshot cleanup task complete"); })); }	can you add a blurb here about why we do this so it doesn't get accidentally removed in the future?
@Before public void initAndResetContext() throws Exception { if (restTestExecutionContext == null) { assert adminExecutionContext == null; assert blacklistPathMatchers == null; final ClientYamlSuiteRestSpec restSpec = ClientYamlSuiteRestSpec.load(SPEC_PATH); validateSpec(restSpec); final List<HttpHost> hosts = getClusterHosts(); Tuple<Version, Version> versionVersionTuple = readVersionsFromCatNodes(adminClient()); final Version esVersion = versionVersionTuple.v1(); final Version masterVersion = versionVersionTuple.v2(); logger.info("initializing client, minimum es version [{}], master version, [{}], hosts {}", esVersion, masterVersion, hosts); final ClientYamlTestClient clientYamlTestClient = initClientYamlTestClient(restSpec, client(), hosts, esVersion, masterVersion); restTestExecutionContext = new ClientYamlTestExecutionContext(clientYamlTestClient, randomizeContentType()); adminExecutionContext = new ClientYamlTestExecutionContext(clientYamlTestClient, false); final String[] blacklist = resolvePathsProperty(REST_TESTS_BLACKLIST, null); blacklistPathMatchers = new ArrayList<>(); for (final String entry : blacklist) { blacklistPathMatchers.add(new BlacklistedPathPatternMatcher(entry)); } } assert restTestExecutionContext != null; assert adminExecutionContext != null; assert blacklistPathMatchers != null; // admin context must be available for @After always, regardless of whether the test was blacklisted adminExecutionContext.clear(); restTestExecutionContext.clear(); }	isn't the catch needed anymore? not too familiar with it though, not sure why it's there
@Before public void initAndResetContext() throws Exception { if (restTestExecutionContext == null) { assert adminExecutionContext == null; assert blacklistPathMatchers == null; final ClientYamlSuiteRestSpec restSpec = ClientYamlSuiteRestSpec.load(SPEC_PATH); validateSpec(restSpec); final List<HttpHost> hosts = getClusterHosts(); Tuple<Version, Version> versionVersionTuple = readVersionsFromCatNodes(adminClient()); final Version esVersion = versionVersionTuple.v1(); final Version masterVersion = versionVersionTuple.v2(); logger.info("initializing client, minimum es version [{}], master version, [{}], hosts {}", esVersion, masterVersion, hosts); final ClientYamlTestClient clientYamlTestClient = initClientYamlTestClient(restSpec, client(), hosts, esVersion, masterVersion); restTestExecutionContext = new ClientYamlTestExecutionContext(clientYamlTestClient, randomizeContentType()); adminExecutionContext = new ClientYamlTestExecutionContext(clientYamlTestClient, false); final String[] blacklist = resolvePathsProperty(REST_TESTS_BLACKLIST, null); blacklistPathMatchers = new ArrayList<>(); for (final String entry : blacklist) { blacklistPathMatchers.add(new BlacklistedPathPatternMatcher(entry)); } } assert restTestExecutionContext != null; assert adminExecutionContext != null; assert blacklistPathMatchers != null; // admin context must be available for @After always, regardless of whether the test was blacklisted adminExecutionContext.clear(); restTestExecutionContext.clear(); }	note here that we use the admin client instead of the plain client. i *think* this means that we do not need the try/catch block anymore.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final boolean namesProvided = request.hasParam("name"); final String[] aliases = request.paramAsStringArrayOrEmptyIfAll("name"); final GetAliasesRequest getAliasesRequest = new GetAliasesRequest(aliases); final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); getAliasesRequest.indices(indices); getAliasesRequest.indicesOptions(IndicesOptions.fromRequest(request, getAliasesRequest.indicesOptions())); getAliasesRequest.local(request.paramAsBoolean("local", getAliasesRequest.local())); return channel -> client.admin().indices().getAliases(getAliasesRequest, new RestBuilderListener<GetAliasesResponse>(channel) { @Override public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final Set<String> indicesToDisplay = new HashSet<>(); final Set<String> returnedAliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : response.getAliases()) { for (final AliasMetaData aliasMetaData : cursor.value) { if (namesProvided) { // display only indices with no aliases indicesToDisplay.add(cursor.key); } returnedAliasNames.add(aliasMetaData.alias()); } } // compute explicitly requested aliases that have not been found final SortedSet<String> missingAliases = new TreeSet<>(); for (int i = 0; i < aliases.length; i++) { if (MetaData.ALL.equals(aliases[i]) || Regex.isSimpleMatchPattern(aliases[i]) || aliases[i].charAt(0) == '-') { // only explicitly requested aliases will be returning 404 continue; } int j = i + 1; for (; j < aliases.length; j++) { if (aliases[j].charAt(0) == '-' && (Regex.isSimpleMatchPattern(aliases[j].substring(1)) || MetaData.ALL.equals(aliases[j].substring(1)))) { // this is an exclude pattern if (Regex.simpleMatch(aliases[j].substring(1), aliases[i]) || MetaData.ALL.equals(aliases[j].substring(1))) { break; } } } if (j == aliases.length) { // explicitly requested alias not excluded by any "-" wildcard in expression if (false == returnedAliasNames.contains(aliases[i])) { missingAliases.add(aliases[i]); } } } final RestStatus status; builder.startObject(); { if (missingAliases.isEmpty()) { status = RestStatus.OK; } else { status = RestStatus.NOT_FOUND; final String message; if (missingAliases.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", Strings.collectionToCommaDelimitedString(missingAliases)); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", Strings.collectionToCommaDelimitedString(missingAliases)); } builder.field("error", message); builder.field("status", status.getStatus()); } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { if (namesProvided == false || (namesProvided && indicesToDisplay.contains(entry.key))) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } } builder.endObject(); return new BytesRestResponse(status, builder); } }); }	why remove this comment?
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final boolean namesProvided = request.hasParam("name"); final String[] aliases = request.paramAsStringArrayOrEmptyIfAll("name"); final GetAliasesRequest getAliasesRequest = new GetAliasesRequest(aliases); final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); getAliasesRequest.indices(indices); getAliasesRequest.indicesOptions(IndicesOptions.fromRequest(request, getAliasesRequest.indicesOptions())); getAliasesRequest.local(request.paramAsBoolean("local", getAliasesRequest.local())); return channel -> client.admin().indices().getAliases(getAliasesRequest, new RestBuilderListener<GetAliasesResponse>(channel) { @Override public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final Set<String> indicesToDisplay = new HashSet<>(); final Set<String> returnedAliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : response.getAliases()) { for (final AliasMetaData aliasMetaData : cursor.value) { if (namesProvided) { // display only indices with no aliases indicesToDisplay.add(cursor.key); } returnedAliasNames.add(aliasMetaData.alias()); } } // compute explicitly requested aliases that have not been found final SortedSet<String> missingAliases = new TreeSet<>(); for (int i = 0; i < aliases.length; i++) { if (MetaData.ALL.equals(aliases[i]) || Regex.isSimpleMatchPattern(aliases[i]) || aliases[i].charAt(0) == '-') { // only explicitly requested aliases will be returning 404 continue; } int j = i + 1; for (; j < aliases.length; j++) { if (aliases[j].charAt(0) == '-' && (Regex.isSimpleMatchPattern(aliases[j].substring(1)) || MetaData.ALL.equals(aliases[j].substring(1)))) { // this is an exclude pattern if (Regex.simpleMatch(aliases[j].substring(1), aliases[i]) || MetaData.ALL.equals(aliases[j].substring(1))) { break; } } } if (j == aliases.length) { // explicitly requested alias not excluded by any "-" wildcard in expression if (false == returnedAliasNames.contains(aliases[i])) { missingAliases.add(aliases[i]); } } } final RestStatus status; builder.startObject(); { if (missingAliases.isEmpty()) { status = RestStatus.OK; } else { status = RestStatus.NOT_FOUND; final String message; if (missingAliases.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", Strings.collectionToCommaDelimitedString(missingAliases)); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", Strings.collectionToCommaDelimitedString(missingAliases)); } builder.field("error", message); builder.field("status", status.getStatus()); } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { if (namesProvided == false || (namesProvided && indicesToDisplay.contains(entry.key))) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } } builder.endObject(); return new BytesRestResponse(status, builder); } }); }	no substantial change was made up until here, or am i missing something?
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final boolean namesProvided = request.hasParam("name"); final String[] aliases = request.paramAsStringArrayOrEmptyIfAll("name"); final GetAliasesRequest getAliasesRequest = new GetAliasesRequest(aliases); final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); getAliasesRequest.indices(indices); getAliasesRequest.indicesOptions(IndicesOptions.fromRequest(request, getAliasesRequest.indicesOptions())); getAliasesRequest.local(request.paramAsBoolean("local", getAliasesRequest.local())); return channel -> client.admin().indices().getAliases(getAliasesRequest, new RestBuilderListener<GetAliasesResponse>(channel) { @Override public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final Set<String> indicesToDisplay = new HashSet<>(); final Set<String> returnedAliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : response.getAliases()) { for (final AliasMetaData aliasMetaData : cursor.value) { if (namesProvided) { // display only indices with no aliases indicesToDisplay.add(cursor.key); } returnedAliasNames.add(aliasMetaData.alias()); } } // compute explicitly requested aliases that have not been found final SortedSet<String> missingAliases = new TreeSet<>(); for (int i = 0; i < aliases.length; i++) { if (MetaData.ALL.equals(aliases[i]) || Regex.isSimpleMatchPattern(aliases[i]) || aliases[i].charAt(0) == '-') { // only explicitly requested aliases will be returning 404 continue; } int j = i + 1; for (; j < aliases.length; j++) { if (aliases[j].charAt(0) == '-' && (Regex.isSimpleMatchPattern(aliases[j].substring(1)) || MetaData.ALL.equals(aliases[j].substring(1)))) { // this is an exclude pattern if (Regex.simpleMatch(aliases[j].substring(1), aliases[i]) || MetaData.ALL.equals(aliases[j].substring(1))) { break; } } } if (j == aliases.length) { // explicitly requested alias not excluded by any "-" wildcard in expression if (false == returnedAliasNames.contains(aliases[i])) { missingAliases.add(aliases[i]); } } } final RestStatus status; builder.startObject(); { if (missingAliases.isEmpty()) { status = RestStatus.OK; } else { status = RestStatus.NOT_FOUND; final String message; if (missingAliases.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", Strings.collectionToCommaDelimitedString(missingAliases)); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", Strings.collectionToCommaDelimitedString(missingAliases)); } builder.field("error", message); builder.field("status", status.getStatus()); } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { if (namesProvided == false || (namesProvided && indicesToDisplay.contains(entry.key))) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } } builder.endObject(); return new BytesRestResponse(status, builder); } }); }	i am not following what changes were made here, can you maybe explain? i was expecting changes needed only in indicesandaliasesresolver to add support for pattern exclusion, why do we need to make changes to the transport action too?
public void onNewInfo(ClusterInfo info) { if (checkInProgress.compareAndSet(false, true) == false) { logger.info("skipping monitor as a check is already in progress"); return; } final ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages(); if (usages == null) { checkFinished(); return; } boolean reroute = false; String explanation = ""; final long currentTimeMillis = currentTimeMillisSupplier.getAsLong(); // Garbage collect nodes that have been removed from the cluster // from the map that tracks watermark crossing final ObjectLookupContainer<String> nodes = usages.keys(); for (String node : nodeHasPassedWatermark) { if (nodes.contains(node) == false) { nodeHasPassedWatermark.remove(node); } } final ClusterState state = clusterStateSupplier.get(); final Set<String> indicesToMarkReadOnly = new HashSet<>(); RoutingNodes routingNodes = state.getRoutingNodes(); Set<String> indicesToMarkIneligibleForAutoRelease = new HashSet<>(); //Ensure we release indices on nodes that have a usage response from node stats markNodesMissingUsageIneligibleForRelease(routingNodes, usages, indicesToMarkIneligibleForAutoRelease); for (final ObjectObjectCursor<String, DiskUsage> entry : usages) { final String node = entry.key; final DiskUsage usage = entry.value; warnAboutDiskIfNeeded(usage); RoutingNode routingNode = routingNodes.node(node); // Only unblock index if all nodes that contain shards of it are below the high disk watermark if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { markIneligiblityForAutoRelease(routingNode, indicesToMarkIneligibleForAutoRelease); } if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) { if (routingNode != null) { // this might happen if we haven't got the full cluster-state yet?! for (ShardRouting routing : routingNode) { indicesToMarkReadOnly.add(routing.index().getName()); } } } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { if (lastRunTimeMillis.get() < currentTimeMillis - diskThresholdSettings.getRerouteInterval().millis()) { reroute = true; explanation = "high disk watermark exceeded on one or more nodes"; } else { logger.debug("high disk watermark exceeded on {} but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } nodeHasPassedWatermark.add(node); } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) { nodeHasPassedWatermark.add(node); } else { if (nodeHasPassedWatermark.contains(node)) { // The node has previously been over the high or // low watermark, but is no longer, so we should // reroute so any unassigned shards can be allocated // if they are able to be if (lastRunTimeMillis.get() < currentTimeMillis - diskThresholdSettings.getRerouteInterval().millis()) { reroute = true; explanation = "one or more nodes has gone under the high or low watermark"; nodeHasPassedWatermark.remove(node); } else { logger.debug("{} has gone below a disk threshold, but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } } } } final ActionListener<Void> listener = new GroupedActionListener<>(ActionListener.wrap(this::checkFinished), 3); if (reroute) { logger.info("rerouting shards: [{}]", explanation); rerouteService.reroute("disk threshold monitor", ActionListener.wrap(r -> { setLastRunTimeMillis(); listener.onResponse(r); }, e -> { logger.debug("reroute failed", e); setLastRunTimeMillis(); listener.onFailure(e); })); } else { listener.onResponse(null); } // Get set of indices that are eligible to be automatically unblocked // Only collect indices that are currently blocked final String[] indices = state.routingTable().indicesRouting().keys().toArray(String.class); Set<String> indicesToAutoRelease = Arrays.stream(indices) .filter(index -> indicesToMarkIneligibleForAutoRelease.contains(index) == false) .filter(index -> state.getBlocks().hasIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_ALLOW_DELETE_BLOCK)) .collect(Collectors.toCollection(HashSet::new)); if (indicesToAutoRelease.isEmpty() == false) { if (diskThresholdSettings.isAutoReleaseIndexEnabled()) { logger.info("Releasing read-only allow delete block on indices: [{}]", indicesToAutoRelease); updateIndicesReadOnly(indicesToAutoRelease, listener, false); } else { deprecationLogger.deprecated("[{}] will be removed in 8.0.0", DiskThresholdSettings.AUTO_RELEASE_INDEX_ENABLED_KEY); listener.onResponse(null); } } else { listener.onResponse(null); } indicesToMarkReadOnly.removeIf(index -> state.getBlocks().indexBlocked(ClusterBlockLevel.WRITE, index)); if (indicesToMarkReadOnly.isEmpty() == false) { updateIndicesReadOnly(indicesToMarkReadOnly, listener, true); } else { listener.onResponse(null); } }	no real need for this comment, the method name says it all: suggestion
public void onNewInfo(ClusterInfo info) { if (checkInProgress.compareAndSet(false, true) == false) { logger.info("skipping monitor as a check is already in progress"); return; } final ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages(); if (usages == null) { checkFinished(); return; } boolean reroute = false; String explanation = ""; final long currentTimeMillis = currentTimeMillisSupplier.getAsLong(); // Garbage collect nodes that have been removed from the cluster // from the map that tracks watermark crossing final ObjectLookupContainer<String> nodes = usages.keys(); for (String node : nodeHasPassedWatermark) { if (nodes.contains(node) == false) { nodeHasPassedWatermark.remove(node); } } final ClusterState state = clusterStateSupplier.get(); final Set<String> indicesToMarkReadOnly = new HashSet<>(); RoutingNodes routingNodes = state.getRoutingNodes(); Set<String> indicesToMarkIneligibleForAutoRelease = new HashSet<>(); //Ensure we release indices on nodes that have a usage response from node stats markNodesMissingUsageIneligibleForRelease(routingNodes, usages, indicesToMarkIneligibleForAutoRelease); for (final ObjectObjectCursor<String, DiskUsage> entry : usages) { final String node = entry.key; final DiskUsage usage = entry.value; warnAboutDiskIfNeeded(usage); RoutingNode routingNode = routingNodes.node(node); // Only unblock index if all nodes that contain shards of it are below the high disk watermark if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { markIneligiblityForAutoRelease(routingNode, indicesToMarkIneligibleForAutoRelease); } if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) { if (routingNode != null) { // this might happen if we haven't got the full cluster-state yet?! for (ShardRouting routing : routingNode) { indicesToMarkReadOnly.add(routing.index().getName()); } } } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) { if (lastRunTimeMillis.get() < currentTimeMillis - diskThresholdSettings.getRerouteInterval().millis()) { reroute = true; explanation = "high disk watermark exceeded on one or more nodes"; } else { logger.debug("high disk watermark exceeded on {} but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } nodeHasPassedWatermark.add(node); } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes() || usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) { nodeHasPassedWatermark.add(node); } else { if (nodeHasPassedWatermark.contains(node)) { // The node has previously been over the high or // low watermark, but is no longer, so we should // reroute so any unassigned shards can be allocated // if they are able to be if (lastRunTimeMillis.get() < currentTimeMillis - diskThresholdSettings.getRerouteInterval().millis()) { reroute = true; explanation = "one or more nodes has gone under the high or low watermark"; nodeHasPassedWatermark.remove(node); } else { logger.debug("{} has gone below a disk threshold, but an automatic reroute has occurred " + "in the last [{}], skipping reroute", node, diskThresholdSettings.getRerouteInterval()); } } } } final ActionListener<Void> listener = new GroupedActionListener<>(ActionListener.wrap(this::checkFinished), 3); if (reroute) { logger.info("rerouting shards: [{}]", explanation); rerouteService.reroute("disk threshold monitor", ActionListener.wrap(r -> { setLastRunTimeMillis(); listener.onResponse(r); }, e -> { logger.debug("reroute failed", e); setLastRunTimeMillis(); listener.onFailure(e); })); } else { listener.onResponse(null); } // Get set of indices that are eligible to be automatically unblocked // Only collect indices that are currently blocked final String[] indices = state.routingTable().indicesRouting().keys().toArray(String.class); Set<String> indicesToAutoRelease = Arrays.stream(indices) .filter(index -> indicesToMarkIneligibleForAutoRelease.contains(index) == false) .filter(index -> state.getBlocks().hasIndexBlock(index, IndexMetaData.INDEX_READ_ONLY_ALLOW_DELETE_BLOCK)) .collect(Collectors.toCollection(HashSet::new)); if (indicesToAutoRelease.isEmpty() == false) { if (diskThresholdSettings.isAutoReleaseIndexEnabled()) { logger.info("Releasing read-only allow delete block on indices: [{}]", indicesToAutoRelease); updateIndicesReadOnly(indicesToAutoRelease, listener, false); } else { deprecationLogger.deprecated("[{}] will be removed in 8.0.0", DiskThresholdSettings.AUTO_RELEASE_INDEX_ENABLED_KEY); listener.onResponse(null); } } else { listener.onResponse(null); } indicesToMarkReadOnly.removeIf(index -> state.getBlocks().indexBlocked(ClusterBlockLevel.WRITE, index)); if (indicesToMarkReadOnly.isEmpty() == false) { updateIndicesReadOnly(indicesToMarkReadOnly, listener, true); } else { listener.onResponse(null); } }	as far as i can tell this does essentially the same thing as the loop on lines 161-163. indicestomarkineligibleforautorelease ends up being indicestomarkreadonly plus any indices with shards on nodes for whom we don't know the disk usage. i think it'd be simpler to use this fact in the calculation of indicestoautorelease below rather than constructing these two almost-identical sets.
@Override protected void updateIndicesReadOnly(Set<String> indicesToMarkReadOnly, ActionListener<Void> listener, boolean readOnly) { assertTrue(indices.compareAndSet(null, indicesToMarkReadOnly)); listener.onResponse(null); }	please assert that the new readonly parameter is as expected.
public void testMarkFloodStageIndicesReadOnly() { Settings settings = Settings.EMPTY; ClusterState clusterState = bootstrapCluster(); ClusterState finalState = clusterState; AtomicBoolean reroute = new AtomicBoolean(false); AtomicReference<Set<String>> indices = new AtomicReference<>(); AtomicLong currentTime = new AtomicLong(); DiskThresholdMonitor monitor = new DiskThresholdMonitor(settings, () -> finalState, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), null, currentTime::get, (reason, listener) -> { assertTrue(reroute.compareAndSet(false, true)); listener.onResponse(null); }) { @Override protected void updateIndicesReadOnly(Set<String> indicesToMarkReadOnly, ActionListener<Void> listener, boolean readOnly) { assertTrue(indices.compareAndSet(null, indicesToMarkReadOnly)); listener.onResponse(null); } }; ImmutableOpenMap.Builder<String, DiskUsage> builder = ImmutableOpenMap.builder(); builder.put("node1", new DiskUsage("node1","node1", "/foo/bar", 100, 4)); builder.put("node2", new DiskUsage("node2","node2", "/foo/bar", 100, 30)); monitor.onNewInfo(new ClusterInfo(builder.build(), null, null, null)); assertFalse(reroute.get()); assertEquals(new HashSet<>(Arrays.asList("test_1", "test_2")), indices.get()); indices.set(null); builder = ImmutableOpenMap.builder(); builder.put("node1", new DiskUsage("node1","node1", "/foo/bar", 100, 4)); builder.put("node2", new DiskUsage("node2","node2", "/foo/bar", 100, 5)); currentTime.addAndGet(randomLongBetween(60001, 120000)); monitor.onNewInfo(new ClusterInfo(builder.build(), null, null, null)); assertTrue(reroute.get()); assertEquals(new HashSet<>(Arrays.asList("test_1", "test_2")), indices.get()); IndexMetaData indexMetaData = IndexMetaData.builder(clusterState.metaData().index("test_2")).settings(Settings.builder() .put(clusterState.metaData() .index("test_2").getSettings()) .put(IndexMetaData.INDEX_BLOCKS_READ_ONLY_ALLOW_DELETE_SETTING.getKey(), true)).build(); // now we mark one index as read-only and assert that we don't mark it as such again final ClusterState anotherFinalClusterState = ClusterState.builder(clusterState).metaData(MetaData.builder(clusterState.metaData()) .put(clusterState.metaData().index("test"), false) .put(clusterState.metaData().index("test_1"), false) .put(indexMetaData, true).build()) .blocks(ClusterBlocks.builder().addBlocks(indexMetaData).build()).build(); assertTrue(anotherFinalClusterState.blocks().indexBlocked(ClusterBlockLevel.WRITE, "test_2")); monitor = new DiskThresholdMonitor(settings, () -> anotherFinalClusterState, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), null, currentTime::get, (reason, listener) -> { assertTrue(reroute.compareAndSet(false, true)); listener.onResponse(null); }) { @Override protected void updateIndicesReadOnly(Set<String> indicesToMarkReadOnly, ActionListener<Void> listener, boolean readOnly) { assertTrue(indices.compareAndSet(null, indicesToMarkReadOnly)); listener.onResponse(null); } }; indices.set(null); reroute.set(false); builder = ImmutableOpenMap.builder(); builder.put("node1", new DiskUsage("node1","node1", "/foo/bar", 100, 4)); builder.put("node2", new DiskUsage("node2","node2", "/foo/bar", 100, 5)); monitor.onNewInfo(new ClusterInfo(builder.build(), null, null, null)); assertTrue(reroute.get()); assertEquals(Collections.singleton("test_1"), indices.get()); }	no need for finalstate any more, it's always equal to clusterstate (which is now effectively final).
@Override protected void updateIndicesReadOnly(Set<String> indicesToMarkReadOnly, ActionListener<Void> listener, boolean readOnly) { assertTrue(indices.compareAndSet(null, indicesToMarkReadOnly)); listener.onResponse(null); }	please assert that the new readonly parameter is as expected.
private static boolean searchWithCollectorManager(SearchContext searchContext, ContextIndexSearcher searcher, Query query, CheckedConsumer<List<LeafReaderContext>, IOException> leafSorter, boolean timeoutSet) throws IOException { final IndexReader reader = searchContext.searcher().getIndexReader(); final int numHits = Math.min(searchContext.from() + searchContext.size(), Math.max(1, reader.numDocs())); final SortAndFormats sortAndFormats = searchContext.sort(); int totalHitsThreshold; TotalHits totalHits; if (searchContext.trackTotalHitsUpTo() == SearchContext.TRACK_TOTAL_HITS_DISABLED) { totalHitsThreshold = 1; totalHits = new TotalHits(0, TotalHits.Relation.GREATER_THAN_OR_EQUAL_TO); } else { int hitCount = shortcutTotalHitCount(reader, query); if (hitCount == -1) { totalHitsThreshold = searchContext.trackTotalHitsUpTo(); totalHits = null; // will be computed via the collector } else { totalHitsThreshold = 1; totalHits = new TotalHits(hitCount, TotalHits.Relation.EQUAL_TO); // don't compute hit counts via the collector } } CollectorManager<TopFieldCollector, TopFieldDocs> sharedManager = TopFieldCollector.createSharedManager( sortAndFormats.sort, numHits, null, totalHitsThreshold); List<LeafReaderContext> leaves = new ArrayList<>(searcher.getIndexReader().leaves()); leafSorter.accept(leaves); try { Weight weight = searcher.createWeight(searcher.rewrite(query), ScoreMode.TOP_SCORES, 1f); searcher.search(leaves, weight, sharedManager, searchContext.queryResult(), sortAndFormats.formats, totalHits); } catch (TimeExceededException e) { assert timeoutSet : "TimeExceededException thrown even though timeout wasn't set"; if (searchContext.request().allowPartialSearchResults() == false) { // Can't rethrow TimeExceededException because not serializable throw new QueryPhaseExecutionException(searchContext.shardTarget(), "Time exceeded"); } searchContext.queryResult().searchTimedOut(true); } finally { searchContext.clearReleasables(SearchContext.Lifetime.COLLECTION); } return false; // no rescoring when sorting by field }	maybe leave a todo about this one, it'd be nice to be able to handle it in a followup
private static Query tryRewriteLongSort(SearchContext searchContext, IndexReader reader, Query query, boolean hasFilterCollector) throws IOException { if (searchContext.searchAfter() != null) return null; if (searchContext.scrollContext() != null) return null; if (searchContext.collapse() != null) return null; if (searchContext.trackScores()) return null; if (searchContext.aggregations() != null) return null; Sort sort = searchContext.sort().sort; SortField sortField = sort.getSort()[0]; if (SortField.Type.LONG.equals(IndexSortConfig.getSortFieldType(sortField)) == false) return null; // check if this is a field of type Long or Date, that is indexed and has doc values String fieldName = sortField.getField(); if (fieldName == null) return null; // happens when _score or _doc is the 1st sort field if (searchContext.mapperService() == null) return null; // mapperService can be null in tests final MappedFieldType fieldType = searchContext.mapperService().fullName(fieldName); if (fieldType == null) return null; // for unmapped fields, default behaviour depending on "unmapped_type" flag if ((fieldType.typeName().equals("long") == false) && (fieldType instanceof DateFieldType == false)) return null; if (fieldType.indexOptions() == IndexOptions.NONE) return null; //TODO: change to pointDataDimensionCount() when implemented if (fieldType.hasDocValues() == false) return null; // check that all sorts are actual document fields or _doc for (int i = 1; i < sort.getSort().length; i++) { SortField sField = sort.getSort()[i]; String sFieldName = sField.getField(); if (sFieldName == null) { if (SortField.FIELD_DOC.equals(sField) == false) return null; } else { if (searchContext.mapperService().fullName(sFieldName) == null) return null; // could be _script field that uses _score } } // check that setting of missing values allows optimization if (sortField.getMissingValue() == null) return null; Long missingValue = (Long) sortField.getMissingValue(); boolean missingValuesAccordingToSort = (sortField.getReverse() && (missingValue == Long.MIN_VALUE)) || ((sortField.getReverse() == false) && (missingValue == Long.MAX_VALUE)); if (missingValuesAccordingToSort == false) return null; int docCount = PointValues.getDocCount(reader, fieldName); // is not worth to run optimization on small index if (docCount <= 512) return null; // check for multiple values if (PointValues.size(reader, fieldName) != docCount) return null; //TODO: handle multiple values // check if the optimization makes sense with the track_total_hits setting if (searchContext.trackTotalHitsUpTo() == Integer.MAX_VALUE) { // with filter, we can't pre-calculate hitsCount, we need to explicitly calculate them => optimization does't make sense if (hasFilterCollector) return null; // if we can't pre-calculate hitsCount based on the query type, optimization does't make sense if (shortcutTotalHitCount(reader, query) == -1) return null; } byte[] minValueBytes = PointValues.getMinPackedValue(reader, fieldName); byte[] maxValueBytes = PointValues.getMaxPackedValue(reader, fieldName); if ((maxValueBytes == null) || (minValueBytes == null)) return null; long minValue = LongPoint.decodeDimension(minValueBytes, 0); long maxValue = LongPoint.decodeDimension(maxValueBytes, 0); Query rewrittenQuery; if (minValue == maxValue) { rewrittenQuery = new DocValuesFieldExistsQuery(fieldName); } else { if (indexFieldHasDuplicateData(reader, fieldName)) return null; long origin = (sortField.getReverse()) ? maxValue : minValue; long pivotDistance = (maxValue - minValue) >>> 1; // division by 2 on the unsigned representation to avoid overflow if (pivotDistance == 0) { // 0 if maxValue = (minValue + 1) pivotDistance = 1; } rewrittenQuery = LongPoint.newDistanceFeatureQuery(sortField.getField(), 1, origin, pivotDistance); } rewrittenQuery = new BooleanQuery.Builder() .add(query, BooleanClause.Occur.FILTER) // filter for original query .add(rewrittenQuery, BooleanClause.Occur.SHOULD) //should for rewrittenQuery .build(); return rewrittenQuery; } /** * Creates a sorter of {@link LeafReaderContext}	could we use sortfield.needsscores to cover scripted fields?
public void writeVInt(int i) throws IOException { /* * Pick the number of bytes that we need based on the value and then * encode the int, unrolling the loops by hand. This allows writing * small numbers to use `writeByte` which is simple and fast. The * unrolling saves a few comparisons and bitwise operations. All * together this saves quite a bit of time compared to a naive * implementation. */ switch (Integer.numberOfLeadingZeros(i)) { case 32: case 31: case 30: case 29: case 28: case 27: case 26: case 25: writeByte((byte) i); return; case 24: case 23: case 22: case 21: case 20: case 19: case 18: byte[] buffer = scratch.get(); buffer[0] = (byte) (i & 0x7f | 0x80); buffer[1] = (byte) (i >>> 7); assert buffer[1] <= 0x7f; writeBytes(buffer, 0, 2); return; case 17: case 16: case 15: case 14: case 13: case 12: case 11: buffer = scratch.get(); buffer[0] = (byte) (i & 0x7f | 0x80); buffer[1] = (byte) ((i >>> 7) & 0x7f | 0x80); buffer[2] = (byte) (i >>> 14); assert buffer[2] <= 0x7f; writeBytes(buffer, 0, 3); return; case 10: case 9: case 8: case 7: case 6: case 5: case 4: buffer = scratch.get(); buffer[0] = (byte) (i & 0x7f | 0x80); buffer[1] = (byte) ((i >>> 7) & 0x7f | 0x80); buffer[2] = (byte) ((i >>> 14) & 0x7f | 0x80); buffer[3] = (byte) (i >>> 21); assert buffer[3] <= 0x7f; writeBytes(buffer, 0, 4); return; case 3: case 2: case 1: case 0: buffer = scratch.get(); buffer[0] = (byte) (i & 0x7f | 0x80); buffer[1] = (byte) ((i >>> 7) & 0x7f | 0x80); buffer[2] = (byte) ((i >>> 14) & 0x7f | 0x80); buffer[3] = (byte) ((i >>> 21) & 0x7f | 0x80); buffer[4] = (byte) (i >>> 28); assert buffer[4] <= 0x7f; writeBytes(buffer, 0, 5); return; default: throw new UnsupportedOperationException( "Can't encode [" + i + "]. Missing case for [" + Integer.numberOfLeadingZeros(i) + "]?" ); } }	i love this until here :) the fact that we can special case number of leading zeros > 24 is pretty significant and i can see the ~30% performance gain as well. hard coding all possible offsets below and doing all the buffer getting and writebytes inline with those hard coded offsets i don't think is a good idea. this blows up the method size significantly for a tiny saving in cpu when it comes to evaluating the loop. i benchmarked both this version and: java public void writevint(int i) throws ioexception { if (integer.numberofleadingzeros(i) > 24) { writebyte((byte) i); } else { final byte[] buffer = scratch.get(); int index = 0; do { buffer[index++] = ((byte) ((i & 0x7f) | 0x80)); i >>>= 7; } while ((i & ~0x7f) != 0); buffer[index++] = ((byte) i); writebytes(buffer, 0, index); } } and i can't see statistically significant difference so that's not worth the complication imo. i would in fact expect the above version with the loop to be faster than what is in this pr in the real world because the smaller method size has a better better chance of getting inlined in some places (73 vs 507 bytes on jdk14/linux for me). i suppose you could work around the code bloat by doing this: java final int leadingzeros = integer.numberofleadingzeros(i); if (integer.numberofleadingzeros(i) > 24) { writebyte((byte) i); } else { final byte[] buffer = scratch.get(); final int length; switch (leadingzeros) { case 24: case 23: case 22: case 21: case 20: case 19: case 18: buffer[0] = (byte) (i & 0x7f | 0x80); buffer[1] = (byte) (i >>> 7); assert buffer[1] <= 0x7f; length = 2; break; case 17: case 16: case 15: case 14: case 13: case 12: case 11: buffer[0] = (byte) (i & 0x7f | 0x80); buffer[1] = (byte) ((i >>> 7) & 0x7f | 0x80); buffer[2] = (byte) (i >>> 14); assert buffer[2] <= 0x7f; length = 3; break; case 10: case 9: case 8: case 7: case 6: case 5: case 4: buffer[0] = (byte) (i & 0x7f | 0x80); buffer[1] = (byte) ((i >>> 7) & 0x7f | 0x80); buffer[2] = (byte) ((i >>> 14) & 0x7f | 0x80); buffer[3] = (byte) (i >>> 21); assert buffer[3] <= 0x7f; length = 4; break; case 3: case 2: case 1: case 0: buffer[0] = (byte) (i & 0x7f | 0x80); buffer[1] = (byte) ((i >>> 7) & 0x7f | 0x80); buffer[2] = (byte) ((i >>> 14) & 0x7f | 0x80); buffer[3] = (byte) ((i >>> 21) & 0x7f | 0x80); buffer[4] = (byte) (i >>> 28); assert buffer[4] <= 0x7f; length = 5; break; default: throw new unsupportedoperationexception( "can't encode [" + i + "]. missing case for [" + integer.numberofleadingzeros(i) + "]?" ); } writebytes(buffer, 0, length); } but i can't measure a performance difference to the loop at all so personally i'd go for the shorter loop just for simplicity's sake.
public void testGetDataFrameAnalytics() throws Exception { RestHighLevelClient client = highLevelClient(); client.machineLearning().putDataFrameAnalytics(new PutDataFrameAnalyticsRequest(DF_ANALYTICS_CONFIG), RequestOptions.DEFAULT); { // tag::get-data-frame-analytics-request GetDataFrameAnalyticsRequest request = new GetDataFrameAnalyticsRequest("my-analytics-config"); // <1> // end::get-data-frame-analytics-request // tag::get-data-frame-analytics-execute GetDataFrameAnalyticsResponse response = client.machineLearning().getDataFrameAnalytics(request, RequestOptions.DEFAULT); // end::get-data-frame-analytics-execute // tag::get-data-frame-analytics-response List<DataFrameAnalyticsConfig> configs = response.getAnalytics(); // end::get-data-frame-analytics-response assertThat(configs.size(), equalTo(1)); } { GetDataFrameAnalyticsRequest request = new GetDataFrameAnalyticsRequest("my-analytics-config"); // tag::get-data-frame-analytics-execute-listener ActionListener<GetDataFrameAnalyticsResponse> listener = new ActionListener<>() { @Override public void onResponse(GetDataFrameAnalyticsResponse response) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::get-data-frame-analytics-execute-listener // Replace the empty listener by a blocking listener in test CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::get-data-frame-analytics-execute-async client.machineLearning().getDataFrameAnalyticsAsync(request, RequestOptions.DEFAULT, listener); // <1> // end::get-data-frame-analytics-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } }	#nit this seems like a weird ordering, having a private static final field between two public instance methods.
public void testGetDataFrameAnalytics() throws Exception { RestHighLevelClient client = highLevelClient(); client.machineLearning().putDataFrameAnalytics(new PutDataFrameAnalyticsRequest(DF_ANALYTICS_CONFIG), RequestOptions.DEFAULT); { // tag::get-data-frame-analytics-request GetDataFrameAnalyticsRequest request = new GetDataFrameAnalyticsRequest("my-analytics-config"); // <1> // end::get-data-frame-analytics-request // tag::get-data-frame-analytics-execute GetDataFrameAnalyticsResponse response = client.machineLearning().getDataFrameAnalytics(request, RequestOptions.DEFAULT); // end::get-data-frame-analytics-execute // tag::get-data-frame-analytics-response List<DataFrameAnalyticsConfig> configs = response.getAnalytics(); // end::get-data-frame-analytics-response assertThat(configs.size(), equalTo(1)); } { GetDataFrameAnalyticsRequest request = new GetDataFrameAnalyticsRequest("my-analytics-config"); // tag::get-data-frame-analytics-execute-listener ActionListener<GetDataFrameAnalyticsResponse> listener = new ActionListener<>() { @Override public void onResponse(GetDataFrameAnalyticsResponse response) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::get-data-frame-analytics-execute-listener // Replace the empty listener by a blocking listener in test CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::get-data-frame-analytics-execute-async client.machineLearning().getDataFrameAnalyticsAsync(request, RequestOptions.DEFAULT, listener); // <1> // end::get-data-frame-analytics-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } }	i think it would be a good idea to have callouts here and in the docs, and use those callouts to reference default values, format, etc.
public void testGetDataFrameAnalytics() throws Exception { RestHighLevelClient client = highLevelClient(); client.machineLearning().putDataFrameAnalytics(new PutDataFrameAnalyticsRequest(DF_ANALYTICS_CONFIG), RequestOptions.DEFAULT); { // tag::get-data-frame-analytics-request GetDataFrameAnalyticsRequest request = new GetDataFrameAnalyticsRequest("my-analytics-config"); // <1> // end::get-data-frame-analytics-request // tag::get-data-frame-analytics-execute GetDataFrameAnalyticsResponse response = client.machineLearning().getDataFrameAnalytics(request, RequestOptions.DEFAULT); // end::get-data-frame-analytics-execute // tag::get-data-frame-analytics-response List<DataFrameAnalyticsConfig> configs = response.getAnalytics(); // end::get-data-frame-analytics-response assertThat(configs.size(), equalTo(1)); } { GetDataFrameAnalyticsRequest request = new GetDataFrameAnalyticsRequest("my-analytics-config"); // tag::get-data-frame-analytics-execute-listener ActionListener<GetDataFrameAnalyticsResponse> listener = new ActionListener<>() { @Override public void onResponse(GetDataFrameAnalyticsResponse response) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::get-data-frame-analytics-execute-listener // Replace the empty listener by a blocking listener in test CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::get-data-frame-analytics-execute-async client.machineLearning().getDataFrameAnalyticsAsync(request, RequestOptions.DEFAULT, listener); // <1> // end::get-data-frame-analytics-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } }	do we want to document the outlierdetection builder methods here as well? there are zero java docs for the class. we need to have at least some documentation (preferably here and some java docs on the methods) on the builder methods.
* @throws IOException in case there is a problem sending the request or parsing back the response */ public GetPipelineResponse getPipeline(GetPipelineRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequestAndParseEntity( request, IngestRequestConverters::getPipeline, options, GetPipelineResponse::fromXContent, Collections.singleton(404)); } /** * Asynchronously get an existing pipeline. * See * <a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/get-pipeline-api.html"> Get Pipeline API on elastic.co</a> * @param request the request * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	big nitpick, mostly for future reference: would be nice we statically imported this, for consistency with emptyset.
public void setMaxModelMemory(long numBytes) { if (numBytes < MIN_MODEL_MEMORY) { throw new IllegalArgumentException("[" + MAX_MODEL_MEMORY.getPreferredName() + "] must be at least 1mb."); } if (numBytes >= FORECAST_LOCAL_STORAGE_LIMIT.getBytes()) { throw ExceptionsHelper.badRequestException( "[{}] must be less than {}", MAX_MODEL_MEMORY.getPreferredName(), FORECAST_LOCAL_STORAGE_LIMIT.getStringRep()); } this.maxModelMemory = numBytes; }	sorry, me again: should we use 0 as default?
public void testRandomMultiLineIntersections() throws IOException { double extentSize = randomDoubleBetween(0.01, 10, true); GeoShapeIndexer indexer = new GeoShapeIndexer(true, "test"); MultiLine geometry = GeometryTestUtils.randomMultiLine(false); geometry = (MultiLine) indexer.prepareForIndexing(geometry); GeometryTreeReader reader = geometryTreeReader(geometry, GeoShapeCoordinateEncoder.INSTANCE); Extent readerExtent = reader.getExtent(); for (Line line : geometry) { // extent that intersects edges assertTrue(reader.intersects(bufferedExtentFromGeoPoint(line.getX(0), line.getY(0), extentSize))); // extent that fully encloses a line in the MultiLine Extent lineExtent = geometryTreeReader(line, GeoShapeCoordinateEncoder.INSTANCE).getExtent(); assertTrue(reader.intersects(lineExtent)); if (lineExtent.minX() != Integer.MIN_VALUE && lineExtent.maxX() != Integer.MAX_VALUE && lineExtent.minY() != Integer.MIN_VALUE && lineExtent.maxY() != Integer.MAX_VALUE) { assertTrue(reader.intersects(Extent.fromPoints(lineExtent.minX() - 1, lineExtent.minY() - 1, lineExtent.maxX() + 1, lineExtent.maxY() + 1))); } } // extent that fully encloses the MultiLine assertTrue(reader.intersects(reader.getExtent())); if (readerExtent.minX() != Integer.MIN_VALUE && readerExtent.maxX() != Integer.MAX_VALUE && readerExtent.minY() != Integer.MIN_VALUE && readerExtent.maxY() != Integer.MAX_VALUE) { assertTrue(reader.intersects(Extent.fromPoints(readerExtent.minX() - 1, readerExtent.minY() - 1, readerExtent.maxX() + 1, readerExtent.maxY() + 1))); } }	is the plan to keep this as awaitsfix until we figure out how to handle the edge (literally) cases?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeByte(searchType.id()); out.writeVInt(indices.length); for (String index : indices) { out.writeString(index); } out.writeOptionalString(routing); out.writeOptionalString(preference); if (scroll == null) { out.writeBoolean(false); } else { out.writeBoolean(true); scroll.writeTo(out); } if (source == null) { out.writeBoolean(false); } else { out.writeBoolean(true); source.writeTo(out); } out.writeStringArray(types); indicesOptions.writeIndicesOptions(out); out.writeOptionalBoolean(requestCache); out.writeOptionalStreamable(template); }	can the source ever be null? i think it shouldn't be optional in readfrom and writeto
public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) { final ExistsRequest existsRequest = new ExistsRequest(Strings.splitStringByCommaToArray(request.param("index"))); existsRequest.indicesOptions(IndicesOptions.fromRequest(request, existsRequest.indicesOptions())); if (RestActions.hasBodyContent(request)) { existsRequest.source(RestActions.getRestContent(request)); } else { QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request); if (queryBuilder != null) { QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder(); querySourceBuilder.setQuery(queryBuilder); existsRequest.source(querySourceBuilder.buildAsBytes()); } } existsRequest.routing(request.param("routing")); existsRequest.minScore(request.paramAsFloat("min_score", DEFAULT_MIN_SCORE)); existsRequest.types(Strings.splitStringByCommaToArray(request.param("type"))); existsRequest.preference(request.param("preference")); client.exists(existsRequest, new RestBuilderListener<ExistsResponse>(channel) { @Override public RestResponse buildResponse(ExistsResponse response, XContentBuilder builder) throws Exception { RestStatus status = response.exists() ? OK : NOT_FOUND; builder.startObject(); builder.field("exists", response.exists()); builder.endObject(); return new BytesRestResponse(status, builder); } }); }	we are doing this because we wanna get rid of exists anyway right?
public static boolean parseSearchSource(final SearchSourceBuilder searchSourceBuilder, RestRequest request) { boolean modified = false; QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request); if (queryBuilder != null) { searchSourceBuilder.query(queryBuilder); modified = true; } int from = request.paramAsInt("from", -1); if (from != -1) { searchSourceBuilder.from(from); modified = true; } int size = request.paramAsInt("size", -1); if (size != -1) { searchSourceBuilder.size(size); modified = true; } if (request.hasParam("explain")) { searchSourceBuilder.explain(request.paramAsBoolean("explain", null)); modified = true; } if (request.hasParam("version")) { searchSourceBuilder.version(request.paramAsBoolean("version", null)); modified = true; } if (request.hasParam("timeout")) { searchSourceBuilder.timeout(request.paramAsTime("timeout", null)); modified = true; } if (request.hasParam("terminate_after")) { int terminateAfter = request.paramAsInt("terminate_after", SearchContext.DEFAULT_TERMINATE_AFTER); if (terminateAfter < 0) { throw new IllegalArgumentException("terminateAfter must be > 0"); } else if (terminateAfter > 0) { searchSourceBuilder.terminateAfter(terminateAfter); modified = true; } } String sField = request.param("fields"); if (sField != null) { if (!Strings.hasText(sField)) { searchSourceBuilder.noFields(); modified = true; } else { String[] sFields = Strings.splitStringByCommaToArray(sField); if (sFields != null) { for (String field : sFields) { searchSourceBuilder.field(field); modified = true; } } } } String sFieldDataFields = request.param("fielddata_fields"); if (sFieldDataFields != null) { if (Strings.hasText(sFieldDataFields)) { String[] sFields = Strings.splitStringByCommaToArray(sFieldDataFields); if (sFields != null) { for (String field : sFields) { searchSourceBuilder.fieldDataField(field); modified = true; } } } } FetchSourceContext fetchSourceContext = FetchSourceContext.parseFromRestRequest(request); if (fetchSourceContext != null) { searchSourceBuilder.fetchSource(fetchSourceContext); modified = true; } if (request.hasParam("track_scores")) { searchSourceBuilder.trackScores(request.paramAsBoolean("track_scores", false)); modified = true; } String sSorts = request.param("sort"); if (sSorts != null) { String[] sorts = Strings.splitStringByCommaToArray(sSorts); for (String sort : sorts) { int delimiter = sort.lastIndexOf(":"); if (delimiter != -1) { String sortField = sort.substring(0, delimiter); String reverse = sort.substring(delimiter + 1); if ("asc".equals(reverse)) { searchSourceBuilder.sort(sortField, SortOrder.ASC); modified = true; } else if ("desc".equals(reverse)) { searchSourceBuilder.sort(sortField, SortOrder.DESC); modified = true; } } else { searchSourceBuilder.sort(sort); modified = true; } } } String sStats = request.param("stats"); if (sStats != null) { searchSourceBuilder.stats(Arrays.asList(Strings.splitStringByCommaToArray(sStats))); modified = true; } String suggestField = request.param("suggest_field"); if (suggestField != null) { String suggestText = request.param("suggest_text", request.param("q")); int suggestSize = request.paramAsInt("suggest_size", 5); String suggestMode = request.param("suggest_mode"); searchSourceBuilder.suggest(new SuggestBuilder().addSuggestion( termSuggestion(suggestField).field(suggestField).text(suggestText).size(suggestSize).suggestMode(suggestMode))); modified = true; } return modified; }	can this be made private?
public static SearchSourceBuilder getRestSearchSource(BytesReference sourceBytes, IndicesQueriesRegistry queryRegistry) throws IOException { XContentParser parser = XContentFactory.xContent(sourceBytes).createParser(sourceBytes); QueryParseContext queryParseContext = new QueryParseContext(queryRegistry); queryParseContext.reset(parser); SearchSourceBuilder source = SearchSourceBuilder.parseSearchSource(parser, queryParseContext); return source; }	wherever we create or reset the queryparsecontext manually, i am afraid we end up not setting the parsefieldmatcher, and its default is empty in that case. the baseresthandler holds the parsefieldmatcher created depending on settings, i think we should set it. it is very error-prone though, it is too easy to forget about it, maybe we should change the api a bit, for instance pass it in with the reset or as a constructor argument?
private void processFailure(SearchContext context, Throwable t) { freeContext(context.id()); try { if (Lucene.isCorruptionException(t)) { context.indexShard().failShard("search execution corruption failure", t); } } catch (Throwable e) { logger.warn("failed to process shard failure to (potentially) send back shard failure on corruption", e); } }	this .parse call confuses me, what does it parse, the source.query() is already parsed no?
private SearchSourceBuilder sourceBuilder() { if (sourceBuilder == null) { sourceBuilder = new SearchSourceBuilder(); } return sourceBuilder; }	nit: given that this method is unused and we cannot return a highlightbuilder anymore, at least for now, i wonder if it makes sense to remove it rather that changing the return type
public static ClusterRerouteRequest createRequest(RestRequest request) throws IOException { ClusterRerouteRequest clusterRerouteRequest = Requests.clusterRerouteRequest(); clusterRerouteRequest.dryRun(request.paramAsBoolean("dry_run", clusterRerouteRequest.dryRun())); clusterRerouteRequest.explain(request.paramAsBoolean("explain", clusterRerouteRequest.explain())); clusterRerouteRequest.timeout(request.paramAsTime("timeout", clusterRerouteRequest.timeout())); clusterRerouteRequest.setRetryFailed(request.paramAsBoolean("retry_failed", clusterRerouteRequest.isRetryFailed())); clusterRerouteRequest.masterNodeTimeout(request.paramAsTime("master_timeout", clusterRerouteRequest.masterNodeTimeout())); request.applyContentParser(parser -> PARSER.parse(parser, clusterRerouteRequest, null)); return clusterRerouteRequest; }	oh seems like you also removed some parsefieldmatcher usages, nice :)
public void close(String reason, boolean flushEngine) throws IOException { synchronized (mutex) { try { changeState(IndexShardState.CLOSED, reason); } finally { final Engine engine = getEngineOrNull(); try { if (engine != null && flushEngine && isEngineResetting() == false) { engine.flushAndClose(); } } finally { // playing safe here and close the engine even if the above succeeds - close can be called multiple times // Also closing refreshListeners to prevent us from accumulating any more listeners IOUtils.close(engineHolder, globalCheckpointListeners, refreshListeners); indexShardOperationPermits.close(); } } } }	i wonder if there's room for a race condition here and whether we should just override flushandclose() on searchonlyengine to call close directly, i.e., make that one explicitly a noop, not requiring this additional boolean check?
private Engine createNewEngine(EngineConfig config) throws IOException { assert Thread.holdsLock(mutex); if (state == IndexShardState.CLOSED) { throw new IndexShardClosedException(shardId, "can't create engine - shard is closed"); } final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); final long globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); final long minRetainedTranslogGen = Translog.readMinTranslogGeneration(translogConfig.getTranslogPath(), translogUUID); store.trimUnsafeCommits(globalCheckpoint, minRetainedTranslogGen, config.getIndexSettings().getIndexVersionCreated()); assertMaxUnsafeAutoIdInCommit(); final Engine engine = engineFactory.newReadWriteEngine(config); onNewEngine(engine); engine.onSettingsChanged(); return engine; }	why call this under mutex?
@Before public void setUp() throws Exception { super.setUp(); disableBeforeIndexDeletion = false; }	there's already a method called assertseqnos. should we call that one first? can you also check assertsamedocidsonshards for relocationit?
public void assertSameDocIdsOnShards() throws Exception { final Map<ShardId, Set<String>> docIdsPerShard = new HashMap<>(); final String[] nodeNames = internalCluster().getNodeNames(); for (String nodeName : nodeNames) { final IndicesService indexServices = internalCluster().getInstance(IndicesService.class, nodeName); for (IndexService indexService : indexServices) { for (IndexShard shard : indexService) { try { final Set<String> docIds = IndexShardTestCase.getShardDocUIDs(shard, true); if (docIdsPerShard.containsKey(shard.shardId())) { assertThat(docIds, equalTo(docIdsPerShard.get(shard.shardId()))); } else { docIdsPerShard.put(shard.shardId(), docIds); } } catch (AlreadyClosedException e) { } } } } } /** * Asserts that all segments are sorted with the provided {@link Sort}	if this fails, we will need more output. can you add information about which two shard copies diverged?
@Override void updateBucket(OrdBucket spare, long globalOrd, long bucketOrd, long docCount) throws IOException { spare.globalOrd = globalOrd; spare.bucketOrd = bucketOrd; spare.docCount = docCount; }	this was causing inaccurate doc counts when shard_min_doc_count was more than 1.
private InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException { B[][] topBucketsPerOrd = buildTopBucketsPerOrd(owningBucketOrds.length); long[] otherDocCounts = new long[owningBucketOrds.length]; for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) { collectZeroDocEntriesIfNeeded(owningBucketOrds[ordIdx]); long bucketsInOrd = bucketOrds.bucketsInOrd(owningBucketOrds[ordIdx]); int size = (int) Math.min(bucketsInOrd, bucketCountThresholds.getShardSize()); PriorityQueue<B> ordered = buildPriorityQueue(size); B spare = null; BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrds[ordIdx]); Supplier<B> emptyBucketBuilder = emptyBucketBuilder(owningBucketOrds[ordIdx]); while (ordsEnum.next()) { long docCount = bucketDocCount(ordsEnum.ord()); otherDocCounts[ordIdx] += docCount; if (docCount < bucketCountThresholds.getShardMinDocCount()) { continue; } if (spare == null) { spare = emptyBucketBuilder.get(); } updateBucket(spare, ordsEnum, docCount); spare = ordered.insertWithOverflow(spare); if (spare == null) { consumeBucketsAndMaybeBreak(1); } } // Get the top buckets B[] bucketsForOrd = buildBuckets(ordered.size()); topBucketsPerOrd[ordIdx] = bucketsForOrd; for (int b = ordered.size() - 1; b >= 0; --b) { topBucketsPerOrd[ordIdx][b] = ordered.pop(); otherDocCounts[ordIdx] -= topBucketsPerOrd[ordIdx][b].getDocCount(); } } buildSubAggs(topBucketsPerOrd); InternalAggregation[] result = new InternalAggregation[owningBucketOrds.length]; for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) { result[ordIdx] = buildResult(owningBucketOrds[ordIdx], otherDocCounts[ordIdx], topBucketsPerOrd[ordIdx]); } return result; }	i flip the if statement here and moved the spare initialization down to right before i need it to line up with the way the map version works.
public Node start() throws NodeValidationException { if (!lifecycle.moveToStarted()) { return this; } logger.info("starting ..."); pluginLifecycleComponents.forEach(LifecycleComponent::start); injector.getInstance(MappingUpdatedAction.class).setClient(client); injector.getInstance(IndicesService.class).start(); injector.getInstance(IndicesClusterStateService.class).start(); injector.getInstance(SnapshotsService.class).start(); injector.getInstance(SnapshotShardsService.class).start(); injector.getInstance(RoutingService.class).start(); injector.getInstance(SearchService.class).start(); nodeService.getMonitorService().start(); final ClusterService clusterService = injector.getInstance(ClusterService.class); final NodeConnectionsService nodeConnectionsService = injector.getInstance(NodeConnectionsService.class); nodeConnectionsService.start(); clusterService.setNodeConnectionsService(nodeConnectionsService); injector.getInstance(ResourceWatcherService.class).start(); injector.getInstance(GatewayService.class).start(); Discovery discovery = injector.getInstance(Discovery.class); clusterService.getMasterService().setClusterStatePublisher(discovery::publish); // Start the transport service now so the publish address will be added to the local disco node in ClusterService TransportService transportService = injector.getInstance(TransportService.class); transportService.getTaskManager().setTaskResultsService(injector.getInstance(TaskResultsService.class)); transportService.start(); assert localNodeFactory.getNode() != null; assert transportService.getLocalNode().equals(localNodeFactory.getNode()) : "transportService has a different local node than the factory provided"; onTransportServiceStarted(); final MetaData onDiskMetadata; try { // we load the global state here (the persistent part of the cluster state stored on disk) to // pass it to the bootstrap checks to allow plugins to enforce certain preconditions based on the recovered state. if (DiscoveryNode.isMasterNode(settings) || DiscoveryNode.isDataNode(settings)) { onDiskMetadata = injector.getInstance(GatewayMetaState.class).loadMetaState(); } else { onDiskMetadata = MetaData.EMPTY_META_DATA; } assert onDiskMetadata != null : "metadata is null but shouldn't"; // this is never null } catch (IOException e) { throw new UncheckedIOException(e); } validateNodeBeforeAcceptingRequests(new BootstrapContext(settings, onDiskMetadata), transportService.boundAddress(), pluginsService .filterPlugins(Plugin .class) .stream() .flatMap(p -> p.getBootstrapChecks().stream()).collect(Collectors.toList())); clusterService.addStateApplier(transportService.getTaskManager()); // start after transport service so the local disco is known discovery.start(); // start before cluster service so that it can set initial state on ClusterApplierService clusterService.start(); assert clusterService.localNode().equals(localNodeFactory.getNode()) : "clusterService has a different local node than the factory provided"; transportService.acceptIncomingRequests(); discovery.startInitialJoin(); final TimeValue initialStateTimeout = DiscoverySettings.INITIAL_STATE_TIMEOUT_SETTING.get(settings); if (initialStateTimeout.millis() > 0) { final ThreadPool thread = injector.getInstance(ThreadPool.class); ClusterState clusterState = clusterService.state(); ClusterStateObserver observer = new ClusterStateObserver(clusterState, clusterService, null, logger, thread.getThreadContext()); if (clusterState.nodes().getMasterNodeId() == null) { logger.debug("waiting to join the cluster. timeout [{}]", initialStateTimeout); final CountDownLatch latch = new CountDownLatch(1); observer.waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { latch.countDown(); } @Override public void onClusterServiceClose() { latch.countDown(); } @Override public void onTimeout(TimeValue timeout) { logger.warn("timed out while waiting for initial discovery state - timeout: {}", initialStateTimeout); latch.countDown(); } }, state -> state.nodes().getMasterNodeId() != null, initialStateTimeout); try { latch.await(); } catch (InterruptedException e) { throw new ElasticsearchTimeoutException("Interrupted while waiting for initial discovery state"); } } } injector.getInstance(HttpServerTransport.class).start(); if (WRITE_PORTS_FILE_SETTING.get(settings)) { TransportService transport = injector.getInstance(TransportService.class); writePortsFile("transport", transport.boundAddress()); HttpServerTransport http = injector.getInstance(HttpServerTransport.class); writePortsFile("http", http.boundAddress()); } logger.info("started"); pluginsService.filterPlugins(ClusterPlugin.class).forEach(ClusterPlugin::onNodeStarted); return this; }	i think you can avoid adding these changes here in node (and mocknode), but instead after initializing the node (but before starting it), call node.injector().getinstance(transportservice.class) to get the transportservice and then register a lifecycle listener on that (addlifecyclelistener) which implements afterstart.
public static void ensureValidLicense(XPackLicenseState licenseState) { if (licenseState.isAllowedByLicense(License.OperationMode.ENTERPRISE) == false) { throw LicenseUtils.newComplianceException("searchable-snapshots"); } }	let's use platinum here to make it available in snapshot builds on cloud
boolean isReadyToTransitionToThisPhase(final String policy, final IndexMetaData indexMetaData, final String phase) { final Settings indexSettings = indexMetaData.getSettings(); if (indexSettings.hasValue(LifecycleSettings.LIFECYCLE_INDEX_CREATION_DATE) == false) { logger.trace("no index creation date has been set yet"); return true; } final long lifecycleDate = indexSettings.getAsLong(LifecycleSettings.LIFECYCLE_INDEX_CREATION_DATE, -1L); assert lifecycleDate >= 0 : "expected index to have a lifecycle date but it did not"; final TimeValue after = stepRegistry.getIndexAgeForPhase(policy, phase); final long now = nowSupplier.getAsLong(); final TimeValue age = new TimeValue(now - lifecycleDate); if (logger.isTraceEnabled()) { logger.trace("[{}] checking for index age to be at least [{}] before performing actions in " + "the \\\\"{}\\\\" phase. Now: {}, lifecycle date: {}, age: [{}/{}s]", indexMetaData.getIndex().getName(), after, phase, new TimeValue(now).seconds(), new TimeValue(lifecycleDate).seconds(), age, age.seconds()); } return now >= lifecycleDate + after.getMillis(); }	should we add extra wording here around how it must be positive? theoretically, it can be set to be negative, right?
public void runPolicy(String policy, IndexMetaData indexMetaData, ClusterState currentState, boolean fromClusterStateChange) { Settings indexSettings = indexMetaData.getSettings(); if (LifecycleSettings.LIFECYCLE_SKIP_SETTING.get(indexSettings)) { logger.info("skipping policy [" + policy + "] for index [" + indexMetaData.getIndex().getName() + "]." + LifecycleSettings.LIFECYCLE_SKIP + "== true"); return; } Step currentStep = getCurrentStep(stepRegistry, policy, indexMetaData.getIndex(), indexSettings); if (currentStep == null) { // This may happen in the case that there is invalid ilm-step index settings or the stepRegistry is out of // sync with the current cluster state logger.warn("current step [" + getCurrentStepKey(indexSettings) + "] for index [" + indexMetaData.getIndex().getName() + "] with policy [" + policy + "] is not recognized"); return; } logger.debug("running policy with current-step [" + currentStep.getKey() + "]"); if (currentStep instanceof TerminalPolicyStep) { logger.debug("policy [" + policy + "] for index [" + indexMetaData.getIndex().getName() + "] complete, skipping execution"); return; } else if (currentStep instanceof ErrorStep) { logger.debug( "policy [" + policy + "] for index [" + indexMetaData.getIndex().getName() + "] on an error step, skipping execution"); return; } else if (currentStep instanceof PhaseCompleteStep) { // Only proceed to the next step if enough time has elapsed to go into the next phase if (isReadyToTransitionToThisPhase(policy, indexMetaData, currentStep.getNextStepKey().getPhase())) { moveToStep(indexMetaData.getIndex(), policy, currentStep.getKey(), currentStep.getNextStepKey()); } return; } if (currentStep instanceof ClusterStateActionStep || currentStep instanceof ClusterStateWaitStep) { executeClusterStateSteps(indexMetaData.getIndex(), policy, currentStep); } else if (currentStep instanceof AsyncWaitStep) { if (fromClusterStateChange == false) { ((AsyncWaitStep) currentStep).evaluateCondition(indexMetaData.getIndex(), new AsyncWaitStep.Listener() { @Override public void onResponse(boolean conditionMet, ToXContentObject stepInfo) { logger.debug("cs-change-async-wait-callback. current-step:" + currentStep.getKey()); if (conditionMet) { moveToStep(indexMetaData.getIndex(), policy, currentStep.getKey(), currentStep.getNextStepKey()); } else if (stepInfo != null) { setStepInfo(indexMetaData.getIndex(), policy, currentStep.getKey(), stepInfo); } } @Override public void onFailure(Exception e) { moveToErrorStep(indexMetaData.getIndex(), policy, currentStep.getKey(), e); } }); } } else if (currentStep instanceof AsyncActionStep) { if (fromClusterStateChange == false) { ((AsyncActionStep) currentStep).performAction(indexMetaData, currentState, new AsyncActionStep.Listener() { @Override public void onResponse(boolean complete) { logger.debug("cs-change-async-action-callback. current-step:" + currentStep.getKey()); if (complete && ((AsyncActionStep) currentStep).indexSurvives()) { moveToStep(indexMetaData.getIndex(), policy, currentStep.getKey(), currentStep.getNextStepKey()); } } @Override public void onFailure(Exception e) { moveToErrorStep(indexMetaData.getIndex(), policy, currentStep.getKey(), e); } }); } } else { throw new IllegalStateException( "Step with key [" + currentStep.getKey() + "] is not a recognised type: [" + currentStep.getClass().getName() + "]"); } }	i do not think what i am asking here should be expanded into the scope of this pr, but i feel it might be relevant. should we add step info when we make this check here to give feedback to the user why we are still in this step if it isn't ready to transition?
public void testIsReadyToTransition() { String policyName = "async_action_policy"; StepKey stepKey = new StepKey("phase", MockAction.NAME, MockAction.NAME); MockAsyncActionStep step = new MockAsyncActionStep(stepKey, null); step.setWillComplete(true); SortedMap<String, LifecyclePolicyMetadata> lifecyclePolicyMap = new TreeMap<>(Collections.singletonMap(policyName, new LifecyclePolicyMetadata(createPolicy(policyName, null, step.getKey()), new HashMap<>()))); Index index = new Index("my_index", "uuid"); Map<String, Step> firstStepMap = Collections.singletonMap(policyName, step); Map<StepKey, Step> policySteps = Collections.singletonMap(step.getKey(), step); Map<String, Map<StepKey, Step>> stepMap = Collections.singletonMap(policyName, policySteps); Map<Index, List<Step>> indexSteps = Collections.singletonMap(index, Collections.singletonList(step)); PolicyStepsRegistry policyStepsRegistry = new PolicyStepsRegistry(lifecyclePolicyMap, firstStepMap, stepMap, indexSteps); ClusterService clusterService = mock(ClusterService.class); long now = 5; IndexLifecycleRunner runner = new IndexLifecycleRunner(policyStepsRegistry, clusterService, () -> now); IndexMetaData indexMetaData = IndexMetaData.builder("my_index").settings(settings(Version.CURRENT)) .numberOfShards(randomIntBetween(1, 5)) .numberOfReplicas(randomIntBetween(0, 5)) .build(); // With no time, always transition assertTrue("index should be able to transition with no creation date", runner.isReadyToTransitionToThisPhase(policyName, indexMetaData, "phase")); indexMetaData = IndexMetaData.builder(indexMetaData) .settings(Settings.builder() .put(indexMetaData.getSettings()) .put(LifecycleSettings.LIFECYCLE_INDEX_CREATION_DATE, 10L) .build()) .build(); // Index is not old enough to transition assertFalse("index is not able to transition if it isn't old enough", runner.isReadyToTransitionToThisPhase(policyName, indexMetaData, "phase")); }	should we add another for when it has a time and it is ready to transition? i guess randomization would also be appropriate here, but that might be overkill
@Override public Object process(Object input) { for (int i = 0; i < processors.size() - 2; i += 2) { if (processors.get(i).process(input)==Boolean.TRUE) { return processors.get(i + 1).process(input); } } // resort to default value return processors.get(processors.size() - 1).process(input); }	so, here you basically take a condition and you evaluate it, and if it holds then you evaluate its results and break out of the loop. if it doesn't hold (evaluates to false), then move on to the next set of [condition, result] and repeat the process. can you add a comment about this, please?
@Nullable public Object readGenericValue() throws IOException { byte type = readByte(); switch (type) { case -1: return null; case 0: return readString(); case 1: return readInt(); case 2: return readLong(); case 3: return readFloat(); case 4: return readDouble(); case 5: return readBoolean(); case 6: return readByteArray(); case 7: return readArrayList(); case 8: return readArray(); case 9: return readLinkedHashMap(); case 10: return readHashMap(); case 11: return readByte(); case 12: return readDate(); case 13: return readDateTime(); case 14: return readBytesReference(); case 15: return readText(); case 16: return readShort(); case 17: return readIntArray(); case 18: return readLongArray(); case 19: return readFloatArray(); case 20: return readDoubleArray(); case 21: return readBytesRef(); case 22: return readGeoPoint(); case 23: return readZonedDateTime(); case 24: return readCollection(StreamInput::readGenericValue, LinkedHashSet::new, Collections.emptySet()); case 25: return readCollection(StreamInput::readGenericValue, HashSet::new, Collections.emptySet()); default: throw new IOException("Can't read unknown type [" + type + "]"); } } /** * Read an {@link Instant}	i think you can just make this readset(streaminput::readgenericvalue)?
public void doRun() { DeleteByQueryRequest expiredDbq = new DeleteByQueryRequest(SecurityIndexManager.SECURITY_INDEX_NAME); if (timeout != TimeValue.MINUS_ONE) { expiredDbq.setTimeout(timeout); expiredDbq.getSearchRequest().source().timeout(timeout); } final Instant now = Instant.now(); expiredDbq .setQuery(QueryBuilders.boolQuery() .filter(QueryBuilders.termsQuery("doc_type", "token")) .filter(QueryBuilders.boolQuery() .must(QueryBuilders.rangeQuery("creation_time").lte(now.minus(24L, ChronoUnit.HOURS).toEpochMilli())))); logger.trace(() -> new ParameterizedMessage("Removing old tokens: [{}]", Strings.toString(expiredDbq))); executeAsyncWithOrigin(client, SECURITY_ORIGIN, DeleteByQueryAction.INSTANCE, expiredDbq, ActionListener.wrap(r -> { debugDbqResponse(r); markComplete(); }, this::onFailure)); }	do we need a bool here? can't we just add the range directly to the filter?
public void testStandardHtmlStripAnalyzerDeprecationWrning() throws IOException { Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) .put(IndexMetaData.SETTING_VERSION_CREATED, VersionUtils.randomVersionBetween(random(), Version.V_6_5_0, Version.CURRENT)) .put("index.analysis.analyzer.custom_analyzer.type", "standard_html_strip") .putList("index.analysis.analyzer.custom_analyzer.stopwords", "a", "b") .build(); IndexSettings idxSettings = IndexSettingsModule.newIndexSettings("index", settings); try (CommonAnalysisPlugin commonAnalysisPlugin = new CommonAnalysisPlugin()) { IndexAnalyzers analyzers = createTestAnalysis(idxSettings, settings, commonAnalysisPlugin).indexAnalyzers; Analyzer analyzer = analyzers.get("custom_analyzer"); assertNotNull(((NamedAnalyzer) analyzer).analyzer()); assertWarnings( "Deprecated analyzer [standard_html_strip] used, replaced by using [html_strip] char_filter"); } }	this test doesn't seem to do what the comment suggests it should do? i think it can be repurposed to check that an exception is thrown if you try to create an analyzer of type standard_html_strip with an index created version greater than or equal to 7.0, and the test above can just check that the warning is emitted on any version before 7.0
@Override public RepositoryData getRepositoryData() { try { final long indexGen = latestIndexBlobId(); final String snapshotsIndexBlobName = INDEX_FILE_PREFIX + Long.toString(indexGen); RepositoryData repositoryData; // EMPTY is safe here because RepositoryData#fromXContent calls namedObject try (InputStream blob = blobContainer().readBlob(snapshotsIndexBlobName); XContentParser parser = XContentType.JSON.xContent().createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, blob)) { repositoryData = RepositoryData.snapshotsFromXContent(parser, indexGen); } // now load the incompatible snapshot ids, if they exist try (InputStream blob = blobContainer().readBlob(INCOMPATIBLE_SNAPSHOTS_BLOB); XContentParser parser = XContentType.JSON.xContent().createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, blob)) { repositoryData = repositoryData.incompatibleSnapshotsFromXContent(parser); } catch (NoSuchFileException e) { if (isReadOnly()) { logger.debug("[{}] Incompatible snapshots blob [{}] does not exist, the likely " + "reason is that there are no incompatible snapshots in the repository", metadata.name(), INCOMPATIBLE_SNAPSHOTS_BLOB); } else { // write an empty incompatible-snapshots blob - we do this so that there // is a blob present, which helps speed up some cloud-based repositories // (e.g. S3), which retry if a blob is missing with exponential backoff, // delaying the read of repository data and sometimes causing a timeout writeIncompatibleSnapshots(RepositoryData.EMPTY); } } return repositoryData; } catch (NoSuchFileException ex) { // repository doesn't have an index blob, its a new blank repo return RepositoryData.EMPTY; } catch (IOException ioe) { throw new RepositoryException(metadata.name(), "could not read repository data from index blob", ioe); } }	can you please wrap the xcontentparser in a try-with-resources statement?
protected boolean skipResolved() { return true; } } abstract static class LogicalPlanAnalyzeRule extends AnalyzeRule<LogicalPlan> { @Override protected LogicalPlan rule(LogicalPlan plan) { if (plan.childrenResolved() == false) { return plan; } return rulePlan(plan); }	i'd pick a different name - maybe defaultanalyzerule or baseanalyzerule.
protected boolean skipResolved() { return true; } } abstract static class LogicalPlanAnalyzeRule extends AnalyzeRule<LogicalPlan> { @Override protected LogicalPlan rule(LogicalPlan plan) { if (plan.childrenResolved() == false) { return plan; } return rulePlan(plan); }	ruleplan is repetitive (the method only accepts logicalplans) - how about dorule or analyze?
protected LogicalPlan rulePlan(LogicalPlan plan) { if (plan.resolved()) { return plan; } if (plan instanceof OrderBy) { OrderBy o = (OrderBy) plan; LogicalPlan child = o.child(); List<Order> maybeResolved = new ArrayList<>(); for (Order or : o.order()) { maybeResolved.add(or.resolved() ? or : tryResolveExpression(or, child)); } Stream<Order> referencesStream = maybeResolved.stream() .filter(Expression::resolved); // if there are any references in the output // try and resolve them to the source in order to compare the source expressions // e.g. ORDER BY a + 1 // \\\\ SELECT a + 1 // a + 1 in SELECT is actually Alias("a + 1", a + 1) and translates to ReferenceAttribute // in the output. However it won't match the unnamed a + 1 despite being the same expression // so explicitly compare the source // if there's a match, remove the item from the reference stream if (Expressions.hasReferenceAttribute(child.outputSet())) { final Map<Attribute, Expression> collectRefs = new LinkedHashMap<>(); // collect aliases child.forEachUp(p -> p.forEachExpressionsUp(e -> { if (e instanceof Alias) { Alias a = (Alias) e; collectRefs.put(a.toAttribute(), a.child()); } })); referencesStream = referencesStream.filter(r -> { for (Attribute attr : child.outputSet()) { if (attr instanceof ReferenceAttribute) { Expression source = collectRefs.getOrDefault(attr, attr); // found a match, no need to resolve it further // so filter it out if (source.equals(r.child())) { return false; } } } return true; }); } AttributeSet resolvedRefs = Expressions.references(referencesStream.collect(toList())); AttributeSet missing = resolvedRefs.subtract(child.outputSet()); if (!missing.isEmpty()) { // Add missing attributes but project them away afterwards List<Attribute> failedAttrs = new ArrayList<>(); LogicalPlan newChild = propagateMissing(o.child(), missing, failedAttrs); // resolution failed and the failed expressions might contain resolution information so copy it over if (!failedAttrs.isEmpty()) { List<Order> newOrders = new ArrayList<>(); // transform the orders with the failed information for (Order order : o.order()) { Order transformed = (Order) order.transformUp(ua -> resolveMetadataToMessage(ua, failedAttrs, "order"), UnresolvedAttribute.class); newOrders.add(order.equals(transformed) ? order : transformed); } return o.order().equals(newOrders) ? o : new OrderBy(o.source(), o.child(), newOrders); } // everything worked return new Project(o.source(), new OrderBy(o.source(), newChild, maybeResolved), o.child().output()); } if (!maybeResolved.equals(o.order())) { return new OrderBy(o.source(), o.child(), maybeResolved); } } if (plan instanceof Filter) { Filter f = (Filter) plan; Expression maybeResolved = tryResolveExpression(f.condition(), f.child()); AttributeSet resolvedRefs = new AttributeSet(maybeResolved.references().stream() .filter(Expression::resolved) .collect(toList())); AttributeSet missing = resolvedRefs.subtract(f.child().outputSet()); if (!missing.isEmpty()) { // Again, add missing attributes and project them away List<Attribute> failedAttrs = new ArrayList<>(); LogicalPlan newChild = propagateMissing(f.child(), missing, failedAttrs); // resolution failed and the failed expressions might contain resolution information so copy it over if (!failedAttrs.isEmpty()) { // transform the orders with the failed information Expression transformed = f.condition().transformUp(ua -> resolveMetadataToMessage(ua, failedAttrs, "filter"), UnresolvedAttribute.class); return f.condition().equals(transformed) ? f : new Filter(f.source(), f.child(), transformed); } return new Project(f.source(), new Filter(f.source(), newChild, maybeResolved), f.child().output()); } if (!maybeResolved.equals(f.condition())) { return new Filter(f.source(), f.child(), maybeResolved); } } // Try to resolve aggregates and groupings based on the child plan if (plan instanceof Aggregate) { Aggregate a = (Aggregate) plan; LogicalPlan child = a.child(); List<Expression> newGroupings = new ArrayList<>(a.groupings().size()); a.groupings().forEach(e -> newGroupings.add(tryResolveExpression(e, child))); List<NamedExpression> newAggregates = new ArrayList<>(a.aggregates().size()); a.aggregates().forEach(e -> newAggregates.add(tryResolveExpression(e, child))); if (newAggregates.equals(a.aggregates()) == false || newGroupings.equals(a.groupings()) == false) { return new Aggregate(a.source(), child, newGroupings, newAggregates); } } return plan; }	i don't think this is needed anymore - see analyzerule which takes care of skipping if the node is resolved and skipresolved is true (default).
public ClusterUpdateSettingsResponse putSettings(ClusterUpdateSettingsRequest clusterUpdateSettingsRequest, RequestOptions options) throws IOException { return restHighLevelClient.performRequestAndParseEntity(clusterUpdateSettingsRequest, RequestConverters::clusterPutSettings, options, ClusterUpdateSettingsResponse::fromXContent, emptySet()); } /** * Updates cluster wide specific settings using the Cluster Update Settings API. * <p> * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html"> Cluster Update Settings * API on elastic.co</a> * @deprecated Prefer {@link #putSettings(ClusterUpdateSettingsRequest, RequestOptions)}	i'm not really sure this needs a param tag but it doesn't hurt anything.
@Deprecated public ClusterUpdateSettingsResponse putSettings(ClusterUpdateSettingsRequest clusterUpdateSettingsRequest, Header... headers) throws IOException { return restHighLevelClient.performRequestAndParseEntity(clusterUpdateSettingsRequest, RequestConverters::clusterPutSettings, ClusterUpdateSettingsResponse::fromXContent, emptySet(), headers); } /** * Asynchronously updates cluster wide specific settings using the Cluster Update Settings API. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html"> Cluster Update Settings * API on elastic.co</a> * @param clusterUpdateSettingsRequest the request * @param options the request options (e.g. headers), or {@link RequestOptions#DEFAULT}	i don't think the@return tag adds anything here. i think it is fairly obvious from the type of the response.
public final Engine.Searcher wrap(EngineConfig engineConfig, Engine.Searcher engineSearcher) throws IOException { final ElasticsearchDirectoryReader elasticsearchDirectoryReader = ElasticsearchDirectoryReader.getElasticsearchDirectoryReader(engineSearcher.getDirectoryReader()); if (elasticsearchDirectoryReader == null) { throw new IllegalStateException("Can't wrap non elasticsearch directory reader"); } NonClosingReaderWrapper nonClosingReaderWrapper = new NonClosingReaderWrapper(engineSearcher.getDirectoryReader()); DirectoryReader reader = wrap(nonClosingReaderWrapper); if (reader != nonClosingReaderWrapper) { if (reader.getCoreCacheKey() != elasticsearchDirectoryReader.getCoreCacheKey()) { throw new IllegalStateException("wrapped directory reader doesn't delegate IndexReader#getCoreCacheKey, wrappers must override this method and delegate" + " to the original readers core cache key. Wrapped readers can't used as cache keys since their are used only per request which would lead to subtile bugs"); } if (ElasticsearchDirectoryReader.getElasticsearchDirectoryReader(reader) != elasticsearchDirectoryReader) { // prevent that somebody wraps with a non-filter reader throw new IllegalStateException("wrapped directory reader hides actual ElasticsearchDirectoryReader but shouldn't"); } } IndexSearcher innerIndexSearcher = new IndexSearcher(reader); innerIndexSearcher.setQueryCache(engineConfig.getQueryCache()); innerIndexSearcher.setQueryCachingPolicy(engineConfig.getQueryCachingPolicy()); innerIndexSearcher.setSimilarity(engineConfig.getSimilarity()); // TODO: Right now IndexSearcher isn't wrapper friendly, when it becomes wrapper friendly we should revise this extension point // For example if IndexSearcher#rewrite() is overwritten than also IndexSearcher#createNormalizedWeight needs to be overwritten // This needs to be fixed before we can allow the IndexSearcher from Engine to be wrapped multiple times IndexSearcher indexSearcher = wrap(engineConfig, innerIndexSearcher); if (reader == nonClosingReaderWrapper && indexSearcher == innerIndexSearcher) { return engineSearcher; } else { final Engine.Searcher newSearcher = new Engine.Searcher(engineSearcher.source(), indexSearcher) { @Override public void close() throws ElasticsearchException { try { reader().close(); // we close the reader to make sure wrappers can release resources if needed.... // our NonClosingReaderWrapper makes sure that our reader is not closed } catch (IOException e) { throw new ElasticsearchException("failed to close reader", e); } finally { engineSearcher.close(); } } }; return newSearcher; } }	s/can't used/can't be used/;s/their/they/;s/subtile/subtle/
public void testSearcherWrapperIsUsed() throws IOException { createIndex("test"); ensureGreen(); IndicesService indicesService = getInstanceFromNode(IndicesService.class); IndexService indexService = indicesService.indexService("test"); IndexShard shard = indexService.getShardOrNull(0); client().prepareIndex("test", "test", "0").setSource("{\\\\"foo\\\\" : \\\\"bar\\\\"}").setRefresh(true).get(); client().prepareIndex("test", "test", "1").setSource("{\\\\"foobar\\\\" : \\\\"bar\\\\"}").setRefresh(true).get(); Engine.GetResult getResult = shard.get(new Engine.Get(false, new Term(UidFieldMapper.NAME, Uid.createUid("test", "1")))); assertTrue(getResult.exists()); assertNotNull(getResult.searcher()); getResult.release(); try (Engine.Searcher searcher = shard.acquireSearcher("test")) { TopDocs search = searcher.searcher().search(new TermQuery(new Term("foo", "bar")), 10); assertEquals(search.totalHits, 1); search = searcher.searcher().search(new TermQuery(new Term("foobar", "bar")), 10); assertEquals(search.totalHits, 1); } ShardRouting routing = new ShardRouting(shard.routingEntry()); shard.close("simon says", true); IndexServicesProvider indexServices = indexService.getIndexServices(); IndexSearcherWrapper wrapper = new IndexSearcherWrapper() { @Override public DirectoryReader wrap(DirectoryReader reader) throws IOException { return new FieldMaskingReader("foo", reader); } @Override public IndexSearcher wrap(EngineConfig engineConfig, IndexSearcher searcher) throws EngineException { return searcher; } }; IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper, indexServices.getIndexingMemoryController()); IndexShard newShard = new IndexShard(shard.shardId(), shard.indexSettings, shard.shardPath(), shard.store(), newProvider); try { ShardRoutingHelper.reinit(routing); newShard.updateRoutingEntry(routing, false); DiscoveryNode localNode = new DiscoveryNode("foo", DummyTransportAddress.INSTANCE, Version.CURRENT); assertTrue(newShard.recoverFromStore(routing, localNode)); routing = new ShardRouting(routing); ShardRoutingHelper.moveToStarted(routing); newShard.updateRoutingEntry(routing, true); try (Engine.Searcher searcher = newShard.acquireSearcher("test")) { TopDocs search = searcher.searcher().search(new TermQuery(new Term("foo", "bar")), 10); assertEquals(search.totalHits, 0); search = searcher.searcher().search(new TermQuery(new Term("foobar", "bar")), 10); assertEquals(search.totalHits, 1); } getResult = newShard.get(new Engine.Get(false, new Term(UidFieldMapper.NAME, Uid.createUid("test", "1")))); assertTrue(getResult.exists()); assertNotNull(getResult.searcher()); // make sure get uses the wrapped reader assertTrue(getResult.searcher().reader() instanceof FieldMaskingReader); getResult.release(); // test global ordinals are evicted MappedFieldType foo = newShard.mapperService().indexName("foo"); IndexFieldData.Global ifd = shard.indexFieldDataService().getForField(foo); FieldDataStats before = shard.fieldData().stats("foo"); FieldDataStats after = null; try (Engine.Searcher searcher = newShard.acquireSearcher("test")) { assumeTrue("we have to have more than one segment", searcher.getDirectoryReader().leaves().size() > 1); IndexFieldData indexFieldData = ifd.loadGlobal(searcher.getDirectoryReader()); after = shard.fieldData().stats("foo"); assertEquals(after.getEvictions(), before.getEvictions()); assertTrue(indexFieldData.toString(), after.getMemorySizeInBytes() > before.getMemorySizeInBytes()); } assertEquals(shard.fieldData().stats("foo").getEvictions(), before.getEvictions()); assertEquals(shard.fieldData().stats("foo").getMemorySizeInBytes(), after.getMemorySizeInBytes()); newShard.flush(new FlushRequest().force(true).waitIfOngoing(true)); newShard.refresh("test"); assertEquals(shard.fieldData().stats("foo").getMemorySizeInBytes(), before.getMemorySizeInBytes()); assertEquals(shard.fieldData().stats("foo").getEvictions(), before.getEvictions()); } catch (Throwable t) { t.printStackTrace(); throw t; } finally { newShard.close("just do it", randomBoolean()); } }	is this catch block a left-over?
@Override public void execute(IngestDocument ingestDocument, BiConsumer<IngestDocument, Exception> handler) { Object o = ingestDocument.getFieldValue(field, Object.class, ignoreMissing); if (o == null) { if (ignoreMissing) { handler.accept(ingestDocument, null); } else { handler.accept(null, new IllegalArgumentException("field [" + field + "] is null, cannot loop over its elements.")); } } else if (o instanceof Map) { @SuppressWarnings("unchecked") Map<String, ?> map = (Map<String, ?>) o; List<String> keys = new ArrayList<>(map.keySet()); innerExecuteMap(0, new HashMap<String, Object>(map), keys, new HashMap<>(map.size()), ingestDocument, handler); } else if (o instanceof List) { List<?> list = (List<?>) o; innerExecuteList(0, new ArrayList<>(list), new ArrayList<>(list.size()), ingestDocument, handler); } else { throw new IllegalArgumentException("field [" + field + "] of type [" + o.getClass().getName() + "] cannot be cast to a " + "list or map"); } }	i think we can cast to map<?, ?> instead? then the suppresswarnings can be removed. in order to get the key a cast to string would required, but that shouldn't generate warnings.
public void testMapIteration() { Map<String, Object> mapValue = Map.of("foo", 1, "bar", 2, "baz", 3); IngestDocument ingestDocument = new IngestDocument("_index", "_id", null, null, null, Map.of("field", mapValue)); List<String> encounteredKeys = new ArrayList<>(); List<Object> encounteredValues = new ArrayList<>(); TestProcessor testProcessor = new TestProcessor(id -> { String key = (String) id.getIngestMetadata().get("_key"); Object value = id.getIngestMetadata().get("_value"); encounteredKeys.add(key); encounteredValues.add(value); if (key.equals("bar")) { id.setFieldValue("_ingest._key", "bar2"); } if (key.equals("baz")) { id.setFieldValue("_ingest._value", 33); } }); ForEachProcessor processor = new ForEachProcessor("_tag", null, "field", testProcessor, true); processor.execute(ingestDocument, (result, e) -> {}); assertThat(testProcessor.getInvokedCounter(), equalTo(3)); assertThat(encounteredKeys.toArray(), arrayContainingInAnyOrder("foo", "bar", "baz")); assertThat(encounteredValues.toArray(), arrayContainingInAnyOrder(1, 2, 3)); assertThat(ingestDocument.getFieldValue("field", Map.class).entrySet().toArray(), arrayContainingInAnyOrder(Map.entry("foo", 1), Map.entry("bar2", 2), Map.entry("baz", 33))); }	maybe also add a nested for each tests for maps like the existing testnestedforeach() test?
@Override protected ValuesSourceType defaultValueSourceType() { // TODO: This should probably be DATE, but we're not failing tests with BYTES, so needs more tests? return CoreValuesSourceType.BYTES; }	i'm aware of this todo, i think it's okay to merge to master as is and fix there.
private void writeTestDoc(MappedFieldType fieldType, String fieldName, RandomIndexWriter iw) throws IOException { String typeName = fieldType.typeName(); ValuesSourceType vst = fieldType.getValuesSourceType(); Document doc = new Document(); String json; if (vst.equals(CoreValuesSourceType.NUMERIC)) { long v; if (typeName.equals(NumberFieldMapper.NumberType.DOUBLE.typeName())) { double d = Math.abs(randomDouble()); v = NumericUtils.doubleToSortableLong(d); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"" + d + "\\\\" }"; } else if (typeName.equals(NumberFieldMapper.NumberType.FLOAT.typeName())) { float f = Math.abs(randomFloat()); v = NumericUtils.floatToSortableInt(f); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"" + f + "\\\\" }"; } else if (typeName.equals(NumberFieldMapper.NumberType.HALF_FLOAT.typeName())) { float f = Math.abs(randomFloat()); v = HalfFloatPoint.halfFloatToSortableShort(f); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"" + f + "\\\\" }"; } else { // smallest numeric is a byte so we select the smallest v = Math.abs(randomByte()); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"" + v + "\\\\" }"; } doc.add(new SortedNumericDocValuesField(fieldName, v)); } else if (vst.equals(CoreValuesSourceType.BYTES)) { if (typeName.equals(BinaryFieldMapper.CONTENT_TYPE)) { doc.add(new BinaryFieldMapper.CustomBinaryDocValuesField(fieldName, new BytesRef("a").bytes)); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"a\\\\" }"; } else { doc.add(new SortedSetDocValuesField(fieldName, new BytesRef("a"))); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"a\\\\" }"; } } else if (vst.equals(CoreValuesSourceType.DATE)) { // positive integer because date_nanos gets unhappy with large longs long v; v = Math.abs(randomInt()); doc.add(new SortedNumericDocValuesField(fieldName, v)); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"" + v + "\\\\" }"; } else if (vst.equals(CoreValuesSourceType.BOOLEAN)) { long v; v = randomBoolean() ? 0 : 1; doc.add(new SortedNumericDocValuesField(fieldName, v)); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"" + (v == 0 ? "false" : "true") + "\\\\" }"; } else if (vst.equals(CoreValuesSourceType.IP)) { InetAddress ip = randomIp(randomBoolean()); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"" + NetworkAddress.format(ip) + "\\\\" }"; doc.add(new SortedSetDocValuesField(fieldName, new BytesRef(InetAddressPoint.encode(ip)))); } else if (vst.equals(CoreValuesSourceType.RANGE)) { Object start; Object end; RangeType rangeType; if (typeName.equals(RangeType.DOUBLE.typeName())) { start = randomDouble(); end = RangeType.DOUBLE.nextUp(start); rangeType = RangeType.DOUBLE; } else if (typeName.equals(RangeType.FLOAT.typeName())) { start = randomFloat(); end = RangeType.FLOAT.nextUp(start); rangeType = RangeType.DOUBLE; } else if (typeName.equals(RangeType.IP.typeName())) { boolean v4 = randomBoolean(); start = randomIp(v4); end = RangeType.IP.nextUp(start); rangeType = RangeType.IP; } else if (typeName.equals(RangeType.LONG.typeName())) { start = randomLong(); end = RangeType.LONG.nextUp(start); rangeType = RangeType.LONG; } else if (typeName.equals(RangeType.INTEGER.typeName())) { start = randomInt(); end = RangeType.INTEGER.nextUp(start); rangeType = RangeType.INTEGER; } else if (typeName.equals(RangeType.DATE.typeName())) { start = randomNonNegativeLong(); end = RangeType.DATE.nextUp(start); rangeType = RangeType.DATE; } else { throw new IllegalStateException("Unknown type of range [" + typeName + "]"); } final RangeFieldMapper.Range range = new RangeFieldMapper.Range(rangeType, start, end, true, true); doc.add(new BinaryDocValuesField(fieldName, rangeType.encodeRanges(Collections.singleton(range)))); json = "{ \\\\"" + fieldName + "\\\\" : { \\\\n" + " \\\\"gte\\\\" : \\\\"" + start + "\\\\",\\\\n" + " \\\\"lte\\\\" : \\\\"" + end + "\\\\"\\\\n" + " }}"; } else if (vst.equals(CoreValuesSourceType.GEOPOINT)) { double lat = randomDouble(); double lon = randomDouble(); doc.add(new LatLonDocValuesField(fieldName, lat, lon)); json = "{ \\\\"" + fieldName + "\\\\" : \\\\"[" + lon + "," + lat + "]\\\\" }"; } else { throw new IllegalStateException("Unknown field type [" + typeName + "]"); } doc.add(new StoredField("_source", new BytesRef(json))); iw.addDocument(doc); }	i believe we lost the check for date_nanos (datefieldmapper.date_nanos_content_type)) which i think will eventually break? we can do a quick follow-on pr instead of changing this one imo.
@Override public void beforeRefresh() throws IOException { old = current; ramBytesUsed.set(0); // Start sending all updates after this point to the new // map. While reopen is running, any lookup will first // try this new map, then fallback to old, then to the // current searcher: current = ConcurrentCollections.newConcurrentMapWithAggressiveConcurrency(); }	which includedeletes? i can't find that option..
@Test @Slow public void testDeleteNotLost() throws Exception { // We require only one shard for this test, so that the 2nd delete provokes pruning the deletes map: client() .admin() .indices() .prepareCreate("test") .setSettings(ImmutableSettings.settingsBuilder() .put("index.number_of_shards", 1)) .execute(). actionGet(); ensureGreen(); HashMap<String,Object> newSettings = new HashMap<>(); newSettings.put("index.gc_deletes", "10ms"); newSettings.put("index.refresh_interval", "10000s"); client() .admin() .indices() .prepareUpdateSettings("test") .setSettings(newSettings) .execute() .actionGet(); // Index a doc: client() .prepareIndex("test", "type", "id") .setSource("foo", "bar") .setOpType(IndexRequest.OpType.INDEX) .setVersion(10) .setVersionType(VersionType.EXTERNAL) .execute() .actionGet(); // Force refresh so the add is visible in the searcher: refresh(); // Delete it client() .prepareDelete("test", "type", "id") .setVersion(11) .setVersionType(VersionType.EXTERNAL) .execute() .actionGet(); // Real-time get should reflect delete: assertThat("doc should have been deleted", client() .prepareGet("test", "type", "id") .execute() .actionGet() .getVersion(), equalTo(-1L)); // ThreadPool.estimatedTimeInMillis has default granularity of 200 msec, so we must sleep at least that long; sleep much longer in // case system is busy: Thread.sleep(1000); // Delete an unrelated doc (provokes pruning deletes from versionMap) client() .prepareDelete("test", "type", "id2") .setVersion(11) .setVersionType(VersionType.EXTERNAL) .execute() .actionGet(); // Real-time get should still reflect delete: assertThat("doc should have been deleted", client() .prepareGet("test", "type", "id") .execute() .actionGet() .getVersion(), equalTo(-1L)); }	i think we can put this under a randomboolean, no? the test should work either way and we get to test two different paths...
private void singleFlush(WriteOperation headOp) throws IOException { try { headOp.flush(); } catch (IOException e) { headOp.getListener().onFailure(e); headOp.close(); throw e; } if (headOp.isFullyFlushed()) { headOp.getListener().onResponse(channel); headOp.close(); } else { queued.push(headOp); } }	maybe do the close in a finally if headop.getlistener().onfailure(e); barfs?
*/ public void deleteAsync(DeleteIndexRequest deleteIndexRequest, RequestOptions options, ActionListener<DeleteIndexResponse> listener) { restHighLevelClient.performRequestAsyncAndParseEntity(deleteIndexRequest, RequestConverters::deleteIndex, options, DeleteIndexResponse::fromXContent, listener, emptySet()); } /** * Asynchronously deletes an index using the Delete Index API. * <p> * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-delete-index.html"> * Delete Index API on elastic.co</a> * @deprecated Prefer {@link #deleteAsync(DeleteIndexRequest, RequestOptions, ActionListener)}	nit: can we call it gettemplate? that's what our spec have, without the final 's'
@Override public Number getKeyAsNumber() { // this method is needed for scripted numeric aggs try { return Long.parseLong(termBytes.utf8ToString()); } catch (NumberFormatException ignored) { //failed to detect an Integer } return Double.parseDouble(termBytes.utf8ToString()); }	@matarrese this is still not what we would want in a comment here. we want to explain why we doing this. please look at the original issue that provoked this change and leave a comment here that explains the why behind first parsing as a long, and then leniently parsing as a double.
* @param listener The listener to pass the authentication result to */ public abstract void authenticate(AuthenticationToken token, ActionListener<AuthenticationResult> listener); /** * Looks up the user identified by the string identifier. A successful lookup * will call the {@link ActionListener#onResponse} with the {@link User} * identified by the username. An unsuccessful lookup call with {@code null} as * the argument. If lookup is not supported, simply return {@code null} when * called.<p> * Default implementation iterates over the list of realms to search user. If it * finds a user then that {@link User} is returned as the argument else it * iterates on the next lookup realm from the list. At the end if it does not * find any user then it will return {@code null} as the argument.<p> * If this realm does not depend on any other realms for lookup, it always * returns {@code null}	why would we need this? why would you want the pki realm to return true when it is just doing a lookup on the ldap realm? can't the ldap realm just handle it itself?
private void lookupRunAsUser(final User user, String runAsUsername, Consumer<User> userConsumer) { final List<Realm> realmsList = realms.asList().stream().filter((realm) -> realm.hasLookupDependency() == false).collect(Collectors.toList()); final BiConsumer<Realm, ActionListener<User>> realmLookupConsumer = (realm, lookupUserListener) -> realm.lookupUser(runAsUsername, ActionListener.wrap((lookedupUser) -> { if (lookedupUser != null) { lookedupBy = new RealmRef(realm.name(), realm.type(), nodeName); lookupUserListener.onResponse(lookedupUser); } else { lookupUserListener.onResponse(null); } }, lookupUserListener::onFailure)); final IteratingActionListener<User, Realm> userLookupListener = new IteratingActionListener<>(ActionListener.wrap((lookupUser) -> { if (lookupUser == null) { // the user does not exist, but we still create a User object, which will later be rejected by authz userConsumer.accept(new User(runAsUsername, null, user)); } else { userConsumer.accept(new User(lookupUser, user)); } }, (e) -> listener.onFailure(request.exceptionProcessingRequest(e, authenticationToken))), realmLookupConsumer, realmsList, threadContext); try { userLookupListener.run(); } catch (Exception e) { listener.onFailure(request.exceptionProcessingRequest(e, authenticationToken)); } }	i think this change only exists because your made that change to lookupuser to delegate to lookup_realms; if we drop that change then we can drop this too.
protected LogicalPlan skipPlan(Filter filter) { return Optimizer.skipPlan(filter); } } static class SkipQueryOnLimitZero extends org.elasticsearch.xpack.ql.optimizer.OptimizerRules.SkipQueryOnLimitZero { @Override protected LogicalPlan skipPlan(Limit limit) { return Optimizer.skipPlan(limit); } } private static LogicalPlan skipPlan(UnaryPlan plan) { return new LocalRelation(plan.source(), new EmptyExecutable(plan.output())); } static class SkipQueryIfFoldingProjection extends OptimizerRule<LogicalPlan> { @Override protected LogicalPlan rule(LogicalPlan plan) { Holder<LocalRelation> optimizedPlan = new Holder<>(); var leafRelation = leafRelation(plan); // exclude LocalRelations that have been introduced by earlier optimizations (skipped ESRelations) var isNonSkippedLocalRelation = leafRelation instanceof LocalRelation && ((LocalRelation) leafRelation).executable() instanceof EmptyExecutable == false; if (isNonSkippedLocalRelation) { plan.forEachDown(Project.class, p -> { List<Object> values = extractConstants(p.projections()); if (values.size() != p.projections().size()) { throw new IllegalStateException("Trying to execute non-constant projection in local relation."); } if (optimizedPlan.get() != null) { throw new IllegalStateException("More than one Project found."); } optimizedPlan.set(new LocalRelation(p.source(), new SingletonExecutable(p.output(), values.toArray()))); }); } if (optimizedPlan.get() != null) { return optimizedPlan.get(); } plan.forEachDown(Aggregate.class, a -> { List<Object> values = extractConstants(a.aggregates()); // aggregations on only constant values like "SELECT 'foo' FROM test GROUP BY 1" // can also be executed locally var onlyConstantAggregations = leafRelation instanceof EsRelation && a.groupings().isEmpty() && values.size() == a.aggregates().size(); if (isNonSkippedLocalRelation || onlyConstantAggregations) { if (values.size() != a.aggregates().size()) { throw new IllegalStateException("Trying to execute non-constant aggregation in local relation."); } if (optimizedPlan.get() != null) { throw new IllegalStateException("More than one Aggregate found."); } optimizedPlan.set(new LocalRelation(a.source(), new SingletonExecutable(a.output(), values.toArray()))); } }); if (optimizedPlan.get() != null) { return optimizedPlan.get(); } return plan; } private LeafPlan leafRelation(LogicalPlan plan) { var result = new Holder<LeafPlan>(); plan.forEachDown(LeafPlan.class, result::set); return result.get(); } private List<Object> extractConstants(List<? extends NamedExpression> named) { List<Object> values = new ArrayList<>(); for (NamedExpression n : named) { if (n instanceof Alias) { Alias a = (Alias) n; if (a.child().foldable()) { values.add(a.child().fold()); } // not everything is foldable, bail out early else { return values; } } else if (n.foldable()) { values.add(n.fold()); } else { // not everything is foldable, bail-out early return values; } } return values; } } abstract static class OptimizerBasicRule extends Rule<LogicalPlan, LogicalPlan> { @Override public abstract LogicalPlan apply(LogicalPlan plan); @Override protected LogicalPlan rule(LogicalPlan plan) { return plan; }	personally, i'm not fun of var in the code, i like more in the tests for simplification, but let's wait for the opinion of the others.
@Override protected LogicalPlan rule(LogicalPlan plan) { Holder<LocalRelation> optimizedPlan = new Holder<>(); var leafRelation = leafRelation(plan); // exclude LocalRelations that have been introduced by earlier optimizations (skipped ESRelations) var isNonSkippedLocalRelation = leafRelation instanceof LocalRelation && ((LocalRelation) leafRelation).executable() instanceof EmptyExecutable == false; if (isNonSkippedLocalRelation) { plan.forEachDown(Project.class, p -> { List<Object> values = extractConstants(p.projections()); if (values.size() != p.projections().size()) { throw new IllegalStateException("Trying to execute non-constant projection in local relation."); } if (optimizedPlan.get() != null) { throw new IllegalStateException("More than one Project found."); } optimizedPlan.set(new LocalRelation(p.source(), new SingletonExecutable(p.output(), values.toArray()))); }); } if (optimizedPlan.get() != null) { return optimizedPlan.get(); } plan.forEachDown(Aggregate.class, a -> { List<Object> values = extractConstants(a.aggregates()); // aggregations on only constant values like "SELECT 'foo' FROM test GROUP BY 1" // can also be executed locally var onlyConstantAggregations = leafRelation instanceof EsRelation && a.groupings().isEmpty() && values.size() == a.aggregates().size(); if (isNonSkippedLocalRelation || onlyConstantAggregations) { if (values.size() != a.aggregates().size()) { throw new IllegalStateException("Trying to execute non-constant aggregation in local relation."); } if (optimizedPlan.get() != null) { throw new IllegalStateException("More than one Aggregate found."); } optimizedPlan.set(new LocalRelation(a.source(), new SingletonExecutable(a.output(), values.toArray()))); } }); if (optimizedPlan.get() != null) { return optimizedPlan.get(); } return plan; }	might be worth adding this statement (or similar) into the optimiser tests too, imo.
public void testRuntimeFieldAndArrayChildren() throws IOException { DocumentMapper mapper = createDocumentMapper(topMapping(b -> { b.field("dynamic", "true"); b.startObject("runtime"); { b.startObject("object").field("type", "test").endObject(); } b.endObject(); })); { ParsedDocument doc = mapper.parse(source(b -> { b.startObject("object"); b.array("array", 1, 2, 3); b.field("foo", "bar"); b.endObject(); })); assertNotNull(doc.rootDoc().getField("object.foo")); assertNotNull(doc.rootDoc().getField("object.array")); } { ParsedDocument doc = mapper.parse(source(b -> { b.startArray("object"); { b.startObject().array("array", 1, 2, 3).endObject(); b.startObject().field("foo", "bar").endObject(); } b.endArray(); })); assertNotNull(doc.rootDoc().getField("object.foo")); assertNotNull(doc.rootDoc().getField("object.array")); } }	maybe calling object is misleading given that it can end up being an array in the docs?
public void testRuntimeFieldAndArrayChildren() throws IOException { DocumentMapper mapper = createDocumentMapper(topMapping(b -> { b.field("dynamic", "true"); b.startObject("runtime"); { b.startObject("object").field("type", "test").endObject(); } b.endObject(); })); { ParsedDocument doc = mapper.parse(source(b -> { b.startObject("object"); b.array("array", 1, 2, 3); b.field("foo", "bar"); b.endObject(); })); assertNotNull(doc.rootDoc().getField("object.foo")); assertNotNull(doc.rootDoc().getField("object.array")); } { ParsedDocument doc = mapper.parse(source(b -> { b.startArray("object"); { b.startObject().array("array", 1, 2, 3).endObject(); b.startObject().field("foo", "bar").endObject(); } b.endArray(); })); assertNotNull(doc.rootDoc().getField("object.foo")); assertNotNull(doc.rootDoc().getField("object.array")); } }	shall we add one scenario where object is an array of e.g. longs?
public void testRuntimeFieldAndArrayChildren() throws IOException { DocumentMapper mapper = createDocumentMapper(topMapping(b -> { b.field("dynamic", "true"); b.startObject("runtime"); { b.startObject("object").field("type", "test").endObject(); } b.endObject(); })); { ParsedDocument doc = mapper.parse(source(b -> { b.startObject("object"); b.array("array", 1, 2, 3); b.field("foo", "bar"); b.endObject(); })); assertNotNull(doc.rootDoc().getField("object.foo")); assertNotNull(doc.rootDoc().getField("object.array")); } { ParsedDocument doc = mapper.parse(source(b -> { b.startArray("object"); { b.startObject().array("array", 1, 2, 3).endObject(); b.startObject().field("foo", "bar").endObject(); } b.endArray(); })); assertNotNull(doc.rootDoc().getField("object.foo")); assertNotNull(doc.rootDoc().getField("object.array")); } }	nit: maybe dynamicmappingstests#dynamicmapping should be moved to mapperservicetestcase? also, isn't dynamic true the default anyways?
public void registerFunctionScoreParser(ScoreFunctionParser<?> parser) { for (String name: parser.getNames()) { Object oldValue = functionScoreParsers.putIfAbsent(name, parser); if (oldValue != null) { throw new IllegalArgumentException("Function score parser [" + oldValue + "] already registered for name [" + name + "]"); } } namedWriteableRegistry.registerPrototype(ScoreFunctionBuilder.class, parser.getBuilderPrototype()); }	did you mean name here instead of oldvalue?
public IndicesQueriesRegistry buildQueryParserRegistry() { Map<String, QueryParser<?>> queryParsersMap = new HashMap<>(); for (Supplier<QueryParser<?>> parserSupplier : queryParsers) { QueryParser<?> parser = parserSupplier.get(); for (String name: parser.names()) { Object oldValue = queryParsersMap.putIfAbsent(name, parser); if (oldValue != null) { throw new IllegalArgumentException("Query parser [" + oldValue + "] already registered for name [" + name + "] while trying to register [" + parser + "]"); } } namedWriteableRegistry.registerPrototype(QueryBuilder.class, parser.getBuilderPrototype()); } return new IndicesQueriesRegistry(settings, queryParsersMap); }	did you mean name here instead of oldvalue?
public static FetchDocValuesContext create(MapperService mapperService, List<FieldAndFormat> fieldPatterns) { int maxAllowedDocvalueFields = mapperService.getIndexSettings().getMaxDocvalueFields(); List<FieldAndFormat> fields = new ArrayList<>(); int mappedFieldCount = 0; for (FieldAndFormat field : fieldPatterns) { Collection<String> fieldNames = mapperService.simpleMatchToFullName(field.field); for (String fieldName: fieldNames) { fields.add(new FieldAndFormat(fieldName, field.format)); // only count the mapped fields towards the limit fedinde by IndexSettings.MAX_DOCVALUE_FIELDS_SEARCH_SETTING if (mapperService.fieldType(fieldName) != null) { mappedFieldCount++; } } } if (mappedFieldCount > maxAllowedDocvalueFields) { throw new IllegalArgumentException( "Trying to retrieve too many mapped docvalue_fields. Must be less than or equal to: [" + maxAllowedDocvalueFields + "] but was [" + mappedFieldCount + "]. This limit can be set by changing the [" + IndexSettings.MAX_DOCVALUE_FIELDS_SEARCH_SETTING.getKey() + "] index level setting."); } return new FetchDocValuesContext(fields); }	for efficiency could we only create a new fieldandformat if the field is mapped? i think we'll end up dropping the field anyways, not even having an entry in the response for it. also small typo, fedinde -> defined.
Predicate<AuditEventMetaInfo> ignorePredicate() { return eventInfo -> { final Collection<String> privileges = IndexPrivilege.findPrivilegesThatGrant(eventInfo.action); return eventInfo.principal != null && ignorePrincipalsPredicate.test(eventInfo.principal) && eventInfo.realm != null && ignoreRealmsPredicate.test(eventInfo.realm) && eventInfo.action != null && ignoreActionsPredicate.test(eventInfo.action) && eventInfo.roles.get().allMatch(role -> role != null && ignoreRolesPredicate.test(role)) && eventInfo.indices.get().allMatch(index -> index != null && ignoreIndicesPredicate.test(index)); }; }	this line is a leftover.
private List<String> randomNonEmptyListOfFilteredActions() { final List<String> filtered = new ArrayList<>(4); final String[] actionPatterns = { "internal:transport/proxy/indices:*", "indices:data/read/*", "internal:transport/proxy/indices:data/read/*", "indices:data/write/index*", "indices:data/write/bulk*", "indices:data/write/index", "indices:data/write/index[*", "indices:data/write/index:op_type/create", "indices:data/write/update*", "indices:data/write/delete*", "indices:data/write/*", "indices:monitor/*", "indices:admin/*", "indices:admin/ilm/*", "indices:admin/refresh*", "indices:admin/flush*", "indices:admin/synced_flush", "indices:admin/forcemerge*", "cluster:admin/xpack/security/*", "cluster:admin/xpack/security/saml/*", "cluster:admin/xpack/security/oidc/*", "cluster:admin/xpack/security/token/*", "cluster:admin/xpack/security/api_key/*", "cluster:monitor/*", "cluster:monitor/xpack/ml/*", "cluster:monitor/text_structure/*", "cluster:monitor/data_frame/*", "cluster:monitor/xpack/watcher/*", "cluster:monitor/xpack/rollup/*", "cluster:*", "indices:admin/index_template/*", "indices:admin/data_stream/*", "cluster:admin/xpack/ml/*", "cluster:admin/data_frame/*", "cluster:monitor/data_frame/*", "cluster:monitor/transform/*", "cluster:admin/transform/*", "cluster:admin/xpack/watcher/*", "cluster:monitor/nodes/liveness", "cluster:monitor/state", "indices:admin/template/*", "cluster:admin/component_template/*", "cluster:admin/ingest/pipeline/*", "cluster:admin/xpack/rollup/*", "cluster:admin/xpack/ccr/*", "cluster:admin/ilm/*", "cluster:admin/slm/*", "cluster:admin/xpack/enrich/*"}; Random random = random(); for (int i = 0; i < randomIntBetween(1, 4); i++) { Object name = actionPatterns[random.nextInt(actionPatterns.length)]; filtered.add((String)name); } return filtered; }	nit you can use randomsubsetof( and have a shorter list.
static Request closeJob(CloseJobRequest closeJobRequest) { String endpoint = new EndpointBuilder() .addPathPartAsIs("_xpack") .addPathPartAsIs("ml") .addPathPartAsIs("anomaly_detectors") .addPathPart(closeJobRequest.getCommaDelimitedJobIdString()) .addPathPartAsIs("_close") .build(); Request request = new Request(HttpPost.METHOD_NAME, endpoint); RequestConverters.Params params = new RequestConverters.Params(request); params.putParam("force", Boolean.toString(closeJobRequest.isForce())); params.putParam("allow_no_jobs", Boolean.toString(closeJobRequest.isAllowNoJobs())); if (closeJobRequest.getTimeout() != null) { params.putParam("timeout", closeJobRequest.getTimeout().getStringRep()); } return request; }	the need to pass these as params instead of serializing the request revealed a bug: we do not support sending these params as the request body for the close job api. it'd be nice to fix that. then this converter can follow suit with all the others.
*/ public void openJobAsync(OpenJobRequest request, RequestOptions options, ActionListener<OpenJobResponse> listener) { restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::openJob, options, OpenJobResponse::fromXContent, listener, Collections.emptySet()); } /** * Closes Machine Learning Jobs * Closes one or more jobs. A job can be opened and closed multiple times throughout its lifecycle. * * A closed job cannot receive data or perform analysis operations, but you can still explore and navigate results. * * @param request request containing jobIds and additional optional options * @param options Additional request options (e.g. headers), use {@link RequestOptions#DEFAULT}	i don't think we should use jobids here. we refer to it as job_id in the open job doc. perhaps rephrase like request containing the job_id for each job to close and additional optional options. hm, typing optional options also felt funny. perhaps remove optional from both?
* @throws IOException when there is a serialization issue sending the request or receiving the response */ public CloseJobResponse closeJob(CloseJobRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::closeJob, options, CloseJobResponse::fromXContent, Collections.emptySet()); } /** * Closes Machine Learning Jobs asynchronously, notifies listener on completion * Closes one or more jobs. A job can be opened and closed multiple times throughout its lifecycle. * * A closed job cannot receive data or perform analysis operations, but you can still explore and navigate results. * * @param request request containing jobIds and additional optional options * @param options Additional request options (e.g. headers), use {@link RequestOptions#DEFAULT}	similar comments as above.
public void testJsonFields() throws Exception { XContentBuilder mapping = XContentFactory.jsonBuilder().startObject() .startObject("type") .startObject("properties") .startObject("headers") .field("type", "json") .endObject() .endObject() .endObject() .endObject(); assertAcked(prepareCreate("test").addMapping("type", mapping)); IndexRequestBuilder indexRequest = client().prepareIndex("test", "type", "1") .setSource(XContentFactory.jsonBuilder() .startObject() .startObject("headers") .field("content-type", "application/json") .endObject() .endObject()); indexRandom(true, false, indexRequest); SearchResponse searchResponse = client().prepareSearch() .setQuery(existsQuery("headers")) .get(); assertHitCount(searchResponse, 1L); searchResponse = client().prepareSearch() .setQuery(existsQuery("headers.content-type")) .get(); assertHitCount(searchResponse, 1L); searchResponse = client().prepareSearch() .setQuery(existsQuery("headers.nonexistent")) .get(); assertHitCount(searchResponse, 0L); }	not especially related to the current pr, but how is case normalization being handled? the normal http header here is content-type with a capital c; is there a lowercase filter being applied anywhere, or is that being ignored for the moment?
protected void handleException(ActionType actionType, RestRequest request, RestChannel channel, Exception e) { logger.debug(new ParameterizedMessage("{} failed for REST request [{}]", actionType.name(), request.uri()), e); final RestStatus restStatus = ExceptionsHelper.status(e); try { channel.sendResponse(new BytesRestResponse(channel, restStatus, e) { @Override protected boolean skipStackTrace() { return restStatus == RestStatus.UNAUTHORIZED; } @Override public Map<String, List<String>> filterHeaders(Map<String, List<String>> headers) { if (actionType != ActionType.RequestHandling || (restStatus == RestStatus.UNAUTHORIZED || restStatus == RestStatus.FORBIDDEN)) { if (headers.containsKey("Warning")) { headers = Maps.copyMapWithRemovedEntry(headers, "Warning"); } if (headers.containsKey("X-elastic-product")) { headers = Maps.copyMapWithRemovedEntry(headers, "X-elastic-product"); } } return headers; } }); } catch (Exception inner) { inner.addSuppressed(e); logger.error((Supplier<?>) () -> new ParameterizedMessage("failed to send failure response for uri [{}]", request.uri()), inner); } }	do we want to remove warning headers on 403 (forbidden)? a 403 means authentication is successful. iiuc, the original intention is to remove warnings if authentication fails, i.e. 401.
public void testPrimaryActionRejectsWrongAidOrWrongTerm() throws Exception { final String index = "test"; final ShardId shardId = new ShardId(index, "_na_", 0); setState(clusterService, state(index, true, ShardRoutingState.STARTED)); final ShardRouting primary = clusterService.state().routingTable().shardRoutingTable(shardId).primaryShard(); final long primaryTerm = clusterService.state().metaData().index(shardId.getIndexName()).primaryTerm(shardId.id()); PlainActionFuture<TestResponse> listener = new PlainActionFuture<>(); final boolean wrongAllocationId = randomBoolean(); final long wrongTerm = primaryTerm + randomIntBetween(1, 10); Request request = new Request(shardId).timeout("1ms"); action.new PrimaryOperationTransportHandler().messageReceived( new TransportReplicationAction.ConcreteShardRequest<>(request, wrongAllocationId ? "_not_a_valid_aid_" : primary.allocationId().getId(), wrongTerm), createTransportChannel(listener), maybeTask() ); try { listener.get(); fail("using a wrong aid didn't fail the operation"); } catch (ExecutionException execException) { Throwable throwable = execException.getCause(); logger.debug("got exception:" , throwable); assertTrue(throwable.getClass() + " is not a retry exception", action.retryPrimaryException(throwable)); if (wrongAllocationId) { assertThat(throwable.getMessage(), containsString("expected aID [_not_a_valid_aid_] but found [" + primary.allocationId().getId() + "]")); } else { assertThat(throwable.getMessage(), containsString("expected aID [" + primary.allocationId().getId() + "] with term [" + wrongTerm + "] but found [" + primaryTerm + "]")); } } }	nit: this doesn't allow for the right term and the wrong aid. something that can't really happen, but is good to test imo.
public static Aggregator.Parser getParser(ParseFieldRegistry<SignificanceHeuristicParser> significanceHeuristicParserRegistry) { ObjectParser<SignificantTermsAggregationBuilder, Void> aggregationParser = new ObjectParser<>(SignificantTermsAggregationBuilder.NAME); ValuesSourceParserHelper.declareAnyFields(aggregationParser, true, true); aggregationParser.declareInt(SignificantTermsAggregationBuilder::shardSize, TermsAggregationBuilder.SHARD_SIZE_FIELD_NAME); aggregationParser.declareLong(SignificantTermsAggregationBuilder::minDocCount, TermsAggregationBuilder.MIN_DOC_COUNT_FIELD_NAME); aggregationParser.declareLong(SignificantTermsAggregationBuilder::shardMinDocCount, TermsAggregationBuilder.SHARD_MIN_DOC_COUNT_FIELD_NAME); aggregationParser.declareInt(SignificantTermsAggregationBuilder::size, TermsAggregationBuilder.REQUIRED_SIZE_FIELD_NAME); aggregationParser.declareString(SignificantTermsAggregationBuilder::executionHint, TermsAggregationBuilder.EXECUTION_HINT_FIELD_NAME); aggregationParser.declareObject(SignificantTermsAggregationBuilder::backgroundFilter, (p, context) -> parseInnerQueryBuilder(p), SignificantTermsAggregationBuilder.BACKGROUND_FILTER); aggregationParser.declareField((b, v) -> b.includeExclude(IncludeExclude.merge(v, b.includeExclude())), IncludeExclude::parseInclude, IncludeExclude.INCLUDE_FIELD, ObjectParser.ValueType.OBJECT_ARRAY_OR_STRING); aggregationParser.declareField((b, v) -> b.includeExclude(IncludeExclude.merge(b.includeExclude(), v)), IncludeExclude::parseExclude, IncludeExclude.EXCLUDE_FIELD, ObjectParser.ValueType.STRING_ARRAY); for (String name : significanceHeuristicParserRegistry.getNames()) { aggregationParser.declareObject(SignificantTermsAggregationBuilder::significanceHeuristic, (p, context) -> { SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserRegistry .lookupReturningNullIfNotFound(name, LoggingDeprecationHandler.INSTANCE); return significanceHeuristicParser.parse(p); }, new ParseField(name)); } return new Aggregator.Parser() { @Override public AggregationBuilder parse(String aggregationName, XContentParser parser) throws IOException { return aggregationParser.parse(parser, new SignificantTermsAggregationBuilder(aggregationName, null), null); } }; }	i think this should be p.getdeprecationhandler().
private String buildDate() { StringBuilder sb = new StringBuilder(); int length = randomIntBetween(4, 9); if (randomBoolean()) { sb.append('-'); } else { if (length > 4) { sb.append('-'); } } for (int i = 1; i <= length; i++) { sb.append(i); } sb.append("-05-10"); return sb.toString(); }	i don't understand this. why is the date starting with -?
@SuppressForbidden(reason = "Channel is based of a socket not a file") @Override public int write(ByteBuffer src) throws IOException { return SocketAccess.doPrivilegedIOException(() -> writeChannel.write(src)); } })); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Uploads a blob using the "multipart upload" method (a single * 'multipart/related' request containing both data and metadata. The request is * gziped), see: * https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload * @param blobInfo the info for the blob to be uploaded * @param inputStream the stream containing the blob data * @param blobSize the size * @param failIfAlreadyExists whether to throw a FileAlreadyExistsException if the given blob already exists */ private void writeBlobMultipart(BlobInfo blobInfo, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) throws IOException { assertLargeBlobUseMultipartUpload(blobSize); final ByteArrayOutputStream baos = new ByteArrayOutputStream(Math.toIntExact(blobSize)); Streams.copy(inputStream, baos); try { final Storage.BlobTargetOption[] targetOptions = failIfAlreadyExists ? new Storage.BlobTargetOption[] { Storage.BlobTargetOption.doesNotExist() } : new Storage.BlobTargetOption[0]; SocketAccess.doPrivilegedVoidIOException( () -> client().create(blobInfo, baos.toByteArray(), targetOptions)); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } // overridable for testing void assertLargeBlobUseMultipartUpload(final long blobSize) { assert blobSize <= LARGE_BLOB_THRESHOLD_BYTE_SIZE : "large blob uploads should use the resumable upload method"; } /** * Deletes the blob from the specific bucket * * @param blobName name of the blob */ void deleteBlob(String blobName) throws IOException { final BlobId blobId = BlobId.of(bucketName, blobName); final boolean deleted = SocketAccess.doPrivilegedIOException(() -> client().delete(blobId)); if (deleted == false) { throw new NoSuchFileException("Blob [" + blobName + "] does not exist"); } } /** * Deletes the given path and all its children. * * @param pathStr Name of path to delete */ DeleteResult deleteDirectory(String pathStr) throws IOException { return SocketAccess.doPrivilegedIOException(() -> { DeleteResult deleteResult = DeleteResult.ZERO; Page<Blob> page = client().get(bucketName).list(BlobListOption.prefix(pathStr)); do { final Collection<String> blobsToDelete = new ArrayList<>(); final AtomicLong blobsDeleted = new AtomicLong(0L); final AtomicLong bytesDeleted = new AtomicLong(0L); page.getValues().forEach(b -> { blobsToDelete.add(b.getName()); blobsDeleted.incrementAndGet(); bytesDeleted.addAndGet(b.getSize()); }); deleteBlobsIgnoringIfNotExists(blobsToDelete); deleteResult = deleteResult.add(blobsDeleted.get(), bytesDeleted.get()); page = page.getNextPage(); } while (page != null); return deleteResult; }); } /** * Deletes multiple blobs from the specific bucket using a batch request * * @param blobNames names of the blobs to delete */ void deleteBlobsIgnoringIfNotExists(Collection<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } final List<BlobId> blobIdsToDelete = blobNames.stream().map(blob -> BlobId.of(bucketName, blob)).collect(Collectors.toList()); final List<BlobId> failedBlobs = Collections.synchronizedList(new ArrayList<>()); final StorageException e = SocketAccess.doPrivilegedIOException(() -> { final AtomicReference<StorageException> ioe = new AtomicReference<>(); final StorageBatch batch = client().batch(); for (BlobId blob : blobIdsToDelete) { batch.delete(blob).notify( new BatchResult.Callback<>() { @Override public void success(Boolean result) { } @Override public void error(StorageException exception) { if (exception.getCode() != HTTP_NOT_FOUND) { failedBlobs.add(blob); if (ioe.compareAndSet(null, exception) == false) { ioe.get().addSuppressed(exception); } } } }); } batch.submit(); return ioe.get(); }	i think it would be nicer to read and less error-prone if we followed the standard pattern here and made assertlargeblobusemultipartupload return boolean so that we can call assertlargeblobusemultipartupload(blobsize);
public void testSnapshotWithLargeSegmentFiles() throws Exception { final String repository = createRepository("repository", Settings.builder() .put("compress", "false") .put(GoogleCloudStorageRepository.CHUNK_SIZE.getKey(), GoogleCloudStorageRepository.CHUNK_SIZE.getDefault(Settings.EMPTY)) .build()); final String index = "index-no-merges"; createIndex(index, Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put(IndexSettings.INDEX_REFRESH_INTERVAL_SETTING.getKey(), TimeValue.MINUS_ONE) .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false) .build()); final int nbDocs = 10_000; try (BackgroundIndexer indexer = new BackgroundIndexer(index, "_doc", client(), nbDocs)) { awaitBusy(() -> indexer.totalIndexedDocs() >= nbDocs); } flushAndRefresh(index); assertHitCount(client().prepareSearch(index).setSize(0).setTrackTotalHits(true).get(), nbDocs); assertSuccessfulSnapshot(client().admin().cluster().prepareCreateSnapshot(repository, "snapshot") .setWaitForCompletion(true).setIndices(index)); assertAcked(client().admin().indices().prepareDelete(index)); assertSuccessfulRestore(client().admin().cluster().prepareRestoreSnapshot(repository, "snapshot").setWaitForCompletion(true)); ensureGreen(index); assertHitCount(client().prepareSearch(index).setSize(0).setTrackTotalHits(true).get(), nbDocs); }	why do we have to actively set the default value here? :)
public void testSnapshotWithLargeSegmentFiles() throws Exception { final String repository = createRepository("repository", Settings.builder() .put("compress", "false") .put(GoogleCloudStorageRepository.CHUNK_SIZE.getKey(), GoogleCloudStorageRepository.CHUNK_SIZE.getDefault(Settings.EMPTY)) .build()); final String index = "index-no-merges"; createIndex(index, Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put(IndexSettings.INDEX_REFRESH_INTERVAL_SETTING.getKey(), TimeValue.MINUS_ONE) .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false) .build()); final int nbDocs = 10_000; try (BackgroundIndexer indexer = new BackgroundIndexer(index, "_doc", client(), nbDocs)) { awaitBusy(() -> indexer.totalIndexedDocs() >= nbDocs); } flushAndRefresh(index); assertHitCount(client().prepareSearch(index).setSize(0).setTrackTotalHits(true).get(), nbDocs); assertSuccessfulSnapshot(client().admin().cluster().prepareCreateSnapshot(repository, "snapshot") .setWaitForCompletion(true).setIndices(index)); assertAcked(client().admin().indices().prepareDelete(index)); assertSuccessfulRestore(client().admin().cluster().prepareRestoreSnapshot(repository, "snapshot").setWaitForCompletion(true)); ensureGreen(index); assertHitCount(client().prepareSearch(index).setSize(0).setTrackTotalHits(true).get(), nbDocs); }	maybe just force merge to a single segment at the end to make this a little easier to read, instead of indrectly getting there by disabling merges here? (it seems that, that's the goal here, having a large enough segment to force a resumable upload?)
@Override protected GoogleCloudStorageService createStorageService() { return new GoogleCloudStorageService() { @Override StorageOptions createStorageOptions(final GoogleCloudStorageClientSettings clientSettings, final HttpTransportOptions httpTransportOptions) { StorageOptions options = super.createStorageOptions(clientSettings, httpTransportOptions); return options.toBuilder() .setRetrySettings(RetrySettings.newBuilder() .setTotalTimeout(options.getRetrySettings().getTotalTimeout()) .setInitialRetryDelay(Duration.ofMillis(10L)) .setRetryDelayMultiplier(options.getRetrySettings().getRetryDelayMultiplier()) .setMaxRetryDelay(Duration.ofSeconds(1L)) .setMaxAttempts(0) .setJittered(false) .setInitialRpcTimeout(options.getRetrySettings().getInitialRpcTimeout()) .setRpcTimeoutMultiplier(options.getRetrySettings().getRpcTimeoutMultiplier()) .setMaxRpcTimeout(options.getRetrySettings().getMaxRpcTimeout()) .build()) .build(); } }; }	is this actually necessary? can't we just make the resumable size configurable and set a low value in tests? it seems this randomizes more than is randomized in reality and i'm not sure what it really adds in terms of test coverage? (maybe there's some motivation i'm missing though)
@Override protected Collection<Class<? extends Plugin>> nodePlugins() { return List.of(TestGoogleCloudStoragePlugin.class, InternalSettingsPlugin.class); }	not even nit: if you use arrays.aslist you can backport without a code change here :p
@Override public String toString() { return LAZY_VALUE; } }, EAGER { @Override public String toString() { return EAGER_VALUE; } }; public static final String KEY = "loading"; public static final String EAGER_VALUE = "eager"; public static final String LAZY_VALUE = "lazy"; public static Loading parse(String loading, Loading defaultValue) { if (Strings.isNullOrEmpty(loading)) { return defaultValue; } else if (EAGER_VALUE.equals(loading)) { return EAGER; } else if (LAZY_VALUE.equals(loading)) { return LAZY; } else { throw new MapperParsingException("Unknown [" + KEY + "] value: [" + loading + "]"); } }	can we do equalsingorecase here?
public void testCustomBoostValues() throws Exception { String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties") .startObject("s_field").field("type", "string").endObject() .startObject("l_field").field("type", "long").startObject("norms").field("enabled", true).endObject().endObject() .startObject("i_field").field("type", "integer").startObject("norms").field("enabled", true).endObject().endObject() .startObject("sh_field").field("type", "short").startObject("norms").field("enabled", true).endObject().endObject() .startObject("b_field").field("type", "byte").startObject("norms").field("enabled", true).endObject().endObject() .startObject("d_field").field("type", "double").startObject("norms").field("enabled", true).endObject().endObject() .startObject("f_field").field("type", "float").startObject("norms").field("enabled", true).endObject().endObject() .startObject("date_field").field("type", "date").startObject("norms").field("enabled", true).endObject().endObject() .endObject().endObject().endObject().string(); DocumentMapper mapper = MapperTestUtils.newParser().parse(mapping); ParsedDocument doc = mapper.parse("type", "1", XContentFactory.jsonBuilder().startObject() .startObject("s_field").field("value", "s_value").field("boost", 2.0f).endObject() .startObject("l_field").field("value", 1l).field("boost", 3.0f).endObject() .startObject("i_field").field("value", 1).field("boost", 4.0f).endObject() .startObject("sh_field").field("value", 1).field("boost", 5.0f).endObject() .startObject("b_field").field("value", 1).field("boost", 6.0f).endObject() .startObject("d_field").field("value", 1).field("boost", 7.0f).endObject() .startObject("f_field").field("value", 1).field("boost", 8.0f).endObject() .startObject("date_field").field("value", "20100101").field("boost", 9.0f).endObject() .endObject().bytes()); assertThat(doc.rootDoc().getField("s_field").boost(), equalTo(2.0f)); assertThat(doc.rootDoc().getField("l_field").boost(), equalTo(3.0f)); assertThat(doc.rootDoc().getField("i_field").boost(), equalTo(4.0f)); assertThat(doc.rootDoc().getField("sh_field").boost(), equalTo(5.0f)); assertThat(doc.rootDoc().getField("b_field").boost(), equalTo(6.0f)); assertThat(doc.rootDoc().getField("d_field").boost(), equalTo(7.0f)); assertThat(doc.rootDoc().getField("f_field").boost(), equalTo(8.0f)); assertThat(doc.rootDoc().getField("date_field").boost(), equalTo(9.0f)); }	should we do this randomly with the new and old style just to make sure it works.
public void testBasicTimeBasedRetenion() throws Exception { final String indexName = "test"; final String policyName = "test-policy"; final String repoId = "my-repo"; int docCount = randomIntBetween(10, 50); List<IndexRequestBuilder> indexReqs = new ArrayList<>(); for (int i = 0; i < docCount; i++) { index(client(), indexName, "" + i, "foo", "bar"); } // Create a snapshot repo inializeRepo(repoId); // Create a policy with a retention period of 1 millisecond createSnapshotPolicy(policyName, "snap", "1 2 3 4 5 ?", repoId, indexName, true, new SnapshotRetentionConfiguration(TimeValue.timeValueMillis(1))); // Manually create a snapshot Response executeResp = client().performRequest(new Request("PUT", "/_slm/policy/" + policyName + "/_execute")); final String snapshotName; try (XContentParser parser = JsonXContent.jsonXContent.createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, EntityUtils.toByteArray(executeResp.getEntity()))) { snapshotName = parser.mapStrings().get("snapshot_name"); // Check that the executed snapshot is created assertBusy(() -> { try { Response response = client().performRequest(new Request("GET", "/_snapshot/" + repoId + "/" + snapshotName)); Map<String, Object> snapshotResponseMap; try (InputStream is = response.getEntity().getContent()) { snapshotResponseMap = XContentHelper.convertToMap(XContentType.JSON.xContent(), is, true); } assertThat(snapshotResponseMap.size(), greaterThan(0)); final Map<String, Object> metadata = extractMetadata(snapshotResponseMap, snapshotName); assertNotNull(metadata); assertThat(metadata.get("policy"), equalTo(policyName)); assertHistoryIsPresent(policyName, true, repoId); } catch (ResponseException e) { fail("expected snapshot to exist but it does not: " + EntityUtils.toString(e.getResponse().getEntity())); } }); } // Run retention every second ClusterUpdateSettingsRequest req = new ClusterUpdateSettingsRequest(); req.transientSettings(Settings.builder().put(LifecycleSettings.SLM_RETENTION_SCHEDULE, "*/1 * * * * ?")); try (XContentBuilder builder = jsonBuilder()) { req.toXContent(builder, ToXContent.EMPTY_PARAMS); Request r = new Request("PUT", "/_cluster/settings"); r.setJsonEntity(Strings.toString(builder)); Response updateSettingsResp = client().performRequest(r); } try { // Check that the snapshot created by the policy has been removed by retention assertBusy(() -> { // We expect a failed response because the snapshot should not exist try { Response response = client().performRequest(new Request("GET", "/_snapshot/" + repoId + "/" + snapshotName)); assertThat(EntityUtils.toString(response.getEntity()), containsString("snapshot_missing_exception")); } catch (ResponseException e) { assertThat(EntityUtils.toString(e.getResponse().getEntity()), containsString("snapshot_missing_exception")); } }); Request delReq = new Request("DELETE", "/_slm/policy/" + policyName); assertOK(client().performRequest(delReq)); // It's possible there could have been a snapshot in progress when the // policy is deleted, so wait for it to be finished assertBusy(() -> { assertThat(wipeSnapshots().size(), equalTo(0)); }); } finally { // Unset retention ClusterUpdateSettingsRequest unsetRequest = new ClusterUpdateSettingsRequest(); unsetRequest.transientSettings(Settings.builder().put(LifecycleSettings.SLM_RETENTION_SCHEDULE, (String) null)); try (XContentBuilder builder = jsonBuilder()) { unsetRequest.toXContent(builder, ToXContent.EMPTY_PARAMS); Request r = new Request("PUT", "/_cluster/settings"); r.setJsonEntity(Strings.toString(builder)); client().performRequest(r); } } }	why is this line in both the try and the catch? shouldn't only one of them be necessary (and the other fail)?
public void testBasicTimeBasedRetenion() throws Exception { final String indexName = "test"; final String policyName = "test-policy"; final String repoId = "my-repo"; int docCount = randomIntBetween(10, 50); List<IndexRequestBuilder> indexReqs = new ArrayList<>(); for (int i = 0; i < docCount; i++) { index(client(), indexName, "" + i, "foo", "bar"); } // Create a snapshot repo inializeRepo(repoId); // Create a policy with a retention period of 1 millisecond createSnapshotPolicy(policyName, "snap", "1 2 3 4 5 ?", repoId, indexName, true, new SnapshotRetentionConfiguration(TimeValue.timeValueMillis(1))); // Manually create a snapshot Response executeResp = client().performRequest(new Request("PUT", "/_slm/policy/" + policyName + "/_execute")); final String snapshotName; try (XContentParser parser = JsonXContent.jsonXContent.createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, EntityUtils.toByteArray(executeResp.getEntity()))) { snapshotName = parser.mapStrings().get("snapshot_name"); // Check that the executed snapshot is created assertBusy(() -> { try { Response response = client().performRequest(new Request("GET", "/_snapshot/" + repoId + "/" + snapshotName)); Map<String, Object> snapshotResponseMap; try (InputStream is = response.getEntity().getContent()) { snapshotResponseMap = XContentHelper.convertToMap(XContentType.JSON.xContent(), is, true); } assertThat(snapshotResponseMap.size(), greaterThan(0)); final Map<String, Object> metadata = extractMetadata(snapshotResponseMap, snapshotName); assertNotNull(metadata); assertThat(metadata.get("policy"), equalTo(policyName)); assertHistoryIsPresent(policyName, true, repoId); } catch (ResponseException e) { fail("expected snapshot to exist but it does not: " + EntityUtils.toString(e.getResponse().getEntity())); } }); } // Run retention every second ClusterUpdateSettingsRequest req = new ClusterUpdateSettingsRequest(); req.transientSettings(Settings.builder().put(LifecycleSettings.SLM_RETENTION_SCHEDULE, "*/1 * * * * ?")); try (XContentBuilder builder = jsonBuilder()) { req.toXContent(builder, ToXContent.EMPTY_PARAMS); Request r = new Request("PUT", "/_cluster/settings"); r.setJsonEntity(Strings.toString(builder)); Response updateSettingsResp = client().performRequest(r); } try { // Check that the snapshot created by the policy has been removed by retention assertBusy(() -> { // We expect a failed response because the snapshot should not exist try { Response response = client().performRequest(new Request("GET", "/_snapshot/" + repoId + "/" + snapshotName)); assertThat(EntityUtils.toString(response.getEntity()), containsString("snapshot_missing_exception")); } catch (ResponseException e) { assertThat(EntityUtils.toString(e.getResponse().getEntity()), containsString("snapshot_missing_exception")); } }); Request delReq = new Request("DELETE", "/_slm/policy/" + policyName); assertOK(client().performRequest(delReq)); // It's possible there could have been a snapshot in progress when the // policy is deleted, so wait for it to be finished assertBusy(() -> { assertThat(wipeSnapshots().size(), equalTo(0)); }); } finally { // Unset retention ClusterUpdateSettingsRequest unsetRequest = new ClusterUpdateSettingsRequest(); unsetRequest.transientSettings(Settings.builder().put(LifecycleSettings.SLM_RETENTION_SCHEDULE, (String) null)); try (XContentBuilder builder = jsonBuilder()) { unsetRequest.toXContent(builder, ToXContent.EMPTY_PARAMS); Request r = new Request("PUT", "/_cluster/settings"); r.setJsonEntity(Strings.toString(builder)); client().performRequest(r); } } }	why not do this in the finally with the rest of the cleanup?
public void testBasicTimeBasedRetenion() throws Exception { final String indexName = "test"; final String policyName = "test-policy"; final String repoId = "my-repo"; int docCount = randomIntBetween(10, 50); List<IndexRequestBuilder> indexReqs = new ArrayList<>(); for (int i = 0; i < docCount; i++) { index(client(), indexName, "" + i, "foo", "bar"); } // Create a snapshot repo inializeRepo(repoId); // Create a policy with a retention period of 1 millisecond createSnapshotPolicy(policyName, "snap", "1 2 3 4 5 ?", repoId, indexName, true, new SnapshotRetentionConfiguration(TimeValue.timeValueMillis(1))); // Manually create a snapshot Response executeResp = client().performRequest(new Request("PUT", "/_slm/policy/" + policyName + "/_execute")); final String snapshotName; try (XContentParser parser = JsonXContent.jsonXContent.createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, EntityUtils.toByteArray(executeResp.getEntity()))) { snapshotName = parser.mapStrings().get("snapshot_name"); // Check that the executed snapshot is created assertBusy(() -> { try { Response response = client().performRequest(new Request("GET", "/_snapshot/" + repoId + "/" + snapshotName)); Map<String, Object> snapshotResponseMap; try (InputStream is = response.getEntity().getContent()) { snapshotResponseMap = XContentHelper.convertToMap(XContentType.JSON.xContent(), is, true); } assertThat(snapshotResponseMap.size(), greaterThan(0)); final Map<String, Object> metadata = extractMetadata(snapshotResponseMap, snapshotName); assertNotNull(metadata); assertThat(metadata.get("policy"), equalTo(policyName)); assertHistoryIsPresent(policyName, true, repoId); } catch (ResponseException e) { fail("expected snapshot to exist but it does not: " + EntityUtils.toString(e.getResponse().getEntity())); } }); } // Run retention every second ClusterUpdateSettingsRequest req = new ClusterUpdateSettingsRequest(); req.transientSettings(Settings.builder().put(LifecycleSettings.SLM_RETENTION_SCHEDULE, "*/1 * * * * ?")); try (XContentBuilder builder = jsonBuilder()) { req.toXContent(builder, ToXContent.EMPTY_PARAMS); Request r = new Request("PUT", "/_cluster/settings"); r.setJsonEntity(Strings.toString(builder)); Response updateSettingsResp = client().performRequest(r); } try { // Check that the snapshot created by the policy has been removed by retention assertBusy(() -> { // We expect a failed response because the snapshot should not exist try { Response response = client().performRequest(new Request("GET", "/_snapshot/" + repoId + "/" + snapshotName)); assertThat(EntityUtils.toString(response.getEntity()), containsString("snapshot_missing_exception")); } catch (ResponseException e) { assertThat(EntityUtils.toString(e.getResponse().getEntity()), containsString("snapshot_missing_exception")); } }); Request delReq = new Request("DELETE", "/_slm/policy/" + policyName); assertOK(client().performRequest(delReq)); // It's possible there could have been a snapshot in progress when the // policy is deleted, so wait for it to be finished assertBusy(() -> { assertThat(wipeSnapshots().size(), equalTo(0)); }); } finally { // Unset retention ClusterUpdateSettingsRequest unsetRequest = new ClusterUpdateSettingsRequest(); unsetRequest.transientSettings(Settings.builder().put(LifecycleSettings.SLM_RETENTION_SCHEDULE, (String) null)); try (XContentBuilder builder = jsonBuilder()) { unsetRequest.toXContent(builder, ToXContent.EMPTY_PARAMS); Request r = new Request("PUT", "/_cluster/settings"); r.setJsonEntity(Strings.toString(builder)); client().performRequest(r); } } }	at some point we should promote this to an @after or something so that we don't have keep copy/pasting it. that doesn't have to be right now, just a thought.
public void operationComplete(ChannelFuture future) throws Exception { if (released.compareAndSet(false, true)) { releasable.release(); } }	what motivated this change? did you hit a double release issue?
public static void ensureAllArraysAreReleased() throws Exception { if (DISCARD) { DISCARD = false; } else { final Map<Object, Object> masterCopy = Maps.newHashMap(ACQUIRED_ARRAYS); if (masterCopy.isEmpty()) { return; } // not empty, we might be executing on a shared cluster that keeps on obtaining // and releasing arrays, lets make sure that after a reasonable timeout, all master // copy (snapshot) have been released boolean success = ElasticsearchTestCase.awaitBusy(new Predicate<Object>() { @Override public boolean apply(Object input) { final Map<Object, Object> copy = Maps.newHashMap(ACQUIRED_ARRAYS); for (Object key : copy.keySet()) { if (masterCopy.containsKey(key)) { return false; } } return true; } }); if (success) { return; } final Map<Object, Object> failures = Maps.newHashMap(ACQUIRED_ARRAYS); for (Map.Entry<Object, Object> entry : masterCopy.entrySet()) { if (ACQUIRED_ARRAYS.containsKey(entry)) { // still around, add to failures failures.put(entry.getKey(), entry.getValue()); } } if (!failures.isEmpty()) { final Object cause = failures.entrySet().iterator().next().getValue(); throw new RuntimeException(failures.size() + " arrays have not been released", cause instanceof Throwable ? (Throwable) cause : null); } } }	i think this can be made easier to read and more concise using set operations like retainall?
public void setRuntimeJavaHome(File runtimeJavaHome) { BuildParams.runtimeJavaHome = requireNonNull(runtimeJavaHome); }	just a typo fixed along the way
private String formatJavaVendorDetails(JvmInstallationMetadata runtimeJdkMetaData) { JvmVendor vendor = runtimeJdkMetaData.getVendor(); return runtimeJdkMetaData.getVendor().getKnownVendor().name() + "/" + vendor.getRawVendor(); }	should we keep jvmvendor here and use jvmvendorspec.x for comparison in fips.gradle ? unless runtimejavadetails will also be used for descriptive purposes somewhere else in the build output
@Override public XContentBuilder toXContent(final XContentBuilder builder, final Params params) throws IOException { final boolean excludeGenerated = params.paramAsBoolean(TransformField.EXCLUDE_GENERATED, false); final boolean forInternalStorage = params.paramAsBoolean(TransformField.FOR_INTERNAL_STORAGE, false); if (forInternalStorage) { assert excludeGenerated == false: "unsupported behavior, exclude_generated is true and for_internal_storage is true"; } builder.startObject(); builder.field(TransformField.ID.getPreferredName(), id); if (excludeGenerated == false) { if (headers.isEmpty() == false && forInternalStorage) { builder.field(HEADERS.getPreferredName(), headers); } if (transformVersion != null) { builder.field(TransformField.VERSION.getPreferredName(), transformVersion); } if (createTime != null) { builder.timeField( TransformField.CREATE_TIME.getPreferredName(), TransformField.CREATE_TIME.getPreferredName() + "_string", createTime.toEpochMilli() ); } if (forInternalStorage) { builder.field(TransformField.INDEX_DOC_TYPE.getPreferredName(), NAME); } } builder.field(TransformField.SOURCE.getPreferredName(), source, params); builder.field(TransformField.DESTINATION.getPreferredName(), dest); if (frequency != null) { builder.field(TransformField.FREQUENCY.getPreferredName(), frequency.getStringRep()); } if (syncConfig != null) { builder.startObject(TransformField.SYNC.getPreferredName()); builder.field(syncConfig.getWriteableName(), syncConfig); builder.endObject(); } if (pivotConfig != null) { builder.field(PIVOT_TRANSFORM.getPreferredName(), pivotConfig); } if (description != null) { builder.field(TransformField.DESCRIPTION.getPreferredName(), description); } builder.field(TransformField.SETTINGS.getPreferredName(), settings); builder.endObject(); return builder; }	nit: assert forinternalstorage && excludegenerated == false:
public void createIndexAndTranslog() throws IOException { assert recoveryState.getRecoverySource().getType() == RecoverySource.Type.EMPTY_STORE; assert shardRouting.primary() && shardRouting.isRelocationTarget() == false; // note: these are set when recovering from the translog final RecoveryState.Translog translogStats = recoveryState().getTranslog(); translogStats.totalOperations(0); translogStats.totalOperationsOnStart(0); globalCheckpointTracker.updateGlobalCheckpointOnReplica(SequenceNumbers.NO_OPS_PERFORMED, "index created"); innerOpenEngineAndTranslog(EngineConfig.OpenMode.CREATE_INDEX_AND_TRANSLOG, false); }	i think a nicer approach (can be a follow-up done by me) would be not to call updateglobalcheckpointonreplica here, but instead call globalcheckpointtracker.activateprimarymode(sequencenumbers.no_ops_performed); either here or in the indexshard constructor (where we create the globalcheckpointtracker) when the recovery source is empty_store.
public void createIndexAndTranslog() throws IOException { assert recoveryState.getRecoverySource().getType() == RecoverySource.Type.EMPTY_STORE; assert shardRouting.primary() && shardRouting.isRelocationTarget() == false; // note: these are set when recovering from the translog final RecoveryState.Translog translogStats = recoveryState().getTranslog(); translogStats.totalOperations(0); translogStats.totalOperationsOnStart(0); globalCheckpointTracker.updateGlobalCheckpointOnReplica(SequenceNumbers.NO_OPS_PERFORMED, "index created"); innerOpenEngineAndTranslog(EngineConfig.OpenMode.CREATE_INDEX_AND_TRANSLOG, false); }	i want to better understand this. when would we expect this to happen?
public void openIndexAndCreateTranslog(boolean forceNewHistoryUUID, long globalCheckpoint) throws IOException { assert recoveryState.getRecoverySource().getType() != RecoverySource.Type.EMPTY_STORE && recoveryState.getRecoverySource().getType() != RecoverySource.Type.EXISTING_STORE; SequenceNumbers.CommitInfo commitInfo = store.loadSeqNoInfo(); if (commitInfo.localCheckpoint < globalCheckpoint) { throw new IllegalArgumentException( "trying to create a shard whose local checkpoint [" + commitInfo.localCheckpoint + "] is > global checkpoint [" + globalCheckpoint + "]"); } globalCheckpointTracker.updateGlobalCheckpointOnReplica(globalCheckpoint, "opening index with a new translog"); innerOpenEngineAndTranslog(EngineConfig.OpenMode.OPEN_INDEX_CREATE_TRANSLOG, forceNewHistoryUUID); }	you mean smaller, not larger?
public void openIndexAndCreateTranslog(boolean forceNewHistoryUUID, long globalCheckpoint) throws IOException { assert recoveryState.getRecoverySource().getType() != RecoverySource.Type.EMPTY_STORE && recoveryState.getRecoverySource().getType() != RecoverySource.Type.EXISTING_STORE; SequenceNumbers.CommitInfo commitInfo = store.loadSeqNoInfo(); if (commitInfo.localCheckpoint < globalCheckpoint) { throw new IllegalArgumentException( "trying to create a shard whose local checkpoint [" + commitInfo.localCheckpoint + "] is > global checkpoint [" + globalCheckpoint + "]"); } globalCheckpointTracker.updateGlobalCheckpointOnReplica(globalCheckpoint, "opening index with a new translog"); innerOpenEngineAndTranslog(EngineConfig.OpenMode.OPEN_INDEX_CREATE_TRANSLOG, forceNewHistoryUUID); }	note that in case of peer recovery with a retry, we could end up with a higher gcp in the globalcheckpointtracker than what we're setting here.
*/ private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException { assert commit == null || commit.getDirectory() == directory; try { return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit); } catch (EOFException eof) { // TODO this should be caught by lucene - EOF is almost certainly an index corruption throw new CorruptIndexException("Read past EOF while reading segment infos", "commit(" + commit + ")", eof); } catch (IOException exception) { throw exception; // IOExceptions like too many open files are not necessarily a corruption - just bubble it up } catch (Exception ex) { throw new CorruptIndexException("Hit unexpected exception while reading segment infos", "commit(" + commit + ")", ex); } } /** * Loads the maximum sequence number and local checkpoint from the latest Lucene commit point. * * @return {@link org.elasticsearch.index.seqno.SequenceNumbers.CommitInfo}	no need for full path here, just {@link sequencenumbers.commitinfo}
public void testRankEval() throws Exception { RankEvalSpec spec = new RankEvalSpec( Collections.singletonList(new RatedRequest("queryId", Collections.emptyList(), new SearchSourceBuilder())), new PrecisionAtK()); String[] indices = randomIndicesNames(0, 5); RankEvalRequest rankEvalRequest = new RankEvalRequest(spec, indices); String wildCardOption = randomFrom("none", "open", "closed", "all"); String ignoreUnavailableOption = randomFrom("true", "false"); String allowNoIndicesOption = randomFrom("true", "false"); rankEvalRequest.indicesOptions(IndicesOptions.fromParameters(wildCardOption, ignoreUnavailableOption, allowNoIndicesOption, SearchRequest.DEFAULT_INDICES_OPTIONS)); Request request = Request.rankEval(rankEvalRequest); StringJoiner endpoint = new StringJoiner("/", "/", ""); String index = String.join(",", indices); if (Strings.hasLength(index)) { endpoint.add(index); } endpoint.add(RestRankEvalAction.ENDPOINT); assertEquals(endpoint.toString(), request.getEndpoint()); assertEquals(3, request.getParameters().size()); assertEquals(allowNoIndicesOption, request.getParameters().get("allow_no_indices")); assertEquals(wildCardOption.equals("all") ? "open,closed" : wildCardOption, request.getParameters().get("expand_wildcards")); assertEquals(ignoreUnavailableOption, request.getParameters().get("ignore_unavailable")); assertToXContentBody(spec, request.getEntity()); }	we have a method for this i think, setrandomindicesoptions
public void testExpiredTokensDeletedAfterExpiration() throws Exception { final RestHighLevelClient restClient = new TestRestHighLevelClient(); CreateTokenResponse response = restClient.security().createToken(CreateTokenRequest.passwordGrant( SecuritySettingsSource.TEST_USER_NAME, SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()), SECURITY_REQUEST_OPTIONS); final String accessToken = response.getAccessToken(); final String refreshToken = response.getRefreshToken(); Instant created = Instant.now(); InvalidateTokenResponse invalidateResponse = restClient.security().invalidateToken( InvalidateTokenRequest.accessToken(accessToken), SECURITY_REQUEST_OPTIONS); assertThat(invalidateResponse.getInvalidatedTokens(), equalTo(1)); assertThat(invalidateResponse.getPreviouslyInvalidatedTokens(), equalTo(0)); assertThat(invalidateResponse.getErrors(), empty()); AtomicReference<String> docId = new AtomicReference<>(); assertBusy(() -> { SearchResponse searchResponse = restClient.search(new SearchRequest(RestrictedIndicesNames.SECURITY_TOKENS_ALIAS) .source(SearchSourceBuilder.searchSource() .size(1) .terminateAfter(1) .query(QueryBuilders.termQuery("doc_type", "token"))), SECURITY_REQUEST_OPTIONS); assertThat(searchResponse.getHits().getTotalHits().value, equalTo(1L)); docId.set(searchResponse.getHits().getAt(0).getId()); }); // hack doc to modify the creation time to the day before Instant yesterday = created.minus(36L, ChronoUnit.HOURS); assertTrue(Instant.now().isAfter(yesterday)); restClient.update(new UpdateRequest(RestrictedIndicesNames.SECURITY_TOKENS_ALIAS, docId.get()) .doc("creation_time", yesterday.toEpochMilli()) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE), SECURITY_REQUEST_OPTIONS); AtomicBoolean deleteTriggered = new AtomicBoolean(false); assertBusy(() -> { if (deleteTriggered.compareAndSet(false, true)) { // invalidate a invalid token... doesn't matter that it is bad... we just want this action to trigger the deletion InvalidateTokenResponse invalidateResponseTwo = restClient.security() .invalidateToken(InvalidateTokenRequest.accessToken("fooobar"), SECURITY_REQUEST_OPTIONS); assertThat(invalidateResponseTwo.getInvalidatedTokens(), equalTo(0)); assertThat(invalidateResponseTwo.getPreviouslyInvalidatedTokens(), equalTo(0)); assertThat(invalidateResponseTwo.getErrors(), empty()); } restClient.indices().refresh(new RefreshRequest(RestrictedIndicesNames.SECURITY_TOKENS_ALIAS), SECURITY_REQUEST_OPTIONS); SearchResponse searchResponse = restClient.search(new SearchRequest(RestrictedIndicesNames.SECURITY_TOKENS_ALIAS) .source(SearchSourceBuilder.searchSource() .query(QueryBuilders.termQuery("doc_type", "token")).terminateAfter(1)), SECURITY_REQUEST_OPTIONS); assertThat(searchResponse.getHits().getTotalHits().value, equalTo(0L)); }, 30, TimeUnit.SECONDS); // Now the documents are deleted, try to invalidate the access token and refresh token again InvalidateTokenResponse invalidateAccessTokenResponse = restClient.security().invalidateToken( InvalidateTokenRequest.accessToken(accessToken), SECURITY_REQUEST_OPTIONS); assertThat(invalidateAccessTokenResponse.getInvalidatedTokens(), equalTo(0)); assertThat(invalidateAccessTokenResponse.getPreviouslyInvalidatedTokens(), equalTo(0)); assertThat(invalidateAccessTokenResponse.getErrors(), empty()); // Weird testing behaviour ahead... // invalidating by access token (above) is a Get, but invalidating by refresh token (below) is a Search // In a multi node cluster, in a small % of cases, the search might find a document that has been deleted but not yet refreshed // from that node's shard. // Our assertion, therefore, is that an attempt to invalidate the refresh token must not actually invalidate // anything (concurrency controls must prevent that), nor may return any errors, // but it might _temporarily_ find an "already deleted" token. final InvalidateTokenRequest invalidateRefreshTokenRequest = InvalidateTokenRequest.refreshToken(refreshToken); InvalidateTokenResponse invalidateRefreshTokenResponse = restClient.security().invalidateToken( invalidateRefreshTokenRequest, SECURITY_REQUEST_OPTIONS); assertThat(invalidateRefreshTokenResponse.getInvalidatedTokens(), equalTo(0)); assertThat(invalidateRefreshTokenResponse.getPreviouslyInvalidatedTokens(), equalTo(0)); assertThat(invalidateRefreshTokenResponse.getErrors(), empty()); // 99% of the time, this will already be empty, but if not ensure it goes to empty within the allowed timeframe if (false == invalidateRefreshTokenResponse.getErrors().isEmpty()) { assertBusy(() -> { var newResponse = restClient.security().invalidateToken(invalidateRefreshTokenRequest, SECURITY_REQUEST_OPTIONS); assertThat(newResponse.getErrors(), empty()); }); } }	i was the reviewer for the previous change (#64757). but somehow i didn't realise that we should wrap geterrors() here instead of getpreviouslyinvalidatedtokens() because: 1. the original failure (#56903) was about geterrors(). 2. getpreviouslyinvalidatedtokens() should always return 0 since the refersh token is never invalidated before. if the cluster behaves really weirdly, we could probably expect getinvalidatedtokens() to be 1, but not getpreviouslyinvalidatedtokens().
public DocValuesField<?> getScriptField(String fieldName) { DocValuesField<?> field = localCacheScriptFieldData.get(fieldName); if (field == null) { final MappedFieldType fieldType = fieldTypeLookup.apply(fieldName); if (fieldType == null) { throw new IllegalArgumentException("no field found for [" + fieldName + "] in mapping"); } // Load the field data on behalf of the script. Otherwise, it would require // additional permissions to deal with pagedbytes/ramusagestimator/etc. field = AccessController.doPrivileged(new PrivilegedAction<DocValuesField<?>>() { @Override public DocValuesField<?> run() { return fieldDataLookup.apply(fieldType).load(reader).getScriptField(); } }); localCacheScriptFieldData.put(fieldName, field); } try { field.setNextDocId(docId); } catch (IOException ioe) { throw ExceptionsHelper.convertToElastic(ioe); } return field; }	does this stay in sync with scriptdocvalues setnextdocid(docid)? is there any chance the two could step on each other?
private synchronized void updateRemoteClusters(Map<String, Tuple<String, List<Tuple<String, Supplier<DiscoveryNode>>>>> seeds, ActionListener<Void> connectionListener) { if (seeds.containsKey(LOCAL_CLUSTER_GROUP_KEY)) { throw new IllegalArgumentException("remote clusters must not have the empty string as its key"); } Map<String, RemoteClusterConnection> remoteClusters = new HashMap<>(); if (seeds.isEmpty()) { connectionListener.onResponse(null); } else { CountDown countDown = new CountDown(seeds.size()); remoteClusters.putAll(this.remoteClusters); for (Map.Entry<String, Tuple<String, List<Tuple<String, Supplier<DiscoveryNode>>>>> entry : seeds.entrySet()) { List<Tuple<String, Supplier<DiscoveryNode>>> seedList = entry.getValue().v2(); String proxyAddress = entry.getValue().v1(); String clusterAlias = entry.getKey(); RemoteClusterConnection remote = this.remoteClusters.get(clusterAlias); ConnectionProfile connectionProfile = this.remoteClusterConnectionProfiles.get(clusterAlias); if (seedList.isEmpty()) { // with no seed nodes we just remove the connection try { IOUtils.close(remote); } catch (IOException e) { logger.warn("failed to close remote cluster connections for cluster: " + clusterAlias, e); } remoteClusters.remove(clusterAlias); continue; } if (remote == null) { // this is a new cluster we have to add a new representation remote = new RemoteClusterConnection(settings, clusterAlias, seedList, transportService, numRemoteConnections, getNodePredicate(settings), proxyAddress, connectionProfile); remoteClusters.put(clusterAlias, remote); } else if (connectionProfileChanged(remote.getConnectionManager().getConnectionProfile(), connectionProfile) || seedsChanged(remote.getSeedNodes(), seedList) || Objects.equals(proxyAddress, remote.getProxyAddress())) { // New ConnectionProfile. Must tear down existing connection try { IOUtils.close(remote); } catch (IOException e) { logger.warn("failed to close remote cluster connections for cluster: " + clusterAlias, e); } remoteClusters.remove(clusterAlias); remote = new RemoteClusterConnection(settings, clusterAlias, seedList, transportService, numRemoteConnections, getNodePredicate(settings), proxyAddress, connectionProfile); remoteClusters.put(clusterAlias, remote); } // now update the seed nodes no matter if it's new or already existing RemoteClusterConnection finalRemote = remote; remote.ensureConnected(ActionListener.wrap( response -> { if (countDown.countDown()) { connectionListener.onResponse(response); } }, exception -> { if (countDown.fastForward()) { connectionListener.onFailure(exception); } if (finalRemote.isClosed() == false) { logger.warn("failed to update seed list for cluster: " + clusterAlias, exception); } })); } } this.remoteClusters = Collections.unmodifiableMap(remoteClusters); }	should this be objects.equals(proxyaddress, remote.getproxyaddress()) == false? might indicate a testing gap as well.
private static List<String> parseClaimValues(JWTClaimsSet claimsSet, String claimName, String settingKey) { List<String> values; final Object claimValueObject = claimsSet.getClaim(claimName); if (claimValueObject == null) { values = List.of(); } else if (claimValueObject instanceof String) { values = List.of((String) claimValueObject); } else if (claimValueObject instanceof Collection) { if (claimValueObject instanceof Collection && ((Collection) claimValueObject).stream().allMatch(c -> c instanceof String)) { values = (List<String>) claimValueObject; } else { throw new SettingsException("Setting [ " + settingKey + " expects a claim with String or a String Array value"); } } else { throw new SettingsException("Setting [ " + settingKey + " expects a claim with String or a String Array value"); } return values; }	* the check claimvalueobject instanceof collection is performed twice, which does not seem to be necessary * the if check only ensure it is a collection<string>, not a list<string>. so there is a theoretical possibility for cast exception. i feel this part of code can be simplified to something like the follows: java } else if (claimvalueobject instanceof collection && ((collection) claimvalueobject).stream().allmatch(c -> c instanceof string)) { values = (collection<string>) claimvalueobject; } else { throw new settingsexception("setting [ " + settingkey + " expects a claim with string or a string array value"); } with above change, the return type of this method needs to be changed to collection<string> as well.
public void testWriteBytesAreIncremented() throws Exception { assertAcked(prepareCreate(INDEX_NAME, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1))); ensureGreen(INDEX_NAME); Tuple<String, String> primaryReplicaNodeNames = getPrimaryReplicaNodeNames(); String primaryName = primaryReplicaNodeNames.v1(); String replicaName = primaryReplicaNodeNames.v2(); String coordinatingOnlyNode = getCoordinatingOnlyNode(); final CountDownLatch replicationSendPointReached = new CountDownLatch(1); final CountDownLatch latchBlockingReplicationSend = new CountDownLatch(1); TransportService primaryService = internalCluster().getInstance(TransportService.class, primaryName); final MockTransportService primaryTransportService = (MockTransportService) primaryService; TransportService replicaService = internalCluster().getInstance(TransportService.class, replicaName); final MockTransportService replicaTransportService = (MockTransportService) replicaService; primaryTransportService.addSendBehavior((connection, requestId, action, request, options) -> { if (action.equals(TransportShardBulkAction.ACTION_NAME + "[r]")) { try { replicationSendPointReached.countDown(); latchBlockingReplicationSend.await(); } catch (InterruptedException e) { throw new IllegalStateException(e); } } connection.sendRequest(requestId, action, request, options); }); final ThreadPool replicaThreadPool = replicaTransportService.getThreadPool(); final Releasable replicaRelease = blockReplicas(replicaThreadPool); final BulkRequest bulkRequest = new BulkRequest(); int totalRequestSize = 0; for (int i = 0; i < 80; ++i) { IndexRequest request = new IndexRequest(INDEX_NAME).id(UUIDs.base64UUID()) .source(Collections.singletonMap("key", randomAlphaOfLength(50))); totalRequestSize += request.ramBytesUsed(); assertTrue(request.ramBytesUsed() > request.source().length()); bulkRequest.add(request); } final long bulkRequestSize = bulkRequest.ramBytesUsed(); final long bulkShardRequestSize = totalRequestSize; try { final ActionFuture<BulkResponse> successFuture = client(coordinatingOnlyNode).bulk(bulkRequest); replicationSendPointReached.await(); WriteMemoryLimits primaryWriteLimits = internalCluster().getInstance(WriteMemoryLimits.class, primaryName); WriteMemoryLimits replicaWriteLimits = internalCluster().getInstance(WriteMemoryLimits.class, replicaName); WriteMemoryLimits coordinatingWriteLimits = internalCluster().getInstance(WriteMemoryLimits.class, coordinatingOnlyNode); assertThat(primaryWriteLimits.getWriteBytes(), greaterThan(bulkShardRequestSize)); assertEquals(0, primaryWriteLimits.getReplicaWriteBytes()); assertEquals(0, replicaWriteLimits.getWriteBytes()); assertEquals(0, replicaWriteLimits.getReplicaWriteBytes()); assertEquals(bulkRequestSize, coordinatingWriteLimits.getWriteBytes()); assertEquals(0, coordinatingWriteLimits.getReplicaWriteBytes()); latchBlockingReplicationSend.countDown(); IndexRequest request = new IndexRequest(INDEX_NAME).id(UUIDs.base64UUID()) .source(Collections.singletonMap("key", randomAlphaOfLength(50))); final BulkRequest secondBulkRequest = new BulkRequest(); secondBulkRequest.add(request); // Use the primary or the replica data node as the coordinating node this time boolean usePrimaryAsCoordinatingNode = randomBoolean(); final ActionFuture<BulkResponse> secondFuture; if (usePrimaryAsCoordinatingNode) { secondFuture = client(primaryName).bulk(secondBulkRequest); } else { secondFuture = client(replicaName).bulk(secondBulkRequest); } final long secondBulkRequestSize = secondBulkRequest.ramBytesUsed(); final long secondBulkShardRequestSize = request.ramBytesUsed(); if (usePrimaryAsCoordinatingNode) { assertThat(primaryWriteLimits.getWriteBytes(), greaterThan(bulkShardRequestSize + secondBulkRequestSize)); assertEquals(0, replicaWriteLimits.getWriteBytes()); } else { assertThat(primaryWriteLimits.getWriteBytes(), greaterThan(bulkShardRequestSize)); assertEquals(secondBulkRequestSize, replicaWriteLimits.getWriteBytes()); } assertEquals(bulkRequestSize, coordinatingWriteLimits.getWriteBytes()); assertBusy(() -> assertThat(replicaWriteLimits.getReplicaWriteBytes(), greaterThan(bulkShardRequestSize + secondBulkShardRequestSize))); replicaRelease.close(); successFuture.actionGet(); secondFuture.actionGet(); assertEquals(0, primaryWriteLimits.getWriteBytes()); assertEquals(0, primaryWriteLimits.getReplicaWriteBytes()); assertEquals(0, replicaWriteLimits.getWriteBytes()); assertEquals(0, replicaWriteLimits.getReplicaWriteBytes()); assertEquals(0, coordinatingWriteLimits.getWriteBytes()); assertEquals(0, coordinatingWriteLimits.getReplicaWriteBytes()); } finally { if (replicationSendPointReached.getCount() > 0) { replicationSendPointReached.countDown(); } replicaRelease.close(); if (latchBlockingReplicationSend.getCount() > 0) { latchBlockingReplicationSend.countDown(); } replicaRelease.close(); primaryTransportService.clearAllRules(); } }	as we discussed, we eventually want to have a test that also covers the rest layer.
static Request templatesExist(GetIndexTemplatesRequest getIndexTemplatesRequest) { if (getIndexTemplatesRequest.names() == null || getIndexTemplatesRequest.names().length == 0) { throw new IllegalArgumentException("Must provide at least one index template name"); } if (Arrays.stream(getIndexTemplatesRequest.names()).anyMatch(indexTemplate -> Strings.hasText(indexTemplate) == false)) { throw new IllegalArgumentException("Index template names must not be null and must be non empty"); } final String endpoint = new RequestConverters.EndpointBuilder() .addPathPartAsIs("_template") .addCommaSeparatedPathParts(getIndexTemplatesRequest.names()) .build(); final Request request = new Request(HttpHead.METHOD_NAME, endpoint); RequestConverters.Params params = new RequestConverters.Params(request); params.withLocal(getIndexTemplatesRequest.local()); params.withMasterTimeout(getIndexTemplatesRequest.masterNodeTimeout()); return request; }	can you create a copy of getindextemplatesrequest and move the validation into that instead? put it in o.e.client.indices. also remove all of the to/from transport action stuff, and do your best to make constructor params final.
* @param ignoreEmptyValue The flag to determine whether to exit quietly when the value produced by TemplatedValue is null or empty * @throws IllegalArgumentException if the path is null, empty, invalid or if the value cannot be set to the * item identified by the provided path. */ public void setFieldValue(TemplateScript.Factory fieldPathTemplate, ValueSource valueSource, boolean ignoreEmptyValue) { Map<String, Object> model = createTemplateModel(); Object value = valueSource.copyAndResolve(model); if (ignoreEmptyValue && valueSource instanceof ValueSource.TemplatedValue){ if (value == null) { return; } String valueStr = (String) value; if (valueStr.isEmpty()) { return; } } setFieldValue(fieldPathTemplate.newInstance(model).execute(), value, false); }	minor code formatting: suggestion if (ignoreemptyvalue && valuesource instanceof valuesource.templatedvalue) {
public void testSeqNoCollision() throws Exception { try (ReplicationGroup shards = createGroup(2)) { shards.startAll(); int initDocs = shards.indexDocs(randomIntBetween(2, 10)); List<IndexShard> replicas = shards.getReplicas(); IndexShard replica1 = replicas.get(0); IndexShard replica2 = replicas.get(1); shards.syncGlobalCheckpoint(); logger.info("--> Isolate replica1"); IndexRequest indexDoc1 = new IndexRequest(index.getName(), "type", "d1").source("{}", XContentType.JSON); BulkShardRequest replicationRequest = indexOnPrimary(indexDoc1, shards.getPrimary()); indexOnReplica(replicationRequest, shards, replica2); final Translog.Operation op1; final List<Translog.Operation> initOperations = new ArrayList<>(initDocs); try (Translog.Snapshot snapshot = getTranslog(replica2).newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 1)); for (int i = 0; i < initDocs; i++) { Translog.Operation op = snapshot.next(); assertThat(op, is(notNullValue())); initOperations.add(op); } op1 = snapshot.next(); assertThat(op1, notNullValue()); assertThat(snapshot.next(), nullValue()); assertThat(snapshot.skippedOperations(), equalTo(0)); } // Make sure that replica2 receives translog ops (eg. op2) from replica1 // and does not overwrite its stale operation (op1) as it is trimmed. logger.info("--> Promote replica1 as the primary"); shards.promoteReplicaToPrimary(replica1).get(); // wait until resync completed. shards.index(new IndexRequest(index.getName(), "type", "d2").source("{}", XContentType.JSON)); final Translog.Operation op2; try (Translog.Snapshot snapshot = getTranslog(replica2).newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 2)); op2 = snapshot.next(); assertThat(op2.seqNo(), equalTo(op1.seqNo())); assertThat(op2.primaryTerm(), greaterThan(op1.primaryTerm())); assertThat("Remaining of snapshot should contain init operations", snapshot, containsOperationsInAnyOrder(initOperations)); assertThat(snapshot.overriddenOperations(), equalTo(0)); assertThat(snapshot.skippedOperations(), equalTo(1)); } // Make sure that peer-recovery transfers all but non-overridden operations. IndexShard replica3 = shards.addReplica(); logger.info("--> Promote replica2 as the primary"); shards.promoteReplicaToPrimary(replica2); logger.info("--> Recover replica3 from replica2"); recoverReplica(replica3, replica2); try (Translog.Snapshot snapshot = getTranslog(replica3).newSnapshot()) { assertThat(snapshot.totalOperations(), equalTo(initDocs + 1)); assertThat(snapshot.next(), equalTo(op2)); assertThat("Remaining of snapshot should contain init operations", snapshot, containsOperationsInAnyOrder(initOperations)); assertThat("Peer-recovery should not send overridden operations", snapshot.skippedOperations(), equalTo(0)); } // TODO: We should assert the content of shards in the ReplicationGroup. // Without rollback replicas(current implementation), we don't have the same content across shards: // - replica1 has {doc1} // - replica2 has {doc1, doc2} // - replica3 can have either {doc2} only if operation-based recovery or {doc1, doc2} if file-based recovery } }	why move from 0?
public void testSnapshotCurrentHasUnexpectedOperationsForTrimmedOperations() throws Exception { int extraDocs = randomIntBetween(10, 15); // increment primaryTerm to avoid potential negative numbers primaryTerm.addAndGet(extraDocs); translog.rollGeneration(); long pt = primaryTerm.get(); for (int op = 0; op < extraDocs; op++) { String ascii = randomAlphaOfLengthBetween(1, 50); Translog.Index operation = new Translog.Index("test", "" + op, op, pt - op, ascii.getBytes("UTF-8")); translog.add(operation); } try { translog.trimOperations(pt, 0); fail(); } catch (AssertionError e) { assertThat(e.getMessage(), is("current should not have any operations with seq#:primaryTerm [1:" + (pt - 1) + "] > 0:" + pt)); } primaryTerm.incrementAndGet(); translog.rollGeneration(); // add a single operation to current with seq# > trimmed seq# but higher primary term Translog.Index operation = new Translog.Index("test", "" + 1, 1L, primaryTerm.get(), randomAlphaOfLengthBetween(1, 50).getBytes("UTF-8")); translog.add(operation); // it is possible to trim after generation rollover translog.trimOperations(pt, 0); }	can we not use two letter variable names?
public void testSnapshotCurrentHasUnexpectedOperationsForTrimmedOperations() throws Exception { int extraDocs = randomIntBetween(10, 15); // increment primaryTerm to avoid potential negative numbers primaryTerm.addAndGet(extraDocs); translog.rollGeneration(); long pt = primaryTerm.get(); for (int op = 0; op < extraDocs; op++) { String ascii = randomAlphaOfLengthBetween(1, 50); Translog.Index operation = new Translog.Index("test", "" + op, op, pt - op, ascii.getBytes("UTF-8")); translog.add(operation); } try { translog.trimOperations(pt, 0); fail(); } catch (AssertionError e) { assertThat(e.getMessage(), is("current should not have any operations with seq#:primaryTerm [1:" + (pt - 1) + "] > 0:" + pt)); } primaryTerm.incrementAndGet(); translog.rollGeneration(); // add a single operation to current with seq# > trimmed seq# but higher primary term Translog.Index operation = new Translog.Index("test", "" + 1, 1L, primaryTerm.get(), randomAlphaOfLengthBetween(1, 50).getBytes("UTF-8")); translog.add(operation); // it is possible to trim after generation rollover translog.trimOperations(pt, 0); }	please don't use two letter variables names for acronyms.
public void testSnapshotCurrentHasUnexpectedOperationsForTrimmedOperations() throws Exception { int extraDocs = randomIntBetween(10, 15); // increment primaryTerm to avoid potential negative numbers primaryTerm.addAndGet(extraDocs); translog.rollGeneration(); long pt = primaryTerm.get(); for (int op = 0; op < extraDocs; op++) { String ascii = randomAlphaOfLengthBetween(1, 50); Translog.Index operation = new Translog.Index("test", "" + op, op, pt - op, ascii.getBytes("UTF-8")); translog.add(operation); } try { translog.trimOperations(pt, 0); fail(); } catch (AssertionError e) { assertThat(e.getMessage(), is("current should not have any operations with seq#:primaryTerm [1:" + (pt - 1) + "] > 0:" + pt)); } primaryTerm.incrementAndGet(); translog.rollGeneration(); // add a single operation to current with seq# > trimmed seq# but higher primary term Translog.Index operation = new Translog.Index("test", "" + 1, 1L, primaryTerm.get(), randomAlphaOfLengthBetween(1, 50).getBytes("UTF-8")); translog.add(operation); // it is possible to trim after generation rollover translog.trimOperations(pt, 0); }	instead of worrying about a roll over, i think we should look at the primary term of the existing op. if it is the same as the current term, use the same source, other wise change. this enforces our semantics regardless of the internals. also, can you streamify this?
public void testSnapshotCurrentHasUnexpectedOperationsForTrimmedOperations() throws Exception { int extraDocs = randomIntBetween(10, 15); // increment primaryTerm to avoid potential negative numbers primaryTerm.addAndGet(extraDocs); translog.rollGeneration(); long pt = primaryTerm.get(); for (int op = 0; op < extraDocs; op++) { String ascii = randomAlphaOfLengthBetween(1, 50); Translog.Index operation = new Translog.Index("test", "" + op, op, pt - op, ascii.getBytes("UTF-8")); translog.add(operation); } try { translog.trimOperations(pt, 0); fail(); } catch (AssertionError e) { assertThat(e.getMessage(), is("current should not have any operations with seq#:primaryTerm [1:" + (pt - 1) + "] > 0:" + pt)); } primaryTerm.incrementAndGet(); translog.rollGeneration(); // add a single operation to current with seq# > trimmed seq# but higher primary term Translog.Index operation = new Translog.Index("test", "" + 1, 1L, primaryTerm.get(), randomAlphaOfLengthBetween(1, 50).getBytes("UTF-8")); translog.add(operation); // it is possible to trim after generation rollover translog.trimOperations(pt, 0); }	why do we need to look at the rollover to determine this parameter? it feels weird to rely on internal semantics for an external operations.
public void testSnapshotCurrentHasUnexpectedOperationsForTrimmedOperations() throws Exception { int extraDocs = randomIntBetween(10, 15); // increment primaryTerm to avoid potential negative numbers primaryTerm.addAndGet(extraDocs); translog.rollGeneration(); long pt = primaryTerm.get(); for (int op = 0; op < extraDocs; op++) { String ascii = randomAlphaOfLengthBetween(1, 50); Translog.Index operation = new Translog.Index("test", "" + op, op, pt - op, ascii.getBytes("UTF-8")); translog.add(operation); } try { translog.trimOperations(pt, 0); fail(); } catch (AssertionError e) { assertThat(e.getMessage(), is("current should not have any operations with seq#:primaryTerm [1:" + (pt - 1) + "] > 0:" + pt)); } primaryTerm.incrementAndGet(); translog.rollGeneration(); // add a single operation to current with seq# > trimmed seq# but higher primary term Translog.Index operation = new Translog.Index("test", "" + 1, 1L, primaryTerm.get(), randomAlphaOfLengthBetween(1, 50).getBytes("UTF-8")); translog.add(operation); // it is possible to trim after generation rollover translog.trimOperations(pt, 0); }	is there anything here that isn't covered by testsnapshottrimmedoperations ?
private Decision underCapacity(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation, boolean moveToNode) { if (awarenessAttributes.isEmpty()) { return allocation.decision(Decision.YES, NAME, "allocation awareness is not enabled, set cluster setting [%s] to enable it", CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING.getKey()); } IndexMetaData indexMetaData = allocation.metaData().getIndexSafe(shardRouting.index()); int shardCount = indexMetaData.getNumberOfReplicas() + 1; // 1 for primary for (String awarenessAttribute : awarenessAttributes) { // the node the shard exists on must be associated with an awareness attribute if (node.node().getAttributes().containsKey(awarenessAttribute) == false) { return allocation.decision(Decision.NO, NAME, "node does not contain the awareness attribute [%s]; required attributes cluster setting [%s=%s]", awarenessAttribute, CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING.getKey(), allocation.debugDecision() ? Strings.collectionToCommaDelimitedString(awarenessAttributes) : null); } // build attr_value -> nodes map ObjectIntHashMap<String> nodesPerAttribute = allocation.routingNodes().nodesPerAttributesCounts(awarenessAttribute); // build the count of shards per attribute value ObjectIntHashMap<String> shardPerAttribute = new ObjectIntHashMap<>(); for (ShardRouting assignedShard : allocation.routingNodes().assignedShards(shardRouting.shardId())) { if (assignedShard.started() || assignedShard.initializing()) { // Note: this also counts relocation targets as that will be the new location of the shard. // Relocation sources should not be counted as the shard is moving away RoutingNode routingNode = allocation.routingNodes().node(assignedShard.currentNodeId()); shardPerAttribute.addTo(routingNode.node().getAttributes().get(awarenessAttribute), 1); } } if (moveToNode) { if (shardRouting.assignedToNode()) { String nodeId = shardRouting.relocating() ? shardRouting.relocatingNodeId() : shardRouting.currentNodeId(); if (node.nodeId().equals(nodeId) == false) { // we work on different nodes, move counts around shardPerAttribute.putOrAdd(allocation.routingNodes().node(nodeId).node().getAttributes().get(awarenessAttribute), 0, -1); shardPerAttribute.addTo(node.node().getAttributes().get(awarenessAttribute), 1); } } else { shardPerAttribute.addTo(node.node().getAttributes().get(awarenessAttribute), 1); } } int numberOfAttributes = nodesPerAttribute.size(); List<String> fullValues = forcedAwarenessAttributes.get(awarenessAttribute); if (fullValues != null) { for (String fullValue : fullValues) { if (shardPerAttribute.containsKey(fullValue) == false) { numberOfAttributes++; } } } // TODO should we remove ones that are not part of full list? final int currentNodeCount = shardPerAttribute.get(node.node().getAttributes().get(awarenessAttribute)); final int maximumNodeCount = (shardCount + numberOfAttributes - 1) / numberOfAttributes; // ceil(shardCount/numberOfAttributes) if (currentNodeCount > maximumNodeCount) { return allocation.decision(Decision.NO, NAME, "there are too many copies of the shard allocated to nodes with attribute [%s], there are [%d] total configured " + "shard copies for this shard id and [%d] total attribute values, expected the allocated shard count per " + "attribute [%d] to be less than or equal to the upper bound of the required number of shards per attribute [%d]", awarenessAttribute, shardCount, numberOfAttributes, currentNodeCount, maximumNodeCount); } if (currentNodeCount == maximumNodeCount && (shardCount % numberOfAttributes) != 0) { for (ObjectCursor<String> cursor : nodesPerAttribute.keys()) { if (shardPerAttribute.get(cursor.value) == 0) { return allocation.decision(Decision.NO, NAME, "there are too many copies of the shard allocated to nodes with attribute [%s=%s], but the awareness attribute [%s] has not allocated", awarenessAttribute, node.node().getAttributes().get(awarenessAttribute), cursor.value); } } } } return allocation.decision(Decision.YES, NAME, "node meets all awareness attribute requirements"); }	this seems wrong and i think would in your example result in always returning no. in your example for rack: maximumnodecount == 1 and currentnodecount will also always be 1 in the initial iteration => we always get no because there's always going to be an element in shardperattribute that is 0?
@Override public BytesReference slice(int from, int length) { FutureObjects.checkFromIndexSize(from, length, this.length); if (length == 0) { return new BytesArray(BytesRef.EMPTY_BYTES); } // for slices we only need to find the start and the end reference // adjust them and pass on the references in between as they are fully contained final int to = from + length; final int limit = getOffsetIndex(to - 1); final int start = getOffsetIndex(from); final BytesReference[] inSlice = new BytesReference[1 + (limit - start)]; for (int i = 0, j = start; i < inSlice.length; i++) { inSlice[i] = references[j++]; } int inSliceOffset = from - offsets[start]; if (inSlice.length == 1) { return inSlice[0].slice(inSliceOffset, length); } // now adjust slices in front and at the end inSlice[0] = inSlice[0].slice(inSliceOffset, inSlice[0].length() - inSliceOffset); inSlice[inSlice.length-1] = inSlice[inSlice.length-1].slice(0, to - offsets[limit]); return new CompositeBytesReference(inSlice); }	can you use bytesarray.empty here?
private static KeyedFilter[] randomOrder(KeyedFilter... filters) { List<KeyedFilter> asList = Arrays.asList(filters); Collections.shuffle(asList); return asList.toArray(new KeyedFilter[filters.length]); }	you could just do return filters?
public static Map<String, Object> resolveV2Mappings(final String requestMappings, final ClusterState currentState, final String templateName, final NamedXContentRegistry xContentRegistry) throws Exception { final Map<String, Object> mappings = Collections.unmodifiableMap(parseV2Mappings(requestMappings, MetadataIndexTemplateService.resolveMappings(currentState, templateName), xContentRegistry)); return mappings; }	super minor, but the indentation for these lines is off now
public ClusterState addIndexTemplateV2(final ClusterState currentState, final boolean create, final String name, final IndexTemplateV2 template) throws Exception { if (create && currentState.metadata().templatesV2().containsKey(name)) { throw new IllegalArgumentException("index template [" + name + "] already exists"); } Map<String, List<String>> overlaps = findConflictingV2Templates(currentState, name, template.indexPatterns(), true, template.priority()); overlaps.remove(name); if (overlaps.size() > 0) { String error = String.format(Locale.ROOT, "index template [%s] has index patterns %s matching patterns from " + "existing templates [%s] with patterns (%s) that have the same priority [%d], multiple index templates may not " + "match during index creation, please use a different priority", name, template.indexPatterns(), Strings.collectionToCommaDelimitedString(overlaps.keySet()), overlaps.entrySet().stream() .map(e -> e.getKey() + " => " + e.getValue()) .collect(Collectors.joining(",")), template.priority()); throw new IllegalArgumentException(error); } overlaps = findConflictingV1Templates(currentState, name, template.indexPatterns()); if (overlaps.size() > 0) { String warning = String.format(Locale.ROOT, "index template [%s] has index patterns %s matching patterns from " + "existing older templates [%s] with patterns (%s); this template [%s] will take precedence during new index creation", name, template.indexPatterns(), Strings.collectionToCommaDelimitedString(overlaps.keySet()), overlaps.entrySet().stream() .map(e -> e.getKey() + " => " + e.getValue()) .collect(Collectors.joining(",")), name); logger.warn(warning); deprecationLogger.deprecatedAndMaybeLog("index_template_pattern_overlap", warning); } IndexTemplateV2 finalIndexTemplate = template; Template innerTemplate = template.template(); if (innerTemplate != null) { // We may need to normalize index settings, so do that also Settings finalSettings = innerTemplate.settings(); if (finalSettings != null) { finalSettings = Settings.builder() .put(finalSettings).normalizePrefix(IndexMetadata.INDEX_SETTING_PREFIX) .build(); } // If an inner template was specified, its mappings may need to be // adjusted (to add _doc) and it should be validated CompressedXContent mappings = innerTemplate.mappings(); String stringMappings = mappings == null ? null : mappings.string(); validateTemplate(finalSettings, stringMappings, indicesService, xContentRegistry); // Mappings in index templates don't include _doc, so update the mappings to include this single type if (stringMappings != null) { Map<String, Object> parsedMappings = MapperService.parseMapping(xContentRegistry, stringMappings); if (parsedMappings.size() > 0) { stringMappings = Strings.toString(XContentFactory.jsonBuilder() .startObject() .field(MapperService.SINGLE_MAPPING_NAME, parsedMappings) .endObject()); } } final Template finalTemplate = new Template(finalSettings, stringMappings == null ? null : new CompressedXContent(stringMappings), innerTemplate.aliases()); finalIndexTemplate = new IndexTemplateV2(template.indexPatterns(), finalTemplate, template.composedOf(), template.priority(), template.version(), template.metadata()); } logger.info("adding index template [{}]", name); return ClusterState.builder(currentState) .metadata(Metadata.builder(currentState.metadata()).put(name, finalIndexTemplate)) .build(); }	same nit about indentation
private AllocateUnassignedDecision decideAllocation(RoutingAllocation allocation, ShardRouting shardRouting) { assert shardRouting.unassigned(); assert ExistingShardsAllocator.EXISTING_SHARDS_ALLOCATOR_SETTING.get( allocation.metadata().getIndexSafe(shardRouting.index()).getSettings() ).equals(ALLOCATOR_NAME); if (shardRouting.recoverySource().getType() == RecoverySource.Type.SNAPSHOT && allocation.snapshotShardSizeInfo().getShardSize(shardRouting) == null) { return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null); } if (SNAPSHOT_PARTIAL_SETTING.get(allocation.metadata().index(shardRouting.index()).getSettings()) && frozenCacheInfoService.isFetching()) { return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null); } final boolean explain = allocation.debugDecision(); // pre-check if it can be allocated to any node that currently exists, so we won't list the cache sizes for it for nothing // TODO: in the following logic, we do not account for existing cache size when handling disk space checks, should and can we // reliably do this in a world of concurrent cache evictions or are we ok with the cache size just being a best effort hint // here? Tuple<Decision, Map<String, NodeAllocationResult>> result = ReplicaShardAllocator.canBeAllocatedToAtLeastOneNode( shardRouting, allocation ); Decision allocateDecision = result.v1(); if (allocateDecision.type() != Decision.Type.YES && (explain == false || asyncFetchStore.get(shardRouting.shardId()) == null)) { // only return early if we are not in explain mode, or we are in explain mode but we have not // yet attempted to fetch any shard data logger.trace("{}: ignoring allocation, can't be allocated on any node", shardRouting); return AllocateUnassignedDecision.no( UnassignedInfo.AllocationStatus.fromDecision(allocateDecision.type()), result.v2() != null ? new ArrayList<>(result.v2().values()) : null ); } final AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchedCacheData = fetchData(shardRouting, allocation); if (fetchedCacheData.hasData() == false) { return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null); } final MatchingNodes matchingNodes = findMatchingNodes(shardRouting, allocation, fetchedCacheData, explain); assert explain == false || matchingNodes.nodeDecisions != null : "in explain mode, we must have individual node decisions"; List<NodeAllocationResult> nodeDecisions = augmentExplanationsWithStoreInfo(result.v2(), matchingNodes.nodeDecisions); if (allocateDecision.type() != Decision.Type.YES) { return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.fromDecision(allocateDecision.type()), nodeDecisions); } else if (matchingNodes.getNodeWithHighestMatch() != null) { RoutingNode nodeWithHighestMatch = allocation.routingNodes().node(matchingNodes.getNodeWithHighestMatch().getId()); // we only check on THROTTLE since we checked before on NO Decision decision = allocation.deciders().canAllocate(shardRouting, nodeWithHighestMatch, allocation); if (decision.type() == Decision.Type.THROTTLE) { // TODO: does this make sense? Unlike with the store we could evict the cache concurrently and wait for nothing? logger.debug( "[{}][{}]: throttling allocation [{}] to [{}] in order to reuse its unallocated persistent cache", shardRouting.index(), shardRouting.id(), shardRouting, nodeWithHighestMatch.node() ); return AllocateUnassignedDecision.throttle(nodeDecisions); } else { logger.debug( "[{}][{}]: allocating [{}] to [{}] in order to reuse its persistent cache", shardRouting.index(), shardRouting.id(), shardRouting, nodeWithHighestMatch.node() ); return AllocateUnassignedDecision.yes(nodeWithHighestMatch.node(), null, nodeDecisions, true); } } // TODO: do we need handling of delayed allocation for leaving replicas here? return AllocateUnassignedDecision.NOT_TAKEN; }	why is this needed? simply for better error message?
@Override public void authorizeIndexAction( RequestInfo requestInfo, AuthorizationInfo authorizationInfo, AsyncSupplier<ResolvedIndices> indicesAsyncSupplier, Map<String, IndexAbstraction> aliasOrIndexLookup, ActionListener<IndexAuthorizationResult> listener ) { assert false : "this method should no longer be used"; throw new UnsupportedOperationException("This method is no longer supported. " + "Instead use authorizeIndexAction(RequestInfo, AuthorizationInfo, AsyncSupplier, Metadata, ActionListener) "); }	why does this method exist if it's not used? just delete it.
private void authorizeIndexActionAndMaybeCache( RequestInfo requestInfo, AuthorizationInfo authorizationInfo, AsyncSupplier<ResolvedIndices> indicesAsyncSupplier, Metadata metadata, ActionListener<IndexAuthorizationResult> listener ) { var cacheKey = tryGetIndexAuthorizationCacheKey(requestInfo, authorizationInfo, metadata); if (cacheKey != null) { var valueAlreadyInCache = new AtomicBoolean(true); ListenableFuture<IndexAuthorizationCacheValue> listenableFuture; try { listenableFuture = indexAuthorizationCache.computeIfAbsent(cacheKey, k -> { valueAlreadyInCache.set(false); return new ListenableFuture<>(); }); } catch (ExecutionException e) { listener.onFailure(e); return; } if (valueAlreadyInCache.get()) { logger.trace("Thread [{}] listening for cacheable index authorization result", Thread.currentThread().getName()); listenableFuture.addListener(ActionListener.wrap(value -> { ((IndicesRequest.Replaceable) requestInfo.getRequest()).indices( value.resolvedIndices.isNoIndicesPlaceholder() ? NO_INDICES_OR_ALIASES_ARRAY : value.resolvedIndices.toArray() ); listener.onResponse(value.indexAuthorizationResult); }, listener::onFailure), threadPool.generic(), threadPool.getThreadContext()); } else { final ActionListener<IndexAuthorizationCacheValue> cachingListener = ActionListener.wrap(value -> { indexAuthorizationCache.invalidate(cacheKey, listenableFuture); listenableFuture.onResponse(value); listener.onResponse(value.indexAuthorizationResult); }, e -> { indexAuthorizationCache.invalidate(cacheKey, listenableFuture); listenableFuture.onFailure(e); listener.onFailure(e); }); indicesAsyncSupplier.getAsync( getResolvedIndicesListener( requestInfo, authorizationInfo, metadata.getIndicesLookup(), cachingListener ) ); } } else { indicesAsyncSupplier.getAsync( getResolvedIndicesListener( requestInfo, authorizationInfo, metadata.getIndicesLookup(), listener.map(value -> value.indexAuthorizationResult) ) ); } }	this is the one place where we have a coupling with the code logic in indicesandaliasesresolver.resolveindicesandaliases. it's not ideal but i don't see a way around it.
private void authorizeIndexActionAndMaybeCache( RequestInfo requestInfo, AuthorizationInfo authorizationInfo, AsyncSupplier<ResolvedIndices> indicesAsyncSupplier, Metadata metadata, ActionListener<IndexAuthorizationResult> listener ) { var cacheKey = tryGetIndexAuthorizationCacheKey(requestInfo, authorizationInfo, metadata); if (cacheKey != null) { var valueAlreadyInCache = new AtomicBoolean(true); ListenableFuture<IndexAuthorizationCacheValue> listenableFuture; try { listenableFuture = indexAuthorizationCache.computeIfAbsent(cacheKey, k -> { valueAlreadyInCache.set(false); return new ListenableFuture<>(); }); } catch (ExecutionException e) { listener.onFailure(e); return; } if (valueAlreadyInCache.get()) { logger.trace("Thread [{}] listening for cacheable index authorization result", Thread.currentThread().getName()); listenableFuture.addListener(ActionListener.wrap(value -> { ((IndicesRequest.Replaceable) requestInfo.getRequest()).indices( value.resolvedIndices.isNoIndicesPlaceholder() ? NO_INDICES_OR_ALIASES_ARRAY : value.resolvedIndices.toArray() ); listener.onResponse(value.indexAuthorizationResult); }, listener::onFailure), threadPool.generic(), threadPool.getThreadContext()); } else { final ActionListener<IndexAuthorizationCacheValue> cachingListener = ActionListener.wrap(value -> { indexAuthorizationCache.invalidate(cacheKey, listenableFuture); listenableFuture.onResponse(value); listener.onResponse(value.indexAuthorizationResult); }, e -> { indexAuthorizationCache.invalidate(cacheKey, listenableFuture); listenableFuture.onFailure(e); listener.onFailure(e); }); indicesAsyncSupplier.getAsync( getResolvedIndicesListener( requestInfo, authorizationInfo, metadata.getIndicesLookup(), cachingListener ) ); } } else { indicesAsyncSupplier.getAsync( getResolvedIndicesListener( requestInfo, authorizationInfo, metadata.getIndicesLookup(), listener.map(value -> value.indexAuthorizationResult) ) ); } }	i do not think this is needed. the original code, without the caching, invoked the authorizeindexactionname before the resolvedindices are resolved, which i see you've maintained, but then still added another such check here.
private Tuple<IndicesPermission, IndicesPermission> getRoleIndicesPermissions(Role role) { final IndicesPermission indicesPermission; final IndicesPermission limitedByIndicesPermission; if (role instanceof LimitedRole) { final LimitedRole limitedRole = (LimitedRole) role; indicesPermission = limitedRole.roleIndices(); limitedByIndicesPermission = limitedRole.getLimitedBy().indices(); } else { indicesPermission = role.indices(); limitedByIndicesPermission = null; } return new Tuple<>(indicesPermission, limitedByIndicesPermission); }	i don't think rbac engine should have code that specifically handles limitedrole. we need to find a way to encapsulate this properly.
@Override public int hashCode() { return Objects.hash(role, authenticatedUserAuthorizationInfo); } } private static boolean isScrollRelatedAction(String action) { return action.equals(SearchScrollAction.NAME) || action.equals(SearchTransportService.FETCH_ID_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.QUERY_FETCH_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.QUERY_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.FREE_CONTEXT_SCROLL_ACTION_NAME) || action.equals(ClearScrollAction.NAME) || action.equals("indices:data/read/sql/close_cursor") || action.equals(SearchTransportService.CLEAR_SCROLL_CONTEXTS_ACTION_NAME); } private static boolean isAsyncRelatedAction(String action) { return action.equals(SubmitAsyncSearchAction.NAME) || action.equals(GetAsyncSearchAction.NAME) || action.equals(DeleteAsyncResultAction.NAME) || action.equals(EqlAsyncActionNames.EQL_ASYNC_GET_RESULT_ACTION_NAME) || action.equals(SqlAsyncActionNames.SQL_ASYNC_GET_RESULT_ACTION_NAME); } // Package private for testing IndexAuthorizationCacheKey tryGetIndexAuthorizationCacheKey( RequestInfo requestInfo, AuthorizationInfo authorizationInfo, Metadata metadata ) { // Cache is disabled if (indexAuthorizationCache == null) { return null; } // Request is not qualified for cache if (false == isRequestCacheableForIndexAuthorization(requestInfo.getRequest())) { return null; } final IndicesRequest indicesRequest = (IndicesRequest) requestInfo.getRequest(); return new IndexAuthorizationCacheKey( requestInfo.getAction(), indicesRequest.indices(), indicesRequest.indicesOptions(), metadata.version(), getRoleIndicesPermissions(ensureRBAC(authorizationInfo).getRole()) ); } /** * Only cache requests that genuinely need wildcard expansion and replacement. */ private boolean isRequestCacheableForIndexAuthorization(TransportRequest request) { return request instanceof IndicesRequest.Replaceable && (false == request instanceof PutMappingRequest || ((PutMappingRequest) request).getConcreteIndex() == null); } static class IndexAuthorizationCacheKey { private final String action; private final String[] requestedIndices; private final IndicesOptions indicesOptions; private final long metadataVersion; // A pair of indicesPermission and limitedByIndicesPermission. The latter is nullable private final Tuple<IndicesPermission, IndicesPermission> indicesPermissions; IndexAuthorizationCacheKey( String action, String[] requestedIndices, IndicesOptions indicesOptions, long metadataVersion, Tuple<IndicesPermission, IndicesPermission> indicesPermissions ) { this.action = action; this.requestedIndices = requestedIndices; this.indicesOptions = indicesOptions; this.metadataVersion = metadataVersion; this.indicesPermissions = indicesPermissions; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; IndexAuthorizationCacheKey cacheKey = (IndexAuthorizationCacheKey) o; return metadataVersion == cacheKey.metadataVersion && Objects.equals(action, cacheKey.action) && Arrays.equals(requestedIndices, cacheKey.requestedIndices) && Objects.equals(indicesOptions, cacheKey.indicesOptions) && Objects.equals(indicesPermissions, cacheKey.indicesPermissions); } @Override public int hashCode() { int result = Objects.hash(action, indicesOptions, metadataVersion, indicesPermissions); result = 31 * result + Arrays.hashCode(requestedIndices); return result; } @Override public String toString() { return "IndexAuthorizationCacheKey{" + "action='" + action + '\\\\'' + ", requestedIndices=" + Arrays.toString(requestedIndices) + ", indicesOptions=" + indicesOptions + ", metadataVersion=" + metadataVersion + ", indicesPermissions=" + indicesPermissions + '}'; } } private static class IndexAuthorizationCacheValue { private final ResolvedIndices resolvedIndices; private final IndexAuthorizationResult indexAuthorizationResult; private IndexAuthorizationCacheValue( ResolvedIndices resolvedIndices, IndexAuthorizationResult indexAuthorizationResult ) { this.resolvedIndices = resolvedIndices; this.indexAuthorizationResult = indexAuthorizationResult; }	do we really need the permissions to know if we can cache the request's authorization result? can't we use the role or limited role instances with an equality check, since those instances are cached?
@Override public int hashCode() { return Objects.hash(role, authenticatedUserAuthorizationInfo); } } private static boolean isScrollRelatedAction(String action) { return action.equals(SearchScrollAction.NAME) || action.equals(SearchTransportService.FETCH_ID_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.QUERY_FETCH_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.QUERY_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.FREE_CONTEXT_SCROLL_ACTION_NAME) || action.equals(ClearScrollAction.NAME) || action.equals("indices:data/read/sql/close_cursor") || action.equals(SearchTransportService.CLEAR_SCROLL_CONTEXTS_ACTION_NAME); } private static boolean isAsyncRelatedAction(String action) { return action.equals(SubmitAsyncSearchAction.NAME) || action.equals(GetAsyncSearchAction.NAME) || action.equals(DeleteAsyncResultAction.NAME) || action.equals(EqlAsyncActionNames.EQL_ASYNC_GET_RESULT_ACTION_NAME) || action.equals(SqlAsyncActionNames.SQL_ASYNC_GET_RESULT_ACTION_NAME); } // Package private for testing IndexAuthorizationCacheKey tryGetIndexAuthorizationCacheKey( RequestInfo requestInfo, AuthorizationInfo authorizationInfo, Metadata metadata ) { // Cache is disabled if (indexAuthorizationCache == null) { return null; } // Request is not qualified for cache if (false == isRequestCacheableForIndexAuthorization(requestInfo.getRequest())) { return null; } final IndicesRequest indicesRequest = (IndicesRequest) requestInfo.getRequest(); return new IndexAuthorizationCacheKey( requestInfo.getAction(), indicesRequest.indices(), indicesRequest.indicesOptions(), metadata.version(), getRoleIndicesPermissions(ensureRBAC(authorizationInfo).getRole()) ); } /** * Only cache requests that genuinely need wildcard expansion and replacement. */ private boolean isRequestCacheableForIndexAuthorization(TransportRequest request) { return request instanceof IndicesRequest.Replaceable && (false == request instanceof PutMappingRequest || ((PutMappingRequest) request).getConcreteIndex() == null); } static class IndexAuthorizationCacheKey { private final String action; private final String[] requestedIndices; private final IndicesOptions indicesOptions; private final long metadataVersion; // A pair of indicesPermission and limitedByIndicesPermission. The latter is nullable private final Tuple<IndicesPermission, IndicesPermission> indicesPermissions; IndexAuthorizationCacheKey( String action, String[] requestedIndices, IndicesOptions indicesOptions, long metadataVersion, Tuple<IndicesPermission, IndicesPermission> indicesPermissions ) { this.action = action; this.requestedIndices = requestedIndices; this.indicesOptions = indicesOptions; this.metadataVersion = metadataVersion; this.indicesPermissions = indicesPermissions; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; IndexAuthorizationCacheKey cacheKey = (IndexAuthorizationCacheKey) o; return metadataVersion == cacheKey.metadataVersion && Objects.equals(action, cacheKey.action) && Arrays.equals(requestedIndices, cacheKey.requestedIndices) && Objects.equals(indicesOptions, cacheKey.indicesOptions) && Objects.equals(indicesPermissions, cacheKey.indicesPermissions); } @Override public int hashCode() { int result = Objects.hash(action, indicesOptions, metadataVersion, indicesPermissions); result = 31 * result + Arrays.hashCode(requestedIndices); return result; } @Override public String toString() { return "IndexAuthorizationCacheKey{" + "action='" + action + '\\\\'' + ", requestedIndices=" + Arrays.toString(requestedIndices) + ", indicesOptions=" + indicesOptions + ", metadataVersion=" + metadataVersion + ", indicesPermissions=" + indicesPermissions + '}'; } } private static class IndexAuthorizationCacheValue { private final ResolvedIndices resolvedIndices; private final IndexAuthorizationResult indexAuthorizationResult; private IndexAuthorizationCacheValue( ResolvedIndices resolvedIndices, IndexAuthorizationResult indexAuthorizationResult ) { this.resolvedIndices = resolvedIndices; this.indexAuthorizationResult = indexAuthorizationResult; }	can we expand this into a series of ifs? it's almost impossible to follow nested && and || conditions like this.
@Override public int hashCode() { return Objects.hash(role, authenticatedUserAuthorizationInfo); } } private static boolean isScrollRelatedAction(String action) { return action.equals(SearchScrollAction.NAME) || action.equals(SearchTransportService.FETCH_ID_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.QUERY_FETCH_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.QUERY_SCROLL_ACTION_NAME) || action.equals(SearchTransportService.FREE_CONTEXT_SCROLL_ACTION_NAME) || action.equals(ClearScrollAction.NAME) || action.equals("indices:data/read/sql/close_cursor") || action.equals(SearchTransportService.CLEAR_SCROLL_CONTEXTS_ACTION_NAME); } private static boolean isAsyncRelatedAction(String action) { return action.equals(SubmitAsyncSearchAction.NAME) || action.equals(GetAsyncSearchAction.NAME) || action.equals(DeleteAsyncResultAction.NAME) || action.equals(EqlAsyncActionNames.EQL_ASYNC_GET_RESULT_ACTION_NAME) || action.equals(SqlAsyncActionNames.SQL_ASYNC_GET_RESULT_ACTION_NAME); } // Package private for testing IndexAuthorizationCacheKey tryGetIndexAuthorizationCacheKey( RequestInfo requestInfo, AuthorizationInfo authorizationInfo, Metadata metadata ) { // Cache is disabled if (indexAuthorizationCache == null) { return null; } // Request is not qualified for cache if (false == isRequestCacheableForIndexAuthorization(requestInfo.getRequest())) { return null; } final IndicesRequest indicesRequest = (IndicesRequest) requestInfo.getRequest(); return new IndexAuthorizationCacheKey( requestInfo.getAction(), indicesRequest.indices(), indicesRequest.indicesOptions(), metadata.version(), getRoleIndicesPermissions(ensureRBAC(authorizationInfo).getRole()) ); } /** * Only cache requests that genuinely need wildcard expansion and replacement. */ private boolean isRequestCacheableForIndexAuthorization(TransportRequest request) { return request instanceof IndicesRequest.Replaceable && (false == request instanceof PutMappingRequest || ((PutMappingRequest) request).getConcreteIndex() == null); } static class IndexAuthorizationCacheKey { private final String action; private final String[] requestedIndices; private final IndicesOptions indicesOptions; private final long metadataVersion; // A pair of indicesPermission and limitedByIndicesPermission. The latter is nullable private final Tuple<IndicesPermission, IndicesPermission> indicesPermissions; IndexAuthorizationCacheKey( String action, String[] requestedIndices, IndicesOptions indicesOptions, long metadataVersion, Tuple<IndicesPermission, IndicesPermission> indicesPermissions ) { this.action = action; this.requestedIndices = requestedIndices; this.indicesOptions = indicesOptions; this.metadataVersion = metadataVersion; this.indicesPermissions = indicesPermissions; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; IndexAuthorizationCacheKey cacheKey = (IndexAuthorizationCacheKey) o; return metadataVersion == cacheKey.metadataVersion && Objects.equals(action, cacheKey.action) && Arrays.equals(requestedIndices, cacheKey.requestedIndices) && Objects.equals(indicesOptions, cacheKey.indicesOptions) && Objects.equals(indicesPermissions, cacheKey.indicesPermissions); } @Override public int hashCode() { int result = Objects.hash(action, indicesOptions, metadataVersion, indicesPermissions); result = 31 * result + Arrays.hashCode(requestedIndices); return result; } @Override public String toString() { return "IndexAuthorizationCacheKey{" + "action='" + action + '\\\\'' + ", requestedIndices=" + Arrays.toString(requestedIndices) + ", indicesOptions=" + indicesOptions + ", metadataVersion=" + metadataVersion + ", indicesPermissions=" + indicesPermissions + '}'; } } private static class IndexAuthorizationCacheValue { private final ResolvedIndices resolvedIndices; private final IndexAuthorizationResult indexAuthorizationResult; private IndexAuthorizationCacheValue( ResolvedIndices resolvedIndices, IndexAuthorizationResult indexAuthorizationResult ) { this.resolvedIndices = resolvedIndices; this.indexAuthorizationResult = indexAuthorizationResult; }	this is the new cachekey class. the improments are: * there is only one cachekey since we now cache the whole index authorization process instead individual components. * it uses requestedindices which is often times much smaller compared to resolvedindices. @albertzaharovits these set of variables seem to be the minimal information required for the cachekey (please also see [my previous reply](https://github.com/elastic/elasticsearch/pull/78358#discussion_r722903576)). please let me know if you think otherwise.
*/ @Deprecated public BulkRequest replicationType(ReplicationType replicationType) { this.replicationType = replicationType; return this; }	maybe add also what the method does, although quite self-explaining
public String toString() { if (gitRevision.get() == null) { try { /* * We want to avoid forking another process to run git rev-parse HEAD. Instead, we will read the refs manually. The * documentation for this follows from https://git-scm.com/docs/gitrepository-layout and * https://git-scm.com/docs/git-worktree. * * There are two cases to consider: * - a plain repository with .git directory at the root of the working tree * - a worktree with a plain text .git file at the root of the working tree * * In each case, our goal is to parse the HEAD file to get either a ref or a bare revision (in the case of being in * detached HEAD state). * * In the case of a plain repository, we can read the HEAD file directly, resolved directly from the .git * directory. * * In the case of a worktree, we read the gitdir from the plain text .git file. This resolves to a directory from * which we read the HEAD file and resolve commondir to the plain git repository. */ final Path dotGit = project.getRootProject().getRootDir().toPath().resolve(".git"); final Path head; final Path gitDir; if (Files.isDirectory(dotGit)) { // this is a git repository, we can read HEAD directly head = dotGit.resolve("HEAD"); gitDir = dotGit; } else { // this is a git worktree, follow the pointer to the repository final Path workTree = Paths.get(readFirstLine(dotGit).substring("gitdir:".length()).trim()); head = workTree.resolve("HEAD"); final Path commonDir = Paths.get(readFirstLine(workTree.resolve("commondir"))); if (commonDir.isAbsolute()) { gitDir = commonDir; } else { // this is the common case gitDir = workTree.resolve(commonDir); } } final String ref = readFirstLine(head); final String revision; if (ref.startsWith("ref:")) { revision = readFirstLine(gitDir.resolve(ref.substring("ref:".length()).trim())); } else { // we are in detached HEAD state revision = ref; } this.gitRevision.compareAndSet(null, revision); } catch (final IOException e) { // for now, do not be lenient until we have better understanding of real-world scenarios where this happens throw new GradleException("unable to read the git revision", e); } } return gitRevision.get(); }	we have a number of gradle integration tests that run in a workspace that is *not* an initialized git repository. this causes [those tests to fail](https://gradle-enterprise.elastic.co/s/6qkkewlbiv77e/tests/orjfcz7hu5lqq-u46z5u74yfscg). i think we should at least build in leniency that if this isn't a git repo (.git is missing) we just just carry on our merry way.
public void testIsCompatible() { assertTrue(isCompatible(Version.CURRENT, Version.CURRENT.minimumCompatibilityVersion())); assertTrue(isCompatible(Version.V_5_6_0, Version.V_6_0_0_alpha2)); assertFalse(isCompatible(Version.fromId(2000099), Version.V_6_0_0_alpha2)); assertFalse(isCompatible(Version.fromId(2000099), Version.V_5_0_0)); assertTrue(isCompatible(Version.fromString("6.0.0"), Version.fromString("7.0.0"))); if (Version.CURRENT.isRelease()) { assertTrue(isCompatible(Version.CURRENT, Version.fromString("7.0.0"))); } else { assertFalse(isCompatible(Version.CURRENT, Version.fromString("7.0.0"))); } assertFalse(isCompatible(Version.V_5_0_0, Version.fromString("6.0.0"))); assertFalse(isCompatible(Version.V_5_0_0, Version.fromString("7.0.0"))); Version a = randomVersion(random()); Version b = randomVersion(random()); assertThat(a.isCompatible(b), equalTo(b.isCompatible(a))); }	why do we remove this assertion, i think it's valid?
public static String retentionLeaseId( final String localClusterName, final String remoteClusterName, final String followerIndexName, final String followerUUID, final String leaderIndexName, final String leaderUUID) { return String.format( Locale.ROOT, "%s/%s/%s-following-%s/%s/%s", localClusterName, followerIndexName, followerUUID, remoteClusterName, leaderIndexName, leaderUUID); }	perhaps pass an index object here for the follower and the leader index
public static String retentionLeaseId( final String localClusterName, final String remoteClusterName, final String followerIndexName, final String followerUUID, final String leaderIndexName, final String leaderUUID) { return String.format( Locale.ROOT, "%s/%s/%s-following-%s/%s/%s", localClusterName, followerIndexName, followerUUID, remoteClusterName, leaderIndexName, leaderUUID); }	should we prefix the retention leases for ccr with ccr-. this will allow us to easily filter on them
public static GeoBoundingBox parseBoundingBox(XContentParser parser) throws IOException, ElasticsearchParseException { XContentParser.Token token = parser.currentToken(); if (token != XContentParser.Token.START_OBJECT) { throw new ElasticsearchParseException("failed to parse bounding box. Expected start object but found [{}]", token); } double top = Double.NaN; double bottom = Double.NaN; double left = Double.NaN; double right = Double.NaN; String currentFieldName; ParseField envelopeType = null; Rectangle envelope = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); parser.nextToken(); if (WKT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { assertNull(envelopeType, WKT_FIELD); try { Geometry geometry = WellKnownText.fromWKT(StandardValidator.instance(true), true, parser.text()); if (ShapeType.ENVELOPE.equals(geometry.type()) == false) { throw new ElasticsearchParseException( "failed to parse WKT bounding box. [" + geometry.type() + "] found. expected [" + ShapeType.ENVELOPE + "]" ); } envelopeType = WKT_FIELD; envelope = (Rectangle) geometry; } catch (ParseException | IllegalArgumentException e) { throw new ElasticsearchParseException("failed to parse WKT bounding box", e); } } else if (GEOTILE_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { assertNull(envelopeType, GEOTILE_FIELD); envelopeType = GEOTILE_FIELD; envelope = GeoTileUtils.toBoundingBox(parser.text()); } else if (GEOHASH_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { assertNull(envelopeType, GEOHASH_FIELD); envelopeType = GEOHASH_FIELD; envelope = Geohash.toBoundingBox(parser.text()); } else if (TOP_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { top = parser.doubleValue(); } else if (BOTTOM_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { bottom = parser.doubleValue(); } else if (LEFT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { left = parser.doubleValue(); } else if (RIGHT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { right = parser.doubleValue(); } else { if (TOP_LEFT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { GeoPoint sparse = GeoUtils.parseGeoPoint(parser, false, GeoUtils.EffectivePoint.TOP_LEFT); top = sparse.getLat(); left = sparse.getLon(); } else if (BOTTOM_RIGHT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { GeoPoint sparse = GeoUtils.parseGeoPoint(parser, false, GeoUtils.EffectivePoint.BOTTOM_RIGHT); bottom = sparse.getLat(); right = sparse.getLon(); } else if (TOP_RIGHT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { GeoPoint sparse = GeoUtils.parseGeoPoint(parser, false, GeoUtils.EffectivePoint.TOP_RIGHT); top = sparse.getLat(); right = sparse.getLon(); } else if (BOTTOM_LEFT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { GeoPoint sparse = GeoUtils.parseGeoPoint(parser, false, GeoUtils.EffectivePoint.BOTTOM_LEFT); bottom = sparse.getLat(); left = sparse.getLon(); } else { throw new ElasticsearchParseException("failed to parse bounding box. unexpected field [{}]", currentFieldName); } } } else { throw new ElasticsearchParseException("failed to parse bounding box. field name expected but [{}] found", token); } } if (envelopeType != null) { if (Double.isNaN(top) == false || Double.isNaN(bottom) == false || Double.isNaN(left) == false || Double.isNaN(right) == false) { throw new ElasticsearchParseException( "failed to parse bounding box. Conflicting definition found using " + envelopeType.getPreferredName() + " and explicit corners." ); } GeoPoint topLeft = new GeoPoint(envelope.getMaxLat(), envelope.getMinLon()); GeoPoint bottomRight = new GeoPoint(envelope.getMinLat(), envelope.getMaxLon()); return new GeoBoundingBox(topLeft, bottomRight); } GeoPoint topLeft = new GeoPoint(top, left); GeoPoint bottomRight = new GeoPoint(bottom, right); return new GeoBoundingBox(topLeft, bottomRight); }	this check helps ensure that never more than one envelope specification is used. but the same check is not done for the specifications that need multiple fields (like top_right, bottom_left, top, bottom, etc.).
@Override public ActionRequestValidationException validate() { ActionRequestValidationException e = searchRequest.validate(); if (searchRequest.source().from() != -1) { e = addValidationError("from is not supported in this context", e); } if (searchRequest.source().storedFields() != null) { e = addValidationError("stored_fields is not supported in this context", e); } if (maxRetries < 0) { e = addValidationError("retries cannnot be negative", e); } if (false == (size == -1 || size > 0)) { e = addValidationError( "size should be greater than 0 if the request is limited to some number of documents or -1 if it isn't but it was [" + size + "]", e); } if (searchRequest.source().slice() != null && !slices.equals(Slices.DEFAULT)) { e = addValidationError("can't specify both slice and workers", e); } return e; }	i imagine workers isn't the right things to say here. that is language left over from my first implementation. can you fix it while you are making this change?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); searchRequest = new SearchRequest(); searchRequest.readFrom(in); abortOnVersionConflict = in.readBoolean(); size = in.readVInt(); refresh = in.readBoolean(); timeout = new TimeValue(in); activeShardCount = ActiveShardCount.readFrom(in); retryBackoffInitialTime = new TimeValue(in); maxRetries = in.readVInt(); requestsPerSecond = in.readFloat(); if (in.getVersion().onOrAfter(Version.V_5_1_1)) { slices = new Slices(in); } else { slices = Slices.DEFAULT; } }	i'm not sure how backwards compatibility should work here. this change uses -1 as the value here for serializing the auto setting, which won't be a valid slices int value in earlier versions. something like readfrom { if version on or after 5.1.1 { slices = new slices(in) } else { slices = default } } writeto { if version on or after 5.1.1 and before 6.1.0 { if slices is auto { throw exception } else { slices.writeto(out) } } else if version on or after 6.1.0 slices.writeto(out) } else { if slices > 1 or slices is auto, throw exception } }
@Override public String toString() { StringBuilder builder = new StringBuilder(); builder.append(getClass().getSimpleName()); builder.append("["); builder.append("took=").append(took).append(','); builder.append("timed_out=").append(timedOut).append(','); status.innerToString(builder); builder.append(",bulk_failures=").append(getBulkFailures().subList(0, min(3, getBulkFailures().size()))); builder.append(",search_failures=").append(getSearchFailures().subList(0, min(3, getSearchFailures().size()))); return builder.append(']').toString(); }	i'd move this to line above, but i like the thought behind the change.
private static FunctionDefinition def(Class<? extends Function> function, FunctionBuilder builder, boolean datetime, String... names) { Check.isTrue(names.length > 0, "At least one name must be provided for the function"); String primaryName = names[0]; List<String> aliases = new ArrayList<>(names.length - 1); for (int i = 1; i < names.length; i++) { aliases.add(names[i]); } FunctionDefinition.Builder realBuilder = (uf, distinct, cfg) -> { try { return builder.build(uf.location(), uf.children(), distinct, cfg); } catch (IllegalArgumentException e) { throw new ParsingException("error building [" + primaryName + "]: " + e.getMessage(), e, uf.location().getLineNumber(), uf.location().getColumnNumber()); } }; return new FunctionDefinition(primaryName, unmodifiableList(aliases), function, datetime, realBuilder); }	i would add a comment here or a javadoc for the method stating that the first name in the array is the actual name of the function and the rest (if any) are aliases.
private void validateSourceIndexRowsCount(StartContext startContext, ActionListener<StartContext> listener) { DataFrameDataExtractorFactory extractorFactory = DataFrameDataExtractorFactory.createForSourceIndices(client, "validate_source_index_has_rows-" + startContext.config.getId(), startContext.config, startContext.extractedFields); extractorFactory.newExtractor(false) .collectDataSummaryAsync(ActionListener.wrap( dataSummary -> { if (dataSummary.rows == 0) { listener.onFailure(ExceptionsHelper.badRequestException( "Unable to start {} as no documents in the source indices [{}] contained all the fields " + "selected for analysis. If you are relying on automatic field selection then there are " + "currently mapped fields that do not exist in any indexed documents, and you will have " + "to switch to explicit field selection and include only fields that exist in indexed " + "documents.", startContext.config.getId(), Strings.arrayToCommaDelimitedString(startContext.config.getSource().getIndex()) )); } else if (Math.floor(startContext.config.getAnalysis().getTrainingPercent() * dataSummary.rows) >= Math.pow(2, 32)) { listener.onFailure(ExceptionsHelper.badRequestException("Unable to start because too many documents " + "(more than 2^32) are included in the analysis. Consider downsampling.")); } else { listener.onResponse(startContext); } }, listener::onFailure )); }	nit, there is no reason to calculate 2^32 here. i would just create a static variable where it is calculated once and thats it.
private static ImmutableOpenMap<String, MappingMetaData> parseMappings(XContentParser parser) throws IOException { ImmutableOpenMap.Builder<String, MappingMetaData> indexMappings = ImmutableOpenMap.builder(); // We start at START_OBJECT since parseIndexEntry ensures that while (parser.nextToken() != Token.END_OBJECT) { ensureExpectedToken(Token.FIELD_NAME, parser.currentToken(), parser::getTokenLocation); parser.nextToken(); if (parser.currentToken() == Token.START_OBJECT) { String mappingType = parser.currentName(); indexMappings.put(mappingType, new MappingMetaData(mappingType, parser.map())); } else if (parser.currentToken() == Token.START_ARRAY) { parser.skipChildren(); } } return indexMappings.build(); }	this changes the parsing of the server-side response back to accept the "old" response format that is including types. i did this because the "new" format is now parsed in the client-side getindexresponse, while this parsing code should only be used by the hlrc deprecated client code that uses this getindexresponse, and that forces "include_type_name=true" all the time. on the server side we are only using this parsing code in round-trip tests (getindexresponsetests)
public void testReload() { Settings settings = Settings.builder() .put("xpack.watcher.enabled", true) .put("path.home", createTempDir()) .build(); NotificationService mockService = mock(NotificationService.class); Watcher watcher = new Watcher(settings); watcher.reloadableServices.clear(); watcher.reloadableServices.add(mockService); verify(mockService, times(0)).reload(settings); watcher.reload(settings); verify(mockService, times(1)).reload(settings); }	instead of exposing the reloadableservices via package private visibility, how about making it protected and do this (otherwise we have another different visibility in the watcher class) watcher watcher = new watcher(settings) { @override public collection<object> createcomponents(client client, clusterservice clusterservice, threadpool threadpool, resourcewatcherservice resourcewatcherservice, scriptservice scriptservice, namedxcontentregistry xcontentregistry, environment environment, nodeenvironment nodeenvironment, namedwriteableregistry namedwriteableregistry) { reloadableservices.add(mockservice); return collections.emptylist(); } };
public void testReload() { Settings settings = Settings.builder() .put("xpack.watcher.enabled", true) .put("path.home", createTempDir()) .build(); NotificationService mockService = mock(NotificationService.class); Watcher watcher = new Watcher(settings); watcher.reloadableServices.clear(); watcher.reloadableServices.add(mockService); verify(mockService, times(0)).reload(settings); watcher.reload(settings); verify(mockService, times(1)).reload(settings); }	there is a verifynointeractions() method, that makes this test a bit easier to read
@Override @SuppressWarnings("unchecked") protected <A extends Aggregator> A createAggregator(Query query, AggregationBuilder aggregationBuilder, IndexSearcher indexSearcher, IndexSettings indexSettings, MultiBucketConsumerService.MultiBucketConsumer bucketConsumer, MappedFieldType... fieldTypes) throws IOException { aggregator = spy(super.createAggregator(query, aggregationBuilder, indexSearcher, indexSettings, bucketConsumer, fieldTypes)); return (A) aggregator; }	im not particularly a fan of using spy() here. there are a couple of reasons for this: 1. the class under is being wrapped by mockito which feels a bit weird to me because we are not really testing the class we intend to test. it also has the danger of the mockito usage being extended in future and us accidentally bypassing the actual logic in the scriptedmetricaggregator itself 2. we are creating a ensurenoselfreferencesinaggstate() method purely so we can do this spying which feels a bit ugly to me. i wonder if we need to do this spying at all and instead could maybe just rely on the existing tests that check we catch when a self-reference occurs and not worry about ensuring its only called once? it feels to me like the downsides of doing this mockito check might outweigh the benefits. @rjernst do you have any thoughts on this?
public T retrieve(String[] path, int index, Map<String, String> params) { if (index >= path.length) return null; String token = path[index]; TrieNode node = children.get(token); boolean usedWildcard; if (node == null) { node = children.get(wildcard); if (node == null) { return null; } usedWildcard = true; } else { /* * If we are at the end of the path, the current node does not have a value but * there is a child wildcard node, use the child wildcard node. */ if (index + 1 == path.length && node.value == null && children.get(wildcard) != null) { node = children.get(wildcard); usedWildcard = true; } else { usedWildcard = token.equals(wildcard); } } put(params, node, token); if (index == (path.length - 1)) { return node.value; } T res = node.retrieve(path, index + 1, params); if (res == null && !usedWildcard) { node = children.get(wildcard); if (node != null) { put(params, node, token); res = node.retrieve(path, index + 1, params); } } return res; }	nit: retrive -> retrieve
public T retrieve(String path, Map<String, String> params, boolean ignoreWildcards) { if (path.length() == 0) { return rootValue; } String[] strings = path.split(SEPARATOR); if (strings.length == 0) { return rootValue; } int index = 0; // supports initial delimiter. if (strings.length > 0 && strings[0].isEmpty()) { index = 1; } if (ignoreWildcards == true) { return root.retrieveExplicit(strings, index, params); } else { return root.retrieve(strings, index, params); } }	we can just write this as if (ignorewildcards); the if (b == false) thing is to avoid writing if (!b) because sometimes the ! gets lost when reading, i guess.
* @return true if the circuit breaker limit must be enforced for processing this request. */ public boolean canTripCircuitBreaker(RestRequest request) { RestHandler handler = getHandler(request, false); return (handler != null) ? handler.canTripCircuitBreaker() : true; }	i think this should be kept as if and only if.
private boolean executeExplicitHandler(RestRequest request, RestChannel channel, NodeClient client) throws Exception { /* * Get the matching explicit handler (ie. ignore wildcard path matches) * for this request, if one exists. */ final RestHandler explicitHandler = getHandler(request, true); if (explicitHandler != null) { /* * Handle valid REST request (the request matches an explicit * handler). */ explicitHandler.handleRequest(request, channel, client); return true; } /* * Get the map of matching explicit handlers (ie. ignore wildcard path * matches) for this request, for the set of HTTP methods. */ final HashSet<RestRequest.Method> explicitPathValidMethodSet = getValidHandlerMethodSet(request, true); if (explicitPathValidMethodSet.size() > 0 && !explicitPathValidMethodSet.contains(request.method()) && request.method() != RestRequest.Method.OPTIONS) { /* * If an alternative handler for an explicit path is registered to a * different HTTP method than the one supplied - return a 405 Method * Not Allowed error. */ handleUnsupportedHttpMethod(request, channel, explicitPathValidMethodSet); return true; } else if (!explicitPathValidMethodSet.contains(request.method()) && request.method() == RestRequest.Method.OPTIONS) { handleOptionsRequest(request, channel, explicitPathValidMethodSet); return true; } return false; }	i think that this block of code is identical to a block of code in restcontroller#executeexplicithandler; can we collapse them into a method?
public void process(RestRequest request, RestChannel channel, NodeClient client, RestFilterChain filterChain) throws Exception { executeHandler(request, channel, client); }	can we keep the newline at the end of the file here?
private static void fromXContent(XContentParser parser, StringBuilder keyBuilder, Settings.Builder builder, boolean allowNullValues) throws IOException { final int length = keyBuilder.length(); while (parser.nextToken() != XContentParser.Token.END_OBJECT) { if (parser.currentToken() == XContentParser.Token.FIELD_NAME) { keyBuilder.setLength(length); keyBuilder.append(parser.currentName()); } else if (parser.currentToken() == XContentParser.Token.START_OBJECT) { keyBuilder.append('.'); fromXContent(parser, keyBuilder, builder, allowNullValues); } else if (parser.currentToken() == XContentParser.Token.START_ARRAY) { List<String> list = new ArrayList<>(); while (parser.nextToken() != XContentParser.Token.END_ARRAY) { if (parser.currentToken() == XContentParser.Token.VALUE_STRING) { list.add(parser.text()); } else if (parser.currentToken() == XContentParser.Token.VALUE_NUMBER) { list.add(parser.text()); // just use the string representation here } else if (parser.currentToken() == XContentParser.Token.VALUE_BOOLEAN) { list.add(String.valueOf(parser.text())); } else { throw new IllegalStateException("only value lists are allowed in serialized settings"); } } String key = keyBuilder.toString(); valdiateValue(key, list, builder, parser, allowNullValues); builder.putArray(key, list); } else if (parser.currentToken() == XContentParser.Token.VALUE_NULL) { String key = keyBuilder.toString(); valdiateValue(key, null, builder, parser, allowNullValues); builder.putNull(key); } else if (parser.currentToken() == XContentParser.Token.VALUE_STRING || parser.currentToken() == XContentParser.Token.VALUE_NUMBER) { String key = keyBuilder.toString(); String value = parser.text(); valdiateValue(key, value, builder, parser, allowNullValues); builder.put(key, value); } else if (parser.currentToken() == XContentParser.Token.VALUE_BOOLEAN) { String key = keyBuilder.toString(); valdiateValue(key, parser.text(), builder, parser, allowNullValues); builder.put(key, parser.booleanValue()); } else { throw new IllegalStateException("Illegal token: " + parser.currentToken()); } } }	nit: typo in validatevalue()
private static void fromXContent(XContentParser parser, StringBuilder keyBuilder, Settings.Builder builder, boolean allowNullValues) throws IOException { final int length = keyBuilder.length(); while (parser.nextToken() != XContentParser.Token.END_OBJECT) { if (parser.currentToken() == XContentParser.Token.FIELD_NAME) { keyBuilder.setLength(length); keyBuilder.append(parser.currentName()); } else if (parser.currentToken() == XContentParser.Token.START_OBJECT) { keyBuilder.append('.'); fromXContent(parser, keyBuilder, builder, allowNullValues); } else if (parser.currentToken() == XContentParser.Token.START_ARRAY) { List<String> list = new ArrayList<>(); while (parser.nextToken() != XContentParser.Token.END_ARRAY) { if (parser.currentToken() == XContentParser.Token.VALUE_STRING) { list.add(parser.text()); } else if (parser.currentToken() == XContentParser.Token.VALUE_NUMBER) { list.add(parser.text()); // just use the string representation here } else if (parser.currentToken() == XContentParser.Token.VALUE_BOOLEAN) { list.add(String.valueOf(parser.text())); } else { throw new IllegalStateException("only value lists are allowed in serialized settings"); } } String key = keyBuilder.toString(); valdiateValue(key, list, builder, parser, allowNullValues); builder.putArray(key, list); } else if (parser.currentToken() == XContentParser.Token.VALUE_NULL) { String key = keyBuilder.toString(); valdiateValue(key, null, builder, parser, allowNullValues); builder.putNull(key); } else if (parser.currentToken() == XContentParser.Token.VALUE_STRING || parser.currentToken() == XContentParser.Token.VALUE_NUMBER) { String key = keyBuilder.toString(); String value = parser.text(); valdiateValue(key, value, builder, parser, allowNullValues); builder.put(key, value); } else if (parser.currentToken() == XContentParser.Token.VALUE_BOOLEAN) { String key = keyBuilder.toString(); valdiateValue(key, parser.text(), builder, parser, allowNullValues); builder.put(key, parser.booleanValue()); } else { throw new IllegalStateException("Illegal token: " + parser.currentToken()); } } }	you can use xcontentparserutils.throwunknowntoken() (that would throw a parsingexception instead but i think it's appropriate here)
private void reloadSslContext() { try { X509ExtendedKeyManager loadedKeyManager = keyConfig.createKeyManager(env); X509ExtendedTrustManager loadedTrustManager = trustConfig.createTrustManager(env); SSLContext loadedSslContext = SSLContext.getInstance(sslContextAlgorithm(sslConfiguration.supportedProtocols())); loadedSslContext.init(new X509ExtendedKeyManager[]{loadedKeyManager}, new X509ExtendedTrustManager[]{loadedTrustManager}, null); supportedCiphers(loadedSslContext.getSupportedSSLParameters().getCipherSuites(), sslConfiguration.cipherSuites(), false); this.context = loadedSslContext; } catch (GeneralSecurityException e) { throw new ElasticsearchException("failed to initialize the SSLContext", e); } }	we'd never call getemptytrustmanager as none of the implementations of createtrustmanager returns null. as for an emtpykeymanager, this could only happen for keyconfig none - so i moved this there
public interface TranslogRecoveryRunner { int run(Engine engine, Translog.Snapshot snapshot) throws IOException; } /** * Returns the maximum sequence number of either update or delete operations have been processed in this engine * or the sequence number from {@link #advanceMaxSeqNoOfUpdatesOrDeletes(long)}. An index request is considered * as an update operation if it overwrites the existing documents in Lucene index with the same document id. * <p> * A note on the optimization using max_seq_no_of_updates_or_deletes: * For each operation O, the key invariants are: * <ol> * <li> I1: There is no operation on docID(O) with seqno that is {@literal > MSU(O) and < seqno(O)} </li> * <li> I2: If {@literal MSU(O) < seqno(O)} then docID(O) did not exist when O was applied; more precisely, if there is any O' * with {@literal seqno(O') < seqno(O) and docID(O') = docID(O)} then the one with the greatest seqno is a delete.</li> * </ol> * <p> * When a receiving shard (either a replica or a follower) receives an operation O, it must first ensure its own MSU at least MSU(O), * and then compares its MSU to its local checkpoint (LCP). If {@literal LCP < MSU} then there's a gap: there may be some operations * that act on docID(O) about which we do not yet know, so we cannot perform an add. Note this also covers the case where a future * operation O' with {@literal seqNo(O') > seqNo(O) and docId(O') = docID(O)} is processed before O. In that case MSU(O') is at least * seqno(O') and this means {@literal MSU >= seqNo(O') > seqNo(O) > LCP} (because O wasn't processed yet). * <p> * However, if {@literal MSU <= LCP} then there is no gap: we have processed every {@literal operation <= LCP}, and no operation O' * with {@literal seqno(O') > LCP and seqno(O') < seqno(O) also has docID(O') = docID(O)}, because such an operation would have * {@literal seqno(O') > LCP >= MSU >= MSU(O)} which contradicts the first invariant. Furthermore in this case we immediately know * that docID(O) has been deleted (or never existed) without needing to check Lucene for the following reason. If there's no earlier * operation on docID(O) then this is clear, so suppose instead that the preceding operation on docID(O) is O': * 1. The first invariant above tells us that {@literal seqno(O') <= MSU(O) <= LCP} so we have already applied O' to Lucene. * 2. Also {@literal MSU(O) <= MSU <= LCP < seqno(O)} (we discard O if {@literal seqno(O) <= LCP}) so the second invariant applies, * meaning that the O' was a delete. * <p> * Therefore, if {@literal MSU <= LCP < seqno(O)}	i think the last 3 lines still accurately describe how the field is used?
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { PutMappingRequest putMappingRequest = putMappingRequest(Strings.splitStringByCommaToArray(request.param("index"))); String type = request.param("type"); Map<String, Object> sourceAsMap = XContentHelper.convertToMap(request.requiredContent(), false, request.getXContentType()).v2(); if (request.hasParam(INCLUDE_TYPE_NAME_PARAMETER) == false) { deprecationLogger.deprecatedAndMaybeLog("put_mapping_with_types", TYPES_DEPRECATION_MESSAGE); } else if (type != null || isMappingSourceTyped(MapperService.SINGLE_MAPPING_NAME, sourceAsMap)) { throw new IllegalArgumentException("Types cannot be provided in put mapping requests, unless " + "the include_type_name parameter is set to true."); } boolean includeTypeName = request.paramAsBoolean(INCLUDE_TYPE_NAME_PARAMETER, DEFAULT_INCLUDE_TYPE_NAME_POLICY); putMappingRequest.type(includeTypeName ? type : MapperService.SINGLE_MAPPING_NAME); putMappingRequest.source(sourceAsMap); if (request.hasParam("update_all_types")) { deprecationLogger.deprecated("[update_all_types] is deprecated since indices may not have more than one type anymore"); } putMappingRequest.updateAllTypes(request.paramAsBoolean("update_all_types", false)); putMappingRequest.timeout(request.paramAsTime("timeout", putMappingRequest.timeout())); putMappingRequest.masterNodeTimeout(request.paramAsTime("master_timeout", putMappingRequest.masterNodeTimeout())); putMappingRequest.indicesOptions(IndicesOptions.fromRequest(request, putMappingRequest.indicesOptions())); return channel -> client.admin().indices().putMapping(putMappingRequest, new RestToXContentListener<>(channel)); }	if i set up include_type_name=true, do we still want to allow typed "put mapping request" ? from what it looks like now, my typed request will fail even if i set include_type_name=true.
@Nullable public Object readGenericValue() throws IOException { byte type = readByte(); switch (type) { case -1: return null; case 0: return readString(); case 1: return readInt(); case 2: return readLong(); case 3: return readFloat(); case 4: return readDouble(); case 5: return readBoolean(); case 6: int bytesSize = readVInt(); byte[] value = new byte[bytesSize]; readBytes(value, 0, bytesSize); return value; case 7: int size = readVInt(); List list = new ArrayList(size); for (int i = 0; i < size; i++) { list.add(readGenericValue()); } return list; case 8: int size8 = readVInt(); Object[] list8 = new Object[size8]; for (int i = 0; i < size8; i++) { list8[i] = readGenericValue(); } return list8; case 9: int size9 = readVInt(); Map map9 = new LinkedHashMap(size9); for (int i = 0; i < size9; i++) { map9.put(readString(), readGenericValue()); } return map9; case 10: int size10 = readVInt(); Map map10 = new HashMap(size10); for (int i = 0; i < size10; i++) { map10.put(readString(), readGenericValue()); } return map10; case 11: return readByte(); case 12: return new Date(readLong()); case 13: final String timeZoneId = readString(); return new DateTime(readLong(), DateTimeZone.forID(timeZoneId)); case 14: return readBytesReference(); case 15: return readText(); case 16: return readShort(); case 17: return readIntArray(); case 18: return readLongArray(); case 19: return readFloatArray(); case 20: return readDoubleArray(); case 21: return readBytesRef(); case 22: return readGeoPoint(); default: throw new IOException("Can't read unknown type [" + type + "]"); } }	can you add minimal javadocs like for writegeopoint?
protected List<Realm> initRealms() throws Exception { Map<RealmConfig.RealmIdentifier, Settings> realmsSettings = RealmSettings.getRealmSettings(settings); Set<String> internalTypes = new HashSet<>(); List<Realm> realms = new ArrayList<>(); List<String> kerberosRealmNames = new ArrayList<>(); Map<String, Set<String>> nameToRealmIdentifier = new HashMap<>(); Set<String> missingOrderRealmSettingKeys = new TreeSet<>(); Map<String, Set<String>> orderToRealmOrderSettingKeys = new HashMap<>(); for (final Map.Entry<RealmConfig.RealmIdentifier, Settings> entry: realmsSettings.entrySet()) { final RealmConfig.RealmIdentifier identifier = entry.getKey(); if (false == entry.getValue().hasValue(RealmSettings.ORDER_SETTING_KEY)) { missingOrderRealmSettingKeys.add(RealmSettings.getFullSettingKey(identifier, RealmSettings.ORDER_SETTING)); } else { orderToRealmOrderSettingKeys.computeIfAbsent(entry.getValue().get(RealmSettings.ORDER_SETTING_KEY), k -> new TreeSet<>()) .add(RealmSettings.getFullSettingKey(identifier, RealmSettings.ORDER_SETTING)); } Realm.Factory factory = factories.get(identifier.getType()); if (factory == null) { throw new IllegalArgumentException("unknown realm type [" + identifier.getType() + "] for realm [" + identifier + "]"); } RealmConfig config = new RealmConfig(identifier, settings, env, threadContext); if (!config.enabled()) { if (logger.isDebugEnabled()) { logger.debug("realm [{}] is disabled", identifier); } continue; } if (FileRealmSettings.TYPE.equals(identifier.getType()) || NativeRealmSettings.TYPE.equals(identifier.getType())) { // this is an internal realm factory, let's make sure we didn't already registered one // (there can only be one instance of an internal realm) if (internalTypes.contains(identifier.getType())) { throw new IllegalArgumentException("multiple [" + identifier.getType() + "] realms are configured. [" + identifier.getType() + "] is an internal realm and therefore there can only be one such realm configured"); } internalTypes.add(identifier.getType()); } if (KerberosRealmSettings.TYPE.equals(identifier.getType())) { kerberosRealmNames.add(identifier.getName()); if (kerberosRealmNames.size() > 1) { throw new IllegalArgumentException("multiple realms " + kerberosRealmNames.toString() + " configured of type [" + identifier.getType() + "], [" + identifier.getType() + "] can only have one such realm " + "configured"); } } Realm realm = factory.create(config); nameToRealmIdentifier.computeIfAbsent(realm.name(), k -> new HashSet<>()).add(RealmSettings.realmSettingPrefix(realm.type()) + realm.name()); realms.add(realm); } if (!realms.isEmpty()) { Collections.sort(realms); } else { // there is no "realms" configuration, add the defaults addNativeRealms(realms); } // always add built in first! realms.add(0, reservedRealm); String duplicateRealms = nameToRealmIdentifier.entrySet().stream() .filter(entry -> entry.getValue().size() > 1) .map(entry -> entry.getKey() + ": " + entry.getValue()) .collect(Collectors.joining("; ")); if (Strings.hasText(duplicateRealms)) { throw new IllegalArgumentException("Found multiple realms configured with the same name: " + duplicateRealms + ""); } if (missingOrderRealmSettingKeys.size() > 0) { new DeprecationLogger(logger).deprecated("Found realms without order config: [{}]. " + "In next major release, node will fail to start with missing realm order.", String.join("; ", missingOrderRealmSettingKeys) ); } final List<String> duplicatedRealmOrderSettingKeys = orderToRealmOrderSettingKeys.entrySet() .stream() .filter(e -> e.getValue().size() > 1) .map(e -> e.getKey() + ": " + String.join(",", e.getValue())) .sorted() .collect(Collectors.toList()); if (false == duplicatedRealmOrderSettingKeys.isEmpty()) { new DeprecationLogger(logger).deprecated("Found multiple realms configured with the same order: [{}]. " + "In next major release, node will fail to start with duplicated realm order.", String.join("; ", duplicatedRealmOrderSettingKeys)); } return realms; }	one comment if we keep this: generally we create one static deprecationlogger for each class similar to the regular logger. it's not too much overhead to create new ones but it's still creating a few unnecessary objects.
@Test public void testInnerQueryReturnsNull() throws IOException { String queryString = "{ \\\\"" + ConstantScoreQueryBuilder.NAME + "\\\\" : { \\\\"filter\\\\" : { } } }"; QueryBuilder<?> innerQueryBuilder = parseQuery(queryString); DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder().add(innerQueryBuilder); assertNull(disMaxBuilder.toQuery(createShardContext())); }	shall we still test the case when null is provided?
@TestLogging(value="org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor:INFO", reason="testing INFO/WARN logging") public void testDiskMonitorLogging() throws IllegalAccessException { final ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)) .nodes(DiscoveryNodes.builder().add(newNode("node1"))).build(); final AtomicReference<ClusterState> clusterStateRef = new AtomicReference<>(clusterState); final LongSupplier timeSupplier = new LongSupplier() { long time; @Override public long getAsLong() { // advance time every check time += DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.get(Settings.EMPTY).getMillis() + 1; return time; } }; final AtomicLong relocatingShardSizeRef = new AtomicLong(); DiskThresholdMonitor monitor = new DiskThresholdMonitor(Settings.EMPTY, clusterStateRef::get, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), null, timeSupplier, (reason, priority, listener) -> listener.onResponse(clusterStateRef.get())) { @Override protected void updateIndicesReadOnly(Set<String> indicesToMarkReadOnly, ActionListener<Void> listener, boolean readOnly) { listener.onResponse(null); } @Override long sizeOfRelocatingShards(ClusterInfo info, ClusterState reroutedClusterState, DiskUsage diskUsage, RoutingNode routingNode) { return relocatingShardSizeRef.get(); } }; final ImmutableOpenMap.Builder<String, DiskUsage> allDisksOkBuilder; allDisksOkBuilder = ImmutableOpenMap.builder(); allDisksOkBuilder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(15, 100))); final ImmutableOpenMap<String, DiskUsage> allDisksOk = allDisksOkBuilder.build(); final ImmutableOpenMap.Builder<String, DiskUsage> aboveLowWatermarkBuilder = ImmutableOpenMap.builder(); aboveLowWatermarkBuilder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(10, 14))); final ImmutableOpenMap<String, DiskUsage> aboveLowWatermark = aboveLowWatermarkBuilder.build(); final ImmutableOpenMap.Builder<String, DiskUsage> aboveHighWatermarkBuilder = ImmutableOpenMap.builder(); aboveHighWatermarkBuilder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(5, 9))); final ImmutableOpenMap<String, DiskUsage> aboveHighWatermark = aboveHighWatermarkBuilder.build(); final ImmutableOpenMap.Builder<String, DiskUsage> aboveFloodStageWatermarkBuilder = ImmutableOpenMap.builder(); aboveFloodStageWatermarkBuilder.put("node1", new DiskUsage("node1", "node1", "/foo/bar", 100, between(0, 4))); final ImmutableOpenMap<String, DiskUsage> aboveFloodStageWatermark = aboveFloodStageWatermarkBuilder.build(); assertNoLogging(monitor, allDisksOk); assertSingleInfoMessage(monitor, aboveLowWatermark, "low disk watermark [85%] exceeded on * replicas will not be assigned to this node"); assertRepeatedWarningMessages(monitor, aboveHighWatermark, "high disk watermark [90%] exceeded on * shards will be relocated away from this node* " + "the node is expected to continue to exceed the high disk watermark when these relocations are complete"); assertRepeatedWarningMessages(monitor, aboveFloodStageWatermark, "flood stage disk watermark [95%] exceeded on * all indices on this node will be marked read-only"); relocatingShardSizeRef.set(-5L); assertSingleInfoMessage(monitor, aboveHighWatermark, "high disk watermark [90%] exceeded on * shards will be relocated away from this node* " + "the node is expected to be below the high disk watermark when these relocations are complete"); relocatingShardSizeRef.set(0L); assertRepeatedWarningMessages(monitor, aboveHighWatermark, "high disk watermark [90%] exceeded on * shards will be relocated away from this node* " + "the node is expected to continue to exceed the high disk watermark when these relocations are complete"); assertSingleInfoMessage(monitor, aboveLowWatermark, "high disk watermark [90%] no longer exceeded on * but low disk watermark [85%] is still exceeded"); assertSingleInfoMessage(monitor, allDisksOk, "low disk watermark [85%] no longer exceeded on *"); }	related to my comment above, would be nice to also assert the message when going directly from above high to below low watermark. i also think we should check that message comes out even if reroute interval did not pass since last reroute?
public void testShardSizeAndRelocatingSize() { ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder(); shardSizes.put("[test][0][r]", 10L); shardSizes.put("[test][1][r]", 100L); shardSizes.put("[test][2][r]", 1000L); shardSizes.put("[other][0][p]", 10000L); ClusterInfo info = new DevNullClusterInfo(ImmutableOpenMap.of(), ImmutableOpenMap.of(), shardSizes.build()); MetaData.Builder metaBuilder = MetaData.builder(); metaBuilder.put(IndexMetaData.builder("test").settings(settings(Version.CURRENT) .put("index.uuid", "1234")).numberOfShards(3).numberOfReplicas(1)); metaBuilder.put(IndexMetaData.builder("other").settings(settings(Version.CURRENT) .put("index.uuid", "5678")).numberOfShards(1).numberOfReplicas(1)); MetaData metaData = metaBuilder.build(); RoutingTable.Builder routingTableBuilder = RoutingTable.builder(); routingTableBuilder.addAsNew(metaData.index("test")); routingTableBuilder.addAsNew(metaData.index("other")); ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.CLUSTER_NAME_SETTING .getDefault(Settings.EMPTY)).metaData(metaData).routingTable(routingTableBuilder.build()).build(); RoutingAllocation allocation = new RoutingAllocation(null, null, clusterState, info, 0); final Index index = new Index("test", "1234"); ShardRouting test_0 = ShardRouting.newUnassigned(new ShardId(index, 0), false, PeerRecoverySource.INSTANCE, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); test_0 = ShardRoutingHelper.initialize(test_0, "node1"); test_0 = ShardRoutingHelper.moveToStarted(test_0); test_0 = ShardRoutingHelper.relocate(test_0, "node2"); ShardRouting test_1 = ShardRouting.newUnassigned(new ShardId(index, 1), false, PeerRecoverySource.INSTANCE, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); test_1 = ShardRoutingHelper.initialize(test_1, "node2"); test_1 = ShardRoutingHelper.moveToStarted(test_1); test_1 = ShardRoutingHelper.relocate(test_1, "node1"); ShardRouting test_2 = ShardRouting.newUnassigned(new ShardId(index, 2), false, PeerRecoverySource.INSTANCE, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); test_2 = ShardRoutingHelper.initialize(test_2, "node1"); test_2 = ShardRoutingHelper.moveToStarted(test_2); assertEquals(1000L, getExpectedShardSize(test_2, 0L, allocation)); assertEquals(100L, getExpectedShardSize(test_1, 0L, allocation)); assertEquals(10L, getExpectedShardSize(test_0, 0L, allocation)); RoutingNode node = new RoutingNode("node1", new DiscoveryNode("node1", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT), test_0, test_1.getTargetRelocatingShard(), test_2); assertEquals(100L, sizeOfRelocatingShards(allocation, node, false, "/dev/null")); assertEquals(90L, sizeOfRelocatingShards(allocation, node, true, "/dev/null")); assertEquals(0L, sizeOfRelocatingShards(allocation, node, true, "/dev/some/other/dev")); assertEquals(0L, sizeOfRelocatingShards(allocation, node, true, "/dev/some/other/dev")); ShardRouting test_3 = ShardRouting.newUnassigned(new ShardId(index, 3), false, PeerRecoverySource.INSTANCE, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); test_3 = ShardRoutingHelper.initialize(test_3, "node1"); test_3 = ShardRoutingHelper.moveToStarted(test_3); assertEquals(0L, getExpectedShardSize(test_3, 0L, allocation)); ShardRouting other_0 = ShardRouting.newUnassigned(new ShardId("other", "5678", 0), randomBoolean(), PeerRecoverySource.INSTANCE, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo")); other_0 = ShardRoutingHelper.initialize(other_0, "node2"); other_0 = ShardRoutingHelper.moveToStarted(other_0); other_0 = ShardRoutingHelper.relocate(other_0, "node1"); node = new RoutingNode("node1", new DiscoveryNode("node1", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT), test_0, test_1.getTargetRelocatingShard(), test_2, other_0.getTargetRelocatingShard()); if (other_0.primary()) { assertEquals(10100L, sizeOfRelocatingShards(allocation, node, false, "/dev/null")); assertEquals(10090L, sizeOfRelocatingShards(allocation, node, true, "/dev/null")); } else { assertEquals(100L, sizeOfRelocatingShards(allocation, node, false, "/dev/null")); assertEquals(90L, sizeOfRelocatingShards(allocation, node, true, "/dev/null")); } }	nit: i would prefer this to have same parameter order as the underlying method, i.e., move allocation last.
public void testGetMissingJob() throws InterruptedException { AtomicReference<Job.Builder> jobHolder = new AtomicReference<>(); AtomicReference<Exception> exceptionHolder = new AtomicReference<>(); blockingCall(actionListener -> jobConfigProvider.getJob("missing", actionListener), jobHolder, exceptionHolder); assertNull(jobHolder.get()); assertNotNull(exceptionHolder.get()); assertEquals(ResourceNotFoundException.class, exceptionHolder.get().getClass()); }	is the true a leftover from debugging?
public static void configureRepositories(Project project) { RepositoryHandler repos = project.getRepositories(); if (System.getProperty("repos.mavenLocal") != null) { // with -Drepos.mavenLocal=true we can force checking the local .m2 repo which is // useful for development ie. bwc tests where we install stuff in the local repository // such that we don't have to pass hardcoded files to gradle repos.mavenLocal(); } repos.mavenCentral(); String luceneVersion = VersionProperties.getLucene(); if (luceneVersion.contains("-snapshot")) { // extract the revision number from the version with a regex matcher Matcher matcher = LUCENE_SNAPSHOT_REGEX.matcher(luceneVersion); if (matcher.find() == false) { throw new GradleException("Malformed lucene snapshot version: " + luceneVersion); } String revision = matcher.group(1); MavenArtifactRepository luceneRepo = repos.maven(repo -> { repo.setName("lucene-snapshots"); repo.setUrl("https://s3.amazonaws.com/download.elasticsearch.org/lucenesnapshots/" + revision); }); repos.exclusiveContent(exclusiveRepo -> { exclusiveRepo.filter( descriptor -> descriptor.includeVersionByRegex("org\\\\\\\\.apache\\\\\\\\.lucene", ".*", ".*-snapshot-" + revision) ); exclusiveRepo.forRepositories(luceneRepo); }); } }	resolving this todo by removing the extra logic for http checks
public void apply(Project project) { File keyStoreDir = new File(project.getBuildDir(), "keystore"); TaskProvider<ExportElasticsearchBuildResourcesTask> exportKeyStore = project.getTasks() .register("copyTestCertificates", ExportElasticsearchBuildResourcesTask.class, (t) -> { t.copy("test/ssl/test-client.crt"); t.copy("test/ssl/test-client.key"); t.copy("test/ssl/test-client.jks"); t.copy("test/ssl/test-node.crt"); t.copy("test/ssl/test-node.key"); t.copy("test/ssl/test-node.jks"); t.setOutputDir(keyStoreDir); }); project.getPlugins() .withType(ForbiddenPatternsPrecommitPlugin.class) .configureEach(plugin -> project.getTasks().named(FORBIDDEN_PATTERNS_TASK_NAME).configure(t -> t.dependsOn(exportKeyStore))); project.getPlugins() .withType(FilePermissionsPrecommitPlugin.class) .configureEach( filePermissionPlugin -> project.getTasks().named(FILEPERMISSIONS_TASK_NAME).configure(t -> t.dependsOn(exportKeyStore)) ); project.getPlugins().withType(StandaloneRestTestPlugin.class).configureEach(restTestPlugin -> { SourceSet testSourceSet = Util.getJavaTestSourceSet(project).get(); testSourceSet.getResources().srcDir(new File(keyStoreDir, "test/ssl")); project.getTasks().named(testSourceSet.getProcessResourcesTaskName()).configure(t -> t.dependsOn(exportKeyStore)); project.getTasks().withType(TestClustersAware.class).configureEach(clusterAware -> clusterAware.dependsOn(exportKeyStore)); // Tell the tests we're running with ssl enabled project.getTasks() .withType(RestIntegTestTask.class) .configureEach(runner -> runner.systemProperty("tests.ssl.enabled", "true")); }); project.getPlugins().withType(TestClustersPlugin.class).configureEach(clustersPlugin -> { File keyMaterialDir = new File(project.getBuildDir(), "keystore/test/ssl"); File nodeKeystore = new File(keyMaterialDir, "test-node.jks"); File nodeCertificate = new File(keyMaterialDir, "test-node.crt"); File nodeKey = new File(keyMaterialDir, "test-node.key"); File clientKeyStore = new File(keyMaterialDir, "test-client.jks"); File clientCertificate = new File(keyMaterialDir, "test-client.crt"); File clientKey = new File(keyMaterialDir, "test-client.key"); @SuppressWarnings("unchecked") NamedDomainObjectContainer<ElasticsearchCluster> clusters = (NamedDomainObjectContainer<ElasticsearchCluster>) project .getExtensions() .getByName(TestClustersPlugin.EXTENSION_NAME); clusters.all(c -> { if (BuildParams.isInFipsJvm()) { c.setting("xpack.security.transport.ssl.key", "test-node.key"); c.keystore("xpack.security.transport.ssl.secure_key_passphrase", "test-node-key-password"); c.setting("xpack.security.transport.ssl.certificate", "test-node.crt"); c.setting("xpack.security.http.ssl.key", "test-node.key"); c.keystore("xpack.security.http.ssl.secure_key_passphrase", "test-node-key-password"); c.setting("xpack.security.http.ssl.certificate", "test-node.crt"); c.extraConfigFile(nodeKey.getName(), nodeKey); c.extraConfigFile(nodeCertificate.getName(), nodeCertificate); c.extraConfigFile(clientKey.getName(), clientKey); c.extraConfigFile(clientCertificate.getName(), clientCertificate); } else { // ceremony to set up ssl c.setting("xpack.security.transport.ssl.keystore.path", "test-node.jks"); c.setting("xpack.security.http.ssl.keystore.path", "test-node.jks"); c.keystore("xpack.security.transport.ssl.keystore.secure_password", "keypass"); c.keystore("xpack.security.http.ssl.keystore.secure_password", "keypass"); // copy keystores & certs into config/ c.extraConfigFile(nodeKeystore.getName(), nodeKeystore); c.extraConfigFile(clientKeyStore.getName(), clientKeyStore); } }); }); project.getTasks() .withType(ForbiddenPatternsTask.class) .configureEach(forbiddenPatternTask -> forbiddenPatternTask.exclude("**/*.crt", "**/*.key")); }	make implicit dependencies explicit
public void prepare() { project = createProject(); task = createDependencyLicensesTask(project); updateShas = createUpdateShasTask(project, task); dependency = project.getDependencies().localGroovy(); task.configure(new Action<DependencyLicensesTask>() { @Override public void execute(DependencyLicensesTask dependencyLicensesTask) { dependencyLicensesTask.mapping(Maps.of("from", "groovy-.*", "to", "groovy")); dependencyLicensesTask.mapping(Maps.of("from", "javaparser-.*", "to", "groovy")); } }); }	groovy is shipped with different artefacts now
public IngestDocument execute(IngestDocument ingestDocument) throws Exception { Authentication authentication = Authentication.getAuthentication(threadContext); if (authentication == null) { throw new IllegalStateException("No user authenticated, only use this processor via authenticated user"); } User user = authentication.getUser(); if (user == null) { throw new IllegalStateException("No user for authentication"); } Object fieldValue = ingestDocument.getFieldValue(field, Object.class, true); @SuppressWarnings("unchecked") Map<String, Object> userObject = fieldValue instanceof Map ? (Map<String, Object>) fieldValue : new HashMap<>(); for (Property property : properties) { switch (property) { case USERNAME: if (user.principal() != null) { userObject.put("username", user.principal()); } break; case FULL_NAME: if (user.fullName() != null) { userObject.put("full_name", user.fullName()); } break; case EMAIL: if (user.email() != null) { userObject.put("email", user.email()); } break; case ROLES: if (user.roles() != null && user.roles().length != 0) { userObject.put("roles", Arrays.asList(user.roles())); } break; case METADATA: if (user.metadata() != null && user.metadata().isEmpty() == false) { userObject.put("metadata", user.metadata()); } break; case API_KEY: final String apiKey = "api_key"; final Object existingApiKeyField = userObject.get(apiKey); @SuppressWarnings("unchecked") final Map<String, Object> apiKeyField = existingApiKeyField instanceof Map ? (Map<String, Object>) existingApiKeyField : new HashMap<>(); Object apiKeyName = authentication.getMetadata().get(ApiKeyService.API_KEY_NAME_KEY); if (apiKeyName != null) { apiKeyField.put("name", apiKeyName); } Object apiKeyId = authentication.getMetadata().get(ApiKeyService.API_KEY_ID_KEY); if (apiKeyId != null) { apiKeyField.put("id", apiKeyId); } if (false == apiKeyField.isEmpty()) { userObject.put(apiKey, apiKeyField); } break; case REALM: final String realmKey = "realm"; final Object existingRealmField = userObject.get(realmKey); @SuppressWarnings("unchecked") final Map<String, Object> realmField = existingRealmField instanceof Map ? (Map<String, Object>) existingRealmField : new HashMap<>(); final Object realmName = ApiKeyService.getCreatorRealmName(authentication); if (realmName != null) { realmField.put("name", realmName); } final Object realmType = ApiKeyService.getCreatorRealmType(authentication); if (realmType != null) { realmField.put("type", realmType); } if (false == realmField.isEmpty()) { userObject.put(realmKey, realmField); } break; case AUTHENTICATION_TYPE: if (authentication.getAuthenticationType() != null) { userObject.put("authentication_type", authentication.getAuthenticationType().toString()); } break; default: throw new UnsupportedOperationException("unsupported property [" + property + "]"); } } ingestDocument.setFieldValue(field, userObject); return ingestDocument; }	in case you authenticate without an api key, this gets you the authentication realm, although it should get the lookup realm, otherwise the set security user processor won't work as expected for run-as users.
private static void checkDateHisto(DateHistogramAggregationBuilder source, List<RollupJobCaps> jobCaps, Set<RollupJobCaps> bestCaps) { ArrayList<RollupJobCaps> localCaps = new ArrayList<>(); for (RollupJobCaps cap : jobCaps) { RollupJobCaps.RollupFieldCaps fieldCaps = cap.getFieldCaps().get(source.field()); if (fieldCaps != null) { for (Map<String, Object> agg : fieldCaps.getAggs()) { if (agg.get(RollupField.AGG).equals(DateHistogramAggregationBuilder.NAME)) { DateHistogramInterval interval = new DateHistogramInterval((String)agg.get(RollupField.INTERVAL)); TimeZone thisTimezone = DateTimeZone.forID((String)agg.get(DateHistogramGroupConfig.TIME_ZONE)).toTimeZone(); TimeZone sourceTimeZone = source.timeZone() == null ? DateTimeZone.UTC.toTimeZone() : source.timeZone().toTimeZone(); // Ensure we are working on the same timezone if (thisTimezone.hasSameRules(sourceTimeZone) == false) { continue; } if (source.dateHistogramInterval() != null) { // Check if both are calendar and validate if they are. // If not, check if both are fixed and validate if (validateCalendarInterval(source.dateHistogramInterval(), interval)) { localCaps.add(cap); } else if (validateFixedInterval(source.dateHistogramInterval(), interval)) { localCaps.add(cap); } } else { // check if config is fixed and validate if it is if (validateFixedInterval(source.interval(), interval)) { localCaps.add(cap); } } // not a candidate if we get here break; } } } } if (localCaps.isEmpty()) { throw new IllegalArgumentException("There is not a rollup job that has a [" + source.getWriteableName() + "] agg on field [" + source.field() + "] which also satisfies all requirements of query."); } // We are a leaf, save our best caps if (source.getSubAggregations().size() == 0) { bestCaps.add(getTopEqualCaps(localCaps)); } else { // otherwise keep working down the tree source.getSubAggregations().forEach(sub -> doFindBestJobs(sub, localCaps, bestCaps)); } }	i don't think we should use timezone, it is a legacy (ish) class in the jdk. use zoneid instead? if there are places needing datetimezone from joda, there is a conversion method in dateutils.
public boolean hasOldVersionSnapshots(String repositoryName, RepositoryData repositoryData, @Nullable SnapshotId excluded) { final Collection<SnapshotId> snapshotIds = repositoryData.getSnapshotIds(); final boolean hasOldFormatSnapshots; if (snapshotIds.isEmpty()) { hasOldFormatSnapshots = false; } else { if (repositoryData.shardGenerations().totalShards() > 0) { hasOldFormatSnapshots = false; } else { final Repository repository = repositoriesService.repository(repositoryName); hasOldFormatSnapshots = snapshotIds.stream().map(repository::getSnapshotInfo).anyMatch( snapshotInfo -> (excluded == null || snapshotInfo.snapshotId().equals(excluded) == false) && snapshotInfo.version().before(SHARD_GEN_IN_REPO_DATA_VERSION)); } } assert hasOldFormatSnapshots == false || repositoryData.shardGenerations().totalShards() == 0 : "Found non-empty shard generations [" + repositoryData.shardGenerations() + "] but repository contained old version snapshots"; return hasOldFormatSnapshots; }	what if we can't load one of these snapshot info? should we be lenient in that case? will this o.w. break snapshotting in a way that wasn't broken before?
@Override protected void doNextBulk(BulkRequest request, ActionListener<BulkResponse> nextPhase) { ClientHelper.executeWithHeadersAsync(transformConfig.getHeaders(), ClientHelper.DATA_FRAME_ORIGIN, client, BulkAction.INSTANCE, request, ActionListener.wrap(bulkResponse -> { if (bulkResponse.hasFailures() && auditBulkFailures.get()) { int failureCount = 0; for(BulkItemResponse item : bulkResponse.getItems()) { if (item.isFailed()) { failureCount++; } } auditor.warning(transformId, "Experienced at least [" + failureCount + "] bulk index failures. See the logs of the node running the transform for details."); auditBulkFailures.set(false); } nextPhase.onResponse(bulkResponse); }, nextPhase::onFailure)); }	it might be nice to also include the result of bulkresponse.buildfailuremessage() here as it might be enough to avoid having to look in logs. we can still prompt them to look in logs for details.
static String buildAnnotatedText(String seq, List<NerResults.EntityGroup> entities) { if (entities.isEmpty()) { return seq; } StringBuilder annotatedResultBuilder = new StringBuilder(); int curPos = 0; for (var entity : entities) { if (entity.getStartPos() == -1) { continue; } if (entity.getStartPos() == curPos) { String entitySeq = seq.substring(entity.getStartPos(), entity.getEndPos()); annotatedResultBuilder.append("[") .append(entitySeq) .append("]") .append("(") .append(entity.getLabel()) .append("&") .append(entitySeq.replace(" ", "+")) .append(")"); curPos = entity.getEndPos(); continue; } annotatedResultBuilder.append(seq, curPos, entity.getStartPos()); String entitySeq = seq.substring(entity.getStartPos(), entity.getEndPos()); annotatedResultBuilder.append("[") .append(entitySeq) .append("]") .append("(") .append(entity.getLabel()) .append("&") .append(entitySeq.replace(" ", "+")) .append(")"); curPos = entity.getEndPos(); } if (curPos < seq.length()) { annotatedResultBuilder.append(seq, curPos, seq.length()); } return annotatedResultBuilder.toString(); }	in order to avoid doing this twice, i think we could do: if (curpos != entity.getstartpos()) { annotatedresultbuilder.append(seq, curpos, entity.getstartpos()); } string entityseq = seq.substring(entity.getstartpos(), entity.getendpos()); annotatedresultbuilder.append("[") .append(entityseq) .append("]") .append("(") .append(entity.getlabel()) .append("&") .append(entityseq.replace(" ", "+")) .append(")"); curpos = entity.getendpos();
private String bindTemplate() { List<String> binding = params.asCodeNames(); String header = null; // sort functions alphabetically if (!functions.isEmpty()) { List<ScriptFunction> ordered = new ArrayList<>(functions); ordered.sort(Comparator.comparing(f -> f.name)); StringJoiner sj = new StringJoiner("\\\\n", "", "\\\\n"); for (ScriptFunction scriptFunction : ordered) { sj.add(scriptFunction.definition); } header = sj.toString(); } String combined = header != null ? header + template : template; return binding.isEmpty() ? combined : format(Locale.ROOT, combined, binding.toArray()); }	i *think* i know why you want them sorted alphabetically but i think it'd be nice to have it in a comment.
private static boolean checkChangePasswordAction(Authentication authentication) { // we need to verify that this user was authenticated by or looked up by a realm type that support password changes // otherwise we open ourselves up to issues where a user in a different realm could be created with the same username // and do malicious things final boolean isRunAs = authentication.getUser().isRunAs(); final String realmType; if (isRunAs) { realmType = authentication.getLookedUpBy().getType(); } else { realmType = authentication.getAuthenticatedBy().getType(); } assert realmType != null; // Ensure that the user is not authenticated with an access token or an API key. // Also ensure that the user was authenticated by a realm that we can change a password for. The native realm is an internal realm // and right now only one can exist in the realm configuration - if this changes we should update this check final Authentication.AuthenticationType authType = authentication.getAuthenticationType(); return (authType.equals(Authentication.AuthenticationType.TOKEN) == false && authType.equals(Authentication.AuthenticationType.API_KEY) == false ) && (ReservedRealm.TYPE.equals(realmType) || NativeRealmSettings.TYPE.equals(realmType)); }	why do we blacklist rather than whitelist here? shouldn't we just check for the positive authtype.equals(authentication.authenticationtype.realm) ?
@Override public String toString() { StringBuilder sb = new StringBuilder(); sb.append("cluster uuid: ").append(metaData.clusterUUID()).append("\\\\n"); sb.append("term: ").append(term).append("\\\\n"); sb.append("version: ").append(version).append("\\\\n"); sb.append("state uuid: ").append(stateUUID).append("\\\\n"); sb.append("last committed config: ").append(getLastCommittedConfiguration()).append("\\\\n"); sb.append("last accepted config: ").append(getLastAcceptedConfiguration()).append("\\\\n"); sb.append("voting tombstones: ").append(votingTombstones.toString()).append("\\\\n"); sb.append("from_diff: ").append(wasReadFromDiff).append("\\\\n"); sb.append("meta data version: ").append(metaData.version()).append("\\\\n"); final String TAB = " "; for (IndexMetaData indexMetaData : metaData) { sb.append(TAB).append(indexMetaData.getIndex()); sb.append(": v[").append(indexMetaData.getVersion()) .append("], mv[").append(indexMetaData.getMappingVersion()) .append("], sv[").append(indexMetaData.getSettingsVersion()) .append("]\\\\n"); for (int shard = 0; shard < indexMetaData.getNumberOfShards(); shard++) { sb.append(TAB).append(TAB).append(shard).append(": "); sb.append("p_term [").append(indexMetaData.primaryTerm(shard)).append("], "); sb.append("isa_ids ").append(indexMetaData.inSyncAllocationIds(shard)).append("\\\\n"); } } if (metaData.customs().isEmpty() == false) { sb.append("metadata customs:\\\\n"); for (final ObjectObjectCursor<String, MetaData.Custom> cursor : metaData.customs()) { final String type = cursor.key; final MetaData.Custom custom = cursor.value; sb.append(TAB).append(type).append(": ").append(custom); } sb.append("\\\\n"); } sb.append(blocks()); sb.append(nodes()); sb.append(routingTable()); sb.append(getRoutingNodes()); if (customs.isEmpty() == false) { sb.append("customs:\\\\n"); for (ObjectObjectCursor<String, Custom> cursor : customs) { final String type = cursor.key; final Custom custom = cursor.value; sb.append(TAB).append(type).append(": ").append(custom); } } return sb.toString(); }	is the call tostring() necessary?
public ClusterState build() { if (UNKNOWN_UUID.equals(uuid)) { uuid = UUIDs.randomBase64UUID(); } return new ClusterState(clusterName, term, version, uuid, metaData, routingTable, nodes, blocks, customs.build(), lastCommittedConfiguration, lastAcceptedConfiguration, Collections.unmodifiableSet(votingTombstones), fromDiff); }	also create copy of set here? o.w. some can still modify the set using original builder. alternatively you can create new sets instead of calling add and clear
public void testRescoreAfterCollapse() throws Exception { assertAcked(prepareCreate("test") .addMapping( "type1", jsonBuilder() .startObject() .startObject("properties") .startObject("group") .field("type", "keyword") .endObject() .endObject() .endObject()) ); indexDocument(1, "value", "a"); indexDocument(2, "one one value", "a"); indexDocument(3, "one one two value", "b"); // should be highest on rescore, but filtered out during collapse indexDocument(4, "one two two value", "b"); refresh("test"); SearchResponse searchResponse = client().prepareSearch("test") .setTypes("type1") .setQuery(new MatchQueryBuilder("name", "one")) .addRescorer(new QueryRescorerBuilder(new MatchQueryBuilder("name", "two"))) .setCollapse(new CollapseBuilder("group")) .execute() .actionGet(); assertThat(searchResponse.getHits().totalHits, equalTo(3L)); assertThat(searchResponse.getHits().getHits().length, equalTo(2)); Map<String, Float> collapsedHits = Arrays .stream(searchResponse.getHits().getHits()) .collect(Collectors.toMap(SearchHit::getId, SearchHit::getScore)); assertThat(collapsedHits.keySet(), containsInAnyOrder("2", "3")); assertThat(collapsedHits.get("3"), greaterThan(collapsedHits.get("2"))); }	the score of this query depends on the number of shards, the default similarity, ... to make sure that we have consistent scoring you can use a function_score query like the following: querybuilder query = functionscorequery( termquery("name", "one"), scorefunctionbuilders.fieldvaluefactorfunction("my_static_doc_score") ).boostmode(combinefunction.replace); ... and add the my_static_doc_score at indexing time.
public void testRescoreAfterCollapse() throws Exception { assertAcked(prepareCreate("test") .addMapping( "type1", jsonBuilder() .startObject() .startObject("properties") .startObject("group") .field("type", "keyword") .endObject() .endObject() .endObject()) ); indexDocument(1, "value", "a"); indexDocument(2, "one one value", "a"); indexDocument(3, "one one two value", "b"); // should be highest on rescore, but filtered out during collapse indexDocument(4, "one two two value", "b"); refresh("test"); SearchResponse searchResponse = client().prepareSearch("test") .setTypes("type1") .setQuery(new MatchQueryBuilder("name", "one")) .addRescorer(new QueryRescorerBuilder(new MatchQueryBuilder("name", "two"))) .setCollapse(new CollapseBuilder("group")) .execute() .actionGet(); assertThat(searchResponse.getHits().totalHits, equalTo(3L)); assertThat(searchResponse.getHits().getHits().length, equalTo(2)); Map<String, Float> collapsedHits = Arrays .stream(searchResponse.getHits().getHits()) .collect(Collectors.toMap(SearchHit::getId, SearchHit::getScore)); assertThat(collapsedHits.keySet(), containsInAnyOrder("2", "3")); assertThat(collapsedHits.get("3"), greaterThan(collapsedHits.get("2"))); }	you can use the same for the rescore with another field for instance
public void testBulkWithWriteIndexAndRouting() { Map<String, Integer> twoShardsSettings = Collections.singletonMap(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 2); client().admin().indices().prepareCreate("index1") .addAlias(new Alias("alias1").indexRouting("0")).setSettings(twoShardsSettings).get(); client().admin().indices().prepareCreate("index2") .addAlias(new Alias("alias1").indexRouting("0").writeIndex(randomFrom(false, null))) .setSettings(twoShardsSettings).get(); client().admin().indices().prepareCreate("index3") .addAlias(new Alias("alias1").indexRouting("1").writeIndex(true)).setSettings(twoShardsSettings).get(); IndexRequest indexRequestWithAlias = new IndexRequest("alias1", "type", "id"); indexRequestWithAlias.source(Collections.singletonMap("foo", "baz")); BulkResponse bulkResponse = client().prepareBulk().add(indexRequestWithAlias).get(); assertThat(bulkResponse.getItems()[0].getResponse().getIndex(), equalTo("index3")); assertThat(bulkResponse.getItems()[0].getResponse().getShardId().getId(), equalTo(0)); assertThat(bulkResponse.getItems()[0].getResponse().getVersion(), equalTo(1L)); assertThat(bulkResponse.getItems()[0].getResponse().status(), equalTo(RestStatus.CREATED)); assertThat(client().prepareGet("index3", "type", "id").setRouting("1").get().getSource().get("foo"), equalTo("baz")); bulkResponse = client().prepareBulk().add(client().prepareUpdate("alias1", "type", "id").setDoc("foo", "updated")).get(); assertFalse(bulkResponse.hasFailures()); assertThat(client().prepareGet("index3", "type", "id").setRouting("1").get().getSource().get("foo"), equalTo("updated")); bulkResponse = client().prepareBulk().add(client().prepareDelete("alias1", "type", "id")).get(); assertFalse(bulkResponse.hasFailures()); assertFalse(client().prepareGet("index3", "type", "id").setRouting("1").get().isExists()); }	randomly set the routing to "1"?
public void testConcurrentConnectsAndDisconnects() throws BrokenBarrierException, InterruptedException { DiscoveryNode node = new DiscoveryNode("", new TransportAddress(InetAddress.getLoopbackAddress(), 0), Version.CURRENT); doAnswer(invocationOnMock -> { Transport.Connection connection = new TestConnect(node); ActionListener<Transport.Connection> listener = (ActionListener<Transport.Connection>) invocationOnMock.getArguments()[2]; if (rarely()) { listener.onResponse(connection); } else if (frequently()) { threadPool.generic().execute(() -> listener.onResponse(connection)); } else { threadPool.generic().execute(() -> listener.onFailure(new IllegalStateException("dummy exception"))); } return null; }).when(transport).openConnection(eq(node), eq(connectionProfile), any(ActionListener.class)); assertFalse(connectionManager.nodeConnected(node)); ConnectionManager.ConnectionValidator validator = (c, p, l) -> { if (rarely()) { l.onResponse(null); } else if (frequently()) { threadPool.generic().execute(() -> l.onResponse(null)); } else { threadPool.generic().execute(() -> l.onFailure(new IllegalStateException("dummy exception"))); } }; List<Thread> threads = new ArrayList<>(); AtomicInteger nodeConnectedCount = new AtomicInteger(); AtomicInteger nodeFailureCount = new AtomicInteger(); CyclicBarrier barrier = new CyclicBarrier(11); for (int i = 0; i < 10; i++) { Thread thread = new Thread(() -> { try { barrier.await(); } catch (InterruptedException | BrokenBarrierException e) { throw new RuntimeException(e); } CountDownLatch latch = new CountDownLatch(1); connectionManager.connectToNode(node, connectionProfile, validator, ActionListener.wrap(c -> { nodeConnectedCount.incrementAndGet(); if (connectionManager.nodeConnected(node) == false) { throw new AssertionError("Expected node to be connected"); } assert latch.getCount() == 1; latch.countDown(); }, e -> { nodeFailureCount.incrementAndGet(); assert latch.getCount() == 1; latch.countDown(); })); try { latch.await(); } catch (InterruptedException e) { throw new IllegalStateException(e); } }); threads.add(thread); thread.start(); } barrier.await(); threads.forEach(t -> { try { t.join(); } catch (InterruptedException e) { throw new IllegalStateException(e); } }); assertEquals(10, nodeConnectedCount.get() + nodeFailureCount.get()); }	could we keep track of all of these connections and assert that all but one of them has been closed when the dust has settled?
public void testConcurrentConnectsAndDisconnects() throws BrokenBarrierException, InterruptedException { DiscoveryNode node = new DiscoveryNode("", new TransportAddress(InetAddress.getLoopbackAddress(), 0), Version.CURRENT); doAnswer(invocationOnMock -> { Transport.Connection connection = new TestConnect(node); ActionListener<Transport.Connection> listener = (ActionListener<Transport.Connection>) invocationOnMock.getArguments()[2]; if (rarely()) { listener.onResponse(connection); } else if (frequently()) { threadPool.generic().execute(() -> listener.onResponse(connection)); } else { threadPool.generic().execute(() -> listener.onFailure(new IllegalStateException("dummy exception"))); } return null; }).when(transport).openConnection(eq(node), eq(connectionProfile), any(ActionListener.class)); assertFalse(connectionManager.nodeConnected(node)); ConnectionManager.ConnectionValidator validator = (c, p, l) -> { if (rarely()) { l.onResponse(null); } else if (frequently()) { threadPool.generic().execute(() -> l.onResponse(null)); } else { threadPool.generic().execute(() -> l.onFailure(new IllegalStateException("dummy exception"))); } }; List<Thread> threads = new ArrayList<>(); AtomicInteger nodeConnectedCount = new AtomicInteger(); AtomicInteger nodeFailureCount = new AtomicInteger(); CyclicBarrier barrier = new CyclicBarrier(11); for (int i = 0; i < 10; i++) { Thread thread = new Thread(() -> { try { barrier.await(); } catch (InterruptedException | BrokenBarrierException e) { throw new RuntimeException(e); } CountDownLatch latch = new CountDownLatch(1); connectionManager.connectToNode(node, connectionProfile, validator, ActionListener.wrap(c -> { nodeConnectedCount.incrementAndGet(); if (connectionManager.nodeConnected(node) == false) { throw new AssertionError("Expected node to be connected"); } assert latch.getCount() == 1; latch.countDown(); }, e -> { nodeFailureCount.incrementAndGet(); assert latch.getCount() == 1; latch.countDown(); })); try { latch.await(); } catch (InterruptedException e) { throw new IllegalStateException(e); } }); threads.add(thread); thread.start(); } barrier.await(); threads.forEach(t -> { try { t.join(); } catch (InterruptedException e) { throw new IllegalStateException(e); } }); assertEquals(10, nodeConnectedCount.get() + nodeFailureCount.get()); }	i think it would be good to have these three branches chosen with more equal probabilities (and similarly for the connection validator lambda). as it is, we only complete both listeners on the same thread one in every 10,000 runs i think.
@Override void executeSearch(final Queue<SearchRequestSlot> requests, final AtomicArray<MultiSearchResponse.Item> responses, final AtomicInteger responseCounter, final ActionListener<MultiSearchResponse> listener, long startTimeInNanos) { expected.set(1000000); super.executeSearch(requests, responses, responseCounter, listener, startTimeInNanos); }	same suggestion about the addition parameter.
DatabaseReader get() throws IOException { if (lastUpdate != 0) { Path fileName = databasePath.getFileName(); if (System.currentTimeMillis() - lastUpdate > Duration.ofDays(30).toMillis()) { throw new IllegalStateException("database [" + fileName + "] was not updated for 30 days and is disabled"); } else if (System.currentTimeMillis() - lastUpdate > Duration.ofDays(25).toMillis() && warningEmitted.getAndSet(true)) { LOGGER.warn("database [{}] was not updated for over 25 days, ingestion will fail if there is no update for 30 days", fileName); } } if (databaseReader.get() == null) { synchronized (databaseReader) { if (databaseReader.get() == null) { databaseReader.set(loader.get()); LOGGER.debug("loaded [{}] geo-IP database", databasePath); } } } return databaseReader.get(); }	this can lead to many logs and flooding the disk. perhaps we should emit just a warning? (using headerwarning.addwarning(...))
@Override public int hashCode() { return Objects.hash(super.hashCode(), type); } } private Explicit<Boolean> ignoreMalformed; private Explicit<Boolean> coerce; private NumberFieldMapper( String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Explicit<Boolean> ignoreMalformed, Explicit<Boolean> coerce, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); this.ignoreMalformed = ignoreMalformed; this.coerce = coerce; } @Override public NumberFieldType fieldType() { return (NumberFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType.typeName(); } @Override protected NumberFieldMapper clone() { return (NumberFieldMapper) super.clone(); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { XContentParser parser = context.parser(); Object value; Number numericValue = null; if (context.externalValueSet()) { value = context.externalValue(); } else if (parser.currentToken() == Token.VALUE_NULL) { value = null; } else if (coerce.value() && parser.currentToken() == Token.VALUE_STRING && parser.textLength() == 0) { value = null; } else { try { numericValue = fieldType().type.parse(parser, coerce.value()); } catch (IllegalArgumentException | JsonParseException e) { if (ignoreMalformed.value()) { context.addIgnoredField(fieldType.name()); return; } else { throw e; } } value = numericValue; } if (value == null) { value = fieldType().nullValue(); } if (value == null) { return; } if (numericValue == null) { numericValue = fieldType().type.parse(value, coerce.value()); } boolean indexed = fieldType().indexOptions() != IndexOptions.NONE; boolean docValued = fieldType().hasDocValues(); boolean stored = fieldType().stored(); fields.addAll(fieldType().type.createFields(fieldType().name(), numericValue, indexed, docValued, stored)); if (docValued == false && (stored || indexed)) { createFieldNamesField(context, fields); } } @Override protected void doMerge(Mapper mergeWith) { super.doMerge(mergeWith); NumberFieldMapper other = (NumberFieldMapper) mergeWith; if (other.ignoreMalformed.explicit()) { this.ignoreMalformed = other.ignoreMalformed; } if (other.coerce.explicit()) { this.coerce = other.coerce; } } @Override protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException { super.doXContentBody(builder, includeDefaults, params); if (includeDefaults || ignoreMalformed.explicit()) { builder.field("ignore_malformed", ignoreMalformed.value()); } if (includeDefaults || coerce.explicit()) { builder.field("coerce", coerce.value()); } if (includeDefaults || fieldType().nullValue() != null) { builder.field("null_value", fieldType().nullValue()); }	i haven't tested this, but i wanted to check that this will not introduce too much leniency. in particular, i'm wondering what happens if we try to pass a json object to a numeric field with ignore_malformed enabled.
public void testIgnoreMalformed() throws Exception { for (String type : TYPES) { Object malformedValue = randomBoolean() ? "a" : false; String mapping = Strings.toString(jsonBuilder().startObject().startObject("type").startObject("properties") .startObject("field").field("type", type).endObject().endObject().endObject().endObject()); DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping)); assertEquals(mapping, mapper.mappingSource().toString()); ThrowingRunnable runnable = () -> mapper.parse(new SourceToParse("test", "type", "1", BytesReference.bytes(jsonBuilder().startObject().field("field", malformedValue).endObject()), XContentType.JSON)); MapperParsingException e = expectThrows(MapperParsingException.class, runnable); if (malformedValue instanceof String) { assertThat(e.getCause().getMessage(), containsString("For input string: \\\\"a\\\\"")); } else { assertThat(e.getCause().getMessage(), containsString("Current token")); assertThat(e.getCause().getMessage(), containsString("not numeric, can not use numeric value accessors")); } mapping = Strings .toString(jsonBuilder().startObject().startObject("type").startObject("properties").startObject("field") .field("type", type).field("ignore_malformed", true).endObject().endObject().endObject().endObject()); DocumentMapper mapper2 = parser.parse("type", new CompressedXContent(mapping)); ParsedDocument doc = mapper2.parse(new SourceToParse("test", "type", "1", BytesReference.bytes(jsonBuilder().startObject().field("field", malformedValue).endObject()), XContentType.JSON)); IndexableField[] fields = doc.rootDoc().getFields("field"); assertEquals(0, fields.length); assertArrayEquals(new String[] { "field" }, doc.rootDoc().getValues("_ignored")); } }	instead of using randomization, it might be good to always test both values through a for loop. we just added some guidance to our testing docs around avoiding randomized testing for coverage: https://github.com/elastic/elasticsearch/blob/master/testing.asciidoc#bad-practices
@Override public Settings additionalSettings() { return Settings.builder().put(super.additionalSettings()) .put(NetworkModule.HTTP_DEFAULT_TYPE_SETTING.getKey(), NETTY_HTTP_TRANSPORT_NAME) .put(NetworkModule.TRANSPORT_DEFAULT_TYPE_SETTING.getKey(), NETTY_TRANSPORT_NAME).build(); }	no need for the super call, it is just the abstract plugin that returns an empty list
public LeafReader wrap(LeafReader leaf) { try { if (leaf.getLiveDocs() == null) { return leaf; } DocIdSetIterator rollbackDocs = DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(ROLLED_BACK_FIELD, leaf); if (rollbackDocs == null) { return new SubReaderWithLiveDocs(leaf, null, leaf.maxDoc()); } int rollbackCount = 0; FixedBitSet liveDocs = new FixedBitSet(leaf.maxDoc()); liveDocs.set(0, liveDocs.length()); int docId; while ((docId = rollbackDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) { assert leaf.getLiveDocs().get(docId) == false : "doc [" + docId + "] is rolled back but not deleted"; liveDocs.clear(docId); rollbackCount++; } return new SubReaderWithLiveDocs(leaf, liveDocs, leaf.maxDoc() - rollbackCount); } catch (IOException ex) { throw new UncheckedIOException(ex); } }	should this be cached somehow? /cc @jpountz
public boolean maybeRollback(MapperService mapperService, Operation newOp) throws IOException { Searcher engineSearcher = null; try { engineSearcher = acquireSearcher("rollback", SearcherScope.INTERNAL); IndexSearcher searcher = new IndexSearcher(Lucene.wrapAllDocsLive(engineSearcher.getDirectoryReader())); searcher.setQueryCache(null); final FieldsVisitor fields; final Query currentVersionQuery = LongPoint.newExactQuery(SeqNoFieldMapper.NAME, newOp.seqNo()); try (ReleasableLock ignored = readLock.acquire()) { final TopDocs collidingDocs = searcher.search(currentVersionQuery, 1, new Sort(new SortedNumericSortField(SeqNoFieldMapper.PRIMARY_TERM_NAME, SortField.Type.LONG, true))); if (collidingDocs.totalHits == 0) { return true; // not found is equivalent to rolled back } List<LeafReaderContext> leaves = searcher.getIndexReader().leaves(); LeafReaderContext collidingSegment = leaves.get(ReaderUtil.subIndex(collidingDocs.scoreDocs[0].doc, leaves)); int collidingDocId = collidingDocs.scoreDocs[0].doc - collidingSegment.docBase; long collidingTerm = Lucene.readNumericDV(collidingSegment.reader(), SeqNoFieldMapper.PRIMARY_TERM_NAME, collidingDocId); if (collidingTerm == newOp.primaryTerm()) { // matches with the existing doc localCheckpointTracker.markSeqNoAsCompleted(newOp.seqNo()); return false; } // Look up the _uid fields = new FieldsVisitor(false); collidingSegment.reader().document(collidingDocId, fields); fields.postProcess(mapperService); } try (ReleasableLock ignored = writeLock.acquire()) { // delete the current version by marking it as rolled back while (tryUpdateDocValues(searcher, currentVersionQuery, softDeleteField, rolledbackField) == false) { final Searcher oldSearcher = engineSearcher; engineSearcher = null; IOUtils.close(oldSearcher); refresh("rollback", SearcherScope.INTERNAL); engineSearcher = acquireSearcher("rollback", SearcherScope.INTERNAL); searcher = new IndexSearcher(Lucene.wrapAllDocsLive(engineSearcher.getDirectoryReader())); searcher.setQueryCache(null); } // restore the previous version if (fields.uid() != null) { final BytesRef uid = Uid.encodeId(fields.uid().id()); try (Releasable uidLock = versionMap.acquireLock(uid)) { restorePreviousVersion(searcher, uid, newOp.seqNo()); } } } return true; } catch (IOException e) { try { maybeFailEngine("rollback", e); } catch (Exception inner) { e.addSuppressed(inner); } throw e; } finally { IOUtils.close(engineSearcher); } }	i think this should be done on the top level try no? before we call acquiresearcher.
public boolean maybeRollback(MapperService mapperService, Operation newOp) throws IOException { Searcher engineSearcher = null; try { engineSearcher = acquireSearcher("rollback", SearcherScope.INTERNAL); IndexSearcher searcher = new IndexSearcher(Lucene.wrapAllDocsLive(engineSearcher.getDirectoryReader())); searcher.setQueryCache(null); final FieldsVisitor fields; final Query currentVersionQuery = LongPoint.newExactQuery(SeqNoFieldMapper.NAME, newOp.seqNo()); try (ReleasableLock ignored = readLock.acquire()) { final TopDocs collidingDocs = searcher.search(currentVersionQuery, 1, new Sort(new SortedNumericSortField(SeqNoFieldMapper.PRIMARY_TERM_NAME, SortField.Type.LONG, true))); if (collidingDocs.totalHits == 0) { return true; // not found is equivalent to rolled back } List<LeafReaderContext> leaves = searcher.getIndexReader().leaves(); LeafReaderContext collidingSegment = leaves.get(ReaderUtil.subIndex(collidingDocs.scoreDocs[0].doc, leaves)); int collidingDocId = collidingDocs.scoreDocs[0].doc - collidingSegment.docBase; long collidingTerm = Lucene.readNumericDV(collidingSegment.reader(), SeqNoFieldMapper.PRIMARY_TERM_NAME, collidingDocId); if (collidingTerm == newOp.primaryTerm()) { // matches with the existing doc localCheckpointTracker.markSeqNoAsCompleted(newOp.seqNo()); return false; } // Look up the _uid fields = new FieldsVisitor(false); collidingSegment.reader().document(collidingDocId, fields); fields.postProcess(mapperService); } try (ReleasableLock ignored = writeLock.acquire()) { // delete the current version by marking it as rolled back while (tryUpdateDocValues(searcher, currentVersionQuery, softDeleteField, rolledbackField) == false) { final Searcher oldSearcher = engineSearcher; engineSearcher = null; IOUtils.close(oldSearcher); refresh("rollback", SearcherScope.INTERNAL); engineSearcher = acquireSearcher("rollback", SearcherScope.INTERNAL); searcher = new IndexSearcher(Lucene.wrapAllDocsLive(engineSearcher.getDirectoryReader())); searcher.setQueryCache(null); } // restore the previous version if (fields.uid() != null) { final BytesRef uid = Uid.encodeId(fields.uid().id()); try (Releasable uidLock = versionMap.acquireLock(uid)) { restorePreviousVersion(searcher, uid, newOp.seqNo()); } } } return true; } catch (IOException e) { try { maybeFailEngine("rollback", e); } catch (Exception inner) { e.addSuppressed(inner); } throw e; } finally { IOUtils.close(engineSearcher); } }	this shouldn't be done here - it's part of the indexing logic.
private <T> T invokeParser(String eql, Function<EqlBaseParser, ParserRuleContext> parseFunction, BiFunction<AstBuilder, ParserRuleContext, T> visitor) { try { EqlBaseLexer lexer = new EqlBaseLexer(new ANTLRInputStream(eql)); lexer.removeErrorListeners(); lexer.addErrorListener(ERROR_LISTENER); CommonTokenStream tokenStream = new CommonTokenStream(lexer); EqlBaseParser parser = new EqlBaseParser(tokenStream); parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames()))); parser.removeErrorListeners(); parser.addErrorListener(ERROR_LISTENER); parser.getInterpreter().setPredictionMode(PredictionMode.SLL); if (DEBUG) { debug(parser); tokenStream.fill(); for (Token t : tokenStream.getTokens()) { String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType()); String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType()); log.info(format(Locale.ROOT, " %-15s '%s'", symbolicName == null ? literalName : symbolicName, t.getText())); } } ParserRuleContext tree = parseFunction.apply(parser); if (DEBUG) { log.info("Parse tree {} " + tree.toStringTree()); } return visitor.apply(new AstBuilder(), tree); } catch (StackOverflowError e) { throw new ParsingException("EQL statement is too large, " + "causing stack overflow when generating the parsing tree: [{}]", eql); } }	without the case insensitive stream the query foo where would be invalid. in fact with the current grammar, any keyword not lowercase would fail. note that the stream is case insensitive for symbols and it doesn't interfere with the identifiers, qualified or not.
private List<Line> decomposeGeometry(Line line, List<Line> lines) { for (Line part : decompose(line)) { double[] lats = new double[part.length()]; double[] lons = new double[part.length()]; for (int i = 0; i < part.length(); i++) { lats[i] = normalizeLat(part.getY(i)); lons[i] = normalizeLonMinus180Inclusive(part.getX(i)); } lines.add(new Line(lons, lats)); } return lines; }	since this is a very specific operation/definition of a normalized lon, maybe javadocs would be helpful here
private static void ensureRepositoryNotInUse(ClusterState clusterState, String repository) { if (isRepositoryInUse(clusterState, repository)) { throw new RepositoryConflictException(repository, "repository conflict," + " trying to modify or unregister repository that is currently used"); } }	no need for the first part of the message imo, the fact that it's a repository conflict is already in the exception type :) "trying to modify or unregister repository that is currently used" is still all we need i think
public void testRepositoryConflict() throws Exception { logger.info("--> creating repository"); final String repo = "test-repo"; assertAcked(client().admin().cluster().preparePutRepository(repo).setType("mock").setSettings( Settings.builder() .put("location", randomRepoPath()) .put("random", randomAlphaOfLength(10)) .put("wait_after_unblock", 200)).get()); logger.info("--> snapshot"); final String index = "test-idx"; assertAcked(prepareCreate(index, 1, Settings.builder().put("number_of_shards", 1).put("number_of_replicas", 0))); for (int i = 0; i < 10; i++) { indexDoc(index, Integer.toString(i), "foo", "bar" + i); } refresh(); final String snapshot1 = "test-snap1"; client().admin().cluster().prepareCreateSnapshot(repo, snapshot1).setWaitForCompletion(true).get(); String blockedNode = internalCluster().getMasterName(); ((MockRepository)internalCluster().getInstance(RepositoriesService.class, blockedNode).repository(repo)).blockOnDataFiles(true); logger.info("--> start deletion of snapshot"); ActionFuture<AcknowledgedResponse> future = client().admin().cluster().prepareDeleteSnapshot(repo, snapshot1).execute(); logger.info("--> waiting for block to kick in on node [{}]", blockedNode); waitForBlock(blockedNode, repo, TimeValue.timeValueSeconds(10)); logger.info("--> try deleting the repository, should fail because the deletion of the snapshot is in progress"); try { client().admin().cluster().prepareDeleteRepository(repo).get(); fail("should not be able to delete a repository while it is being used"); } catch (RepositoryConflictException e) { assertThat(e.status(), equalTo(RestStatus.CONFLICT)); assertThat(e.getMessage(), containsString("repository conflict," + " trying to modify or unregister repository that is currently used")); } logger.info("--> try updating the repository, should fail because the deletion of the snapshot is in progress"); try { client().admin().cluster().preparePutRepository(repo).setType("mock").setSettings( Settings.builder() .put("location", randomRepoPath())).get(); fail("should not be able to update a repository while it is being used"); } catch (RepositoryConflictException e) { assertThat(e.status(), equalTo(RestStatus.CONFLICT)); assertThat(e.getMessage(), containsString("repository conflict," + " trying to modify or unregister repository that is currently used")); } logger.info("--> unblocking blocked node [{}]", blockedNode); unblockNode(repo, blockedNode); logger.info("--> wait until snapshot deletion is finished"); assertAcked(future.actionGet()); }	no need to index any documents for this test, even without any documents in it the index will have some empty data files that will be copied to the repository and trigger the block.
public void testRepositoryConflict() throws Exception { logger.info("--> creating repository"); final String repo = "test-repo"; assertAcked(client().admin().cluster().preparePutRepository(repo).setType("mock").setSettings( Settings.builder() .put("location", randomRepoPath()) .put("random", randomAlphaOfLength(10)) .put("wait_after_unblock", 200)).get()); logger.info("--> snapshot"); final String index = "test-idx"; assertAcked(prepareCreate(index, 1, Settings.builder().put("number_of_shards", 1).put("number_of_replicas", 0))); for (int i = 0; i < 10; i++) { indexDoc(index, Integer.toString(i), "foo", "bar" + i); } refresh(); final String snapshot1 = "test-snap1"; client().admin().cluster().prepareCreateSnapshot(repo, snapshot1).setWaitForCompletion(true).get(); String blockedNode = internalCluster().getMasterName(); ((MockRepository)internalCluster().getInstance(RepositoriesService.class, blockedNode).repository(repo)).blockOnDataFiles(true); logger.info("--> start deletion of snapshot"); ActionFuture<AcknowledgedResponse> future = client().admin().cluster().prepareDeleteSnapshot(repo, snapshot1).execute(); logger.info("--> waiting for block to kick in on node [{}]", blockedNode); waitForBlock(blockedNode, repo, TimeValue.timeValueSeconds(10)); logger.info("--> try deleting the repository, should fail because the deletion of the snapshot is in progress"); try { client().admin().cluster().prepareDeleteRepository(repo).get(); fail("should not be able to delete a repository while it is being used"); } catch (RepositoryConflictException e) { assertThat(e.status(), equalTo(RestStatus.CONFLICT)); assertThat(e.getMessage(), containsString("repository conflict," + " trying to modify or unregister repository that is currently used")); } logger.info("--> try updating the repository, should fail because the deletion of the snapshot is in progress"); try { client().admin().cluster().preparePutRepository(repo).setType("mock").setSettings( Settings.builder() .put("location", randomRepoPath())).get(); fail("should not be able to update a repository while it is being used"); } catch (RepositoryConflictException e) { assertThat(e.status(), equalTo(RestStatus.CONFLICT)); assertThat(e.getMessage(), containsString("repository conflict," + " trying to modify or unregister repository that is currently used")); } logger.info("--> unblocking blocked node [{}]", blockedNode); unblockNode(repo, blockedNode); logger.info("--> wait until snapshot deletion is finished"); assertAcked(future.actionGet()); }	we actually have a neat short-cut for testing this kind of spot with the expectthrows method that keeps this much shorter. see https://github.com/elastic/elasticsearch/blob/master/server/src/test/java/org/elasticsearch/snapshots/sharedclustersnapshotrestoreit.java#l2169 for an example.
public void testRepositoryConflict() throws Exception { logger.info("--> creating repository"); final String repo = "test-repo"; assertAcked(client().admin().cluster().preparePutRepository(repo).setType("mock").setSettings( Settings.builder() .put("location", randomRepoPath()) .put("random", randomAlphaOfLength(10)) .put("wait_after_unblock", 200)).get()); logger.info("--> snapshot"); final String index = "test-idx"; assertAcked(prepareCreate(index, 1, Settings.builder().put("number_of_shards", 1).put("number_of_replicas", 0))); for (int i = 0; i < 10; i++) { indexDoc(index, Integer.toString(i), "foo", "bar" + i); } refresh(); final String snapshot1 = "test-snap1"; client().admin().cluster().prepareCreateSnapshot(repo, snapshot1).setWaitForCompletion(true).get(); String blockedNode = internalCluster().getMasterName(); ((MockRepository)internalCluster().getInstance(RepositoriesService.class, blockedNode).repository(repo)).blockOnDataFiles(true); logger.info("--> start deletion of snapshot"); ActionFuture<AcknowledgedResponse> future = client().admin().cluster().prepareDeleteSnapshot(repo, snapshot1).execute(); logger.info("--> waiting for block to kick in on node [{}]", blockedNode); waitForBlock(blockedNode, repo, TimeValue.timeValueSeconds(10)); logger.info("--> try deleting the repository, should fail because the deletion of the snapshot is in progress"); try { client().admin().cluster().prepareDeleteRepository(repo).get(); fail("should not be able to delete a repository while it is being used"); } catch (RepositoryConflictException e) { assertThat(e.status(), equalTo(RestStatus.CONFLICT)); assertThat(e.getMessage(), containsString("repository conflict," + " trying to modify or unregister repository that is currently used")); } logger.info("--> try updating the repository, should fail because the deletion of the snapshot is in progress"); try { client().admin().cluster().preparePutRepository(repo).setType("mock").setSettings( Settings.builder() .put("location", randomRepoPath())).get(); fail("should not be able to update a repository while it is being used"); } catch (RepositoryConflictException e) { assertThat(e.status(), equalTo(RestStatus.CONFLICT)); assertThat(e.getMessage(), containsString("repository conflict," + " trying to modify or unregister repository that is currently used")); } logger.info("--> unblocking blocked node [{}]", blockedNode); unblockNode(repo, blockedNode); logger.info("--> wait until snapshot deletion is finished"); assertAcked(future.actionGet()); }	same here, expectthrows is nicer i think :)
private void assertApiKeyMetadata() { assert (AuthenticationType.API_KEY.equals(this.type) == false) || (this.metadata.get(AuthenticationField.API_KEY_ID_KEY) != null) : "API KEY authentication requires metadata to contain API KEY id, and the value must be non-null."; }	we can wrap this method inside a if (assertions.enabled) { ... }
static CharMatcher parseTokenChars(Settings settings) { List<String> characterClasses = settings.getAsList("token_chars"); if (characterClasses == null || characterClasses.isEmpty()) { return null; } CharMatcher.Builder builder = new CharMatcher.Builder(); for (String characterClass : characterClasses) { characterClass = characterClass.toLowerCase(Locale.ROOT).trim(); CharMatcher matcher = MATCHERS.get(characterClass); if (matcher == null) { if (characterClass.equals("custom") == false) { throw new IllegalArgumentException("Unknown token type: '" + characterClass + "', must be one of " + MATCHERS.keySet()); } String customCharacters = settings.get("custom_token_chars"); if (customCharacters == null) { throw new IllegalArgumentException("Token type: 'custom' requires setting `custom_token_chars`"); } final Set<Integer> customCharSet = customCharacters.chars().boxed().collect(Collectors.toSet()); matcher = new CharMatcher() { @Override public boolean isTokenChar(int c) { return customCharSet.contains(c); } }; } builder.or(matcher); } return builder.build(); }	i think we need to include custom in the list here as well?
public Map<String, DocumentField> getFields() { Map<String, DocumentField> fields = new HashMap<>(); fields.putAll(metaFields); fields.putAll(documentFields); return fields; }	why clear? we should just set to maps.
public Map<String, DocumentField> getFields() { Map<String, DocumentField> fields = new HashMap<>(); fields.putAll(metaFields); fields.putAll(documentFields); return fields; }	while null is allowed right now, i don't think we ever actually pass it. i see 3 non-test uses of this method, all of which pass a non-null map. i think you can add a requiresnonnull here.
public static void ensureImageIsLoaded(Distribution distribution) { Shell.Result result = sh.run("docker image ls --format '{{.Repository}}' " + distribution.flavor.name); final long count = Arrays.stream(result.stdout.split("\\\\n")) .map(String::trim) .filter(s -> s.isEmpty() == false) .count(); if (count != 0) { return; } logger.info("Loading Docker image: " + distribution.path); sh.run("docker load -i " + distribution.path); }	nit: this can be simplified as .filter(not(string::isempty))
private static String normalize(String name) { // translate CamelCase to camel_case return StringUtils.camelCaseToUnderscore(name); }	returning a list instead of the treeset so it can be concatenated into the list makes things easier - the consumer of the list doesn't care about the ordering.
public void testOctetLength() { StringProcessor proc = new StringProcessor(StringOperation.OCTET_LENGTH); assertNull(proc.process(null)); assertEquals(7, proc.process("foo bar")); assertEquals(0, proc.process("")); assertEquals(1, proc.process('f')); stringCharInputValidation(proc); }	maybe add a test with utf-16 string.
void deleteResponse(AsyncSearchId searchId, ActionListener<DeleteResponse> listener) { DeleteRequest request = new DeleteRequest(INDEX).id(searchId.getDocId()); client.delete(request, listener); } /** * Returns the {@link AsyncSearchTask} if the provided <code>searchId</code> * is registered in the task manager, <code>null</code> otherwise. * * This method throws a {@link ResourceNotFoundException}	i tried to make this logic more readable. i found the boolean flag hard to reason about especially as it was provided true only once. i moved the listener wrapping to the callers, where each caller needs to do something different.
protected void doExecute(Task task, GetAsyncSearchAction.Request request, ActionListener<AsyncSearchResponse> listener) { try { long nowInMillis = System.currentTimeMillis(); AsyncSearchId searchId = AsyncSearchId.decode(request.getId()); DiscoveryNode node = clusterService.state().nodes().get(searchId.getTaskId().getNodeId()); if (clusterService.localNode().getId().equals(searchId.getTaskId().getNodeId()) || node == null) { if (request.getKeepAlive().getMillis() > 0) { long expirationTime = nowInMillis + request.getKeepAlive().getMillis(); store.updateExpirationTime(searchId.getDocId(), expirationTime, ActionListener.wrap( p -> getSearchResponseFromTask(searchId, request, nowInMillis, expirationTime, listener), exc -> { //don't even log when: the async search document or its index is not found. That can happen if an invalid //search id is provided and no async search initial response has been stored yet. if (exc.getCause() instanceof DocumentMissingException == false && exc.getCause() instanceof IndexNotFoundException == false) { logger.error(() -> new ParameterizedMessage("failed to update expiration time for async-search [{}]", searchId.getEncoded()), exc); } listener.onFailure(new ResourceNotFoundException(searchId.getEncoded())); } )); } else { getSearchResponseFromTask(searchId, request, nowInMillis, -1, listener); } } else { TransportRequestOptions.Builder builder = TransportRequestOptions.builder(); transportService.sendRequest(node, GetAsyncSearchAction.NAME, request, builder.build(), new ActionListenerResponseHandler<>(listener, AsyncSearchResponse::new, ThreadPool.Names.SAME)); } } catch (Exception exc) { listener.onFailure(exc); } }	i was wondering if we are sure about exc.getcause here. what's the top level exception?
public Builder setType(Type type) { this.type = type; return this; }	nit: it's allowedstackcomponents here in the builder and allowedelasticproductorigins elsewhere, we should probably be consistent.
*/ public long generation() { return generation; } /** * Returns the pending repository generation. {@link RepositoryData} for this generation and all generations down to the safe * generation {@link #generation} may exist in the repository and should not be reused for writing new {@link RepositoryData} to the * repository. * See package level documentation for the blob store based repositories {@link org.elasticsearch.repositories.blobstore}	do we have clean-up for older generations? (not just n-1)
*/ @Override public void applyClusterState(ClusterChangedEvent event) { try { final ClusterState state = event.state(); RepositoriesMetaData oldMetaData = event.previousState().getMetaData().custom(RepositoriesMetaData.TYPE); RepositoriesMetaData newMetaData = state.getMetaData().custom(RepositoriesMetaData.TYPE); // Check if repositories got changed if ((oldMetaData == null && newMetaData == null) || (oldMetaData != null && oldMetaData.equalsIgnoreGenerations(newMetaData))) { for (Repository repo : repositories.values()) { repo.updateState(state); } return; } logger.trace("processing new index repositories for state version [{}]", event.state().version()); Map<String, Repository> survivors = new HashMap<>(); // First, remove repositories that are no longer there for (Map.Entry<String, Repository> entry : repositories.entrySet()) { if (newMetaData == null || newMetaData.repository(entry.getKey()) == null) { logger.debug("unregistering repository [{}]", entry.getKey()); closeRepository(entry.getValue()); } else { survivors.put(entry.getKey(), entry.getValue()); } } Map<String, Repository> builder = new HashMap<>(); if (newMetaData != null) { // Now go through all repositories and update existing or create missing for (RepositoryMetaData repositoryMetaData : newMetaData.repositories()) { Repository repository = survivors.get(repositoryMetaData.name()); if (repository != null) { // Found previous version of this repository RepositoryMetaData previousMetadata = repository.getMetadata(); if (previousMetadata.type().equals(repositoryMetaData.type()) == false || previousMetadata.settings().equals(repositoryMetaData.settings()) == false) { // Previous version is different from the version in settings logger.debug("updating repository [{}]", repositoryMetaData.name()); closeRepository(repository); repository = null; try { repository = createRepository(repositoryMetaData, typesRegistry); } catch (RepositoryException ex) { // TODO: this catch is bogus, it means the old repo is already closed, // but we have nothing to replace it logger.warn(() -> new ParameterizedMessage("failed to change repository [{}]", repositoryMetaData.name()), ex); } } } else { try { repository = createRepository(repositoryMetaData, typesRegistry); } catch (RepositoryException ex) { logger.warn(() -> new ParameterizedMessage("failed to create repository [{}]", repositoryMetaData.name()), ex); } } if (repository != null) { logger.debug("registering repository [{}]", repositoryMetaData.name()); builder.put(repositoryMetaData.name(), repository); } } } for (Repository repo : builder.values()) { repo.updateState(state); } repositories = Collections.unmodifiableMap(builder); } catch (Exception ex) { logger.warn("failure updating cluster state ", ex); } }	are you following this up with a change that looks at the actual repositories instead of event.previousstate()?
@Override public boolean isReadOnly() { return readOnly; } /** * Writing a new index generation is a three step process. * First, the {@link RepositoryMetaData} entry for this repository is set into a pending state by incrementing its * pending generation {@code P} while its safe generation {@code N} remains unchanged. * Second, the updated {@link RepositoryData} is written to generation {@code P + 1}. * Lastly, the {@link RepositoryMetaData} entry for this repository is updated to the new generation {@code P + 1} and thus * pending and safe generation are set to the same value marking the end of the update of the repository data. * * @param repositoryData RepositoryData to write * @param expectedGen expected repository generation at the start of the operation * @param writeShardGens whether to write {@link ShardGenerations} to the new {@link RepositoryData}	could this happen by 2 concurrent requests within the same cluster?
public ClusterState execute(ClusterState currentState) { final RepositoryMetaData meta = getRepoMetaData(currentState); final String repoName = metadata.name(); final long genInState = meta.generation(); // TODO: Remove all usages of this variable, instead initialize the generation when loading RepositoryData final boolean uninitializedMeta = meta.generation() == RepositoryData.UNKNOWN_REPO_GEN; if (uninitializedMeta == false && meta.pendingGeneration() != genInState) { logger.info("Trying to write new repository data over unfinished write, repo is in state [{}]", meta); } assert expectedGen == RepositoryData.EMPTY_REPO_GEN || RepositoryData.UNKNOWN_REPO_GEN == meta.generation() || expectedGen == meta.generation() : "Expected non-empty generation [" + expectedGen + "] does not match generation tracked in [" + meta + "]"; // If we run into the empty repo generation for the expected gen, the repo has been is assumed to have been cleared of // all contents by an external process so we reset the safe generation to the empty generation. final long safeGeneration = expectedGen == RepositoryData.EMPTY_REPO_GEN ? RepositoryData.EMPTY_REPO_GEN : (uninitializedMeta ? expectedGen : genInState); // Regardless of whether or not the safe generation has been reset, the pending generation always increments so that // even if a repository has been manually cleared of all contents we will never reuse the same repository generation. // This is motivated by the consistency behavior the S3 based blob repository implementation has to support which does // not offer any consistency guarantees when it comes to overwriting the same blob name with different content. newGen = uninitializedMeta ? expectedGen + 1: metadata.pendingGeneration() + 1; assert newGen > latestKnownRepoGen.get() : "Attempted new generation [" + newGen + "] must be larger than latest known generation [" + latestKnownRepoGen.get() + "]"; return ClusterState.builder(currentState).metaData(MetaData.builder(currentState.getMetaData()) .putCustom(RepositoriesMetaData.TYPE, currentState.metaData().<RepositoriesMetaData>custom(RepositoriesMetaData.TYPE).withUpdatedGeneration( repoName, safeGeneration, newGen)).build()).build(); }	it's actually important that we wrap the failedtocommitclusterstateexception / nomasterexception exception here, otherwise, this might lead to a coordinator node retry (see transportmasternodeaction).
public ClusterState execute(ClusterState currentState) { final RepositoryMetaData meta = getRepoMetaData(currentState); final String repoName = metadata.name(); final long genInState = meta.generation(); // TODO: Remove all usages of this variable, instead initialize the generation when loading RepositoryData final boolean uninitializedMeta = meta.generation() == RepositoryData.UNKNOWN_REPO_GEN; if (uninitializedMeta == false && meta.pendingGeneration() != genInState) { logger.info("Trying to write new repository data over unfinished write, repo is in state [{}]", meta); } assert expectedGen == RepositoryData.EMPTY_REPO_GEN || RepositoryData.UNKNOWN_REPO_GEN == meta.generation() || expectedGen == meta.generation() : "Expected non-empty generation [" + expectedGen + "] does not match generation tracked in [" + meta + "]"; // If we run into the empty repo generation for the expected gen, the repo has been is assumed to have been cleared of // all contents by an external process so we reset the safe generation to the empty generation. final long safeGeneration = expectedGen == RepositoryData.EMPTY_REPO_GEN ? RepositoryData.EMPTY_REPO_GEN : (uninitializedMeta ? expectedGen : genInState); // Regardless of whether or not the safe generation has been reset, the pending generation always increments so that // even if a repository has been manually cleared of all contents we will never reuse the same repository generation. // This is motivated by the consistency behavior the S3 based blob repository implementation has to support which does // not offer any consistency guarantees when it comes to overwriting the same blob name with different content. newGen = uninitializedMeta ? expectedGen + 1: metadata.pendingGeneration() + 1; assert newGen > latestKnownRepoGen.get() : "Attempted new generation [" + newGen + "] must be larger than latest known generation [" + latestKnownRepoGen.get() + "]"; return ClusterState.builder(currentState).metaData(MetaData.builder(currentState.getMetaData()) .putCustom(RepositoriesMetaData.TYPE, currentState.metaData().<RepositoriesMetaData>custom(RepositoriesMetaData.TYPE).withUpdatedGeneration( repoName, safeGeneration, newGen)).build()).build(); }	this is something that's guaranteed by the cluster state publication layer
Float parse(Object value, boolean coerce) { final Float result; if (value instanceof Number) { result = ((Number) value).floatValue(); } else { if (value instanceof BytesRef) { value = ((BytesRef) value).utf8ToString(); } result = Float.parseFloat(value.toString()); } validateParsed(result); return result; }	let's store it as a float in order to delay boxing as much as possible?
public void testParseOutOfRangeValues() throws IOException { final List<OutOfRangeSpec<Object>> inputs = Arrays.asList( OutOfRangeSpec.of(NumberType.HALF_FLOAT, "65504.1", "[half_float] supports only finite values"), OutOfRangeSpec.of(NumberType.FLOAT, "3.4028235E39", "[float] supports only finite values"), OutOfRangeSpec.of(NumberType.DOUBLE, "1.7976931348623157E309", "[double] supports only finite values"), OutOfRangeSpec.of(NumberType.HALF_FLOAT, 65504.1, "[half_float] supports only finite values"), OutOfRangeSpec.of(NumberType.FLOAT, 3.4028235E39, "[float] supports only finite values"), OutOfRangeSpec.of(NumberType.DOUBLE, new BigDecimal("1.7976931348623157E309"), "[double] supports only finite values"), OutOfRangeSpec.of(NumberType.HALF_FLOAT, Float.NaN, "[half_float] supports only finite values"), OutOfRangeSpec.of(NumberType.FLOAT, Float.NaN, "[float] supports only finite values"), OutOfRangeSpec.of(NumberType.DOUBLE, Double.NaN, "[double] supports only finite values"), OutOfRangeSpec.of(NumberType.HALF_FLOAT, Float.POSITIVE_INFINITY, "[half_float] supports only finite values"), OutOfRangeSpec.of(NumberType.FLOAT, Float.POSITIVE_INFINITY, "[float] supports only finite values"), OutOfRangeSpec.of(NumberType.DOUBLE, Double.POSITIVE_INFINITY, "[double] supports only finite values") ); for (OutOfRangeSpec<Object> item: inputs) { try { item.type.parse(item.value, false); fail("Parsing exception expected for [" + item.type + "] with value [" + item.value + "]"); } catch (IllegalArgumentException e) { assertThat("Incorrect error message for [" + item.type + "] with value [" + item.value + "]", e.getMessage(), containsString(item.message)); } } }	can you test negative values too?
public void testParseOutOfRangeValues() throws IOException { final List<OutOfRangeSpec<Object>> inputs = Arrays.asList( OutOfRangeSpec.of(NumberType.HALF_FLOAT, "65504.1", "[half_float] supports only finite values"), OutOfRangeSpec.of(NumberType.FLOAT, "3.4028235E39", "[float] supports only finite values"), OutOfRangeSpec.of(NumberType.DOUBLE, "1.7976931348623157E309", "[double] supports only finite values"), OutOfRangeSpec.of(NumberType.HALF_FLOAT, 65504.1, "[half_float] supports only finite values"), OutOfRangeSpec.of(NumberType.FLOAT, 3.4028235E39, "[float] supports only finite values"), OutOfRangeSpec.of(NumberType.DOUBLE, new BigDecimal("1.7976931348623157E309"), "[double] supports only finite values"), OutOfRangeSpec.of(NumberType.HALF_FLOAT, Float.NaN, "[half_float] supports only finite values"), OutOfRangeSpec.of(NumberType.FLOAT, Float.NaN, "[float] supports only finite values"), OutOfRangeSpec.of(NumberType.DOUBLE, Double.NaN, "[double] supports only finite values"), OutOfRangeSpec.of(NumberType.HALF_FLOAT, Float.POSITIVE_INFINITY, "[half_float] supports only finite values"), OutOfRangeSpec.of(NumberType.FLOAT, Float.POSITIVE_INFINITY, "[float] supports only finite values"), OutOfRangeSpec.of(NumberType.DOUBLE, Double.POSITIVE_INFINITY, "[double] supports only finite values") ); for (OutOfRangeSpec<Object> item: inputs) { try { item.type.parse(item.value, false); fail("Parsing exception expected for [" + item.type + "] with value [" + item.value + "]"); } catch (IllegalArgumentException e) { assertThat("Incorrect error message for [" + item.type + "] with value [" + item.value + "]", e.getMessage(), containsString(item.message)); } } }	could you use expectthrows instead of try/fail/catch/assert?
private void performStateRecovery(boolean enforceRecoverAfterTime, String reason) { final Gateway.GatewayStateRecoveredListener recoveryListener = new GatewayRecoveryListener(); if (enforceRecoverAfterTime && recoverAfterTime != null) { if (scheduledRecovery.compareAndSet(false, true)) { logger.info("delaying initial state recovery for [{}]. {}", recoverAfterTime, reason); threadPool.schedule(recoverAfterTime, ThreadPool.Names.GENERIC, () -> { if (recovered.compareAndSet(false, true)) { logger.info("recover_after_time [{}] elapsed. performing state recovery...", recoverAfterTime); gateway.performStateRecovery(recoveryListener, Math.max(1, recoverAfterMasterNodes)); } }); } } else { if (recovered.compareAndSet(false, true)) { threadPool.generic().execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { logger.warn("Recovery failed", e); // we reset `recovered` in the listener don't reset it here otherwise there might be a race // that resets it to false while a new recover is already running? recoveryListener.onFailure("state recovery failed: " + e.getMessage()); } @Override protected void doRun() throws Exception { gateway.performStateRecovery(recoveryListener, Math.max(1, recoverAfterMasterNodes)); } }); } } }	higher up in this file we refer to min master nodes explicitly with a vague todo which you missed because it's not done via a code link. i think it's ok to leave the hard coded string for now (as we plan to move this things to another place anyway), but i think it's good to change the comment to explain that you can't use the settings object because it's a module and that there are plans to fix it.
private Set<String> getSupportedTypes() { Set<String> supportedTypes = new TreeSet<>(NUMERICAL_TYPES); if (config.getAnalysis().supportsCategoricalFields()) { supportedTypes.addAll(CATEGORICAL_TYPES); } supportedTypes.add(BooleanFieldMapper.CONTENT_TYPE); return supportedTypes; }	whoops! good catch :d
public static void close(Releasable... releasables) { if (releasables != null) { close(Arrays.asList(releasables)); } } /** Release the provided {@link Releasable}	it's kind of rude to pass a null list here.
private void rebucket() { rebucketCount++; LongKeyedBucketOrds oldOrds = bucketOrds; boolean success = false; try { long[] mergeMap = new long[Math.toIntExact(oldOrds.size())]; bucketOrds = new LongKeyedBucketOrds.FromMany(bigArrays()); success = true; for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) { LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd); Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)]; while (ordsEnum.next()) { long oldKey = ordsEnum.value(); long newKey = preparedRounding.round(oldKey); long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey); mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd; } liveBucketCountUnderestimate = bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1); liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd))); } merge(mergeMap, bucketOrds.size()); } finally { if (success) { oldOrds.close(); } } }	i think the problem here is that we may close oldords twice, right?
protected TopMetricsAggregator createInternal(Aggregator parent, CardinalityUpperBound cardinality, Map<String, Object> metadata) throws IOException { int maxBucketSize = MAX_BUCKET_SIZE.get(context.getIndexSettings().getSettings()); if (size > maxBucketSize) { throw new IllegalArgumentException( "[top_metrics.size] must not be more than [" + maxBucketSize + "] but was [" + size + "]. This limit can be set by changing the [" + MAX_BUCKET_SIZE.getKey() + "] index level setting." ); } MetricValues[] metricValues = new MetricValues[metricFields.size()]; for (int i = 0; i < metricFields.size(); i++) { MultiValuesSourceFieldConfig config = metricFields.get(i); ValuesSourceConfig vsConfig = ValuesSourceConfig.resolve( context, null, config.getFieldName(), config.getScript(), config.getMissing(), config.getTimeZone(), null, CoreValuesSourceType.NUMERIC ); MetricValuesSupplier supplier = context.getValuesSourceRegistry().getAggregator(REGISTRY_KEY, vsConfig); boolean success = false; try { metricValues[i] = supplier.build(size, context.bigArrays(), config.getFieldName(), vsConfig); success = true; } finally { if (success == false) { Releasables.close(metricValues); } } } boolean success = false; try { final TopMetricsAggregator aggregator = new TopMetricsAggregator( name, context, parent, metadata, size, sortBuilders.get(0), metricValues ); success = true; return aggregator; } finally { if (success == false) { Releasables.close(metricValues); } } }	i wonder if it's more clear to do this with one try block.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { return builder .field(TERM_PARSE_FIELD.getPreferredName(), term) .field(LAST_COMMITTED_CONFIGURATION_FIELD.getPreferredName(), lastCommittedConfiguration) .field(LAST_ACCEPTED_CONFIGURATION_FIELD.getPreferredName(), lastAcceptedConfiguration) .field(VOTING_TOMBSTONES_FIELD.getPreferredName(), votingTombstones); }	nit: indenting seems inconsistent here.
private Set<VotingTombstone> randomVotingTombstones() { final int size = randomIntBetween(1, 10); final Set<VotingTombstone> nodes = new HashSet<>(size); while (nodes.size() < size) { assertTrue(nodes.add(new VotingTombstone(randomAlphaOfLength(10), randomAlphaOfLength(10)))); } return nodes; }	we should probably test the empty case too.
public boolean canMatch(ShardSearchRequest request) throws IOException { try (DefaultSearchContext context = createSearchContext(request, defaultSearchTimeout, null)) { SearchSourceBuilder source = context.request().source(); if (source != null) { QueryBuilder queryBuilder = source.query(); AggregatorFactories.Builder aggregations = source.aggregations(); boolean hasGlobalAggs = aggregations != null && aggregations.hasGlobalAggregationBuilder(); if (queryBuilder != null && hasGlobalAggs == false) { // we need to executed hasGlobalAggs is equivalent to match all return queryBuilder instanceof MatchNoneQueryBuilder == false; } } return true; // null query means match_all } }	as far as i can see the only time this will be hit is if the query is a simple range query which does not overlap with the data on the shard as we only check the root query type. this means that if you have a boolean query with a must/filter range clause and other clauses this won't be rewritten to a match none query and therefore will still cause the search request to hit that shard. to me this seems like a fairly common case for search. maybe we should change the rewrite of the boolquerybuilder to rewrite to a match none query if any of the must/filter clauses are match_none to catch these cases too? (i can add this in a separate pr after this is merged)
public Query toQuery(QueryParseContext parseContext) throws IOException, QueryParsingException { if (this.ids.isEmpty()) { return Queries.newMatchNoDocsQuery(); } Collection<String> typesForQuery; if (types == null || types.length == 0) { typesForQuery = parseContext.queryTypes(); } else if (types.length == 1 && MetaData.ALL.equals(types[0])) { typesForQuery = parseContext.mapperService().types(); } else { typesForQuery = Sets.newHashSet(types); } TermsQuery query = new TermsQuery(UidFieldMapper.NAME, Uid.createTypeUids(typesForQuery, ids)); query.setBoost(boost); if (queryName != null) { parseContext.addNamedQuery(queryName, query); } return query; }	there is a problem here when changing ids fro list<string> to set<string> that i ran into when running the tests. the way uid.createtypeuids handles this internally is that it will cast the set to object and then treat that as a one element list when creating the query. not obvious from this line, but you can run searchquerytests#testbasicquerybyid(). i don't know whats better, changing the code in uid.createtypeuids or converting back to list here. i like the internal usage of set for the ids, although i wonder if at some point we might need a fixed ordering of the elements.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final boolean includeTypeName = request.paramAsBoolean(INCLUDE_TYPE_NAME_PARAMETER, DEFAULT_INCLUDE_TYPE_NAME_POLICY); if (request.hasParam(INCLUDE_TYPE_NAME_PARAMETER)) { deprecationLogger.deprecatedAndMaybeLog("index_rollover_with_types", TYPES_DEPRECATION_MESSAGE); } RolloverRequest rolloverIndexRequest = new RolloverRequest(request.param("index"), request.param("new_index")); if (includeTypeName == false) { rolloverIndexRequest.setNoTypeNameForMappingParsing(); } request.applyContentParser(rolloverIndexRequest::fromXContent); rolloverIndexRequest.dryRun(request.paramAsBoolean("dry_run", false)); rolloverIndexRequest.timeout(request.paramAsTime("timeout", rolloverIndexRequest.timeout())); rolloverIndexRequest.masterNodeTimeout(request.paramAsTime("master_timeout", rolloverIndexRequest.masterNodeTimeout())); rolloverIndexRequest.getCreateIndexRequest().waitForActiveShards( ActiveShardCount.parseString(request.param("wait_for_active_shards"))); return channel -> client.admin().indices().rolloverIndex(rolloverIndexRequest, new RestToXContentListener<>(channel)); }	this overall approach looks good to me, but i wonder if instead of setting a special flag on the request here, we can just pass a boolean includetypename to the fromxcontent method? in particular, parse methods take an arbitary context parameter that i think you could use to pass in the boolean. here is an example from fieldcapabilities where we are passing in a string name to the objectparser: https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/action/fieldcaps/fieldcapabilities.java#l136-l138
public void downloadAndExtract(String name) throws IOException { HttpDownloadHelper downloadHelper = new HttpDownloadHelper(); HttpDownloadHelper.DownloadProgress progress; if (outputMode == OutputMode.SILENT) { progress = new HttpDownloadHelper.NullProgress(); } else { progress = new HttpDownloadHelper.VerboseProgress(System.out); } File pluginDir = environment.pluginsFile(); if (!pluginDir.exists()) { FileSystemUtils.mkdirs(pluginDir); } else if (!pluginDir.canWrite()) { System.err.println(); throw new IOException("plugin directory " + pluginDir + " is read only"); } Plugin plugin = new Plugin(name, environment); if (plugin.isInstalled()) { throw new IOException("plugin directory " + plugin.extractedDir().getAbsolutePath() + " already exists. To update the plugin, uninstall it first using -remove " + name + " command"); } // download & unpack the plugin plugin.download(downloadHelper, progress, url, timeout); plugin.unpack(); }	maybe this should be log("")?
public void downloadAndExtract(String name) throws IOException { HttpDownloadHelper downloadHelper = new HttpDownloadHelper(); HttpDownloadHelper.DownloadProgress progress; if (outputMode == OutputMode.SILENT) { progress = new HttpDownloadHelper.NullProgress(); } else { progress = new HttpDownloadHelper.VerboseProgress(System.out); } File pluginDir = environment.pluginsFile(); if (!pluginDir.exists()) { FileSystemUtils.mkdirs(pluginDir); } else if (!pluginDir.canWrite()) { System.err.println(); throw new IOException("plugin directory " + pluginDir + " is read only"); } Plugin plugin = new Plugin(name, environment); if (plugin.isInstalled()) { throw new IOException("plugin directory " + plugin.extractedDir().getAbsolutePath() + " already exists. To update the plugin, uninstall it first using -remove " + name + " command"); } // download & unpack the plugin plugin.download(downloadHelper, progress, url, timeout); plugin.unpack(); }	while it's being messed with, is it perhaps worth adding a -update option that implies this like an upsert (this error would still be relevant)?
void addCollectors(Set<Collector> collectors) { this.collectors.addAll(Objects.requireNonNull(collectors)); scheduleExecution(); }	i don't see the benefit to making the monitoringservice mutable in this way. disabling the scheduler in the same way that setelasticsearchcollectionenabled seems like it's enough and then the collectors can be immutable and never shuffled around. i would change scheduleexecution and dorun to diff + /** + * determine if the monitoring service should schedule the collection of elasticsearch stats. + */ + public boolean shouldscheduleexecution() { + return iselasticsearchcollectionenabled() && ismonitoringactive(); + } void scheduleexecution() { if (scheduler != null) { cancelexecution(); } + if (shouldscheduleexecution()) { - if (ismonitoringactive()) { scheduler = threadpool.schedulewithfixeddelay(monitor, interval, threadpoolname()); } } // ... // ... @override public void dorun() { + if (shouldscheduleexecution() == false) { - if (ismonitoringactive() == false) { // ... + if (shouldscheduleexecution()) { - if (ismonitoringactive()) { exporters.export(results, actionlistener.wrap(r -> semaphore.release(), this::onfailure)); } else { semaphore.release(); } and then drop all of the other changes from this pr not related to setting / getting elasticsearchcollectionenabled. this also keeps the collectors a little simpler by allowing them to run even if this is set (as it currently behaves).
public void reset(int top, int bottom, int negLeft, int negRight, int posLeft, int posRight) { this.top = top; this.bottom = bottom; this.negLeft = negLeft; this.negRight = negRight; this.posLeft = posLeft; this.posRight = posRight; }	i think this assertions are actually important, why did you remove them?
public int getHighestDimension() throws IOException { ShapeType shapeType = getShapeType(); switch (shapeType) { case POINT: case MULTIPOINT: return 1; case LINESTRING: case LINEARRING: case MULTILINESTRING: return 2; case POLYGON: case MULTIPOLYGON: case ENVELOPE: case CIRCLE: return 3; case GEOMETRYCOLLECTION: return input.readVInt(); default: throw new IllegalStateException("unexpected shape-type [" + shapeType + "]"); } }	maybe we do not need an extra byte here, maybe we can have three different geometry collections depending on the highest dimension?
public boolean isSystemName(String name) { return systemNameRunAutomaton.run(name); } /** * Determines whether a given index is a system index by comparing its name to the collection of loaded {@link SystemIndexDescriptor}s * @param index the {@link Index} object to check against loaded {@link SystemIndexDescriptor}s * @return true if the {@link Index}'s name matches a pattern from a {@link SystemIndexDescriptor}	todo: add a little javadoc
private static Automaton buildDataStreamAutomaton(Map<String, Feature> descriptors) { Optional<Automaton> automaton = descriptors.values().stream() .map(feature -> feature.getDataStreamDescriptors().stream() .map(SystemDataStreamDescriptor::getDataStreamName) .map(dsName -> SystemIndexDescriptor.buildAutomaton(dsName, null)) .reduce(Operations::union) .orElse(EMPTY)) .reduce(Operations::union); return automaton.isPresent() ? MinimizationOperations.minimize(automaton.get(), Integer.MAX_VALUE) : EMPTY; }	we don't need a separate inner stream here - we should use flatmap to flatten to a single stream of data stream names as in builddatastreamnamepredicate, then apply .map(dsname -> systemindexdescriptor.buildautomaton(dsname, null)).reduce(operations::union) to that stream. doing it that way is both simpler because we can get rid of the "inner loop", and more efficient because we'll have one terminal operation on the stream.
private boolean isConcreteRestrictedIndex(String indexPattern) { if (Regex.isSimpleMatchPattern(indexPattern) || Automatons.isLuceneRegex(indexPattern)) { return false; } CharacterRunAutomaton runAutomaton = new CharacterRunAutomaton(Arrays.stream(groups).map(g -> g.restrictedNamesAutomaton).findFirst().orElse(Automatons.EMPTY)); return runAutomaton.run(indexPattern); }	we can cut a few lines of code by factoring out a getrestrictednamesautomaton method that handles finding the first automaton in the stream.
public static Builder builder(Automaton restrictedIndices, String... names) { return new Builder(restrictedIndices, names); }	todo: javadoc here (or renamed variable), as it's not very clear in tests what automatons.empty is doing.
@Test public void testMoreLikeAllUniqueValues() throws ExecutionException, InterruptedException { logger.info("Creating the index ..."); assertAcked(prepareCreate("test") .addMapping("type1", "text", "type=string,index=not_analyzed") .setSettings(SETTING_NUMBER_OF_SHARDS, 1)); ensureGreen(); logger.info("Indexing one doc with all unique values ..."); String[] values = new String[randomIntBetween(1, 10)]; for (int i = 0; i < values.length; i++) { values[i] = "tag_" + i; } indexRandom(true, client().prepareIndex("test", "type1", "0").setSource("text", values)); logger.info("Ensuring we still return a result ..."); MoreLikeThisQueryBuilder mltQuery = moreLikeThisQuery("text") .ids("0") .include(true) .minDocFreq(1) .minTermFreq(randomIntBetween(2,10)); SearchResponse response = client().prepareSearch("test").setTypes("type1").setQuery(mltQuery).get(); assertSearchResponse(response); assertHitCount(response, 1); }	could this be a unit test instead of an integration test?
public void testFromJsonDeprecatedSyntax() throws IOException { IdsQueryBuilder tempQuery = createTestQueryBuilder(); assumeTrue("test requires at least one type", tempQuery.types() != null && tempQuery.types().length > 0); String type = tempQuery.types()[0]; IdsQueryBuilder testQuery = new IdsQueryBuilder(type); //single value type can also be called _type String contentString = "{\\\\n" + " \\\\"ids\\\\" : {\\\\n" + " \\\\"_type\\\\" : \\\\"" + type + "\\\\",\\\\n" + " \\\\"values\\\\" : []\\\\n" + " }\\\\n" + "}"; IdsQueryBuilder parsed = (IdsQueryBuilder) parseQuery(contentString, ParseFieldMatcher.EMPTY); assertEquals(testQuery, parsed); try { parseQuery(contentString); fail("parse should have failed"); } catch(IllegalArgumentException e) { assertEquals("Deprecated field [_type] used, expected [type] instead", e.getMessage()); } //array of types can also be called type rather than types contentString = "{\\\\n" + " \\\\"ids\\\\" : {\\\\n" + " \\\\"types\\\\" : [\\\\"" + type + "\\\\"],\\\\n" + " \\\\"values\\\\" : []\\\\n" + " }\\\\n" + "}"; parsed = (IdsQueryBuilder) parseQuery(contentString, ParseFieldMatcher.EMPTY); assertEquals(testQuery, parsed); try { parseQuery(contentString); fail("parse should have failed"); } catch(IllegalArgumentException e) { assertEquals("Deprecated field [types] used, expected [type] instead", e.getMessage()); } }	maybe expectthrows would be easier.
LeafBucketCollector getLeafCollector(Comparable forceLeadSourceValue, LeafReaderContext context, LeafBucketCollector in) throws IOException { int last = arrays.length - 1; LeafBucketCollector collector = in; boolean requiresRehashing = false; while (last > 0) { SingleDimensionValuesSource<?> valuesSource = arrays[last--]; requiresRehashing |= valuesSource.requiresRehashingWhenSwitchingLeafReaders(); collector = valuesSource.getLeafCollector(context, collector); } SingleDimensionValuesSource<?> valuesSource = arrays[last]; requiresRehashing |= valuesSource.requiresRehashingWhenSwitchingLeafReaders(); if (forceLeadSourceValue != null) { collector = valuesSource.getLeafCollector(forceLeadSourceValue, context, collector); } else { collector = valuesSource.getLeafCollector(context, collector); } if (requiresRehashing) { List<Map.Entry<Slot, Integer>> entries = map.entrySet().stream().collect(Collectors.toList()); map.clear(); entries.forEach(e -> map.put(e.getKey(), e.getValue())); } return collector; }	we call this function multiple times per leaf in the sorted case. so the remapping is not always necessary. we should compare the context.ord with the previous one to avoid doing the work when it's not needed.
public DataStream replaceBackingIndex(Index existingBackingIndex, Index newBackingIndex) { List<Index> backingIndices = new ArrayList<>(indices); int backingIndexPosition = backingIndices.indexOf(existingBackingIndex); if (backingIndexPosition == -1) { throw new IllegalArgumentException( String.format(Locale.ROOT, "index [%s] is not part of data stream [%s]", existingBackingIndex.getName(), name) ); } if (indices.size() == (backingIndexPosition + 1)) { throw new IllegalArgumentException( String.format( Locale.ROOT, "cannot replace backing index [%s] of data stream [%s] because it is the write index", existingBackingIndex.getName(), name ) ); } backingIndices.set(backingIndexPosition, newBackingIndex); return new DataStream( name, timeStampField, backingIndices, generation + 1, metadata, hidden, replicated, system, allowCustomRouting ); } /** * Adds the specified index as a backing index and returns a new {@code DataStream} instance with the new combination * of backing indices. * * @param index index to add to the data stream * @return new {@code DataStream} instance with the added backing index * @throws IllegalArgumentException if {@code index}	@martijnvg, when you have a minute, could you review this one? the main change is in this line though there are a couple others that i mentioned in the pr description.
public void testForceMerge() { String[] indices = randomIndicesNames(0, 5); ForceMergeRequest forceMergeRequest = new ForceMergeRequest(indices); Map<String, String> expectedParams = new HashMap<>(); setRandomIndicesOptions(forceMergeRequest::indicesOptions, forceMergeRequest::indicesOptions, expectedParams); if (randomBoolean()) { forceMergeRequest.maxNumSegments(randomInt()); } expectedParams.put("max_num_segments", Integer.toString(forceMergeRequest.maxNumSegments())); if (randomBoolean()) { forceMergeRequest.onlyExpungeDeletes(randomBoolean()); } expectedParams.put("only_expunge_deletes", Boolean.toString(forceMergeRequest.onlyExpungeDeletes())); if (randomBoolean()) { forceMergeRequest.flush(randomBoolean()); } expectedParams.put("flush", Boolean.toString(forceMergeRequest.flush())); Request request = Request.forceMerge(forceMergeRequest); StringJoiner endpoint = new StringJoiner("/", "/", ""); if (indices.length > 0) { endpoint.add(String.join(",", indices)); } endpoint.add("_forcemerge"); assertThat(request.getEndpoint(), equalTo(endpoint.toString())); assertThat(request.getParameters(), equalTo(expectedParams)); assertThat(request.getEntity(), nullValue()); assertThat(request.getMethod(), equalTo(HttpPost.METHOD_NAME)); }	can you add a test for the case when indices are not set or set to null? i found also in the clear cache pr that things break :) (also with refresh and flush)
public void testBulkWithWriteIndexAndRouting() { Map<String, Integer> twoShardsSettings = Collections.singletonMap(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 2); client().admin().indices().prepareCreate("index1") .addAlias(new Alias("alias1").indexRouting("0")).setSettings(twoShardsSettings).get(); client().admin().indices().prepareCreate("index2") .addAlias(new Alias("alias1").indexRouting("0").writeIndex(randomFrom(false, null))) .setSettings(twoShardsSettings).get(); client().admin().indices().prepareCreate("index3") .addAlias(new Alias("alias1").indexRouting("1").writeIndex(true)).setSettings(twoShardsSettings).get(); IndexRequest indexRequest = new IndexRequest("index3", "type", "id"); boolean indexWithRouting = randomBoolean(); if (indexWithRouting) { indexRequest.routing("0"); } indexRequest.source(Collections.singletonMap("foo", "bar")); IndexRequest indexRequestWithAlias = new IndexRequest("alias1", "type", "id"); indexRequestWithAlias.source(Collections.singletonMap("foo", "baz")); BulkResponse bulkResponse = client().prepareBulk() .add(indexRequest).add(indexRequestWithAlias).get(); assertThat(bulkResponse.getItems()[0].getResponse().getIndex(), equalTo("index3")); assertThat(bulkResponse.getItems()[0].getResponse().getShardId().getId(), equalTo(1)); assertThat(bulkResponse.getItems()[0].getResponse().getVersion(), equalTo(1L)); assertThat(bulkResponse.getItems()[0].getResponse().status(), equalTo(RestStatus.CREATED)); assertThat(bulkResponse.getItems()[1].getResponse().getIndex(), equalTo("index3")); assertThat(bulkResponse.getItems()[1].getResponse().getShardId().getId(), equalTo(0)); assertThat(bulkResponse.getItems()[1].getResponse().getVersion(), equalTo(1L)); assertThat(bulkResponse.getItems()[1].getResponse().status(), equalTo(RestStatus.CREATED)); GetRequestBuilder getBuilder = client().prepareGet("index3", "type", "id"); if (indexWithRouting) { getBuilder.setRouting("0"); } assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertThat(client().prepareGet("index3", "type", "id").setRouting("1").get().getSource().get("foo"), equalTo("baz")); client().prepareBulk().add(client().prepareUpdate("alias1", "type", "id").setDoc("foo", "updated")).get(); assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertThat(client().prepareGet("index3", "type", "id").setRouting("1").get().getSource().get("foo"), equalTo("updated")); client().prepareBulk().add(client().prepareDelete("alias1", "type", "id")).get(); assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertFalse(client().prepareGet("index3", "type", "id").setRouting("1").get().isExists()); }	can we use different ids for the different indices? i find this super confusing to reason about. maybe also add the routing value you expect to be used to the id.
public void testBulkWithWriteIndexAndRouting() { Map<String, Integer> twoShardsSettings = Collections.singletonMap(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 2); client().admin().indices().prepareCreate("index1") .addAlias(new Alias("alias1").indexRouting("0")).setSettings(twoShardsSettings).get(); client().admin().indices().prepareCreate("index2") .addAlias(new Alias("alias1").indexRouting("0").writeIndex(randomFrom(false, null))) .setSettings(twoShardsSettings).get(); client().admin().indices().prepareCreate("index3") .addAlias(new Alias("alias1").indexRouting("1").writeIndex(true)).setSettings(twoShardsSettings).get(); IndexRequest indexRequest = new IndexRequest("index3", "type", "id"); boolean indexWithRouting = randomBoolean(); if (indexWithRouting) { indexRequest.routing("0"); } indexRequest.source(Collections.singletonMap("foo", "bar")); IndexRequest indexRequestWithAlias = new IndexRequest("alias1", "type", "id"); indexRequestWithAlias.source(Collections.singletonMap("foo", "baz")); BulkResponse bulkResponse = client().prepareBulk() .add(indexRequest).add(indexRequestWithAlias).get(); assertThat(bulkResponse.getItems()[0].getResponse().getIndex(), equalTo("index3")); assertThat(bulkResponse.getItems()[0].getResponse().getShardId().getId(), equalTo(1)); assertThat(bulkResponse.getItems()[0].getResponse().getVersion(), equalTo(1L)); assertThat(bulkResponse.getItems()[0].getResponse().status(), equalTo(RestStatus.CREATED)); assertThat(bulkResponse.getItems()[1].getResponse().getIndex(), equalTo("index3")); assertThat(bulkResponse.getItems()[1].getResponse().getShardId().getId(), equalTo(0)); assertThat(bulkResponse.getItems()[1].getResponse().getVersion(), equalTo(1L)); assertThat(bulkResponse.getItems()[1].getResponse().status(), equalTo(RestStatus.CREATED)); GetRequestBuilder getBuilder = client().prepareGet("index3", "type", "id"); if (indexWithRouting) { getBuilder.setRouting("0"); } assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertThat(client().prepareGet("index3", "type", "id").setRouting("1").get().getSource().get("foo"), equalTo("baz")); client().prepareBulk().add(client().prepareUpdate("alias1", "type", "id").setDoc("foo", "updated")).get(); assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertThat(client().prepareGet("index3", "type", "id").setRouting("1").get().getSource().get("foo"), equalTo("updated")); client().prepareBulk().add(client().prepareDelete("alias1", "type", "id")).get(); assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertFalse(client().prepareGet("index3", "type", "id").setRouting("1").get().isExists()); }	we should check errors here?
public void testBulkWithWriteIndexAndRouting() { Map<String, Integer> twoShardsSettings = Collections.singletonMap(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 2); client().admin().indices().prepareCreate("index1") .addAlias(new Alias("alias1").indexRouting("0")).setSettings(twoShardsSettings).get(); client().admin().indices().prepareCreate("index2") .addAlias(new Alias("alias1").indexRouting("0").writeIndex(randomFrom(false, null))) .setSettings(twoShardsSettings).get(); client().admin().indices().prepareCreate("index3") .addAlias(new Alias("alias1").indexRouting("1").writeIndex(true)).setSettings(twoShardsSettings).get(); IndexRequest indexRequest = new IndexRequest("index3", "type", "id"); boolean indexWithRouting = randomBoolean(); if (indexWithRouting) { indexRequest.routing("0"); } indexRequest.source(Collections.singletonMap("foo", "bar")); IndexRequest indexRequestWithAlias = new IndexRequest("alias1", "type", "id"); indexRequestWithAlias.source(Collections.singletonMap("foo", "baz")); BulkResponse bulkResponse = client().prepareBulk() .add(indexRequest).add(indexRequestWithAlias).get(); assertThat(bulkResponse.getItems()[0].getResponse().getIndex(), equalTo("index3")); assertThat(bulkResponse.getItems()[0].getResponse().getShardId().getId(), equalTo(1)); assertThat(bulkResponse.getItems()[0].getResponse().getVersion(), equalTo(1L)); assertThat(bulkResponse.getItems()[0].getResponse().status(), equalTo(RestStatus.CREATED)); assertThat(bulkResponse.getItems()[1].getResponse().getIndex(), equalTo("index3")); assertThat(bulkResponse.getItems()[1].getResponse().getShardId().getId(), equalTo(0)); assertThat(bulkResponse.getItems()[1].getResponse().getVersion(), equalTo(1L)); assertThat(bulkResponse.getItems()[1].getResponse().status(), equalTo(RestStatus.CREATED)); GetRequestBuilder getBuilder = client().prepareGet("index3", "type", "id"); if (indexWithRouting) { getBuilder.setRouting("0"); } assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertThat(client().prepareGet("index3", "type", "id").setRouting("1").get().getSource().get("foo"), equalTo("baz")); client().prepareBulk().add(client().prepareUpdate("alias1", "type", "id").setDoc("foo", "updated")).get(); assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertThat(client().prepareGet("index3", "type", "id").setRouting("1").get().getSource().get("foo"), equalTo("updated")); client().prepareBulk().add(client().prepareDelete("alias1", "type", "id")).get(); assertThat(getBuilder.get().getSource().get("foo"), equalTo("bar")); assertFalse(client().prepareGet("index3", "type", "id").setRouting("1").get().isExists()); }	i'm not sure what we're testing here?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); cause = in.readString(); index = in.readString(); settings = readSettingsFromStream(in); int size = in.readVInt(); for (int i = 0; i < size; i++) { final String type = in.readString(); String source = in.readString(); if (in.getVersion().before(Version.V_6_0_0_alpha1)) { // TODO change to 5.3.0 after backport // we do not know the content type that comes from earlier versions so we autodetect and convert source = XContentHelper.convertToJson(new BytesArray(source), false, false, XContentFactory.xContentType(source)); } mappings.put(type, source); } if (in.getVersion().before(Version.V_7_0_0_alpha1)) { // This used to be the size of custom metadata classes int customSize = in.readVInt(); assert customSize == 0 : "unexpected custom metadata when none is supported"; } int aliasesSize = in.readVInt(); for (int i = 0; i < aliasesSize; i++) { aliases.add(Alias.read(in)); } if (in.getVersion().before(Version.V_7_0_0_alpha1)) { in.readBoolean(); // updateAllTypes } waitForActiveShards = ActiveShardCount.readFrom(in); }	hmm do we need to skip the size if we are in production? i mean that assert will not trip if we run without -ea
private IndexMetadata reuseMappings(IndexMetadata indexMetadata) { if (indexMetadata.mapping() == null) { return indexMetadata; } String digest = indexMetadata.mapping().getSha256(); MappingMetadata entry = cache.get(digest); if (entry != null) { IndexMetadata.Builder imBuilder = new IndexMetadata.Builder(indexMetadata); imBuilder.putMapping(entry); return imBuilder.build(); } else { cache.put(digest, indexMetadata.mapping()); return indexMetadata; } }	one more thing, cache (or whatever we might rename it to) should have a little bit of javadoc that roughly explains what's going on here.
private IndexMetadata reuseMappings(IndexMetadata indexMetadata) { if (indexMetadata.mapping() == null) { return indexMetadata; } String digest = indexMetadata.mapping().getSha256(); MappingMetadata entry = cache.get(digest); if (entry != null) { IndexMetadata.Builder imBuilder = new IndexMetadata.Builder(indexMetadata); imBuilder.putMapping(entry); return imBuilder.build(); } else { cache.put(digest, indexMetadata.mapping()); return indexMetadata; } }	this builder is fairly expensive if i remember correctly with all the setting lookup and the like. i wonder if we could just make this a copy constructor that swaps out the mapping metadata only like we did elsewhere and ourselves the trouble of setting up a builder. also, in this case you could make it transparent easily, and not create a new instance even in case the mappings are already instance equal?
private static String sha256(BytesReference data) { MessageDigest messageDigest = MessageDigests.sha256(); try { data.writeTo(new DigestOutputStream(Streams.NULL_OUTPUT_STREAM, messageDigest)); } catch (IOException bogus) { // cannot happen throw new Error(bogus); } return MessageDigests.toHexString(messageDigest.digest()); }	maybe we should base64 encode these instead to save some bytes?
public void testNewContextWithClearedTransients() { ThreadContext threadContext = new ThreadContext(Settings.EMPTY); threadContext.putTransient("foo", "bar"); threadContext.putTransient("bar", "baz"); threadContext.putHeader("foo", "bar"); threadContext.putHeader("baz", "bar"); threadContext.addResponseHeader("foo", "bar"); threadContext.addResponseHeader("bar", "qux"); // this is missing or null if (randomBoolean()) { threadContext.putTransient("acme", null); } // foo is the only existing transient header that is cleared try (ThreadContext.StoredContext stashed = threadContext.newStoredContext(false, randomFrom(List.of("foo", "foo"), List.of("foo"), List.of("foo", "acme")))) { // only the requested transient header is cleared assertNull(threadContext.getTransient("foo")); // missing header is still missing assertNull(threadContext.getTransient("acme")); // other headers are preserved assertEquals("baz", threadContext.getTransient("bar")); assertEquals("bar", threadContext.getHeader("foo")); assertEquals("bar", threadContext.getHeader("baz")); assertEquals("bar", threadContext.getResponseHeaders().get("foo").get(0)); assertEquals("qux", threadContext.getResponseHeaders().get("bar").get(0)); // try override stashed header threadContext.putTransient("foo", "acme"); assertEquals("acme", threadContext.getTransient("foo")); // add new headers threadContext.putTransient("baz", "bar"); threadContext.putHeader("bar", "baz"); threadContext.addResponseHeader("baz", "bar"); threadContext.addResponseHeader("foo", "baz"); } // original is restored (it is not overridden) assertEquals("bar", threadContext.getTransient("foo")); // headers added inside the stash are NOT preserved assertNull(threadContext.getTransient("baz")); assertNull(threadContext.getHeader("bar")); assertNull(threadContext.getResponseHeaders().get("baz")); // original headers are restored assertEquals("bar", threadContext.getHeader("foo")); assertEquals("bar", threadContext.getHeader("baz")); assertEquals("bar", threadContext.getResponseHeaders().get("foo").get(0)); assertEquals(1, threadContext.getResponseHeaders().get("foo").size()); assertEquals("qux", threadContext.getResponseHeaders().get("bar").get(0)); if (randomBoolean()) { threadContext.putTransient("acme", null); } // test stashed missing header stays missing try (ThreadContext.StoredContext stashed = threadContext.newStoredContext(randomBoolean(), randomFrom(Arrays.asList("acme", "acme"), Arrays.asList("acme")))) { assertNull(threadContext.getTransient("acme")); threadContext.putTransient("acme", "foo"); } assertNull(threadContext.getTransient("acme")); // test preserved response headers try (ThreadContext.StoredContext stashed = threadContext.newStoredContext(true, randomFrom(List.of("foo", "foo"), List.of("foo"), List.of("foo", "acme")))) { threadContext.addResponseHeader("baz", "bar"); threadContext.addResponseHeader("foo", "baz"); } assertEquals("bar", threadContext.getResponseHeaders().get("foo").get(0)); assertEquals("baz", threadContext.getResponseHeaders().get("foo").get(1)); assertEquals(2, threadContext.getResponseHeaders().get("foo").size()); assertEquals("bar", threadContext.getResponseHeaders().get("baz").get(0)); assertEquals(1, threadContext.getResponseHeaders().get("baz").size()); }	this is a duplicate of line 70-72.
*/ public void authorize(final Authentication authentication, final String action, final TransportRequest originalRequest, final ActionListener<Void> listener) throws ElasticsearchSecurityException { /* authorization fills in certain transient headers (that must be observed in the listener as well), therefore we * begin by clearing the existing ones up (as they might be already set by the authorization of a previous parent * action (which ran under the same context (on the same node))). * When the returned {@code StoredContext} is closed, ALL the original headers are restored. */ try (ThreadContext.StoredContext ignore = threadContext.newStoredContext(false, AuthorizationServiceField.ALL_AUTHORIZATION_KEYS)) { // prior to doing any authorization lets set the originating action in the context only threadContext.putTransient(AuthorizationServiceField.ORIGINATING_ACTION_KEY, action); String auditId = AuditUtil.extractRequestId(threadContext); if (auditId == null) { // We would like to assert that there is an existing request-id, but if this is a system action, then that might not be // true because the request-id is generated during authentication if (isInternalUser(authentication.getUser()) != false) { auditId = AuditUtil.getOrGenerateRequestId(threadContext); } else { auditTrailService.get().tamperedRequest(null, authentication, action, originalRequest); final String message = "Attempt to authorize action [" + action + "] for [" + authentication.getUser().principal() + "] without an existing request-id"; assert false : message; listener.onFailure(new ElasticsearchSecurityException(message)); } } // sometimes a request might be wrapped within another, which is the case for proxied // requests and concrete shard requests final TransportRequest unwrappedRequest = maybeUnwrapRequest(authentication, originalRequest, action, auditId); if (SystemUser.is(authentication.getUser())) { // this never goes async so no need to wrap the listener authorizeSystemUser(authentication, action, auditId, unwrappedRequest, listener); } else { final String finalAuditId = auditId; final RequestInfo requestInfo = new RequestInfo(authentication, unwrappedRequest, action); final ActionListener<AuthorizationInfo> authzInfoListener = wrapPreservingContext(ActionListener.wrap( authorizationInfo -> { threadContext.putTransient(AUTHORIZATION_INFO_KEY, authorizationInfo); maybeAuthorizeRunAs(requestInfo, finalAuditId, authorizationInfo, listener); }, listener::onFailure), threadContext); getAuthorizationEngine(authentication).resolveAuthorizationInfo(requestInfo, authzInfoListener); } } }	is this the right semantic? i understand this is the reason why restsqlsecurityit needs to be updated. technical details aside, if a parent action invokes a child action, should the "originating action" still be the parent action? the change here makes it to be the child action. if it's always the child action, why does it need to be called "originating" action?
public void testIndexFallBehind() throws Exception { final int numberOfPrimaryShards = randomIntBetween(1, 3); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, between(0, 1), singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("index1").setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderYellow("index1"); final int numDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as first batch", numDocs); for (int i = 0; i < numDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } final PutFollowAction.Request followRequest = putFollow("index1", "index2"); PutFollowAction.Response response = followerClient().execute(PutFollowAction.INSTANCE, followRequest).get(); assertTrue(response.isFollowIndexCreated()); assertTrue(response.isFollowIndexShardsAcked()); assertTrue(response.isIndexFollowingStarted()); final Map<ShardId, Long> firstBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] firstBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : firstBatchShardStats) { if (shardStats.getShardRouting().primary()) { long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; firstBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, firstBatchNumDocsPerShard)); for (int i = 0; i < numDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); for (int i = 0; i < numDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i * 2); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); leaderClient().admin().indices().flush(new FlushRequest("index1").force(true)).actionGet(); } leaderClient().prepareDelete("index1", "doc", "1").get(); leaderClient().admin().indices().refresh(new RefreshRequest("index1")).actionGet(); ForceMergeRequest forceMergeRequest = new ForceMergeRequest("index1"); forceMergeRequest.maxNumSegments(1); leaderClient().admin().indices().forceMerge(forceMergeRequest).actionGet(); followerClient().execute(ResumeFollowAction.INSTANCE, followRequest.getFollowRequest()).get(); assertBusy(() -> { List<ShardFollowNodeTaskStatus> statuses = getFollowTaskStatuses("index2"); Set<ResourceNotFoundException> exceptions = statuses.stream() .map(ShardFollowNodeTaskStatus::getFatalException) .filter(Objects::nonNull) .map(ExceptionsHelper::unwrapCause) .filter(e -> e instanceof ResourceNotFoundException) .map(e -> (ResourceNotFoundException) e) .collect(Collectors.toSet()); assertThat(exceptions.size(), greaterThan(0)); }); pauseFollow("index2"); followerClient().admin().indices().prepareClose("index2").get(); final PutFollowAction.Request followRequest2 = putFollow("index1", "index2"); PutFollowAction.Response response2 = followerClient().execute(PutFollowAction.INSTANCE, followRequest2).get(); assertTrue(response2.isFollowIndexCreated()); assertTrue(response2.isFollowIndexShardsAcked()); assertTrue(response2.isIndexFollowingStarted()); final Map<ShardId, Long> secondBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] secondBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : secondBatchShardStats) { if (shardStats.getShardRouting().primary()) { long indexCount = shardStats.getStats().getIndexing().getTotal().getIndexCount(); long deleteCount = shardStats.getStats().getIndexing().getTotal().getDeleteCount(); final long value = deleteCount + indexCount - 1; secondBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, secondBatchNumDocsPerShard)); for (int i = 2; i < numDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i, i * 2)); } }	nit: maybe use "assertindexfullyreplicatedtofollower"?
public void testIndexFallBehind() throws Exception { final int numberOfPrimaryShards = randomIntBetween(1, 3); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, between(0, 1), singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("index1").setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderYellow("index1"); final int numDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as first batch", numDocs); for (int i = 0; i < numDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } final PutFollowAction.Request followRequest = putFollow("index1", "index2"); PutFollowAction.Response response = followerClient().execute(PutFollowAction.INSTANCE, followRequest).get(); assertTrue(response.isFollowIndexCreated()); assertTrue(response.isFollowIndexShardsAcked()); assertTrue(response.isIndexFollowingStarted()); final Map<ShardId, Long> firstBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] firstBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : firstBatchShardStats) { if (shardStats.getShardRouting().primary()) { long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; firstBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, firstBatchNumDocsPerShard)); for (int i = 0; i < numDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); for (int i = 0; i < numDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i * 2); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); leaderClient().admin().indices().flush(new FlushRequest("index1").force(true)).actionGet(); } leaderClient().prepareDelete("index1", "doc", "1").get(); leaderClient().admin().indices().refresh(new RefreshRequest("index1")).actionGet(); ForceMergeRequest forceMergeRequest = new ForceMergeRequest("index1"); forceMergeRequest.maxNumSegments(1); leaderClient().admin().indices().forceMerge(forceMergeRequest).actionGet(); followerClient().execute(ResumeFollowAction.INSTANCE, followRequest.getFollowRequest()).get(); assertBusy(() -> { List<ShardFollowNodeTaskStatus> statuses = getFollowTaskStatuses("index2"); Set<ResourceNotFoundException> exceptions = statuses.stream() .map(ShardFollowNodeTaskStatus::getFatalException) .filter(Objects::nonNull) .map(ExceptionsHelper::unwrapCause) .filter(e -> e instanceof ResourceNotFoundException) .map(e -> (ResourceNotFoundException) e) .collect(Collectors.toSet()); assertThat(exceptions.size(), greaterThan(0)); }); pauseFollow("index2"); followerClient().admin().indices().prepareClose("index2").get(); final PutFollowAction.Request followRequest2 = putFollow("index1", "index2"); PutFollowAction.Response response2 = followerClient().execute(PutFollowAction.INSTANCE, followRequest2).get(); assertTrue(response2.isFollowIndexCreated()); assertTrue(response2.isFollowIndexShardsAcked()); assertTrue(response2.isIndexFollowingStarted()); final Map<ShardId, Long> secondBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] secondBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : secondBatchShardStats) { if (shardStats.getShardRouting().primary()) { long indexCount = shardStats.getStats().getIndexing().getTotal().getIndexCount(); long deleteCount = shardStats.getStats().getIndexing().getTotal().getDeleteCount(); final long value = deleteCount + indexCount - 1; secondBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, secondBatchNumDocsPerShard)); for (int i = 2; i < numDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i, i * 2)); } }	nit: one flush before the force_merge is good enough :).
public void testIndexFallBehind() throws Exception { final int numberOfPrimaryShards = randomIntBetween(1, 3); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, between(0, 1), singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("index1").setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderYellow("index1"); final int numDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as first batch", numDocs); for (int i = 0; i < numDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } final PutFollowAction.Request followRequest = putFollow("index1", "index2"); PutFollowAction.Response response = followerClient().execute(PutFollowAction.INSTANCE, followRequest).get(); assertTrue(response.isFollowIndexCreated()); assertTrue(response.isFollowIndexShardsAcked()); assertTrue(response.isIndexFollowingStarted()); final Map<ShardId, Long> firstBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] firstBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : firstBatchShardStats) { if (shardStats.getShardRouting().primary()) { long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; firstBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, firstBatchNumDocsPerShard)); for (int i = 0; i < numDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); for (int i = 0; i < numDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i * 2); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); leaderClient().admin().indices().flush(new FlushRequest("index1").force(true)).actionGet(); } leaderClient().prepareDelete("index1", "doc", "1").get(); leaderClient().admin().indices().refresh(new RefreshRequest("index1")).actionGet(); ForceMergeRequest forceMergeRequest = new ForceMergeRequest("index1"); forceMergeRequest.maxNumSegments(1); leaderClient().admin().indices().forceMerge(forceMergeRequest).actionGet(); followerClient().execute(ResumeFollowAction.INSTANCE, followRequest.getFollowRequest()).get(); assertBusy(() -> { List<ShardFollowNodeTaskStatus> statuses = getFollowTaskStatuses("index2"); Set<ResourceNotFoundException> exceptions = statuses.stream() .map(ShardFollowNodeTaskStatus::getFatalException) .filter(Objects::nonNull) .map(ExceptionsHelper::unwrapCause) .filter(e -> e instanceof ResourceNotFoundException) .map(e -> (ResourceNotFoundException) e) .collect(Collectors.toSet()); assertThat(exceptions.size(), greaterThan(0)); }); pauseFollow("index2"); followerClient().admin().indices().prepareClose("index2").get(); final PutFollowAction.Request followRequest2 = putFollow("index1", "index2"); PutFollowAction.Response response2 = followerClient().execute(PutFollowAction.INSTANCE, followRequest2).get(); assertTrue(response2.isFollowIndexCreated()); assertTrue(response2.isFollowIndexShardsAcked()); assertTrue(response2.isIndexFollowingStarted()); final Map<ShardId, Long> secondBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] secondBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : secondBatchShardStats) { if (shardStats.getShardRouting().primary()) { long indexCount = shardStats.getStats().getIndexing().getTotal().getIndexCount(); long deleteCount = shardStats.getStats().getIndexing().getTotal().getDeleteCount(); final long value = deleteCount + indexCount - 1; secondBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, secondBatchNumDocsPerShard)); for (int i = 2; i < numDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i, i * 2)); } }	maybe also exclude exception that do not have es.requested_operations_missing as metadata? then we are even more sure that the exception is the right one.
public void testIndexFallBehind() throws Exception { final int numberOfPrimaryShards = randomIntBetween(1, 3); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, between(0, 1), singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("index1").setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderYellow("index1"); final int numDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as first batch", numDocs); for (int i = 0; i < numDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } final PutFollowAction.Request followRequest = putFollow("index1", "index2"); PutFollowAction.Response response = followerClient().execute(PutFollowAction.INSTANCE, followRequest).get(); assertTrue(response.isFollowIndexCreated()); assertTrue(response.isFollowIndexShardsAcked()); assertTrue(response.isIndexFollowingStarted()); final Map<ShardId, Long> firstBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] firstBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : firstBatchShardStats) { if (shardStats.getShardRouting().primary()) { long value = shardStats.getStats().getIndexing().getTotal().getIndexCount() - 1; firstBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, firstBatchNumDocsPerShard)); for (int i = 0; i < numDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } pauseFollow("index2"); for (int i = 0; i < numDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i * 2); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); leaderClient().admin().indices().flush(new FlushRequest("index1").force(true)).actionGet(); } leaderClient().prepareDelete("index1", "doc", "1").get(); leaderClient().admin().indices().refresh(new RefreshRequest("index1")).actionGet(); ForceMergeRequest forceMergeRequest = new ForceMergeRequest("index1"); forceMergeRequest.maxNumSegments(1); leaderClient().admin().indices().forceMerge(forceMergeRequest).actionGet(); followerClient().execute(ResumeFollowAction.INSTANCE, followRequest.getFollowRequest()).get(); assertBusy(() -> { List<ShardFollowNodeTaskStatus> statuses = getFollowTaskStatuses("index2"); Set<ResourceNotFoundException> exceptions = statuses.stream() .map(ShardFollowNodeTaskStatus::getFatalException) .filter(Objects::nonNull) .map(ExceptionsHelper::unwrapCause) .filter(e -> e instanceof ResourceNotFoundException) .map(e -> (ResourceNotFoundException) e) .collect(Collectors.toSet()); assertThat(exceptions.size(), greaterThan(0)); }); pauseFollow("index2"); followerClient().admin().indices().prepareClose("index2").get(); final PutFollowAction.Request followRequest2 = putFollow("index1", "index2"); PutFollowAction.Response response2 = followerClient().execute(PutFollowAction.INSTANCE, followRequest2).get(); assertTrue(response2.isFollowIndexCreated()); assertTrue(response2.isFollowIndexShardsAcked()); assertTrue(response2.isIndexFollowingStarted()); final Map<ShardId, Long> secondBatchNumDocsPerShard = new HashMap<>(); final ShardStats[] secondBatchShardStats = leaderClient().admin().indices().prepareStats("index1").get().getIndex("index1").getShards(); for (final ShardStats shardStats : secondBatchShardStats) { if (shardStats.getShardRouting().primary()) { long indexCount = shardStats.getStats().getIndexing().getTotal().getIndexCount(); long deleteCount = shardStats.getStats().getIndexing().getTotal().getDeleteCount(); final long value = deleteCount + indexCount - 1; secondBatchNumDocsPerShard.put(shardStats.getShardRouting().shardId(), value); } } assertBusy(assertTask(numberOfPrimaryShards, secondBatchNumDocsPerShard)); for (int i = 2; i < numDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i, i * 2)); } }	nit: maybe use "assertindexfullyreplicatedtofollower"?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(index); out.writeInt(routingNumShards); out.writeLong(version); out.writeVLong(mappingVersion); out.writeVLong(settingsVersion); if (out.getVersion().onOrAfter(Version.V_7_2_0)) { out.writeVLong(aliasesVersion); } out.writeByte(state.id); Settings.writeSettingsToStream(settings, out); out.writeVLongArray(primaryTerms); mappings.writeTo(out); aliases.writeTo(out); customData.writeTo(out); inSyncAllocationIds.writeTo(out); rolloverInfos.writeTo(out); if (out.getVersion().onOrAfter(SYSTEM_INDEX_FLAG_ADDED)) { out.writeBoolean(isSystem); } timestampRange.writeTo(out); }	let us rename the loop variable to mapping here.
private static ClusterState executeIndicesChangesTest(final ClusterState previousState, final TombstoneDeletionQuantity deletionQuantity) { final int numAdd = randomIntBetween(0, 5); // add random # of indices to the next cluster state final List<Index> stateIndices = new ArrayList<>(); for (IndexMetadata value : previousState.metadata().indices().values()) { stateIndices.add(value.getIndex()); } final int numDel; switch (deletionQuantity) { case DELETE_ALL: { numDel = stateIndices.size(); break; } case DELETE_NONE: { numDel = 0; break; } case DELETE_RANDOM: { numDel = randomIntBetween(0, Math.max(stateIndices.size() - 1, 0)); break; } default: throw new AssertionError("Unhandled mode [" + deletionQuantity + "]"); } final boolean changeClusterUUID = randomBoolean(); final List<Index> addedIndices = addIndices(numAdd, randomAlphaOfLengthBetween(5, 10)); List<Index> delIndices; if (changeClusterUUID) { delIndices = new ArrayList<>(); } else { delIndices = delIndices(numDel, stateIndices); } final ClusterState newState = nextState(previousState, changeClusterUUID, addedIndices, delIndices, 0); ClusterChangedEvent event = new ClusterChangedEvent("_na_", newState, previousState); final List<String> addsFromEvent = event.indicesCreated(); List<Index> delsFromEvent = event.indicesDeleted(); assertThat(new HashSet<>(addsFromEvent), equalTo(addedIndices.stream().map(Index::getName).collect(Collectors.toSet()))); assertThat(new HashSet<>(delsFromEvent), equalTo(new HashSet<>(delIndices))); assertThat(event.metadataChanged(), equalTo(changeClusterUUID || addedIndices.size() > 0 || delIndices.size() > 0)); final IndexGraveyard newGraveyard = event.state().metadata().indexGraveyard(); final IndexGraveyard oldGraveyard = event.previousState().metadata().indexGraveyard(); assertThat(((IndexGraveyard.IndexGraveyardDiff)newGraveyard.diff(oldGraveyard)).getAdded().size(), equalTo(delIndices.size())); return newState; }	rename value to indexmetadata?
void closeStreams() { interrupt(); try { if (os != null) os.close(); } catch (IOException e) { // ignore } try { if (is != null) is.close(); } catch (IOException e) { // ignore } if (!success && dest != null && dest.exists()) { dest.delete(); } }	you can do ioutils.closewhilehandlingexception to close these streams only if they are not null and silently
public boolean download(URL source, File dest, @Nullable DownloadProgress progress, TimeValue timeout) throws IOException { if (dest.exists() && skipExisting) { return true; } //don't do any progress, unless asked if (progress == null) { progress = new NullProgress(); } //set the timestamp to the file date. long timestamp = 0; boolean hasTimestamp = false; if (useTimestamp && dest.exists()) { timestamp = dest.lastModified(); hasTimestamp = true; } GetThread getThread = new GetThread(source, dest, hasTimestamp, timestamp, progress); getThread.setDaemon(true); getThread.start(); try { getThread.join(timeout.millis()); } catch (InterruptedException ie) { // ignore } if (getThread.isAlive()) { String msg = "The GET operation took longer than " + timeout + ", stopping it."; getThread.closeStreams(); throw new ElasticSearchTimeoutException(msg); } return getThread.wasSuccessful(); }	not related to your commit but i think we should return false immediately in case of an interruptedexception.
private void postIndexAsyncActions(String[] indices, List<CountDownLatch> inFlightAsyncOperations, boolean maybeFlush) throws InterruptedException { if (rarely()) { if (rarely()) { client().admin().indices().prepareRefresh(indices).setIndicesOptions(IndicesOptions.lenientExpandOpen()).execute( new LatchedActionListener<RefreshResponse>(newLatch(inFlightAsyncOperations))); } else if (maybeFlush && rarely()) { if (randomBoolean()) { client().admin().indices().prepareFlush(indices).setIndicesOptions(IndicesOptions.lenientExpandOpen()).execute( new LatchedActionListener<FlushResponse>(newLatch(inFlightAsyncOperations))); } else { InternalTestCluster internalTestCluster = null; if (!isInternalCluster()) { // bwc test if (cluster() instanceof CompositeTestCluster) { CompositeTestCluster compositeTestCluster = (CompositeTestCluster) cluster(); if (compositeTestCluster.numNewDataNodes() > 0) { internalTestCluster = compositeTestCluster.internalCluster(); } } } else { internalTestCluster = internalCluster(); } if (internalTestCluster != null) { internalTestCluster.getInstance(SyncedFlushService.class).attemptSyncedFlush(indices, IndicesOptions.lenientExpandOpen(), new LatchedActionListener<IndicesSyncedFlushResult>(newLatch(inFlightAsyncOperations))); } } } else if (rarely()) { client().admin().indices().prepareOptimize(indices).setIndicesOptions(IndicesOptions.lenientExpandOpen()).setMaxNumSegments(between(1, 10)).setFlush(maybeFlush && randomBoolean()).execute( new LatchedActionListener<OptimizeResponse>(newLatch(inFlightAsyncOperations))); } } while (inFlightAsyncOperations.size() > MAX_IN_FLIGHT_ASYNC_INDEXES) { int waitFor = between(0, inFlightAsyncOperations.size() - 1); inFlightAsyncOperations.remove(waitFor).await(); } } /** * The scope of a test cluster used together with * {@link org.elasticsearch.test.ElasticsearchIntegrationTest.ClusterScope} annotations on {@link org.elasticsearch.test.ElasticsearchIntegrationTest}	can we add a comment as to why this is needed here? i'm not sure it's trivial to figure out.
@Override public InternalWeightedAvg doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) { CompensatedSum sumCompensation = CompensatedSum.newZeroInstance(); CompensatedSum weightCompensation = CompensatedSum.newZeroInstance(); // Compute the sum of double values with Kahan summation algorithm which is more // accurate than naive summation. for (InternalAggregation aggregation : aggregations) { InternalWeightedAvg avg = (InternalWeightedAvg) aggregation; weightCompensation = weightCompensation.add(avg.weight); sumCompensation = sumCompensation.add(avg.sum); } return new InternalWeightedAvg(getName(), sumCompensation.value(), weightCompensation.value(), format, pipelineAggregators(), getMetaData()); }	i think this inline comment is worth preserving; let's just copy it to the same clause in the new class.
public void testIllegalArguments() { expectThrows(IllegalArgumentException.class, () -> new RandomScoreFunctionBuilder().seed(null)); expectThrows(IllegalArgumentException.class, () -> new ScriptScoreFunctionBuilder((Script) null)); expectThrows(IllegalArgumentException.class, () -> new FieldValueFactorFunctionBuilder((String) null)); expectThrows(IllegalArgumentException.class, () -> new FieldValueFactorFunctionBuilder("").modifier(null)); expectThrows(IllegalArgumentException.class, () -> new GaussDecayFunctionBuilder(null, "", "", "")); expectThrows(IllegalArgumentException.class, () -> new GaussDecayFunctionBuilder("", "", null, "")); expectThrows(IllegalArgumentException.class, () -> new GaussDecayFunctionBuilder("", "", null, "", randomDouble())); expectThrows(IllegalArgumentException.class, () -> new LinearDecayFunctionBuilder(null, "", "", "")); expectThrows(IllegalArgumentException.class, () -> new LinearDecayFunctionBuilder("", "", null, "")); expectThrows(IllegalArgumentException.class, () -> new LinearDecayFunctionBuilder("", "", null, "", randomDouble())); expectThrows(IllegalArgumentException.class, () -> new ExponentialDecayFunctionBuilder(null, "", "", "")); expectThrows(IllegalArgumentException.class, () -> new ExponentialDecayFunctionBuilder("", "", null, "")); expectThrows(IllegalArgumentException.class, () -> new ExponentialDecayFunctionBuilder("", "", null, "", randomDouble())); }	this test suggests its testing without a seed but then sets one here?
public TermSuggestion innerExecute(String name, TermSuggestionContext suggestion, IndexSearcher searcher, CharsRefBuilder spare) throws IOException { DirectSpellChecker directSpellChecker = SuggestUtils.getDirectSpellChecker(suggestion.getDirectSpellCheckerSettings()); final IndexReader indexReader = searcher.getIndexReader(); TermSuggestion response = new TermSuggestion( name, suggestion.getSize(), suggestion.getDirectSpellCheckerSettings().sort() ); List<Token> tokens = queryTerms(suggestion, spare); for (Token token : tokens) { // TODO: Extend DirectSpellChecker in 4.1, to get the raw suggested words as BytesRef SuggestWord[] suggestedWords = directSpellChecker.suggestSimilar( token.term, suggestion.getShardSize(), indexReader, suggestion.getDirectSpellCheckerSettings().suggestMode() ); Text key = new BytesText(new BytesArray(token.term.bytes())); TermSuggestion.Entry resultEntry = new TermSuggestion.Entry(key, token.startOffset, token.endOffset - token.startOffset); if (suggestion.getDirectSpellCheckerSettings().exactMatch()){ final TermsEnum termsEnum = MultiFields.getTerms(indexReader, token.term.field()).iterator(); if (termsEnum.seekExact(token.term.bytes())) { Text word = new StringText(token.term.text()); resultEntry.addOption(new TermSuggestion.Entry.Option(word, termsEnum.docFreq(), 1f)); } } for (SuggestWord suggestWord : suggestedWords) { Text word = new StringText(suggestWord.string); resultEntry.addOption(new TermSuggestion.Entry.Option(word, suggestWord.freq, suggestWord.score)); } response.addTerm(resultEntry); } return response; }	indentation is off here
private List<Tuple<PluginInfo,Plugin>> loadBundles(Set<Bundle> bundles) { List<Tuple<PluginInfo, Plugin>> plugins = new ArrayList<>(); Map<String, Plugin> loaded = new HashMap<>(); Map<String, Set<URL>> transitiveUrls = new HashMap<>(); List<Bundle> sortedBundles = sortBundles(bundles); for (Bundle bundle : sortedBundles) { checkBundleJarHell(JarHell.parseClassPath(), bundle, transitiveUrls); final Plugin plugin = loadBundle(bundle, loaded); plugins.add(new Tuple<>(bundle.plugin, plugin)); } loadExtensions(plugins); return Collections.unmodifiableList(plugins); }	we have a utility method for this instanceof pattern, filterplugins. i think it would also be clearer to use a normal for loop rather than foreach?
private List<Tuple<PluginInfo,Plugin>> loadBundles(Set<Bundle> bundles) { List<Tuple<PluginInfo, Plugin>> plugins = new ArrayList<>(); Map<String, Plugin> loaded = new HashMap<>(); Map<String, Set<URL>> transitiveUrls = new HashMap<>(); List<Bundle> sortedBundles = sortBundles(bundles); for (Bundle bundle : sortedBundles) { checkBundleJarHell(JarHell.parseClassPath(), bundle, transitiveUrls); final Plugin plugin = loadBundle(bundle, loaded); plugins.add(new Tuple<>(bundle.plugin, plugin)); } loadExtensions(plugins); return Collections.unmodifiableList(plugins); }	this could just be created in loadextensions, no need to create a new one for each individual plugin? afaict it always operates on the same stream of plugins passed in.
public void testExtensiblePlugin() { TestExtensiblePlugin extensiblePlugin = new TestExtensiblePlugin(); PluginsService.loadExtensions(List.of( Tuple.tuple(new PluginInfo("extensible", null, null, null, null, null, List.of(), false), extensiblePlugin) )); assertThat(extensiblePlugin.extensions, notNullValue()); assertThat(extensiblePlugin.extensions, hasSize(0)); extensiblePlugin = new TestExtensiblePlugin(); TestPlugin testPlugin = new TestPlugin(); PluginsService.loadExtensions(List.of( Tuple.tuple(new PluginInfo("extensible", null, null, null, null, null, List.of(), false), extensiblePlugin), Tuple.tuple(new PluginInfo("test", null, null, null, null, null, List.of("extensible"), false), testPlugin) )); assertThat(extensiblePlugin.extensions, notNullValue()); assertThat(extensiblePlugin.extensions, hasSize(2)); assertThat(extensiblePlugin.extensions.get(0), instanceOf(TestExtension1.class)); assertThat(extensiblePlugin.extensions.get(1), instanceOf(TestExtension2.class)); assertThat(((TestExtension2) extensiblePlugin.extensions.get(1)).plugin, sameInstance(testPlugin)); }	can we have some tests for the error cases? for example, no valid ctor exists, or we fail when calling the ctor?
public static URI appendSegmentToPath(URI uri, String segment) { if (segment == null || segment.isEmpty() || "/".equals(segment)) { return uri; } String path = uri.getPath(); String concatenatedPath = ""; String cleanSegment = segment.startsWith("/") ? segment.substring(1) : segment; if (path == null || path.isEmpty()) { path = "/"; } if (path.charAt(path.length() - 1) == '/') { concatenatedPath = path + cleanSegment; } else { concatenatedPath = path + "/" + cleanSegment; } try { return new URI(uri.getScheme(), uri.getUserInfo(), uri.getHost(), uri.getPort(), concatenatedPath, uri.getQuery(), uri.getFragment()); } catch (URISyntaxException e) { throw new IllegalArgumentException("Invalid segment [" + segment + "] for URI [" + uri + "]: " + e.getMessage(), e); } }	why not using new uri(concatenatedpath) ?
@Override public void run() { synchronized (danglingMutex) { DanglingIndex remove = danglingIndices.remove(metaData.index()); // no longer there... if (remove == null) { return; } logger.warn("[{}] deleting dangling index", metaData.index()); try { indicesService.deleteIndexStore("deleting dangling index", metaData, clusterService.state()); } catch (Exception ex) { logger.debug("failed to delete dangling index", ex); } } } } static class DanglingIndex { public final String index; public final ScheduledFuture future; DanglingIndex(String index, ScheduledFuture future) { this.index = index; this.future = future; }	if we pass the state imo we should document what state it is? i also wonder if we should get the metadata from the state direclty instead of passing it. and then just pass the index name?
@Override public BulkShardRequest routedBasedOnClusterVersion(long routedBasedOnClusterVersion) { return super.routedBasedOnClusterVersion(routedBasedOnClusterVersion); }	nit: might be worth adding some comment like //public for testing or so :)
@Override protected void doRun() { setPhase(task, "routing"); final ClusterState state = observer.setAndGetObservedState(); final ClusterBlockException blockException = blockExceptions(state, request.shardId().getIndexName()); if (blockException != null) { if (blockException.retryable()) { logger.trace("cluster is blocked, scheduling a retry", blockException); retry(blockException); } else { finishAsFailed(blockException); } } else { final IndexMetaData indexMetaData = state.metaData().index(request.shardId().getIndex()); if (indexMetaData == null) { // ensure that the cluster state on the node is at least as high as the node that decided that the index was there if (state.version() < request.routedBasedOnClusterVersion()) { logger.trace("failed to find index [{}] for request [{}] despite sender thinking it would be here. " + "Local cluster state version [{}]] is older than on sending node (version [{}]), scheduling a retry...", request.shardId().getIndex(), request, state.version(), request.routedBasedOnClusterVersion()); retry(new IndexNotFoundException("failed to find index as current cluster state with version [" + state.version() + "] is stale (expected at least [" + request.routedBasedOnClusterVersion() + "]", request.shardId().getIndexName())); return; } else { finishAsFailed(new IndexNotFoundException(request.shardId().getIndex())); return; } } if (indexMetaData.getState() == IndexMetaData.State.CLOSE) { finishAsFailed(new IndexClosedException(indexMetaData.getIndex())); return; } if (request.waitForActiveShards() == ActiveShardCount.DEFAULT) { // if the wait for active shard count has not been set in the request, // resolve it from the index settings request.waitForActiveShards(indexMetaData.getWaitForActiveShards()); } assert request.waitForActiveShards() != ActiveShardCount.DEFAULT : "request waitForActiveShards must be set in resolveRequest"; final ShardRouting primary = primary(state); if (primary == null || primary.active() == false) { logger.trace("primary shard [{}] is not yet active, scheduling a retry: action [{}], request [{}], " + "cluster state version [{}]", request.shardId(), actionName, request, state.version()); retryBecauseUnavailable(request.shardId(), "primary shard is not active"); return; } if (state.nodes().nodeExists(primary.currentNodeId()) == false) { logger.trace("primary shard [{}] is assigned to an unknown node [{}], scheduling a retry: action [{}], request [{}], " + "cluster state version [{}]", request.shardId(), primary.currentNodeId(), actionName, request, state.version()); retryBecauseUnavailable(request.shardId(), "primary shard isn't assigned to a known node."); return; } final DiscoveryNode node = state.nodes().get(primary.currentNodeId()); if (primary.currentNodeId().equals(state.nodes().getLocalNodeId())) { performLocalAction(state, primary, node, indexMetaData); } else { performRemoteAction(state, primary, node); } } }	since you inlined it here you can delete delete retryifunavailable (or revert the inlining ... though i like it better in-line i think :)).
public void testDeleteIndexWhileIndexing() throws Exception { String index = "deleted_while_indexing"; createIndex(index); AtomicBoolean stopped = new AtomicBoolean(); Thread[] threads = new Thread[between(1, 4)]; AtomicInteger docID = new AtomicInteger(); for (int i = 0; i < threads.length; i++) { threads[i] = new Thread(() -> { while (stopped.get() == false && docID.get() < 5000) { String id = Integer.toString(docID.incrementAndGet()); try { IndexResponse response = client().prepareIndex(index).setId(id) .setSource(Map.of("f" + randomIntBetween(1, 10), randomNonNegativeLong()), XContentType.JSON).get(); assertThat(response.getResult(), isOneOf(CREATED, UPDATED)); logger.info("--> index id={} seq_no={}", response.getId(), response.getSeqNo()); } catch (ElasticsearchException ignore) { logger.info("--> fail to index id={}", id); } } }); threads[i].start(); } ensureGreen(index); assertBusy(() -> assertThat(docID.get(), greaterThanOrEqualTo(1))); assertAcked(client().admin().indices().prepareDelete(index)); stopped.set(true); for (Thread thread : threads) { thread.join(10000); // only wait 10 seconds here if (thread.isAlive()) { throw new IllegalStateException("indexing should have terminated"); } } }	maybe just make it 30s ... 10s for this kind of thing alway seems like it could randomly fail, maybe not as likely anymore but why take the risk? also, maybe make it relative to org.elasticsearch.action.support.replication.replicationrequest#default_timeout to make the intent of the test clearer clearer?
public void testDeleteIndexWhileIndexing() throws Exception { String index = "deleted_while_indexing"; createIndex(index); AtomicBoolean stopped = new AtomicBoolean(); Thread[] threads = new Thread[between(1, 4)]; AtomicInteger docID = new AtomicInteger(); for (int i = 0; i < threads.length; i++) { threads[i] = new Thread(() -> { while (stopped.get() == false && docID.get() < 5000) { String id = Integer.toString(docID.incrementAndGet()); try { IndexResponse response = client().prepareIndex(index).setId(id) .setSource(Map.of("f" + randomIntBetween(1, 10), randomNonNegativeLong()), XContentType.JSON).get(); assertThat(response.getResult(), isOneOf(CREATED, UPDATED)); logger.info("--> index id={} seq_no={}", response.getId(), response.getSeqNo()); } catch (ElasticsearchException ignore) { logger.info("--> fail to index id={}", id); } } }); threads[i].start(); } ensureGreen(index); assertBusy(() -> assertThat(docID.get(), greaterThanOrEqualTo(1))); assertAcked(client().admin().indices().prepareDelete(index)); stopped.set(true); for (Thread thread : threads) { thread.join(10000); // only wait 10 seconds here if (thread.isAlive()) { throw new IllegalStateException("indexing should have terminated"); } } }	nit: maybe use assertfalse?
private void trimUnsafeCommits() throws IOException { assert currentEngineReference.get() == null || currentEngineReference.get() instanceof ReadOnlyEngine : "a write engine is running"; final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); final long globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); final long minRetainedTranslogGen = Translog.readMinTranslogGeneration(translogConfig.getTranslogPath(), translogUUID); assertMaxUnsafeAutoIdInCommit(); store.trimUnsafeCommits(globalCheckpoint, minRetainedTranslogGen, indexSettings.getIndexVersionCreated()); }	you've moved this out-side of the mutex. is that safe to do?
void resetEngineToGlobalCheckpoint() throws IOException { assert getActiveOperationsCount() == 0 : "Ongoing writes [" + getActiveOperations() + "]"; sync(); // persist the global checkpoint to disk Engine readOnlyEngine = null; try { final SeqNoStats seqNoStats = seqNoStats(); final TranslogStats translogStats = translogStats(); // flush to make sure the latest commit, which will be opened by the read-only engine, includes all operations. flush(new FlushRequest()); readOnlyEngine = new ReadOnlyEngine(newEngineConfig(), seqNoStats, translogStats, false, Function.identity()); synchronized (mutex) { verifyNotClosed(); IOUtils.close(currentEngineReference.getAndSet(readOnlyEngine)); readOnlyEngine = null; } } finally { IOUtils.close(readOnlyEngine); } Engine newEngine = null; try { final long globalCheckpoint = getGlobalCheckpoint(); trimUnsafeCommits(); newEngine = engineFactory.newReadWriteEngine(newEngineConfig()); onNewEngine(newEngine); newEngine.onSettingsChanged(); newEngine.advanceMaxSeqNoOfUpdatesOrDeletes(globalCheckpoint); final Engine.TranslogRecoveryRunner translogRunner = (engine, snapshot) -> runTranslogRecovery( engine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { // TODO: add a dedicate recovery stats for the reset translog }); newEngine.recoverFromTranslog(translogRunner, globalCheckpoint); synchronized (mutex) { verifyNotClosed(); IOUtils.close(currentEngineReference.getAndSet(newEngine)); active.set(true); newEngine = null; } } finally { IOUtils.close(newEngine); } } /** * Returns the maximum sequence number of either update or delete operations have been processed in this shard * or the sequence number from {@link #advanceMaxSeqNoOfUpdatesOrDeletes(long)}	isn't this supposed to be called after the new engine has been set on currentenginereference?
protected String parseSourceValue(Object value) { return value.toString(); }	keywords shouldn't also consider ignore_above and the extraction from _source to happen with this value in mind?
public void testMeta() throws Exception { IndexService indexService = createIndex("test"); String mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("_doc") .startObject("properties").startObject("field").field("type", "constant_keyword") .field("meta", Collections.singletonMap("foo", "bar")) .endObject().endObject().endObject().endObject()); DocumentMapper mapper = indexService.mapperService().merge("_doc", new CompressedXContent(mapping), MergeReason.MAPPING_UPDATE); assertEquals(mapping, mapper.mappingSource().toString()); String mapping2 = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("_doc") .startObject("properties").startObject("field").field("type", "constant_keyword") .endObject().endObject().endObject().endObject()); mapper = indexService.mapperService().merge("_doc", new CompressedXContent(mapping2), MergeReason.MAPPING_UPDATE); assertEquals(mapping2, mapper.mappingSource().toString()); String mapping3 = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("_doc") .startObject("properties").startObject("field").field("type", "constant_keyword") .field("meta", Collections.singletonMap("baz", "quux")) .endObject().endObject().endObject().endObject()); mapper = indexService.mapperService().merge("_doc", new CompressedXContent(mapping3), MergeReason.MAPPING_UPDATE); assertEquals(mapping3, mapper.mappingSource().toString()); }	really minor: all the indentations for the lines containing method calls in testlookupvalues are inconsistent compared to the other methods in this class.
@Override public ValuesSourceType getValuesSourceType() { return CoreValuesSourceType.BYTES; } } static class WildcardBytesBinaryIndexFieldData extends BytesBinaryIndexFieldData { WildcardBytesBinaryIndexFieldData(Index index, String fieldName) { super(index, fieldName); } @Override public SortField sortField(Object missingValue, MultiValueMode sortMode, Nested nested, boolean reverse) { XFieldComparatorSource source = new BytesRefFieldComparatorSource(this, missingValue, sortMode, nested); return new SortField(getFieldName(), source, reverse); } } private int ignoreAbove; private WildcardFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, int ignoreAbove, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); this.ignoreAbove = ignoreAbove; assert fieldType.indexOptions() == IndexOptions.DOCS; ngramFieldType = fieldType.clone(); ngramFieldType.setTokenized(true); ngramFieldType.freeze(); } /** Values that have more chars than the return value of this method will * be skipped at parsing time. */ // pkg-private for testing int ignoreAbove() { return ignoreAbove; } @Override protected WildcardFieldMapper clone() { return (WildcardFieldMapper) super.clone(); } @Override public WildcardFieldType fieldType() { return (WildcardFieldType) super.fieldType(); } @Override protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException { super.doXContentBody(builder, includeDefaults, params); if (includeDefaults || ignoreAbove != Defaults.IGNORE_ABOVE) { builder.field("ignore_above", ignoreAbove); } } @Override protected void parseCreateField(ParseContext context) throws IOException { final String value; if (context.externalValueSet()) { value = context.externalValue().toString(); } else { XContentParser parser = context.parser(); if (parser.currentToken() == XContentParser.Token.VALUE_NULL) { value = fieldType().nullValueAsString(); } else { value = parser.textOrNull(); } } ParseContext.Document parseDoc = context.doc(); List<IndexableField> fields = new ArrayList<>(); createFields(value, parseDoc, fields); parseDoc.addAll(fields); } @Override protected String parseSourceValue(Object value) { return value.toString(); } // For internal use by Lucene only - used to define ngram index final MappedFieldType ngramFieldType; void createFields(String value, Document parseDoc, List<IndexableField>fields) throws IOException { if (value == null || value.length() > ignoreAbove) { return; } String ngramValue = TOKEN_START_OR_END_CHAR + value + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR; Field ngramField = new Field(fieldType().name(), ngramValue, ngramFieldType); fields.add(ngramField); CustomBinaryDocValuesField dvField = (CustomBinaryDocValuesField) parseDoc.getByKey(fieldType().name()); if (dvField == null) { dvField = new CustomBinaryDocValuesField(fieldType().name(), value.getBytes(StandardCharsets.UTF_8)); parseDoc.addWithKey(fieldType().name(), dvField); } else { dvField.add(value.getBytes(StandardCharsets.UTF_8)); } } @Override protected String contentType() { return CONTENT_TYPE; }	same comment as the one for keyword field: shouldn't ignore_above be considered when parsing the source?
private List<String> cleanupStaleRootFiles(Collection<SnapshotId> deletedSnapshots, List<String> blobsToDelete) { if (blobsToDelete.isEmpty()) { return blobsToDelete; } try { if (logger.isInfoEnabled()) { // If we're running root level cleanup as part of a snapshot delete we should not log the snapshot- and global metadata // blobs associated with the just deleted snapshots as they are expected to exist and not stale. Otherwise every snapshot // delete would also log a confusing INFO message about "stale blobs". final Set<String> blobNamesToIgnore = deletedSnapshots.stream().flatMap( snapshotId -> Stream.of(globalMetadataFormat.blobName(snapshotId.getUUID()), snapshotFormat.blobName(snapshotId.getUUID()))).collect(Collectors.toSet()); final List<String> blobsToLog = blobsToDelete.stream().filter(b -> blobNamesToIgnore.contains(b) == false) .collect(Collectors.toList()); if (blobsToLog.isEmpty() == false) { logger.info("[{}] Found stale root level blobs {}. Cleaning them up", metadata.name(), blobsToLog); } } blobContainer().deleteBlobsIgnoringIfNotExists(blobsToDelete); return blobsToDelete; } catch (IOException e) { logger.warn(() -> new ParameterizedMessage( "[{}] The following blobs are no longer part of any snapshot [{}] but failed to remove them", metadata.name(), blobsToDelete), e); } catch (Exception e) { // TODO: We shouldn't be blanket catching and suppressing all exceptions here and instead handle them safely upstream. // Currently this catch exists as a stop gap solution to tackle unexpected runtime exceptions from implementations // bubbling up and breaking the snapshot functionality. assert false : e; logger.warn(new ParameterizedMessage("[{}] Exception during cleanup of root level blobs", metadata.name()), e); } return Collections.emptyList(); }	i think it would be helpful to continue to log all the blobs at trace level; i'd do both if there are unexpected ones as it's interesting to know that they were unexpected.
private static boolean isNotQueryWithFromClauseAndFilterFoldedToFalse(UnaryPlan plan) { return (!(plan.child() instanceof LocalRelation) || (plan.child() instanceof LocalRelation && !(((LocalRelation) plan.child()).executable() instanceof EmptyExecutable))); }	is transformdown really needed here or foreachdown is enough?
public Map<String, Long> getClientConnections() { // create a (weakly consistent) snapshot of the map so that it does not get modified when serializing it for stats reporting return Collections.unmodifiableMap(new HashMap<>(clientConnections)); }	i think this is subject to the same race, concurrent modification during the copy? it won't manifest as an inconsistent snapshot of size and number of entries when iterating for serialization, but instead a concurrent modification exception thrown during the copy?
public SearchScript search(SearchLookup lookup, Script script, ScriptContext scriptContext, Map<String, String> params) { CompiledScript compiledScript = compile(script, scriptContext, params); return search(lookup, compiledScript, script.getParams()); }	can you add some minimal javadocs?
public void testToQuery() throws IOException { for (int runs = 0; runs < NUMBER_OF_TESTQUERIES; runs++) { QueryShardContext context = createShardContext(); assert context.isCachable(); context.setAllowUnmappedFields(true); QB firstQuery = createTestQueryBuilder(); QB controlQuery = copyQuery(firstQuery); setSearchContext(randomTypes, context); // only set search context for toQuery to be more realistic /* we use a private rewrite context here since we want the most realistic way of asserting that we are cachabel or not. * We do it this way in SearchService where * we first rewrite the query with a private context, then reset the context and then build the actual lucene query*/ QueryBuilder rewritten = rewriteQuery(firstQuery, new QueryShardContext(context)); Query firstLuceneQuery = rewritten.toQuery(context); if (isCachable(firstQuery)) { assert context.isCachable() : firstQuery.toString(); } else { assert context.isCachable() == false : firstQuery.toString(); } assertNotNull("toQuery should not return null", firstLuceneQuery); assertLuceneQuery(firstQuery, firstLuceneQuery, context); //remove after assertLuceneQuery since the assertLuceneQuery impl might access the context as well SearchContext.removeCurrent(); assertTrue( "query is not equal to its copy after calling toQuery, firstQuery: " + firstQuery + ", secondQuery: " + controlQuery, firstQuery.equals(controlQuery)); assertTrue("equals is not symmetric after calling toQuery, firstQuery: " + firstQuery + ", secondQuery: " + controlQuery, controlQuery.equals(firstQuery)); assertThat("query copy's hashcode is different from original hashcode after calling toQuery, firstQuery: " + firstQuery + ", secondQuery: " + controlQuery, controlQuery.hashCode(), equalTo(firstQuery.hashCode())); QB secondQuery = copyQuery(firstQuery); // query _name never should affect the result of toQuery, we randomly set it to make sure if (randomBoolean()) { secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10)); } setSearchContext(randomTypes, context); Query secondLuceneQuery = rewriteQuery(secondQuery, context).toQuery(context); assertNotNull("toQuery should not return null", secondLuceneQuery); assertLuceneQuery(secondQuery, secondLuceneQuery, context); SearchContext.removeCurrent(); assertEquals("two equivalent query builders lead to different lucene queries", rewrite(secondLuceneQuery), rewrite(firstLuceneQuery)); if (supportsBoostAndQueryName()) { secondQuery.boost(firstQuery.boost() + 1f + randomFloat()); setSearchContext(randomTypes, context); Query thirdLuceneQuery = rewriteQuery(secondQuery, context).toQuery(context); SearchContext.removeCurrent(); assertNotEquals("modifying the boost doesn't affect the corresponding lucene query", rewrite(firstLuceneQuery), rewrite(thirdLuceneQuery)); } // check that context#isFilter is not changed by invoking toQuery/rewrite boolean filterFlag = randomBoolean(); context.setIsFilter(filterFlag); rewriteQuery(firstQuery, context).toQuery(context); assertEquals("isFilter should be unchanged", filterFlag, context.isFilter()); } }	since it's a test case, should it use the junit assertions instead of the assert keyword?
public void testGetFieldMapping() throws IOException { GetFieldMappingsRequest getFieldMappingsRequest = new GetFieldMappingsRequest(); String[] indices = Strings.EMPTY_ARRAY; if (randomBoolean()) { indices = randomIndicesNames(0, 5); getFieldMappingsRequest.indices(indices); } else if (randomBoolean()) { getFieldMappingsRequest.indices((String[]) null); } String type = null; if (randomBoolean()) { type = randomAlphaOfLengthBetween(3, 10); getFieldMappingsRequest.types(type); } else if (randomBoolean()) { getFieldMappingsRequest.types((String[]) null); } String[] fields = null; if (randomBoolean()) { fields = new String[randomIntBetween(1, 5)]; for (int i = 0; i < fields.length; i++) { fields[i] = randomAlphaOfLengthBetween(3, 10); } getFieldMappingsRequest.fields(fields); } else if (randomBoolean()) { getFieldMappingsRequest.fields((String[]) null); } Map<String, String> expectedParams = new HashMap<>(); setRandomIndicesOptions(getFieldMappingsRequest::indicesOptions, getFieldMappingsRequest::indicesOptions, expectedParams); if (randomBoolean()) { boolean local = randomBoolean(); getFieldMappingsRequest.local(local); if (local) { expectedParams.put("local", String.valueOf(local)); } } Request request = RequestConverters.getFieldMapping(getFieldMappingsRequest); StringJoiner endpoint = new StringJoiner("/", "/", ""); String index = String.join(",", indices); if (Strings.hasLength(index)) { endpoint.add(index); } endpoint.add("_mapping"); if (type != null) { endpoint.add(type); } endpoint.add("field"); if (fields != null) { endpoint.add(String.join(",", fields)); } assertThat(endpoint.toString(), equalTo(request.getEndpoint())); assertThat(expectedParams, equalTo(request.getParameters())); assertThat(HttpGet.METHOD_NAME, equalTo(request.getMethod())); }	you can use setrandomlocal
public void testGetFieldMapping() throws IOException, InterruptedException { RestHighLevelClient client = highLevelClient(); { CreateIndexResponse createIndexResponse = client.indices().create(new CreateIndexRequest("twitter"), RequestOptions.DEFAULT); assertTrue(createIndexResponse.isAcknowledged()); PutMappingRequest request = new PutMappingRequest("twitter"); request.type("tweet"); request.source( "{\\\\n" + " \\\\"properties\\\\": {\\\\n" + " \\\\"message\\\\": {\\\\n" + " \\\\"type\\\\": \\\\"text\\\\"\\\\n" + " },\\\\n" + " \\\\"timestamp\\\\": {\\\\n" + " \\\\"type\\\\": \\\\"date\\\\"\\\\n" + " }\\\\n" + " }\\\\n" + "}", // <1> XContentType.JSON); PutMappingResponse putMappingResponse = client.indices().putMapping(request, RequestOptions.DEFAULT); assertTrue(putMappingResponse.isAcknowledged()); } // tag::get-field-mapping-request GetFieldMappingsRequest request = new GetFieldMappingsRequest(); // <1> request.indices("twitter"); // <2> request.types("tweet"); // <3> request.fields("message", "timestamp"); // <4> // end::get-field-mapping-request // tag::get-field-mapping-request-indicesOptions request.indicesOptions(IndicesOptions.lenientExpandOpen()); // <1> // end::get-field-mapping-request-indicesOptions // tag::get-field-mapping-request-local request.local(true); // <1> // end::get-field-mapping-request-local { // tag::get-field-mapping-execute GetFieldMappingsResponse response = client.indices().getFieldMapping(request, RequestOptions.DEFAULT); // end::get-field-mapping-execute // tag::get-field-mapping-response final Map<String, Map<String, Map<String, GetFieldMappingsResponse.FieldMappingMetaData>>> mappings = response.mappings();// <1> final Map<String, GetFieldMappingsResponse.FieldMappingMetaData> typeMappings = mappings.get("twitter").get("tweet"); // <2> final GetFieldMappingsResponse.FieldMappingMetaData metaData = typeMappings.get("message");// <3> final String fullName = metaData.fullName();// <4> final Map<String, Object> source = metaData.sourceAsMap(); // <5> // end::get-field-mapping-response assertThat(fullName, equalTo("message")); assertThat(source, equalTo(Collections.singletonMap("message", Collections.singletonMap("type", "text")))); } { // tag::get-field-mapping-execute-listener ActionListener<GetFieldMappingsResponse> listener = new ActionListener<GetFieldMappingsResponse>() { @Override public void onResponse(GetFieldMappingsResponse putMappingResponse) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::get-field-mapping-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); final ActionListener<GetFieldMappingsResponse> latchListener = new LatchedActionListener<>(listener, latch); listener = ActionListener.wrap(r -> { final Map<String, Map<String, Map<String, GetFieldMappingsResponse.FieldMappingMetaData>>> mappings = r.mappings(); final Map<String, GetFieldMappingsResponse.FieldMappingMetaData> typeMappings = mappings.get("twitter").get("tweet"); final GetFieldMappingsResponse.FieldMappingMetaData metaData1 = typeMappings.get("message"); final String fullName = metaData1.fullName(); final Map<String, Object> source = metaData1.sourceAsMap(); assertThat(fullName, equalTo("message")); assertThat(source, equalTo(Collections.singletonMap("message", Collections.singletonMap("type", "text")))); latchListener.onResponse(r); }, e -> { latchListener.onFailure(e); fail("should not fail"); }); // tag::get-field-mapping-execute-async client.indices().getFieldMappingAsync(request, RequestOptions.DEFAULT, listener); // <1> // end::get-field-mapping-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } }	do we need these assertions here? given that this test is added to test the docs snippets, i would avoid testing the functionality itself.
public static GetFieldMappingsResponse fromXContent(XContentParser parser) throws IOException { final Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings = new HashMap<>(); final Map<String, Object> map = parser.map(); for (Map.Entry<String, Object> entry : map.entrySet()) { final Object value = entry.getValue(); Map<String, Map<String, FieldMappingMetaData>> typeMappings = new HashMap<>(); mappings.put(entry.getKey(), typeMappings); if (value instanceof Map) { final Object o = ((Map) value).get(MAPPINGS.getPreferredName()); if (!(o instanceof Map)) { throw new ParsingException(parser.getTokenLocation(), "Nested " + MAPPINGS.getPreferredName() + " is not found"); } Map<String, Object> map1 = (Map) o; for (Map.Entry<String, Object> typeObjectEntry : map1.entrySet()) { Map<String, FieldMappingMetaData> fieldsMapping = new HashMap<>(); typeMappings.put(typeObjectEntry.getKey(), fieldsMapping); final Object o1 = typeObjectEntry.getValue(); if (!(o1 instanceof Map)) { throw new ParsingException(parser.getTokenLocation(), "Nested type mapping is not found"); } Map<String, Object> map2 = (Map) o1; for (Map.Entry<String, Object> e : map2.entrySet()) { final Object o2 = e.getValue(); if (!(o2 instanceof Map)) { throw new ParsingException(parser.getTokenLocation(), "Nested field mapping is not found"); } Map<String, Object> map3 = (Map) o2; String fullName = (String) map3.get(FieldMappingMetaData.FULL_NAME.getPreferredName()); XContentBuilder builder = jsonBuilder(); final Map<String, ?> values = (Map<String, ?>) map3.get(FieldMappingMetaData.MAPPING.getPreferredName()); builder.map(values); final BytesReference source = BytesReference.bytes(builder); FieldMappingMetaData metaData = new FieldMappingMetaData(fullName, source); fieldsMapping.put(e.getKey(), metaData); } } } } return new GetFieldMappingsResponse(mappings); }	is it necessary to parse everything into a map in the first place? at some point later on we do need a map, but i wonder if we can postpone calling parser.map till then.
*/ public void updatePrimaryTerm(final long newPrimaryTerm) { assert shardRouting.primary() : "primary term can only be explicitly updated on a primary shard"; synchronized (mutex) { if (newPrimaryTerm != primaryTerm) { // Note that due to cluster state batching an initializing primary shard term can failed and re-assigned // in one state causing it's term to be incremented. Note that if both current shard state and new // shard state are initializing, we could replace the current shard and reinitialize it. It is however // possible that this shard is being started. This can happen if: // 1) Shard is post recovery and sends shard started to the master // 2) Node gets disconnected and rejoins // 3) Master assigns the shard back to the node // 4) Master processes the shard started and starts the shard // 5) The node process the cluster state where the shard is both started and primary term is incremented. // // We could fail the shard in that case, but this will cause it to be removed from the insync allocations list // potentially preventing re-allocation. assert shardRouting.initializing() == false : "a started primary shard should never update its term; " + "shard " + shardRouting + ", " + "current term [" + primaryTerm + "], " + "new term [" + newPrimaryTerm + "]"; assert newPrimaryTerm > primaryTerm : "primary terms can only go up; current term [" + primaryTerm + "], new term [" + newPrimaryTerm + "]"; /* * Before this call returns, we are guaranteed that all future operations are delayed and so this happens before we * increment the primary term. The latch is needed to ensure that we do not unblock operations before the primary term is * incremented. */ final CountDownLatch latch = new CountDownLatch(1); indexShardOperationPermits.asyncBlockOperations( 30, TimeUnit.MINUTES, () -> { latch.await(); try { getEngine().fillSeqNoGaps(newPrimaryTerm); } catch (final AlreadyClosedException e) { // okay, the index was deleted or this shard was never activated after a relocation. } }, e -> failShard("exception during primary term transition", e)); primaryTerm = newPrimaryTerm; latch.countDown(); } } }	what do you mean with this shard was never activated post relocation?
public void testUnconsumedParametersDidYouMean() throws Exception { final AtomicBoolean executed = new AtomicBoolean(); BaseRestHandler handler = new BaseRestHandler(Settings.EMPTY) { @Override protected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException { request.param("consumed"); request.param("field"); request.param("tokenizer"); request.param("very_close_to_parameter_1"); request.param("very_close_to_parameter_2"); return channel -> executed.set(true); } @Override protected Set<String> responseParams() { return Collections.singleton("response_param"); } @Override public String getName() { return "test_unconsumed_did_you_mean_response_action"; } }; final HashMap<String, String> params = new HashMap<>(); params.put("consumed", randomAlphaOfLength(8)); params.put("flied", randomAlphaOfLength(8)); params.put("response_param", randomAlphaOfLength(8)); params.put("tokenzier", randomAlphaOfLength(8)); params.put("very_close_to_parametre", randomAlphaOfLength(8)); params.put("very_far_from_every_consumed_parameter", randomAlphaOfLength(8)); RestRequest request = new FakeRestRequest.Builder(xContentRegistry()).withParams(params).build(); RestChannel channel = new FakeRestChannel(request, randomBoolean(), 1); final IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> handler.handleRequest(request, channel, mock(NodeClient.class))); assertThat( e, hasToString(containsString( "request [/] contains unrecognized parameters: " + "[flied] -> did you mean [field]?, " + "[respones_param] -> did you mean [response_param]?, " + "[tokenzier] -> did you mean [tokenizer]?, " + "[very_close_to_parametre] -> did you mean any of [very_close_to_parameter_1, very_close_to_parameter_2]?, " + "[very_far_from_every_consumed_parameter]"))); assertFalse(executed.get()); }	this misspelling was deliberate - it's testing the "did you mean" functionality.
public void testResolveSingleValuedAttributeFromCachedAttributes() throws Exception { resolver = new LdapMetaDataResolver(Arrays.asList("cn", "uid"), true); final Collection<Attribute> attributes = Arrays.asList( new Attribute("cn", "Clint Barton"), new Attribute("uid", "hawkeye"), new Attribute("email", "clint.barton@shield.gov"), new Attribute("memberOf", "cn=staff,ou=groups,dc=example,dc=com", "cn=admin,ou=groups,dc=example,dc=com") ); final Map<String, Object> map = resolve(attributes); assertThat(map.size(), equalTo(2)); assertThat(map.get("cn"), equalTo("Clint Barton")); assertThat(map.get("uid"), equalTo("hawkeye")); }	@tvernum this fix looks ok to me, but just checking with you that it wasn't a deliberate typo?
void addRecoveredBytes(long bytes) { assert reused == false : "file is marked as reused, can't update recovered bytes"; assert bytes >= 0 : "can't recovered negative bytes. got [" + bytes + "]"; recovered += bytes; }	i think this roughly makes sense but might cause confusion for tools that are watching for recovery progress if they see it go backwards. we're planning on breaking out bytes recovered from snapshot vs from the primary in a followup right? at which point we should be able to drop this again. let's add a // todo here about that.
private static void purgePackagesLinux() { final Shell sh = new Shell(); if (isRPM()) { // this may leave behind config files in /etc/elasticsearch, but a later step in this cleanup will get them sh.runIgnoreExitCode("rpm --quiet -e elasticsearch elasticsearch-oss"); } if (isDPKG()) { sh.runIgnoreExitCode("dpkg --purge elasticsearch elasticsearch-oss"); } }	on oel 6, yum downloads a lot of stuff when this is invoked. we don't call out to yum or apt get anywhere else in the tests, and we always interact with rpm or dpkg directly, so it seemed fine to take this out
public void testIso8601Parsing() { DateFormatter formatter = DateFormatters.forPattern("iso8601"); // timezone not allowed with just date formatter.format(formatter.parse("2018-05-15")); formatter.format(formatter.parse("2018-05-15T17")); formatter.format(formatter.parse("2018-05-15T17Z")); formatter.format(formatter.parse("2018-05-15T17+0100")); formatter.format(formatter.parse("2018-05-15T17+01:00")); formatter.format(formatter.parse("2018-05-15T17:14")); formatter.format(formatter.parse("2018-05-15T17:14Z")); formatter.format(formatter.parse("2018-05-15T17:14-0100")); formatter.format(formatter.parse("2018-05-15T17:14-01:00")); formatter.format(formatter.parse("2018-05-15T17:14:56")); formatter.format(formatter.parse("2018-05-15T17:14:56Z")); formatter.format(formatter.parse("2018-05-15T17:14:56+0100")); formatter.format(formatter.parse("2018-05-15T17:14:56+01:00")); // milliseconds can be separated using comma or decimal point formatter.format(formatter.parse("2018-05-15T17:14:56.123")); formatter.format(formatter.parse("2018-05-15T17:14:56.123Z")); formatter.format(formatter.parse("2018-05-15T17:14:56.123-0100")); formatter.format(formatter.parse("2018-05-15T17:14:56.123-01:00")); formatter.format(formatter.parse("2018-05-15T17:14:56,123")); formatter.format(formatter.parse("2018-05-15T17:14:56,123Z")); formatter.format(formatter.parse("2018-05-15T17:14:56,123+0100")); formatter.format(formatter.parse("2018-05-15T17:14:56,123+01:00")); // microseconds can be separated using comma or decimal point formatter.format(formatter.parse("2018-05-15T17:14:56.123456")); formatter.format(formatter.parse("2018-05-15T17:14:56.123456Z")); formatter.format(formatter.parse("2018-05-15T17:14:56.123456+0100")); formatter.format(formatter.parse("2018-05-15T17:14:56.123456+01:00")); formatter.format(formatter.parse("2018-05-15T17:14:56,123456")); formatter.format(formatter.parse("2018-05-15T17:14:56,123456Z")); formatter.format(formatter.parse("2018-05-15T17:14:56,123456-0100")); formatter.format(formatter.parse("2018-05-15T17:14:56,123456-01:00")); // nanoseconds can be separated using comma or decimal point formatter.format(formatter.parse("2018-05-15T17:14:56.123456789")); formatter.format(formatter.parse("2018-05-15T17:14:56.123456789Z")); formatter.format(formatter.parse("2018-05-15T17:14:56.123456789-0100")); formatter.format(formatter.parse("2018-05-15T17:14:56.123456789-01:00")); formatter.format(formatter.parse("2018-05-15T17:14:56,123456789")); formatter.format(formatter.parse("2018-05-15T17:14:56,123456789Z")); formatter.format(formatter.parse("2018-05-15T17:14:56,123456789+0100")); formatter.format(formatter.parse("2018-05-15T17:14:56,123456789+01:00")); }	are we sure we need this? i am not sure if iso8601 is really specifying this all, but java.time' datetimeformatter.iso_date_time is a bit more strict. it would fail for this and many others: datetimeformatter.iso_date_time.parse("2018-05-15t17") datetimeformatter.iso_date_time.parse("2018-05-15t17+01:00") etc..
@Override public void deleteBlob(String blobName) throws IOException { logger.trace("deleteBlob({})", blobName); if (!blobExists(blobName)) { throw new IOException("Blob [" + blobName + "] does not exist"); } try { blobStore.deleteBlob(blobStore.container(), buildKey(blobName)); } catch (URISyntaxException | StorageException e) { logger.warn("can not access [{}] in container {{}}: {}", blobName, blobStore.container(), e.getMessage()); throw new IOException(e); } }	should it be a nosuchfileexception here?
public void reportBucket(Bucket bucket) { currentTimingStats.updateStats(bucket.getProcessingTimeMs()); currentTimingStats.setLatestRecordTimestamp(bucket.getTimestamp().toInstant()); if (differSignificantly(currentTimingStats, persistedTimingStats)) { flush(); } }	bucket timestamp is the start of the bucket. if the timestamp falls into the next window for calculating exp average then that new window should have the incremented value. with the datafeed the lastest record timestamp represents the end of the bucket and should count in the previous. in this case the the method you should be calling is set*earliest*recordtimestamp
public static NamedClusterPrivilege resolve(String name) { name = Objects.requireNonNull(name).toLowerCase(Locale.ROOT); if (isClusterAction(name)) { return new ActionClusterPrivilege(name, Set.of(actionToPattern(name))); } final NamedClusterPrivilege fixedPrivilege = VALUES.get(name); if (fixedPrivilege != null) { return fixedPrivilege; } String errorMessage = "unknown cluster privilege [" + name + "]. a privilege must be either " + "one of the predefined cluster privilege names [" + Strings.collectionToCommaDelimitedString(VALUES.keySet()) + "] or a pattern over one of the available " + "cluster actions"; logger.error(errorMessage); throw new IllegalArgumentException(errorMessage); }	i don't think this should be an error. we would typically not log _anything_ for problems that are also reported in the rest response. if there's a strong argument for doing so here, then it should be warning or lower, but i would suggest it's just a debug - the server admin doesn't need to care about this, the person submiting the request does.
public void execute(List<PluginDescriptor> plugins) throws Exception { if (plugins.isEmpty()) { throw new UserException(ExitCodes.USAGE, "at least one plugin id is required"); } final Set<String> uniquePluginIds = new HashSet<>(); for (final PluginDescriptor plugin : plugins) { if (uniquePluginIds.add(plugin.getId()) == false) { throw new UserException(ExitCodes.USAGE, "duplicate plugin id [" + plugin.getId() + "]"); } } final String logPrefix = terminal.isHeadless() ? "" : "-> "; final Map<String, List<Path>> deleteOnFailures = new LinkedHashMap<>(); for (final PluginDescriptor plugin : plugins) { final String pluginId = plugin.getId(); terminal.println(logPrefix + "Installing " + pluginId); try { if ("x-pack".equals(pluginId)) { throw new UserException(ExitCodes.CONFIG, "this distribution of Elasticsearch contains X-Pack by default"); } if (PLUGINS_CONVERTED_TO_MODULES.contains(pluginId)) { // This deliberately does not throw an exception in order to avoid failing automation that relies on installing this // plugin during deployment. terminal.errorPrintln( "[" + pluginId + "] is no longer a plugin but instead a module packaged with this distribution of Elasticsearch" ); continue; } final List<Path> deleteOnFailure = new ArrayList<>(); deleteOnFailures.put(pluginId, deleteOnFailure); final Path pluginZip = download(plugin, env.tmpFile()); final Path extractedZip = unzip(pluginZip, env.pluginsFile()); deleteOnFailure.add(extractedZip); final PluginInfo pluginInfo = installPlugin(plugin, extractedZip, deleteOnFailure); terminal.println(logPrefix + "Installed " + pluginInfo.getName()); // swap the entry by plugin id for one with the installed plugin name, it gives a cleaner error message for URL installs deleteOnFailures.remove(pluginId); deleteOnFailures.put(pluginInfo.getName(), deleteOnFailure); } catch (final Exception installProblem) { terminal.println(logPrefix + "Failed installing " + pluginId); for (final Map.Entry<String, List<Path>> deleteOnFailureEntry : deleteOnFailures.entrySet()) { terminal.println(logPrefix + "Rolling back " + deleteOnFailureEntry.getKey()); boolean success = false; try { IOUtils.rm(deleteOnFailureEntry.getValue().toArray(new Path[0])); success = true; } catch (final IOException exceptionWhileRemovingFiles) { final Exception exception = new Exception( "failed rolling back installation of [" + deleteOnFailureEntry.getKey() + "]", exceptionWhileRemovingFiles ); installProblem.addSuppressed(exception); terminal.println(logPrefix + "Failed rolling back " + deleteOnFailureEntry.getKey()); } if (success) { terminal.println(logPrefix + "Rolled back " + deleteOnFailureEntry.getKey()); } } throw installProblem; } } if (terminal.isHeadless() == false) { terminal.println("-> Please restart Elasticsearch to activate any plugins installed"); } }	do we even need to handle this anymore at this point. the "x-pack" plugin hasn't been a thing for many versions. i assume this was only added to ease migration after x-pack was folded into es.
private List<RoleReference> buildRoleReferencesForApiKey() { if (version.before(VERSION_API_KEY_ROLES_AS_BYTES)) { return buildRolesReferenceForApiKeyBwc(); } final String apiKeyId = (String) metadata.get(AuthenticationField.API_KEY_ID_KEY); final BytesReference roleDescriptorsBytes = (BytesReference) metadata.get(API_KEY_ROLE_DESCRIPTORS_KEY); final BytesReference limitedByRoleDescriptorsBytes = getLimitedByRoleDescriptorsBytes(); if (roleDescriptorsBytes == null && limitedByRoleDescriptorsBytes == null) { throw new ElasticsearchSecurityException("no role descriptors found for API key"); } final RoleReference.ApiKeyRoleReference limitedByRoleReference = new RoleReference.ApiKeyRoleReference( apiKeyId, limitedByRoleDescriptorsBytes, true ); if (isEmptyRoleDescriptorsBytes(roleDescriptorsBytes)) { return List.of(limitedByRoleReference); } return List.of(new RoleReference.ApiKeyRoleReference(apiKeyId, roleDescriptorsBytes, false), limitedByRoleReference); }	it's probably a _nit_, but i think an enum would be better than a boolean. something like: enum apikeyroletype { assigned_roles, limited_by_roles } (i'm happy with different names for them enum class, or members)
void copyIndex(final Path src, final String indexName, final Path... dests) throws IOException { for (Path dest : dests) { Path indexDir = dest.resolve(indexName); assertFalse(Files.exists(indexDir)); Files.createDirectories(indexDir); } Files.walkFileTree(src, new SimpleFileVisitor<Path>() { @Override public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException { Path relativeDir = src.relativize(dir); for (Path dest : dests) { Path destDir = dest.resolve(indexName).resolve(relativeDir); Files.createDirectories(destDir); } return FileVisitResult.CONTINUE; } @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { if (file.getFileName().toString().equals(IndexWriter.WRITE_LOCK_NAME)) { // skip lock file, we don't need it logger.trace("Skipping lock file: " + file.toString()); return FileVisitResult.CONTINUE; } Path relativeFile = src.relativize(file); Path destFile = dests[randomInt(dests.length - 1)].resolve(indexName).resolve(relativeFile); logger.trace("--> Moving " + relativeFile.toString() + " to " + destFile.toString()); Files.move(file, destFile); assertFalse(Files.exists(file)); assertTrue(Files.exists(destFile)); return FileVisitResult.CONTINUE; } }); }	you don't trust files.move :)
@Override public void writeTo(StreamOutput out) throws IOException { out.writeByte(unit.id()); out.writeString(timeZone.getID()); } } static class TimeIntervalRounding extends TimeZoneRounding { final static byte ID = 2; private long interval; private DateTimeZone timeZone; private boolean isUtc; TimeIntervalRounding() { // for serialization } TimeIntervalRounding(long interval, DateTimeZone timeZone) { if (interval < 1) throw new ElasticsearchIllegalArgumentException("Negative time interval not supported"); this.interval = interval; this.timeZone = timeZone; this.isUtc = timeZone.equals(DateTimeZone.UTC); } @Override public byte id() { return ID; } @Override public long roundKey(long utcMillis) { long timeLocal = utcMillis; if (!isUtc) { timeLocal = timeZone.convertUTCToLocal(utcMillis); } return Rounding.Interval.roundKey(timeLocal, interval); } @Override public long valueForKey(long key) { long localTime = Rounding.Interval.roundValue(key, interval); if (!isUtc) { return timeZone.convertLocalToUTC(localTime, true); } else { return localTime; } } @Override public long nextRoundingValue(long time) { long timeLocal = time; if (!isUtc) { timeLocal = timeZone.convertUTCToLocal(time); } long next = timeLocal + interval; if (!isUtc){ return timeZone.convertLocalToUTC(next, true); } else { return next; } } @Override public void readFrom(StreamInput in) throws IOException { interval = in.readVLong(); timeZone = DateTimeZone.forID(in.readString()); isUtc = timeZone.equals(DateTimeZone.UTC); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeVLong(interval); out.writeString(timeZone.getID()); }	not sure if we need this boolean to guard against some timezone conversions. when the tz is utc, it looks like the conversion is going to be very cheap anyway (just substracting a 0 if i'm not mistaken). it would also make the impl simpler?
@Override public long roundKey(long utcMillis) { long timeLocal = utcMillis; if (!isUtc) { timeLocal = timeZone.convertUTCToLocal(utcMillis); } long rounded = field.roundFloor(timeLocal); if (!isUtc) { return timeZone.convertLocalToUTC(rounded, true, utcMillis); } else { return rounded; } }	@jpountz doing the conversion back from local time to utc here makes more sense to me, but i'm still wondering about performance maybe beeing better if this would be in valueforkey(). on the other hand that would implicitly create a dependency about how the two methods should be used. should i make performance tests here?
@Override public long roundKey(long utcMillis) { long timeLocal = utcMillis; if (!isUtc) { timeLocal = timeZone.convertUTCToLocal(utcMillis); } long rounded = field.roundFloor(timeLocal); if (!isUtc) { return timeZone.convertLocalToUTC(rounded, true, utcMillis); } else { return rounded; } }	same question as above, only for timeintervalrounding i left the conversion here. either way, in the end i'd like the two implementations to be similar, will change one or the other.
public void testCancellation() throws Exception { assertAcked( client().admin() .indices() .prepareCreate("test") .setMapping("val", "type=integer", "event_type", "type=keyword", "@timestamp", "type=date") .get() ); createIndex("idx_unmapped"); int numDocs = randomIntBetween(6, 20); List<IndexRequestBuilder> builders = new ArrayList<>(); for (int i = 0; i < numDocs; i++) { int fieldValue = randomIntBetween(0, 10); builders.add( client().prepareIndex("test") .setSource( jsonBuilder().startObject() .field("val", fieldValue) .field("event_type", "my_event") .field("@timestamp", "2020-04-09T12:35:48Z") .endObject() ) ); } indexRandom(true, builders); boolean cancelDuringSearch = randomBoolean(); List<SearchBlockPlugin> plugins = initBlockFactory(cancelDuringSearch, cancelDuringSearch == false); SqlQueryRequest request = new SqlQueryRequestBuilder(client(), SqlQueryAction.INSTANCE).query( "SELECT event_type FROM test WHERE val=1" ).request(); String id = randomAlphaOfLength(10); logger.trace("Preparing search"); // We might perform field caps on the same thread if it is local client, so we cannot use the standard mechanism Future<SqlQueryResponse> future = executorService.submit( () -> client().filterWithHeader(Collections.singletonMap(Task.X_OPAQUE_ID_HTTP_HEADER, id)) .execute(SqlQueryAction.INSTANCE, request) .get() ); logger.trace("Waiting for block to be established"); if (cancelDuringSearch) { awaitForBlockedSearches(plugins, "test"); } else { awaitForBlockedFieldCaps(plugins); } logger.trace("Block is established"); cancelTaskWithXOpaqueId(id, SqlQueryAction.NAME); disableBlocks(plugins); Exception exception = expectThrows(Exception.class, future::get); assertNotNull(ExceptionsHelper.unwrap(exception, TaskCancelledException.class)); if (cancelDuringSearch) { // Make sure we cancelled inside search assertThat(getNumberOfContexts(plugins), greaterThan(0)); } else { // Make sure we were not cancelled inside search assertThat(getNumberOfContexts(plugins), equalTo(0)); } }	why is this line not needed anymore?
@Override public final Tuple<List<AggregationBuilder>, List<PipelineAggregationBuilder>> aggs(String actualField, String predictedField) { // Store given {@code actualField} for the purpose of generating error message in {@code process}. this.actualField = actualField; List<AggregationBuilder> aggs = new ArrayList<>(); List<PipelineAggregationBuilder> pipelineAggs = new ArrayList<>(); if (overallAccuracy == null) { aggs.add(AggregationBuilders.avg(OVERALL_ACCURACY_AGG_NAME).script(buildScript(actualField, predictedField))); } if (result == null) { Tuple<List<AggregationBuilder>, List<PipelineAggregationBuilder>> matrixAggs = matrix.aggs(actualField, predictedField); aggs.addAll(matrixAggs.v1()); pipelineAggs.addAll(matrixAggs.v2()); } return Tuple.tuple(aggs, pipelineAggs); }	i don't think we should do this. it is a very unexpected side-effect. since there are only two fields actual and predicted i think it is satisfactory for the error message to simply indicate cardinality of actual field is too high
static void setupRunnerTask(Project project, RestIntegTestTask testTask, SourceSet sourceSet) { testTask.setTestClassesDirs(sourceSet.getOutput().getClassesDirs()); testTask.setClasspath(sourceSet.getRuntimeClasspath()); }	why is this no longer needed?
private static Bundle readPluginBundle(final Set<Bundle> bundles, final Path plugin, String type) throws IOException { LogManager.getLogger(PluginsService.class).trace("--- adding [{}] [{}]", type, plugin.toAbsolutePath()); final PluginInfo info; try { info = PluginInfo.readFromProperties(plugin); } catch (final IOException e) { throw new IllegalStateException("Could not load plugin descriptor for " + type + " directory [" + plugin.getFileName() + "]", e); } final Bundle bundle = new Bundle(info, plugin); if (bundles.add(bundle) == false) { throw new IllegalStateException("duplicate " + type + ": " + info); } if (type.equals("module") && info.getName().startsWith("test-") && Build.CURRENT.isSnapshot() == false) { throw new IllegalStateException("external test module [" + plugin.getFileName() + "] found in non-snapshot build"); } return bundle; }	seems basing this on name is a little brittle. is there no way to add arbitrary metadata to modules?
private LocalCheckpointTracker createLocalCheckpointTracker( BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier) throws IOException { switch (openMode) { case CREATE_INDEX_AND_TRANSLOG: return localCheckpointTrackerSupplier.apply(SequenceNumbers.NO_OPS_PERFORMED, SequenceNumbers.NO_OPS_PERFORMED); case OPEN_INDEX_CREATE_TRANSLOG: final Tuple<Long, Long> seqNoInfo = store.loadSeqNoInfo(null); logger.trace("recovered maximum sequence number [{}] and local checkpoint [{}]", seqNoInfo.v1(), seqNoInfo.v2()); return localCheckpointTrackerSupplier.apply(seqNoInfo.v1(), seqNoInfo.v2()); case OPEN_INDEX_AND_TRANSLOG: // When recovering from a previous commit point, we use the local checkpoint from that commit, // but the max_seqno from the last commit. This allows use to throw away stale operations. assert startingCommit != null; final long localCheckpoint = store.loadSeqNoInfo(startingCommit).v2(); final long maxSeqNo = store.loadSeqNoInfo(null).v1(); logger.trace("recovered maximum sequence number [{}] and local checkpoint [{}]", maxSeqNo, localCheckpoint); return localCheckpointTrackerSupplier.apply(maxSeqNo, localCheckpoint); default: throw new IllegalArgumentException("unknown type: " + openMode); } }	i like the final approach better this gives a uniform return value (easier to understand) and it makes sure these things are set.
private void startNewShards(SnapshotsInProgress.Entry entry, Map<ShardId, IndexShardSnapshotStatus> startedShards) { threadPool.executor(ThreadPool.Names.SNAPSHOT).execute(() -> { final Snapshot snapshot = entry.snapshot(); final Map<String, IndexId> indicesMap = entry.indices().stream().collect(Collectors.toMap(IndexId::getName, Function.identity())); for (final Map.Entry<ShardId, IndexShardSnapshotStatus> shardEntry : startedShards.entrySet()) { final ShardId shardId = shardEntry.getKey(); final IndexShardSnapshotStatus snapshotStatus = shardEntry.getValue(); final IndexId indexId = indicesMap.get(shardId.getIndexName()); assert indexId != null; assert entry.useShardGenerations() || snapshotStatus.generation() == null : "Found non-null shard generation [" + snapshotStatus.generation() + "] for snapshot with old-format compatibility"; snapshot(shardId, snapshot, indexId, snapshotStatus, entry.useShardGenerations(), new ActionListener<>() { @Override public void onResponse(String newGeneration) { assert newGeneration != null; assert newGeneration.equals(snapshotStatus.generation()); if (logger.isDebugEnabled()) { final IndexShardSnapshotStatus.Copy lastSnapshotStatus = snapshotStatus.asCopy(); logger.debug("snapshot [{}] completed to [{}] with [{}] at generation [{}]", snapshot, snapshot.getRepository(), lastSnapshotStatus, snapshotStatus.generation()); } notifySuccessfulSnapshotShard(snapshot, shardId, newGeneration); } @Override public void onFailure(Exception e) { final String failure = ExceptionsHelper.stackTrace(e); snapshotStatus.moveToFailed(threadPool.absoluteTimeInMillis(), failure); logger.warn(() -> new ParameterizedMessage("[{}][{}] failed to snapshot shard", shardId, snapshot), e); notifyFailedSnapshotShard(snapshot, shardId, failure); } }); } }); }	ew. can we follow up with a change that keeps the exception as an exception rather than converting it to a string here and in a few other places? looks nontrivial because bwc, of course.
public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception { logger.info("--> starting a master node and two data nodes"); internalCluster().startMasterOnlyNode(); internalCluster().startDataOnlyNodes(2); logger.info("--> creating repository"); assertAcked(client().admin().cluster().preparePutRepository("test-repo") .setType("mock").setSettings(Settings.builder() .put("location", randomRepoPath()) .put("compress", randomBoolean()) .put("chunk_size", randomIntBetween(100, 1000), ByteSizeUnit.BYTES))); assertAcked(prepareCreate("test-idx", 0, Settings.builder() .put("number_of_shards", 5).put("number_of_replicas", 0))); ensureGreen(); logger.info("--> indexing some data"); final int numdocs = randomIntBetween(50, 100); IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs]; for (int i = 0; i < builders.length; i++) { builders[i] = client().prepareIndex("test-idx").setId(Integer.toString(i)).setSource("field1", "bar " + i); } indexRandom(true, builders); flushAndRefresh(); final String dataNode = blockNodeWithIndex("test-repo", "test-idx"); logger.info("--> snapshot"); ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH); setDisruptionScheme(disruption); client(internalCluster().getMasterName()).admin().cluster() .prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(false).setIndices("test-idx").get(); disruption.startDisrupting(); logger.info("--> restarting data node, which should cause primary shards to be failed"); internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK); logger.info("--> wait for shard snapshots to show as failed"); assertBusy(() -> assertThat( client().admin().cluster().prepareSnapshotStatus("test-repo").setSnapshots("test-snap").get().getSnapshots() .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS); unblockNode("test-repo", dataNode); disruption.stopDisrupting(); // check that snapshot completes assertBusy(() -> { GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster() .prepareGetSnapshots("test-repo").setSnapshots("test-snap").setIgnoreUnavailable(true).get(); assertEquals(1, snapshotsStatusResponse.getSnapshots("test-repo").size()); SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots("test-repo").get(0); assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed()); }, 60L, TimeUnit.SECONDS); }	before this change we would sometimes unblock the node and stop the disruption before the first shard failure. i think this change makes the test weaker. i'm guessing it's invalid to do this after disruption.stopdisrupting()? if so, can we for instance only do it sometimes (with a comment saying why we don't always do it)?
static <T> T deepCopyParams(T original) { T clone; if (original instanceof Map) { Map<?, ?> originalMap = (Map<?, ?>) original; Map<Object, Object> clonedMap = new HashMap<>(); for (Map.Entry<?, ?> e : originalMap.entrySet()) { clonedMap.put(deepCopyParams(e.getKey()), deepCopyParams(e.getValue())); } clone = (T) clonedMap; } else if (original instanceof List) { List<?> originalList = (List<?>) original; List<Object> clonedList = new ArrayList<>(); for (Object o : originalList) { clonedList.add(deepCopyParams(o)); } clone = (T) clonedList; } else if (original instanceof String || original instanceof Integer || original instanceof Long || original instanceof Short || original instanceof Byte || original instanceof Float || original instanceof Double || original instanceof Character || original instanceof Boolean) { clone = original; } else { throw new IllegalArgumentException( "Can only clone primitives, String, ArrayList, and HashMap. Found: " + original.getClass().getCanonicalName() ); } return clone; }	does changing the exception type here preserve the http status code if we hit this error? if not, that might be a breaking change. i think i saw a couple of other exception type changes, same question applies.
public void testDeleteOnlyShouldNotMakeIndexReadonly() throws Exception { createIndexWithSettings(index, Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)); createNewSingletonPolicy("delete", new DeleteAction(), TimeValue.timeValueHours(1)); updatePolicy(index, policy); assertBusy(() -> { assertThat(getStepKeyForIndex(index).getAction(), equalTo("complete")); Map<String, Object> settings = getOnlyIndexSettings(index); assertThat(settings.get(IndexMetaData.INDEX_BLOCKS_WRITE_SETTING.getKey()), not("true")); }); }	can you index a doc here to make sure it goes through okay? checking the settings is good but it might be nice to be paranoid :)
public void testNodesUpdatedAfterClusterStatePublished() throws Exception { ThreadPool threadPool = new TestThreadPool(getClass().getName()); Settings settings = Settings.builder() // randomly make minimum_master_nodes a value higher than we have nodes for, so it will force failure .put(DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), randomBoolean() ? "3" : "1").build(); Map<String, MockNode> nodes = new HashMap<>(); ZenDiscovery zenDiscovery = null; ClusterService clusterService = null; try { Set<DiscoveryNode> expectedFDNodes = null; // create master node and its mocked up services MockNode master = createMockNode("master", settings, null, threadPool, logger, nodes).setAsMaster(); ClusterState state = master.clusterState; // initial cluster state // build the zen discovery and cluster service clusterService = createClusterService(threadPool, master.discoveryNode); setState(clusterService, state); zenDiscovery = buildZenDiscovery(settings, master, clusterService, threadPool); // a new cluster state with a new discovery node (we will test if the cluster state // was updated by the presence of this node in NodesFaultDetection) MockNode newNode = createMockNode("new_node", settings, null, threadPool, logger, nodes); ClusterState newState = ClusterState.builder(state).incrementVersion().nodes( DiscoveryNodes.builder(state.nodes()).add(newNode.discoveryNode).masterNodeId(master.discoveryNode.getId()) ).build(); try { // publishing a new cluster state ClusterChangedEvent clusterChangedEvent = new ClusterChangedEvent("testing", newState, state); AssertingAckListener listener = new AssertingAckListener(newState.nodes().getSize() - 1); expectedFDNodes = zenDiscovery.getFaultDetectionNodes(); zenDiscovery.publish(clusterChangedEvent, listener); listener.await(1, TimeUnit.HOURS); // publish was a success, update expected FD nodes based on new cluster state expectedFDNodes = fdNodesForState(newState, master.discoveryNode); } catch (Discovery.FailedToCommitClusterStateException e) { // not successful, so expectedFDNodes above should remain what it was originally assigned } assertEquals(expectedFDNodes, zenDiscovery.getFaultDetectionNodes()); } finally { // clean close of transport service and publish action for each node zenDiscovery.close(); clusterService.close(); for (MockNode curNode : nodes.values()) { curNode.action.close(); curNode.service.close(); } terminate(threadPool); } }	can we capture whether we set min master node to 3 or 1 and check here that the exception is really expected?
public static void main(String[] args) throws IOException { Class<?> testClass = null; Class<?> integTestClass = null; String rootPathList = null; boolean skipIntegTestsInDisguise = false; boolean checkMainClasses = false; for (int i = 0; i < args.length; i++) { String arg = args[i]; switch (arg) { case "--test-class": testClass = loadClassWithoutInitializing(args[++i]); break; case "--integ-test-class": integTestClass = loadClassWithoutInitializing(args[++i]); break; case "--skip-integ-tests-in-disguise": skipIntegTestsInDisguise = true; break; case "--main": checkMainClasses = true; break; case "--": rootPathList = args[++i]; break; default: fail("unsupported argument '" + arg + "'"); } } NamingConventionsCheck check = new NamingConventionsCheck(testClass, integTestClass); for (String rootDir : rootPathList.split(Pattern.quote(File.pathSeparator))) { Path rootPath = Paths.get(rootDir); if (checkMainClasses) { check.checkMain(rootPath); } else { check.checkTests(rootPath, skipIntegTestsInDisguise); } } // Now we should have no violations int exitCode = 0 ; exitCode += countAndPrintViolations( "Not all subclasses of " + check.testClass.getSimpleName() + " match the naming convention. Concrete classes must end with [Tests]", check.missingSuffix) ; exitCode += countAndPrintViolations( "Classes ending with [Tests] are abstract or interfaces", check.notRunnable ); exitCode += countAndPrintViolations( "Found inner classes that are tests, which are excluded from the test runner", check.innerClasses ); exitCode += countAndPrintViolations( "Pure Unit-Test found must subclass [" + check.testClass.getSimpleName() + "]", check.pureUnitTest ); exitCode += countAndPrintViolations( "Classes ending with [Tests] must subclass [" + check.testClass.getSimpleName() + "]", check.notImplementing ); exitCode += countAndPrintViolations( "Classes ending with [Tests] or [IT] or extending [" + check.testClass.getSimpleName() + "] must be in src/test/java", check.testsInMain ); if (skipIntegTestsInDisguise == false) { exitCode += countAndPrintViolations("Subclasses of " + check.integTestClass.getSimpleName() + " should end with IT as they are integration tests", check.integTestsInDisguise ); } System.exit(exitCode); }	i don't think we should call system.exit. instead, throw a gradleexception with a message indicating there were violations.
private long toHistogramKeyToEpoch(Object key) { if (key instanceof ZonedDateTime) { return ((ZonedDateTime)keEy).toInstant().toEpochMilli(); } else if (key instanceof Double) { return ((Double)key).longValue(); } else if (key instanceof Long) { return (Long)key; } else { throw new IllegalStateException("Histogram key [" + key + "] cannot be converted to a timestamp"); } }	i believe this introduced a typo: keey instead of key
private void registerAggregations(List<SearchPlugin> plugins) { registerAggregation(new AggregationSpec(AvgAggregationBuilder.NAME, AvgAggregationBuilder::new, AvgAggregationBuilder::parse) .addResultReader(InternalAvg::new)); registerAggregation(new AggregationSpec(WeightedAvgAggregationBuilder.NAME, WeightedAvgAggregationBuilder::new, WeightedAvgAggregationBuilder::parse).addResultReader(InternalWeightedAvg::new)); registerAggregation(new AggregationSpec(SumAggregationBuilder.NAME, SumAggregationBuilder::new, SumAggregationBuilder::parse) .addResultReader(InternalSum::new)); registerAggregation(new AggregationSpec(MinAggregationBuilder.NAME, MinAggregationBuilder::new, MinAggregationBuilder::parse) .addResultReader(InternalMin::new)); registerAggregation(new AggregationSpec(MaxAggregationBuilder.NAME, MaxAggregationBuilder::new, MaxAggregationBuilder::parse) .addResultReader(InternalMax::new)); registerAggregation(new AggregationSpec(StatsAggregationBuilder.NAME, StatsAggregationBuilder::new, StatsAggregationBuilder::parse) .addResultReader(InternalStats::new)); registerAggregation(new AggregationSpec(ExtendedStatsAggregationBuilder.NAME, ExtendedStatsAggregationBuilder::new, ExtendedStatsAggregationBuilder::parse).addResultReader(InternalExtendedStats::new)); registerAggregation(new AggregationSpec(ValueCountAggregationBuilder.NAME, ValueCountAggregationBuilder::new, ValueCountAggregationBuilder::parse).addResultReader(InternalValueCount::new)); registerAggregation(new AggregationSpec(PercentilesAggregationBuilder.NAME, PercentilesAggregationBuilder::new, PercentilesAggregationBuilder::parse) .addResultReader(InternalTDigestPercentiles.NAME, InternalTDigestPercentiles::new) .addResultReader(InternalHDRPercentiles.NAME, InternalHDRPercentiles::new)); registerAggregation(new AggregationSpec(PercentileRanksAggregationBuilder.NAME, PercentileRanksAggregationBuilder::new, PercentileRanksAggregationBuilder::parse) .addResultReader(InternalTDigestPercentileRanks.NAME, InternalTDigestPercentileRanks::new) .addResultReader(InternalHDRPercentileRanks.NAME, InternalHDRPercentileRanks::new)); registerAggregation(new AggregationSpec(MedianAbsoluteDeviationAggregationBuilder.NAME, MedianAbsoluteDeviationAggregationBuilder::new, MedianAbsoluteDeviationAggregationBuilder::parse) .addResultReader(InternalMedianAbsoluteDeviation::new)); registerAggregation(new AggregationSpec(CardinalityAggregationBuilder.NAME, CardinalityAggregationBuilder::new, CardinalityAggregationBuilder::parse).addResultReader(InternalCardinality::new)); registerAggregation(new AggregationSpec(GlobalAggregationBuilder.NAME, GlobalAggregationBuilder::new, GlobalAggregationBuilder::parse).addResultReader(InternalGlobal::new)); registerAggregation(new AggregationSpec(MissingAggregationBuilder.NAME, MissingAggregationBuilder::new, MissingAggregationBuilder::parse).addResultReader(InternalMissing::new)); registerAggregation(new AggregationSpec(FilterAggregationBuilder.NAME, FilterAggregationBuilder::new, FilterAggregationBuilder::parse).addResultReader(InternalFilter::new)); registerAggregation(new AggregationSpec(FiltersAggregationBuilder.NAME, FiltersAggregationBuilder::new, FiltersAggregationBuilder::parse).addResultReader(InternalFilters::new)); registerAggregation(new AggregationSpec(AdjacencyMatrixAggregationBuilder.NAME, AdjacencyMatrixAggregationBuilder::new, AdjacencyMatrixAggregationBuilder::parse).addResultReader(InternalAdjacencyMatrix::new)); registerAggregation(new AggregationSpec(SamplerAggregationBuilder.NAME, SamplerAggregationBuilder::new, SamplerAggregationBuilder::parse) .addResultReader(InternalSampler.NAME, InternalSampler::new) .addResultReader(UnmappedSampler.NAME, UnmappedSampler::new)); registerAggregation(new AggregationSpec(DiversifiedAggregationBuilder.NAME, DiversifiedAggregationBuilder::new, DiversifiedAggregationBuilder::parse) /* Reuses result readers from SamplerAggregator*/); registerAggregation(new AggregationSpec(TermsAggregationBuilder.NAME, TermsAggregationBuilder::new, TermsAggregationBuilder::parse) .addResultReader(StringTerms.NAME, StringTerms::new) .addResultReader(UnmappedTerms.NAME, UnmappedTerms::new) .addResultReader(LongTerms.NAME, LongTerms::new) .addResultReader(DoubleTerms.NAME, DoubleTerms::new)); registerAggregation(new AggregationSpec(RareTermsAggregationBuilder.NAME, RareTermsAggregationBuilder::new, RareTermsAggregationBuilder::parse) .addResultReader(StringRareTerms.NAME, StringRareTerms::new) .addResultReader(UnmappedRareTerms.NAME, UnmappedRareTerms::new) .addResultReader(LongRareTerms.NAME, LongRareTerms::new) .addResultReader(DoubleRareTerms.NAME, DoubleRareTerms::new)); registerAggregation(new AggregationSpec(SignificantTermsAggregationBuilder.NAME, SignificantTermsAggregationBuilder::new, SignificantTermsAggregationBuilder.getParser(significanceHeuristicParserRegistry)) .addResultReader(SignificantStringTerms.NAME, SignificantStringTerms::new) .addResultReader(SignificantLongTerms.NAME, SignificantLongTerms::new) .addResultReader(UnmappedSignificantTerms.NAME, UnmappedSignificantTerms::new)); registerAggregation(new AggregationSpec(SignificantTextAggregationBuilder.NAME, SignificantTextAggregationBuilder::new, SignificantTextAggregationBuilder.getParser(significanceHeuristicParserRegistry))); registerAggregation(new AggregationSpec(RangeAggregationBuilder.NAME, RangeAggregationBuilder::new, RangeAggregationBuilder::parse).addResultReader(InternalRange::new)); registerAggregation(new AggregationSpec(DateRangeAggregationBuilder.NAME, DateRangeAggregationBuilder::new, DateRangeAggregationBuilder::parse).addResultReader(InternalDateRange::new)); registerAggregation(new AggregationSpec(IpRangeAggregationBuilder.NAME, IpRangeAggregationBuilder::new, IpRangeAggregationBuilder::parse).addResultReader(InternalBinaryRange::new)); registerAggregation(new AggregationSpec(HistogramAggregationBuilder.NAME, HistogramAggregationBuilder::new, HistogramAggregationBuilder::parse).addResultReader(InternalHistogram::new)); registerAggregation(new AggregationSpec(DateHistogramAggregationBuilder.NAME, DateHistogramAggregationBuilder::new, DateHistogramAggregationBuilder::parse).addResultReader(InternalDateHistogram::new)); registerAggregation(new AggregationSpec(AutoDateHistogramAggregationBuilder.NAME, AutoDateHistogramAggregationBuilder::new, AutoDateHistogramAggregationBuilder::parse).addResultReader(InternalAutoDateHistogram::new)); registerAggregation(new AggregationSpec(GeoDistanceAggregationBuilder.NAME, GeoDistanceAggregationBuilder::new, GeoDistanceAggregationBuilder::parse).addResultReader(InternalGeoDistance::new)); registerAggregation(new AggregationSpec(GeoGridAggregationBuilder.NAME, GeoGridAggregationBuilder::new, GeoGridAggregationBuilder::parse).addResultReader(InternalGeoHashGrid::new)); registerAggregation(new AggregationSpec(NestedAggregationBuilder.NAME, NestedAggregationBuilder::new, NestedAggregationBuilder::parse).addResultReader(InternalNested::new)); registerAggregation(new AggregationSpec(ReverseNestedAggregationBuilder.NAME, ReverseNestedAggregationBuilder::new, ReverseNestedAggregationBuilder::parse).addResultReader(InternalReverseNested::new)); registerAggregation(new AggregationSpec(TopHitsAggregationBuilder.NAME, TopHitsAggregationBuilder::new, TopHitsAggregationBuilder::parse).addResultReader(InternalTopHits::new)); registerAggregation(new AggregationSpec(GeoBoundsAggregationBuilder.NAME, GeoBoundsAggregationBuilder::new, GeoBoundsAggregationBuilder::parse).addResultReader(InternalGeoBounds::new)); registerAggregation(new AggregationSpec(GeoCentroidAggregationBuilder.NAME, GeoCentroidAggregationBuilder::new, GeoCentroidAggregationBuilder::parse).addResultReader(InternalGeoCentroid::new)); registerAggregation(new AggregationSpec(ScriptedMetricAggregationBuilder.NAME, ScriptedMetricAggregationBuilder::new, ScriptedMetricAggregationBuilder::parse).addResultReader(InternalScriptedMetric::new)); registerAggregation((new AggregationSpec(CompositeAggregationBuilder.NAME, CompositeAggregationBuilder::new, CompositeAggregationBuilder::parse).addResultReader(InternalComposite::new))); registerFromPlugin(plugins, SearchPlugin::getAggregations, this::registerAggregation); }	nit: can we make the indentation here match the indentation of the other entries?
public void testIncompatibleDiffResendsFullState() { final Cluster cluster = new Cluster(randomIntBetween(3, 5)); cluster.runRandomly(); cluster.stabilise(); final ClusterNode leader = cluster.getAnyLeader(); final ClusterNode follower = cluster.getAnyNodeExcept(leader); logger.info("--> blackholing {}", follower); follower.blackhole(); final PublishClusterStateStats prePublishStats = follower.coordinator.stats().getPublishStats(); logger.info("--> submitting first value to {}", leader); leader.submitValue(randomLong()); cluster.runFor(DEFAULT_CLUSTER_STATE_UPDATE_DELAY, "publish first state"); logger.info("--> healing {}", follower); follower.heal(); logger.info("--> submitting second value to {}", leader); leader.submitValue(randomLong()); cluster.stabilise(); final PublishClusterStateStats postPublishStats = follower.coordinator.stats().getPublishStats(); assertEquals(prePublishStats.getFullClusterStateReceivedCount() + 1, postPublishStats.getFullClusterStateReceivedCount()); assertEquals(prePublishStats.getCompatibleClusterStateDiffReceivedCount(), postPublishStats.getCompatibleClusterStateDiffReceivedCount()); assertEquals(prePublishStats.getIncompatibleClusterStateDiffReceivedCount() + 1, postPublishStats.getIncompatibleClusterStateDiffReceivedCount()); }	i think we should have an explicit duration here - we do not expect there to be an election so it should not be necessary to wait for a full stabilisation.
public static Object process(Object truncateTo, Object timestamp, ZoneId zoneId) { if (truncateTo == null || timestamp == null) { return null; } if (truncateTo instanceof String == false) { throw new SqlIllegalArgumentException("A string is required; received [{}]", truncateTo); } Part truncateDateField = Part.resolve((String) truncateTo); if (truncateDateField == null) { List<String> similar = Part.findSimilar((String) truncateTo); if (similar.isEmpty()) { throw new SqlIllegalArgumentException("A value of {} or their aliases is required; received [{}]", Part.values(), truncateTo); } else { throw new SqlIllegalArgumentException("Received value [{}] is not valid date part for truncation; " + "did you mean {}?", truncateTo, similar); } } if (timestamp instanceof ZonedDateTime == false && timestamp instanceof IntervalYearMonth == false && timestamp instanceof IntervalDayTime == false) { throw new SqlIllegalArgumentException("A date/datetime/interval is required; received [{}]", timestamp); } if (truncateDateField == Part.WEEK && (timestamp instanceof IntervalDayTime || timestamp instanceof IntervalYearMonth)) { throw new SqlIllegalArgumentException("Truncating intervals is not supported for {} units", truncateTo); } if (timestamp instanceof ZonedDateTime) { return truncateDateField.truncate(((ZonedDateTime) timestamp).withZoneSameInstant(zoneId)); } else if (timestamp instanceof IntervalYearMonth) { return truncateDateField.truncate((IntervalYearMonth) timestamp); } else { return truncateDateField.truncate((IntervalDayTime) timestamp); } }	this truncateto can sit on the previous line.
public void testLoadMalformedWatchRecord() throws Exception { client().prepareIndex(Watch.INDEX, Watch.DOC_TYPE, "_id") .setSource(jsonBuilder().startObject() .startObject(WatchField.TRIGGER.getPreferredName()) .startObject("schedule") .field("cron", "0/5 * * * * ? 2050") .endObject() .endObject() .startObject(WatchField.ACTIONS.getPreferredName()) .endObject() .endObject()) .get(); // valid watch record: ZonedDateTime now = ZonedDateTime.now(ZoneOffset.UTC); Wid wid = new Wid("_id", now); ScheduleTriggerEvent event = new ScheduleTriggerEvent("_id", now, now); ExecutableCondition condition = InternalAlwaysCondition.INSTANCE; String index = HistoryStoreField.getHistoryIndexNameForTime(now); client().prepareIndex(index, HistoryStore.DOC_TYPE, wid.value()) .setSource(jsonBuilder().startObject() .startObject(WatchRecord.TRIGGER_EVENT.getPreferredName()) .field(event.type(), event) .endObject() .startObject(WatchField.CONDITION.getPreferredName()) .field(condition.type(), condition) .endObject() .startObject(WatchField.INPUT.getPreferredName()) .startObject("none").endObject() .endObject() .endObject()) .setWaitForActiveShards(ActiveShardCount.ALL) .setRefreshPolicy(IMMEDIATE) .get(); // unknown condition: wid = new Wid("_id", now); client().prepareIndex(index, HistoryStore.DOC_TYPE, wid.value()) .setSource(jsonBuilder().startObject() .startObject(WatchRecord.TRIGGER_EVENT.getPreferredName()) .field(event.type(), event) .endObject() .startObject(WatchField.CONDITION.getPreferredName()) .startObject("unknown").endObject() .endObject() .startObject(WatchField.INPUT.getPreferredName()) .startObject("none").endObject() .endObject() .endObject()) .setWaitForActiveShards(ActiveShardCount.ALL) .setRefreshPolicy(IMMEDIATE) .get(); // unknown trigger: wid = new Wid("_id", now); client().prepareIndex(index, HistoryStore.DOC_TYPE, wid.value()) .setSource(jsonBuilder().startObject() .startObject(WatchRecord.TRIGGER_EVENT.getPreferredName()) .startObject("unknown").endObject() .endObject() .startObject(WatchField.CONDITION.getPreferredName()) .field(condition.type(), condition) .endObject() .startObject(WatchField.INPUT.getPreferredName()) .startObject("none").endObject() .endObject() .endObject()) .setWaitForActiveShards(ActiveShardCount.ALL) .setRefreshPolicy(IMMEDIATE) .get(); stopWatcher(); startWatcher(); assertBusy(() -> { WatcherStatsResponse response = watcherClient().prepareWatcherStats().get(); assertThat(response.getWatchesCount(), equalTo(1L)); }); }	this test would fail approximately 1 in 50 times due to this bug. the final assertbusy would fail because the returned getwatchescount() would be 1 less than expected (numwatches) due to the size of the internal map becoming corrupt. if you iterated through the map and explicitly counted the number of items, it was correct, but the size() returned an incorrect value.
@Override public void validateSearchContext(SearchContext context, TransportRequest transportRequest) { Searcher engineSearcher = context.searcher().getEngineSearcher(); LazyDirectoryReader lazyDirectoryReader = unwrapLazyReader(engineSearcher.getDirectoryReader()); if (lazyDirectoryReader != null) { try { lazyDirectoryReader.reset(); } catch (IOException e) { throw new UncheckedIOException(e); } } // also register a release resource in this case if we have multiple roundtrips like in DFS onNewContext(context); }	nit: maybe only call this if we have a lazydirectoryreader and the rest was successful? slightly easier to follow the logic.
private TermVectorsResponse.TermVector randomTermVector(boolean hasFieldStatistics, boolean hasTermStatistics, boolean hasScores, boolean hasOffsets, boolean hasPositions, boolean hasPayloads) { TermVectorsResponse.TermVector.FieldStatistics fs = null; if (hasFieldStatistics) { long sumDocFreq = randomNonNegativeLong(); int docCount = randomInt(1000); long sumTotalTermFreq = randomNonNegativeLong(); fs = new TermVectorsResponse.TermVector.FieldStatistics(sumDocFreq, docCount, sumTotalTermFreq); } int termsCount = randomIntBetween(1, 5); List<TermVectorsResponse.TermVector.Term> terms = new ArrayList<>(termsCount); for (int i = 0; i < termsCount; i++) { terms.add(randomTerm(hasTermStatistics, hasScores, hasOffsets, hasPositions, hasPayloads)); } TermVectorsResponse.TermVector tv = new TermVectorsResponse.TermVector("field" + randomAlphaOfLength(7), fs, terms); return tv; }	i think it'd be nice to check for duplicated before adding to the list and retry. or to pass in a unique prefix to this method to be sure we don't have duplicates.
public static SnapshotLifecyclePolicy randomSnapshotLifecyclePolicy(String id) { Map<String, Object> config = new HashMap<>(); for (int i = 0; i < randomIntBetween(2, 5); i++) { config.put(randomAlphaOfLength(4), randomAlphaOfLength(4)); } return new SnapshotLifecyclePolicy(id, randomAlphaOfLength(4), randomSchedule(), randomAlphaOfLength(4), config, randomRetention()); }	this is pretty nitpicky but this method can be reused between this and snapshotlifecyclepolicymetadatatests instead of copy/pasted. if the base is in core i think it could also be reused in snapshotlifecyclepolicytests.
public static SnapshotLifecyclePolicy randomSnapshotLifecyclePolicy(String id) { Map<String, Object> config = new HashMap<>(); for (int i = 0; i < randomIntBetween(2, 5); i++) { config.put(randomAlphaOfLength(4), randomAlphaOfLength(4)); } return new SnapshotLifecyclePolicy(id, randomAlphaOfLength(4), randomSchedule(), randomAlphaOfLength(4), config, randomRetention()); }	i think this should occasionally return an empty configuration - doing so would have caught the equals npe bug i also commented on.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeStringArray(indices); out.writeVInt(mappings.size()); for (ObjectObjectCursor<String, MappingMetadata> indexEntry : mappings) { out.writeString(indexEntry.key); if (out.getVersion().before(Version.V_8_0_0)) { out.writeVInt(indexEntry.value == MappingMetadata.EMPTY_MAPPINGS ? 0 : 1); if (indexEntry.value != MappingMetadata.EMPTY_MAPPINGS) { out.writeString(MapperService.SINGLE_MAPPING_NAME); indexEntry.value.writeTo(out); } } else { out.writeBoolean(indexEntry.value != MappingMetadata.EMPTY_MAPPINGS); if (indexEntry.value != MappingMetadata.EMPTY_MAPPINGS) { indexEntry.value.writeTo(out); } } } out.writeVInt(aliases.size()); for (ObjectObjectCursor<String, List<AliasMetadata>> indexEntry : aliases) { out.writeString(indexEntry.key); out.writeVInt(indexEntry.value.size()); for (AliasMetadata aliasEntry : indexEntry.value) { aliasEntry.writeTo(out); } } out.writeVInt(settings.size()); for (ObjectObjectCursor<String, Settings> indexEntry : settings) { out.writeString(indexEntry.key); Settings.writeSettingsToStream(indexEntry.value, out); } out.writeVInt(defaultSettings.size()); for (ObjectObjectCursor<String, Settings> indexEntry : defaultSettings) { out.writeString(indexEntry.key); Settings.writeSettingsToStream(indexEntry.value, out); } out.writeVInt(dataStreams.size()); for (ObjectObjectCursor<String, String> indexEntry : dataStreams) { out.writeString(indexEntry.key); out.writeOptionalString(indexEntry.value); } }	same applies for this serialization logic.
private void mergeBucketsWithPlan(List<Bucket> buckets, List<BucketRange> plan, ReduceContext reduceContext){ for(int i = plan.size() - 1; i >= 0; i--) { BucketRange range = plan.get(i); int endIdx = range.endIdx; int startIdx = range.startIdx; if(startIdx == endIdx) continue; List<Bucket> toMerge = new ArrayList<>(); for(int idx = endIdx; idx > startIdx; idx--){ toMerge.add(buckets.get(idx)); buckets.remove(idx); } toMerge.add(buckets.get(startIdx)); // Don't remove the startIdx bucket because it will be replaced by the merged bucket int toRemove = toMerge.stream().mapToInt(b -> countInnerBucket(b)+1).sum(); reduceContext.consumeBucketsAndMaybeBreak(-toRemove + 1); Bucket merged_bucket = reduceBucket(toMerge, reduceContext); buckets.set(startIdx, merged_bucket); } }	is this an actual bug you found while making this change?
void connect(ActionListener<Void> connectListener) { final boolean runConnect; final ActionListener<Void> listener = ContextPreservingActionListener.wrapPreservingContext(connectListener, threadPool.getThreadContext()); synchronized (mutex) { if (closed.get()) { if (connectListener != null) { connectListener.onFailure(new AlreadyClosedException("connect handler is already closed")); } return; } if (queue.offer(listener) == false) { listener.onFailure(new RejectedExecutionException("connect queue is full")); return; } runConnect = queue.size() == 1; } if (runConnect) { ExecutorService executor = threadPool.executor(ThreadPool.Names.MANAGEMENT); executor.submit(new AbstractRunnable() { @Override public void onFailure(Exception e) { final Collection<ActionListener<Void>> toNotify = new ArrayList<>(); synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onFailure(toNotify, e); } @Override protected void doRun() { final Collection<ActionListener<Void>> toNotify = new ArrayList<>(); ActionListener<Void> listener1 = ActionListener.wrap((x) -> { synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onResponse(toNotify, x); }, (e) -> { synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onFailure(toNotify, e); }); collectRemoteNodes(seedNodes.stream().map(Tuple::v2).iterator(), listener1); } }); } }	do we even need these synchronized blocks? drainto is atomic anyway so the mechanics of runconnect = queue.size() == 1; shouldn't be affected? (imo not, we're using runconnect outside the sychronized block anyway) the same even holds true for close it seems, since we're not guarding what's inside if (runconnect) { against a concurrent close it seems to me all but the synchronized block above https://github.com/elastic/elasticsearch/pull/44825/files#diff-d390b91a2cae82a4f90f1386f8428695r373 are technically redundant?
void connect(ActionListener<Void> connectListener) { final boolean runConnect; final ActionListener<Void> listener = ContextPreservingActionListener.wrapPreservingContext(connectListener, threadPool.getThreadContext()); synchronized (mutex) { if (closed.get()) { if (connectListener != null) { connectListener.onFailure(new AlreadyClosedException("connect handler is already closed")); } return; } if (queue.offer(listener) == false) { listener.onFailure(new RejectedExecutionException("connect queue is full")); return; } runConnect = queue.size() == 1; } if (runConnect) { ExecutorService executor = threadPool.executor(ThreadPool.Names.MANAGEMENT); executor.submit(new AbstractRunnable() { @Override public void onFailure(Exception e) { final Collection<ActionListener<Void>> toNotify = new ArrayList<>(); synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onFailure(toNotify, e); } @Override protected void doRun() { final Collection<ActionListener<Void>> toNotify = new ArrayList<>(); ActionListener<Void> listener1 = ActionListener.wrap((x) -> { synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onResponse(toNotify, x); }, (e) -> { synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onFailure(toNotify, e); }); collectRemoteNodes(seedNodes.stream().map(Tuple::v2).iterator(), listener1); } }); } }	nit: maybe just inline listener1, imo that's much easier to read here.
void connect(ActionListener<Void> connectListener) { final boolean runConnect; final ActionListener<Void> listener = ContextPreservingActionListener.wrapPreservingContext(connectListener, threadPool.getThreadContext()); synchronized (mutex) { if (closed.get()) { if (connectListener != null) { connectListener.onFailure(new AlreadyClosedException("connect handler is already closed")); } return; } if (queue.offer(listener) == false) { listener.onFailure(new RejectedExecutionException("connect queue is full")); return; } runConnect = queue.size() == 1; } if (runConnect) { ExecutorService executor = threadPool.executor(ThreadPool.Names.MANAGEMENT); executor.submit(new AbstractRunnable() { @Override public void onFailure(Exception e) { final Collection<ActionListener<Void>> toNotify = new ArrayList<>(); synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onFailure(toNotify, e); } @Override protected void doRun() { final Collection<ActionListener<Void>> toNotify = new ArrayList<>(); ActionListener<Void> listener1 = ActionListener.wrap((x) -> { synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onResponse(toNotify, x); }, (e) -> { synchronized (mutex) { queue.drainTo(toNotify); } ActionListener.onFailure(toNotify, e); }); collectRemoteNodes(seedNodes.stream().map(Tuple::v2).iterator(), listener1); } }); } }	this isn't introduced here, but the way we handle the listeners seems strange in case a listener throws an exception this happens: say we have three listeners and listener1.onresponse is called. now the first two listeners do their thing when iterating over them, but the third one throws in its onresponse method. => actionlistener.onresponse will throw the exception from the last listener, just to the onfailure method of listener1 and invoke the first two listeners again since they're still in tonotify? that seems weird doesn't it?
private void handleNodes(Iterator<DiscoveryNode> nodesIter) { while (nodesIter.hasNext()) { final DiscoveryNode node = maybeAddProxyAddress(proxyAddress, nodesIter.next()); if (nodePredicate.test(node) && connectionManager.size() < maxNumRemoteConnections) { connectionManager.connectToNode(node, null, transportService.connectionValidator(node), new ActionListener<>() { @Override public void onResponse(Void aVoid) { handleNodes(nodesIter); } @Override public void onFailure(Exception e) { if (e instanceof ConnectTransportException || e instanceof IllegalStateException) { // ISE if we fail the handshake with an version incompatible node // fair enough we can't connect just move on logger.debug(() -> new ParameterizedMessage("failed to connect to node {}", node), e); handleNodes(nodesIter); } else { logger.warn(() -> new ParameterizedMessage("fetching nodes from external cluster {} failed", clusterAlias), e); IOUtils.closeWhileHandlingException(connection); collectRemoteNodes(seedNodes, listener); } } }); return; } } // we have to close this connection before we notify listeners - this is mainly needed for test correctness // since if we do it afterwards we might fail assertions that check if all high level connections are closed. // from a code correctness perspective we could also close it afterwards. This try/with block will // maintain the possibly exceptions thrown from within the try block and suppress the ones that are possible thrown // by closing the connection IOUtils.closeWhileHandlingException(connection); listener.onResponse(null); }	this comment could use some love :) we don't have a try-with block anymore and can probably just delete the last sentence. nit: maybe capitalize the sentence beginnings?
public synchronized void afterIndexShardClosed(ShardId shardId, @Nullable IndexShard indexShard, Settings indexSettings) { if (indexShard != null) { HashSet<String> sessions = sessionsForShard.remove(indexShard); if (sessions != null) { for (String sessionUUID : sessions) { RestoreSession restore = onGoingRestores.remove(sessionUUID); restore.decRef(); } } } }	can this return null?
public void testSuccess() throws InterruptedException { final String index = "test"; clusterService.setState(stateWithStartedPrimary(index, true, randomInt(5))); String indexUUID = clusterService.state().metaData().index(index).getIndexUUID(); AtomicBoolean success = new AtomicBoolean(); CountDownLatch latch = new CountDownLatch(1); ShardRouting shardRouting = getRandomShardRouting(index); shardStateAction.shardFailed(shardRouting, indexUUID, "test", getSimulatedFailure(), new ShardStateAction.Listener() { @Override public void onSuccess() { success.set(true); latch.countDown(); } }); CapturingTransport.CapturedRequest[] capturedRequests = transport.getCapturedRequestsAndClear(); assertEquals(1, capturedRequests.length); // the request is a shard failed request assertThat(capturedRequests[0].request, is(instanceOf(ShardStateAction.ShardRoutingEntry.class))); ShardStateAction.ShardRoutingEntry shardRoutingEntry = (ShardStateAction.ShardRoutingEntry)capturedRequests[0].request; // for the right shard assertEquals(shardRouting, shardRoutingEntry.getShardRouting()); // sent to the master assertEquals(clusterService.state().nodes().masterNode().getId(), capturedRequests[0].node.getId()); transport.handleResponse(capturedRequests[0].requestId, TransportResponse.Empty.INSTANCE); latch.await(); assertTrue(success.get()); }	can we make sure onshardfailedfailure is not called?
private Cipher createCipher(int opmode, char[] password, byte[] salt, byte[] iv) throws GeneralSecurityException { PBEKeySpec keySpec = new PBEKeySpec(password, salt, KDF_ITERS, CIPHER_KEY_BITS); SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(KDF_ALGO); SecretKey secretKey; try { secretKey = keyFactory.generateSecret(keySpec); } catch (AssertionError ae) { throw new GeneralSecurityException("Error generating an encryption key from the provided password. ", ae); } SecretKeySpec secret = new SecretKeySpec(secretKey.getEncoded(), CIPHER_ALGO); GCMParameterSpec spec = new GCMParameterSpec(GCM_TAG_BITS, iv); Cipher cipher = Cipher.getInstance(CIPHER_ALGO + "/" + CIPHER_MODE + "/" + CIPHER_PADDING); cipher.init(opmode, secret, spec); cipher.updateAAD(salt); return cipher; }	i wonder whether we could just be very liberal and catch a throwable here. so we are sure nothing is going to trip it regardless of the provider.
private Cipher createCipher(int opmode, char[] password, byte[] salt, byte[] iv) throws GeneralSecurityException { PBEKeySpec keySpec = new PBEKeySpec(password, salt, KDF_ITERS, CIPHER_KEY_BITS); SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(KDF_ALGO); SecretKey secretKey; try { secretKey = keyFactory.generateSecret(keySpec); } catch (AssertionError ae) { throw new GeneralSecurityException("Error generating an encryption key from the provided password. ", ae); } SecretKeySpec secret = new SecretKeySpec(secretKey.getEncoded(), CIPHER_ALGO); GCMParameterSpec spec = new GCMParameterSpec(GCM_TAG_BITS, iv); Cipher cipher = Cipher.getInstance(CIPHER_ALGO + "/" + CIPHER_MODE + "/" + CIPHER_PADDING); cipher.init(opmode, secret, spec); cipher.updateAAD(salt); return cipher; }	we throw elasticsearchexception in other places while throwing generalsecurityexception here. do we want to be consistent? nit: extra space (and the period?) at the end of the error message string :)
public HasChildQueryBuilder minMaxChildren(int minChildren, int maxChildren) { if (minChildren < 1) { throw new IllegalArgumentException("[" + NAME + "] requires non-negative, non-zero 'min_children' field"); } if (maxChildren < 0) { throw new IllegalArgumentException("[" + NAME + "] requires non-negative 'max_children' field"); } if (maxChildren < minChildren) { throw new IllegalArgumentException("[" + NAME + "] 'max_children' is less than 'min_children'"); } this.minChildren = minChildren; this.maxChildren = maxChildren; return this; }	instead of "non-negative, non-zero", we can just use "positive"
@Override public String[] indices() { final PointInTimeBuilder pit = pointInTimeBuilder(); if (pit != null) { return pit.getActualIndices(); } else { return indices; } }	shall we assert here that indices is empty?
public static void parseSearchRequest( SearchRequest searchRequest, RestRequest request, XContentParser requestContentParser, NamedWriteableRegistry namedWriteableRegistry, IntConsumer setSize, BiConsumer<RestRequest, SearchRequest> extraParamParser ) throws IOException { if (request.getRestApiVersion() == RestApiVersion.V_7 && request.hasParam("type")) { request.param("type"); deprecationLogger.compatibleCritical("search_with_types", TYPES_DEPRECATION_MESSAGE); } if (searchRequest.source() == null) { searchRequest.source(new SearchSourceBuilder()); } searchRequest.indices(Strings.splitStringByCommaToArray(request.param("index"))); if (requestContentParser != null) { searchRequest.source().parseXContent(requestContentParser, true); } final int batchedReduceSize = request.paramAsInt("batched_reduce_size", searchRequest.getBatchedReduceSize()); searchRequest.setBatchedReduceSize(batchedReduceSize); if (request.hasParam("pre_filter_shard_size")) { searchRequest.setPreFilterShardSize(request.paramAsInt("pre_filter_shard_size", SearchRequest.DEFAULT_PRE_FILTER_SHARD_SIZE)); } if (request.hasParam("enable_fields_emulation")) { // this flag is a no-op from 8.0 on, we only want to consume it so its presence doesn't cause errors request.paramAsBoolean("enable_fields_emulation", false); } if (request.hasParam("max_concurrent_shard_requests")) { // only set if we have the parameter since we auto adjust the max concurrency on the coordinator // based on the number of nodes in the cluster final int maxConcurrentShardRequests = request.paramAsInt( "max_concurrent_shard_requests", searchRequest.getMaxConcurrentShardRequests() ); searchRequest.setMaxConcurrentShardRequests(maxConcurrentShardRequests); } if (request.hasParam("allow_partial_search_results")) { // only set if we have the parameter passed to override the cluster-level default searchRequest.allowPartialSearchResults(request.paramAsBoolean("allow_partial_search_results", null)); } searchRequest.searchType(request.param("search_type")); parseSearchSource(searchRequest.source(), request, setSize); searchRequest.requestCache(request.paramAsBoolean("request_cache", searchRequest.requestCache())); String scroll = request.param("scroll"); if (scroll != null) { searchRequest.scroll(new Scroll(parseTimeValue(scroll, null, "scroll"))); } searchRequest.routing(request.param("routing")); searchRequest.preference(request.param("preference")); final IndicesOptions indicesOptions = IndicesOptions.fromRequest(request, searchRequest.indicesOptions()); if (searchRequest.indicesOptions().equals(indicesOptions) == false) { searchRequest.indicesOptions(indicesOptions); } checkRestTotalHits(request, searchRequest); if (searchRequest.pointInTimeBuilder() != null) { if (request.paramAsBoolean("ccs_minimize_roundtrips", false)) { throw new IllegalArgumentException("[ccs_minimize_roundtrips] cannot be used with point in time"); } } else { searchRequest.setCcsMinimizeRoundtrips( request.paramAsBoolean("ccs_minimize_roundtrips", searchRequest.isCcsMinimizeRoundtrips()) ); } extraParamParser.accept(request, searchRequest); }	is this new conditional necessary? what if we set them anyway?
public void testCloseFreezeAndOpen() throws Exception { String indexName = "index"; createIndex(indexName, Settings.builder().put("index.number_of_shards", 2).build()); client().prepareIndex(indexName).setId("1").setSource("field", "value").setRefreshPolicy(IMMEDIATE).get(); client().prepareIndex(indexName).setId("2").setSource("field", "value").setRefreshPolicy(IMMEDIATE).get(); client().prepareIndex(indexName).setId("3").setSource("field", "value").setRefreshPolicy(IMMEDIATE).get(); assertAcked(client().execute(FreezeIndexAction.INSTANCE, new FreezeRequest(indexName)).actionGet()); expectThrows( ClusterBlockException.class, () -> client().prepareIndex(indexName).setId("4").setSource("field", "value").setRefreshPolicy(IMMEDIATE).get() ); IndicesService indexServices = getInstanceFromNode(IndicesService.class); Index index = resolveIndex(indexName); IndexService indexService = indexServices.indexServiceSafe(index); IndexShard shard = indexService.getShard(0); Engine engine = IndexShardTestCase.getEngine(shard); assertEquals(0, shard.refreshStats().getTotal()); boolean useDFS = randomBoolean(); assertHitCount( client().prepareSearch() .setIndicesOptions(IndicesOptions.STRICT_EXPAND_OPEN_FORBID_CLOSED) .setSearchType(useDFS ? SearchType.DFS_QUERY_THEN_FETCH : SearchType.QUERY_THEN_FETCH) .get(), 3 ); assertThat(engine, Matchers.instanceOf(FrozenEngine.class)); assertEquals(useDFS ? 3 : 2, shard.refreshStats().getTotal()); assertFalse(((FrozenEngine) engine).isReaderOpen()); assertTrue(indexService.getIndexSettings().isSearchThrottled()); // now scroll SearchResponse searchResponse = client().prepareSearch() .setIndicesOptions(IndicesOptions.STRICT_EXPAND_OPEN_FORBID_CLOSED) .setScroll(TimeValue.timeValueMinutes(1)) .setSize(1) .get(); do { assertHitCount(searchResponse, 3); assertEquals(1, searchResponse.getHits().getHits().length); SearchService searchService = getInstanceFromNode(SearchService.class); assertThat(searchService.getActiveContexts(), Matchers.greaterThanOrEqualTo(1)); for (int i = 0; i < 2; i++) { shard = indexService.getShard(i); engine = IndexShardTestCase.getEngine(shard); // scrolls keep the reader open assertTrue(((FrozenEngine) engine).isReaderOpen()); } searchResponse = client().prepareSearchScroll(searchResponse.getScrollId()).setScroll(TimeValue.timeValueMinutes(1)).get(); } while (searchResponse.getHits().getHits().length > 0); client().prepareClearScroll().addScrollId(searchResponse.getScrollId()).get(); String pitId = openReaders(TimeValue.timeValueMinutes(1), indexName); try { for (int from = 0; from < 3; from++) { searchResponse = client().prepareSearch().setPointInTime(new PointInTimeBuilder(pitId)).setSize(1).setFrom(from).get(); assertHitCount(searchResponse, 3); assertEquals(1, searchResponse.getHits().getHits().length); SearchService searchService = getInstanceFromNode(SearchService.class); assertThat(searchService.getActiveContexts(), Matchers.greaterThanOrEqualTo(1)); for (int i = 0; i < 2; i++) { shard = indexService.getShard(i); engine = IndexShardTestCase.getEngine(shard); assertFalse(((FrozenEngine) engine).isReaderOpen()); } } assertWarnings(TransportSearchAction.FROZEN_INDICES_DEPRECATION_MESSAGE.replace("{}", indexName)); } finally { client().execute(ClosePointInTimeAction.INSTANCE, new ClosePointInTimeRequest(pitId)).get(); } }	what is the effect of no longer setting the indices options here? they had no effect anyways?
private void injectPointInTimeIfNeeded( Tuple<String, SearchRequest> namedSearchRequest, ActionListener<Tuple<String, SearchRequest>> listener ) { SearchRequest searchRequest = namedSearchRequest.v2(); if (disablePit || searchRequest.indices().length == 0) { listener.onResponse(namedSearchRequest); return; } PointInTimeBuilder pit = namedPits.get(namedSearchRequest.v1()); if (pit != null) { searchRequest.indices(Strings.EMPTY_ARRAY); searchRequest.source().pointInTimeBuilder(pit); listener.onResponse(namedSearchRequest); return; } // no pit, create a new one OpenPointInTimeRequest pitRequest = new OpenPointInTimeRequest(searchRequest.indices()).keepAlive(PIT_KEEP_ALIVE); ClientHelper.executeWithHeadersAsync( transformConfig.getHeaders(), ClientHelper.TRANSFORM_ORIGIN, client, OpenPointInTimeAction.INSTANCE, pitRequest, ActionListener.wrap(response -> { PointInTimeBuilder newPit = new PointInTimeBuilder(response.getPointInTimeId()).setKeepAlive(PIT_KEEP_ALIVE); namedPits.put(namedSearchRequest.v1(), newPit); searchRequest.indices(Strings.EMPTY_ARRAY); searchRequest.source().pointInTimeBuilder(newPit); pitCheckpoint = getNextCheckpoint().getCheckpoint(); logger.trace( "[{}] using pit search context with id [{}]; request [{}]", getJobId(), newPit.getEncodedId(), namedSearchRequest.v1() ); listener.onResponse(namedSearchRequest); }, e -> { Throwable unwrappedException = ExceptionsHelper.findSearchExceptionRootCause(e); // if point in time is not supported, disable it but do not remember forever (stopping and starting will give it another // try) if (unwrappedException instanceof ActionNotFoundTransportException) { logger.warn( "[{}] source does not support point in time reader, falling back to normal search (more resource intensive)", getJobId() ); auditor.warning( getJobId(), "Source does not support point in time reader, falling back to normal search (more resource intensive)" ); disablePit = true; } else { logger.warn( new ParameterizedMessage( "[{}] Failed to create a point in time reader, falling back to normal search.", getJobId() ), e ); } listener.onResponse(namedSearchRequest); }) ); }	do we need to set empty indices here now?
@Override public synchronized void clusterChanged(ClusterChangedEvent event) { if (event.localNodeMaster()) { final var clusterState = event.state(); final var desiredNodes = DesiredNodes.latestFromClusterState(clusterState); if (desiredNodes == null) { return; } if (event.nodesChanged()) { final var nodesDelta = event.nodesDelta(); for (DiscoveryNode addedNode : nodesDelta.addedNodes()) { final var desiredNode = desiredNodes.find(addedNode.getExternalId()); if (desiredNode != null) { members.add(desiredNode); } } for (DiscoveryNode removedNode : nodesDelta.removedNodes()) { final var desiredNode = desiredNodes.find(removedNode.getExternalId()); if (desiredNode != null) { members.remove(desiredNode); } } } else if (event.changedCustomMetadataSet().contains(DesiredNodesMetadata.TYPE)) { if (desiredNodes.historyID().equals(latestHistoryId) == false) { members.clear(); } latestHistoryId = desiredNodes.historyID(); final Set<DesiredNode> unknownDesiredNodes = new HashSet<>(members); for (DiscoveryNode node : clusterState.nodes()) { final var desiredNode = desiredNodes.find(node.getExternalId()); if (desiredNode != null) { members.add(desiredNode); unknownDesiredNodes.remove(desiredNode); } } members.removeAll(unknownDesiredNodes); } } else if (event.previousState().nodes().isLocalNodeElectedMaster()) { members.clear(); } else { assert members.isEmpty(); } }	let us also remove this.
public static Optional<NodesShutdownMetadata> getShutdowns(final ClusterState state) { return Optional.ofNullable(state) .map(ClusterState::metadata) .map(m -> m.custom(TYPE)); }	should we not require state to be non-null here? seems like a bug to call with state==null?
* @param currentState the current {@link ClusterState} * @return a new {@link Assignment} */ private <Params extends PersistentTaskParams> Assignment createAssignment(final String taskName, final Params taskParams, final ClusterState currentState) { PersistentTasksExecutor<Params> persistentTasksExecutor = registry.getPersistentTaskExecutorSafe(taskName); AssignmentDecision decision = enableDecider.canAssign(); if (decision.getType() == AssignmentDecision.Type.NO) { return unassignedAssignment("persistent task [" + taskName + "] cannot be assigned [" + decision.getReason() + "]"); } // Filter all nodes that are marked as shutting down, because we do not // want to assign a persistent task to a node that will shortly be // leaving the cluster final List<DiscoveryNode> candidateNodes = currentState.nodes().mastersFirstStream() .filter(dn -> isNodeShuttingDown(currentState, dn.getId()) == false) .collect(Collectors.toList()); // Task assignment should not rely on node order Randomness.shuffle(candidateNodes); final Assignment assignment = persistentTasksExecutor.getAssignment(taskParams, candidateNodes, currentState); assert (assignment == null || isNodeShuttingDown(currentState, assignment.getExecutorNode()) == false) : "expected task [" + taskName + "] to be assigned to a node that is not marked as shutting down, but " + assignment.getExecutorNode() + " is currently marked as shutting down"; return assignment; }	nit: maybe we should add a stream method? seems confusing to ask for a masters first stream without using it...
@Override public PersistentTasksCustomMetadata.Assignment getAssignment(StartDatafeedAction.DatafeedParams params, Collection<DiscoveryNode> candidateNodes, ClusterState clusterState) { return new DatafeedNodeSelector(clusterState, resolver, params.getDatafeedId(), params.getJobId(), params.getDatafeedIndices(), params.getIndicesOptions()).selectNode(); }	this may belong in a subsequent pr, but perhaps selectnode() should filter the node based on candidatenodes to ensure the assertion in persistenttasksclusterservice is true? might just be me not seeing the light on how the jobtask is ensured to not have the shutting down node. could be worth a comment to explain.
@Override public void validate(OpenJobAction.JobParams params, ClusterState clusterState) { final Job job = params.getJob(); final String jobId = params.getJobId(); validateJobAndId(jobId, job); // If we already know that we can't find an ml node because all ml nodes are running at capacity or // simply because there are no ml nodes in the cluster then we fail quickly here: PersistentTasksCustomMetadata.Assignment assignment = getAssignment(params, clusterState.nodes().getAllNodes(), clusterState); if (assignment.equals(AWAITING_UPGRADE)) { throw makeCurrentlyBeingUpgradedException(logger, params.getJobId()); } if (assignment.getExecutorNode() == null && assignment.equals(AWAITING_LAZY_ASSIGNMENT) == false) { throw makeNoSuitableNodesException(logger, params.getJobId(), assignment.getExplanation()); } }	this is tricky, it is possible that this validation passes as all the possible assigning nodes are shutting down, but we don't catch that. the ml team might remove that last validation (awaiting_lazy_assignment) as now it is unreliable.
public void testGetProgress() throws Exception { createReviewsIndex(); SourceConfig sourceConfig = new SourceConfig(REVIEWS_INDEX_NAME); DestConfig destConfig = new DestConfig("unnecessary"); GroupConfig histgramGroupConfig = new GroupConfig(Collections.emptyMap(), Collections.singletonMap("every_50", new HistogramGroupSource("count", 50.0))); AggregatorFactories.Builder aggs = new AggregatorFactories.Builder(); aggs.addAggregator(AggregationBuilders.avg("avg_rating").field("stars")); AggregationConfig aggregationConfig = new AggregationConfig(Collections.emptyMap(), aggs); PivotConfig pivotConfig = new PivotConfig(histgramGroupConfig, aggregationConfig, null); DataFrameTransformConfig config = new DataFrameTransformConfig("get_progress_transform", sourceConfig, destConfig, null, pivotConfig, null); final RestHighLevelClient restClient = new TestRestHighLevelClient(); SearchResponse response = restClient.search(TransformProgressGatherer.getSearchRequest(config), RequestOptions.DEFAULT); DataFrameTransformProgress progress = TransformProgressGatherer.searchResponseToDataFrameTransformProgressFunction().apply(response); assertThat(progress.getTotalDocs(), equalTo(1000L)); assertThat(progress.getRemainingDocs(), equalTo(1000L)); assertThat(progress.getPercentComplete(), equalTo(0.0)); QueryConfig queryConfig = new QueryConfig(Collections.emptyMap(), QueryBuilders.termQuery("user_id", "user_26")); pivotConfig = new PivotConfig(histgramGroupConfig, aggregationConfig, null); sourceConfig = new SourceConfig(new String[]{REVIEWS_INDEX_NAME}, queryConfig); config = new DataFrameTransformConfig("get_progress_transform", sourceConfig, destConfig, null, pivotConfig, null); response = restClient.search(TransformProgressGatherer.getSearchRequest(config), RequestOptions.DEFAULT); progress = TransformProgressGatherer.searchResponseToDataFrameTransformProgressFunction().apply(response); assertThat(progress.getTotalDocs(), equalTo(35L)); assertThat(progress.getRemainingDocs(), equalTo(35L)); assertThat(progress.getPercentComplete(), equalTo(0.0)); histgramGroupConfig = new GroupConfig(Collections.emptyMap(), Collections.singletonMap("every_50", new HistogramGroupSource("missing_field", 50.0))); pivotConfig = new PivotConfig(histgramGroupConfig, aggregationConfig, null); config = new DataFrameTransformConfig("get_progress_transform", sourceConfig, destConfig, null, pivotConfig, null); response = restClient.search(TransformProgressGatherer.getSearchRequest(config), RequestOptions.DEFAULT); progress = TransformProgressGatherer.searchResponseToDataFrameTransformProgressFunction().apply(response); assertThat(progress.getTotalDocs(), equalTo(0L)); assertThat(progress.getRemainingDocs(), equalTo(0L)); assertThat(progress.getPercentComplete(), equalTo(100.0)); deleteIndex(REVIEWS_INDEX_NAME); }	does this method cover multiple independent test case scenarios? how hard would it be to split it into a few independent test cases?
public static SearchRequest getSearchRequest(DataFrameTransformConfig config) { SearchRequest request = new SearchRequest(config.getSource().getIndex()); request.allowPartialSearchResults(false); BoolQueryBuilder existsClauses = QueryBuilders.boolQuery(); config.getPivotConfig() .getGroupConfig() .getGroups() .values() // TODO change once we allow missing_buckets .forEach(src -> existsClauses.must(QueryBuilders.existsQuery(src.getField()))); request.source(new SearchSourceBuilder() .size(0) .trackTotalHits(true) .query(QueryBuilders.boolQuery() .filter(config.getSource().getQueryConfig().getQuery()) .filter(existsClauses))); return request; }	[optional] imo in this case the old good foreach syntax is both shorter and more readable: for (singlegroupsource src : config.getpivotconfig().getgroupconfig().getgroups().values()) { existsclauses.must(querybuilders.existsquery(src.getfield()))); } but leaving the choice up to you.
@SuppressForbidden(reason = "ok to open connection here") private static String getDefaultProjectId() throws IOException { String metaHost = System.getenv("GCE_METADATA_HOST"); if (metaHost == null) { metaHost = "metadata.google.internal"; } URL url = new URL("http://" + metaHost + "/computeMetadata/v1/project/project-id"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setConnectTimeout(5000); connection.setReadTimeout(5000); connection.setRequestProperty("Metadata-Flavor", "Google"); try (InputStream input = connection.getInputStream()) { if (connection.getResponseCode() == 200) { try (BufferedReader reader = new BufferedReader(new InputStreamReader(input, UTF_8))) { return reader.readLine(); } } } return null; }	i was wondering if there is ever a case where the [metadata server](https://cloud.google.com/compute/docs/storing-retrieving-metadata#querying) is not metadata.google.internal, but i suppose a) there is no guarantee this will remain for ever the same b) it might be useful esp. under additional layers under k8s etc.
static DeprecationIssue checkImplicitlyDisabledNativeRealms(final Settings settings, final PluginsAndModules pluginsAndModules) { final Map<RealmConfig.RealmIdentifier, Settings> realmSettings = RealmSettings.getRealmSettings(settings); if (realmSettings.isEmpty()) { return null; } // If all configured realms are disabled, this equals to no realm is configured. The implicitly behaviour in this case // is to add file and native realms. So we are good here. if (false == realmSettings.entrySet().stream().anyMatch( e -> e.getValue().getAsBoolean(RealmSettings.ENABLED_SETTING_KEY, true))) { return null; } final List<String> implicitlyDisabledNativeRealmTypes = new ArrayList<>(org.elasticsearch.common.collect.List.of(FileRealmSettings.TYPE, NativeRealmSettings.TYPE)); realmSettings.keySet().forEach(ri -> implicitlyDisabledNativeRealmTypes.remove(ri.getType())); if (implicitlyDisabledNativeRealmTypes.isEmpty()) { return null; } final String details = String.format( Locale.ROOT, "Found implicitly disabled native %s: [%s]. %s disabled because there are other explicitly configured realms." + "In next major release, native realms will always be enabled unless explicitly disabled.", implicitlyDisabledNativeRealmTypes.size() == 1 ? "realm" : "realms", Strings.collectionToDelimitedString(implicitlyDisabledNativeRealmTypes, ","), implicitlyDisabledNativeRealmTypes.size() == 1 ? "It is" : "They are"); return new DeprecationIssue( DeprecationIssue.Level.WARNING, "File and/or native realms are enabled by default in next major release.", "https://www.elastic.co/guide/en/elasticsearch/reference/7.13/deprecated-7.13.html#implicitly-disabled-native-realms", details ); }	are we good? what will this config do in 8.0? xpack.security.authc.realms: native.native_realm: enabled: false ldap.corp_ldap: enabled: false in 7.x it will result in both a file and native realm enabled (i think) i believe the proposal is that in 8.x it will result in just a file realm. should we issue a deprecation warning (because the behaviour will change) in this case too? (a separate pr is fine though)
static DeprecationIssue checkImplicitlyDisabledNativeRealms(final Settings settings, final PluginsAndModules pluginsAndModules) { final Map<RealmConfig.RealmIdentifier, Settings> realmSettings = RealmSettings.getRealmSettings(settings); if (realmSettings.isEmpty()) { return null; } // If all configured realms are disabled, this equals to no realm is configured. The implicitly behaviour in this case // is to add file and native realms. So we are good here. if (false == realmSettings.entrySet().stream().anyMatch( e -> e.getValue().getAsBoolean(RealmSettings.ENABLED_SETTING_KEY, true))) { return null; } final List<String> implicitlyDisabledNativeRealmTypes = new ArrayList<>(org.elasticsearch.common.collect.List.of(FileRealmSettings.TYPE, NativeRealmSettings.TYPE)); realmSettings.keySet().forEach(ri -> implicitlyDisabledNativeRealmTypes.remove(ri.getType())); if (implicitlyDisabledNativeRealmTypes.isEmpty()) { return null; } final String details = String.format( Locale.ROOT, "Found implicitly disabled native %s: [%s]. %s disabled because there are other explicitly configured realms." + "In next major release, native realms will always be enabled unless explicitly disabled.", implicitlyDisabledNativeRealmTypes.size() == 1 ? "realm" : "realms", Strings.collectionToDelimitedString(implicitlyDisabledNativeRealmTypes, ","), implicitlyDisabledNativeRealmTypes.size() == 1 ? "It is" : "They are"); return new DeprecationIssue( DeprecationIssue.Level.WARNING, "File and/or native realms are enabled by default in next major release.", "https://www.elastic.co/guide/en/elasticsearch/reference/7.13/deprecated-7.13.html#implicitly-disabled-native-realms", details ); }	native here means "file or native" right? do we need a better word?
public ActionRequestValidationException validate(ActionRequestValidationException validationException) { // TODO: make this dependent on search.max_buckets if (maxPageSearchSize != null && (maxPageSearchSize < 10 || maxPageSearchSize > 65_536)) { validationException = addValidationError( "settings.max_page_search_size [" + maxPageSearchSize + "] must be greater than 10 and less than 65,536", validationException ); } return validationException; }	the limit is 65_535. you might stumbled upon https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html, which says 65_536. i think this is wrong, i will check this.
public ActionRequestValidationException validate(ActionRequestValidationException validationException) { // TODO: make this dependent on search.max_buckets if (maxPageSearchSize != null && (maxPageSearchSize < 10 || maxPageSearchSize > 65_536)) { validationException = addValidationError( "settings.max_page_search_size [" + maxPageSearchSize + "] must be greater than 10 and less than 65,536", validationException ); } return validationException; }	you found a bug, this error message is mathematically not correct, because 10 and 10,000 are allowed values. i am checking this and might come back with an improved wording.
public ActionRequestValidationException validate(ActionRequestValidationException validationException) { if (maxPageSearchSize != null && (maxPageSearchSize < 10 || maxPageSearchSize > 65_536)) { validationException = addValidationError( "pivot.max_page_search_size [" + maxPageSearchSize + "] must be greater than 10 and less than 65,536", validationException ); } validationException = groups.validate(validationException); validationException = aggregationConfig.validate(validationException); List<String> usedNames = new ArrayList<>(); usedNames.addAll(groups.getUsedNames()); usedNames.addAll(aggregationConfig.getUsedNames()); for (String failure : aggFieldValidation(usedNames)) { validationException = addValidationError(failure, validationException); } return validationException; }	~same here, 65535~ redacted, 65536 is correct
@Override protected DateTimeFormatProcessor mutateInstance(DateTimeFormatProcessor instance) { Formatter replaced = randomValueOtherThan(instance.formatter(), () -> randomFrom(Formatter.values())); return new DateTimeFormatProcessor( new ConstantProcessor(DateTimeTestUtils.nowWithMillisResolution()), new ConstantProcessor(ESTestCase.randomRealisticUnicodeOfLength(128)), randomZone(), replaced ); }	nit: you could avoid the variable and use the randomvalueotherthan() here directly.
public final void execute(Task task, Request request, ActionListener<Response> listener) { ActionRequestValidationException validationException = request.validate(); if (validationException != null) { listener.onFailure(validationException); return; } prepareRequest(request); if (task != null && request.getShouldStoreResult()) { listener = new TaskResultStoringActionListener<>(taskManager, task, listener); } RequestFilterChain<Request, Response> requestFilterChain = new RequestFilterChain<>(this, logger); requestFilterChain.proceed(task, actionName, request, listener); }	i tried to make this an actionfilter, but we need a hack for this. the new action filter must be executed before the security action filter, but the security action filter already has the lowest order (integer.min_value).
@Override public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; boolean scroll = scroll() != null; if (scroll) { if (source != null) { if (source.trackTotalHitsUpTo() != null && source.trackTotalHitsUpTo() != SearchContext.TRACK_TOTAL_HITS_ACCURATE) { validationException = addValidationError("disabling [track_total_hits] is not allowed in a scroll context", validationException); } if (source.from() > 0) { validationException = addValidationError("using [from] is not allowed in a scroll context", validationException); } if (source.size() == 0) { validationException = addValidationError("[size] cannot be [0] in a scroll context", validationException); } if (source.rescores() != null && source.rescores().isEmpty() == false) { validationException = addValidationError("using [rescore] is not allowed in a scroll context", validationException); } } if (requestCache != null && requestCache) { validationException = addValidationError("[request_cache] cannot be used in a scroll context", validationException); } } if (source != null) { if (source.aggregations() != null) { validationException = source.aggregations().validate(validationException); } } if (reader() != null) { if (indices.length > 0) { validationException = addValidationError("[index] cannot be used with reader contexts", validationException); } if (routing() != null) { validationException = addValidationError("[routing] cannot be used with reader contexts", validationException); } if (preference() != null) { validationException = addValidationError("[preference] cannot be used with reader contexts", validationException); } } return validationException; }	we should also check if scroll is not set ?
public CanMatchResponse canMatch(ShardSearchRequest request) throws IOException { assert request.searchType() == SearchType.QUERY_THEN_FETCH : "unexpected search type: " + request.searchType(); assert request.readerId() == null : "request with reader_id bypass can_match phase"; IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex()); IndexShard indexShard = indexService.getShard(request.shardId().getId()); // we don't want to use the reader wrapper since it could run costly operations // and we can afford false positives. try (Engine.Searcher searcher = indexShard.acquireCanMatchSearcher()) { final boolean aliasFilterCanMatch = request.getAliasFilter() .getQueryBuilder() instanceof MatchNoneQueryBuilder == false; QueryShardContext context = indexService.newQueryShardContext(request.shardId().id(), searcher, request::nowInMillis, request.getClusterAlias()); Rewriteable.rewrite(request.getRewriteable(), context, false); FieldSortBuilder sortBuilder = FieldSortBuilder.getPrimaryFieldSortOrNull(request.source()); MinAndMax<?> minMax = sortBuilder != null ? FieldSortBuilder.getMinMaxOrNull(context, sortBuilder) : null; if (canRewriteToMatchNone(request.source())) { QueryBuilder queryBuilder = request.source().query(); return new CanMatchResponse( aliasFilterCanMatch && queryBuilder instanceof MatchNoneQueryBuilder == false, minMax ); } // null query means match_all return new CanMatchResponse(aliasFilterCanMatch, minMax); } }	is it temporary ? we've said that it should be possible to run the can_match phase as long as we retrieve the reader and extend the keep alive if the request cannot match ?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject() .field("access_token", tokenString) .field("type", "Bearer") .field("expires_in", expiresIn.seconds()); if (refreshToken != null) { builder.field("refresh_token", refreshToken); } // only show the scope if it is not null if (scope != null) { builder.field("scope", scope); } if (kerberosAuthenticationResponseToken != null) { builder.field("kerberos_authentication_response_token", kerberosAuthenticationResponseToken); } builder.field("authentication", authentication); return builder.endObject(); }	i think we need a null check here to not show the field at all if the value is null. as far as i understand it, this is the general preference (see other fields in this method). this comment applies to other places where xcontent is built with authentication field.
private void handleJoinRequest(final DiscoveryNode node, final MembershipAction.JoinCallback callback) { if (!master) { throw new ElasticsearchIllegalStateException("Node [" + localNode + "] not master for join request from [" + node + "]"); } if (!transportService.addressSupported(node.address().getClass())) { // TODO, what should we do now? Maybe inform that node that its crap? logger.warn("received a wrong address type from [{}], ignoring...", node); } else { // try and connect to the node, if it fails, we can raise an exception back to the client... transportService.connectToNode(node); // validate the join request, will throw a failure if it fails, which will get back to the // node calling the join request membership.sendValidateJoinRequestBlocking(node, joinTimeout); processJoinRequests.add(new Tuple<>(node, callback)); clusterService.submitStateUpdateTask("zen-disco-receive(join from node[" + node + "])", Priority.IMMEDIATE, new ProcessedClusterStateUpdateTask() { private final List<Tuple<DiscoveryNode, MembershipAction.JoinCallback>> drainedTasks = new ArrayList<>(); @Override public ClusterState execute(ClusterState currentState) { processJoinRequests.drainTo(drainedTasks); if (drainedTasks.isEmpty()) { return currentState; } boolean modified = false; DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder(currentState.nodes()); for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> task : drainedTasks) { DiscoveryNode node = task.v1(); if (currentState.nodes().nodeExists(node.id())) { logger.info("received a join request for an existing node [{}]", node); } else { modified = true; nodesBuilder.put(node); for (DiscoveryNode existingNode : currentState.nodes()) { if (node.address().equals(existingNode.address())) { nodesBuilder.remove(existingNode.id()); logger.warn("received join request from node [{}], but found existing node {} with same address, removing existing node", node, existingNode); } } } } ClusterState.Builder stateBuilder = ClusterState.builder(currentState); if (modified) { stateBuilder.nodes(latestDiscoNodes = nodesBuilder.build()); } return stateBuilder.build(); } @Override public void onFailure(String source, Throwable t) { if (t instanceof ClusterService.NoLongerMasterException) { logger.debug("not processing [{}] as we are no longer master", source); } else { logger.error("unexpected failure during [{}]", t, source); } for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> drainedTask : drainedTasks) { drainedTask.v2().onFailure(t); } } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> drainedTask : drainedTasks) { drainedTask.v2().onSuccess(); } } }); } }	while we are at it, can we move this log to debug? its very noisy and it can happen with join retry logic
private void handleJoinRequest(final DiscoveryNode node, final MembershipAction.JoinCallback callback) { if (!master) { throw new ElasticsearchIllegalStateException("Node [" + localNode + "] not master for join request from [" + node + "]"); } if (!transportService.addressSupported(node.address().getClass())) { // TODO, what should we do now? Maybe inform that node that its crap? logger.warn("received a wrong address type from [{}], ignoring...", node); } else { // try and connect to the node, if it fails, we can raise an exception back to the client... transportService.connectToNode(node); // validate the join request, will throw a failure if it fails, which will get back to the // node calling the join request membership.sendValidateJoinRequestBlocking(node, joinTimeout); processJoinRequests.add(new Tuple<>(node, callback)); clusterService.submitStateUpdateTask("zen-disco-receive(join from node[" + node + "])", Priority.IMMEDIATE, new ProcessedClusterStateUpdateTask() { private final List<Tuple<DiscoveryNode, MembershipAction.JoinCallback>> drainedTasks = new ArrayList<>(); @Override public ClusterState execute(ClusterState currentState) { processJoinRequests.drainTo(drainedTasks); if (drainedTasks.isEmpty()) { return currentState; } boolean modified = false; DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder(currentState.nodes()); for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> task : drainedTasks) { DiscoveryNode node = task.v1(); if (currentState.nodes().nodeExists(node.id())) { logger.info("received a join request for an existing node [{}]", node); } else { modified = true; nodesBuilder.put(node); for (DiscoveryNode existingNode : currentState.nodes()) { if (node.address().equals(existingNode.address())) { nodesBuilder.remove(existingNode.id()); logger.warn("received join request from node [{}], but found existing node {} with same address, removing existing node", node, existingNode); } } } } ClusterState.Builder stateBuilder = ClusterState.builder(currentState); if (modified) { stateBuilder.nodes(latestDiscoNodes = nodesBuilder.build()); } return stateBuilder.build(); } @Override public void onFailure(String source, Throwable t) { if (t instanceof ClusterService.NoLongerMasterException) { logger.debug("not processing [{}] as we are no longer master", source); } else { logger.error("unexpected failure during [{}]", t, source); } for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> drainedTask : drainedTasks) { drainedTask.v2().onFailure(t); } } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> drainedTask : drainedTasks) { drainedTask.v2().onSuccess(); } } }); } }	can we do the assignment in another line, very sneaky :)
* @return the collected mapped field types */ static Map<String, MappedFieldType> collectFieldTypes(Collection<RuntimeField> runtimeFields) { return runtimeFields.stream() .flatMap(runtimeField -> { List<String> names = runtimeField.asMappedFieldTypes().stream().map(MappedFieldType::name) .filter(name -> name.equals(runtimeField.name()) == false && (name.startsWith(runtimeField.name() + ".") == false || name.length() > runtimeField.name().length() + 1 == false)) .collect(Collectors.toList()); if (names.isEmpty() == false) { throw new IllegalStateException("Found sub-fields with name not belonging to the parent field they are part of " + names); } return runtimeField.asMappedFieldTypes().stream(); }) .collect(Collectors.toUnmodifiableMap(MappedFieldType::name, mappedFieldType -> mappedFieldType, (t, t2) -> { throw new IllegalArgumentException("Found two runtime fields with same name [" + t.name() + "]"); })); }	was this enforced at all before, or would one runtime field just override another? if not it could be worth calling out in the 'migration' docs in case some user runs into it.
protected ScriptTemplate scriptWithAggregate(AggregateFunction aggregate) { Tuple<String, DataType> template = basicTemplate(aggregate); ParamsBuilder paramsBuilder = paramsBuilder().agg(aggregate); if (template.v2() != null) { paramsBuilder.variable(template.v2().name()); } return new ScriptTemplate(processScript(template.v1()), paramsBuilder.build(), dataType()); }	returning the tuple is clunky since the initial method was designed to basic template as a string and allow postprocessing. instead do the numeric handling after the script creation and bind the type as a parameter to the newly created script. looking at the code this needs to occur only in scriptwithaggregate so it can be localized there and avoid over generalization.
public static Number nullSafeCastNumeric(Number number, String typeName) { DataType toType = fromTypeName(typeName); if (toType.isNumeric() == false) { throw new QlIllegalArgumentException("Casting target [" + typeName + "] is not a numerical type"); } return number == null || Double.isNaN(number.doubleValue()) ? null : (Number) convert(number, toType); }	is this ever needed? being extra careful isn't an issue however this code is called only when doing a cast so it 'seems' safe. furthermore, convert method already does safe checking so we should be fine.
public void writeTo(StreamOutput out) throws IOException { out.writeString(name); out.writeString(property); out.writeOptionalString(innerKey); if (out.getVersion().onOrAfter(Version.V_7_13_0)) { out.writeOptionalString(dataType == null ? null : dataType.name()); } else { out.writeBoolean(isDateBased(dataType)); } }	same comment as above.
public void testTranslateInExpression_HavingClause_Painless() { LogicalPlan p = plan("SELECT keyword, max(int) FROM test GROUP BY keyword HAVING max(int) IN (10, 20, 30 - 10)"); assertTrue(p instanceof Filter); Expression condition = ((Filter) p).condition(); assertFalse(condition.foldable()); QueryTranslation translation = translateWithAggs(condition); assertNull(translation.query); AggFilter aggFilter = translation.aggFilter; assertEquals("InternalQlScriptUtils.nullSafeFilter(InternalQlScriptUtils.in(" + "InternalQlScriptUtils.nullSafeCastNumeric(params.a0,params.v0), params.v1))", aggFilter.scriptTemplate().toString()); assertThat(aggFilter.scriptTemplate().params().toString(), startsWith("[{a=max(int)")); assertThat(aggFilter.scriptTemplate().params().toString(), endsWith(", {v=[10, 20]}	if only there was a way to externalize these test declarations... /cc @matriv
protected ScriptTemplate scriptWithAggregate(AggregateFunction aggregate) { Tuple<String, Integer> template = basicTemplate(aggregate); return new ScriptTemplate(processScript(template.v1()), repeatedParamBuilder(template.v2(), p -> p.agg(aggregate)).build(), dataType()); }	why this approach with duplicating the _same_ parameter? from what i tested, the query generated with the help of this method looks more or less like this (a snippet): "aggregations": { "4b4abc8": { "stats": { "field": "nr" } }, "having.beb296e4": { "bucket_selector": { "buckets_path": { "a0": "4b4abc8.sum", "a1": "4b4abc8.sum" }, "script": { "source": "internalqlscriptutils.nullsafefilter(internalqlscriptutils.gt(internalsqlscriptutils.div(double.nan.compareto(params.a0) == 0 ? null : ((number) params.a1).longvalue(),params.v0),params.v1))", "lang": "painless", "params": { "v0": 2, "v1": 5 } }, "gap_policy": "skip" } } } can we improve this? "buckets_path": { "a0": "4b4abc8.sum", "a1": "4b4abc8.sum" } and use only one parameter both for the script source itself and for the value of that parameter? something like the following: "aggregations": { "4b4abc8": { "stats": { "field": "nr" } }, "having.beb296e4": { "bucket_selector": { "buckets_path": { "a0": "4b4abc8.sum" }, "script": { "source": "internalqlscriptutils.nullsafefilter(internalqlscriptutils.gt(internalsqlscriptutils.div(double.nan.compareto(params.a0) == 0 ? null : ((number) params.a0).longvalue(),params.v0),params.v1))", "lang": "painless", "params": { "v0": 2, "v1": 5 } }, "gap_policy": "skip" } } }
public static SubAggCollectionMode parse(String value) { SubAggCollectionMode[] modes = SubAggCollectionMode.values(); for (SubAggCollectionMode mode : modes) { if (mode.parseField.match(value, LoggingDeprecationHandler.INSTANCE)) { return mode; } } throw new ElasticsearchParseException("no [{}] found for value [{}]", KEY.getPreferredName(), value); }	i think it'd be cleaner to take the deprecationhandler as an argument here. i really want to minimize the places where we use the static instance of loggingdeprecationhandler. my ulterior motive is that we *might* be able to remove it entirely at some point which would be super neat.
public static TopHitsAggregationBuilder parse(String aggregationName, XContentParser parser) throws IOException { TopHitsAggregationBuilder factory = new TopHitsAggregationBuilder(aggregationName); XContentParser.Token token; String currentFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token.isValue()) { if (SearchSourceBuilder.FROM_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.from(parser.intValue()); } else if (SearchSourceBuilder.SIZE_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.size(parser.intValue()); } else if (SearchSourceBuilder.VERSION_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.version(parser.booleanValue()); } else if (SearchSourceBuilder.EXPLAIN_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.explain(parser.booleanValue()); } else if (SearchSourceBuilder.TRACK_SCORES_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.trackScores(parser.booleanValue()); } else if (SearchSourceBuilder._SOURCE_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.fetchSource(FetchSourceContext.fromXContent(parser)); } else if (SearchSourceBuilder.STORED_FIELDS_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.storedFieldsContext = StoredFieldsContext.fromXContent(SearchSourceBuilder.STORED_FIELDS_FIELD.getPreferredName(), parser); } else if (SearchSourceBuilder.SORT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.sort(parser.text()); } else { throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation()); } } else if (token == XContentParser.Token.START_OBJECT) { if (SearchSourceBuilder._SOURCE_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.fetchSource(FetchSourceContext.fromXContent(parser)); } else if (SearchSourceBuilder.SCRIPT_FIELDS_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { List<ScriptField> scriptFields = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { String scriptFieldName = parser.currentName(); token = parser.nextToken(); if (token == XContentParser.Token.START_OBJECT) { Script script = null; boolean ignoreFailure = false; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token.isValue()) { if (SearchSourceBuilder.SCRIPT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { script = Script.parse(parser); } else if (SearchSourceBuilder.IGNORE_FAILURE_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { ignoreFailure = parser.booleanValue(); } else { throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation()); } } else if (token == XContentParser.Token.START_OBJECT) { if (SearchSourceBuilder.SCRIPT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { script = Script.parse(parser); } else { throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation()); } } else { throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation()); } } scriptFields.add(new ScriptField(scriptFieldName, script, ignoreFailure)); } else { throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation()); } } factory.scriptFields(scriptFields); } else if (SearchSourceBuilder.HIGHLIGHT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.highlighter(HighlightBuilder.fromXContent(parser)); } else if (SearchSourceBuilder.SORT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { List<SortBuilder<?>> sorts = SortBuilder.fromXContent(parser); factory.sorts(sorts); } else { throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation()); } } else if (token == XContentParser.Token.START_ARRAY) { if (SearchSourceBuilder.STORED_FIELDS_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.storedFieldsContext = StoredFieldsContext.fromXContent(SearchSourceBuilder.STORED_FIELDS_FIELD.getPreferredName(), parser); } else if (SearchSourceBuilder.DOCVALUE_FIELDS_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { List<String> fieldDataFields = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token == XContentParser.Token.VALUE_STRING) { fieldDataFields.add(parser.text()); } else { throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation()); } } factory.fieldDataFields(fieldDataFields); } else if (SearchSourceBuilder.SORT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { List<SortBuilder<?>> sorts = SortBuilder.fromXContent(parser); factory.sorts(sorts); } else if (SearchSourceBuilder._SOURCE_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { factory.fetchSource(FetchSourceContext.fromXContent(parser)); } else { throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation()); } } else { throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation()); } } return factory; }	just like last time, would you mind indenting this one more level?
public static Installation install(Distribution distribution, String version) { final Result result = runInstallCommand(distribution, version); if (result.exitCode != 0) { throw new RuntimeException("Installing distribution " + distribution + " version " + version + " failed: " + result); } return Installation.ofPackage(distribution.packaging); }	nit: the more i see of this, the more i think that it's worth extending the ondpkg type of api to be able to produce values so reading trough the code is more consistent.
public GeoGridAggregationBuilder shardSize(int shardSize) { if (shardSize < -1 || shardSize == 0) { throw new IllegalArgumentException( "[shardSize] must be greater than 0. Found [" + shardSize + "] in [" + name + "]"); } this.shardSize = shardSize; return this; }	shardsize <= 0 ? it seems weird to accept a shardsize of -1, if the user wants to let es decide the best value for the shardsize (which is the meaning of -1) maybe it is best to not tweak this value at all ?
public void testTwoNodesSingleDoc() throws Exception { logger.info("--> cleaning nodes"); logger.info("--> starting 2 nodes"); internalCluster().startNodes(2); logger.info("--> indexing a simple document"); client().prepareIndex("test").setId("1").setSource("field1", "value1").setRefreshPolicy(IMMEDIATE).get(); logger.info("--> waiting for green status"); ClusterHealthResponse health = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus() .setWaitForNodes("2").execute().actionGet(); assertThat(health.isTimedOut(), equalTo(false)); logger.info("--> verify 1 doc in the index"); for (int i = 0; i < 10; i++) { assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1L); } logger.info("--> closing test index..."); assertAcked(client().admin().indices().prepareClose("test")); ClusterStateResponse stateResponse = client().admin().cluster().prepareState().execute().actionGet(); assertThat(stateResponse.getState().metaData().index("test").getState(), equalTo(IndexMetaData.State.CLOSE)); assertThat(stateResponse.getState().routingTable().index("test"), notNullValue()); logger.info("--> opening the index..."); client().admin().indices().prepareOpen("test").execute().actionGet(); logger.info("--> waiting for green status"); health = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().setWaitForNodes("2") .execute().actionGet(); assertThat(health.isTimedOut(), equalTo(false)); logger.info("--> verify 1 doc in the index"); assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1L); for (int i = 0; i < 10; i++) { assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1L); } }	i know this is not your change, but the name "writebrokenmeta" looks invalid for two reasons - in this test we write well-formed metadata, this method performs full-cluster restart and i think this should be reflected in the method name.
@Override protected void masterOperation(final ClusterStateRequest request, final ClusterState state, ActionListener<ClusterStateResponse> listener) { ClusterState currentState = clusterService.state(); logger.trace("Serving cluster state request using version {}", currentState.version()); ClusterState.Builder builder = ClusterState.builder(currentState.getClusterName()); builder.version(currentState.version()); builder.uuid(currentState.stateUUID()); if (request.nodes()) { builder.nodes(currentState.nodes()); } if (request.routingTable()) { if (request.indices().length > 0) { RoutingTable.Builder routingTableBuilder = RoutingTable.builder(); for (String filteredIndex : request.indices()) { if (currentState.routingTable().getIndicesRouting().containsKey(filteredIndex)) { routingTableBuilder.add(currentState.routingTable().getIndicesRouting().get(filteredIndex)); } } builder.routingTable(routingTableBuilder); } else { builder.routingTable(currentState.routingTable()); } } if (request.blocks()) { builder.blocks(currentState.blocks()); } if (request.metaData()) { MetaData.Builder mdBuilder; if (request.indices().length == 0) { mdBuilder = MetaData.builder(currentState.metaData()); } else { mdBuilder = MetaData.builder(); } if (request.indices().length > 0) { String[] indices = currentState.metaData().concreteIndices(request.indicesOptions(), request.indices()); for (String filteredIndex : indices) { IndexMetaData indexMetaData = currentState.metaData().index(filteredIndex); if (indexMetaData != null) { mdBuilder.put(indexMetaData, false); } } } // Filter our metadata that shouldn't be returned by API for(ObjectObjectCursor<String, Custom> custom : currentState.metaData().customs()) { if(!custom.value.context().contains(MetaData.XContentContext.API)) { mdBuilder.removeCustom(custom.key); } } builder.metaData(mdBuilder); } if (request.customs()) { builder.customs(currentState.customs()); } listener.onResponse(new ClusterStateResponse(clusterName, builder.build())); }	should we rename it in the builder as well?
public static void toXContent(MetaData metaData, XContentBuilder builder, ToXContent.Params params) throws IOException { XContentContext context = XContentContext.valueOf(params.param(CONTEXT_MODE_PARAM, "API")); builder.startObject("meta-data"); builder.field("version", metaData.version()); builder.field("cluster_uuid", metaData.clusterUUID); if (!metaData.persistentSettings().getAsMap().isEmpty()) { builder.startObject("settings"); for (Map.Entry<String, String> entry : metaData.persistentSettings().getAsMap().entrySet()) { builder.field(entry.getKey(), entry.getValue()); } builder.endObject(); } if (context == XContentContext.API && !metaData.transientSettings().getAsMap().isEmpty()) { builder.startObject("transient_settings"); for (Map.Entry<String, String> entry : metaData.transientSettings().getAsMap().entrySet()) { builder.field(entry.getKey(), entry.getValue()); } builder.endObject(); } builder.startObject("templates"); for (ObjectCursor<IndexTemplateMetaData> cursor : metaData.templates().values()) { IndexTemplateMetaData.Builder.toXContent(cursor.value, builder, params); } builder.endObject(); if (context == XContentContext.API && !metaData.indices().isEmpty()) { builder.startObject("indices"); for (IndexMetaData indexMetaData : metaData) { IndexMetaData.Builder.toXContent(indexMetaData, builder, params); } builder.endObject(); } for (ObjectObjectCursor<String, Custom> cursor : metaData.customs()) { Custom proto = lookupPrototypeSafe(cursor.key); if (proto.context().contains(context)) { builder.startObject(cursor.key); cursor.value.toXContent(builder, params); builder.endObject(); } } builder.endObject(); }	should we mention this in the breaking changes?
private static boolean useUnpooled(long heapSizeInBytes, long g1RegionSize) { if (userForcedUnpooled()) { return true; } else { boolean heapIsOneGBOrLess = heapSizeInBytes <= 1 << 30; boolean g1gcRegionIsLessThan1MB = g1RegionSize < 1 << 20; boolean unknownRegionSize = g1RegionSize != -1; return heapIsOneGBOrLess || (g1gcRegionIsLessThan1MB && unknownRegionSize); } }	region size cannot be smaller than 1mb, so hopefully this never triggers. i suppose a future jdk could lower the minimum region size so ok to keep the check, but i think we should add a comment about this. it is hard to assert anything meaningful about in a test without spinning up a small separate jvm. you could also change the maxorder calculation to be something like log2(g1regionsize / pagesize) and trigger unpooled when smaller than 5. not necessary though, current code is fine too with a comment.
private static boolean useUnpooled(long heapSizeInBytes, long g1RegionSize) { if (userForcedUnpooled()) { return true; } else { boolean heapIsOneGBOrLess = heapSizeInBytes <= 1 << 30; boolean g1gcRegionIsLessThan1MB = g1RegionSize < 1 << 20; boolean unknownRegionSize = g1RegionSize != -1; return heapIsOneGBOrLess || (g1gcRegionIsLessThan1MB && unknownRegionSize); } }	i think this should be: suggestion return heapisonegborless || (g1gcregionislessthan1mb && unknownregionsize == false);
static ClusterState createDataStream(MetaDataCreateIndexService metaDataCreateIndexService, ClusterState currentState, Request request) throws Exception { if (currentState.metaData().dataStreams().containsKey(request.name)) { throw new IllegalArgumentException("data_stream [" + request.name + "] already exists"); } MetaDataCreateIndexService.validateIndexOrAliasName(request.name, (s1, s2) -> new IllegalArgumentException("data_stream [" + s1 + "] " + s2)); String firstBackingIndexName = request.name + "-000000"; CreateIndexClusterStateUpdateRequest createIndexRequest = new CreateIndexClusterStateUpdateRequest("initialize_data_stream", firstBackingIndexName, firstBackingIndexName); currentState = metaDataCreateIndexService.applyCreateIndexRequest(currentState, createIndexRequest, false); IndexMetaData firstBackingIndex = currentState.metaData().index(firstBackingIndexName); MetaData.Builder builder = MetaData.builder(currentState.metaData()).put( new DataStream(request.name, request.timestampFieldName, List.of(firstBackingIndex.getIndex()))); logger.info("adding data stream [{}]", request.name); return ClusterState.builder(currentState).metaData(builder).build(); }	i think i remember @dakrone mentioning something about not starting from 0, rather from 1 like described in the [ilm getting started guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index-lifecycle-management.html#ilm-gs-bootstrap). i am ok either way.
static ClusterState createDataStream(MetaDataCreateIndexService metaDataCreateIndexService, ClusterState currentState, Request request) throws Exception { if (currentState.metaData().dataStreams().containsKey(request.name)) { throw new IllegalArgumentException("data_stream [" + request.name + "] already exists"); } MetaDataCreateIndexService.validateIndexOrAliasName(request.name, (s1, s2) -> new IllegalArgumentException("data_stream [" + s1 + "] " + s2)); String firstBackingIndexName = request.name + "-000000"; CreateIndexClusterStateUpdateRequest createIndexRequest = new CreateIndexClusterStateUpdateRequest("initialize_data_stream", firstBackingIndexName, firstBackingIndexName); currentState = metaDataCreateIndexService.applyCreateIndexRequest(currentState, createIndexRequest, false); IndexMetaData firstBackingIndex = currentState.metaData().index(firstBackingIndexName); MetaData.Builder builder = MetaData.builder(currentState.metaData()).put( new DataStream(request.name, request.timestampFieldName, List.of(firstBackingIndex.getIndex()))); logger.info("adding data stream [{}]", request.name); return ClusterState.builder(currentState).metaData(builder).build(); }	we need to set the index.hidden setting here and test in testcreatedatastream() that it has been set.
static ClusterState createDataStream(MetaDataCreateIndexService metaDataCreateIndexService, ClusterState currentState, Request request) throws Exception { if (currentState.metaData().dataStreams().containsKey(request.name)) { throw new IllegalArgumentException("data_stream [" + request.name + "] already exists"); } MetaDataCreateIndexService.validateIndexOrAliasName(request.name, (s1, s2) -> new IllegalArgumentException("data_stream [" + s1 + "] " + s2)); String firstBackingIndexName = request.name + "-000000"; CreateIndexClusterStateUpdateRequest createIndexRequest = new CreateIndexClusterStateUpdateRequest("initialize_data_stream", firstBackingIndexName, firstBackingIndexName); currentState = metaDataCreateIndexService.applyCreateIndexRequest(currentState, createIndexRequest, false); IndexMetaData firstBackingIndex = currentState.metaData().index(firstBackingIndexName); MetaData.Builder builder = MetaData.builder(currentState.metaData()).put( new DataStream(request.name, request.timestampFieldName, List.of(firstBackingIndex.getIndex()))); logger.info("adding data stream [{}]", request.name); return ClusterState.builder(currentState).metaData(builder).build(); }	it would be good to add: suggestion assert firstbackingindex != null; since that is guaranteed to fail tests (the npe occurring further down could be swallowed).
private boolean isNonEmpty(List<IndexMetaData> idxMetas) { return (Objects.isNull(idxMetas) || idxMetas.isEmpty()) == false; } } class DataStream implements IndexAbstraction { private final org.elasticsearch.cluster.metadata.DataStream dataStream; private final List<IndexMetaData> dataStreamIndices; private final IndexMetaData writeIndex; public DataStream(org.elasticsearch.cluster.metadata.DataStream dataStream, List<IndexMetaData> dataStreamIndices, IndexMetaData writeIndex) { this.dataStream = dataStream; this.dataStreamIndices = dataStreamIndices; this.writeIndex = writeIndex; } @Override public String getName() { return dataStream.getName(); } @Override public Type getType() { return Type.DATA_STREAM; } @Override public List<IndexMetaData> getIndices() { return dataStreamIndices; } public IndexMetaData getWriteIndex() { return writeIndex; } @Override public boolean isHidden() { return false; }	this looks unused? i think it makes more sense to add this when also building the indiceslookup.
private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLookup) { DataStreamMetadata dsMetadata = (DataStreamMetadata) customs.get(DataStreamMetadata.TYPE); if (dsMetadata != null) { for (DataStream ds : dsMetadata.dataStreams().values()) { IndexAbstraction existing = indicesLookup.get(ds.getName()); if (existing != null && existing.getType() != IndexAbstraction.Type.DATA_STREAM) { throw new IllegalStateException("data stream [" + ds.getName() + "] conflicts with existing index or alias"); } SortedMap<String, IndexAbstraction> map = indicesLookup.subMap(ds.getName() + "-", ds.getName() + "."); // '.' is the char after '-' if (map.size() != 0) { if (map.size() == ds.getIndices().size()) { int numValidIndices = 0; for (int i = 0; i < map.size(); i++) { IndexAbstraction space = map.get(String.format(Locale.ROOT, "%s-%06d", ds.getName(), i)); if (space != null && space.getType() == IndexAbstraction.Type.CONCRETE_INDEX) { numValidIndices++; } } if (numValidIndices == map.size()) { continue; } } throw new IllegalStateException("data stream [" + ds.getName() + "] could create backing indices that conflict with " + map.size() + " existing index(s) or alias(s)" + " including '" + map.firstKey() + "'"); } } } }	we should end up asserting that existing != null, but that can be done in a followup together with populating indiceslookup
private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLookup) { DataStreamMetadata dsMetadata = (DataStreamMetadata) customs.get(DataStreamMetadata.TYPE); if (dsMetadata != null) { for (DataStream ds : dsMetadata.dataStreams().values()) { IndexAbstraction existing = indicesLookup.get(ds.getName()); if (existing != null && existing.getType() != IndexAbstraction.Type.DATA_STREAM) { throw new IllegalStateException("data stream [" + ds.getName() + "] conflicts with existing index or alias"); } SortedMap<String, IndexAbstraction> map = indicesLookup.subMap(ds.getName() + "-", ds.getName() + "."); // '.' is the char after '-' if (map.size() != 0) { if (map.size() == ds.getIndices().size()) { int numValidIndices = 0; for (int i = 0; i < map.size(); i++) { IndexAbstraction space = map.get(String.format(Locale.ROOT, "%s-%06d", ds.getName(), i)); if (space != null && space.getType() == IndexAbstraction.Type.CONCRETE_INDEX) { numValidIndices++; } } if (numValidIndices == map.size()) { continue; } } throw new IllegalStateException("data stream [" + ds.getName() + "] could create backing indices that conflict with " + map.size() + " existing index(s) or alias(s)" + " including '" + map.firstKey() + "'"); } } } }	let's add this in another change as well.
private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLookup) { DataStreamMetadata dsMetadata = (DataStreamMetadata) customs.get(DataStreamMetadata.TYPE); if (dsMetadata != null) { for (DataStream ds : dsMetadata.dataStreams().values()) { IndexAbstraction existing = indicesLookup.get(ds.getName()); if (existing != null && existing.getType() != IndexAbstraction.Type.DATA_STREAM) { throw new IllegalStateException("data stream [" + ds.getName() + "] conflicts with existing index or alias"); } SortedMap<String, IndexAbstraction> map = indicesLookup.subMap(ds.getName() + "-", ds.getName() + "."); // '.' is the char after '-' if (map.size() != 0) { if (map.size() == ds.getIndices().size()) { int numValidIndices = 0; for (int i = 0; i < map.size(); i++) { IndexAbstraction space = map.get(String.format(Locale.ROOT, "%s-%06d", ds.getName(), i)); if (space != null && space.getType() == IndexAbstraction.Type.CONCRETE_INDEX) { numValidIndices++; } } if (numValidIndices == map.size()) { continue; } } throw new IllegalStateException("data stream [" + ds.getName() + "] could create backing indices that conflict with " + map.size() + " existing index(s) or alias(s)" + " including '" + map.firstKey() + "'"); } } } }	i think this should instead validate that the entries in map are the same as those in ds.getindices(). right now it works in the initial create case, but not after we have deleted the first index. also it would be good to add a metadatatests test that validates that a data-stream pointing to backing indices (with random suffix number) works.
@Override public IdsQueryBuilder boost(float boost) { this.boost = boost; return this; }	one more thing, these are going to be user facing public methods, we should add javadocs for each of them
public MatchAllQueryBuilder boost(float boost) { this.boost = boost; return this; }	if we want to go with pure getter, which is fine with me, we should probably change our setters too then for consistency? i think we have public matchallquerybuilder boost(float boost) here. not talking about the fluent thing, just the name of the method, either get and set or no get nor set prefix .
@Override public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException { MatchAllQueryBuilder query = new MatchAllQueryBuilder(); query.fromXContent(parseContext); return query.toQuery(parseContext); }	i guess we could generify this on the base class since the last two lines are going to be the same everywhere? and have a new abstract method newquery() that creates a new instance of the query(builder) ?
@Override public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException { MatchAllQueryBuilder query = new MatchAllQueryBuilder(); query.fromXContent(parseContext); return query.toQuery(parseContext); }	do we need the queryparsecontext argument here? we may need it in some other query?
public static void toXContent(IndexMetadata indexMetadata, XContentBuilder builder, ToXContent.Params params) throws IOException { Metadata.XContentContext context = Metadata.XContentContext.valueOf( params.param(CONTEXT_MODE_PARAM, Metadata.CONTEXT_MODE_API)); builder.startObject(indexMetadata.getIndex().getName()); builder.field(KEY_VERSION, indexMetadata.getVersion()); builder.field(KEY_MAPPING_VERSION, indexMetadata.getMappingVersion()); builder.field(KEY_SETTINGS_VERSION, indexMetadata.getSettingsVersion()); builder.field(KEY_ALIASES_VERSION, indexMetadata.getAliasesVersion()); builder.field(KEY_ROUTING_NUM_SHARDS, indexMetadata.getRoutingNumShards()); builder.field(KEY_STATE, indexMetadata.getState().toString().toLowerCase(Locale.ENGLISH)); boolean binary = params.paramAsBoolean("binary", false); builder.startObject(KEY_SETTINGS); if (context != Metadata.XContentContext.API) { indexMetadata.getSettings().toXContent(builder, new MapParams(Collections.singletonMap("flat_settings", "true"))); } else { indexMetadata.getSettings().toXContent(builder, params); } builder.endObject(); if (context != Metadata.XContentContext.API) { builder.startArray(KEY_MAPPINGS); MappingMetadata mmd = indexMetadata.mapping(); if (mmd != null) { if (binary) { builder.value(mmd.source().compressed()); } else { builder.map(XContentHelper.convertToMap(new BytesArray(mmd.source().uncompressed()), true).v2()); } } builder.endArray(); } else { builder.startObject(KEY_MAPPINGS); MappingMetadata mmd = indexMetadata.mapping(); if (mmd != null) { Map<String, Object> mapping = XContentHelper.convertToMap(new BytesArray(mmd.source().uncompressed()), false).v2(); if (mapping.size() == 1 && mapping.containsKey(mmd.type())) { // the type name is the root value, reduce it mapping = (Map<String, Object>) mapping.get(mmd.type()); } builder.field(mmd.type()); builder.map(mapping); } builder.endObject(); } for (ObjectObjectCursor<String, DiffableStringMap> cursor : indexMetadata.customData) { builder.field(cursor.key); builder.map(cursor.value); } if (context != Metadata.XContentContext.API) { builder.startObject(KEY_ALIASES); for (ObjectCursor<AliasMetadata> cursor : indexMetadata.getAliases().values()) { AliasMetadata.Builder.toXContent(cursor.value, builder, params); } builder.endObject(); builder.startArray(KEY_PRIMARY_TERMS); for (int i = 0; i < indexMetadata.getNumberOfShards(); i++) { builder.value(indexMetadata.primaryTerm(i)); } builder.endArray(); } else { builder.startArray(KEY_ALIASES); for (ObjectCursor<String> cursor : indexMetadata.getAliases().keys()) { builder.value(cursor.value); } builder.endArray(); builder.startObject(IndexMetadata.KEY_PRIMARY_TERMS); for (int shard = 0; shard < indexMetadata.getNumberOfShards(); shard++) { builder.field(Integer.toString(shard), indexMetadata.primaryTerm(shard)); } builder.endObject(); } builder.startObject(KEY_IN_SYNC_ALLOCATIONS); for (IntObjectCursor<Set<String>> cursor : indexMetadata.inSyncAllocationIds) { builder.startArray(String.valueOf(cursor.key)); for (String allocationId : cursor.value) { builder.value(allocationId); } builder.endArray(); } builder.endObject(); builder.startObject(KEY_ROLLOVER_INFOS); for (ObjectCursor<RolloverInfo> cursor : indexMetadata.getRolloverInfos().values()) { cursor.value.toXContent(builder, params); } builder.endObject(); builder.field("system", indexMetadata.isSystem); builder.endObject(); }	nit: could we make this a constant, like the other field keys here?
static IndexMetadata buildIndexMetadata(String indexName, List<AliasMetadata> aliases, Supplier<DocumentMapper> documentMapperSupplier, Settings indexSettings, int routingNumShards, @Nullable IndexMetadata sourceMetadata) { return buildIndexMetadata(indexName, aliases, documentMapperSupplier, indexSettings, routingNumShards, sourceMetadata, false); }	it looks like this version of this method is just used in a single test, so i think we should delete this version of this method and just add the extra false to the call in testbuildindexmetadata()
public synchronized void markAsProcessed(ClusterState state) { if (findState(state.stateUUID()) == null) { throw new IllegalStateException("can't resolve processed cluster state with uuid [" + state.stateUUID() + "], version [" + state.version() + "]"); } final DiscoveryNode currentMaster = state.nodes().getMasterNode(); assert currentMaster != null : "processed cluster state mast have a master. " + state; // fail or remove any incoming state from a different master // respond to any committed state from the same master with same or lower version (we processed a higher version) ArrayList<ClusterStateContext> contextsToRemove = new ArrayList<>(); for (int index = 0; index < pendingStates.size(); index++) { final ClusterStateContext pendingContext = pendingStates.get(index); final ClusterState pendingState = pendingContext.state; final DiscoveryNode pendingMasterNode = pendingState.nodes().getMasterNode(); if (Objects.equals(currentMaster, pendingMasterNode) == false) { contextsToRemove.add(pendingContext); if (pendingContext.committed()) { // this is a committed state , warn logger.warn("received a cluster state (uuid[{}]/v[{}]) from a different master than the current one, rejecting (received {}, current {})", pendingState.stateUUID(), pendingState.version(), pendingMasterNode, currentMaster); pendingContext.listener.onNewClusterStateFailed( new IllegalStateException("cluster state from a different master than the current one, rejecting (received " + pendingMasterNode + ", current " + currentMaster + ")") ); } else { logger.trace("removing non-committed state with uuid[{}]/v[{}] from [{}] - a state from [{}] was successfully processed", pendingState.stateUUID(), pendingState.version(), pendingMasterNode, currentMaster ); } } else if (state.version() >= pendingState.version()) { assert state.supersedes(pendingState) || ( state.nodes().getMasterNodeId() != null && state.nodes().getMasterNodeId().equals(pendingState.nodes().getMasterNodeId())); logger.trace("processing pending state uuid[{}]/v[{}] together with state uuid[{}]/v[{}]", pendingState.stateUUID(), pendingState.version(), state.stateUUID(), state.version() ); contextsToRemove.add(pendingContext); if (pendingContext.committed()) { pendingContext.listener.onNewClusterStateProcessed(); } } else if (pendingState.stateUUID().equals(state.stateUUID())) { assert pendingContext.committed() : "processed cluster state is not committed " + state; contextsToRemove.add(pendingContext); pendingContext.listener.onNewClusterStateProcessed(); } } // now ack the processed state pendingStates.removeAll(contextsToRemove); assert findState(state.stateUUID()) == null : "state was marked as processed but can still be found in pending list " + state; }	is we're here, then the masters are equal. in this case supersedes == false, means version equality (because it's can't be lower. also state.nodes().getmasternodeid() is never null (and assigned to currentmaster.getid())) . i'm not sure what we're asserting than. also note that it disables the next if clause, checking for uuid equality, so maybe we want to do the next if clause first?
void validateIncomingState(ClusterState incomingState, ClusterState lastSeenClusterState) { final ClusterName incomingClusterName = incomingState.getClusterName(); if (!incomingClusterName.equals(this.clusterName)) { logger.warn("received cluster state from [{}] which is also master but with a different cluster name [{}]", incomingState.nodes().getMasterNode(), incomingClusterName); throw new IllegalStateException("received state from a node that is not part of the cluster"); } final DiscoveryNodes currentNodes = clusterStateSupplier.get().nodes(); if (currentNodes.getLocalNode().equals(incomingState.nodes().getLocalNode()) == false) { logger.warn("received a cluster state from [{}] and not part of the cluster, should not happen", incomingState.nodes().getMasterNode()); throw new IllegalStateException("received state from local node that does not match the current local node"); } ZenDiscovery.validateStateIsFromCurrentMaster(logger, currentNodes, incomingState); if (lastSeenClusterState != null && lastSeenClusterState.supersedes(incomingState)) { final String message = String.format( Locale.ROOT, "received cluster state from current master superseded by last seen cluster state; " + "received version [%d] with uuid [%s], last seen version [%d] with uuid [%s]", incomingState.version(), incomingState.stateUUID(), lastSeenClusterState.version(), lastSeenClusterState.stateUUID() ); logger.warn(message); throw new IllegalStateException(message); } final ClusterState state = clusterStateSupplier.get(); if (state.nodes().getMasterNodeId() != null && incomingState.version() <= state.version()) { assert !incomingState.stateUUID().equals(state.stateUUID()); final String message = String.format( Locale.ROOT, "received cluster state older than current cluster state; " + "received version [%d] with uuid [%s], current version [%d]", incomingState.version(), incomingState.stateUUID(), state.version() ); logger.warn(message); throw new IllegalStateException(message); } }	i think we want it to read "received state _with_ a local node that doesn't match the current one"
void validateIncomingState(ClusterState incomingState, ClusterState lastSeenClusterState) { final ClusterName incomingClusterName = incomingState.getClusterName(); if (!incomingClusterName.equals(this.clusterName)) { logger.warn("received cluster state from [{}] which is also master but with a different cluster name [{}]", incomingState.nodes().getMasterNode(), incomingClusterName); throw new IllegalStateException("received state from a node that is not part of the cluster"); } final DiscoveryNodes currentNodes = clusterStateSupplier.get().nodes(); if (currentNodes.getLocalNode().equals(incomingState.nodes().getLocalNode()) == false) { logger.warn("received a cluster state from [{}] and not part of the cluster, should not happen", incomingState.nodes().getMasterNode()); throw new IllegalStateException("received state from local node that does not match the current local node"); } ZenDiscovery.validateStateIsFromCurrentMaster(logger, currentNodes, incomingState); if (lastSeenClusterState != null && lastSeenClusterState.supersedes(incomingState)) { final String message = String.format( Locale.ROOT, "received cluster state from current master superseded by last seen cluster state; " + "received version [%d] with uuid [%s], last seen version [%d] with uuid [%s]", incomingState.version(), incomingState.stateUUID(), lastSeenClusterState.version(), lastSeenClusterState.stateUUID() ); logger.warn(message); throw new IllegalStateException(message); } final ClusterState state = clusterStateSupplier.get(); if (state.nodes().getMasterNodeId() != null && incomingState.version() <= state.version()) { assert !incomingState.stateUUID().equals(state.stateUUID()); final String message = String.format( Locale.ROOT, "received cluster state older than current cluster state; " + "received version [%d] with uuid [%s], current version [%d]", incomingState.version(), incomingState.stateUUID(), state.version() ); logger.warn(message); throw new IllegalStateException(message); } }	i wonder if we should strengthen zendiscovery.shouldignoreorrejectnewclusterstate to check for version equality (and assert uuid) and use it [in line 410](https://github.com/elastic/elasticsearch/pull/17038/commits/cffc315dca66fa6af109a7cb5768acfb05e140f7#diff-594996d316ac34afe009d496db4abe34r410) then all of this can be change to just throwing an exception if it doesn't like it.
public void testQueueStats() { List<ClusterState> states = randomStates(scaledRandomIntBetween(10, 100), "master"); PendingClusterStatesQueue queue = createQueueWithStates(states); assertThat(queue.stats().getTotal(), equalTo(states.size())); assertThat(queue.stats().getPending(), equalTo(states.size())); assertThat(queue.stats().getCommitted(), equalTo(0)); List<ClusterStateContext> committedContexts = randomCommitStates(queue); assertThat(queue.stats().getTotal(), equalTo(states.size())); assertThat(queue.stats().getPending(), equalTo(states.size() - committedContexts.size())); assertThat(queue.stats().getCommitted(), equalTo(committedContexts.size())); ClusterState highestCommitted = null; for (ClusterStateContext context : committedContexts) { if (highestCommitted == null || context.state.supersedes(highestCommitted)) { highestCommitted = context.state; } } assert highestCommitted != null; queue.markAsProcessed(highestCommitted); assertThat((long)queue.stats().getTotal(), equalTo(states.size() - (1 + highestCommitted.version()))); assertThat((long)queue.stats().getPending(), equalTo(states.size() - (1 + highestCommitted.version()))); assertThat(queue.stats().getCommitted(), equalTo(0)); }	why is this needed?
public void testOutOfOrderCommitMessages() throws Throwable { MockNode node = createMockNode("node").setAsMaster(); final CapturingTransportChannel channel = new CapturingTransportChannel(); List<ClusterState> states = new ArrayList<>(); final int numOfStates = scaledRandomIntBetween(3, 25); for (int i = 1; i <= numOfStates; i++) { states.add(ClusterState.builder(node.clusterState).version(i).stateUUID(ClusterState.UNKNOWN_UUID).build()); } final ClusterState finalState = states.get(numOfStates - 1); logger.info("--> publishing states"); for (ClusterState state : states) { node.action.handleIncomingClusterStateRequest( new BytesTransportRequest(PublishClusterStateAction.serializeFullClusterState(state, Version.CURRENT), Version.CURRENT), channel); assertThat(channel.response.get(), equalTo((TransportResponse) TransportResponse.Empty.INSTANCE)); assertThat(channel.error.get(), nullValue()); channel.clear(); } logger.info("--> committing states"); long largestVersionSeen = Long.MIN_VALUE; Randomness.shuffle(states); for (ClusterState state : states) { node.action.handleCommitRequest(new PublishClusterStateAction.CommitClusterStateRequest(state.stateUUID()), channel); if (largestVersionSeen < state.getVersion()) { assertThat(channel.response.get(), equalTo((TransportResponse) TransportResponse.Empty.INSTANCE)); if (channel.error.get() != null) { throw channel.error.get(); } largestVersionSeen = state.getVersion(); } else { assertNotNull(channel.error.get()); assertThat(channel.error.get(), instanceOf(IllegalStateException.class)); } channel.clear(); } //now check the last state held assertSameState(node.clusterState, finalState); }	can we add a comment as to why we expect an error?
public InternalEngine recoverFromTranslog() throws IOException { flushLock.lock(); try (ReleasableLock lock = readLock.acquire()) { ensureOpen(); if (openMode != EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG) { throw new IllegalStateException("Can't recover from translog with open mode: " + openMode); } if (allowCommits.get()) { throw new IllegalStateException("Engine has already been recovered"); } try { recoverFromTranslog(engineConfig.getTranslogRecoveryPerformer()); } catch (Throwable t) { allowCommits.set(false); // just play safe and never allow commits on this failEngine("failed to recover from translog", t); throw t; } } finally { flushLock.unlock(); } return this; }	maybe replace this with ensureopen in the beginning? feels cleaner to me
public void finalizeRecovery() { recoveryState().setStage(RecoveryState.Stage.FINALIZE); Engine engine = getEngine(); engine.refresh("recovery_finalization"); engine.config().setEnableGcDeletes(true); }	good! no config reference locally
public void testSycnedFlushSurvivesEngineRestart() throws IOException { final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20); ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null); engine.index(new Engine.Index(newUid("1"), doc)); final Engine.CommitId commitID = engine.flush(); assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID), Engine.SyncedFlushResult.SUCCESS); assertEquals(store.readLastCommittedSegmentsInfo().getUserData().get(Engine.SYNC_COMMIT_ID), syncId); assertEquals(engine.getLastCommittedSegmentInfos().getUserData().get(Engine.SYNC_COMMIT_ID), syncId); EngineConfig config = engine.config(); if (randomBoolean()) { engine.close(); } else { engine.flushAndClose(); } engine = new InternalEngine(copy(config, EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG)); if (randomBoolean()) { engine.recoverFromTranslog(); } assertEquals(engine.getLastCommittedSegmentInfos().getUserData().get(Engine.SYNC_COMMIT_ID), syncId); }	this is different than what it was - before if we had false, we'll ignore the local translog and commit a checkpoint. now we just ignore the recovery, but do not commit. i think we should keep old behavior and use open_index_create_translog randomly.
public EngineConfig config(IndexSettings indexSettings, Store store, Path translogPath, MergePolicy mergePolicy) { IndexWriterConfig iwc = newIndexWriterConfig(); final EngineConfig.OpenMode openMode; try { if (Lucene.indexExists(store.directory()) == false) { openMode = EngineConfig.OpenMode.CREATE_INDEX_AND_TRANSLOG; } else { openMode = EngineConfig.OpenMode.OPEN_INDEX_CREATE_TRANSLOG; } } catch (IOException e) { throw new ElasticsearchException("can't find index?", e); } TranslogConfig translogConfig = new TranslogConfig(shardId, translogPath, indexSettings, BigArrays.NON_RECYCLING_INSTANCE); EngineConfig config = new EngineConfig(openMode, shardId, threadPool, indexSettings , null, store, createSnapshotDeletionPolicy(), mergePolicy, iwc.getAnalyzer(), iwc.getSimilarity() , new CodecService(null, logger), new Engine.EventListener() { @Override public void onFailedEngine(String reason, @Nullable Throwable t) { // we don't need to notify anybody in this test }}, null, IndexSearcher.getDefaultQueryCache(), IndexSearcher.getDefaultQueryCachingPolicy(), translogConfig, TimeValue.timeValueMinutes(5)); return config; }	not sure if it's important here, but we have the same issue - this will not cause an extra commit on open, which we used to do.
public static boolean isFromDocValuesOnly(DataType dataType) { return dataType == KEYWORD // because of ignore_above. Extracting this from _source wouldn't make sense || dataType == DATE // because of date formats || dataType == DATETIME || dataType == SCALED_FLOAT // because of scaling_factor || dataType == CONSTANT_KEYWORD || dataType == GEO_POINT || dataType == SHAPE; }	@elastic/es-ql does anyone know why the geo_shape field was here as isfromdocvaluesonly when the field did not have doc values?
public void testOfficialPluginsIncludesXpack() throws Exception { MockTerminal terminal = new MockTerminal(); new InstallPluginCommand().main(new String[] { "--help" }, terminal); assertTrue(terminal.getOutput(), terminal.getOutput().contains("x-pack")); }	thanks for this test!
@Override public long estimatedNumOperations() { // Grabbing the features from the doc + the depth of the tree return (long)Math.ceil(Math.log(nodes.size())) + featureNames.size(); }	it's probably worth making nodes.isempty() == false an invariant for this class. this line is assuming that nodes.size() > 0, as is line 168. but there are a couple of other places in the file where there are if (nodes.isempty()) { checks. since it's clearly never intended to be empty it's probably best to validate that in the constructor and not bother to check anywhere else.
public void search(Query query, BucketCollector bucketCollector) throws IOException { int seen = 0; query = searcher.rewrite(query); Weight weight = searcher.createWeight(query, bucketCollector.scoreMode(), 1); // Create LeafWalker for each subreader List<LeafWalker> leafWalkers = new ArrayList<>(); for (LeafReaderContext leaf : searcher.getIndexReader().leaves()) { if (++seen % CHECK_CANCELLED_SCORER_INTERVAL == 0) { checkCancelled(); } Scorer scorer = weight.scorer(leaf); if (scorer != null) { LeafWalker leafWalker = new LeafWalker(leaf, scorer, bucketCollector, leaf); if (leafWalker.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) { leafWalkers.add(leafWalker); } } else { // Even though we will not walk through this aggregation as a part of normal processing // this is needed to trigger actions in some bucketCollectors that bypass the normal iteration logic // for example, global aggregator triggers a separate iterator that ignores the query but still needs // to know all leaves bucketCollector.getLeafCollector(new AggregationExecutionContext(leaf, null)); } } PriorityQueue<LeafWalker> queue = new PriorityQueue<>(searcher.getIndexReader().leaves().size()) { @Override protected boolean lessThan(LeafWalker a, LeafWalker b) { return a.timestamp > b.timestamp; } }; // The priority queue is filled for each TSID in order. When a walker moves // to the next TSID it is removed from the queue. Once the queue is empty, // we refill it with walkers positioned on the next TSID. Within the queue // walkers are ordered by timestamp. while (populateQueue(leafWalkers, queue)) { do { if (++seen % CHECK_CANCELLED_SCORER_INTERVAL == 0) { checkCancelled(); } LeafWalker walker = queue.top(); walker.collectCurrent(); if (walker.nextDoc() == DocIdSetIterator.NO_MORE_DOCS || walker.shouldPop()) { queue.pop(); } else { queue.updateTop(); } } while (queue.size() > 0); } }	i have changed the sorting order because i was receiving documents in a wrong timestamp order when iterating over multiple segments. by changing this, the doc order problem was fixed. i think this bug is addressed by this pr: https://github.com/elastic/elasticsearch/pull/85526
private Index[] resolveLocalIndices(OriginalIndices localIndices, IndicesOptions indicesOptions, ClusterState clusterState, SearchTimeProvider timeProvider) { if (localIndices == null) { return Index.EMPTY_ARRAY; //don't search on any local index (happens when only remote indices were specified) } return indexNameExpressionResolver.concreteIndices(clusterState, indicesOptions, localIndices, timeProvider.getAbsoluteStartMillis()); }	i think we should change this resolvelocalindices() method here. i think we should change the indicesoptions parameter to request (and pass down the request from where it is being invoked). then change the concreteindices() method this method invokes. remove the options and indexexpressions parameters and add request parameter (of type indicesrequest). then in the body of this method derive the indicesoptions and indices from the request parameter.
public void testCanMatch() throws Exception { String localIndex = "test_can_match_local_index"; String remoteIndex = "test_can_match_remote_index"; try (RestHighLevelClient localClient = newLocalClient(); RestHighLevelClient remoteClient = newRemoteClient()) { localClient.indices().create(new CreateIndexRequest(localIndex) .settings(Settings.builder().put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, between(5, 20))), RequestOptions.DEFAULT); int localNumDocs = indexDocs(localClient, localIndex, between(10, 100)); remoteClient.indices().create(new CreateIndexRequest(remoteIndex) .settings(Settings.builder().put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, between(5, 20))), RequestOptions.DEFAULT); int remoteNumDocs = indexDocs(remoteClient, remoteIndex, between(10, 100)); configureRemoteClusters(getNodes(remoteClient.getLowLevelClient())); int iterations = between(1, 10); for (int i = 0; i < iterations; i++) { verifySearch(localIndex, localNumDocs, CLUSTER_ALIAS + ":" + remoteIndex, remoteNumDocs, between(1, 10)); } localClient.indices().delete(new DeleteIndexRequest(localIndex), RequestOptions.DEFAULT); remoteClient.indices().delete(new DeleteIndexRequest(remoteIndex), RequestOptions.DEFAULT); } }	i don't know how to verify sent/received can_match requests. previously, i added logs and verified them manually.
public void testRefreshAsDifferentUser() throws IOException { final RestHighLevelClient restClient = new TestRestHighLevelClient(); CreateTokenResponse createTokenResponse = restClient.security() .createToken( CreateTokenRequest.passwordGrant( SecuritySettingsSource.TEST_USER_NAME, SecuritySettingsSourceField.TEST_PASSWORD.toCharArray() ), SECURITY_REQUEST_OPTIONS ); assertNotNull(createTokenResponse.getRefreshToken()); ElasticsearchStatusException e = expectThrows( ElasticsearchStatusException.class, () -> restClient.security() .createToken( CreateTokenRequest.refreshTokenGrant(createTokenResponse.getRefreshToken()), RequestOptions.DEFAULT.toBuilder() .addHeader( "Authorization", UsernamePasswordToken.basicAuthHeaderValue( SecuritySettingsSource.ES_TEST_ROOT_USER, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING ) ) .build() ) ); assertThat(e.getCause().getMessage(), containsString("invalid_grant")); assertEquals(RestStatus.BAD_REQUEST, e.status()); assertThat(e.getCause().getMessage(), containsString("tokens must be refreshed by the creating client")); }	the orignal test was almost identical to testrefreshingtoken. i therefore recycled it into a token create/refresh with run-as .
private void disableTaskByType(TaskContainer tasks, Class<? extends Task> type) { tasks.withType(type, task -> { task.getLogger().info("test fixtures: Disabling tasks of type {}", type); task.setEnabled(false); }); }	maybe log this only at the debug level?
* @throws InterruptedException if the thread was interrupted while blocking on the condition */ public void waitForOpsToComplete(final long seqNo) throws InterruptedException { getEngine().seqNoService().waitForOpsToComplete(seqNo); } /** * Marks the shard with the provided allocation ID as in-sync with the primary shard. See * {@link org.elasticsearch.index.seqno.GlobalCheckpointTracker#markAllocationIdAsInSync(String, long)}	outdated comment? where is the latch?
* @param activeAllocationIds the allocation IDs of the currently active shard copies * @param initializingAllocationIds the allocation IDs of the currently initializing shard copies */ public void updateAllocationIdsFromMaster( final long applyingClusterStateVersion, final Set<String> activeAllocationIds, final Set<String> initializingAllocationIds) { verifyPrimary(); final Engine engine = getEngineOrNull(); // if the engine is not yet started, we are not ready yet and can just ignore this if (engine != null) { engine.seqNoService().updateAllocationIdsFromMaster(applyingClusterStateVersion, activeAllocationIds, initializingAllocationIds); } }	add shardrouting to message?
*/ public PrimaryContext primaryContext() { verifyPrimary(); assert shardRouting.relocating() : "primary context can only be obtained from a relocating primary but was " + shardRouting; assert !shardRouting.isRelocationTarget() : "primary context can only be obtained from relocation source but was " + shardRouting; return getEngine().seqNoService().primaryContext(); }	this follows from the assertion in the previous line (hence is a noop). a relocating shard is always a source shard per definition of shardrouting.relocating
public void finalizeRecovery(final long targetLocalCheckpoint) { if (shard.state() == IndexShardState.CLOSED) { throw new IndexShardClosedException(request.shardId()); } cancellableThreads.checkForCancel(); StopWatch stopWatch = new StopWatch().start(); logger.trace("finalizing recovery"); cancellableThreads.execute(() -> { /* * Before marking the shard as in-sync we acquire an operation permit. We do this so that there is a barrier between marking a * shard as in-sync and relocating a shard. If we acquire the permit then no relocation handoff can complete before we are done * marking the shard as in-sync. If the relocation handoff holds all the permits then after the handoff completes and we acquire * the permit then the state of the shard will be relocated and this recovery will fail. */ final PlainActionFuture<Releasable> onAcquired = new PlainActionFuture<>(); shard.acquirePrimaryOperationPermit(onAcquired, ThreadPool.Names.SAME); try (Releasable ignored = onAcquired.actionGet()) { if (shard.state() == IndexShardState.RELOCATED) { throw new IndexShardRelocatedException(shard.shardId()); } shard.markAllocationIdAsInSync(request.targetAllocationId(), targetLocalCheckpoint); } recoveryTarget.finalizeRecovery(shard.getGlobalCheckpoint()); }); if (request.isPrimaryRelocation()) { // in case of primary relocation we have to ensure that the cluster state on the primary relocation target has all // replica shards that have recovered or are still recovering from the current primary, otherwise replication actions // will not be send to these replicas. To accomplish this, first block new recoveries, then take version of latest cluster // state. This means that no new recovery can be completed based on information of a newer cluster state than the current one. try (Releasable ignored = delayNewRecoveries.apply("primary relocation hand-off in progress or completed for " + shardId)) { final long currentClusterStateVersion = currentClusterStateVersionSupplier.get(); logger.trace("waiting on remote node to have cluster state with version [{}]", currentClusterStateVersion); cancellableThreads.execute(() -> recoveryTarget.ensureClusterStateVersion(currentClusterStateVersion)); logger.trace("performing relocation hand-off"); cancellableThreads.execute( () -> shard.relocated( "to " + request.targetNode(), () -> recoveryTarget.handoffPrimaryContext(shard.primaryContext()))); } /* * if the recovery process fails after setting the shard state to RELOCATED, both relocation source and * target are failed (see {@link IndexShard#updateRoutingEntry}). */ } stopWatch.stop(); logger.trace("finalizing recovery took [{}]", stopWatch.totalTime()); }	i wrote a long paragraph explaining why we would need to do this as i missed this somehow in my initial pass. i would like to have an assertion that no more updates to the globalcheckpointtracker are happening once the primarycontext is sampled and successfully communicated to the primary relocation target. for this, we could introduce a sealed boolean to globalcheckpointtracker. when sampling the primarycontext, this is set to true. if there are any failures then later in the relocated, we could unseal the globalcheckpointtracker again (can be solved nicely with a releaseable that is returned by primarycontext sampling method). we could then add checks on every update method of globalcheckpointtracker that it is not sealed.
public static String parseStringTimestamp(String timestampAsString, FormatDateTimeFormatter dateTimeFormatter) throws TimestampParsingException { try { return Long.toString(dateTimeFormatter.parser().parseMillis(timestampAsString)); } catch (RuntimeException e1) { throw new TimestampParsingException(timestampAsString); } }	i realize this was there before, but can we not lose the original exception?
@Test public void localDependentDateTests() throws Exception { assumeFalse("Locals are buggy on JDK9EA", Constants.JRE_IS_MINIMUM_JAVA9 && systemPropertyAsBoolean("tests.security.manager", false)); assertAcked(prepareCreate("test") .addMapping("type1", jsonBuilder().startObject() .startObject("type1") .startObject("properties") .startObject("date_field") .field("type", "date") .field("format", "E, d MMM yyyy HH:mm:ss Z") .field("locale", "de") .endObject() .endObject() .endObject() .endObject())); ensureGreen(); for (int i = 0; i < 10; i++) { client().prepareIndex("test", "type1", "" + i).setSource("date_field", "Mi, 06 Dez 2000 02:55:00 -0800").execute().actionGet(); client().prepareIndex("test", "type1", "" + (10 + i)).setSource("date_field", "Do, 07 Dez 2000 02:55:00 -0800").execute().actionGet(); } refresh(); for (int i = 0; i < 10; i++) { CountResponse countResponse = client().prepareCount("test") .setQuery(QueryBuilders.rangeQuery("date_field").gte("Di, 05 Dez 2000 02:55:00 -0800").lte("Do, 07 Dez 2000 00:00:00 -0800")) .execute().actionGet(); assertHitCount(countResponse, 10l); countResponse = client().prepareCount("test") .setQuery(QueryBuilders.rangeQuery("date_field").gte("Di, 05 Dez 2000 02:55:00 -0800").lte("Fr, 08 Dez 2000 00:00:00 -0800")) .execute().actionGet(); assertHitCount(countResponse, 20l); } }	does this really need to be an integration test? could it be in simpledatemappingtests?
@Override public void optimize(Optimize optimize) throws EngineException { if (optimizeMutex.compareAndSet(false, true)) { try (InternalLock _ = readLock.acquire()) { final IndexWriter writer = currentIndexWriter(); /* * The way we implement upgrades is a bit hackish in the sense that we set an instance * variable and that this setting will thus apply to the next forced merge that will be run. * This is ok because (1) this is the only place we call forceMerge, (2) we have a single * thread for optimize, and the 'optimizeMutex' guarding this code, and (3) ConcurrentMergeScheduler * syncs calls to findForcedMerges. */ assert writer.getConfig().getMergePolicy() instanceof ElasticsearchMergePolicy; if (optimize.upgrade()) { ((ElasticsearchMergePolicy)writer.getConfig().getMergePolicy()).setNextForceIsUpgrade(true); } if (optimize.onlyExpungeDeletes()) { writer.forceMergeDeletes(false); } else if (optimize.maxNumSegments() <= 0) { writer.maybeMerge(); possibleMergeNeeded = false; } else { writer.forceMerge(optimize.maxNumSegments(), false); } } catch (Throwable t) { maybeFailEngine(t, "optimize"); throw new OptimizeFailedEngineException(shardId, t); } finally { optimizeMutex.set(false); } } // wait for the merges outside of the read lock if (optimize.waitForMerge()) { waitForMerges(optimize.flush()); } else if (optimize.flush()) { // we only need to monitor merges for async calls if we are going to flush threadPool.executor(ThreadPool.Names.OPTIMIZE).execute(new AbstractRunnable() { @Override public void onFailure(Throwable t) { logger.error("Exception while waiting for merges asynchronously after optimize", t); } @Override protected void doRun() throws Exception { waitForMerges(true); } }); } }	can we have an assertion message here to tell what we got instead of the esmp?
private static ObjectParser<Builder, Void> createParser(boolean ignoreUnknownFields) { ObjectParser<Builder, Void> parser = new ObjectParser<>("datafeed_config", ignoreUnknownFields, Builder::new); parser.declareString(Builder::setId, ID); parser.declareString((c, s) -> {}, CONFIG_TYPE); parser.declareString(Builder::setJobId, Job.ID); parser.declareStringArray(Builder::setIndices, INDEXES); parser.declareStringArray(Builder::setIndices, INDICES); parser.declareString((builder, val) -> builder.setQueryDelay(TimeValue.parseTimeValue(val, QUERY_DELAY.getPreferredName())), QUERY_DELAY); parser.declareString((builder, val) -> builder.setFrequency(TimeValue.parseTimeValue(val, FREQUENCY.getPreferredName())), FREQUENCY); parser.declareObject((builder, val) -> builder.setQuery(val, ignoreUnknownFields), (p, c) -> p.mapOrdered(), QUERY); parser.declareObject((builder, val) -> builder.setAggregations(val, ignoreUnknownFields), (p, c) -> p.mapOrdered(), AGGREGATIONS); parser.declareObject((builder, val) -> builder.setAggregations(val, ignoreUnknownFields), (p, c) -> p.mapOrdered(), AGGS); parser.declareObject(Builder::setScriptFields, (p, c) -> { List<SearchSourceBuilder.ScriptField> parsedScriptFields = new ArrayList<>(); while (p.nextToken() != XContentParser.Token.END_OBJECT) { parsedScriptFields.add(new SearchSourceBuilder.ScriptField(p)); } parsedScriptFields.sort(Comparator.comparing(SearchSourceBuilder.ScriptField::fieldName)); return parsedScriptFields; }, SCRIPT_FIELDS); parser.declareInt(Builder::setScrollSize, SCROLL_SIZE); parser.declareObject(Builder::setChunkingConfig, ignoreUnknownFields ? ChunkingConfig.LENIENT_PARSER : ChunkingConfig.STRICT_PARSER, CHUNKING_CONFIG); if (ignoreUnknownFields) { // Headers are not parsed by the strict (config) parser, so headers supplied in the _body_ of a REST request will be rejected. // (For config, headers are explicitly transferred from the auth headers by code in the put/update datafeed actions.) parser.declareObject(Builder::setHeaders, (p, c) -> p.mapStrings(), HEADERS); } parser.declareObject(Builder::setDelayedDataCheckConfig, ignoreUnknownFields ? DelayedDataCheckConfig.LENIENT_PARSER : DelayedDataCheckConfig.STRICT_PARSER, DELAYED_DATA_CHECK_CONFIG); return parser; }	it is possible to refactor this so that specifying _both_ aggs _and_ aggregations is an error. one way to do this is demonstrated in https://github.com/elastic/elasticsearch/pull/38706/files. the other way is in aggregatorfactories, although i think this may be less suitable as it involves hooking into the parser at a lower level, which will complicate this class a lot.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(id); out.writeOptionalString(jobId); out.writeOptionalTimeValue(queryDelay); out.writeOptionalTimeValue(frequency); if (indices != null) { out.writeBoolean(true); out.writeStringCollection(indices); } else { out.writeBoolean(false); } // Write the now removed types to prior versions. // An empty list is expected if (out.getVersion().before(Version.V_7_0_0)) { out.writeBoolean(true); out.writeStringCollection(Collections.emptyList()); } if (out.getVersion().before(Version.V_7_1_0)) { out.writeNamedWriteable(lazyQueryParser.apply(query, id, new ArrayList<>())); out.writeOptionalWriteable(lazyAggParser.apply(aggregations, id, new ArrayList<>())); } else { out.writeMap(query); out.writeBoolean(aggregations != null); if (aggregations != null) { out.writeMap(aggregations); } } if (scriptFields != null) { out.writeBoolean(true); out.writeList(scriptFields); } else { out.writeBoolean(false); } out.writeOptionalVInt(scrollSize); out.writeOptionalWriteable(chunkingConfig); if (out.getVersion().onOrAfter(Version.V_6_6_0)) { out.writeOptionalWriteable(delayedDataCheckConfig); } }	the ouput function needs to be out.writeoptionalnamedwriteable, as that's what the pre-7.1 code expects.
public void testPastQueryConfigParse() throws IOException { try(XContentParser parser = XContentFactory.xContent(XContentType.JSON) .createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, ANACHRONISTIC_QUERY_DATAFEED)) { DatafeedConfig config = DatafeedConfig.LENIENT_PARSER.apply(parser, null).build(); ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> config.getParsedQuery()); assertEquals("[match] query doesn't support multiple fields, found [query] and [type]", e.getCause().getMessage()); } try(XContentParser parser = XContentFactory.xContent(XContentType.JSON) .createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, ANACHRONISTIC_QUERY_DATAFEED)) { XContentParseException e = expectThrows(XContentParseException.class, () -> DatafeedConfig.STRICT_PARSER.apply(parser, null).build()); assertEquals("[6:64] [datafeed_config] failed to parse field [query]", e.getMessage()); } }	it's probably best to assert that e.getcause() is not null before this line and print e, otherwise if it ever does fail there will just be a npe with no indication of what the unexpected exception was. same on lines 252 and 447
public void testRequestStats() throws Exception { final String repository = createRepository(randomName()); final String index = "index-no-merges"; createIndex(index, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .build()); final long nbDocs = randomLongBetween(100, 1000); try (BackgroundIndexer indexer = new BackgroundIndexer(index, "_doc", client(), (int) nbDocs)) { waitForDocs(nbDocs, indexer); } flushAndRefresh(index); ForceMergeResponse forceMerge = client().admin().indices().prepareForceMerge(index).setFlush(true).setMaxNumSegments(1).get(); assertThat(forceMerge.getSuccessfulShards(), equalTo(1)); assertHitCount(client().prepareSearch(index).setSize(0).setTrackTotalHits(true).get(), nbDocs); final String snapshot = "snapshot"; assertSuccessfulSnapshot(client().admin().cluster().prepareCreateSnapshot(repository, snapshot) .setWaitForCompletion(true).setIndices(index)); assertAcked(client().admin().indices().prepareDelete(index)); assertSuccessfulRestore(client().admin().cluster().prepareRestoreSnapshot(repository, snapshot).setWaitForCompletion(true)); ensureGreen(index); assertHitCount(client().prepareSearch(index).setSize(0).setTrackTotalHits(true).get(), nbDocs); assertAcked(client().admin().cluster().prepareDeleteSnapshot(repository, snapshot).get()); final RepositoryStats repositoryStats = StreamSupport.stream( internalCluster().getInstances(RepositoriesService.class).spliterator(), false) .map(repositoriesService -> { try { return repositoriesService.repository(repository); } catch (RepositoryMissingException e) { return null; } }) .filter(r -> r != null) .map(r -> r.stats()) .reduce((s1, s2) -> s1.combine(s2)) .get(); final long sdkGetCalls = repositoryStats.requestCounts.get("GET"); final long sdkListCalls = repositoryStats.requestCounts.get("LIST"); final long getCalls = handlers.values().stream() .mapToLong(h -> { if (h instanceof S3HttpHandler) { return ((S3HttpHandler) h).getCalls.get(); } else if (h instanceof S3ErroneousHttpHandler) { return ((S3ErroneousHttpHandler) h).getCalls.get(); } else { return 0L; } }) .sum(); final long listCalls = handlers.values().stream().filter(h -> h instanceof S3HttpHandler) .mapToLong(h -> { if (h instanceof S3HttpHandler) { return ((S3HttpHandler) h).listCalls.get(); } else if (h instanceof S3ErroneousHttpHandler) { return ((S3ErroneousHttpHandler) h).listCalls.get(); } else { return 0L; } }) .sum(); logger.info("SDK sent {} GET calls and handler measured {} GET calls", sdkGetCalls, getCalls); logger.info("SDK sent {} LIST calls and handler measured {} LIST calls", sdkListCalls, listCalls); assertEquals(getCalls, sdkGetCalls); assertEquals(listCalls, sdkListCalls); }	this won't work (i think that's the reason for the test failure i mentioned). the handlers field does not contain the erroneous handler wrappers.see org.elasticsearch.repositories.blobstore.esmockapibasedrepositoryintegtestcase#setuphttpserver: @before public void setuphttpserver() { handlers = createhttphandlers(); handlers.foreach((c, h) -> httpserver.createcontext(c, wrap(randomboolean() ? createerroneoushttphandler(h) : h, logger))); } you'll have to put the erroneous handlers into another map (so you get the counts from both the normal and the erroneous wrappers by going over both maps) or similar.
public void testRequestStats() throws Exception { final String repository = createRepository(randomName()); final String index = "index-no-merges"; createIndex(index, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .build()); final long nbDocs = randomLongBetween(100, 1000); try (BackgroundIndexer indexer = new BackgroundIndexer(index, "_doc", client(), (int) nbDocs)) { waitForDocs(nbDocs, indexer); } flushAndRefresh(index); ForceMergeResponse forceMerge = client().admin().indices().prepareForceMerge(index).setFlush(true).setMaxNumSegments(1).get(); assertThat(forceMerge.getSuccessfulShards(), equalTo(1)); assertHitCount(client().prepareSearch(index).setSize(0).setTrackTotalHits(true).get(), nbDocs); final String snapshot = "snapshot"; assertSuccessfulSnapshot(client().admin().cluster().prepareCreateSnapshot(repository, snapshot) .setWaitForCompletion(true).setIndices(index)); assertAcked(client().admin().indices().prepareDelete(index)); assertSuccessfulRestore(client().admin().cluster().prepareRestoreSnapshot(repository, snapshot).setWaitForCompletion(true)); ensureGreen(index); assertHitCount(client().prepareSearch(index).setSize(0).setTrackTotalHits(true).get(), nbDocs); assertAcked(client().admin().cluster().prepareDeleteSnapshot(repository, snapshot).get()); final RepositoryStats repositoryStats = StreamSupport.stream( internalCluster().getInstances(RepositoriesService.class).spliterator(), false) .map(repositoriesService -> { try { return repositoriesService.repository(repository); } catch (RepositoryMissingException e) { return null; } }) .filter(r -> r != null) .map(r -> r.stats()) .reduce((s1, s2) -> s1.combine(s2)) .get(); final long sdkGetCalls = repositoryStats.requestCounts.get("GET"); final long sdkListCalls = repositoryStats.requestCounts.get("LIST"); final long getCalls = handlers.values().stream() .mapToLong(h -> { if (h instanceof S3HttpHandler) { return ((S3HttpHandler) h).getCalls.get(); } else if (h instanceof S3ErroneousHttpHandler) { return ((S3ErroneousHttpHandler) h).getCalls.get(); } else { return 0L; } }) .sum(); final long listCalls = handlers.values().stream().filter(h -> h instanceof S3HttpHandler) .mapToLong(h -> { if (h instanceof S3HttpHandler) { return ((S3HttpHandler) h).listCalls.get(); } else if (h instanceof S3ErroneousHttpHandler) { return ((S3ErroneousHttpHandler) h).listCalls.get(); } else { return 0L; } }) .sum(); logger.info("SDK sent {} GET calls and handler measured {} GET calls", sdkGetCalls, getCalls); logger.info("SDK sent {} LIST calls and handler measured {} LIST calls", sdkListCalls, listCalls); assertEquals(getCalls, sdkGetCalls); assertEquals(listCalls, sdkListCalls); }	you probably don't want this filter because it prevents you from seeing the erroneous handlers below in the mapping.
public void putAsync(ILMHistoryItem item) { if (ilmHistoryEnabled == false) { logger.trace( "not recording ILM history item because [{}] is [false]: [{}]", LIFECYCLE_HISTORY_INDEX_ENABLED_SETTING.getKey(), item ); return; } logger.trace("queueing ILM history item for indexing [{}]: [{}]", ILM_HISTORY_DATA_STREAM, item); try (XContentBuilder builder = XContentFactory.jsonBuilder()) { item.toXContent(builder, ToXContent.EMPTY_PARAMS); IndexRequest request = new IndexRequest(ILM_HISTORY_DATA_STREAM).source(builder).opType(DocWriteRequest.OpType.CREATE); // TODO: remove the threadpool wrapping when the .add call is non-blocking // (it can currently execute the bulk request occasionally) // see: https://github.com/elastic/elasticsearch/issues/50440 threadPool.executor(ThreadPool.Names.MANAGEMENT).execute(() -> { try { processor.add(request); } catch (Exception e) { logger.error( new ParameterizedMessage( "failed add ILM history item to queue for index [{}]: [{}]", ILM_HISTORY_DATA_STREAM, item ), e ); } }); } catch (IOException exception) { logger.error( new ParameterizedMessage("failed to queue ILM history item in index [{}]: [{}]", ILM_HISTORY_DATA_STREAM, item), exception ); } }	i thought about this some more and looked at some recent sdhs. we already have a bit of an issue with slow management threads from stats. i think, even though this is a theoretically more correct. fix as we discussed, this has quite some potential to end weird (thinking about submitting 10k of these tasks in a hot loop and 5 of them blocking the management pool forever if the data node is slow for some reason). -> i'd say, lets just stick with generic here and assume that bulk size limit increase will be all that's needed. maybe we can go for a configurable value here via an undocumented system property just so we have an out n case this causes trouble or is not enough in some corner case?
public void usedDeprecatedName(String parserName, Supplier<XContentLocation> location, String usedName, String modernName) { if (parserName != null) { deprecationLogger.deprecated("[{}][{}] Deprecated field [{}] used, expected [{}] instead", parserName, location.get(), usedName, modernName); } else { deprecationLogger.deprecated("Deprecated field [{}] used, expected [{}] instead", usedName, modernName); } }	the only suggestion i'd make is about whether it's possible to consolidate the log messages. for example: suggestion string prefix = parsername == null ? "" : "[" + parsername + "][" + location.get() + "] "; deprecationlogger.deprecated("{}deprecated field [{}] used, expected [{}] instead", prefix, usedname, modernname);
public void writeTo(StreamOutput out) throws IOException { out.writeString(specId); if (testRequest != null) { out.writeBoolean(true); testRequest.writeTo(out); } else { out.writeBoolean(false); } out.writeInt(indices.size()); for (String index : indices) { out.writeString(index); } out.writeInt(types.size()); for (String type : types) { out.writeString(type); } out.writeInt(ratedDocs.size()); for (RatedDocument ratedDoc : ratedDocs) { ratedDoc.writeTo(out); } out.writeMap(params); out.writeInt(summaryFields.size()); for (String fieldName : summaryFields) { out.writeString(fieldName); } }	nit: you could use writeoptionalwriteable() here
protected void doExecute(RankEvalRequest request, ActionListener<RankEvalResponse> listener) { RankEvalSpec qualityTask = request.getRankEvalSpec(); Collection<RatedRequest> specifications = qualityTask.getSpecifications(); AtomicInteger responseCounter = new AtomicInteger(specifications.size()); Map<String, EvalQueryQuality> partialResults = new ConcurrentHashMap<>(specifications.size()); Map<String, Exception> errors = new ConcurrentHashMap<>(specifications.size()); for (RatedRequest querySpecification : specifications) { final RankEvalActionListener searchListener = new RankEvalActionListener(listener, qualityTask.getMetric(), querySpecification, partialResults, errors, responseCounter); SearchSourceBuilder specRequest = querySpecification.getTestRequest(); if (specRequest == null) { Map<String, Object> params = querySpecification.getParams(); Script scriptWithParams = new Script(qualityTask.getTemplate().getType(), qualityTask.getTemplate().getLang(), qualityTask.getTemplate().getIdOrCode(), params); String resolvedRequest = ((BytesReference) (scriptService.executable(scriptWithParams, ScriptContext.Standard.SEARCH) .run())).utf8ToString(); logger.error("RANTRANTRANT" + resolvedRequest); try (XContentParser subParser = XContentFactory.xContent(resolvedRequest).createParser(resolvedRequest)) { QueryParseContext parseContext = new QueryParseContext(searchRequestParsers.queryParsers, subParser, parseFieldMatcher); specRequest = SearchSourceBuilder.fromXContent(parseContext, searchRequestParsers.aggParsers, searchRequestParsers.suggesters, searchRequestParsers.searchExtParsers); } catch (IOException e) { listener.onFailure(e); } } List<String> summaryFields = querySpecification.getSummaryFields(); if (summaryFields.isEmpty()) { specRequest.fetchSource(false); } else { specRequest.fetchSource(summaryFields.toArray(new String[summaryFields.size()]), new String[0]); } String[] indices = new String[querySpecification.getIndices().size()]; querySpecification.getIndices().toArray(indices); SearchRequest templatedRequest = new SearchRequest(indices, specRequest); String[] types = new String[querySpecification.getTypes().size()]; querySpecification.getTypes().toArray(types); templatedRequest.types(types); client.search(templatedRequest, searchListener); } }	just a question: this means we are failing the whole rank_eval request here, no? i think this is okay, as long as this is what you intended.
public void testSimulatePipeline() throws IOException { XContentType xContentType = randomFrom(XContentType.values()); XContentBuilder builder = XContentBuilder.builder(xContentType.xContent()); boolean isVerbose = randomBoolean(); builder.startObject(); { builder.field("pipeline"); buildRandomXContentPipeline(builder); builder.startArray("docs"); { builder.startObject() .field("_index", "index") .field("_type", "doc") .field("_id", "doc_" + 1) .startObject("_source").field("foo", "rab_" + 1).field("rank", "1234").endObject() .endObject(); builder.startObject() .field("_index", "index") .field("_type", "doc") .field("_id", "doc_" + 2) .startObject("_source").field("foo", "rab_" + 1).field("rank", "non-int").endObject() .endObject(); } builder.endArray(); } builder.endObject(); SimulatePipelineRequest request = new SimulatePipelineRequest( BytesReference.bytes(builder), builder.contentType() ); request.setVerbose(isVerbose); SimulatePipelineResponse simulatePipelineResponse = execute(request, highLevelClient().ingest()::simulatePipeline, highLevelClient().ingest()::simulatePipelineAsync); SimulateDocumentResult result0 = simulatePipelineResponse.getResults().get(0); SimulateDocumentResult result1 = simulatePipelineResponse.getResults().get(1); if (isVerbose) { assertTrue(result0 instanceof SimulateDocumentVerboseResult); SimulateDocumentVerboseResult verboseResult = (SimulateDocumentVerboseResult)result0; SimulateDocumentVerboseResult failedVerboseResult = (SimulateDocumentVerboseResult)result1; assertTrue(verboseResult.getProcessorResults().size() > 0); assertEquals( verboseResult.getProcessorResults().get(0).getIngestDocument() .getFieldValue("foo", String.class), "bar" ); assertEquals( Integer.valueOf(1234), verboseResult.getProcessorResults().get(1).getIngestDocument() .getFieldValue("rank", Integer.class) ); assertNotNull(failedVerboseResult.getProcessorResults().get(1).getFailure()); } else { assertTrue(result0 instanceof SimulateDocumentBaseResult); SimulateDocumentBaseResult baseResult = (SimulateDocumentBaseResult)result0; SimulateDocumentBaseResult failedBaseResult = (SimulateDocumentBaseResult)result1; assertNotNull(baseResult.getIngestDocument()); assertEquals( baseResult.getIngestDocument().getFieldValue("foo", String.class), "bar" ); assertEquals( Integer.valueOf(1234), baseResult.getIngestDocument() .getFieldValue("rank", Integer.class) ); assertNotNull(failedBaseResult.getFailure()); } }	i think i have left a comment on this before, can we avoid randomizing values in integration tests and rather have two tests, one for verbose and one without verbose? same for failures
public void testDynamicDisabled() throws IOException { Settings settings = Settings.settingsBuilder().put("index.mapper.dynamic", false).build(); String name = internalCluster().startNode(settings); try { client(name).prepareIndex("index", "type", "1").setSource("foo", 3).get(); fail("Indexing request should have failed"); } catch (TypeMissingException e) { // expected } }	this setting is per index and it is honored every time (no bug). what's not working is when you set this setting for all indices. to do this you can override nodesettings in your test: protected settings nodesettings(int nodeordinal) { return settingsbuilder().put(super.nodesettings(nodeordinal)) .put("index.mapper.dynamic", "false") .build(); }
public void testRestoreIndexWithShardsMissingInLocalGateway() throws Exception { logger.info("--> start 2 nodes"); internalCluster().startNodes(2); cluster().wipeIndices("_all"); createRepository("test-repo", "fs"); int numberOfShards = 6; logger.info("--> create an index that will have some unallocated shards"); assertAcked(prepareCreate("test-idx", 2, indexSettingsNoReplicas(numberOfShards))); ensureGreen(); indexRandomDocs("test-idx", 100); logger.info("--> force merging down to a single segment to get a deterministic set of files"); assertEquals(client().admin().indices().prepareForceMerge("test-idx").setMaxNumSegments(1).setFlush(true).get().getFailedShards(), 0); createSnapshot("test-repo", "test-snap-1", Collections.singletonList("test-idx")); logger.info("--> close the index"); assertAcked(client().admin().indices().prepareClose("test-idx")); logger.info("--> shutdown one of the nodes that should make half of the shards unavailable"); internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() { @Override public boolean clearData(String nodeName) { return true; } }); assertThat(clusterAdmin().prepareHealth().setWaitForEvents(Priority.LANGUID).setTimeout("1m").setWaitForNodes("2") .execute().actionGet().isTimedOut(), equalTo(false)); logger.info("--> restore index snapshot"); assertThat(clusterAdmin().prepareRestoreSnapshot("test-repo", "test-snap-1").setRestoreGlobalState(false) .setWaitForCompletion(true).get().getRestoreInfo().successfulShards(), equalTo(6)); ensureGreen("test-idx"); IntSet reusedShards = new IntHashSet(); List<RecoveryState> recoveryStates = client().admin().indices().prepareRecoveries("test-idx").get() .shardRecoveryStates().get("test-idx"); for (RecoveryState recoveryState : recoveryStates) { if (recoveryState.getIndex().reusedBytes() > 0) { reusedShards.add(recoveryState.getShardId().getId()); } } logger.info("--> check that at least half of the shards had some reuse: [{}]", reusedShards); assertThat(reusedShards.size(), greaterThanOrEqualTo(numberOfShards / 2)); }	these are unnecessary, we already set this rebalance to none in the parent (abstractsnapshotintegtestcase)
private synchronized void reQueueThrottledSearch() { currentMaximumRequestsPerSecond = getMaximumRequestsPerSecond(); if (scheduledNextSearch != null) { TimeValue executionDelay = calculateThrottlingDelay( currentMaximumRequestsPerSecond, lastDocCount, lastSearchStartTimeNanos, getTimeNanos() ); logger.debug( "rethrottling job [{}], wait for {} ({} {})", getJobId(), executionDelay, currentMaximumRequestsPerSecond, lastDocCount ); scheduledNextSearch.reschedule(executionDelay); } }	this formula implies that requestspersecond is really desireddocspersecond. if that's correct then requestspersecond seems like it will cause confusion in the future because i would assume requestspersecond referred to the number of searches, each of which could return many documents. for example, if i saw a configuration parameter requests_per_second i might decide to set it to 2 so that i'd get a maximum of 2 search requests per second from this functionality. but then if one of my searches returns 1000 documents then i get a 500 second wait until the next search.
public BinaryScriptLeafFieldData loadDirect(LeafReaderContext context) throws Exception { IpFieldScript script = leafFactory.newInstance(context); return new BinaryScriptLeafFieldData() { @Override public DocValuesField<?> getScriptField(String name) { return new IpDocValuesField(getBytesValues(), name); } @Override public SortedBinaryDocValues getBytesValues() { return new org.elasticsearch.index.fielddata.IpScriptDocValues(script); } }; }	this should take in a toscriptfield and follow our standard plumbing pattern to this point. i think there's a real possibility this may be required for source fallback.
FlushOperation buildNetworkFlushOperation() { int pageCount = pages.size(); ByteBuffer[] byteBuffers = new ByteBuffer[pageCount]; Page[] pagesToClose = new Page[pageCount]; for (int i = 0; i < pageCount; ++i) { Page page = pages.removeFirst(); pagesToClose[i] = page; byteBuffers[i] = page.byteBuffer(); } return new FlushOperation(byteBuffers, (r, e) -> { try { IOUtils.close(pagesToClose); } catch (Exception ex) { ex.addSuppressed(e); assert false : ex; throw new ElasticsearchException(ex); } }); }	e is normally null which i think is forbidden to suppress?
private void checkForBearerToken() { final SecureString bearerToken = tokenService.extractBearerTokenFromHeader(threadContext); serviceAccountService.tryAuthenticateBearerToken(bearerToken, nodeName, ActionListener.wrap(authentication -> { if (authentication != null) { this.authenticatedBy = authentication.getAuthenticatedBy(); writeAuthToContext(authentication); } else { tokenService.tryAuthenticateToken(bearerToken, ActionListener.wrap(userToken -> { if (userToken != null) { writeAuthToContext(userToken.getAuthentication()); } else { checkForApiKey(); } }, e -> { logger.debug(new ParameterizedMessage("Failed to validate token authentication for request [{}]", request), e); if (e instanceof ElasticsearchSecurityException && false == tokenService.isExpiredTokenException((ElasticsearchSecurityException) e)) { // intentionally ignore the returned exception; we call this primarily // for the auditing as we already have a purpose built exception request.tamperedRequest(); } listener.onFailure(e); })); } }, e -> { logger.debug(new ParameterizedMessage("Failed to validate service account token for request [{}]", request), e); listener.onFailure(request.exceptionProcessingRequest(e, null)); })); }	i think we need to revisit the null here - we should be auditing with an authenticationtoken since we have a credential.
@Override protected void restoreFiles(List<FileInfo> filesToRecover, Store store) { logger.trace("[{}] starting CCR restore of {} files", shardId, filesToRecover); final PlainActionFuture<Void> restoreFilesFuture = new PlainActionFuture<>(); final List<StoreFileMetaData> mds = filesToRecover.stream().map(FileInfo::metadata).collect(Collectors.toList()); final MultiFileTransfer<FileChunk> multiFileTransfer = new MultiFileTransfer<>( logger, threadPool.getThreadContext(), restoreFilesFuture, ccrSettings.getMaxConcurrentFileChunks(), mds) { final MultiFileWriter multiFileWriter = new MultiFileWriter(store, recoveryState.getIndex(), "", logger, () -> {}); long offset = 0; @Override protected void onNewFile(StoreFileMetaData md) { offset = 0; } @Override protected FileChunk nextChunkRequest(StoreFileMetaData md) { final int bytesRequested = Math.toIntExact(Math.min(ccrSettings.getChunkSize().getBytes(), md.length() - offset)); offset += bytesRequested; return new FileChunk(md, bytesRequested, offset == md.length()); } @Override protected void sendChunkRequest(FileChunk request, ActionListener<Void> listener) { final ActionListener<GetCcrRestoreFileChunkAction.GetCcrRestoreFileChunkResponse> threadedListener = new ThreadedActionListener<>(logger, threadPool, ThreadPool.Names.GENERIC, ActionListener.wrap( r -> { writeFileChunk(request.md, r); listener.onResponse(null); }, listener::onFailure), false); remoteClient.execute(GetCcrRestoreFileChunkAction.INSTANCE, new GetCcrRestoreFileChunkRequest(node, sessionUUID, request.md.name(), request.bytesRequested), ListenerTimeouts.wrapWithTimeout(threadPool, threadedListener, ccrSettings.getRecoveryActionTimeout(), ThreadPool.Names.GENERIC, GetCcrRestoreFileChunkAction.NAME)); } private void writeFileChunk(StoreFileMetaData md, GetCcrRestoreFileChunkAction.GetCcrRestoreFileChunkResponse r) throws Exception { final int actualChunkSize = r.getChunk().length(); logger.trace("[{}] [{}] got response for file [{}], offset: {}, length: {}", shardId, snapshotId, md.name(), r.getOffset(), actualChunkSize); final long nanosPaused = ccrSettings.getRateLimiter().maybePause(actualChunkSize); throttleListener.accept(nanosPaused); multiFileWriter.incRef(); try (Releasable ignored = multiFileWriter::decRef) { final boolean lastChunk = r.getOffset() + actualChunkSize >= md.length(); multiFileWriter.writeFileChunk(md, r.getOffset(), r.getChunk(), lastChunk); } catch (Exception e) { handleError(md, e); throw e; } } @Override protected void handleError(StoreFileMetaData md, Exception e) throws Exception { final IOException corruptIndexException; if ((corruptIndexException = ExceptionsHelper.unwrapCorruption(e)) != null) { try { store.markStoreCorrupted(corruptIndexException); } catch (IOException ioe) { logger.warn("store cannot be marked as corrupted", e); } throw corruptIndexException; } throw e; } @Override public void close() { multiFileWriter.close(); } }; multiFileTransfer.start(); restoreFilesFuture.actionGet(ccrSettings.getRecoveryActionTimeout()); logger.trace("[{}] completed CCR restore", shardId); }	i wonder if we should call this handlechunkrequest or executechunkrequest
@Override protected void restoreFiles(List<FileInfo> filesToRecover, Store store) { logger.trace("[{}] starting CCR restore of {} files", shardId, filesToRecover); final PlainActionFuture<Void> restoreFilesFuture = new PlainActionFuture<>(); final List<StoreFileMetaData> mds = filesToRecover.stream().map(FileInfo::metadata).collect(Collectors.toList()); final MultiFileTransfer<FileChunk> multiFileTransfer = new MultiFileTransfer<>( logger, threadPool.getThreadContext(), restoreFilesFuture, ccrSettings.getMaxConcurrentFileChunks(), mds) { final MultiFileWriter multiFileWriter = new MultiFileWriter(store, recoveryState.getIndex(), "", logger, () -> {}); long offset = 0; @Override protected void onNewFile(StoreFileMetaData md) { offset = 0; } @Override protected FileChunk nextChunkRequest(StoreFileMetaData md) { final int bytesRequested = Math.toIntExact(Math.min(ccrSettings.getChunkSize().getBytes(), md.length() - offset)); offset += bytesRequested; return new FileChunk(md, bytesRequested, offset == md.length()); } @Override protected void sendChunkRequest(FileChunk request, ActionListener<Void> listener) { final ActionListener<GetCcrRestoreFileChunkAction.GetCcrRestoreFileChunkResponse> threadedListener = new ThreadedActionListener<>(logger, threadPool, ThreadPool.Names.GENERIC, ActionListener.wrap( r -> { writeFileChunk(request.md, r); listener.onResponse(null); }, listener::onFailure), false); remoteClient.execute(GetCcrRestoreFileChunkAction.INSTANCE, new GetCcrRestoreFileChunkRequest(node, sessionUUID, request.md.name(), request.bytesRequested), ListenerTimeouts.wrapWithTimeout(threadPool, threadedListener, ccrSettings.getRecoveryActionTimeout(), ThreadPool.Names.GENERIC, GetCcrRestoreFileChunkAction.NAME)); } private void writeFileChunk(StoreFileMetaData md, GetCcrRestoreFileChunkAction.GetCcrRestoreFileChunkResponse r) throws Exception { final int actualChunkSize = r.getChunk().length(); logger.trace("[{}] [{}] got response for file [{}], offset: {}, length: {}", shardId, snapshotId, md.name(), r.getOffset(), actualChunkSize); final long nanosPaused = ccrSettings.getRateLimiter().maybePause(actualChunkSize); throttleListener.accept(nanosPaused); multiFileWriter.incRef(); try (Releasable ignored = multiFileWriter::decRef) { final boolean lastChunk = r.getOffset() + actualChunkSize >= md.length(); multiFileWriter.writeFileChunk(md, r.getOffset(), r.getChunk(), lastChunk); } catch (Exception e) { handleError(md, e); throw e; } } @Override protected void handleError(StoreFileMetaData md, Exception e) throws Exception { final IOException corruptIndexException; if ((corruptIndexException = ExceptionsHelper.unwrapCorruption(e)) != null) { try { store.markStoreCorrupted(corruptIndexException); } catch (IOException ioe) { logger.warn("store cannot be marked as corrupted", e); } throw corruptIndexException; } throw e; } @Override public void close() { multiFileWriter.close(); } }; multiFileTransfer.start(); restoreFilesFuture.actionGet(ccrSettings.getRecoveryActionTimeout()); logger.trace("[{}] completed CCR restore", shardId); }	this would allow recovery from remote to only run for 60 seconds before aborting. i think we want to indefinitely wait here and only have the timeouts on the individual requests.
@Override public InferencePipelineAggregationBuilder rewrite(QueryRewriteContext context) { if (model != null) { return this; } SetOnce<LocalModel> loadedModel = new SetOnce<>(); BiConsumer<Client, ActionListener<?>> modelLoadAction = (client, listener) -> modelLoadingService.get().getModelForSearch(modelId, ActionListener.delegateFailure(listener, (delegate, model) -> { loadedModel.set(model); boolean isLicensed = licenseState.checkFeature(XPackLicenseState.Feature.MACHINE_LEARNING) || licenseState.isAllowedByLicense(model.getLicenseLevel()); if (isLicensed) { delegate.onResponse(null); } else { delegate.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)); } })); context.registerAsyncAction((client, listener) -> { if (licenseState.isSecurityEnabled()) { // check the user has ml privileges SecurityContext securityContext =new SecurityContext(Settings.EMPTY, client.threadPool().getThreadContext()); useSecondaryAuthIfAvailable(securityContext, () -> { final String username = securityContext.getUser().principal(); final HasPrivilegesRequest privRequest = new HasPrivilegesRequest(); privRequest.username(username); privRequest.clusterPrivileges("manage_ml", "monitor_ml"); privRequest.indexPrivileges(new RoleDescriptor.IndicesPrivileges[]{}); privRequest.applicationPrivileges(new RoleDescriptor.ApplicationResourcePrivileges[]{}); ActionListener<HasPrivilegesResponse> privResponseListener = ActionListener.wrap( r -> { if (hasMlPrivilege(r)) { modelLoadAction.accept(client, listener); } else { listener.onFailure(Exceptions.authorizationError( "user [" + username + "] is not an ml user and does not have sufficient privilege " + "to use ml inference")); } }, listener::onFailure); client.execute(HasPrivilegesAction.INSTANCE, privRequest, privResponseListener); }); } else { modelLoadAction.accept(client, listener); } }); return new InferencePipelineAggregationBuilder(name, bucketPathMap, loadedModel::get, modelId, inferenceConfig, licenseState); }	does this stop really high privilege users from using inference? for example, the actions permitted by manage_ml are a subset of those permitted by manage (which means manage everything except security), but literally checking for manage_ml might not return true for a user who has manage. i may be wrong as i haven't tested it, but it seems likely that the security code will resolve high level cluster privilege names to sets of actions they allow, but not to other high level cluster privilege names. you could check for just gettrainedmodelsaction.name here instead to solve that potential problem and also make the later code simpler as you'll only be testing for one privilege: suggestion privrequest.clusterprivileges(gettrainedmodelsaction.name);
@Override public InferencePipelineAggregationBuilder rewrite(QueryRewriteContext context) { if (model != null) { return this; } SetOnce<LocalModel> loadedModel = new SetOnce<>(); BiConsumer<Client, ActionListener<?>> modelLoadAction = (client, listener) -> modelLoadingService.get().getModelForSearch(modelId, ActionListener.delegateFailure(listener, (delegate, model) -> { loadedModel.set(model); boolean isLicensed = licenseState.checkFeature(XPackLicenseState.Feature.MACHINE_LEARNING) || licenseState.isAllowedByLicense(model.getLicenseLevel()); if (isLicensed) { delegate.onResponse(null); } else { delegate.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)); } })); context.registerAsyncAction((client, listener) -> { if (licenseState.isSecurityEnabled()) { // check the user has ml privileges SecurityContext securityContext =new SecurityContext(Settings.EMPTY, client.threadPool().getThreadContext()); useSecondaryAuthIfAvailable(securityContext, () -> { final String username = securityContext.getUser().principal(); final HasPrivilegesRequest privRequest = new HasPrivilegesRequest(); privRequest.username(username); privRequest.clusterPrivileges("manage_ml", "monitor_ml"); privRequest.indexPrivileges(new RoleDescriptor.IndicesPrivileges[]{}); privRequest.applicationPrivileges(new RoleDescriptor.ApplicationResourcePrivileges[]{}); ActionListener<HasPrivilegesResponse> privResponseListener = ActionListener.wrap( r -> { if (hasMlPrivilege(r)) { modelLoadAction.accept(client, listener); } else { listener.onFailure(Exceptions.authorizationError( "user [" + username + "] is not an ml user and does not have sufficient privilege " + "to use ml inference")); } }, listener::onFailure); client.execute(HasPrivilegesAction.INSTANCE, privRequest, privResponseListener); }); } else { modelLoadAction.accept(client, listener); } }); return new InferencePipelineAggregationBuilder(name, bucketPathMap, loadedModel::get, modelId, inferenceConfig, licenseState); }	this method won't be needed if just checking one privilege.
@Override public String toString() { String sSource = "_na_"; try { if (source.length() > MAX_SOURCE_LENGTH_IN_TOSTRING) { sSource = "too big: " + ByteSizeValue.formatBytesSizeValue(source.length()); } else { sSource = XContentHelper.convertToJson(source, false); } } catch (Exception e) { // ignore } return "index {[" + index + "][" + type + "][" + id + "], source[" + sSource + "]}"; }	what about "source is too big, max length [2048b] but was [xxx]"
private static MappingLookup createMappingLookup() { return new MappingLookup(Mapping.EMPTY, emptyList(), emptyList(), emptyList(), null, null, null); }	i think you can use mappinglookup.empty here?
private void getAutoconfiguredUser(final String username, SecureString credentials, ActionListener<ReservedUserInfo> listener) { if (autoconfigured && username.equals(ElasticUser.NAME) && bootstrapUserInfo.verifyPassword(credentials)) { nativeUsersStore.storeAutoconfiguredElasticUser(bootstrapUserInfo, listener); } else { listener.onResponse(getDefaultUserInfo(username)); } }	naming is hard.. does this get the/an auto configured user ? don't have a good suggestion
private void registerGeoShapeGridAggregators(ValuesSourceRegistry.Builder builder) { builder.register(GeoHashGridAggregationBuilder.NAME, GeoShapeValuesSourceType.instance(), (GeoGridAggregatorSupplier) (name, factories, valuesSource, precision, geoBoundingBox, requiredSize, shardSize, aggregationContext, parent, metadata) -> { if (getLicenseState().isAllowed(XPackLicenseState.Feature.SPATIAL_GEO_GRID)) { final GeoGridTiler tiler; if (geoBoundingBox.isUnbounded()) { tiler = new GeoHashGridTiler(); } else { tiler = new BoundedGeoHashGridTiler(geoBoundingBox); } GeoShapeCellIdSource cellIdSource = new GeoShapeCellIdSource((GeoShapeValuesSource) valuesSource, precision, tiler); GeoShapeHashGridAggregator agg = new GeoShapeHashGridAggregator(name, factories, cellIdSource, requiredSize, shardSize, aggregationContext, parent, metadata); cellIdSource.setCircuitBreakerConsumer(agg::addRequestBytes); return agg; } throw LicenseUtils.newComplianceException("geohash_grid aggregation on geo_shape fields"); }); builder.register(GeoTileGridAggregationBuilder.NAME, GeoShapeValuesSourceType.instance(), (GeoGridAggregatorSupplier) (name, factories, valuesSource, precision, geoBoundingBox, requiredSize, shardSize, aggregationContext, parent, metadata) -> { if (getLicenseState().isAllowed(XPackLicenseState.Feature.SPATIAL_GEO_GRID)) { final GeoGridTiler tiler; if (geoBoundingBox.isUnbounded()) { tiler = new GeoTileGridTiler(); } else { tiler = new BoundedGeoTileGridTiler(geoBoundingBox); } GeoShapeCellIdSource cellIdSource = new GeoShapeCellIdSource((GeoShapeValuesSource) valuesSource, precision, tiler); GeoShapeTileGridAggregator agg = new GeoShapeTileGridAggregator(name, factories, cellIdSource, requiredSize, shardSize, aggregationContext, parent, metadata); cellIdSource.setCircuitBreakerConsumer(agg::addRequestBytes); return agg; } throw LicenseUtils.newComplianceException("geotile_grid aggregation on geo_shape fields"); }); }	i think i figured out the circuit-breaking logic! this is my least favorite part. a hack to give geoshapecellidsource access to aggregatorbase's circuit-breaker logic, which must be created before the aggregator is created.
private static void verifyDefaultInstallation(Installation es, Distribution distribution) throws IOException { Stream.of( "elasticsearch-certgen", "elasticsearch-certutil", "elasticsearch-croneval", "elasticsearch-saml-metadata", "elasticsearch-setup-passwords", "elasticsearch-sql-cli", "elasticsearch-syskeygen", "elasticsearch-users", "elasticsearch-service-tokens", "x-pack-env", "x-pack-security-env", "x-pack-watcher-env" ).forEach(executable -> assertThat(es.bin(executable), file(File, "root", "root", p755))); // at this time we only install the current version of archive distributions, but if that changes we'll need to pass // the version through here assertThat(es.bin("elasticsearch-sql-cli-" + distribution.version + ".jar"), file(File, "root", "root", p755)); Stream.of("users", "users_roles", "roles.yml", "role_mapping.yml", "log4j2.properties") .forEach(configFile -> assertThat(es.config(configFile), file(File, "root", "elasticsearch", p660))); verifySecurityAutoConfigured(es, distribution); }	i'd really rather not continue to add test coverage as assertions to verifyinstallation(). we should implement this as a new distinct test suite. we also need to add _a lot_ more coverage here for all the other scenarios as this only covers the "happy path". for example: - verify the correct behavior (still under debate) when elasticsearch.yml file is non-writable - verify we don't auto config security if security settings already exist - verify we don't auto config security on an upgrade there are likely other more specific scenarios that probably are better implemented as unit tests for the cli tool for which no tests currently yet exist.
public synchronized void startDisrupting() { if (suspendedThreads == null) { boolean success = false; try { suspendedThreads = ConcurrentHashMap.newKeySet(); final String currentThreadName = Thread.currentThread().getName(); assert currentThreadName.contains("[" + disruptedNode + "]") == false : "current thread match pattern. thread name: " + currentThreadName + ", node: " + disruptedNode; // we spawn a background thread to protect against deadlock which can happen // if there are shared resources between caller thread and and suspended threads // see unsafeClasses to how to avoid that final AtomicReference<Exception> stoppingError = new AtomicReference<>(); final Thread stoppingThread = new Thread(new AbstractRunnable() { @Override public void onFailure(Exception e) { stoppingError.set(e); } @Override protected void doRun() throws Exception { while (stopNodeThreads(disruptedNode, suspendedThreads)) ; } }); stoppingThread.setName(currentThreadName + "[LongGCDisruption][threadStopper]"); stoppingThread.start(); try { stoppingThread.join(getStoppingTimeoutInMillis()); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } if (stoppingError.get() != null) { throw new RuntimeException("unknown error while stopping threads", stoppingError.get()); } if (stoppingThread.isAlive()) { logger.warn("failed to stop node [{}]'s threads within [{}] millis. Stopping thread stack trace:\\\\n {}" , disruptedNode, getStoppingTimeoutInMillis(), stackTrace(stoppingThread)); stoppingThread.interrupt(); // best effort; throw new RuntimeException("stopping node threads took too long"); } success = true; } finally { if (success == false) { // resume threads if failed resumeThreads(suspendedThreads); suspendedThreads = null; } } } else { throw new IllegalStateException("can't disrupt twice, call stopDisrupting() first"); } }	i think it's worth leaving a comment, why this empty while loop is here.
public synchronized void startDisrupting() { if (suspendedThreads == null) { boolean success = false; try { suspendedThreads = ConcurrentHashMap.newKeySet(); final String currentThreadName = Thread.currentThread().getName(); assert currentThreadName.contains("[" + disruptedNode + "]") == false : "current thread match pattern. thread name: " + currentThreadName + ", node: " + disruptedNode; // we spawn a background thread to protect against deadlock which can happen // if there are shared resources between caller thread and and suspended threads // see unsafeClasses to how to avoid that final AtomicReference<Exception> stoppingError = new AtomicReference<>(); final Thread stoppingThread = new Thread(new AbstractRunnable() { @Override public void onFailure(Exception e) { stoppingError.set(e); } @Override protected void doRun() throws Exception { while (stopNodeThreads(disruptedNode, suspendedThreads)) ; } }); stoppingThread.setName(currentThreadName + "[LongGCDisruption][threadStopper]"); stoppingThread.start(); try { stoppingThread.join(getStoppingTimeoutInMillis()); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } if (stoppingError.get() != null) { throw new RuntimeException("unknown error while stopping threads", stoppingError.get()); } if (stoppingThread.isAlive()) { logger.warn("failed to stop node [{}]'s threads within [{}] millis. Stopping thread stack trace:\\\\n {}" , disruptedNode, getStoppingTimeoutInMillis(), stackTrace(stoppingThread)); stoppingThread.interrupt(); // best effort; throw new RuntimeException("stopping node threads took too long"); } success = true; } finally { if (success == false) { // resume threads if failed resumeThreads(suspendedThreads); suspendedThreads = null; } } } else { throw new IllegalStateException("can't disrupt twice, call stopDisrupting() first"); } }	why are we still marching on after being interrupted?
public synchronized void startDisrupting() { if (suspendedThreads == null) { boolean success = false; try { suspendedThreads = ConcurrentHashMap.newKeySet(); final String currentThreadName = Thread.currentThread().getName(); assert currentThreadName.contains("[" + disruptedNode + "]") == false : "current thread match pattern. thread name: " + currentThreadName + ", node: " + disruptedNode; // we spawn a background thread to protect against deadlock which can happen // if there are shared resources between caller thread and and suspended threads // see unsafeClasses to how to avoid that final AtomicReference<Exception> stoppingError = new AtomicReference<>(); final Thread stoppingThread = new Thread(new AbstractRunnable() { @Override public void onFailure(Exception e) { stoppingError.set(e); } @Override protected void doRun() throws Exception { while (stopNodeThreads(disruptedNode, suspendedThreads)) ; } }); stoppingThread.setName(currentThreadName + "[LongGCDisruption][threadStopper]"); stoppingThread.start(); try { stoppingThread.join(getStoppingTimeoutInMillis()); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } if (stoppingError.get() != null) { throw new RuntimeException("unknown error while stopping threads", stoppingError.get()); } if (stoppingThread.isAlive()) { logger.warn("failed to stop node [{}]'s threads within [{}] millis. Stopping thread stack trace:\\\\n {}" , disruptedNode, getStoppingTimeoutInMillis(), stackTrace(stoppingThread)); stoppingThread.interrupt(); // best effort; throw new RuntimeException("stopping node threads took too long"); } success = true; } finally { if (success == false) { // resume threads if failed resumeThreads(suspendedThreads); suspendedThreads = null; } } } else { throw new IllegalStateException("can't disrupt twice, call stopDisrupting() first"); } }	i don't see where the interrupt flag is ever checked on the stopping thread, what am i missing?
public synchronized void startDisrupting() { if (suspendedThreads == null) { boolean success = false; try { suspendedThreads = ConcurrentHashMap.newKeySet(); final String currentThreadName = Thread.currentThread().getName(); assert currentThreadName.contains("[" + disruptedNode + "]") == false : "current thread match pattern. thread name: " + currentThreadName + ", node: " + disruptedNode; // we spawn a background thread to protect against deadlock which can happen // if there are shared resources between caller thread and and suspended threads // see unsafeClasses to how to avoid that final AtomicReference<Exception> stoppingError = new AtomicReference<>(); final Thread stoppingThread = new Thread(new AbstractRunnable() { @Override public void onFailure(Exception e) { stoppingError.set(e); } @Override protected void doRun() throws Exception { while (stopNodeThreads(disruptedNode, suspendedThreads)) ; } }); stoppingThread.setName(currentThreadName + "[LongGCDisruption][threadStopper]"); stoppingThread.start(); try { stoppingThread.join(getStoppingTimeoutInMillis()); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } if (stoppingError.get() != null) { throw new RuntimeException("unknown error while stopping threads", stoppingError.get()); } if (stoppingThread.isAlive()) { logger.warn("failed to stop node [{}]'s threads within [{}] millis. Stopping thread stack trace:\\\\n {}" , disruptedNode, getStoppingTimeoutInMillis(), stackTrace(stoppingThread)); stoppingThread.interrupt(); // best effort; throw new RuntimeException("stopping node threads took too long"); } success = true; } finally { if (success == false) { // resume threads if failed resumeThreads(suspendedThreads); suspendedThreads = null; } } } else { throw new IllegalStateException("can't disrupt twice, call stopDisrupting() first"); } }	do we also need to indicate higher up that the disruption failed to be applied?
public synchronized void startDisrupting() { if (suspendedThreads == null) { boolean success = false; try { suspendedThreads = ConcurrentHashMap.newKeySet(); final String currentThreadName = Thread.currentThread().getName(); assert currentThreadName.contains("[" + disruptedNode + "]") == false : "current thread match pattern. thread name: " + currentThreadName + ", node: " + disruptedNode; // we spawn a background thread to protect against deadlock which can happen // if there are shared resources between caller thread and and suspended threads // see unsafeClasses to how to avoid that final AtomicReference<Exception> stoppingError = new AtomicReference<>(); final Thread stoppingThread = new Thread(new AbstractRunnable() { @Override public void onFailure(Exception e) { stoppingError.set(e); } @Override protected void doRun() throws Exception { while (stopNodeThreads(disruptedNode, suspendedThreads)) ; } }); stoppingThread.setName(currentThreadName + "[LongGCDisruption][threadStopper]"); stoppingThread.start(); try { stoppingThread.join(getStoppingTimeoutInMillis()); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } if (stoppingError.get() != null) { throw new RuntimeException("unknown error while stopping threads", stoppingError.get()); } if (stoppingThread.isAlive()) { logger.warn("failed to stop node [{}]'s threads within [{}] millis. Stopping thread stack trace:\\\\n {}" , disruptedNode, getStoppingTimeoutInMillis(), stackTrace(stoppingThread)); stoppingThread.interrupt(); // best effort; throw new RuntimeException("stopping node threads took too long"); } success = true; } finally { if (success == false) { // resume threads if failed resumeThreads(suspendedThreads); suspendedThreads = null; } } } else { throw new IllegalStateException("can't disrupt twice, call stopDisrupting() first"); } }	how about return arrays.stream(thread.currentthread().getstacktrace()).map(object::tostring).collect(collectors.joining("\\\\n")); since stacktraceelement#tostring includes all the information that you're including, with some smarts for handling things like line numbers that do not exist, etc.?
static List<String> resolveAuthorizedIndicesFromRole(Role role, RequestInfo requestInfo, Map<String, IndexAbstraction> lookup) { Predicate<String> predicate = role.allowedIndicesMatcher(requestInfo.getAction()); // do not include data streams for actions that do not operate on data streams TransportRequest request = requestInfo.getRequest(); boolean includeDataStreams = (request instanceof IndicesRequest) && ((IndicesRequest) request).includeDataStreams(); List<String> indicesAndAliases = new ArrayList<>(); // TODO: can this be done smarter? I think there are usually more indices/aliases in the cluster then indices defined a roles? for (Map.Entry<String, IndexAbstraction> entry : lookup.entrySet()) { String aliasOrIndex = entry.getKey(); if (predicate.test(aliasOrIndex)) { if (entry.getValue().getType() != IndexAbstraction.Type.DATA_STREAM || includeDataStreams) { indicesAndAliases.add(aliasOrIndex); } } } return Collections.unmodifiableList(indicesAndAliases); }	i gave it more thought than it probably warrants. overall i am ok with the change, but i would suggest we move this whole logic( if (entry.getvalue().gettype() != indexabstraction.type.data_stream || includedatastreams) {) to https://github.com/elastic/elasticsearch/blob/2925056e78d87069aa4dcdf1c7fb398830906315/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/indicesandaliasesresolver.java#l113 this is the place where we deal with the entities (open/close indices and aliases) behind the "authorised" names, and i believe it would help us to keep complexity under control in an already messy spaghetti.
public void testCreateIndex() throws IOException { CreateIndexRequest createIndexRequest = new CreateIndexRequest(); String indexName = "index-" + randomAlphaOfLengthBetween(2, 5); createIndexRequest.index(indexName); Map<String, String> expectedParams = new HashMap<>(); setRandomTimeout(createIndexRequest::timeout, AcknowledgedRequest.DEFAULT_ACK_TIMEOUT, expectedParams); setRandomMasterTimeout(createIndexRequest, expectedParams); setRandomWaitForActiveShards(createIndexRequest::waitForActiveShards, expectedParams); boolean updateAllTypes = randomBoolean(); createIndexRequest.updateAllTypes(updateAllTypes); if (updateAllTypes) { expectedParams.put("update_all_types", Boolean.TRUE.toString()); } Request request = Request.createIndex(createIndexRequest); assertEquals("/" + indexName, request.getEndpoint()); assertEquals(expectedParams, request.getParameters()); assertEquals("PUT", request.getMethod()); assertToXContentBody(createIndexRequest, request.getEntity()); }	can you set it only randomly so we also exercise the default value for it?
private void assertAliasesEqual(Set<Alias> expected, Set<Alias> actual) throws IOException { assertEquals(expected, actual); for (Alias expectedAlias : expected) { for (Alias actualAlias : actual) { if (expectedAlias.equals(actualAlias)) { assertEquals(expectedAlias.filter(), actualAlias.filter()); } } } } /** * Returns a random {@link CreateIndexRequest}	i guess we will never get here if they are not equal?
@Override public SortedNumericDocValues longValues(LeafReaderContext ctx) { return new CellValues(valuesSource.geoPointValues(ctx), precision, geoBoundingBox, encoder); }	i wonder if we should have two different cellvalues object, one for the unbounded case and another for the bounded case. this is performance critical and it won't be nice that this change affects performance of the general case.
private void forbidCircularReferences(String patternName, List<String> path, String pattern) { if (pattern.contains("%{" + patternName + "}") || pattern.contains("%{" + patternName + ":")) { String message; if (path.isEmpty()) { message = "circular reference in pattern [" + patternName + "][" + pattern + "]"; } else { message = "circular reference in pattern [" + path.remove(path.size() - 1) + "][" + pattern + "] back to pattern [" + patternName + "]"; // add rest of the path: if (path.isEmpty() == false) { message += " via patterns [" + String.join("=>", path) + "]"; } } throw new IllegalArgumentException(message); } for (int i = pattern.indexOf("%{"); i != -1; i = pattern.indexOf("%{", i + 1)) { int begin = i + 2; int brackedIndex = pattern.indexOf('}', begin); int columnIndex = pattern.indexOf(':', begin); int end; if (brackedIndex != -1 && columnIndex == -1) { end = brackedIndex; } else if (columnIndex != -1 && brackedIndex == -1) { end = columnIndex; } else if (brackedIndex != -1 && columnIndex != -1) { end = Math.min(brackedIndex, columnIndex); } else { throw new IllegalArgumentException("pattern [" + pattern + "] has circular references to other pattern definitions"); }	this didn't need to be public and it confused me that it was.
private String groupMatch(String name, Region region, String pattern) { int number = GROK_PATTERN_REGEX.nameToBackrefNumber(name.getBytes(StandardCharsets.UTF_8), 0, name.getBytes(StandardCharsets.UTF_8).length, region); int begin = region.beg[number]; int end = region.end[number]; if (begin < 0) { // no match found return null; } return new String(pattern.getBytes(StandardCharsets.UTF_8), begin, end - begin, StandardCharsets.UTF_8); }	this *feels* private, but it is used by tests. i tightened the visibility while i was there, but not didn't refactor a bunch of tests.
static DeprecationIssue checkTemplatesWithMultipleTypes(ClusterState state) { Set<String> templatesWithMultipleTypes = new HashSet<>(); state.getMetadata().getTemplates().forEach((templateCursor) -> { String templateName = templateCursor.key; ImmutableOpenMap<String, CompressedXContent> mappings = templateCursor.value.mappings(); if (mappings != null && mappings.size() > 1) { templatesWithMultipleTypes.add(templateName); } }); if (templatesWithMultipleTypes.isEmpty()) { return null; } return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, "Some index templates contain multiple mapping types", "https://www.elastic.co/guide/en/elasticsearch/reference/master/removal-of-types.html", "Index templates " + templatesWithMultipleTypes + " define multiple types and so will cause errors when used in index creation" ); }	just for my knowledge, does critical mean we'll prevent the upgrade from proceeding?
public void testGetShardSnapshotFailureHandlingLetOtherRepositoriesRequestsMakeProgress() throws Exception { final String failingRepoName = randomAlphaOfLength(10); createRepository(failingRepoName, "mock"); int repoCount = randomIntBetween(1, 10); List<String> workingRepoNames = new ArrayList<>(); for (int i = 0; i < repoCount; i++) { final String repoName = randomAlphaOfLength(10); createRepository(repoName, "fs"); workingRepoNames.add(repoName); } final String indexName = "test-idx"; createIndexWithContent(indexName); int snapshotIdx = 0; createSnapshot(failingRepoName, "empty-snap-" + snapshotIdx++, Collections.singletonList(indexName)); SnapshotInfo latestSnapshot = null; for (String workingRepoName : workingRepoNames) { latestSnapshot = createSnapshot(workingRepoName, "empty-snap-" + snapshotIdx++, Collections.singletonList(indexName)); } final MockRepository repository = getRepositoryOnMaster(failingRepoName); if (randomBoolean()) { repository.setBlockAndFailOnReadIndexFiles(); } else { repository.setBlockAndFailOnReadSnapFiles(); } PlainActionFuture<GetShardSnapshotResponse> future = getLatestSnapshotForShardFuture( CollectionUtils.appendToCopy(workingRepoNames, failingRepoName), indexName, 0 ); waitForBlock(internalCluster().getMasterName(), failingRepoName); repository.unblock(); final GetShardSnapshotResponse response = future.actionGet(); final Optional<RepositoryException> error = response.getFailureForRepository(failingRepoName); assertThat(error.isPresent(), is(equalTo(true))); assertThat( error.get().getMessage(), equalTo(String.format(Locale.ROOT, "[%s] Unable to find the latest snapshot for shard [[%s][0]]", failingRepoName, indexName)) ); for (String workingRepoName : workingRepoNames) { assertThat(response.getFailureForRepository(workingRepoName).isEmpty(), is(equalTo(true))); } Optional<ShardSnapshotInfo> shardSnapshotInfoOpt = response.getLatestShardSnapshot(); assertThat(shardSnapshotInfoOpt.isPresent(), equalTo(true)); ShardSnapshotInfo shardSnapshotInfo = shardSnapshotInfoOpt.get(); assertThat(shardSnapshotInfo.getSnapshot(), equalTo(latestSnapshot.snapshot())); assertThat(shardSnapshotInfo.getRepository(), equalTo(latestSnapshot.repository())); }	i'm fine with giving all the snapshots unique names, but i'm curious whether this was necessary (and why).
public SearchResponse searchWithRetry(SearchRequest searchRequest, String jobId, Supplier<Boolean> shouldRetry, Consumer<String> msgHandler) { RetryContext retryContext = new RetryContext(jobId, shouldRetry, msgHandler); while (true) { String failureMessage; try { SearchResponse searchResponse = client.search(searchRequest).actionGet(); if (RestStatus.OK.equals(searchResponse.status())) { return searchResponse; } failureMessage = searchResponse.status().toString(); } catch (ElasticsearchException e) { LOGGER.warn("[" + jobId + "] Exception while executing search action", e); failureMessage = e.getDetailedMessage(); if (isIrrecoverable(e)) { LOGGER.warn(new ParameterizedMessage("[{}] experienced irrecoverable failure", jobId), e); throw new ElasticsearchException("{} experienced failure that cannot be automatically retried", e, jobId); } } retryContext.nextIteration("search", failureMessage); } }	could you make this message consistent with others? "...failure that cannot be..."
public SearchResponse searchWithRetry(SearchRequest searchRequest, String jobId, Supplier<Boolean> shouldRetry, Consumer<String> msgHandler) { RetryContext retryContext = new RetryContext(jobId, shouldRetry, msgHandler); while (true) { String failureMessage; try { SearchResponse searchResponse = client.search(searchRequest).actionGet(); if (RestStatus.OK.equals(searchResponse.status())) { return searchResponse; } failureMessage = searchResponse.status().toString(); } catch (ElasticsearchException e) { LOGGER.warn("[" + jobId + "] Exception while executing search action", e); failureMessage = e.getDetailedMessage(); if (isIrrecoverable(e)) { LOGGER.warn(new ParameterizedMessage("[{}] experienced irrecoverable failure", jobId), e); throw new ElasticsearchException("{} experienced failure that cannot be automatically retried", e, jobId); } } retryContext.nextIteration("search", failureMessage); } }	should this be "private"?
public void testSearchWithRetries_FailureOnIrrecoverableError() { resultsPersisterService.setMaxFailureRetries(5); doAnswer(withFailure(new ElasticsearchStatusException("bad search request", RestStatus.BAD_REQUEST))) .when(client).execute(eq(SearchAction.INSTANCE), eq(SEARCH_REQUEST), any()); ElasticsearchException e = expectThrows( ElasticsearchException.class, () -> resultsPersisterService.searchWithRetry(SEARCH_REQUEST, JOB_ID, () -> true, (s) -> {})); assertThat(e.getMessage(), containsString("experienced failure that cannot be automatically retried")); verify(client, times(1)).execute(eq(SearchAction.INSTANCE), eq(SEARCH_REQUEST), any()); }	i think times(1) is default and you can drop it.
protected void onNodeFailure(DiscoveryNode node, int nodeIndex, Throwable t) { String nodeId = node.getId(); logger.debug(new ParameterizedMessage("failed to execute [{}] on node [{}]", actionName, nodeId), t); // this is defensive to protect against the possibility of double invocation // the current implementation of TransportService#sendRequest guards against this // but concurrency is hard, safety is important, and the small performance loss here does not matter if (responses.compareAndSet(nodeIndex, null, new FailedNodeException(nodeId, "Failed node [" + nodeId + "]", t))) { if (counter.incrementAndGet() == responses.length()) { onCompletion(); } } }	nit: maybe keep the logger.isdebugenabled() here and in the other 2 spots changed so that we don't have to create the message object needlessly? (doesn't look too performance critical ever :) so pretty optional)
static Request clusterHealth(ClusterHealthRequest healthRequest) { Params params = Params.builder(); params.withWaitForStatus(healthRequest.waitForStatus()); params.withWaitForNoRelocatingShards(healthRequest.waitForNoRelocatingShards()); params.withWaitForNoInitializingShards(healthRequest.waitForNoInitializingShards()); params.withWaitForActiveShards(healthRequest.waitForActiveShards()); params.withWaitForNodes(healthRequest.waitForNodes()); params.withTimeout(healthRequest.timeout()); params.withMasterTimeout(healthRequest.masterNodeTimeout()); params.withLocal(healthRequest.local()); params.putParam("level", "shards"); String[] indices = healthRequest.indices() == null ? Strings.EMPTY_ARRAY : healthRequest.indices(); String endpoint = endpoint(indices, "_cluster/health"); return new Request(HttpGet.METHOD_NAME, endpoint, params.getParams(), null); }	i see. right now you request the whole thing because it is simpler that way. i think this is fine. i think if we decide we'd like to support level as something else then we can do it in a followup. what do you think, @javanna?
public void testClusterHealth() throws IOException { ClusterHealthRequest request = new ClusterHealthRequest(); ClusterHealthResponse response = execute(request, highLevelClient().cluster()::health, highLevelClient().cluster()::healthAsync); assertThat(response, notNullValue()); assertThat(response.isTimedOut(), is(false)); assertThat(response.status(), is(RestStatus.OK)); assertThat(response.getStatus(), is(ClusterHealthStatus.GREEN)); assertThat(response.getIndices(), is(emptyMap())); assertThat(response.getActivePrimaryShards(), is(0)); assertThat(response.getActiveShards(), is(0)); assertThat(response.getDelayedUnassignedShards(), is(0)); assertThat(response.getInitializingShards(), is(0)); assertThat(response.getUnassignedShards(), is(0)); assertThat(response.getActiveShardsPercent(), is(100d)); }	we should probably add a test that provides indices and the level parameter
public void testClusterHealth() { ClusterHealthRequest healthRequest = new ClusterHealthRequest(); Map<String, String> expectedParams = new HashMap<>(); setRandomLocal(healthRequest, expectedParams); setRandomTimeout(healthRequest::timeout, AcknowledgedRequest.DEFAULT_ACK_TIMEOUT, expectedParams); setRandomMasterTimeout(healthRequest, expectedParams); expectedParams.put("level", "shards"); // Default value in ClusterHealthRequest is NONE but in Request.Params::withWaitForActiveShards is DEFAULT expectedParams.put("wait_for_active_shards", "0"); //TODO add random filling for other properties Request request = Request.clusterHealth(healthRequest); assertThat(request, is(notNullValue())); assertThat(request.getMethod(), is(HttpGet.METHOD_NAME)); assertThat(request.getEntity(), is(nullValue())); assertThat(request.getEndpoint(), is("/_cluster/health")); assertThat(request.getParameters(), is(expectedParams)); }	i see, you may have to make changes here like we already did in the 6.x branch where we have a different default for some api.
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ClusterHealthResponse that = (ClusterHealthResponse) o; return numberOfPendingTasks == that.numberOfPendingTasks && numberOfInFlightFetch == that.numberOfInFlightFetch && delayedUnassignedShards == that.delayedUnassignedShards && timedOut == that.timedOut && Objects.equals(clusterName, that.clusterName) && Objects.equals(taskMaxWaitingTime, that.taskMaxWaitingTime) && Objects.equals(clusterStateHealth, that.clusterStateHealth) && clusterHealthStatus == that.clusterHealthStatus; }	can you put these in the same order as hashcode?
* @return false if the array contains an element, true if not or the array is null. */ public static boolean isEmpty(Object[] array) { return array == null || array.length == 0; }	i'd prefer not do have this and to do the null check at the call site to honest.
public static <E> List<List<E>> eagerPartition(List<E> list, int size) { if (list == null) { throw new NullPointerException("list"); } if (size <= 0) { throw new IllegalArgumentException("size <= 0"); } List<List<E>> result = new ArrayList<>((int) Math.ceil(list.size() / size)); List<E> accumulator = new ArrayList<>(size); int count = 0; for (E element : list) { if (count == size) { result.add(accumulator); accumulator = new ArrayList<>(size); count = 0; } accumulator.add(element); count++; } if (count > 0) { result.add(accumulator); } return result; } /** * Creates a {@code HashMap} instance, with a high enough "initial capacity" that it <i>should</i> hold * {@code expectedSize} elements without growth. This behavior cannot be broadly guaranteed, but it is observed * to be true for OpenJDK 1.7. * It also can't be guaranteed that the method isn't inadvertently <i>oversizing</i> the returned map. * * <p>From Guava 24.1-jre. <b>com.google.common.collect.Maps </b></p> * * @param expectedSize the number of entries you expect to add to the returned map * @return a new, empty {@code HashMap} with enough capacity to hold {@code expectedSize} entries without resizing * @throws IllegalArgumentException if {@code expectedSize}	i would refrain from adding these to be honest, especially if only used in our client/parsing code.
private Repository createRepository(RepositoryMetaData repositoryMetaData, Map<String, Repository.Factory> factories) { logger.debug("creating repository [{}][{}]", repositoryMetaData.type(), repositoryMetaData.name()); validate(repositoryMetaData.name()); Repository.Factory factory = factories.get(repositoryMetaData.type()); if (factory == null) { throw new RepositoryException(repositoryMetaData.name(), "repository type [" + repositoryMetaData.type() + "] does not exist"); } try { Repository repository = factory.create(repositoryMetaData, factories::get); repository.start(); return repository; } catch (Exception e) { logger.warn(new ParameterizedMessage("failed to create repository [{}][{}]", repositoryMetaData.type(), repositoryMetaData.name()), e); throw new RepositoryException(repositoryMetaData.name(), "failed to create repository", e); } }	this is a method that is internally called both on repository creation, but also after a restart when a node rejoins a cluster with an existing repository. i think we should do the validation only for new repository creation requests, and not retroactively apply to old repositories (which will break them). this means that the validation should be done in registerrepository, not createrepository.
private static void validate(final String repositoryName) { if (Strings.hasLength(repositoryName) == false) { throw new RepositoryException(repositoryName, "cannot be empty"); } if (repositoryName.contains(" ")) { throw new RepositoryException(repositoryName, "must not contain whitespace"); } if (repositoryName.contains(",")) { throw new RepositoryException(repositoryName, "must not contain ','"); } if (repositoryName.contains("#")) { throw new RepositoryException(repositoryName, "must not contain '#'"); } if (Strings.validFileName(repositoryName) == false) { throw new RepositoryException(repositoryName, "must not contain the following characters " + Strings.INVALID_FILENAME_CHARS); } }	this is already covered by filename check
private static void validate(final String repositoryName) { if (Strings.hasLength(repositoryName) == false) { throw new RepositoryException(repositoryName, "cannot be empty"); } if (repositoryName.contains(" ")) { throw new RepositoryException(repositoryName, "must not contain whitespace"); } if (repositoryName.contains(",")) { throw new RepositoryException(repositoryName, "must not contain ','"); } if (repositoryName.contains("#")) { throw new RepositoryException(repositoryName, "must not contain '#'"); } if (Strings.validFileName(repositoryName) == false) { throw new RepositoryException(repositoryName, "must not contain the following characters " + Strings.INVALID_FILENAME_CHARS); } }	this is already covered by filename check
public void testGetSnapshotsWithSnapshotInProgress() throws Exception { createRepository("test-repo", "mock", Settings.builder().put("location", randomRepoPath()).put("block_on_data", true)); String indexName = "test-idx-1"; createIndexWithContent(indexName, indexSettingsNoReplicas(randomIntBetween(1, 10)).build()); ensureGreen(); ActionFuture<CreateSnapshotResponse> createSnapshotResponseActionFuture = startFullSnapshot("test-repo", "test-snap"); logger.info("--> wait for data nodes to get blocked"); waitForBlockOnAnyDataNode("test-repo"); awaitNumberOfSnapshotsInProgress(1); GetSnapshotsResponse response1 = client().admin() .cluster() .prepareGetSnapshots("test-repo") .setSnapshots("test-snap") .setIgnoreUnavailable(true) .get(); List<SnapshotInfo> snapshotInfoList = response1.getSnapshots(); assertEquals(1, snapshotInfoList.size()); SnapshotInfo snapshotInfo = snapshotInfoList.get(0); assertEquals(SnapshotState.IN_PROGRESS, snapshotInfo.state()); SnapshotStatus snapshotStatus = client().admin().cluster().prepareSnapshotStatus().execute().actionGet().getSnapshots().get(0); assertThat(snapshotInfo.totalShards(), equalTo(snapshotStatus.getIndices().get(indexName).getShardsStats().getTotalShards())); assertThat(snapshotInfo.successfulShards(), equalTo(snapshotStatus.getIndices().get(indexName).getShardsStats().getDoneShards())); assertThat(snapshotInfo.shardFailures().size(), equalTo(0)); String notExistedSnapshotName = "snapshot_not_exist"; GetSnapshotsResponse response2 = client().admin() .cluster() .prepareGetSnapshots("test-repo") .setSnapshots(notExistedSnapshotName) .setIgnoreUnavailable(true) .get(); assertEquals(0, response2.getSnapshots().size()); expectThrows( SnapshotMissingException.class, () -> client().admin() .cluster() .prepareGetSnapshots("test-repo") .setSnapshots(notExistedSnapshotName) .setIgnoreUnavailable(false) .execute() .actionGet() ); logger.info("--> unblock all data nodes"); unblockAllDataNodes("test-repo"); assertSuccessful(createSnapshotResponseActionFuture); }	let's use get instead of actionget() please so we don't swallow stack traces that make understanding failures a pain :)
*/ private void writeToChannel(WriteOperation writeOperation) { assertOnSelectorThread(); SocketChannelContext context = writeOperation.getChannel(); if (context.isOpen() == false) { executeFailedListener(writeOperation.getListener(), new ClosedChannelException()); } else if (context.getSelectionKey() == null) { // This should very rarely happen. The only times a channel is exposed outside the event loop, // but might not registered is through the exception handler and channel accepted callbacks. executeFailedListener(writeOperation.getListener(), new IllegalStateException("Channel not registered")); } else { // If the channel does not currently have anything that is ready to flush, we should flush after // the write operation is queued. boolean shouldFlushAfterQueuing = context.readyForFlush() == false; try { context.queueWriteOperation(writeOperation); } catch (Exception e) { shouldFlushAfterQueuing = false; executeFailedListener(writeOperation.getListener(), e); } if (shouldFlushAfterQueuing) { if (context.selectorShouldClose() == false) { handleWrite(context); } eventHandler.postHandling(context); } } }	nit: random empty line
@Override public int consumeReads(InboundChannelBuffer channelBuffer) throws IOException { return fn.apply(channelBuffer); } } private static class TestSelectionKey extends AbstractSelectionKey { @Override public SelectableChannel channel() { return null; } @Override public Selector selector() { return null; } @Override public int interestOps() { return 0; } @Override public SelectionKey interestOps(int ops) { return null; } @Override public int readyOps() { return 0; }	nit: might as well just use mock(abstractselectionkey.class) instead and save a few lines here?
public IndexShardRoutingTable build() { // don't allow more than one shard copy with same id to be allocated to same node assert distinctNodes() : "more than one shard with same id assigned to same node (shards: " + shards + ")"; return new IndexShardRoutingTable(shardId, Collections.unmodifiableList(new ArrayList<>(shards))); }	if you passed in shards here, you could make this static and public, and then unit test it. what do you think?
public TaskParams getParams() { return params; }	i think this still needs to be called. if you are concerned, maybe wrap the listener and call this on response/failure?
public StoredContext stashContext() { final ThreadContextStruct context = threadLocal.get(); /** * X-Opaque-ID should be preserved in a threadContext in order to propagate this across threads. * This is needed so the DeprecationLogger in another thread can see the value of X-Opaque-ID provided by a user. * Otherwise when context is stash, it should be empty. */ if (context.requestHeaders.containsKey(Task.X_OPAQUE_ID)) { ThreadContextStruct threadContextStruct = DEFAULT_CONTEXT.putHeaders(Map.of(Task.X_OPAQUE_ID, context.requestHeaders.get(Task.X_OPAQUE_ID))); threadLocal.set(threadContextStruct); } else { threadLocal.set(DEFAULT_CONTEXT); } return () -> { // If the node and thus the threadLocal get closed while this task // is still executing, we don't want this runnable to fail with an // uncaught exception threadLocal.set(context); }; } /** * Removes the specified transient headers from the current context. When the returned * {@link StoredContext} is closed, it will restore these transient headers to their original * value (including restoring them to an <i>unset</i> value if they did not originally exist). * Closing the {@code StoredContext} has no affect on any other header - any headers * (other than those names specified in {@code transientHeadersToStash} that were * added to the {@code ThreadContext} will be retained. * * For example, at the end of the following code, the ThreadContext will have transient * values {@code "a"=1}, {@code "b"=1}, {@code "d"=2} and {@code "c"} will not be set. * <pre> * threadContext.putTransient("a", 1); * threadContext.putTransient("b", 1); * try (ThreadContext.StoredContext restore = threadContext.stashTransientContext(List.of("b", "c")) ) { * threadContext.putTransient("b", 2); * threadContext.putTransient("c", 2); * threadContext.putTransient("d", 2); * }	i think this should actually make a copy of the struct and restore it, otherwise any additional headers will be leaked back after closing the storedcontext. suggestion final threadcontextstruct context = threadlocal.get(); return () -> threadlocal.set(context); can you also add a test to ensure this? scenario i am thinking will fail as the code is written: java assertnull(threadcontext.gettransient("a"); try (threadcontext.storedcontext restore = threadcontext.stashtransientcontext(list.of())) { threadcontext.puttransient("a", "1"); } assertnull(threadcontext.gettransient("a");
public StoredContext stashContext() { final ThreadContextStruct context = threadLocal.get(); /** * X-Opaque-ID should be preserved in a threadContext in order to propagate this across threads. * This is needed so the DeprecationLogger in another thread can see the value of X-Opaque-ID provided by a user. * Otherwise when context is stash, it should be empty. */ if (context.requestHeaders.containsKey(Task.X_OPAQUE_ID)) { ThreadContextStruct threadContextStruct = DEFAULT_CONTEXT.putHeaders(Map.of(Task.X_OPAQUE_ID, context.requestHeaders.get(Task.X_OPAQUE_ID))); threadLocal.set(threadContextStruct); } else { threadLocal.set(DEFAULT_CONTEXT); } return () -> { // If the node and thus the threadLocal get closed while this task // is still executing, we don't want this runnable to fail with an // uncaught exception threadLocal.set(context); }; } /** * Removes the specified transient headers from the current context. When the returned * {@link StoredContext} is closed, it will restore these transient headers to their original * value (including restoring them to an <i>unset</i> value if they did not originally exist). * Closing the {@code StoredContext} has no affect on any other header - any headers * (other than those names specified in {@code transientHeadersToStash} that were * added to the {@code ThreadContext} will be retained. * * For example, at the end of the following code, the ThreadContext will have transient * values {@code "a"=1}, {@code "b"=1}, {@code "d"=2} and {@code "c"} will not be set. * <pre> * threadContext.putTransient("a", 1); * threadContext.putTransient("b", 1); * try (ThreadContext.StoredContext restore = threadContext.stashTransientContext(List.of("b", "c")) ) { * threadContext.putTransient("b", 2); * threadContext.putTransient("c", 2); * threadContext.putTransient("d", 2); * }	why can't we just restore the beforecontext? suggestion return () -> threadlocal.set(beforecontext);
public static FieldAttribute checkIsFieldAttribute(Expression e) { Check.isTrue(e instanceof FieldAttribute, "Line {}: Expected [{}] to be FieldAttribute but was not.", e.sourceLocation(), e); return (FieldAttribute) e; }	the style of the error message should be consistent with the other uses of the check.istrue method in similar situations, ie expected a fieldattribute but received [{}].
public void start(Settings settings, TransportService transportService, ClusterService clusterService, MetaStateService metaStateService, MetaDataIndexUpgradeService metaDataIndexUpgradeService, MetaDataUpgrader metaDataUpgrader, LucenePersistedStateFactory lucenePersistedStateFactory) { assert persistedState.get() == null : "should only start once, but already have " + persistedState.get(); if (DiscoveryNode.isMasterNode(settings) || DiscoveryNode.isDataNode(settings)) { try { LucenePersistedStateFactory.OnDiskState onDiskState = lucenePersistedStateFactory.loadBestOnDiskState(); if (onDiskState == LucenePersistedStateFactory.NO_ON_DISK_STATE) { assert Version.CURRENT.major <= Version.V_7_0_0.major + 1 : "legacy metadata loader is not needed anymore from v9 onwards"; final Tuple<Manifest, MetaData> legacyState = metaStateService.loadFullState(); if (legacyState.v1().isEmpty() == false) { onDiskState = new LucenePersistedStateFactory.OnDiskState(lucenePersistedStateFactory.getNodeId(), null, legacyState.v1().getCurrentTerm(), legacyState.v1().getClusterStateVersion(), legacyState.v2()); } } final LucenePersistedStateFactory.Writer persistenceWriter = lucenePersistedStateFactory.createWriter(); final LucenePersistedStateFactory.LucenePersistedState lucenePersistedState; boolean success = false; try { final ClusterState clusterState = prepareInitialClusterState(transportService, clusterService, ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.get(settings)) .version(onDiskState.lastAcceptedVersion) .metaData(upgradeMetaDataForNode(onDiskState.metaData, metaDataIndexUpgradeService, metaDataUpgrader)) .build()); lucenePersistedState = new LucenePersistedStateFactory.LucenePersistedState( persistenceWriter, onDiskState.currentTerm, clusterState); // Write the whole state out to be sure it's fresh and using the latest format. Called during initialisation, so that // (1) throwing an IOException is enough to halt the node, and // (2) the index is currently empty since it was opened with IndexWriterConfig.OpenMode.CREATE // In the common case it's actually sufficient to commit() the existing state and not do any indexing. For instance, // this is true if there's only one data path on this master node, and the commit we just loaded was already written out // by this version of Elasticsearch. TODO TBD should we avoid indexing when possible? persistenceWriter.writeFullStateAndCommit(onDiskState.currentTerm, clusterState); metaStateService.deleteAll(); // delete legacy files success = true; } finally { if (success == false) { IOUtils.closeWhileHandlingException(persistenceWriter); } } persistedState.set(lucenePersistedState); } catch (IOException e) { throw new ElasticsearchException("failed to load metadata", e); } } else { persistedState.set( new InMemoryPersistedState(0L, ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.get(settings)).build())); } }	we also return lucenepersistedstatefactory.no_on_disk_state if starting up a brand-new node? (i know this is merely moving code from an earlier pr, i missed this last time)
public void start(Settings settings, TransportService transportService, ClusterService clusterService, MetaStateService metaStateService, MetaDataIndexUpgradeService metaDataIndexUpgradeService, MetaDataUpgrader metaDataUpgrader, LucenePersistedStateFactory lucenePersistedStateFactory) { assert persistedState.get() == null : "should only start once, but already have " + persistedState.get(); if (DiscoveryNode.isMasterNode(settings) || DiscoveryNode.isDataNode(settings)) { try { LucenePersistedStateFactory.OnDiskState onDiskState = lucenePersistedStateFactory.loadBestOnDiskState(); if (onDiskState == LucenePersistedStateFactory.NO_ON_DISK_STATE) { assert Version.CURRENT.major <= Version.V_7_0_0.major + 1 : "legacy metadata loader is not needed anymore from v9 onwards"; final Tuple<Manifest, MetaData> legacyState = metaStateService.loadFullState(); if (legacyState.v1().isEmpty() == false) { onDiskState = new LucenePersistedStateFactory.OnDiskState(lucenePersistedStateFactory.getNodeId(), null, legacyState.v1().getCurrentTerm(), legacyState.v1().getClusterStateVersion(), legacyState.v2()); } } final LucenePersistedStateFactory.Writer persistenceWriter = lucenePersistedStateFactory.createWriter(); final LucenePersistedStateFactory.LucenePersistedState lucenePersistedState; boolean success = false; try { final ClusterState clusterState = prepareInitialClusterState(transportService, clusterService, ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.get(settings)) .version(onDiskState.lastAcceptedVersion) .metaData(upgradeMetaDataForNode(onDiskState.metaData, metaDataIndexUpgradeService, metaDataUpgrader)) .build()); lucenePersistedState = new LucenePersistedStateFactory.LucenePersistedState( persistenceWriter, onDiskState.currentTerm, clusterState); // Write the whole state out to be sure it's fresh and using the latest format. Called during initialisation, so that // (1) throwing an IOException is enough to halt the node, and // (2) the index is currently empty since it was opened with IndexWriterConfig.OpenMode.CREATE // In the common case it's actually sufficient to commit() the existing state and not do any indexing. For instance, // this is true if there's only one data path on this master node, and the commit we just loaded was already written out // by this version of Elasticsearch. TODO TBD should we avoid indexing when possible? persistenceWriter.writeFullStateAndCommit(onDiskState.currentTerm, clusterState); metaStateService.deleteAll(); // delete legacy files success = true; } finally { if (success == false) { IOUtils.closeWhileHandlingException(persistenceWriter); } } persistedState.set(lucenePersistedState); } catch (IOException e) { throw new ElasticsearchException("failed to load metadata", e); } } else { persistedState.set( new InMemoryPersistedState(0L, ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.get(settings)).build())); } }	perhaps we could remove the datapath from ondiskstate in a follow-up to avoid the null here?
public HotThreads type(ReportType type) { this.type = type; return this; }	please remove this method as it is public and will be confusing to users of the hotthreads class inside elasticsearch.
Map<Long, ThreadTimeAccumulator> getAllValidThreadInfos(ThreadMXBean threadBean, long currentThreadId) { long[] threadIds = threadBean.getAllThreadIds(); ThreadInfo[] threadInfos = threadBean.getThreadInfo(threadIds); Map<Long, ThreadTimeAccumulator> result = new HashMap<>(threadIds.length); for (int i = 0; i < threadIds.length; i++) { if (threadInfos[i] == null || threadIds[i] == currentThreadId) { continue; } long cpuTime = threadBean.getThreadCpuTime(threadIds[i]); if (cpuTime == INVALID_TIMING) { continue; } long allocatedBytes = type == ReportType.MEM ? sunThreadInfo.getThreadAllocatedBytes(threadIds[i]) : 0; result.put(threadIds[i], new ThreadTimeAccumulator(threadInfos[i], cpuTime, allocatedBytes)); } return result; }	add an sunthreadinfo sunthreadinfo argument to getallvalidthreadinfos right after threadmxbean to pass either the real sunthreadinfo or the mocked one.
@Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { return new ConstantKeywordIndexFieldData.Builder(mapperService -> fullyQualifiedIndexName); } } private IndexFieldMapper(Settings indexSettings, MappedFieldType existing) { this(existing == null ? Defaults.FIELD_TYPE.clone() : existing, indexSettings); } private IndexFieldMapper(MappedFieldType fieldType, Settings indexSettings) { super(NAME, fieldType, Defaults.FIELD_TYPE, indexSettings); } @Override public void preParse(ParseContext context) throws IOException {} @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException {} @Override protected String contentType() { return CONTENT_TYPE; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { return builder; }	to me it seemed more consistent to call this constantindexfielddata. it matches constantfieldtype, which is shared between the _index and constant_keyword fields. we don't yet have other constant_* field types, so there's not much room for confusion as to what constantindexfielddata represents.
@Override protected QueryBuilder doRewrite(QueryRewriteContext queryRewriteContext) throws IOException { if (ids.isEmpty()) { return new MatchNoneQueryBuilder(); } QueryShardContext context = queryRewriteContext.convertToShardContext(); if (context != null) { if (context.fieldMapper(IdFieldMapper.NAME) == null) { // no mappings yet return new MatchNoneQueryBuilder(); } } return super.doRewrite(queryRewriteContext); }	small comment, could be if(idfield == null || ids.isempty()) { ... }.
@Override protected QueryBuilder doRewrite(QueryRewriteContext queryRewriteContext) throws IOException { QueryShardContext context = queryRewriteContext.convertToShardContext(); if (context != null) { MappedFieldType fieldType = context.fieldMapper(this.fieldName); if (fieldType == null) { return new MatchNoneQueryBuilder(); } else if (fieldType instanceof ConstantFieldType) { // This logic is correct for all field types, but by only applying it to constant // fields we also have the guarantee that it doesn't perform I/O, which is important // since rewrites might happen on a network thread. Query query = fieldType.wildcardQuery(value, null, context); // the rewrite method doesn't matter if (query instanceof MatchAllDocsQuery) { return new MatchAllQueryBuilder().queryName(queryName).boost(boost); } else if (query instanceof MatchNoDocsQuery) { return new MatchNoneQueryBuilder(); } } } return super.doRewrite(queryRewriteContext); }	for my understanding, what field types would perform i/o when constructing a query? that seemed very unusual to me, are we just being cautious here?
@Override public ConstantKeywordFieldMapper build(BuilderContext context) { setupFieldType(context); return new ConstantKeywordFieldMapper( name, fieldType, defaultFieldType, context.indexSettings()); } } public static class TypeParser implements Mapper.TypeParser { @Override public Mapper.Builder<?,?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { final Object value = node.remove("value"); if (value == null) { throw new MapperParsingException("Property [value] of field [" + name + "] is required and can't be null."); } if (value instanceof Number == false && value instanceof CharSequence == false) { throw new MapperParsingException("Property [value] of field [" + name + "] must be a number or a string, but got [" + value + "]"); } return new ConstantKeywordFieldMapper.Builder(name, value.toString()); } } public static final class ConstantKeywordFieldType extends ConstantFieldType { private String value; public ConstantKeywordFieldType() { super(); } protected ConstantKeywordFieldType(ConstantKeywordFieldType ref) { super(ref); this.value = ref.value; } public ConstantKeywordFieldType clone() { return new ConstantKeywordFieldType(this); } @Override public boolean equals(Object o) { if (super.equals(o) == false) { return false; } ConstantKeywordFieldType other = (ConstantKeywordFieldType) o; return Objects.equals(value, other.value); } @Override public void checkCompatibility(MappedFieldType otherFT, List<String> conflicts) { super.checkCompatibility(otherFT, conflicts); ConstantKeywordFieldType other = (ConstantKeywordFieldType) otherFT; if (Objects.equals(value, other.value) == false) { conflicts.add("mapper [" + name() + "] has different [value]"); } } @Override public int hashCode() { return 31 * super.hashCode() + Objects.hashCode(value); } /** Return the value that this field wraps. */ public String value() { return value; } /** Set the value. */ public void setValue(String value) { checkIfFrozen(); this.value = Objects.requireNonNull(value); } @Override public String typeName() { return CONTENT_TYPE; } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { return new ConstantKeywordIndexFieldData.Builder(mapperService -> value); } private static String valueToString(Object v) { if (v instanceof BytesRef) { return ((BytesRef) v).utf8ToString(); } else { return v.toString(); } } @Override public Query termQuery(Object value, QueryShardContext context) { if (Objects.equals(valueToString(value), this.value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } @Override public Query termsQuery(List<?> values, QueryShardContext context) { for (Object v : values) { if (Objects.equals(valueToString(v), value)) { return new MatchAllDocsQuery(); } } return new MatchNoDocsQuery(); } @Override public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, QueryShardContext context) { if (this.value.startsWith(value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } public Query wildcardQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, QueryShardContext context) { if (Regex.simpleMatch(value, this.value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } } protected ConstantKeywordFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Settings indexSettings) { super(simpleName, fieldType, defaultFieldType, indexSettings, MultiFields.empty(), CopyTo.empty()); } @Override protected ConstantKeywordFieldMapper clone() { return (ConstantKeywordFieldMapper) super.clone(); } @Override public ConstantKeywordFieldType fieldType() { return (ConstantKeywordFieldType) super.fieldType(); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { String value; if (context.externalValueSet()) { value = context.externalValue().toString(); } else { XContentParser parser = context.parser(); if (parser.currentToken() == XContentParser.Token.VALUE_NULL) { value = fieldType().nullValueAsString(); } else { value = parser.textOrNull(); } } if (Objects.equals(fieldType().value, value) == false) { throw new IllegalArgumentException("[constant_keyword] field [" + name() + "] only accepts values that are equal to the wrapped value [" + fieldType().value() + "], but got [" + value + "]"); } } @Override protected String contentType() { return CONTENT_TYPE; }	small comment, maybe we could actually list the values in the error message for clarity.
@Override public ConstantKeywordFieldMapper build(BuilderContext context) { setupFieldType(context); return new ConstantKeywordFieldMapper( name, fieldType, defaultFieldType, context.indexSettings()); } } public static class TypeParser implements Mapper.TypeParser { @Override public Mapper.Builder<?,?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { final Object value = node.remove("value"); if (value == null) { throw new MapperParsingException("Property [value] of field [" + name + "] is required and can't be null."); } if (value instanceof Number == false && value instanceof CharSequence == false) { throw new MapperParsingException("Property [value] of field [" + name + "] must be a number or a string, but got [" + value + "]"); } return new ConstantKeywordFieldMapper.Builder(name, value.toString()); } } public static final class ConstantKeywordFieldType extends ConstantFieldType { private String value; public ConstantKeywordFieldType() { super(); } protected ConstantKeywordFieldType(ConstantKeywordFieldType ref) { super(ref); this.value = ref.value; } public ConstantKeywordFieldType clone() { return new ConstantKeywordFieldType(this); } @Override public boolean equals(Object o) { if (super.equals(o) == false) { return false; } ConstantKeywordFieldType other = (ConstantKeywordFieldType) o; return Objects.equals(value, other.value); } @Override public void checkCompatibility(MappedFieldType otherFT, List<String> conflicts) { super.checkCompatibility(otherFT, conflicts); ConstantKeywordFieldType other = (ConstantKeywordFieldType) otherFT; if (Objects.equals(value, other.value) == false) { conflicts.add("mapper [" + name() + "] has different [value]"); } } @Override public int hashCode() { return 31 * super.hashCode() + Objects.hashCode(value); } /** Return the value that this field wraps. */ public String value() { return value; } /** Set the value. */ public void setValue(String value) { checkIfFrozen(); this.value = Objects.requireNonNull(value); } @Override public String typeName() { return CONTENT_TYPE; } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { return new ConstantKeywordIndexFieldData.Builder(mapperService -> value); } private static String valueToString(Object v) { if (v instanceof BytesRef) { return ((BytesRef) v).utf8ToString(); } else { return v.toString(); } } @Override public Query termQuery(Object value, QueryShardContext context) { if (Objects.equals(valueToString(value), this.value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } @Override public Query termsQuery(List<?> values, QueryShardContext context) { for (Object v : values) { if (Objects.equals(valueToString(v), value)) { return new MatchAllDocsQuery(); } } return new MatchNoDocsQuery(); } @Override public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, QueryShardContext context) { if (this.value.startsWith(value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } public Query wildcardQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, QueryShardContext context) { if (Regex.simpleMatch(value, this.value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } } protected ConstantKeywordFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Settings indexSettings) { super(simpleName, fieldType, defaultFieldType, indexSettings, MultiFields.empty(), CopyTo.empty()); } @Override protected ConstantKeywordFieldMapper clone() { return (ConstantKeywordFieldMapper) super.clone(); } @Override public ConstantKeywordFieldType fieldType() { return (ConstantKeywordFieldType) super.fieldType(); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { String value; if (context.externalValueSet()) { value = context.externalValue().toString(); } else { XContentParser parser = context.parser(); if (parser.currentToken() == XContentParser.Token.VALUE_NULL) { value = fieldType().nullValueAsString(); } else { value = parser.textOrNull(); } } if (Objects.equals(fieldType().value, value) == false) { throw new IllegalArgumentException("[constant_keyword] field [" + name() + "] only accepts values that are equal to the wrapped value [" + fieldType().value() + "], but got [" + value + "]"); } } @Override protected String contentType() { return CONTENT_TYPE; }	i don't think we'll ever have a null_value set, we could just skip this check?
@Override public ConstantKeywordFieldMapper build(BuilderContext context) { setupFieldType(context); return new ConstantKeywordFieldMapper( name, fieldType, defaultFieldType, context.indexSettings()); } } public static class TypeParser implements Mapper.TypeParser { @Override public Mapper.Builder<?,?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { final Object value = node.remove("value"); if (value == null) { throw new MapperParsingException("Property [value] of field [" + name + "] is required and can't be null."); } if (value instanceof Number == false && value instanceof CharSequence == false) { throw new MapperParsingException("Property [value] of field [" + name + "] must be a number or a string, but got [" + value + "]"); } return new ConstantKeywordFieldMapper.Builder(name, value.toString()); } } public static final class ConstantKeywordFieldType extends ConstantFieldType { private String value; public ConstantKeywordFieldType() { super(); } protected ConstantKeywordFieldType(ConstantKeywordFieldType ref) { super(ref); this.value = ref.value; } public ConstantKeywordFieldType clone() { return new ConstantKeywordFieldType(this); } @Override public boolean equals(Object o) { if (super.equals(o) == false) { return false; } ConstantKeywordFieldType other = (ConstantKeywordFieldType) o; return Objects.equals(value, other.value); } @Override public void checkCompatibility(MappedFieldType otherFT, List<String> conflicts) { super.checkCompatibility(otherFT, conflicts); ConstantKeywordFieldType other = (ConstantKeywordFieldType) otherFT; if (Objects.equals(value, other.value) == false) { conflicts.add("mapper [" + name() + "] has different [value]"); } } @Override public int hashCode() { return 31 * super.hashCode() + Objects.hashCode(value); } /** Return the value that this field wraps. */ public String value() { return value; } /** Set the value. */ public void setValue(String value) { checkIfFrozen(); this.value = Objects.requireNonNull(value); } @Override public String typeName() { return CONTENT_TYPE; } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { return new ConstantKeywordIndexFieldData.Builder(mapperService -> value); } private static String valueToString(Object v) { if (v instanceof BytesRef) { return ((BytesRef) v).utf8ToString(); } else { return v.toString(); } } @Override public Query termQuery(Object value, QueryShardContext context) { if (Objects.equals(valueToString(value), this.value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } @Override public Query termsQuery(List<?> values, QueryShardContext context) { for (Object v : values) { if (Objects.equals(valueToString(v), value)) { return new MatchAllDocsQuery(); } } return new MatchNoDocsQuery(); } @Override public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, QueryShardContext context) { if (this.value.startsWith(value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } public Query wildcardQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, QueryShardContext context) { if (Regex.simpleMatch(value, this.value)) { return new MatchAllDocsQuery(); } else { return new MatchNoDocsQuery(); } } } protected ConstantKeywordFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Settings indexSettings) { super(simpleName, fieldType, defaultFieldType, indexSettings, MultiFields.empty(), CopyTo.empty()); } @Override protected ConstantKeywordFieldMapper clone() { return (ConstantKeywordFieldMapper) super.clone(); } @Override public ConstantKeywordFieldType fieldType() { return (ConstantKeywordFieldType) super.fieldType(); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { String value; if (context.externalValueSet()) { value = context.externalValue().toString(); } else { XContentParser parser = context.parser(); if (parser.currentToken() == XContentParser.Token.VALUE_NULL) { value = fieldType().nullValueAsString(); } else { value = parser.textOrNull(); } } if (Objects.equals(fieldType().value, value) == false) { throw new IllegalArgumentException("[constant_keyword] field [" + name() + "] only accepts values that are equal to the wrapped value [" + fieldType().value() + "], but got [" + value + "]"); } } @Override protected String contentType() { return CONTENT_TYPE; }	small comment, it might be clearer to say 'that are equal to the value declared in the mapping'.
protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) { String index = restRequest.param(INDEX.getPreferredName()); IndicesOptions options = IndicesOptions.fromRequest(restRequest, IndicesOptions.STRICT_EXPAND_OPEN_FORBID_CLOSED); GetRollupIndexCapsAction.Request request = new GetRollupIndexCapsAction.Request(Strings.commaDelimitedListToStringArray(index), options); return channel -> client.execute(GetRollupIndexCapsAction.INSTANCE, request, new RestToXContentListener<>(channel)); }	can you use strings.splitstringbycommatoarray, it should be equivalent but that's the function used by the other action when they parse the index param.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(fieldName); script.writeTo(out); out.writeBoolean(ignoreFailure); }	can you add a note to this method about it being used to generate the cache key for the search request and having to be consistent? hopefully we'll be more careful about it.
public PhraseSuggestionBuilder collateParams(Map<String, Object> collateParams) { Objects.requireNonNull(collateParams, "collate parameters cannot be null."); this.collateParams = new LinkedHashMap<>(collateParams); return this; }	i think if we need to be careful about ordering the parameters for serialization we should do it during serialization. in this case we'll just get them in whatever order the collateparams iterates which i imagine is fairly inconsistent. my gut says hashmaps are going to be the most common parameter here and they will iterate in a vaguely consistent order. like, the same map contents won't always have a consistent order because they could have a different underlying hash table size. so i think the sorting you do is during writing is better. no matter what how crazy we are with the map we always will get in order writes.
static Long parseForcedMemoryInBytes(List<String> userDefinedJvmOptions) { String totalMemoryBytesOption = userDefinedJvmOptions.stream() .filter(option -> option.startsWith("-Des.total_memory_bytes=")) .findFirst() .orElse(null); if (totalMemoryBytesOption == null) { return null; } try { return Long.parseLong(totalMemoryBytesOption.split("=", 2)[1]); } catch (NumberFormatException e) { throw new IllegalArgumentException("Unable to parse number of bytes from [" + totalMemoryBytesOption + "]"); } }	i think you don't need an intermediate variable if you used optional.map? suggestion return userdefinedjvmoptions.stream() .filter(option -> option.startswith("-des.total_memory_bytes=")) .findfirst() .map(totalmemorybytesoption -> { try { return long.parselong(totalmemorybytesoption.split("=", 2)[1]); } catch (numberformatexception e) { throw new illegalargumentexception("unable to parse number of bytes from [" + totalmemorybytesoption + "]"); } }) .orelse(null);
public void testTotalMemoryOverride() { assertThat(OsProbe.getTotalMemoryOverride("123456789"), is(123456789L)); assertThat(OsProbe.getTotalMemoryOverride("-1"), nullValue()); assertThat(OsProbe.getTotalMemoryOverride("123456789123456789"), is(123456789123456789L)); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> OsProbe.getTotalMemoryOverride("abc")); assertThat(e.getMessage(), is("Invalid value for [es.total_memory_bytes]: [abc]")); // Although numeric, this value overflows long. This won't be a problem if the override is set appropriately on a 64 bit machine. e = expectThrows(IllegalArgumentException.class, () -> OsProbe.getTotalMemoryOverride("123456789123456789123456789")); assertThat(e.getMessage(), is("Invalid value for [es.total_memory_bytes]: [123456789123456789123456789]")); }	i don't understand why this won't be a problem on a 64 bit machine - can you elaborate?
private <T> T convert(int columnIndex, Class<T> type) throws SQLException { Object val = column(columnIndex); if (val == null) { return null; } DataType columnType = cursor.columns().get(columnIndex - 1).type; String typeString = type != null ? type.getSimpleName() : columnType.getName(); return TypeConverter.convert(val, columnType, type, typeString); }	why is this block of code no longer needed?
@Override public Literal visitIntervalLiteral(IntervalLiteralContext ctx) { IntervalContext interval = ctx.interval(); TimeUnit leading = visitIntervalField(interval.leading); TimeUnit trailing = visitIntervalField(interval.trailing); // only YEAR TO MONTH or DAY TO HOUR/MINUTE/SECOND are valid declaration if (trailing != null) { if (leading == TimeUnit.YEAR && trailing != TimeUnit.MONTH) { throw new ParsingException(source(interval.trailing), "Invalid interval declaration; YEAR trailing unit required to be MONTH, received {}", trailing); } else { if (trailing.ordinal() <= leading.ordinal()) { EnumSet<TimeUnit> range = EnumSet.range(leading, TimeUnit.SECOND); range.remove(leading); throw new ParsingException(source(interval.trailing), "Invalid interval declaration; trailing unit [{}] needs to be smaller than leading unit[{}], " + "expected one of {}", trailing, leading, range); } } } DataType intervalType = IntervalUtils.intervalType(source(interval), leading, trailing); boolean negative = interval.sign != null && interval.sign.getType() == SqlBaseParser.MINUS; TemporalAmount value = null; String valueAsText = null; if (interval.valueNumeric != null) { if (trailing != null) { throw new ParsingException(source(interval.trailing), "Invalid interval declaration; trailing unit [{}] specified but the value is with numeric (single unit), " + "use the string notation instead", trailing); } value = of(interval.valueNumeric, negative, leading); valueAsText = interval.valueNumeric.getText(); } else { value = visitIntervalValue(interval.valuePattern, negative, intervalType); valueAsText = interval.valuePattern.getText(); } String name = "INTERVAL " + valueAsText + " " + leading.name() + (trailing != null ? " TO " + trailing.name() : ""); Interval<?> timeInterval = value instanceof Period ? new IntervalYearMonth((Period) value, intervalType) : new IntervalDayTime((Duration) value, intervalType); return new Literal(source(ctx), name, timeInterval, intervalType); }	you could also put it as default of the switch block.
@Override protected void validateRequest(CreateIndexRequest request, ClusterState state) throws Exception { MetaDataCreateIndexService.validateIndexName(request.index(), state); }	i see where you're heading with the extra state validation, but name collisions are a rare occasion and i'm not sure it's worth it to have the extra api complexity in order to fail early. if we remove that we can move the name validation to org.elasticsearch.action.admin.indices.create.createindexrequest#validate
@Override protected void validateRequest(GetIndexRequest request, ClusterState state) throws Exception { for(String index: request.indices()) { // Create index service also does validation on the index name which does not require // cluster state, so it does not check if things like the index already exists in the cluster MetaDataCreateIndexService.validateIndexOrAliasName(index, InvalidIndexNameException::new); } }	i'm not sure where you're headed, but this thing is better done in org.elasticsearch.action.admin.indices.get.getindexrequest#validate
private void addSortField(SearchContext context, List<SortField> sortFields, String fieldName, boolean reverse, String unmappedType, @Nullable final String missing, MultiValueMode sortMode, String nestedPath, Filter nestedFilter) { if (SCORE_FIELD_NAME.equals(fieldName)) { if (reverse) { sortFields.add(SORT_SCORE_REVERSE); } else { sortFields.add(SORT_SCORE); } } else if (DOC_FIELD_NAME.equals(fieldName)) { if (reverse) { sortFields.add(SORT_DOC_REVERSE); } else { sortFields.add(SORT_DOC); } } else { FieldMapper<?> fieldMapper = context.smartNameFieldMapper(fieldName); if (fieldMapper == null) { if (unmappedType != null) { fieldMapper = context.mapperService().unmappedFieldMapper(unmappedType); } else { throw new SearchParseException(context, "No mapping found for [" + fieldName + "] in order to sort on"); } } if (!fieldMapper.isSortable()) { throw new SearchParseException(context, "Sorting not supported for field[" + fieldName + "]"); } // Enable when we also know how to detect fields that do tokenize, but only emit one token /*if (fieldMapper instanceof StringFieldMapper) { StringFieldMapper stringFieldMapper = (StringFieldMapper) fieldMapper; if (stringFieldMapper.fieldType().tokenized()) { // Fail early throw new SearchParseException(context, "Can't sort on tokenized string field[" + fieldName + "]"); } }*/ // We only support AVG and SUM on number based fields if (!(fieldMapper instanceof NumberFieldMapper) && (sortMode == MultiValueMode.SUM || sortMode == MultiValueMode.AVG)) { sortMode = null; } if (sortMode == null) { sortMode = resolveDefaultSortMode(reverse); } ObjectMapper objectMapper = null; if (nestedPath != null) { ObjectMappers objectMappers = context.mapperService().objectMapper(nestedPath); if (objectMappers == null) { throw new ElasticsearchIllegalArgumentException("failed to find nested object mapping for explicit nested path [" + nestedPath + "]"); } objectMapper = objectMappers.mapper(); if (!objectMapper.nested().isNested()) { throw new ElasticsearchIllegalArgumentException("mapping for explicit nested path is not mapped as nested: [" + nestedPath + "]"); } } else if (!(context instanceof TopHitsContext)) { // Only automatically resolve nested path when sort isn't defined for top_hits objectMapper = context.mapperService().resolveClosestNestedObjectMapper(fieldName); } final Nested nested; if (objectMapper != null && objectMapper.nested().isNested()) { Filter rootDocumentsFilter = context.filterCache().cache(NonNestedDocsFilter.INSTANCE); Filter innerDocumentsFilter; if (nestedFilter != null) { innerDocumentsFilter = context.filterCache().cache(nestedFilter); } else { innerDocumentsFilter = context.filterCache().cache(objectMapper.nestedTypeFilter()); } nested = new Nested(rootDocumentsFilter, innerDocumentsFilter); } else { nested = null; } IndexFieldData.XFieldComparatorSource fieldComparatorSource = context.fieldData().getForField(fieldMapper) .comparatorSource(missing, sortMode, nested); sortFields.add(new SortField(fieldMapper.names().indexName(), fieldComparatorSource, reverse)); } }	what if there are two levels of nested docs and hits are on the 1st level while we are sorting on a property of the 2nd level?
@Override public void setupSuiteScopeCluster() throws Exception { createIndex("idx"); createIndex("empty"); assertAcked(prepareCreate("articles").addMapping("article", jsonBuilder().startObject().startObject("article").startObject("properties") .startObject("comments") .field("type", "nested") .startObject("properties") .startObject("date") .field("type", "long") .endObject() .startObject("message") .field("type", "string") .field("store", true) .field("term_vector", "with_positions_offsets") .field("index_options", "offsets") .endObject() .endObject() .endObject() .endObject().endObject().endObject())); List<IndexRequestBuilder> builders = new ArrayList<>(); for (int i = 0; i < 50; i++) { builders.add(client().prepareIndex("idx", "type", Integer.toString(i)).setSource(jsonBuilder() .startObject() .field(TERMS_AGGS_FIELD, "val" + (i / 10)) .field(SORT_FIELD, i + 1) .field("text", "some text to entertain") .field("field1", 5) .endObject())); } builders.add(client().prepareIndex("idx", "field-collapsing", "1").setSource(jsonBuilder() .startObject() .field("group", "a") .field("text", "term x y z b") .endObject())); builders.add(client().prepareIndex("idx", "field-collapsing", "2").setSource(jsonBuilder() .startObject() .field("group", "a") .field("text", "term x y z n rare") .endObject())); builders.add(client().prepareIndex("idx", "field-collapsing", "3").setSource(jsonBuilder() .startObject() .field("group", "b") .field("text", "x y z term") .endObject())); builders.add(client().prepareIndex("idx", "field-collapsing", "4").setSource(jsonBuilder() .startObject() .field("group", "b") .field("text", "x y term") .endObject())); builders.add(client().prepareIndex("idx", "field-collapsing", "5").setSource(jsonBuilder() .startObject() .field("group", "b") .field("text", "x term") .endObject())); builders.add(client().prepareIndex("idx", "field-collapsing", "6").setSource(jsonBuilder() .startObject() .field("group", "b") .field("text", "term rare") .endObject())); builders.add(client().prepareIndex("idx", "field-collapsing", "7").setSource(jsonBuilder() .startObject() .field("group", "c") .field("text", "x y z term") .endObject())); builders.add(client().prepareIndex("idx", "field-collapsing", "8").setSource(jsonBuilder() .startObject() .field("group", "c") .field("text", "x y term b") .endObject())); builders.add(client().prepareIndex("idx", "field-collapsing", "9").setSource(jsonBuilder() .startObject() .field("group", "c") .field("text", "rare x term") .endObject())); numArticles = scaledRandomIntBetween(10, 100); numArticles -= (numArticles % 5); for (int i = 0; i < numArticles; i++) { final XContentBuilder builder; if (randomBoolean()) { builder = jsonBuilder(); } else if (randomBoolean()) { builder = yamlBuilder(); } else { builder = smileBuilder(); } builder.startObject().field("date", i).startArray("comments"); for (int j = 0; j < i; j++) { String user = Integer.toString(j); builder.startObject().field("id", j).field("user", user).field("message", "some comment").endObject(); } builder.endArray().endObject(); builders.add( client().prepareIndex("articles", "article").setCreate(true).setSource(builder) ); } builders.add( client().prepareIndex("articles", "article", "1") .setSource(jsonBuilder().startObject().field("title", "title 1").field("body", "some text").startArray("comments") .startObject().field("user", "a").field("date", 1l).field("message", "some comment").endObject() .startObject().field("user", "b").field("date", 2l).field("message", "some other comment").endObject() .endArray().endObject()) ); builders.add( client().prepareIndex("articles", "article", "2") .setSource(jsonBuilder().startObject().field("title", "title 2").field("body", "some different text").startArray("comments") .startObject().field("user", "b").field("date", 3l).field("message", "some comment").endObject() .startObject().field("user", "c").field("date", 4l).field("message", "some other comment").endObject() .endArray().endObject()) ); indexRandom(true, builders); ensureSearchable(); }	you can do builder = randomfrom(jsonbuilder(), yamlbuilder(), smilebuilder());.
@Test public void testTopHitsInNestedSimple() throws Exception { SearchResponse searchResponse = client().prepareSearch("articles") .setQuery(matchQuery("title", "title")) .addAggregation( nested("to-comments") .path("comments") .subAggregation( terms("users") .field("comments.user") .subAggregation( topHits("top-comments").addSort("comments.date", SortOrder.ASC) ) ) ) .get(); Nested nested = searchResponse.getAggregations().get("to-comments"); assertThat(nested.getDocCount(), equalTo(4l)); Terms terms = nested.getAggregations().get("users"); Terms.Bucket bucket = terms.getBucketByKey("a"); assertThat(bucket.getDocCount(), equalTo(1l)); TopHits topHits = bucket.getAggregations().get("top-comments"); SearchHits searchHits = topHits.getHits(); assertThat(searchHits.totalHits(), equalTo(1l)); assertThat(searchHits.getAt(0).getNestedPath(), equalTo("comments")); assertThat(searchHits.getAt(0).getNestedOffset(), equalTo(0)); assertThat((Integer) searchHits.getAt(0).getSource().get("date"), equalTo(1)); bucket = terms.getBucketByKey("b"); assertThat(bucket.getDocCount(), equalTo(2l)); topHits = bucket.getAggregations().get("top-comments"); searchHits = topHits.getHits(); assertThat(searchHits.totalHits(), equalTo(2l)); assertThat(searchHits.getAt(0).getNestedPath(), equalTo("comments")); assertThat(searchHits.getAt(0).getNestedOffset(), equalTo(1)); assertThat((Integer) searchHits.getAt(0).getSource().get("date"), equalTo(2)); assertThat(searchHits.getAt(1).getNestedPath(), equalTo("comments")); assertThat(searchHits.getAt(1).getNestedOffset(), equalTo(0)); assertThat((Integer) searchHits.getAt(1).getSource().get("date"), equalTo(3)); bucket = terms.getBucketByKey("c"); assertThat(bucket.getDocCount(), equalTo(1l)); topHits = bucket.getAggregations().get("top-comments"); searchHits = topHits.getHits(); assertThat(searchHits.totalHits(), equalTo(1l)); assertThat(searchHits.getAt(0).getNestedPath(), equalTo("comments")); assertThat(searchHits.getAt(0).getNestedOffset(), equalTo(1)); assertThat((Integer) searchHits.getAt(0).getSource().get("date"), equalTo(4)); }	you can also do string hltype = randomfrom("plain", "fvh", "postings");
@Override void validateParsed(Number value) { if (!Double.isFinite(value.doubleValue())) { throw new IllegalArgumentException("[double] supports only finite values, but got [" + value.toString() + "]"); } }	can we use this message format where we show what we got as an invalid value in the error messages for the other types too?
public void testOutOfRangeValue() { final Map<String, String> outOfRangeValues = new HashMap<>(); outOfRangeValues.put("float", new BigDecimal("3.4028235E39").toString()); outOfRangeValues.put("double", new BigDecimal("1.7976931348623157E309").toString()); outOfRangeValues.put("half_float", new BigDecimal("65504.1").toString()); outOfRangeValues.forEach((type, value) -> { try { createDocumentMapper(type).parse(createIndexRequest(value)); fail("Mapper parsing exception expected for [" + type + "]"); } catch (MapperParsingException e) { assertThat(e.getCause().getMessage(), containsString("[" + type + "] supports only finite values")); } catch (IOException e) { throw new RuntimeException(e); } }); }	as well as these values can we add infinite and nan values to this test?
static DeprecationIssue checkSamlNameIdFormatSetting(final Settings settings, final PluginsAndModules pluginsAndModules, final ClusterState clusterState, final XPackLicenseState licenseState) { final String principalKeySuffix = ".attributes.principal"; List<String> detailsList = PRINCIPAL_ATTRIBUTE.getAttribute().getAllConcreteSettings(settings).sorted(Comparator.comparing(Setting::getKey)) .map(concreteSamlPrincipalSetting -> { String concreteSamlPrincipalSettingKey = concreteSamlPrincipalSetting.getKey(); int principalKeySuffixIndex = concreteSamlPrincipalSettingKey.indexOf(principalKeySuffix); if (principalKeySuffixIndex > 0) { String realm = concreteSamlPrincipalSettingKey.substring(0, principalKeySuffixIndex); String concreteNameIdFormatSettingKey = realm + ".nameid_format"; if (settings.get(concreteNameIdFormatSettingKey) == null) { return String.format(Locale.ROOT, "no value for [%s] set in realm [%s]", concreteNameIdFormatSettingKey, realm); } } return null; }) .filter(detail -> detail != null).collect(Collectors.toList()); if (detailsList.isEmpty()) { return null; } else { String message = "if nameid_format is not explicitly set, the previous default of " + "'urn:oasis:names:tc:SAML:2.0:nameid-format:transient' is no longer used"; String url = "https://www.elastic.co/guide/en/elasticsearch/reference/master/saml-guide.html"; String details = detailsList.stream().collect(Collectors.joining(",")); return new DeprecationIssue(DeprecationIssue.Level.WARNING, message, url, details, false, null); } }	i think we either need some more text here or a section in the breaking release notes and link from here ( i think the latter gives us more flexibility ). i can open a pr for the docs and reference the logs here once we have a link
private void bootstrapAppendOnlyInfoFromWriter(IndexWriter writer) { for (Map.Entry<String, String> entry : writer.getLiveCommitData()) { final String key = entry.getKey(); if (key.equals(MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID)) { maxUnsafeAutoIdTimestamp.set(Math.max(maxUnsafeAutoIdTimestamp.get(), Long.parseLong(entry.getValue()))); } if (key.equals(SequenceNumbers.MAX_SEQ_NO)) { maxSeqNoOfNonAppendOnlyOperations.set(Math.max(maxSeqNoOfNonAppendOnlyOperations.get(), Long.parseLong(entry.getValue()))); } } }	question - why the leniency with max?
private IndexingStrategy planIndexingAsNonPrimary(Index index) throws IOException { final IndexingStrategy plan; final boolean appendOnlyRequest = canOptimizeAddDocument(index); if (appendOnlyRequest && mayHaveBeenIndexedBefore(index) == false && index.seqNo() > maxSeqNoOfNonAppendOnlyOperations.get()) { /* * As soon as an append-only request was indexed into the primary, it can be exposed to a search then users can issue * a follow-up operation on it. In rare cases, the follow up operation can be arrived and processed on a replica before * the original append-only. In this case we can't simply proceed with the append only without consulting the version map. * If a replica has seen a non-append-only operation with a higher seqno than the seqno of an append-only, it may have seen * the document of that append-only request. However if the seqno of an append-only is higher than seqno of any non-append-only * requests, we can assert the replica have not seen the document of that append-only request, thus we can apply optimization. */ assert index.version() == 1L : "can optimize on replicas but incoming version is [" + index.version() + "]"; plan = IndexingStrategy.optimizedAppendOnly(index.seqNo()); } else { if (appendOnlyRequest == false) { maxSeqNoOfNonAppendOnlyOperations.updateAndGet(curr -> Math.max(index.seqNo(), curr)); assert maxSeqNoOfNonAppendOnlyOperations.get() >= index.seqNo() : "max_seqno of non-append-only was not updated;" + "max_seqno non-append-only [" + maxSeqNoOfNonAppendOnlyOperations.get() + "], seqno of index [" + index.seqNo() + "]"; } versionMap.enforceSafeAccess(); // drop out of order operations assert index.versionType().versionTypeForReplicationAndRecovery() == index.versionType() : "resolving out of order delivery based on versioning but version type isn't fit for it. got [" + index.versionType() + "]"; // unlike the primary, replicas don't really care to about creation status of documents // this allows to ignore the case where a document was found in the live version maps in // a delete state and return false for the created flag in favor of code simplicity final OpVsLuceneDocStatus opVsLucene; if (index.seqNo() <= localCheckpointTracker.getCheckpoint()){ // the operation seq# is lower then the current local checkpoint and thus was already put into lucene // this can happen during recovery where older operations are sent from the translog that are already // part of the lucene commit (either from a peer recovery or a local translog) // or due to concurrent indexing & recovery. For the former it is important to skip lucene as the operation in // question may have been deleted in an out of order op that is not replayed. // See testRecoverFromStoreWithOutOfOrderDelete for an example of local recovery // See testRecoveryWithOutOfOrderDelete for an example of peer recovery opVsLucene = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL; } else { opVsLucene = compareOpToLuceneDocBasedOnSeqNo(index); } if (opVsLucene == OpVsLuceneDocStatus.OP_STALE_OR_EQUAL) { plan = IndexingStrategy.processButSkipLucene(false, index.seqNo(), index.version()); } else { plan = IndexingStrategy.processNormally( opVsLucene == OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND, index.seqNo(), index.version() ); } } return plan; }	can we introduce a method similar to mayhavebeenindexedbefore that does the check and also updates the maxseqnoofnonappendonlyoperations? i think it's good to have both marker handling consistent.
public void testTrackMaxSeqNoOfNonAppendOnlyOperations() throws Exception { IOUtils.close(engine, store); store = createStore(); final Path translogPath = createTempDir(); final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.UNASSIGNED_SEQ_NO); try (InternalEngine engine = createEngine(store, translogPath, globalCheckpoint::get)) { final CountDownLatch latch = new CountDownLatch(1); final Thread appendOnlyIndexer = new Thread(() -> { try { latch.countDown(); final int numDocs = scaledRandomIntBetween(100, 1000); for (int i = 0; i < numDocs; i++) { ParsedDocument doc = testParsedDocument("append-only" + i, null, testDocumentWithTextField(), SOURCE, null); if (randomBoolean()) { engine.index(appendOnlyReplica(doc, randomBoolean(), 1, engine.getLocalCheckpointTracker().generateSeqNo())); } else { engine.index(appendOnlyPrimary(doc, randomBoolean(), randomNonNegativeLong())); } } } catch (Exception ex) { throw new RuntimeException("Failed to index", ex); } }); appendOnlyIndexer.setName("append-only indexer"); appendOnlyIndexer.start(); latch.await(); long maxSeqNoOfNonAppendOnly = SequenceNumbers.NO_OPS_PERFORMED; final int numOps = scaledRandomIntBetween(100, 1000); for (int i = 0; i < numOps; i++) { ParsedDocument parsedDocument = testParsedDocument(Integer.toString(i), null, testDocumentWithTextField(), SOURCE, null); if (randomBoolean()) { // On replica - update max_seqno for non-append-only operations final long seqno = engine.getLocalCheckpointTracker().generateSeqNo(); final Engine.Index doc = replicaIndexForDoc(parsedDocument, 1, seqno, randomBoolean()); if (randomBoolean()) { engine.index(doc); } else { engine.delete(new Engine.Delete(doc.type(), doc.id(), doc.uid(), seqno, doc.primaryTerm(), doc.version(), doc.versionType(), doc.origin(), threadPool.relativeTimeInMillis())); } maxSeqNoOfNonAppendOnly = seqno; } else { // On primary - do not update max_seqno for non-append-only operations if (randomBoolean()) { engine.index(indexForDoc(parsedDocument)); } else { engine.delete(new Engine.Delete(parsedDocument.type(), parsedDocument.id(), newUid(parsedDocument.id()))); } } } appendOnlyIndexer.join(120_000); assertThat(engine.getMaxSeqNoOfNonAppendOnlyOperations(), equalTo(maxSeqNoOfNonAppendOnly)); globalCheckpoint.set(engine.getLocalCheckpointTracker().getCheckpoint()); engine.syncTranslog(); engine.flush(); } try (InternalEngine engine = createEngine(store, translogPath, globalCheckpoint::get)) { assertThat("max_seqno from non-append-only was not bootstrap from the safe commit", engine.getMaxSeqNoOfNonAppendOnlyOperations(), equalTo(globalCheckpoint.get())); } }	can we also test a delete?
public static BytesReference toXContent(ToXContent toXContent, XContentType xContentType, Params params, boolean humanReadable) throws IOException { try (XContentBuilder builder = XContentBuilder.builder(xContentType.xContent())) { builder.humanReadable(humanReadable); if (toXContent.isFragment()) { builder.startObject(); } toXContent.toXContent(builder, params); if (toXContent.isFragment()) { builder.endObject(); } return BytesReference.bytes(builder); } }	probably deserves javadoc too.
@Override public String toString() { List<String> components = new ArrayList<>(6); components.add("shard id [" + shardId + "]"); components.add("allocation id [" + allocationId + "]"); components.add("primary term [" + primaryTerm + "]"); components.add("message [" + message + "]"); components.add("markAsStale [" + markAsStale + "]"); if (failure != null) { components.add("failure [" + ExceptionsHelper.stackTrace(failure) + "]"); } return String.join(", ", components); }	also changed the order of elements here, so the stack trace is last.
public String toString() { return "failed shard, shard " + routingEntry + ", message [" + message + "], markAsStale [" + markAsStale + "], failure [" + ExceptionsHelper.stackTrace(failure) + "]"; }	i changed the order of fields here so that the stack trace comes last.
@Override public String toString() { try { XContentBuilder builder = XContentFactory.jsonBuilder(); builder.prettyPrint(); toXContent(builder, EMPTY_PARAMS); return Strings.toString(builder); } catch (Exception e) { throw new RuntimeException(e); } }	do we have a test for this failure case so we know the response is not already too far written to throw/serialize an exception like this?
void assertBasicSearchWorks(int count) throws IOException { logger.info("--> testing basic search"); { Map<String, Object> response = entityAsMap(client().performRequest(new Request("GET", "/" + index + "/_search"))); assertNoFailures(response); int numDocs = (int) XContentMapValues.extractValue("hits.total", response); logger.info("Found {} in old index", numDocs); assertEquals(count, numDocs); } logger.info("--> testing basic search with sort"); { Request searchRequest = new Request("GET", "/" + index + "/_search"); searchRequest.setJsonEntity("{ \\\\"sort\\\\": [{ \\\\"int\\\\" : \\\\"asc\\\\" }]}"); Map<String, Object> response = entityAsMap(client().performRequest(searchRequest)); assertNoFailures(response); assertTotalHits(count, response); } logger.info("--> testing exists filter"); { Request searchRequest = new Request("GET", "/" + index + "/_search"); searchRequest.setJsonEntity("{ \\\\"query\\\\": { \\\\"exists\\\\" : {\\\\"field\\\\": \\\\"string\\\\"} }}"); Map<String, Object> response = entityAsMap(client().performRequest(searchRequest)); assertNoFailures(response); assertTotalHits(count, response); } logger.info("--> testing field with dots in the name"); { Request searchRequest = new Request("GET", "/" + index + "/_search"); searchRequest.setJsonEntity("{ \\\\"query\\\\": { \\\\"exists\\\\" : {\\\\"field\\\\": \\\\"field.with.dots\\\\"} }}"); Map<String, Object> response = entityAsMap(client().performRequest(searchRequest)); assertNoFailures(response); assertTotalHits(count, response); } }	this line was missing _explain so it was passing even though it wasn't really running the test.
@Override public void onFailure(Throwable e) { try { timeBuckets[bucketId] = -1; docBuckets[bucketId] = -1; } finally { super.onFailure(e); } } } protected String nodeName() { return clusterService.localNode().name(); } protected String nodeId() { return clusterService.localNode().id(); } private final boolean assertBuckets(long[] buckets) { for (int i = 0; i < buckets.length; i++) { assert buckets[i] >= 0 : "Bucket value was negative: " + buckets[i] + " bucket id: " + i; } return true; } /** * Bind the originating request to the benchmark's search requests so that the security credentials are passed through. * * @param searchRequests Benchmark search requests * @param originalRequest Originating action request * @return Benchmark search requests re-constructed to use credentials of originating request */ private List<SearchRequest> bindOriginalRequest(final List<SearchRequest> searchRequests, final ActionRequest originalRequest) { final List<SearchRequest> newSearchRequests = new ArrayList<>(searchRequests.size()); for (final SearchRequest searchRequest : searchRequests) { newSearchRequests.add(new SearchRequest(searchRequest, originalRequest)); }	@uboness @javanna could you comment as to whether this is the correct way to pass the original request headers and context through to sub-searches?
@Override protected ClusterStatsNodeResponse nodeOperation(ClusterStatsNodeRequest nodeRequest) { NodeInfo nodeInfo = nodeService.info(true, true, false, true, false, true, false, true, false, false); NodeStats nodeStats = nodeService.stats(CommonStatsFlags.NONE, false, true, true, false, true, false, false, false, false, false, false); List<ShardStats> shardsStats = new ArrayList<>(); for (IndexService indexService : indicesService) { for (IndexShard indexShard : indexService) { if (indexShard.routingEntry() != null && indexShard.routingEntry().active()) { // only report on fully started shards shardsStats.add(new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indicesService.getIndicesQueryCache(), indexShard, SHARD_STATS_FLAGS), indexShard.commitStats())); } } } ClusterHealthStatus clusterStatus = null; if (clusterService.state().nodes().isLocalNodeElectedMaster()) { clusterStatus = new ClusterStateHealth(clusterService.state()).getStatus(); } return new ClusterStatsNodeResponse(nodeInfo.getNode(), clusterStatus, nodeInfo, nodeStats, shardsStats.toArray(new ShardStats[shardsStats.size()])); }	ignoring that we need it for network_types, i kind of like that settings are now on by default.
public boolean hasFieldOrDocumentLevelSecurity() { return indices.hasFieldOrDocumentLevelSecurity(); } /** * @param restrictedIndices An automaton that can determine whether a string names * a restricted index. For simple unit tests, this can be * {@link Automatons#EMPTY}	if the pr gets merged, this new method here can be used in indicespermission#authorize to avoid the [loop](https://github.com/elastic/elasticsearch/blob/92db6a62c0b7f827e25cb992da5bac9cfdf43f34/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/security/authz/permission/indicespermission.java#l385) for computing dls and fls controls when the roles does not have it.
public static License fromXContent(XContentParser parser) throws IOException { Builder builder = new Builder(); XContentParser.Token token; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { String currentFieldName = parser.currentName(); token = parser.nextToken(); if (token.isValue()) { if (Fields.UID.equals(currentFieldName)) { builder.uid(parser.text()); } else if (Fields.TYPE.equals(currentFieldName)) { builder.type(parser.text()); } else if (Fields.SUBSCRIPTION_TYPE.equals(currentFieldName)) { builder.subscriptionType(parser.text()); } else if (Fields.ISSUE_DATE.equals(currentFieldName)) { builder.issueDate(parseDate(parser, "issue", false)); } else if (Fields.ISSUE_DATE_IN_MILLIS.equals(currentFieldName)) { builder.issueDate(parser.longValue()); } else if (Fields.FEATURE.equals(currentFieldName)) { builder.feature(parser.text()); } else if (Fields.EXPIRY_DATE.equals(currentFieldName)) { builder.expiryDate(parseDate(parser, "expiration", true)); } else if (Fields.EXPIRY_DATE_IN_MILLIS.equals(currentFieldName)) { builder.expiryDate(parser.longValue()); } else if (Fields.START_DATE.equals(currentFieldName)) { builder.startDate(parseDate(parser, "start", false)); } else if (Fields.START_DATE_IN_MILLIS.equals(currentFieldName)) { builder.startDate(parser.longValue()); } else if (Fields.MAX_NODES.equals(currentFieldName)) { builder.maxNodes(parser.intValue()); } else if (Fields.ISSUED_TO.equals(currentFieldName)) { builder.issuedTo(parser.text()); } else if (Fields.ISSUER.equals(currentFieldName)) { builder.issuer(parser.text()); } else if (Fields.SIGNATURE.equals(currentFieldName)) { builder.signature(parser.text()); } else if (Fields.VERSION.equals(currentFieldName)) { builder.version(parser.intValue()); } // Ignore unknown elements - might be new version of license } else if (token == XContentParser.Token.START_ARRAY) { // It was probably created by newer version - ignoring parser.skipChildren(); } else if (token == XContentParser.Token.START_OBJECT) { // It was probably created by newer version - ignoring parser.skipChildren(); } } } // not a license spec if (builder.signature != null) { int version; // In case the signature is truncated/malformed we might end up with fewer than 4 bytes in the byteBuffer // or with a string that cannot be base64 decoded. In either case return a more friendly error instead of // just throwing the BufferUnderflowException or the IllegalArgumentException try { byte[] signatureBytes = Base64.getDecoder().decode(builder.signature); ByteBuffer byteBuffer = ByteBuffer.wrap(signatureBytes); version = byteBuffer.getInt(); } catch (BufferUnderflowException | IllegalArgumentException e) { throw new ElasticsearchException("malformed signature for license [" + builder.uid + "]", e); } // we take the absolute version, because negative versions // mean that the license was generated by the cluster (see TrialLicense) // and positive version means that the license was signed if (version < 0) { version *= -1; } if (version == 0) { throw new ElasticsearchException("malformed signature for license [" + builder.uid + "]"); } else if (version > VERSION_CURRENT) { throw new ElasticsearchException("Unknown license version found, please upgrade all nodes to the latest " + "elasticsearch-license plugin"); } // signature version is the source of truth builder.version(version); } return builder.build(); }	is it intentional that you don't just catch exception? i don't think there's any exception that could be thrown here that we wouldn't want to wrap with a better message.
public void testCheck_GivenMixedMissingAndExistingConcreteSourceIndex() throws InterruptedException { assertValidation( listener -> simpleNonRemoteValidator.validate( CLUSTER_STATE, new String[] { SOURCE_1, "missing" }, "dest", TEST_VALIDATIONS, listener ), (Boolean) null, e -> { assertEquals(1, e.validationErrors().size()); assertThat(e.validationErrors().get(0), equalTo("no such index [missing]")); } ); // assertValidation( // listener -> simpleNonRemoteValidator.validate( // CLUSTER_STATE, // new String[] { SOURCE_1, "missing" }, // "dest", // Collections.emptyList(), // listener // ), // true, // null // ); }	^ needs re-activation or should be removed
private TaskProvider<Zip> createBundleTasks(final Project project, PluginPropertiesExtension extension) { final var pluginMetadata = project.file("src/main/plugin-metadata"); final var buildProperties = project.getTasks().register("pluginProperties", GeneratePluginPropertiesTask.class, task -> { /*task.doFirst(t -> { if (extension.getName() == null) { throw new InvalidUserDataException("name is a required setting for esplugin"); } if (extension.getDescription() == null) { throw new InvalidUserDataException("description is a required setting for esplugin"); } if (extension.getType().equals(PluginType.BOOTSTRAP) == false && extension.getClassname() == null) { throw new InvalidUserDataException("classname is a required setting for esplugin"); } });*/ task.getPluginName().set(providerFactory.provider(extension::getName)); task.getPluginDescription().set(providerFactory.provider(extension::getDescription)); task.getPluginVersion().set(providerFactory.provider(extension::getVersion)); task.getElasticsearchVersion() .set(Version.fromString(VersionProperties.getElasticsearch()).toString()); var javaExtension = project.getExtensions().getByType(JavaPluginExtension.class); task.getJavaVersion().set(providerFactory.provider(() -> javaExtension.getTargetCompatibility().toString())); task.getClassname().set(providerFactory.provider(extension::getClassname)); task.getExtendedPlugins().set(providerFactory.provider(extension::getExtendedPlugins)); task.getHasNativeController().set(providerFactory.provider(extension::isHasNativeController)); task.getRequiresKeystore().set(providerFactory.provider(extension::isRequiresKeystore)); task.getPluginType().set(providerFactory.provider(extension::getType)); task.getJavaOpts().set(providerFactory.provider(extension::getJavaOpts)); task.getIsLicensed().set(providerFactory.provider(extension::isLicensed)); var mainSourceSet = project.getExtensions().getByType(SourceSetContainer.class).getByName(SourceSet.MAIN_SOURCE_SET_NAME); FileCollection moduleInfoFile = mainSourceSet.getJava().matching(pattern -> pattern.include("module-info.java")); task.getModuleInfoFile().setFrom(moduleInfoFile); }); // add the plugin properties and metadata to test resources, so unit tests can // know about the plugin (used by test security code to statically initialize the plugin in unit tests) var testSourceSet = project.getExtensions().getByType(SourceSetContainer.class).getByName("test"); Map<String, Object> map = Map.of("builtBy", buildProperties); testSourceSet.getOutput().dir(map, new File(project.getBuildDir(), "generated-resources")); testSourceSet.getResources().srcDir(pluginMetadata); var bundleSpec = createBundleSpec(project, pluginMetadata, buildProperties); extension.setBundleSpec(bundleSpec); // create the actual bundle task, which zips up all the files for the plugin final var bundle = project.getTasks().register("bundlePlugin", Zip.class, zip -> zip.with(bundleSpec)); project.getTasks().named(BasePlugin.ASSEMBLE_TASK_NAME).configure(task -> task.dependsOn(bundle)); // also make the zip available as a configuration (used when depending on this project) var configuration = project.getConfigurations().create("zip"); configuration.getAttributes().attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.ZIP_TYPE); project.getArtifacts().add("zip", bundle); var explodedBundle = project.getTasks().register(EXPLODED_BUNDLE_PLUGIN_TASK_NAME, Sync.class, sync -> { sync.with(bundleSpec); sync.into(new File(project.getBuildDir(), "explodedBundle/" + extension.getName())); }); // also make the exploded bundle available as a configuration (used when depending on this project) var explodedBundleZip = project.getConfigurations().create(EXPLODED_BUNDLE_CONFIG); explodedBundleZip.setCanBeResolved(false); explodedBundleZip.setCanBeConsumed(true); explodedBundleZip.getAttributes().attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.DIRECTORY_TYPE); project.getArtifacts().add(EXPLODED_BUNDLE_CONFIG, explodedBundle); return bundle; }	putting this in a dofirst block changes the behaviour here in a way that we fail later than before. if we keep the check in the configuration block we would fail when the task is materialized which is usually during task graph calculation. imo we should just keep that behaviour and fail as early as we reliable can to shorten the feedback cycle for the user.
private TaskProvider<Zip> createBundleTasks(final Project project, PluginPropertiesExtension extension) { final var pluginMetadata = project.file("src/main/plugin-metadata"); final var buildProperties = project.getTasks().register("pluginProperties", GeneratePluginPropertiesTask.class, task -> { /*task.doFirst(t -> { if (extension.getName() == null) { throw new InvalidUserDataException("name is a required setting for esplugin"); } if (extension.getDescription() == null) { throw new InvalidUserDataException("description is a required setting for esplugin"); } if (extension.getType().equals(PluginType.BOOTSTRAP) == false && extension.getClassname() == null) { throw new InvalidUserDataException("classname is a required setting for esplugin"); } });*/ task.getPluginName().set(providerFactory.provider(extension::getName)); task.getPluginDescription().set(providerFactory.provider(extension::getDescription)); task.getPluginVersion().set(providerFactory.provider(extension::getVersion)); task.getElasticsearchVersion() .set(Version.fromString(VersionProperties.getElasticsearch()).toString()); var javaExtension = project.getExtensions().getByType(JavaPluginExtension.class); task.getJavaVersion().set(providerFactory.provider(() -> javaExtension.getTargetCompatibility().toString())); task.getClassname().set(providerFactory.provider(extension::getClassname)); task.getExtendedPlugins().set(providerFactory.provider(extension::getExtendedPlugins)); task.getHasNativeController().set(providerFactory.provider(extension::isHasNativeController)); task.getRequiresKeystore().set(providerFactory.provider(extension::isRequiresKeystore)); task.getPluginType().set(providerFactory.provider(extension::getType)); task.getJavaOpts().set(providerFactory.provider(extension::getJavaOpts)); task.getIsLicensed().set(providerFactory.provider(extension::isLicensed)); var mainSourceSet = project.getExtensions().getByType(SourceSetContainer.class).getByName(SourceSet.MAIN_SOURCE_SET_NAME); FileCollection moduleInfoFile = mainSourceSet.getJava().matching(pattern -> pattern.include("module-info.java")); task.getModuleInfoFile().setFrom(moduleInfoFile); }); // add the plugin properties and metadata to test resources, so unit tests can // know about the plugin (used by test security code to statically initialize the plugin in unit tests) var testSourceSet = project.getExtensions().getByType(SourceSetContainer.class).getByName("test"); Map<String, Object> map = Map.of("builtBy", buildProperties); testSourceSet.getOutput().dir(map, new File(project.getBuildDir(), "generated-resources")); testSourceSet.getResources().srcDir(pluginMetadata); var bundleSpec = createBundleSpec(project, pluginMetadata, buildProperties); extension.setBundleSpec(bundleSpec); // create the actual bundle task, which zips up all the files for the plugin final var bundle = project.getTasks().register("bundlePlugin", Zip.class, zip -> zip.with(bundleSpec)); project.getTasks().named(BasePlugin.ASSEMBLE_TASK_NAME).configure(task -> task.dependsOn(bundle)); // also make the zip available as a configuration (used when depending on this project) var configuration = project.getConfigurations().create("zip"); configuration.getAttributes().attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.ZIP_TYPE); project.getArtifacts().add("zip", bundle); var explodedBundle = project.getTasks().register(EXPLODED_BUNDLE_PLUGIN_TASK_NAME, Sync.class, sync -> { sync.with(bundleSpec); sync.into(new File(project.getBuildDir(), "explodedBundle/" + extension.getName())); }); // also make the exploded bundle available as a configuration (used when depending on this project) var explodedBundleZip = project.getConfigurations().create(EXPLODED_BUNDLE_CONFIG); explodedBundleZip.setCanBeResolved(false); explodedBundleZip.setCanBeConsumed(true); explodedBundleZip.getAttributes().attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.DIRECTORY_TYPE); project.getArtifacts().add(EXPLODED_BUNDLE_CONFIG, explodedBundle); return bundle; }	we could make this cleaner by finally porting the plugin extension to use lazy properties. but we should do this in a dedicated pr. i've created https://github.com/elastic/elasticsearch/issues/86057 to track this
private void syncShardStatsOnNewMaster(ClusterChangedEvent event) { SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE); if (snapshotsInProgress == null) { return; } final DiscoveryNode masterNode = event.state().nodes().getMasterNode(); for (SnapshotsInProgress.Entry snapshot : snapshotsInProgress.entries()) { if (snapshot.state() == State.STARTED || snapshot.state() == State.ABORTED) { Map<ShardId, IndexShardSnapshotStatus> localShards = currentSnapshotShards(snapshot.snapshot()); if (localShards != null) { ImmutableOpenMap<ShardId, ShardSnapshotStatus> masterShards = snapshot.shards(); for(Map.Entry<ShardId, IndexShardSnapshotStatus> localShard : localShards.entrySet()) { ShardId shardId = localShard.getKey(); ShardSnapshotStatus masterShard = masterShards.get(shardId); if (masterShard != null && masterShard.state().completed() == false) { final IndexShardSnapshotStatus.Copy indexShardSnapshotStatus = localShard.getValue().asCopy(); final Stage stage = indexShardSnapshotStatus.getStage(); // Master knows about the shard and thinks it has not completed if (stage == Stage.DONE) { // but we think the shard is done - we need to make new master know that the shard is done logger.debug("[{}] new master thinks the shard [{}] is not completed but the shard is done locally, " + "updating status on the master", snapshot.snapshot(), shardId); notifySuccessfulSnapshotShard(snapshot.snapshot(), shardId, masterNode); } else if (stage == Stage.FAILURE) { // but we think the shard failed - we need to make new master know that the shard failed logger.debug("[{}] new master thinks the shard [{}] is not completed but the shard failed locally, " + "updating status on master", snapshot.snapshot(), shardId); notifyFailedSnapshotShard(snapshot.snapshot(), shardId, indexShardSnapshotStatus.getFailure(), masterNode); } } } } } } }	all this passing down of masternode is different from the 7.x/master version since we need the bwc path in the status update sending below.
@Override public int compareTo(ByteSizeValue other) { long unitValue = unit.toBytes(1); long otherUnitValue = other.unit.toBytes(1); ByteSizeUnit minUnit = unitValue < otherUnitValue ? unit : other.unit; long thisValue = this.convert(minUnit); long otherValue = other.convert(minUnit); if (thisValue == otherValue) { if (thisValue == Long.MAX_VALUE || thisValue == Long.MIN_VALUE) { if (unitValue > otherUnitValue) { return 1; } else if (unitValue < otherUnitValue) { return -1; } } return 0; } else if (thisValue < otherValue) { return -1; } else { return 1; } }	what about just converting to bytes and comparing? the way you have it now this isn't consistent with equals.... also the _cat api we call tostring which doesn't really use the unit anyway.
@Override public int compareTo(SizeValue other) { long unitSize = sizeUnit.toSingles(1); long otherUnitSize = other.sizeUnit.toSingles(1); SizeUnit minUnit = unitSize < otherUnitSize ? sizeUnit : other.sizeUnit; long thisSize = this.convert(minUnit); long otherSize = other.convert(minUnit); if (thisSize == otherSize) { if (thisSize == Long.MAX_VALUE || thisSize == Long.MIN_VALUE) { if (unitSize < otherUnitSize) { return -1; } else if (unitSize > otherUnitSize) { return 1; } } return 0; } else if (thisSize < otherSize) { return -1; } else { return 1; } }	same comment as the last class - i'd just convert to single here. the equals method is funny on this class - i'd change it so it lines converts to single as well.
@Override public int compareTo(TimeValue timeValue) { Byte thisByte = TIME_UNIT_BYTE_MAP.get(timeUnit); Byte otherByte = TIME_UNIT_BYTE_MAP.get(timeValue.timeUnit); TimeUnit minUnit = BYTE_TIME_UNIT_MAP.get(thisByte > otherByte ? thisByte : otherByte); long thisDuration = minUnit.convert(duration, timeUnit); long otherDuration = minUnit.convert(timeValue.duration, timeValue.timeUnit); if (thisDuration == otherDuration) { if (thisDuration == Long.MAX_VALUE || thisDuration == Long.MIN_VALUE) { if (thisByte > otherByte) { return 1; } else if (thisByte < otherByte) { return -1; } } return 0; } else if (thisDuration < otherDuration) { return -1; } else { return 1; } }	same deal, i'd just convert it to nanos.
public static RestResponse buildTextPlainResponse(Table table, RestChannel channel) throws IOException { RestRequest request = channel.request(); boolean verbose = request.paramAsBoolean("v", false); List<DisplayHeader> headers = buildDisplayHeaders(table, request); int[] width = buildWidths(table, request, verbose, headers); BytesStreamOutput bytesOut = channel.bytesOutput(); UTF8StreamWriter out = new UTF8StreamWriter().setOutput(bytesOut); int lastHeader = headers.size() - 1; if (verbose) { for (int col = 0; col < headers.size(); col++) { DisplayHeader header = headers.get(col); boolean isLastColumn = col == lastHeader; pad(new Table.Cell(header.display, table.findHeaderByName(header.name)), width[col], request, out, isLastColumn); if (!isLastColumn) { out.append(" "); } } out.append("\\\\n"); } List<Integer> rowOrder = getRowOrder(table, request); for (Integer row: rowOrder) { for (int col = 0; col < headers.size(); col++) { DisplayHeader header = headers.get(col); boolean isLastColumn = col == lastHeader; pad(table.getAsMap().get(header.name).get(row), width[col], request, out, isLastColumn); if (!isLastColumn) { out.append(" "); } } out.append("\\\\n"); } out.close(); return new BytesRestResponse(RestStatus.OK, BytesRestResponse.TEXT_CONTENT_TYPE, bytesOut.bytes()); }	maybe return an int[] so this doesn't have to box?
static List<Integer> getRowOrder(Table table, RestRequest request) { String[] columnOrdering = request.paramAsStringArray("s", null); List<Integer> rowOrder = new ArrayList<>(); for(int i = 0; i < table.getRows().size(); i++) { rowOrder.add(i); } if(columnOrdering != null) { Map<String, String> headerAliasMap = table.getAliasMap(); List<ColumnOrderElement> ordering = new ArrayList<>(); for(int i = 0; i < columnOrdering.length; i++) { String[] orderingEntry = columnOrdering[i].split(":"); boolean reverse = (orderingEntry.length > 1 && orderingEntry[1].equals("desc")); if(headerAliasMap.containsKey(orderingEntry[0])) { ordering.add(new ColumnOrderElement(headerAliasMap.get(orderingEntry[0]), reverse)); } else { ordering.add(new ColumnOrderElement(orderingEntry[0], reverse)); } } Collections.sort(rowOrder, new TableIndexComparator(table, ordering)); } return rowOrder; }	nit: we usually add a space here. we don't have anything that enforces this style but we usually do.
static List<Integer> getRowOrder(Table table, RestRequest request) { String[] columnOrdering = request.paramAsStringArray("s", null); List<Integer> rowOrder = new ArrayList<>(); for(int i = 0; i < table.getRows().size(); i++) { rowOrder.add(i); } if(columnOrdering != null) { Map<String, String> headerAliasMap = table.getAliasMap(); List<ColumnOrderElement> ordering = new ArrayList<>(); for(int i = 0; i < columnOrdering.length; i++) { String[] orderingEntry = columnOrdering[i].split(":"); boolean reverse = (orderingEntry.length > 1 && orderingEntry[1].equals("desc")); if(headerAliasMap.containsKey(orderingEntry[0])) { ordering.add(new ColumnOrderElement(headerAliasMap.get(orderingEntry[0]), reverse)); } else { ordering.add(new ColumnOrderElement(orderingEntry[0], reverse)); } } Collections.sort(rowOrder, new TableIndexComparator(table, ordering)); } return rowOrder; }	i'd do something like boolean reverse = false; if (columnordering[i].endswith(":desc")) { columnorder[i] = columnorder[i].substring(...); } else if (columnordering[i].endswith(":asc")) { columnorder[i] = columnorder[i].substring(...); } or something like that. i think we should complain if it ends in something other than :desc and :asc.
static List<Integer> getRowOrder(Table table, RestRequest request) { String[] columnOrdering = request.paramAsStringArray("s", null); List<Integer> rowOrder = new ArrayList<>(); for(int i = 0; i < table.getRows().size(); i++) { rowOrder.add(i); } if(columnOrdering != null) { Map<String, String> headerAliasMap = table.getAliasMap(); List<ColumnOrderElement> ordering = new ArrayList<>(); for(int i = 0; i < columnOrdering.length; i++) { String[] orderingEntry = columnOrdering[i].split(":"); boolean reverse = (orderingEntry.length > 1 && orderingEntry[1].equals("desc")); if(headerAliasMap.containsKey(orderingEntry[0])) { ordering.add(new ColumnOrderElement(headerAliasMap.get(orderingEntry[0]), reverse)); } else { ordering.add(new ColumnOrderElement(orderingEntry[0], reverse)); } } Collections.sort(rowOrder, new TableIndexComparator(table, ordering)); } return rowOrder; }	i think we should complain if we don't find the header name.
protected FeatureImportance createTestInstance() { return new FeatureImportance( randomAlphaOfLength(10), randomDoubleBetween(-10.0, 10.0, false), Stream.generate(() -> randomAlphaOfLength(10)) .limit(randomLongBetween(2, 10)) .map(name -> new FeatureImportance.ClassImportance(name, randomDoubleBetween(-10, 10, false))) .collect(Collectors.toList())); }	the list<classimportance> parameter can still be null
* @see #merge(boolean, ReducedQueryPhase, Collection, IntFunction) */ public InternalSearchResponse buildResponse(SearchHits hits, Collection<? extends SearchPhaseResult> fetchResults) { SearchProfileResults profileResults = mergeProfile(fetchResults); return new InternalSearchResponse(hits, aggregations, suggest, profileResults, timedOut, terminatedEarly, numReducePhases); }	i guess this splits merging across two places: the caller searchphasecontroller#merge and here in buildresponse. i like the direction this change is going though, it feels like reducedqueryphase should own more of the merging logic.
@Override public FetchSubPhaseProcessor getProcessor(FetchContext fetchContext) { FetchSourceContext fetchSourceContext = fetchContext.fetchSourceContext(); if (fetchSourceContext == null || fetchSourceContext.fetchSource() == false) { return null; } String index = fetchContext.getIndexName(); assert fetchSourceContext.fetchSource(); return new FetchSubPhaseProcessor() { private int fastPath; private int loadedNested; @Override public void setNextReader(LeafReaderContext readerContext) { } @Override public void process(HitContext hitContext) { if (fetchContext.getSearchExecutionContext().isSourceEnabled() == false) { if (containsFilters(fetchSourceContext)) { throw new IllegalArgumentException( "unable to fetch fields from _source field: _source is disabled in the mappings for index [" + index + "]"); } return; } hitExecute(fetchSourceContext, hitContext); } @SuppressWarnings("unchecked") private void hitExecute(FetchSourceContext fetchSourceContext, HitContext hitContext) { final boolean nestedHit = hitContext.hit().getNestedIdentity() != null; SourceLookup source = hitContext.sourceLookup(); // If this is a parent document and there are no source filters, then add the source as-is. if (nestedHit == false && containsFilters(fetchSourceContext) == false) { hitContext.hit().sourceRef(source.internalSourceRef()); fastPath++; return; } // Otherwise, filter the source and add it to the hit. Object value = source.filter(fetchSourceContext); if (nestedHit) { loadedNested++; value = getNestedSource((Map<String, Object>) value, hitContext); } try { final int initialCapacity = nestedHit ? 1024 : Math.min(1024, source.internalSourceRef().length()); BytesStreamOutput streamOutput = new BytesStreamOutput(initialCapacity); XContentBuilder builder = new XContentBuilder(source.sourceContentType().xContent(), streamOutput); if (value != null) { builder.value(value); } else { // This happens if the source filtering could not find the specified in the _source. // Just doing `builder.value(null)` is valid, but the xcontent validation can't detect what format // it is. In certain cases, for example response serialization we fail if no xcontent type can't be // detected. So instead we just return an empty top level object. Also this is in inline with what was // being return in this situation in 5.x and earlier. builder.startObject(); builder.endObject(); } hitContext.hit().sourceRef(BytesReference.bytes(builder)); } catch (IOException e) { throw new ElasticsearchException("Error filtering source", e); } } @Override public Map<String, Object> getDebugInfo() { return Map.of("fast_path", fastPath, "loaded_nested", loadedNested); } }; }	does loadnested give additional insights? is this just the same as the number of inner hits? also it's not so much of a "load" as a "pull out an inner map object".
public void testMerge() { List<CompletionSuggestion> suggestions = new ArrayList<>(); int maxSuggestSize = 0; for (int i = 0; i < randomIntBetween(1, 5); i++) { int size = randomIntBetween(1, 20); maxSuggestSize += size; suggestions.add(new CompletionSuggestion(randomAlphaOfLength(randomIntBetween(1, 5)), size, false)); } int nShards = randomIntBetween(1, 20); int queryResultSize = randomBoolean() ? 0 : randomIntBetween(1, nShards * 2); boolean profile = randomBoolean(); for (int trackTotalHits : new int[] { SearchContext.TRACK_TOTAL_HITS_DISABLED, SearchContext.TRACK_TOTAL_HITS_ACCURATE }) { AtomicArray<SearchPhaseResult> queryResults = generateQueryResults(nShards, suggestions, queryResultSize, false, profile); SearchPhaseController.ReducedQueryPhase reducedQueryPhase = searchPhaseController.reducedQueryPhase( queryResults.asList(), new ArrayList<>(), new ArrayList<>(), new SearchPhaseController.TopDocsStats(trackTotalHits), 0, true, InternalAggregationTestCase.emptyReduceContextBuilder(), true ); List<SearchShardTarget> shards = queryResults.asList().stream().map(SearchPhaseResult::getSearchShardTarget).collect(toList()); AtomicArray<SearchPhaseResult> fetchResults = generateFetchResults( shards, reducedQueryPhase.sortedTopDocs.scoreDocs, reducedQueryPhase.suggest, profile ); InternalSearchResponse mergedResponse = searchPhaseController.merge(false, reducedQueryPhase, fetchResults.asList(), fetchResults::get); if (trackTotalHits == SearchContext.TRACK_TOTAL_HITS_DISABLED) { assertNull(mergedResponse.hits.getTotalHits()); } else { assertThat(mergedResponse.hits.getTotalHits().value, equalTo(0L)); assertEquals(mergedResponse.hits.getTotalHits().relation, Relation.EQUAL_TO); } for (SearchHit hit : mergedResponse.hits().getHits()) { SearchPhaseResult searchPhaseResult = fetchResults.get(hit.getShard().getShardId().id()); assertSame(searchPhaseResult.getSearchShardTarget(), hit.getShard()); } int suggestSize = 0; for (Suggest.Suggestion<?> s : reducedQueryPhase.suggest) { suggestSize += s.getEntries().stream().mapToInt(e -> e.getOptions().size()).sum(); } assertThat(suggestSize, lessThanOrEqualTo(maxSuggestSize)); assertThat(mergedResponse.hits().getHits().length, equalTo(reducedQueryPhase.sortedTopDocs.scoreDocs.length - suggestSize)); Suggest suggestResult = mergedResponse.suggest(); for (Suggest.Suggestion<?> suggestion : reducedQueryPhase.suggest) { assertThat(suggestion, instanceOf(CompletionSuggestion.class)); if (suggestion.getEntries().get(0).getOptions().size() > 0) { CompletionSuggestion suggestionResult = suggestResult.getSuggestion(suggestion.getName()); assertNotNull(suggestionResult); List<CompletionSuggestion.Entry.Option> options = suggestionResult.getEntries().get(0).getOptions(); assertThat(options.size(), equalTo(suggestion.getEntries().get(0).getOptions().size())); for (CompletionSuggestion.Entry.Option option : options) { assertNotNull(option.getHit()); SearchPhaseResult searchPhaseResult = fetchResults.get(option.getHit().getShard().getShardId().id()); assertSame(searchPhaseResult.getSearchShardTarget(), option.getHit().getShard()); } } } if (profile) { assertThat(mergedResponse.profile().entrySet(), hasSize(nShards)); assertThat( // All shards should have a query profile mergedResponse.profile().toString(), mergedResponse.profile().values().stream().filter(r -> r.getQueryProfileResults() != null).count(), equalTo((long) nShards) ); assertThat( // Some or all shards should have a fetch profile mergedResponse.profile().toString(), mergedResponse.profile().values().stream().filter(r -> r.getFetch() != null).count(), both(greaterThan(0L)).and(lessThanOrEqualTo((long) nShards)) ); } else { assertThat(mergedResponse.profile(), is(anEmptyMap())); } } } /** * Generate random query results received from the provided number of shards, including the provided * number of search hits and randomly generated completion suggestions based on the name and size of the provided ones. * Note that <code>shardIndex</code> is already set to the generated completion suggestions to simulate what * {@link SearchPhaseController#reducedQueryPhase} does, * meaning that the returned query results can be fed directly to {@link SearchPhaseController#sortDocs}	not a big deal, but did you intend to move this into the loop?
public void testRetentionLeasesEstablishedWhenPromotingPrimary() throws Exception { final String index = "recover_and_create_leases_in_promotion"; if (CLUSTER_TYPE == ClusterType.OLD) { Settings.Builder settings = Settings.builder() .put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), between(1, 5)) .put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), between(1, 2)) // triggers nontrivial promotion .put(INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), "100ms") .put(SETTING_ALLOCATION_MAX_RETRY.getKey(), "0") // fail faster .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true); createIndex(index, settings.build()); int numDocs = randomInt(10); indexDocs(index, 0, numDocs); if (randomBoolean()) { client().performRequest(new Request("POST", "/" + index + "/_flush")); } } ensureGreen(index); if (CLUSTER_TYPE == ClusterType.UPGRADED) { assertAllCopiesHaveRetentionLeases(index); } }	perhaps randomly 0 or 1 replica?
synchronized ClusterState updateSettings(final ClusterState currentState, Settings transientToApply, Settings persistentToApply) { Settings.Builder transientSettings = Settings.builder(); transientSettings.put(currentState.metaData().transientSettings()); Settings.Builder persistentSettings = Settings.builder(); persistentSettings.put(currentState.metaData().persistentSettings()); boolean changed = clusterSettings.updateDynamicSettings(transientToApply, transientSettings, transientUpdates, "transient") | clusterSettings.updateDynamicSettings(persistentToApply, persistentSettings, persistentUpdates, "persistent"); if (!changed) { return currentState; } MetaData.Builder metaData = MetaData.builder(currentState.metaData()) .persistentSettings(persistentSettings.build()) .transientSettings(transientSettings.build()); ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks()); boolean updatedReadOnly = MetaData.SETTING_READ_ONLY_SETTING.get(metaData.persistentSettings()) || MetaData.SETTING_READ_ONLY_SETTING.get(metaData.transientSettings()); if (updatedReadOnly) { blocks.addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK); } else { blocks.removeGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK); } ClusterState build = builder(currentState).metaData(metaData).blocks(blocks).build(); Settings settings = build.metaData().settings(); // now we try to apply things and if they are invalid we fail // this dryRun will validate & parse settings but won't actually apply them. clusterSettings.validateUpdate(settings); return build; }	i think this was more readable the old way.
public static GeoDistanceSortBuilder fromXContent(QueryParseContext context, String elementName) throws IOException { XContentParser parser = context.parser(); ParseFieldMatcher parseFieldMatcher = context.getParseFieldMatcher(); String fieldName = null; List<GeoPoint> geoPoints = new ArrayList<>(); DistanceUnit unit = DistanceUnit.DEFAULT; GeoDistance geoDistance = GeoDistance.DEFAULT; SortOrder order = SortOrder.ASC; SortMode sortMode = null; Optional<QueryBuilder> nestedFilter = Optional.empty(); String nestedPath = null; boolean coerce = GeoValidationMethod.DEFAULT_LENIENT_PARSING; boolean ignoreMalformed = GeoValidationMethod.DEFAULT_LENIENT_PARSING; GeoValidationMethod validation = null; XContentParser.Token token; String currentName = parser.currentName(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentName = parser.currentName(); } else if (token == XContentParser.Token.START_ARRAY) { parseGeoPoints(parser, geoPoints); fieldName = currentName; } else if (token == XContentParser.Token.START_OBJECT) { if (parseFieldMatcher.match(currentName, NESTED_FILTER_FIELD)) { nestedFilter = context.parseInnerQueryBuilder(); } else { // the json in the format of -> field : { lat : 30, lon : 12 } if (fieldName != null && fieldName.equals(currentName) == false) { throw new ParsingException( parser.getTokenLocation(), "Trying to reset fieldName to [{}], already set to [{}].", currentName, fieldName); } fieldName = currentName; GeoPoint point = new GeoPoint(); GeoUtils.parseGeoPoint(parser, point); geoPoints.add(point); } } else if (token.isValue()) { if (parseFieldMatcher.match(currentName, ORDER_FIELD)) { order = SortOrder.fromString(parser.text()); } else if (parseFieldMatcher.match(currentName, UNIT_FIELD)) { unit = DistanceUnit.fromString(parser.text()); } else if (parseFieldMatcher.match(currentName, DISTANCE_TYPE_FIELD)) { geoDistance = GeoDistance.fromString(parser.text()); } else if (parseFieldMatcher.match(currentName, COERCE_FIELD)) { coerce = parser.booleanValue(); if (!coerce) { ignoreMalformed = true; } } else if (parseFieldMatcher.match(currentName, IGNORE_MALFORMED_FIELD)) { boolean ignore_malformed_value = parser.booleanValue(); if (coerce == false) { ignoreMalformed = ignore_malformed_value; } } else if (parseFieldMatcher.match(currentName, VALIDATION_METHOD_FIELD)) { validation = GeoValidationMethod.fromString(parser.text()); } else if (parseFieldMatcher.match(currentName, SORTMODE_FIELD)) { sortMode = SortMode.fromString(parser.text()); } else if (parseFieldMatcher.match(currentName, NESTED_PATH_FIELD)) { nestedPath = parser.text(); } else if (token == Token.VALUE_STRING){ if (fieldName != null && fieldName.equals(currentName) == false) { throw new ParsingException( parser.getTokenLocation(), "Trying to reset fieldName to [{}], already set to [{}].", currentName, fieldName); } GeoPoint point = new GeoPoint(); point.resetFromString(parser.text()); geoPoints.add(point); fieldName = currentName; } else { throw new ParsingException( parser.getTokenLocation(), "Only geohashes of type string supported for field [{}]", currentName); } } } GeoDistanceSortBuilder result = new GeoDistanceSortBuilder(fieldName, geoPoints.toArray(new GeoPoint[geoPoints.size()])); result.geoDistance(geoDistance); result.unit(unit); result.order(order); if (sortMode != null) { result.sortMode(sortMode); } nestedFilter.ifPresent(result::setNestedFilter); result.setNestedPath(nestedPath); if (validation == null) { // looks like either validation was left unset or we are parsing old validation json result.validation(GeoValidationMethod.infer(coerce, ignoreMalformed)); } else { // ignore deprecated coerce/ignore_malformed result.validation(validation); } return result; }	this one is changed.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeNamedWriteable(type); out.writeString(name); out.writeMap(phases, StreamOutput::writeString, (o, val) -> val.writeTo(o)); if (this.metadata == null) { out.writeBoolean(false); } else { out.writeBoolean(true); out.writeMap(this.metadata); } } /** * @return the name of this {@link LifecyclePolicy}	same here about version control, something like: java if (out.getversion().onorafter(version.v8_0_0)) { out.writemap(this.metadata); }
void validateCacheSettings(Settings settings) { ContextSettings contextSettings = new ContextSettings(settings, contexts.keySet()); if (contextSettings.useContextSet) { deprecationLogger.warn(DeprecationCategory.SCRIPTING, "scripting-context-cache", USE_CONTEXT_RATE_KEY_DEPRECATION_MESSAGE); } else if (contextSettings.hasContextSettings()) { deprecationLogger.warn(DeprecationCategory.SCRIPTING, "scripting-context-cache", contextSettings.deprecationMessage()); } if (contextSettings.incompatibleSettings()) { throw new IllegalArgumentException(contextSettings.incompatibleSettingsMessage()); } if (SCRIPT_DISABLE_MAX_COMPILATIONS_RATE_SETTING.get(settings)) { if (contextSettings.compilationContexts.size() > 0) { throw new IllegalArgumentException( "Cannot set custom context compilation rates [" + String.join(", ", contextSettings.contextCompilationKeys()) + "] if compile rates disabled via [" + SCRIPT_DISABLE_MAX_COMPILATIONS_RATE_SETTING.getKey() + "]" ); } if (contextSettings.useContextSet == false && contextSettings.isGeneralCompilationRateSet) { throw new IllegalArgumentException( "Cannot set custom general compilation rates [" + SCRIPT_GENERAL_MAX_COMPILATIONS_RATE_SETTING.getKey() + "] to [" + SCRIPT_GENERAL_MAX_COMPILATIONS_RATE_SETTING.get(settings) + "] if compile rates disabled via [" + SCRIPT_DISABLE_MAX_COMPILATIONS_RATE_SETTING.getKey() + "]" ); } } } /** * Collect settings related to script context and general caches. * * The general cache is used by default. * The context cache is used if {@code script.max_compilations_rate} is {@code "use-context"}, a deprecated value. * The context cache is used implicitly if {@code script.max_compilations_rate} is unset and any of the context * cache family of settings is used: * {@code script.context.*.max_compilations_rate}, {@link ScriptService#SCRIPT_MAX_COMPILATIONS_RATE_SETTING} * {@code script.context.*.cache_max_size}, {@link ScriptService#SCRIPT_CACHE_SIZE_SETTING} * {@code script.context.*.cache_expire}, {@link ScriptService#SCRIPT_CACHE_EXPIRE_SETTING}	@stu-elastic you mentioned this setting is only used as part of testing. if a user were to set this are there any other problems that may occur?
private Map<String, Object> rewriteMetadataIfNecessary(Version version, Authentication authentication) { Map<String, Object> metadata = authentication.getMetadata(); if (authentication.getAuthenticationType() == AuthenticationType.API_KEY && authentication.getVersion().onOrAfter(Version.V_7_9_0) && version.before(Version.V_7_9_0)) { metadata = new HashMap<>(metadata); metadata.put( API_KEY_ROLE_DESCRIPTORS_KEY, XContentHelper.convertToMap((BytesReference) metadata.get(API_KEY_ROLE_DESCRIPTORS_KEY), false).v2()); metadata.put( API_KEY_LIMITED_ROLE_DESCRIPTORS_KEY, XContentHelper.convertToMap((BytesReference) metadata.get(API_KEY_LIMITED_ROLE_DESCRIPTORS_KEY), false).v2()); } return metadata; }	we should use the version of converttomap that takes an explicit xcontenttype.json
public BytesReference getRoleDescriptorsBytesForApiKey(Authentication authentication, boolean limitedBy) { if (authentication.getAuthenticationType() != AuthenticationType.API_KEY) { throw new IllegalStateException("authentication type must be api key but is " + authentication.getAuthenticationType()); } final Map<String, Object> metadata = authentication.getMetadata(); return (BytesReference) metadata.get(limitedBy ? API_KEY_LIMITED_ROLE_DESCRIPTORS_KEY : API_KEY_ROLE_DESCRIPTORS_KEY); }	i think having getroleforapikey, getroledescriptorsbytesforapikey and getroledescriptorsforapikey is confusing and likely to get us into trouble in the future. is it necessary for the _caller_ to have to know which method to use depending on the version of the authentication object it has? i worry about the fragility of code that calls getroledescriptorsforapikey and works on 7.9+ but then fails in a mixed cluster. would it be possible to merge getroledescriptorsforapikey and getroleforapikey into a single method, or at least make it save to call getroledescriptorsforapikey on an old style authentication object?
private void getOrBuildRoleForApiKey(Authentication authentication, boolean limitedBy, ActionListener<Role> roleActionListener) { final BytesReference roleDescriptorsBytes = apiKeyService.getRoleDescriptorsBytesForApiKey(authentication, limitedBy); final String roleDescriptorsHash = new String(Hasher.SHA256.hash(new SecureString(roleDescriptorsBytes.toString().toCharArray()))); final RoleKey roleKey = new RoleKey(Set.of(roleDescriptorsHash), limitedBy ? "limited_role_desc" : "role_desc"); final Role existing = roleCache.get(roleKey); if (existing == null) { final long invalidationCounter = numInvalidation.get(); final List<RoleDescriptor> roleDescriptors = apiKeyService.getRoleDescriptorsForApiKey(authentication, limitedBy); buildThenMaybeCacheRole(roleKey, roleDescriptors, Collections.emptySet(), true, invalidationCounter, roleActionListener); } else { roleActionListener.onResponse(existing); } }	bytesreference.tostring() is not guaranteed to be what you want here. you want either utf8tostring or just use the bytes directly. i'm not sure whether we really even need to hash these - we can, but it might not be necessary. i think the options are either: 1. don't bother hashing, but that will mean using a separate cache for api key roles (because the cache key is different) suggestion final byteskey cachekey = new byteskey(bytesreference.tobytes(roledescriptorsbytes)); 2. hash using messagedigest suggestion messagedigest digest = messagedigests.sha256(); digest.update(bytesreference.tobytes(roledescriptorsbytes)); final string roledescriptorshash = messagedigests.tohexstring(digest.digest());
private void getOrBuildRoleForApiKey(Authentication authentication, boolean limitedBy, ActionListener<Role> roleActionListener) { final BytesReference roleDescriptorsBytes = apiKeyService.getRoleDescriptorsBytesForApiKey(authentication, limitedBy); final String roleDescriptorsHash = new String(Hasher.SHA256.hash(new SecureString(roleDescriptorsBytes.toString().toCharArray()))); final RoleKey roleKey = new RoleKey(Set.of(roleDescriptorsHash), limitedBy ? "limited_role_desc" : "role_desc"); final Role existing = roleCache.get(roleKey); if (existing == null) { final long invalidationCounter = numInvalidation.get(); final List<RoleDescriptor> roleDescriptors = apiKeyService.getRoleDescriptorsForApiKey(authentication, limitedBy); buildThenMaybeCacheRole(roleKey, roleDescriptors, Collections.emptySet(), true, invalidationCounter, roleActionListener); } else { roleActionListener.onResponse(existing); } }	i think it's best if the cache explicitly states that it is an api key. suggestion final rolekey rolekey = new rolekey(set.of("apikey:" + roledescriptorshash), limitedby ? "apikey_limited_role" : "apikey_role");
public void testValueSHA256Digest() throws Exception { final KeyStoreWrapper keystore = KeyStoreWrapper.create(); final String stringSettingKeyName = randomAlphaOfLength(5).toLowerCase(Locale.ROOT) + "1"; final String stringSettingValue = randomAlphaOfLength(32); keystore.setString(stringSettingKeyName, stringSettingValue.toCharArray()); final String fileSettingKeyName = randomAlphaOfLength(5).toLowerCase(Locale.ROOT) + "2"; final byte[] fileSettingValue = randomByteArrayOfLength(32); keystore.setFile(fileSettingKeyName, fileSettingValue); final byte[] stringSettingHash = MessageDigest.getInstance("SHA-256").digest(stringSettingValue.getBytes(StandardCharsets.UTF_8)); assertTrue(Arrays.equals(keystore.getSHA256Digest(stringSettingKeyName), stringSettingHash)); final byte[] fileSettingHash = MessageDigest.getInstance("SHA-256").digest(fileSettingValue); assertTrue(Arrays.equals(keystore.getSHA256Digest(fileSettingKeyName), fileSettingHash)); keystore.close(); // value hashes accessible even when the keystore is closed assertTrue(Arrays.equals(keystore.getSHA256Digest(stringSettingKeyName), stringSettingHash)); assertTrue(Arrays.equals(keystore.getSHA256Digest(fileSettingKeyName), fileSettingHash)); }	using assertthat(keystore.getsha256digest(stringsettingkeyname), equalto(stringsettinghash)) should work, and will result in descriptive error messages when there is a difference. i think we should use that throughout these tests where arrays are compared.
private void registerSetting(Setting<?> setting) { if (setting.isFiltered()) { if (settingsFilterPattern.contains(setting.getKey()) == false) { registerSettingsFilter(setting.getKey()); } } if (setting.hasNodeScope() || setting.hasIndexScope()) { if (setting.hasNodeScope()) { Setting<?> existingSetting = nodeSettings.get(setting.getKey()); if (existingSetting != null) { throw new IllegalArgumentException("Cannot register setting [" + setting.getKey() + "] twice"); } if (setting.isConsistent()) { if (setting instanceof Setting.AffixSetting<?>) { if (((Setting.AffixSetting<?>)setting).getConcreteSettingForNamespace("_na_") instanceof SecureSetting<?>) { consistentSettings.add(setting); } else { throw new IllegalArgumentException("Invalid consistent secure setting [" + setting.getKey() + "]"); } } else if (setting instanceof SecureSetting<?>) { consistentSettings.add(setting); } else { throw new IllegalArgumentException("Invalid consistent secure setting [" + setting.getKey() + "]"); } } nodeSettings.put(setting.getKey(), setting); } if (setting.hasIndexScope()) { Setting<?> existingSetting = indexSettings.get(setting.getKey()); if (existingSetting != null) { throw new IllegalArgumentException("Cannot register setting [" + setting.getKey() + "] twice"); } if (setting.isConsistent()) { throw new IllegalStateException("Consistent setting [" + setting.getKey() + "] cannot be index scoped"); } indexSettings.put(setting.getKey(), setting); } } else { throw new IllegalArgumentException("No scope found for setting [" + setting.getKey() + "]"); } }	if we are only going to support this setting property for secure settings, we should clarify that in the javadocs. but why couldn't this work for normal settings as well?
public void testFilterBySLMPolicy() throws Exception { final String repoName = "test-repo"; AbstractSnapshotIntegTestCase.createRepository(logger, repoName, "fs"); AbstractSnapshotIntegTestCase.createNSnapshots(logger, repoName, randomIntBetween(1, 5)); final List<SnapshotInfo> snapshotsWithoutPolicy = clusterAdmin().prepareGetSnapshots("*").setSnapshots("*") .setSort(GetSnapshotsRequest.SortBy.NAME).get().getSnapshots(); final String snapshotWithPolicy = "snapshot-with-policy"; final String policyName = "some-policy"; final SnapshotInfo withPolicy = AbstractSnapshotIntegTestCase.assertSuccessful( logger, clusterAdmin().prepareCreateSnapshot(repoName, snapshotWithPolicy) .setUserMetadata(Map.of(SnapshotsService.POLICY_ID_METADATA_FIELD, policyName)) .setWaitForCompletion(true) .execute() ); assertThat(getAllSnapshotsForPolicies(policyName), is(List.of(withPolicy))); assertThat(getAllSnapshotsForPolicies("some-*"), is(List.of(withPolicy))); assertThat(getAllSnapshotsForPolicies("*", "-" + policyName), empty()); assertThat(getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN), is(snapshotsWithoutPolicy)); assertThat( getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN, "-" + policyName), is(snapshotsWithoutPolicy)); assertThat(getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN), is(snapshotsWithoutPolicy)); assertThat(getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN, "-*"), is(snapshotsWithoutPolicy)); assertThat(getAllSnapshotsForPolicies("no-such-policy"), empty()); assertThat(getAllSnapshotsForPolicies("no-such-policy*"), empty()); final String snapshotWithOtherPolicy = "snapshot-with-other-policy"; final String otherPolicyName = "other-policy"; final SnapshotInfo withOtherPolicy = AbstractSnapshotIntegTestCase.assertSuccessful( logger, clusterAdmin().prepareCreateSnapshot(repoName, snapshotWithOtherPolicy) .setUserMetadata(Map.of(SnapshotsService.POLICY_ID_METADATA_FIELD, otherPolicyName)) .setWaitForCompletion(true) .execute() ); assertThat(getAllSnapshotsForPolicies(policyName, otherPolicyName), is(List.of(withOtherPolicy, withPolicy))); assertThat(getAllSnapshotsForPolicies(policyName, otherPolicyName, "no-such-policy*"), is(List.of(withOtherPolicy, withPolicy))); final List<SnapshotInfo> allSnapshots = clusterAdmin().prepareGetSnapshots("*") .setSnapshots("*") .setSort(GetSnapshotsRequest.SortBy.NAME) .get() .getSnapshots(); assertThat( getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN, policyName, otherPolicyName), is(allSnapshots) ); assertThat( getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN, "*"), is(allSnapshots) ); }	i think just * case is missing? assertthat(getallsnapshotsforpolicies("*"), is(list.of(withotherpolicy, withpolicy)));
public void testFilterBySLMPolicy() throws Exception { final String repoName = "test-repo"; createRepository(repoName, "fs"); createNSnapshots(repoName, randomIntBetween(1, 5)); final List<SnapshotInfo> snapshotsWithoutPolicy = clusterAdmin().prepareGetSnapshots("*") .setSnapshots("*") .setSort(GetSnapshotsRequest.SortBy.NAME) .get() .getSnapshots(); final String snapshotWithPolicy = "snapshot-with-policy"; final String policyName = "some-policy"; final SnapshotInfo withPolicy = assertSuccessful( clusterAdmin().prepareCreateSnapshot(repoName, snapshotWithPolicy) .setUserMetadata(Map.of(SnapshotsService.POLICY_ID_METADATA_FIELD, policyName)) .setWaitForCompletion(true) .execute() ); assertThat(getAllSnapshotsForPolicies(policyName), is(List.of(withPolicy))); assertThat(getAllSnapshotsForPolicies("some-*"), is(List.of(withPolicy))); assertThat(getAllSnapshotsForPolicies("*", "-" + policyName), empty()); assertThat(getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN), is(snapshotsWithoutPolicy)); assertThat(getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN, "-" + policyName), is(snapshotsWithoutPolicy)); assertThat(getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN), is(snapshotsWithoutPolicy)); assertThat(getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN, "-*"), is(snapshotsWithoutPolicy)); assertThat(getAllSnapshotsForPolicies("no-such-policy"), empty()); assertThat(getAllSnapshotsForPolicies("no-such-policy*"), empty()); final String snapshotWithOtherPolicy = "snapshot-with-other-policy"; final String otherPolicyName = "other-policy"; final SnapshotInfo withOtherPolicy = assertSuccessful( clusterAdmin().prepareCreateSnapshot(repoName, snapshotWithOtherPolicy) .setUserMetadata(Map.of(SnapshotsService.POLICY_ID_METADATA_FIELD, otherPolicyName)) .setWaitForCompletion(true) .execute() ); assertThat(getAllSnapshotsForPolicies(policyName, otherPolicyName), is(List.of(withOtherPolicy, withPolicy))); assertThat(getAllSnapshotsForPolicies(policyName, otherPolicyName, "no-such-policy*"), is(List.of(withOtherPolicy, withPolicy))); final List<SnapshotInfo> allSnapshots = clusterAdmin().prepareGetSnapshots("*") .setSnapshots("*") .setSort(GetSnapshotsRequest.SortBy.NAME) .get() .getSnapshots(); assertThat( getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN, policyName, otherPolicyName), is(allSnapshots) ); assertThat(getAllSnapshotsForPolicies(TransportGetSnapshotsAction.NO_POLICY_PATTERN, "*"), is(allSnapshots)); }	also add just * case here: assertthat(getallsnapshotsforpolicies("*"), is(list.of(withotherpolicy, withpolicy)));
@Override public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (repositories == null || repositories.length == 0) { validationException = addValidationError("repositories are missing", validationException); } if (size == 0 || size < NO_LIMIT) { validationException = addValidationError("size must be -1 or greater than 0", validationException); } if (verbose == false) { if (sort != SortBy.START_TIME) { validationException = addValidationError("can't use non-default sort with verbose=false", validationException); } if (size > 0) { validationException = addValidationError("can't use size limit with verbose=false", validationException); } if (offset > 0) { validationException = addValidationError("can't use offset with verbose=false", validationException); } if (after != null) { validationException = addValidationError("can't use after with verbose=false", validationException); } if (order != SortOrder.ASC) { validationException = addValidationError("can't use non-default sort order with verbose=false", validationException); } if (policies.length != 0) { validationException = addValidationError("can't use slm policy filter with verbose=false", validationException); } } else if (after != null && offset > 0) { validationException = addValidationError("can't use after and offset simultaneously", validationException); } return validationException; }	can we add a generic validation to not use -_none and perhaps even not allow _ or -_ prefixed policies?
public synchronized IndexService createIndex(String sIndexName, @IndexSettings Settings settings, String localNodeId) { if (!lifecycle.started()) { throw new IllegalStateException("Can't create an index [" + sIndexName + "], node is closed"); } Index index = new Index(sIndexName); if (indices.containsKey(index.name())) { throw new IndexAlreadyExistsException(index); } indicesLifecycle.beforeIndexCreated(index, settings); logger.debug("creating Index [{}], shards [{}]/[{}{}]", sIndexName, settings.get(SETTING_NUMBER_OF_SHARDS), settings.get(SETTING_NUMBER_OF_REPLICAS), IndexMetaData.isIndexUsingShadowReplicas(settings) ? "s" : ""); Settings indexSettings = settingsBuilder() .put(this.settings) .put(settings) .build(); ModulesBuilder modules = new ModulesBuilder(); modules.add(new IndexNameModule(index)); modules.add(new LocalNodeIdModule(localNodeId)); modules.add(new IndexSettingsModule(index, indexSettings)); modules.add(new IndexStoreModule(indexSettings)); modules.add(new AnalysisModule(indexSettings, indicesAnalysisService)); modules.add(new SimilarityModule(indexSettings)); modules.add(new IndexCacheModule(indexSettings)); modules.add(new IndexFieldDataModule(indexSettings)); modules.add(new MapperServiceModule()); modules.add(new IndexAliasesServiceModule()); modules.add(new IndexModule(indexSettings)); for (Module pluginModule : pluginsService.indexModules(indexSettings)) { modules.add(pluginModule); } pluginsService.processModules(modules); Injector indexInjector; try { indexInjector = modules.createChildInjector(injector); } catch (CreationException e) { throw new IndexCreationException(index, Injectors.getFirstErrorFailure(e)); } catch (Throwable e) { throw new IndexCreationException(index, e); } IndexService indexService = indexInjector.getInstance(IndexService.class); indicesLifecycle.afterIndexCreated(indexService); indices = newMapBuilder(indices).put(index.name(), new Tuple<>(indexService, indexInjector)).immutableMap(); return indexService; }	is this actually used anywhere? i mean why would you add your own index modules? i really wonder if we need this kind of extension mechanism? i know it's unrelated but maybe we need a followup issue?
static Request setUpgradeMode(SetUpgradeModeRequest setUpgradeModeRequest) { String endpoint = new EndpointBuilder().addPathPartAsIs("_ml", "set_upgrade_mode").build(); Request request = new Request(HttpPost.METHOD_NAME, endpoint); RequestConverters.Params params = new RequestConverters.Params(request); params.putParam(SetUpgradeModeRequest.ENABLED.getPreferredName(), Boolean.toString(setUpgradeModeRequest.isEnabled())); if (setUpgradeModeRequest.getTimeout() != null) { params.putParam(SetUpgradeModeRequest.TIMEOUT.getPreferredName(), setUpgradeModeRequest.getTimeout().toString()); } return request; }	remember to change _ml to _xpack/ml when backporting to 6.7. the same applies in several places in this pr, including in the docs.
private SingleForecast forecast(IndexAbstraction.DataStream stream, long forecastWindow, long now) { List<IndexMetadata> indices = stream.getIndices(); if (dataStreamAllocatedToNodes(indices) == false) return null; long minCreationDate = Long.MAX_VALUE; long totalSize = 0; int count = 0; while (count < indices.size()) { ++count; IndexMetadata indexMetadata = indices.get(indices.size() - count); long creationDate = indexMetadata.getCreationDate(); if (creationDate < 0) { return null; } minCreationDate = Math.min(minCreationDate, creationDate); totalSize += state.getRoutingTable().allShards(indexMetadata.getIndex().getName()).stream().mapToLong(this::sizeOf).sum(); // we terminate loop after collecting data to ensure we consider at least the forecast window (and likely some more). if (creationDate <= now - forecastWindow) { break; } } if (totalSize == 0) { return null; } // round up long avgSizeCeil = (totalSize - 1) / count + 1; long actualWindow = now - minCreationDate; if (actualWindow == 0) { return null; } // rather than simulate rollover, we copy the index meta data and do minimal adjustments. long scaledTotalSize; int numberNewIndices; if (actualWindow > forecastWindow) { scaledTotalSize = BigInteger.valueOf(totalSize) .multiply(BigInteger.valueOf(forecastWindow)) .divide(BigInteger.valueOf(actualWindow)) .longValueExact(); // round up numberNewIndices = (int) Math.min((scaledTotalSize - 1) / avgSizeCeil + 1, indices.size()); if (scaledTotalSize == 0) { return null; } } else { numberNewIndices = count; scaledTotalSize = totalSize; } IndexMetadata writeIndex = stream.getWriteIndex(); Map<IndexMetadata, Long> newIndices = new HashMap<>(); DataStream dataStream = stream.getDataStream(); for (int i = 0; i < numberNewIndices; ++i) { final String uuid = UUIDs.randomBase64UUID(); dataStream = dataStream.rollover(state.metadata(), uuid); // this unintentionally copies the in-sync allocation ids too. This has the fortunate effect of these indices // not being regarded new by the disk threshold decider, thereby respecting the low watermark threshold even for primaries. // This is highly desirable so fixing this to clear the in-sync allocation ids will require a more elaborate solution, // ensuring at least that when replicas are involved, we still respect the low watermark. This is therefore left as is // for now with the intention to fix in a follow-up. IndexMetadata newIndex = IndexMetadata.builder(writeIndex) .index(dataStream.getWriteIndex().getName()) .settings(Settings.builder().put(writeIndex.getSettings()).put(IndexMetadata.SETTING_INDEX_UUID, uuid)) .build(); long size = Math.min(avgSizeCeil, scaledTotalSize - (avgSizeCeil * i)); assert size > 0; newIndices.put(newIndex, size); } return new SingleForecast(newIndices, dataStream); }	a [prototype](https://github.com/elastic/elasticsearch/compare/master...henningandersen:fix_autoscaling_proactive_replicas?expand=1) for a fix has been made, but since the original version already triggers at low watermark, adding it will mostly be a refinement. i will follow up on that subsequently.
public void testWarningHeader() throws Exception { Request request = new Request("GET", "/_security/user"); RequestOptions.Builder options = request.getOptions().toBuilder(); options.addHeader("Authorization", basicAuthHeaderValue(SecuritySettingsSource.TEST_USER_NAME, new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()))); request.setOptions(options); Response response = getRestClient().performRequest(request); List<String> beforeWarningHeaders = getWarningHeaders(response.getHeaders()); assertTrue(beforeWarningHeaders.isEmpty()); License.OperationMode mode = randomFrom(License.OperationMode.GOLD, License.OperationMode.PLATINUM, License.OperationMode.ENTERPRISE, License.OperationMode.STANDARD); long now = System.currentTimeMillis(); long newExpirationDate = now + LICENSE_EXPIRATION_WARNING_PERIOD.getMillis() - 1; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); List<String> afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertTrue(afterWarningHeaders.size() == 1); assertTrue(afterWarningHeaders.stream().anyMatch(v ->v.contains("Your license will expire in [6] days. " + "Contact your administrator or update your license for continued use of features"))); newExpirationDate = now + 300000; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertTrue(afterWarningHeaders.size() == 1); assertTrue(afterWarningHeaders.stream().anyMatch(v ->v.contains("Your license expires today. " + "Contact your administrator or update your license for continued use of features"))); newExpirationDate = now - 300000; setLicensingExpirationDate(mode, newExpirationDate); response = getRestClient().performRequest(request); afterWarningHeaders= getWarningHeaders(response.getHeaders()); assertTrue(afterWarningHeaders.size() == 1); long finalNewExpirationDate = newExpirationDate; String expiredMessage = String.format(Locale.ROOT, "Your license expired on [%s]. ", LicenseService.DATE_FORMATTER.formatMillis(finalNewExpirationDate)); assertTrue(afterWarningHeaders.stream().anyMatch(v ->v.contains(expiredMessage + "Contact your administrator or update your license for continued use of features"))); }	in general we prefer to use matcher based assertions because they provide better context when something fails. suggestion assertthat(afterwarningheaders, matchers.hassize(1)); assertthat(afterwarningheaders, matchers.contains("your license will expire in [6] days. " + "contact your administrator or update your license for continued use of features")); (although, technically, the first assertion is redundant, because the contains matcher already checks size)
default Map<String, Repository.Factory> getRepositories(Environment env, NamedXContentRegistry namedXContentRegistry, ThreadPool threadPool) { return Collections.emptyMap(); } /** * Returns internal repository types added by this plugin. Internal repositories cannot be registered * through the external API. * * @param env The environment for the local node, which may be used for the local settings and path.repo * * The key of the returned {@link Map} is the type name of the repository and * the value is a factory to construct the {@link Repository}	can you also make the same change to getinternalrepositories and then remove the setonce<threadpool> threadpool in the ccr class that implements repositoryplugin
@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, ActionListener<Void> listener) { if (isReadOnly()) { listener.onFailure(new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository")); } else { SnapshotInfo snapshot = null; try { snapshot = getSnapshotInfo(snapshotId); } catch (SnapshotMissingException ex) { listener.onFailure(ex); return; } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) { logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex); } // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots final RepositoryData repositoryData; final RepositoryData updatedRepositoryData; try { repositoryData = getRepositoryData(); updatedRepositoryData = repositoryData.removeSnapshot(snapshotId); writeIndexGen(updatedRepositoryData, repositoryStateId); } catch (Exception ex) { listener.onFailure(new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex)); return; } deleteSnapshotBlobs(snapshot, snapshotId, repositoryData, updatedRepositoryData, listener); } }	can we do the chaining of the individual substeps here? i think it makes the flow clearer instead of jumping from one method to the next
@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, ActionListener<Void> listener) { if (isReadOnly()) { listener.onFailure(new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository")); } else { SnapshotInfo snapshot = null; try { snapshot = getSnapshotInfo(snapshotId); } catch (SnapshotMissingException ex) { listener.onFailure(ex); return; } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) { logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex); } // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots final RepositoryData repositoryData; final RepositoryData updatedRepositoryData; try { repositoryData = getRepositoryData(); updatedRepositoryData = repositoryData.removeSnapshot(snapshotId); writeIndexGen(updatedRepositoryData, repositoryStateId); } catch (Exception ex) { listener.onFailure(new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex)); return; } deleteSnapshotBlobs(snapshot, snapshotId, repositoryData, updatedRepositoryData, listener); } }	use actionrunnable, which subclasses abstractrunnable as deletesnapshotblobignoringerrors will only ignore ioexception, not all other exceptions. we should make sure to always calls the listener.
public static void main(final String[] args) throws InterruptedException, IOException { if (args.length != 1) { throw new IllegalArgumentException("expected one argument specifying path to jvm.options but was " + Arrays.toString(args)); } final List<String> jvmOptions = new ArrayList<>(); final SortedMap<Integer, String> invalidLines = new TreeMap<>(); try (InputStream is = Files.newInputStream(Paths.get(args[0])); Reader reader = new InputStreamReader(is, StandardCharsets.UTF_8); BufferedReader br = new BufferedReader(reader)) { parse( JavaVersion.majorVersion(JavaVersion.CURRENT), br, new JvmOptionConsumer() { @Override public void accept(final String jvmOption) { jvmOptions.add(jvmOption); } }, new InvalidLineConsumer() { @Override public void accept(final int lineNumber, final String line) { invalidLines.put(lineNumber, line); } }); } if (invalidLines.isEmpty()) { // now append the JVM options from ES_JAVA_OPTS final String environmentJvmOptions = System.getenv("ES_JAVA_OPTS"); if (environmentJvmOptions != null) { jvmOptions.addAll(Arrays.stream(environmentJvmOptions.split("\\\\\\\\s+")) .filter(Predicate.not(String::isBlank)) .collect(Collectors.toUnmodifiableList())); } final List<String> substitutedJvmOptions = substitutePlaceholders(jvmOptions, Map.of("ES_TMPDIR", System.getenv("ES_TMPDIR"))); final List<String> ergonomicJvmOptions = JvmErgonomics.choose(substitutedJvmOptions); final List<String> finalJvmOptions = Stream.concat( SystemJvmOptions.systemJvmOptions().stream(), Stream.concat(substitutedJvmOptions.stream(), ergonomicJvmOptions.stream())) .collect(Collectors.toList()); final String spaceDelimitedJvmOptions = spaceDelimitJvmOptions(finalJvmOptions); Launchers.outPrintln(spaceDelimitedJvmOptions); Launchers.exit(0); } else { final String errorMessage = String.format( Locale.ROOT, "encountered [%d] error%s parsing [%s]", invalidLines.size(), invalidLines.size() == 1 ? "" : "s", args[0]); Launchers.errPrintln(errorMessage); int count = 0; for (final Map.Entry<Integer, String> entry : invalidLines.entrySet()) { count++; final String message = String.format( Locale.ROOT, "[%d]: encountered improperly formatted JVM option line [%s] on line number [%d]", count, entry.getValue(), entry.getKey()); Launchers.errPrintln(message); } Launchers.exit(1); } }	i wondered if this would be any better using a plain old list: java final list<string> finaljvmoptions = new arraylist<>(systemjvmoptions.systemjvmoptions()); finaljvmoptions.addall(substitutedjvmoptions); finaljvmoptions.addall(ergonomicjvmoptions);
private static CacheKey buildCacheKey(Document document) { return new CacheKey( getValue(document, SNAPSHOT_ID_FIELD), getValue(document, INDEX_NAME_FIELD), new ShardId( new Index(getValue(document, SHARD_INDEX_NAME_FIELD), getValue(document, SHARD_INDEX_ID_FIELD)), Integer.parseInt(getValue(document, SHARD_ID_FIELD)) ), getValue(document, FILE_NAME_FIELD) ); }	maybe rename this to snapshot_index_name_field?
public void execute(SearchContext context) { if (LOGGER.isTraceEnabled()) { LOGGER.trace("{}", new SearchContextSourcePrinter(context)); } if (context.docIdsToLoadSize() == 0) { // no individual hits to process, so we shortcut context.fetchResult().hits(new SearchHits(new SearchHit[0], context.queryResult().getTotalHits(), context.queryResult().getMaxScore())); return; } DocIdToIndex[] docs = new DocIdToIndex[context.docIdsToLoadSize()]; for (int index = 0; index < context.docIdsToLoadSize(); index++) { docs[index] = new DocIdToIndex(context.docIdsToLoad()[context.docIdsToLoadFrom() + index], index); } Arrays.sort(docs); Map<String, Set<String>> storedToRequestedFields = new HashMap<>(); FieldsVisitor fieldsVisitor = createStoredFieldsVisitor(context, storedToRequestedFields); FetchContext fetchContext = FetchContext.fromSearchContext(context); SearchHit[] hits = new SearchHit[context.docIdsToLoadSize()]; Map<String, Object> sharedCache = new HashMap<>(); List<FetchSubPhaseProcessor> processors = getProcessors(context.shardTarget(), fetchContext); int currentReaderIndex = -1; LeafReaderContext currentReaderContext = null; for (int index = 0; index < context.docIdsToLoadSize(); index++) { if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } int docId = docs[index].docId; try { int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves()); if (currentReaderIndex != readerIndex) { currentReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex); currentReaderIndex = readerIndex; for (FetchSubPhaseProcessor processor : processors) { processor.setNextReader(currentReaderContext); } } assert currentReaderContext != null; HitContext hit = prepareHitContext(context, fieldsVisitor, docId, storedToRequestedFields, currentReaderContext, sharedCache); for (FetchSubPhaseProcessor processor : processors) { processor.process(hit); } hits[docs[index].index] = hit.hit(); } catch (Exception e) { throw new FetchPhaseExecutionException(context.shardTarget(), "Error running fetch phase for doc [" + docId + "]", e); } } if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } TotalHits totalHits = context.queryResult().getTotalHits(); context.fetchResult().hits(new SearchHits(hits, totalHits, context.queryResult().getMaxScore())); }	we were checking in a couple of subphases for things like "how many docs are we executing against" or "is this a suggest-only response". this pulls this check up to the top level, which also means we don't need to include this information in the fetchcontext.
*/ public boolean enableUser(EnableUserRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequest(request, SecurityRequestConverters::enableUser, options, RestHighLevelClient::convertExistsResponse, emptySet()); } /** * Enable a native realm or built-in user synchronously. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html"> * the docs</a> for more. * * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT} if nothing needs to be customized * @param request the request with the user to enable * @return {@code true} if the request succeeded (the user is enabled) * @throws IOException in case there is a problem sending the request or parsing back the response * @deprecated use {@link #enableUser(EnableUserRequest, RequestOptions)}	why are we flipping the order of these parameters ? it can result in a user fixing the deprecated warning only to get a new deprecated warning. not a big deal, but kinda confusing.
*/ public boolean enableUser(EnableUserRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequest(request, SecurityRequestConverters::enableUser, options, RestHighLevelClient::convertExistsResponse, emptySet()); } /** * Enable a native realm or built-in user synchronously. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html"> * the docs</a> for more. * * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT} if nothing needs to be customized * @param request the request with the user to enable * @return {@code true} if the request succeeded (the user is enabled) * @throws IOException in case there is a problem sending the request or parsing back the response * @deprecated use {@link #enableUser(EnableUserRequest, RequestOptions)}	can you make this method delegate to the one that is not deprecated ?
*/ public boolean disableUser(DisableUserRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequest(request, SecurityRequestConverters::disableUser, options, RestHighLevelClient::convertExistsResponse, emptySet()); } /** * Disable a native realm or built-in user synchronously. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-disable-user.html"> * the docs</a> for more. * * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT} if nothing needs to be customized * @param request the request with the user to disable * @return {@code true} if the request succeeded (the user is disabled) * @throws IOException in case there is a problem sending the request or parsing back the response * @deprecated use {@link #disableUser(DisableUserRequest, RequestOptions)}	can you make this method delegate to the one that is not deprecated.
*/ public void changePasswordAsync(ChangePasswordRequest request, RequestOptions options, ActionListener<Boolean> listener) { restHighLevelClient.performRequestAsync(request, SecurityRequestConverters::changePassword, options, RestHighLevelClient::convertExistsResponse, listener, emptySet()); } /** * Change the password of a user of a native realm or built-in user asynchronously. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html"> * the docs</a> for more. * * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT} if nothing needs to be customized * @param request the request with the user's new password * @param listener the listener to be notified upon request completion * @deprecated use {@link #changePasswordAsync(ChangePasswordRequest, RequestOptions, ActionListener)}	can you have this method delegate to the non-deprecated one.
@Override protected Client buildClient(Settings headersSettings, GenericAction[] testedActions) { transportService = MockTransportService.createNewService(Settings.EMPTY, Version.CURRENT, threadPool, null); transportService.start(); transportService.acceptIncomingRequests(); String transport = randomBoolean() ? NioTransportPlugin.NIO_TRANSPORT_NAME : MockTcpTransportPlugin.MOCK_TCP_TRANSPORT_NAME; TransportClient client = new MockTransportClient(Settings.builder() .put("client.transport.sniff", false) .put("cluster.name", "cluster1") .put("node.name", "transport_client_" + this.getTestName()) .put(NetworkModule.TRANSPORT_TYPE_SETTING.getKey(), transport) .put(headersSettings) .build(), InternalTransportServiceInterceptor.TestPlugin.class); InternalTransportServiceInterceptor.TestPlugin plugin = client.injector.getInstance(PluginsService.class) .filterPlugins(InternalTransportServiceInterceptor.TestPlugin.class).stream().findFirst().get(); plugin.instance.threadPool = client.threadPool(); plugin.instance.address = transportService.boundAddress().publishAddress(); client.addTransportAddress(transportService.boundAddress().publishAddress()); return client; }	should we push this into a util method in estestcase?
private static Collection<Class<? extends Plugin>> addMockTransportIfMissing(Settings settings, Collection<Class<? extends Plugin>> plugins) { boolean settingExists = NetworkModule.TRANSPORT_TYPE_SETTING.exists(settings); String transportType = NetworkModule.TRANSPORT_TYPE_SETTING.get(settings); if (settingExists == false || MockTcpTransportPlugin.MOCK_TCP_TRANSPORT_NAME.equals(transportType)) { if (plugins.contains(MockTcpTransportPlugin.class)) { return plugins; } else { plugins = new ArrayList<>(plugins); plugins.add(MockTcpTransportPlugin.class); return plugins; } } else if (NioTransportPlugin.NIO_TRANSPORT_NAME.equals(transportType)) { if (plugins.contains(NioTransportPlugin.class)) { return plugins; } else { plugins = new ArrayList<>(plugins); plugins.add(NioTransportPlugin.class); return plugins; } } return plugins; }	should this also check if the setting exists?
public boolean checkFeature(Feature feature) { boolean allowed = isAllowed(feature); LongAccumulator maxEpochAccumulator = lastUsed.get(feature); if (maxEpochAccumulator != null) { maxEpochAccumulator.accumulate(epochMillisProvider.getAsLong()); } return allowed; } /** * Marks a licensed feature as <em>on by configuration</em>. * By default {@link #getFeatureUsage()} method returns the last time {@link #checkFeature(Feature)} was called. * One this method is called, the specified feature will be marked as "always in use",and {@link #getFeatureUsage()}	a few interesting points here: * do we need a pairing unset method? this could be useful if we have any "on-by-configuration" features that can turned on and off on the fly? or if the license expires or gets deleted or is somehow downgraded, i assume it's the caller's responsibility to call the unset method so it's removed from the tracking? or should unlicensed features be filtered out at getfeatureusage time? * if the feature is not trackable, should we throw an assert error to notify the developer that license is not configured correctly? * how can we ensure that every "on-by-default" feature (past and future) is indeed registered with this method?
public static ShardAllocationDecision fromDecision(Decision decision, @Nullable String assignedNodeId, boolean explain, @Nullable Map<String, WeightedDecision> nodeExplanations) { final Type decisionType = decision.type(); AllocationStatus allocationStatus = decisionType != Type.YES ? AllocationStatus.fromDecision(decisionType) : null; String explanation = null; if (explain) { if (decision.type() == Type.YES) { assert assignedNodeId != null; explanation = "shard assigned to node [" + assignedNodeId + "]"; } else if (decision.type() == Type.THROTTLE) { assert assignedNodeId != null; explanation = "shard assignment throttled on node [" + assignedNodeId + "]"; } else { explanation = "shard cannot be assigned to any node in the cluster"; } } return new ShardAllocationDecision(decisionType, allocationStatus, explanation, assignedNodeId, null, nodeExplanations, null); }	we can leave this here for now but i think that the explanation string should go into the explain api and not here.
@Override public <T> T compile(String scriptName, String scriptSource, ScriptContext<T> context, Map<String, String> params) { if (context.instanceClazz.equals(SearchScript.class)) { GenericElasticsearchScript painlessScript = (GenericElasticsearchScript)compile(contextsToCompilers.get(context), scriptName, scriptSource, params); SearchScript.Factory factory = (p, lookup) -> new SearchScript.LeafFactory() { @Override public SearchScript newInstance(final LeafReaderContext context) { return new ScriptImpl(painlessScript, p, lookup, context); } @Override public boolean needsScores() { return painlessScript.needs_score(); } }; return context.factoryClazz.cast(factory); } else if (context.instanceClazz.equals(ExecutableScript.class)) { GenericElasticsearchScript painlessScript = (GenericElasticsearchScript)compile(contextsToCompilers.get(context), scriptName, scriptSource, params); ExecutableScript.Factory factory = (p) -> new ScriptImpl(painlessScript, p, null, null); return context.factoryClazz.cast(factory); } else { // Check we ourselves are not being called by unprivileged code. SpecialPermission.check(); // Create our loader (which loads compiled code with no permissions). final Loader loader = AccessController.doPrivileged(new PrivilegedAction<Loader>() { @Override public Loader run() { return new Loader(getClass().getClassLoader()); } }); compile(contextsToCompilers.get(context), loader, scriptName, scriptSource, params); if (context.statefulFactoryClazz != null) { return generateFactory(loader, context, generateStatefulFactory(loader, context)); } else { return generateFactory(loader, context, WriterConstants.CLASS_TYPE); } } } /** * Generates a stateful factory class that will return script instances. Acts as a middle man between * the {@link ScriptContext#factoryClazz} and the {@link ScriptContext#instanceClazz} when used so that * the stateless factory can be used for caching and the stateful factory can act as a cache for new * script instances. Uses the newInstance method from a {@link ScriptContext#statefulFactoryClazz} to * define the factory method to create new instances of the {@link ScriptContext#instanceClazz}. * @param loader The {@link ClassLoader} that is used to define the factory class and script class. * @param context The {@link ScriptContext}	as a followup i think we should have the scriptcontext validate this method exists, so then we won't need to look it up here, but just grab the method from the context.
public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); if (indices == null) { out.writeVInt(0); } else { out.writeStringArray(indices); } out.writeTimeValue(timeout); if (waitForStatus == null) { out.writeBoolean(false); } else { out.writeBoolean(true); out.writeByte(waitForStatus.value()); } out.writeBoolean(waitForNoRelocatingShards); waitForActiveShards.writeTo(out); out.writeString(waitForNodes); if (waitForEvents == null) { out.writeBoolean(false); } else { out.writeBoolean(true); Priority.writeTo(waitForEvents, out); } out.writeBoolean(waitForNoInitializingShards); if (out.getVersion().onOrAfter(Version.V_7_2_0)) { indicesOptions.writeIndicesOptions(out); } if (out.getVersion().onOrAfter(Version.V_7_16_0)) { out.writeBoolean(return200ForClusterHealthTimeout); } else if (return200ForClusterHealthTimeout) { throw new IllegalArgumentException("Can't fix response code in a cluster involving nodes with version " + out.getVersion()); } }	we won't ever be talking to a version prior to 7.16.0 so this will always be true - you can just remove the else branch.
private static Map<String, RoleDescriptor> initializeReservedRoles() { return MapBuilder.<String, RoleDescriptor>newMapBuilder() .put("superuser", new RoleDescriptor("superuser", new String[] { "all" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("all").build()}, new String[] { "*" }, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("transport_client", new RoleDescriptor("transport_client", new String[] { "transport_client" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_user", new RoleDescriptor("kibana_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("manage", "read", "index", "delete") .build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("all").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("monitoring_user", new RoleDescriptor("monitoring_user", new String[] { "cluster:monitor/main" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_agent", new RoleDescriptor("remote_monitoring_agent", new String[] { "manage_index_templates", "manage_ingest_pipelines", "monitor", "cluster:monitor/xpack/watcher/watch/get", "cluster:admin/xpack/watcher/watch/put", "cluster:admin/xpack/watcher/watch/delete", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("stack_monitoring_agent", new RoleDescriptor( "stack_monitoring_agent", new String[] { "monitor", "manage_index_templates" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("monitor").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("read").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*", "metricbeat-*").privileges("index", "create_index").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null )) .put("ingest_admin", new RoleDescriptor("ingest_admin", new String[] { "manage_index_templates", "manage_pipeline" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) // reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role .put("reporting_user", new RoleDescriptor("reporting_user", null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_dashboard_only_user", new RoleDescriptor( "kibana_dashboard_only_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*").privileges("read", "view_index_metadata").build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put(KibanaUser.ROLE_NAME, new RoleDescriptor(KibanaUser.ROLE_NAME, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*", ".reporting-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build() }, null, new ConditionalClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("logstash_system", new RoleDescriptor("logstash_system", new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("beats_admin", new RoleDescriptor("beats_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.BEATS_ROLE, new RoleDescriptor(UsernamesField.BEATS_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.APM_ROLE, new RoleDescriptor(UsernamesField.APM_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_user", new RoleDescriptor("machine_learning_user", new String[] { "monitor_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-notifications").privileges("view_index_metadata", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_admin", new RoleDescriptor("machine_learning_admin", new String[] { "manage_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-*").privileges("view_index_metadata", "read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_admin", new RoleDescriptor("watcher_admin", new String[] { "manage_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX, TriggeredWatchStoreField.INDEX_NAME, HistoryStoreField.INDEX_PREFIX + "*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_user", new RoleDescriptor("watcher_user", new String[] { "monitor_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX) .privileges("read") .build(), RoleDescriptor.IndicesPrivileges.builder().indices(HistoryStoreField.INDEX_PREFIX + "*") .privileges("read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("logstash_admin", new RoleDescriptor("logstash_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".logstash*") .privileges("create", "delete", "index", "manage", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_user", new RoleDescriptor("rollup_user", new String[] { "monitor_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_admin", new RoleDescriptor("rollup_admin", new String[] { "manage_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .immutableMap(); }	@ycombinator can you please explain why manage_index_templates is required ? isn't .indices(".monitoring-*", "metricbeat-*").privileges("index", "create_index") enough?
private static Map<String, RoleDescriptor> initializeReservedRoles() { return MapBuilder.<String, RoleDescriptor>newMapBuilder() .put("superuser", new RoleDescriptor("superuser", new String[] { "all" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("all").build()}, new String[] { "*" }, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("transport_client", new RoleDescriptor("transport_client", new String[] { "transport_client" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_user", new RoleDescriptor("kibana_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("manage", "read", "index", "delete") .build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("all").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("monitoring_user", new RoleDescriptor("monitoring_user", new String[] { "cluster:monitor/main" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_agent", new RoleDescriptor("remote_monitoring_agent", new String[] { "manage_index_templates", "manage_ingest_pipelines", "monitor", "cluster:monitor/xpack/watcher/watch/get", "cluster:admin/xpack/watcher/watch/put", "cluster:admin/xpack/watcher/watch/delete", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("stack_monitoring_agent", new RoleDescriptor( "stack_monitoring_agent", new String[] { "monitor", "manage_index_templates" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("monitor").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("read").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*", "metricbeat-*").privileges("index", "create_index").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null )) .put("ingest_admin", new RoleDescriptor("ingest_admin", new String[] { "manage_index_templates", "manage_pipeline" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) // reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role .put("reporting_user", new RoleDescriptor("reporting_user", null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_dashboard_only_user", new RoleDescriptor( "kibana_dashboard_only_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*").privileges("read", "view_index_metadata").build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put(KibanaUser.ROLE_NAME, new RoleDescriptor(KibanaUser.ROLE_NAME, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*", ".reporting-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build() }, null, new ConditionalClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("logstash_system", new RoleDescriptor("logstash_system", new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("beats_admin", new RoleDescriptor("beats_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.BEATS_ROLE, new RoleDescriptor(UsernamesField.BEATS_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.APM_ROLE, new RoleDescriptor(UsernamesField.APM_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_user", new RoleDescriptor("machine_learning_user", new String[] { "monitor_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-notifications").privileges("view_index_metadata", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_admin", new RoleDescriptor("machine_learning_admin", new String[] { "manage_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-*").privileges("view_index_metadata", "read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_admin", new RoleDescriptor("watcher_admin", new String[] { "manage_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX, TriggeredWatchStoreField.INDEX_NAME, HistoryStoreField.INDEX_PREFIX + "*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_user", new RoleDescriptor("watcher_user", new String[] { "monitor_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX) .privileges("read") .build(), RoleDescriptor.IndicesPrivileges.builder().indices(HistoryStoreField.INDEX_PREFIX + "*") .privileges("read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("logstash_admin", new RoleDescriptor("logstash_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".logstash*") .privileges("create", "delete", "index", "manage", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_user", new RoleDescriptor("rollup_user", new String[] { "monitor_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_admin", new RoleDescriptor("rollup_admin", new String[] { "manage_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .immutableMap(); }	also, could you please explain why .indices(".kibana*").privileges("read") is required?
private DateHistogramValuesSourceBuilder randomDateHistogramSourceBuilder() { DateHistogramValuesSourceBuilder histo = new DateHistogramValuesSourceBuilder(randomAlphaOfLengthBetween(5, 10)); if (randomBoolean()) { histo.field(randomAlphaOfLengthBetween(1, 20)); } else { histo.script(new Script(randomAlphaOfLengthBetween(10, 20))); } if (randomBoolean()) { histo.calendarInterval(randomFrom(DateHistogramInterval.days(1), DateHistogramInterval.minutes(1), DateHistogramInterval.weeks(1))); } else { histo.fixedInterval(randomFrom(new DateHistogramInterval(randomNonNegativeLong() + "ms"), DateHistogramInterval.days(10), DateHistogramInterval.hours(10))); } if (randomBoolean()) { histo.timeZone(randomZone()); } if (randomBoolean()) { histo.missingBucket(true); } return histo; }	maybe we should test the legacy interval here too?
public static Automaton toCaseInsensitiveString(String s, int maxDeterminizedStates) { List<Automaton> list = new ArrayList<>(); Iterator<Integer> iter = s.codePoints().iterator(); while (iter.hasNext()) { list.add(toCaseInsensitiveChar(iter.next(), maxDeterminizedStates)); } Automaton a = Operations.concatenate(list); a = MinimizationOperations.minimize(a, maxDeterminizedStates); return a; }	wanted to use this, let me know if this shouldn't be that open. another option is simpyl copying the logic over i think.
@Override public Query prefixQuery(String value, MultiTermQuery.RewriteMethod method, boolean caseInsensitve, QueryShardContext context) { return wildcardQuery(value + "*", method, caseInsensitve, context); }	typo - missing i in insensitive.
private boolean canRetry(BulkResponse bulkItemResponses) { if (!backoff.hasNext()) { return false; } for (BulkItemResponse bulkItemResponse : bulkItemResponses) { if (bulkItemResponse.isFailed()) { final RestStatus status = bulkItemResponse.getFailure().getStatus(); if (this.retryOnStatus != status) { return false; } } } return true; }	just to double check: we don't need to unwrap here anymore because the status of the root cause is propagated to its ancestor?
private void executeBulkRejectionLoad(BackoffPolicy backoffPolicy, boolean rejectedExecutionExpected) throws Throwable { final CorrelatingBackoffPolicy internalPolicy = new CorrelatingBackoffPolicy(backoffPolicy); int numberOfAsyncOps = randomIntBetween(600, 700); final CountDownLatch latch = new CountDownLatch(numberOfAsyncOps); final Set<Object> responses = Collections.newSetFromMap(new ConcurrentHashMap<>()); assertAcked(prepareCreate(INDEX_NAME)); ensureGreen(); BulkProcessor bulkProcessor = BulkProcessor.builder(client(), new BulkProcessor.Listener() { @Override public void beforeBulk(long executionId, BulkRequest request) { // no op } @Override public void afterBulk(long executionId, BulkRequest request, BulkResponse response) { internalPolicy.logResponse(response); responses.add(response); latch.countDown(); } @Override public void afterBulk(long executionId, BulkRequest request, Throwable failure) { responses.add(failure); latch.countDown(); } }).setBulkActions(1) // zero means that we're in the sync case, more means that we're in the async case .setConcurrentRequests(randomIntBetween(0, 100)) .setBackoffPolicy(internalPolicy) .build(); indexDocs(bulkProcessor, numberOfAsyncOps); latch.await(10, TimeUnit.SECONDS); bulkProcessor.close(); assertThat(responses.size(), equalTo(numberOfAsyncOps)); // validate all responses for (Object response : responses) { if (response instanceof BulkResponse) { BulkResponse bulkResponse = (BulkResponse) response; for (BulkItemResponse bulkItemResponse : bulkResponse.getItems()) { if (bulkItemResponse.isFailed()) { BulkItemResponse.Failure failure = bulkItemResponse.getFailure(); if (failure.getStatus() == RestStatus.TOO_MANY_REQUESTS) { if (!rejectedExecutionExpected) { Throwable rootCause = ExceptionsHelper.unwrapCause(failure.getCause()); Iterator<TimeValue> backoffState = internalPolicy.backoffStateFor(bulkResponse); assertNotNull("backoffState is null (indicates a bulk request got rejected without retry)", backoffState); if (backoffState.hasNext()) { // we're not expecting that we overwhelmed it even once when we maxed out the number of retries throw new AssertionError("Got rejected although backoff policy would allow more retries", rootCause); } else { logger.debug("We maxed out the number of bulk retries and got rejected (this is ok)."); } } } else { throw new AssertionError("Unexpected failure status: " + failure.getStatus()); } } } } else { Throwable t = (Throwable) response; // we're not expecting any other errors throw new AssertionError("Unexpected failure", t); } } client().admin().indices().refresh(new RefreshRequest()).get(); // validate we did not create any duplicates due to retries Matcher<Long> searchResultCount; // it is ok if we lost some index operations to rejected executions (which is possible even when backing off although less likely) searchResultCount = lessThanOrEqualTo((long) numberOfAsyncOps); SearchResponse results = client() .prepareSearch(INDEX_NAME) .setTypes(TYPE_NAME) .setQuery(QueryBuilders.matchAllQuery()) .setSize(0) .get(); assertThat(results.getHits().getTotalHits(), searchResultCount); }	can you leave the previous rejectedexecutionexpected == false please? we prefer this one for readability.
private void executeBulkRejectionLoad(BackoffPolicy backoffPolicy, boolean rejectedExecutionExpected) throws Throwable { final CorrelatingBackoffPolicy internalPolicy = new CorrelatingBackoffPolicy(backoffPolicy); int numberOfAsyncOps = randomIntBetween(600, 700); final CountDownLatch latch = new CountDownLatch(numberOfAsyncOps); final Set<Object> responses = Collections.newSetFromMap(new ConcurrentHashMap<>()); assertAcked(prepareCreate(INDEX_NAME)); ensureGreen(); BulkProcessor bulkProcessor = BulkProcessor.builder(client(), new BulkProcessor.Listener() { @Override public void beforeBulk(long executionId, BulkRequest request) { // no op } @Override public void afterBulk(long executionId, BulkRequest request, BulkResponse response) { internalPolicy.logResponse(response); responses.add(response); latch.countDown(); } @Override public void afterBulk(long executionId, BulkRequest request, Throwable failure) { responses.add(failure); latch.countDown(); } }).setBulkActions(1) // zero means that we're in the sync case, more means that we're in the async case .setConcurrentRequests(randomIntBetween(0, 100)) .setBackoffPolicy(internalPolicy) .build(); indexDocs(bulkProcessor, numberOfAsyncOps); latch.await(10, TimeUnit.SECONDS); bulkProcessor.close(); assertThat(responses.size(), equalTo(numberOfAsyncOps)); // validate all responses for (Object response : responses) { if (response instanceof BulkResponse) { BulkResponse bulkResponse = (BulkResponse) response; for (BulkItemResponse bulkItemResponse : bulkResponse.getItems()) { if (bulkItemResponse.isFailed()) { BulkItemResponse.Failure failure = bulkItemResponse.getFailure(); if (failure.getStatus() == RestStatus.TOO_MANY_REQUESTS) { if (!rejectedExecutionExpected) { Throwable rootCause = ExceptionsHelper.unwrapCause(failure.getCause()); Iterator<TimeValue> backoffState = internalPolicy.backoffStateFor(bulkResponse); assertNotNull("backoffState is null (indicates a bulk request got rejected without retry)", backoffState); if (backoffState.hasNext()) { // we're not expecting that we overwhelmed it even once when we maxed out the number of retries throw new AssertionError("Got rejected although backoff policy would allow more retries", rootCause); } else { logger.debug("We maxed out the number of bulk retries and got rejected (this is ok)."); } } } else { throw new AssertionError("Unexpected failure status: " + failure.getStatus()); } } } } else { Throwable t = (Throwable) response; // we're not expecting any other errors throw new AssertionError("Unexpected failure", t); } } client().admin().indices().refresh(new RefreshRequest()).get(); // validate we did not create any duplicates due to retries Matcher<Long> searchResultCount; // it is ok if we lost some index operations to rejected executions (which is possible even when backing off although less likely) searchResultCount = lessThanOrEqualTo((long) numberOfAsyncOps); SearchResponse results = client() .prepareSearch(INDEX_NAME) .setTypes(TYPE_NAME) .setQuery(QueryBuilders.matchAllQuery()) .setSize(0) .get(); assertThat(results.getHits().getTotalHits(), searchResultCount); }	maybe we do not even need to unwrap it here anymore? could we just use getcause instead when throwing assertion error below?
@Override public void readFrom(StreamInput in) throws IOException { indexCount = in.readVLong(); indexTimeInMillis = in.readVLong(); indexCurrent = in.readVLong(); indexFailedCount = in.readVLong(); deleteCount = in.readVLong(); deleteTimeInMillis = in.readVLong(); deleteCurrent = in.readVLong(); noopUpdateCount = in.readVLong(); isThrottled = in.readBoolean(); throttleTimeInMillis = in.readLong(); }	i imagine we'll need the if (version.onorafter(version.v_2_1_0)) dance around these reads and writes here. we didn't need it in 2.0 because we don't have multi-version compatibility but this will probably go to 2.1 and that'll have to be compatible with 2.0.
@Test public void simpleStats() throws Exception { createIndex("test1", "test2"); ensureGreen(); client().prepareIndex("test1", "type1", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test1", "type2", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test2", "type", Integer.toString(1)).setSource("field", "value").execute().actionGet(); refresh(); NumShards test1 = getNumShards("test1"); long test1ExpectedWrites = 2 * test1.dataCopies; NumShards test2 = getNumShards("test2"); long test2ExpectedWrites = test2.dataCopies; long totalExpectedWrites = test1ExpectedWrites + test2ExpectedWrites; IndicesStatsResponse stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getPrimaries().getDocs().getCount(), equalTo(3l)); assertThat(stats.getTotal().getDocs().getCount(), equalTo(totalExpectedWrites)); assertThat(stats.getPrimaries().getIndexing().getTotal().getIndexCount(), equalTo(3l)); assertThat(stats.getPrimaries().getIndexing().getTotal().getIndexFailedCount(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTotal().isThrottled(), equalTo(false)); assertThat(stats.getPrimaries().getIndexing().getTotal().getThrottleTimeInMillis(), equalTo(0l)); assertThat(stats.getTotal().getIndexing().getTotal().getIndexCount(), equalTo(totalExpectedWrites)); assertThat(stats.getTotal().getStore(), notNullValue()); assertThat(stats.getTotal().getMerge(), notNullValue()); assertThat(stats.getTotal().getFlush(), notNullValue()); assertThat(stats.getTotal().getRefresh(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getDocs().getCount(), equalTo(2l)); assertThat(stats.getIndex("test1").getTotal().getDocs().getCount(), equalTo(test1ExpectedWrites)); assertThat(stats.getIndex("test1").getPrimaries().getStore(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getMerge(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getFlush(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getRefresh(), notNullValue()); assertThat(stats.getIndex("test2").getPrimaries().getDocs().getCount(), equalTo(1l)); assertThat(stats.getIndex("test2").getTotal().getDocs().getCount(), equalTo(test2ExpectedWrites)); // make sure that number of requests in progress is 0 assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getIndexCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getDeleteCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getSearch().getTotal().getFetchCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getSearch().getTotal().getQueryCurrent(), equalTo(0l)); // check flags stats = client().admin().indices().prepareStats().clear() .setFlush(true) .setRefresh(true) .setMerge(true) .execute().actionGet(); assertThat(stats.getTotal().getDocs(), nullValue()); assertThat(stats.getTotal().getStore(), nullValue()); assertThat(stats.getTotal().getIndexing(), nullValue()); assertThat(stats.getTotal().getMerge(), notNullValue()); assertThat(stats.getTotal().getFlush(), notNullValue()); assertThat(stats.getTotal().getRefresh(), notNullValue()); // check types stats = client().admin().indices().prepareStats().setTypes("type1", "type").execute().actionGet(); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type").getIndexCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexFailedCount(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type2"), nullValue()); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexCurrent(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getDeleteCurrent(), equalTo(0l)); assertThat(stats.getTotal().getGet().getCount(), equalTo(0l)); // check get GetResponse getResponse = client().prepareGet("test1", "type1", "1").execute().actionGet(); assertThat(getResponse.isExists(), equalTo(true)); stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getTotal().getGet().getCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getExistsCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getMissingCount(), equalTo(0l)); // missing get getResponse = client().prepareGet("test1", "type1", "2").execute().actionGet(); assertThat(getResponse.isExists(), equalTo(false)); stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getTotal().getGet().getCount(), equalTo(2l)); assertThat(stats.getTotal().getGet().getExistsCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getMissingCount(), equalTo(1l)); // clear all stats = client().admin().indices().prepareStats() .setDocs(false) .setStore(false) .setIndexing(false) .setFlush(true) .setRefresh(true) .setMerge(true) .clear() // reset defaults .execute().actionGet(); assertThat(stats.getTotal().getDocs(), nullValue()); assertThat(stats.getTotal().getStore(), nullValue()); assertThat(stats.getTotal().getIndexing(), nullValue()); assertThat(stats.getTotal().getGet(), nullValue()); assertThat(stats.getTotal().getSearch(), nullValue()); // index failed try { client().prepareIndex("test1", "type1", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } try { client().prepareIndex("test1", "type2", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } try { client().prepareIndex("test2", "type", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } refresh(); stats = client().admin().indices().prepareStats().setTypes("type1").execute().actionGet(); assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(2l)); assertThat(stats.getIndex("test2").getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexFailedCount(), equalTo(1L)); assertThat(stats.getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(3L)); }	i'd stick a fail("expected a version conflict"); right here just to make sure the failure occurs.
@Test public void simpleStats() throws Exception { createIndex("test1", "test2"); ensureGreen(); client().prepareIndex("test1", "type1", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test1", "type2", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test2", "type", Integer.toString(1)).setSource("field", "value").execute().actionGet(); refresh(); NumShards test1 = getNumShards("test1"); long test1ExpectedWrites = 2 * test1.dataCopies; NumShards test2 = getNumShards("test2"); long test2ExpectedWrites = test2.dataCopies; long totalExpectedWrites = test1ExpectedWrites + test2ExpectedWrites; IndicesStatsResponse stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getPrimaries().getDocs().getCount(), equalTo(3l)); assertThat(stats.getTotal().getDocs().getCount(), equalTo(totalExpectedWrites)); assertThat(stats.getPrimaries().getIndexing().getTotal().getIndexCount(), equalTo(3l)); assertThat(stats.getPrimaries().getIndexing().getTotal().getIndexFailedCount(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTotal().isThrottled(), equalTo(false)); assertThat(stats.getPrimaries().getIndexing().getTotal().getThrottleTimeInMillis(), equalTo(0l)); assertThat(stats.getTotal().getIndexing().getTotal().getIndexCount(), equalTo(totalExpectedWrites)); assertThat(stats.getTotal().getStore(), notNullValue()); assertThat(stats.getTotal().getMerge(), notNullValue()); assertThat(stats.getTotal().getFlush(), notNullValue()); assertThat(stats.getTotal().getRefresh(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getDocs().getCount(), equalTo(2l)); assertThat(stats.getIndex("test1").getTotal().getDocs().getCount(), equalTo(test1ExpectedWrites)); assertThat(stats.getIndex("test1").getPrimaries().getStore(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getMerge(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getFlush(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getRefresh(), notNullValue()); assertThat(stats.getIndex("test2").getPrimaries().getDocs().getCount(), equalTo(1l)); assertThat(stats.getIndex("test2").getTotal().getDocs().getCount(), equalTo(test2ExpectedWrites)); // make sure that number of requests in progress is 0 assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getIndexCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getDeleteCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getSearch().getTotal().getFetchCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getSearch().getTotal().getQueryCurrent(), equalTo(0l)); // check flags stats = client().admin().indices().prepareStats().clear() .setFlush(true) .setRefresh(true) .setMerge(true) .execute().actionGet(); assertThat(stats.getTotal().getDocs(), nullValue()); assertThat(stats.getTotal().getStore(), nullValue()); assertThat(stats.getTotal().getIndexing(), nullValue()); assertThat(stats.getTotal().getMerge(), notNullValue()); assertThat(stats.getTotal().getFlush(), notNullValue()); assertThat(stats.getTotal().getRefresh(), notNullValue()); // check types stats = client().admin().indices().prepareStats().setTypes("type1", "type").execute().actionGet(); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type").getIndexCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexFailedCount(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type2"), nullValue()); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexCurrent(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getDeleteCurrent(), equalTo(0l)); assertThat(stats.getTotal().getGet().getCount(), equalTo(0l)); // check get GetResponse getResponse = client().prepareGet("test1", "type1", "1").execute().actionGet(); assertThat(getResponse.isExists(), equalTo(true)); stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getTotal().getGet().getCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getExistsCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getMissingCount(), equalTo(0l)); // missing get getResponse = client().prepareGet("test1", "type1", "2").execute().actionGet(); assertThat(getResponse.isExists(), equalTo(false)); stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getTotal().getGet().getCount(), equalTo(2l)); assertThat(stats.getTotal().getGet().getExistsCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getMissingCount(), equalTo(1l)); // clear all stats = client().admin().indices().prepareStats() .setDocs(false) .setStore(false) .setIndexing(false) .setFlush(true) .setRefresh(true) .setMerge(true) .clear() // reset defaults .execute().actionGet(); assertThat(stats.getTotal().getDocs(), nullValue()); assertThat(stats.getTotal().getStore(), nullValue()); assertThat(stats.getTotal().getIndexing(), nullValue()); assertThat(stats.getTotal().getGet(), nullValue()); assertThat(stats.getTotal().getSearch(), nullValue()); // index failed try { client().prepareIndex("test1", "type1", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } try { client().prepareIndex("test1", "type2", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } try { client().prepareIndex("test2", "type", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } refresh(); stats = client().admin().indices().prepareStats().setTypes("type1").execute().actionGet(); assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(2l)); assertThat(stats.getIndex("test2").getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexFailedCount(), equalTo(1L)); assertThat(stats.getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(3L)); }	it probably isn't a big deal, but is the refresh required here?
@Test public void simpleStats() throws Exception { createIndex("test1", "test2"); ensureGreen(); client().prepareIndex("test1", "type1", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test1", "type2", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test2", "type", Integer.toString(1)).setSource("field", "value").execute().actionGet(); refresh(); NumShards test1 = getNumShards("test1"); long test1ExpectedWrites = 2 * test1.dataCopies; NumShards test2 = getNumShards("test2"); long test2ExpectedWrites = test2.dataCopies; long totalExpectedWrites = test1ExpectedWrites + test2ExpectedWrites; IndicesStatsResponse stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getPrimaries().getDocs().getCount(), equalTo(3l)); assertThat(stats.getTotal().getDocs().getCount(), equalTo(totalExpectedWrites)); assertThat(stats.getPrimaries().getIndexing().getTotal().getIndexCount(), equalTo(3l)); assertThat(stats.getPrimaries().getIndexing().getTotal().getIndexFailedCount(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTotal().isThrottled(), equalTo(false)); assertThat(stats.getPrimaries().getIndexing().getTotal().getThrottleTimeInMillis(), equalTo(0l)); assertThat(stats.getTotal().getIndexing().getTotal().getIndexCount(), equalTo(totalExpectedWrites)); assertThat(stats.getTotal().getStore(), notNullValue()); assertThat(stats.getTotal().getMerge(), notNullValue()); assertThat(stats.getTotal().getFlush(), notNullValue()); assertThat(stats.getTotal().getRefresh(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getDocs().getCount(), equalTo(2l)); assertThat(stats.getIndex("test1").getTotal().getDocs().getCount(), equalTo(test1ExpectedWrites)); assertThat(stats.getIndex("test1").getPrimaries().getStore(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getMerge(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getFlush(), notNullValue()); assertThat(stats.getIndex("test1").getPrimaries().getRefresh(), notNullValue()); assertThat(stats.getIndex("test2").getPrimaries().getDocs().getCount(), equalTo(1l)); assertThat(stats.getIndex("test2").getTotal().getDocs().getCount(), equalTo(test2ExpectedWrites)); // make sure that number of requests in progress is 0 assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getIndexCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getDeleteCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getSearch().getTotal().getFetchCurrent(), equalTo(0l)); assertThat(stats.getIndex("test1").getTotal().getSearch().getTotal().getQueryCurrent(), equalTo(0l)); // check flags stats = client().admin().indices().prepareStats().clear() .setFlush(true) .setRefresh(true) .setMerge(true) .execute().actionGet(); assertThat(stats.getTotal().getDocs(), nullValue()); assertThat(stats.getTotal().getStore(), nullValue()); assertThat(stats.getTotal().getIndexing(), nullValue()); assertThat(stats.getTotal().getMerge(), notNullValue()); assertThat(stats.getTotal().getFlush(), notNullValue()); assertThat(stats.getTotal().getRefresh(), notNullValue()); // check types stats = client().admin().indices().prepareStats().setTypes("type1", "type").execute().actionGet(); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type").getIndexCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexFailedCount(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type2"), nullValue()); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexCurrent(), equalTo(0l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getDeleteCurrent(), equalTo(0l)); assertThat(stats.getTotal().getGet().getCount(), equalTo(0l)); // check get GetResponse getResponse = client().prepareGet("test1", "type1", "1").execute().actionGet(); assertThat(getResponse.isExists(), equalTo(true)); stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getTotal().getGet().getCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getExistsCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getMissingCount(), equalTo(0l)); // missing get getResponse = client().prepareGet("test1", "type1", "2").execute().actionGet(); assertThat(getResponse.isExists(), equalTo(false)); stats = client().admin().indices().prepareStats().execute().actionGet(); assertThat(stats.getTotal().getGet().getCount(), equalTo(2l)); assertThat(stats.getTotal().getGet().getExistsCount(), equalTo(1l)); assertThat(stats.getTotal().getGet().getMissingCount(), equalTo(1l)); // clear all stats = client().admin().indices().prepareStats() .setDocs(false) .setStore(false) .setIndexing(false) .setFlush(true) .setRefresh(true) .setMerge(true) .clear() // reset defaults .execute().actionGet(); assertThat(stats.getTotal().getDocs(), nullValue()); assertThat(stats.getTotal().getStore(), nullValue()); assertThat(stats.getTotal().getIndexing(), nullValue()); assertThat(stats.getTotal().getGet(), nullValue()); assertThat(stats.getTotal().getSearch(), nullValue()); // index failed try { client().prepareIndex("test1", "type1", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } try { client().prepareIndex("test1", "type2", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } try { client().prepareIndex("test2", "type", Integer.toString(1)).setSource("field", "value").setVersion(1) .setVersionType(VersionType.EXTERNAL).execute().actionGet(); } catch (VersionConflictEngineException e) { //expected } refresh(); stats = client().admin().indices().prepareStats().setTypes("type1").execute().actionGet(); assertThat(stats.getIndex("test1").getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(2l)); assertThat(stats.getIndex("test2").getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(1l)); assertThat(stats.getPrimaries().getIndexing().getTypeStats().get("type1").getIndexFailedCount(), equalTo(1L)); assertThat(stats.getTotal().getIndexing().getTotal().getIndexFailedCount(), equalTo(3L)); }	s/"type1"/"type1", "type2"/ ? i think you need them both, right?
@Override public void readFrom(final StreamInput in) throws IOException { super.readFrom(in); followerIndex = in.readString(); maxReadRequestOperationCount = in.readOptionalVInt(); maxOutstandingReadRequests = in.readOptionalVInt(); maxReadRequestSize = in.readOptionalWriteable(ByteSizeValue::new); maxWriteRequestOperationCount = in.readOptionalVInt(); maxWriteRequestSize = in.readOptionalWriteable(ByteSizeValue::new); maxOutstandingWriteRequests = in.readOptionalVInt(); maxWriteBufferCount = in.readOptionalVInt(); maxWriteBufferSize = in.readOptionalWriteable(ByteSizeValue::new); maxRetryDelay = in.readOptionalTimeValue(); readPollTimeout = in.readOptionalTimeValue(); }	would you mind moving maxreadrequestsize above maxoutstandingreadrequests so that we have the same order for both read and write.
public void testDefaultCredential() throws Exception { Environment env = TestEnvironment.newEnvironment(Settings.builder().put("path.home", createTempDir()).build()); GoogleCredential cred = GoogleCredential.fromStream(getDummyCredentialStream()); InternalGoogleCloudStorageService service = new InternalGoogleCloudStorageService(env, Collections.emptyMap()) { @Override GoogleCredential getDefaultCredential() throws IOException { return cred; } }; assertSame(cred, service.getCredential("default")); service.new DefaultHttpRequestInitializer(cred, null, null); }	is this a typo? not sure how this compiles...
public void testRange() { RollupJobConfig.Builder job = ConfigTestHelpers.getRollupJob("foo"); GroupConfig.Builder group = ConfigTestHelpers.getGroupConfig(); group.setDateHisto(new DateHistoGroupConfig.Builder().setField("foo").setInterval(new DateHistogramInterval("1h")).build()); job.setGroupConfig(group.build()); RollupJobCaps cap = new RollupJobCaps(job.build()); Set<RollupJobCaps> caps = new HashSet<>(); caps.add(cap); QueryBuilder rewritten = null; try { rewritten = TransportRollupSearchAction.rewriteQuery(new RangeQueryBuilder("foo").gt(1).timeZone("UTC"), caps); } catch (Exception e) { fail("Should not have thrown exception when parsing query."); } assertThat(rewritten, instanceOf(RangeQueryBuilder.class)); assertThat(((RangeQueryBuilder)rewritten).fieldName(), equalTo("foo.date_histogram.timestamp")); }	maybe not needed to catch the exception here and just make testrange throws exception ?
private void tryAllHandlers(final RestRequest request, final RestChannel channel, final ThreadContext threadContext) throws Exception { for (final RestHeaderDefinition restHeader : headersToCopy) { final String name = restHeader.getName(); final List<String> headerValues = request.getAllHeaderValues(name); if (headerValues != null && headerValues.isEmpty() == false) { final List<String> distinctHeaderValues = headerValues.stream().distinct().collect(Collectors.toList()); if (restHeader.isMultiValueAllowed() == false && distinctHeaderValues.size() > 1) { channel.sendResponse( BytesRestResponse. createSimpleErrorResponse(channel, BAD_REQUEST, "multiple values for single-valued header [" + name + "].")); return; } else if (name.equals(Task.TRACE_PARENT)) { String traceparent = distinctHeaderValues.get(0); if (traceparent.length() >= 55) { threadContext.putTransient(Task.TRACE_ID, traceparent.substring(3, 35)); } threadContext.putHeader(name, traceparent); } else { threadContext.putHeader(name, String.join(",", distinctHeaderValues)); } } } // error_trace cannot be used when we disable detailed errors // we consume the error_trace parameter first to ensure that it is always consumed if (request.paramAsBoolean("error_trace", false) && channel.detailedErrorsEnabled() == false) { channel.sendResponse( BytesRestResponse.createSimpleErrorResponse(channel, BAD_REQUEST, "error traces in responses are disabled.")); return; } final String rawPath = request.rawPath(); final String uri = request.uri(); final RestRequest.Method requestMethod; RestApiVersion restApiVersion = request.getRestApiVersion(); try { // Resolves the HTTP method and fails if the method is invalid requestMethod = request.method(); // Loop through all possible handlers, attempting to dispatch the request Iterator<MethodHandlers> allHandlers = getAllHandlers(request.params(), rawPath); while (allHandlers.hasNext()) { final RestHandler handler; final MethodHandlers handlers = allHandlers.next(); if (handlers == null) { handler = null; } else { handler = handlers.getHandler(requestMethod, restApiVersion); } if (handler == null) { if (handleNoHandlerFound(rawPath, requestMethod, uri, channel)) { return; } } else { dispatchRequest(request, channel, handler, restApiVersion); return; } } } catch (final IllegalArgumentException e) { handleUnsupportedHttpMethod(uri, null, channel, getValidHandlerMethodSet(rawPath), e); return; } // If request has not been handled, fallback to a bad request error. handleBadRequest(uri, requestMethod, channel); }	an alternative to traceidconverter and setting the trace.is as a transient header is to set org.apache.logging.log4j.threadcontext.put("trace.id", traceid);. this is how the java agent handles log correlation. the ecs-logging-java lib will automatically add the trace id to the logs then. some caveats: * at the end of the request org.apache.logging.log4j.threadcontext#clearall has to be called * if some work is passed off to a thread pool, the context has to be propagated manually. not sure if that's something es does and whether threadcontext is already propagated.
* @param targetBlob new name of the blob in the same bucket */ void moveBlob(String sourceBlobName, String targetBlobName) throws IOException { final BlobId sourceBlobId = BlobId.of(bucketName, sourceBlobName); final BlobId targetBlobId = BlobId.of(bucketName, targetBlobName); final CopyRequest request = CopyRequest.newBuilder() .setSource(sourceBlobId) .setTarget(targetBlobId) .build(); // There's no atomic "move" in GCS so we need to copy and delete storageAccessConsumer(storage -> storage.copy(request).getResult()); final boolean deleted = storageAccess(storage -> storage.delete(sourceBlobId)); if (deleted == false) { throw new IOException("Failed to move source [" + sourceBlobName + "] to target [" + targetBlobName + "]"); } }	this is where a safeclient() would be helpful, so that you have less chance that the underlying storage instance changed between the copy and delete calls
* @return true if the bucket exists, false otherwise */ boolean doesBucketExist(String bucketName) { try { final Bucket bucket = storageAccess(storage -> storage.get(bucketName)); return bucket != null; } catch (final Exception e) { throw new BlobStoreException("Unable to check if bucket [" + bucketName + "] exists", e); } }	following my previous comment, this could be: socketaccess.doprivilegedioexception(() -> safeclient().get(bucketname));
@Override public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMappingException { SourceFieldMapper sourceMergeWith = (SourceFieldMapper) mergeWith; if (mergeResult.simulate()) { if (this.enabled != sourceMergeWith.enabled) { mergeResult.addConflict("Cannot update enabled setting for [_source]"); } if (Arrays.equals(this.includes, sourceMergeWith.includes) == false) { mergeResult.addConflict("Cannot update includes setting for [_source]"); } if (Arrays.equals(this.excludes, sourceMergeWith.excludes) == false) { mergeResult.addConflict("Cannot update excludes setting for [_source]"); } } else { if (sourceMergeWith.compress != null) { this.compress = sourceMergeWith.compress; } if (sourceMergeWith.compressThreshold != -1) { this.compressThreshold = sourceMergeWith.compressThreshold; } } }	will arrays.equal handle correctly null arrays?
@Override public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMappingException { SourceFieldMapper sourceMergeWith = (SourceFieldMapper) mergeWith; if (mergeResult.simulate()) { if (this.enabled != sourceMergeWith.enabled) { mergeResult.addConflict("Cannot update enabled setting for [_source]"); } if (Arrays.equals(this.includes, sourceMergeWith.includes) == false) { mergeResult.addConflict("Cannot update includes setting for [_source]"); } if (Arrays.equals(this.excludes, sourceMergeWith.excludes) == false) { mergeResult.addConflict("Cannot update excludes setting for [_source]"); } } else { if (sourceMergeWith.compress != null) { this.compress = sourceMergeWith.compress; } if (sourceMergeWith.compressThreshold != -1) { this.compressThreshold = sourceMergeWith.compressThreshold; } } }	i think we should also emit conflicts if simulate is false?
public Object value(Aggregation agg, String fieldType, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { SingleBucketAggregation aggregation = (SingleBucketAggregation) agg; if (aggregation.getAggregations().iterator().hasNext() == false) { return aggregation.getDocCount(); } HashMap<String, Object> nested = new HashMap<>(); for (Aggregation subAgg : aggregation.getAggregations()) { String subLookupFieldPrefix = lookupFieldPrefix.isEmpty() ? agg.getName() : lookupFieldPrefix + "." + agg.getName(); nested.put( subAgg.getName(), getExtractor(subAgg).value( subAgg, fieldTypeMap.get(subLookupFieldPrefix + "." + subAgg.getName()), fieldTypeMap, subLookupFieldPrefix ) ); } return nested; } } static class ScriptedMetricAggExtractor implements AggValueExtractor { @Override public Object value(Aggregation agg, String fieldType, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { ScriptedMetric aggregation = (ScriptedMetric) agg; return aggregation.aggregation(); } } static class GeoCentroidAggExtractor implements AggValueExtractor { @Override public Object value(Aggregation agg, String fieldType, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { GeoCentroid aggregation = (GeoCentroid) agg; // if the account is `0` iff there is no contained centroid return aggregation.count() > 0 ? aggregation.centroid().toString() : null; } } static class GeoBoundsAggExtractor implements AggValueExtractor { @Override public Object value(Aggregation agg, String fieldType, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { GeoBounds aggregation = (GeoBounds) agg; if (aggregation.bottomRight() == null || aggregation.topLeft() == null) { return null; } final Map<String, Object> geoShape = new HashMap<>(); // If the two geo_points are equal, it is a point if (aggregation.topLeft().equals(aggregation.bottomRight())) { geoShape.put(ShapeParser.FIELD_TYPE.getPreferredName(), PointBuilder.TYPE.shapeName()); geoShape.put( ShapeParser.FIELD_COORDINATES.getPreferredName(), Arrays.asList(aggregation.topLeft().getLon(), aggregation.bottomRight().getLat()) ); // If only the lat or the lon of the two geo_points are equal, than we know it should be a line } else if (Double.compare(aggregation.topLeft().getLat(), aggregation.bottomRight().getLat()) == 0 || Double.compare(aggregation.topLeft().getLon(), aggregation.bottomRight().getLon()) == 0) { geoShape.put(ShapeParser.FIELD_TYPE.getPreferredName(), LineStringBuilder.TYPE.shapeName()); geoShape.put( ShapeParser.FIELD_COORDINATES.getPreferredName(), Arrays.asList( new Double[] { aggregation.topLeft().getLon(), aggregation.topLeft().getLat() }, new Double[] { aggregation.bottomRight().getLon(), aggregation.bottomRight().getLat() } ) ); } else { // neither points are equal, we have a polygon that is a square geoShape.put(ShapeParser.FIELD_TYPE.getPreferredName(), PolygonBuilder.TYPE.shapeName()); final GeoPoint tl = aggregation.topLeft(); final GeoPoint br = aggregation.bottomRight(); geoShape.put( ShapeParser.FIELD_COORDINATES.getPreferredName(), Collections.singletonList( Arrays.asList( new Double[] { tl.getLon(), tl.getLat() }, new Double[] { br.getLon(), tl.getLat() }, new Double[] { br.getLon(), br.getLat() }, new Double[] { tl.getLon(), br.getLat() }, new Double[] { tl.getLon(), tl.getLat() } ) ) ); } return geoShape; }	it strikes me that supplying fieldtype is no longer necessary. anything that desires to know fieldtype could look it up via its own agg name combined with the prefix (if not empty).
@Override public double getVariancePopulation() { if(count < 1){ return Double.NaN; } if(Double.isNaN(m2)){ double variance = (sumOfSqrs - ((sum * sum) / count)) / count; return variance < 0 ? 0 : variance + 2; } else{ return m2 / count; } }	i think it is worth a comment that this can happen if we combine with a node that doesn't calculate m2.
private OsStats.Cgroup getCgroup() { try { if (!areCgroupStatsAvailable()) { return null; } else { final Map<String, String> controllerMap = getControlGroups(); assert !controllerMap.isEmpty(); final String cpuAcctControlGroup = controllerMap.get("cpuacct"); if (cpuAcctControlGroup == null) { logger.debug("no [cpuacct] data found in control group stats"); return null; } final long cgroupCpuAcctUsageNanos = getCgroupCpuAcctUsageNanos(cpuAcctControlGroup); final String cpuControlGroup = controllerMap.get("cpu"); if (cpuControlGroup == null) { logger.debug("no [cpu] data found in control group stats"); return null; } final long cgroupCpuAcctCpuCfsPeriodMicros = getCgroupCpuAcctCpuCfsPeriodMicros(cpuControlGroup); final long cgroupCpuAcctCpuCfsQuotaMicros = getCgroupCpuAcctCpuCfsQuotaMicros(cpuControlGroup); final OsStats.Cgroup.CpuStat cpuStat = getCgroupCpuAcctCpuStat(cpuControlGroup); final String memoryControlGroup = controllerMap.get("memory"); if (memoryControlGroup == null) { logger.debug("no [memory] data found in control group stats"); return null; } final String cgroupMemoryLimitInBytes = getCgroupMemoryLimitInBytes(memoryControlGroup); final String cgroupMemoryUsageInBytes = getCgroupMemoryUsageInBytes(memoryControlGroup); return new OsStats.Cgroup( cpuAcctControlGroup, cgroupCpuAcctUsageNanos, cpuControlGroup, cgroupCpuAcctCpuCfsPeriodMicros, cgroupCpuAcctCpuCfsQuotaMicros, cpuStat, memoryControlGroup, cgroupMemoryLimitInBytes, cgroupMemoryUsageInBytes); } } catch (final IOException e) { logger.debug("error reading control group stats", e); return null; } }	i would suggest cgroup instead of control group since we refer to these as cgroup in json responses, etc.
public void postCreate(Engine.Create create) { long took = create.endTime() - create.startTime(); totalStats.indexMetric.inc(took); totalStats.indexCurrent.dec(); StatsHolder typeStats = typeStats(create.type()); typeStats.indexMetric.inc(took); typeStats.indexCurrent.dec(); slowLog.postCreate(create, took); for (IndexingOperationListener listener : listeners) { try { listener.postCreate(create); } catch (Exception e) { logger.warn("postCreate listener [{}] failed", e, listener); } } }	we shouldn't catch the exception here to be honest. i think in all the preindex|create|delete we should just fail hard and bubble up the exception. it would be awesome if we could document that on the interface that failures in this method will be fatal and prevent the document from being indexed|created|deleted?
* @return the delay to the next time the maintenance should be triggered */ private static TimeValue delayToNextTime(ClusterName clusterName) { Random random = new Random(clusterName.hashCode()); int minutesOffset = random.ints(0, MAX_TIME_OFFSET_MINUTES).findFirst().getAsInt(); ZonedDateTime now = ZonedDateTime.now(Clock.systemDefaultZone()); ZonedDateTime next = now.plusSeconds(10); return TimeValue.timeValueMillis(next.toInstant().toEpochMilli() - now.toInstant().toEpochMilli()); }	why was this changed to just 10 seconds into the future?
protected void createIndicesWithRandomAliases(String... indices) { createIndex(indices); if (frequently()) { boolean noAliasAdded = true; IndicesAliasesRequestBuilder builder = client().admin().indices().prepareAliases(); for (String index : indices) { if (frequently()) { //one alias per index with prefix "alias-" builder.addAlias(index, "alias-" + index); noAliasAdded = false; } } // If we get to this point and we haven't added an alias to the request we need to add one // or the request will fail so use noAliasAdded to force adding the alias in this case if (noAliasAdded || randomBoolean()) { //one alias pointing to all indices for (String index : indices) { builder.addAlias(index, "alias"); } } assertAcked(builder); } for (String index : indices) { client().prepareIndex(index, "type").setSource("field", "value").get(); } refresh(indices); }	i think aliasadded = false; is clearer.
private void addEmptyBuckets(List<Bucket> list, ReduceContext reduceContext) { /* * Make sure we have space for the empty buckets we're going to add by * counting all of the empties we plan to add and firing them into * consumeBucketsAndMaybeBreak. * * We don't count all of the buckets we plan to allocate and then report * them all at once because we you can configure the date_histogram to * create an astounding number of buckets. It'd take a while to count * that high only to abort. So we report every couple thousand buckets. * It's be simpler to report every single bucket we plan to allocate * one at a time but that'd cause needless overhead on the circuit * breakers. Counting a couple thousand buckets is plenty fast to fail * this quickly in pathological cases and plenty large to keep the * overhead minimal. */ int reportEmptyEvery = 10000; class Counter implements LongConsumer { private int size = list.size(); @Override public void accept(long key) { size++; if (size >= reportEmptyEvery) { reduceContext.consumeBucketsAndMaybeBreak(size); size = 0; } } } Counter counter = new Counter(); iterateEmptyBuckets(list, list.listIterator(), counter); reduceContext.consumeBucketsAndMaybeBreak(counter.size); InternalAggregations reducedEmptySubAggs = InternalAggregations.reduce(List.of(emptyBucketInfo.subAggregations), reduceContext); ListIterator<Bucket> iter = list.listIterator(); iterateEmptyBuckets(list, iter, key -> iter.add(new InternalDateHistogram.Bucket(key, 0, keyed, format, reducedEmptySubAggs))); }	should this be a constant?
private void inferSingleDocAgainstAllocatedModel( Task task, String modelId, InferenceConfigUpdate inferenceConfigUpdate, Map<String, Object> doc, ActionListener<InferenceResults> listener ) { TaskId taskId = new TaskId(clusterService.localNode().getId(), task.getId()); InferTrainedModelDeploymentAction.Request request = new InferTrainedModelDeploymentAction.Request( modelId, inferenceConfigUpdate, Collections.singletonList(doc), TimeValue.MAX_VALUE ); request.setParentTaskId(taskId); executeAsyncWithOrigin( new ParentTaskAssigningClient(client, taskId), ML_ORIGIN, InferTrainedModelDeploymentAction.INSTANCE, request, ActionListener.wrap(r -> listener.onResponse(r.getResults()), e -> { Throwable unwrapped = ExceptionsHelper.unwrapCause(e); if (unwrapped instanceof ElasticsearchStatusException) { ElasticsearchStatusException ex = (ElasticsearchStatusException) unwrapped; if (ex.status().equals(RestStatus.TOO_MANY_REQUESTS)) { listener.onFailure(ex); } else { listener.onResponse(new WarningInferenceResults(ex.getMessage())); } } else { listener.onResponse(new WarningInferenceResults(e.getMessage())); } }) ); }	parenttaskassigningclient calls request.setparenttaskid(taskid); exactly as you have done in line 193 you should either set request.setparenttaskid(taskid); yourself or use the parenttaskassigningclient not both. https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/client/parenttaskassigningclient.java#l54
private String opaqueId() { String opaqueId = threadContext.getHeader(Task.X_OPAQUE_ID); if (opaqueId != null) { return opaqueId; } else { return ""; } }	i think we can change this to return ", opaque_id=[" + opaqueid "]"; and in the log message , opaque_id=[{}] will just be {} since we always add this at the end and we keep consistency with the way we handle indices.
private String opaqueId() { String opaqueId = threadContext.getHeader(Task.X_OPAQUE_ID); if (opaqueId != null) { return opaqueId; } else { return ""; } }	generally we don't output empty brackets <field>=[], eg. indices, so opaque_id=[] is better to not being displayed at all.
private Query createTypeFilter(String[] types) { if (types != null && types.length >= 1) { if (Arrays.asList(types).contains(MapperService.SINGLE_MAPPING_NAME)) { // Typeless APIs use _doc as a type name internally, so we need // to make filtering on _type:_doc a no-op. return null; } MappedFieldType ft = mapperService().fullName(TypeFieldMapper.NAME); if (ft != null) { // ft might be null if no documents have been indexed yet return ft.termsQuery(Arrays.asList(types), queryShardContext); } } return null; }	i think this was added to deal with 'explain' requests, but for me this makes the overall search behavior a bit unintuitive. if a user explicitly adds a (deprecated) type filter on _doc in a search, i think they would expect the filter to be applied instead of being ignored. from a user's perspective the search apis differ from the document crud apis, where the _doc is instead seen as part of the endpoint (and not an explicit filter).
@Override public Settings threadPoolSettings() { return Settings.builder().put(super.threadPoolSettings()).put("thread_pool.estimated_time_interval", "5ms").build(); }	small comment: maybe this test and the next should have consistent names (like testtypelessdelete and testtypelessget)?
private void handlePrivsResponse(String username, DataFrameTransformConfig config, HasPrivilegesResponse privilegesResponse, ActionListener<AcknowledgedResponse> listener) throws IOException { if (privilegesResponse.isCompleteMatch()) { putDataFrame(config, listener); } else { List<String> indices = privilegesResponse.getIndexPrivileges() .stream() .map(ResourcePrivileges::getResource) .collect(Collectors.toList()); listener.onFailure(Exceptions.authorizationError( "Cannot create data frame transform [{}] because user {} lacks the all the required permissions for indices: [{}]", config.getId(), username, Strings.collectionToCommaDelimitedString(indices))); } }	1. ... lacks the all the required...: remove the first the 2. nit: no need to use strings.collectiontocommadelimitedstring(indices). you could just use indices as long as you remove the square brackets around the placeholder. a list's tostring method will comma-separate and wrap in square brackets on its own.
public void testGetSnapshots() { Map<String, String> expectedParams = new HashMap<>(); String repository = randomIndicesNames(1, 1)[0]; String snapshot1 = "snapshot1-" + randomAlphaOfLengthBetween(2, 5).toLowerCase(Locale.ROOT); String snapshot2 = "snapshot2-" + randomAlphaOfLengthBetween(2, 5).toLowerCase(Locale.ROOT); String endpoint = String.format(Locale.ROOT, "/_snapshot/%s/%s,%s", repository, snapshot1, snapshot2); GetSnapshotsRequest getSnapshotsRequest = new GetSnapshotsRequest(); getSnapshotsRequest.repository(repository); getSnapshotsRequest.snapshots(Arrays.asList(snapshot1, snapshot2).toArray(new String[0])); setRandomMasterTimeout(getSnapshotsRequest, expectedParams); if (randomBoolean()) { getSnapshotsRequest.ignoreUnavailable(true); expectedParams.put("ignore_unavailable", Boolean.TRUE.toString()); } if (randomBoolean() == false) { getSnapshotsRequest.verbose(false); expectedParams.put("verbose", Boolean.FALSE.toString()); } Request request = RequestConverters.getSnapshots(getSnapshotsRequest); assertThat(endpoint, equalTo(request.getEndpoint())); assertThat(HttpGet.METHOD_NAME, equalTo(request.getMethod())); assertThat(expectedParams, equalTo(request.getParameters())); assertNull(request.getEntity()); }	why not randomalphaoflength(5) or something?
public void testGetSnapshots() { Map<String, String> expectedParams = new HashMap<>(); String repository = randomIndicesNames(1, 1)[0]; String snapshot1 = "snapshot1-" + randomAlphaOfLengthBetween(2, 5).toLowerCase(Locale.ROOT); String snapshot2 = "snapshot2-" + randomAlphaOfLengthBetween(2, 5).toLowerCase(Locale.ROOT); String endpoint = String.format(Locale.ROOT, "/_snapshot/%s/%s,%s", repository, snapshot1, snapshot2); GetSnapshotsRequest getSnapshotsRequest = new GetSnapshotsRequest(); getSnapshotsRequest.repository(repository); getSnapshotsRequest.snapshots(Arrays.asList(snapshot1, snapshot2).toArray(new String[0])); setRandomMasterTimeout(getSnapshotsRequest, expectedParams); if (randomBoolean()) { getSnapshotsRequest.ignoreUnavailable(true); expectedParams.put("ignore_unavailable", Boolean.TRUE.toString()); } if (randomBoolean() == false) { getSnapshotsRequest.verbose(false); expectedParams.put("verbose", Boolean.FALSE.toString()); } Request request = RequestConverters.getSnapshots(getSnapshotsRequest); assertThat(endpoint, equalTo(request.getEndpoint())); assertThat(HttpGet.METHOD_NAME, equalTo(request.getMethod())); assertThat(expectedParams, equalTo(request.getParameters())); assertNull(request.getEntity()); }	i don't know that == false is worth it here.
public void testGetSnapshots() throws IOException { String repository = "test_repository"; String snapshot1 = "test_snapshot1"; String snapshot2 = "test_snapshot2"; PutRepositoryResponse putRepositoryResponse = createTestRepository(repository, FsRepository.TYPE, "{\\\\"location\\\\": \\\\".\\\\"}"); assertTrue(putRepositoryResponse.isAcknowledged()); Response putSnapshotResponse1 = createTestSnapshot(repository, snapshot1); Response putSnapshotResponse2 = createTestSnapshot(repository, snapshot2); // check that the request went ok without parsing JSON here. When using the high level client, check acknowledgement instead. assertEquals(200, putSnapshotResponse1.getStatusLine().getStatusCode()); assertEquals(200, putSnapshotResponse2.getStatusLine().getStatusCode()); GetSnapshotsRequest request; if (randomBoolean()) { request = new GetSnapshotsRequest(repository); } else if (randomBoolean()) { request = new GetSnapshotsRequest(repository, Collections.singletonList("_all").toArray(new String[0])); } else { request = new GetSnapshotsRequest(repository, Arrays.asList(snapshot1, snapshot2).toArray(new String[0])); } GetSnapshotsResponse response = execute(request, highLevelClient().snapshot()::get, highLevelClient().snapshot()::getAsync); assertEquals(2, response.getSnapshots().size()); assertThat(response.getSnapshots().stream().map((s) -> s.snapshotId().getName()).collect(Collectors.toList()), contains("test_snapshot1", "test_snapshot2")); }	why not new string[] {"_all"}?
public void testGetSnapshots() throws IOException { String repository = "test_repository"; String snapshot1 = "test_snapshot1"; String snapshot2 = "test_snapshot2"; PutRepositoryResponse putRepositoryResponse = createTestRepository(repository, FsRepository.TYPE, "{\\\\"location\\\\": \\\\".\\\\"}"); assertTrue(putRepositoryResponse.isAcknowledged()); Response putSnapshotResponse1 = createTestSnapshot(repository, snapshot1); Response putSnapshotResponse2 = createTestSnapshot(repository, snapshot2); // check that the request went ok without parsing JSON here. When using the high level client, check acknowledgement instead. assertEquals(200, putSnapshotResponse1.getStatusLine().getStatusCode()); assertEquals(200, putSnapshotResponse2.getStatusLine().getStatusCode()); GetSnapshotsRequest request; if (randomBoolean()) { request = new GetSnapshotsRequest(repository); } else if (randomBoolean()) { request = new GetSnapshotsRequest(repository, Collections.singletonList("_all").toArray(new String[0])); } else { request = new GetSnapshotsRequest(repository, Arrays.asList(snapshot1, snapshot2).toArray(new String[0])); } GetSnapshotsResponse response = execute(request, highLevelClient().snapshot()::get, highLevelClient().snapshot()::getAsync); assertEquals(2, response.getSnapshots().size()); assertThat(response.getSnapshots().stream().map((s) -> s.snapshotId().getName()).collect(Collectors.toList()), contains("test_snapshot1", "test_snapshot2")); }	why not new string[] {snapshot1, snapshot2}?
public static SnapshotInfo fromXContent(final XContentParser parser) throws IOException { String name = null; String uuid = null; Version version = Version.CURRENT; SnapshotState state = SnapshotState.IN_PROGRESS; String reason = null; List<String> indices = Collections.emptyList(); long startTime = 0; long endTime = 0; int totalShards = 0; int successfulShards = 0; Boolean includeGlobalState = null; List<SnapshotShardFailure> shardFailures = Collections.emptyList(); if (parser.currentToken() == null) { // fresh parser? move to the first token parser.nextToken(); } XContentParser.Token token = parser.currentToken(); ensureExpectedToken(XContentParser.Token.START_OBJECT, token, parser::getTokenLocation); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { String currentFieldName = parser.currentName(); token = parser.nextToken(); if (token.isValue()) { if (SNAPSHOT.equals(currentFieldName)) { name = parser.text(); } else if (UUID.equals(currentFieldName)) { uuid = parser.text(); } else if (VERSION_ID.equals(currentFieldName)) { version = Version.fromId(parser.intValue()); } else if (INCLUDE_GLOBAL_STATE.equals(currentFieldName)) { includeGlobalState = parser.booleanValue(); } else if (STATE.equals(currentFieldName)) { state = SnapshotState.valueOf(parser.text()); } else if (REASON.equals(currentFieldName)) { reason = parser.text(); } else if (START_TIME_IN_MILLIS.equals(currentFieldName)) { startTime = parser.longValue(); } else if (END_TIME_IN_MILLIS.equals(currentFieldName)) { endTime = parser.longValue(); } } else if (token == XContentParser.Token.START_ARRAY) { if (INDICES.equals(currentFieldName)) { ArrayList<String> indicesArray = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { indicesArray.add(parser.text()); } indices = Collections.unmodifiableList(indicesArray); } else if (FAILURES.equals(currentFieldName)) { ArrayList<SnapshotShardFailure> shardFailureArrayList = new ArrayList<>(); while (parser.nextToken() != XContentParser.Token.END_ARRAY) { shardFailureArrayList.add(SnapshotShardFailure.fromXContent(parser)); } shardFailures = Collections.unmodifiableList(shardFailureArrayList); } else { // It was probably created by newer version - ignoring parser.skipChildren(); } } else if (token == XContentParser.Token.START_OBJECT) { if (SHARDS.equals(currentFieldName)) { while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); token = parser.nextToken(); if (token.isValue()) { if (TOTAL.equals(currentFieldName)) { totalShards = parser.intValue(); } else if (SUCCESSFUL.equals(currentFieldName)) { successfulShards = parser.intValue(); } } } } } else { // It was probably created by newer version - ignoring parser.skipChildren(); } } } } if (uuid == null) { // the old format where there wasn't a UUID uuid = name; } return new SnapshotInfo(new SnapshotId(name, uuid), indices, state, reason, version, startTime, endTime, totalShards, successfulShards, shardFailures, includeGlobalState); }	i think this one is a fine candidate for objectparser.
public void test82CustomJvmOptionsDirectoryFile() throws Exception { final Path heapOptions = installation.config(Paths.get("jvm.options.d", "heap.options")); try { append(heapOptions, "-Xms512m\\\\n-Xmx512m\\\\n"); startElasticsearch(); final String nodesResponse = makeRequest(Request.Get("http://localhost:9200/_nodes")); assertThat(nodesResponse, CoreMatchers.containsString("\\\\"heap_init_in_bytes\\\\":536870912")); stopElasticsearch(); } finally { rm(heapOptions); } }	nit - i'd use a static import of matchers, personally.
private static byte safeToByte(Number n) throws SQLException { if (n instanceof BigInteger) { try { return ((BigInteger) n).byteValueExact(); } catch (ArithmeticException ae) { throw new SQLException(format(Locale.ROOT, "Numeric %s out of range", n)); } } long x = n.longValue(); if (x > Byte.MAX_VALUE || x < Byte.MIN_VALUE) { throw new SQLException(format(Locale.ROOT, "Numeric %s out of range", n)); } return (byte) x; }	i don't think this is correct - %s means the formatting is that of a string, for numbers it should be %d. maybe the current behavior works however i would argue the changes don't improve the situation, rather the opposite.
private static String resolveColumnType(String type) { switch (type.toLowerCase(Locale.ROOT)) { case "s": return "string"; case "b": return "boolean"; case "i": return "integer"; case "l": return "long"; case "f": return "float"; case "d": return "double"; case "ts": return "timestamp"; case "bt": return "byte"; case "sh": return "short"; case "ul": return "bigdecimal"; // CSV default: return type; } }	should it be bigdecimal or biginteger / unsigned long?
private static void checkClientSupportsDataTypes(LogicalPlan p, Set<Failure> localFailures, SqlVersion version) { p.output().forEach(e -> { if (e.resolved() && isTypeSupportedInVersion(e.dataType(), version) == false) { localFailures.add( fail( e, "Cannot use field [" + e.name() + "] with type [" + e.dataType() + "] unsupported in version [" + version + "], upgrade required (to version [" + INTRODUCING_UNSIGNED_LONG + "] or higher)" ) ); } }); }	this fixes the version to introducing_unsigned_long but istypesupportedinversion is generic. when adding more types this might lead to subtle errors where the wrong required version is suggested. maybe istypesupportedinversion could return the required version?
@Override public DataType dataType() { DataType dt = field().dataType(); return dt.isInteger() ? (dt == UNSIGNED_LONG ? UNSIGNED_LONG : LONG) : DOUBLE; }	i think the result type should be double for ulong. at least that's how es behaves: given a mapping like { "mappings": { "properties": { "l": { "type": "unsigned_long" } } } } post http://localhost:9201/ulong/_search content-type: application/json { "size": 0, "aggs": { "avgl": {"sum":{"field": "l"}} } } retrieves { ... "aggregations": { "avgl": { "value": 1.8446744073709552e19, "value_as_string": "1.8446744073709552e19" } } } also, the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/unsigned-long.html#_aggregations) mention that aggs are always on the double type for unsigned_long.
@Override public Literal visitIntegerLiteral(IntegerLiteralContext ctx) { Tuple<Source, String> tuple = withMinus(ctx); Number value; try { value = StringUtils.parseIntegral(tuple.v2()); } catch (QlIllegalArgumentException siae) { throw new ParsingException(tuple.v1(), siae.getMessage()); } DataType type; if (value instanceof BigInteger) { type = DataTypes.UNSIGNED_LONG; } else { assert value instanceof Long : "Expected value [" + value + "] of type Long but got: " + value.getClass(); // try to downsize to int if possible (since that's the most common type) if (value.longValue() == value.intValue()) { type = DataTypes.INTEGER; value = Integer.valueOf(value.intValue()); } else { type = DataTypes.LONG; } } return new Literal(tuple.v1(), value, type); }	could parseintegral also directly return the best fitting number type? right now the downsizing from biginteger to long happens in parseintegral and from long to integer here and in the eql expressionbuilder
@Override public Literal visitIntegerLiteral(IntegerLiteralContext ctx) { Tuple<Source, String> tuple = withMinus(ctx); Number value; try { value = StringUtils.parseIntegral(tuple.v2()); } catch (QlIllegalArgumentException siae) { throw new ParsingException(tuple.v1(), siae.getMessage()); } DataType type; if (value instanceof BigInteger) { type = DataTypes.UNSIGNED_LONG; } else { assert value instanceof Long : "Expected value [" + value + "] of type Long but got: " + value.getClass(); // try to downsize to int if possible (since that's the most common type) if (value.longValue() == value.intValue()) { type = DataTypes.INTEGER; value = Integer.valueOf(value.intValue()); } else { type = DataTypes.LONG; } } return new Literal(tuple.v1(), value, type); }	assert should probably be replaced with an exception?
@Override public void execute(SqlSession session, ActionListener<Page> listener) { String cluster = session.indexResolver().clusterName(); String cat = hasText(catalog) ? catalog : session.configuration().catalog(); String idx = index != null ? index : (pattern != null ? pattern.asIndexNameWildcard() : "*"); idx = hasText(cat) && cat.equals(cluster) == false ? buildRemoteIndexName(cat, idx) : idx; boolean withFrozen = includeFrozen || session.configuration().includeFrozen(); session.indexResolver().resolveAsMergedMapping(idx, withFrozen, emptyMap(), ActionListener.wrap(indexResult -> { List<List<?>> rows = emptyList(); if (indexResult.isValid()) { rows = new ArrayList<>(); fillInRows(indexResult.get().mapping(), null, session.configuration().version(), rows); } listener.onResponse(of(session, rows)); }, listener::onFailure)); }	same question here: if we show unsupported fields, we should do the same for supported types when reporting to old clients.
public void testShowColumns() { String prefix = "myIndex"; List<List<?>> rows = new ArrayList<>(); ShowColumns.fillInRows(loadMapping("mapping-multi-field-variation.json", true), prefix, SqlVersion.fromId(CURRENT.id), rows); List<List<?>> expect = asList( asList("bool", JDBCType.BOOLEAN.getName(), BOOLEAN.typeName()), asList("int", JDBCType.INTEGER.getName(), INTEGER.typeName()), asList("unsigned_long", JDBCType.NUMERIC.getName(), UNSIGNED_LONG.typeName()), asList("float", JDBCType.REAL.getName(), FLOAT.typeName()), asList("text", JDBCType.VARCHAR.getName(), TEXT.typeName()), asList("keyword", JDBCType.VARCHAR.getName(), KEYWORD.typeName()), asList("date", JDBCType.TIMESTAMP.getName(), DATETIME.typeName()), asList("date_nanos", JDBCType.TIMESTAMP.getName(), DATETIME.typeName()), asList("unsupported", JDBCType.OTHER.getName(), UNSUPPORTED.typeName()), asList("some", JDBCType.STRUCT.getName(), OBJECT.typeName()), asList("some.dotted", JDBCType.STRUCT.getName(), OBJECT.typeName()), asList("some.dotted.field", JDBCType.VARCHAR.getName(), KEYWORD.typeName()), asList("some.string", JDBCType.VARCHAR.getName(), TEXT.typeName()), asList("some.string.normalized", JDBCType.VARCHAR.getName(), KEYWORD.typeName()), asList("some.string.typical", JDBCType.VARCHAR.getName(), KEYWORD.typeName()), asList("some.ambiguous", JDBCType.VARCHAR.getName(), TEXT.typeName()), asList("some.ambiguous.one", JDBCType.VARCHAR.getName(), KEYWORD.typeName()), asList("some.ambiguous.two", JDBCType.VARCHAR.getName(), KEYWORD.typeName()), asList("some.ambiguous.normalized", JDBCType.VARCHAR.getName(), KEYWORD.typeName()), asList("foo_type", JDBCType.OTHER.getName(), UNSUPPORTED.typeName()), asList("point", JDBC_TYPE_GEOMETRY, GEO_POINT.typeName()), asList("shape", JDBC_TYPE_GEOMETRY, GEO_SHAPE.typeName()), asList("nested", JDBCType.STRUCT.getName(), NESTED.typeName()), asList("nested.point", JDBC_TYPE_GEOMETRY, GEO_POINT.typeName()) ); assertEquals(expect.size(), rows.size()); assertEquals(expect.get(0).size(), rows.get(0).size()); for (int i = 0; i < expect.size(); i++) { List<?> expectedRow = expect.get(i); List<?> receivedRow = rows.get(i); assertEquals("Name mismatch in row " + i, prefix + "." + expectedRow.get(0), receivedRow.get(0)); assertEquals("Type mismatch in row " + i, expectedRow.get(1), receivedRow.get(1)); assertEquals("Mapping mismatch in row " + i, expectedRow.get(2), receivedRow.get(2)); } }	why do you loadmapping every time? i see this as unnecessary time-consuming operations.
public void testUnsignedLongFiltering() { List<SqlVersion> versions = List.of( SqlVersion.fromId(INTRODUCING_UNSIGNED_LONG.id - SqlVersion.MINOR_MULTIPLIER), INTRODUCING_UNSIGNED_LONG, SqlVersion.fromId(INTRODUCING_UNSIGNED_LONG.id + SqlVersion.MINOR_MULTIPLIER), SqlVersion.fromId(Version.CURRENT.id) ); for (Mode mode : List.of(Mode.JDBC, Mode.ODBC)) { for (SqlVersion version : versions) { List<List<?>> rows = new ArrayList<>(); SysColumns.fillInRows( "test", "index", loadMapping("mapping-multi-field-variation.json", true), null, rows, null, mode, version ); List<String> types = rows.stream().map(row -> name(row).toString()).collect(Collectors.toList()); assertEquals( isTypeSupportedInVersion(UNSIGNED_LONG, version), types.contains(UNSIGNED_LONG.toString().toLowerCase(Locale.ROOT)) ); } } }	this list is the same as https://github.com/elastic/elasticsearch/pull/65145/files#diff-8f9e4dcb53d1a3bb501882f5c67dfb19be7a0606fd27c2762060721d8d7fe2f3r108. worth extracting it to a common place?
public void testUnsignedLongFiltering() { List<SqlVersion> versions = List.of( SqlVersion.fromId(INTRODUCING_UNSIGNED_LONG.id - SqlVersion.MINOR_MULTIPLIER), INTRODUCING_UNSIGNED_LONG, SqlVersion.fromId(INTRODUCING_UNSIGNED_LONG.id + SqlVersion.MINOR_MULTIPLIER), SqlVersion.fromId(Version.CURRENT.id) ); for (Mode mode : List.of(Mode.JDBC, Mode.ODBC)) { for (SqlVersion version : versions) { List<List<?>> rows = new ArrayList<>(); SysColumns.fillInRows( "test", "index", loadMapping("mapping-multi-field-variation.json", true), null, rows, null, mode, version ); List<String> types = rows.stream().map(row -> name(row).toString()).collect(Collectors.toList()); assertEquals( isTypeSupportedInVersion(UNSIGNED_LONG, version), types.contains(UNSIGNED_LONG.toString().toLowerCase(Locale.ROOT)) ); } } }	same here about loading the same mapping file multiple times. isn't it enough to load the file once?
public void testUnsignedLongFiltering() { Set<SqlVersion> versions = new HashSet<>( List.of( SqlVersion.fromId(INTRODUCING_UNSIGNED_LONG.id - SqlVersion.MINOR_MULTIPLIER), INTRODUCING_UNSIGNED_LONG, SqlVersion.fromId(INTRODUCING_UNSIGNED_LONG.id + SqlVersion.MINOR_MULTIPLIER), SqlVersion.fromId(Version.CURRENT.id) ) ); versions.add(null); for (SqlVersion version : versions) { for (Mode mode : Mode.values()) { Tuple<Command, SqlSession> cmd = sql("SYS TYPES", mode, version); cmd.v1().execute(cmd.v2(), wrap(p -> { SchemaRowSet r = (SchemaRowSet) p.rowSet(); List<String> types = new ArrayList<>(); r.forEachRow(rv -> types.add((String) rv.column(0))); assertEquals( isTypeSupportedInVersion(UNSIGNED_LONG, cmd.v2().configuration().version()), types.contains(UNSIGNED_LONG.toString()) ); }, ex -> fail(ex.getMessage()))); } } }	same here: this list can be extracted in a common place and re-used.
* @param discoveryNodes */ public ShardIterator onlyNodeSelectorActiveInitializingShardsIt(String[] nodeAttribute, DiscoveryNodes discoveryNodes) { ArrayList<ShardRouting> ordered = new ArrayList<>(activeShards.size() + allInitializingShards.size()); Set<String> selectedNodes = Sets.newHashSet(discoveryNodes.resolveNodesIds(nodeAttribute)); int seed = shuffler.nextSeed(); for (ShardRouting shardRouting : shuffler.shuffle(activeShards,seed)) { if (selectedNodes.contains(shardRouting.currentNodeId())) { ordered.add(shardRouting); } } for (ShardRouting shardRouting : shuffler.shuffle(allInitializingShards,seed)) { if (selectedNodes.contains(shardRouting.currentNodeId())) { ordered.add(shardRouting); } } if (ordered.isEmpty()) { throw new ElasticsearchIllegalArgumentException("No data nodes with critera(s) [" + Strings.arrayToCommaDelimitedString(nodeAttribute) + "] found for shard:" + shardId()); } return new PlainShardIterator(shardId, ordered); }	do you think that nodeattributes would be a better name now?
@Test public void testNodeSelectorRouting(){ AllocationService strategy = createAllocationService(settingsBuilder() .put("cluster.routing.allocation.concurrent_recoveries", 10) .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always") .build()); MetaData metaData = MetaData.builder() .put(IndexMetaData.builder("test").numberOfShards(1).numberOfReplicas(1)) .build(); RoutingTable routingTable = RoutingTable.builder() .addAsNew(metaData.index("test")) .build(); ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build(); clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder() .put(newNode("fred","node1", ImmutableMap.of("disk", "ebs"))) .put(newNode("barney","node2", ImmutableMap.of("disk", "ephemeral"))) .localNodeId("node1") ).build(); routingTable = strategy.reroute(clusterState).routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); ShardsIterator shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt(new String[] {"disk:ebs"} ,clusterState.nodes()); assertThat(shardsIterator.size(), equalTo(1)); assertThat(shardsIterator.nextOrNull().currentNodeId(),equalTo("node1")); shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt(new String[] {"dis*:eph*"},clusterState.nodes()); assertThat(shardsIterator.size(), equalTo(1)); assertThat(shardsIterator.nextOrNull().currentNodeId(),equalTo("node2")); shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt(new String[] {"dis*:ebs*","di*:eph*"},clusterState.nodes()); assertThat(shardsIterator.size(), equalTo(2)); shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt(new String[] {"fred"} ,clusterState.nodes()); assertThat(shardsIterator.size(), equalTo(1)); assertThat(shardsIterator.nextOrNull().currentNodeId(),equalTo("node1")); shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt(new String[] {"bar*"},clusterState.nodes()); assertThat(shardsIterator.size(), equalTo(1)); assertThat(shardsIterator.nextOrNull().currentNodeId(),equalTo("node2")); try { shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt(new String[] {"welma"}, clusterState.nodes()); fail("shouldve raised illegalArgumentException"); } catch (ElasticsearchIllegalArgumentException illegal) { //expected exception } shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt(new String[] {"fred"},clusterState.nodes()); assertThat(shardsIterator.size(), equalTo(1)); assertThat(shardsIterator.nextOrNull().currentNodeId(),equalTo("node1")); }	should the fail message make it clear that we are now expecting an elasticsearchillegalargumentexception?
final void add(QueryToFilterAdapter<?> filter) throws IOException { if (valid == false) { return; } QueryToFilterAdapter<?> mergedFilter = filter.union(rewrittenTopLevelQuery); if (mergedFilter.isInefficientUnion()) { /* * For now any complex union kicks us out of filter by filter * mode. Its possible that this de-optimizes many "filters" * aggregations but likely correct when "range", "date_histogram", * or "terms" are converted to this agg. We investigated a sort * of "combined" iteration mechanism and its complex *and* slower * than the native implementations of the aggs above. */ valid = false; return; } if (filters.size() == 1) { /* * When we add the second filter we check if there are any _doc_count * fields and bail out of filter-by filter mode if there are. _doc_count * fields are expensive to decode and the overhead of iterating per * filter causes us to decode doc counts over and over again. */ Term term = new Term(DocCountFieldMapper.NAME, DocCountFieldMapper.NAME); for (LeafReaderContext c : context.searcher().getLeafContexts()) { if (c.reader().docFreq(term) > 0) { valid = false; return; } } } filters.add(mergedFilter); } /** * Build the the adapter or {@code null}	i wonder if this is worth moving up to the aggregationcontext. i feel like a hasdoccountfield() method might be useful beyond this, and i suspect it is intuitive to look for such a method on aggregationcontext, but less so to look for it here. easy enough to do in a follow up if we want to though.
public List<Route> routes() { return List.of( Route.builder(DELETE, BASE_PATH + "_delete_expired_data/{" + Job.ID + "}") .replaces(DELETE, PRE_V7_BASE_PATH + "_delete_expired_data/{" + Job.ID + "}", RestApiVersion.V_7).build(), Route.builder(DELETE, BASE_PATH + "_delete_expired_data") .replaces(DELETE, PRE_V7_BASE_PATH + "_delete_expired_data", RestApiVersion.V_7).build() ); }	should we add the pre_v7 path here? it wasn't added. until 7.x
public List<Route> routes() { return List.of( Route.builder(GET, BASE_PATH + "datafeeds/{" + DatafeedConfig.ID + "}/_preview") .replaces(GET, PRE_V7_BASE_PATH + "datafeeds/{" + DatafeedConfig.ID + "}/_preview", RestApiVersion.V_7).build(), Route.builder(GET, BASE_PATH + "datafeeds/_preview") .replaces(GET, PRE_V7_BASE_PATH + "datafeeds/_preview", RestApiVersion.V_7).build(), Route.builder(POST, BASE_PATH + "datafeeds/{" + DatafeedConfig.ID + "}/_preview") .replaces(POST, PRE_V7_BASE_PATH + "datafeeds/{" + DatafeedConfig.ID + "}/_preview", RestApiVersion.V_7).build(), Route.builder(POST, BASE_PATH + "datafeeds/_preview") .replaces(POST, PRE_V7_BASE_PATH + "datafeeds/_preview", RestApiVersion.V_7).build() ); }	these post urls were not added until 7.x. should we have bwc with pre v7?
public List<Route> routes() { final String msg = "Posting data directly to anomaly detection jobs is deprecated, " + "in a future major version it will be compulsory to use a datafeed"; return List.of( Route.builder(POST, BASE_PATH + "anomaly_detectors/{" + Job.ID + "}/_data").deprecated(msg, RestApiVersion.V_8).build(), Route.builder(POST, PRE_V7_BASE_PATH + "anomaly_detectors/{" + Job.ID + "}/_data").deprecated(msg, RestApiVersion.V_7).build() ); }	is there a way to do a replaces and have the additional deprecation message? it seems we are losing that the _xpack url is old.
private long toHistogramKeyToEpoch(Object key) { if (key instanceof DateTime) {//TODO what do we use as key in aggregations return ((DateTime)key).getMillis(); } else if (key instanceof Double) { return ((Double)key).longValue(); } else if (key instanceof Long){ return (Long)key; } else { throw new IllegalStateException("Histogram key [" + key + "] cannot be converted to a timestamp"); } }	is this a question for the ml folk?
protected long calculateDelay(long previousDelay) { return Math.min(previousDelay * 2, Integer.MAX_VALUE); }	could you please add some unit indication? like minimumdelaymillis()?
public BulkResponse bulkIndexWithHeadersWithRetry(Map<String, String> headers, BulkRequest bulkRequest, String jobId, Supplier<Boolean> shouldRetry, Consumer<String> retryMsgHandler) { return bulkIndexWithRetry(bulkRequest, jobId, shouldRetry, retryMsgHandler, (providedBulkRequest, listener) -> ClientHelper.executeWithHeadersAsync( headers, ClientHelper.ML_ORIGIN, client, BulkAction.INSTANCE, providedBulkRequest, listener)); }	good catch, it was sending the wrong bulkrequest before
private static Translog.Location locationToSync(Translog.Location current, Translog.Location next) { /* here we are moving forward in the translog with each operation. Under the hood this might * cross translog files which is ok since from the user perspective the translog is like a * tape where only the highest location needs to be fsynced in order to sync all previous * locations even though they are not in the same file. When the translog rolls over files * the previous file is fsynced on after closing if needed.*/ assert next != null : "next operation can't be null"; assert current == null || current.compareTo(next) < 0 : "translog locations are not increasing"; return next; } /** * Execute the given {@link IndexRequest} on a replica shard, throwing a * {@link RetryOnReplicaException}	a reason that i prefer the line-per-parameter form for methods definitions that spill over multiple lines is because had you maintained it in this case the diff would simply be: diff --- a/core/src/main/java/org/elasticsearch/action/bulk/transportshardbulkaction.java +++ b/core/src/main/java/org/elasticsearch/action/bulk/transportshardbulkaction.java @@ -531,6 +531,7 @@ public class transportshardbulkaction extends transportwriteaction<bulkshardrequ private static engine.indexresult executeindexrequestonreplica( docwriteresponse primaryresponse, indexrequest request, + long primaryterm, indexshard replica) throws ioexception { final engine.index operation; and git blame would trace that additional parameter back to this pr rather than the entire method declaration.
* name. * * @param operationPrimaryTerm the operation primary term * @param onPermitAcquired the listener for permit acquisition * @param executorOnDelay the name of the executor to invoke the listener on if permit acquisition is delayed */ public void acquireReplicaOperationPermit( final long operationPrimaryTerm, final ActionListener<Releasable> onPermitAcquired, final String executorOnDelay) { verifyNotClosed(); verifyReplicationTarget(); if (operationPrimaryTerm > primaryTerm) { synchronized (primaryTermMutex) { if (operationPrimaryTerm > primaryTerm) { try { indexShardOperationPermits.blockOperations(30, TimeUnit.MINUTES, () -> { assert operationPrimaryTerm > primaryTerm : "shard term already update. op term [" + operationPrimaryTerm + "], shardTerm [" + primaryTerm + "]"; primaryTerm = operationPrimaryTerm; getEngine().getTranslog().rollGeneration(); }); } catch (final InterruptedException | TimeoutException | IOException | AlreadyClosedException e) { onPermitAcquired.onFailure(e); return; } } } } assert operationPrimaryTerm <= primaryTerm : "operation primary term [" + operationPrimaryTerm + "] should be at most [" + primaryTerm + "]"; indexShardOperationPermits.acquire( new ActionListener<Releasable>() { @Override public void onResponse(final Releasable releasable) { if (operationPrimaryTerm < primaryTerm) { releasable.close(); final String message = String.format( Locale.ROOT, "%s operation primary term [%d] is too old (current [%d])", shardId, operationPrimaryTerm, primaryTerm); onPermitAcquired.onFailure(new IllegalStateException(message)); } else { onPermitAcquired.onResponse(releasable); } } @Override public void onFailure(final Exception e) { onPermitAcquired.onFailure(e); } }, executorOnDelay, true); }	maybe just catch (exception)
* @param repository repository id * @param snapshots list of snapshots that will be used as a filter, empty list means no snapshots are filtered * @return list of metadata for currently running snapshots */ public static List<SnapshotsInProgress.Entry> currentSnapshots( @Nullable SnapshotsInProgress snapshotsInProgress, String repository, List<String> snapshots ) { if (snapshotsInProgress == null || snapshotsInProgress.isEmpty()) { return Collections.emptyList(); } if ("_all".equals(repository)) { return snapshotsInProgress.asStream().collect(Collectors.toUnmodifiableList()); } if (snapshots.isEmpty()) { return snapshotsInProgress.forRepo(repository); } List<SnapshotsInProgress.Entry> builder = new ArrayList<>(); for (SnapshotsInProgress.Entry entry : snapshotsInProgress.forRepo(repository)) { for (String snapshot : snapshots) { if (entry.snapshot().getSnapshotId().getName().equals(snapshot)) { builder.add(entry); break; } } } return unmodifiableList(builder); }	nit: now we can just do this, no need for the intermediate builder and wrapper: suggestion return list.of(entry);
* @param repository repository id * @param snapshots list of snapshots that will be used as a filter, empty list means no snapshots are filtered * @return list of metadata for currently running snapshots */ public static List<SnapshotsInProgress.Entry> currentSnapshots( @Nullable SnapshotsInProgress snapshotsInProgress, String repository, List<String> snapshots ) { if (snapshotsInProgress == null || snapshotsInProgress.isEmpty()) { return Collections.emptyList(); } if ("_all".equals(repository)) { return snapshotsInProgress.asStream().collect(Collectors.toUnmodifiableList()); } if (snapshots.isEmpty()) { return snapshotsInProgress.forRepo(repository); } List<SnapshotsInProgress.Entry> builder = new ArrayList<>(); for (SnapshotsInProgress.Entry entry : snapshotsInProgress.forRepo(repository)) { for (String snapshot : snapshots) { if (entry.snapshot().getSnapshotId().getName().equals(snapshot)) { builder.add(entry); break; } } } return unmodifiableList(builder); }	likewise: suggestion return list.of();
public Map<String, Processor.Factory> getProcessors(Processor.Parameters parameters) { return Collections.singletonMap(InferenceProcessor.TYPE, new InferenceProcessor.Factory()); }	a secondary idea is that we don't have a single inference processor. instead, we have an individual processor for each supported model type. this may make sense, depending on how the models will be gathered and cached.
ClusterState improveConfiguration(ClusterState clusterState) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; final Set<DiscoveryNode> liveNodes = StreamSupport.stream(clusterState.nodes().spliterator(), false) .filter(DiscoveryNode::isMasterNode).filter(this::hasJoinVoteFrom).collect(Collectors.toSet()); final VotingConfiguration newConfig = reconfigurator.reconfigure(liveNodes, Stream.concat( clusterState.getVotingConfigExclusions().stream().map(VotingConfigExclusion::getNodeId), StreamSupport.stream(clusterState.nodes().spliterator(), false) .filter(Predicate.not(DiscoveryNode::isMasterNode)).map(DiscoveryNode::getId)).collect(Collectors.toSet()), getLocalNode(), clusterState.getLastAcceptedConfiguration()); if (newConfig.equals(clusterState.getLastAcceptedConfiguration()) == false) { assert coordinationState.get().joinVotesHaveQuorumFor(newConfig); return ClusterState.builder(clusterState).metaData(MetaData.builder(clusterState.metaData()) .coordinationMetaData(CoordinationMetaData.builder(clusterState.coordinationMetaData()) .lastAcceptedConfiguration(newConfig).build())).build(); } return clusterState; }	is this change still necessary? hasjoinvotefrom will only return true for a master node.
ClusterState improveConfiguration(ClusterState clusterState) { assert Thread.holdsLock(mutex) : "Coordinator mutex not held"; final Set<DiscoveryNode> liveNodes = StreamSupport.stream(clusterState.nodes().spliterator(), false) .filter(DiscoveryNode::isMasterNode).filter(this::hasJoinVoteFrom).collect(Collectors.toSet()); final VotingConfiguration newConfig = reconfigurator.reconfigure(liveNodes, Stream.concat( clusterState.getVotingConfigExclusions().stream().map(VotingConfigExclusion::getNodeId), StreamSupport.stream(clusterState.nodes().spliterator(), false) .filter(Predicate.not(DiscoveryNode::isMasterNode)).map(DiscoveryNode::getId)).collect(Collectors.toSet()), getLocalNode(), clusterState.getLastAcceptedConfiguration()); if (newConfig.equals(clusterState.getLastAcceptedConfiguration()) == false) { assert coordinationState.get().joinVotesHaveQuorumFor(newConfig); return ClusterState.builder(clusterState).metaData(MetaData.builder(clusterState.metaData()) .coordinationMetaData(CoordinationMetaData.builder(clusterState.coordinationMetaData()) .lastAcceptedConfiguration(newConfig).build())).build(); } return clusterState; }	can you add a comment to say "auto-exclude non master-eligible nodes that are in the voting config" and only set those that are actually in the config? this will make the logging less confusing in reconfigurator.
@Override public void onResponse(PublishWithJoinResponse response) { if (isFailed()) { logger.debug("PublishResponseHandler.handleResponse: already failed, ignoring response from [{}]", discoveryNode); assert publicationCompletedIffAllTargetsInactiveOrCancelled(); return; } if (response.getJoin().isPresent()) { final Join join = response.getJoin().get(); assert discoveryNode.equals(join.getSourceNode()); assert join.getTerm() == response.getPublishResponse().getTerm() : response; logger.trace("handling join within publish response: {}", join); onJoin(join); } else { logger.trace("publish response from {} contained no join", discoveryNode); onMissingJoin(discoveryNode); } assert state == PublicationTargetState.SENT_PUBLISH_REQUEST : state + " -> " + PublicationTargetState.WAITING_FOR_QUORUM; state = PublicationTargetState.WAITING_FOR_QUORUM; try { handlePublishResponse(response.getPublishResponse()); } catch (Exception e) { setFailed(e); onPossibleCommitFailure(); } finally { assert publicationCompletedIffAllTargetsInactiveOrCancelled(); } }	should we move this exception handling to the handlepublishresponse method? it should only wrap the publication.this.handlepublishresponse part
ClusterNode restartedNode(Function<MetaData, MetaData> adaptGlobalMetaData, Function<Long, Long> adaptCurrentTerm, Settings nodeSettings) { final TransportAddress address = randomBoolean() ? buildNewFakeTransportAddress() : localNode.getAddress(); final DiscoveryNode newLocalNode = new DiscoveryNode(localNode.getName(), localNode.getId(), UUIDs.randomBase64UUID(random()), // generated deterministically for repeatable tests address.address().getHostString(), address.getAddress(), address, Collections.emptyMap(), localNode.isMasterNode() && Node.NODE_MASTER_SETTING.get(nodeSettings) ? DiscoveryNodeRole.BUILT_IN_ROLES : emptySet(), Version.CURRENT); return new ClusterNode(nodeIndex, newLocalNode, node -> new MockPersistedState(newLocalNode, persistedState, adaptGlobalMetaData, adaptCurrentTerm), nodeSettings); }	i think we should make it so that the settings always properly reflect the roles and use discoverynode.getrolesfromsettings(nodesettings) here.
public <Request extends ActionRequest, Response extends ActionResponse> Task registerAndExecute(String type, TransportAction<Request, Response> action, Request request, BiConsumer<Task, Response> onResponse, BiConsumer<Task, Exception> onFailure) { Task task = register(type, action.actionName, request); // NOTE: ActionListener cannot infer Response, see https://bugs.openjdk.java.net/browse/JDK-8203195 action.execute(task, request, new ActionListener<Response>() { @Override public void onResponse(Response response) { try { unregister(task); } finally { onResponse.accept(task, response); } } @Override public void onFailure(Exception e) { try { unregister(task); } finally { onFailure.accept(task, e); } } }); return task; }	i agree that placement of this method in transportaction might not be ideal, but i feel like taskmanager shouldn't be in the business of executing actions, it is busy enough registering and unregistering things. would it make more sense to just place this code into nodeclient or transportservice?
private void registerAggregations(List<SearchPlugin> plugins) { registerAggregation(new AggregationSpec(AvgAggregationBuilder.NAME, AvgAggregationBuilder::new, AvgAggregationBuilder::parse) .addResultReader(InternalAvg::new)); registerAggregation(new AggregationSpec(WeightedAvgAggregationBuilder.NAME, WeightedAvgAggregationBuilder::new, WeightedAvgAggregationBuilder::parse).addResultReader(InternalWeightedAvg::new)); registerAggregation(new AggregationSpec(SumAggregationBuilder.NAME, SumAggregationBuilder::new, SumAggregationBuilder::parse) .addResultReader(InternalSum::new)); registerAggregation(new AggregationSpec(MinAggregationBuilder.NAME, MinAggregationBuilder::new, MinAggregationBuilder::parse) .addResultReader(InternalMin::new)); registerAggregation(new AggregationSpec(MaxAggregationBuilder.NAME, MaxAggregationBuilder::new, MaxAggregationBuilder::parse) .addResultReader(InternalMax::new)); registerAggregation(new AggregationSpec(StatsAggregationBuilder.NAME, StatsAggregationBuilder::new, StatsAggregationBuilder::parse) .addResultReader(InternalStats::new)); registerAggregation(new AggregationSpec(ExtendedStatsAggregationBuilder.NAME, ExtendedStatsAggregationBuilder::new, ExtendedStatsAggregationBuilder::parse).addResultReader(InternalExtendedStats::new)); registerAggregation(new AggregationSpec(ValueCountAggregationBuilder.NAME, ValueCountAggregationBuilder::new, ValueCountAggregationBuilder::parse).addResultReader(InternalValueCount::new)); registerAggregation(new AggregationSpec(PercentilesAggregationBuilder.NAME, PercentilesAggregationBuilder::new, PercentilesAggregationBuilder::parse) .addResultReader(InternalTDigestPercentiles.NAME, InternalTDigestPercentiles::new) .addResultReader(InternalHDRPercentiles.NAME, InternalHDRPercentiles::new) .setAggregatorRegistrar(PercentilesAggregationBuilder::registerAggregators)); registerAggregation(new AggregationSpec(PercentileRanksAggregationBuilder.NAME, PercentileRanksAggregationBuilder::new, PercentileRanksAggregationBuilder::parse) .addResultReader(InternalTDigestPercentileRanks.NAME, InternalTDigestPercentileRanks::new) .addResultReader(InternalHDRPercentileRanks.NAME, InternalHDRPercentileRanks::new)); registerAggregation(new AggregationSpec(MedianAbsoluteDeviationAggregationBuilder.NAME, MedianAbsoluteDeviationAggregationBuilder::new, MedianAbsoluteDeviationAggregationBuilder::parse) .addResultReader(InternalMedianAbsoluteDeviation::new)); registerAggregation(new AggregationSpec(CardinalityAggregationBuilder.NAME, CardinalityAggregationBuilder::new, CardinalityAggregationBuilder::parse).addResultReader(InternalCardinality::new) .setAggregatorRegistrar(CardinalityAggregationBuilder::registerAggregators)); registerAggregation(new AggregationSpec(GlobalAggregationBuilder.NAME, GlobalAggregationBuilder::new, GlobalAggregationBuilder::parse).addResultReader(InternalGlobal::new)); registerAggregation(new AggregationSpec(MissingAggregationBuilder.NAME, MissingAggregationBuilder::new, MissingAggregationBuilder::parse).addResultReader(InternalMissing::new)); registerAggregation(new AggregationSpec(FilterAggregationBuilder.NAME, FilterAggregationBuilder::new, FilterAggregationBuilder::parse).addResultReader(InternalFilter::new)); registerAggregation(new AggregationSpec(FiltersAggregationBuilder.NAME, FiltersAggregationBuilder::new, FiltersAggregationBuilder::parse).addResultReader(InternalFilters::new)); registerAggregation(new AggregationSpec(AdjacencyMatrixAggregationBuilder.NAME, AdjacencyMatrixAggregationBuilder::new, AdjacencyMatrixAggregationBuilder::parse).addResultReader(InternalAdjacencyMatrix::new)); registerAggregation(new AggregationSpec(SamplerAggregationBuilder.NAME, SamplerAggregationBuilder::new, SamplerAggregationBuilder::parse) .addResultReader(InternalSampler.NAME, InternalSampler::new) .addResultReader(UnmappedSampler.NAME, UnmappedSampler::new)); registerAggregation(new AggregationSpec(DiversifiedAggregationBuilder.NAME, DiversifiedAggregationBuilder::new, DiversifiedAggregationBuilder::parse) /* Reuses result readers from SamplerAggregator*/); registerAggregation(new AggregationSpec(TermsAggregationBuilder.NAME, TermsAggregationBuilder::new, TermsAggregationBuilder::parse) .addResultReader(StringTerms.NAME, StringTerms::new) .addResultReader(UnmappedTerms.NAME, UnmappedTerms::new) .addResultReader(LongTerms.NAME, LongTerms::new) .addResultReader(DoubleTerms.NAME, DoubleTerms::new) .setAggregatorRegistrar(TermsAggregationBuilder::registerAggregators)); registerAggregation(new AggregationSpec(RareTermsAggregationBuilder.NAME, RareTermsAggregationBuilder::new, RareTermsAggregationBuilder::parse) .addResultReader(StringRareTerms.NAME, StringRareTerms::new) .addResultReader(UnmappedRareTerms.NAME, UnmappedRareTerms::new) .addResultReader(LongRareTerms.NAME, LongRareTerms::new) .setAggregatorRegistrar(RareTermsAggregationBuilder::registerAggregators)); registerAggregation(new AggregationSpec(SignificantTermsAggregationBuilder.NAME, SignificantTermsAggregationBuilder::new, SignificantTermsAggregationBuilder::parse) .addResultReader(SignificantStringTerms.NAME, SignificantStringTerms::new) .addResultReader(SignificantLongTerms.NAME, SignificantLongTerms::new) .addResultReader(UnmappedSignificantTerms.NAME, UnmappedSignificantTerms::new)); registerAggregation(new AggregationSpec(SignificantTextAggregationBuilder.NAME, SignificantTextAggregationBuilder::new, SignificantTextAggregationBuilder::parse)); registerAggregation(new AggregationSpec(RangeAggregationBuilder.NAME, RangeAggregationBuilder::new, RangeAggregationBuilder::parse).addResultReader(InternalRange::new)); registerAggregation(new AggregationSpec(DateRangeAggregationBuilder.NAME, DateRangeAggregationBuilder::new, DateRangeAggregationBuilder::parse).addResultReader(InternalDateRange::new)); registerAggregation(new AggregationSpec(IpRangeAggregationBuilder.NAME, IpRangeAggregationBuilder::new, IpRangeAggregationBuilder::parse).addResultReader(InternalBinaryRange::new)); registerAggregation(new AggregationSpec(HistogramAggregationBuilder.NAME, HistogramAggregationBuilder::new, HistogramAggregationBuilder::parse).addResultReader(InternalHistogram::new) .setAggregatorRegistrar(HistogramAggregationBuilder::registerAggregators)); registerAggregation(new AggregationSpec(DateHistogramAggregationBuilder.NAME, DateHistogramAggregationBuilder::new, DateHistogramAggregationBuilder::parse).addResultReader(InternalDateHistogram::new)); registerAggregation(new AggregationSpec(AutoDateHistogramAggregationBuilder.NAME, AutoDateHistogramAggregationBuilder::new, AutoDateHistogramAggregationBuilder::parse).addResultReader(InternalAutoDateHistogram::new)); registerAggregation(new AggregationSpec(GeoDistanceAggregationBuilder.NAME, GeoDistanceAggregationBuilder::new, GeoDistanceAggregationBuilder::parse).addResultReader(InternalGeoDistance::new)); registerAggregation(new AggregationSpec(GeoHashGridAggregationBuilder.NAME, GeoHashGridAggregationBuilder::new, GeoHashGridAggregationBuilder::parse).addResultReader(InternalGeoHashGrid::new)); registerAggregation(new AggregationSpec(GeoTileGridAggregationBuilder.NAME, GeoTileGridAggregationBuilder::new, GeoTileGridAggregationBuilder::parse).addResultReader(InternalGeoTileGrid::new)); registerAggregation(new AggregationSpec(NestedAggregationBuilder.NAME, NestedAggregationBuilder::new, NestedAggregationBuilder::parse).addResultReader(InternalNested::new)); registerAggregation(new AggregationSpec(ReverseNestedAggregationBuilder.NAME, ReverseNestedAggregationBuilder::new, ReverseNestedAggregationBuilder::parse).addResultReader(InternalReverseNested::new)); registerAggregation(new AggregationSpec(TopHitsAggregationBuilder.NAME, TopHitsAggregationBuilder::new, TopHitsAggregationBuilder::parse).addResultReader(InternalTopHits::new)); registerAggregation(new AggregationSpec(GeoBoundsAggregationBuilder.NAME, GeoBoundsAggregationBuilder::new, GeoBoundsAggregationBuilder::parse).addResultReader(InternalGeoBounds::new)); registerAggregation(new AggregationSpec(GeoCentroidAggregationBuilder.NAME, GeoCentroidAggregationBuilder::new, GeoCentroidAggregationBuilder::parse).addResultReader(InternalGeoCentroid::new)); registerAggregation(new AggregationSpec(ScriptedMetricAggregationBuilder.NAME, ScriptedMetricAggregationBuilder::new, ScriptedMetricAggregationBuilder.PARSER).addResultReader(InternalScriptedMetric::new)); registerAggregation((new AggregationSpec(CompositeAggregationBuilder.NAME, CompositeAggregationBuilder::new, CompositeAggregationBuilder.PARSER).addResultReader(InternalComposite::new))); registerFromPlugin(plugins, SearchPlugin::getAggregations, this::registerAggregation); }	nit - align indentation please.
public ShardId getShardId() { return shardId; } } public static class NodeGatewayStartedShards extends BaseNodeResponse { private String allocationId = null; private boolean primary = false; private Exception storeException = null; public NodeGatewayStartedShards() { } public NodeGatewayStartedShards(DiscoveryNode node, String allocationId, boolean primary) { this(node, allocationId, primary, null); } public NodeGatewayStartedShards(DiscoveryNode node, String allocationId, boolean primary, Exception storeException) { super(node); this.allocationId = allocationId; this.primary = primary; this.storeException = storeException; } public String allocationId() { return this.allocationId; } public boolean primary() { return this.primary; } public Exception storeException() { return this.storeException; } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); if (in.getVersion().before(Version.V_6_0_0_alpha1_UNRELEASED)) { // legacy version in.readLong(); } allocationId = in.readOptionalString(); primary = in.readBoolean(); if (in.readBoolean()) { storeException = in.readException(); } } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); if (out.getVersion().before(Version.V_6_0_0_alpha1_UNRELEASED)) { // legacy version out.writeLong(-1L); } out.writeOptionalString(allocationId); out.writeBoolean(primary); if (storeException != null) { out.writeBoolean(true); out.writeException(storeException); } else { out.writeBoolean(false); } } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } NodeGatewayStartedShards that = (NodeGatewayStartedShards) o; if (primary != that.primary) { return false; } if (allocationId != null ? !allocationId.equals(that.allocationId) : that.allocationId != null) { return false; } return storeException != null ? storeException.equals(that.storeException) : that.storeException == null; } @Override public int hashCode() { int result = (allocationId != null ? allocationId.hashCode() : 0); result = 31 * result + (primary ? 1 : 0); result = 31 * result + (storeException != null ? storeException.hashCode() : 0); return result; } @Override public String toString() { StringBuilder buf = new StringBuilder(); buf.append("NodeGatewayStartedShards[") .append("allocationId=").append(allocationId) .append(",primary=").append(primary); if (storeException != null) { buf.append(",storeException=").append(storeException); } buf.append("]"); return buf.toString(); }	the allocation id now should only be null if there is a storeexception, correct? perhaps an assert to that effect would be useful here.
public IdsQueryBuilder addIds(List<String> ids) { values.addAll(ids); return this; }	minor nitpick: maybe list could be a collection instead?
public void testValidTile() { int precision = randomIntBetween(0, GeoTileUtils.MAX_ZOOM); int tiles = 1 << precision; int x = randomIntBetween(0, tiles - 1); int y = randomIntBetween(0, tiles - 1); Rectangle rectangle = GeoTileUtils.toBoundingBox(x, y, precision); GeoBoundingBox bbox = new GeoBoundingBox( new GeoPoint(rectangle.getMaxLat(), rectangle.getMinLon()), new GeoPoint(rectangle.getMinLat(), rectangle.getMaxLon()) ); long hash = GeoTileUtils.longEncodeTiles(precision, x, y); GeoTileBoundedPredicate predicate = new GeoTileBoundedPredicate(precision, bbox); int minX = GeoTileUtils.getXTile(bbox.left(), tiles); int maxX = GeoTileUtils.getXTile(bbox.right(), tiles); int minY = GeoTileUtils.getYTile(bbox.bottom(), tiles); int maxY = GeoTileUtils.getYTile(bbox.top(), tiles); assertPredicates(hash, predicate, minX, minY, precision); assertPredicates(hash, predicate, maxX, minY, precision); assertPredicates(hash, predicate, minX, maxY, precision); assertPredicates(hash, predicate, maxX, maxY, precision); for (int i = 0; i < 1000; i++) { assertPredicates(hash, predicate, randomIntBetween(0, (1 << i) - 1), randomIntBetween(0, (1 << i) - 1), precision); } }	i ran this test and found all four corners tested here result in the predicate returning false? somehow i was expecting at least one of the corners to return true based on the min/max comment in the predicate.
public void testValidTile() { int precision = randomIntBetween(0, GeoTileUtils.MAX_ZOOM); int tiles = 1 << precision; int x = randomIntBetween(0, tiles - 1); int y = randomIntBetween(0, tiles - 1); Rectangle rectangle = GeoTileUtils.toBoundingBox(x, y, precision); GeoBoundingBox bbox = new GeoBoundingBox( new GeoPoint(rectangle.getMaxLat(), rectangle.getMinLon()), new GeoPoint(rectangle.getMinLat(), rectangle.getMaxLon()) ); long hash = GeoTileUtils.longEncodeTiles(precision, x, y); GeoTileBoundedPredicate predicate = new GeoTileBoundedPredicate(precision, bbox); int minX = GeoTileUtils.getXTile(bbox.left(), tiles); int maxX = GeoTileUtils.getXTile(bbox.right(), tiles); int minY = GeoTileUtils.getYTile(bbox.bottom(), tiles); int maxY = GeoTileUtils.getYTile(bbox.top(), tiles); assertPredicates(hash, predicate, minX, minY, precision); assertPredicates(hash, predicate, maxX, minY, precision); assertPredicates(hash, predicate, minX, maxY, precision); assertPredicates(hash, predicate, maxX, maxY, precision); for (int i = 0; i < 1000; i++) { assertPredicates(hash, predicate, randomIntBetween(0, (1 << i) - 1), randomIntBetween(0, (1 << i) - 1), precision); } }	when running a test, only 2 out of the 1000 tiles matched the predicate. i worry that with tests that only assert that two similar calculations give the same result, we are not anchoring onto any known true or false results.
public void testApiKeyServiceDisabled() throws Exception { final Settings settings = Settings.builder().put(XPackSettings.API_KEY_SERVICE_ENABLED_SETTING.getKey(), false).build(); final ApiKeyService service = createApiKeyService(settings); ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> service.getApiKeys(randomAlphaOfLength(6), randomAlphaOfLength(8), null, null, new PlainActionFuture<>())); assertThat(e, instanceOf(FeatureNotEnabledException.class)); assertThat(e, throwableWithMessage("api keys are not enabled")); assertThat(e.getMetadata(FeatureNotEnabledException.DISABLED_FEATURE_METADATA), contains("api_keys")); }	nit: maybe replace "api_keys" string literal with api_key_service.getfeaturename()(this needs adding the getfeaturename method as well). i generaly prefer to have less string literals.
public void testTokenServiceDisabled() throws Exception { TokenService tokenService = new TokenService(Settings.builder() .put(XPackSettings.TOKEN_SERVICE_ENABLED_SETTING.getKey(), false) .build(), Clock.systemUTC(), client, licenseState, securityContext, securityMainIndex, securityTokensIndex, clusterService); ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> tokenService.createOAuth2Tokens(null, null, null, true, null)); assertThat(e, throwableWithMessage("security tokens are not enabled")); assertThat(e, instanceOf(FeatureNotEnabledException.class)); assertThat(e.getMetadata(FeatureNotEnabledException.DISABLED_FEATURE_METADATA), contains("security_tokens")); PlainActionFuture<UserToken> future = new PlainActionFuture<>(); tokenService.getAndValidateToken(null, future); assertNull(future.get()); PlainActionFuture<TokensInvalidationResult> invalidateFuture = new PlainActionFuture<>(); e = expectThrows(ElasticsearchException.class, () -> tokenService.invalidateAccessToken((String) null, invalidateFuture)); assertThat(e, throwableWithMessage("security tokens are not enabled")); assertThat(e, instanceOf(FeatureNotEnabledException.class)); assertThat(e.getMetadata(FeatureNotEnabledException.DISABLED_FEATURE_METADATA), contains("security_tokens")); }	similar nitpick for "security_tokens" as above.
public void testTokenServiceDisabled() throws Exception { TokenService tokenService = new TokenService(Settings.builder() .put(XPackSettings.TOKEN_SERVICE_ENABLED_SETTING.getKey(), false) .build(), Clock.systemUTC(), client, licenseState, securityContext, securityMainIndex, securityTokensIndex, clusterService); ElasticsearchException e = expectThrows(ElasticsearchException.class, () -> tokenService.createOAuth2Tokens(null, null, null, true, null)); assertThat(e, throwableWithMessage("security tokens are not enabled")); assertThat(e, instanceOf(FeatureNotEnabledException.class)); assertThat(e.getMetadata(FeatureNotEnabledException.DISABLED_FEATURE_METADATA), contains("security_tokens")); PlainActionFuture<UserToken> future = new PlainActionFuture<>(); tokenService.getAndValidateToken(null, future); assertNull(future.get()); PlainActionFuture<TokensInvalidationResult> invalidateFuture = new PlainActionFuture<>(); e = expectThrows(ElasticsearchException.class, () -> tokenService.invalidateAccessToken((String) null, invalidateFuture)); assertThat(e, throwableWithMessage("security tokens are not enabled")); assertThat(e, instanceOf(FeatureNotEnabledException.class)); assertThat(e.getMetadata(FeatureNotEnabledException.DISABLED_FEATURE_METADATA), contains("security_tokens")); }	again same nitpick here as well for "security_token".
public void resync(final IndexShard indexShard, final ActionListener<ResyncTask> listener) { ActionListener<ResyncTask> resyncListener = null; try { final long startingSeqNo = indexShard.getGlobalCheckpoint() + 1; final SeqNoStats seqNoStats = indexShard.seqNoStats(); final long maxSeqNo = seqNoStats != null ? seqNoStats.getMaxSeqNo() : SequenceNumbers.UNASSIGNED_SEQ_NO; Translog.Snapshot snapshot = indexShard.newTranslogSnapshotFromMinSeqNo(startingSeqNo); resyncListener = new ActionListener<ResyncTask>() { @Override public void onResponse(final ResyncTask resyncTask) { try { snapshot.close(); listener.onResponse(resyncTask); } catch (final Exception e) { onFailure(e); } } @Override public void onFailure(final Exception e) { try { snapshot.close(); } catch (final Exception inner) { e.addSuppressed(inner); } finally { listener.onFailure(e); } } }; ShardId shardId = indexShard.shardId(); // Wrap translog snapshot to make it synchronized as it is accessed by different threads through SnapshotSender. // Even though those calls are not concurrent, snapshot.next() uses non-synchronized state and is not multi-thread-compatible // Also fail the resync early if the shard is shutting down Translog.Snapshot wrappedSnapshot = new Translog.Snapshot() { @Override public synchronized void close() throws IOException { snapshot.close(); } @Override public synchronized int totalOperations() { return snapshot.totalOperations(); } @Override public synchronized Translog.Operation next() throws IOException { IndexShardState state = indexShard.state(); if (state == IndexShardState.CLOSED) { throw new IndexShardClosedException(shardId); } else { assert state == IndexShardState.STARTED : "resync should only happen on a started shard, but state was: " + state; } return snapshot.next(); } }; resync(shardId, indexShard.routingEntry().allocationId().getId(), indexShard.getPrimaryTerm(), wrappedSnapshot, startingSeqNo, maxSeqNo, resyncListener); } catch (Exception e) { if (resyncListener != null) { resyncListener.onFailure(e); } else { listener.onFailure(e); } } }	when can this be null?
static Checkpoint emptyTranslogCheckpoint(final long offset, final long generation, final long globalCheckpoint, long minTranslogGeneration) { final long minSeqNo = SequenceNumbers.NO_OPS_PERFORMED; final long maxSeqNo = SequenceNumbers.NO_OPS_PERFORMED; return new Checkpoint(offset, 0, generation, minSeqNo, maxSeqNo, globalCheckpoint, minTranslogGeneration, maxSeqNo); }	can we use sequencenumbers.unassigned_seq_no? then we can see if things were trimmed or not.
public Translog.Operation next() throws IOException { while (readOperations < totalOperations) { final Translog.Operation operation = readOperation(); if (operation.seqNo() <= checkpoint.trimmedAboveSeqNo || checkpoint.trimmedAboveSeqNo < 0) { return operation; } skippedOperations++; } return null; }	can hard check timmerdaboveseqno to unassinged_seq_no? i don't like the implicit lower than 0. it's hard to grep for usages like that.
@Override public <T> Map<String, T> map( Supplier<Map<String, T>> mapFactory, CheckedFunction<XContentParser, T, IOException> mapValueParser) throws IOException { final Map<String, T> map = mapFactory.get(); if (findNonEmptyMapStart(this) == false) { return map; } assert currentToken() == Token.FIELD_NAME : "Expected field name but saw [" + currentToken() + "]"; do { // Must point to field name String fieldName = currentName(); // And then the value... nextToken(); T value = mapValueParser.apply(this); map.put(fieldName, value); } while (nextToken() == XContentParser.Token.FIELD_NAME); return map; }	duplicating this loop here since there's not efficient way of not having either a capturing lambda (and even a non-capturing one might not inline so great) or a strange api here.
*/ static <Response> void onResponse(Iterable<ActionListener<Response>> listeners, Response response) { RuntimeException exception = null; for (ActionListener<Response> listener : listeners) { try { listener.onResponse(response); } catch (Exception ex) { try { listener.onFailure(ex); } catch (Exception ex1) { if (exception != null) { exception = new RuntimeException(ex1); } else { exception.addSuppressed(ex1); } } } } if (exception != null) { throw exception; } } /** * Notifies every given listener with the failure passed to {@link #onFailure(Exception)}	this whole try is exactly the same code as in the method above. is it worth to try and share it?
private void onTimeoutInternal(List<? extends BatchedTask> tasks, TimeValue timeout) { final Set<IdentityWrapper> ids = new HashSet<>(tasks.size()); final List<BatchedTask> toRemove = new ArrayList<>(tasks.size()); for (BatchedTask task : tasks) { if (!task.processed.getAndSet(true)) { logger.debug("task [{}] timed out after [{}]", task.source, timeout); ids.add(new IdentityWrapper(task.getTask())); toRemove.add(task); } } if (toRemove.isEmpty()) { return; } final BatchedTask firstTask = toRemove.get(0); final Object batchingKey = firstTask.batchingKey; assert tasks.stream().allMatch(t -> t.batchingKey == batchingKey) : "tasks submitted in a batch should share the same batching key: " + tasks; tasksPerBatchingKey.computeIfPresent( batchingKey, (k, v) -> { if (v.size() == ids.size() && ids.containsAll(v.keySet())) { // Special case when all the tasks timed out return null; } else { final Map<IdentityWrapper, BatchedTask> merged = new LinkedHashMap<>(v.size()); v.forEach((id, task) -> { if (!ids.contains(id)) { merged.put(id, task); } }); return merged; } }); onTimeout(toRemove, timeout); }	please return to the == false style and use it everywhere. reasonable people can disagree on which is preferred between ! and == false but == false is the style that we use in our codebase (we do it this way because of the risk ! being overlooked).
public void testExplainWithWhere() throws IOException { index("test", body -> body.field("test_field", "test_value1").field("i", 1)); index("test", body -> body.field("test_field", "test_value2").field("i", 2)); assertThat(command("EXPLAIN (PLAN PARSED) SELECT * FROM test WHERE i = 2"), containsString("plan")); assertThat(readLine(), startsWith("----------")); assertThat(readLine(), startsWith("With[{}]")); assertThat(readLine(), startsWith("\\\\\\\\_Project[[?*]]")); assertThat(readLine(), startsWith(" \\\\\\\\_Filter[Equals[?i")); assertThat(readLine(), startsWith(" \\\\\\\\_UnresolvedRelation[test]")); assertEquals("", readLine()); assertThat(command("EXPLAIN " + (randomBoolean() ? "" : "(PLAN ANALYZED) ") + "SELECT * FROM test WHERE i = 2"), containsString("plan")); assertThat(readLine(), startsWith("----------")); assertThat(readLine(), startsWith("Project[[i{f}#")); assertThat(readLine(), startsWith("\\\\\\\\_Filter[Equals[i")); assertThat(readLine(), startsWith(" \\\\\\\\_EsRelation[test][i{f}#")); assertEquals("", readLine()); assertThat(command("EXPLAIN (PLAN OPTIMIZED) SELECT * FROM test WHERE i = 2"), containsString("plan")); assertThat(readLine(), startsWith("----------")); assertThat(readLine(), startsWith("Project[[i{f}#")); assertThat(readLine(), startsWith("\\\\\\\\_Filter[Equals[i")); assertThat(readLine(), startsWith(" \\\\\\\\_EsRelation[test][i{f}#")); assertEquals("", readLine()); assertThat(command("EXPLAIN (PLAN EXECUTABLE) SELECT * FROM test WHERE i = 2"), containsString("plan")); assertThat(readLine(), startsWith("----------")); assertThat(readLine(), startsWith("EsQueryExec[test,{")); assertThat(readLine(), startsWith(" \\\\"query\\\\" : {")); assertThat(readLine(), startsWith(" \\\\"term\\\\" : {")); assertThat(readLine(), startsWith(" \\\\"i\\\\" : {")); assertThat(readLine(), startsWith(" \\\\"value\\\\" : 2,")); assertThat(readLine(), startsWith(" \\\\"boost\\\\" : 1.0")); assertThat(readLine(), startsWith(" }")); assertThat(readLine(), startsWith(" }")); assertThat(readLine(), startsWith(" },")); assertThat(readLine(), startsWith(" \\\\"_source\\\\" : {")); assertThat(readLine(), startsWith(" \\\\"includes\\\\" : [")); assertThat(readLine(), startsWith(" \\\\"test_field\\\\"")); assertThat(readLine(), startsWith(" ],")); assertThat(readLine(), startsWith(" \\\\"excludes\\\\" : [ ]")); assertThat(readLine(), startsWith(" },")); assertThat(readLine(), startsWith(" \\\\"docvalue_fields\\\\" : [")); assertThat(readLine(), startsWith(" {")); assertThat(readLine(), startsWith(" \\\\"field\\\\" : \\\\"i\\\\"")); assertThat(readLine(), startsWith(" }")); assertThat(readLine(), startsWith(" ],")); assertThat(readLine(), startsWith(" \\\\"sort\\\\" : [")); assertThat(readLine(), startsWith(" {")); assertThat(readLine(), startsWith(" \\\\"_doc\\\\" :")); assertThat(readLine(), startsWith(" \\\\"order\\\\" : \\\\"asc\\\\"")); assertThat(readLine(), startsWith(" }")); assertThat(readLine(), startsWith(" }")); assertThat(readLine(), startsWith(" ]")); assertThat(readLine(), startsWith("}	here and in the other places: can't we also still check the 2?
private static boolean checkGroupByOrder(LogicalPlan p, Set<Failure> localFailures, Set<LogicalPlan> groupingFailures) { if (p instanceof OrderBy) { OrderBy o = (OrderBy) p; LogicalPlan child = o.child(); if (child instanceof Project) { child = ((Project) child).child(); } if (child instanceof Filter) { child = ((Filter) child).child(); } if (child instanceof Aggregate) { Aggregate a = (Aggregate) child; Map<Expression, Node<?>> missing = new LinkedHashMap<>(); // track aggs and non-aggs - to keep the final modifier, use an array final Expression[] aggAndNonAgg = new Expression[2]; o.order().forEach(oe -> { Expression e = oe.child(); if (Functions.isAggregate(e) || e instanceof AggregateFunctionAttribute) { if (aggAndNonAgg[0] != null) { return; } else { aggAndNonAgg[0] = e; if (aggAndNonAgg[1] == null) { return; } } } else { aggAndNonAgg[1] = e; } if (aggAndNonAgg[0] != null && aggAndNonAgg[1] != null) { localFailures.add(fail(oe, "Cannot order by aggregated [{}] and non-aggregated [{}] columns at the same time; " + "use either one or the other", Expressions.name(aggAndNonAgg[0]), Expressions.name(aggAndNonAgg[1]))); return; } // take aliases declared inside the aggregates which point to the grouping (but are not included in there) // to correlate them to the order List<Expression> groupingAndMatchingAggregatesAliases = new ArrayList<>(a.groupings()); a.aggregates().forEach(as -> { if (as instanceof Alias) { Alias al = (Alias) as; if (Expressions.anyMatch(a.groupings(), g -> Expressions.equalsAsAttribute(al.child(), g))) { groupingAndMatchingAggregatesAliases.add(al); } } }); // Make sure you can apply functions on top of the grouped by expressions in the ORDER BY: // e.g.: if "GROUP BY f2(f1(field))" you can "ORDER BY f4(f3(f2(f1(field))))" // // Also, make sure to compare attributes directly if (e.anyMatch(expression -> Expressions.anyMatch(groupingAndMatchingAggregatesAliases, g -> expression.semanticEquals(expression instanceof Attribute ? Expressions.attribute(g) : g)))) { return; } // nothing matched, cannot group by it missing.put(e, oe); }); if (!missing.isEmpty()) { String plural = missing.size() > 1 ? "s" : StringUtils.EMPTY; // get the location of the first missing expression as the order by might be on a different line localFailures.add( fail(missing.values().iterator().next(), "Cannot order by non-grouped column" + plural + " {}, expected {}", Expressions.names(missing.keySet()), Expressions.names(a.groupings()))); groupingFailures.add(a); return false; } } } return true; }	wouldn't have been more clear/easier to manage with two separate expressions named accordingly? (ie agg/nonagg)
@Override public void execute(SqlSession session, ActionListener<SchemaRowSet> listener) { Querier scroller = new Querier(session); // List<ExpressionId> ids = queryContainer.columns(); // // int[] extIds = new int[output.size()]; // // for (int i = 0; i < output.size(); i++) { // Attribute o = output.get(i); // int indexOf = ids.indexOf(o.id()); // if (indexOf == -1) { // throw new SqlIllegalArgumentException("Cannot find extractor for column [{}]", o.name()); // } // extIds[i] = indexOf; // } scroller.query(output, queryContainer, index, listener); }	are these still needed?
public void testUnsupportedTypeInOrder() { assertEquals("1:29: Cannot use field [unsupported] type [ip_range] as is unsupported", error("SELECT * FROM test ORDER BY unsupported")); } // public void testGroupByOrderByNonKey() { // assertEquals("1:52: Cannot order by non-grouped column [a], expected [bool]", // error("SELECT AVG(int) a FROM test GROUP BY bool ORDER BY a")); // }	why removing this method? why not only adding the new one?
@Override public boolean equals(Object o) { if (this == o) return true; if ((o instanceof SingleNodeShutdownMetadata) == false) return false; SingleNodeShutdownMetadata that = (SingleNodeShutdownMetadata) o; return getStartedAtMillis() == that.getStartedAtMillis() && getNodeId().equals(that.getNodeId()) && getType() == that.getType() && getReason().equals(that.getReason()) && Objects.equals(getShardReallocationDelay(), that.getShardReallocationDelay()); }	i would prefer to use the native field to avoid two semantically equivalent objects being equal. not that i know a case where it will cause issues, but i find it odd to have two equal objects producing different xcontent. need to update hashcode too.
*/ public long getRemainingDelay( final long nanoTimeNow, final Settings indexSettings, final Map<String, SingleNodeShutdownMetadata> nodesShutdownMap ) { long delayTimeoutNanos = Optional.ofNullable(lastAllocatedNodeId) .map(nodesShutdownMap::get) .filter(shutdownMetadata -> SingleNodeShutdownMetadata.Type.RESTART.equals(shutdownMetadata.getType())) .map(SingleNodeShutdownMetadata::getShardReallocationDelay) .map(TimeValue::nanos) .orElse(INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.get(indexSettings).nanos()); assert nanoTimeNow >= unassignedTimeNanos; return Math.max(0L, delayTimeoutNanos - (nanoTimeNow - unassignedTimeNanos)); }	it is a bit odd that if a node crashes and then has a shutdown indication added, we extend the node left timeout to be the possibly longer shutdown node left timeout. i wonder if we could instead capture whether the node was ready for shutdown when we lost it and use that to determine which of the delays to use?
*/ public long getRemainingDelay( final long nanoTimeNow, final Settings indexSettings, final Map<String, SingleNodeShutdownMetadata> nodesShutdownMap ) { long delayTimeoutNanos = Optional.ofNullable(lastAllocatedNodeId) .map(nodesShutdownMap::get) .filter(shutdownMetadata -> SingleNodeShutdownMetadata.Type.RESTART.equals(shutdownMetadata.getType())) .map(SingleNodeShutdownMetadata::getShardReallocationDelay) .map(TimeValue::nanos) .orElse(INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.get(indexSettings).nanos()); assert nanoTimeNow >= unassignedTimeNanos; return Math.max(0L, delayTimeoutNanos - (nanoTimeNow - unassignedTimeNanos)); }	in the case where index_delayed_node_left_timeout_setting setting is greater than the shutdown reallocation delay this means that the shutdown delay overrides the index_delayed_node_left_timeout_setting. that seems wrong? i think we should take the greater of the shutdown delay and the delayed allocation delay.
private void disassociateDeadNodes(RoutingAllocation allocation) { Map<String, SingleNodeShutdownMetadata> nodesShutdownMetadata = allocation.metadata().nodeShutdowns(); for (Iterator<RoutingNode> it = allocation.routingNodes().mutableIterator(); it.hasNext(); ) { RoutingNode node = it.next(); if (allocation.nodes().getDataNodes().containsKey(node.nodeId())) { // its a live node, continue continue; } // now, go over all the shards routing on the node, and fail them for (ShardRouting shardRouting : node.copyShards()) { final IndexMetadata indexMetadata = allocation.metadata().getIndexSafe(shardRouting.index()); boolean delayed = Optional.ofNullable(nodesShutdownMetadata.get(node.nodeId())) // If we know this node is restarting, then the allocation should be delayed .map(shutdownMetadata -> SingleNodeShutdownMetadata.Type.RESTART.equals(shutdownMetadata.getType())) // Otherwise, use the "normal" allocation delay logic .orElse(INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.get(indexMetadata.getSettings()).nanos() > 0); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.NODE_LEFT, "node_left [" + node.nodeId() + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), delayed, AllocationStatus.NO_ATTEMPT, Collections.emptySet(), shardRouting.currentNodeId()); allocation.routingNodes().failShard(logger, shardRouting, unassignedInfo, indexMetadata, allocation.changes()); } // its a dead node, remove it, note, its important to remove it *after* we apply failed shard // since it relies on the fact that the RoutingNode exists in the list of nodes it.remove(); } }	i think this means that if the shutdown type is remove or replace, we now ignore the delayed left timeout? also, i wonder if we should only use the shutdown indication if it had completed, i.e., the node node was ready to restart when this happened? since if it crashes on its own and they setup no delay for their indices, we should probably honor that. a flapping node during a restart could result in a long period of not assigning the shard, since we recalculate the delay every time the shard is recovered and the node subsequently dies. finally, if the shutdown reallocation delay and the delayed allocation delay are both 0, we should ensure delayed=false here.
public void testRestGetSourceAction() throws Exception { final BytesReference source = new BytesArray("{\\\\"foo\\\\": \\\\"bar\\\\"}"); final GetResponse response = new GetResponse(new GetResult("index1", "1", UNASSIGNED_SEQ_NO, 0, -1, true, source, emptyMap(), null)); final RestResponse restResponse = listener.buildResponse(response); assertThat(restResponse.status(), equalTo(OK)); assertThat(restResponse.contentType(), equalTo("application/json"));//dropping charset as it was not on a request assertThat(restResponse.content(), equalTo(new BytesArray("{\\\\"foo\\\\": \\\\"bar\\\\"}"))); }	only if charset was on a request it will be returned back
public void testThatExpectedContentTypeOverridesReturnedContentType() throws Exception { HttpRequestTemplate template = HttpRequestTemplate.builder("http:://127.0.0.1:12345").build(); HttpInput httpInput = new HttpInput(template, HttpContentType.TEXT, null); ExecutableHttpInput input = new ExecutableHttpInput(httpInput, httpClient, templateEngine); Map<String, String[]> headers = new HashMap<>(1); String contentType = randomFrom("application/json", "application/json;charset=utf-8", "text/html", "application/yaml", "application/smile", "application/cbor"); headers.put("Content-Type", new String[] { contentType }); String body = "{\\\\"foo\\\\":\\\\"bar\\\\"}"; HttpResponse httpResponse = new HttpResponse(200, body, headers); when(httpClient.execute(any())).thenReturn(httpResponse); WatchExecutionContext ctx = WatcherTestUtils.createWatchExecutionContext(); HttpInput.Result result = input.execute(ctx, Payload.EMPTY); assertThat(result.payload().data(), hasEntry("_value", body)); assertThat(result.payload().data(), not(hasKey("foo"))); }	just to make it consistent with other usages of charset in our codebase, but it actually is not relevant to the pr
@Override public BucketedSort newBucketedSort(BigArrays bigArrays, SortOrder sortOrder, DocValueFormat format, int bucketSize, BucketedSort.ExtraData extra) { return new BucketedSort.ForDoubles(bigArrays, sortOrder, format, bucketSize, extra) { private final double dMissingValue = (Double) missingObject(missingValue, sortOrder == SortOrder.DESC); @Override public Leaf forLeaf(LeafReaderContext ctx) throws IOException { return new Leaf(ctx) { private final NumericDoubleValues values = getNumericDocValues(ctx, dMissingValue); private double docValue; @Override protected boolean advanceExact(int doc) throws IOException { if (values.advanceExact(doc)) { docValue = values.doubleValue(); return true; } return false; } @Override protected double docValue() { return docValue; } }; } }; }	i don't think i understand the need for this change. can you add a comment or two for why we want to do it this way, please?
@Override public BucketedSort newBucketedSort(BigArrays bigArrays, SortOrder sortOrder, DocValueFormat format, int bucketSize, BucketedSort.ExtraData extra) { return new BucketedSort.ForDoubles(bigArrays, sortOrder, format, bucketSize, extra) { private final double dMissingValue = (Double) missingObject(missingValue, sortOrder == SortOrder.DESC); @Override public Leaf forLeaf(LeafReaderContext ctx) throws IOException { return new Leaf(ctx) { private final NumericDoubleValues values = getNumericDocValues(ctx, dMissingValue); private double docValue; @Override protected boolean advanceExact(int doc) throws IOException { if (values.advanceExact(doc)) { docValue = values.doubleValue(); return true; } return false; } @Override protected double docValue() { return docValue; } }; } }; }	nit: in the other two copies of this function, this variable is just called "value"
public <T, C> T parseNamedObject(Class<T> categoryClass, String name, XContentParser parser, C context) throws IOException { Map<String, Entry> parsers = registry.get(categoryClass); if (parsers == null) { if (registry.isEmpty()) { // The "empty" registry will never work so we throw a better exception as a hint. throw new XContentParseException("named objects are not supported for this parser"); } throw new XContentParseException("unknown named object category [" + categoryClass.getName() + "]"); } Entry entry = parsers.get(name); if (entry == null) { throw new NamedObjectNotFoundException(parser.getTokenLocation(), "unknown field [" + name + "]", parsers.keySet()); } if (false == entry.name.match(name, parser.getDeprecationHandler())) { /* Note that this shouldn't happen because we already looked up the entry using the names but we need to call `match` anyway * because it is responsible for logging deprecation warnings. */ throw new XContentParseException(parser.getTokenLocation(), "unable to parse " + categoryClass.getSimpleName() + " with name [" + name + "]: parser didn't match"); } return categoryClass.cast(entry.parser.parse(parser, context)); }	all of the exceptions that i changed to xcontentparseexception are really more development errors than usage errors.
private StoreFilesMetaData listStoreMetaData(ShardId shardId) throws IOException { logger.trace("listing store meta data for {}", shardId); long startTimeNS = System.nanoTime(); boolean exists = false; try { IndexService indexService = indicesService.indexService(shardId.getIndex()); if (indexService != null) { IndexShard indexShard = indexService.getShardOrNull(shardId.id()); if (indexShard != null) { try { final StoreFilesMetaData storeFilesMetaData = new StoreFilesMetaData(shardId, indexShard.snapshotStoreMetadata()); exists = true; return storeFilesMetaData; } catch (org.apache.lucene.index.IndexNotFoundException e) { logger.trace(new ParameterizedMessage("[{}] node is missing index, responding with empty", shardId), e); return new StoreFilesMetaData(shardId, Store.MetadataSnapshot.EMPTY); } catch (IOException e) { logger.warn(new ParameterizedMessage("[{}] can't read metadata from store, responding with empty", shardId), e); return new StoreFilesMetaData(shardId, Store.MetadataSnapshot.EMPTY); } } } // try and see if we an list unallocated IndexMetaData metaData = clusterService.state().metaData().index(shardId.getIndex()); if (metaData == null) { // we may send this requests while processing the cluster state that recovered the index // sometimes the request comes in before the local node processed that cluster state // in such cases we can load it from disk metaData = IndexMetaData.FORMAT.loadLatestState(logger, namedXContentRegistry, nodeEnv.indexPaths(shardId.getIndex())); } if (metaData == null) { logger.trace("{} node doesn't have meta data for the requests index, responding with empty", shardId); return new StoreFilesMetaData(shardId, Store.MetadataSnapshot.EMPTY); } final IndexSettings indexSettings = indexService != null ? indexService.getIndexSettings() : new IndexSettings(metaData, settings); final ShardPath shardPath = ShardPath.loadShardPath(logger, nodeEnv, shardId, indexSettings); if (shardPath == null) { return new StoreFilesMetaData(shardId, Store.MetadataSnapshot.EMPTY); } // note that this may fail if it can't get access to the shard lock. Since we check above there is an active shard, this means: // 1) a shard is being constructed, which means the master will not use a copy of this replica // 2) A shard is shutting down and has not cleared it's content within lock timeout. In this case the master may not // reuse local resources. return new StoreFilesMetaData(shardId, Store.readMetadataSnapshot(shardPath.resolveIndex(), shardId, nodeEnv::shardLock, logger)); } finally { TimeValue took = new TimeValue(System.nanoTime() - startTimeNS, TimeUnit.NANOSECONDS); if (exists) { logger.debug("{} loaded store meta data (took [{}])", shardId, took); } else { logger.trace("{} didn't find any store meta data to load (took [{}])", shardId, took); } } }	should we only ignore the exception if shard is recovering?
*/ public ShardRouting startShard(Logger logger, ShardRouting initializingShard, RoutingChangesObserver routingChangesObserver) { ensureMutable(); ShardRouting startedShard = started(initializingShard); logger.trace("{} marked shard as started (routing: {})", initializingShard.shardId(), initializingShard); routingChangesObserver.shardStarted(initializingShard, startedShard); if (initializingShard.relocatingNodeId() != null) { // relocation target has been started, remove relocation source RoutingNode relocationSourceNode = node(initializingShard.relocatingNodeId()); ShardRouting relocationSourceShard = relocationSourceNode.getByShardId(initializingShard.shardId()); assert relocationSourceShard.isRelocationSourceOf(initializingShard); assert relocationSourceShard.getTargetRelocatingShard() == initializingShard : "relocation target mismatch, expected: " + initializingShard + " but was: " + relocationSourceShard.getTargetRelocatingShard(); remove(relocationSourceShard); routingChangesObserver.relocationCompleted(relocationSourceShard); // if this is a primary shard with ongoing replica recoveries, reinitialize them as their recovery source changed if (startedShard.primary()) { List<ShardRouting> assignedShards = assignedShards(startedShard.shardId()); // copy list to prevent ConcurrentModificationException for (ShardRouting routing : new ArrayList<>(assignedShards)) { if (routing.initializing() && routing.primary() == false) { if (routing.isRelocationTarget()) { // find the relocation source ShardRouting sourceShard = getByAllocationId(routing.shardId(), routing.allocationId().getRelocationId()); // cancel relocation and start relocation to same node again ShardRouting startedReplica = cancelRelocation(sourceShard); remove(routing); routingChangesObserver.shardFailed(routing, new UnassignedInfo(UnassignedInfo.Reason.REINITIALIZED, "primary changed"), true); relocateShard(startedReplica, sourceShard.relocatingNodeId(), sourceShard.getExpectedShardSize(), routingChangesObserver); } else { ShardRouting reinitializedReplica = reinitReplica(routing); routingChangesObserver.initializedReplicaReinitialized(routing, reinitializedReplica); } } } } } return startedShard; }	the value here does not matter (as the shard is initializing)
public void failShard(Logger logger, ShardRouting failedShard, UnassignedInfo unassignedInfo, boolean markAsStale, IndexMetaData indexMetaData, RoutingChangesObserver routingChangesObserver) { ensureMutable(); assert failedShard.assignedToNode() : "only assigned shards can be failed"; assert indexMetaData.getIndex().equals(failedShard.index()) : "shard failed for unknown index (shard entry: " + failedShard + ")"; assert getByAllocationId(failedShard.shardId(), failedShard.allocationId().getId()) == failedShard : "shard routing to fail does not exist in routing table, expected: " + failedShard + " but was: " + getByAllocationId(failedShard.shardId(), failedShard.allocationId().getId()); logger.debug("{} failing shard {} with unassigned info ({})", failedShard.shardId(), failedShard, unassignedInfo.shortSummary()); // if this is a primary, fail initializing replicas first (otherwise we move RoutingNodes into an inconsistent state) if (failedShard.primary()) { List<ShardRouting> assignedShards = assignedShards(failedShard.shardId()); if (assignedShards.isEmpty() == false) { // copy list to prevent ConcurrentModificationException for (ShardRouting routing : new ArrayList<>(assignedShards)) { if (!routing.primary() && routing.initializing()) { // re-resolve replica as earlier iteration could have changed source/target of replica relocation ShardRouting replicaShard = getByAllocationId(routing.shardId(), routing.allocationId().getId()); assert replicaShard != null : "failed to re-resolve " + routing + " when failing replicas"; UnassignedInfo primaryFailedUnassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.PRIMARY_FAILED, "primary failed while replica initializing", null, 0, unassignedInfo.getUnassignedTimeInNanos(), unassignedInfo.getUnassignedTimeInMillis(), false, AllocationStatus.NO_ATTEMPT); failShard(logger, replicaShard, primaryFailedUnassignedInfo, true, indexMetaData, routingChangesObserver); } } } } if (failedShard.relocating()) { // find the shard that is initializing on the target node ShardRouting targetShard = getByAllocationId(failedShard.shardId(), failedShard.allocationId().getRelocationId()); assert targetShard.isRelocationTargetOf(failedShard); if (failedShard.primary()) { logger.trace("{} is removed due to the failure/cancellation of the source shard", targetShard); // cancel and remove target shard remove(targetShard); routingChangesObserver.shardFailed(targetShard, unassignedInfo, markAsStale); } else { logger.trace("{}, relocation source failed / cancelled, mark as initializing without relocation source", targetShard); // promote to initializing shard without relocation source and ensure that removed relocation source // is not added back as unassigned shard removeRelocationSource(targetShard); routingChangesObserver.relocationSourceRemoved(targetShard); } } // fail actual shard if (failedShard.initializing()) { if (failedShard.relocatingNodeId() == null) { if (failedShard.primary()) { // promote active replica to primary if active replica exists (only the case for shadow replicas) ShardRouting activeReplica = activeReplicaWithHighestVersion(failedShard.shardId()); if (activeReplica == null) { moveToUnassigned(failedShard, unassignedInfo); } else { movePrimaryToUnassignedAndDemoteToReplica(failedShard, unassignedInfo); promoteReplicaToPrimary(activeReplica, indexMetaData, routingChangesObserver); } } else { // initializing shard that is not relocation target, just move to unassigned moveToUnassigned(failedShard, unassignedInfo); } } else { // The shard is a target of a relocating shard. In that case we only need to remove the target shard and cancel the source // relocation. No shard is left unassigned logger.trace("{} is a relocation target, resolving source to cancel relocation ({})", failedShard, unassignedInfo.shortSummary()); ShardRouting sourceShard = getByAllocationId(failedShard.shardId(), failedShard.allocationId().getRelocationId()); assert sourceShard.isRelocationSourceOf(failedShard); logger.trace("{}, resolved source to [{}]. canceling relocation ... ({})", failedShard.shardId(), sourceShard, unassignedInfo.shortSummary()); cancelRelocation(sourceShard); remove(failedShard); } routingChangesObserver.shardFailed(failedShard, unassignedInfo, markAsStale); } else { assert failedShard.active(); if (failedShard.primary()) { // promote active replica to primary if active replica exists ShardRouting activeReplica = activeReplicaWithHighestVersion(failedShard.shardId()); if (activeReplica == null) { moveToUnassigned(failedShard, unassignedInfo); } else { movePrimaryToUnassignedAndDemoteToReplica(failedShard, unassignedInfo); promoteReplicaToPrimary(activeReplica, indexMetaData, routingChangesObserver); } } else { assert failedShard.primary() == false; if (failedShard.relocating()) { remove(failedShard); } else { moveToUnassigned(failedShard, unassignedInfo); } } routingChangesObserver.shardFailed(failedShard, unassignedInfo, markAsStale); } assert node(failedShard.currentNodeId()).getByShardId(failedShard.shardId()) == null : "failedShard " + failedShard + " was matched but wasn't removed"; }	an initializing shard, so marking as stale does not matter.
public void processExistingRecoveries(RoutingAllocation allocation) { MetaData metaData = allocation.metaData(); RoutingNodes routingNodes = allocation.routingNodes(); List<Runnable> shardCancellationActions = new ArrayList<>(); for (RoutingNode routingNode : routingNodes) { for (ShardRouting shard : routingNode) { if (shard.primary()) { continue; } if (shard.initializing() == false) { continue; } if (shard.relocatingNodeId() != null) { continue; } // if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one... if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) { continue; } AsyncShardFetch.FetchResult<NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation); if (shardStores.hasData() == false) { logger.trace("{}: fetching new stores for initializing shard", shard); continue; // still fetching } ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId()); assert primaryShard != null : "the replica shard can be allocated on at least one node, so there must be an active primary"; TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryShard, allocation, shardStores); if (primaryStore == null) { // if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed) // just let the recovery find it out, no need to do anything about it for the initializing shard logger.trace("{}: no primary shard store found or allocated, letting actual allocation figure it out", shard); continue; } MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, primaryStore, shardStores, false); if (matchingNodes.getNodeWithHighestMatch() != null) { DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId()); DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch(); // current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider final String currentSyncId; if (shardStores.getData().containsKey(currentNode)) { currentSyncId = shardStores.getData().get(currentNode).storeFilesMetaData().syncId(); } else { currentSyncId = null; } if (currentNode.equals(nodeWithHighestMatch) == false && Objects.equals(currentSyncId, primaryStore.syncId()) == false && matchingNodes.isNodeMatchBySyncID(nodeWithHighestMatch)) { // we found a better match that has a full sync id match, the existing allocation is not fully synced // so we found a better one, cancel this one logger.debug("cancelling allocation of replica on [{}], sync id match found on node [{}]", currentNode, nodeWithHighestMatch); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + currentNode + "] cancelled, sync id match found on node ["+ nodeWithHighestMatch + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT); // don't cancel shard in the loop as it will cause a ConcurrentModificationException shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, true, metaData.getIndexSafe(shard.index()), allocation.changes())); } } } } for (Runnable action : shardCancellationActions) { action.run(); } } /** * Is the allocator responsible for allocating the given {@link ShardRouting}	an initializing shard, so marking as stale does not matter.
static void checkOutputFeatureUniqueness(List<ProcessedField> processedFields, Set<String> selectedFields) { Set<String> processInputs = processedFields.stream() .map(ProcessedField::getInputFieldNames) .flatMap(List::stream) .collect(Collectors.toSet()); // All analysis fields that we include that are NOT processed // This indicates that they are sent as is Set<String> organicFields = Sets.difference(selectedFields, processInputs); Set<String> processedFeatures = new HashSet<>(); Set<String> duplicatedFields = new HashSet<>(); for (ProcessedField processedField : processedFields) { for (String output : processedField.getOutputFieldNames()) { if(processedFeatures.add(output) == false) { duplicatedFields.add(output); } } } if (duplicatedFields.isEmpty() == false) { throw ExceptionsHelper.badRequestException( "feature_processors must define unique output field names; duplicate fields {}", duplicatedFields); } Set<String> duplicateOrganicAndProcessed = Sets.intersection(organicFields, processedFeatures); if (duplicateOrganicAndProcessed.isEmpty() == false) { throw ExceptionsHelper.badRequestException( "feature_processors output fields must not include non-processed analysis fields; duplicate fields {}", duplicateOrganicAndProcessed); } }	nit: space after if
* * @return a map of grouped remote and local indices */ protected Map<String, List<String>> groupClusterIndices(Set<String> remoteClusterNames, String[] requestIndices) { Map<String, List<String>> perClusterIndices = new HashMap<>(); for (String index : requestIndices) { int i = index.indexOf(RemoteClusterService.REMOTE_CLUSTER_INDEX_SEPARATOR); if (i >= 0) { if (isRemoteClusterClientEnabled == false) { assert remoteClusterNames.isEmpty() : remoteClusterNames; throw new IllegalStateException("node [" + nodeName + "] does not have the remote cluster client role enabled"); } String remoteClusterName = index.substring(0, i); List<String> clusters = clusterNameResolver.resolveClusterNames(remoteClusterNames, remoteClusterName); String indexName = index.substring(i + 1); for (String clusterName : clusters) { perClusterIndices.computeIfAbsent(clusterName, k -> new ArrayList<>()).add(indexName); } } else { perClusterIndices.computeIfAbsent(RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY, k -> new ArrayList<>()).add(index); } } return perClusterIndices; }	i am not too sure about illegalstateexception here. it's more like a user error as they should rather send the request to another node? maybe illegalargumentexception would be more appropriate but i am not 100% happy with it either
public static void main(String[] args) throws Exception { if (args == null || args.length != 1) { throw new IllegalArgumentException("AmazonS3Fixture <working directory>"); } final InetSocketAddress socketAddress = new InetSocketAddress(InetAddress.getLoopbackAddress(), 0); final HttpServer httpServer = MockHttpServer.createHttp(socketAddress, 0); try { final Path workingDirectory = workingDir(args[0]); /// Writes the PID of the current Java process in a `pid` file located in the working directory writeFile(workingDirectory, "pid", ManagementFactory.getRuntimeMXBean().getName().split("@")[0]); final String addressAndPort = addressToString(httpServer.getAddress()); // Writes the address and port of the http server in a `ports` file located in the working directory writeFile(workingDirectory, "ports", addressAndPort); // Emulates S3 final String storageUrl = "http://" + addressAndPort; final AmazonS3TestServer storageTestServer = new AmazonS3TestServer(storageUrl); storageTestServer.createBucket("permanent_bucket_test"); storageTestServer.createBucket("temporary_bucket_test"); httpServer.createContext("/", new ResponseHandler(storageTestServer)); httpServer.start(); // Wait to be killed Thread.sleep(Long.MAX_VALUE); } finally { httpServer.stop(0); } }	can we revert this please?
public static void main(String[] args) throws Exception { if (args == null || args.length != 1) { throw new IllegalArgumentException("AmazonS3Fixture <working directory>"); } final InetSocketAddress socketAddress = new InetSocketAddress(InetAddress.getLoopbackAddress(), 0); final HttpServer httpServer = MockHttpServer.createHttp(socketAddress, 0); try { final Path workingDirectory = workingDir(args[0]); /// Writes the PID of the current Java process in a `pid` file located in the working directory writeFile(workingDirectory, "pid", ManagementFactory.getRuntimeMXBean().getName().split("@")[0]); final String addressAndPort = addressToString(httpServer.getAddress()); // Writes the address and port of the http server in a `ports` file located in the working directory writeFile(workingDirectory, "ports", addressAndPort); // Emulates S3 final String storageUrl = "http://" + addressAndPort; final AmazonS3TestServer storageTestServer = new AmazonS3TestServer(storageUrl); storageTestServer.createBucket("permanent_bucket_test"); storageTestServer.createBucket("temporary_bucket_test"); httpServer.createContext("/", new ResponseHandler(storageTestServer)); httpServer.start(); // Wait to be killed Thread.sleep(Long.MAX_VALUE); } finally { httpServer.stop(0); } }	can we revert this please, and pass the bucket names as program args?
*/ public Response handle(final String method, final String path, final String query, final Map<String, List<String>> headers, byte[] body) throws IOException { final long requestId = requests.incrementAndGet(); final Map<String, String> params = new HashMap<>(); if (query != null) { RestUtils.decodeQueryString(query, 0, params); } final List<String> authorizations = headers.get("Authorization"); if (authorizations == null || authorizations.size() != 1) { return newError(requestId, RestStatus.FORBIDDEN, "AccessDenied", "No authorization", ""); } final String authorization = authorizations.get(0); final String permittedBucket; if (authorization.contains("s3_integration_test_permanent_access_key")) { final List<String> sessionTokens = headers.get("x-amz-security-token"); if (sessionTokens != null) { return newError(requestId, RestStatus.FORBIDDEN, "AccessDenied", "Unexpected session token", ""); } permittedBucket = "permanent_bucket_test"; } else if (authorization.contains("s3_integration_test_temporary_access_key")) { final List<String> sessionTokens = headers.get("x-amz-security-token"); if (sessionTokens == null || sessionTokens.size() != 1) { return newError(requestId, RestStatus.FORBIDDEN, "AccessDenied", "No session token", ""); } final String sessionToken = sessionTokens.get(0); if (sessionToken.equals("s3_integration_test_temporary_session_token") == false) { return newError(requestId, RestStatus.FORBIDDEN, "AccessDenied", "Bad session token", ""); } permittedBucket = "temporary_bucket_test"; } else { return newError(requestId, RestStatus.FORBIDDEN, "AccessDenied", "Bad access key", ""); } final RequestHandler handler = handlers.retrieve(method + " " + path, params); if (handler != null) { final String bucket = params.get("bucket"); if (bucket != null && permittedBucket.equals(bucket) == false) { // allow a null bucket to support bucket-free APIs like ListBuckets? return newError(requestId, RestStatus.FORBIDDEN, "AccessDenied", "Bad bucket", ""); } return handler.execute(params, headers, body, requestId); } else { return newInternalError(requestId, "No handler defined for request [method: " + method + ", path: " + path + "]"); } }	> allow a null bucket to support bucket-free apis like listbuckets? i don't think it's needed, returning an error seems the right thing to do
public void updateNumAllocations( TrainedModelDeploymentTask task, int numAllocationThreads, TimeValue timeout, ActionListener<ThreadSettings> listener ) { var processContext = getProcessContext(task, listener::onFailure); if (processContext == null) { // error reporting handled in the call to getProcessContext return; } final long requestId = requestIdCounter.getAndIncrement(); ControlMessagePyTorchAction controlMessageAction = new ControlMessagePyTorchAction( task.getModelId(), requestId, numAllocationThreads, timeout, processContext, threadPool, listener ); try { processContext.getExecutorService().execute(controlMessageAction); } catch (EsRejectedExecutionException e) { processContext.getRejectedExecutionCount().incrementAndGet(); controlMessageAction.onFailure(e); } catch (Exception e) { controlMessageAction.onFailure(e); } }	there seems to be another method which we could extract here that is something like executepytorchaction and takes in an abstractpytorchaction. then we could reuse it when we fire either the inference action or the control message action and it does the getting of the process context and the try-catch of running the action. i might be missing something that makes it impossible to do this. in any case, just a thought.
public DataSummary collectDataSummary() { SearchRequestBuilder searchRequestBuilder = new SearchRequestBuilder(client, SearchAction.INSTANCE) .setIndices(context.indices) .setSize(0) .setQuery(context.query) .setTrackTotalHits(true); SearchResponse searchResponse = executeSearchRequest(searchRequestBuilder); return new DataSummary(searchResponse.getHits().getTotalHits().value, context.extractedFields.getAllFields().size()); }	it'd be nice to factor out searchrequestbuilder searchdatasummary() so that if we need to change the search we only need to do it in a single place.
public void testTimestampFieldTypeExposedByAllIndicesServices() throws Exception { internalCluster().startNodes(between(2, 4)); final String locale = randomFrom("", "en_GB", "fr_FR"); assertAcked(prepareCreate("index") .setSettings(Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1)) .setMapping(jsonBuilder().startObject().startObject("_doc").startObject("properties") .startObject(DataStream.TimestampField.FIXED_TIMESTAMP_FIELD) .field("type", "date") .field("format", "strict_date_hour_minute_second_fraction") .field("locale", locale) .endObject() .endObject().endObject().endObject())); final Index index = client().admin().cluster().prepareState().clear().setIndices("index").setMetadata(true) .get().getState().metadata().index("index").getIndex(); ensureGreen("index"); if (randomBoolean()) { client().prepareIndex("index").setSource(DataStream.TimestampField.FIXED_TIMESTAMP_FIELD, "2010-01-06T02:03:04.567").get(); } for (final IndicesService indicesService : internalCluster().getInstances(IndicesService.class)) { assertNull(indicesService.getTimestampFieldType(index)); } assertAcked(client().execute(FreezeIndexAction.INSTANCE, new FreezeRequest("index")).actionGet()); ensureGreen("index"); for (final IndicesService indicesService : internalCluster().getInstances(IndicesService.class)) { final PlainActionFuture<DateFieldMapper.DateFieldType> timestampFieldTypeFuture = new PlainActionFuture<>(); assertBusy(() -> { final DateFieldMapper.DateFieldType timestampFieldType = indicesService.getTimestampFieldType(index); assertNotNull(timestampFieldType); timestampFieldTypeFuture.onResponse(timestampFieldType); }); assertTrue(timestampFieldTypeFuture.isDone()); assertThat(timestampFieldTypeFuture.get().dateTimeFormatter().locale().toString(), equalTo(locale)); } assertAcked(client().execute(FreezeIndexAction.INSTANCE, new FreezeRequest("index").setFreeze(false)).actionGet()); ensureGreen("index"); for (final IndicesService indicesService : internalCluster().getInstances(IndicesService.class)) { assertNull(indicesService.getTimestampFieldType(index)); } }	is the future necessary? won't assertbusy throw an exception after the timeout if the block didn't succeed?
public void testTimestampFieldTypeExposedByAllIndicesServices() throws Exception { internalCluster().startNodes(between(2, 4)); final String locale = randomFrom("", "en_GB", "fr_FR"); assertAcked(prepareCreate("index") .setSettings(Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1)) .setMapping(jsonBuilder().startObject().startObject("_doc").startObject("properties") .startObject(DataStream.TimestampField.FIXED_TIMESTAMP_FIELD) .field("type", "date") .field("format", "strict_date_hour_minute_second_fraction") .field("locale", locale) .endObject() .endObject().endObject().endObject())); final Index index = client().admin().cluster().prepareState().clear().setIndices("index").setMetadata(true) .get().getState().metadata().index("index").getIndex(); ensureGreen("index"); if (randomBoolean()) { client().prepareIndex("index").setSource(DataStream.TimestampField.FIXED_TIMESTAMP_FIELD, "2010-01-06T02:03:04.567").get(); } for (final IndicesService indicesService : internalCluster().getInstances(IndicesService.class)) { assertNull(indicesService.getTimestampFieldType(index)); } assertAcked(client().execute(FreezeIndexAction.INSTANCE, new FreezeRequest("index")).actionGet()); ensureGreen("index"); for (final IndicesService indicesService : internalCluster().getInstances(IndicesService.class)) { final PlainActionFuture<DateFieldMapper.DateFieldType> timestampFieldTypeFuture = new PlainActionFuture<>(); assertBusy(() -> { final DateFieldMapper.DateFieldType timestampFieldType = indicesService.getTimestampFieldType(index); assertNotNull(timestampFieldType); timestampFieldTypeFuture.onResponse(timestampFieldType); }); assertTrue(timestampFieldTypeFuture.isDone()); assertThat(timestampFieldTypeFuture.get().dateTimeFormatter().locale().toString(), equalTo(locale)); } assertAcked(client().execute(FreezeIndexAction.INSTANCE, new FreezeRequest("index").setFreeze(false)).actionGet()); ensureGreen("index"); for (final IndicesService indicesService : internalCluster().getInstances(IndicesService.class)) { assertNull(indicesService.getTimestampFieldType(index)); } }	maybe we can add a an assertion that checks that datefieldmapper.datefieldtype#parse works with the original timestamp string?
public void testRelocationWaitsForPreWarm() throws Exception { internalCluster().startMasterOnlyNode(); final String firstDataNode = internalCluster().startDataOnlyNode(); final String index = "test-idx"; createIndexWithContent(index, indexSettingsNoReplicas(1).build()); final String repoName = "test-repo"; createRepository(repoName, "mock"); final String snapshotName = "test-snapshot"; createSnapshot(repoName, snapshotName, List.of(index)); assertAcked(client().admin().indices().prepareDelete(index)); final String restoredIndex = mountSnapshot(repoName, snapshotName, index, Settings.EMPTY); ensureGreen(restoredIndex); final String secondDataNode = internalCluster().startDataOnlyNode(); final ThreadPool threadPool = internalCluster().getInstance(ThreadPool.class, secondDataNode); final int preWarmThreads = threadPool.info(SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME).getMax(); final Executor executor = threadPool.executor(SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME); final CyclicBarrier barrier = new CyclicBarrier(preWarmThreads + 1); final CountDownLatch latch = new CountDownLatch(1); for (int i = 0; i < preWarmThreads; i++) { executor.execute(() -> { try { barrier.await(); latch.await(); } catch (Exception e) { throw new AssertionError(e); } }); } logger.info("--> waiting for prewarm threads to all become blocked"); barrier.await(); logger.info("--> force index [{}] to relocate to [{}]", index, secondDataNode); assertAcked( client().admin() .indices() .prepareUpdateSettings(restoredIndex) .setSettings( Settings.builder() .put( IndexMetadata.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getConcreteSettingForNamespace("_name").getKey(), secondDataNode ) ) ); assertBusy(() -> { final List<RecoveryState> recoveryStates = getActiveRestores(restoredIndex); assertThat(recoveryStates, Matchers.hasSize(1)); final RecoveryState shardRecoveryState = recoveryStates.get(0); assertEquals(firstDataNode, shardRecoveryState.getSourceNode().getName()); assertEquals(secondDataNode, shardRecoveryState.getTargetNode().getName()); }); logger.info("--> sleep for 5s to ensure we are actually stuck at the FINALIZE stage and that the primary has not yet relocated"); TimeUnit.SECONDS.sleep(5L); final RecoveryState recoveryState = getActiveRestores(restoredIndex).get(0); assertSame(RecoveryState.Stage.TRANSLOG, recoveryState.getStage()); final ClusterState state = client().admin().cluster().prepareState().get().getState(); final String primaryNodeId = state.routingTable().index(restoredIndex).shard(0).primaryShard().currentNodeId(); final DiscoveryNode primaryNode = state.nodes().resolveNode(primaryNodeId); assertEquals(firstDataNode, primaryNode.getName()); logger.info("--> unblocking prewarm threads"); latch.countDown(); assertFalse( client().admin() .cluster() .prepareHealth(restoredIndex) .setWaitForNoRelocatingShards(true) .setWaitForEvents(Priority.LANGUID) .get() .isTimedOut() ); assertBusy(() -> assertThat(getActiveRestores(restoredIndex), Matchers.empty())); }	could we instead find the shard using internalcluster().getinstance(indicesservice.class, node) and assertbusy that it has a pending after cleanup action?
public void testRandomReads() throws Exception { try (CacheService cacheService = randomCacheService()) { cacheService.start(); SnapshotId snapshotId = new SnapshotId("_name", "_uuid"); IndexId indexId = new IndexId("_name", "_uuid"); ShardId shardId = new ShardId("_name", "_uuid", 0); for (int i = 0; i < 5; i++) { final String fileName = randomAlphaOfLength(10); final byte[] input = randomUnicodeOfLength(randomIntBetween(1, 100_000)).getBytes(StandardCharsets.UTF_8); final String blobName = randomUnicodeOfLength(10); final StoreFileMetadata metadata = new StoreFileMetadata(fileName, input.length, "_na", Version.CURRENT.luceneVersion); final int partSize = randomBoolean() ? input.length : randomIntBetween(1, input.length); final BlobStoreIndexShardSnapshot snapshot = new BlobStoreIndexShardSnapshot( snapshotId.getName(), 0L, List.of(new BlobStoreIndexShardSnapshot.FileInfo(blobName, metadata, new ByteSizeValue(partSize))), 0L, 0L, 0, 0L ); final boolean prewarmEnabled = randomBoolean(); final BlobContainer singleBlobContainer = singleSplitBlobContainer(blobName, input, partSize); final BlobContainer blobContainer; if (input.length == partSize && input.length <= cacheService.getCacheSize() && prewarmEnabled == false) { blobContainer = new CountingBlobContainer(singleBlobContainer, cacheService.getRangeSize()); } else { blobContainer = singleBlobContainer; } final Path shardDir; try { shardDir = new NodeEnvironment.NodePath(createTempDir()).resolve(shardId); } catch (IOException e) { throw new UncheckedIOException(e); } final ShardPath shardPath = new ShardPath(false, shardDir, shardDir, shardId); final Path cacheDir = createTempDir(); try ( SearchableSnapshotDirectory directory = new SearchableSnapshotDirectory( () -> blobContainer, () -> snapshot, new NoopBlobStoreCacheService(), "_repo", snapshotId, indexId, shardId, Settings.builder() .put(SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true) .put(SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), prewarmEnabled) .build(), () -> 0L, cacheService, cacheDir, shardPath, threadPool ) ) { RecoveryState recoveryState = createRecoveryState(); final PlainActionFuture<Void> future = PlainActionFuture.newFuture(); final boolean loaded = directory.loadSnapshot(recoveryState, future); future.get(); assertThat("Failed to load snapshot", loaded, is(true)); assertThat("Snapshot should be loaded", directory.snapshot(), notNullValue()); assertThat("BlobContainer should be loaded", directory.blobContainer(), notNullValue()); try (IndexInput indexInput = directory.openInput(fileName, newIOContext(random()))) { assertEquals(input.length, indexInput.length()); assertEquals(0, indexInput.getFilePointer()); byte[] output = randomReadAndSlice(indexInput, input.length); assertArrayEquals(input, output); } } if (blobContainer instanceof CountingBlobContainer) { long numberOfRanges = TestUtils.numberOfRanges(input.length, cacheService.getRangeSize()); assertThat( "Expected at most " + numberOfRanges + " ranges fetched from the source", ((CountingBlobContainer) blobContainer).totalOpens.sum(), lessThanOrEqualTo(numberOfRanges) ); assertThat( "All bytes should have been read from source", ((CountingBlobContainer) blobContainer).totalBytes.sum(), equalTo((long) input.length) ); // busy assert that closing of all streams happened because they are closed on background fetcher threads assertBusy( () -> assertEquals( "All open streams should have been closed", 0, ((CountingBlobContainer) blobContainer).openStreams.get() ) ); } } } finally { assertThreadPoolNotBusy(threadPool); } }	maybe only do this line randomly to avoid waiting for prewarming before doing all the reads below?
* @throws IOException if the blob can not be read. */ InputStream readBlob(String blobName) throws IOException; /** * Creates a new {@link InputStream} that can be used to read the given blob starting from * a specific {@code position} in the blob. The {@code length} is an indication of the * number of bytes that are expected to be read from the {@link InputStream}. * * @param blobName The name of the blob to get an {@link InputStream} for. * @param position The position in the blob where the next byte will be read. * @param length An indication of the number of bytes to be read. * @return The {@code InputStream}	:+1: noting that (contrary to our conversation this morning) this means that we only support a shared filesystem repository at the moment.
public void testUpsertDoc() throws Exception { createTestIndex(); ensureGreen(); UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1") .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject()) .setDocAsUpsert(true) .setFetchSource(true) .execute().actionGet(); assertThat(updateResponse.getIndex(), equalTo("test")); assertThat(updateResponse.getGetResult(), notNullValue()); assertThat(updateResponse.getGetResult().getIndex(), equalTo("test")); assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz")); }	hmmm. i *think* this is true by default. can you check what happens if you drop the .setfetchsource(true) part?
@Override public void handleException(TransportException exp) { assertThat(exp, instanceOf(ReceiveTimeoutTransportException.class)); assertThat(exp.getStackTrace().length, equalTo(0)); }	is this change related to this pr?
public void datafeedTimingStats(List<String> jobIds, ActionListener<Map<String, DatafeedTimingStats>> listener) { if (jobIds.isEmpty()) { listener.onResponse(Map.of()); return; } MultiSearchRequestBuilder msearchRequestBuilder = client.prepareMultiSearch(); for (String jobId : jobIds) { String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId); msearchRequestBuilder.add(createLatestDatafeedTimingStatsSearch(indexName, jobId)); } MultiSearchRequest msearchRequest = msearchRequestBuilder.request(); executeAsyncWithOrigin( client.threadPool().getThreadContext(), ML_ORIGIN, msearchRequest, ActionListener.<MultiSearchResponse>wrap( msearchResponse -> { Map<String, DatafeedTimingStats> timingStatsByJobId = new HashMap<>(); for (int i = 0; i < msearchResponse.getResponses().length; i++) { String jobId = jobIds.get(i); MultiSearchResponse.Item itemResponse = msearchResponse.getResponses()[i]; if (itemResponse.isFailure()) { listener.onFailure(itemResponse.getFailure()); return; } SearchResponse searchResponse = itemResponse.getResponse(); ShardSearchFailure[] shardFailures = searchResponse.getShardFailures(); int unavailableShards = searchResponse.getTotalShards() - searchResponse.getSuccessfulShards(); if (shardFailures != null && shardFailures.length > 0) { LOGGER.error("[{}] Search request returned shard failures: {}", jobId, Arrays.toString(shardFailures)); listener.onFailure( new ElasticsearchException(ExceptionsHelper.shardFailuresToErrorMsg(jobId, shardFailures))); return; } if (unavailableShards > 0) { listener.onFailure( new ElasticsearchException( "[" + jobId + "] Search request encountered [" + unavailableShards + "] unavailable shards")); return; } SearchHits hits = searchResponse.getHits(); long hitsCount = hits.getHits().length; if (hitsCount == 0 || hitsCount > 1) { SearchRequest searchRequest = msearchRequest.requests().get(i); LOGGER.debug("Found {} hits for [{}]", hitsCount == 0 ? "0" : "multiple", new Object[]{searchRequest.indices()}); return; } SearchHit hit = hits.getHits()[0]; try { DatafeedTimingStats timingStats = parseSearchHit(hit, DatafeedTimingStats.PARSER); timingStatsByJobId.put(jobId, timingStats); } catch (Exception e) { listener.onFailure(e); return; } } listener.onResponse(timingStatsByJobId); }, listener::onFailure ), client::multiSearch); }	is there an redundant indentation here or is it github messing with me?
@Override public void onResponse(Releasable releasable) { assert replica.getActiveOperationsCount() != 0 : "must perform shard operation under a permit"; shardOperationOnReplica(replicaRequest.getRequest(), replica, ActionListener.wrap((replicaResult) -> replicaResult.runPostReplicaActions( ActionListener.wrap(r -> { final ReplicaResponse response = new ReplicaResponse(replica.getLocalCheckpoint(), replica.getLastSyncedGlobalCheckpoint()); releasable.close(); // release shard operation lock before responding to caller if (logger.isTraceEnabled()) { logger.trace("action [{}] completed on shard [{}] for request [{}]", transportReplicaAction, replicaRequest.getRequest().shardId(), replicaRequest.getRequest()); } setPhase(task, "finished"); onCompletionListener.onResponse(response); }, e -> { Releasables.closeWhileHandlingException(releasable); // release shard operation lock before responding to caller responseWithFailure(e); }) ), e -> { Releasables.closeWhileHandlingException(releasable); // release shard operation lock before responding to caller AsyncReplicaAction.this.onFailure(e); })); }	what if shardoperationonreplica throws an exception? we're not releasing the releasable then?
private CloseableHttpAsyncClient createHttpClient() { //default timeouts are all infinite RequestConfig.Builder requestConfigBuilder = RequestConfig.custom() .setConnectTimeout(DEFAULT_CONNECT_TIMEOUT_MILLIS) .setSocketTimeout(DEFAULT_SOCKET_TIMEOUT_MILLIS); if (requestConfigCallback != null) { requestConfigBuilder = requestConfigCallback.customizeRequestConfig(requestConfigBuilder); } try { HttpAsyncClientBuilder httpClientBuilder = HttpAsyncClientBuilder.create().setDefaultRequestConfig(requestConfigBuilder.build()) //default settings for connection pooling may be too constraining .setMaxConnPerRoute(DEFAULT_MAX_CONN_PER_ROUTE).setMaxConnTotal(DEFAULT_MAX_CONN_TOTAL) .setSSLContext(SSLContext.getDefault()) .setUserAgent(String.format(Locale.ROOT, "elasticsearch-java/%s (Java/%s)", VERSION.isEmpty() ? "Unknown" : VERSION, System.getProperty("java.version")) ) .setTargetAuthenticationStrategy(new PersistentCredentialsAuthenticationStrategy()); if (httpClientConfigCallback != null) { httpClientBuilder = httpClientConfigCallback.customizeHttpClient(httpClientBuilder); } // Always add metadata header last so that it's not overwritten final String metadataHeaderValue = getMetadataHeaderValue(); httpClientBuilder.addInterceptorLast((HttpRequest request, HttpContext context) -> { if (metadataHeaderValue != null) { request.setHeader(METADATA_HEADER, metadataHeaderValue); } else { request.removeHeaders(METADATA_HEADER); } }); final HttpAsyncClientBuilder finalBuilder = httpClientBuilder; return AccessController.doPrivileged((PrivilegedAction<CloseableHttpAsyncClient>) finalBuilder::build); } catch (NoSuchAlgorithmException e) { throw new IllegalStateException("could not create the default ssl context", e); } }	these are all things that will not change for the lifetime of the jvm, yet http client version and the "runtime metadata" seem to be inspected on every invocation of this method. instead, we could have a single method that gets invoked to generate this data, and statically cache the result. we can then be more stringent in failures, since if metadata is enabled, we shouldn't have silent failures, which in that case may only cause confusion. the error handling can be simpler then, with a single try/catch that rethrows as a runtime exception.
public void setUp() throws Exception { assert "false".equals(System.getProperty("tests.security.manager")) : "-Dtests.security.manager=false has to be set"; super.setUp(); LogConfigurator.registerErrorListener(); }	did this slip in, or is it intentional ?
public static Query doTranslate(ScalarFunction f, TranslatorHandler handler) { Query q = ExpressionTranslators.Scalars.doKnownTranslate(f, handler); if (q != null) { return q; } if (f instanceof CIDRMatch) { CIDRMatch cm = (CIDRMatch) f; if (cm.input() instanceof FieldAttribute && Expressions.foldable(cm.addresses())) { String targetFieldName = handler.nameOf(((FieldAttribute) cm.input()).exactAttribute()); Set<Object> set = new LinkedHashSet<>(CollectionUtils.mapSize(cm.addresses().size())); for (Expression e : cm.addresses()) { set.add(valueOf(e)); } return new TermsQuery(f.source(), targetFieldName, set); } } if (f instanceof StringContains) { StringContains sc = (StringContains) f; if (sc.isCaseSensitive() && sc.string() instanceof FieldAttribute && sc.substring().foldable()) { String targetFieldName = handler.nameOf(((FieldAttribute) sc.string()).exactAttribute()); String substring = (String) sc.substring().fold(); return new WildcardQuery(f.source(), targetFieldName, "*" + substring + "*"); } } return handler.wrapFunctionQuery(f, f, new ScriptQuery(f.source(), f.asScript())); }	wouldn't this work better in expressiontranslators#scalars where startswith is handled?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); targetIndexRequest.writeTo(out); out.writeString(sourceIndex); if (out.getVersion().onOrAfter(ResizeAction.COMPATIBILITY_VERSION)) { out.writeEnum(type); } if (out.getVersion().onOrAfter(Version.V_7_0_0_alpha1)) { out.writeBoolean(copySettings); } }	this will be adjusted after backporting.
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); targetIndexRequest = new CreateIndexRequest(); targetIndexRequest.readFrom(in); sourceIndex = in.readString(); if (in.getVersion().onOrAfter(ResizeAction.COMPATIBILITY_VERSION)) { type = in.readEnum(ResizeType.class); } else { type = ResizeType.SHRINK; // BWC this used to be shrink only } if (in.getVersion().onOrAfter(Version.V_7_0_0_alpha1)) { copySettings = in.readBoolean(); } else { copySettings = false; } }	this will be adjusted after backporting.
public void parse(ParseContext context) throws IOException { try { parseCreateField(context); } catch (Exception e) { String valuePreview = ""; try { XContentParser parser = context.parser(); Object complexValue = AbstractXContentParser.readValue(parser, HashMap::new); if (complexValue == null) { valuePreview = "null"; } else { valuePreview = complexValue.toString(); } } catch (Exception innerException) { throw new MapperParsingException("failed to parse field [{}] of type [{}] in document with id '{}'. " + "Could not parse field value preview,", e, fieldType().name(), fieldType().typeName(), context.sourceToParse().id()); } throw new MapperParsingException("failed to parse field [{}] of type [{}] in document with id '{}'. " + "Preview of field's value: '{}'", e, fieldType().name(), fieldType().typeName(), context.sourceToParse().id(), valuePreview); } multiFields.parse(this, context); } /** * Returns a post-parse executor for this field mapper, or {@code null}	maybe the post parse phase should become a class of its own: i am thinking this could help readability, clarify what it needs and what it outputs, and possibly even include some logic from indextimescriptcontext
@Override public CollapseType collapseType() { return CollapseType.NUMERIC; } } private final NumberType type; private final boolean indexed; private final boolean hasDocValues; private final boolean stored; private final Explicit<Boolean> ignoreMalformed; private final Explicit<Boolean> coerce; private final Number nullValue; private final ScriptParams.CompiledScriptParameter<NumberMapperScript> script; private final boolean ignoreMalformedByDefault; private final boolean coerceByDefault; private NumberFieldMapper( String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo); this.type = builder.type; this.indexed = builder.indexed.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); this.stored = builder.stored.getValue(); this.ignoreMalformed = builder.ignoreMalformed.getValue(); this.coerce = builder.coerce.getValue(); this.nullValue = builder.nullValue.getValue(); this.ignoreMalformedByDefault = builder.ignoreMalformed.getDefaultValue().value(); this.coerceByDefault = builder.coerce.getDefaultValue().value(); this.script = builder.script.get(); } boolean coerce() { return coerce.value(); } boolean ignoreMalformed() { return ignoreMalformed.value(); } @Override public NumberFieldType fieldType() { return (NumberFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType().type.typeName(); } @Override protected void parseCreateField(ParseContext context) throws IOException { if (this.script != null) { throw new IllegalArgumentException("Cannot index data directly into scripted field"); } XContentParser parser = context.parser(); Object value; Number numericValue = null; if (context.externalValueSet()) { value = context.externalValue(); } else if (parser.currentToken() == Token.VALUE_NULL) { value = null; } else if (coerce.value() && parser.currentToken() == Token.VALUE_STRING && parser.textLength() == 0) { value = null; } else { try { numericValue = fieldType().type.parse(parser, coerce.value()); } catch (InputCoercionException | IllegalArgumentException | JsonParseException e) { if (ignoreMalformed.value() && parser.currentToken().isValue()) { context.addIgnoredField(mappedFieldType.name()); return; } else { throw e; } } value = numericValue; } if (value == null) { value = nullValue; } if (value == null) { return; } if (numericValue == null) { numericValue = fieldType().type.parse(value, coerce.value()); } indexValue(context, numericValue); } private void indexValue(ParseContext context, Number numericValue) { context.doc().addAll(fieldType().type.createFields(fieldType().name(), numericValue, indexed, hasDocValues, stored)); if (hasDocValues == false && (stored || indexed)) { createFieldNamesField(context); } } @Override public CheckedConsumer<IndexTimeScriptContext, IOException> getPostParsePhase() { if (this.script == null) { return null; } return c -> this.script.compiledScript.executeAndIndex( c.searchLookup, c.leafReaderContext, 0, v -> this.indexValue(c.pc, v) ); } @Override public FieldMapper.Builder getMergeBuilder() { return new Builder(simpleName(), type, ignoreMalformedByDefault, coerceByDefault).init(this); } private interface NumberMapperScript { void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter); } private static class LongMapperScript implements NumberMapperScript { final LongFieldScript.Factory scriptFactory; final Map<String, Object> scriptParams; final String fieldName; private LongMapperScript(String fieldName, LongFieldScript.Factory scriptFactory, Map<String, Object> scriptParams) { this.scriptFactory = scriptFactory; this.scriptParams = scriptParams; this.fieldName = fieldName; } @Override public void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { LongFieldScript s = scriptFactory .newFactory(fieldName, scriptParams, lookup) .newInstance(ctx); s.runForDoc(doc); long[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } } } private static class DoubleMapperScript implements NumberMapperScript { final DoubleFieldScript.Factory scriptFactory; final Map<String, Object> scriptParams; final String fieldName; private DoubleMapperScript(String fieldName, DoubleFieldScript.Factory scriptFactory, Map<String, Object> scriptParams) { this.scriptFactory = scriptFactory; this.scriptParams = scriptParams; this.fieldName = fieldName; } @Override public void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { DoubleFieldScript s = scriptFactory .newFactory(fieldName, scriptParams, lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } }	just by looking at it, it is hard to tell why this class is needed. i think it holds a compiled script, as scripts get compiled when the mappings are parsed. maybe we should better define what abstractions are needed: it seems that script is not enough as it is not compiled, and we need some representation for a compiled script, that then gets executed once for each document being indexed. i even wonder if this should be an interface or rather a base class, because there are aspects of a compiled script that all of the types share (params, fieldname)?
@Override public CollapseType collapseType() { return CollapseType.NUMERIC; } } private final NumberType type; private final boolean indexed; private final boolean hasDocValues; private final boolean stored; private final Explicit<Boolean> ignoreMalformed; private final Explicit<Boolean> coerce; private final Number nullValue; private final ScriptParams.CompiledScriptParameter<NumberMapperScript> script; private final boolean ignoreMalformedByDefault; private final boolean coerceByDefault; private NumberFieldMapper( String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo); this.type = builder.type; this.indexed = builder.indexed.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); this.stored = builder.stored.getValue(); this.ignoreMalformed = builder.ignoreMalformed.getValue(); this.coerce = builder.coerce.getValue(); this.nullValue = builder.nullValue.getValue(); this.ignoreMalformedByDefault = builder.ignoreMalformed.getDefaultValue().value(); this.coerceByDefault = builder.coerce.getDefaultValue().value(); this.script = builder.script.get(); } boolean coerce() { return coerce.value(); } boolean ignoreMalformed() { return ignoreMalformed.value(); } @Override public NumberFieldType fieldType() { return (NumberFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType().type.typeName(); } @Override protected void parseCreateField(ParseContext context) throws IOException { if (this.script != null) { throw new IllegalArgumentException("Cannot index data directly into scripted field"); } XContentParser parser = context.parser(); Object value; Number numericValue = null; if (context.externalValueSet()) { value = context.externalValue(); } else if (parser.currentToken() == Token.VALUE_NULL) { value = null; } else if (coerce.value() && parser.currentToken() == Token.VALUE_STRING && parser.textLength() == 0) { value = null; } else { try { numericValue = fieldType().type.parse(parser, coerce.value()); } catch (InputCoercionException | IllegalArgumentException | JsonParseException e) { if (ignoreMalformed.value() && parser.currentToken().isValue()) { context.addIgnoredField(mappedFieldType.name()); return; } else { throw e; } } value = numericValue; } if (value == null) { value = nullValue; } if (value == null) { return; } if (numericValue == null) { numericValue = fieldType().type.parse(value, coerce.value()); } indexValue(context, numericValue); } private void indexValue(ParseContext context, Number numericValue) { context.doc().addAll(fieldType().type.createFields(fieldType().name(), numericValue, indexed, hasDocValues, stored)); if (hasDocValues == false && (stored || indexed)) { createFieldNamesField(context); } } @Override public CheckedConsumer<IndexTimeScriptContext, IOException> getPostParsePhase() { if (this.script == null) { return null; } return c -> this.script.compiledScript.executeAndIndex( c.searchLookup, c.leafReaderContext, 0, v -> this.indexValue(c.pc, v) ); } @Override public FieldMapper.Builder getMergeBuilder() { return new Builder(simpleName(), type, ignoreMalformedByDefault, coerceByDefault).init(this); } private interface NumberMapperScript { void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter); } private static class LongMapperScript implements NumberMapperScript { final LongFieldScript.Factory scriptFactory; final Map<String, Object> scriptParams; final String fieldName; private LongMapperScript(String fieldName, LongFieldScript.Factory scriptFactory, Map<String, Object> scriptParams) { this.scriptFactory = scriptFactory; this.scriptParams = scriptParams; this.fieldName = fieldName; } @Override public void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { LongFieldScript s = scriptFactory .newFactory(fieldName, scriptParams, lookup) .newInstance(ctx); s.runForDoc(doc); long[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } } } private static class DoubleMapperScript implements NumberMapperScript { final DoubleFieldScript.Factory scriptFactory; final Map<String, Object> scriptParams; final String fieldName; private DoubleMapperScript(String fieldName, DoubleFieldScript.Factory scriptFactory, Map<String, Object> scriptParams) { this.scriptFactory = scriptFactory; this.scriptParams = scriptParams; this.fieldName = fieldName; } @Override public void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { DoubleFieldScript s = scriptFactory .newFactory(fieldName, scriptParams, lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } }	i find the name a bit distracting because effectively indexing is not what we do in all cases. do we rather collect values and provide them one by one to the emitter consumer?
@Override public CollapseType collapseType() { return CollapseType.NUMERIC; } } private final NumberType type; private final boolean indexed; private final boolean hasDocValues; private final boolean stored; private final Explicit<Boolean> ignoreMalformed; private final Explicit<Boolean> coerce; private final Number nullValue; private final ScriptParams.CompiledScriptParameter<NumberMapperScript> script; private final boolean ignoreMalformedByDefault; private final boolean coerceByDefault; private NumberFieldMapper( String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo); this.type = builder.type; this.indexed = builder.indexed.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); this.stored = builder.stored.getValue(); this.ignoreMalformed = builder.ignoreMalformed.getValue(); this.coerce = builder.coerce.getValue(); this.nullValue = builder.nullValue.getValue(); this.ignoreMalformedByDefault = builder.ignoreMalformed.getDefaultValue().value(); this.coerceByDefault = builder.coerce.getDefaultValue().value(); this.script = builder.script.get(); } boolean coerce() { return coerce.value(); } boolean ignoreMalformed() { return ignoreMalformed.value(); } @Override public NumberFieldType fieldType() { return (NumberFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType().type.typeName(); } @Override protected void parseCreateField(ParseContext context) throws IOException { if (this.script != null) { throw new IllegalArgumentException("Cannot index data directly into scripted field"); } XContentParser parser = context.parser(); Object value; Number numericValue = null; if (context.externalValueSet()) { value = context.externalValue(); } else if (parser.currentToken() == Token.VALUE_NULL) { value = null; } else if (coerce.value() && parser.currentToken() == Token.VALUE_STRING && parser.textLength() == 0) { value = null; } else { try { numericValue = fieldType().type.parse(parser, coerce.value()); } catch (InputCoercionException | IllegalArgumentException | JsonParseException e) { if (ignoreMalformed.value() && parser.currentToken().isValue()) { context.addIgnoredField(mappedFieldType.name()); return; } else { throw e; } } value = numericValue; } if (value == null) { value = nullValue; } if (value == null) { return; } if (numericValue == null) { numericValue = fieldType().type.parse(value, coerce.value()); } indexValue(context, numericValue); } private void indexValue(ParseContext context, Number numericValue) { context.doc().addAll(fieldType().type.createFields(fieldType().name(), numericValue, indexed, hasDocValues, stored)); if (hasDocValues == false && (stored || indexed)) { createFieldNamesField(context); } } @Override public CheckedConsumer<IndexTimeScriptContext, IOException> getPostParsePhase() { if (this.script == null) { return null; } return c -> this.script.compiledScript.executeAndIndex( c.searchLookup, c.leafReaderContext, 0, v -> this.indexValue(c.pc, v) ); } @Override public FieldMapper.Builder getMergeBuilder() { return new Builder(simpleName(), type, ignoreMalformedByDefault, coerceByDefault).init(this); } private interface NumberMapperScript { void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter); } private static class LongMapperScript implements NumberMapperScript { final LongFieldScript.Factory scriptFactory; final Map<String, Object> scriptParams; final String fieldName; private LongMapperScript(String fieldName, LongFieldScript.Factory scriptFactory, Map<String, Object> scriptParams) { this.scriptFactory = scriptFactory; this.scriptParams = scriptParams; this.fieldName = fieldName; } @Override public void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { LongFieldScript s = scriptFactory .newFactory(fieldName, scriptParams, lookup) .newInstance(ctx); s.runForDoc(doc); long[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } } } private static class DoubleMapperScript implements NumberMapperScript { final DoubleFieldScript.Factory scriptFactory; final Map<String, Object> scriptParams; final String fieldName; private DoubleMapperScript(String fieldName, DoubleFieldScript.Factory scriptFactory, Map<String, Object> scriptParams) { this.scriptFactory = scriptFactory; this.scriptParams = scriptParams; this.fieldName = fieldName; } @Override public void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { DoubleFieldScript s = scriptFactory .newFactory(fieldName, scriptParams, lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } }	do you think it'd be cool if longfieldscript took a longconsumer?
@Override public CollapseType collapseType() { return CollapseType.NUMERIC; } } private final NumberType type; private final boolean indexed; private final boolean hasDocValues; private final boolean stored; private final Explicit<Boolean> ignoreMalformed; private final Explicit<Boolean> coerce; private final Number nullValue; private final ScriptParams.CompiledScriptParameter<NumberMapperScript> script; private final boolean ignoreMalformedByDefault; private final boolean coerceByDefault; private NumberFieldMapper( String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo); this.type = builder.type; this.indexed = builder.indexed.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); this.stored = builder.stored.getValue(); this.ignoreMalformed = builder.ignoreMalformed.getValue(); this.coerce = builder.coerce.getValue(); this.nullValue = builder.nullValue.getValue(); this.ignoreMalformedByDefault = builder.ignoreMalformed.getDefaultValue().value(); this.coerceByDefault = builder.coerce.getDefaultValue().value(); this.script = builder.script.get(); } boolean coerce() { return coerce.value(); } boolean ignoreMalformed() { return ignoreMalformed.value(); } @Override public NumberFieldType fieldType() { return (NumberFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType().type.typeName(); } @Override protected void parseCreateField(ParseContext context) throws IOException { if (this.script != null) { throw new IllegalArgumentException("Cannot index data directly into scripted field"); } XContentParser parser = context.parser(); Object value; Number numericValue = null; if (context.externalValueSet()) { value = context.externalValue(); } else if (parser.currentToken() == Token.VALUE_NULL) { value = null; } else if (coerce.value() && parser.currentToken() == Token.VALUE_STRING && parser.textLength() == 0) { value = null; } else { try { numericValue = fieldType().type.parse(parser, coerce.value()); } catch (InputCoercionException | IllegalArgumentException | JsonParseException e) { if (ignoreMalformed.value() && parser.currentToken().isValue()) { context.addIgnoredField(mappedFieldType.name()); return; } else { throw e; } } value = numericValue; } if (value == null) { value = nullValue; } if (value == null) { return; } if (numericValue == null) { numericValue = fieldType().type.parse(value, coerce.value()); } indexValue(context, numericValue); } private void indexValue(ParseContext context, Number numericValue) { context.doc().addAll(fieldType().type.createFields(fieldType().name(), numericValue, indexed, hasDocValues, stored)); if (hasDocValues == false && (stored || indexed)) { createFieldNamesField(context); } } @Override public CheckedConsumer<IndexTimeScriptContext, IOException> getPostParsePhase() { if (this.script == null) { return null; } return c -> this.script.compiledScript.executeAndIndex( c.searchLookup, c.leafReaderContext, 0, v -> this.indexValue(c.pc, v) ); } @Override public FieldMapper.Builder getMergeBuilder() { return new Builder(simpleName(), type, ignoreMalformedByDefault, coerceByDefault).init(this); } private interface NumberMapperScript { void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter); } private static class LongMapperScript implements NumberMapperScript { final LongFieldScript.Factory scriptFactory; final Map<String, Object> scriptParams; final String fieldName; private LongMapperScript(String fieldName, LongFieldScript.Factory scriptFactory, Map<String, Object> scriptParams) { this.scriptFactory = scriptFactory; this.scriptParams = scriptParams; this.fieldName = fieldName; } @Override public void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { LongFieldScript s = scriptFactory .newFactory(fieldName, scriptParams, lookup) .newInstance(ctx); s.runForDoc(doc); long[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } } } private static class DoubleMapperScript implements NumberMapperScript { final DoubleFieldScript.Factory scriptFactory; final Map<String, Object> scriptParams; final String fieldName; private DoubleMapperScript(String fieldName, DoubleFieldScript.Factory scriptFactory, Map<String, Object> scriptParams) { this.scriptFactory = scriptFactory; this.scriptParams = scriptParams; this.fieldName = fieldName; } @Override public void executeAndIndex(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { DoubleFieldScript s = scriptFactory .newFactory(fieldName, scriptParams, lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } }	can't help but wonder if these two impls could share more of their code. i guess that it all starts with the factories not sharing a base class, and i vaguely remember talking with @nik9000 about this not being straight-forward...
@Override public ValueFetcher valueFetcher(SearchExecutionContext context, String format) { if (format != null) { throw new IllegalArgumentException("Field [" + name() + "] of type [" + typeName() + "] doesn't support formats."); } if (this.script != null) { // values don't live in source - either pull from dv, or re-calculate if (hasDocValues()) { return new DocValueFetcher(DocValueFormat.RAW, context.getForField(this)); } return new ValueFetcher() { LeafReaderContext ctx; @Override public void setNextReader(LeafReaderContext context) { this.ctx = context; } @Override public List<Object> fetchValues(SourceLookup lookup) { List<Object> values = new ArrayList<>(); script.executeAndIndex(context.lookup(), ctx, lookup.docId(), values::add); values.sort(Comparator.comparingLong(a -> (Long) a)); return values; } }; } return new SourceValueFetcher(name(), context, nullValue) { @Override protected Object parseSourceValue(Object value) { if (value.equals("")) { return nullValue; } return type.parse(value, coerce); } }; }	do we have tests for this scenario?
@Override public ValueFetcher valueFetcher(SearchExecutionContext context, String format) { if (format != null) { throw new IllegalArgumentException("Field [" + name() + "] of type [" + typeName() + "] doesn't support formats."); } if (this.script != null) { // values don't live in source - either pull from dv, or re-calculate if (hasDocValues()) { return new DocValueFetcher(DocValueFormat.RAW, context.getForField(this)); } return new ValueFetcher() { LeafReaderContext ctx; @Override public void setNextReader(LeafReaderContext context) { this.ctx = context; } @Override public List<Object> fetchValues(SourceLookup lookup) { List<Object> values = new ArrayList<>(); script.executeAndIndex(context.lookup(), ctx, lookup.docId(), values::add); values.sort(Comparator.comparingLong(a -> (Long) a)); return values; } }; } return new SourceValueFetcher(name(), context, nullValue) { @Override protected Object parseSourceValue(Object value) { if (value.equals("")) { return nullValue; } return type.parse(value, coerce); } }; }	i believe we're ok with getting the values back in whatever order. we don't, for example, sort the values from source in searchvaluefetcher.
* @param newLookup new mappings of index + snapshot to index metadata identifier * @param newIdentifiers new mappings of index metadata identifier to blob id * @return instance with added snapshot */ public IndexMetaDataGenerations withAddedSnapshot(SnapshotId snapshotId, Map<IndexId, String> newLookup, Map<String, String> newIdentifiers) { final Map<String, String> identifierDeduplicator = new HashMap<>(this.identifiers.size()); for (String identifier : identifiers.keySet()) { identifierDeduplicator.put(identifier, identifier); } final Map<SnapshotId, Map<IndexId, String>> updatedIndexMetaLookup = new HashMap<>(this.lookup); final Map<String, String> updatedIndexMetaIdentifiers = new HashMap<>(identifiers); updatedIndexMetaIdentifiers.putAll(newIdentifiers); updatedIndexMetaLookup.compute(snapshotId, (snId, lookup) -> { if (lookup == null) { if (newLookup.isEmpty()) { return null; } final Map<IndexId, String> fixedLookup = new HashMap<>(newLookup.size()); for (Map.Entry<IndexId, String> entry : newLookup.entrySet()) { final String generation = entry.getValue(); fixedLookup.put(entry.getKey(), identifierDeduplicator.getOrDefault(generation, generation)); } return Map.copyOf(fixedLookup); } else { final Map<IndexId, String> updated = new HashMap<>(lookup); updated.putAll(newLookup); return Map.copyOf(updated); } }); return new IndexMetaDataGenerations(updatedIndexMetaLookup, updatedIndexMetaIdentifiers); }	this is a little brute force but i figured it was the easiest change i can make here to fix the problem. there's some possible ways to optimize this by a lot more but i didn't see the point in investing too much time here for now when this fixes things just as well.
*/ public int countAggregators() { return factories.length; }	haha was this even used anywhere? i don't think i've seen it before :man_facepalming:
public static void registerReaperService(Project project, ProjectLayout projectLayout, boolean internal) { if (project != project.getRootProject()) { throw new IllegalArgumentException("ReaperPlugin can only be applied to the root project of a build"); } File inputDir = projectLayout.getProjectDirectory() .dir(".gradle") .dir("reaper") .dir("build-" + ProcessHandle.current().pid()) .getAsFile(); project.getGradle().getSharedServices().registerIfAbsent(REAPER_SERVICE_NAME, ReaperService.class, spec -> { // Provide some parameters spec.getParameters().getInputDir().set(inputDir); spec.getParameters().getBuildDir().set(projectLayout.getBuildDirectory()); spec.getParameters().setInternal(internal); }); }	remove internal buildparams usage as external usage is never internal anyway.
public void apply(final Project project) { PrecommitTasks.create(project); project.getPluginManager().apply(JavaPlugin.class); project.getPluginManager().apply(TestClustersPlugin.class); project.getPluginManager().apply(CompileOnlyResolvePlugin.class); var extension = project.getExtensions().create(PLUGIN_EXTENSION_NAME, PluginPropertiesExtension.class, project); configureDependencies(project); final var bundleTask = createBundleTasks(project, extension); project.afterEvaluate(project1 -> { project1.getExtensions().getByType(PluginPropertiesExtension.class).getExtendedPlugins().forEach(pluginName -> { // Auto add dependent modules to the test cluster if (project1.findProject(":modules:" + pluginName) != null) { NamedDomainObjectContainer<ElasticsearchCluster> testClusters = testClusters(project, "testClusters"); testClusters.all(elasticsearchCluster -> elasticsearchCluster.module(":modules:" + pluginName)); } }); final var extension1 = project1.getExtensions().getByType(PluginPropertiesExtension.class); configurePublishing(project1, extension1); var name = extension1.getName(); project1.setProperty("archivesBaseName", name); project1.setDescription(extension1.getDescription()); if (extension1.getName() == null) { throw new InvalidUserDataException("name is a required setting for esplugin"); } if (extension1.getDescription() == null) { throw new InvalidUserDataException("description is a required setting for esplugin"); } if (extension1.getType().equals(PluginType.BOOTSTRAP) == false && extension1.getClassname() == null) { throw new InvalidUserDataException("classname is a required setting for esplugin"); } Map<String, Object> map = new LinkedHashMap<>(12); map.put("name", extension1.getName()); map.put("description", extension1.getDescription()); map.put("version", extension1.getVersion()); map.put("elasticsearchVersion", Version.fromString(VersionProperties.getElasticsearch()).toString()); map.put("javaVersion", project1.getExtensions().getByType(JavaPluginExtension.class).getTargetCompatibility().toString()); map.put("classname", extension1.getType().equals(PluginType.BOOTSTRAP) ? "" : extension1.getClassname()); map.put("extendedPlugins", extension1.getExtendedPlugins().stream().collect(Collectors.joining(","))); map.put("hasNativeController", extension1.isHasNativeController()); map.put("requiresKeystore", extension1.isRequiresKeystore()); map.put("type", extension1.getType().toString()); map.put("javaOpts", extension1.getJavaOpts()); map.put("licensed", extension1.isLicensed()); project1.getTasks().withType(Copy.class).named("pluginProperties").configure(copy -> { copy.expand(map); copy.getInputs().properties(map); }); }); project.getConfigurations().getByName("default").extendsFrom(project.getConfigurations().getByName("runtimeClasspath")); // allow running ES with this plugin in the foreground of a build var testClusters = testClusters(project, TestClustersPlugin.EXTENSION_NAME); final var runCluster = testClusters.create("runTask", cluster -> { if (GradleUtils.isModuleProject(project.getPath())) { cluster.module(bundleTask.flatMap((Transformer<Provider<RegularFile>, Zip>) zip -> zip.getArchiveFile())); } else { cluster.plugin(bundleTask.flatMap((Transformer<Provider<RegularFile>, Zip>) zip -> zip.getArchiveFile())); } }); project.getTasks().register("run", RunTask.class, runTask -> { runTask.useCluster(runCluster); runTask.dependsOn(project.getTasks().named("bundlePlugin")); }); }	all internal specific logic moved into internalpluginbuildplugin
private static void configureDependencies(final Project project) { var dependencies = project.getDependencies(); dependencies.add("compileOnly", "org.elasticsearch:elasticsearch:" + VersionProperties.getElasticsearch()); dependencies.add("testImplementation", "org.elasticsearch.test:framework:" + VersionProperties.getElasticsearch()); // we "upgrade" these optional deps to provided for plugins, since they will run // with a full elasticsearch server that includes optional deps dependencies.add("compileOnly", "org.locationtech.spatial4j:spatial4j:" + VersionProperties.getVersions().get("spatial4j")); dependencies.add("compileOnly", "org.locationtech.jts:jts-core:" + VersionProperties.getVersions().get("jts")); dependencies.add("compileOnly", "org.apache.logging.log4j:log4j-api:" + VersionProperties.getVersions().get("log4j")); dependencies.add("compileOnly", "org.apache.logging.log4j:log4j-core:" + VersionProperties.getVersions().get("log4j")); dependencies.add("compileOnly", "org.elasticsearch:jna:" + VersionProperties.getVersions().get("jna")); }	we handle this differently and substitute it in our internal build to avoid using internal buildparams
public TaskProvider<? extends Task> createTask(Project project) { Configuration jarHellConfig = project.getConfigurations().create("jarHell"); TaskProvider<JarHellTask> jarHell = project.getTasks().register("jarHell", JarHellTask.class); jarHell.configure(t -> { SourceSet testSourceSet = Util.getJavaTestSourceSet(project).get(); t.setClasspath(testSourceSet.getRuntimeClasspath()); t.setJarHellRuntimeClasspath(jarHellConfig); }); return jarHell; }	we moved setting the dependencies here to internalprecommittask. to not rely on internal buildparams and keep this public plugin cleaner with none internal specific polluted code. we could alternatively have an internaljarhellprecommitplugin but as we want to revisit default dependency declarations in our plugins and our internal build that's likely too much.
public static void writeRawField(String field, BytesReference source, XContentBuilder builder, ToXContent.Params params) throws IOException { Compressor compressor = CompressorFactory.compressor(source); if (compressor != null) { InputStream compressedStreamInput = compressor.streamInput(source.streamInput()); builder.rawField(field, compressedStreamInput); } else { builder.rawField(field, source); } } /** * Returns the bytes that represent the XContent output of the provided {@link ToXContent} object, using the provided * {@link XContentType}	can we have a test for this?
public static void assertDirectoryExists(Path dir) { assertFileExists(dir); assertThat("file [" + dir + "] should be a directory.", Files.isDirectory(dir), is(true)); } /** * Asserts that the provided {@link BytesReference}	i removed this shenanigans on master: https://github.com/elastic/elasticsearch/commit/38914f17edff05801ccda1f818bf7fef0cd33896 cc @tlrx @cbuescher
@Override protected Collection<Class<? extends Plugin>> nodePlugins() { return Arrays.asList(LocalStateEQLXPackPlugin.class, ReindexPlugin.class); }	what's the usage of reindexplugin?
private static boolean checkGroupBy(LogicalPlan p, Set<Failure> localFailures, Map<String, Function> resolvedFunctions, Set<LogicalPlan> groupingFailures) { return checkGroupByAgg(p, localFailures, resolvedFunctions) && checkGroupByOrder(p, localFailures, groupingFailures) && checkGroupByHaving(p, localFailures, groupingFailures, resolvedFunctions); }	i assume the removed parameters were not needed inside the methods?
private static void validateConditional(LogicalPlan p, Set<Failure> localFailures) { p.forEachExpressions(e -> e.forEachUp((ConditionalFunction cf) -> { int idx = 0; DataType dt = DataType.NULL; // Find the data type of the first non-null argument for (; idx < cf.children().size(); idx++) { DataType childDT = cf.children().get(idx).dataType(); if (childDT != DataType.NULL) { dt = childDT; break; } } // Compare the data type of the rest of the arguments with the data type of first non-null argument for (; idx < cf.children().size(); idx++) { Expression value = cf.children().get(idx); if (areTypesCompatible(dt, value.dataType()) == false) { localFailures.add(fail(value, "expected data type [%s], value provided is of type [%s]", dt, value.dataType())); return; } } }, ConditionalFunction.class)); }	i opt for expressions.isnull as it might catch null literals with a different type than null.
private static void validateConditional(LogicalPlan p, Set<Failure> localFailures) { p.forEachExpressions(e -> e.forEachUp((ConditionalFunction cf) -> { int idx = 0; DataType dt = DataType.NULL; // Find the data type of the first non-null argument for (; idx < cf.children().size(); idx++) { DataType childDT = cf.children().get(idx).dataType(); if (childDT != DataType.NULL) { dt = childDT; break; } } // Compare the data type of the rest of the arguments with the data type of first non-null argument for (; idx < cf.children().size(); idx++) { Expression value = cf.children().get(idx); if (areTypesCompatible(dt, value.dataType()) == false) { localFailures.add(fail(value, "expected data type [%s], value provided is of type [%s]", dt, value.dataType())); return; } } }, ConditionalFunction.class)); }	the loops should be folded into one for-loop with an if/else: java datatype dt = datatype.null; for (expression child : cf.children()) { if (dt == null) { if (expressions.isnull(child) == false) { dt = child.datatype(); } } else { if (aretypescompatible(...)) { ... } } }
@Override public DocValuesFormat getDocValuesFormatForField(String field) { FieldMappers indexName = mapperService.indexName(field); if (indexName == null) { // The _parent field produces doc values fields that start with the prefix: _parent // Since those fields don't match with the index field name of the _parent field itself, this would print // a warning. Catching this on this point seems like the cleanest solution to me. if (field.startsWith(ParentFieldMapper.NAME)) { int indexOf = field.indexOf('#'); if (indexOf != -1) { String type = field.substring(indexOf + 1); DocumentMapper documentMapper = mapperService.documentMapper(type); if (documentMapper != null) { indexName = documentMapper.mappers().indexName(ParentFieldMapper.NAME); } } } } if (indexName == null) { logger.warn("no index mapper found for field: [{}] returning default doc values format", field); return defaultDocValuesFormat; } DocValuesFormatProvider docValuesFormat = indexName.mapper().docValuesFormatProvider(); return docValuesFormat != null ? docValuesFormat.get() : defaultDocValuesFormat; }	maybe it could be more concise by having something like: java private static final string parent_prefix = parentfieldmapper.name + "#"; public docvaluesformat getdocvaluesformatforfield(string field) { fieldmappers indexname = mapperservice.indexname(field); if (indexname == null && field.startswith(parent_prefix)) { string type = field.substring(parent_prefix.length()); [...] } }
public Settings indexSettings() { return ImmutableSettings.builder() .put(super.indexSettings()) // Before 1.4.0 we don't store parent ids in doc values, after 1.4.0 we do. .put(IndexMetaData.SETTING_VERSION_CREATED, randomBoolean() ? Version.V_1_3_0 : Version.V_1_4_0) .put(ParentChildIndexFieldData.PARENT_DOC_VALUES, randomBoolean()) .build(); }	should it rather be in the random index template in elasticsearchintegrationtest?
@Override public Expression visitComparison(ComparisonContext ctx) { Expression left = expression(ctx.left); Expression right = expression(ctx.right); TerminalNode op = (TerminalNode) ctx.comparisonOperator().getChild(0); Location loc = source(ctx); switch (op.getSymbol().getType()) { case SqlBaseParser.EQ: return new Equals(loc, left, right); case SqlBaseParser.NULLEQ: // Simplify to IS NULL to avoid special handling in QueryTranslator later on if (right.equals(NULL)) { return new IsNull(loc, left); } if (left.equals(NULL)) { return new IsNull(loc, right); } return new NullEquals(loc, left, right); case SqlBaseParser.NEQ: return new NotEquals(loc, left, right); case SqlBaseParser.LT: return new LessThan(loc, left, right); case SqlBaseParser.LTE: return new LessThanOrEqual(loc, left, right); case SqlBaseParser.GT: return new GreaterThan(loc, left, right); case SqlBaseParser.GTE: return new GreaterThanOrEqual(loc, left, right); default: throw new ParsingException(loc, "Unknown operator {}", op.getSymbol().getText()); } }	this logic should be optimizer (changing the tree) instead of the parser which should be pretty accurate to the actual query (helps with reporting errors). further more it would make this apply if one of the expressions becomes null in folding and such.
public void testPivotWithWeightedAvgAgg() throws Exception { String transformId = "weightedAvgAggTransform"; String dataFrameIndex = "weighted_avg_pivot_reviews"; setupDataAccessRole(DATA_ACCESS_ROLE, REVIEWS_INDEX_NAME, dataFrameIndex); final Request createDataframeTransformRequest = createRequestWithAuth("PUT", DATAFRAME_ENDPOINT + transformId, BASIC_AUTH_VALUE_DATA_FRAME_ADMIN_WITH_SOME_DATA_ACCESS); String config = "{" + " \\\\"source\\\\": {\\\\"index\\\\":\\\\"" + REVIEWS_INDEX_NAME + "\\\\"}," + " \\\\"dest\\\\": {\\\\"index\\\\":\\\\"" + dataFrameIndex + "\\\\"},"; config += " \\\\"pivot\\\\": {" + " \\\\"group_by\\\\": {" + " \\\\"reviewer\\\\": {" + " \\\\"terms\\\\": {" + " \\\\"field\\\\": \\\\"user_id\\\\"" + " } } }," + " \\\\"aggregations\\\\": {" + " \\\\"avg_rating\\\\": {" + " \\\\"weighted_avg\\\\": {" + " \\\\"value\\\\": {\\\\"field\\\\": \\\\"stars\\\\"}," + " \\\\"weight\\\\": {\\\\"field\\\\": \\\\"stars\\\\"}" + "} } } }" + "}"; createDataframeTransformRequest.setJsonEntity(config); Map<String, Object> createDataframeTransformResponse = entityAsMap(client().performRequest(createDataframeTransformRequest)); assertThat(createDataframeTransformResponse.get("acknowledged"), equalTo(Boolean.TRUE)); startAndWaitForTransform(transformId, dataFrameIndex, BASIC_AUTH_VALUE_DATA_FRAME_ADMIN_WITH_SOME_DATA_ACCESS); assertTrue(indexExists(dataFrameIndex)); Map<String, Object> searchResult = getAsMap(dataFrameIndex + "/_search?q=reviewer:user_4"); assertEquals(1, XContentMapValues.extractValue("hits.total.value", searchResult)); Number actual = (Number) ((List<?>) XContentMapValues.extractValue("hits.hits._source.avg_rating", searchResult)).get(0); assertEquals(4.47169811, actual.doubleValue(), 0.000001); }	no need to do concatenation, can be done in declaration of "string config".
public void testAddressInterfaceLookup() throws Exception { for (NetworkInterface netIf : NetworkUtils.getInterfaces()) { try { if (!netIf.isUp() || Collections.list(netIf.getInetAddresses()).isEmpty()) { continue; } } catch (SocketException e) { // On some systems listing all network interfaces returns interfaces that throw // java.net.SocketException: No such device (getFlags() failed) on an isUp check. We just skip those cases here // and test the properly behaved interfaces. continue; } String name = netIf.getName(); InetAddress[] expectedAddresses = Collections.list(netIf.getInetAddresses()).toArray(new InetAddress[0]); InetAddress[] foundAddresses = NetworkUtils.getAddressesForInterface(name); assertArrayEquals(expectedAddresses, foundAddresses); } }	i would rather that we gather some reconnaissance on what the failing interface is. i suspect it's a docker veth interface, but i don't think to date we've collected any evidence of this. i would rather we do something like report the interface and *still* fail the test today (so that we are compelled to look closely at what is going on). it's only when we understand the failure more deeply that we can consider the right solution.
private static String[] splitAndValidatePath(String fullFieldPath) { if (fullFieldPath.contains(".")) { String[] parts = fullFieldPath.split("\\\\\\\\."); for (String part : parts) { if (Strings.hasText(part) == false) { // check if the field name contains only white spaces if (Strings.isEmpty(part) == false) { throw new IllegalArgumentException( "object field cannot contain only white spaces: ['" + fullFieldPath + "']"); } throw new IllegalArgumentException( "object field starting or ending with a [.] makes object resolution ambiguous: [" + fullFieldPath + "]"); } } return parts; } else { if (Strings.isEmpty(fullFieldPath)) { throw new IllegalArgumentException("field name cannot be an empty string"); } return new String[] {fullFieldPath}; } }	this should be "white spaces" -> "whitespace"
public void testDynamicFieldsEmptyName() throws Exception { BytesReference bytes = XContentFactory.jsonBuilder() .startObject().startArray("top.") .startObject() .startObject("aoeu") .field("a", 1).field(" ", 2) .endObject() .endObject().endArray() .endObject().bytes(); IllegalArgumentException emptyFieldNameException = expectThrows(IllegalArgumentException.class, () -> client().prepareIndex("idx", "type").setSource(bytes, XContentType.JSON).get()); assertThat(emptyFieldNameException.getMessage(), containsString( "object field cannot contain only white spaces: ['top.aoeu. ']")); }	white spaces -> "whitespace"
@Override protected void masterOperation(final RestoreSnapshotRequest request, final ClusterState state, final ActionListener<RestoreSnapshotResponse> listener) { RestoreService.RestoreRequest restoreRequest = new RestoreService.RestoreRequest(request.repository(), request.snapshot(), request.indices(), request.indicesOptions(), request.renamePattern(), request.renameReplacement(), request.settings(), request.masterNodeTimeout(), request.includeGlobalState(), request.partial(), request.includeAliases(), request.indexSettings(), request.ignoreIndexSettings(), "restore_snapshot[" + request.snapshot() + "]"); restoreService.restoreSnapshot(restoreRequest, new ActionListener<RestoreCompletionResponse>() { @Override public void onResponse(RestoreCompletionResponse restoreCompletionResponse) { if (restoreCompletionResponse.getRestoreInfo() == null && request.waitForCompletion()) { final Snapshot snapshot = restoreCompletionResponse.getSnapshot(); String uuid = restoreCompletionResponse.getUuid(); ClusterStateListener clusterStateListener = new ClusterStateListener() { @Override public void clusterChanged(ClusterChangedEvent changedEvent) { final RestoreInProgress.Entry prevEntry = restoreInProgress(changedEvent.previousState(), uuid); final RestoreInProgress.Entry newEntry = restoreInProgress(changedEvent.state(), uuid); if (prevEntry == null) { // When there is a master failure after a restore has been started, this listener might not be registered // on the current master and as such it might miss some intermediary cluster states due to batching. // Clean up listener in that case and acknowledge completion of restore operation to client. clusterService.removeListener(this); listener.onResponse(new RestoreSnapshotResponse(null)); } else if (newEntry == null) { clusterService.removeListener(this); ImmutableOpenMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards = prevEntry.shards(); assert prevEntry.state().completed() : "expected completed snapshot state but was " + prevEntry.state(); assert RestoreService.completed(shards) : "expected all restore entries to be completed"; RestoreInfo ri = new RestoreInfo(prevEntry.snapshot().getSnapshotId().getName(), prevEntry.indices(), shards.size(), shards.size() - RestoreService.failedShards(shards)); RestoreSnapshotResponse response = new RestoreSnapshotResponse(ri); logger.debug("restore of [{}] completed", snapshot); listener.onResponse(response); } else { // restore not completed yet, wait for next cluster state update } } }; clusterService.addListener(clusterStateListener); } else { listener.onResponse(new RestoreSnapshotResponse(restoreCompletionResponse.getRestoreInfo())); } } @Override public void onFailure(Exception t) { listener.onFailure(t); } }); }	lookup of in progress restores works by string uuid now :)
@Override public Decision canAllocate(final ShardRouting shardRouting, final RoutingAllocation allocation) { final RecoverySource recoverySource = shardRouting.recoverySource(); if (recoverySource == null || recoverySource.getType() != RecoverySource.Type.SNAPSHOT) { return allocation.decision(Decision.YES, NAME, "ignored as shard is not being recovered from a snapshot"); } RecoverySource.SnapshotRecoverySource source = (RecoverySource.SnapshotRecoverySource) recoverySource; final RestoreInProgress restoresInProgress = allocation.custom(RestoreInProgress.TYPE); if (restoresInProgress != null) { RestoreInProgress.Entry restoreInProgress = restoresInProgress.entries().get(source.restoreUUID()); if (restoreInProgress != null) { RestoreInProgress.ShardRestoreStatus shardRestoreStatus = restoreInProgress.shards().get(shardRouting.shardId()); if (shardRestoreStatus != null && shardRestoreStatus.state().completed() == false) { assert shardRestoreStatus.state() != RestoreInProgress.State.SUCCESS : "expected shard [" + shardRouting + "] to be in initializing state but got [" + shardRestoreStatus.state() + "]"; return allocation.decision(Decision.YES, NAME, "shard is currently being restored"); } } } return allocation.decision(Decision.NO, NAME, "shard has failed to be restored from the snapshot [%s] because of [%s] - " + "manually close or delete the index [%s] in order to retry to restore the snapshot again or use the reroute API to force the " + "allocation of an empty primary shard", source.snapshot(), shardRouting.unassignedInfo().getDetails(), shardRouting.getIndexName()); }	i added the new restore operation uuid to instances of recoverysource.snapshotrecoverysource => now we have a simple map lookup on the restore entries map here instead of the loop.
public static RestoreInProgress.Entry restoreInProgress(ClusterState state, String uuid) { final RestoreInProgress restoreInProgress = state.custom(RestoreInProgress.TYPE); if (restoreInProgress != null) { return restoreInProgress.entries().get(uuid); } return null; }	identify task by restore uuid now so we can have multiple snapshots from the same snapshot.
protected static DocValueFormat randomNumericDocValueFormat() { final List<Supplier<DocValueFormat>> formats = new ArrayList<>(3); formats.add(() -> DocValueFormat.RAW); formats.add(() -> DocValueFormat.BOOLEAN); formats.add(() -> new DocValueFormat.Decimal(randomFrom("###.##", "###,###.##"))); return randomFrom(formats).get(); }	is this a numeric doc value format?
public void testSimple() throws Exception { MappedFieldType fieldType = new KeywordFieldMapper.KeywordFieldType("string", randomBoolean(), true, null); TermsAggregationBuilder aggregationBuilder = new TermsAggregationBuilder("_name") .executionHint(randomFrom(TermsAggregatorFactory.ExecutionMode.values()).toString()) .field("string") .order(BucketOrder.key(true)); testCase(aggregationBuilder, new MatchAllDocsQuery(), iw -> { iw.addDocument(doc(fieldType, "a", "b")); iw.addDocument(doc(fieldType, "", "c", "a")); iw.addDocument(doc(fieldType, "b", "d")); iw.addDocument(doc(fieldType, "")); }, (InternalTerms<?, ?> result) -> { assertEquals(5, result.getBuckets().size()); assertEquals("", result.getBuckets().get(0).getKeyAsString()); assertEquals(2L, result.getBuckets().get(0).getDocCount()); assertEquals("a", result.getBuckets().get(1).getKeyAsString()); assertEquals(2L, result.getBuckets().get(1).getDocCount()); assertEquals("b", result.getBuckets().get(2).getKeyAsString()); assertEquals(2L, result.getBuckets().get(2).getDocCount()); assertEquals("c", result.getBuckets().get(3).getKeyAsString()); assertEquals(1L, result.getBuckets().get(3).getDocCount()); assertEquals("d", result.getBuckets().get(4).getKeyAsString()); assertEquals(1L, result.getBuckets().get(4).getDocCount()); assertTrue(AggregationInspectionHelper.hasValue(result)); }, fieldType); }	since i had to do some large-ish changes to this anyway i decided to move it to our normal testcase method.
@Override public DenseVectorFieldMapper build(BuilderContext context) { setupFieldType(context); return new DenseVectorFieldMapper( name, fieldType, defaultFieldType, context.indexSettings(), multiFieldsBuilder.build(this, context), copyTo); } } public static class TypeParser implements Mapper.TypeParser { @Override public Mapper.Builder<?,?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { DenseVectorFieldMapper.Builder builder = new DenseVectorFieldMapper.Builder(name); return builder; } } public static final class DenseVectorFieldType extends MappedFieldType { public DenseVectorFieldType() {} protected DenseVectorFieldType(DenseVectorFieldType ref) { super(ref); } public DenseVectorFieldType clone() { return new DenseVectorFieldType(this); } @Override public String typeName() { return CONTENT_TYPE; } @Override public DocValueFormat docValueFormat(String format, DateTimeZone timeZone) { return DocValueFormat.BINARY; } @Override public Query existsQuery(QueryShardContext context) { return new DocValuesFieldExistsQuery(name()); } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { throw new UnsupportedOperationException("[dense_vector] fields do not support sorting, scripting or aggregating"); } @Override public Query termQuery(Object value, QueryShardContext context) { throw new UnsupportedOperationException("Queries on [dense_vector] fields are not supported"); } } private DenseVectorFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); assert fieldType.indexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) <= 0; } @Override protected DenseVectorFieldMapper clone() { return (DenseVectorFieldMapper) super.clone(); } @Override public DenseVectorFieldType fieldType() { return (DenseVectorFieldType) super.fieldType(); } @Override public void parse(ParseContext context) throws IOException { if (context.externalValueSet()) { throw new IllegalArgumentException("[dense_vector] field can't be used in multi-fields"); } // encode array of floats into buf // buf contains an integer - number of dimensions, followed byn array of floats encoded as integers byte[] buf = new byte[INT_BYTES + 10 * INT_BYTES]; // initially allocating buffer for 10 dimensions int offset = INT_BYTES; int dim = 0; for (Token token = context.parser().nextToken(); token != Token.END_ARRAY; token = context.parser().nextToken()) { if (token == Token.VALUE_NUMBER) { float value = context.parser().floatValue(true); if (buf.length < (offset + INT_BYTES)) { buf = ArrayUtil.grow(buf, (offset + INT_BYTES)); } NumericUtils.intToSortableBytes(Float.floatToIntBits(value), buf, offset); offset = offset + INT_BYTES; dim++; } else { throw new IllegalArgumentException("[dense_vector] field takes an array of floats, but got unexpected token " + token); } } NumericUtils.intToSortableBytes(dim, buf, 0); //recording number of dimensions at the beginning BinaryDocValuesField field = new BinaryDocValuesField(fieldType().name(), new BytesRef(buf, 0, offset)); context.doc().addWithKey(fieldType().name(), field); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) { throw new AssertionError("parse is implemented directly"); } @Override protected String contentType() { return CONTENT_TYPE; } //**************STATIC HELPER METHODS*********************************** // Decodes a BytesRef into a dense array <code>vector</code> // TODO: possibly have another type of DocValuesField where an array of floats can be already decoded public static float[] decodeVector(BytesRef vectorBR) { int dimCount = NumericUtils.sortableBytesToInt(vectorBR.bytes, vectorBR.offset); float[] vector = new float[dimCount]; int offset = vectorBR.offset; for (int dim = 0; dim < dimCount; dim++) { offset = offset + INT_BYTES; vector[dim] = Float.intBitsToFloat(NumericUtils.sortableBytesToInt(vectorBR.bytes, offset)); }	do we need sortability or are we using this existing method for convenience?
@Override public DenseVectorFieldMapper build(BuilderContext context) { setupFieldType(context); return new DenseVectorFieldMapper( name, fieldType, defaultFieldType, context.indexSettings(), multiFieldsBuilder.build(this, context), copyTo); } } public static class TypeParser implements Mapper.TypeParser { @Override public Mapper.Builder<?,?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { DenseVectorFieldMapper.Builder builder = new DenseVectorFieldMapper.Builder(name); return builder; } } public static final class DenseVectorFieldType extends MappedFieldType { public DenseVectorFieldType() {} protected DenseVectorFieldType(DenseVectorFieldType ref) { super(ref); } public DenseVectorFieldType clone() { return new DenseVectorFieldType(this); } @Override public String typeName() { return CONTENT_TYPE; } @Override public DocValueFormat docValueFormat(String format, DateTimeZone timeZone) { return DocValueFormat.BINARY; } @Override public Query existsQuery(QueryShardContext context) { return new DocValuesFieldExistsQuery(name()); } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { throw new UnsupportedOperationException("[dense_vector] fields do not support sorting, scripting or aggregating"); } @Override public Query termQuery(Object value, QueryShardContext context) { throw new UnsupportedOperationException("Queries on [dense_vector] fields are not supported"); } } private DenseVectorFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); assert fieldType.indexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) <= 0; } @Override protected DenseVectorFieldMapper clone() { return (DenseVectorFieldMapper) super.clone(); } @Override public DenseVectorFieldType fieldType() { return (DenseVectorFieldType) super.fieldType(); } @Override public void parse(ParseContext context) throws IOException { if (context.externalValueSet()) { throw new IllegalArgumentException("[dense_vector] field can't be used in multi-fields"); } // encode array of floats into buf // buf contains an integer - number of dimensions, followed byn array of floats encoded as integers byte[] buf = new byte[INT_BYTES + 10 * INT_BYTES]; // initially allocating buffer for 10 dimensions int offset = INT_BYTES; int dim = 0; for (Token token = context.parser().nextToken(); token != Token.END_ARRAY; token = context.parser().nextToken()) { if (token == Token.VALUE_NUMBER) { float value = context.parser().floatValue(true); if (buf.length < (offset + INT_BYTES)) { buf = ArrayUtil.grow(buf, (offset + INT_BYTES)); } NumericUtils.intToSortableBytes(Float.floatToIntBits(value), buf, offset); offset = offset + INT_BYTES; dim++; } else { throw new IllegalArgumentException("[dense_vector] field takes an array of floats, but got unexpected token " + token); } } NumericUtils.intToSortableBytes(dim, buf, 0); //recording number of dimensions at the beginning BinaryDocValuesField field = new BinaryDocValuesField(fieldType().name(), new BytesRef(buf, 0, offset)); context.doc().addWithKey(fieldType().name(), field); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) { throw new AssertionError("parse is implemented directly"); } @Override protected String contentType() { return CONTENT_TYPE; } //**************STATIC HELPER METHODS*********************************** // Decodes a BytesRef into a dense array <code>vector</code> // TODO: possibly have another type of DocValuesField where an array of floats can be already decoded public static float[] decodeVector(BytesRef vectorBR) { int dimCount = NumericUtils.sortableBytesToInt(vectorBR.bytes, vectorBR.offset); float[] vector = new float[dimCount]; int offset = vectorBR.offset; for (int dim = 0; dim < dimCount; dim++) { offset = offset + INT_BYTES; vector[dim] = Float.intBitsToFloat(NumericUtils.sortableBytesToInt(vectorBR.bytes, offset)); }	do we need this? we could figure out the number of dimensions by dividing the array length by 4?
@Override public DenseVectorFieldMapper build(BuilderContext context) { setupFieldType(context); return new DenseVectorFieldMapper( name, fieldType, defaultFieldType, context.indexSettings(), multiFieldsBuilder.build(this, context), copyTo); } } public static class TypeParser implements Mapper.TypeParser { @Override public Mapper.Builder<?,?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { DenseVectorFieldMapper.Builder builder = new DenseVectorFieldMapper.Builder(name); return builder; } } public static final class DenseVectorFieldType extends MappedFieldType { public DenseVectorFieldType() {} protected DenseVectorFieldType(DenseVectorFieldType ref) { super(ref); } public DenseVectorFieldType clone() { return new DenseVectorFieldType(this); } @Override public String typeName() { return CONTENT_TYPE; } @Override public DocValueFormat docValueFormat(String format, DateTimeZone timeZone) { return DocValueFormat.BINARY; } @Override public Query existsQuery(QueryShardContext context) { return new DocValuesFieldExistsQuery(name()); } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { throw new UnsupportedOperationException("[dense_vector] fields do not support sorting, scripting or aggregating"); } @Override public Query termQuery(Object value, QueryShardContext context) { throw new UnsupportedOperationException("Queries on [dense_vector] fields are not supported"); } } private DenseVectorFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); assert fieldType.indexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) <= 0; } @Override protected DenseVectorFieldMapper clone() { return (DenseVectorFieldMapper) super.clone(); } @Override public DenseVectorFieldType fieldType() { return (DenseVectorFieldType) super.fieldType(); } @Override public void parse(ParseContext context) throws IOException { if (context.externalValueSet()) { throw new IllegalArgumentException("[dense_vector] field can't be used in multi-fields"); } // encode array of floats into buf // buf contains an integer - number of dimensions, followed byn array of floats encoded as integers byte[] buf = new byte[INT_BYTES + 10 * INT_BYTES]; // initially allocating buffer for 10 dimensions int offset = INT_BYTES; int dim = 0; for (Token token = context.parser().nextToken(); token != Token.END_ARRAY; token = context.parser().nextToken()) { if (token == Token.VALUE_NUMBER) { float value = context.parser().floatValue(true); if (buf.length < (offset + INT_BYTES)) { buf = ArrayUtil.grow(buf, (offset + INT_BYTES)); } NumericUtils.intToSortableBytes(Float.floatToIntBits(value), buf, offset); offset = offset + INT_BYTES; dim++; } else { throw new IllegalArgumentException("[dense_vector] field takes an array of floats, but got unexpected token " + token); } } NumericUtils.intToSortableBytes(dim, buf, 0); //recording number of dimensions at the beginning BinaryDocValuesField field = new BinaryDocValuesField(fieldType().name(), new BytesRef(buf, 0, offset)); context.doc().addWithKey(fieldType().name(), field); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) { throw new AssertionError("parse is implemented directly"); } @Override protected String contentType() { return CONTENT_TYPE; } //**************STATIC HELPER METHODS*********************************** // Decodes a BytesRef into a dense array <code>vector</code> // TODO: possibly have another type of DocValuesField where an array of floats can be already decoded public static float[] decodeVector(BytesRef vectorBR) { int dimCount = NumericUtils.sortableBytesToInt(vectorBR.bytes, vectorBR.offset); float[] vector = new float[dimCount]; int offset = vectorBR.offset; for (int dim = 0; dim < dimCount; dim++) { offset = offset + INT_BYTES; vector[dim] = Float.intBitsToFloat(NumericUtils.sortableBytesToInt(vectorBR.bytes, offset)); }	it seems like this shouldn't support multi-valued fields. you could do that by failing if context.doc().getbykey is not null.
protected void doExecute(Task task, CreateTokenRequest request, ActionListener<CreateTokenResponse> listener) { tokenService.refreshToken(request.getRefreshToken(), ActionListener.wrap(tokenResult -> { final String scope = getResponseScopeValue(request.getScope()); final CreateTokenResponse response = new CreateTokenResponse(tokenResult.getAccessToken(), tokenService.getExpirationDelay(), scope, tokenResult.getRefreshToken(), null, tokenResult.getAuthentication()); listener.onResponse(response); }, listener::onFailure)); }	there are 3 authentication objects here: 1. the originating authentication 2. the authentication for which the token is created 3. the token's own authentication we had brief discussion during the team meeting and agreed that above item 2 should be returned. however, we are returning item 3 here. the tokenresult.getauthentication() is the authentication object of the newly created token, while no.2 is technically the old access token's authentication object. i don't think it could cause any practical problem, in fact, no. 2 and 3 should be identical here. but it is strictly speaking an inconsistency. i am raising it here in case it's worth any discussion.
boolean isExpirationInProgress() { return expiredTokenRemover.isExpirationInProgress(); }	this class can be static since it does not need need access any instance variables.
private void createOAuth2Tokens(String accessToken, String refreshToken, Version tokenVersion, SecurityIndexManager tokensIndex, Authentication authentication, Authentication originatingClientAuth, Map<String, Object> metadata, ActionListener<CreateTokenResult> listener) { assert accessToken.length() == TOKEN_LENGTH : "We assume token ids have a fixed length for nodes of a certain version." + " When changing the token length, be careful that the inferences about its length still hold."; ensureEnabled(); if (authentication == null) { listener.onFailure(traceLog("create token", new IllegalArgumentException("authentication must be provided"))); } else if (originatingClientAuth == null) { listener.onFailure(traceLog("create token", new IllegalArgumentException("originating client authentication must be provided"))); } else { final Authentication tokenAuth = new Authentication(authentication.getUser(), authentication.getAuthenticatedBy(), authentication.getLookedUpBy(), tokenVersion, AuthenticationType.TOKEN, authentication.getMetadata()); final String storedAccessToken; final String storedRefreshToken; if (tokenVersion.onOrAfter(VERSION_HASHED_TOKENS)) { storedAccessToken = hashTokenString(accessToken); storedRefreshToken = (null == refreshToken) ? null : hashTokenString(refreshToken); } else { storedAccessToken = accessToken; storedRefreshToken = refreshToken; } final UserToken userToken = new UserToken(storedAccessToken, tokenVersion, tokenAuth, getExpirationTime(), metadata); final BytesReference tokenDocument = createTokenDocument(userToken, storedRefreshToken, originatingClientAuth); final String documentId = getTokenDocumentId(storedAccessToken); final IndexRequest indexTokenRequest = client.prepareIndex(tokensIndex.aliasName()).setId(documentId) .setOpType(OpType.CREATE) .setSource(tokenDocument, XContentType.JSON) .setRefreshPolicy(RefreshPolicy.WAIT_UNTIL) .request(); tokensIndex.prepareIndexIfNeededThenExecute( ex -> listener.onFailure(traceLog("prepare tokens index [" + tokensIndex.aliasName() + "]", documentId, ex)), () -> executeAsyncWithOrigin(client, SECURITY_ORIGIN, IndexAction.INSTANCE, indexTokenRequest, ActionListener.wrap(indexResponse -> { if (indexResponse.getResult() == Result.CREATED) { final String versionedAccessToken = prependVersionAndEncodeAccessToken(tokenVersion, accessToken); if (tokenVersion.onOrAfter(VERSION_TOKENS_INDEX_INTRODUCED)) { final String versionedRefreshToken = refreshToken != null ? prependVersionAndEncodeRefreshToken(tokenVersion, refreshToken) : null; listener.onResponse(new CreateTokenResult(versionedAccessToken, versionedRefreshToken, tokenAuth)); } else { // prior versions of the refresh token are not version-prepended, as nodes on those // versions don't expect it. // Such nodes might exist in a mixed cluster during a rolling upgrade. listener.onResponse(new CreateTokenResult(versionedAccessToken, refreshToken,tokenAuth)); } } else { listener.onFailure(traceLog("create token", new ElasticsearchException("failed to create token document [{}]", indexResponse))); } }, listener::onFailure))); } }	please also update the javadoc
private Tuple<String, String> storeToken(String userTokenId, String refreshToken, SamlNameId nameId, String session) { Authentication authentication = new Authentication(new User("bob"), new RealmRef("native", NativeRealmSettings.TYPE, "node01"), null); final Map<String, Object> metadata = samlRealm.createTokenMetadata(nameId, session); final PlainActionFuture<TokenService.CreateTokenResult> future = new PlainActionFuture<>(); tokenService.createOAuth2Tokens(userTokenId, refreshToken, authentication, authentication, metadata, future); return new Tuple<>(future.actionGet().getAccessToken(), future.actionGet().getRefreshToken()); }	nit: i think we can update those methods in here to return a createtokenresult and we could also test that the authentication object is what we expect
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); index = in.readString(); type = in.readString(); id = in.readString(); version = in.readLong(); created = in.readBoolean(); noop = in.readBoolean(); if (in.readBoolean()) { getResult = GetResult.readGetResult(in); } }	i suspect we need version checking for this unless its _just_ going into 2.0
public static Integer compare(Object l, Object r) { // typical number comparison if (l instanceof Number && r instanceof Number) { return compare((Number) l, (Number) r); } try { if (l instanceof Comparable && r instanceof Comparable) { return Integer.valueOf(((Comparable) l).compareTo(r)); } } catch (ClassCastException cce) { // types are not compatible // return null } return null; }	what are the side effects on not catching this and letting it bubble out? or even wrapping it? i think it'd be nice to have a big comment here about why returning null is ok. also, can you move the try into the if statement so it looks smaller? that'd just make me a bit more comfortable. if returning null is right here then i think you should return it from the catch rather than fall out. that feels a little safer even if it isn't.
@Override public boolean foldable() { if (lower.foldable() && upper.foldable()) { return (excludingBoundaries() || value.foldable()); } return false; }	can you remove the extra ( and )? i don't think they help here.
public void testSingleItemBulkActionIngestLocal() throws Exception { Exception exception = new Exception("fake exception"); IndexRequest indexRequest = new IndexRequest("index", "type", "id"); indexRequest.source(Collections.emptyMap()); indexRequest.setPipeline("testpipeline"); AtomicBoolean responseCalled = new AtomicBoolean(false); AtomicBoolean failureCalled = new AtomicBoolean(false); singleItemBulkWriteAction.execute(null, indexRequest, ActionListener.wrap( response -> { responseCalled.set(true); }, e -> { assertThat(e, sameInstance(exception)); failureCalled.set(true); })); // check failure works, and passes through to the listener assertFalse(action.isExecuted); // haven't executed yet assertFalse(responseCalled.get()); assertFalse(failureCalled.get()); verify(executionService).executeBulkRequest(bulkDocsItr.capture(), failureHandler.capture(), completionHandler.capture()); completionHandler.getValue().accept(exception); assertTrue(failureCalled.get()); // now check success indexRequest.setPipeline(null); // this is done by the real pipeline execution service when processing completionHandler.getValue().accept(null); assertTrue(action.isExecuted); assertFalse(responseCalled.get()); // listener would only be called by real index action, not our mocked one verifyZeroInteractions(transportService); }	i think we should also verify that interaction happened with ingestservice mock here?
public DataStream rollover(Index writeIndex, long generation) { ensureNotReplicated(); return unsafeRollover(writeIndex, generation); }	maybe use {@link #rollover(index, long)} instead of #rollover in java docs?
private SingleForecast forecast(Metadata metadata, IndexAbstraction.DataStream stream, long forecastWindow, long now) { List<Index> indices = stream.getIndices(); if (dataStreamAllocatedToNodes(metadata, indices) == false) return null; long minCreationDate = Long.MAX_VALUE; long totalSize = 0; int count = 0; while (count < indices.size()) { ++count; IndexMetadata indexMetadata = metadata.index(indices.get(indices.size() - count)); long creationDate = indexMetadata.getCreationDate(); if (creationDate < 0) { return null; } minCreationDate = Math.min(minCreationDate, creationDate); totalSize += state.getRoutingTable().allShards(indexMetadata.getIndex().getName()).stream().mapToLong(this::sizeOf).sum(); // we terminate loop after collecting data to ensure we consider at least the forecast window (and likely some more). if (creationDate <= now - forecastWindow) { break; } } if (totalSize == 0) { return null; } // round up long avgSizeCeil = (totalSize - 1) / count + 1; long actualWindow = now - minCreationDate; if (actualWindow == 0) { return null; } // rather than simulate rollover, we copy the index meta data and do minimal adjustments. long scaledTotalSize; int numberNewIndices; if (actualWindow > forecastWindow) { scaledTotalSize = BigInteger.valueOf(totalSize) .multiply(BigInteger.valueOf(forecastWindow)) .divide(BigInteger.valueOf(actualWindow)) .longValueExact(); // round up numberNewIndices = (int) Math.min((scaledTotalSize - 1) / avgSizeCeil + 1, indices.size()); if (scaledTotalSize == 0) { return null; } } else { numberNewIndices = count; scaledTotalSize = totalSize; } IndexMetadata writeIndex = metadata.index(stream.getWriteIndex()); Map<IndexMetadata, Long> newIndices = new HashMap<>(); DataStream dataStream = stream.getDataStream(); for (int i = 0; i < numberNewIndices; ++i) { final String uuid = UUIDs.randomBase64UUID(); long generation = dataStream.getGeneration() + 1; final String indexName = DataStream.getDefaultBackingIndexName(dataStream.getName(), generation, now) + "-" + uuid; dataStream = dataStream.unsafeRollover(new Index(indexName, uuid), generation); // this unintentionally copies the in-sync allocation ids too. This has the fortunate effect of these indices // not being regarded new by the disk threshold decider, thereby respecting the low watermark threshold even for primaries. // This is highly desirable so fixing this to clear the in-sync allocation ids will require a more elaborate solution, // ensuring at least that when replicas are involved, we still respect the low watermark. This is therefore left as is // for now with the intention to fix in a follow-up. IndexMetadata newIndex = IndexMetadata.builder(writeIndex) .index(indexName) .settings(Settings.builder().put(writeIndex.getSettings()).put(IndexMetadata.SETTING_INDEX_UUID, uuid)) .build(); long size = Math.min(avgSizeCeil, scaledTotalSize - (avgSizeCeil * i)); assert size > 0; newIndices.put(newIndex, size); } return new SingleForecast(newIndices, dataStream); }	maybe add a unsafenextwriteindexandgeneration(...) and use it here? that way the exact same logic remains to be used.
@Override protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) throws IOException { // This will allow to cancel the search request if the http channel is closed RestCancellableNodeClient cancellableNodeClient = new RestCancellableNodeClient(client, restRequest.getHttpChannel()); KnnSearchRequestBuilder request = KnnSearchRequestBuilder.parseRestRequest(restRequest); SearchRequestBuilder searchRequestBuilder = cancellableNodeClient.prepareSearch(); // Forbid filtered aliases in _knn_search request IndicesOptions indicesOptions = searchRequestBuilder.request().indicesOptions(); EnumSet<IndicesOptions.Option> options = indicesOptions.getOptions(); options.add(IndicesOptions.Option.FORBID_FILTERED_ALIASES); IndicesOptions newIndicesOptions = new IndicesOptions(options, indicesOptions.getExpandWildcards()); searchRequestBuilder.setIndicesOptions(newIndicesOptions); request.build(searchRequestBuilder); return channel -> searchRequestBuilder.execute(new RestStatusToXContentListener<>(channel)); }	it'd be nice to move this into knnsearchrequestbuilder, so all request building logic is in one place. this would also let us add a unit test that checks the indices options are as expected.
@Override protected DateTimeProcessor mutateInstance(DateTimeProcessor instance) { DateTimeExtractor replaced = randomValueOtherThan(instance.extractor(), () -> randomFrom(DateTimeExtractor.values())); return new DateTimeProcessor(replaced, randomValueOtherThan(UTC, ESTestCase::randomZone)); }	why do you use here random other than utc?
@Override public void writeByte(byte b) throws IOException { ensureOpen(); ensureCapacity(count); byte[] page = pages.get(count / pageSize).v(); int offset = count % pageSize; page[offset] = b; count++; }	when doing many small writes, it may help to keep a reference on the current page to not have to recompute it every time?
*/ public final void fieldCapsAsync(FieldCapabilitiesRequest fieldCapabilitiesRequest, RequestOptions options, ActionListener<FieldCapabilitiesResponse> listener) { performRequestAsyncAndParseEntity(fieldCapabilitiesRequest, RequestConverters::fieldCaps, options, FieldCapabilitiesResponse::fromXContent, listener, emptySet()); } /** * @deprecated If creating a new HLRC ReST API call, consider creating new actions instead of reusing server actions. The Validation * layer has been added to the ReST client, and requests should extend * {@link org.elasticsearch.protocol.xpack.common.ValidationException}	the new recommended way requires using classes that sit under xpack packages?
*/ public final void fieldCapsAsync(FieldCapabilitiesRequest fieldCapabilitiesRequest, RequestOptions options, ActionListener<FieldCapabilitiesResponse> listener) { performRequestAsyncAndParseEntity(fieldCapabilitiesRequest, RequestConverters::fieldCaps, options, FieldCapabilitiesResponse::fromXContent, listener, emptySet()); } /** * @deprecated If creating a new HLRC ReST API call, consider creating new actions instead of reusing server actions. The Validation * layer has been added to the ReST client, and requests should extend * {@link org.elasticsearch.protocol.xpack.common.ValidationException}	aren't we deprecating the way that all our current api are implemented?
protected final <Req extends Validatable, Resp> Resp performRequestAndParseEntity(Req request, CheckedFunction<Req, Request, IOException> requestConverter, RequestOptions options, CheckedFunction<XContentParser, Resp, IOException> entityParser, Set<Integer> ignores) throws IOException { return performRequest(request, requestConverter, options, response -> parseEntity(response.getEntity(), entityParser), ignores); } /** * @deprecated If creating a new HLRC ReST API call, consider creating new actions instead of reusing server actions. The Validation * layer has been added to the ReST client, and requests should extend * {@link org.elasticsearch.protocol.xpack.common.ValidationException}	shouldn't this link to ... validatable not ... validationexception. same with the other comments
protected void doExecute(Task task, final MultiTermVectorsRequest request, final ActionListener<MultiTermVectorsResponse> listener) { ClusterState clusterState = clusterService.state(); clusterState.blocks().globalBlockedRaiseException(ClusterBlockLevel.READ); final AtomicArray<MultiTermVectorsItemResponse> responses = new AtomicArray<>(request.requests.size()); Map<ShardId, MultiTermVectorsShardRequest> shardRequests = new HashMap<>(); for (int i = 0; i < request.requests.size(); i++) { TermVectorsRequest termVectorsRequest = request.requests.get(i); ShardId shardId; try { termVectorsRequest.routing(clusterState.metadata().resolveIndexRouting(termVectorsRequest.routing(), termVectorsRequest.index())); String concreteSingleIndex = indexNameExpressionResolver.concreteSingleIndex(clusterState, termVectorsRequest).getName(); shardId = clusterService.operationRouting().shardId(clusterState, concreteSingleIndex, termVectorsRequest.id(), termVectorsRequest.routing()); } catch (Exception e) { responses.set(i, new MultiTermVectorsItemResponse(null, new MultiTermVectorsResponse.Failure(termVectorsRequest.index(), termVectorsRequest.id(), e))); continue; } MultiTermVectorsShardRequest shardRequest = shardRequests.get(shardId); if (shardRequest == null) { shardRequest = new MultiTermVectorsShardRequest(shardId.getIndexName(), shardId.id()); shardRequest.preference(request.preference); shardRequests.put(shardId, shardRequest); } shardRequest.add(i, termVectorsRequest); } if (shardRequests.size() == 0) { // only failures.. listener.onResponse(new MultiTermVectorsResponse(responses.toArray(new MultiTermVectorsItemResponse[responses.length()]))); } executeShardAction(listener, responses, shardRequests); }	i think this changes the way we report a missing routing to list the original request index (which could be an alias) rather than the concrete index. i think we want to report the concrete index here? same in transportmultigetaction. i suppose one could argue that reporting the original index-name is just as good since the info is baked into the exception message. but it does open a discussion on this being a breaking change (though a very small one)?
private <Params extends PersistentTaskParams> void startTask(PersistentTask<Params> taskInProgress) { PersistentTasksExecutor<Params> executor = persistentTasksExecutorRegistry.getPersistentTaskExecutorSafe(taskInProgress.getTaskName()); TaskAwareRequest request = new TaskAwareRequest() { TaskId parentTaskId = new TaskId("cluster", taskInProgress.getAllocationId()); @Override public void setParentTask(TaskId taskId) { throw new UnsupportedOperationException("parent task if for persistent tasks shouldn't change"); } @Override public TaskId getParentTask() { return parentTaskId; } @Override public Task createTask(long id, String type, String action, TaskId parentTaskId, Map<String, String> headers) { return executor.createTask(id, type, action, parentTaskId, taskInProgress, headers); } }; AllocatedPersistentTask task; try { task = (AllocatedPersistentTask) taskManager.register("persistent", taskInProgress.getTaskName() + "[c]", request); } catch (Exception e) { logger.error("Fatal error registering persistent task [" + taskInProgress.getTaskName() + "] with id [" + taskInProgress.getId() + "] and allocation id [" + taskInProgress.getAllocationId() + "], removing from persistent tasks", e); notifyMasterOfFailedTask(taskInProgress, e); return; } boolean processed = false; Exception initializationException = null; try { task.init(persistentTasksService, taskManager, taskInProgress.getId(), taskInProgress.getAllocationId()); logger.trace("Persistent task [{}] with id [{}] and allocation id [{}] was created", task.getAction(), task.getPersistentTaskId(), task.getAllocationId()); try { runningTasks.put(taskInProgress.getAllocationId(), task); nodePersistentTasksExecutor.executeTask(taskInProgress.getParams(), taskInProgress.getState(), task, executor); } catch (Exception e) { // Submit task failure task.markAsFailed(e); } processed = true; } catch (Exception e) { initializationException = e; } finally { if (processed == false) { // something went wrong - unregistering task logger.warn("Persistent task [{}] with id [{}] and allocation id [{}] failed to create", task.getAction(), task.getPersistentTaskId(), task.getAllocationId()); taskManager.unregister(task); if (initializationException != null) { notifyMasterOfFailedTask(taskInProgress, initializationException); } } } }	i wonder if we should use task.markasfailed instead (which also unregisters) and if we can, then we can unify this into one catch block a few lines up? i think that would make this easier to read.
* @param andThen executed if the index exists or after preparation is performed successfully */ public void prepareIndexIfNeededThenExecute(final Consumer<Exception> consumer, final Runnable andThen) { final State indexState = this.indexState; // use a local copy so all checks execute against the same state! try { // TODO we should improve this so we don't fire off a bunch of requests to do the same thing (create or update mappings) if (indexState == State.UNRECOVERED_STATE) { throw new ElasticsearchStatusException( "Cluster state has not been recovered yet, cannot write to the [" + indexState.concreteIndexName + "] index", RestStatus.SERVICE_UNAVAILABLE); } else if (indexState.indexExists() && indexState.isIndexUpToDate == false) { throw new IllegalStateException("Index [" + indexState.concreteIndexName + "] is not on the current version." + "Security features relying on the index will not be available until the upgrade API is run on the index"); } else if (indexState.indexExists() == false) { assert indexState.concreteIndexName != null; logger.info("security index does not exist, creating [{}] with alias [{}]", indexState.concreteIndexName, this.aliasName); // `TransportCreateIndexAction` will automatically apply the right mappings, settings and aliases, so none // of that needs to be specified here. CreateIndexRequest request = new CreateIndexRequest(indexState.concreteIndexName).waitForActiveShards(ActiveShardCount.ALL); executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, request, new ActionListener<CreateIndexResponse>() { @Override public void onResponse(CreateIndexResponse createIndexResponse) { if (createIndexResponse.isAcknowledged()) { andThen.run(); } else { consumer.accept(new ElasticsearchException("Failed to create security index")); } } @Override public void onFailure(Exception e) { final Throwable cause = ExceptionsHelper.unwrapCause(e); if (cause instanceof ResourceAlreadyExistsException) { // the index already exists - it was probably just created so this // node hasn't yet received the cluster state update with the index andThen.run(); } else { consumer.accept(e); } } }, client.admin().indices()::create); } else if (indexState.mappingUpToDate == false) { // Updating the mappings is the job of SystemIndexManager. consumer.accept(new RuntimeException("security index mappings are not up-to-date yet")); } else { andThen.run(); } } catch (Exception e) { consumer.accept(e); } }	do we need to validate the mappings are good to go in cases where this node is not the master and there are mixed versions in the cluster?
public boolean hasGlobalBlock(int blockId) { for (ClusterBlock clusterBlock : global) { if (clusterBlock.id() == blockId) { return true; } } return false; }	can we maybe do global(level).size() > 0 ?
@Override public void onFailure(String source, Throwable t) { if (!(t instanceof ClusterService.NoLongerMasterException)) { ClusterState state = clusterService.state(); logger.error("unexpected failure during [{}], current state:\\\\n{}", t, source, state.prettyPrint()); } }	i'd appreciate if we can do == false it's just so much easier to read
@Override public void onFailure(String source, Throwable t) { if (!(t instanceof ClusterService.NoLongerMasterException)) { ClusterState state = clusterService.state(); logger.error("unexpected failure during [{}], current state:\\\\n{}", t, source, state.prettyPrint()); } }	are we sure the clusterservice.state() can never be null?
public boolean waitForInitialState(TimeValue timeValue) throws InterruptedException { if (timeValue.millis() > 0) { latch.await(timeValue.millis(), TimeUnit.MILLISECONDS); } return initialStateReceived; } } private final TimeValue initialStateTimeout; private final Discovery discovery; private InitialStateListener initialStateListener; private final DiscoverySettings discoverySettings; @Inject public DiscoveryService(Settings settings, DiscoverySettings discoverySettings, Discovery discovery) { super(settings); this.discoverySettings = discoverySettings; this.discovery = discovery; this.initialStateTimeout = componentSettings.getAsTime("initial_state_timeout", TimeValue.timeValueSeconds(30)); } public ClusterBlock getNoMasterBlock() { return discoverySettings.getNoMasterBlock(); } @Override protected void doStart() throws ElasticsearchException { initialStateListener = new InitialStateListener(); discovery.addListener(initialStateListener); discovery.start(); logger.info(discovery.nodeDescription()); } public void waitForInitialState() { try { if (!initialStateListener.waitForInitialState(initialStateTimeout)) { logger.warn("waited for {} and no initial state was set by the discovery", initialStateTimeout); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new ElasticsearchTimeoutException("Interrupted while waiting for initial discovery state"); } } @Override protected void doStop() throws ElasticsearchException { if (initialStateListener != null) { discovery.removeListener(initialStateListener); } discovery.stop(); } @Override protected void doClose() throws ElasticsearchException { discovery.close(); } public DiscoveryNode localNode() { return discovery.localNode(); } /** * Returns <tt>true</tt> if the initial state was received within the timeout waiting for it * on {@link #doStart()}. */ public boolean initialStateReceived() { return initialStateListener.initialStateReceived; } public String nodeDescription() { return discovery.nodeDescription(); } /** * Publish all the changes to the cluster from the master (can be called just by the master). The publish * process should not publish this state to the master as well! (the master is sending it...). * <p/> * The {@link org.elasticsearch.discovery.Discovery.AckListener} allows to acknowledge the publish * event based on the response gotten from all nodes */ public void publish(ClusterState clusterState, Discovery.AckListener ackListener) { if (lifecycle.started()) { discovery.publish(clusterState, ackListener); } } public static String generateNodeId(Settings settings) { String seed = settings.get("discovery.id.seed"); if (seed != null) { return Strings.randomBase64UUID(new Random(Long.parseLong(seed))); }	can we maybe make this a constant? the setting i mean?
@Override public void onRefreshSettings(Settings settings) { TimeValue newPublishTimeout = settings.getAsTime(PUBLISH_TIMEOUT, null); if (newPublishTimeout != null) { if (newPublishTimeout.millis() != publishTimeout.millis()) { logger.info("updating [{}] from [{}] to [{}]", PUBLISH_TIMEOUT, publishTimeout, newPublishTimeout); publishTimeout = newPublishTimeout; } } String newNoMasterBlockValue = settings.get(NO_MASTER_BLOCK); if (newNoMasterBlockValue != null) { ClusterBlock newNoMasterBlock = parseNoMasterBlock(newNoMasterBlockValue); if (newNoMasterBlock != noMasterBlock) { noMasterBlock = newNoMasterBlock; } } } } private ClusterBlock parseNoMasterBlock(String value) { if ("all".equals(value)) { return NO_MASTER_BLOCK_ALL; } else if ("write".equals(value)) { return NO_MASTER_BLOCK_WRITES; } else { throw new ElasticsearchIllegalArgumentException("invalid master block [" + value + "]"); }	can we use a switch / case statement here it's easier to read and 1.7 supports strings
private void handleJoinRequest(final DiscoveryNode node, final MembershipAction.JoinCallback callback) { if (!master) { throw new ElasticsearchIllegalStateException("Node [" + localNode + "] not master for join request from [" + node + "]"); } if (!transportService.addressSupported(node.address().getClass())) { // TODO, what should we do now? Maybe inform that node that its crap? logger.warn("received a wrong address type from [{}], ignoring...", node); } else { // try and connect to the node, if it fails, we can raise an exception back to the client... transportService.connectToNode(node); // validate the join request, will throw a failure if it fails, which will get back to the // node calling the join request membership.sendValidateJoinRequestBlocking(node, joinTimeout); processJoinRequests.add(new Tuple<>(node, callback)); clusterService.submitStateUpdateTask("zen-disco-receive(join from node[" + node + "])", Priority.IMMEDIATE, new ProcessedClusterStateUpdateTask() { private final List<Tuple<DiscoveryNode, MembershipAction.JoinCallback>> drainedTasks = new ArrayList<>(); @Override public ClusterState execute(ClusterState currentState) { processJoinRequests.drainTo(drainedTasks); if (drainedTasks.isEmpty()) { return currentState; } boolean modified = false; DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder(currentState.nodes()); for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> task : drainedTasks) { DiscoveryNode node = task.v1(); if (currentState.nodes().nodeExists(node.id())) { logger.debug("received a join request for an existing node [{}]", node); } else { modified = true; nodesBuilder.put(node); for (DiscoveryNode existingNode : currentState.nodes()) { if (node.address().equals(existingNode.address())) { nodesBuilder.remove(existingNode.id()); logger.warn("received join request from node [{}], but found existing node {} with same address, removing existing node", node, existingNode); } } } } ClusterState.Builder stateBuilder = ClusterState.builder(currentState); if (modified) { latestDiscoNodes = nodesBuilder.build(); stateBuilder.nodes(latestDiscoNodes); } return stateBuilder.build(); } @Override public void onFailure(String source, Throwable t) { if (t instanceof ClusterService.NoLongerMasterException) { logger.debug("not processing [{}] as we are no longer master", source); } else { logger.error("unexpected failure during [{}]", t, source); } for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> drainedTask : drainedTasks) { drainedTask.v2().onFailure(t); } } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { for (Tuple<DiscoveryNode, MembershipAction.JoinCallback> drainedTask : drainedTasks) { drainedTask.v2().onSuccess(); } } }); } }	should we do this in a try / catch fashion? and make sure we call it on all of them?
private DiscoveryNode findMaster() { ZenPing.PingResponse[] fullPingResponses = pingService.pingAndWait(pingTimeout); if (fullPingResponses == null) { logger.trace("No full ping responses"); return null; } if (logger.isTraceEnabled()) { StringBuilder sb = new StringBuilder("full ping responses:"); if (fullPingResponses.length == 0) { sb.append(" {none}"); } else { for (ZenPing.PingResponse pingResponse : fullPingResponses) { sb.append("\\\\n\\\\t--> ").append("target [").append(pingResponse.target()).append("], master [").append(pingResponse.master()).append("]"); } } logger.trace(sb.toString()); } // filter responses List<ZenPing.PingResponse> pingResponses = Lists.newArrayList(); for (ZenPing.PingResponse pingResponse : fullPingResponses) { DiscoveryNode node = pingResponse.target(); if (masterElectionFilterClientNodes && (node.clientNode() || (!node.masterNode() && !node.dataNode()))) { // filter out the client node, which is a client node, or also one that is not data and not master (effectively, client) } else if (masterElectionFilterDataNodes && (!node.masterNode() && node.dataNode())) { // filter out data node that is not also master } else { pingResponses.add(pingResponse); } } if (logger.isDebugEnabled()) { StringBuilder sb = new StringBuilder("filtered ping responses: (filter_client[").append(masterElectionFilterClientNodes).append("], filter_data[").append(masterElectionFilterDataNodes).append("])"); if (pingResponses.isEmpty()) { sb.append(" {none}"); } else { for (ZenPing.PingResponse pingResponse : pingResponses) { sb.append("\\\\n\\\\t--> ").append("target [").append(pingResponse.target()).append("], master [").append(pingResponse.master()).append("]"); } } logger.debug(sb.toString()); } List<DiscoveryNode> pingMasters = newArrayList(); for (ZenPing.PingResponse pingResponse : pingResponses) { if (pingResponse.master() != null) { // We can't include the local node in pingMasters list, otherwise we may up electing ourselves without // any check / verifications from other nodes in ZenDiscover#innerJoinCluster() if (!localNode.equals(pingResponse.master())) { pingMasters.add(pingResponse.master()); } } } Set<DiscoveryNode> possibleMasterNodes = Sets.newHashSet(); if (localNode.masterNode()) { possibleMasterNodes.add(localNode); } for (ZenPing.PingResponse pingResponse : pingResponses) { possibleMasterNodes.add(pingResponse.target()); } if (pingMasters.isEmpty()) { // if we don't have enough master nodes, we bail, because there are not enough master to elect from if (!electMaster.hasEnoughMasterNodes(possibleMasterNodes)) { logger.trace("not enough master nodes [{}]", possibleMasterNodes); return null; } // lets tie break between discovered nodes return electMaster.electMaster(possibleMasterNodes); } else { return electMaster.electMaster(pingMasters); } }	it might be good to have an assertion somewhere that make sure it's not there?
private DiscoveryNode findMaster() { ZenPing.PingResponse[] fullPingResponses = pingService.pingAndWait(pingTimeout); if (fullPingResponses == null) { logger.trace("No full ping responses"); return null; } if (logger.isTraceEnabled()) { StringBuilder sb = new StringBuilder("full ping responses:"); if (fullPingResponses.length == 0) { sb.append(" {none}"); } else { for (ZenPing.PingResponse pingResponse : fullPingResponses) { sb.append("\\\\n\\\\t--> ").append("target [").append(pingResponse.target()).append("], master [").append(pingResponse.master()).append("]"); } } logger.trace(sb.toString()); } // filter responses List<ZenPing.PingResponse> pingResponses = Lists.newArrayList(); for (ZenPing.PingResponse pingResponse : fullPingResponses) { DiscoveryNode node = pingResponse.target(); if (masterElectionFilterClientNodes && (node.clientNode() || (!node.masterNode() && !node.dataNode()))) { // filter out the client node, which is a client node, or also one that is not data and not master (effectively, client) } else if (masterElectionFilterDataNodes && (!node.masterNode() && node.dataNode())) { // filter out data node that is not also master } else { pingResponses.add(pingResponse); } } if (logger.isDebugEnabled()) { StringBuilder sb = new StringBuilder("filtered ping responses: (filter_client[").append(masterElectionFilterClientNodes).append("], filter_data[").append(masterElectionFilterDataNodes).append("])"); if (pingResponses.isEmpty()) { sb.append(" {none}"); } else { for (ZenPing.PingResponse pingResponse : pingResponses) { sb.append("\\\\n\\\\t--> ").append("target [").append(pingResponse.target()).append("], master [").append(pingResponse.master()).append("]"); } } logger.debug(sb.toString()); } List<DiscoveryNode> pingMasters = newArrayList(); for (ZenPing.PingResponse pingResponse : pingResponses) { if (pingResponse.master() != null) { // We can't include the local node in pingMasters list, otherwise we may up electing ourselves without // any check / verifications from other nodes in ZenDiscover#innerJoinCluster() if (!localNode.equals(pingResponse.master())) { pingMasters.add(pingResponse.master()); } } } Set<DiscoveryNode> possibleMasterNodes = Sets.newHashSet(); if (localNode.masterNode()) { possibleMasterNodes.add(localNode); } for (ZenPing.PingResponse pingResponse : pingResponses) { possibleMasterNodes.add(pingResponse.target()); } if (pingMasters.isEmpty()) { // if we don't have enough master nodes, we bail, because there are not enough master to elect from if (!electMaster.hasEnoughMasterNodes(possibleMasterNodes)) { logger.trace("not enough master nodes [{}]", possibleMasterNodes); return null; } // lets tie break between discovered nodes return electMaster.electMaster(possibleMasterNodes); } else { return electMaster.electMaster(pingMasters); } }	maybe we can turn this around and do java if (electmaster.hasenoughmasternodes(possiblemasternodes)) { // lets tie break between discovered nodes return electmaster.electmaster(possiblemasternodes); } else { logger.trace("not enough master nodes [{}]", possiblemasternodes); return null; }
@Override public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) { handleNewClusterStateFromMaster(clusterState, newStateProcessed); } } private class MembershipListener implements MembershipAction.MembershipListener { @Override public void onJoin(DiscoveryNode node, MembershipAction.JoinCallback callback) { handleJoinRequest(node, callback); } @Override public void onLeave(DiscoveryNode node) { handleLeaveRequest(node); } } private class NodeFaultDetectionListener extends NodesFaultDetection.Listener { private final AtomicInteger pingsWhileMaster = new AtomicInteger(0); @Override public void onNodeFailure(DiscoveryNode node, String reason) { handleNodeFailure(node, reason); } @Override public void onPingReceived(final NodesFaultDetection.PingRequest pingRequest) { // if we are master, we don't expect any fault detection from another node. If we get it // means we potentially have two masters in the cluster. if (!master) { pingsWhileMaster.set(0); return; } // nodes pre 1.4.0 do not send this information if (pingRequest.masterNode() == null) { return; } if (pingsWhileMaster.incrementAndGet() < maxPingsFromAnotherMaster) { logger.trace("got a ping from another master {}. current ping count: [{}]", pingRequest.masterNode(), pingsWhileMaster.get()); return; } logger.debug("got a ping from another master {}. resolving who should rejoin. current ping count: [{}]", pingRequest.masterNode(), pingsWhileMaster.get()); clusterService.submitStateUpdateTask("ping from another master", Priority.URGENT, new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) throws Exception { pingsWhileMaster.set(0); return handleAnotherMaster(currentState, pingRequest.masterNode(), pingRequest.clusterStateVersion(), "node fd ping"); } @Override public void onFailure(String source, Throwable t) { logger.debug("unexpected error during cluster state update task after pings from another master", t); } }); } } private class MasterNodeFailureListener implements MasterFaultDetection.Listener { @Override public void onMasterFailure(DiscoveryNode masterNode, String reason) { handleMasterGone(masterNode, reason); } @Override public void onDisconnectedFromMaster() { // got disconnected from the master, send a join request DiscoveryNode masterNode = latestDiscoNodes.masterNode(); try { membership.sendJoinRequest(masterNode, localNode); } catch (Exception e) { logger.warn("failed to send join request on disconnection from master [{}]", masterNode); } } } boolean isRejoinOnMasterGone() { return rejoinOnMasterGone; } static class RejoinClusterRequest extends TransportRequest { private String fromNodeId; RejoinClusterRequest(String fromNodeId) { this.fromNodeId = fromNodeId; } RejoinClusterRequest() { } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); fromNodeId = in.readOptionalString(); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeOptionalString(fromNodeId); } } class RejoinClusterRequestHandler extends BaseTransportRequestHandler<RejoinClusterRequest> { @Override public RejoinClusterRequest newInstance() { return new RejoinClusterRequest(); } @Override public void messageReceived(final RejoinClusterRequest request, final TransportChannel channel) throws Exception { clusterService.submitStateUpdateTask("received a request to rejoin the cluster from [" + request.fromNodeId + "]", Priority.URGENT, new ClusterStateNonMasterUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { try { channel.sendResponse(TransportResponse.Empty.INSTANCE); } catch (Exception e) { logger.warn("failed to send response on rejoin cluster request handling", e); } return rejoin(currentState, "received a request to rejoin the cluster from [" + request.fromNodeId + "]"); } @Override public void onFailure(String source, Throwable t) { if (t instanceof ClusterService.NoLongerMasterException) { logger.debug("not processing [{}] as we are no longer master", source); } else { logger.error("unexpected failure during [{}]", t, source); } } }); } @Override public String executor() { return ThreadPool.Names.SAME; } } class ApplySettings implements NodeSettingsService.Listener { @Override public void onRefreshSettings(Settings settings) { int minimumMasterNodes = settings.getAsInt(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, ZenDiscovery.this.electMaster.minimumMasterNodes()); if (minimumMasterNodes != ZenDiscovery.this.electMaster.minimumMasterNodes()) { logger.info("updating {} from [{}] to [{}]", ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, ZenDiscovery.this.electMaster.minimumMasterNodes(), minimumMasterNodes); handleMinimumMasterNodesChanged(minimumMasterNodes); } boolean rejoinOnMasterGone = settings.getAsBoolean(REJOIN_ON_MASTER_GONE, ZenDiscovery.this.rejoinOnMasterGone); if (rejoinOnMasterGone != ZenDiscovery.this.rejoinOnMasterGone) { logger.info("updating {} from [{}] to [{}]", REJOIN_ON_MASTER_GONE, ZenDiscovery.this.rejoinOnMasterGone, rejoinOnMasterGone); ZenDiscovery.this.rejoinOnMasterGone = rejoinOnMasterGone; } }	maybe add a logging statement here?
@Override public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) { handleNewClusterStateFromMaster(clusterState, newStateProcessed); } } private class MembershipListener implements MembershipAction.MembershipListener { @Override public void onJoin(DiscoveryNode node, MembershipAction.JoinCallback callback) { handleJoinRequest(node, callback); } @Override public void onLeave(DiscoveryNode node) { handleLeaveRequest(node); } } private class NodeFaultDetectionListener extends NodesFaultDetection.Listener { private final AtomicInteger pingsWhileMaster = new AtomicInteger(0); @Override public void onNodeFailure(DiscoveryNode node, String reason) { handleNodeFailure(node, reason); } @Override public void onPingReceived(final NodesFaultDetection.PingRequest pingRequest) { // if we are master, we don't expect any fault detection from another node. If we get it // means we potentially have two masters in the cluster. if (!master) { pingsWhileMaster.set(0); return; } // nodes pre 1.4.0 do not send this information if (pingRequest.masterNode() == null) { return; } if (pingsWhileMaster.incrementAndGet() < maxPingsFromAnotherMaster) { logger.trace("got a ping from another master {}. current ping count: [{}]", pingRequest.masterNode(), pingsWhileMaster.get()); return; } logger.debug("got a ping from another master {}. resolving who should rejoin. current ping count: [{}]", pingRequest.masterNode(), pingsWhileMaster.get()); clusterService.submitStateUpdateTask("ping from another master", Priority.URGENT, new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) throws Exception { pingsWhileMaster.set(0); return handleAnotherMaster(currentState, pingRequest.masterNode(), pingRequest.clusterStateVersion(), "node fd ping"); } @Override public void onFailure(String source, Throwable t) { logger.debug("unexpected error during cluster state update task after pings from another master", t); } }); } } private class MasterNodeFailureListener implements MasterFaultDetection.Listener { @Override public void onMasterFailure(DiscoveryNode masterNode, String reason) { handleMasterGone(masterNode, reason); } @Override public void onDisconnectedFromMaster() { // got disconnected from the master, send a join request DiscoveryNode masterNode = latestDiscoNodes.masterNode(); try { membership.sendJoinRequest(masterNode, localNode); } catch (Exception e) { logger.warn("failed to send join request on disconnection from master [{}]", masterNode); } } } boolean isRejoinOnMasterGone() { return rejoinOnMasterGone; } static class RejoinClusterRequest extends TransportRequest { private String fromNodeId; RejoinClusterRequest(String fromNodeId) { this.fromNodeId = fromNodeId; } RejoinClusterRequest() { } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); fromNodeId = in.readOptionalString(); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeOptionalString(fromNodeId); } } class RejoinClusterRequestHandler extends BaseTransportRequestHandler<RejoinClusterRequest> { @Override public RejoinClusterRequest newInstance() { return new RejoinClusterRequest(); } @Override public void messageReceived(final RejoinClusterRequest request, final TransportChannel channel) throws Exception { clusterService.submitStateUpdateTask("received a request to rejoin the cluster from [" + request.fromNodeId + "]", Priority.URGENT, new ClusterStateNonMasterUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { try { channel.sendResponse(TransportResponse.Empty.INSTANCE); } catch (Exception e) { logger.warn("failed to send response on rejoin cluster request handling", e); } return rejoin(currentState, "received a request to rejoin the cluster from [" + request.fromNodeId + "]"); } @Override public void onFailure(String source, Throwable t) { if (t instanceof ClusterService.NoLongerMasterException) { logger.debug("not processing [{}] as we are no longer master", source); } else { logger.error("unexpected failure during [{}]", t, source); } } }); } @Override public String executor() { return ThreadPool.Names.SAME; } } class ApplySettings implements NodeSettingsService.Listener { @Override public void onRefreshSettings(Settings settings) { int minimumMasterNodes = settings.getAsInt(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, ZenDiscovery.this.electMaster.minimumMasterNodes()); if (minimumMasterNodes != ZenDiscovery.this.electMaster.minimumMasterNodes()) { logger.info("updating {} from [{}] to [{}]", ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, ZenDiscovery.this.electMaster.minimumMasterNodes(), minimumMasterNodes); handleMinimumMasterNodesChanged(minimumMasterNodes); } boolean rejoinOnMasterGone = settings.getAsBoolean(REJOIN_ON_MASTER_GONE, ZenDiscovery.this.rejoinOnMasterGone); if (rejoinOnMasterGone != ZenDiscovery.this.rejoinOnMasterGone) { logger.info("updating {} from [{}] to [{}]", REJOIN_ON_MASTER_GONE, ZenDiscovery.this.rejoinOnMasterGone, rejoinOnMasterGone); ZenDiscovery.this.rejoinOnMasterGone = rejoinOnMasterGone; } }	it doens't matter if that one overflows no? i mean grows larger than maxpingsfromanothermaster?
@Override public void messageReceived(final RejoinClusterRequest request, final TransportChannel channel) throws Exception { clusterService.submitStateUpdateTask("received a request to rejoin the cluster from [" + request.fromNodeId + "]", Priority.URGENT, new ClusterStateNonMasterUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { try { channel.sendResponse(TransportResponse.Empty.INSTANCE); } catch (Exception e) { logger.warn("failed to send response on rejoin cluster request handling", e); } return rejoin(currentState, "received a request to rejoin the cluster from [" + request.fromNodeId + "]"); } @Override public void onFailure(String source, Throwable t) { if (t instanceof ClusterService.NoLongerMasterException) { logger.debug("not processing [{}] as we are no longer master", source); } else { logger.error("unexpected failure during [{}]", t, source); } } }); }	i start to see this a lot, can we have a static helper somewhere?
public boolean hasEnoughMasterNodes(Iterable<DiscoveryNode> nodes) { if (minimumMasterNodes < 1) { return true; } int count = 0; for (DiscoveryNode node : nodes) { if (node.masterNode()) { count++; } } return count >= minimumMasterNodes; }	can't this be list from the beginning?
private void handleTransportDisconnect(DiscoveryNode node) { synchronized (masterNodeMutex) { if (!node.equals(this.masterNode)) { return; } if (connectOnNetworkDisconnect) { try { transportService.connectToNode(node); // if all is well, make sure we restart the pinger if (masterPinger != null) { masterPinger.stop(); } this.masterPinger = new MasterPinger(); // we use schedule with a 0 time value to run the pinger on the pool as it will run on later threadPool.schedule(TimeValue.timeValueMillis(0), ThreadPool.Names.SAME, masterPinger); } catch (Exception e) { logger.trace("[master] [{}] transport disconnected (with verified connect)", masterNode); notifyMasterFailure(masterNode, "transport disconnected (with verified connect)"); } } else { logger.trace("[master] [{}] transport disconnected", node); notifyMasterFailure(node, "transport disconnected"); } } }	i am not sure i understand this change here?
void sendPings(final TimeValue timeout, @Nullable TimeValue waitTime, final SendPingsHandler sendPingsHandler) { final UnicastPingRequest pingRequest = new UnicastPingRequest(); pingRequest.id = sendPingsHandler.id(); pingRequest.timeout = timeout; DiscoveryNodes discoNodes = nodesProvider.nodes(); pingRequest.pingResponse = new PingResponse(discoNodes.localNode(), discoNodes.masterNode(), clusterName); HashSet<DiscoveryNode> nodesToPingSet = new HashSet<>(); for (PingResponse temporalResponse : temporalResponses) { // Only send pings to nodes that have the same cluster name. if (clusterName.equals(temporalResponse.clusterName())) { nodesToPingSet.add(temporalResponse.target()); } } for (UnicastHostsProvider provider : hostsProviders) { nodesToPingSet.addAll(provider.buildDynamicNodes()); } // add all possible master nodes that were active in the last known cluster configuration for (ObjectCursor<DiscoveryNode> masterNode : discoNodes.getMasterNodes().values()) { nodesToPingSet.add(masterNode.value); } // sort the nodes by likelihood of being an active master List<DiscoveryNode> sortedNodesToPing = electMasterService.sortByMasterLikelihood(nodesToPingSet); // new add the the unicast targets first ArrayList<DiscoveryNode> nodesToPing = Lists.newArrayList(nodes); nodesToPing.addAll(sortedNodesToPing); final CountDownLatch latch = new CountDownLatch(nodesToPing.size()); for (final DiscoveryNode node : nodesToPing) { // make sure we are connected boolean nodeFoundByAddressX; DiscoveryNode nodeToSendX = discoNodes.findByAddress(node.address()); if (nodeToSendX != null) { nodeFoundByAddressX = true; } else { nodeToSendX = node; nodeFoundByAddressX = false; } final DiscoveryNode nodeToSend = nodeToSendX; final boolean nodeFoundByAddress = nodeFoundByAddressX; if (!transportService.nodeConnected(nodeToSend)) { if (sendPingsHandler.isClosed()) { return; } // only disconnect from nodes that we will end up creating a light connection to, as they are temporal // if we find on the disco nodes a matching node by address, we are going to restore the connection // anyhow down the line if its not connected... if (!nodeFoundByAddress) { sendPingsHandler.nodeToDisconnect.add(nodeToSend); } // fork the connection to another thread sendPingsHandler.executor().execute(new Runnable() { @Override public void run() { if (sendPingsHandler.isClosed()) { return; } boolean success = false; try { // connect to the node, see if we manage to do it, if not, bail if (!nodeFoundByAddress) { logger.trace("[{}] connecting (light) to {}", sendPingsHandler.id(), nodeToSend); transportService.connectToNodeLight(nodeToSend); } else { logger.trace("[{}] connecting to {}", sendPingsHandler.id(), nodeToSend); transportService.connectToNode(nodeToSend); } logger.trace("[{}] connected to {}", sendPingsHandler.id(), node); if (receivedResponses.containsKey(sendPingsHandler.id())) { // we are connected and still in progress, send the ping request sendPingRequestToNode(sendPingsHandler.id(), timeout, pingRequest, latch, node, nodeToSend); } else { // connect took too long, just log it and bail latch.countDown(); logger.trace("[{}] connect to {} was too long outside of ping window, bailing", sendPingsHandler.id(), node); } success = true; } catch (ConnectTransportException e) { // can't connect to the node - this is a more common path! logger.trace("[{}] failed to connect to {}", e, sendPingsHandler.id(), nodeToSend); } catch (Throwable e) { logger.warn("[{}] failed send ping to {}", e, sendPingsHandler.id(), nodeToSend); } finally { if (!success) { latch.countDown(); } } } }); } else { sendPingRequestToNode(sendPingsHandler.id(), timeout, pingRequest, latch, node, nodeToSend); } } if (waitTime != null) { try { latch.await(waitTime.millis(), TimeUnit.MILLISECONDS); } catch (InterruptedException e) { // ignore } } }	it took me a while to figure out what all these different node lists / sets are maybe you can find a better name for this.nodes?
public static List<NamedWriteableRegistry.Entry> getNamedWriteables() { List<NamedWriteableRegistry.Entry> entries = new ArrayList<>(); entries.addAll(org.elasticsearch.xpack.ql.expression.processor.Processors.getNamedWriteables()); // base entries.add(new Entry(Processor.class, CastProcessor.NAME, CastProcessor::new)); entries.add(new Entry(Converter.class, SqlConverter.NAME, SqlConverter::read)); // arithmetic // binary arithmetics are pluggable entries.add(new Entry(BinaryArithmeticOperation.class, SqlBinaryArithmeticOperation.NAME, SqlBinaryArithmeticOperation::read)); // comparators entries.add(new Entry(Processor.class, InProcessor.NAME, InProcessor::new)); // conditionals entries.add(new Entry(Processor.class, CaseProcessor.NAME, CaseProcessor::new)); entries.add(new Entry(Processor.class, CheckNullProcessor.NAME, CheckNullProcessor::new)); entries.add(new Entry(Processor.class, ConditionalProcessor.NAME, ConditionalProcessor::new)); entries.add(new Entry(Processor.class, NullIfProcessor.NAME, NullIfProcessor::new)); // datetime entries.add(new Entry(Processor.class, DateAddProcessor.NAME, DateAddProcessor::new)); entries.add(new Entry(Processor.class, DateDiffProcessor.NAME, DateDiffProcessor::new)); entries.add(new Entry(Processor.class, DatePartProcessor.NAME, DatePartProcessor::new)); entries.add(new Entry(Processor.class, DateTimeParseProcessor.NAME, DateTimeParseProcessor::new)); entries.add(new Entry(Processor.class, DateTimeFormatProcessor.NAME, DateTimeFormatProcessor::new)); entries.add(new Entry(Processor.class, DateTimeProcessor.NAME, DateTimeProcessor::new)); entries.add(new Entry(Processor.class, DateTruncProcessor.NAME, DateTruncProcessor::new)); entries.add(new Entry(Processor.class, NamedDateTimeProcessor.NAME, NamedDateTimeProcessor::new)); entries.add(new Entry(Processor.class, NonIsoDateTimeProcessor.NAME, NonIsoDateTimeProcessor::new)); entries.add(new Entry(Processor.class, QuarterProcessor.NAME, QuarterProcessor::new)); entries.add(new Entry(Processor.class, TimeProcessor.NAME, TimeProcessor::new)); // math entries.add(new Entry(Processor.class, BinaryMathProcessor.NAME, BinaryMathProcessor::new)); entries.add(new Entry(Processor.class, BinaryOptionalMathProcessor.NAME, BinaryOptionalMathProcessor::new)); entries.add(new Entry(Processor.class, MathProcessor.NAME, MathProcessor::new)); // string entries.add(new Entry(Processor.class, StringProcessor.NAME, StringProcessor::new)); entries.add(new Entry(Processor.class, BinaryStringNumericProcessor.NAME, BinaryStringNumericProcessor::new)); entries.add(new Entry(Processor.class, BinaryStringStringProcessor.NAME, BinaryStringStringProcessor::new)); entries.add(new Entry(Processor.class, ConcatFunctionProcessor.NAME, ConcatFunctionProcessor::new)); entries.add(new Entry(Processor.class, InsertFunctionProcessor.NAME, InsertFunctionProcessor::new)); entries.add(new Entry(Processor.class, LocateFunctionProcessor.NAME, LocateFunctionProcessor::new)); entries.add(new Entry(Processor.class, ReplaceFunctionProcessor.NAME, ReplaceFunctionProcessor::new)); entries.add(new Entry(Processor.class, SubstringFunctionProcessor.NAME, SubstringFunctionProcessor::new)); // geo entries.add(new Entry(Processor.class, GeoProcessor.NAME, GeoProcessor::new)); entries.add(new Entry(Processor.class, StWkttosqlProcessor.NAME, StWkttosqlProcessor::new)); entries.add(new Entry(Processor.class, StDistanceProcessor.NAME, StDistanceProcessor::new)); return entries; }	please keep the alphabetical ordering here.
@Override public Query geoShapeQuery(SearchExecutionContext context, String fieldName, ShapeRelation relation, LatLonGeometry... geometries) { return new GeoPointScriptFieldGeoShapeQuery(script, leafFactory(context), fieldName, relation, geometries); }	are we loosing an optimisation here by deleting the check for any geometries not being points when we have contains?
@Override public Query doToQuery(SearchExecutionContext context) { MappedFieldType fieldType = context.getFieldType(fieldName); if (fieldType == null) { if (ignoreUnmapped) { return new MatchNoDocsQuery(); } else { throw new QueryShardException(context, "failed to find geo field [" + fieldName + "]"); } } if ((fieldType instanceof GeoShapeQueryable) == false) { throw new QueryShardException( context, "Field [" + fieldName + "] is of unsupported type [" + fieldType.typeName() + "] for [" + NAME + "] query" ); } QueryValidationException exception = checkLatLon(); if (exception != null) { throw new QueryShardException(context, "couldn't validate latitude/ longitude values", exception); } GeoPoint luceneTopLeft = new GeoPoint(geoBoundingBox.topLeft()); GeoPoint luceneBottomRight = new GeoPoint(geoBoundingBox.bottomRight()); if (GeoValidationMethod.isCoerce(validationMethod)) { // Special case: if the difference between the left and right is 360 and the right is greater than the left, we are asking for // the complete longitude range so need to set longitude to the complete longitude range double right = luceneBottomRight.getLon(); double left = luceneTopLeft.getLon(); boolean completeLonRange = ((right - left) % 360 == 0 && right > left); GeoUtils.normalizePoint(luceneTopLeft, true, completeLonRange == false); GeoUtils.normalizePoint(luceneBottomRight, true, completeLonRange == false); if (completeLonRange) { luceneTopLeft.resetLon(-180); luceneBottomRight.resetLon(180); } } final GeoShapeQueryable geoShapeQueryable = (GeoShapeQueryable) fieldType; final Rectangle rectangle = new Rectangle( luceneTopLeft.getLon(), luceneBottomRight.getLon(), luceneTopLeft.getLat(), luceneBottomRight.getLat() ); return geoShapeQueryable.geoShapeQuery(context, fieldType.name(), SpatialStrategy.RECURSIVE, ShapeRelation.INTERSECTS, rectangle); }	just call the new api directly.
public void testCleanUpSnapshotShardSizes() throws Exception { final Repository mockRepository = new FilterRepository(mock(Repository.class)) { @Override public IndexShardSnapshotStatus getShardSnapshotStatus(SnapshotId snapshotId, IndexId indexId, ShardId shardId) { if (randomBoolean()) { throw new SnapshotException(new Snapshot("_repo", snapshotId), "simulated"); } else { return IndexShardSnapshotStatus.newDone(0L, 0L, 0, 0, 0L, randomNonNegativeLong(), null); } } }; when(repositoriesService.repository("_repo")).thenReturn(mockRepository); final InternalSnapshotsInfoService snapshotsInfoService = new InternalSnapshotsInfoService(Settings.EMPTY, clusterService, () -> repositoriesService, () -> rerouteService); final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final int nbShards = randomIntBetween(1, 10); applyClusterState("new snapshot restore for index " + indexName, clusterState -> addUnassignedShards(clusterState, indexName, nbShards)); // waiting for snapshot shard size fetches to be executed, as we want to verify that they are clean up assertBusy(() -> assertThat( snapshotsInfoService.numberOfFailedSnapshotShardSizes() + snapshotsInfoService.numberOfKnownSnapshotShardSizes(), equalTo(nbShards))); if (randomBoolean()) { // simulate initialization and start of the shards final AllocationService allocationService = ESAllocationTestCase.createAllocationService(Settings.builder() .put(CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), nbShards) .put(CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING.getKey(), nbShards) .build(), snapshotsInfoService); applyClusterState("starting shards for " + indexName, clusterState -> ESAllocationTestCase.startInitializingShardsAndReroute(allocationService, clusterState, indexName)); assertTrue(clusterService.state().routingTable().shardsWithState(ShardRoutingState.UNASSIGNED).isEmpty()); } else { // simulate deletion of the index applyClusterState("delete index " + indexName, clusterState -> deleteIndex(clusterState, indexName)); assertFalse(clusterService.state().metadata().hasIndex(indexName)); } assertThat(snapshotsInfoService.numberOfKnownSnapshotShardSizes(), equalTo(0)); assertThat(snapshotsInfoService.numberOfUnknownSnapshotShardSizes(), equalTo(0)); assertThat(snapshotsInfoService.numberOfFailedSnapshotShardSizes(), equalTo(0)); }	typo: are cleaned up
private ClusterState addUnassignedShards(final ClusterState currentState, String indexName, int numberOfShards) { assertThat(currentState.metadata().hasIndex(indexName), is(false)); final IndexMetadata.Builder indexMetadataBuilder = IndexMetadata.builder(indexName) .settings(Settings.builder() .put(SETTING_VERSION_CREATED, Version.CURRENT) .put(SETTING_NUMBER_OF_SHARDS, numberOfShards) .put(SETTING_NUMBER_OF_REPLICAS, 0) .put(SETTING_CREATION_DATE, System.currentTimeMillis())); for (int i = 0; i < numberOfShards; i++) { indexMetadataBuilder.putInSyncAllocationIds(i, Collections.singleton(AllocationId.newInitializing().getId())); } final Metadata.Builder metadata = Metadata.builder(currentState.metadata()) .put(indexMetadataBuilder.build(), true) .generateClusterUuidIfNeeded(); final RecoverySource.SnapshotRecoverySource recoverySource = new RecoverySource.SnapshotRecoverySource( UUIDs.randomBase64UUID(random()), new Snapshot("_repo", new SnapshotId(randomAlphaOfLength(5), UUIDs.randomBase64UUID(random()))), Version.CURRENT, new IndexId(indexName, UUIDs.randomBase64UUID(random())) ); final IndexMetadata indexMetadata = metadata.get(indexName); final Index index = indexMetadata.getIndex(); final RoutingTable.Builder routingTable = RoutingTable.builder(currentState.routingTable()); routingTable.add(IndexRoutingTable.builder(index).initializeAsNewRestore(indexMetadata, recoverySource, new IntHashSet()).build()); final RestoreInProgress.Builder restores; if (currentState.custom(RestoreInProgress.TYPE) != null) { restores = new RestoreInProgress.Builder(currentState.custom(RestoreInProgress.TYPE)); } else { restores = new RestoreInProgress.Builder(); } final ImmutableOpenMap.Builder<ShardId, RestoreInProgress.ShardRestoreStatus> shards = ImmutableOpenMap.builder(); for (int i = 0; i < indexMetadata.getNumberOfShards(); i++) { shards.put( new ShardId(index, i), new RestoreInProgress.ShardRestoreStatus(clusterService.state().nodes().getLocalNodeId())); } restores.add(new RestoreInProgress.Entry(recoverySource.restoreUUID(), recoverySource.snapshot(), RestoreInProgress.State.INIT, Collections.singletonList(indexName), shards.build())); return ClusterState.builder(currentState) .putCustom(RestoreInProgress.TYPE, restores.build()) .routingTable(routingTable.build()) .metadata(metadata) .build(); }	i made these changes so that the cluster state is better in line with a real cluster state containing an index to restore. also, it allows to use an allocationservice to update the cluster state in order to initialize or start shards.
private ClusterState addUnassignedShards(final ClusterState currentState, String indexName, int numberOfShards) { assertThat(currentState.metadata().hasIndex(indexName), is(false)); final IndexMetadata.Builder indexMetadataBuilder = IndexMetadata.builder(indexName) .settings(Settings.builder() .put(SETTING_VERSION_CREATED, Version.CURRENT) .put(SETTING_NUMBER_OF_SHARDS, numberOfShards) .put(SETTING_NUMBER_OF_REPLICAS, 0) .put(SETTING_CREATION_DATE, System.currentTimeMillis())); for (int i = 0; i < numberOfShards; i++) { indexMetadataBuilder.putInSyncAllocationIds(i, Collections.singleton(AllocationId.newInitializing().getId())); } final Metadata.Builder metadata = Metadata.builder(currentState.metadata()) .put(indexMetadataBuilder.build(), true) .generateClusterUuidIfNeeded(); final RecoverySource.SnapshotRecoverySource recoverySource = new RecoverySource.SnapshotRecoverySource( UUIDs.randomBase64UUID(random()), new Snapshot("_repo", new SnapshotId(randomAlphaOfLength(5), UUIDs.randomBase64UUID(random()))), Version.CURRENT, new IndexId(indexName, UUIDs.randomBase64UUID(random())) ); final IndexMetadata indexMetadata = metadata.get(indexName); final Index index = indexMetadata.getIndex(); final RoutingTable.Builder routingTable = RoutingTable.builder(currentState.routingTable()); routingTable.add(IndexRoutingTable.builder(index).initializeAsNewRestore(indexMetadata, recoverySource, new IntHashSet()).build()); final RestoreInProgress.Builder restores; if (currentState.custom(RestoreInProgress.TYPE) != null) { restores = new RestoreInProgress.Builder(currentState.custom(RestoreInProgress.TYPE)); } else { restores = new RestoreInProgress.Builder(); } final ImmutableOpenMap.Builder<ShardId, RestoreInProgress.ShardRestoreStatus> shards = ImmutableOpenMap.builder(); for (int i = 0; i < indexMetadata.getNumberOfShards(); i++) { shards.put( new ShardId(index, i), new RestoreInProgress.ShardRestoreStatus(clusterService.state().nodes().getLocalNodeId())); } restores.add(new RestoreInProgress.Entry(recoverySource.restoreUUID(), recoverySource.snapshot(), RestoreInProgress.State.INIT, Collections.singletonList(indexName), shards.build())); return ClusterState.builder(currentState) .putCustom(RestoreInProgress.TYPE, restores.build()) .routingTable(routingTable.build()) .metadata(metadata) .build(); }	nit: you can save the conditional here by using: final restoreinprogress.builder restores = new restoreinprogress.builder(currentstate.custom(restoreinprogress.type, restoreinprogress.empty))
public static String getErrorCause(HttpResponse httpResponse) { final Object error = httpResponse.getResponseBody().get("error"); if (error == null) { return null; } if (error instanceof Map) { Object reason = ((Map) error).get("reason"); if (reason != null) { return reason.toString(); } final Object root = ((Map) error).get("root_cause"); if (root != null && root instanceof Map) { reason = ((Map) root).get("reason"); if (reason != null) { return reason.toString(); } final Object type = ((Map) root).get("type"); if (type != null) { return (String) type; } } return String.valueOf(((Map) error).get("type")); } return error.toString(); }	i don't think force flag makes sense in this method. if a user of the commandlinehttpclient doesn't care about cluster health and would force execution eitherway, they can simply not call checkclusterhealthwithretrieswaitingforcluster at all
private void sendRequestToChannel(final DiscoveryNode node, final Channel targetChannel, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options, Version channelVersion, byte status) throws IOException, TransportException { if (compress) { options = TransportRequestOptions.builder(options).withCompress(true).build(); } boolean compressMessage = options.compress() && canCompress(request); status = TransportStatus.setRequest(status); ReleasableBytesStreamOutput bStream = new ReleasableBytesStreamOutput(bigArrays); final CompressibleBytesOutputStream stream = new CompressibleBytesOutputStream(bStream, compressMessage); boolean addedReleaseListener = false; try { // only compress if asked, and, the request is not bytes, since then only // the header part is compressed, and the "body" can't be extracted as compressed if (compressMessage) { status = TransportStatus.setCompress(status); } // we pick the smallest of the 2, to support both backward and forward compatibility // note, this is the only place we need to do this, since from here on, we use the serialized version // as the version to use also when the node receiving this request will send the response with Version version = Version.min(getCurrentVersion(), channelVersion); stream.setVersion(version); threadPool.getThreadContext().writeTo(stream); stream.writeString(action); BytesReference message = buildMessage(requestId, status, node.getVersion(), request, stream); final TransportRequestOptions finalOptions = options; // this might be called in a different thread SendListener onRequestSent = new SendListener(stream, () -> transportServiceAdapter.onRequestSent(node, requestId, action, request, finalOptions)); internalSendMessage(targetChannel, message, onRequestSent); addedReleaseListener = true; } finally { if (!addedReleaseListener) { IOUtils.close(stream); } } }	you probably should move this comment.
public void testJwtAuthenticationTokenParse() throws Exception { final String signatureAlgorithm = randomFrom(JwtRealmSettings.SUPPORTED_SIGNATURE_ALGORITHMS); final JWK jwk = JwtTestCase.randomJwk(JWSAlgorithm.parse(signatureAlgorithm)); final JWSSigner jwsSigner = JwtUtil.createJwsSigner(jwk); final String serializedJWTOriginal = JwtTestCase.randomValidSignedJWT(jwsSigner, signatureAlgorithm).serialize(); final SecureString jwt = new SecureString(serializedJWTOriginal.toCharArray()); final SecureString clientSharedSecret = randomBoolean() ? null : new SecureString(randomAlphaOfLengthBetween(10, 20).toCharArray()); final JwtAuthenticationToken jwtAuthenticationToken = new JwtAuthenticationToken(jwt, clientSharedSecret); final SecureString endUserSignedJwt = jwtAuthenticationToken.getEndUserSignedJwt(); final SecureString clientAuthorizationSharedSecret = jwtAuthenticationToken.getClientAuthorizationSharedSecret(); Assert.assertEquals(serializedJWTOriginal, endUserSignedJwt.toString()); Assert.assertEquals(serializedJWTOriginal, jwtAuthenticationToken.getSignedJwt().serialize()); Assert.assertEquals(clientSharedSecret, clientAuthorizationSharedSecret); jwtAuthenticationToken.clearCredentials(); // verify references to SecureString throw exception when calling their methods final Exception exception1 = expectThrows(IllegalStateException.class, endUserSignedJwt::length); assertThat(exception1.getMessage(), equalTo("SecureString has already been closed")); if (clientAuthorizationSharedSecret != null) { final Exception exception2 = expectThrows(IllegalStateException.class, clientAuthorizationSharedSecret::length); assertThat(exception2.getMessage(), equalTo("SecureString has already been closed")); } // verify token returns nulls assertThat(jwtAuthenticationToken.principal(), is(nullValue())); assertThat(jwtAuthenticationToken.credentials(), is(nullValue())); assertThat(jwtAuthenticationToken.getEndUserSignedJwt(), is(nullValue())); assertThat(jwtAuthenticationToken.getClientAuthorizationSharedSecret(), is(nullValue())); assertThat(jwtAuthenticationToken.getSignedJwt(), is(nullValue())); assertThat(jwtAuthenticationToken.getJwsHeader(), is(nullValue())); assertThat(jwtAuthenticationToken.getJwtClaimsSet(), is(nullValue())); assertThat(jwtAuthenticationToken.getJwtSignature(), is(nullValue())); assertThat(jwtAuthenticationToken.getIssuerClaim(), is(nullValue())); assertThat(jwtAuthenticationToken.getAudiencesClaim(), is(nullValue())); assertThat(jwtAuthenticationToken.getSubjectClaim(), is(nullValue())); }	i think we will need a few more test methods in this class: - verify that jwts with invalid signatures are rejected - verify jwts that are tampered are rejected - verify unsigned jwts are rejected - verify jwts with algorithm mixup attack are rejected ( check openidconnectauthenticatortests) - verify jwts with none algorithm are rejected - verify jwts with timing issues i know you handle some (like timing) in the unit testsof jwtutil but i'd be keen on having tests that show that authentication fails
private boolean helpTestSignatureAlgorithm(final JWSAlgorithm signatureAlgorithm) throws Exception { LOGGER.info("Testing signature algorithm " + signatureAlgorithm); // randomSecretOrSecretKeyOrKeyPair() randomizes which JwtUtil methods to call, so it indirectly covers most JwtUtil code final JWK jwk = JwtUtilTests.randomJwk(signatureAlgorithm); final JWSSigner jwsSigner = JwtUtil.createJwsSigner(jwk); final JWSVerifier jwkVerifier = JwtUtil.createJwsVerifier(jwk); final String serializedJWTOriginal = JwtUtilTests.randomValidSignedJWT(jwsSigner, signatureAlgorithm.toString()).serialize(); final SignedJWT parsedSignedJWT = SignedJWT.parse(serializedJWTOriginal); return JwtUtil.verifySignedJWT(jwkVerifier, parsedSignedJWT); }	randomsecretorsecretkeyorkeypair is not here any more, is the "it indirectly covers most jwtutil code" covered by something else ?
private ObjectPath getWatchHistoryEntry(String watchId, String state) throws Exception { final AtomicReference<ObjectPath> objectPathReference = new AtomicReference<>(); assertBusy(() -> { client().performRequest(new Request("POST", "/.watcher-history-*/_refresh")); try (XContentBuilder builder = jsonBuilder()) { builder.startObject(); builder.startObject("query").startObject("bool").startArray("must"); builder.startObject().startObject("term").startObject("watch_id").field("value", watchId).endObject().endObject() .endObject(); if (Strings.isNullOrEmpty(state) == false) { builder.startObject().startObject("term").startObject("state").field("value", state).endObject().endObject() .endObject(); } builder.endArray().endObject().endObject(); builder.startArray("sort").startObject().startObject("trigger_event.triggered_time").field("order", "desc").endObject() .endObject().endArray(); builder.endObject(); Request searchRequest = new Request("POST", "/.watcher-history-*/_search"); searchRequest.addParameter(TOTAL_HITS_AS_INT_PARAM, "true"); searchRequest.setJsonEntity(Strings.toString(builder)); Response response = client().performRequest(searchRequest); ObjectPath objectPath = ObjectPath.createFromResponse(response); int totalHits = objectPath.evaluate("hits.total"); assertThat(totalHits, is(greaterThanOrEqualTo(1))); String watchid = objectPath.evaluate("hits.hits.0._source.watch_id"); assertThat(watchid, is(watchId)); objectPathReference.set(objectPath); } catch (ResponseException e) { final String err = "Failed to perform search of watcher history - " + e; logger.info(err); fail(err); } }); return objectPathReference.get(); }	any particular reason not to use suggestion logger.info("failed to perform search of watcher history", e); here instead so that you get the whole stacktrace?
public void testEnabled() { boolean enabled = randomBoolean(); Settings.Builder settings = Settings.builder(); boolean isExplicitlyEnabled = false; if (enabled) { if (randomBoolean()) { settings.put("xpack.sql.enabled", enabled); isExplicitlyEnabled = true; } } else { settings.put("xpack.sql.enabled", enabled); isExplicitlyEnabled = true; } SqlInfoTransportAction featureSet = new SqlInfoTransportAction( mock(TransportService.class), mock(ActionFilters.class), settings.build(), licenseState); assertThat(featureSet.enabled(), is(enabled)); if (isExplicitlyEnabled) { assertSettingDeprecationsAndWarnings(new Setting<?>[] { XPackSettings.SQL_ENABLED } ); } }	nit: this seems more "isexplicitlyset" (as you have in another test here) than "enabled", since it might actually be disabled
public void collect(int docId, long owningBucketOrd) throws IOException { if (parentDocs.get(docId) && globalOrdinals.advanceExact(docId)) { int globalOrdinal = (int) globalOrdinals.nextOrd(); assert globalOrdinal != -1 && globalOrdinals.nextOrd() == SortedSetDocValues.NO_MORE_ORDS; collectionStrategy.add(owningBucketOrd, globalOrdinal); } }	i renamed this method to make it a bit more clear what it is for.
@Override public int hashCode() { return nodeId.hashCode() ^ 31 * nodeName.hashCode() ^ 31 * Long.hashCode(freeBytes) ^ 31 * Long.hashCode(totalBytes); }	that's java 1.8 construct
public void process(RestRequest request, RestChannel channel, RestFilterChain filterChain) throws Exception { executeHandler(request, channel); } } private static final class RestHandlerHolder { private final RestHandler restHandler; private final boolean canTripCircuitBreaker; public RestHandlerHolder(RestHandler restHandler, boolean canTripCircuitBreaker) { this.restHandler = restHandler; this.canTripCircuitBreaker = canTripCircuitBreaker; } public RestHandler getRestHandler() { return restHandler; } public boolean canTripCircuitBreaker() { return canTripCircuitBreaker; }	would it make more sense to add a method to the resthandler interface default boolean cantripcircuitbreaker() { return true; } that would safe us all the extra indirection and is simple to override?
public void testSigmoid() { double eps = 0.000001; List<Tuple<Double, Double>> expectations = Arrays.asList( Tuple.tuple(0.0, 0.5), Tuple.tuple(0.5, 0.62245933), Tuple.tuple(1.0, 0.73105857), Tuple.tuple(10000.0, 1.0), Tuple.tuple(-0.5, 0.3775406), Tuple.tuple(-1.0, 0.2689414), Tuple.tuple(-10000.0, 0.0) ); for (Tuple<Double, Double> expectation : expectations) { assertThat(Statistics.sigmoid(expectation.v1()), closeTo(expectation.v2(), eps)); } }	maybe rename to argumentandexpectedresult or inputandexpectedoutput? i know it's lengthy but expectations does not really tell what this list is about without looking at the assertion below. alternatively, you could get rid of this list and rewrite assertions as: assertthat(statistics.sigmoid(0.0), closeto(0.5, eps)); it's a little bit code duplication but i thinks acceptable in tests.
@SuppressForbidden(reason = "Channel is based of a socket not a file") @Override public int write(ByteBuffer src) throws IOException { return SocketAccess.doPrivilegedIOException(() -> writeChannel.write(src)); } })); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Uploads a blob using the "multipart upload" method (a single * 'multipart/related' request containing both data and metadata. The request is * gziped), see: * https://cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload * @param blobInfo the info for the blob to be uploaded * @param inputStream the stream containing the blob data * @param blobSize the size * @param failIfAlreadyExists whether to throw a FileAlreadyExistsException if the given blob already exists */ private void writeBlobMultipart(BlobInfo blobInfo, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) throws IOException { assert blobSize <= LARGE_BLOB_THRESHOLD_BYTE_SIZE : "large blob uploads should use the resumable upload method"; final ByteArrayOutputStream baos = new ByteArrayOutputStream(Math.toIntExact(blobSize)); Streams.copy(inputStream, baos); try { final Storage.BlobTargetOption[] targetOptions = failIfAlreadyExists ? new Storage.BlobTargetOption[] { Storage.BlobTargetOption.doesNotExist() } : new Storage.BlobTargetOption[0]; SocketAccess.doPrivilegedVoidIOException( () -> client().create(blobInfo, baos.toByteArray(), targetOptions)); } catch (final StorageException se) { if (failIfAlreadyExists && se.getCode() == HTTP_PRECON_FAILED) { throw new FileAlreadyExistsException(blobInfo.getBlobId().getName(), null, se.getMessage()); } throw se; } } /** * Deletes the blob from the specific bucket * * @param blobName name of the blob */ void deleteBlob(String blobName) throws IOException { final BlobId blobId = BlobId.of(bucketName, blobName); final boolean deleted = SocketAccess.doPrivilegedIOException(() -> client().delete(blobId)); if (deleted == false) { throw new NoSuchFileException("Blob [" + blobName + "] does not exist"); } } /** * Deletes multiple blobs from the specific bucket all of which have prefixed names * * @param prefix prefix of the blobs to delete */ private void deleteBlobsByPrefix(String prefix) throws IOException { deleteBlobsIgnoringIfNotExists(listBlobsByPrefix("", prefix).keySet()); } /** * Deletes multiple blobs from the specific bucket using a batch request * * @param blobNames names of the blobs to delete */ void deleteBlobsIgnoringIfNotExists(Collection<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } // for a single op submit a simple delete instead of a batch of size 1 if (blobNames.size() == 1) { deleteBlob(blobNames.iterator().next()); return; } final List<BlobId> blobIdsToDelete = blobNames.stream().map(blob -> BlobId.of(bucketName, blob)).collect(Collectors.toList()); final StorageException e = SocketAccess.doPrivilegedIOException(() -> { final AtomicReference<StorageException> ioe = new AtomicReference<>(); final StorageBatch batch = client().batch(); for (BlobId blob : blobIdsToDelete) { batch.delete(blob).notify( new BatchResult.Callback<>() { @Override public void success(Boolean result) { } @Override public void error(StorageException exception) { if (exception.getCode() != HTTP_NOT_FOUND) { if (ioe.compareAndSet(null, exception) == false) { ioe.get().addSuppressed(exception); } } } }); } batch.submit(); return ioe.get(); }	this now throws the storageexception, and not an ioexception like s3's blobcontainer. let's use one pattern (the one we had for s3), not two different ones.
public void testDanglingIndicesImportedAndDeletedCannotBeReimported() throws Exception { final Settings settings = buildSettings(1, true); internalCluster().startNodes(3, settings); final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME); final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME); final AtomicBoolean isImporting = new AtomicBoolean(); final Thread[] importThreads = new Thread[2]; for (int i = 0; i < importThreads.length; i++) { importThreads[i] = new Thread(() -> { //noinspection StatementWithEmptyBody while (isImporting.get() == false) { } while (isImporting.get()) { try { client().admin().cluster().importDanglingIndex(new ImportDanglingIndexRequest(danglingIndexUUID, true)).get(); } catch (Exception e) { // failures are expected } } }); importThreads[i].start(); } isImporting.set(true); final TimeValue timeout = TimeValue.timeValueSeconds(10); final long endTimeMillis = System.currentTimeMillis() + timeout.millis(); while (isImporting.get() && System.currentTimeMillis() < endTimeMillis) { try { client().admin().indices().prepareDelete(INDEX_NAME).get(timeout); isImporting.set(false); } catch (Exception e) { // failures are expected } } try { if (isImporting.get()) { isImporting.set(false); try { client().admin().indices().prepareDelete(INDEX_NAME).get(timeout); } catch (Exception e) { throw new AssertionError("delete index never succeeded", e); } throw new AssertionError("delete index succeeded but too late"); } } finally { for (final Thread importThread : importThreads) { importThread.join(); } } final Metadata metadata = client().admin().cluster().prepareState().clear().setMetadata(true).get().getState().metadata(); assertTrue(metadata.indexGraveyard().toString(), metadata.indexGraveyard().containsIndex(new Index(INDEX_NAME, danglingIndexUUID))); assertNull(Strings.toString(metadata, true, true), metadata.index(INDEX_NAME)); }	nit: i prefer a latch just to not burn cycles on ci unnecessarily. also, this avoids the very unlikely edge case of the main thread passing through 10 seconds of trying to delete the index either before the thread gets here or in one of the loop iterations.
public void testGetIndexWriteRequest() throws Exception { IndexRequest indexRequest = new IndexRequest("index", "type", "id1").source(Collections.emptyMap()); UpdateRequest upsertRequest = new UpdateRequest("index", "type", "id1").upsert(indexRequest).script(mockScript("1")); UpdateRequest docAsUpsertRequest = new UpdateRequest("index", "type", "id2").doc(indexRequest).docAsUpsert(true); UpdateRequest scriptedUpsert = new UpdateRequest("index", "type", "id2").upsert(indexRequest).script(mockScript("1")) .scriptedUpsert(true); assertEquals(TransportBulkAction.getIndexWriteRequest(indexRequest), indexRequest); assertEquals(TransportBulkAction.getIndexWriteRequest(upsertRequest), indexRequest); assertEquals(TransportBulkAction.getIndexWriteRequest(docAsUpsertRequest), indexRequest); assertEquals(TransportBulkAction.getIndexWriteRequest(scriptedUpsert), indexRequest); DeleteRequest deleteRequest = new DeleteRequest("index", "id"); assertNull(TransportBulkAction.getIndexWriteRequest(deleteRequest)); UpdateRequest badUpsertRequest = new UpdateRequest("index", "type", "id1"); assertNull(TransportBulkAction.getIndexWriteRequest(badUpsertRequest)); }	nit: random empty line :)
static void validateForFips(Settings settings, XPackLicenseState licenseState) { final List<String> validationErrors = new ArrayList<>(); Settings keystoreTypeSettings = settings.filter(k -> k.endsWith("keystore.type")) .filter(k -> settings.get(k).equalsIgnoreCase("jks")); if (keystoreTypeSettings.isEmpty() == false) { validationErrors.add("JKS Keystores cannot be used in a FIPS 140 compliant JVM. Please " + "revisit [" + keystoreTypeSettings.toDelimitedString(',') + "] settings"); } Settings keystorePathSettings = settings.filter(k -> k.endsWith("keystore.path")) .filter(k -> settings.hasValue(k.replace(".path", ".type")) == false); // Default Keystore type is JKS in JDK8 and PKCS12 in > JDK9 if not explicitly set if (keystorePathSettings.isEmpty() == false && JavaVersion.current().compareTo(JavaVersion.parse("9")) < 0) { validationErrors.add("JKS Keystores cannot be used in a FIPS 140 compliant JVM. Please " + "revisit [" + keystorePathSettings.toDelimitedString(',') + "] settings"); } final String selectedAlgorithm = XPackSettings.PASSWORD_HASHING_ALGORITHM.get(settings); if (selectedAlgorithm.toLowerCase(Locale.ROOT).startsWith("pbkdf2") == false) { validationErrors.add("Only PBKDF2 is allowed for password hashing in a FIPS 140 JVM. Please set the " + "appropriate value for [ " + XPackSettings.PASSWORD_HASHING_ALGORITHM.getKey() + " ] setting."); } if (licenseState != null && FIPS_ALLOWED_LICENSE_OPERATION_MODES.contains(licenseState.getOperationMode()) == false) { validationErrors.add("FIPS mode is only allowed with a Platinum or Trial license"); } if (validationErrors.isEmpty() == false) { final StringBuilder sb = new StringBuilder(); sb.append("Validation for FIPS 140 mode failed: \\\\n"); int index = 0; for (String error : validationErrors) { sb.append(++index).append(": ").append(error).append(";\\\\n"); } throw new IllegalArgumentException(sb.toString()); } }	this is not the case. we explicitly set the keystore type to jks, and don't depend on the jvm default. https://github.com/elastic/elasticsearch/blob/9945d5cf35183263184e518aa73a72959fb15a0e/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ssl/sslconfigurationsettings.java#l245-l252 perhaps we should call inferkeystoretype here instead.
static void validateForFips(Settings settings, XPackLicenseState licenseState) { final List<String> validationErrors = new ArrayList<>(); Settings keystoreTypeSettings = settings.filter(k -> k.endsWith("keystore.type")) .filter(k -> settings.get(k).equalsIgnoreCase("jks")); if (keystoreTypeSettings.isEmpty() == false) { validationErrors.add("JKS Keystores cannot be used in a FIPS 140 compliant JVM. Please " + "revisit [" + keystoreTypeSettings.toDelimitedString(',') + "] settings"); } Settings keystorePathSettings = settings.filter(k -> k.endsWith("keystore.path")) .filter(k -> settings.hasValue(k.replace(".path", ".type")) == false); // Default Keystore type is JKS in JDK8 and PKCS12 in > JDK9 if not explicitly set if (keystorePathSettings.isEmpty() == false && JavaVersion.current().compareTo(JavaVersion.parse("9")) < 0) { validationErrors.add("JKS Keystores cannot be used in a FIPS 140 compliant JVM. Please " + "revisit [" + keystorePathSettings.toDelimitedString(',') + "] settings"); } final String selectedAlgorithm = XPackSettings.PASSWORD_HASHING_ALGORITHM.get(settings); if (selectedAlgorithm.toLowerCase(Locale.ROOT).startsWith("pbkdf2") == false) { validationErrors.add("Only PBKDF2 is allowed for password hashing in a FIPS 140 JVM. Please set the " + "appropriate value for [ " + XPackSettings.PASSWORD_HASHING_ALGORITHM.getKey() + " ] setting."); } if (licenseState != null && FIPS_ALLOWED_LICENSE_OPERATION_MODES.contains(licenseState.getOperationMode()) == false) { validationErrors.add("FIPS mode is only allowed with a Platinum or Trial license"); } if (validationErrors.isEmpty() == false) { final StringBuilder sb = new StringBuilder(); sb.append("Validation for FIPS 140 mode failed: \\\\n"); int index = 0; for (String error : validationErrors) { sb.append(++index).append(": ").append(error).append(";\\\\n"); } throw new IllegalArgumentException(sb.toString()); } }	the error message should be built from fips_allowed_license_operation_modes
public void testJoinValidatorForFIPSLicense() throws Exception { DiscoveryNode node = new DiscoveryNode("foo", buildNewFakeTransportAddress(), VersionUtils.randomVersionBetween(random(), null, Version.CURRENT)); MetaData.Builder builder = MetaData.builder(); License license = TestUtils.generateSignedLicense(TimeValue.timeValueHours(24)); TestUtils.putLicense(builder, license); ClusterState state = ClusterState.builder(ClusterName.DEFAULT).metaData(builder.build()).build(); new Security.ValidateLicenseForFIPS(false).accept(node, state); final boolean isLicenseValidForFips = Security.FIPS_ALLOWED_LICENSE_OPERATION_MODES.contains(license.operationMode()); if (isLicenseValidForFips) { new Security.ValidateLicenseForFIPS(true).accept(node, state); } else { IllegalStateException e = expectThrows(IllegalStateException.class, () -> new Security.ValidateLicenseForFIPS(true).accept(node, state)); assertThat(e.getMessage(), containsString("FIPS mode cannot be used")); } }	this (existing) test is testing 2 different scenarios based on randomness in the license type. can we move it to 2 explicit test cases?
public void testTemplateExists() throws IOException { try (XContentBuilder builder = jsonBuilder()) { builder.startObject(); { builder.array("index_patterns", "*"); builder.startObject("settings"); { builder.field("number_of_replicas", 0); } builder.endObject(); } builder.endObject(); Request request = new Request("PUT", "/_template/template"); if (inFipsJvm()) { request.setOptions(expectWarnings( "legacy template [template] has index patterns [*] matching patterns from existing composable templates " + "[.deprecation-indexing-template,.slm-history,.triggered_watches,.watch-history-14,.watches,ilm-history,logs," + "metrics,synthetics] with patterns (.deprecation-indexing-template => [.logs-deprecation-elasticsearch.default]," + ".slm-history => [.slm-history-5*],.triggered_watches => [.triggered_watches*]," + ".watch-history-14 => [.watcher-history-14*],.watches => [.watches*],ilm-history => [ilm-history-5*]," + "logs => [logs-*-*],metrics => [metrics-*-*],synthetics => [synthetics-*-*]" + "); this template [template] may be ignored in favor of a composable template at index creation time")); } request.setJsonEntity(Strings.toString(builder)); client().performRequest(request); headTestCase("/_template/template", emptyMap(), greaterThan(0)); } }	looking at the naming scheme (https://www.elastic.co/blog/an-introduction-to-the-elastic-data-stream-naming-scheme) we are using here elasticsearch.default as the namespace which i'm not sure if this is on purpose? can you point me to the discussion with the ecs folks to better understand the structure here? is it expected that all stack components ship data to a single data stream or each service will have its own data stream with its own data structure?
private static int findNextMarker(byte marker, int from, BytesReference data, int length) { final int res = data.findNextMarker(marker, from, length); if (res > -1) { return res; } if (from != length) { throw new IllegalArgumentException("The msearch request must be terminated by a newline [\\\\n]"); } return -1; }	this should be != -1 and assert that it's positive instead?
public void testBulkIndexFailuresCauseTaskToFail() throws Exception { String transformId = "bulk-failure-pivot"; String dataFrameIndex = "pivot-failure-index"; createPivotReviewsTransform(transformId, dataFrameIndex, null, null, null); try (XContentBuilder builder = jsonBuilder()) { builder.startObject(); { builder.startObject("mappings") .startObject("properties") .startObject("reviewer") // This type should cause mapping coercion type conflict on bulk index .field("type", "long") .endObject() .endObject() .endObject(); } builder.endObject(); final StringEntity entity = new StringEntity(Strings.toString(builder), ContentType.APPLICATION_JSON); Request req = new Request("PUT", dataFrameIndex); req.setEntity(entity); client().performRequest(req); } startDataframeTransform(transformId, false, null); assertBusy(() -> assertEquals(DataFrameTransformTaskState.FAILED.value(), getDataFrameTaskState(transformId)), 120, TimeUnit.SECONDS); Map<?, ?> state = getDataFrameState(transformId); assertThat((String) XContentMapValues.extractValue("state.reason", state), containsString("task encountered more than 10 failures; latest failure: Bulk index experienced failures.")); // Force stop the transform as bulk indexing caused it to go into a failed state stopDataFrameTransform(transformId, true); deleteIndex(dataFrameIndex); }	i delete the index here as these tests keep created indices by default. i delete it so that it can be ran with repeatedly with the -dtests.iters flag
@Override protected void doNextBulk(BulkRequest request, ActionListener<BulkResponse> nextPhase) { ClientHelper.executeWithHeadersAsync(transformConfig.getHeaders(), ClientHelper.DATA_FRAME_ORIGIN, client, BulkAction.INSTANCE, request, ActionListener.wrap(bulkResponse -> { if (bulkResponse.hasFailures()) { int failureCount = 0; for(BulkItemResponse item : bulkResponse.getItems()) { if (item.isFailed()) { failureCount++; } // TODO gather information on irrecoverable failures and update isIrrecoverableFailure } if (auditBulkFailures) { auditor.warning(transformId, "Experienced at least [" + failureCount + "] bulk index failures. See the logs of the node running the transform for details. " + bulkResponse.buildFailureMessage()); auditBulkFailures = false; } // This calls AsyncTwoPhaseIndexer#finishWithIndexingFailure // It increments the indexing failure, and then calls the `onFailure` logic nextPhase.onFailure( new BulkIndexFailure("Bulk index experienced failures. " + "See the logs of the node running the transform for details.")); } else { auditBulkFailures = true; nextPhase.onResponse(bulkResponse); } }, nextPhase::onFailure)); }	we should only write this large audit once per page. now that we fail on bulk failures, it could occur more than once per page (possibly causing the task to fail).
@Override protected void doNextBulk(BulkRequest request, ActionListener<BulkResponse> nextPhase) { ClientHelper.executeWithHeadersAsync(transformConfig.getHeaders(), ClientHelper.DATA_FRAME_ORIGIN, client, BulkAction.INSTANCE, request, ActionListener.wrap(bulkResponse -> { if (bulkResponse.hasFailures()) { int failureCount = 0; for(BulkItemResponse item : bulkResponse.getItems()) { if (item.isFailed()) { failureCount++; } // TODO gather information on irrecoverable failures and update isIrrecoverableFailure } if (auditBulkFailures) { auditor.warning(transformId, "Experienced at least [" + failureCount + "] bulk index failures. See the logs of the node running the transform for details. " + bulkResponse.buildFailureMessage()); auditBulkFailures = false; } // This calls AsyncTwoPhaseIndexer#finishWithIndexingFailure // It increments the indexing failure, and then calls the `onFailure` logic nextPhase.onFailure( new BulkIndexFailure("Bulk index experienced failures. " + "See the logs of the node running the transform for details.")); } else { auditBulkFailures = true; nextPhase.onResponse(bulkResponse); } }, nextPhase::onFailure)); }	i manually created a logging message here so that it is reliably the same thing each time. there could be different bulk failures on each attempt and thus gets logged/audit too many times as we continue to retry the indexing request.
@Override protected void doNextBulk(BulkRequest request, ActionListener<BulkResponse> nextPhase) { ClientHelper.executeWithHeadersAsync(transformConfig.getHeaders(), ClientHelper.DATA_FRAME_ORIGIN, client, BulkAction.INSTANCE, request, ActionListener.wrap(bulkResponse -> { if (bulkResponse.hasFailures()) { int failureCount = 0; for(BulkItemResponse item : bulkResponse.getItems()) { if (item.isFailed()) { failureCount++; } // TODO gather information on irrecoverable failures and update isIrrecoverableFailure } if (auditBulkFailures) { auditor.warning(transformId, "Experienced at least [" + failureCount + "] bulk index failures. See the logs of the node running the transform for details. " + bulkResponse.buildFailureMessage()); auditBulkFailures = false; } // This calls AsyncTwoPhaseIndexer#finishWithIndexingFailure // It increments the indexing failure, and then calls the `onFailure` logic nextPhase.onFailure( new BulkIndexFailure("Bulk index experienced failures. " + "See the logs of the node running the transform for details.")); } else { auditBulkFailures = true; nextPhase.onResponse(bulkResponse); } }, nextPhase::onFailure)); }	we can audit the bulk failures again on the next page
public void putAsync(ILMHistoryItem item) { if (ilmHistoryEnabled == false) { logger.trace("not recording ILM history item because [{}] is [false]: [{}]", LIFECYCLE_HISTORY_INDEX_ENABLED_SETTING.getKey(), item); return; } logger.trace("queueing ILM history item for indexing [{}]: [{}]", ILM_HISTORY_ALIAS, item); try (XContentBuilder builder = XContentFactory.jsonBuilder()) { item.toXContent(builder, ToXContent.EMPTY_PARAMS); IndexRequest request = new IndexRequest(ILM_HISTORY_ALIAS).source(builder); // TODO: remove the threadpool wrapping when the .add call is non-blocking // (it can currently execute the bulk request occasionally) threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> { try { processor.add(request); } catch (Exception e) { logger.error(new ParameterizedMessage("failed add ILM history item to queue for index [{}]: [{}]", ILM_HISTORY_ALIAS, item), e); } }); } catch (IOException exception) { logger.error(new ParameterizedMessage("failed to queue ILM history item in index [{}]: [{}]", ILM_HISTORY_ALIAS, item), exception); } }	oh, this is not intuitive/obvious. maybe in a follow-up pr shall we rename the add method to addandmaybeexecute or something along those lines?
public void testAllocateEmptyPrimaryResetGlobalCheckpoint() throws Exception { internalCluster().startMasterOnlyNode(Settings.EMPTY); final List<String> dataNodes = internalCluster().startDataOnlyNodes(2); final Settings nodeSettings = internalCluster().dataPathSettings(randomFrom(dataNodes)); final String indexName = "test"; assertAcked(client().admin().indices().prepareCreate(indexName).setSettings(Settings.builder() .put("index.number_of_shards", 1).put("index.number_of_replicas", 1) .put(MockEngineSupport.DISABLE_FLUSH_ON_CLOSE.getKey(), randomBoolean())).get()); final List<IndexRequestBuilder> indexRequests = IntStream.range(0, between(10, 500)) .mapToObj(n -> client().prepareIndex(indexName, "type").setSource("foo", "bar")) .collect(Collectors.toList()); indexRandom(randomBoolean(), true, true, indexRequests); ensureGreen(); internalCluster().stopRandomDataNode(); internalCluster().stopRandomDataNode(); final String nodeWithoutData = internalCluster().startDataOnlyNode(); assertAcked(client().admin().cluster().prepareReroute() .add(new AllocateEmptyPrimaryAllocationCommand(indexName, 0, nodeWithoutData, true)).get()); internalCluster().startDataOnlyNode(nodeSettings); ensureGreen(); for (ShardStats shardStats : client().admin().indices().prepareStats(indexName).get().getIndex(indexName).getShards()) { assertThat(shardStats.getSeqNoStats().getMaxSeqNo(), equalTo(SequenceNumbers.NO_OPS_PERFORMED)); assertThat(shardStats.getSeqNoStats().getLocalCheckpoint(), equalTo(SequenceNumbers.NO_OPS_PERFORMED)); assertThat(shardStats.getSeqNoStats().getGlobalCheckpoint(), equalTo(SequenceNumbers.NO_OPS_PERFORMED)); } }	perhaps name this randomnodedatapathsettings
public static boolean isAccessibleDirectory(File directory, ESLogger logger) { assert directory != null && logger != null; if (!directory.exists()) { logger.debug("plugins directory does not exist [{}].", directory.getAbsolutePath()); return false; } if (!directory.isDirectory()) { logger.debug("plugins directory is not a directory [{}].", directory.getAbsolutePath()); return false; } if (!directory.canRead()) { logger.debug("plugins directory is not readable [{}].", directory.getAbsolutePath()); return false; } return true; }	this is more generic now, not plugins anymore... same for the lines below
private static String encodePart(String pathPart) { try { //encode each part (e.g. index, type and id) separately before merging them into the path //we prepend "/" to the path part to make this pate absolute, otherwise there can be issues with //paths that start with `-` or contain `:` URI uri = new URI(null, null, null, -1, "/" + pathPart, null, null); //manually encode any slash that each part may contain return uri.getRawPath().substring(1).replaceAll("/", "%2F").replaceAll("\\\\\\\\+", "%2B"); } catch (URISyntaxException e) { throw new IllegalArgumentException("Path part [" + pathPart + "] couldn't be encoded", e); } }	why are we doing this manually and piecemeal instead of using urlencoder?
protected void doExecute(GetCategoriesAction.Request request, ActionListener<GetCategoriesAction.Response> listener) { jobManager.getJobOrThrowIfUnknown(request.getJobId()); Integer from = request.getPageParams() != null ? request.getPageParams().getFrom() : null; Integer size = request.getPageParams() != null ? request.getPageParams().getSize() : null; jobProvider.categoryDefinitions(request.getJobId(), request.getCategoryId(), true, from, size, r -> listener.onResponse(new GetCategoriesAction.Response(r)), listener::onFailure, client); }	should this be exposed as a request parameter?
* @param from Skip the first N categories. This parameter is for paging * @param size Take only this number of categories */ public void categoryDefinitions(String jobId, Long categoryId, boolean augment, Integer from, Integer size, Consumer<QueryPage<CategoryDefinition>> handler, Consumer<Exception> errorHandler, Client client) { if (categoryId != null && (from != null || size != null)) { throw new IllegalStateException("Both categoryId and pageParams are specified"); } String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId); LOGGER.trace("ES API CALL: search all of category definitions from index {} sort ascending {} from {} size {}", indexName, CategoryDefinition.CATEGORY_ID.getPreferredName(), from, size); SearchRequest searchRequest = new SearchRequest(indexName); searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions())); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); if (categoryId != null) { sourceBuilder.query(QueryBuilders.termQuery(CategoryDefinition.CATEGORY_ID.getPreferredName(), categoryId)); } else if (from != null && size != null) { sourceBuilder.from(from).size(size) .query(QueryBuilders.existsQuery(CategoryDefinition.CATEGORY_ID.getPreferredName())) .sort(new FieldSortBuilder(CategoryDefinition.CATEGORY_ID.getPreferredName()).order(SortOrder.ASC)); } else { throw new IllegalStateException("Both categoryId and pageParams are not specified"); } searchRequest.source(sourceBuilder); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest, ActionListener.<SearchResponse>wrap(searchResponse -> { SearchHit[] hits = searchResponse.getHits().getHits(); List<CategoryDefinition> results = new ArrayList<>(hits.length); for (SearchHit hit : hits) { BytesReference source = hit.getSourceRef(); try (InputStream stream = source.streamInput(); XContentParser parser = XContentFactory.xContent(XContentHelper.xContentType(source)) .createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) { CategoryDefinition categoryDefinition = CategoryDefinition.LENIENT_PARSER.apply(parser, null); if (augment) { augmentWithGrokPattern(categoryDefinition); } results.add(categoryDefinition); } catch (IOException e) { throw new ElasticsearchParseException("failed to parse category definition", e); } } QueryPage<CategoryDefinition> result = new QueryPage<>(results, searchResponse.getHits().getTotalHits(), CategoryDefinition.RESULTS_FIELD); handler.accept(result); }, e -> errorHandler.accept(mapAuthFailure(e, jobId, GetCategoriesAction.NAME))), client::search); }	the javadoc will need updating for the new parameter
public void processQueryString(String queryString, ActionListener<SamlValidateAuthnRequestResponse> listener) { final ParsedQueryString parsedQueryString; try { parsedQueryString = parseQueryString(queryString); } catch (ElasticsearchSecurityException e) { logger.debug("Failed to parse query string for SAML AuthnRequest", e); listener.onFailure(e); return; } try { // We consciously parse the AuthnRequest before we validate its signature as we need to get the Issuer, in order to // verify if we know of this SP and get its credentials for signature verification final Element root = parseSamlMessage(inflate(decodeBase64(parsedQueryString.samlRequest))); if (samlFactory.elementNameMatches(root, "urn:oasis:names:tc:SAML:2.0:protocol", "AuthnRequest") == false) { logAndRespond(new ParameterizedMessage("SAML message [{}] is not an AuthnRequest", samlFactory.text(root, 128)), listener); return; } final AuthnRequest authnRequest = samlFactory.buildXmlObject(root, AuthnRequest.class); getSpFromIssuer(authnRequest.getIssuer(), ActionListener.wrap( sp -> { try { validateAuthnRequest(authnRequest, sp, parsedQueryString, listener); } catch (ElasticsearchSecurityException e) { logger.debug("Could not validate AuthnRequest", e); listener.onFailure(e); } catch (Exception e) { logAndRespond("Could not validate AuthnRequest", e, listener); } }, listener::onFailure )); } catch (ElasticsearchSecurityException e) { logger.debug("Could not process AuthnRequest", e); listener.onFailure(e); } catch (Exception e) { logAndRespond("Could not process AuthnRequest", e, listener); } }	todo: this return value (boolean) is obsolete. it was there from intellij's refactoring and i forgot to fix it.
public void processQueryString(String queryString, ActionListener<SamlValidateAuthnRequestResponse> listener) { final ParsedQueryString parsedQueryString; try { parsedQueryString = parseQueryString(queryString); } catch (ElasticsearchSecurityException e) { logger.debug("Failed to parse query string for SAML AuthnRequest", e); listener.onFailure(e); return; } try { // We consciously parse the AuthnRequest before we validate its signature as we need to get the Issuer, in order to // verify if we know of this SP and get its credentials for signature verification final Element root = parseSamlMessage(inflate(decodeBase64(parsedQueryString.samlRequest))); if (samlFactory.elementNameMatches(root, "urn:oasis:names:tc:SAML:2.0:protocol", "AuthnRequest") == false) { logAndRespond(new ParameterizedMessage("SAML message [{}] is not an AuthnRequest", samlFactory.text(root, 128)), listener); return; } final AuthnRequest authnRequest = samlFactory.buildXmlObject(root, AuthnRequest.class); getSpFromIssuer(authnRequest.getIssuer(), ActionListener.wrap( sp -> { try { validateAuthnRequest(authnRequest, sp, parsedQueryString, listener); } catch (ElasticsearchSecurityException e) { logger.debug("Could not validate AuthnRequest", e); listener.onFailure(e); } catch (Exception e) { logAndRespond("Could not validate AuthnRequest", e, listener); } }, listener::onFailure )); } catch (ElasticsearchSecurityException e) { logger.debug("Could not process AuthnRequest", e); listener.onFailure(e); } catch (Exception e) { logAndRespond("Could not process AuthnRequest", e, listener); } }	again - i see the pattern here.. - i dislike the fact that we have multiple methods with the same name doing similar stuff. arguments can be made that signature verification _is_ part of the validation of the authnrequest so i'm fine with how you moved that around but could we simple replace this with suggestion checkdestination(authnrequest); checkacs(authnrequest, sp); ? i see no reason keeping the second validateauthnrequest now
@Override public URL getSingleLogoutEndpoint(String binding) { return sloEndpoints.get(binding); } /** * Asynchronously lookup the specified {@link SamlServiceProvider} by entity-id. * @param listener Responds with the requested Service Provider object, or {@code null} if no such SP exists. * {@link ActionListener#onFailure}	nit: entityid, entityid, entityid all work better than entity-id :)
public void testRecordCorrectSegmentCountsWithBackgroundMerges() throws Exception { final String repoName = "test-repo"; createRepository(repoName, "fs"); final String indexName = "test"; // disable merges assertAcked(prepareCreate(indexName).setSettings(indexSettingsNoReplicas(1).put(MergePolicyConfig.INDEX_MERGE_ENABLED, "false"))); // create an empty snapshot so that later snapshots run as quickly as possible createFullSnapshot(repoName, "empty"); // create a situation where we temporarily have a bunch of segments until the merges can catch up long id = 0; final int rounds = scaledRandomIntBetween(3, 5); for (int i = 0; i < rounds; ++i) { final int numDocs = scaledRandomIntBetween(100, 1000); BulkRequestBuilder request = client().prepareBulk().setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); for (int j = 0; j < numDocs; ++j) { request.add( Requests.indexRequest(indexName) .id(Long.toString(id++)) .source(jsonBuilder().startObject().field("l", randomLong()).endObject()) ); } assertNoFailures(request.get()); } // snapshot with a bunch of unmerged segments final SnapshotInfo before = createFullSnapshot(repoName, "snapshot-before"); final SnapshotInfo.IndexSnapshotDetails beforeIndexDetails = before.indexSnapshotDetails().get(indexName); final long beforeSegmentCount = beforeIndexDetails.getMaxSegmentsPerShard(); // reactivate merges assertAcked(admin().indices().prepareClose(indexName).get()); assertAcked( admin().indices() .prepareUpdateSettings(indexName) .setSettings( Settings.builder() .put(MergePolicyConfig.INDEX_MERGE_POLICY_SEGMENTS_PER_TIER_SETTING.getKey(), "2") .put(MergePolicyConfig.INDEX_MERGE_ENABLED, "true") ) ); assertAcked(admin().indices().prepareOpen(indexName).get()); assertEquals(0, admin().indices().prepareForceMerge(indexName).setFlush(true).get().getFailedShards()); // wait for merges to reduce segment count assertBusy(() -> { IndicesStatsResponse stats = client().admin().indices().prepareStats(indexName).setSegments(true).get(); assertThat(stats.getIndex(indexName).getPrimaries().getSegments().getCount(), lessThan(beforeSegmentCount)); }, 30L, TimeUnit.SECONDS); final SnapshotInfo after = createFullSnapshot(repoName, "snapshot-after"); final int incrementalFileCount = clusterAdmin().prepareSnapshotStatus() .setRepository(repoName) .setSnapshots(after.snapshotId().getName()) .get() .getSnapshots() .get(0) .getStats() .getIncrementalFileCount(); assertEquals(0, incrementalFileCount); logger.info("--> no files have changed between snapshots, asserting that segment counts are constant as well"); final SnapshotInfo.IndexSnapshotDetails afterIndexDetails = after.indexSnapshotDetails().get(indexName); assertEquals(beforeSegmentCount, afterIndexDetails.getMaxSegmentsPerShard()); }	we know we did >=3 rounds, each of which made >=1 segment, and they're all pretty small (<=2mb) so this guarantees a merge iiuc.
@Override public Iterator<Setting<?>> settings() { return dependencies.iterator(); } } public DataTierAllocationDecider() { } @Override public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { return shouldFilter(shardRouting, node.node(), allocation); } @Override public Decision canAllocate(IndexMetadata indexMetadata, RoutingNode node, RoutingAllocation allocation) { return shouldFilter(indexMetadata, node.node().getRoles(), allocation); } @Override public Decision canRemain(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { return shouldFilter(shardRouting, node.node(), allocation); } @Override public Decision shouldAutoExpandToNode(IndexMetadata indexMetadata, DiscoveryNode node, RoutingAllocation allocation) { return shouldFilter(indexMetadata, node.getRoles(), allocation); } private Decision shouldFilter(ShardRouting shardRouting, DiscoveryNode node, RoutingAllocation allocation) { return shouldFilter(allocation.metadata().getIndexSafe(shardRouting.index()), node.getRoles(), allocation); } public Decision shouldFilter(IndexMetadata indexMd, Set<DiscoveryNodeRole> roles, RoutingAllocation allocation) { return shouldFilter(indexMd, roles, DataTierAllocationDecider::preferredAvailableTier, allocation); } public interface PreferredTierFunction { Optional<String> apply(String tierPreference, DiscoveryNodes nodes); } public Decision shouldFilter(IndexMetadata indexMd, Set<DiscoveryNodeRole> roles, PreferredTierFunction preferredTierFunction, RoutingAllocation allocation) { Decision decision = shouldIndexPreferTier(indexMd, roles, preferredTierFunction, allocation); if (decision != null) { return decision; } return allocation.decision(Decision.YES, NAME, "node passes tier preference filters"); } private Decision shouldIndexPreferTier(IndexMetadata indexMetadata, Set<DiscoveryNodeRole> roles, PreferredTierFunction preferredTierFunction, RoutingAllocation allocation) { Settings indexSettings = indexMetadata.getSettings(); String tierPreference = INDEX_ROUTING_PREFER_SETTING.get(indexSettings); if (Strings.hasText(tierPreference)) { Optional<String> tier = preferredTierFunction.apply(tierPreference, allocation.nodes()); if (tier.isPresent()) { String tierName = tier.get(); if (allocationAllowed(tierName, roles)) { return allocation.decision(Decision.YES, NAME, "index has a preference for tiers [%s] and node has tier [%s]", tierPreference, tierName); } else { return allocation.decision(Decision.NO, NAME, "index has a preference for tiers [%s] and node does not meet the required [%s] tier", tierPreference, tierName); } } else { return allocation.decision(Decision.NO, NAME, "index has a preference for tiers [%s], " + "but no nodes for any of those tiers are available in the cluster", tierPreference); } } return null; } /** * Given a string of comma-separated prioritized tiers (highest priority * first) and an allocation, find the highest priority tier for which nodes * exist. If no nodes for any of the tiers are available, returns an empty * {@code Optional<String>}. */ public static Optional<String> preferredAvailableTier(String prioritizedTiers, DiscoveryNodes nodes) { for (String tier : parseTierList(prioritizedTiers)) { if (tierNodesPresent(tier, nodes)) { return Optional.of(tier); } } return Optional.empty(); } public static String[] parseTierList(String tiers) { if (Strings.hasText(tiers) == false) { // avoid parsing overhead in the null/empty string case return Strings.EMPTY_ARRAY; } else { return tiers.split(","); } } static boolean tierNodesPresent(String singleTier, DiscoveryNodes nodes) { assert singleTier.equals(DiscoveryNodeRole.DATA_ROLE.roleName()) || DataTier.validTierName(singleTier) : "tier " + singleTier + " is an invalid tier name"; for (ObjectCursor<DiscoveryNode> node : nodes.getNodes().values()) { for (DiscoveryNodeRole discoveryNodeRole : node.value.getRoles()) { String s = discoveryNodeRole.roleName(); if (s.equals(DiscoveryNodeRole.DATA_ROLE.roleName()) || s.equals(singleTier)) { return true; } } } return false; } private static boolean allocationAllowed(String tierName, Set<DiscoveryNodeRole> roles) { assert Strings.hasText(tierName) : "tierName must be not null and non-empty, but was [" + tierName + "]"; if (roles.contains(DiscoveryNodeRole.DATA_ROLE)) { // generic "data" roles are considered to have all tiers return true; } else { for (DiscoveryNodeRole role : roles) { if (tierName.equals(role.roleName())) { return true; } } return false; }	this changes the behavior slightly, for example, this would work previously: _tier_preference: "_data_hot, _data_cold" but now it will not, because .split(",") does not call .trim() on its individual tokens. are we sure we're okay with that behavioral change? this could cause a big behavioral change going from 7.15 -> 7.16
@SuppressWarnings("unchecked") public void testPreviewTransform() throws Exception { final Request createPreviewRequest = new Request("POST", DATAFRAME_ENDPOINT + "_preview"); String config = "{" + " \\\\"source\\\\": \\\\"reviews\\\\"," + " \\\\"id\\\\": \\\\"doesnot-matter\\\\"," + " \\\\"dest\\\\": \\\\"doesnot-matter\\\\","; config += " \\\\"pivot\\\\": {" + " \\\\"group_by\\\\": {" + " \\\\"reviewer\\\\": {\\\\"terms\\\\": { \\\\"field\\\\": \\\\"user_id\\\\" }}," + " \\\\"by_day\\\\": {\\\\"date_histogram\\\\": {\\\\"interval\\\\": \\\\"1d\\\\",\\\\"field\\\\":\\\\"timestamp\\\\",\\\\"format\\\\":\\\\"yyyy-MM-DD\\\\"}}}," + " \\\\"aggregations\\\\": {" + " \\\\"avg_rating\\\\": {" + " \\\\"avg\\\\": {" + " \\\\"field\\\\": \\\\"stars\\\\"" + " } } } }" + "}"; createPreviewRequest.setJsonEntity(config); Map<String, Object> previewDataframeResponse = entityAsMap(client().performRequest(createPreviewRequest)); List<Map<String, Object>> preview = (List<Map<String, Object>>)previewDataframeResponse.get("data_frame_preview"); assertThat(preview.size(), equalTo(393)); Set<String> expectedFields = new HashSet<>(Arrays.asList("reviewer", "by_day", "avg_rating")); preview.forEach(p -> { Set<String> keys = p.keySet(); assertThat(keys, equalTo(expectedFields)); }); }	we do not need the id for a preview endpoint, can we make this optional?
@SuppressWarnings("unchecked") public void testPreviewTransform() throws Exception { final Request createPreviewRequest = new Request("POST", DATAFRAME_ENDPOINT + "_preview"); String config = "{" + " \\\\"source\\\\": \\\\"reviews\\\\"," + " \\\\"id\\\\": \\\\"doesnot-matter\\\\"," + " \\\\"dest\\\\": \\\\"doesnot-matter\\\\","; config += " \\\\"pivot\\\\": {" + " \\\\"group_by\\\\": {" + " \\\\"reviewer\\\\": {\\\\"terms\\\\": { \\\\"field\\\\": \\\\"user_id\\\\" }}," + " \\\\"by_day\\\\": {\\\\"date_histogram\\\\": {\\\\"interval\\\\": \\\\"1d\\\\",\\\\"field\\\\":\\\\"timestamp\\\\",\\\\"format\\\\":\\\\"yyyy-MM-DD\\\\"}}}," + " \\\\"aggregations\\\\": {" + " \\\\"avg_rating\\\\": {" + " \\\\"avg\\\\": {" + " \\\\"field\\\\": \\\\"stars\\\\"" + " } } } }" + "}"; createPreviewRequest.setJsonEntity(config); Map<String, Object> previewDataframeResponse = entityAsMap(client().performRequest(createPreviewRequest)); List<Map<String, Object>> preview = (List<Map<String, Object>>)previewDataframeResponse.get("data_frame_preview"); assertThat(preview.size(), equalTo(393)); Set<String> expectedFields = new HashSet<>(Arrays.asList("reviewer", "by_day", "avg_rating")); preview.forEach(p -> { Set<String> keys = p.keySet(); assertThat(keys, equalTo(expectedFields)); }); }	same as for id, would be good if it works without dest
private void warnAboutDiskIfNeeded(DiskUsage usage) { // Check absolute disk values if (usage.getFreeBytes() < DiskThresholdDecider.this.freeBytesThresholdHigh.bytes()) { logger.warn("high disk watermark [{}] exceeded on {}, shards will be relocated away from this node", DiskThresholdDecider.this.freeBytesThresholdHigh, usage); } else if (usage.getFreeBytes() < DiskThresholdDecider.this.freeBytesThresholdLow.bytes()) { logger.info("low disk watermark [{}] exceeded on {}, replicas will not be assigned to this node", DiskThresholdDecider.this.freeBytesThresholdLow, usage); } // Check percentage disk values if (usage.getFreeDiskAsPercentage() < DiskThresholdDecider.this.freeDiskThresholdHigh) { logger.warn("high disk watermark [{}] exceeded on {}, shards will be relocated away from this node", Strings.format1Decimals(DiskThresholdDecider.this.freeDiskThresholdHigh, "%"), usage); } else if (usage.getFreeDiskAsPercentage() < DiskThresholdDecider.this.freeDiskThresholdLow) { logger.info("low disk watermark [{}] exceeded on {}, replicas will not be assigned to this node", Strings.format1Decimals(100.0 - DiskThresholdDecider.this.freeDiskThresholdLow, "%"), usage); } }	i believe this should be 100.0 - diskthresholddecider.this.freediskthresholdhigh, just like below, right?
private boolean checkIndexMappingUpToDate(ClusterState clusterState) { return checkIndexMappingVersionMatches(clusterState, Version.CURRENT::equals); }	shouldn't this be onorafter?
public CacheBuilder<K, V> setMaximumWeight(long maximumWeight) { if (maximumWeight < 0) { throw new IllegalArgumentException("maximumWeight < 0"); } this.maximumWeight = maximumWeight; return this; } /** * Sets the amount of time before an entry in the cache expires after it was last accessed. * * @param expireAfterAccess The amount of time before an entry expires after it was last accessed. Must not be {@code null}	it should say greater than zero, 0 is not permitted.
public CacheBuilder<K, V> setExpireAfterAccess(TimeValue expireAfterAccess) { Objects.requireNonNull(expireAfterAccess); final long expireAfterAccessNanos = expireAfterAccess.getNanos(); if (expireAfterAccessNanos <= 0) { throw new IllegalArgumentException("expireAfterAccess <= 0"); } this.expireAfterAccessNanos = expireAfterAccessNanos; return this; } /** * Sets the amount of time before an entry in the cache expires after it was written. * * @param expireAfterWrite The amount of time before an entry expires after it was written. Must not be {@code null}	it should say greater than zero, 0 is not permitted.
protected String hostSettingValue(InetAddress[] allAddresses) { if (Arrays.stream(allAddresses).anyMatch(InetAddress::isSiteLocalAddress)) { return "0.0.0.0"; } else { return "_local_"; } }	is the if condition required? i think filling in 0.0.0.0 all the time is ok?
private static void setRandomIncludeDefaults(GetIndexRequest request, Map<String, String> expectedParams) { if (randomBoolean()) { request.includeDefaults(true); expectedParams.put("include_defaults", Boolean.TRUE.toString()); } }	can we also randomize the value, just to test what we do when the value is explicitly set to false? it is the default value but i would prefer to test that codepath too. same for the other new methods.
public void cancel() { assert responseHandlers.contains(requestId) == false : "cancel must be called after the requestId [" + requestId + "] has been removed from clientHandlers"; FutureUtils.cancel(future); }	one nit: timeouthandler -> timeout handler so that we're speaking plain english instead of code in these messages
public String getFollowerIndex() { return followerIndex; }	i wonder if we should use waitforactiveshards where we would provide the option to specify an activeshardcount (defaulting to none here), similar as for the createindexrequest request. this would bring the put follower index api closer to the create index / open index / freeze index / resize index api. the way to implement this then would be to use the standard wait_for_completion on the restore request if shard count = 1, and if higher, wait using the activeshardsobserver mechanism that previously existed in transportputfollowaction.
private void createFollowerIndex( final IndexMetaData leaderIndexMetaData, final PutFollowAction.Request request, final ActionListener<PutFollowAction.Response> listener) { if (leaderIndexMetaData == null) { listener.onFailure(new IllegalArgumentException("leader index [" + request.getLeaderIndex() + "] does not exist")); return; } // soft deletes are enabled by default on indices created on 7.0.0 or later if (leaderIndexMetaData.getSettings().getAsBoolean(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), IndexMetaData.SETTING_INDEX_VERSION_CREATED.get(leaderIndexMetaData.getSettings()).onOrAfter(Version.V_7_0_0)) == false) { listener.onFailure( new IllegalArgumentException("leader index [" + request.getLeaderIndex() + "] does not have soft deletes enabled")); return; } String remoteCluster = request.getRemoteCluster(); Client client = CcrLicenseChecker.wrapClient(this.client, threadPool.getThreadContext().getHeaders()); final ActionListener<PutFollowAction.Response> followingListener; final ActionListener<PutFollowAction.Response> restoreInitiatedListener; if (request.getWaitForRestore()) { followingListener = listener; restoreInitiatedListener = NOOP_LISTENER; } else { followingListener = NOOP_LISTENER; restoreInitiatedListener = listener; } ActionListener<RestoreSnapshotResponse> restoreCompleteHandler = new ActionListener<RestoreSnapshotResponse>() { @Override public void onResponse(RestoreSnapshotResponse restoreSnapshotResponse) { RestoreInfo restoreInfo = restoreSnapshotResponse.getRestoreInfo(); if (restoreInfo == null) { // If restoreInfo is null then it is possible there was a master failure during the // restore. followingListener.onResponse(new PutFollowAction.Response(true, false, false)); } else if (restoreInfo.failedShards() == 0) { initiateFollowing(client, request, followingListener); } else { // Has failed shards followingListener.onResponse(new PutFollowAction.Response(true, false, false)); } } @Override public void onFailure(Exception e) { followingListener.onFailure(e); } }; Settings.Builder settingsBuilder = Settings.builder() .put(IndexMetaData.SETTING_INDEX_PROVIDED_NAME, request.getFollowRequest().getFollowerIndex()) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true); String leaderClusterRepoName = CcrRepository.NAME_PREFIX + remoteCluster; RestoreSnapshotRequest restoreRequest = new RestoreSnapshotRequest(leaderClusterRepoName, CcrRepository.LATEST) .indices(request.getLeaderIndex()).indicesOptions(request.indicesOptions()).renamePattern("^(.*)$") .renameReplacement(request.getFollowRequest().getFollowerIndex()).masterNodeTimeout(request.masterNodeTimeout()) .indexSettings(settingsBuilder); initiateRestore(restoreRequest, restoreInitiatedListener, restoreCompleteHandler); }	i found the flow a bit hard to read with these two listeners. i've tried refactoring it as follows. let me know whichever you prefer. diff --git a/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/transportputfollowaction.java b/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/transportputfollowaction.java index 65c310859a9..1772054192e 100644 --- a/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/transportputfollowaction.java +++ b/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/transportputfollowaction.java @@ -42,8 +42,6 @@ import java.util.objects; public final class transportputfollowaction extends transportmasternodeaction<putfollowaction.request, putfollowaction.response> { - private static final actionlistener<putfollowaction.response> noop_listener = actionlistener.wrap(() -> {}); - private final client client; private final restoreservice restoreservice; private final ccrlicensechecker ccrlicensechecker; @@ -124,61 +122,18 @@ public final class transportputfollowaction return; } - string remotecluster = request.getremotecluster(); - - client client = ccrlicensechecker.wrapclient(this.client, threadpool.getthreadcontext().getheaders()); - - final actionlistener<putfollowaction.response> followinglistener; - final actionlistener<putfollowaction.response> restoreinitiatedlistener; - if (request.getwaitforrestore()) { - followinglistener = listener; - restoreinitiatedlistener = noop_listener; - } else { - followinglistener = noop_listener; - restoreinitiatedlistener = listener; - } - - actionlistener<restoresnapshotresponse> restorecompletehandler = new actionlistener<restoresnapshotresponse>() { - @override - public void onresponse(restoresnapshotresponse restoresnapshotresponse) { - restoreinfo restoreinfo = restoresnapshotresponse.getrestoreinfo(); - - if (restoreinfo == null) { - // if restoreinfo is null then it is possible there was a master failure during the - // restore. - followinglistener.onresponse(new putfollowaction.response(true, false, false)); - } else if (restoreinfo.failedshards() == 0) { - initiatefollowing(client, request, followinglistener); - } else { - // has failed shards - followinglistener.onresponse(new putfollowaction.response(true, false, false)); - } - } - - @override - public void onfailure(exception e) { - followinglistener.onfailure(e); - } - }; - - settings.builder settingsbuilder = settings.builder() + final settings.builder settingsbuilder = settings.builder() .put(indexmetadata.setting_index_provided_name, request.getfollowrequest().getfollowerindex()) .put(ccrsettings.ccr_following_index_setting.getkey(), true); - string leaderclusterreponame = ccrrepository.name_prefix + remotecluster; - restoresnapshotrequest restorerequest = new restoresnapshotrequest(leaderclusterreponame, ccrrepository.latest) + final string leaderclusterreponame = ccrrepository.name_prefix + request.getremotecluster(); + final restoresnapshotrequest restorerequest = new restoresnapshotrequest(leaderclusterreponame, ccrrepository.latest) .indices(request.getleaderindex()).indicesoptions(request.indicesoptions()).renamepattern("^(.*)$") .renamereplacement(request.getfollowrequest().getfollowerindex()).masternodetimeout(request.masternodetimeout()) .indexsettings(settingsbuilder); - initiaterestore(restorerequest, restoreinitiatedlistener, restorecompletehandler); - } - - private void initiaterestore(restoresnapshotrequest restorerequest, actionlistener<putfollowaction.response> restoreinitiatedlistener, - actionlistener<restoresnapshotresponse> restorecompletelistener) { threadpool.executor(threadpool.names.snapshot).execute(new abstractrunnable() { @override public void onfailure(exception e) { - restoreinitiatedlistener.onfailure(e); - restorecompletelistener.onfailure(e); + listener.onfailure(e); } @override @@ -186,14 +141,12 @@ public final class transportputfollowaction restoreservice.restoresnapshot(restorerequest, new actionlistener<restoreservice.restorecompletionresponse>() { @override public void onresponse(restoreservice.restorecompletionresponse response) { - restoreinitiatedlistener.onresponse(new putfollowaction.response(true, false, false)); - restoreclusterstatelistener.createandregisterlistener(clusterservice, response, restorecompletelistener); + afterrestorestarted(request, listener, response); } @override public void onfailure(exception e) { - restoreinitiatedlistener.onfailure(e); - restorecompletelistener.onfailure(e); + listener.onfailure(e); } }); } @@ -201,10 +154,57 @@ public final class transportputfollowaction } + private void afterrestorestarted( + final putfollowaction.request request, + final actionlistener<putfollowaction.response> originallistener, + final restoreservice.restorecompletionresponse response) { + + final actionlistener<putfollowaction.response> listener; + if (request.getwaitforrestore() == false) { + originallistener.onresponse(new putfollowaction.response(true, false, false)); + listener = new actionlistener<putfollowaction.response>() { + @override + public void onresponse(putfollowaction.response response) { + logger.debug("put follow completed with {}", response); // todo: implement tostring() on response object + } + + @override + public void onfailure(exception e) { + logger.debug("put follow failed after restore initiated", e); + } + }; + } else { + listener = originallistener; + } + + restoreclusterstatelistener.createandregisterlistener(clusterservice, response, + new actionlistener<restoresnapshotresponse>() { + @override + public void onresponse(restoresnapshotresponse restoresnapshotresponse) { + final restoreinfo restoreinfo = restoresnapshotresponse.getrestoreinfo(); + + if (restoreinfo == null) { + // if restoreinfo is null then it is possible there was a master failure during the restore. + listener.onresponse(new putfollowaction.response(true, false, false)); + } else if (restoreinfo.failedshards() == 0) { + initiatefollowing(request, listener); + } else { + // has failed shards + listener.onresponse(new putfollowaction.response(true, false, false)); + } + } + + @override + public void onfailure(exception e) { + listener.onfailure(e); + } + }); + } + private void initiatefollowing( - final client client, final putfollowaction.request request, final actionlistener<putfollowaction.response> listener) { + client client = ccrlicensechecker.wrapclient(this.client, threadpool.getthreadcontext().getheaders()); client.execute(resumefollowaction.instance, request.getfollowrequest(), actionlistener.wrap( r -> listener.onresponse(new putfollowaction.response(true, true, r.isacknowledged())), listener::onfailure
public void testFollowIndexWithConcurrentMappingChanges() throws Exception { final int numberOfPrimaryShards = randomIntBetween(1, 3); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, between(0, 1), singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate("index1").setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderYellow("index1"); final int firstBatchNumDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as first batch", firstBatchNumDocs); for (int i = 0; i < firstBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } AtomicBoolean isRunning = new AtomicBoolean(true); // Concurrently index new docs with mapping changes Thread thread = new Thread(() -> { int docID = 10000; char[] chars = "abcdeghijklmnopqrstuvwxyz".toCharArray(); for (char c : chars) { if (isRunning.get() == false) { break; } final String source; long valueToPutInDoc = randomLongBetween(0, 50000); if (randomBoolean()) { source = String.format(Locale.ROOT, "{\\\\"%c\\\\":%d}", c, valueToPutInDoc); } else { source = String.format(Locale.ROOT, "{\\\\"%c\\\\":\\\\"%d\\\\"}", c, valueToPutInDoc); } for (int i = 1; i < 10; i++) { if (isRunning.get() == false) { break; } leaderClient().prepareIndex("index1", "doc", Long.toString(docID++)).setSource(source, XContentType.JSON).get(); if (rarely()) { leaderClient().admin().indices().prepareFlush("index1").setForce(true).get(); } } leaderClient().admin().indices().prepareFlush("index1").setForce(true).setWaitIfOngoing(true).get(); } }); thread.start(); final PutFollowAction.Request followRequest = putFollow("index1", "index2", false); followerClient().execute(PutFollowAction.INSTANCE, followRequest).get(); ensureFollowerGreen("index2"); for (int i = 0; i < firstBatchNumDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } final int secondBatchNumDocs = randomIntBetween(2, 64); logger.info("Indexing [{}] docs as second batch", secondBatchNumDocs); for (int i = firstBatchNumDocs; i < firstBatchNumDocs + secondBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } for (int i = firstBatchNumDocs; i < firstBatchNumDocs + secondBatchNumDocs; i++) { assertBusy(assertExpectedDocumentRunnable(i)); } isRunning.set(false); thread.join(); }	check that the index is indeed created at this point (i.e. when put follow returns)?
public static Query doTranslate(Range r, TranslatorHandler handler) { Expression val = r.value(); Query query = null; Holder<Object> lower = new Holder<>(valueOf(r.lower())); Holder<Object> upper = new Holder<>(valueOf(r.upper())); Holder<String> format = new Holder<>(handler.dateFormat(val)); // for a date constant comparison, we need to use a format for the date, to make sure that the format is the same // no matter the timezone provided by the user if (format.get() == null) { DateFormatter formatter = null; if (lower.get() instanceof ZonedDateTime || upper.get() instanceof ZonedDateTime) { formatter = DateFormatter.forPattern(DATE_FORMAT); } else if (lower.get() instanceof OffsetTime || upper.get() instanceof OffsetTime) { formatter = DateFormatter.forPattern(TIME_FORMAT); } if (formatter != null) { // RangeQueryBuilder accepts an Object as its parameter, but it will call .toString() on the ZonedDateTime // instance which can have a slightly different format depending on the ZoneId used to create the ZonedDateTime // Since RangeQueryBuilder can handle date as String as well, we'll format it as String and provide the format. if (lower.get() instanceof ZonedDateTime || lower.get() instanceof OffsetTime) { lower.set(formatter.format((TemporalAccessor) lower.get())); } if (upper.get() instanceof ZonedDateTime || upper.get() instanceof OffsetTime) { upper.set(formatter.format((TemporalAccessor) upper.get())); } format.set(formatter.pattern()); } } query = handler.wrapFunctionQuery(r, val, new RangeQuery(r.source(), handler.nameOf(val), lower.get(), r.includeLower(), upper.get(), r.includeUpper(), format.get())); return query; } } public static class InComparisons extends ExpressionTranslator<In> { protected Query asQuery(In in, TranslatorHandler handler) { return doTranslate(in, handler); } public static Query doTranslate(In in, TranslatorHandler handler) { Query q; if (in.value() instanceof FieldAttribute) { // equality should always be against an exact match (which is important for strings) FieldAttribute fa = (FieldAttribute) in.value(); List<Expression> list = in.list(); // TODO: this needs to be handled inside the optimizer list.removeIf(e -> DataTypes.isNull(e.dataType())); DataType dt = list.get(0).dataType(); Set<Object> set = new LinkedHashSet<>(CollectionUtils.mapSize(list.size())); for (Expression e : list) { set.add(valueOf(e)); } q = new TermsQuery(in.source(), fa.exactAttribute().name(), set); } else { q = new ScriptQuery(in.source(), in.asScript()); } return handler.wrapFunctionQuery(in, in.value(), q); } } public static class Scalars extends ExpressionTranslator<ScalarFunction> { @Override protected Query asQuery(ScalarFunction f, TranslatorHandler handler) { return doTranslate(f, handler); } public static Query doTranslate(ScalarFunction f, TranslatorHandler handler) { return handler.wrapFunctionQuery(f, f, new ScriptQuery(f.source(), f.asScript())); } } public static Object valueOf(Expression e) { if (e.foldable()) { return e.fold(); } throw new QlIllegalArgumentException("Cannot determine value for {}", e); } public static Query or(Source source, Query left, Query right) { return boolQuery(source, left, right, false); } public static Query and(Source source, Query left, Query right) { return boolQuery(source, left, right, true); } private static Query boolQuery(Source source, Query left, Query right, boolean isAnd) { Check.isTrue(left != null || right != null, "Both expressions are null"); if (left == null) { return right; } if (right == null) { return left; }	behavior slightly changed. see comment in sql https://github.com/elastic/elasticsearch/pull/53244/files#r394547453
*/ public PrimaryContext primaryContext() { verifyPrimary(); assert shardRouting.relocating(); assert !shardRouting.isRelocationTarget(); return new PrimaryContext(getEngine().seqNoService().seqNoPrimaryContext()); }	please add a message
private StoreFilesMetaData listStoreMetaData(ShardId shardId) throws IOException { logger.trace("listing store meta data for {}", shardId); long startTimeNS = System.nanoTime(); boolean exists = false; try { IndexService indexService = indicesService.indexService(shardId.getIndex()); if (indexService != null) { IndexShard indexShard = indexService.getShardOrNull(shardId.id()); if (indexShard != null) { final Store store = indexShard.store(); store.incRef(); try { exists = true; return new StoreFilesMetaData(shardId, store.getMetadataOrEmpty()); } finally { store.decRef(); } } } // try and see if we an list unallocated IndexMetaData metaData = clusterService.state().metaData().index(shardId.getIndex()); if (metaData == null) { // we may send this requests while processing the cluster state that recovered the index // sometimes the request comes in before the local node processed that cluster state // in such cases we can load it from disk metaData = IndexMetaData.FORMAT.loadLatestState(logger, nodeEnv.indexPaths(shardId.getIndex())); } if (metaData == null) { logger.trace("{} node doesn't have meta data for the requests index, responding with empty", shardId); return new StoreFilesMetaData(shardId, Store.MetadataSnapshot.EMPTY); } final IndexSettings indexSettings = indexService != null ? indexService.getIndexSettings() : new IndexSettings(metaData, settings); final ShardPath shardPath = ShardPath.loadShardPath(logger, nodeEnv, shardId, indexSettings); if (shardPath == null) { return new StoreFilesMetaData(shardId, Store.MetadataSnapshot.EMPTY); } // note that this may fail if it can't get access to the shard lock. Since we check above there is an active shard, this means: // 1) a shard is being constructed, which means the master will not use a copy of this replcia return new StoreFilesMetaData(shardId, Store.readMetadataSnapshot(shardPath.resolveIndex(), shardId, nodeEnv::shardLock, logger)); } finally { TimeValue took = new TimeValue(System.nanoTime() - startTimeNS, TimeUnit.NANOSECONDS); if (exists) { logger.debug("{} loaded store meta data (took [{}])", shardId, took); } else { logger.trace("{} didn't find any store meta data to load (took [{}])", shardId, took); } } }	did you mean to add a 2) here?
@Override protected void masterOperation(final RolloverRequest rolloverRequest, final ClusterState state, final ActionListener<RolloverResponse> listener) { final MetaData metaData = state.metaData(); validate(metaData, rolloverRequest); final AliasOrIndex.Alias alias = (AliasOrIndex.Alias) metaData.getAliasAndIndexLookup().get(rolloverRequest.getAlias()); final IndexMetaData indexMetaData = alias.getWriteIndex(); final boolean explicitWriteIndex = Boolean.TRUE.equals(indexMetaData.getAliases().get(alias.getAliasName()).writeIndex()); final String sourceProvidedName = indexMetaData.getSettings().get(IndexMetaData.SETTING_INDEX_PROVIDED_NAME, indexMetaData.getIndex().getName()); final String sourceIndexName = indexMetaData.getIndex().getName(); final String unresolvedName = (rolloverRequest.getNewIndexName() != null) ? rolloverRequest.getNewIndexName() : generateRolloverIndexName(sourceProvidedName, indexNameExpressionResolver); final String rolloverIndexName = indexNameExpressionResolver.resolveDateMathExpression(unresolvedName); MetaDataCreateIndexService.validateIndexName(rolloverIndexName, state); // will fail if the index already exists checkNoDuplicatedAliasInIndexTemplate(metaData, rolloverIndexName, rolloverRequest.getAlias()); client.admin().indices().prepareStats(rolloverRequest.getAlias()).clear().setDocs(true).execute( new ActionListener<IndicesStatsResponse>() { @Override public void onResponse(IndicesStatsResponse statsResponse) { final Map<String, Boolean> conditionResults = evaluateConditions(rolloverRequest.getConditions().values(), metaData.index(sourceIndexName), statsResponse); if (rolloverRequest.isDryRun()) { listener.onResponse( new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, true, false, false, false)); return; } List<Condition<?>> metConditions = rolloverRequest.getConditions().values().stream() .filter(condition -> conditionResults.get(condition.toString())).collect(Collectors.toList()); if (conditionResults.size() == 0 || metConditions.size() > 0) { CreateIndexClusterStateUpdateRequest updateRequest = prepareCreateIndexRequest(unresolvedName, rolloverIndexName, rolloverRequest); createIndexService.createIndex(updateRequest, ActionListener.wrap(createIndexClusterStateUpdateResponse -> { final IndicesAliasesClusterStateUpdateRequest aliasesUpdateRequest; if (explicitWriteIndex) { aliasesUpdateRequest = prepareRolloverAliasesWriteIndexUpdateRequest(sourceIndexName, rolloverIndexName, rolloverRequest); } else { aliasesUpdateRequest = prepareRolloverAliasesUpdateRequest(sourceIndexName, rolloverIndexName, rolloverRequest); } indexAliasesService.indicesAliases(aliasesUpdateRequest, ActionListener.wrap(aliasClusterStateUpdateResponse -> { if (aliasClusterStateUpdateResponse.isAcknowledged()) { clusterService.submitStateUpdateTask("update_rollover_info", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { RolloverInfo rolloverInfo = new RolloverInfo(rolloverRequest.getAlias(), metConditions, threadPool.absoluteTimeInMillis()); return ClusterState.builder(currentState) .metaData(MetaData.builder(currentState.metaData()) .put(IndexMetaData.builder(currentState.metaData().index(sourceIndexName)) .putRolloverInfo(rolloverInfo))).build(); } @Override public void onFailure(String source, Exception e) { listener.onFailure(e); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { activeShardsObserver.waitForActiveShards(new String[]{rolloverIndexName}, rolloverRequest.getCreateIndexRequest().waitForActiveShards(), rolloverRequest.masterNodeTimeout(), isShardsAcknowledged -> listener.onResponse(new RolloverResponse( sourceIndexName, rolloverIndexName, conditionResults, false, true, true, isShardsAcknowledged)), listener::onFailure); } }); } else { listener.onResponse(new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, false, true, false, false)); } }, listener::onFailure)); }, listener::onFailure)); } else { // conditions not met listener.onResponse( new RolloverResponse(sourceIndexName, rolloverIndexName, conditionResults, false, false, false, false) ); } } @Override public void onFailure(Exception e) { listener.onFailure(e); } } ); }	i will have to check, but i'm not sure this is the same thing. will stats for shards of all the matched read-aliases be returned in this case? for example, if the alias points to multiple indices, then we may be checking stats of indices that are of no concern to us.
public void testIndicesDeletedFromRepository() throws Exception { Client client = client(); logger.info("--> creating repository"); final String repoName = "test-repo"; assertAcked(client.admin().cluster().preparePutRepository(repoName) .setType("fs").setSettings(Settings.builder().put("location", randomRepoPath()))); createIndex("test-idx-1", "test-idx-2", "test-idx-3"); ensureGreen(); logger.info("--> indexing some data"); for (int i = 0; i < 20; i++) { index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i); index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i); index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i); } refresh(); logger.info("--> take a snapshot"); CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot(repoName, "test-snap").setWaitForCompletion(true).get(); assertEquals(createSnapshotResponse.getSnapshotInfo().successfulShards(), createSnapshotResponse.getSnapshotInfo().totalShards()); logger.info("--> indexing more data"); for (int i = 20; i < 40; i++) { index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i); index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i); index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i); } logger.info("--> take another snapshot with only 2 of the 3 indices"); createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot(repoName, "test-snap2") .setWaitForCompletion(true) .setIndices("test-idx-1", "test-idx-2") .get(); assertEquals(createSnapshotResponse.getSnapshotInfo().successfulShards(), createSnapshotResponse.getSnapshotInfo().totalShards()); logger.info("--> verify index directories exist in the repository"); RepositoriesService repositoriesSvc = internalCluster().getInstance(RepositoriesService.class, internalCluster().getMasterName()); @SuppressWarnings("unchecked") BlobStoreRepository repository = (BlobStoreRepository) repositoriesSvc.repository(repoName); BlobContainer indicesBlobContainer = repository.blobStore().blobContainer(repository.basePath().add("indices")); RepositoryData repositoryData = repository.getRepositoryData(); for (IndexId indexId : repositoryData.getIndices().values()) { assertTrue(indicesBlobContainer.blobExists(indexId.getId())); } logger.info("--> delete first snapshot"); assertAcked(client().admin().cluster().prepareDeleteSnapshot(repoName, "test-snap").get()); logger.info("--> verify index folder deleted from blob container"); for (IndexId indexId : repositoryData.getIndices().values()) { if (indexId.getName().equals("test-idx-3")) { assertFalse(indicesBlobContainer.blobExists(indexId.getId())); // deleted index } else { assertTrue(indicesBlobContainer.blobExists(indexId.getId())); } } }	this seems wrong to me: this class is abstract and contains tests that should work with any type of repository. we should not force the repository type to fs but rely on the createtestrepository(reponame) method instead like it is done in the other test.
static AggProvider fromXContent(XContentParser parser, boolean lenient) throws IOException { Map<String, Object> aggs = parser.mapOrdered(); boolean rewroteAggs = false; if (lenient) { rewroteAggs = rewriteDateHistogramInterval(aggs, false); } AggregatorFactories.Builder parsedAggs = null; Exception exception = null; try { if (aggs.isEmpty()) { throw new Exception("aggs cannot be empty"); } parsedAggs = XContentObjectTransformer.aggregatorTransformer(parser.getXContentRegistry()).fromMap(aggs); } catch (Exception ex) { if (ex.getCause() instanceof IllegalArgumentException) { ex = (Exception) ex.getCause(); } exception = ex; if (lenient) { logger.warn(Messages.DATAFEED_CONFIG_AGG_BAD_FORMAT, ex); } else { throw ExceptionsHelper.badRequestException(Messages.DATAFEED_CONFIG_AGG_BAD_FORMAT, ex); } } return new AggProvider(aggs, parsedAggs, exception, rewroteAggs); }	i think the changes in this file should be reverted, because 8.x needs to be able to coexist with 7.16.
@Override protected void assertFromXContent(InternalMatrixStats expected, ParsedAggregation parsedAggregation) throws IOException { assertTrue(parsedAggregation instanceof ParsedMatrixStats); ParsedMatrixStats actual = (ParsedMatrixStats) parsedAggregation; for (String field : fields) { assertEquals(expected.getFieldCount(field), actual.getFieldCount(field)); assertEquals(expected.getMean(field), actual.getMean(field), 0.0); assertEquals(expected.getVariance(field), actual.getVariance(field), 0.0); assertEquals(expected.getSkewness(field), actual.getSkewness(field), 0.0); assertEquals(expected.getKurtosis(field), actual.getKurtosis(field), 0.0); for (String other : fields) { assertEquals(expected.getCovariance(field, other), actual.getCovariance(field, other), 0.0); assertEquals(expected.getCorrelation(field, other), actual.getCorrelation(field, other), 0.0); } } final String unknownField = randomAlphaOfLength(3); final String fieldX = randomFrom(unknownField, randomAlphaOfLength(3)); final String fieldY = randomAlphaOfLength(3); for (MatrixStats matrix : Arrays.asList(actual)) { // getFieldCount returns 0 for unknown fields assertEquals(0.0, matrix.getFieldCount(unknownField), 0.0); if (hasMatrixStatsResults) { expectThrows(IllegalArgumentException.class, () -> matrix.getMean(unknownField)); expectThrows(IllegalArgumentException.class, () -> matrix.getVariance(unknownField)); expectThrows(IllegalArgumentException.class, () -> matrix.getSkewness(unknownField)); expectThrows(IllegalArgumentException.class, () -> matrix.getKurtosis(unknownField)); expectThrows(IllegalArgumentException.class, () -> matrix.getCovariance(fieldX, fieldY)); expectThrows(IllegalArgumentException.class, () -> matrix.getCorrelation(fieldX, fieldY)); // getCorrelation returns 1.0 on same fields assertEquals(1.0, matrix.getCorrelation(unknownField, unknownField), 0.0); } else { assertEquals(Double.NaN, matrix.getMean(unknownField), 0.0); assertEquals(Double.NaN, matrix.getVariance(unknownField), 0.0); assertEquals(Double.NaN, matrix.getSkewness(unknownField), 0.0); assertEquals(Double.NaN, matrix.getKurtosis(unknownField), 0.0); assertEquals(Double.NaN, matrix.getCovariance(fieldX, fieldY), 0.0); assertEquals(Double.NaN, matrix.getCorrelation(fieldX, fieldY), 0.0); // getCorrelation returns NaN on same fields assertEquals(Double.NaN, matrix.getCorrelation(unknownField, unknownField), 0.0); } } }	i'm not sure i understand what this is doing?
protected void execute(Terminal terminal, OptionSet options, Environment env) throws Exception { String arg = arguments.value(options); execute(terminal, arg, env); }	i'd just say "remove the plugin named {@code plugin named}".
* @param env the environment for the local node, which may be used for the local settings and path * * @throws UserException if plugin name is null * @throws UserException if plugin directory does not exist * @throws UserException if plugin bin directory does not exist */ void execute(Terminal terminal, String pluginName, Environment env) throws Exception { if (pluginName == null) { throw new UserException(ExitCodes.USAGE, "plugin name is required"); } terminal.println("-> Removing " + Strings.coalesceToEmpty(pluginName) + "..."); final Path pluginDir = env.pluginsFile().resolve(pluginName); if (Files.exists(pluginDir) == false) { throw new UserException( ExitCodes.CONFIG, "plugin " + pluginName + " not found; run 'elasticsearch-plugin list' to get list of installed plugins"); } final List<Path> pluginPaths = new ArrayList<>(); final Path pluginBinDir = env.binFile().resolve(pluginName); if (Files.exists(pluginBinDir)) { if (Files.isDirectory(pluginBinDir) == false) { throw new UserException(ExitCodes.IO_ERROR, "Bin dir for " + pluginName + " is not a directory"); } pluginPaths.add(pluginBinDir); terminal.println(VERBOSE, "Removing: " + pluginBinDir); } terminal.println(VERBOSE, "Removing: " + pluginDir); final Path tmpPluginDir = env.pluginsFile().resolve(".removing-" + pluginName); try { Files.move(pluginDir, tmpPluginDir, StandardCopyOption.ATOMIC_MOVE); } catch (final AtomicMoveNotSupportedException e) { // this can happen on a union filesystem when a plugin is not installed on the top layer; we fall back to a non-atomic move Files.move(pluginDir, tmpPluginDir); } pluginPaths.add(tmpPluginDir); IOUtils.rm(pluginPaths.toArray(new Path[pluginPaths.size()])); // we preserve the config files in case the user is upgrading the plugin, but we print // a message so the user knows in case they want to remove manually final Path pluginConfigDir = env.configFile().resolve(pluginName); if (Files.exists(pluginConfigDir)) { terminal.println( "-> Preserving plugin config files [" + pluginConfigDir + "] in case of upgrade, delete manually if not needed"); } }	these are not the only exceptions that can be thrown. there a bunch of situations in which an ioexception can be thrown. it's probably worth changing the signature to reflect this too.
public void testCompression() throws Exception { Logger logger = Loggers.getLogger(UUIDTests.class); // Low number so that the test runs quickly, but the results are more interesting with larger numbers // of indexed documents assertThat(testCompression(100000, 10000, 3, logger), Matchers.lessThan(14d)); // ~12 in practice assertThat(testCompression(100000, 1000, 3, logger), Matchers.lessThan(15d)); // ~13 in practice assertThat(testCompression(100000, 100, 3, logger), Matchers.lessThan(21d)); // ~20 in practice }	is there any reason why these values are not stored in a variable?
@Override public TimeValue getPendingTaskTimeInQueue() { return updateTasksExecutor.getPendingTaskTimeInQueue(); }	i think this should be implemented by scanning all tasks in the queue since the first one isn't really the max value which is what we care about. also it would be great if it can share the task normalization code in pendingtasks
public void testSlowGcLogging() { final ESLogger logger = mock(ESLogger.class); when(logger.isWarnEnabled()).thenReturn(true); when(logger.isInfoEnabled()).thenReturn(true); when(logger.isDebugEnabled()).thenReturn(true); final JvmGcMonitorService.JvmMonitor.Threshold threshold = randomFrom(JvmGcMonitorService.JvmMonitor.Threshold.values()); final String name = randomAsciiOfLength(16); final long seq = randomIntBetween(1, 1 << 30); final int elapsedValue = randomIntBetween(1, 1 << 10); final TimeValue elapsed = TimeValue.timeValueMillis(randomIntBetween(1, 1 << 10)); final long totalCollectionCount = randomIntBetween(1, 16); final long currentCollectionCount = randomIntBetween(1, 16); final TimeValue totalCollectionTime = TimeValue.timeValueMillis(randomIntBetween(1, elapsedValue)); final TimeValue currentColletionTime = TimeValue.timeValueMillis(randomIntBetween(1, elapsedValue)); final ByteSizeValue lastHeapUsed = new ByteSizeValue(randomIntBetween(1, 1 << 10)); final ByteSizeValue currentHeapUsed = new ByteSizeValue(randomIntBetween(1, 1 << 10)); final ByteSizeValue maxHeapUsed = new ByteSizeValue(Math.max(lastHeapUsed.bytes(), currentHeapUsed.bytes()) + 1 << 10); JvmGcMonitorService.logSlowGc( logger, threshold, name, seq, elapsed, totalCollectionCount, currentCollectionCount, totalCollectionTime, currentColletionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.WARN) { verify(logger).isWarnEnabled(); verify(logger).warn( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.INFO) { verify(logger).isInfoEnabled(); verify(logger).info( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.DEBUG) { verify(logger).isDebugEnabled(); verify(logger).debug( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else { fail("unexpected threshold [" + threshold + "]"); } verifyNoMoreInteractions(logger); }	for this block i'd probably use a switch. the warning you'd get when you declare a new value for threshold is handy.
public void testSlowGcLogging() { final ESLogger logger = mock(ESLogger.class); when(logger.isWarnEnabled()).thenReturn(true); when(logger.isInfoEnabled()).thenReturn(true); when(logger.isDebugEnabled()).thenReturn(true); final JvmGcMonitorService.JvmMonitor.Threshold threshold = randomFrom(JvmGcMonitorService.JvmMonitor.Threshold.values()); final String name = randomAsciiOfLength(16); final long seq = randomIntBetween(1, 1 << 30); final int elapsedValue = randomIntBetween(1, 1 << 10); final TimeValue elapsed = TimeValue.timeValueMillis(randomIntBetween(1, 1 << 10)); final long totalCollectionCount = randomIntBetween(1, 16); final long currentCollectionCount = randomIntBetween(1, 16); final TimeValue totalCollectionTime = TimeValue.timeValueMillis(randomIntBetween(1, elapsedValue)); final TimeValue currentColletionTime = TimeValue.timeValueMillis(randomIntBetween(1, elapsedValue)); final ByteSizeValue lastHeapUsed = new ByteSizeValue(randomIntBetween(1, 1 << 10)); final ByteSizeValue currentHeapUsed = new ByteSizeValue(randomIntBetween(1, 1 << 10)); final ByteSizeValue maxHeapUsed = new ByteSizeValue(Math.max(lastHeapUsed.bytes(), currentHeapUsed.bytes()) + 1 << 10); JvmGcMonitorService.logSlowGc( logger, threshold, name, seq, elapsed, totalCollectionCount, currentCollectionCount, totalCollectionTime, currentColletionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.WARN) { verify(logger).isWarnEnabled(); verify(logger).warn( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.INFO) { verify(logger).isInfoEnabled(); verify(logger).info( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.DEBUG) { verify(logger).isDebugEnabled(); verify(logger).debug( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else { fail("unexpected threshold [" + threshold + "]"); } verifyNoMoreInteractions(logger); }	you just mocked this behavior so it doesn't make much sense to verify it.
public void testSlowGcLogging() { final ESLogger logger = mock(ESLogger.class); when(logger.isWarnEnabled()).thenReturn(true); when(logger.isInfoEnabled()).thenReturn(true); when(logger.isDebugEnabled()).thenReturn(true); final JvmGcMonitorService.JvmMonitor.Threshold threshold = randomFrom(JvmGcMonitorService.JvmMonitor.Threshold.values()); final String name = randomAsciiOfLength(16); final long seq = randomIntBetween(1, 1 << 30); final int elapsedValue = randomIntBetween(1, 1 << 10); final TimeValue elapsed = TimeValue.timeValueMillis(randomIntBetween(1, 1 << 10)); final long totalCollectionCount = randomIntBetween(1, 16); final long currentCollectionCount = randomIntBetween(1, 16); final TimeValue totalCollectionTime = TimeValue.timeValueMillis(randomIntBetween(1, elapsedValue)); final TimeValue currentColletionTime = TimeValue.timeValueMillis(randomIntBetween(1, elapsedValue)); final ByteSizeValue lastHeapUsed = new ByteSizeValue(randomIntBetween(1, 1 << 10)); final ByteSizeValue currentHeapUsed = new ByteSizeValue(randomIntBetween(1, 1 << 10)); final ByteSizeValue maxHeapUsed = new ByteSizeValue(Math.max(lastHeapUsed.bytes(), currentHeapUsed.bytes()) + 1 << 10); JvmGcMonitorService.logSlowGc( logger, threshold, name, seq, elapsed, totalCollectionCount, currentCollectionCount, totalCollectionTime, currentColletionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.WARN) { verify(logger).isWarnEnabled(); verify(logger).warn( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.INFO) { verify(logger).isInfoEnabled(); verify(logger).info( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.DEBUG) { verify(logger).isDebugEnabled(); verify(logger).debug( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else { fail("unexpected threshold [" + threshold + "]"); } verifyNoMoreInteractions(logger); }	does it make sense to use the constant in this case? i know you have strong opinions on when constants should be referenced vs when we should just use the string but in this case i think the constant is good?
public void testSlowGcLogging() { final ESLogger logger = mock(ESLogger.class); when(logger.isWarnEnabled()).thenReturn(true); when(logger.isInfoEnabled()).thenReturn(true); when(logger.isDebugEnabled()).thenReturn(true); final JvmGcMonitorService.JvmMonitor.Threshold threshold = randomFrom(JvmGcMonitorService.JvmMonitor.Threshold.values()); final String name = randomAsciiOfLength(16); final long seq = randomIntBetween(1, 1 << 30); final int elapsedValue = randomIntBetween(1, 1 << 10); final TimeValue elapsed = TimeValue.timeValueMillis(randomIntBetween(1, 1 << 10)); final long totalCollectionCount = randomIntBetween(1, 16); final long currentCollectionCount = randomIntBetween(1, 16); final TimeValue totalCollectionTime = TimeValue.timeValueMillis(randomIntBetween(1, elapsedValue)); final TimeValue currentColletionTime = TimeValue.timeValueMillis(randomIntBetween(1, elapsedValue)); final ByteSizeValue lastHeapUsed = new ByteSizeValue(randomIntBetween(1, 1 << 10)); final ByteSizeValue currentHeapUsed = new ByteSizeValue(randomIntBetween(1, 1 << 10)); final ByteSizeValue maxHeapUsed = new ByteSizeValue(Math.max(lastHeapUsed.bytes(), currentHeapUsed.bytes()) + 1 << 10); JvmGcMonitorService.logSlowGc( logger, threshold, name, seq, elapsed, totalCollectionCount, currentCollectionCount, totalCollectionTime, currentColletionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.WARN) { verify(logger).isWarnEnabled(); verify(logger).warn( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.INFO) { verify(logger).isInfoEnabled(); verify(logger).info( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else if (threshold == JvmGcMonitorService.JvmMonitor.Threshold.DEBUG) { verify(logger).isDebugEnabled(); verify(logger).debug( "[gc][{}][{}][{}] duration [{}], collections [{}]/[{}], total [{}]/[{}], memory [{}]->[{}]/[{}], all_pools {}", name, seq, totalCollectionCount, currentColletionTime, currentCollectionCount, elapsed, currentColletionTime, totalCollectionTime, lastHeapUsed, currentHeapUsed, maxHeapUsed, "asdf"); } else { fail("unexpected threshold [" + threshold + "]"); } verifyNoMoreInteractions(logger); }	the mockito folks [advise](http://site.mockito.org/mockito/docs/current/org/mockito/mockito.html#finding_redundant_invocations) against using this method too much and this isn't one of those times where i'd generally feel compelled to use it.
private ShardIterator preferenceActiveShardIterator(IndexShardRoutingTable indexShard, String localNodeId, DiscoveryNodes nodes, @Nullable String preference) { if (preference == null || preference.isEmpty()) { String[] awarenessAttributes = awarenessAllocationDecider.awarenessAttributes(); if (awarenessAttributes.length == 0) { return indexShard.activeInitializingShardsRandomIt(); } else { return indexShard.preferAttributesActiveInitializingShardsIt(awarenessAttributes, nodes); } } if (preference.charAt(0) == '_') { Preference preferenceType = Preference.parse(preference); if (preferenceType == Preference.SHARDS) { // starts with _shards, so execute on specific ones int index = preference.indexOf(';'); String shards; if (index == -1) { shards = preference.substring(Preference.SHARDS.type().length() + 1); } else { shards = preference.substring(Preference.SHARDS.type().length() + 1, index); } String[] ids = Strings.splitStringByCommaToArray(shards); boolean found = false; for (String id : ids) { if (Integer.parseInt(id) == indexShard.shardId().id()) { found = true; break; } } if (!found) { return null; } // no more preference if (index == -1 || index == preference.length() - 1) { String[] awarenessAttributes = awarenessAllocationDecider.awarenessAttributes(); if (awarenessAttributes.length == 0) { return indexShard.activeInitializingShardsRandomIt(); } else { return indexShard.preferAttributesActiveInitializingShardsIt(awarenessAttributes, nodes); } } else { // update the preference and continue preference = preference.substring(index + 1); } } preferenceType = Preference.parse(preference); if (preferenceType == Preference.PREFER_NODE) { return indexShard.preferNodeActiveInitializingShardsIt(preference.substring(Preference.PREFER_NODE.type().length() + 1)); } if (preferenceType == Preference.LOCAL) { return indexShard.preferNodeActiveInitializingShardsIt(localNodeId); } if (preferenceType == Preference.PRIMARY) { return indexShard.primaryActiveInitializingShardIt(); } if (preferenceType == Preference.PRIMARY_FIRST) { return indexShard.primaryFirstActiveInitializingShardsIt(); } if (preferenceType == Preference.ONLY_LOCAL) { return indexShard.onlyNodeActiveInitializingShardsIt(localNodeId); } if (preferenceType == Preference.ONLY_NODE) { String nodeId = preference.substring(Preference.ONLY_NODE.type().length() + 1); ensureNodeIdExists(nodes, nodeId); return indexShard.onlyNodeActiveInitializingShardsIt(nodeId); } } // if not, then use it as the index String[] awarenessAttributes = awarenessAllocationDecider.awarenessAttributes(); if (awarenessAttributes.length == 0) { return indexShard.activeInitializingShardsIt(DjbHashFunction.DJB_HASH(preference)); } else { return indexShard.preferAttributesActiveInitializingShardsIt(awarenessAttributes, nodes, DjbHashFunction.DJB_HASH(preference)); } }	i think now that we have an enum we should just use a switch / case statement here?
private static Query translate(InsensitiveBinaryComparison bc, FieldAttribute field, TranslatorHandler handler) { Source source = bc.source(); String name = field.exactAttribute().name(); Object value = valueOf(bc.right()); if (bc instanceof InsensitiveEquals || bc instanceof InsensitiveNotEquals) { Query query = new TermQuery(source, name, value, true); if (bc instanceof InsensitiveNotEquals) { query = new NotQuery(source, query); } return query; } throw new QlIllegalArgumentException("Don't know how to translate binary comparison [{}] in [{}]", bc.right().nodeString(), bc); } } public static class BinaryLogic extends ExpressionTranslator<org.elasticsearch.xpack.ql.expression.predicate.logical.BinaryLogic> { @Override protected Query asQuery(org.elasticsearch.xpack.ql.expression.predicate.logical.BinaryLogic e, TranslatorHandler handler) { if (e instanceof And) { return and(e.source(), toQuery(e.left(), handler), toQuery(e.right(), handler)); } if (e instanceof Or) { return or(e.source(), toQuery(e.left(), handler), toQuery(e.right(), handler)); } return null; } } public static Object valueOf(Expression e) { if (e.foldable()) { return e.fold(); } throw new QlIllegalArgumentException("Cannot determine value for {}", e); } public static class Scalars extends ExpressionTranslator<ScalarFunction> { @Override protected Query asQuery(ScalarFunction f, TranslatorHandler handler) { return doTranslate(f, handler); } public static Query doTranslate(ScalarFunction f, TranslatorHandler handler) { if (f instanceof CIDRMatch) { CIDRMatch cm = (CIDRMatch) f; if (cm.input() instanceof FieldAttribute && Expressions.foldable(cm.addresses())) { String targetFieldName = handler.nameOf(((FieldAttribute) cm.input()).exactAttribute()); Set<Object> set = new LinkedHashSet<>(CollectionUtils.mapSize(cm.addresses().size())); for (Expression e : cm.addresses()) { set.add(valueOf(e)); } return new TermsQuery(f.source(), targetFieldName, set); } } return handler.wrapFunctionQuery(f, f, (field) -> new ScriptQuery(f.source(), f.asScript())); } } public static class CaseInsensitiveScalarFunctions extends ExpressionTranslator<CaseInsensitiveScalarFunction> { @Override protected Query asQuery(CaseInsensitiveScalarFunction f, TranslatorHandler handler) { return doTranslate(f, handler); } public static Query doTranslate(CaseInsensitiveScalarFunction f, TranslatorHandler handler) { Query q = ExpressionTranslators.Scalars.doKnownTranslate(f, handler); if (q != null) { return q; } if (f instanceof BinaryComparisonCaseInsensitiveFunction) { BinaryComparisonCaseInsensitiveFunction bccif = (BinaryComparisonCaseInsensitiveFunction) f; String targetFieldName = null; String wildcardQuery = null; Expression field = bccif.left(); Expression constant = bccif.right(); if (field instanceof FieldAttribute && constant.foldable()) { targetFieldName = handler.nameOf(((FieldAttribute) field).exactAttribute()); String string = (String) constant.fold(); if (f instanceof StringContains) { wildcardQuery = WILDCARD + string + WILDCARD; } else if (f instanceof EndsWith) { wildcardQuery = WILDCARD + string; } } q = wildcardQuery != null ? new WildcardQuery(f.source(), targetFieldName, wildcardQuery, f.isCaseInsensitive()) : null; } return q; }	i tend not to agree with the additional fieldattribute parameter. it bothers me that the methods receive a redundant parameter. in this case bc.left() is the same the field parametere. [here](https://github.com/elastic/elasticsearch/pull/76424/files#diff-385222df7bc48aa29c7282fd51bd3f3de071de85974214e19a44670750637182r238) field is the same as isnull.field(). this adds confusion: what's the point of the field if that same value can be extracted from the first parameter. just to save on if (x.field() instanceof fieldattribute)? i'd like to know what others think about this. @bpintea @costin
public static Query doTranslate(In in, TranslatorHandler handler) { return handler.wrapFunctionQuery(in, in.value(), (field) -> translate(in, field, handler)); }	i guess the [removed tests](https://github.com/elastic/elasticsearch/pull/76424/files#r688298282) were "enabled" by this - now removed - conversion, but i wonder why was it needed before: just for datetime's? i guess the rest of the types are conveniently converted by es.
@Override public void describeTo(Description description) { description.appendText(failureMsg); } } public static class ContainingInAnyOrderMatcher extends TypeSafeDiagnosingMatcher<Translog.Snapshot> { private final Collection<Translog.Operation> expectedOps; static List<Translog.Operation> drainAll(Translog.Snapshot snapshot) throws IOException { final List<Translog.Operation> actualOps = new ArrayList<>(); Translog.Operation op; while ((op = snapshot.next()) != null) { actualOps.add(op); } return actualOps; } public ContainingInAnyOrderMatcher(Collection<Translog.Operation> expectedOps) { this.expectedOps = expectedOps; } @Override protected boolean matchesSafely(Translog.Snapshot snapshot, Description mismatchDescription) { try { List<Translog.Operation> actualOps = drainAll(snapshot); List<Translog.Operation> notFound = expectedOps.stream() .filter(o -> actualOps.contains(o) == false) .collect(Collectors.toList()); if (notFound.isEmpty() == false) { mismatchDescription .appendText(" Operations not found").appendValueList("[", ", ", "]", notFound); } List<Translog.Operation> notExpected = actualOps.stream() .filter(o -> expectedOps.contains(o) == false) .collect(Collectors.toList()); if (notExpected.isEmpty() == false) { mismatchDescription .appendText(" Operations not expected ").appendValueList("[", ", ", "]", notExpected); } return notFound.isEmpty() && notExpected.isEmpty(); } catch (IOException ex) { throw new ElasticsearchException("failed to read snapshot content", ex); } } @Override public void describeTo(Description description) { }	should this be empty?
public static SearchHit createTestItem(XContentType xContentType, boolean withOptionalInnerHits, boolean withShardTarget) { int internalId = randomInt(); String uid = randomAlphaOfLength(10); Text type = new Text(randomAlphaOfLengthBetween(5, 10)); NestedIdentity nestedIdentity = null; if (randomBoolean()) { nestedIdentity = NestedIdentityTests.createTestItem(randomIntBetween(0, 2)); } Map<String, DocumentField> fields = new HashMap<>(); if (randomBoolean()) { fields = GetResultTests.randomDocumentFields(xContentType).v2(); } SearchHit hit = new SearchHit(internalId, uid, type, nestedIdentity, fields); if (frequently()) { if (rarely()) { hit.score(Float.NaN); } else { hit.score(randomFloat()); } } if (frequently()) { hit.sourceRef(RandomObjects.randomSource(random(), xContentType)); } if (randomBoolean()) { hit.version(randomLong()); } if (randomBoolean()) { hit.sortValues(SearchSortValuesTests.createTestItem()); } if (randomBoolean()) { int size = randomIntBetween(0, 5); Map<String, HighlightField> highlightFields = new HashMap<>(size); for (int i = 0; i < size; i++) { HighlightField testItem = HighlightFieldTests.createTestItem(); highlightFields.put(testItem.getName(), testItem); } hit.highlightFields(highlightFields); } if (randomBoolean()) { int size = randomIntBetween(0, 5); String[] matchedQueries = new String[size]; for (int i = 0; i < size; i++) { matchedQueries[i] = randomAlphaOfLength(5); } hit.matchedQueries(matchedQueries); } if (randomBoolean()) { hit.explanation(createExplanation(randomIntBetween(0, 5))); } if (withOptionalInnerHits) { int innerHitsSize = randomIntBetween(0, 3); if (innerHitsSize > 0) { Map<String, SearchHits> innerHits = new HashMap<>(innerHitsSize); for (int i = 0; i < innerHitsSize; i++) { innerHits.put(randomAlphaOfLength(5), SearchHitsTests.createTestItem(xContentType, false, withShardTarget)); } hit.setInnerHits(innerHits); } } if (withShardTarget && randomBoolean()) { String index = randomAlphaOfLengthBetween(5, 10); String clusterAlias = randomBoolean() ? null : randomAlphaOfLengthBetween(5, 10); hit.shard(new SearchShardTarget(randomAlphaOfLengthBetween(5, 10), new ShardId(new Index(index, randomAlphaOfLengthBetween(5, 10)), randomInt()), clusterAlias, OriginalIndices.NONE)); } return hit; }	nice, i was going to ask you about this previous change too.
static Properties loadSmtpProperties(Settings settings) { Settings.Builder builder = Settings.builder().put(DEFAULT_SMTP_TIMEOUT_SETTINGS).put(settings); replaceTimeValue(builder, "connection_timeout", "connectiontimeout"); replaceTimeValue(builder, "write_timeout", "writetimeout"); replaceTimeValue(builder, "timeout", "timeout"); replace(builder, "local_address", "localaddress"); replace(builder, "local_port", "localport"); replace(builder, "send_partial", "sendpartial"); replace(builder, "wait_on_quit", "quitwait"); settings = builder.build(); Properties props = new Properties(); for (String key : settings.keySet()) { // Secure strings can not be retreived out of a settings object and should be handled differently if (key.startsWith("secure_") == false) { props.setProperty(SMTP_SETTINGS_PREFIX + key, settings.get(key)); } } return props; }	how about using settings.filter(s -> s.startswith("_secure") == false).keyset()?
public static Boolean parseBoolean(String value, Boolean defaultValue) { if (hasText(value)) { return parseBoolean(value); } return defaultValue; } /** * Returns {@code false} if text is in "false", "0", "off", "no"; else, {@code true}. * * @deprecated Only kept to provide automatic upgrades for pre 6.0 indices. Use {@link #parseBoolean(String, Boolean)}	<code> is still valid but {@code} is the "standard" way to do it in javadoc and i was converting the <tt>s on the same line. i certainly wouldn't go out of my way to replace <code> with {@code} but this one was right there....
private int getNumClientNodes() { ClusterScope annotation = getAnnotation(this.getClass(), ClusterScope.class); return annotation == null ? InternalTestCluster.DEFAULT_NUM_CLIENT_NODES : annotation.numClientNodes(); } /** * This method is used to obtain settings for the {@code Nth}	to be pedantic it should be nth.
public License sign(License licenseSpec) throws IOException { XContentBuilder contentBuilder = XContentFactory.contentBuilder(XContentType.JSON); final Map<String, String> licenseSpecViewMode = Collections.singletonMap(License.LICENSE_SPEC_VIEW_MODE, "true"); licenseSpec.toXContent(contentBuilder, new ToXContent.MapParams(licenseSpecViewMode)); final byte[] signedContent; final boolean legacy = licenseSpec.version() < License.VERSION_CRYPTO_ALGORITHMS; try { final Signature rsa = Signature.getInstance("SHA512withRSA"); PrivateKey decryptedPrivateKey = CryptUtils.readEncryptedPrivateKey(Files.readAllBytes(privateKeyPath)); rsa.initSign(decryptedPrivateKey); final BytesRefIterator iterator = BytesReference.bytes(contentBuilder).iterator(); BytesRef ref; while((ref = iterator.next()) != null) { rsa.update(ref.bytes, ref.offset, ref.length); } signedContent = rsa.sign(); } catch (InvalidKeyException | IOException | NoSuchAlgorithmException | SignatureException e) { throw new IllegalStateException(e); } final byte[] magic = new byte[MAGIC_LENGTH]; SecureRandom random = new SecureRandom(); random.nextBytes(magic); final byte[] publicKeyBytes = Files.readAllBytes(publicKeyPath); PublicKey publicKey = CryptUtils.readPublicKey(publicKeyBytes); final byte[] pubKeyFingerprint = legacy ? Base64.getEncoder().encode(CryptUtils.writeEncryptedPublicKey(publicKey)) : getPublicKeyFingerprint(publicKeyBytes); byte[] bytes = new byte[4 + 4 + MAGIC_LENGTH + 4 + pubKeyFingerprint.length + 4 + signedContent.length]; ByteBuffer byteBuffer = ByteBuffer.wrap(bytes); byteBuffer.putInt(licenseSpec.version()) .putInt(magic.length) .put(magic) .putInt(pubKeyFingerprint.length) .put(pubKeyFingerprint) .putInt(signedContent.length) .put(signedContent); return License.builder() .fromLicenseSpec(licenseSpec, Base64.getEncoder().encodeToString(bytes)) .build(); }	can we use prev4 instead of legacy here?
private static byte[] decrypt(byte[] encryptedData, char[] passPhrase) { try { final Cipher cipher = getDecryptionCipher(deriveSecretKey(passPhrase)); return cipher.doFinal(encryptedData); } catch (IllegalBlockSizeException | BadPaddingException e) { throw new IllegalStateException(e); } }	style wise, i think getv3key reads easier
public static boolean verifyLicense(final License license) { final byte[] publicKeyBytes; try (InputStream is = LicenseVerifier.class.getResourceAsStream("/public.key")) { ByteArrayOutputStream out = new ByteArrayOutputStream(); Streams.copy(is, out); publicKeyBytes = out.toByteArray(); } catch (IOException ex) { throw new IllegalStateException(ex); } return verifyLicense(license, publicKeyBytes); }	nit: unneeded extra line
@Override protected ValuesSourceAggregatorFactory innerBuild(QueryShardContext queryShardContext, ValuesSourceConfig config, AggregatorFactory parent, Builder subFactoriesBuilder) throws IOException { PercentilesConfig percentilesConfig; if (method.equals(PercentilesMethod.TDIGEST)) { percentilesConfig = new PercentilesConfig.TDigestConfig(compression); } else if (method.equals(PercentilesMethod.HDR)) { percentilesConfig = new PercentilesConfig.HdrHistoConfig(numberOfSignificantValueDigits); } else { throw new IllegalStateException("Illegal method [" + method + "]"); } return new PercentileRanksAggregatorFactory(name, config, values, percentilesConfig, keyed, queryShardContext, parent, subFactoriesBuilder, metaData); }	we've got this same cascading if in two places now, and cascading if or switch blocks on enums are kind of dodgy to begin with. i propose a method on percentilesmethod that takes two args, compression and number of significant digits, ignores whichever doesn't apply to that method, and returns the config. this has the added benefit that we don't need the unreachable throw block here (unreachable since it would have already thrown trying to parse the bad method name)
public void testMultiBlockUpload() throws Exception { final BlobStoreRepository repo = getRepository(); // The configured threshold for this test suite is 1mb final int blobSize = ByteSizeUnit.MB.toIntBytes(2); final Executor executor = repo.threadPool().generic(); PlainActionFuture<Void> future = PlainActionFuture.newFuture(); executor.execute(ActionRunnable.run(future, () -> { final BlobContainer blobContainer = repo.blobStore().blobContainer(repo.basePath().add("large_write")); blobContainer.writeBlob(UUIDs.base64UUID(), new ByteArrayInputStream(randomByteArrayOfLength(blobSize)), blobSize, false); blobContainer.delete(); })); future.get(); }	nit: just inline repo.threadpool().generic() here, you're only using executor once
@Override public void applyClusterState(ClusterChangedEvent event) { try { if (event.localNodeMaster()) { // We don't remove old master when master flips anymore. So, we need to check for change in master final SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE); if (snapshotsInProgress != null) { if (removedNodesCleanupNeeded(snapshotsInProgress, event.nodesDelta().removedNodes())) { processSnapshotsOnRemovedNodes(); } if (event.routingTableChanged() && waitingShardsStartedOrUnassigned(snapshotsInProgress, event)) { processStartedShards(); } // Cleanup all snapshots that have no more work left: // 1. Completed snapshots // 2. Snapshots in state INIT that the previous master failed to start // 3. Snapshots in any other state that have all their shard tasks completed snapshotsInProgress.entries().stream().filter( entry -> entry.state().completed() || entry.state() == State.INIT && initializingSnapshots.contains(entry.snapshot()) == false || entry.state() != State.INIT && completed(entry.shards().values()) ).forEach(this::endSnapshot); } if (event.previousState().nodes().isLocalNodeElectedMaster() == false) { finalizeSnapshotDeletionFromPreviousMaster(event); } } } catch (Exception e) { logger.warn("Failed to update snapshot state ", e); } }	simplified the logic here a little to avoid the endless null check nestings that make it really hard to figure out what line of conditions led to something being executed.
@Override public void applyClusterState(ClusterChangedEvent event) { try { if (event.localNodeMaster()) { // We don't remove old master when master flips anymore. So, we need to check for change in master final SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE); if (snapshotsInProgress != null) { if (removedNodesCleanupNeeded(snapshotsInProgress, event.nodesDelta().removedNodes())) { processSnapshotsOnRemovedNodes(); } if (event.routingTableChanged() && waitingShardsStartedOrUnassigned(snapshotsInProgress, event)) { processStartedShards(); } // Cleanup all snapshots that have no more work left: // 1. Completed snapshots // 2. Snapshots in state INIT that the previous master failed to start // 3. Snapshots in any other state that have all their shard tasks completed snapshotsInProgress.entries().stream().filter( entry -> entry.state().completed() || entry.state() == State.INIT && initializingSnapshots.contains(entry.snapshot()) == false || entry.state() != State.INIT && completed(entry.shards().values()) ).forEach(this::endSnapshot); } if (event.previousState().nodes().isLocalNodeElectedMaster() == false) { finalizeSnapshotDeletionFromPreviousMaster(event); } } } catch (Exception e) { logger.warn("Failed to update snapshot state ", e); } }	all snapshot ending happens here now. 1. this should prevent any future stale snapshots that have all their shards completed. 2. makes it much easier to reason about master failovers.
private void finalizeSnapshotDeletionFromPreviousMaster(ClusterChangedEvent event) { SnapshotDeletionsInProgress deletionsInProgress = event.state().custom(SnapshotDeletionsInProgress.TYPE); if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) { assert deletionsInProgress.getEntries().size() == 1 : "only one in-progress deletion allowed per cluster"; SnapshotDeletionsInProgress.Entry entry = deletionsInProgress.getEntries().get(0); deleteSnapshotFromRepository(entry.getSnapshot(), null, entry.getRepositoryStateId()); } }	this is now automatically covered by the applyclusterstate hook
private void processSnapshotsOnRemovedNodes() { clusterService.submitStateUpdateTask("update snapshot state after node removal", new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { DiscoveryNodes nodes = currentState.nodes(); SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE); if (snapshots == null) { return currentState; } boolean changed = false; ArrayList<SnapshotsInProgress.Entry> entries = new ArrayList<>(); for (final SnapshotsInProgress.Entry snapshot : snapshots.entries()) { SnapshotsInProgress.Entry updatedSnapshot = snapshot; if (snapshot.state() == State.STARTED || snapshot.state() == State.ABORTED) { ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder(); boolean snapshotChanged = false; for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards()) { ShardSnapshotStatus shardStatus = shardEntry.value; if (!shardStatus.state().completed() && shardStatus.nodeId() != null) { if (nodes.nodeExists(shardStatus.nodeId())) { shards.put(shardEntry.key, shardEntry.value); } else { // TODO: Restart snapshot on another node? snapshotChanged = true; logger.warn("failing snapshot of shard [{}] on closed node [{}]", shardEntry.key, shardStatus.nodeId()); shards.put(shardEntry.key, new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "node shutdown")); } } } if (snapshotChanged) { changed = true; ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardsMap = shards.build(); if (!snapshot.state().completed() && completed(shardsMap.values())) { updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, State.SUCCESS, shardsMap); } else { updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, snapshot.state(), shardsMap); } } entries.add(updatedSnapshot); } else if (snapshot.state() == State.INIT && initializingSnapshots.contains(snapshot.snapshot()) == false) { changed = true; // Mark the snapshot as aborted as it failed to start from the previous master updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, State.ABORTED, snapshot.shards()); entries.add(updatedSnapshot); // Clean up the snapshot that failed to start from the old master deleteSnapshot(snapshot.snapshot(), new ActionListener<Void>() { @Override public void onResponse(Void aVoid) { logger.debug("cleaned up abandoned snapshot {} in INIT state", snapshot.snapshot()); } @Override public void onFailure(Exception e) { logger.warn("failed to clean up abandoned snapshot {} in INIT state", snapshot.snapshot()); } }, updatedSnapshot.getRepositoryStateId(), false); } } if (changed) { return ClusterState.builder(currentState) .putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(unmodifiableList(entries))).build(); } return currentState; } @Override public void onFailure(String source, Exception e) { logger.warn("failed to update snapshot state after node removal"); } }); }	this should be more stable and easier to reason about. it's weird that we check newmaster on some version of the state and then "later" on run this code based on whether or not we failed over earlier.
private static boolean removedNodesCleanupNeeded(SnapshotsInProgress snapshotsInProgress, List<DiscoveryNode> removedNodes) { // If at least one shard was running on a removed node - we need to fail it return removedNodes.isEmpty() == false && snapshotsInProgress.entries().stream().flatMap(snapshot -> StreamSupport.stream(((Iterable<ShardSnapshotStatus>) () -> snapshot.shards().valuesIt()).spliterator(), false) .filter(s -> s.state().completed() == false).map(ShardSnapshotStatus::nodeId)) .anyMatch(removedNodes.stream().map(DiscoveryNode::getId).collect(Collectors.toSet())::contains); }	this could be way simplified now too since we're already cleaning up snapshots in success and init state at the top level of applyclusterstate.
private PublishWithJoinResponse handleIncomingPublishRequest(BytesTransportRequest request) throws IOException { final Compressor compressor = CompressorFactory.compressor(request.bytes()); StreamInput in = request.bytes().streamInput(); try { if (compressor != null) { in = compressor.streamInput(in); } in = new NamedWriteableAwareStreamInput(in, namedWriteableRegistry); in.setVersion(request.version()); // If true we received full cluster state - otherwise diffs if (in.readBoolean()) { final ClusterState incomingState; try { incomingState = ClusterState.readFrom(in, transportService.getLocalNode()); // Close early to release resources used by the de-compression as early as possible in.close(); } catch (Exception e){ logger.warn("unexpected error while deserializing an incoming cluster state", e); throw e; } fullClusterStateReceivedCount.incrementAndGet(); logger.debug("received full cluster state version [{}] with size [{}]", incomingState.version(), request.bytes().length()); final PublishWithJoinResponse response = acceptState(incomingState); lastSeenClusterState.set(incomingState); return response; } else { final ClusterState lastSeen = lastSeenClusterState.get(); if (lastSeen == null) { logger.debug("received diff for but don't have any local cluster state - requesting full state"); incompatibleClusterStateDiffReceivedCount.incrementAndGet(); throw new IncompatibleClusterStateVersionException("have no local cluster state"); } else { ClusterState incomingState; try { Diff<ClusterState> diff = ClusterState.readDiffFrom(in, lastSeen.nodes().getLocalNode()); // Close early to release resources used by the de-compression as early as possible in.close(); incomingState = diff.apply(lastSeen); // might throw IncompatibleClusterStateVersionException } catch (IncompatibleClusterStateVersionException e) { incompatibleClusterStateDiffReceivedCount.incrementAndGet(); throw e; } catch (Exception e){ logger.warn("unexpected error while deserializing an incoming cluster state", e); throw e; } compatibleClusterStateDiffReceivedCount.incrementAndGet(); logger.debug("received diff cluster state version [{}] with uuid [{}], diff size [{}]", incomingState.version(), incomingState.stateUUID(), request.bytes().length()); final PublishWithJoinResponse response = acceptState(incomingState); lastSeenClusterState.compareAndSet(lastSeen, incomingState); return response; } } } finally { IOUtils.close(in); } }	we do have compressed mapping sources in the cluster state that itself comes to us in compressed form so not closing here would actually lead to not having nested decompression streams so i added the early close.
private PublishWithJoinResponse handleIncomingPublishRequest(BytesTransportRequest request) throws IOException { final Compressor compressor = CompressorFactory.compressor(request.bytes()); StreamInput in = request.bytes().streamInput(); try { if (compressor != null) { in = compressor.streamInput(in); } in = new NamedWriteableAwareStreamInput(in, namedWriteableRegistry); in.setVersion(request.version()); // If true we received full cluster state - otherwise diffs if (in.readBoolean()) { final ClusterState incomingState; try { incomingState = ClusterState.readFrom(in, transportService.getLocalNode()); // Close early to release resources used by the de-compression as early as possible in.close(); } catch (Exception e){ logger.warn("unexpected error while deserializing an incoming cluster state", e); throw e; } fullClusterStateReceivedCount.incrementAndGet(); logger.debug("received full cluster state version [{}] with size [{}]", incomingState.version(), request.bytes().length()); final PublishWithJoinResponse response = acceptState(incomingState); lastSeenClusterState.set(incomingState); return response; } else { final ClusterState lastSeen = lastSeenClusterState.get(); if (lastSeen == null) { logger.debug("received diff for but don't have any local cluster state - requesting full state"); incompatibleClusterStateDiffReceivedCount.incrementAndGet(); throw new IncompatibleClusterStateVersionException("have no local cluster state"); } else { ClusterState incomingState; try { Diff<ClusterState> diff = ClusterState.readDiffFrom(in, lastSeen.nodes().getLocalNode()); // Close early to release resources used by the de-compression as early as possible in.close(); incomingState = diff.apply(lastSeen); // might throw IncompatibleClusterStateVersionException } catch (IncompatibleClusterStateVersionException e) { incompatibleClusterStateDiffReceivedCount.incrementAndGet(); throw e; } catch (Exception e){ logger.warn("unexpected error while deserializing an incoming cluster state", e); throw e; } compatibleClusterStateDiffReceivedCount.incrementAndGet(); logger.debug("received diff cluster state version [{}] with uuid [{}], diff size [{}]", incomingState.version(), incomingState.stateUUID(), request.bytes().length()); final PublishWithJoinResponse response = acceptState(incomingState); lastSeenClusterState.compareAndSet(lastSeen, incomingState); return response; } } } finally { IOUtils.close(in); } }	is it that important to close before applying the diff here? otherwise you could use try with resources
StreamInput streamInput(StreamInput in) throws IOException; /** * Creates a new stream output that compresses the contents and writes to the provided stream * output. Closing the returned {@link StreamOutput}	maybe name this method something like unsafestreamoutput or threadlocalstreamoutput?
public StreamInput streamInput(StreamInput in) throws IOException { final byte[] headerBytes = new byte[HEADER.length]; int len = 0; while (len < headerBytes.length) { final int read = in.read(headerBytes, len, headerBytes.length - len); if (read == -1) { break; } len += read; } if (len != HEADER.length || Arrays.equals(headerBytes, HEADER) == false) { throw new IllegalArgumentException("Input stream is not compressed with DEFLATE!"); } final ReleasableReference<Inflater> current = inflaterForStreamRef.get(); final Releasable releasable; final Inflater inflater; if (current.inUse) { // Nested de-compression streams should not happen but we still handle them safely by using a fresh Inflater inflater = new Inflater(true); releasable = inflater::end; } else { inflater = current.get(); releasable = current; } InputStream decompressedIn = new InflaterInputStream(in, inflater, BUFFER_SIZE) { @Override public void close() throws IOException { try { super.close(); } finally { releasable.close(); } } }; return new InputStreamStreamInput(new BufferedInputStream(decompressedIn, BUFFER_SIZE)); }	no need for this kind of extra atomic boolean+check, the bufferedinputstream will only close once.
@Override public void close() { if (Assertions.ENABLED) { assert user == Thread.currentThread() : "Opened on [" + user.getName() + "] but closed on [" + Thread.currentThread().getName() + "]"; user = null; } assert inUse; inUse = false; releasable.close(); } } @Override public StreamInput streamInput(StreamInput in) throws IOException { final byte[] headerBytes = new byte[HEADER.length]; int len = 0; while (len < headerBytes.length) { final int read = in.read(headerBytes, len, headerBytes.length - len); if (read == -1) { break; } len += read; } if (len != HEADER.length || Arrays.equals(headerBytes, HEADER) == false) { throw new IllegalArgumentException("Input stream is not compressed with DEFLATE!"); } final ReleasableReference<Inflater> current = inflaterForStreamRef.get(); final Releasable releasable; final Inflater inflater; if (current.inUse) { // Nested de-compression streams should not happen but we still handle them safely by using a fresh Inflater inflater = new Inflater(true); releasable = inflater::end; } else { inflater = current.get(); releasable = current; } InputStream decompressedIn = new InflaterInputStream(in, inflater, BUFFER_SIZE) { @Override public void close() throws IOException { try { super.close(); } finally { releasable.close(); } } }; return new InputStreamStreamInput(new BufferedInputStream(decompressedIn, BUFFER_SIZE)); } @Override public StreamOutput streamOutput(OutputStream out) throws IOException { out.write(HEADER); final ReleasableReference<Deflater> current = deflaterForStreamRef.get(); final Releasable releasable; final Deflater deflater; if (current.inUse) { // Nested compression streams should not happen but we still handle them safely by using a fresh Deflater deflater = new Deflater(LEVEL, true); releasable = deflater::end; } else { deflater = current.get(); releasable = current; } final boolean syncFlush = true; DeflaterOutputStream deflaterOutputStream = new DeflaterOutputStream(out, deflater, BUFFER_SIZE, syncFlush) { @Override public void close() throws IOException { try { super.close(); } finally { releasable.close(); } } }; return new OutputStreamStreamOutput(new BufferedOutputStream(deflaterOutputStream, BUFFER_SIZE)); } private static final ThreadLocal<BytesStreamOutput> baos = ThreadLocal.withInitial(BytesStreamOutput::new); // Reusable Inflater reference. Note: This is a separate instance from the one used for the decompressing stream wrapper because we // want to be able to deal with decompressing bytes references that were read from a decompressing stream. private static final ThreadLocal<Inflater> inflaterRef = ThreadLocal.withInitial(() -> new Inflater(true)); @Override public BytesReference uncompress(BytesReference bytesReference) throws IOException { final BytesStreamOutput buffer = baos.get(); final Inflater inflater = inflaterRef.get(); inflater.reset(); try (InflaterOutputStream ios = new InflaterOutputStream(buffer, inflater)) { bytesReference.slice(HEADER.length, bytesReference.length() - HEADER.length).writeTo(ios); } final BytesReference res = buffer.copyBytes(); buffer.reset(); return res; } // Reusable Deflater reference. Note: This is a separate instance from the one used for the compressing stream wrapper because we // want to be able to deal with compressing bytes references to a decompressing stream. private static final ThreadLocal<Deflater> deflaterRef = ThreadLocal.withInitial(() -> new Deflater(LEVEL, true)); @Override public BytesReference compress(BytesReference bytesReference) throws IOException { final BytesStreamOutput buffer = baos.get(); final Deflater deflater = deflaterRef.get(); deflater.reset(); buffer.write(HEADER); try (DeflaterOutputStream dos = new DeflaterOutputStream(buffer, deflater, true)) { bytesReference.writeTo(dos); }	no need for this kind of extra atomic boolean+check, the bufferedoutputstream is a filter output stream that will only close once.
private RepositoryData repositoryDataFromCachedEntry(Tuple<Long, BytesReference> cacheEntry) throws IOException { try (StreamInput input = CompressorFactory.COMPRESSOR.streamInput(cacheEntry.v2().streamInput())) { return RepositoryData.snapshotsFromXContent( XContentType.JSON.xContent().createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, input), cacheEntry.v1(), false); } }	this was just a bug where we failed to close the stream and that became visible with the changes in this pr.
public static MultiSearchTemplateRequest parseRequest(RestRequest restRequest, boolean allowExplicitIndex) throws IOException { MultiSearchTemplateRequest multiRequest = new MultiSearchTemplateRequest(); if (restRequest.hasParam("max_concurrent_searches")) { multiRequest.maxConcurrentSearchRequests(restRequest.paramAsInt("max_concurrent_searches", 0)); } RestMultiSearchAction.parseMultiLineRequest(restRequest, multiRequest.indicesOptions(), allowExplicitIndex, (searchRequest, bytes) -> { if (searchRequest.types().length > 0) { deprecationLogger.deprecatedAndMaybeLog("msearch_template_with_types", RestMultiSearchAction.TYPES_DEPRECATION_MESSAGE); } SearchTemplateRequest searchTemplateRequest = SearchTemplateRequest.fromXContent(bytes); if (searchTemplateRequest.getScript() != null) { searchTemplateRequest.setRequest(searchRequest); multiRequest.add(searchTemplateRequest); } else { throw new IllegalArgumentException("Malformed search template"); } }); return multiRequest; }	do we want to use restmultisearchtemplateaction here and the same with logger on line 44?
@Override protected void masterOperation(Task task, DeleteEnrichPolicyAction.Request request, ClusterState state, ActionListener<AcknowledgedResponse> listener) throws Exception { try { enrichPolicyLocks.lockPolicy(request.getName()); } catch (EsRejectedExecutionException e) { listener.onFailure(new ElasticsearchStatusException("Could not delete policy [{}] because it is currently executing", RestStatus.CONFLICT, request.getName())); return; } EnrichStore.deletePolicy(request.getName(), clusterService, e -> { enrichPolicyLocks.releasePolicy(request.getName()); if (e == null) { listener.onResponse(new AcknowledgedResponse(true)); } else { listener.onFailure(e); } }); }	can we just throw the esrejectedexecutionexception that lockpolicy (...) threw?
@Override public Iterator<Document> iterator() { return documents.iterator(); } } public abstract Iterable<Document> nonRootIterator(); public abstract DocumentMapperParser docMapperParser(); /** * Return a new context that will be within a copy-to operation. */ public final ParseContext createCopyToContext() { return new FilterParseContext(this) { @Override public boolean isWithinCopyTo() { return true; } }; } public boolean isWithinCopyTo() { return false; } /** * Return a new context that will be within multi-fields. */ public final ParseContext createMultiFieldContext() { return new FilterParseContext(this) { @Override public boolean isWithinMultiFields() { return true; } }; } /** * Return a new context that will be used within a nested document. */ public final ParseContext createNestedContext(String fullPath) { final Document doc = new Document(fullPath, doc()); addDoc(doc); return switchDoc(doc); } /** * Return a new context that has the provided document as the current document. */ public final ParseContext switchDoc(final Document document) { return new FilterParseContext(this) { @Override public Document doc() { return document; } }; } /** * Return a new context that will have the provided path. */ public final ParseContext overridePath(final ContentPath path) { return new FilterParseContext(this) { @Override public ContentPath path() { return path; } }; } public boolean isWithinMultiFields() { return false; } @Nullable public abstract Settings indexSettings(); public abstract SourceToParse sourceToParse(); public abstract ContentPath path(); public abstract XContentParser parser(); public abstract Document rootDoc(); public abstract Document doc(); protected abstract void addDoc(Document doc); public abstract RootObjectMapper root(); public abstract DocumentMapper docMapper(); public abstract MapperService mapperService(); public abstract Field version(); public abstract void version(Field version); public abstract SeqNoFieldMapper.SequenceIDFields seqID(); public abstract void seqID(SeqNoFieldMapper.SequenceIDFields seqID); /** * Return a new context that will have the external value set. */ public final ParseContext createExternalValueContext(final Object externalValue) { return new FilterParseContext(this) { @Override public boolean externalValueSet() { return true; } @Override public Object externalValue() { return externalValue; } }; } public boolean externalValueSet() { return false; } public Object externalValue() { throw new IllegalStateException("External value is not set"); } /** * Try to parse an externalValue if any * @param clazz Expected class for external value * @return null if no external value has been set or the value */ public final <T> T parseExternalValue(Class<T> clazz) { if (!externalValueSet() || externalValue() == null) { return null; } if (!clazz.isInstance(externalValue())) { throw new IllegalArgumentException("illegal external value class [" + externalValue().getClass().getName() + "]. Should be " + clazz.getName()); }	i think it's confusing that the name says iterator but the return value is an iterable? maybe nonrootdocuments?
@Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { boolean needsOriginalSource = false; BytesReference source = context.sourceToParse().source(); if (enabled && fieldType().stored() && source != null) { // Percolate and tv APIs may not set the source and that is ok, because these APIs will not index any data if (filter != null) { // we don't update the context source if we filter, we want to keep it as is... Tuple<XContentType, Map<String, Object>> mapTuple = XContentHelper.convertToMap(source, true, context.sourceToParse().getXContentType()); Map<String, Object> filteredSource = filter.apply(mapTuple.v2()); BytesStreamOutput bStream = new BytesStreamOutput(); XContentType contentType = mapTuple.v1(); XContentBuilder builder = XContentFactory.contentBuilder(contentType, bStream).map(filteredSource); builder.close(); needsOriginalSource = true; source = bStream.bytes(); } BytesRef ref = source.toBytesRef(); fields.add(new StoredField(fieldType().name(), ref.bytes, ref.offset, ref.length)); } }	this variable is never read?
public void writeTo(StreamOutput out) throws IOException { out.writeString(type); out.writeString(id); out.writeString(path); if (out.getVersion().onOrAfter(Version.V_6_0_0_beta1)) { out.writeString(index); } else { out.writeOptionalString(index); } out.writeOptionalString(routing); }	can you remove this empty line? :p
protected void search(SearchRequest search, ActionListener<SearchResponse> listener) { // no pitId, ask for one if (pitId == null) { openPIT(listener, () -> searchWithPIT(search, listener)); } else { searchWithPIT(search, listener); } }	@jimczi @dnhatn fyi, this is the current code i've used for updating pit when dealing with multisearch.
* @return file info */ public static FileInfo fromXContent(XContentParser parser) throws IOException { XContentParser.Token token = parser.currentToken(); String name = null; String physicalName = null; long length = -1; String checksum = null; ByteSizeValue partSize = null; Version writtenBy = null; String writtenByStr = null; BytesRef metaHash = new BytesRef(); if (token == XContentParser.Token.START_OBJECT) { while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { String currentFieldName = parser.currentName(); token = parser.nextToken(); if (token.isValue()) { if (NAME.equals(currentFieldName)) { name = parser.text(); } else if (PHYSICAL_NAME.equals(currentFieldName)) { physicalName = parser.text(); } else if (LENGTH.equals(currentFieldName)) { length = parser.longValue(); } else if (CHECKSUM.equals(currentFieldName)) { checksum = parser.text(); } else if (PART_SIZE.equals(currentFieldName)) { partSize = new ByteSizeValue(parser.longValue()); } else if (WRITTEN_BY.equals(currentFieldName)) { writtenByStr = parser.text(); writtenBy = Lucene.parseVersionLenient(writtenByStr, null); } else if (META_HASH.equals(currentFieldName)) { metaHash.bytes = parser.binaryValue(); metaHash.offset = 0; metaHash.length = metaHash.bytes.length; } else { throw new ElasticsearchParseException("unknown parameter [{}]", currentFieldName); } } else { throw new ElasticsearchParseException("unexpected token [{}]", token); } } else { throw new ElasticsearchParseException("unexpected token [{}]",token); } } } // Verify that file information is complete if (name == null || Strings.validFileName(name) == false) { throw new ElasticsearchParseException("missing or invalid file name [" + name + "]"); } else if (physicalName == null || Strings.validFileName(physicalName) == false) { throw new ElasticsearchParseException("missing or invalid physical file name [" + physicalName + "]"); } else if (length < 0) { throw new ElasticsearchParseException("missing or invalid file length"); } else if (checksum == null) { if (physicalName.startsWith("segments_")) { // its possible the checksum is null for segments_N files that belong to a shard with no data, // so we will assign it _na_ for now and try to get the checksum from the file itself later checksum = StoreFileMetaData.UNKNOWN_CHECKSUM; } else { throw new ElasticsearchParseException("missing checksum for name [" + name + "]"); } } else if (writtenBy == null) { throw new ElasticsearchParseException("missing or invalid written_by [" + writtenByStr + "]"); } return new FileInfo(name, new StoreFileMetaData(physicalName, length, checksum, writtenBy, metaHash), partSize); }	make this an illegalstateexception?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); recoveryId = in.readLong(); shardId = ShardId.readShardId(in); String name = in.readString(); position = in.readVLong(); long length = in.readVLong(); String checksum = in.readString(); content = in.readBytesReference(); Version writtenBy = Lucene.parseVersionLenient(in.readString(), null); metaData = new StoreFileMetaData(name, length, checksum, writtenBy); lastChunk = in.readBoolean(); totalTranslogOps = in.readVInt(); sourceThrottleTimeInNanos = in.readLong(); }	should we just use version.parselenient(in.readstring()) it must be non-null right?
@Override public Query rewrite(IndexReader reader) throws IOException { boolean fieldExists = false; boolean hasPayloads = false; for (LeafReaderContext context : reader.leaves()) { final Terms terms = context.reader().terms(term.field()); if (terms != null) { fieldExists = true; if (terms.hasPayloads()) { hasPayloads = true; break; } } } if (fieldExists == false) { return new MatchNoDocsQuery(); } if (hasPayloads == false) { TermQuery rewritten = new TermQuery(term); rewritten.setBoost(getBoost()); return rewritten; } return this; }	can you propagate the boost to this query? if this query is enclosed in a booleanquery, it could have an impact on the normalization factor.
static OperationMode fromString(String string) { return EnumSet.allOf(OperationMode.class).stream() .filter(e -> string.equalsIgnoreCase(e.name())).findFirst() .orElseThrow(() -> new IllegalArgumentException(String.format(Locale.ENGLISH, "%s is not a valid operation_mode", string))); }	we use locale.root for as many things as we can, rather than the english locale here
public void testUCaseWithAZTRLocale() { Locale.setDefault(Locale.forLanguageTag("tr")); StringProcessor proc = new StringProcessor(StringOperation.UCASE); // ES-SQL is not Locale sensitive (so far). // in Turkish locale, small letter "i" is uppercased to "I" with a dot above (unicode 130), otherwise in "i" (unicode 49) assertEquals("\\\\u0049", proc.process("\\\\u0069")); Locale.setDefault(Locale.forLanguageTag("az")); assertEquals("\\\\u0049", proc.process("\\\\u0069")); }	i think it might makes sense to restore the original locale after the test so other tests in this class don't run in this local most of the time.
@Override public Object parse(String line) { return line.endsWith(";") ? line.substring(0, line.length() - 1) : line; } } static SqlSpecParser specParser() { return new SqlSpecParser(); } public SqlSpecTestCase(String fileName, String groupName, String testName, Integer lineNumber, String query) { super(fileName, groupName, testName, lineNumber); this.query = query; } @Override protected final void doTest() throws Throwable { String[] badLocales = new String[] {"tr", "az", "tr-TR", "tr-CY", "az-Latn", "az-Cyrl", "az-Latn-AZ", "az-Cyrl-AZ"}; boolean goodLocale = !Arrays.stream(badLocales) .anyMatch((l) -> Locale.getDefault().equals(new Locale.Builder().setLanguageTag(l).build())); if (fileName.startsWith("case-functions")) { Assume.assumeTrue(goodLocale); } try (Connection h2 = H2.get(); Connection es = esJdbc()) { ResultSet expected, elasticResults; expected = executeJdbcQuery(h2, query); elasticResults = executeJdbcQuery(es, query); assertResults(expected, elasticResults); }	these locales are not bad :) it's just some application are buggy and cannot work with them correctly. so, maybe we should call them h2incompatiblelocales or dottedilocales or something like this.
@Override protected TrainedModelConfig createTestInstance() { List<String> tags = null; if (randomBoolean()) { int tagsCount = randomIntBetween(0, 5); tags = new ArrayList<>(tagsCount); for (int i = 0; i < tagsCount; i++) { tags.add(randomAlphaOfLength(10)); } } return new TrainedModelConfig( randomAlphaOfLength(10), randomAlphaOfLength(10), Version.CURRENT, randomBoolean() ? null : randomAlphaOfLength(100), Instant.ofEpochMilli(randomNonNegativeLong()), randomBoolean() ? null : TrainedModelDefinitionTests.createRandomBuilder().build(), tags, randomBoolean() ? null : Collections.singletonMap(randomAlphaOfLength(10), randomAlphaOfLength(10))); }	nit, stream.generate is awesome. suggestion randomboolean() ? null : stream.generate(() -> randomalphaoflength(10)).limit(randomintbetween(0, 5).collect(collectors.tolist()),
private void processResult(AnalyticsResult result, DataFrameRowsJoiner resultsJoiner) { RowResults rowResults = result.getRowResults(); if (rowResults != null) { resultsJoiner.processRowResults(rowResults); } Integer progressPercent = result.getProgressPercent(); if (progressPercent != null) { progressTracker.analyzingPercent.set(progressPercent); } TrainedModelDefinition inferenceModel = result.getInferenceModel(); if (inferenceModel != null) { createAndIndexInferenceModel(inferenceModel); } }	i think it would be a good idea to write an audit message saying that the trained model was created and its id.
@Override protected void parseCreateField(ParseContext context) throws IOException { for (String field : context.getIgnoredFields()) { if (!fieldType().hasDocValues()) { // Use legacy field type if there are no doc values context.doc().add(new Field(NAME, field, Defaults.LEGACY_FIELD_TYPE)); continue; } final BytesRef binaryValue = new BytesRef(field); context.doc().add(new SortedSetDocValuesField(fieldType().name(), binaryValue)); } }	we still want to index the field so we should always add the field but switch to the right field_type depending on the created version.
public void testWithCustomFormatSortValueOfDateField() throws Exception { final XContentBuilder mappings = jsonBuilder(); mappings.startObject().startObject("properties"); { mappings.startObject("start_date"); mappings.field("type", "date"); mappings.field("format", "yyyy-MM-dd"); mappings.endObject(); } { mappings.startObject("end_date"); mappings.field("type", "date"); mappings.field("format", "yyyy-MM-dd"); mappings.endObject(); } mappings.endObject().endObject(); assertAcked(client().admin().indices().prepareCreate("test") .setSettings(Settings.builder().put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, between(1, 3))) .setMapping(mappings)); client().prepareBulk().setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) .add(new IndexRequest("test").id("1").source("start_date", "2019-03-24", "end_date", "2020-01-21")) .add(new IndexRequest("test").id("2").source("start_date", "2018-04-23", "end_date", "2021-02-22")) .add(new IndexRequest("test").id("3").source("start_date", "2015-01-22", "end_date", "2022-07-23")) .add(new IndexRequest("test").id("4").source("start_date", "2016-02-21", "end_date", "2024-03-24")) .add(new IndexRequest("test").id("5").source("start_date", "2017-01-20", "end_date", "2025-05-28")) .get(); SearchResponse resp = client().prepareSearch("test") .addSort(SortBuilders.fieldSort("start_date").setFormat("dd/MM/yyyy")) .addSort(SortBuilders.fieldSort("end_date").setFormat("yyyy-MM-dd")) .setSize(2) .get(); assertNoFailures(resp); assertThat(resp.getHits().getHits()[0].getSortValues(), arrayContaining("22/01/2015", "2022-07-23")); assertThat(resp.getHits().getHits()[1].getSortValues(), arrayContaining("21/02/2016", "2024-03-24")); resp = client().prepareSearch("test") .addSort(SortBuilders.fieldSort("start_date").setFormat("dd/MM/yyyy")) .addSort(SortBuilders.fieldSort("end_date").setFormat("yyyy-MM-dd")) .searchAfter(new String[]{"21/02/2016", "2024-03-24"}) .setSize(2) .get(); assertNoFailures(resp); assertThat(resp.getHits().getHits()[0].getSortValues(), arrayContaining("20/01/2017", "2025-05-28")); assertThat(resp.getHits().getHits()[1].getSortValues(), arrayContaining("23/04/2018", "2021-02-22")); resp = client().prepareSearch("test") .addSort(SortBuilders.fieldSort("start_date").setFormat("dd/MM/yyyy")) .addSort(SortBuilders.fieldSort("end_date")) // it's okay as we use the default format, but the output does not format .searchAfter(new String[]{"21/02/2016", "2024-03-24"}) .setSize(2) .get(); assertNoFailures(resp); assertThat(resp.getHits().getHits()[0].getSortValues(), arrayContaining("20/01/2017", 1748390400000L)); assertThat(resp.getHits().getHits()[1].getSortValues(), arrayContaining("23/04/2018", 1613952000000L)); SearchRequestBuilder searchRequest = client().prepareSearch("test") .addSort(SortBuilders.fieldSort("start_date").setFormat("dd/MM/yyyy")) .addSort(SortBuilders.fieldSort("end_date").setFormat("epoch_millis")) .searchAfter(new Object[]{"21/02/2016", 1748390400000L}) .setSize(2); assertNoFailures(searchRequest.get()); searchRequest = client().prepareSearch("test") .addSort(SortBuilders.fieldSort("start_date").setFormat("dd/MM/yyyy")) .addSort(SortBuilders.fieldSort("end_date").setFormat("epoch_millis")) // wrong format .searchAfter(new Object[]{"21/02/2016", "23/04/2018"}) .setSize(2); assertFailures(searchRequest, RestStatus.BAD_REQUEST, containsString("failed to parse date field [23/04/2018] with format [epoch_millis]")); }	isn't there a requirement that searchafter formats and formats in sort should match? or it is ok because "yyyy-mm-dd" is a default formant for end_date?
private static void internalParseDocument(MappingLookup lookup, MetadataFieldMapper[] metadataFieldsMappers, ParseContext context, XContentParser parser) throws IOException { RootObjectMapper root = lookup.getMapping().getRoot(); final boolean emptyDoc = isEmptyDoc(root, parser); for (MetadataFieldMapper metadataMapper : metadataFieldsMappers) { metadataMapper.preParse(context); } if (root.isEnabled() == false) { // entire type is disabled parser.skipChildren(); } else if (emptyDoc == false) { parseObjectOrNested(context, root); } PostParsePhase.executePostParsePhases(lookup, context); for (MetadataFieldMapper metadataMapper : metadataFieldsMappers) { metadataMapper.postParse(context); } }	doesn't the context already expose the mapping lookup?
private static void internalParseDocument(MappingLookup lookup, MetadataFieldMapper[] metadataFieldsMappers, ParseContext context, XContentParser parser) throws IOException { RootObjectMapper root = lookup.getMapping().getRoot(); final boolean emptyDoc = isEmptyDoc(root, parser); for (MetadataFieldMapper metadataMapper : metadataFieldsMappers) { metadataMapper.preParse(context); } if (root.isEnabled() == false) { // entire type is disabled parser.skipChildren(); } else if (emptyDoc == false) { parseObjectOrNested(context, root); } PostParsePhase.executePostParsePhases(lookup, context); for (MetadataFieldMapper metadataMapper : metadataFieldsMappers) { metadataMapper.postParse(context); } }	i believe that the post parse phase is inspired by this existing postparse method, i was wondering if it would make sense to name the new concept differently, and tie it to script execution since that is what it's used for. the additional level of indirection is there to avoid iterating over all of the field mappers and calling a method on them that may do nothing in most cases? wouldn't it be simpler though if field mapper exposed an executescript method or something along those lines instead of a post parse executor? the reader and enhanced context could be created lazily once the first scripted field is found, maybe through a new method exposed by parsecontext? i guess that the concern is to not cause overhead when no scripts are declared, but i wonder what kind of overhead it would be. another idea could be to make mapping lookup hold a set of field names that declare a script (or even a reference to the mappers themselves) which are the only ones that we will call that executescript method against?
@Override public CollapseType collapseType() { return CollapseType.NUMERIC; } } private final NumberType type; private final boolean indexed; private final boolean hasDocValues; private final boolean stored; private final Explicit<Boolean> ignoreMalformed; private final Explicit<Boolean> coerce; private final Number nullValue; private final MapperScript<Number> script; private final String onScriptError; private final boolean ignoreMalformedByDefault; private final boolean coerceByDefault; private NumberFieldMapper( String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo); this.type = builder.type; this.indexed = builder.indexed.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); this.stored = builder.stored.getValue(); this.ignoreMalformed = builder.ignoreMalformed.getValue(); this.coerce = builder.coerce.getValue(); this.nullValue = builder.nullValue.getValue(); this.ignoreMalformedByDefault = builder.ignoreMalformed.getDefaultValue().value(); this.coerceByDefault = builder.coerce.getDefaultValue().value(); this.script = builder.script.get(); this.onScriptError = builder.onScriptError.get(); } boolean coerce() { return coerce.value(); } boolean ignoreMalformed() { return ignoreMalformed.value(); } String onScriptError() { return onScriptError; } @Override public NumberFieldType fieldType() { return (NumberFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType().type.typeName(); } @Override protected void parseCreateField(ParseContext context) throws IOException { if (this.script != null) { throw new IllegalArgumentException("Cannot index data directly into a field with a [script] parameter"); } XContentParser parser = context.parser(); Object value; Number numericValue = null; if (context.externalValueSet()) { value = context.externalValue(); } else if (parser.currentToken() == Token.VALUE_NULL) { value = null; } else if (coerce.value() && parser.currentToken() == Token.VALUE_STRING && parser.textLength() == 0) { value = null; } else { try { numericValue = fieldType().type.parse(parser, coerce.value()); } catch (InputCoercionException | IllegalArgumentException | JsonParseException e) { if (ignoreMalformed.value() && parser.currentToken().isValue()) { context.addIgnoredField(mappedFieldType.name()); return; } else { throw e; } } value = numericValue; } if (value == null) { value = nullValue; } if (value == null) { return; } if (numericValue == null) { numericValue = fieldType().type.parse(value, coerce.value()); } indexValue(context, numericValue); } private void indexValue(ParseContext context, Number numericValue) { context.doc().addAll(fieldType().type.createFields(fieldType().name(), numericValue, indexed, hasDocValues, stored)); if (hasDocValues == false && (stored || indexed)) { createFieldNamesField(context); } } @Override public PostParseExecutor getPostParseExecutor() { if (this.script == null) { return null; } return new PostParseExecutor() { @Override public void execute(PostParseContext context) { script.executeAndEmit( context.searchLookup, context.leafReaderContext, 0, v -> indexValue(context.pc, v)); } @Override public void onError(PostParseContext context, Exception e) throws IOException { if ("ignore".equals(onScriptError)) { context.pc.addIgnoredField(name()); } else { throw new IOException("Error executing script on field [" + name() + "]", e); } } }; }	if indexvalue is called for every field value, don't we end up calling the _field_names code block once per value rather than once per field? that looks like it's how things already work without your change though.
@Override public BiFunction<Script, ScriptCompiler, MapperScript<Number>> compiler(String fieldName) { return (script, service) -> new MapperScript<>(script) { final DoubleFieldScript.Factory scriptFactory = service.compile(script, DoubleFieldScript.CONTEXT); @Override public void executeAndEmit(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { DoubleFieldScript s = scriptFactory .newFactory(fieldName, script.getParams(), lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } } }; }	nit: would you mind renaming service to compiler?
@Override public BiFunction<Script, ScriptCompiler, MapperScript<Number>> compiler(String fieldName) { return (script, service) -> new MapperScript<>(script) { final DoubleFieldScript.Factory scriptFactory = service.compile(script, DoubleFieldScript.CONTEXT); @Override public void executeAndEmit(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { DoubleFieldScript s = scriptFactory .newFactory(fieldName, script.getParams(), lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } } }; }	is auto-boxing here not a concern for the values? i am referring to the fact that we want out of our way to avoid it in the script classes, but here we lose that. is it worth using a longconsumer etc.?
@Override public BiFunction<Script, ScriptCompiler, MapperScript<Number>> compiler(String fieldName) { return (script, service) -> new MapperScript<>(script) { final DoubleFieldScript.Factory scriptFactory = service.compile(script, DoubleFieldScript.CONTEXT); @Override public void executeAndEmit(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { DoubleFieldScript s = scriptFactory .newFactory(fieldName, script.getParams(), lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } } }; }	we don't care about reusing the same script instance across different docs in the same leaf reader context do we? i guess that we have a context holding a single doc anyways?
@Override public BiFunction<Script, ScriptCompiler, MapperScript<Number>> compiler(String fieldName) { return (script, service) -> new MapperScript<>(script) { final DoubleFieldScript.Factory scriptFactory = service.compile(script, DoubleFieldScript.CONTEXT); @Override public void executeAndEmit(SearchLookup lookup, LeafReaderContext ctx, int doc, Consumer<Number> emitter) { DoubleFieldScript s = scriptFactory .newFactory(fieldName, script.getParams(), lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { emitter.accept(vs[i]); } } }; }	would it be possible to make the second part of this method part of the field script object itself? it would also be fantastic to use the same mechanism on both sides, but i know it is a bit tricky for different reasons: 1) that array that we reuse without resizing 2) we try so hard to avoid boxing 3) the usage pattern is slightly different if we compare doc_values, query and index time execution. i do wonder if it is worth investing on this, possibly having our own primitiveiterator or something that allows us to expose a clearer script api to access the computed values. for later i guess.
@Override public ValueFetcher valueFetcher(SearchExecutionContext context, String format) { if (format != null) { throw new IllegalArgumentException("Field [" + name() + "] of type [" + typeName() + "] doesn't support formats."); } if (this.script != null) { return new ValueFetcher() { LeafReaderContext ctx; @Override public void setNextReader(LeafReaderContext context) { this.ctx = context; } @Override public List<Object> fetchValues(SourceLookup lookup) { List<Object> values = new ArrayList<>(); try { script.executeAndEmit(context.lookup(), ctx, lookup.docId(), values::add); } catch (Exception e) { // ignore errors - if they exist here then they existed at index time // and were ignored, so we ignore here too } return values; } }; } return new SourceValueFetcher(name(), context, nullValue) { @Override protected Object parseSourceValue(Object value) { if (value.equals("")) { return nullValue; } return type.parse(value, coerce); } }; }	shouldn't we honor the configured on_error behaviour for the field?
protected final boolean assertCurrentThreadMayAccessBlobStore() { final String threadName = Thread.currentThread().getName(); assert threadName.contains('[' + ThreadPool.Names.SNAPSHOT + ']') || threadName.contains('[' + ThreadPool.Names.GENERIC + ']') || threadName.contains('[' + ThreadPool.Names.SEARCH + ']') || threadName.contains('[' + ThreadPool.Names.SEARCH_THROTTLED + ']') // Cache asynchronous fetching runs on a dedicated thread pool. || threadName.contains('[' + SearchableSnapshots.CACHE_FETCH_ASYNC_THREAD_POOL_NAME + ']') // Cache prewarming also runs on a dedicated thread pool. || threadName.contains('[' + SearchableSnapshots.CACHE_PREWARMING_THREAD_POOL_NAME + ']') // Unit tests access the blob store on the main test thread, or via an asynchronous // checkindex call; // simplest just to permit this rather than have them override this // method somehow. || threadName.startsWith("TEST-") || threadName.startsWith("async-check-index") || threadName.startsWith("LuceneTestCase") : "current thread [" + Thread.currentThread() + "] may not read " + fileInfo; return true; }	we should not read directly from async-check-index unless the current indexinput is an instance of directblobcontainerindexinput or we're reading bytes directly using readdirectlyifalreadyclosed() because we failed to read bytes from caches. i wonder if we could pass a direct parameter to assertcurrentthreadmayaccessblobstore() and be more permissive on thread names when direct is true.
@Override public short shortValue(boolean coerce) throws IOException { Token token = currentToken(); if (token == Token.VALUE_STRING) { checkCoerceString(coerce, Short.class); double doubleValue = Double.parseDouble(text()); if (doubleValue < Short.MIN_VALUE || doubleValue > Short.MAX_VALUE) { throw new IllegalArgumentException("Value [" + text() + "] is out of range for a short"); } return (short) doubleValue; } short result = doShortValue(); ensureNumberConversion(coerce, result, Short.class); return result; }	would it work if we only had checkcoercestring within this block? from what i read, doshortvalue already has the ability to parse strings.
@Override protected Object unwrapCustomValue(Object values) { DataType dataType = dataType(); if (dataType == DATETIME) { if (values instanceof String) { try { return DateUtils.asDateTimeWithNanos(values.toString(), zoneId()); } catch (IllegalArgumentException | DateTimeParseException e) { // For bwc compatibility during rolling upgrade return parseDateString(values); } } } return null; }	i'd wait for the fields api from @astefan to land before going for this one. hopefully that eliminates the internal handling of data format and thus this try/catch as well.
public boolean grants(ApplicationPrivilege other, String resource) { Automaton resourceAutomaton = Automatons.patterns(resource); final boolean matched = permissions.stream().anyMatch(e -> e.grants(other, resourceAutomaton)); logger.trace("Permission [{}] {} grant [{} , {}]", this, matched ? "does" : "does not", other, resource); return matched; } /** * Determines if this {@link ApplicationPermission} is a subset of other * application permission. It does this by checking whether each of the * permissions for this is granted by the other application permissions. * * @param other application permission * @return {@code true} if this is subset of other else it is {@code false}	i find this a bit hard to follow at first. maybe do this: java for (...) { final boolean issubset = ... ; if (issubset == false) { return false; } } return true;
public Automaton allowedActionsMatcher(String index) { List<Automaton> automatonList = new ArrayList<>(); for (Group group : groups) { if (group.indexNameMatcher.test(index)) { automatonList.add(group.privilege.getAutomaton()); } } return automatonList.isEmpty() ? Automatons.EMPTY : Automatons.unionAndMinimize(automatonList); } /** * Determines if this {@link IndicesPermission} is a subset of other indices * permission. This iteratively determines if all of the * {@link IndicesPermission.Group} for this IndicesPermission is a subset of * some group in the given indices permission. * * @param other indices permission * @return in case it is not a subset then returns * {@link SubsetResult.Result#NO} and if it is clearly a subset will return * {@link SubsetResult.Result#YES}. It will return * {@link SubsetResult.Result#MAYBE}	maybe add since we cannot determine if a query returns a subset of documents?
public SubsetResult isSubsetOf(final IndicesPermission other) { if (this.groups() == null || this.groups().length == 0) { return SubsetResult.isASubset(); } SubsetResult finalResult = null; for (Group thisGroup : this.groups()) { SubsetResult resultForThisGroup = null; if (other.groups() != null) { for (Group otherGroup : other.groups()) { final SubsetResult result = thisGroup.isSubsetOf(otherGroup); resultForThisGroup = SubsetResult.merge(resultForThisGroup, result); } } if (resultForThisGroup == null || resultForThisGroup.result() == SubsetResult.Result.NO) { finalResult = SubsetResult.isNotASubset(); break; } else { finalResult = SubsetResult.merge(finalResult, resultForThisGroup); } } return finalResult; }	i'd just go with return subsetresult.isnotasubset() here
public SubsetResult isSubsetOf(Group other) { SubsetResult result = SubsetResult.isNotASubset(); final boolean areIndicesASubset = Operations.subsetOf(Automatons.patterns(this.indices()), Automatons.patterns(other.indices())); if (areIndicesASubset) { final boolean arePrivilegesASubset = Operations.subsetOf(this.privilege().getAutomaton(), other.privilege().getAutomaton()); if (arePrivilegesASubset) { final Automaton thisFieldsPermissionAutomaton = FieldPermissions .initializePermittedFieldsAutomaton(this.getFieldPermissions().getFieldPermissionsDefinition()); final Automaton otherFieldsPermissionAutomaton = FieldPermissions .initializePermittedFieldsAutomaton(other.getFieldPermissions().getFieldPermissionsDefinition()); final boolean areFieldPermissionsASubset = Operations.subsetOf(thisFieldsPermissionAutomaton, otherFieldsPermissionAutomaton); if (areFieldPermissionsASubset == true) { if (this.getQuery() == null || other.getQuery() == null) { result = SubsetResult.isASubset(); } else { if (Sets.difference(this.getQuery(), other.getQuery()).isEmpty()) { result = SubsetResult.isASubset(); } else { result = SubsetResult.mayBeASubset(Sets.newHashSet(this.indices())); } } } } } return result; }	this is a lot of if levels. lets use methods to simplify the way the method reads. ideally we can condense this to: java final boolean issubsetexcludingdls = areindicesasubset(other) && areprivilegesasubset(other) && arefieldpermissionsasubset(other); if (issubsetexcludingdls) { return isdlsasubset(other); } else { return subsetresult.isnotasubset(); }
public RunAsPermission runAs() { return runAs; } /** * Determines if this {@link Role} is a subset of other role. It does this * by checking whether the {@link ClusterPermission}, * {@link ApplicationPermission}, {@link RunAsPermission} and * {@link IndicesPermission} is a subset of corresponding permissions from * other role. * * @param other role * @return in case it is not a subset then returns * {@link SubsetResult.Result#NO} and if it is clearly a subset will return * {@link SubsetResult.Result#YES}. It will return * {@link SubsetResult.Result#MAYBE}	we don't need this assignment/variable. just use: java if (issubset) { return this.indices().issubsetof(other.indices()); } else { return subsetresult.isnotasubset(); }
private Path total() { Path res = new Path(); Set<String> seenDevices = new HashSet<>(paths.length); Set<String> seenMounts = new HashSet<>(paths.length); boolean seenDevice; boolean seenMount; for (Path subPath : paths) { if (subPath.path != null) { seenDevice = !seenDevices.add(subPath.path); } else { seenDevice = true; } if (subPath.mount != null) { seenMount = !seenMounts.add(subPath.mount); } else { seenMount = true; } if (seenMount || (seenDevice && seenMount)) { continue; // already added numbers for this device/mount point; } res.add(subPath); } return res; }	this condition is equivalent to simply seenmount. effectively, seendevice is unused. i think this means something is missing/wrong in the test cases.
static ConnectionProfile buildDefaultConnectionProfile(Settings settings) { int connectionsPerNodeRecovery = CONNECTIONS_PER_NODE_RECOVERY.get(settings); int connectionsPerNodeBulk = CONNECTIONS_PER_NODE_BULK.get(settings); int connectionsPerNodeReg = CONNECTIONS_PER_NODE_REG.get(settings); int connectionsPerNodeState = CONNECTIONS_PER_NODE_STATE.get(settings); int connectionsPerNodePing = CONNECTIONS_PER_NODE_PING.get(settings); ConnectionProfile.Builder builder = new ConnectionProfile.Builder(); builder.addConnections(connectionsPerNodeBulk, TransportRequestOptions.Type.BULK); builder.addConnections(connectionsPerNodePing, TransportRequestOptions.Type.PING); Set<TransportRequestOptions.Type> regSet = EnumSet.of(TransportRequestOptions.Type.REG); if (DiscoveryNode.isMasterNode(settings)) { builder.addConnections(connectionsPerNodeState, TransportRequestOptions.Type.STATE); } else { // if we are not master eligible we don't need a dedicated channel to publish the state regSet.add(TransportRequestOptions.Type.STATE); } if (DiscoveryNode.isDataNode(settings)) { builder.addConnections(connectionsPerNodeRecovery, TransportRequestOptions.Type.RECOVERY); } else { // if we are not a data-node we don't need any dedicated channels for recovery regSet.add(TransportRequestOptions.Type.RECOVERY); } builder.addConnections(connectionsPerNodeReg, regSet.toArray(new TransportRequestOptions.Type[regSet.size()])); return builder.build(); }	the only thing that i do not like about this is how it silently ignores inconsistencies in the values of connectionspernodereg and connectionspernodestate.
static ConnectionProfile buildDefaultConnectionProfile(Settings settings) { int connectionsPerNodeRecovery = CONNECTIONS_PER_NODE_RECOVERY.get(settings); int connectionsPerNodeBulk = CONNECTIONS_PER_NODE_BULK.get(settings); int connectionsPerNodeReg = CONNECTIONS_PER_NODE_REG.get(settings); int connectionsPerNodeState = CONNECTIONS_PER_NODE_STATE.get(settings); int connectionsPerNodePing = CONNECTIONS_PER_NODE_PING.get(settings); ConnectionProfile.Builder builder = new ConnectionProfile.Builder(); builder.addConnections(connectionsPerNodeBulk, TransportRequestOptions.Type.BULK); builder.addConnections(connectionsPerNodePing, TransportRequestOptions.Type.PING); Set<TransportRequestOptions.Type> regSet = EnumSet.of(TransportRequestOptions.Type.REG); if (DiscoveryNode.isMasterNode(settings)) { builder.addConnections(connectionsPerNodeState, TransportRequestOptions.Type.STATE); } else { // if we are not master eligible we don't need a dedicated channel to publish the state regSet.add(TransportRequestOptions.Type.STATE); } if (DiscoveryNode.isDataNode(settings)) { builder.addConnections(connectionsPerNodeRecovery, TransportRequestOptions.Type.RECOVERY); } else { // if we are not a data-node we don't need any dedicated channels for recovery regSet.add(TransportRequestOptions.Type.RECOVERY); } builder.addConnections(connectionsPerNodeReg, regSet.toArray(new TransportRequestOptions.Type[regSet.size()])); return builder.build(); }	same comment here for connectionspernodereg and connectionspernoderecovery.
@Override protected void masterOperation(final ClusterHealthRequest request, final ClusterState unusedState, final ActionListener<ClusterHealthResponse> listener) throws ElasticsearchException { long endTime = System.currentTimeMillis() + request.timeout().millis(); if (request.waitForEvents() != null) { final CountDownLatch latch = new CountDownLatch(1); final AtomicReference<ElasticsearchException> failure = new AtomicReference<>(); clusterService.submitStateUpdateTask("cluster_health (wait_for_events [" + request.waitForEvents() + "])", request.waitForEvents(), new ProcessedClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) { return currentState; } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { latch.countDown(); } @Override public void onFailure(String source, Throwable t) { logger.error("unexpected failure during [{}]", t, source); failure.set(new ElasticsearchException("Error while waiting for events", t)); latch.countDown(); } @Override public boolean runOnlyOnMaster() { return !request.local(); } }); try { latch.await(request.timeout().millis(), TimeUnit.MILLISECONDS); } catch (InterruptedException e) { // ignore } if (failure.get() != null) { throw failure.get(); } } int waitFor = 5; if (request.waitForStatus() == null) { waitFor--; } if (request.waitForRelocatingShards() == -1) { waitFor--; } if (request.waitForActiveShards() == -1) { waitFor--; } if (request.waitForNodes().isEmpty()) { waitFor--; } if (request.indices().length == 0) { // check that they actually exists in the meta data waitFor--; } if (waitFor == 0) { // no need to wait for anything ClusterState clusterState = clusterService.state(); listener.onResponse(clusterHealth(request, clusterState)); return; } while (true) { int waitForCounter = 0; ClusterState clusterState = clusterService.state(); ClusterHealthResponse response = clusterHealth(request, clusterState); if (request.waitForStatus() != null && response.getStatus().value() <= request.waitForStatus().value()) { waitForCounter++; } if (request.waitForRelocatingShards() != -1 && response.getRelocatingShards() <= request.waitForRelocatingShards()) { waitForCounter++; } if (request.waitForActiveShards() != -1 && response.getActiveShards() >= request.waitForActiveShards()) { waitForCounter++; } if (request.indices().length > 0) { try { clusterState.metaData().concreteIndices(IndicesOptions.strictExpand(), request.indices()); waitForCounter++; } catch (IndexMissingException e) { response.status = ClusterHealthStatus.RED; // no indices, make sure its RED // missing indices, wait a bit more... } } if (!request.waitForNodes().isEmpty()) { if (request.waitForNodes().startsWith(">=")) { int expected = Integer.parseInt(request.waitForNodes().substring(2)); if (response.getNumberOfNodes() >= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("ge(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() >= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("<=")) { int expected = Integer.parseInt(request.waitForNodes().substring(2)); if (response.getNumberOfNodes() <= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("le(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() <= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith(">")) { int expected = Integer.parseInt(request.waitForNodes().substring(1)); if (response.getNumberOfNodes() > expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("gt(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() > expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("<")) { int expected = Integer.parseInt(request.waitForNodes().substring(1)); if (response.getNumberOfNodes() < expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("lt(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() < expected) { waitForCounter++; } } else { int expected = Integer.parseInt(request.waitForNodes()); if (response.getNumberOfNodes() == expected) { waitForCounter++; } } } if (waitForCounter == waitFor) { listener.onResponse(response); return; } if (System.currentTimeMillis() > endTime) { response.timedOut = true; listener.onResponse(response); return; } try { Thread.sleep(200); } catch (InterruptedException e) { response.timedOut = true; listener.onResponse(response); return; } } }	can this be throwable, and not wrapped in elasticsearch exception?
private Translog openTranslog(EngineConfig engineConfig, IndexWriter writer, TranslogDeletionPolicy translogDeletionPolicy, LongSupplier globalCheckpointSupplier) throws IOException { assert openMode != null; final TranslogConfig translogConfig = engineConfig.getTranslogConfig(); final String translogUUID; final String historyUUID; final boolean requiresCommit; switch (openMode) { case CREATE_INDEX_AND_TRANSLOG: translogUUID = null; historyUUID = UUIDs.randomBase64UUID(); requiresCommit = true; break; case OPEN_INDEX_CREATE_TRANSLOG: translogUUID = null; // create a new one; historyUUID = loadHistoryUUIDFromCommit(writer); requiresCommit = true; break; case OPEN_INDEX_AND_TRANSLOG: translogUUID = loadTranslogUUIDFromCommit(writer); // We expect that this shard already exists, so it must already have an existing translog else something is badly wrong! if (translogUUID == null) { throw new IndexFormatTooOldException("translog", "translog has no generation nor a UUID - this might be an index from a previous version consider upgrading to N-1 first"); } final String existingHistoryUUID = loadHistoryUUIDFromCommit(writer); if (existingHistoryUUID.equals(HISTORY_UUID_NA)) { // we are recovering an old primary, generate a history historyUUID = UUIDs.randomBase64UUID(); requiresCommit = true; } else { historyUUID = existingHistoryUUID; requiresCommit = false; } break; default: throw new IllegalStateException("unknown mode: " + openMode); } final Translog translog = new Translog(translogConfig, translogUUID, historyUUID, translogDeletionPolicy, globalCheckpointSupplier); if (requiresCommit) { assert openMode != EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG || config().getIndexSettings().getIndexVersionCreated().before(Version.V_6_0_0_rc1) : "OpenMode must not be" + EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG + ", index is post 6.0.0rc1"; boolean success = false; try { commitIndexWriter(writer, translog, openMode == EngineConfig.OpenMode.OPEN_INDEX_CREATE_TRANSLOG ? commitDataAsMap(writer).get(SYNC_COMMIT_ID) : null); success = true; } finally { if (success == false) { IOUtils.closeWhileHandlingException(translog); } } } return translog; }	i recognize that this is only moving code but can you use version.current to specify a more precise version in the error message rather than a generic message?
private static Checkpoint writeCheckpoint( ChannelFactory channelFactory, long syncPosition, int numOperations, long minSeqNo, long maxSeqNo, long globalCheckpoint, long minTranslogGeneration, Path translogFile, long generation, String translogUUID, String historyUUID) throws IOException { final Checkpoint checkpoint = new Checkpoint(syncPosition, numOperations, generation, minSeqNo, maxSeqNo, globalCheckpoint, minTranslogGeneration, translogUUID, historyUUID); writeCheckpoint(channelFactory, translogFile, checkpoint); return checkpoint; }	can you wrap these to the next two lines like the rest of the parameters?
@Override public int hashCode() { if (type == null) { return Objects.hash(index, id, path, routing); } else { return Objects.hash(index, type, id, path, routing); } }	i don't think we should change hashcode as long as "type" is still a field in this class, even if it can be null, but in can still be set.
public void writeTo(StreamOutput out) throws IOException { if (out.getVersion().onOrAfter(Version.V_7_0_0)) { out.writeOptionalString(type); } else { out.writeString(type); } out.writeString(id); out.writeString(path); if (out.getVersion().onOrAfter(Version.V_6_0_0_beta1)) { out.writeString(index); } else { out.writeOptionalString(index); } out.writeOptionalString(routing); }	i think this might break if type is "null". we should write some dummy type to the stream then?
public static QueryBuilder createQuery(Random r) { switch (RandomNumbers.randomIntBetween(r, 0, 3)) { case 0: return new MatchAllQueryBuilderTests().createTestQueryBuilder(); case 1: return new TermQueryBuilderTests().createTestQueryBuilder(); case 2: return new ExistsQueryBuilder(randomAlphaOfLength(5)); case 3: return createMultiTermQuery(r); default: throw new UnsupportedOperationException(); } } /** * Create a new multi term query of a random type * @param r random seed * @return a random {@link MultiTermQueryBuilder}	was this changed so we don't get warnings all over the place? if so, how about nulling the "type" after creating the random query builder? i think it would be nice to keep this the same type of query builder as before, just to not have to many changes in the tests at once that might be introduced by changing this to "exists", although that might be paranoid.
private TermsLookup randomTermsLookup() { TermsLookup lookup = randomBoolean() ? new TermsLookup(randomAlphaOfLength(10), randomAlphaOfLength(10), termsPath) : new TermsLookup(randomAlphaOfLength(10), randomAlphaOfLength(10), randomAlphaOfLength(10), termsPath); lookup.routing(randomBoolean() ? randomAlphaOfLength(10) : null); return lookup; }	nit: add comment to understand why these two cases are necesarry. it makes sense in the context of this pr but i doubt someone will make sense of it in two months from now. maybe also name the "type" argument specifically just to make sure the two cases here are related to types removal to make further refactoring easier.
@Test public void testMultiIndex() throws Exception { createIndex("test1"); createIndex("test2"); ensureGreen(); client().prepareIndex("test1", "type1", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test1", "type2", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test2", "type", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().admin().indices().prepareRefresh().execute().actionGet(); int numShards1 = getNumShards("test1").totalNumShards; int numShards2 = getNumShards("test2").totalNumShards; IndicesStatsRequestBuilder builder = client().admin().indices().prepareStats(); IndicesStatsResponse stats = builder.execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("_all").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("_all").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("*").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("test1").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1)); stats = builder.setIndices("test1", "test2").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("*2").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards2)); }	you can use the common method refresh() here which checks also if there are failures in the response.
static List<Integer> getRowOrder(Table table, RestRequest request) { String[] columnOrdering = request.paramAsStringArray("s", null); List<Integer> rowOrder = new ArrayList<>(); for (int i = 0; i < table.getRows().size(); i++) { rowOrder.add(i); } if (columnOrdering != null) { Map<String, String> headerAliasMap = table.getAliasMap(); List<ColumnOrderElement> ordering = new ArrayList<>(); for (int i = 0; i < columnOrdering.length; i++) { String columnHeader = columnOrdering[i]; boolean reverse = false; if (columnHeader.length() > 5 && columnHeader.endsWith(":desc")) { columnHeader = columnHeader.substring(0, columnHeader.length() - 5); reverse = true; } else if (columnHeader.length() > 4 && columnHeader.endsWith(":asc")) { columnHeader = columnHeader.substring(0, columnHeader.length() - 4); } if (headerAliasMap.containsKey(columnHeader)) { ordering.add(new ColumnOrderElement(headerAliasMap.get(columnHeader), reverse)); } else { throw new UnsupportedOperationException("Unable to sort by unknown sort key " + columnHeader); } } Collections.sort(rowOrder, new TableIndexComparator(table, ordering)); } return rowOrder; }	i'm not sure the length comparison buys you much. i'm fairly sure string#endswith already checks that super duper early.
static List<Integer> getRowOrder(Table table, RestRequest request) { String[] columnOrdering = request.paramAsStringArray("s", null); List<Integer> rowOrder = new ArrayList<>(); for (int i = 0; i < table.getRows().size(); i++) { rowOrder.add(i); } if (columnOrdering != null) { Map<String, String> headerAliasMap = table.getAliasMap(); List<ColumnOrderElement> ordering = new ArrayList<>(); for (int i = 0; i < columnOrdering.length; i++) { String columnHeader = columnOrdering[i]; boolean reverse = false; if (columnHeader.length() > 5 && columnHeader.endsWith(":desc")) { columnHeader = columnHeader.substring(0, columnHeader.length() - 5); reverse = true; } else if (columnHeader.length() > 4 && columnHeader.endsWith(":asc")) { columnHeader = columnHeader.substring(0, columnHeader.length() - 4); } if (headerAliasMap.containsKey(columnHeader)) { ordering.add(new ColumnOrderElement(headerAliasMap.get(columnHeader), reverse)); } else { throw new UnsupportedOperationException("Unable to sort by unknown sort key " + columnHeader); } } Collections.sort(rowOrder, new TableIndexComparator(table, ordering)); } return rowOrder; }	rather than - 5 i'd do - ":desc".length() just to make it super clear what is up.
public void testUnknownHeader() { Table table = new Table(); table.startHeaders(); table.addCell("compare"); table.endHeaders(); restRequest.params().put("s", "notaheader"); expectThrows(UnsupportedOperationException.class, () -> { RestTable.getRowOrder(table, restRequest); }); }	i think it'd be nice to do: java exception e = expectthrows(unsupportedoperationexception.class, () -> { resttable.getroworder(table, restrequest); }); assertequals("blah blah blah", e.getmessage()); it is just nice to assert something about the message so you know _which_ unsupportedoperationexception you get.
private MockTerminal executeCommand(ElasticsearchNodeCommand command, Environment environment, boolean abort) throws Exception { final MockTerminal terminal = new MockTerminal(); final OptionSet options = command.getParser().parse(); final String input; if (abort) { input = randomValueOtherThanMany(c -> c.equalsIgnoreCase("y"), () -> randomAlphaOfLength(1)); } else { input = randomBoolean() ? "y" : "Y"; } terminal.addTextInput(input); try { command.execute(terminal, options, environment); } finally { assertThat(terminal.getOutput(), containsString(ElasticsearchNodeCommand.STOP_WARNING_MSG)); } return terminal; }	this option is documented in docs/reference/commands/node-tool.asciidoc, so those docs need adjusting too.
public static MappingLookup fromMapping(Mapping mapping, Analyzer defaultIndex) { List<ObjectMapper> newObjectMappers = new ArrayList<>(); Map<String, FieldMapper> newFieldMappers = new HashMap<>(); List<FieldAliasMapper> newFieldAliasMappers = new ArrayList<>(); collect(mapping.root, newObjectMappers, newFieldMappers, newFieldAliasMappers); //add runtime fields and replace concrete fields with runtime fields with the same name for (RuntimeFieldMapper runtimeMapper : mapping.root.runtimeMappers()) { newFieldMappers.put(runtimeMapper.name(), runtimeMapper); } //metadata fields cannot be replaced for (MetadataFieldMapper metadataMapper : mapping.metadataMappers) { if (metadataMapper != null) { newFieldMappers.put(metadataMapper.name(), metadataMapper); } } return new MappingLookup(newFieldMappers.values(), newObjectMappers, newFieldAliasMappers, mapping.metadataMappers.length, defaultIndex); }	how does this work when parsing documents? we presumably want to ensure that an incoming doc is parsed using the non-runtime version of the mapper if this exists?
@Override protected void doStop() { synchronized (httpServerChannels) { if (httpServerChannels.isEmpty() == false) { try { CloseableChannel.closeChannels(new ArrayList<>(httpServerChannels), true); } catch (Exception e) { logger.warn("exception while closing channels", e); } finally { httpServerChannels.clear(); } } } try { CloseableChannel.closeChannels(new ArrayList<>(httpChannels), true); assert httpChannels.isEmpty() : "all channels should have been closed but saw [" + httpChannels + "]"; } catch (Exception e) { logger.warn("unexpected exception while closing http channels", e); } stopInternal(); }	i think this is ok, closeablechannel.closechannels(..., true) adds a new close listener to all the channels and i am assuming that the listeners definitely complete in the order in which they were added.
protected void serverAcceptedChannel(HttpChannel httpChannel) { boolean addedOnThisCall = httpChannels.add(httpChannel); httpChannel.addCloseListener(ActionListener.wrap(() -> httpChannels.remove(httpChannel))); assert addedOnThisCall : "Channel should only be added to http channel set once"; totalChannelsAccepted.incrementAndGet(); addClientStats(httpChannel); logger.trace(() -> new ParameterizedMessage("Http channel accepted: {}", httpChannel)); }	would kind of prefer having the assertion straight after the assignment of addedonthiscall
private HttpStats.ClientStats addClientStats(final HttpChannel httpChannel) { if (clientStatsEnabled) { final HttpStats.ClientStats clientStats; if (httpChannel != null) { clientStats = new HttpStats.ClientStats(threadPool.absoluteTimeInMillis()); httpChannelStats.put(HttpStats.ClientStats.getChannelKey(httpChannel), clientStats); httpChannel.addCloseListener(ActionListener.wrap(() -> { try { HttpStats.ClientStats disconnectedClientStats = httpChannelStats.get(HttpStats.ClientStats.getChannelKey(httpChannel)); if (disconnectedClientStats != null) { disconnectedClientStats.closedTimeMillis = threadPool.absoluteTimeInMillis(); } } catch (Exception e) { // the listener code above should never throw logger.trace("error removing HTTP channel listener", e); } })); } else { clientStats = null; } pruneClientStats(true); return clientStats; } else { return null; } }	while we're here, let's add an assert false : e in the catch block for good measure rather than just commenting that it doesn't throw.
@Override public String toString() { StringBuilder builder = new StringBuilder(); builder.append("DeleteResponse["); builder.append("index=").append(getIndex()); builder.append(",type=").append(getType()); builder.append(",id=").append(getId()); builder.append(",version=").append(getVersion()); builder.append(",found=").append(isFound()); builder.append(",operation=").append(getOperation().getLowercase()); builder.append(",shards=").append(getShardInfo()); return builder.append("]").toString(); }	i don't think you need to keep the found part of the tostring. i'd just nuke this line.
@Override public String toString() { StringBuilder builder = new StringBuilder(); builder.append("DeleteResponse["); builder.append("index=").append(getIndex()); builder.append(",type=").append(getType()); builder.append(",id=").append(getId()); builder.append(",version=").append(getVersion()); builder.append(",found=").append(isFound()); builder.append(",operation=").append(getOperation().getLowercase()); builder.append(",shards=").append(getShardInfo()); return builder.append("]").toString(); }	if you always want to use the lowercased version of the constant maybe just implement tostring to return lowercased? i'm not sure what the right thing is here but it isn't a big deal either way.
@Override public String toString() { StringBuilder builder = new StringBuilder(); builder.append("IndexResponse["); builder.append("index=").append(getIndex()); builder.append(",type=").append(getType()); builder.append(",id=").append(getId()); builder.append(",version=").append(getVersion()); builder.append(",created=").append(isCreated()); builder.append(",operation=").append(getOperation().getLowercase()); builder.append(",shards=").append(getShardInfo()); return builder.append("]").toString(); }	same feedback as the last tostring - i think you can remove created and might want to look at operation's tostring.
@Override public String toString() { StringBuilder builder = new StringBuilder(); builder.append("UpdateResponse["); builder.append("index=").append(getIndex()); builder.append(",type=").append(getType()); builder.append(",id=").append(getId()); builder.append(",version=").append(getVersion()); builder.append(",created=").append(isCreated()); builder.append(",operation=").append(getOperation().getLowercase()); builder.append(",shards=").append(getShardInfo()); return builder.append("]").toString(); }	same deal as the last tostring.
private void innerJoinCluster() { DiscoveryNode masterNode = null; final Thread currentThread = Thread.currentThread(); nodeJoinController.startAccumulatingJoins(); while (masterNode == null && joinThreadControl.joinThreadActive(currentThread)) { masterNode = findMaster(); } if (!joinThreadControl.joinThreadActive(currentThread)) { logger.trace("thread is no longer in currentJoinThread. Stopping."); return; } if (clusterService.localNode().equals(masterNode)) { final int requiredJoins = Math.max(0, electMaster.minimumMasterNodes() - 1); // we count as one logger.debug("elected as master, waiting for incoming joins ([{}] needed)", requiredJoins); nodeJoinController.waitToBeElectedAsMaster(requiredJoins, masterElectionWaitForJoinsTimeout, new NodeJoinController.Callback() { @Override public void onElectedAsMaster(ClusterState state) { joinThreadControl.markThreadAsDone(currentThread); // we only starts nodesFD if we are master (it may be that we received a cluster state while pinging) nodesFD.updateNodesAndPing(state); // start the nodes FD sendInitialStateEventIfNeeded(); long count = clusterJoinsCounter.incrementAndGet(); logger.trace("cluster joins counter set to [{}] (elected as master)", count); } @Override public void onFailure(Throwable t) { logger.trace("failed while waiting for nodes to join, rejoining", t); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); } } ); } else { // process any incoming joins (they will fail because we are not the master) nodeJoinController.stopAccumulatingJoins(); // send join request final boolean success = joinElectedMaster(masterNode); // finalize join through the cluster state update thread final DiscoveryNode finalMasterNode = masterNode; clusterService.submitStateUpdateTask("finalize_join (" + masterNode + ")", new ClusterStateNonMasterUpdateTask() { @Override public ClusterState execute(ClusterState currentState) throws Exception { if (!success) { // failed to join. Try again... joinThreadControl.markThreadAsDoneAndStartNew(currentThread); return currentState; } if (currentState.getNodes().masterNode() == null) { // Post 1.3.0, the master should publish a new cluster state before acking our join request. we now should have // a valid master. logger.debug("no master node is set, despite of join request completing. retrying pings."); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); return currentState; } if (!currentState.getNodes().masterNode().equals(finalMasterNode)) { return joinThreadControl.stopRunningThreadAndRejoin(currentState, "master_switched_while_finalizing_join"); } // Note: we do not have to start master fault detection here because it's set at {@link #handleNewClusterStateFromMaster } // when the first cluster state arrives. joinThreadControl.markThreadAsDone(currentThread); return currentState; } @Override public void onFailure(String source, @Nullable Throwable t) { logger.error("unexpected error while trying to finalize cluster join", t); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); } }); } }	why are we going to accumulate join request while we don't know that we are going to become master master?
public static void rejectNewClusterStateIfNeeded(ESLogger logger, DiscoveryNodes currentNodes, ClusterState newClusterState) { if (currentNodes.masterNodeId() == null) { return; } if (!currentNodes.masterNodeId().equals(newClusterState.nodes().masterNodeId())) { logger.warn("received a cluster state from a different master then the current one, rejecting (received {}, current {})", newClusterState.nodes().masterNode(), currentNodes.masterNode()); throw new IllegalStateException("cluster state from a different master then the current one, rejecting (received " + newClusterState.nodes().masterNode() + ", current " + currentNodes.masterNode() + ")"); } }	maybe instead log: received a cluster state the follows a different elected master than we are following?
public static void rejectNewClusterStateIfNeeded(ESLogger logger, DiscoveryNodes currentNodes, ClusterState newClusterState) { if (currentNodes.masterNodeId() == null) { return; } if (!currentNodes.masterNodeId().equals(newClusterState.nodes().masterNodeId())) { logger.warn("received a cluster state from a different master then the current one, rejecting (received {}, current {})", newClusterState.nodes().masterNode(), currentNodes.masterNode()); throw new IllegalStateException("cluster state from a different master then the current one, rejecting (received " + newClusterState.nodes().masterNode() + ", current " + currentNodes.masterNode() + ")"); } }	is it really possible that we receive join requests if zendiscover#dostart() hasn't been completed yet? this feels odd to me
@Test public void testNodesFDAfterMasterReelection() throws Exception { startCluster(4); logger.info("--> stopping current master"); internalCluster().stopCurrentMasterNode(); ensureStableCluster(3); logger.info("--> reducing min master nodes to 2"); assertAcked(client().admin().cluster().prepareUpdateSettings() .setTransientSettings(Settings.builder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 2)).get()); String master = internalCluster().getMasterName(); String nonMaster = null; for (String node : internalCluster().getNodeNames()) { if (!node.equals(master)) { nonMaster = node; } } logger.info("--> isolating [{}]", nonMaster); addRandomIsolation(nonMaster).startDisrupting(); logger.info("--> waiting for master to remove it"); ensureStableCluster(2, master); }	what is the reason for changing this test?
@Override public List<RestHandler> getRestHandlers(Settings settings, RestController restController, ClusterSettings clusterSettings, IndexScopedSettings indexScopedSettings, SettingsFilter settingsFilter, IndexNameExpressionResolver indexNameExpressionResolver, Supplier<DiscoveryNodes> nodesInCluster) { List<RestHandler> handlers = new ArrayList<>(); handlers.add(new RestXPackInfoAction()); handlers.add(new RestXPackUsageAction()); handlers.add(new RestReloadAnalyzersAction()); handlers.addAll(licensing.getRestHandlers(settings, restController, clusterSettings, indexScopedSettings, settingsFilter, indexNameExpressionResolver, nodesInCluster)); handlers.add(new RestNodeEnrollmentAction()); return handlers; }	this rest handler and the corresponding transport handler are registered here in xpack core instead of security. but i wonder whether it would be better to register them under xpack security because this api assumes security being enabled (which really is by design). when we bootstrap a new cluster, the init process manipulates the elasticsearch.yml file. so we have the guarantee that security is enabled. but we don't have the same guarantee for an existing cluster. iiuc, users can still explicitly disable security in v8.0. so for a cluster where security is explicitly disabled, it does not seem to have much sense to keep this api around? it's likely error out because there may not be any keystore file for it to read. it would probably be even worse if there are keystore files (maybe leftovers before security is disabled) because a successful response could give false impression of security. if we register this api under security, it will not be available when security is explicitly disabled which naturally solves the aforementioned issue. one advantage that i can think of for having it under core is that we can return an error response says "please enable security". but this can probably be replaced by documentation and also we don't have this type of error messages for any other security apis. in summary, i'd suggest to register this api and likely other related apis, as well as the setting for enrollment mode, under security.
public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (definition == null) { return builder.value(name); } return definition.toXContent(builder, params); } } public AnalyzeRequest() { } /** * Constructs a new analyzer request for the provided index. * * @param index The text to analyze */ public AnalyzeRequest(String index) { this.index = index; } /** * Set the index that the request should be run against */ public AnalyzeRequest index(String index) { this.index = index; return this; } /** * Returns the index that the request should be executed against, or {@code null} if * no index is specified */ public String index() { return this.index; } /** * Returns the text to be analyzed */ public String[] text() { return this.text; } /** * Set the text to be analyzed */ public AnalyzeRequest text(String... text) { this.text = text; return this; } /** * Use a defined analyzer */ public AnalyzeRequest analyzer(String analyzer) { this.analyzer = analyzer; return this; } public String analyzer() { return this.analyzer; } public AnalyzeRequest tokenizer(String tokenizer) { this.tokenizer = new NameOrDefinition(tokenizer); return this; } public AnalyzeRequest tokenizer(Map<String, ?> tokenizer) { this.tokenizer = new NameOrDefinition(tokenizer); return this; } public NameOrDefinition tokenizer() { return this.tokenizer; } public AnalyzeRequest addTokenFilter(String tokenFilter) { this.tokenFilters.add(new NameOrDefinition(tokenFilter)); return this; } public AnalyzeRequest addTokenFilter(Map<String, ?> tokenFilter) { this.tokenFilters.add(new NameOrDefinition(tokenFilter)); return this; } public List<NameOrDefinition> tokenFilters() { return this.tokenFilters; } public AnalyzeRequest addCharFilter(Map<String, ?> charFilter) { this.charFilters.add(new NameOrDefinition(charFilter)); return this; } public AnalyzeRequest addCharFilter(String charFilter) { this.charFilters.add(new NameOrDefinition(charFilter)); return this; } public List<NameOrDefinition> charFilters() { return this.charFilters; } public AnalyzeRequest field(String field) { this.field = field; return this; } public String field() { return this.field; } public AnalyzeRequest explain(boolean explain) { this.explain = explain; return this; } public boolean explain() { return this.explain; } public AnalyzeRequest attributes(String... attributes) { if (attributes == null) { throw new IllegalArgumentException("attributes must not be null"); } this.attributes = attributes; return this; } public String[] attributes() { return this.attributes; } public String normalizer() { return this.normalizer; } public AnalyzeRequest normalizer(String normalizer) { this.normalizer = normalizer; return this; } @Override public Optional<ValidationException> validate() { final ValidationException validationException = new ValidationException(); if (text == null || text.length == 0) { validationException.addValidationError("text is missing"); } if ((index == null || index.length() == 0) && normalizer != null) { validationException.addValidationError("index is required if normalizer is specified"); } if (normalizer != null && (tokenizer != null || analyzer != null)) { validationException.addValidationError("tokenizer/analyze should be null if normalizer is specified"); } if (validationException.validationErrors().isEmpty()) { return Optional.empty(); } return Optional.of(validationException); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("text", text); if (Strings.isNullOrEmpty(analyzer) == false) { builder.field("analyzer", analyzer); } if (tokenizer != null) { tokenizer.toXContent(builder, params); } if (tokenFilters.size() > 0) { builder.field("filter", tokenFilters); } if (charFilters.size() > 0) { builder.field("char_filter", charFilters); } if (Strings.isNullOrEmpty(field) == false) { builder.field("field", field); } if (explain) { builder.field("explain", true); } if (attributes.length > 0) { builder.field("attributes", attributes); } if (Strings.isNullOrEmpty(normalizer) == false) { builder.field("normalizer", normalizer); }	lets get rid of the builder lyfe code in the setters, which requires this horrible validate() method. please ensure that all mandatory fields are set and validated in the constructor, and just have get/sets for the rest.
public void testIncludeInAll() throws Exception { String mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field").field("type", "ip") .field("include_in_all", true).endObject().endObject() .endObject().endObject().string(); DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping)); assertEquals(mapping, mapper.mappingSource().toString()); ParsedDocument doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder() .startObject() .field("field", "::1") .endObject() .bytes()); IndexableField[] fields = doc.rootDoc().getFields("_all"); assertEquals(1, fields.length); assertEquals("::1", fields[0].stringValue()); mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field").field("type", "ip").endObject().endObject() .endObject().endObject().string(); mapper = parser.parse("type", new CompressedXContent(mapping)); assertEquals(mapping, mapper.mappingSource().toString()); doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder() .startObject() .field("field", "::1") .endObject() .bytes()); fields = doc.rootDoc().getFields("_all"); assertEquals(0, fields.length); }	these tests now check that overriding to true works. can we have tests for the default, ie that by default the field is not added to _all?
public void testIncludeInAll() throws Exception { String mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field").field("type", "ip") .field("include_in_all", true).endObject().endObject() .endObject().endObject().string(); DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping)); assertEquals(mapping, mapper.mappingSource().toString()); ParsedDocument doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder() .startObject() .field("field", "::1") .endObject() .bytes()); IndexableField[] fields = doc.rootDoc().getFields("_all"); assertEquals(1, fields.length); assertEquals("::1", fields[0].stringValue()); mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field").field("type", "ip").endObject().endObject() .endObject().endObject().string(); mapper = parser.parse("type", new CompressedXContent(mapping)); assertEquals(mapping, mapper.mappingSource().toString()); doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder() .startObject() .field("field", "::1") .endObject() .bytes()); fields = doc.rootDoc().getFields("_all"); assertEquals(0, fields.length); }	this is for checking that it is not included when unset
private void handleShardFailureOnMaster(final ShardRoutingEntry shardRoutingEntry) { logger.warn("{} received shard failed for {}", shardRoutingEntry.failure, shardRoutingEntry.shardRouting.shardId(), shardRoutingEntry); clusterService.submitStateUpdateTask( "shard-failed (" + shardRoutingEntry.shardRouting + "), message [" + shardRoutingEntry.message + "]", shardRoutingEntry, ClusterStateTaskConfig.build(Priority.HIGH), shardFailedClusterStateHandler, shardFailedClusterStateHandler); }	double check - this is going to change to a request specific listener , right?
@Override public BatchResult<ShardRoutingEntry> execute(ClusterState currentState, List<ShardRoutingEntry> tasks) throws Exception { BatchResult.Builder<ShardRoutingEntry> builder = BatchResult.builder(); ClusterState accumulator = ClusterState.builder(currentState).build(); for (ShardRoutingEntry task : tasks) { task.processed = true; try { RoutingAllocation.Result result = allocationService.applyFailedShard( currentState, new FailedRerouteAllocation.FailedShard(task.shardRouting, task.message, task.failure)); builder.success(task); if (result.changed()) { accumulator = ClusterState.builder(accumulator).routingResult(result).build(); } } catch (Throwable t) { builder.failure(task, t); } } return builder.build(accumulator); }	i think we are returning a new cluster state even if nothing happened.
public void testKeepOnlyStartingCommitOnInit() throws Exception { final AtomicLong globalCheckpoint = new AtomicLong(randomNonNegativeLong()); TranslogDeletionPolicy translogPolicy = createTranslogDeletionPolicy(); final UUID translogUUID = UUID.randomUUID(); final List<IndexCommit> commitList = new ArrayList<>(); int totalCommits = between(2, 20); for (int i = 0; i < totalCommits; i++) { commitList.add(mockIndexCommit(randomNonNegativeLong(), translogUUID, randomNonNegativeLong())); } final IndexCommit startingCommit = randomFrom(commitList); CombinedDeletionPolicy indexPolicy = new CombinedDeletionPolicy( OPEN_INDEX_AND_TRANSLOG, translogPolicy, globalCheckpoint::get, startingCommit); indexPolicy.onInit(commitList); for (IndexCommit commit : commitList) { if (commit.equals(startingCommit) == false) { verify(commit, times(1)).delete(); } } verify(startingCommit, never()).delete(); assertThat(translogPolicy.getMinTranslogGenerationForRecovery(), equalTo(Long.parseLong(startingCommit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY)))); assertThat(translogPolicy.getTranslogGenerationOfLastCommit(), equalTo(Long.parseLong(startingCommit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY)))); }	can you add java docs explaining why this behavior is needed. i think it will be hard to figure out.
public MappedFieldType get(String field) { String concreteField = aliasToConcreteName.getOrDefault(field, field); MappedFieldType fieldType = fullNameToFieldType.get(concreteField); if (fieldType != null) { return fieldType; } // If the mapping contains JSON fields, check if this could correspond // to a keyed JSON field of the form 'path_to_json_field.path_to_key'. return !fullNameToJsonMapper.isEmpty() ? getKeyedJsonField(field) : null; }	here we consider all possible splits on . to try to find one with a json field name on the left-hand side. i'm a little concerned about the cost of this if the search field is deeply nested. note that we don't just incur this cost when searching on a valid keyed json field, but also when the field is unmapped.
public static Query doTranslate(IsNotNull isNotNull, TranslatorHandler handler) { return handler.wrapFunctionQuery(isNotNull, isNotNull.field(), () -> translate(isNotNull, handler)); }	use a different name to avoid confusion, it's counter intuitive for dotranslate to call translate, i would have expected it to be the other way around. maybe createquery or internaltranslate
public static Query doTranslate(BinaryComparison bc, TranslatorHandler handler) { checkBinaryComparison(bc); return handler.wrapFunctionQuery(bc, bc.left(), () -> translate(bc, handler)); }	same here even though translate existed before it should be renamed.
default Query wrapFunctionQuery(ScalarFunction sf, Expression field, Supplier<Query> querySupplier) { if (field instanceof FieldAttribute) { return ExpressionTranslator.wrapIfNested(querySupplier.get(), field); } return new ScriptQuery(sf.source(), sf.asScript()); }	loosely coupled to this refactoring, but: couldn't nameof() also be offered a static implementation here? all implementations could actually have it as a static method and sql's (whose implementation lives actually in querytranslator) only treats the datetimefunction case additionally, compared to ql's. so a bit more duplication could be stripped away.
public void testTranslateStDistanceToQuery() { String operator = randomFrom("<", "<="); LogicalPlan p = plan("SELECT shape FROM test WHERE ST_Distance(point, ST_WKTToSQL('point (10 20)')) " + operator + " 25"); assertThat(p, instanceOf(Project.class)); assertThat(p.children().get(0), instanceOf(Filter.class)); Expression condition = ((Filter) p.children().get(0)).condition(); assertFalse(condition.foldable()); QueryTranslation translation = translate(condition); assertNull(translation.aggFilter); assertTrue(translation.query instanceof GeoDistanceQuery); GeoDistanceQuery gq = (GeoDistanceQuery) translation.query; assertEquals("point", gq.field()); assertEquals(20.0, gq.lat(), 0.00001); assertEquals(10.0, gq.lon(), 0.00001); assertEquals(25.0, gq.distance(), 0.00001); optimizeAndPlan(p); }	? - there are no extra assertions after this call.
@Override protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) throws IOException { String modelId = restRequest.param(TrainedModelConfig.MODEL_ID.getPreferredName()); if (Strings.isNullOrEmpty(modelId)) { modelId = Metadata.ALL; } List<String> tags = asList(restRequest.paramAsStringArray(TrainedModelConfig.TAGS.getPreferredName(), Strings.EMPTY_ARRAY)); Set<String> includes = new HashSet<>( asList( restRequest.paramAsStringArray( GetTrainedModelsAction.Request.INCLUDE.getPreferredName(), Strings.EMPTY_ARRAY))); final GetTrainedModelsAction.Request request; if (restRequest.hasParam(GetTrainedModelsAction.Request.INCLUDE_MODEL_DEFINITION)) { deprecationLogger.deprecate( GetTrainedModelsAction.Request.INCLUDE_MODEL_DEFINITION, "[{}] parameter is deprecated! Use parameter [{}] and supply the flag 'definition' instead.", GetTrainedModelsAction.Request.INCLUDE_MODEL_DEFINITION, GetTrainedModelsAction.Request.INCLUDE.getPreferredName()); request = new GetTrainedModelsAction.Request(modelId, restRequest.paramAsBoolean(GetTrainedModelsAction.Request.INCLUDE_MODEL_DEFINITION, false), tags); } else { request = new GetTrainedModelsAction.Request(modelId, tags, includes); } if (restRequest.hasParam(PageParams.FROM.getPreferredName()) || restRequest.hasParam(PageParams.SIZE.getPreferredName())) { request.setPageParams(new PageParams(restRequest.paramAsInt(PageParams.FROM.getPreferredName(), PageParams.DEFAULT_FROM), restRequest.paramAsInt(PageParams.SIZE.getPreferredName(), PageParams.DEFAULT_SIZE))); } request.setAllowNoResources(restRequest.paramAsBoolean(ALLOW_NO_MATCH.getPreferredName(), request.isAllowNoResources())); return channel -> client.execute(GetTrainedModelsAction.INSTANCE, request, new RestToXContentListenerWithDefaultValues<>(channel, DEFAULT_TO_XCONTENT_VALUES)); }	should this constructor now go away? public request(string id, boolean includemodeldefinition, list<string> tags) {
public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject("discovery_types"); for (final Map.Entry<String, AtomicInteger> entry : discoveryTypes.entrySet()) { builder.field(entry.getKey(), entry.getValue().get()); } builder.endObject(); return builder; } } static class PackagingTypes implements ToXContentFragment { private final Map<Tuple<String, String>, AtomicInteger> packagingTypes; PackagingTypes(final List<NodeInfo> nodeInfos) { final var packagingTypes = new HashMap<Tuple<String, String>, AtomicInteger>(); for (final var nodeInfo : nodeInfos) { final var flavor = nodeInfo.getBuild().flavor().displayName(); final var type = nodeInfo.getBuild().type().displayName(); packagingTypes.computeIfAbsent(Tuple.tuple(flavor, type), k -> new AtomicInteger()).incrementAndGet(); } this.packagingTypes = Collections.unmodifiableMap(packagingTypes); } @Override public XContentBuilder toXContent(final XContentBuilder builder, final Params params) throws IOException { builder.startArray("packaging_types"); { for (final var entry : packagingTypes.entrySet()) { builder.startObject(); { builder.field("flavor", entry.getKey().v1()); builder.field("type", entry.getKey().v2()); builder.field("count", entry.getValue().get()); } builder.endObject(); } } builder.endArray(); return builder; } } static class IngestStats implements ToXContentFragment { final int pipelineCount; final Map<String, long[]> stats; IngestStats(final List<NodeStats> nodeStats) { Set<String> pipelineIds = new HashSet<>(); SortedMap<String, long[]> stats = new TreeMap<>(); for (NodeStats nodeStat : nodeStats) { if (nodeStat.getIngestStats() != null) { for (Map.Entry<String, List<org.elasticsearch.ingest.IngestStats.ProcessorStat>> processorStats : nodeStat.getIngestStats() .getProcessorStats().entrySet()) { pipelineIds.add(processorStats.getKey()); for (org.elasticsearch.ingest.IngestStats.ProcessorStat stat : processorStats.getValue()) { stats.compute(stat.getType(), (k, v) -> { if (v == null) { return new long[] { stat.getStats().getIngestCount(), stat.getStats().getIngestFailedCount() }; } else { v[0] += stat.getStats().getIngestCount(); v[1] += stat.getStats().getIngestFailedCount(); return v; } }); } } } } this.pipelineCount = pipelineIds.size(); this.stats = Collections.unmodifiableMap(stats); } @Override public XContentBuilder toXContent(final XContentBuilder builder, final Params params) throws IOException { builder.startObject("ingest"); { builder.field("number_of_pipelines", pipelineCount); builder.startObject("processor_stats"); for (Map.Entry<String, long[]> stat : stats.entrySet()) { builder.startObject(stat.getKey()); builder.field("count", stat.getValue()[0]); builder.field("fail_count", stat.getValue()[1]); builder.endObject(); } builder.endObject(); } builder.endObject(); return builder; }	should we also add ingesttimeinmillis here?
public XContentBuilder toXContent(final XContentBuilder builder, final Params params) throws IOException { builder.startArray("packaging_types"); { for (final var entry : packagingTypes.entrySet()) { builder.startObject(); { builder.field("flavor", entry.getKey().v1()); builder.field("type", entry.getKey().v2()); builder.field("count", entry.getValue().get()); } builder.endObject(); } } builder.endArray(); return builder; } } static class IngestStats implements ToXContentFragment { final int pipelineCount; final Map<String, long[]> stats; IngestStats(final List<NodeStats> nodeStats) { Set<String> pipelineIds = new HashSet<>(); SortedMap<String, long[]> stats = new TreeMap<>(); for (NodeStats nodeStat : nodeStats) { if (nodeStat.getIngestStats() != null) { for (Map.Entry<String, List<org.elasticsearch.ingest.IngestStats.ProcessorStat>> processorStats : nodeStat.getIngestStats() .getProcessorStats().entrySet()) { pipelineIds.add(processorStats.getKey()); for (org.elasticsearch.ingest.IngestStats.ProcessorStat stat : processorStats.getValue()) { stats.compute(stat.getType(), (k, v) -> { if (v == null) { return new long[] { stat.getStats().getIngestCount(), stat.getStats().getIngestFailedCount() }; } else { v[0] += stat.getStats().getIngestCount(); v[1] += stat.getStats().getIngestFailedCount(); return v; } }); } } } } this.pipelineCount = pipelineIds.size(); this.stats = Collections.unmodifiableMap(stats); } @Override public XContentBuilder toXContent(final XContentBuilder builder, final Params params) throws IOException { builder.startObject("ingest"); { builder.field("number_of_pipelines", pipelineCount); builder.startObject("processor_stats"); for (Map.Entry<String, long[]> stat : stats.entrySet()) { builder.startObject(stat.getKey()); builder.field("count", stat.getValue()[0]); builder.field("fail_count", stat.getValue()[1]); builder.endObject(); } builder.endObject(); } builder.endObject(); return builder; }	use map#copyof(...) here instead?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); { for (final String index : indices) { builder.startObject(index); { builder.startObject("aliases"); List<AliasMetaData> indexAliases = aliases.get(index); if (indexAliases != null) { for (final AliasMetaData alias : indexAliases) { AliasMetaData.Builder.toXContent(alias, builder, params); } } builder.endObject(); ImmutableOpenMap<String, MappingMetaData> indexMappings = mappings.get(index); // the default on 6.x should be true to include types in the response boolean includeTypeName = params.paramAsBoolean(INCLUDE_TYPE_NAME_PARAMETER, true); if (includeTypeName) { builder.startObject("mappings"); if (indexMappings != null) { for (final ObjectObjectCursor<String, MappingMetaData> typeEntry : indexMappings) { builder.field(typeEntry.key); builder.map(typeEntry.value.sourceAsMap()); } } builder.endObject(); } else { MappingMetaData mappings = null; for (final ObjectObjectCursor<String, MappingMetaData> typeEntry : indexMappings) { if (typeEntry.key.equals(MapperService.DEFAULT_MAPPING) == false) { assert mappings == null; mappings = typeEntry.value; } } if (mappings == null) { // no mappings yet builder.startObject("mappings").endObject(); } else { builder.field("mappings", mappings.sourceAsMap()); } } builder.startObject("settings"); Settings indexSettings = settings.get(index); if (indexSettings != null) { indexSettings.toXContent(builder, params); } builder.endObject(); Settings defaultIndexSettings = defaultSettings.get(index); if (defaultIndexSettings != null && defaultIndexSettings.isEmpty() == false) { builder.startObject("defaults"); defaultIndexSettings.toXContent(builder, params); builder.endObject(); } } builder.endObject(); } } builder.endObject(); return builder; }	i now have this default setting defined in the constant baseresthandler.default_include_type_name_policy
public Object[] value(SearchHit hit, Function<String, ExtractedField> fieldExtractor) { List<String> inputFields = getInputFieldNames(); Map<String, Object> inputs = new HashMap<>(inputFields.size(), 1.0f); for (String field : inputFields) { ExtractedField extractedField = fieldExtractor.apply(field); if (extractedField == null) { return new Object[0]; } Object[] values = extractedField.value(hit); if (values == null || values.length == 0) { continue; } final Object value = values[0]; if (values.length == 1 && (isValidValue(value))) { inputs.put(field, value); } } preProcessor.process(inputs); return preProcessor.outputFields().stream().sorted().map(inputs::get).toArray(); }	if the preprocessor already returns the output fields sorted, we shouldn't need to sort here. now unfortunately there is not sorted list interface. but as we're dealing with fields, if we want to enforce this to the preprocessor interface we should switch to sortedset.
@Override public void hitExecute(SearchContext context, HitContext hitContext) { Map<String, HighlightField> highlightFields = newHashMap(); for (SearchContextHighlight.Field field : context.highlight().fields()) { List<String> fieldNamesToHighlight; if (Regex.isSimpleMatchPattern(field.field())) { DocumentMapper documentMapper = context.mapperService().documentMapper(hitContext.hit().type()); fieldNamesToHighlight = documentMapper.mappers().simpleMatchToFullName(field.field()); } else { fieldNamesToHighlight = ImmutableList.of(field.field()); } if (context.highlight().forceSource(field)) { SourceFieldMapper sourceFieldMapper = context.mapperService().documentMapper(hitContext.hit().type()).sourceMapper(); if (!sourceFieldMapper.enabled()) { throw new IllegalArgumentException("source is forced for fields " + fieldNamesToHighlight + " but type [" + hitContext.hit().type() + "] has disabled _source"); } } boolean fieldNameContainsWildcards = field.field().contains("*"); for (String fieldName : fieldNamesToHighlight) { FieldMapper fieldMapper = getMapperForField(fieldName, context, hitContext); if (fieldMapper == null) { continue; } String highlighterType = field.fieldOptions().highlighterType(); if (highlighterType == null) { boolean useFastVectorHighlighter = fieldMapper.fieldType().storeTermVectors() && fieldMapper.fieldType().storeTermVectorOffsets() && fieldMapper.fieldType().storeTermVectorPositions(); if (useFastVectorHighlighter) { highlighterType = "fvh"; } else if (fieldMapper.fieldType().indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) { highlighterType = "postings"; } else { highlighterType = "plain"; } } Highlighter highlighter = highlighters.get(highlighterType); if (highlighter == null) { throw new IllegalArgumentException("unknown highlighter type [" + highlighterType + "] for the field [" + fieldName + "]"); } Query highlightQuery = field.fieldOptions().highlightQuery() == null ? context.parsedQuery().query() : field.fieldOptions().highlightQuery(); HighlighterContext highlighterContext = new HighlighterContext(fieldName, field, fieldMapper, context, hitContext, highlightQuery); if (highlighter.cannotHighlight(fieldMapper) && fieldNameContainsWildcards) { // if several fieldnames matched the wildcard then we want to skip those that we cannot highlight continue; } try { HighlightField highlightField = highlighter.highlight(highlighterContext); if (highlightField != null) { highlightFields.put(highlightField.name(), highlightField); } } catch (FetchPhaseExecutionException e) { if (fieldNameContainsWildcards && e.getCause() instanceof BytesRefHash.MaxBytesLengthExceededException) { // we only get to this field because of wildcard expansion, ignore the error. // this can happen if for example a field is not_analyzed and ignore_above option is set. // the field will be ignored when indexing but the huge term is still in the source and // the plain highlighter will parse the source and try to analyze it. continue; } else { throw e; } } } } hitContext.hit().highlightFields(highlightFields); }	thinking out loud, maybe i am getting confused, but in order for a field to get highlighted, doesn't it need to be stored too or we need to have the _source at least? but metadata fields, which match * are not part of the _source hence they need to be stored or excluded from highlighting by definition. i have the feeling we should do something more to address that...
@Override public void hitExecute(SearchContext context, HitContext hitContext) { Map<String, HighlightField> highlightFields = newHashMap(); for (SearchContextHighlight.Field field : context.highlight().fields()) { List<String> fieldNamesToHighlight; if (Regex.isSimpleMatchPattern(field.field())) { DocumentMapper documentMapper = context.mapperService().documentMapper(hitContext.hit().type()); fieldNamesToHighlight = documentMapper.mappers().simpleMatchToFullName(field.field()); } else { fieldNamesToHighlight = ImmutableList.of(field.field()); } if (context.highlight().forceSource(field)) { SourceFieldMapper sourceFieldMapper = context.mapperService().documentMapper(hitContext.hit().type()).sourceMapper(); if (!sourceFieldMapper.enabled()) { throw new IllegalArgumentException("source is forced for fields " + fieldNamesToHighlight + " but type [" + hitContext.hit().type() + "] has disabled _source"); } } boolean fieldNameContainsWildcards = field.field().contains("*"); for (String fieldName : fieldNamesToHighlight) { FieldMapper fieldMapper = getMapperForField(fieldName, context, hitContext); if (fieldMapper == null) { continue; } String highlighterType = field.fieldOptions().highlighterType(); if (highlighterType == null) { boolean useFastVectorHighlighter = fieldMapper.fieldType().storeTermVectors() && fieldMapper.fieldType().storeTermVectorOffsets() && fieldMapper.fieldType().storeTermVectorPositions(); if (useFastVectorHighlighter) { highlighterType = "fvh"; } else if (fieldMapper.fieldType().indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) { highlighterType = "postings"; } else { highlighterType = "plain"; } } Highlighter highlighter = highlighters.get(highlighterType); if (highlighter == null) { throw new IllegalArgumentException("unknown highlighter type [" + highlighterType + "] for the field [" + fieldName + "]"); } Query highlightQuery = field.fieldOptions().highlightQuery() == null ? context.parsedQuery().query() : field.fieldOptions().highlightQuery(); HighlighterContext highlighterContext = new HighlighterContext(fieldName, field, fieldMapper, context, hitContext, highlightQuery); if (highlighter.cannotHighlight(fieldMapper) && fieldNameContainsWildcards) { // if several fieldnames matched the wildcard then we want to skip those that we cannot highlight continue; } try { HighlightField highlightField = highlighter.highlight(highlighterContext); if (highlightField != null) { highlightFields.put(highlightField.name(), highlightField); } } catch (FetchPhaseExecutionException e) { if (fieldNameContainsWildcards && e.getCause() instanceof BytesRefHash.MaxBytesLengthExceededException) { // we only get to this field because of wildcard expansion, ignore the error. // this can happen if for example a field is not_analyzed and ignore_above option is set. // the field will be ignored when indexing but the huge term is still in the source and // the plain highlighter will parse the source and try to analyze it. continue; } else { throw e; } } } } hitContext.hit().highlightFields(highlightFields); }	is this a plain highlighter only problem? not sure, but if so maybe we could handle it in the plainhighlighter directly rather than in the highlightphase?
@Override public void hitExecute(SearchContext context, HitContext hitContext) { Map<String, HighlightField> highlightFields = newHashMap(); for (SearchContextHighlight.Field field : context.highlight().fields()) { List<String> fieldNamesToHighlight; if (Regex.isSimpleMatchPattern(field.field())) { DocumentMapper documentMapper = context.mapperService().documentMapper(hitContext.hit().type()); fieldNamesToHighlight = documentMapper.mappers().simpleMatchToFullName(field.field()); } else { fieldNamesToHighlight = ImmutableList.of(field.field()); } if (context.highlight().forceSource(field)) { SourceFieldMapper sourceFieldMapper = context.mapperService().documentMapper(hitContext.hit().type()).sourceMapper(); if (!sourceFieldMapper.enabled()) { throw new IllegalArgumentException("source is forced for fields " + fieldNamesToHighlight + " but type [" + hitContext.hit().type() + "] has disabled _source"); } } boolean fieldNameContainsWildcards = field.field().contains("*"); for (String fieldName : fieldNamesToHighlight) { FieldMapper fieldMapper = getMapperForField(fieldName, context, hitContext); if (fieldMapper == null) { continue; } String highlighterType = field.fieldOptions().highlighterType(); if (highlighterType == null) { boolean useFastVectorHighlighter = fieldMapper.fieldType().storeTermVectors() && fieldMapper.fieldType().storeTermVectorOffsets() && fieldMapper.fieldType().storeTermVectorPositions(); if (useFastVectorHighlighter) { highlighterType = "fvh"; } else if (fieldMapper.fieldType().indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) { highlighterType = "postings"; } else { highlighterType = "plain"; } } Highlighter highlighter = highlighters.get(highlighterType); if (highlighter == null) { throw new IllegalArgumentException("unknown highlighter type [" + highlighterType + "] for the field [" + fieldName + "]"); } Query highlightQuery = field.fieldOptions().highlightQuery() == null ? context.parsedQuery().query() : field.fieldOptions().highlightQuery(); HighlighterContext highlighterContext = new HighlighterContext(fieldName, field, fieldMapper, context, hitContext, highlightQuery); if (highlighter.cannotHighlight(fieldMapper) && fieldNameContainsWildcards) { // if several fieldnames matched the wildcard then we want to skip those that we cannot highlight continue; } try { HighlightField highlightField = highlighter.highlight(highlighterContext); if (highlightField != null) { highlightFields.put(highlightField.name(), highlightField); } } catch (FetchPhaseExecutionException e) { if (fieldNameContainsWildcards && e.getCause() instanceof BytesRefHash.MaxBytesLengthExceededException) { // we only get to this field because of wildcard expansion, ignore the error. // this can happen if for example a field is not_analyzed and ignore_above option is set. // the field will be ignored when indexing but the huge term is still in the source and // the plain highlighter will parse the source and try to analyze it. continue; } else { throw e; } } } } hitContext.hit().highlightFields(highlightFields); }	i think we should try and use the cannothighlight method also when it comes to choosing the type of highlighter based on the field mapper (when no highlighter type is specified). thoughts?
public Executable assemble(List<List<Attribute>> listOfKeys, List<PhysicalPlan> plans, Attribute timestamp, Attribute tiebreaker, OrderDirection direction, TimeValue maxSpan, Limit limit) { FieldExtractorRegistry extractorRegistry = new FieldExtractorRegistry(); boolean descending = direction == OrderDirection.DESC; // fields HitExtractor tsExtractor = timestampExtractor(hitExtractor(timestamp, extractorRegistry)); HitExtractor tbExtractor = Expressions.isPresent(tiebreaker) ? hitExtractor(tiebreaker, extractorRegistry) : null; // NB: since there's no aliasing inside EQL, the attribute name is the same as the underlying field name String timestampName = Expressions.name(timestamp); String tiebreakerName = Expressions.isPresent(tiebreaker) ? Expressions.name(tiebreaker) : null; Criterion<QueryRequest> base = null; // secondary criteria List<Criterion<BoxedQueryRequest>> criteria = new ArrayList<>(plans.size() - 1); // build a criterion for each query for (int i = 0; i < plans.size(); i++) { List<Attribute> keys = listOfKeys.get(i); List<HitExtractor> keyExtractors = hitExtractors(keys, extractorRegistry); PhysicalPlan query = plans.get(i); // search query if (query instanceof EsQueryExec) { QueryRequest original = ((EsQueryExec) query).queryRequest(session); BoxedQueryRequest boxedRequest = new BoxedQueryRequest(original, timestampName, tiebreakerName); Criterion<BoxedQueryRequest> criterion = new Criterion<>(i, boxedRequest, keyExtractors, tsExtractor, tbExtractor, i> 0 && descending); criteria.add(criterion); } else { // until if (i != plans.size() - 1) { throw new EqlIllegalArgumentException("Expected a query but got [{}]", query.getClass()); } else { criteria.add(null); } } } int completionStage = criteria.size() - 1; Matcher matcher = new Matcher(completionStage, maxSpan, limit); TumblingWindow w = new TumblingWindow(new BasicQueryClient(session), criteria.subList(0, completionStage), criteria.get(completionStage), matcher); return w; }	this one doesn't seem to actually be used.
public static <T> Set<T> union(Set<T> left, Set<T> right) { Objects.requireNonNull(left); Objects.requireNonNull(right); Set<T> union = new HashSet<>(left); union.addAll(right); return union; }	not directly related to you change, but i'm seeing some inconsistencies wrt whether the return value of this method is assumed mutable. reconfigurator seems to assume it is immutable and creates a treeset out of it while restgetmappingaction modidies the result of this method. the reason why i was looking at this was to figure out whether we should actually have a tosortedset method in this class, or whether what we actually needed was a tounmodifiablesortedset.
public void testKeyOrderingLong() { CompositeAggregation composite = mock(CompositeAggregation.class); when(composite.getBuckets()).thenAnswer((Answer<List<CompositeAggregation.Bucket>>) invocationOnMock -> { List<CompositeAggregation.Bucket> foos = new ArrayList<>(); CompositeAggregation.Bucket bucket = mock(CompositeAggregation.Bucket.class); Map<String, Object> keys = Maps.newLinkedHashMapWithExpectedSize(3); keys.put("foo.date_histogram", 123L); char[] charArray = new char[IndexWriter.MAX_TERM_LENGTH]; Arrays.fill(charArray, 'a'); keys.put("bar.terms", new String(charArray)); keys.put("abc.histogram", 1.9); keys = shuffleMap((LinkedHashMap<String, Object>) keys, Collections.emptySet()); when(bucket.getKey()).thenReturn(keys); List<Aggregation> list = new ArrayList<>(3); InternalNumericMetricsAggregation.SingleValue mockAgg = mock(InternalNumericMetricsAggregation.SingleValue.class); when(mockAgg.getName()).thenReturn("123"); list.add(mockAgg); InternalNumericMetricsAggregation.SingleValue mockAgg2 = mock(InternalNumericMetricsAggregation.SingleValue.class); when(mockAgg2.getName()).thenReturn("abc"); list.add(mockAgg2); InternalNumericMetricsAggregation.SingleValue mockAgg3 = mock(InternalNumericMetricsAggregation.SingleValue.class); when(mockAgg3.getName()).thenReturn("yay"); list.add(mockAgg3); Collections.shuffle(list, random()); Aggregations aggs = new Aggregations(list); when(bucket.getAggregations()).thenReturn(aggs); when(bucket.getDocCount()).thenReturn(1L); foos.add(bucket); return foos; }); GroupConfig groupConfig = new GroupConfig(randomDateHistogramGroupConfig(random()), new HistogramGroupConfig(1, "abc"), null); List<IndexRequest> docs = IndexerUtils.processBuckets(composite, "foo", new RollupIndexerJobStats(), groupConfig, "foo") .collect(Collectors.toList()); assertThat(docs.size(), equalTo(1)); assertThat(docs.get(0).id(), equalTo("foo$VAFKZpyaEqYRPLyic57_qw")); }	this and above type cast could be avoided. see my other comment
@Override public void onFailure(Exception e) { if ((e instanceof ResourceNotFoundException && Strings.isAllOrWildcard(new String[]{request.getJobId()})) == false) { final int slot = counter.incrementAndGet(); failures.set(slot - 1, e); if (slot == numberOfJobs) { sendResponseOrFailure(request.getJobId(), listener, failures); } } }	i think this line needs to come before the if statement, otherwise we won't respond to the caller if the race occurs.
@Override public void onFailure(Exception e) { if ((e instanceof ResourceNotFoundException && Strings.isAllOrWildcard(new String[]{request.getJobId()})) == false) { final int slot = counter.incrementAndGet(); failures.set(slot - 1, e); if (slot == numberOfJobs) { sendResponseOrFailure(request.getJobId(), listener, failures); } } }	i think this if statement needs to be after the scope of if ((e instanceof resourcenotfoundexception && strings.isallorwildcard(new string[]{request.getjobid()})) == false) { ends, otherwise we won't respond to the caller if the race occurs on the very last job.
@Override public void onFailure(Exception e) { if ((e instanceof ResourceNotFoundException && Strings.isAllOrWildcard(new String[]{request.getDatafeedId()})) == false) { final int slot = counter.incrementAndGet(); failures.set(slot - 1, e); if (slot == startedDatafeeds.size()) { sendResponseOrFailure(request.getDatafeedId(), listener, failures); } } }	same problem as for jobs - only failures.set(slot - 1, e); should be within the scope of the newly added if.
public void testKeepTranslogAfterGlobalCheckpoint() throws Exception { IOUtils.close(engine, store); final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.UNASSIGNED_SEQ_NO); final BiFunction<EngineConfig, SeqNoStats, SequenceNumbersService> seqNoServiceSupplier = (config, seqNoStats) -> new SequenceNumbersService( config.getShardId(), config.getAllocationId(), config.getIndexSettings(), seqNoStats.getMaxSeqNo(), seqNoStats.getLocalCheckpoint(), seqNoStats.getGlobalCheckpoint()) { @Override public long getGlobalCheckpoint() { return globalCheckpoint.get(); } }; final IndexSettings indexSettings = new IndexSettings(defaultSettings.getIndexMetaData(), defaultSettings.getNodeSettings(), defaultSettings.getScopedSettings()); IndexMetaData.Builder builder = IndexMetaData.builder(indexSettings.getIndexMetaData()); builder.settings(Settings.builder().put(indexSettings.getSettings()) .put(IndexSettings.INDEX_TRANSLOG_RETENTION_AGE_SETTING.getKey(), randomBoolean() ? "-1" : TimeValue.timeValueMillis(randomNonNegativeLong()).getStringRep()) .put(IndexSettings.INDEX_TRANSLOG_RETENTION_SIZE_SETTING.getKey(), randomBoolean() ? "-1" : new ByteSizeValue(randomNonNegativeLong()).toString()) ); indexSettings.updateIndexMetaData(builder.build()); final Path translogPath = createTempDir(); store = createStore(); try (InternalEngine engine = new InternalEngine(config(indexSettings, store, translogPath, NoMergePolicy.INSTANCE, null), seqNoServiceSupplier)) { int numDocs = scaledRandomIntBetween(10, 100); int uncommittedOps = 0; for (int i = 0; i < numDocs; i++) { ParseContext.Document document = testDocumentWithTextField(); document.add(new Field(SourceFieldMapper.NAME, BytesReference.toBytes(B_1), SourceFieldMapper.Defaults.FIELD_TYPE)); engine.index(indexForDoc(testParsedDocument(Integer.toString(i), null, document, B_1, null))); uncommittedOps++; if (frequently()) { globalCheckpoint.set(randomIntBetween( Math.toIntExact(engine.seqNoService().getGlobalCheckpoint()), Math.toIntExact(engine.seqNoService().getLocalCheckpoint()))); } if (frequently()) { engine.flush(randomBoolean(), true); uncommittedOps = 0; } if (rarely()) { engine.rollTranslogGeneration(); } assertThat(engine.getTranslog().uncommittedOperations(), equalTo(uncommittedOps)); try (Translog.Snapshot snapshot = engine.getTranslog().newSnapshot()) { assertThat(snapshot, containsSeqNoRange(Math.max(0, globalCheckpoint.get() + 1), engine.seqNoService().getLocalCheckpoint())); } } engine.flush(randomBoolean(), true); } // Reopen engine to test onInit with existing index commits. try (InternalEngine engine = new InternalEngine(config(indexSettings, store, translogPath, NoMergePolicy.INSTANCE, null), seqNoServiceSupplier)) { try (Translog.Snapshot snapshot = engine.getTranslog().newSnapshot()) { assertThat(snapshot, containsSeqNoRange(Math.max(0, globalCheckpoint.get() + 1), engine.seqNoService().getLocalCheckpoint())); } } }	hmm.. not sure about this - i think it's very unlikely we'll end up with a small value that will cause rollovers. wdyt?
private long readFileBytes(String fileName, BytesReference reference) throws IOException { Releasable lock = keyedLock.tryAcquire(fileName); if (lock == null) { throw new IllegalStateException("can't read from the same file on the same session concurrently"); } try (Releasable releasable = lock) { final IndexInput indexInput = cachedInputs.computeIfAbsent(fileName, f -> { try { return commitRef.getIndexCommit().getDirectory().openInput(fileName, IOContext.READONCE); } catch (IOException e) { throw new UncheckedIOException(e); } }); BytesRefIterator refIterator = reference.iterator(); BytesRef ref; while ((ref = refIterator.next()) != null) { byte[] refBytes = ref.bytes; indexInput.readBytes(refBytes, 0, refBytes.length); } long offsetAfterRead = indexInput.getFilePointer(); if (offsetAfterRead == indexInput.length()) { cachedInputs.remove(fileName); IOUtils.closeWhileHandlingException(indexInput); } return offsetAfterRead; } }	lets not swallow exceptions here. just close it.
@Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } Request other = (Request) obj; return Objects.equals(filter, other.filter); } } public static class RequestBuilder extends ActionRequestBuilder<Request, Response> { public RequestBuilder(ElasticsearchClient client) { super(client, INSTANCE, new Request()); } } public static class Response extends ActionResponse implements ToXContentObject { private MlFilter filter; Response() { } public Response(MlFilter calendar) { this.filter = calendar; } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); filter = new MlFilter(in); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); filter.writeTo(out); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { return filter.toXContent(builder, params); } public MlFilter getFilter() { return filter; } @Override public int hashCode() { return Objects.hash(filter); } @Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } Response other = (Response) obj; return Objects.equals(filter, other.filter); }	rename calendar -> filter
@Override public Failure readFrom(StreamInput in) throws IOException { return new Failure(in.readString(), in.readString(), in.readOptionalString(), in.readThrowable()); }	just use a new failure(in); here
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); if (in.getVersion().before(Version.V_2_3_0)) { type = in.readString(); } else { type = in.readOptionalString(); } id = in.readOptionalString(); routing = in.readOptionalString(); parent = in.readOptionalString(); timestamp = in.readOptionalString(); if (in.getVersion().before(Version.V_2_2_0)) { long ttl = in.readLong(); if (ttl == -1) { this.ttl = null; } else { ttl(ttl); } } else { ttl = in.readBoolean() ? TimeValue.readTimeValue(in) : null; } source = in.readBytesReference(); opType = OpType.fromId(in.readByte()); refresh = in.readBoolean(); version = in.readLong(); versionType = VersionType.fromValue(in.readByte()); autoGeneratedId = in.readBoolean(); }	i believe this to be the one api change that needs backwards compatibility in this whole proposal. the reindex api itself is entirely new so it will fail to run at all if it is sent to an old node version. a 2.3 versioned node should be able to coordinate a reindex for any 2.x versioned cluster though i haven't tested it.
public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) { SearchRequest searchRequest = new SearchRequest(); RestSearchAction.parseSearchRequest(searchRequest, request, parseFieldMatcher, null); client.search(searchRequest, new RestStatusToXContentListener<SearchResponse>(channel)); }	this diverges a bit from how master works because most of the parsing can't be done on the coordinating node. so this change is smaller than master. here we just change this api so that it can modify a searchrequest rather than create one. this lets reindex create a searchrequest, change the defaults on it to be reindex's defaults, and then pass the search request through this api so that rest parameters still work in the same way. mostly. reindex does quite a bit of hacking on the body to transform bits of it's api into one that this search request likes. we'll get to that later.
public final RestResponse buildResponse(Response response, XContentBuilder builder) throws Exception { builder.startObject(); response.toXContent(builder, channel.request()); builder.endObject(); return new BytesRestResponse(getStatus(response), builder); }	this seemed like a simple place to implement request status. i'm frankly surprised no one did this before. anyway, this is exactly how it works in master.
@Override public void getSnapshotInfo(GetSnapshotInfoContext context) { final List<SnapshotId> snapshotIds = context.snapshotIds(); assert snapshotIds.size() == 1 && SNAPSHOT_ID.equals(snapshotIds.iterator().next()) : "RemoteClusterRepository only supports " + SNAPSHOT_ID + " as the SnapshotId but saw " + snapshotIds; Client remoteClient = getRemoteClusterClient(); ClusterStateResponse response = remoteClient.admin().cluster().prepareState().clear().setMetadata(true).setNodes(true) .get(ccrSettings.getRecoveryActionTimeout()); Metadata metadata = response.getState().metadata(); ImmutableOpenMap<String, IndexMetadata> indicesMap = metadata.indices(); ArrayList<String> indices = new ArrayList<>(indicesMap.size()); indicesMap.keysIt().forEachRemaining(indices::add); context.onResponse( new SnapshotInfo( SNAPSHOT_ID, indices, new ArrayList<>(metadata.dataStreams().keySet()), Collections.emptyList(), response.getState().getNodes().getMaxNodeVersion(), SnapshotState.SUCCESS ) ); }	oh oops i just saw this. think we need to take steps to put this on the snapshot_meta thread now.
* @param isHidden Whether or not this is a hidden index */ public boolean validateDotIndex(String index, @Nullable Boolean isHidden) { boolean isSystem = false; if (index.charAt(0) == '.') { SystemIndexDescriptor matchingDescriptor = systemIndices.findMatchingDescriptor(index); if (matchingDescriptor == null && (isHidden == null || isHidden == Boolean.FALSE)) { deprecationLogger.deprecate("index_name_starts_with_dot", "index name [{}] starts with a dot '.', in the next major version, index names " + "starting with a dot are reserved for hidden indices and system indices", index); } else { isSystem = true; } } return isSystem; }	this if/else, as currently written, will cause hidden indices to be marked as system indices. when a hidden index is created, matchingdescriptor is null and ishidden is true, which does not match the if clause, and so falls triggers the else, which marks the index as a system index. changing this to the following fixes the issue: java if (index.charat(0) == '.') { systemindexdescriptor matchingdescriptor = systemindices.findmatchingdescriptor(index); if (matchingdescriptor != null) { logger.trace("index [{}] is a system index because it matches index pattern [{}] with description [{}]", index, matchingdescriptor.getindexpattern(), matchingdescriptor.getdescription()); issystem = true; } else if (ishidden) { logger.trace("index [{}] is a hidden index", index); } else { deprecationlogger.deprecate("index_name_starts_with_dot", "index name [{}] starts with a dot '.', in the next major version, index names " + "starting with a dot are reserved for hidden indices and system indices", index); } } adding asserts to check the boolean returned by this method in testvalidatedotindex catches the issue as well. it might be a good idea to throw an exception or at least emit a warning if (ishidden && issystem) as well.
private void handleJoinRequest(JoinRequest joinRequest, JoinHelper.JoinCallback joinCallback) { assert Thread.holdsLock(mutex) == false; assert getLocalNode().isMasterNode() : getLocalNode() + " received a join but is not master-eligible"; logger.trace("handleJoinRequest: as {}, handling {}", mode, joinRequest); if (singleNodeDiscovery && joinRequest.getSourceNode().equals(getLocalNode()) == false) { joinCallback.onFailure(new IllegalStateException("cannot join node with single-node discovery")); return; } transportService.connectToNode(joinRequest.getSourceNode()); final ClusterState stateForJoinValidation = getStateForMasterService(); if (stateForJoinValidation.nodes().isLocalNodeElectedMaster()) { onJoinValidators.forEach(a -> a.accept(joinRequest.getSourceNode(), stateForJoinValidation)); if (stateForJoinValidation.getBlocks().hasGlobalBlock(STATE_NOT_RECOVERED_BLOCK) == false) { // we do this in a couple of places including the cluster update thread. This one here is really just best effort // to ensure we fail as fast as possible. JoinTaskExecutor.ensureMajorVersionBarrier(joinRequest.getSourceNode().getVersion(), stateForJoinValidation.getNodes().getMinNodeVersion()); } sendValidateJoinRequest(stateForJoinValidation, joinRequest, joinCallback); } else { processJoinRequest(joinRequest, joinCallback); } }	suggest cannot join node with [discovery.type] set to [single-node] so that it's clearer what setting to look for.
private void returnIfNeeded() { if (completionCounter.decrementAndGet() == 0) { List<ShardResponse> responses = new ArrayList<>(); List<Failure> failureList = new ArrayList<>(); int total = groups.totalSize(); int pending = 0; int successful = 0; for (int i = 0; i < shardsResponses.length(); i++) { ShardActionResult shardActionResult = shardsResponses.get(i); if (shardActionResult == null) { assert !accumulateExceptions(); continue; } if (shardActionResult.isFailure()) { assert accumulateExceptions() && shardActionResult.shardFailure != null; // Set the status here, since it is a failure on primary shard // The failure doesn't include the node id, maybe add it to ShardOperationFailedException... ShardOperationFailedException sf = shardActionResult.shardFailure; ShardIterator thisShardIterator = null; ShardId shardId = new ShardId(sf.index(), sf.shardId()); for (ShardIterator shardIterator : groups) { if (shardIterator.shardId().equals(shardId)) { thisShardIterator = shardIterator; break; } } assert thisShardIterator != null; for (ShardRouting shardRouting = thisShardIterator.nextOrNull(); shardRouting != null; shardRouting = thisShardIterator.nextOrNull()) { if (shardRouting.primary()) { failureList.add(new Failure(sf.index(), sf.shardId(), shardRouting.currentNodeId(), sf.reason(), sf.status(), true)); } else { failureList.add(new Failure(sf.index(), sf.shardId(), shardRouting.currentNodeId(), "Not executed because operation failed on primary shard", sf.status(), false)); } } } else { pending += shardActionResult.shardResponse.getShardInfo().getPending(); successful += shardActionResult.shardResponse.getShardInfo().getSuccessful(); failureList.addAll(Arrays.asList(shardActionResult.shardResponse.getShardInfo().getFailures())); responses.add(shardActionResult.shardResponse); } } assert failureList.size() == 0 || numShardGroupFailures(failureList) == failureCounter.get(); final Failure[] failures; if (failureList.isEmpty()) { failures = ActionWriteResponse.EMPTY; } else { failures = failureList.toArray(new Failure[failureList.size()]); } listener.onResponse(newResponseInstance(request, responses, new ActionWriteResponse.ShardInfo(total, successful, pending, failures))); } }	why not capture the shard info during the failure handling? then we don't need to search for the iterator..
public RestResponse buildResponse(DeleteResponse result, XContentBuilder builder) throws Exception { ActionWriteResponse.ShardInfo shardInfo = result.getShardInfo(); builder.startObject().field(Fields.FOUND, result.isFound()) .field(Fields._INDEX, result.getIndex()) .field(Fields._TYPE, result.getType()) .field(Fields._ID, result.getId()) .field(Fields._VERSION, result.getVersion()) .value(shardInfo) .endObject(); RestStatus status = shardInfo.status(); if (!result.isFound()) { status = NOT_FOUND; } return new BytesRestResponse(status, builder); }	i think we want shardinfo.toxcontent(builder, request); here? like this we loose the request as params
public RestResponse buildResponse(IndexResponse response, XContentBuilder builder) throws Exception { builder.startObject(); ActionWriteResponse.ShardInfo shardInfo = response.getShardInfo(); builder.field(Fields._INDEX, response.getIndex()) .field(Fields._TYPE, response.getType()) .field(Fields._ID, response.getId()) .field(Fields._VERSION, response.getVersion()) .value(shardInfo) .field(Fields.CREATED, response.isCreated()); builder.endObject(); RestStatus status = shardInfo.status(); if (response.isCreated()) { status = CREATED; } return new BytesRestResponse(status, builder); }	i think we want shardinfo.toxcontent(builder, request); here? like this we loose the request as params
public RestResponse buildResponse(UpdateResponse response, XContentBuilder builder) throws Exception { builder.startObject(); ActionWriteResponse.ShardInfo shardInfo = response.getShardInfo(); builder.field(Fields._INDEX, response.getIndex()) .field(Fields._TYPE, response.getType()) .field(Fields._ID, response.getId()) .field(Fields._VERSION, response.getVersion()) .value(shardInfo); if (response.getGetResult() != null) { builder.startObject(Fields.GET); response.getGetResult().toXContentEmbedded(builder, request); builder.endObject(); } builder.endObject(); RestStatus status = shardInfo.status(); if (response.isCreated()) { status = CREATED; } return new BytesRestResponse(status, builder); }	i think we want shardinfo.toxcontent(builder, request); here? like this we loose the request as params
LogEntryBuilder withForwardedFor(ThreadContext threadContext) { final String forwardedFor = threadContext.getHeader(AuditTrail.X_FORWARDED_FOR_HEADER); if (forwardedFor != null) { with(FORWARDED_FOR_FIELD_NAME, forwardedFor.split("\\\\\\\\s+|,\\\\\\\\s*")); } return this; }	why allow splitting based on a space alone? the format should always be delimited by a comma followed by a space.
private void checkRealms(Iterable<Realm> realms, Settings globalSettings) { final Map<String, Settings> settingsByRealm = RealmSettings.getRealmSettings(globalSettings); for (Realm realm : realms) { final Settings realmSettings = settingsByRealm.get(realm.name()); if (realmSettings != null && DelegatedAuthorizationSettings.AUTHZ_REALMS.exists(realmSettings)) { throw new IllegalArgumentException("cannot use realm [" + realm + "] as an authorization realm - it is already delegating authorization to [" + DelegatedAuthorizationSettings.AUTHZ_REALMS.get(realmSettings) + "]"); } } }	nit: rename this method name to describe what it checks, would also rename realms as that confused me while reading the code.
public void roles(Set<String> roleNames, FieldPermissionsCache fieldPermissionsCache, ActionListener<Role> roleActionListener) { Role existing = roleCache.get(roleNames); if (existing != null) { roleActionListener.onResponse(existing); } else { final long invalidationCounter = numInvalidation.get(); roleDescriptors(roleNames, ActionListener.wrap( rolesRetrievalResult -> { final boolean missingRoles = rolesRetrievalResult.getMissingRoles().isEmpty() == false; if (missingRoles) { logger.debug("Could not find roles with names {}", rolesRetrievalResult.getMissingRoles()); } final Set<RoleDescriptor> effectiveDescriptors; if (licenseState.isDocumentAndFieldLevelSecurityAllowed()) { effectiveDescriptors = rolesRetrievalResult.getRoleDescriptors(); } else { effectiveDescriptors = rolesRetrievalResult.getRoleDescriptors().stream() .filter((rd) -> rd.isUsingDocumentOrFieldLevelSecurity() == false) .collect(Collectors.toSet()); } logger.trace("Building role from descriptors [{}] for names [{}]", effectiveDescriptors, roleNames); buildRoleFromDescriptors(effectiveDescriptors, fieldPermissionsCache, privilegeStore, ActionListener.wrap(role -> { if (role != null) { if (rolesRetrievalResult.isFailure() == false) { try (ReleasableLock ignored = readLock.acquire()) { /* this is kinda spooky. We use a read/write lock to ensure we don't modify the cache if we hold * the write lock (fetching stats for instance - which is kinda overkill?) but since we fetching * stuff in an async fashion we need to make sure that if the cache got invalidated since we * started the request we don't put a potential stale result in the cache, hence the * numInvalidation.get() comparison to the number of invalidation when we started. we just try to * be on the safe side and don't cache potentially stale results */ if (invalidationCounter == numInvalidation.get()) { roleCache.computeIfAbsent(roleNames, (s) -> role); } } for (String missingRole : rolesRetrievalResult.getMissingRoles()) { negativeLookupCache.computeIfAbsent(missingRole, s -> Boolean.TRUE); } } } roleActionListener.onResponse(role); }, roleActionListener::onFailure)); }, roleActionListener::onFailure)); } }	i think merging the two if will not hurt readability.
public final void testFetchMany() throws IOException { MapperService mapperService = randomFetchTestMapper(); try { MappedFieldType ft = mapperService.fieldType("field"); int count = between(2, 10); List<Object> values = new ArrayList<>(count); while (values.size() < count) { // values should be unique since we also get de-duplication fetching from docvalues values.add(randomValueOtherThanMany(v -> values.contains(v), () -> generateRandomInputValue(ft))); } assertFetch(mapperService, "field", values, randomFetchTestFormat()); } finally { assertParseMinimalWarnings(); } }	i'm not entirely sure if it is better to fix this in the setup or in the assertion in the end. we might also de-dup the results from the native fetchers before comparing, wdyt?
public void onResponse(SearchResponse response) { List<Resource> docs = new ArrayList<>(); Set<String> foundResourceIds = new HashSet<>(); long totalHitCount = response.getHits().getTotalHits().value; for (SearchHit hit : response.getHits().getHits()) { BytesReference docSource = hit.getSourceRef(); try (InputStream stream = docSource.streamInput(); XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser( xContentRegistry, LoggingDeprecationHandler.INSTANCE, stream)) { Resource resource = parse(parser); // Do not include a resource with the same ID twice if (foundResourceIds.contains(extractIdFromResource(resource)) == false) { docs.add(resource); foundResourceIds.add(extractIdFromResource(resource)); } } catch (IOException e) { this.onFailure(e); } } ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, request.isAllowNoResources()); requiredMatches.filterMatchedIds(foundResourceIds); if (requiredMatches.hasUnmatchedIds()) { listener.onFailure(notFoundException(requiredMatches.unmatchedIdsString())); } else { listener.onResponse(new QueryPage<>(docs, totalHitCount, getResultsField())); } }	extractidfromresource(resource) could be extracted to a variable "resourceid" so that it doesn't have to be computed twice.
private void handlePrivsResponse(String username, Request request, DataFrameTransformConfig config, DataFrameTransformsConfigManager.SeqNoPrimaryTermAndIndex seqNoPrimaryTermPair, ClusterState clusterState, HasPrivilegesResponse privilegesResponse, ActionListener<Response> listener) { if (privilegesResponse.isCompleteMatch()) { updateDataFrame(request, config, seqNoPrimaryTermPair, clusterState, listener); } else { List<String> indices = privilegesResponse.getIndexPrivileges() .stream() .map(ResourcePrivileges::getResource) .collect(Collectors.toList()); listener.onFailure(Exceptions.authorizationError( "Cannot update data frame transform [{}] because user {} lacks all the required permissions for indices: {}", request.getId(), username, indices)); } }	could you also refactor parameter names?
public static void readMultiLineFormat(BytesReference data, XContent xContent, CheckedBiConsumer<SearchRequest, XContentParser, IOException> consumer, String[] indices, IndicesOptions indicesOptions, String routing, String searchType, Boolean ccsMinimizeRoundtrips, NamedXContentRegistry registry, boolean allowExplicitIndex, RestApiVersion restApiVersion) throws IOException { readMultiLineFormat(data, xContent, consumer, indices, indicesOptions, routing, searchType, ccsMinimizeRoundtrips, registry, allowExplicitIndex, restApiVersion, false); }	rather than a boolean, could we add a specific callback interface to allow moving the parsing of the extra request parameters to the fleet module? something like interface handleextraparameter { public boolean handle(string name, object value, searchrequest searchrequest); } with the boolean result signaling whether the name was recognized or not.
* @param listener for the refresh. * @return did we call the listener (true) or register the listener to call later (false)? */ public boolean addOrNotify(long checkpoint, ActionListener<Void> listener) { assert checkpoint >= SequenceNumbers.NO_OPS_PERFORMED; if (checkpoint <= lastRefreshedCheckpoint) { listener.onResponse(null); return true; } long maxIssuedSequenceNumber = maxIssuedSeqNoSupplier.getAsLong(); if (checkpoint > maxIssuedSequenceNumber) { IllegalArgumentException e = new IllegalArgumentException("Cannot wait for unissued seqNo checkpoint [wait_for_checkpoint=" + checkpoint + ", max_issued_seqNo=" + maxIssuedSequenceNumber + "]"); listener.onFailure(e); return true; } synchronized (this) { if (closed) { listener.onFailure(new IllegalStateException("can't wait for refresh on a closed index")); return true; } List<Tuple<Long, ActionListener<Void>>> listeners = checkpointRefreshListeners; final int maxRefreshes = getMaxRefreshListeners.getAsInt(); if (refreshForcers == 0 && roomForListener(maxRefreshes, locationRefreshListeners, listeners)) { addCheckpointListener(checkpoint, listener, listeners); return false; } } // No free slot so force a refresh and call the listener in this thread forceRefresh.run(); if (checkpoint <= lastRefreshedCheckpoint) { listener.onResponse(null); return true; } else { List<Tuple<Long, ActionListener<Void>>> listeners = checkpointRefreshListeners; addCheckpointListener(checkpoint, listener, listeners); return false; } }	in principle, a fast enough burst of many wait for refreshes above gcp can fill the list way above the limit?
public void testDisallowAddListeners() throws Exception { assertEquals(0, listeners.pendingCount()); TestLocationListener listener = new TestLocationListener(); assertFalse(listeners.addOrNotify(index("1").getTranslogLocation(), listener)); TestSeqNoListener seqNoListener = new TestSeqNoListener(); assertFalse(listeners.addOrNotify(index("1").getSeqNo(), seqNoListener)); engine.refresh("I said so"); assertFalse(listener.forcedRefresh.get()); listener.assertNoError(); assertTrue(seqNoListener.isDone.get()); try (Releasable releaseable1 = listeners.forceRefreshes()) { listener = new TestLocationListener(); assertTrue(listeners.addOrNotify(index("1").getTranslogLocation(), listener)); assertTrue(listener.forcedRefresh.get()); listener.assertNoError(); seqNoListener = new TestSeqNoListener(); assertTrue(listeners.addOrNotify(index("1").getSeqNo(), seqNoListener)); assertTrue(seqNoListener.isDone.get()); assertEquals(0, listeners.pendingCount()); try (Releasable releaseable2 = listeners.forceRefreshes()) { listener = new TestLocationListener(); assertTrue(listeners.addOrNotify(index("1").getTranslogLocation(), listener)); assertTrue(listener.forcedRefresh.get()); listener.assertNoError(); seqNoListener = new TestSeqNoListener(); assertTrue(listeners.addOrNotify(index("1").getSeqNo(), seqNoListener)); assertTrue(seqNoListener.isDone.get()); assertEquals(0, listeners.pendingCount()); } listener = new TestLocationListener(); assertTrue(listeners.addOrNotify(index("1").getTranslogLocation(), listener)); assertTrue(listener.forcedRefresh.get()); listener.assertNoError(); seqNoListener = new TestSeqNoListener(); assertTrue(listeners.addOrNotify(index("1").getSeqNo(), seqNoListener)); assertTrue(seqNoListener.isDone.get()); assertEquals(0, listeners.pendingCount()); } assertFalse(listeners.addOrNotify(index("1").getTranslogLocation(), new TestLocationListener())); assertFalse(listeners.addOrNotify(index("1").getSeqNo(), new TestSeqNoListener())); assertEquals(2, listeners.pendingCount()); }	can we add verification that the forcinglistener is completed once the processed seq-no advances?
@Override protected void doClose() throws ElasticsearchException { if (client != null) { client.shutdown(); } // Ensure that IdleConnectionReaper is shutdown IdleConnectionReaper.shutdown(); }	nice. i was not aware of this class / feature.
private FieldStats randomFieldStats() throws UnknownHostException { int type = randomInt(5); switch (type) { case 0: return new FieldStats.Long(randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomBoolean(), randomBoolean(), randomLong(), randomLong()); case 1: return new FieldStats.Double(randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomBoolean(), randomBoolean(), randomDouble(), randomDouble()); case 2: return new FieldStats.Date(randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomBoolean(), randomBoolean(), Joda.forPattern("basicDate"), new Date().getTime(), new Date().getTime()); case 3: return new FieldStats.Text(randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomBoolean(), randomBoolean(), new BytesRef(randomAsciiOfLength(10)), new BytesRef(randomAsciiOfLength(20))); case 4: return new FieldStats.Ip(randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomBoolean(), randomBoolean(), InetAddress.getByName("::1"), InetAddress.getByName("::1")); case 5: return new FieldStats.Ip(randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomPositiveLong(), randomBoolean(), randomBoolean(), InetAddress.getByName("1.2.3.4"), InetAddress.getByName("1.2.3.4")); default: throw new IllegalArgumentException("Invalid type"); } }	i think it is good enough to call output.bytes().steaminput().
* @param settings the settings to check for path.data and default.path.data * @param nodeEnv the current node environment * @param logger a logger where messages regarding the detection will be logged * @throws IOException if an I/O exception occurs reading the directory structure */ static void checkForIndexDataInDefaultPathData( final Settings settings, final NodeEnvironment nodeEnv, final Logger logger) throws IOException { if (!Environment.PATH_DATA_SETTING.exists(settings) || !Environment.DEFAULT_PATH_DATA_SETTING.exists(settings)) { return; } boolean clean = true; for (final String defaultPathData : Environment.DEFAULT_PATH_DATA_SETTING.get(settings)) { final Path defaultNodeDirectory = NodeEnvironment.resolveNodePath(getPath(defaultPathData), nodeEnv.getNodeLockId()); if (Files.exists(defaultNodeDirectory) == false) { continue; } boolean includedInPathData = false; for (final NodeEnvironment.NodePath dataPath : nodeEnv.nodePaths()) { includedInPathData |= Files.isSameFile(dataPath.path, defaultNodeDirectory); } if (includedInPathData) { continue; } final NodeEnvironment.NodePath nodePath = new NodeEnvironment.NodePath(defaultNodeDirectory); final Set<String> availableIndexFolders = nodeEnv.availableIndexFoldersForPath(nodePath); if (availableIndexFolders.isEmpty()) { continue; } clean = false; logger.error("detected index data in default.path.data [{}] where there should not be any", nodePath.indicesPath); for (final String availableIndexFolder : availableIndexFolders) { logger.info( "index folder [{}] in default.path.data [{}] must be moved to any of {}", availableIndexFolder, nodePath.indicesPath, Arrays.stream(nodeEnv.nodePaths()).map(np -> np.indicesPath).collect(Collectors.toList())); } } if (clean) { return; } final String message = String.format( Locale.ROOT, "detected index data in default.path.data %s where there should not be any; check the logs for details", Environment.DEFAULT_PATH_DATA_SETTING.get(settings)); throw new IllegalStateException(message); }	maybe move the loop in a method then we can return early and don't need a label / boolean var in the outer loop
@AfterClass public static void logTotalExecutionTime() { LOGGER.info("Total time: {} ms", totalTime); }	why removing this method?
XContentParser createParser(NamedXContentRegistry xContentRegistry, DeprecationHandler deprecationHandler, Reader reader) throws IOException; /** * Low level implementation detail of {@link XContentGenerator#copyCurrentStructure(XContentParser)}	should it be in the xcontentgenerator class then?
private void unknownValue(Object value, boolean ensureNoSelfReferences) throws IOException { if (value == null) { nullValue(); return; } Writer writer = WRITERS.get(value.getClass()); if (writer != null) { writer.write(this, value); } else if (value instanceof Path) { //Path implements Iterable<Path> and causes endless recursion and a StackOverFlow if treated as an Iterable here value((Path) value); } else if (value instanceof Map) { @SuppressWarnings("unchecked") final Map<String, ?> valueMap = (Map<String, ?>) value; map(valueMap, ensureNoSelfReferences); } else if (value instanceof Iterable) { value((Iterable<?>) value, ensureNoSelfReferences); } else if (value instanceof Object[]) { values((Object[]) value, ensureNoSelfReferences); } else if (value instanceof ToXContent) { value((ToXContent) value); } else if (value instanceof Enum<?>) { // Write out the Enum toString value(Objects.toString(value)); } else { throw new IllegalArgumentException("cannot write xcontent for unknown value of type " + value.getClass()); } }	i tend to put a line break after the annotation. force of habit, really.
* @param out the stream to copy to * @return the number of bytes copied * @throws IOException in case of I/O errors */ private static long copyStream(InputStream in, OutputStream out) throws IOException { Objects.requireNonNull(in, "No InputStream specified"); Objects.requireNonNull(out, "No OutputStream specified"); final byte[] buffer = new byte[8192]; boolean success = false; try { long byteCount = 0; int bytesRead; while ((bytesRead = in.read(buffer)) != -1) { out.write(buffer, 0, bytesRead); byteCount += bytesRead; } out.flush(); success = true; return byteCount; } finally { if (success) { IOUtils.close(in, out); } else { IOUtils.closeWhileHandlingException(in, out); } } }	just a note: java 9 provides this for us, we may want to copy streams (at least this method) to core and then make core a mr jar so we can make use of the java 9 method: https://docs.oracle.com/javase/9/docs/api/java/io/inputstream.html#transferto-java.io.outputstream-
public FilterPath matchProperty(String name) { if ((next != null) && (simpleWildcard || doubleWildcard || simpleMatch(segment, name))) { return next; } return null; }	i don't really want to leak our "funny" simplematch behavior into different places. i wonder if can accept two nullable tokenfilters and the caller can use filterpath if it wants.
@Override protected void doXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(NAME); builder.field(DOCUMENT_TYPE_FIELD.getPreferredName(), documentType); builder.field(QUERY_FIELD.getPreferredName(), field); if (name != null) { builder.field(NAME_FIELD.getPreferredName(), name); } if (documents.isEmpty() == false) { builder.startArray(DOCUMENTS_FIELD.getPreferredName()); for (BytesReference document : documents) { try (XContentParser parser = XContentHelper.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, document)) { parser.nextToken(); XContent.copyCurrentStructure(builder.generator(), parser); } } builder.endArray(); } if (indexedDocumentIndex != null || indexedDocumentType != null || indexedDocumentId != null) { if (indexedDocumentIndex != null) { builder.field(INDEXED_DOCUMENT_FIELD_INDEX.getPreferredName(), indexedDocumentIndex); } if (indexedDocumentType != null) { builder.field(INDEXED_DOCUMENT_FIELD_TYPE.getPreferredName(), indexedDocumentType); } if (indexedDocumentId != null) { builder.field(INDEXED_DOCUMENT_FIELD_ID.getPreferredName(), indexedDocumentId); } if (indexedDocumentRouting != null) { builder.field(INDEXED_DOCUMENT_FIELD_ROUTING.getPreferredName(), indexedDocumentRouting); } if (indexedDocumentPreference != null) { builder.field(INDEXED_DOCUMENT_FIELD_PREFERENCE.getPreferredName(), indexedDocumentPreference); } if (indexedDocumentVersion != null) { builder.field(INDEXED_DOCUMENT_FIELD_VERSION.getPreferredName(), indexedDocumentVersion); } } printBoostAndQueryName(builder); builder.endObject(); }	can we call builder.copycurrentstructure(parser) instead?
@Override public Collection<Object> createComponents(Client client, ClusterService clusterService, ThreadPool threadPool, ResourceWatcherService resourceWatcherService, ScriptService scriptService, NamedXContentRegistry xContentRegistry, Environment environment, NodeEnvironment nodeEnvironment, NamedWriteableRegistry namedWriteableRegistry) { if (enabled == false || transportClientMode) { return emptyList(); } indexLifecycleInitialisationService.set(new IndexLifecycleService(settings, client, clusterService, threadPool, getClock(), System::currentTimeMillis, xContentRegistry)); clusterService.addLifecycleListener(new LifecycleListener() { @Override public void beforeStop() { indexLifecycleInitialisationService.get().close(); } }); return Collections.singletonList(indexLifecycleInitialisationService.get()); }	indexlifecycle plugin closes the scheduler when it itself is closed. is this not done by the node where it calls close() on all the plugins?
@Override public void onCancelled() { logger.info("[{}] received cancellation request for transform, state: [{}].", getTransformId(), context.getTaskState()); ClientTransformIndexer theIndexer = getIndexer(); if (theIndexer != null && theIndexer.abort()) { // there is no background transform running, we can shutdown safely shutdown(); } }	do we need synchronized(context) here too?
@Override public Explanation explain(int topLevelDocId, IndexSearcher searcher, RescoreContext rescoreContext, Explanation sourceExplanation) throws IOException { ExampleRescoreContext context = (ExampleRescoreContext) rescoreContext; // Note that this is inaccurate because it ignores factor field return Explanation.match(context.factor, "test", singletonList(sourceExplanation)); }	the new signature is a bit weird, the only option is to call createweight on the searcher but it's obfuscated so you need to check an actual implementation to realize that.
@Override public void execute(SearchContext context) { try { ObjectObjectHashMap<String, CollectionStatistics> fieldStatistics = HppcMaps.newNoNullKeysMap(); Map<Term, TermStatistics> stats = new HashMap<>(); IndexSearcher searcher = new IndexSearcher(context.searcher().getIndexReader()) { @Override public TermStatistics termStatistics(Term term, TermStates states) throws IOException { if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } TermStatistics ts = super.termStatistics(term, states); if (ts != null) { stats.put(term, ts); } return ts; } @Override public CollectionStatistics collectionStatistics(String field) throws IOException { if (context.isCancelled()) { throw new TaskCancelledException("cancelled"); } CollectionStatistics cs = super.collectionStatistics(field); if (cs != null) { fieldStatistics.put(field, cs); } return cs; } }; searcher.createWeight(context.searcher().rewrite(context.query()), ScoreMode.COMPLETE, 1); for (RescoreContext rescoreContext : context.rescore()) { rescoreContext.rescorer().extractTerms(searcher, rescoreContext); } Term[] terms = stats.keySet().toArray(new Term[0]); TermStatistics[] termStatistics = new TermStatistics[terms.length]; for (int i = 0; i < terms.length; i++) { termStatistics[i] = stats.get(terms[i]); } context.dfsResult().termsStatistics(terms, termStatistics) .fieldStatistics(fieldStatistics) .maxDoc(context.searcher().getIndexReader().maxDoc()); } catch (Exception e) { throw new DfsPhaseExecutionException(context, "Exception during dfs phase", e); } }	i wonder if this is really equivalent. some queries are going to build term statistics even though they don't add terms in extractterms for various reasons (globalordinalsquery for instance) so we'll end up with more terms than before. however the good side of this change is that we'd extract only the terms that are used for scoring instead of all terms that are present in the query. the api change for the rescorer is too obfuscated imo but one thing i fully agree with is that we don't need the special map so we could rely on plain hashmap to cleanup the code a bit ?
public void testAsciidocDocument() throws Exception { Map<String, Object> attachmentData = parseDocument("asciidoc.asciidoc", processor); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content_type", "content", "content_length")); assertThat(attachmentData.get("content_type").toString(), containsString("text/x-asciidoc")); }	this test will remain unchanged: suggestion assertthat(attachmentdata.get("content_type").tostring(), containsstring("text/plain"));
public void testParseAsBytesArray() throws Exception { String path = "/org/elasticsearch/ingest/attachment/test/sample-files/text-in-english.txt"; byte[] bytes; try (InputStream is = AttachmentProcessorTests.class.getResourceAsStream(path)) { bytes = IOUtils.toByteArray(is); } Map<String, Object> document = new HashMap<>(); document.put("source_field", bytes); document.put("resource_name", path); IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document); processor.execute(ingestDocument); @SuppressWarnings("unchecked") Map<String, Object> attachmentData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field"); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("en")); assertThat(attachmentData.get("content"), is("\\\\"God Save the Queen\\\\" (alternatively \\\\"God Save the King\\\\"")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(notNullValue())); }	let's remove this line so these tests remain unchanged as well.
private Map<String, Object> parseDocument(String file, AttachmentProcessor processor, Map<String, Object> optionalFields) throws Exception { Map<String, Object> document = new HashMap<>(); document.put("source_field", getAsBinaryOrBase64(file)); document.put("resource_name", file); document.putAll(optionalFields); IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document); processor.execute(ingestDocument); @SuppressWarnings("unchecked") Map<String, Object> attachmentData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field"); return attachmentData; }	could you add an override for the parsedocument method that adds a boolean includeresourcename parameter and only add resource_name if that parameter is true?
public void testIndexedChars() throws Exception { processor = new AttachmentProcessor(randomAlphaOfLength(10), null, "source_field", "target_field", EnumSet.allOf(AttachmentProcessor.Property.class), 19, false, null, "resource_name"); Map<String, Object> attachmentData = parseDocument("text-in-english.txt", processor); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("en")); assertThat(attachmentData.get("content"), is("\\\\"God Save the Queen")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(19L)); processor = new AttachmentProcessor(randomAlphaOfLength(10), null, "source_field", "target_field", EnumSet.allOf(AttachmentProcessor.Property.class), 19, false, "max_length", "resource_name"); attachmentData = parseDocument("text-in-english.txt", processor); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("en")); assertThat(attachmentData.get("content"), is("\\\\"God Save the Queen")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(19L)); attachmentData = parseDocument("text-in-english.txt", processor, Collections.singletonMap("max_length", 10)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("sk")); assertThat(attachmentData.get("content"), is("\\\\"God Save")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(10L)); attachmentData = parseDocument("text-in-english.txt", processor, Collections.singletonMap("max_length", 100)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("en")); assertThat(attachmentData.get("content"), is("\\\\"God Save the Queen\\\\" (alternatively \\\\"God Save the King\\\\"")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(56L)); attachmentData = parseDocument("text-cjk-big5.txt", processor, Collections.singletonMap("max_length", 100)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("content").toString(), containsString("")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_type").toString(), containsString("charset=Big5")); assertThat(attachmentData.get("content_length"), is(100L)); attachmentData = parseDocument("text-cjk-gbk.txt", processor, Collections.singletonMap("max_length", 100)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("content").toString(), containsString("")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_type").toString(), containsString("charset=GB18030")); assertThat(attachmentData.get("content_length"), is(100L)); attachmentData = parseDocument("text-cjk-euc-jp.txt", processor, Collections.singletonMap("max_length", 100)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("content").toString(), containsString("\\\\n")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_type").toString(), containsString("charset=EUC-JP")); assertThat(attachmentData.get("content_length"), is(100L)); }	let's not change the behavior of these tests: suggestion "target_field", enumset.allof(attachmentprocessor.property.class), 19, false, null, null);
public void testIndexedChars() throws Exception { processor = new AttachmentProcessor(randomAlphaOfLength(10), null, "source_field", "target_field", EnumSet.allOf(AttachmentProcessor.Property.class), 19, false, null, "resource_name"); Map<String, Object> attachmentData = parseDocument("text-in-english.txt", processor); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("en")); assertThat(attachmentData.get("content"), is("\\\\"God Save the Queen")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(19L)); processor = new AttachmentProcessor(randomAlphaOfLength(10), null, "source_field", "target_field", EnumSet.allOf(AttachmentProcessor.Property.class), 19, false, "max_length", "resource_name"); attachmentData = parseDocument("text-in-english.txt", processor); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("en")); assertThat(attachmentData.get("content"), is("\\\\"God Save the Queen")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(19L)); attachmentData = parseDocument("text-in-english.txt", processor, Collections.singletonMap("max_length", 10)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("sk")); assertThat(attachmentData.get("content"), is("\\\\"God Save")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(10L)); attachmentData = parseDocument("text-in-english.txt", processor, Collections.singletonMap("max_length", 100)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("language"), is("en")); assertThat(attachmentData.get("content"), is("\\\\"God Save the Queen\\\\" (alternatively \\\\"God Save the King\\\\"")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_length"), is(56L)); attachmentData = parseDocument("text-cjk-big5.txt", processor, Collections.singletonMap("max_length", 100)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("content").toString(), containsString("")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_type").toString(), containsString("charset=Big5")); assertThat(attachmentData.get("content_length"), is(100L)); attachmentData = parseDocument("text-cjk-gbk.txt", processor, Collections.singletonMap("max_length", 100)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("content").toString(), containsString("")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_type").toString(), containsString("charset=GB18030")); assertThat(attachmentData.get("content_length"), is(100L)); attachmentData = parseDocument("text-cjk-euc-jp.txt", processor, Collections.singletonMap("max_length", 100)); assertThat(attachmentData.keySet(), containsInAnyOrder("language", "content", "content_type", "content_length")); assertThat(attachmentData.get("content").toString(), containsString("\\\\n")); assertThat(attachmentData.get("content_type").toString(), containsString("text/plain")); assertThat(attachmentData.get("content_type").toString(), containsString("charset=EUC-JP")); assertThat(attachmentData.get("content_length"), is(100L)); }	can you add these tests to a new test method named something like testindexedcharswithresourcename that calls your new overloaded parsedocument method with includeresourcename as true so that it separately tests your new functionality without changing the existing tests?
public void createStandardProcessor() { processor = new AttachmentProcessor(randomAlphaOfLength(10), null, "source_field", "target_field", EnumSet.allOf(AttachmentProcessor.Property.class), 10000, false, null, "resource_name"); }	let's leave this unspecified so the existing behavior of the processor without a resource_name is still verified. suggestion "target_field", enumset.allof(attachmentprocessor.property.class), 10000, false, null, null);
public void createStandardProcessor() { processor = new AttachmentProcessor(randomAlphaOfLength(10), null, "source_field", "target_field", EnumSet.allOf(AttachmentProcessor.Property.class), 10000, false, null, "resource_name"); }	same here: suggestion "target_field", selectedproperties, 10000, false, null, null);
BytesRef getEncodedVector() { try { docValues.setNextDocId(scoreScript._getDocId()); } catch (IOException e) { throw ExceptionsHelper.convertToElastic(e); } BytesRef vector = docValues.getEncodedValue(); if (vector == null) { throw new IllegalArgumentException("A document doesn't have a value for a vector field!"); } return vector; }	it's great we were able to remove these references to scorescript#_getindexversion. i think this method can now be deleted?
public void test52BundledJdkRemoved() throws Exception { assumeThat(distribution().hasJdk, is(true)); Path relocatedJdk = installation.bundledJdk.getParent().resolve("jdk.relocated"); try { mv(installation.bundledJdk, relocatedJdk); Platforms.onLinux(() -> { String systemJavaHome1 = sh.run("echo $SYSTEM_JAVA_HOME").stdout.trim(); sh.getEnv().put("JAVA_HOME", systemJavaHome1); }); Platforms.onWindows(() -> { final String systemJavaHome1 = sh.run("$Env:SYSTEM_JAVA_HOME").stdout.trim(); sh.getEnv().put("JAVA_HOME", systemJavaHome1); }); startElasticsearch(); ServerUtils.runElasticsearchTests(); stopElasticsearch(); String systemJavaHome1 = sh.getEnv().get("JAVA_HOME"); assertThat(FileUtils.slurpAllLogs(installation.logs, "elasticsearch.log", "*.log.gz"), containsString(systemJavaHome1)); } finally { mv(relocatedJdk, installation.bundledJdk); } }	this does not spark joy.
public void updateMapping(final IndexMetadata currentIndexMetadata, final IndexMetadata newIndexMetadata) throws IOException { assert newIndexMetadata.getIndex().equals(index()) : "index mismatch: expected " + index() + " but was " + newIndexMetadata.getIndex(); if (currentIndexMetadata != null && currentIndexMetadata.getMappingVersion() == newIndexMetadata.getMappingVersion()) { assertMappingVersion(currentIndexMetadata, newIndexMetadata, this.mapper); return; } MappingMetadata newMappingMetadata = newIndexMetadata.mapping(); if (newMappingMetadata != null) { String type = newMappingMetadata.type(); CompressedXContent incomingMappingSource = newMappingMetadata.source(); Mapping incomingMapping = parseMappings(type, incomingMappingSource); DocumentMapper previousMapper; synchronized (this) { previousMapper = this.mapper; assertRefreshIsNotNeeded(previousMapper, type, incomingMappingSource, incomingMapping); this.mapper = newDocumentMapper(incomingMapping, MergeReason.MAPPING_RECOVERY); } String op = previousMapper != null ? "updated" : "added"; if (logger.isDebugEnabled() && incomingMappingSource.compressed().length < 512) { logger.debug("[{}] {} mapping, source [{}]", index(), op, incomingMappingSource.string()); } else if (logger.isTraceEnabled()) { logger.trace("[{}] {} mapping, source [{}]", index(), op, incomingMappingSource.string()); } else { logger.debug("[{}] {} mapping (source suppressed due to length, use TRACE level if needed)", index(), op); } } }	i prefer to make this a method that returns a boolean (return true), and then call that as assert assertrefreshisnotneeded(...). makes it clearer that this method is not even called when assertions are not enabled
public void updateMapping(final IndexMetadata currentIndexMetadata, final IndexMetadata newIndexMetadata) throws IOException { assert newIndexMetadata.getIndex().equals(index()) : "index mismatch: expected " + index() + " but was " + newIndexMetadata.getIndex(); if (currentIndexMetadata != null && currentIndexMetadata.getMappingVersion() == newIndexMetadata.getMappingVersion()) { assertMappingVersion(currentIndexMetadata, newIndexMetadata, this.mapper); return; } MappingMetadata newMappingMetadata = newIndexMetadata.mapping(); if (newMappingMetadata != null) { String type = newMappingMetadata.type(); CompressedXContent incomingMappingSource = newMappingMetadata.source(); Mapping incomingMapping = parseMappings(type, incomingMappingSource); DocumentMapper previousMapper; synchronized (this) { previousMapper = this.mapper; assertRefreshIsNotNeeded(previousMapper, type, incomingMappingSource, incomingMapping); this.mapper = newDocumentMapper(incomingMapping, MergeReason.MAPPING_RECOVERY); } String op = previousMapper != null ? "updated" : "added"; if (logger.isDebugEnabled() && incomingMappingSource.compressed().length < 512) { logger.debug("[{}] {} mapping, source [{}]", index(), op, incomingMappingSource.string()); } else if (logger.isTraceEnabled()) { logger.trace("[{}] {} mapping, source [{}]", index(), op, incomingMappingSource.string()); } else { logger.debug("[{}] {} mapping (source suppressed due to length, use TRACE level if needed)", index(), op); } } }	is that acceptable or should we throw an assertion error here too?
@SuppressWarnings("unchecked") public PutIndexTemplateRequest source(Map<String, Object> templateSource) { Map<String, Object> source = templateSource; for (Map.Entry<String, Object> entry : source.entrySet()) { String name = entry.getKey(); if (name.equals("index_patterns")) { if(entry.getValue() instanceof String) { patterns(Collections.singletonList((String) entry.getValue())); } else if (entry.getValue() instanceof List) { List<String> elements = ((List<?>) entry.getValue()).stream().map(Object::toString).collect(Collectors.toList()); patterns(elements); } else { throw new IllegalArgumentException("Malformed [index_patterns] value, should be a string or a list of strings"); } } else if (name.equals("order")) { order(XContentMapValues.nodeIntegerValue(entry.getValue(), order())); } else if ("version".equals(name)) { if ((entry.getValue() instanceof Integer) == false) { throw new IllegalArgumentException("Malformed [version] value, should be an integer"); } version((Integer)entry.getValue()); } else if (name.equals("settings")) { if ((entry.getValue() instanceof Map) == false) { throw new IllegalArgumentException("Malformed [settings] section, should include an inner object"); } settings((Map<String, Object>) entry.getValue()); } else if (name.equals("mappings")) { Map<String, Object> mappings = (Map<String, Object>) entry.getValue(); mapping(mappings); } else if (name.equals("aliases")) { aliases((Map<String, Object>) entry.getValue()); } else { throw new ElasticsearchParseException("unknown key [{}] in the template ", name); } } return this; }	this is a hard break -- users who specified template in the source would not have seen a deprecation warning about it, since we convert it to index_patterns before sending the rest request to the server. i wasn't sure how to address this, any guidance here would be great.
@Override public Map<String, Mapper.TypeParser> getMappers() { Map<String, Mapper.TypeParser> mappers = new HashMap<>(super.getMappers()); mappers.put(ShapeFieldMapper.CONTENT_TYPE, ShapeFieldMapper.PARSER); mappers.put(PointFieldMapper.CONTENT_TYPE, PointFieldMapper.PARSER); mappers.put(GeoShapeWithDocValuesFieldMapper.CONTENT_TYPE, new GeoShapeWithDocValuesFieldMapper.TypeParser(vectorTileExtension)); return Collections.unmodifiableMap(mappers); }	the way elasticsearch bootstrap system is setup the loadextensions() should be always called before getmappers() is called. the plugin class and bootstrap system could have been designed better to avoid temporal coupling, but there is a lot of legacy here that we cannot deal with yet. i don't think should propagate this temporal coupling beyond the plugin classes to the rest of the system. i think we can just add an assertion here that loadextensions was indeed called before getmappers() is called just to be sure, and then use vectortileextension from here on instead of carrying setonce into the mappers.
public static Query doTranslate(ScalarFunction f, TranslatorHandler handler) { Query q = ExpressionTranslators.Scalars.doKnownTranslate(f, handler); if (q != null) { return q; } if (f instanceof CIDRMatch) { CIDRMatch cm = (CIDRMatch) f; if (cm.input() instanceof FieldAttribute && Expressions.foldable(cm.addresses())) { String targetFieldName = handler.nameOf(((FieldAttribute) cm.input()).exactAttribute()); Set<Object> set = new LinkedHashSet<>(CollectionUtils.mapSize(cm.addresses().size())); for (Expression e : cm.addresses()) { set.add(valueOf(e)); } return new TermsQuery(f.source(), targetFieldName, set); } } return handler.wrapFunctionQuery(f, f, new ScriptQuery(f.source(), f.asScript())); } } public static class CaseSensitiveScalarFunctions extends ExpressionTranslator<CaseSensitiveScalarFunction> { @Override protected Query asQuery(CaseSensitiveScalarFunction f, TranslatorHandler handler) { return f.isCaseSensitive() ? doTranslate(f, handler) : null; } public static Query doTranslate(CaseSensitiveScalarFunction f, TranslatorHandler handler) { Expression field = null; Expression constant = null; if (f instanceof StringContains) { StringContains sc = (StringContains) f; field = sc.string(); constant = sc.substring(); } else { return null; } if (field instanceof FieldAttribute && constant.foldable()) { String targetFieldName = handler.nameOf(((FieldAttribute) field).exactAttribute()); String substring = (String) constant.fold(); return new WildcardQuery(f.source(), targetFieldName, "*" + substring + "*"); } return null; }	this looks wrong - in the case in particular - if the function is stringcontains do something, otherwise return null? why not make the expressiontranslator for stringcontains only instead of casesensitivescalarfunction?
public void test12InstallDockerDistribution() throws Exception { assumeTrue(distribution().packaging == Distribution.Packaging.DOCKER); installation = Docker.runContainer(distribution()); try { waitForPathToExist(installation.config("elasticsearch.keystore")); } catch (InterruptedException e) { throw new RuntimeException(e); } final Installation.Executables bin = installation.executables(); Shell.Result r = sh.runIgnoreExitCode(bin.keystoreTool.toString() + " has-passwd"); assertThat("has-passwd should fail", r.exitCode, not(is(0))); assertThat("has-passwd should fail", r.stdout, containsString("ERROR: Keystore is not password-protected")); Shell.Result r2 = bin.keystoreTool.run("list"); assertThat(r2.stdout, containsString("keystore.seed")); }	since the previous line checked that the command failed, how about changing the message here: suggestion assertthat("has-passwd didn't fail with the expected error", r.stdout, containsstring("error: keystore is not password-protected"));
public final void freezeContext() { this.frozen.set(Boolean.TRUE); } /** * Marks this context as not cacheable. * This method fails if {@link #freezeContext()}	it's a bit weird that failiffrozen disables the cache, given that it sounds like it's just checking state, not mutating it. maybe we should think about renaming this in a followup?
public void testFetchFieldValue() throws IOException { MapperService mapperService = createMapperService( fieldMapping(b -> b.field("type", "keyword")) ); String index = mapperService.index().getName(); withLuceneIndex(mapperService, iw -> { SourceToParse source = source(index, "id", b -> b.field("field", "value"), "", Map.of()); iw.addDocument(mapperService.documentMapper().parse(source).rootDoc()); }, iw -> { IndexFieldMapper.IndexFieldType ft = (IndexFieldMapper.IndexFieldType) mapperService.fieldType("_index"); SearchLookup lookup = new SearchLookup(mapperService::fieldType, fieldDataLookup()); SearchExecutionContext searchExecutionContext = createSearchExecutionContext(mapperService); // when(searchExecutionContext.getFullyQualifiedIndex()).thenReturn(new Index(index, "indexUUid")); ValueFetcher valueFetcher = ft.valueFetcher(searchExecutionContext, null); IndexSearcher searcher = newSearcher(iw); LeafReaderContext context = searcher.getIndexReader().leaves().get(0); lookup.source().setSegmentAndDocument(context, 0); valueFetcher.setNextReader(context); assertEquals(List.of(index), valueFetcher.fetchValues(lookup.source(), Collections.emptyList())); }); }	nit: remove this entirely?
@Override public GeoIpProcessor create(Map<String, Processor.Factory> registry, String processorTag, Map<String, Object> config) throws Exception { String ipField = readStringProperty(TYPE, processorTag, config, "field"); String targetField = readStringProperty(TYPE, processorTag, config, "target_field", "geoip"); String databaseFile = readStringProperty(TYPE, processorTag, config, "database_file", "GeoLite2-City.mmdb"); List<String> propertyNames = readOptionalList(TYPE, processorTag, config, "properties"); boolean ignoreMissing = readBooleanProperty(TYPE, processorTag, config, "ignore_missing", false); DatabaseReaderLazyLoader lazyLoader = databaseReaders.get(databaseFile); if (lazyLoader == null) { throw newConfigurationException(TYPE, processorTag, "database_file", "database file [" + databaseFile + "] doesn't exist"); } final Supplier<DatabaseReader> databaseReader = () -> { try { return lazyLoader.get(); } catch (final IOException e) { throw new UncheckedIOException(e); } }; final Supplier<Set<Property>> propertiesSupplier; if (propertyNames != null) { final AtomicBoolean set = new AtomicBoolean(); final Set<Property> properties = EnumSet.noneOf(Property.class); propertiesSupplier = () -> { if (set.compareAndSet(false, true)) { for (String fieldName : propertyNames) { try { properties.add(Property.parseProperty(databaseReader.get().getMetadata().getDatabaseType(), fieldName)); } catch (IllegalArgumentException e) { throw newConfigurationException(TYPE, processorTag, "properties", e.getMessage()); } } } return properties; }; } else { final AtomicBoolean set = new AtomicBoolean(); final Set<Property> properties = EnumSet.noneOf(Property.class); propertiesSupplier = () -> { if (set.compareAndSet(false, true)) { final String databaseType = databaseReader.get().getMetadata().getDatabaseType(); if (databaseType.endsWith(CITY_DB_SUFFIX)) { properties.addAll(DEFAULT_CITY_PROPERTIES); } else if (databaseType.endsWith(COUNTRY_DB_SUFFIX)) { properties.addAll(DEFAULT_COUNTRY_PROPERTIES); } else if (databaseType.endsWith(ASN_DB_SUFFIX)) { properties.addAll(DEFAULT_ASN_PROPERTIES); } else { throw newConfigurationException(TYPE, processorTag, "database_file", "Unsupported database type [" + databaseType + "]"); } } return properties; }; } return new GeoIpProcessor(processorTag, ipField, databaseReader, targetField, propertiesSupplier, ignoreMissing, cache); } } // Geoip2's AddressNotFoundException is checked and due to the fact that we need run their code // inside a PrivilegedAction code block, we are forced to catch any checked exception and rethrow // it with an unchecked exception. //package private for testing static final class AddressNotFoundRuntimeException extends RuntimeException { AddressNotFoundRuntimeException(Throwable cause) { super(cause); } } enum Property { IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_ISO_CODE, REGION_NAME, CITY_NAME, TIMEZONE, LOCATION, ASN, ORGANIZATION_NAME; static final EnumSet<Property> ALL_CITY_PROPERTIES = EnumSet.of( Property.IP, Property.COUNTRY_ISO_CODE, Property.COUNTRY_NAME, Property.CONTINENT_NAME, Property.REGION_ISO_CODE, Property.REGION_NAME, Property.CITY_NAME, Property.TIMEZONE, Property.LOCATION ); static final EnumSet<Property> ALL_COUNTRY_PROPERTIES = EnumSet.of( Property.IP, Property.CONTINENT_NAME, Property.COUNTRY_NAME, Property.COUNTRY_ISO_CODE ); static final EnumSet<Property> ALL_ASN_PROPERTIES = EnumSet.of( Property.IP, Property.ASN, Property.ORGANIZATION_NAME ); public static Property parseProperty(String databaseType, String value) { Set<Property> validProperties = EnumSet.noneOf(Property.class); if (databaseType.endsWith(CITY_DB_SUFFIX)) { validProperties = ALL_CITY_PROPERTIES; } else if (databaseType.endsWith(COUNTRY_DB_SUFFIX)) { validProperties = ALL_COUNTRY_PROPERTIES; } else if (databaseType.endsWith(ASN_DB_SUFFIX)) { validProperties = ALL_ASN_PROPERTIES; } try { Property property = valueOf(value.toUpperCase(Locale.ROOT)); if (validProperties.contains(property) == false) { throw new IllegalArgumentException("invalid"); } return property; } catch (IllegalArgumentException e) { throw new IllegalArgumentException("illegal property value [" + value + "]. valid values are " + Arrays.toString(validProperties.toArray())); } }	i think this publishes properties racily? e.g. one thread could enter the block guarded by the atomicboolean and is adding properties to the set in the loop while another one could read properties which is not fully initialized yet? also, i am not aware that there is a happens-before relation between enumset#add and enumset#iterator()?
@Override public GeoIpProcessor create(Map<String, Processor.Factory> registry, String processorTag, Map<String, Object> config) throws Exception { String ipField = readStringProperty(TYPE, processorTag, config, "field"); String targetField = readStringProperty(TYPE, processorTag, config, "target_field", "geoip"); String databaseFile = readStringProperty(TYPE, processorTag, config, "database_file", "GeoLite2-City.mmdb"); List<String> propertyNames = readOptionalList(TYPE, processorTag, config, "properties"); boolean ignoreMissing = readBooleanProperty(TYPE, processorTag, config, "ignore_missing", false); DatabaseReaderLazyLoader lazyLoader = databaseReaders.get(databaseFile); if (lazyLoader == null) { throw newConfigurationException(TYPE, processorTag, "database_file", "database file [" + databaseFile + "] doesn't exist"); } final Supplier<DatabaseReader> databaseReader = () -> { try { return lazyLoader.get(); } catch (final IOException e) { throw new UncheckedIOException(e); } }; final Supplier<Set<Property>> propertiesSupplier; if (propertyNames != null) { final AtomicBoolean set = new AtomicBoolean(); final Set<Property> properties = EnumSet.noneOf(Property.class); propertiesSupplier = () -> { if (set.compareAndSet(false, true)) { for (String fieldName : propertyNames) { try { properties.add(Property.parseProperty(databaseReader.get().getMetadata().getDatabaseType(), fieldName)); } catch (IllegalArgumentException e) { throw newConfigurationException(TYPE, processorTag, "properties", e.getMessage()); } } } return properties; }; } else { final AtomicBoolean set = new AtomicBoolean(); final Set<Property> properties = EnumSet.noneOf(Property.class); propertiesSupplier = () -> { if (set.compareAndSet(false, true)) { final String databaseType = databaseReader.get().getMetadata().getDatabaseType(); if (databaseType.endsWith(CITY_DB_SUFFIX)) { properties.addAll(DEFAULT_CITY_PROPERTIES); } else if (databaseType.endsWith(COUNTRY_DB_SUFFIX)) { properties.addAll(DEFAULT_COUNTRY_PROPERTIES); } else if (databaseType.endsWith(ASN_DB_SUFFIX)) { properties.addAll(DEFAULT_ASN_PROPERTIES); } else { throw newConfigurationException(TYPE, processorTag, "database_file", "Unsupported database type [" + databaseType + "]"); } } return properties; }; } return new GeoIpProcessor(processorTag, ipField, databaseReader, targetField, propertiesSupplier, ignoreMissing, cache); } } // Geoip2's AddressNotFoundException is checked and due to the fact that we need run their code // inside a PrivilegedAction code block, we are forced to catch any checked exception and rethrow // it with an unchecked exception. //package private for testing static final class AddressNotFoundRuntimeException extends RuntimeException { AddressNotFoundRuntimeException(Throwable cause) { super(cause); } } enum Property { IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_ISO_CODE, REGION_NAME, CITY_NAME, TIMEZONE, LOCATION, ASN, ORGANIZATION_NAME; static final EnumSet<Property> ALL_CITY_PROPERTIES = EnumSet.of( Property.IP, Property.COUNTRY_ISO_CODE, Property.COUNTRY_NAME, Property.CONTINENT_NAME, Property.REGION_ISO_CODE, Property.REGION_NAME, Property.CITY_NAME, Property.TIMEZONE, Property.LOCATION ); static final EnumSet<Property> ALL_COUNTRY_PROPERTIES = EnumSet.of( Property.IP, Property.CONTINENT_NAME, Property.COUNTRY_NAME, Property.COUNTRY_ISO_CODE ); static final EnumSet<Property> ALL_ASN_PROPERTIES = EnumSet.of( Property.IP, Property.ASN, Property.ORGANIZATION_NAME ); public static Property parseProperty(String databaseType, String value) { Set<Property> validProperties = EnumSet.noneOf(Property.class); if (databaseType.endsWith(CITY_DB_SUFFIX)) { validProperties = ALL_CITY_PROPERTIES; } else if (databaseType.endsWith(COUNTRY_DB_SUFFIX)) { validProperties = ALL_COUNTRY_PROPERTIES; } else if (databaseType.endsWith(ASN_DB_SUFFIX)) { validProperties = ALL_ASN_PROPERTIES; } try { Property property = valueOf(value.toUpperCase(Locale.ROOT)); if (validProperties.contains(property) == false) { throw new IllegalArgumentException("invalid"); } return property; } catch (IllegalArgumentException e) { throw new IllegalArgumentException("illegal property value [" + value + "]. valid values are " + Arrays.toString(validProperties.toArray())); } }	see my comment above.
public static int compare(String tier1, String tier2) { if (tier1.equals(DATA_CONTENT)) { tier1 = DATA_HOT; } if (tier2.equals(DATA_CONTENT)) { tier2 = DATA_HOT; } int indexOfTier1 = ORDERED_FROZEN_TO_HOT_TIERS.indexOf(tier1); assert indexOfTier1 >= 0 : "expecting a valid tier to compare but got:" + tier1; int indexOfTier2 = ORDERED_FROZEN_TO_HOT_TIERS.indexOf(tier2); assert indexOfTier2 >= 0 : "expecting a valid tier to compare but got:" + tier2; if (indexOfTier1 == indexOfTier2) { return 0; } else { return indexOfTier1 < indexOfTier2 ? -1 : 1; } } /** * This setting provider injects the setting allocating all newly created indices with * {@code index.routing.allocation.include._tier_preference: "data_hot"} for a data stream index * or {@code index.routing.allocation.include._tier_preference: "data_content"}	@dakrone handling the case where an index has the following configuration. not entirely happy with treating data_content as equal to data_hot (as they are not comparable, or shouldn't be) but i guess it'll have to do unless you have a better suggestion? "routing" : { "allocation" : { "include" : { "_tier_preference" : "data_content" }, "require" : { "data" : "warm" } } }
public XPackLicenseState copyCurrentLicenseState() { return executeAgainstStatus(status -> new XPackLicenseState(listeners, isSecurityEnabled, isSecurityExplicitlyEnabled, status, lastUsed, epochMillisProvider)); }	this worries me a bit since a copy can now mutate the original object. it is not an issue for the time being since no such usage exists. but might be a confusing behaviour in future?
public void intercept(RequestInfo requestInfo, AuthorizationEngine authorizationEngine, AuthorizationInfo authorizationInfo, ActionListener<Void> listener) { if (requestInfo.getRequest() instanceof IndicesAliasesRequest) { final IndicesAliasesRequest request = (IndicesAliasesRequest) requestInfo.getRequest(); final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState(); final AuditTrail auditTrail = auditTrailService.get(); if (frozenLicenseState.isSecurityEnabled()) { var licenseChecker = new MemoizedSupplier<>(() -> licenseState.checkFeature(Feature.SECURITY_DLS_FLS)); IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY); for (IndicesAliasesRequest.AliasActions aliasAction : request.getAliasActions()) { if (aliasAction.actionType() == IndicesAliasesRequest.AliasActions.Type.ADD) { for (String index : aliasAction.indices()) { IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(index); if (indexAccessControl != null) { final boolean fls = indexAccessControl.getFieldPermissions().hasFieldLevelSecurity(); final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions(); if ((fls || dls) && licenseChecker.get()) { listener.onFailure(new ElasticsearchSecurityException("Alias requests are not allowed for " + "users who have field or document level security enabled on one of the indices", RestStatus.BAD_REQUEST)); return; } } } } } Map<String, List<String>> indexToAliasesMap = request.getAliasActions().stream() .filter(aliasAction -> aliasAction.actionType() == IndicesAliasesRequest.AliasActions.Type.ADD) .flatMap(aliasActions -> Arrays.stream(aliasActions.indices()) .map(indexName -> new Tuple<>(indexName, Arrays.asList(aliasActions.aliases())))) .collect(Collectors.toMap(Tuple::v1, Tuple::v2, (existing, toMerge) -> { List<String> list = new ArrayList<>(existing.size() + toMerge.size()); list.addAll(existing); list.addAll(toMerge); return list; })); authorizationEngine.validateIndexPermissionsAreSubset(requestInfo, authorizationInfo, indexToAliasesMap, wrapPreservingContext(ActionListener.wrap(authzResult -> { if (authzResult.isGranted()) { // do not audit success again listener.onResponse(null); } else { auditTrail.accessDenied(AuditUtil.extractRequestId(threadContext), requestInfo.getAuthentication(), requestInfo.getAction(), request, authorizationInfo); listener.onFailure(Exceptions.authorizationError("Adding an alias is not allowed when the alias " + "has more permissions than any of the indices")); } }, listener::onFailure), threadContext)); } else { listener.onResponse(null); } } else { listener.onResponse(null); } }	we are using both frozenlicensestate here and the original licensestate with memoizedsupplier below. given the current change, there is no benefit of having frozenlicensestate anymore. it can probably just be removed.
@Override public boolean available() { return licenseState != null && licenseState.checkFeature(Feature.VOTING_ONLY); }	this method does not seem to be used anywhere? but the inner class votingonlynodefeatureset#usageinfoaction also has a availabe() method which internally uses licensestate.checkfeature. but all other xpackinfofeaturetransportaction uses licensestate.isallowed. is this inconsistency intended?
@Override protected InternalGeoHashGrid createTestInstance(String name, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData, InternalAggregations aggregations) { int size = randomNumberOfBuckets(); List<InternalGeoHashGrid.Bucket> buckets = new ArrayList<>(size); for (int i = 0; i < size; i++) { double latitude = randomDoubleBetween(-90.0, 90.0, false); double longitude = randomDoubleBetween(-180.0, 180.0, false); long geoHashAsLong = GeoHashUtils.longEncode(longitude, latitude, 4); buckets.add(new InternalGeoHashGrid.Bucket(GeoHashType.GEOHASH, geoHashAsLong, randomInt(IndexWriter.MAX_DOCS), aggregations)); } return new InternalGeoHashGrid(name, size, buckets, pipelineAggregators, metaData); }	if we randomize geohashtype here at some point and check the bucket key (as geopoint) after the xcontent roundtrip in parsedgeohashgrip.bucket, i think the xcontent tests will currently fail.
@Override public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (script.getType() != ScriptType.INLINE) { validationException = addValidationError("only inline scripts are supported", validationException); } if (needDocumentAndIndex(executeScriptContext.scriptContext)) { if (executeScriptContext.index == null) { validationException = addValidationError("index is a required parameter for current context", validationException); } if (executeScriptContext.document == null) { validationException = addValidationError("document is a required parameter for current context", validationException); } } return validationException; }	isn't the execute action only in 6.x right now? why would wire compat be necessary when no release includes it yet?
@Override public String errorMessage() { return String.format( Locale.ROOT, "OnOutOfMemoryError [%s] requires forking but is prevented by system call filters ([%s=true]);" + " upgrade to at least Java 8u92 and use ExitOnOutOfMemoryError", onOutOfMemoryError(), BootstrapSettings.SYSTEM_CALL_FILTER_SETTING.getKey()); } } /** * Bootstrap check for early-access builds from OpenJDK. */ static class EarlyAccessCheck implements BootstrapCheck { @Override public boolean check() { if ("Oracle Corporation".equals(jvmVendor())) { final String javaVersion = javaVersion(); return javaVersion.endsWith("-ea"); } else { return false; } } String jvmVendor() { return Constants.JVM_VENDOR; } String javaVersion() { return Constants.JAVA_VERSION; } @Override public String errorMessage() { return String.format( Locale.ROOT, "Java version [%s] is an early-access build, only use release builds", javaVersion()); } } /** * Bootstrap check for versions of HotSpot that are known to have issues that can lead to index corruption when G1GC is enabled. */ static class G1GCCheck implements BootstrapCheck { @Override public boolean check() { if ("Oracle Corporation".equals(jvmVendor()) && isJava8() && isG1GCEnabled()) { final String jvmVersion = jvmVersion(); // HotSpot versions on Java 8 match this regular expression; note that this changes with Java 9 after JEP-223 final Pattern pattern = Pattern.compile("(\\\\\\\\d+)\\\\\\\\.(\\\\\\\\d+)-b\\\\\\\\d+"); final Matcher matcher = pattern.matcher(jvmVersion); final boolean matches = matcher.matches(); assert matches : jvmVersion; final int major = Integer.parseInt(matcher.group(1)); final int update = Integer.parseInt(matcher.group(2)); // HotSpot versions for Java 8 have major version 25, the bad versions are all versions prior to update 40 return major == 25 && update < 40; } else { return false; } } // visible for testing String jvmVendor() { return Constants.JVM_VENDOR; } // visible for testing boolean isG1GCEnabled() { assert "Oracle Corporation".equals(jvmVendor()); return JvmInfo.jvmInfo().useG1GC().equals("true"); } // visible for testing String jvmVersion() { assert "Oracle Corporation".equals(jvmVendor()); return Constants.JVM_VERSION; } // visible for testing boolean isJava8() { assert "Oracle Corporation".equals(jvmVendor()); return JavaVersion.current().equals(JavaVersion.parse("1.8")); } @Override public String errorMessage() { return String.format( Locale.ROOT, "JVM version [%s] can cause data corruption when used with G1GC; upgrade to at least Java 8u40", jvmVersion()); }	nit: return "oracle corporation".equals(jvmvendor()) && javaversion().endswith("-ea") more succinct?
public void testInvalidHostType() throws InterruptedException { Settings nodeSettings = Settings.builder() .put(DISCOVERY_EC2.HOST_TYPE_SETTING.getKey(), "does_not_exist") .build(); try { buildDynamicNodes(nodeSettings, 1); fail("Expected IllegalArgumentException"); } catch (IllegalArgumentException e) { assertThat(e.getMessage(), containsString("does_not_exist is unknown for discovery.ec2.host_type")); } }	can you use expectthrows instead?
private static void checkFilterOnAggs(LogicalPlan p, Set<Failure> localFailures) { if (p instanceof Filter) { Filter filter = (Filter) p; if ((filter.child() instanceof Aggregate) == false) { filter.condition().forEachDown(e -> { if (Functions.isAggregate(e) || e instanceof AggregateFunctionAttribute) { localFailures.add( fail(e, "Cannot use WHERE filtering on aggregate function [%s], use HAVING instead", Expressions.name(e))); } }, Expression.class); } } }	maybe it's too verbose, but i suggest to rephrase to apply filtering on the argument of the grouping function instead, as grouped field maybe be confusing.
@Override protected ActionListener<BulkShardOperationsResponse> getResponseActionListener( ActionListener<BulkShardOperationsResponse> referenceClosingListener, IndexShard shard) { ActionListener<BulkShardOperationsResponse> listener = super.getResponseActionListener(referenceClosingListener, shard); return wrapListener(listener, shard); }	nit: listener can just be inlined into the next line
@Override protected void performOnPrimary(IndexShard primary, BulkShardOperationsRequest request, ActionListener<PrimaryResult> listener) { final PlainActionFuture<Releasable> permitFuture = new PlainActionFuture<>(); primary.acquirePrimaryOperationPermit(permitFuture, ThreadPool.Names.SAME, request); final TransportWriteAction.WritePrimaryResult<BulkShardOperationsRequest, BulkShardOperationsResponse> ccrResult; try (Releasable ignored = permitFuture.get()) { ccrResult = TransportBulkShardOperationsAction.shardOperationOnPrimary(primary.shardId(), request.getHistoryUUID(), request.getOperations(), request.getMaxSeqNoOfUpdatesOrDeletes(), primary, logger); ActionTestUtils.assertNoFailureListener(result -> { TransportWriteActionTestHelper.performPostWriteActions(primary, request, ccrResult.location, logger); listener.onResponse(new PrimaryResult(ccrResult.replicaRequest(), ccrResult.finalResponseIfSuccessful)); }).onResponse(ccrResult); } catch (InterruptedException | ExecutionException | IOException e) { throw new AssertionError(e); } }	nit: this is kind of complicated with the redundant listener wrapper isn't it? maybe just: diff diff --git a/x-pack/plugin/ccr/src/test/java/org/elasticsearch/xpack/ccr/action/shardfollowtaskreplicationtests.java b/x-pack/plugin/ccr/src/test/java/org/elasticsearch/xpack/ccr/action/shardfollowtaskreplicationtests.java index e58d16dee11..3dd52f8cb0e 100644 --- a/x-pack/plugin/ccr/src/test/java/org/elasticsearch/xpack/ccr/action/shardfollowtaskreplicationtests.java +++ b/x-pack/plugin/ccr/src/test/java/org/elasticsearch/xpack/ccr/action/shardfollowtaskreplicationtests.java @@ -677,11 +677,9 @@ public class shardfollowtaskreplicationtests extends esindexlevelreplicationtest try (releasable ignored = permitfuture.get()) { ccrresult = transportbulkshardoperationsaction.shardoperationonprimary(primary.shardid(), request.gethistoryuuid(), request.getoperations(), request.getmaxseqnoofupdatesordeletes(), primary, logger); - actiontestutils.assertnofailurelistener(result -> { - transportwriteactiontesthelper.performpostwriteactions(primary, request, ccrresult.location, logger); - listener.onresponse(new primaryresult(ccrresult.replicarequest(), ccrresult.finalresponseifsuccessful)); - }).onresponse(ccrresult); - } catch (interruptedexception | executionexception | ioexception e) { + transportwriteactiontesthelper.performpostwriteactions(primary, request, ccrresult.location, logger); + listener.onresponse(new primaryresult(ccrresult.replicarequest(), ccrresult.finalresponseifsuccessful)); + } catch (exception e) { throw new assertionerror(e); } } ?
@Override protected void performOnPrimary(IndexShard primary, BulkShardOperationsRequest request, ActionListener<PrimaryResult> listener) { final PlainActionFuture<Releasable> permitFuture = new PlainActionFuture<>(); primary.acquirePrimaryOperationPermit(permitFuture, ThreadPool.Names.SAME, request); final TransportWriteAction.WritePrimaryResult<BulkShardOperationsRequest, BulkShardOperationsResponse> ccrResult; try (Releasable ignored = permitFuture.get()) { ccrResult = TransportBulkShardOperationsAction.shardOperationOnPrimary(primary.shardId(), request.getHistoryUUID(), request.getOperations(), request.getMaxSeqNoOfUpdatesOrDeletes(), primary, logger); ActionTestUtils.assertNoFailureListener(result -> { TransportWriteActionTestHelper.performPostWriteActions(primary, request, ccrResult.location, logger); listener.onResponse(new PrimaryResult(ccrResult.replicaRequest(), ccrResult.finalResponseIfSuccessful)); }).onResponse(ccrResult); } catch (InterruptedException | ExecutionException | IOException e) { throw new AssertionError(e); } }	nit: can be shortened to java try (releasable ignored = plainactionfuture.get(f -> replica.acquirereplicaoperationpermit( getprimaryshard().getpendingprimaryterm(), getprimaryshard().getlastknownglobalcheckpoint(), getprimaryshard().getmaxseqnoofupdatesordeletes(), f, threadpool.names.same, request))) {
public void testCurrentExamplePlugin() throws IOException { FileUtils.copyDirectory(examplePlugin, tmpDir.getRoot()); adaptBuildScriptForTest(); Files.write( tmpDir.newFile("NOTICE.txt").toPath(), "dummy test notice".getBytes(StandardCharsets.UTF_8) ); GradleRunner.create() .withProjectDir(tmpDir.getRoot()) .withTestKitDir(tmpDir.newFolder("testkit")) .withArguments("clean", "check", "-s", "-i", "--warning-mode=all", "--scan") .withPluginClasspath() .build(); }	this shouldn't be necessary. we are already running each of these test cases in a new temporary directory. by default testkit is going to create a .gradle-test-kit directory under that folder. really all this is doing is renaming that folder testkit instead. if the example plugin tests are failing, it's not due to testkit directory reuse.
@Override protected void masterOperation(Task task, final ResizeRequest resizeRequest, final ClusterState state, final ActionListener<ResizeResponse> listener) { // there is no need to fetch docs stats for split but we keep it simple and do it anyway for simplicity of the code final String sourceIndex = indexNameExpressionResolver.resolveDateMathExpression(resizeRequest.getSourceIndex()); final String targetIndex = indexNameExpressionResolver.resolveDateMathExpression(resizeRequest.getTargetIndexRequest().index()); final IndexMetadata sourceMetadata = state.metadata().index(sourceIndex); if (sourceMetadata == null) { listener.onFailure(new IndexNotFoundException(sourceIndex)); return; } IndicesStatsRequestBuilder statsRequestBuilder = client.admin().indices().prepareStats(sourceIndex).clear().setDocs(true); IndicesStatsRequest statsRequest = statsRequestBuilder.request(); statsRequest.setParentTask(clusterService.localNode().getId(), task.getId()); // TODO: only fetch indices stats for shrink type resize requests and move request validation (outside of doc counts) to before // fetching the stats client.execute(IndicesStatsAction.INSTANCE, statsRequest, ActionListener.delegateFailure(listener, (delegatedListener, indicesStatsResponse) -> { final CreateIndexClusterStateUpdateRequest updateRequest; try { updateRequest = prepareCreateIndexRequest(resizeRequest, sourceMetadata, i -> { IndexShardStats shard = indicesStatsResponse.getIndex(sourceIndex).getIndexShards().get(i); return shard == null ? null : shard.getPrimary().getDocs(); }, targetIndex); } catch (Exception e) { delegatedListener.onFailure(e); return; } createIndexService.createIndex( updateRequest, ActionListener.map(delegatedListener, response -> new ResizeResponse(response.isAcknowledged(), response.isShardsAcknowledged(), updateRequest.index())) ); })); }	failing the listener on a missing index isn't strictly necessary here because i added the try-catch around preparecreateindexrequest anyway but i figured it more nicely motivates this todo and follow-up cleanup to the logic here that previously relied on the exceptions just bubbling up all the way.
@Override public <Request extends ActionRequest, Response extends ActionResponse> void doExecute(ActionType<Response> action, Request request, ActionListener<Response> listener) { // Discard the task because the Client interface doesn't use it. try { executeLocally(action, request, listener); } catch (Exception e) { // #executeLocally returns the task and throws RuntimeException if it fails to register the task so we forward them to listener // since this API does not concern itself with the specifics of the task handling listener.onFailure(e); } } /** * Execute an {@link ActionType} locally, returning that {@link Task} used to track it, and linking an {@link ActionListener}. * Prefer this method if you don't need access to the task when listening for the response. This is the method used to * implement the {@link Client}	i think without further refactoring of the signature of registerandexecute and this is about as low as we can go on the stack in terms of passing these exceptions to the listener directly. otherwise we'll have to adjust a non-trivial number of spots that make use of the returned task from that method.
@Override public <Request extends ActionRequest, Response extends ActionResponse> void doExecute(ActionType<Response> action, Request request, ActionListener<Response> listener) { // Discard the task because the Client interface doesn't use it. try { executeLocally(action, request, listener); } catch (Exception e) { // #executeLocally returns the task and throws RuntimeException if it fails to register the task so we forward them to listener // since this API does not concern itself with the specifics of the task handling listener.onFailure(e); } } /** * Execute an {@link ActionType} locally, returning that {@link Task} used to track it, and linking an {@link ActionListener}. * Prefer this method if you don't need access to the task when listening for the response. This is the method used to * implement the {@link Client}	i think we should add a comment to javadoc here and in the other executelocally that they can throw taskcancelledexception?
public < Request extends ActionRequest, Response extends ActionResponse > Task executeLocally(ActionType<Response> action, Request request, ActionListener<Response> listener) { return taskManager.registerAndExecute("transport", transportAction(action), request, (t, r) -> { try { listener.onResponse(r); } catch (Exception e) { assert false : new AssertionError("callback must handle its own exceptions", e); throw e; } }, (t, e) -> { try { listener.onFailure(e); } catch (Exception ex) { ex.addSuppressed(e); assert false : new AssertionError("callback must handle its own exceptions", ex); throw ex; } }); } /** * Execute an {@link ActionType} locally, returning that {@link Task} used to track it, and linking an {@link TaskListener}	at least in tests, this ensures no double invocation of the listener with the new try-catch i suppose technically speaking it would be nice to do the same for the override below that uses a tasklistener but i chose not to for now since as you can see this already required fixing up a number of spots that relied on the listener's onresponse throwing and handling exceptions upstream.
@SuppressWarnings("unchecked") private <T> void doTestParameter(String paramName, String paramValue, T expectedValue, Function<SubmitAsyncSearchRequest, T> valueAccessor) throws Exception { SetOnce<Boolean> executeCalled = new SetOnce<>(); verifyingClient.setExecuteLocallyVerifier((actionType, request) -> { assertThat(request, instanceOf(SubmitAsyncSearchRequest.class)); assertThat(valueAccessor.apply((SubmitAsyncSearchRequest) request), equalTo(expectedValue)); executeCalled.set(true); return new AsyncSearchResponse("", randomBoolean(), randomBoolean(), 0L, 0L); }); Map<String, String> params = new HashMap<>(); params.put(paramName, paramValue); RestRequest submitAsyncRestRequest = new FakeRestRequest.Builder(xContentRegistry()).withMethod(RestRequest.Method.POST) .withPath("/test_index/_async_search") .withParams(params) .withContent(new BytesArray("{}"), XContentType.JSON).build(); // Get a new context each time, so we don't get exceptions due to trying to add the same header multiple times try (ThreadContext.StoredContext context = verifyingClient.threadPool().getThreadContext().stashContext()) { dispatchRequest(submitAsyncRestRequest); } assertThat(executeCalled.get(), equalTo(true)); verifyingClient.reset(); }	this was pretty dirty to begin with. without this change and the change in the test verifying client we would always throw an npe in the background (form trying to serialize a null response) but that would never fail any tests. with the new assertions that wasn't possible anymore so i had to fix this up here.
public void onResponse(GetIndexResponse getIndexResponse) { try { validateMappings(getIndexResponse); } catch (Exception e) { listener.onFailure(e); return; } prepareAndCreateEnrichIndex(); }	obvious spot where we have a non-wrap listener and this would just bubble up all the way if the request is executed locally on same.
public final Task execute(Request request, ActionListener<Response> listener) { Task task = taskManager.register("transport", actionName, request); if (task == null) { execute(null, request, listener); } else { execute(task, request, new ActionListener<Response>() { @Override public void onResponse(Response response) { taskManager.unregister(task); listener.onResponse(response); } @Override public void onFailure(Throwable e) { taskManager.unregister(task); listener.onFailure(e); } }); } return task; }	when do we expect task to be null?
@Override protected void masterOperation(Request request, ClusterState state, ActionListener<Response> listener) { Map<String, String> filteredHeaders = threadPool.getThreadContext().getHeaders().entrySet().stream() .filter(e -> ClientHelper.SECURITY_HEADER_FILTERS.contains(e.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); clusterService.submitStateUpdateTask("put-lifecycle-" + request.getPolicy().getName(), new AckedClusterStateUpdateTask<Response>(request, listener) { @Override protected Response newResponse(boolean acknowledged) { return new Response(acknowledged); } @Override public ClusterState execute(ClusterState currentState) throws Exception { ClusterState.Builder newState = ClusterState.builder(currentState); IndexLifecycleMetadata currentMetadata = currentState.metaData().custom(IndexLifecycleMetadata.TYPE); if (currentMetadata == null) { // first time using index-lifecycle feature, bootstrap metadata currentMetadata = IndexLifecycleMetadata.EMPTY; } if (currentMetadata.getPolicyMetadatas().containsKey(request.getPolicy().getName()) && IndexLifecycleRunner .canUpdatePolicy(request.getPolicy().getName(), request.getPolicy(), currentState) == false) { throw new ResourceAlreadyExistsException("Lifecycle policy already exists: {}", request.getPolicy().getName()); } // NORELEASE Check if current step exists in new policy and if not move to next available step SortedMap<String, LifecyclePolicyMetadata> newPolicies = new TreeMap<>(currentMetadata.getPolicyMetadatas()); LifecyclePolicyMetadata lifecyclePolicyMetadata = new LifecyclePolicyMetadata(request.getPolicy(), filteredHeaders); newPolicies.put(lifecyclePolicyMetadata.getName(), lifecyclePolicyMetadata); IndexLifecycleMetadata newMetadata = new IndexLifecycleMetadata(newPolicies, OperationMode.RUNNING); newState.metaData(MetaData.builder(currentState.getMetaData()) .putCustom(IndexLifecycleMetadata.TYPE, newMetadata).build()); return newState.build(); } }); }	this might be worth a comment explaining why this needs to be here and not in the task so someone doesn't move it into the task without realising it will break things?
private List<B> reduceMergeSort(List<InternalAggregation> aggregations, BucketOrder thisReduceOrder, InternalAggregation.ReduceContext reduceContext) { assert isKeyOrder(thisReduceOrder); final Comparator<Bucket> cmp = thisReduceOrder.comparator(); final PriorityQueue<IteratorAndCurrent<B>> pq = new PriorityQueue<>(aggregations.size()) { @Override protected boolean lessThan(IteratorAndCurrent<B> a, IteratorAndCurrent<B> b) { return cmp.compare(a.current(), b.current()) < 0; } }; for (InternalAggregation aggregation : aggregations) { @SuppressWarnings("unchecked") A terms = (A) aggregation; if (terms.getBuckets().isEmpty() == false) { pq.add(new IteratorAndCurrent<>(terms.getBuckets().iterator())); } } List<B> reducedBuckets = new ArrayList<>(); // list of buckets coming from different shards that have the same key List<B> currentBuckets = new ArrayList<>(); B lastBucket = null; while (pq.size() > 0) { final IteratorAndCurrent<B> top = pq.top(); assert lastBucket == null || cmp.compare(top.current(), lastBucket) >= 0; if (lastBucket != null && cmp.compare(top.current(), lastBucket) != 0) { // the key changes, reduce what we already buffered and reset the buffer for current buckets final B reduced = reduceBucket(currentBuckets, reduceContext); reducedBuckets.add(reduced); currentBuckets.clear(); } lastBucket = top.current(); currentBuckets.add(top.current()); if (top.hasNext()) { top.next(); /* * Typically the bucket keys are strictly increasing, but when we merge aggs from two different indices * we can promote long and unsigned long keys to double, which can cause 2 long keys to be promoted into * the same double key. */ assert cmp.compare(top.current(), lastBucket) >= 0 : "shards must return data sorted by key"; pq.updateTop(); } else { pq.pop(); } } if (currentBuckets.isEmpty() == false) { final B reduced = reduceBucket(currentBuckets, reduceContext); reducedBuckets.add(reduced); } return reducedBuckets; }	still kinda shocking to me it took this long for the assertion to fail. really would have expected a test to have triggered it by now :(
public void setInternalActionLongTimeout(TimeValue internalActionLongTimeout) { this.internalActionLongTimeout = internalActionLongTimeout; }	nit: i think this getter/setter just got moved too, could we move it back to clarify it was unchanged?
* * @param key the key of the entry to get from the cache * @param now the access time of this entry * @param isExpired test if the entry is expired * @return the entry if there was one, otherwise null */ Entry<K, V> get(K key, long now, Predicate<Entry<K, V>> isExpired) { CompletableFuture<Entry<K, V>> future; Entry<K, V> entry = null; try (ReleasableLock ignored = readLock.acquire()) { future = map.get(key); } if (future != null) { try { entry = future.handle((ok, ex) -> { if (ok != null && !isExpired.test(ok)) { segmentStats.hit(); ok.accessTime = now; return ok; } else { segmentStats.miss(); return null; } }).get(); } catch (ExecutionException | InterruptedException e) { throw new IllegalStateException(e); } } else { segmentStats.miss(); } return entry; }	maybe add something about how if the entry is expired, it is not removed from the cache but is instead ignored?
public ImmutableOpenMap<String, AliasMetadata> getAliases() { return this.aliases; } /** * Lazy loaded cache for tier preference setting. We can't eager load this setting because * {@link IndexMetadataVerifier#convertSharedCacheTierPreference(IndexMetadata)} might not have acted on this index yet and thus the * setting validation for this setting could fail for metadata loaded from a snapshot or disk after an upgrade. * Note: this field needs no synchronization since its a pure function of the immutable {@link #settings}, similar to how * {@link String#hashCode()}	an alternative to lazy loading would be to store a null here if parsing the field fails when building and then parse the field every time when gettierpreference is called (or parse it to throw the exception and then if it succeeds throw another exception). that seems slightly better to me, since then the field is final.
public ImmutableOpenMap<String, AliasMetadata> getAliases() { return this.aliases; } /** * Lazy loaded cache for tier preference setting. We can't eager load this setting because * {@link IndexMetadataVerifier#convertSharedCacheTierPreference(IndexMetadata)} might not have acted on this index yet and thus the * setting validation for this setting could fail for metadata loaded from a snapshot or disk after an upgrade. * Note: this field needs no synchronization since its a pure function of the immutable {@link #settings}, similar to how * {@link String#hashCode()}	i think we should this still mark this volatile to avoid unsafe publishing. neither list.of nor string constructor promises to only have finalized fields and while it sort of looks like that is the case today, it could change in jdk updates. also, a slight modification to datatier.parsetierprefence could cause this without anyone noticing. the difference to string.hashcode is that it is a primitive with no references out (plus it is in the jdk so could special handle it if needed).
@Override public final NodeChannels openConnection(DiscoveryNode node, ConnectionProfile connectionProfile) throws IOException { if (node == null) { throw new ConnectTransportException(null, "can't open connection to a null node"); } boolean success = false; NodeChannels nodeChannels = null; connectionProfile = resolveConnectionProfile(connectionProfile, defaultConnectionProfile); closeLock.readLock().lock(); // ensure we don't open connections while we are closing try { ensureOpen(); try { final AtomicBoolean runOnce = new AtomicBoolean(false); final AtomicReference<NodeChannels> connectionRef = new AtomicReference<>(); Consumer<Channel> onClose = c -> { assert isOpen(c) == false : "channel is still open when onClose is called"; try { onChannelClosed(c); } finally { // we only need to disconnect from the nodes once since all other channels // will also try to run this we protect it from running multiple times. if (runOnce.compareAndSet(false, true)) { NodeChannels connection = connectionRef.get(); if (connection != null) { disconnectFromNodeCloseAndNotify(node, connection); } } } }; nodeChannels = connectToChannels(node, connectionProfile, this::onChannelOpen, onClose); final Channel channel = nodeChannels.getChannels().get(0); // one channel is guaranteed by the connection profile final TimeValue connectTimeout = connectionProfile.getConnectTimeout() == null ? defaultConnectionProfile.getConnectTimeout() : connectionProfile.getConnectTimeout(); final TimeValue handshakeTimeout = connectionProfile.getHandshakeTimeout() == null ? connectTimeout : connectionProfile.getHandshakeTimeout(); final Version version = executeHandshake(node, channel, handshakeTimeout); nodeChannels = new NodeChannels(nodeChannels, version); // clone the channels - we now have the correct version transportService.onConnectionOpened(nodeChannels); connectionRef.set(nodeChannels); if (!Arrays.stream(nodeChannels.channels).allMatch(this::isOpen)) { throw new ConnectTransportException(node, "a channel closed while connecting"); } success = true; return nodeChannels; } catch (ConnectTransportException e) { throw e; } catch (Exception e) { // ConnectTransportExceptions are handled specifically on the caller end - we wrap the actual exception to ensure // only relevant exceptions are logged on the caller end.. this is the same as in connectToNode throw new ConnectTransportException(node, "general node connection failure", e); } finally { if (success == false) { IOUtils.closeWhileHandlingException(nodeChannels); } } } finally { closeLock.readLock().unlock(); } }	can we use == false?
public Object value(Aggregation agg, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { SingleBucketAggregation aggregation = (SingleBucketAggregation) agg; if (aggregation.getAggregations().iterator().hasNext() == false) { return aggregation.getDocCount(); } HashMap<String, Object> nested = new HashMap<>(); for (Aggregation subAgg : aggregation.getAggregations()) { nested.put( subAgg.getName(), getExtractor(subAgg).value( subAgg, fieldTypeMap, lookupFieldPrefix.isEmpty() ? agg.getName() : lookupFieldPrefix + "." + agg.getName() ) ); } return nested; } } static class MultiBucketsAggExtractor implements AggValueExtractor { @Override public Object value(Aggregation agg, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { MultiBucketsAggregation aggregation = (MultiBucketsAggregation) agg; HashMap<String, Object> nested = new HashMap<>(); for (MultiBucketsAggregation.Bucket bucket : aggregation.getBuckets()) { if (bucket.getAggregations().iterator().hasNext() == false) { nested.put(bucket.getKeyAsString(), bucket.getDocCount()); } else { HashMap<String, Object> nestedBucketObject = new HashMap<>(); for (Aggregation subAgg : bucket.getAggregations()) { nestedBucketObject.put( subAgg.getName(), getExtractor(subAgg).value( subAgg, fieldTypeMap, lookupFieldPrefix.isEmpty() ? agg.getName() : lookupFieldPrefix + "." + agg.getName() ) ); } nested.put(bucket.getKeyAsString(), nestedBucketObject); } } return nested; } } static class ScriptedMetricAggExtractor implements AggValueExtractor { @Override public Object value(Aggregation agg, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { ScriptedMetric aggregation = (ScriptedMetric) agg; return aggregation.aggregation(); } } static class GeoCentroidAggExtractor implements AggValueExtractor { @Override public Object value(Aggregation agg, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { GeoCentroid aggregation = (GeoCentroid) agg; // if the account is `0` iff there is no contained centroid return aggregation.count() > 0 ? aggregation.centroid().toString() : null; } } static class GeoBoundsAggExtractor implements AggValueExtractor { @Override public Object value(Aggregation agg, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { GeoBounds aggregation = (GeoBounds) agg; if (aggregation.bottomRight() == null || aggregation.topLeft() == null) { return null; } final Map<String, Object> geoShape = new HashMap<>(); // If the two geo_points are equal, it is a point if (aggregation.topLeft().equals(aggregation.bottomRight())) { geoShape.put(ShapeParser.FIELD_TYPE.getPreferredName(), PointBuilder.TYPE.shapeName()); geoShape.put( ShapeParser.FIELD_COORDINATES.getPreferredName(), Arrays.asList(aggregation.topLeft().getLon(), aggregation.bottomRight().getLat()) ); // If only the lat or the lon of the two geo_points are equal, than we know it should be a line } else if (Double.compare(aggregation.topLeft().getLat(), aggregation.bottomRight().getLat()) == 0 || Double.compare(aggregation.topLeft().getLon(), aggregation.bottomRight().getLon()) == 0) { geoShape.put(ShapeParser.FIELD_TYPE.getPreferredName(), LineStringBuilder.TYPE.shapeName()); geoShape.put( ShapeParser.FIELD_COORDINATES.getPreferredName(), Arrays.asList( new Double[] { aggregation.topLeft().getLon(), aggregation.topLeft().getLat() }, new Double[] { aggregation.bottomRight().getLon(), aggregation.bottomRight().getLat() } ) ); } else { // neither points are equal, we have a polygon that is a square geoShape.put(ShapeParser.FIELD_TYPE.getPreferredName(), PolygonBuilder.TYPE.shapeName()); final GeoPoint tl = aggregation.topLeft(); final GeoPoint br = aggregation.bottomRight(); geoShape.put( ShapeParser.FIELD_COORDINATES.getPreferredName(), Collections.singletonList( Arrays.asList( new Double[] { tl.getLon(), tl.getLat() }, new Double[] { br.getLon(), tl.getLat() }, new Double[] { br.getLon(), br.getLat() }, new Double[] { tl.getLon(), br.getLat() }, new Double[] { tl.getLon(), tl.getLat() } ) ) ); } return geoShape; }	it would be good to cover this branch and have a test with nested terms aggs, like your common user example, broke down by e.g. businesses or filtered by something
public void testWarningHeadersRegex() { final String testHeader = HeaderWarning.formatWarning("test"); final String realisticTestHeader = HeaderWarning.formatWarning("index template [my-it] has index patterns [test-*] matching " + "patterns from existing older templates [global] with patterns (global => [*]); this template [my-it] will take " + "precedence during new index creation"); final String testHeaderWithQuotesAndBackslashes = HeaderWarning.formatWarning("test \\\\"with quotes and \\\\\\\\ backslashes\\\\""); //require header and it matches (basic example) DoSection section = new DoSection(new XContentLocation(1, 1)); section.setExpectedWarningHeadersRegex(singletonList(Pattern.compile(".*"))); section.checkWarningHeaders(singletonList(testHeader), Version.CURRENT); //require header and it matches (realistic example) section = new DoSection(new XContentLocation(1, 1)); section.setExpectedWarningHeadersRegex( singletonList(Pattern.compile("^index template \\\\\\\\[(.+)\\\\\\\\] has index patterns \\\\\\\\[(.+)\\\\\\\\] matching patterns from existing " + "older templates \\\\\\\\[(.+)\\\\\\\\] with patterns \\\\\\\\((.+)\\\\\\\\); this template \\\\\\\\[(.+)\\\\\\\\] will " + "take precedence during new index creation$"))); section.checkWarningHeaders(singletonList(realisticTestHeader), Version.CURRENT); //require header, but no headers returned section = new DoSection(new XContentLocation(1, 1)); section.setExpectedWarningHeadersRegex(singletonList(Pattern.compile("junk"))); DoSection finalSection = section; AssertionError error = expectThrows(AssertionError.class, () -> finalSection.checkWarningHeaders(emptyList(), Version.CURRENT)); assertEquals("the following regular expression did not match any warning header [\\\\n\\\\tjunk\\\\n]\\\\n", error.getMessage()); //require multiple header, but none returned (plural error message) section = new DoSection(new XContentLocation(1, 1)); section.setExpectedWarningHeadersRegex(List.of(Pattern.compile("junk"), Pattern.compile("junk2"))); DoSection finalSection2 = section; error = expectThrows(AssertionError.class, () -> finalSection2.checkWarningHeaders(emptyList(), Version.CURRENT)); assertEquals("the following regular expressions did not match any warning header [\\\\n\\\\tjunk\\\\n\\\\tjunk2\\\\n]\\\\n", error.getMessage()); //require header, got one back, but not matched section = new DoSection(new XContentLocation(1, 1)); section.setExpectedWarningHeadersRegex(singletonList(Pattern.compile("junk"))); DoSection finalSection3 = section; error = expectThrows(AssertionError.class, () -> finalSection3.checkWarningHeaders(singletonList(testHeader), Version.CURRENT)); assertTrue(error.getMessage().contains("got unexpected warning header") && error.getMessage().contains("test")); assertTrue(error.getMessage().contains("the following regular expression did not match any warning header [\\\\n\\\\tjunk\\\\n]\\\\n")); //allow header section = new DoSection(new XContentLocation(1, 1)); section.setAllowedWarningHeadersRegex(singletonList(Pattern.compile("test"))); section.checkWarningHeaders(singletonList(testHeader), Version.CURRENT); //allow only one header section = new DoSection(new XContentLocation(1, 1)); section.setExpectedWarningHeadersRegex(singletonList(Pattern.compile("test"))); DoSection finalSection4 = section; error = expectThrows(AssertionError.class, () -> finalSection4.checkWarningHeaders(List.of(testHeader, realisticTestHeader), Version.CURRENT)); assertTrue(error.getMessage().contains("got unexpected warning header") && error.getMessage().contains("precedence during")); //the non-regex version does not need to worry about escaping since it is an exact match, and the code ensures that both //sides of the match are escaped the same... however for the regex version, it is done against the raw string minus the //prefix. For example, the raw string looks like this: //299 Elasticsearch-8.0.0-SNAPSHOT-d0ea206e300dab312f47611e22850bf799ca6192 "test" //where 299 Elasticsearch-8.0.0-SNAPSHOT-d0ea206e300dab312f47611e22850bf799ca6192 is the prefix, //so the match is against [test] (no brackets). If the message itself has quotes/backslashes then the raw string will look like: //299 Elasticsearch-8.0.0-SNAPSHOT-d0ea206e300dab312f47611e22850bf799ca6192 "test \\\\"with quotes and \\\\\\\\ backslashes\\\\"" //and the match is against [test \\\\"with quotes and \\\\\\\\ backslashes\\\\"] (no brackets) .. so the regex needs account for the extra //backslashes. Escaping escape characters is annoying but it should be very rare and the non-regex version should be preferred section = new DoSection(new XContentLocation(1, 1)); section.setAllowedWarningHeadersRegex(singletonList( Pattern.compile("^test \\\\\\\\\\\\\\\\\\\\"with quotes and \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ backslashes\\\\\\\\\\\\\\\\\\\\"$"))); section.checkWarningHeaders(singletonList(testHeaderWithQuotesAndBackslashes), Version.CURRENT); }	no actual changes here...just realized we didn't have any coverage for escaped character matching.
DocValueFormat format() { return format; } } public static class Factory extends InternalRange.Factory<InternalDateRange.Bucket, InternalDateRange> { @Override public Type type() { return DateRangeAggregationBuilder.TYPE; } @Override public ValueType getValueType() { return ValueType.DATE; } @Override public InternalDateRange create(String name, List<InternalDateRange.Bucket> ranges, DocValueFormat formatter, boolean keyed, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) { return new InternalDateRange(name, ranges, formatter, keyed, pipelineAggregators, metaData); } @Override public InternalDateRange create(List<Bucket> ranges, InternalDateRange prototype) { return new InternalDateRange(prototype.name, ranges, prototype.format, prototype.keyed, prototype.pipelineAggregators(), prototype.metaData); } @Override public Bucket createBucket(String key, double from, double to, long docCount, InternalAggregations aggregations, boolean keyed, DocValueFormat formatter) { return new Bucket(key, from, to, docCount, aggregations, keyed, formatter); } @Override public Bucket createBucket(InternalAggregations aggregations, Bucket prototype) { return new Bucket(prototype.getKey(), prototype.internalGetFrom(), prototype.internalGetTo(), prototype.getDocCount(), aggregations, prototype.getKeyed(), prototype.getFormat()); } } InternalDateRange(String name, List<InternalDateRange.Bucket> ranges, DocValueFormat formatter, boolean keyed, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) { super(name, ranges, formatter, keyed, pipelineAggregators, metaData); } /** * Read from a stream. */ public InternalDateRange(StreamInput in) throws IOException { super(in); } @Override public String getWriteableName() { return DateRangeAggregationBuilder.NAME; }	let's not add getters and just access the members directly.
public static String asJsonArray(Stream<String> stream) { return "[" + stream .map(ESLogMessage::inQuotes) .collect(Collectors.joining(", ")) + "]"; }	since we eventually collect the arguments using .with( we could probably structure this in a way that avoids repeating them. it's probably worth it even if we end up using a builder. i'm thinking something similar to: https://github.com/logstash/logstash-logback-encoder#event-specific-custom-fields
public ReduceResult reduce(boolean onlyCount, List<PercolateShardResponse> shardResponses) throws IOException { if (onlyCount) { long finalCount = 0; for (PercolateShardResponse shardResponse : shardResponses) { finalCount += shardResponse.topDocs().totalHits; } InternalAggregations reducedAggregations = reduceAggregations(shardResponses); return new PercolatorService.ReduceResult(finalCount, reducedAggregations); } else { int requestedSize = shardResponses.get(0).requestedSize(); TopDocs[] shardResults = new TopDocs[shardResponses.size()]; long foundMatches = 0; for (int i = 0; i < shardResults.length; i++) { TopDocs shardResult = shardResponses.get(i).topDocs(); foundMatches += shardResult.totalHits; shardResults[i] = shardResult; } TopDocs merged = TopDocs.merge(requestedSize, shardResults); PercolateResponse.Match[] matches = new PercolateResponse.Match[merged.scoreDocs.length]; for (int i = 0; i < merged.scoreDocs.length; i++) { ScoreDoc doc = merged.scoreDocs[i]; PercolateShardResponse shardResponse = shardResponses.get(doc.shardIndex); String id = shardResponse.ids().get(doc.doc); Map<String, HighlightField> hl = shardResponse.hls().get(doc.doc); matches[i] = new PercolateResponse.Match(new Text(shardResponse.getIndex()), new Text(id), doc.score, hl); } InternalAggregations reducedAggregations = reduceAggregations(shardResponses); return new PercolatorService.ReduceResult(foundMatches, matches, reducedAggregations); } }	indentation looks wrong here
public Aggregator wrap(final Aggregator in) { return new WrappedAggregator(in); }	i'm not super comfortable about exposing this but i can't think of a much better way to actually close the aggregations during testing without reworking aggregations to make parent aggs close their subaggregations.
@SuppressWarnings("unchecked") public void testPreviewTransform() throws Exception { final Request createPreviewRequest = new Request("POST", DATAFRAME_ENDPOINT + "_preview"); String config = "{" + " \\\\"source\\\\": \\\\"reviews\\\\"," + " \\\\"id\\\\": \\\\"doesnot-matter\\\\"," + " \\\\"dest\\\\": \\\\"doesnot-matter\\\\","; config += " \\\\"pivot\\\\": {" + " \\\\"group_by\\\\": [" + " {\\\\"reviewer\\\\": {\\\\"terms\\\\": { \\\\"field\\\\": \\\\"user_id\\\\" }}}," + " {\\\\"by_day\\\\": {\\\\"date_histogram\\\\": {\\\\"interval\\\\": \\\\"1d\\\\",\\\\"field\\\\":\\\\"timestamp\\\\",\\\\"format\\\\":\\\\"yyyy-MM-DD\\\\"}}}]," + " \\\\"aggregations\\\\": {" + " \\\\"avg_rating\\\\": {" + " \\\\"avg\\\\": {" + " \\\\"field\\\\": \\\\"stars\\\\"" + " } } } }" + "}"; createPreviewRequest.setJsonEntity(config); Map<String, Object> previewDataframeResponse = entityAsMap(client().performRequest(createPreviewRequest)); List<Map<String, Object>> preview = (List<Map<String, Object>>)previewDataframeResponse.get("data_frame_preview"); assertThat(preview.size(), equalTo(393)); Set<String> expectedFields = new HashSet<>(Arrays.asList("reviewer", "by_day", "avg_rating")); preview.forEach(p -> { Set<String> keys = p.keySet(); assertThat(keys, equalTo(expectedFields)); }); }	the [, ] need to be removed
static void fillInRows(String clusterName, String indexName, Map<String, EsField> mapping, String prefix, List<List<?>> rows, Pattern columnMatcher, boolean isOdbcClient) { int pos = 0; for (Map.Entry<String, EsField> entry : mapping.entrySet()) { pos++; // JDBC is 1-based so we start with 1 here String name = entry.getKey(); name = prefix != null ? prefix + "." + name : name; EsField field = entry.getValue(); DataType type = field.getDataType(); // skip the nested and object types only for ODBC // https://github.com/elastic/elasticsearch/issues/35376 boolean isObjectType = type == NESTED || type == OBJECT; if (!(isObjectType && isOdbcClient)) { if (columnMatcher == null || columnMatcher.matcher(name).matches()) { rows.add(asList(clusterName, // schema is not supported null, indexName, name, odbcCompatible(type.sqlType.getVendorTypeNumber(), isOdbcClient), type.esType.toUpperCase(Locale.ROOT), type.displaySize, // TODO: is the buffer_length correct? type.size, // no DECIMAL support null, odbcCompatible(DataTypes.metaSqlRadix(type), isOdbcClient), // everything is nullable odbcCompatible(DatabaseMetaData.columnNullable, isOdbcClient), // no remarks null, // no column def null, // SQL_DATA_TYPE apparently needs to be same as DATA_TYPE except for datetime and interval data types odbcCompatible(DataTypes.metaSqlDataType(type), isOdbcClient), // SQL_DATETIME_SUB ? odbcCompatible(DataTypes.metaSqlDateTimeSub(type), isOdbcClient), // char octet length type.isString() || type == DataType.BINARY ? type.size : null, // position pos, "YES", null, null, null, null, "NO", "NO" )); } } if (field.getProperties() != null) { fillInRows(clusterName, indexName, field.getProperties(), name, rows, columnMatcher, isOdbcClient); } } }	there's already a method doing that on datatype - isprimitive.
private Builder initializeEmpty(IndexMetaData indexMetaData, UnassignedInfo unassignedInfo) { assert indexMetaData.getIndex().equals(index); if (!shards.isEmpty()) { throw new IllegalStateException("trying to initialize an index with fresh shards, but already has shards created"); } for (int shardNumber = 0; shardNumber < indexMetaData.getNumberOfShards(); shardNumber++) { ShardId shardId = new ShardId(index, shardNumber); final RecoverySource primaryRecoverySource; if (indexMetaData.inSyncAllocationIds(shardNumber).isEmpty() == false) { // we have previous valid copies for this shard. use them for recovery primaryRecoverySource = StoreRecoverySource.EXISTING_STORE_INSTANCE; } else if (indexMetaData.getCreationVersion().before(Version.V_5_0_0_alpha1) && unassignedInfo.getReason() != UnassignedInfo.Reason.INDEX_CREATED // tests can create old indices ) { // the index is old and didn't maintain inSyncAllocationIds. Fall back to all behavior and require // finding existing copies primaryRecoverySource = StoreRecoverySource.EXISTING_STORE_INSTANCE; } else if (indexMetaData.getMergeSourceIndex() != null) { // this is a new index but the initial shards should merged from another index primaryRecoverySource = LocalShardsRecoverySource.INSTANCE; } else { // a freshly created index with no restriction primaryRecoverySource = StoreRecoverySource.EMPTY_STORE_INSTANCE; } IndexShardRoutingTable.Builder indexShardRoutingBuilder = new IndexShardRoutingTable.Builder(shardId); for (int i = 0; i <= indexMetaData.getNumberOfReplicas(); i++) { boolean primary = i == 0; indexShardRoutingBuilder.addShard(ShardRouting.newUnassigned(shardId, primary, primary ? primaryRecoverySource : PeerRecoverySource.INSTANCE, unassignedInfo)); } shards.put(shardNumber, indexShardRoutingBuilder.build()); } return this; }	fall back to _old_ behavior
public void testObject() throws Exception { IndexService indexService = createIndex("test"); DocumentMapperParser parser = indexService.mapperService().documentMapperParser(); String mapping = Strings.toString(XContentFactory.jsonBuilder().startObject() .startObject("type").endObject() .endObject()); DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping)); assertEquals(mapping, serialize(mapper)); Mapper update = parse(mapper, parser, XContentFactory.jsonBuilder().startObject().startObject("foo").startObject("bar") .field("baz", "foo").endObject().endObject().endObject()); assertNotNull(update); // original mapping not modified assertEquals(mapping, serialize(mapper)); // but we have an update assertEquals(Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties") .startObject("foo").startObject("properties").startObject("bar").startObject("properties").startObject("baz") .field("type", "text") .startObject("fields").startObject("keyword").field("type", "keyword") .field("ignore_above", 256).endObject() .endObject().endObject().endObject().endObject().endObject().endObject() .endObject().endObject().endObject()), serialize(update)); }	i tend to try an indent these manually so they *look* a little more like json. not that this is required, but it does help when they get big like this.
private Iterable<ListBlobItem> getListBlobItems(CloudBlobContainer blobContainer, String path, OperationContext opContext) { return blobContainer.listBlobs(path,true, EnumSet.noneOf(BlobListingDetails.class),null, opContext); }	nit: missing spaces before ,true and ,null
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(name); out.writeInt(threads); out.writeInt(queue); out.writeInt(active); out.writeLong(rejected); out.writeInt(largest); out.writeLong(completed); out.writeInt(min); out.writeInt(max); }	same as above, we need to write or not depending on the version of the node we are talking to.
@Override public void readFrom(StreamInput in) throws IOException { name = in.readString(); threads = in.readInt(); queue = in.readInt(); active = in.readInt(); rejected = in.readLong(); largest = in.readInt(); completed = in.readLong(); min = in.readInt(); max = in.readInt(); }	this one breaks backwards compatibility on the transport layer. can you add a check for the version and read these new fields or not depending on it? if (in.getversion().onorafter(version.v_1_2_0)) { min = in.readint(); max = in.readint(); }
@Override public void onCancelled() { // The task cancellation task automatically finds children and cancels them, nothing extra to do for leaders if (isWorker()) { workerState.handleCancel(); } }	it'd be nice to also mention that tasks that do not yet know if they are leaders or followers also have nothing to do as they can wait.
static Request closeIndex(CloseIndexRequest closeIndexRequest) { String endpoint = RequestConverters.endpoint(closeIndexRequest.indices(), "_close"); Request request = new Request(HttpPost.METHOD_NAME, endpoint); RequestConverters.Params parameters = new RequestConverters.Params(); parameters.withTimeout(closeIndexRequest.timeout()); parameters.withMasterTimeout(closeIndexRequest.masterNodeTimeout()); parameters.withIndicesOptions(closeIndexRequest.indicesOptions()); parameters.withWaitForActiveShards(closeIndexRequest.waitForActiveShards()); request.addParameters(parameters.asMap()); return request; }	this is actually a bugfix, we didn't respect this parameter before this change.
@Override public void error(String msg) { logger.error(msg); }	this is horrific, thank you for catching this, i need to take this one out of this pr right now and bring it all the way back to the 5.4 branch which i'm not likely to do with the remainder of these changes.
public void getForecastStats(String jobId, Consumer<ForecastStats> handler, Consumer<Exception> errorHandler) { String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId); QueryBuilder termQuery = new TermsQueryBuilder(Result.RESULT_TYPE.getPreferredName(), ForecastRequestStats.RESULT_TYPE_VALUE); QueryBuilder jobQuery = new TermsQueryBuilder(Job.ID.getPreferredName(), jobId); QueryBuilder finalQuery = new BoolQueryBuilder().filter(termQuery).filter(jobQuery); SearchRequest searchRequest = new SearchRequest(indexName); searchRequest.indicesOptions(MlIndicesUtils.addIgnoreUnavailable(searchRequest.indicesOptions())); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(finalQuery); sourceBuilder.aggregation( AggregationBuilders.stats(ForecastStats.Fields.MEMORY).field(ForecastRequestStats.MEMORY_USAGE.getPreferredName())); sourceBuilder.aggregation(AggregationBuilders.stats(ForecastStats.Fields.RECORDS) .field(ForecastRequestStats.PROCESSED_RECORD_COUNT.getPreferredName())); sourceBuilder.aggregation( AggregationBuilders.stats(ForecastStats.Fields.RUNTIME).field(ForecastRequestStats.PROCESSING_TIME_MS.getPreferredName())); Script durationScript = new Script("doc['" + ForecastRequestStats.END_TIME + "'].value.getMillis() - doc['" + ForecastRequestStats.START_TIME + "'].value.getMillis()"); sourceBuilder.aggregation(AggregationBuilders.stats(Fields.DURATION).script(durationScript)); sourceBuilder.aggregation( AggregationBuilders.terms(ForecastStats.Fields.STATUSES).field(ForecastRequestStats.STATUS.getPreferredName())); sourceBuilder.size(0); searchRequest.source(sourceBuilder); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest, ActionListener.<SearchResponse>wrap(searchResponse -> { long totalHits = searchResponse.getHits().getTotalHits(); Aggregations aggregations = searchResponse.getAggregations(); if (totalHits == 0 || aggregations == null) { handler.accept(new ForecastStats()); return; } Map<String, Aggregation> aggregationsAsMap = aggregations.asMap(); StatsAccumulator memoryStats = StatsAccumulator .fromStatsAggregation((Stats) aggregationsAsMap.get(ForecastStats.Fields.MEMORY)); StatsAccumulator recordStats = StatsAccumulator .fromStatsAggregation((Stats) aggregationsAsMap.get(ForecastStats.Fields.RECORDS)); StatsAccumulator runtimeStats = StatsAccumulator .fromStatsAggregation((Stats) aggregationsAsMap.get(ForecastStats.Fields.RUNTIME)); StatsAccumulator durationStats = StatsAccumulator .fromStatsAggregation((Stats) aggregationsAsMap.get(ForecastStats.Fields.DURATION)); CountAccumulator statusCount = CountAccumulator .fromTermsAggregation((StringTerms) aggregationsAsMap.get(ForecastStats.Fields.STATUSES)); ForecastStats forecastStats = new ForecastStats(totalHits, memoryStats, recordStats, runtimeStats, durationStats, statusCount); handler.accept(forecastStats); }, errorHandler), client::search); }	it would be nice to have test coverage of this. there is a jobproviderit class could you index some forecast stats in that and assert this method.
public GetResult extractGetResult(final UpdateRequest request, String concreteIndex, long version, final Map<String, Object> source, XContentType sourceContentType, @Nullable final BytesReference sourceAsBytes) { if ((request.fields() == null || request.fields().length == 0) && (request.fetchSource() == null || request.fetchSource().fetchSource() == false)) { return null; } SourceLookup sourceLookup = new SourceLookup(); sourceLookup.setSource(source); boolean sourceRequested = false; Map<String, GetField> fields = null; if (request.fields() != null && request.fields().length > 0) { for (String field : request.fields()) { if (field.equals("_source")) { sourceRequested = true; continue; } Object value = sourceLookup.extractValue(field); if (value != null) { if (fields == null) { fields = new HashMap<>(2); } GetField getField = fields.get(field); if (getField == null) { getField = new GetField(field, new ArrayList<>(2)); fields.put(field, getField); } getField.getValues().add(value); } } } BytesReference sourceFilteredAsBytes = sourceAsBytes; if (request.fetchSource() != null) { sourceRequested = true; Object value = sourceLookup.filter(request.fetchSource().includes(), request.fetchSource().excludes()); try { final int initialCapacity = Math.min(1024, sourceAsBytes.length()); BytesStreamOutput streamOutput = new BytesStreamOutput(initialCapacity); try (XContentBuilder builder = new XContentBuilder(sourceContentType.xContent(), streamOutput)) { builder.value(value); sourceFilteredAsBytes = builder.bytes(); } } catch (IOException e) { throw new ElasticsearchException("Error filtering source", e); } } // TODO when using delete/none, we can still return the source as bytes by generating it (using the sourceContentType) return new GetResult(concreteIndex, request.type(), request.id(), version, true, sourceRequested ? sourceFilteredAsBytes : null, fields); }	is it worth optimizing the case where fetchsource is true and includes and excludes are empty and the xcontents line up? then you can just return the bytes you have like we do for search lookups.
* @param generation generation of the data stream * @return backing index name */ public static String getDefaultBackingIndexName(String dataStreamName, long generation) { return String.format(Locale.ROOT, ".ds-%s-%06d", dataStreamName, generation); }	maybe turn .ds- into a constant?
private HitContext prepareNonNestedHitContext(SearchContext context, SearchLookup lookup, FieldsVisitor fieldsVisitor, int docId, Map<String, Set<String>> storedToRequestedFields, LeafReaderContext subReaderContext, CheckedBiConsumer<Integer, FieldsVisitor, IOException> fieldReader) throws IOException { int subDocId = docId - subReaderContext.docBase; if (fieldsVisitor == null) { SearchHit hit = new SearchHit(docId, null, null, null); lookup.source().setSegmentAndDocument(subReaderContext, subDocId); return new HitContext(hit, subReaderContext, subDocId, lookup.source()); } else { SearchHit hit; loadStoredFields(context.getQueryShardContext()::getFieldType, fieldReader, fieldsVisitor, subDocId); if (fieldsVisitor.fields().isEmpty() == false) { Map<String, DocumentField> docFields = new HashMap<>(); Map<String, DocumentField> metaFields = new HashMap<>(); fillDocAndMetaFields(context, fieldsVisitor, storedToRequestedFields, docFields, metaFields); hit = new SearchHit(docId, fieldsVisitor.id(), docFields, metaFields); } else { hit = new SearchHit(docId, fieldsVisitor.id(), emptyMap(), emptyMap()); } Source source = fieldsVisitor.source() == null ? Source.EMPTY_SOURCE : Source.fromBytes(fieldsVisitor.source()); return new HitContext(hit, subReaderContext, subDocId, source); } } /** * Resets the provided {@link HitContext} with information on the current * nested document. This includes the following: * - Adding an initial {@link SearchHit} instance. * - Loading the document source, filtering it based on the nested document ID, then * setting it on {@link SourceLookup}	i think that this preloaded source will no longer be set on the fetch searchlookup, meaning it's not shared with runtime fields or other fetch phases that use scripts. i opened #65292 to help simplify + clarify source sharing. would you be okay reviewing that first, then returning to this pr? sorry for 'cutting in line', i just think that change will help you move forward on a couple prs while avoiding regressions.
protected abstract void scriptChangedRouting(RequestWrapper<?> request, Object to); } public enum OpType { NOOP("noop"), INDEX("index"), DELETE("delete"); private final String id; OpType(String id) { this.id = id; } public static OpType fromString(String opType) { String lowerOpType = opType.toLowerCase(Locale.ROOT); switch (lowerOpType) { case "noop": return OpType.NOOP; case "index": return OpType.INDEX; case "delete": return OpType.DELETE; default: throw new IllegalArgumentException("Operation type [" + lowerOpType + "] not allowed, only " + Arrays.toString(values()) + " are allowed"); } } @Override public String toString() { return id.toLowerCase(Locale.ROOT); } } static class ScrollConsumableHitsResponse { private final ScrollableHitSource.AsyncResponse asyncResponse; private final List<? extends ScrollableHitSource.Hit> hits; private final AtomicInteger consumedOffset = new AtomicInteger(0); ScrollConsumableHitsResponse(ScrollableHitSource.AsyncResponse asyncResponse) { this.asyncResponse = asyncResponse; this.hits = asyncResponse.response().getHits(); } ScrollableHitSource.Response response() { return asyncResponse.response(); } List<? extends ScrollableHitSource.Hit> consumeRemainingHits() { return consumeHits(remainingHits()); } List<? extends ScrollableHitSource.Hit> consumeHits(int numberOfHits) { if (numberOfHits < 0) { throw new IllegalArgumentException("Invalid number of hits to consume [" + numberOfHits + "]"); } if (numberOfHits > remainingHits()) { throw new IllegalArgumentException( "Unable to provide [" + numberOfHits + "] hits as there are only [" + remainingHits() + "] hits available" ); } int start = consumedOffset.get(); int end = consumedOffset.addAndGet(numberOfHits); return hits.subList(start, end); } boolean hasRemainingHits() { return remainingHits() > 0; } int remainingHits() { return hits.size() - consumedOffset.get(); } void done(TimeValue extraKeepAlive) { asyncResponse.done(extraKeepAlive); }	i think this can just be an int?
public static TransformNodeAssignments transformTaskNodes(List<String> transformIds, ClusterState clusterState) { Set<String> executorNodes = new HashSet<>(); Set<String> assigned = new HashSet<>(); Set<String> waitingForAssignment = new HashSet<>(); PersistentTasksCustomMetadata tasksMetadata = PersistentTasksCustomMetadata.getPersistentTasksCustomMetadata(clusterState); if (tasksMetadata != null) { Set<String> transformIdsSet = new HashSet<>(transformIds); Collection<PersistentTasksCustomMetadata.PersistentTask<?>> tasks = tasksMetadata.findTasks( TransformField.TASK_NAME, t -> transformIdsSet.contains(t.getId()) ); for (PersistentTasksCustomMetadata.PersistentTask<?> task : tasks) { if (task.isAssigned()) { executorNodes.add(task.getExecutorNode()); assigned.add(task.getId()); } else { waitingForAssignment.add(task.getId()); } } } Set<String> stopped = transformIds.stream() .filter(id -> (assigned.contains(id) || waitingForAssignment.contains(id) == false)) .collect(Collectors.toSet()); return new TransformNodeAssignments(executorNodes, assigned, waitingForAssignment, stopped); }	shouldn't the parentheses be different here? .filter(id -> (assigned.contains(id) || waitingforassignment.contains(id)) == false)
@Override protected void doClose() { BlobStore store = blobStore.get(); if (store != null) { try { store.close(); } catch (Exception t) { logger.warn("cannot close blob store", t); } } }	this is not concurrency-safe. closing can be happening concurrently to this.
@Override public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (source != null && source.trackTotalHits() == false && scroll() != null) { validationException = addValidationError("disabling [track_total_hits] is not allowed in a scroll context", validationException); } if (source != null && source.from() > 0 && scroll() != null) { validationException = addValidationError("using [from] is not allowed in a scroll context", validationException); } if (requestCache != null && requestCache && scroll() != null) { validationException = addValidationError("[request_cache] cannot be used in a a scroll context", validationException); } return validationException; }	so this will not be backported to 6.x and instead we will print a deprecation log?
public void testAppendingUniqueValueToScalar() throws Exception { IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random()); String originalValue = randomAlphaOfLengthBetween(1, 10); String field = RandomDocumentPicks.addRandomField(random(), ingestDocument, originalValue); List<Object> valuesToAppend = new ArrayList<>(); String newValue = originalValue + randomAlphaOfLengthBetween(1, 10); valuesToAppend.add(newValue); Processor appendProcessor = createAppendProcessor(field, valuesToAppend, false); appendProcessor.execute(ingestDocument); List<?> list = ingestDocument.getFieldValue(field, List.class); assertThat(list.size(), equalTo(2)); assertThat(list, equalTo(List.of(originalValue, newValue))); }	maybe use randomvalueotherthan? it'll communicate intent better, i think
@Override public RemoveProcessor create( Map<String, Processor.Factory> registry, String processorTag, String description, Map<String, Object> config ) throws Exception { final List<TemplateScript.Factory> compiledTemplatesToRemove = getTemplates(processorTag, config, "field"); final List<TemplateScript.Factory> compiledTemplatesToKeep = getTemplates(processorTag, config, "keep"); if (compiledTemplatesToRemove.isEmpty() && compiledTemplatesToKeep.isEmpty()) { throw new IllegalArgumentException( "missing field [processors.remove.keep] or [processors.remove.field]. Please specify one of them." ); } if (compiledTemplatesToRemove.isEmpty() == false && compiledTemplatesToKeep.isEmpty() == false) { throw new IllegalArgumentException( "Too many fields specified. Please specify either [processors.remove.keep] or [processors.remove.field]." ); } boolean ignoreMissing = ConfigurationUtils.readBooleanProperty(TYPE, processorTag, config, "ignore_missing", false); return new RemoveProcessor(processorTag, description, compiledTemplatesToRemove, compiledTemplatesToKeep, ignoreMissing); }	for the sake of consistency, let's use the same exception and message style as other processors with mutually exclusive configuration options. see the [network direction processor](https://github.com/elastic/elasticsearch/blob/master/modules/ingest-common/src/main/java/org/elasticsearch/ingest/common/networkdirectionprocessor.java#l269) for an example.
static Request putLicense(PutLicenseRequest putLicenseRequest) { Request request = new Request(HttpPut.METHOD_NAME, "/_xpack/license"); Params parameters = new Params(request); parameters.withTimeout(putLicenseRequest.timeout()); parameters.withMasterTimeout(putLicenseRequest.masterNodeTimeout()); if (putLicenseRequest.isAcknowledge()) { parameters.putParam("acknowledge", "true"); } request.setJsonEntity(putLicenseRequest.getLicenseDefinition()); return request; }	please use the endpointbuilder here
static Request putLicense(PutLicenseRequest putLicenseRequest) { Request request = new Request(HttpPut.METHOD_NAME, "/_xpack/license"); Params parameters = new Params(request); parameters.withTimeout(putLicenseRequest.timeout()); parameters.withMasterTimeout(putLicenseRequest.masterNodeTimeout()); if (putLicenseRequest.isAcknowledge()) { parameters.putParam("acknowledge", "true"); } request.setJsonEntity(putLicenseRequest.getLicenseDefinition()); return request; }	any reason u went with setjsonentity vs the existing which sets a bytearrayentity? edit: im falling down a rabbit hole now on blocking vs nonblocking entity use on our hlrc :)
@Override public AutoscalingDeciderResult scale(Settings configuration, AutoscalingDeciderContext context) { AutoscalingCapacity autoscalingCapacity = context.currentCapacity(); if (autoscalingCapacity == null || autoscalingCapacity.total().storage() == null) { return new AutoscalingDeciderResult(null, new ReactiveReason("current capacity not available", -1, -1)); } AllocationState allocationState = allocationState(context); var assignedBytesUnmovableShards = allocationState.storagePreventsRemainOrMove(); var unassignedBytesUnassignedShards = allocationState.storagePreventsAllocation(); long unassignedBytes = unassignedBytesUnassignedShards.bytes(); long assignedBytes = assignedBytesUnmovableShards.bytes(); long maxShardSize = allocationState.maxShardSize(); assert assignedBytes >= 0; assert unassignedBytes >= 0; assert maxShardSize >= 0; String message = message(unassignedBytes, assignedBytes); AutoscalingCapacity requiredCapacity = AutoscalingCapacity.builder() .total(autoscalingCapacity.total().storage().getBytes() + unassignedBytes + assignedBytes, null) .node(maxShardSize, null) .build(); return new AutoscalingDeciderResult( requiredCapacity, new ReactiveReason( message, unassignedBytes, assignedBytes, unassignedBytesUnassignedShards.shardIds(), assignedBytesUnmovableShards.shardIds() ) ); }	nit: i wonder if it makes sense to keep the total size and shard ids together as they are related?
@Override public AutoscalingDeciderResult scale(Settings configuration, AutoscalingDeciderContext context) { AutoscalingCapacity autoscalingCapacity = context.currentCapacity(); if (autoscalingCapacity == null || autoscalingCapacity.total().storage() == null) { return new AutoscalingDeciderResult(null, new ReactiveReason("current capacity not available", -1, -1)); } AllocationState allocationState = allocationState(context); var assignedBytesUnmovableShards = allocationState.storagePreventsRemainOrMove(); var unassignedBytesUnassignedShards = allocationState.storagePreventsAllocation(); long unassignedBytes = unassignedBytesUnassignedShards.bytes(); long assignedBytes = assignedBytesUnmovableShards.bytes(); long maxShardSize = allocationState.maxShardSize(); assert assignedBytes >= 0; assert unassignedBytes >= 0; assert maxShardSize >= 0; String message = message(unassignedBytes, assignedBytes); AutoscalingCapacity requiredCapacity = AutoscalingCapacity.builder() .total(autoscalingCapacity.total().storage().getBytes() + unassignedBytes + assignedBytes, null) .node(maxShardSize, null) .build(); return new AutoscalingDeciderResult( requiredCapacity, new ReactiveReason( message, unassignedBytes, assignedBytes, unassignedBytesUnassignedShards.shardIds(), assignedBytesUnmovableShards.shardIds() ) ); }	i think we mentioned that it would be nice to provide some the allocation debug info for few of the unassigned shards, so it is easier to understand why a shard is unassigned? wdyt @henningandersen?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { throw new UnsupportedOperationException(); } } } public static class ReactiveReason implements AutoscalingDeciderResult.Reason { private final String reason; private final long unassigned; private final long assigned; private final SortedSet<ShardId> unassignedShardIds; private final SortedSet<ShardId> assignedShardIds; public ReactiveReason(String reason, long unassigned, long assigned) { this(reason, unassigned, assigned, Collections.emptySortedSet(), Collections.emptySortedSet()); } ReactiveReason( String reason, long unassigned, long assigned, SortedSet<ShardId> unassignedShardIds, SortedSet<ShardId> assignedShardIds ) { this.reason = reason; this.unassigned = unassigned; this.assigned = assigned; this.unassignedShardIds = unassignedShardIds; this.assignedShardIds = assignedShardIds; } public ReactiveReason(StreamInput in) throws IOException { this.reason = in.readString(); this.unassigned = in.readLong(); this.assigned = in.readLong(); if (in.getVersion().onOrAfter(Version.V_8_3_0)) { unassignedShardIds = Collections.unmodifiableSortedSet(new TreeSet<>(in.readSet(ShardId::new))); assignedShardIds = Collections.unmodifiableSortedSet(new TreeSet<>(in.readSet(ShardId::new))); } else { unassignedShardIds = Collections.emptySortedSet(); assignedShardIds = Collections.emptySortedSet(); } } @Override public String summary() { return reason; } public long unassigned() { return unassigned; } public long assigned() { return assigned; } public SortedSet<ShardId> unassignedShardIds() { return unassignedShardIds; } public SortedSet<ShardId> assignedShardIds() { return assignedShardIds; } @Override public String getWriteableName() { return ReactiveStorageDeciderService.NAME; } @Override public void writeTo(StreamOutput out) throws IOException { out.writeString(reason); out.writeLong(unassigned); out.writeLong(assigned); if (out.getVersion().onOrAfter(Version.V_8_3_0)) { out.writeCollection(unassignedShardIds); out.writeCollection(assignedShardIds); } } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("reason", reason); builder.field("unassigned", unassigned); builder.field("assigned", assigned); builder.field("unassigned_shard_ids", unassignedShardIds); builder.field("assigned_shard_ids", assignedShardIds.stream().limit(512).collect(Collectors.toList())); builder.field("amount_of_assigned_shards", assignedShardIds.size()); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ReactiveReason that = (ReactiveReason) o; return unassigned == that.unassigned && assigned == that.assigned && reason.equals(that.reason) && unassignedShardIds.equals(that.unassignedShardIds) && assignedShardIds.equals(that.assignedShardIds); }	nit: can you extract that version to a constant?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(reason); out.writeLong(unassigned); out.writeLong(assigned); if (out.getVersion().onOrAfter(Version.V_8_3_0)) { out.writeCollection(unassignedShardIds); out.writeCollection(assignedShardIds); } }	nit: can you extract that version to a constant?
InternalInferModelAction.Request buildRequest(IngestDocument ingestDocument) { Map<String, Object> fields = new HashMap<>(ingestDocument.getSourceAndMetadata()); if (fieldMapping != null) { fieldMapping.forEach((src, dest) -> { Object srcValue = MapHelper.dig(src, fields); if (srcValue != null) { fields.put(dest, srcValue); } }); } return new InternalInferModelAction.Request(modelId, fields, inferenceConfig, previouslyLicensed); }	this is not the same as src is left in the fields map. i don't think it matters in terms of the logic only that a larger map than is necessary it being passed around.
private void replaceCallWithConstant( InvokeCallMemberNode irInvokeCallMemberNode, Consumer<ExpressionNode> scope, Method javaMethod, Object receiver ) { Object[] args = new Object[irInvokeCallMemberNode.getArgumentNodes().size()]; for (int i = 0; i < irInvokeCallMemberNode.getArgumentNodes().size(); i++) { ExpressionNode argNode = irInvokeCallMemberNode.getArgumentNodes().get(i); IRDConstant constantDecoration = argNode.getDecoration(IRDConstant.class); if (constantDecoration == null) { throw irInvokeCallMemberNode.getLocation() .createError( new IllegalArgumentException( "The [" + javaMethod.getName() + "] method is sad because it needs constant arguments to work properly. " + "Please provide a constant to the [" + (i + 1) + "] argument" ) ); } args[i] = constantDecoration.getValue(); } Object result; try { result = javaMethod.invoke(receiver, args); } catch (IllegalAccessException | IllegalArgumentException e) { throw irInvokeCallMemberNode.getLocation() .createError(new IllegalArgumentException("error invoking [" + irInvokeCallMemberNode + "] at compile time", e)); } catch (InvocationTargetException e) { throw irInvokeCallMemberNode.getLocation() .createError(new IllegalArgumentException("error invoking [" + irInvokeCallMemberNode + "] at compile time", e.getCause())); } ConstantNode replacement = new ConstantNode(irInvokeCallMemberNode.getLocation()); replacement.attachDecoration(new IRDConstant(result)); replacement.attachDecoration(irInvokeCallMemberNode.getDecoration(IRDExpressionType.class)); scope.accept(replacement); }	i suppose it's kinder, but i was really looking for something different. i'm not totally sure what - that's why i was vague in the issue. i think my concerns were more around things like "if there is a single argument this sentence sounds silly" and "maybe we should use ordinal numbers like second instead of [2]". i don't think being polite is really important here.
@Override public int parseInto(DateTimeParserBucket bucket, String text, int position) { boolean isPositive = text.startsWith("-") == false; int firstDotIndex = text.indexOf((int)'.'); boolean isTooLong = (firstDotIndex == -1 ? text.length() : firstDotIndex) > estimateParsedLength(); if (bucket.getZone() != DateTimeZone.UTC) { String format = hasMilliSecondPrecision ? "epoch_millis" : "epoch_second"; throw new IllegalArgumentException("time_zone must be UTC for format [" + format + "]"); } else if (isPositive && isTooLong) { return -1; } int factor = hasMilliSecondPrecision ? 1 : 1000; try { long millis = new BigDecimal(text).longValue() * factor; DateTime dt = new DateTime(millis, DateTimeZone.UTC); bucket.saveField(DateTimeFieldType.year(), dt.getYear()); bucket.saveField(DateTimeFieldType.monthOfYear(), dt.getMonthOfYear()); bucket.saveField(DateTimeFieldType.dayOfMonth(), dt.getDayOfMonth()); bucket.saveField(DateTimeFieldType.hourOfDay(), dt.getHourOfDay()); bucket.saveField(DateTimeFieldType.minuteOfHour(), dt.getMinuteOfHour()); bucket.saveField(DateTimeFieldType.secondOfMinute(), dt.getSecondOfMinute()); bucket.saveField(DateTimeFieldType.millisOfSecond(), dt.getMillisOfSecond()); bucket.setZone(DateTimeZone.UTC); } catch (Exception e) { return -1; } return text.length(); } } public static class EpochTimePrinter implements DateTimePrinter { private boolean hasMilliSecondPrecision; public EpochTimePrinter(boolean hasMilliSecondPrecision) { this.hasMilliSecondPrecision = hasMilliSecondPrecision; } @Override public int estimatePrintedLength() { return hasMilliSecondPrecision ? 19 : 16; } /** * We adjust the instant by displayOffset to adjust for the offset that might have been added in * {@link DateTimeFormatter#printTo(Appendable, long, Chronology)} when using a time zone. */ @Override public void printTo(StringBuffer buf, long instant, Chronology chrono, int displayOffset, DateTimeZone displayZone, Locale locale) { if (hasMilliSecondPrecision) { buf.append(instant - displayOffset); } else { buf.append((instant - displayOffset) / 1000); } } /** * We adjust the instant by displayOffset to adjust for the offset that might have been added in * {@link DateTimeFormatter#printTo(Appendable, long, Chronology)} when using a time zone. */ @Override public void printTo(Writer out, long instant, Chronology chrono, int displayOffset, DateTimeZone displayZone, Locale locale) throws IOException { if (hasMilliSecondPrecision) { out.write(String.valueOf(instant - displayOffset)); } else { out.append(String.valueOf((instant - displayOffset) / 1000)); } } @Override public void printTo(StringBuffer buf, ReadablePartial partial, Locale locale) { if (hasMilliSecondPrecision) { buf.append(String.valueOf(getDateTimeMillis(partial))); } else { buf.append(String.valueOf(getDateTimeMillis(partial) / 1000)); } } @Override public void printTo(Writer out, ReadablePartial partial, Locale locale) throws IOException { if (hasMilliSecondPrecision) { out.append(String.valueOf(getDateTimeMillis(partial))); } else { out.append(String.valueOf(getDateTimeMillis(partial) / 1000)); } } private long getDateTimeMillis(ReadablePartial partial) { int year = partial.get(DateTimeFieldType.year()); int monthOfYear = partial.get(DateTimeFieldType.monthOfYear()); int dayOfMonth = partial.get(DateTimeFieldType.dayOfMonth()); int hourOfDay = partial.get(DateTimeFieldType.hourOfDay()); int minuteOfHour = partial.get(DateTimeFieldType.minuteOfHour()); int secondOfMinute = partial.get(DateTimeFieldType.secondOfMinute()); int millisOfSecond = partial.get(DateTimeFieldType.millisOfSecond()); return partial.getChronology().getDateTimeMillis(year, monthOfYear, dayOfMonth, hourOfDay, minuteOfHour, secondOfMinute, millisOfSecond); }	nit: i think we don't need the int cast here, it is done implicitely. at least my ide removes it on "save"
@Override public int parseInto(DateTimeParserBucket bucket, String text, int position) { boolean isPositive = text.startsWith("-") == false; int firstDotIndex = text.indexOf((int)'.'); boolean isTooLong = (firstDotIndex == -1 ? text.length() : firstDotIndex) > estimateParsedLength(); if (bucket.getZone() != DateTimeZone.UTC) { String format = hasMilliSecondPrecision ? "epoch_millis" : "epoch_second"; throw new IllegalArgumentException("time_zone must be UTC for format [" + format + "]"); } else if (isPositive && isTooLong) { return -1; } int factor = hasMilliSecondPrecision ? 1 : 1000; try { long millis = new BigDecimal(text).longValue() * factor; DateTime dt = new DateTime(millis, DateTimeZone.UTC); bucket.saveField(DateTimeFieldType.year(), dt.getYear()); bucket.saveField(DateTimeFieldType.monthOfYear(), dt.getMonthOfYear()); bucket.saveField(DateTimeFieldType.dayOfMonth(), dt.getDayOfMonth()); bucket.saveField(DateTimeFieldType.hourOfDay(), dt.getHourOfDay()); bucket.saveField(DateTimeFieldType.minuteOfHour(), dt.getMinuteOfHour()); bucket.saveField(DateTimeFieldType.secondOfMinute(), dt.getSecondOfMinute()); bucket.saveField(DateTimeFieldType.millisOfSecond(), dt.getMillisOfSecond()); bucket.setZone(DateTimeZone.UTC); } catch (Exception e) { return -1; } return text.length(); } } public static class EpochTimePrinter implements DateTimePrinter { private boolean hasMilliSecondPrecision; public EpochTimePrinter(boolean hasMilliSecondPrecision) { this.hasMilliSecondPrecision = hasMilliSecondPrecision; } @Override public int estimatePrintedLength() { return hasMilliSecondPrecision ? 19 : 16; } /** * We adjust the instant by displayOffset to adjust for the offset that might have been added in * {@link DateTimeFormatter#printTo(Appendable, long, Chronology)} when using a time zone. */ @Override public void printTo(StringBuffer buf, long instant, Chronology chrono, int displayOffset, DateTimeZone displayZone, Locale locale) { if (hasMilliSecondPrecision) { buf.append(instant - displayOffset); } else { buf.append((instant - displayOffset) / 1000); } } /** * We adjust the instant by displayOffset to adjust for the offset that might have been added in * {@link DateTimeFormatter#printTo(Appendable, long, Chronology)} when using a time zone. */ @Override public void printTo(Writer out, long instant, Chronology chrono, int displayOffset, DateTimeZone displayZone, Locale locale) throws IOException { if (hasMilliSecondPrecision) { out.write(String.valueOf(instant - displayOffset)); } else { out.append(String.valueOf((instant - displayOffset) / 1000)); } } @Override public void printTo(StringBuffer buf, ReadablePartial partial, Locale locale) { if (hasMilliSecondPrecision) { buf.append(String.valueOf(getDateTimeMillis(partial))); } else { buf.append(String.valueOf(getDateTimeMillis(partial) / 1000)); } } @Override public void printTo(Writer out, ReadablePartial partial, Locale locale) throws IOException { if (hasMilliSecondPrecision) { out.append(String.valueOf(getDateTimeMillis(partial))); } else { out.append(String.valueOf(getDateTimeMillis(partial) / 1000)); } } private long getDateTimeMillis(ReadablePartial partial) { int year = partial.get(DateTimeFieldType.year()); int monthOfYear = partial.get(DateTimeFieldType.monthOfYear()); int dayOfMonth = partial.get(DateTimeFieldType.dayOfMonth()); int hourOfDay = partial.get(DateTimeFieldType.hourOfDay()); int minuteOfHour = partial.get(DateTimeFieldType.minuteOfHour()); int secondOfMinute = partial.get(DateTimeFieldType.secondOfMinute()); int millisOfSecond = partial.get(DateTimeFieldType.millisOfSecond()); return partial.getChronology().getDateTimeMillis(year, monthOfYear, dayOfMonth, hourOfDay, minuteOfHour, secondOfMinute, millisOfSecond); }	nice, so this can handle all kinds of formats it seems.
public void testFloatEpochFormat() throws IOException { String mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field").field("type", "date") .field("format", "epoch_millis").endObject().endObject() .endObject().endObject().string(); DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping)); assertEquals(mapping, mapper.mappingSource().toString()); long millisFromEpoch = randomNonNegativeLong(); String epochFloatValue = String.format(Locale.US, "%d.%d", millisFromEpoch, randomNonNegativeLong()); ParsedDocument doc = mapper.parse(SourceToParse.source("test", "type", "1", XContentFactory.jsonBuilder() .startObject() .field("field", epochFloatValue) .endObject() .bytes(), XContentType.JSON)); IndexableField[] fields = doc.rootDoc().getFields("field"); assertEquals(2, fields.length); IndexableField pointField = fields[0]; assertEquals(millisFromEpoch, pointField.numericValue().longValue()); }	maybe also randomly append a negative prefix to also test parsing negative values here?
@Override public void doTestCoerce(String type) throws IOException { XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field").field("type", type); if (type.equals("date_range")) { mapping = mapping.field("format", DATE_FORMAT); } mapping = mapping.endObject().endObject().endObject().endObject(); DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping.string())); assertEquals(mapping.string(), mapper.mappingSource().toString()); ParsedDocument doc = mapper.parse(SourceToParse.source("test", "type", "1", XContentFactory.jsonBuilder() .startObject() .startObject("field") .field(getFromField(), getFrom(type)) .field(getToField(), getTo(type)) .endObject() .endObject().bytes(), XContentType.JSON)); IndexableField[] fields = doc.rootDoc().getFields("field"); assertEquals(2, fields.length); IndexableField dvField = fields[0]; assertEquals(DocValuesType.BINARY, dvField.fieldType().docValuesType()); IndexableField pointField = fields[1]; assertEquals(2, pointField.fieldType().pointDimensionCount()); // date_range ignores the coerce parameter and epoch_millis date format truncates floats (see issue: #14641) if (type.equals("date_range")) { return; } mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field").field("type", type).field("coerce", false).endObject().endObject() .endObject().endObject(); DocumentMapper mapper2 = parser.parse("type", new CompressedXContent(mapping.string())); assertEquals(mapping.string(), mapper2.mappingSource().toString()); ThrowingRunnable runnable = () -> mapper2.parse(SourceToParse.source("test", "type", "1", XContentFactory.jsonBuilder() .startObject() .startObject("field") .field(getFromField(), "5.2") .field(getToField(), "10") .endObject() .endObject().bytes(), XContentType.JSON)); MapperParsingException e = expectThrows(MapperParsingException.class, runnable); assertThat(e.getCause().getMessage(), anyOf(containsString("passed as String"), containsString("failed to parse date"), containsString("is not an IP string literal"))); }	nit: maybe just personal preference, but early returns in test look strange to me. can you change this to execute the rest of the test only for type.equals("date_range") == false)
private Settings getRandomNodeSettings(long seed) { Random random = new Random(seed); Builder builder = Settings.settingsBuilder(); if (isLocalTransportConfigured() == false) { builder.put(Transport.TRANSPORT_TCP_COMPRESS.getKey(), rarely(random)); } if (random.nextBoolean()) { builder.put("cache.recycler.page.type", RandomPicks.randomFrom(random, PageCacheRecycler.Type.values())); } if (random.nextInt(10) == 0) { // 10% of the nodes have a very frequent check interval builder.put(SearchService.KEEPALIVE_INTERVAL_SETTING.getKey(), TimeValue.timeValueMillis(10 + random.nextInt(2000))); } else if (random.nextInt(10) != 0) { // 90% of the time - 10% of the time we don't set anything builder.put(SearchService.KEEPALIVE_INTERVAL_SETTING.getKey(), TimeValue.timeValueSeconds(10 + random.nextInt(5 * 60))); } if (random.nextBoolean()) { // sometimes set a builder.put(SearchService.DEFAULT_KEEPALIVE_SETTING.getKey(), TimeValue.timeValueSeconds(100 + random.nextInt(5 * 60))); } builder.put(EsExecutors.PROCESSORS_SETTING.getKey(), 1 + random.nextInt(3)); if (random.nextBoolean()) { if (random.nextBoolean()) { builder.put("indices.fielddata.cache.size", 1 + random.nextInt(1000), ByteSizeUnit.MB); } } // randomize netty settings if (random.nextBoolean()) { builder.put(NettyTransport.WORKER_COUNT, random.nextInt(3) + 1); builder.put(NettyTransport.CONNECTIONS_PER_NODE_RECOVERY, random.nextInt(2) + 1); builder.put(NettyTransport.CONNECTIONS_PER_NODE_BULK, random.nextInt(3) + 1); builder.put(NettyTransport.CONNECTIONS_PER_NODE_REG, random.nextInt(6) + 1); } if (random.nextBoolean()) { builder.put(MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING.getKey(), new TimeValue(RandomInts.randomIntBetween(random, 10, 30), TimeUnit.SECONDS)); } if (random.nextInt(10) == 0) { builder.put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING.getKey(), "noop"); builder.put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING.getKey(), "noop"); } if (random.nextBoolean()) { builder.put(IndexModule.INDEX_QUERY_CACHE_TYPE_SETTING.getKey(), random.nextBoolean() ? IndexModule.INDEX_QUERY_CACHE : IndexModule.NONE_QUERY_CACHE); } if (random.nextBoolean()) { builder.put(IndexModule.INDEX_QUERY_CACHE_EVERYTHING_SETTING.getKey(), random.nextBoolean()); } if (random.nextBoolean()) { if (random.nextInt(10) == 0) { // do something crazy slow here builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 10), ByteSizeUnit.MB)); } else { builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 10, 200), ByteSizeUnit.MB)); } } if (random.nextBoolean()) { builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING.getKey(), RandomPicks.randomFrom(random, StoreRateLimiting.Type.values())); } if (random.nextBoolean()) { if (random.nextInt(10) == 0) { // do something crazy slow here builder.put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 10), ByteSizeUnit.MB)); } else { builder.put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 10, 200), ByteSizeUnit.MB)); } } if (random.nextBoolean()) { builder.put(NettyTransport.PING_SCHEDULE, RandomInts.randomIntBetween(random, 100, 2000) + "ms"); } if (random.nextBoolean()) { builder.put(ScriptService.SCRIPT_CACHE_SIZE_SETTING.getKey(), RandomInts.randomIntBetween(random, 0, 2000)); } if (random.nextBoolean()) { builder.put(ScriptService.SCRIPT_CACHE_EXPIRE_SETTING, TimeValue.timeValueMillis(RandomInts.randomIntBetween(random, 750, 10000000))); } // always default delayed allocation to 0 to make sure we have tests are not delayed builder.put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), 0); return builder.build(); }	is the effect the same as before? we were changing this only rarely before while now we do it all the time?
static List<FollowerInfo> getFollowInfos(List<String> concreteFollowerIndices, ClusterState state) { List<FollowerInfo> followerInfos = new ArrayList<>(); PersistentTasksCustomMetaData persistentTasks = state.metaData().custom(PersistentTasksCustomMetaData.TYPE); for (String index : concreteFollowerIndices) { IndexMetaData indexMetaData = state.metaData().index(index); Map<String, String> ccrCustomData = indexMetaData.getCustomData(Ccr.CCR_CUSTOM_METADATA_KEY); if (ccrCustomData != null) { Optional<ShardFollowTask> result; if (persistentTasks != null) { result = persistentTasks.taskMap().values().stream() .map(persistentTask -> (ShardFollowTask) persistentTask.getParams()) .filter(shardFollowTask -> index.equals(shardFollowTask.getFollowShardId().getIndexName())) .findAny(); } else { result = Optional.empty(); } String followerIndex = indexMetaData.getIndex().getName(); String remoteCluster = ccrCustomData.get(Ccr.CCR_CUSTOM_METADATA_REMOTE_CLUSTER_NAME_KEY); String leaderIndex = ccrCustomData.get(Ccr.CCR_CUSTOM_METADATA_LEADER_INDEX_NAME_KEY); if (result.isPresent()) { ShardFollowTask params = result.get(); FollowParameters followParameters = new FollowParameters( params.getMaxReadRequestOperationCount(), params.getMaxReadRequestSize(), params.getMaxOutstandingReadRequests(), params.getMaxWriteRequestOperationCount(), params.getMaxWriteRequestSize(), params.getMaxOutstandingWriteRequests(), params.getMaxWriteBufferCount(), params.getMaxWriteBufferSize(), params.getMaxRetryDelay(), params.getReadPollTimeout() ); followerInfos.add(new FollowerInfo(followerIndex, remoteCluster, leaderIndex, Status.ACTIVE, followParameters)); } else { followerInfos.add(new FollowerInfo(followerIndex, remoteCluster, leaderIndex, Status.PAUSED, null)); } } } return followerInfos; }	i think we need to filter shardfollowtask task before casting. maybe use persistenttasks.findtasks api. i will make a pr to inject other persistent tasks randomly in our it tests.
static DeprecationIssue nodeLeftDelayedTimeCheck(IndexMetaData indexMetaData) { String setting = UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(); String value = indexMetaData.getSettings().get(setting); if (Strings.isNullOrEmpty(value) == false) { TimeValue parsedValue = TimeValue.parseTimeValue(value, setting); if (parsedValue.getNanos() < 0) { return new DeprecationIssue(DeprecationIssue.Level.WARNING, "Negative values for index.unassigned.node_left.delayed_timeout are deprecated and should be set to 0", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-7.0.html" + "#_literal_index_unassigned_node_left_delayed_timeout_literal_may_no_longer_be_negative", "The index [" + indexMetaData.getIndex().getName() + "] is set to " + value); } } return null; }	please could you use setting.getkey() instead of the literal "index.unassigned.node_left.delayed_timeout"?
static DeprecationIssue nodeLeftDelayedTimeCheck(IndexMetaData indexMetaData) { String setting = UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(); String value = indexMetaData.getSettings().get(setting); if (Strings.isNullOrEmpty(value) == false) { TimeValue parsedValue = TimeValue.parseTimeValue(value, setting); if (parsedValue.getNanos() < 0) { return new DeprecationIssue(DeprecationIssue.Level.WARNING, "Negative values for index.unassigned.node_left.delayed_timeout are deprecated and should be set to 0", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-7.0.html" + "#_literal_index_unassigned_node_left_delayed_timeout_literal_may_no_longer_be_negative", "The index [" + indexMetaData.getIndex().getName() + "] is set to " + value); } } return null; }	i think it's more clear if this details message reads something like: this index has [index.unassigned.node_left.delayed_timeout] set to [-43d], but negative values are not allowed
static void innerHitsExecute(Query mainQuery, IndexSearcher indexSearcher, SearchHit[] hits, Version indexVersionCreated) throws IOException { List<PercolateQuery> percolateQueries = locatePercolatorQuery(mainQuery); if (percolateQueries.isEmpty()) { return; } boolean singlePercolateQuery = percolateQueries.size() == 1; for (PercolateQuery percolateQuery : percolateQueries) { String fieldName = singlePercolateQuery ? FIELD_NAME_PREFIX : FIELD_NAME_PREFIX + "_" + percolateQuery.getName(); IndexSearcher percolatorIndexSearcher = percolateQuery.getPercolatorIndexSearcher(); Query nonNestedQuery = Queries.newNonNestedFilter(indexVersionCreated); Weight weight = percolatorIndexSearcher.createWeight(percolatorIndexSearcher.rewrite(nonNestedQuery), ScoreMode.COMPLETE_NO_SCORES, 1f); Scorer s = weight.scorer(percolatorIndexSearcher.getIndexReader().leaves().get(0)); int memoryIndexMaxDoc = percolatorIndexSearcher.getIndexReader().maxDoc(); BitSet rootDocs = BitSet.of(s.iterator(), memoryIndexMaxDoc); int[] rootDocsBySlot = null; boolean hasNestedDocs = rootDocs.cardinality() != percolatorIndexSearcher.getIndexReader().numDocs(); if (hasNestedDocs) { rootDocsBySlot = buildRootDocsSlots(rootDocs); } PercolateQuery.QueryStore queryStore = percolateQuery.getQueryStore(); List<LeafReaderContext> ctxs = indexSearcher.getIndexReader().leaves(); for (SearchHit hit : hits) { LeafReaderContext ctx = ctxs.get(ReaderUtil.subIndex(hit.docId(), ctxs)); int segmentDocId = hit.docId() - ctx.docBase; Query query = queryStore.getQueries(ctx).apply(segmentDocId); if (query == null) { // This is not a document with a percolator field. continue; } TopDocs topDocs = percolatorIndexSearcher.search(query, memoryIndexMaxDoc, new Sort(SortField.FIELD_DOC)); if (topDocs.totalHits.value == 0) { // This hit didn't match with a percolate query, // likely to happen when percolating multiple documents continue; } Map<String, DocumentField> fields = hit.fieldsOrNull(); if (fields == null) { fields = new HashMap<>(); hit.fields(fields); } IntStream slots = convertTopDocsToSlots(topDocs, rootDocsBySlot); fields.put(fieldName, new DocumentField(fieldName, slots.boxed().collect(Collectors.toList()))); } } }	we can remove the version here too since it only checks if the index was created before 6.1.0. we can do this in a follow up but we probably don't need to add the indexversioncreated param here and just use current until we remove it from newnonnestedfilter ?
public void performOn( final ShardRouting replica, final ReplicaRequest request, final long globalCheckpoint, final long maxSeqNoOfUpdatesOrDeletes, final ActionListener<ReplicationOperation.ReplicaResponse> listener) { String nodeId = replica.currentNodeId(); final DiscoveryNode node = clusterService.state().nodes().get(nodeId); if (node == null) { listener.onFailure(new NoNodeAvailableException("unknown node [" + nodeId + "]")); return; } final ConcreteReplicaRequest<ReplicaRequest> replicaRequest = new ConcreteReplicaRequest<>( request, replica.allocationId().getId(), primaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes); final ActionListenerResponseHandler<ReplicaResponse> handler = new ActionListenerResponseHandler<>(listener, in -> { ReplicaResponse replicaResponse = new ReplicaResponse(); replicaResponse.readFrom(in); return replicaResponse; }); transportService.sendRequest(node, transportReplicaAction, replicaRequest, transportOptions, handler); }	perhaps, while you're at it, implement the streaminput constructor for replicaresponse and override readfrom by throwing exception, just like addvotingconfigexclusionsresponse does. this can then just be new actionlistenerresponsehandler<>(listener, replicaresponse::new)
InferenceConfig inferenceConfigFromMap(Map<String, Object> inferenceConfig) { ExceptionsHelper.requireNonNull(inferenceConfig, INFERENCE_CONFIG); //throw ExceptionsHelper.badRequestException("Cannot create processor as configured." + // " The following fields contain conflicting values {}", // stringUniquenessVerifier.getFieldsWithDuplicateValues()); if (inferenceConfig.size() != 1) { throw ExceptionsHelper.badRequestException("{} must be an object with one inference type mapped to an object.", INFERENCE_CONFIG); } Object value = inferenceConfig.values().iterator().next(); if ((value instanceof Map<?, ?>) == false) { throw ExceptionsHelper.badRequestException("{} must be an object with one inference type mapped to an object.", INFERENCE_CONFIG); } @SuppressWarnings("unchecked") Map<String, Object> valueMap = (Map<String, Object>)value; if (inferenceConfig.containsKey(ClassificationConfig.NAME)) { checkSupportedVersion(ClassificationConfig.EMPTY_PARAMS); ClassificationConfig config = ClassificationConfig.fromMap(valueMap); checkFieldUniqueness(config.getResultsField(), config.getTopClassesResultsField()); return config; } else if (inferenceConfig.containsKey(RegressionConfig.NAME)) { checkSupportedVersion(RegressionConfig.EMPTY_PARAMS); RegressionConfig config = RegressionConfig.fromMap(valueMap); checkFieldUniqueness(config.getResultsField()); return config; } else { throw ExceptionsHelper.badRequestException("unrecognized inference configuration type {}. Supported types {}", inferenceConfig.keySet(), Arrays.asList(ClassificationConfig.NAME, RegressionConfig.NAME)); } }	i think these commented lines should be removed.
@Override protected void sendApplyCommit(DiscoveryNode destination, ApplyCommitRequest applyCommit, ActionListener<Empty> responseActionListener) { publicationContext.sendApplyCommit(destination, applyCommit, wrapWithMutex(responseActionListener)); } } public static Settings.Builder addZen1Attribute(Settings.Builder builder) { return builder.put("node.attr.zen1", true); }	perhaps insert: // only here temporarily for bwc development, todo remove once complete assuming we are going to remove this before ga?
@Override protected void sendApplyCommit(DiscoveryNode destination, ApplyCommitRequest applyCommit, ActionListener<Empty> responseActionListener) { publicationContext.sendApplyCommit(destination, applyCommit, wrapWithMutex(responseActionListener)); } } public static Settings.Builder addZen1Attribute(Settings.Builder builder) { return builder.put("node.attr.zen1", true); }	similarly, if the zen1 attribute is going to go away then we should note this here.
private void handleWakeUp() { if (running() == false) { logger.trace("handleWakeUp: not running"); return; } final FollowerCheckRequest request = new FollowerCheckRequest(fastResponseState.term, transportService.getLocalNode()); logger.trace("handleWakeUp: checking {} with {}", discoveryNode, request); final String actionName; final TransportRequest transportRequest; if (Coordinator.isZen1Node(discoveryNode)) { actionName = NodesFaultDetection.PING_ACTION_NAME; transportRequest = new NodesFaultDetection.PingRequest(discoveryNode, ClusterName.CLUSTER_NAME_SETTING.get(settings), transportService.getLocalNode(), 0L); } else { actionName = FOLLOWER_CHECK_ACTION_NAME; transportRequest = request; } transportService.sendRequest(discoveryNode, actionName, transportRequest, TransportRequestOptions.builder().withTimeout(followerCheckTimeout).withType(Type.PING).build(), new TransportResponseHandler<Empty>() { @Override public Empty read(StreamInput in) { return Empty.INSTANCE; } @Override public void handleResponse(Empty response) { if (running() == false) { logger.trace("{} no longer running", FollowerChecker.this); return; } failureCountSinceLastSuccess = 0; logger.trace("{} check successful", FollowerChecker.this); scheduleNextWakeUp(); } @Override public void handleException(TransportException exp) { if (running() == false) { logger.debug(new ParameterizedMessage("{} no longer running", FollowerChecker.this), exp); return; } failureCountSinceLastSuccess++; final String reason; if (failureCountSinceLastSuccess >= followerCheckRetryCount) { logger.debug(() -> new ParameterizedMessage("{} failed too many times", FollowerChecker.this), exp); reason = "followers check retry count exceeded"; } else if (exp instanceof ConnectTransportException || exp.getCause() instanceof ConnectTransportException) { logger.debug(() -> new ParameterizedMessage("{} disconnected", FollowerChecker.this), exp); reason = "disconnected"; } else { logger.debug(() -> new ParameterizedMessage("{} failed, retrying", FollowerChecker.this), exp); scheduleNextWakeUp(); return; } failNode(reason); } @Override public String executor() { return Names.SAME; } }); }	maybe clusterstate.unknown_version instead of 0l?
private void requestPeers() { assert holdsLock() : "PeerFinder mutex not held"; assert peersRequestInFlight == false : "PeersRequest already in flight"; assert active; final DiscoveryNode discoveryNode = getDiscoveryNode(); assert discoveryNode != null : "cannot request peers without first connecting"; logger.trace("{} requesting peers", this); peersRequestInFlight = true; final List<DiscoveryNode> knownNodes = getFoundPeersUnderLock(); final TransportResponseHandler<PeersResponse> peersResponseHandler = new TransportResponseHandler<PeersResponse>() { @Override public PeersResponse read(StreamInput in) throws IOException { return new PeersResponse(in); } @Override public void handleResponse(PeersResponse response) { logger.trace("{} received {}", Peer.this, response); synchronized (mutex) { if (active == false) { return; } peersRequestInFlight = false; response.getMasterNode().map(DiscoveryNode::getAddress).ifPresent(PeerFinder.this::startProbe); response.getKnownPeers().stream().map(DiscoveryNode::getAddress).forEach(PeerFinder.this::startProbe); } if (response.getMasterNode().equals(Optional.of(discoveryNode))) { // Must not hold lock here to avoid deadlock assert holdsLock() == false : "PeerFinder mutex is held in error"; onActiveMasterFound(discoveryNode, response.getTerm()); } } @Override public void handleException(TransportException exp) { peersRequestInFlight = false; logger.debug(new ParameterizedMessage("{} peers request failed", Peer.this), exp); } @Override public String executor() { return Names.GENERIC; } }; final String actionName; final TransportRequest transportRequest; final TransportResponseHandler<?> transportResponseHandler; if (Coordinator.isZen1Node(discoveryNode)) { actionName = UnicastZenPing.ACTION_NAME; transportRequest = new UnicastZenPing.UnicastPingRequest(1, ZenDiscovery.PING_TIMEOUT_SETTING.get(settings), new ZenPing.PingResponse(getLocalNode(), null, ClusterName.CLUSTER_NAME_SETTING.get(settings), 0L)); transportResponseHandler = peersResponseHandler.wrap(ucResponse -> { Optional<DiscoveryNode> optionalMasterNode = Arrays.stream(ucResponse.pingResponses) .filter(pr -> discoveryNode.equals(pr.node()) && discoveryNode.equals(pr.master())) .map(ZenPing.PingResponse::node) .findFirst(); List<DiscoveryNode> discoveredNodes = new ArrayList<>(); if (optionalMasterNode.isPresent() == false) { Arrays.stream(ucResponse.pingResponses).map(pr -> pr.master()).filter(Objects::nonNull) .forEach(discoveredNodes::add); Arrays.stream(ucResponse.pingResponses).map(pr -> pr.node()).forEach(discoveredNodes::add); } return new PeersResponse(optionalMasterNode, discoveredNodes, 0L); }, UnicastZenPing.UnicastPingResponse::new); } else { actionName = REQUEST_PEERS_ACTION_NAME; transportRequest = new PeersRequest(getLocalNode(), knownNodes); transportResponseHandler = peersResponseHandler; } transportService.sendRequest(discoveryNode, actionName, transportRequest, TransportRequestOptions.builder().withTimeout(requestPeersTimeout).build(), transportResponseHandler); }	maybe clusterstate.unknown_version instead of 0l?
@Override public void messageReceived(UnicastZenPing.UnicastPingRequest request, TransportChannel channel, Task task) throws Exception { final PeersRequest peersRequest = new PeersRequest(request.pingResponse.node(), Optional.ofNullable(request.pingResponse.master()).map(Collections::singletonList).orElse(Collections.emptyList())); final PeersResponse peersResponse = handlePeersRequest(peersRequest); final List<ZenPing.PingResponse> pingResponses = new ArrayList<>(); final ClusterName clusterName = ClusterName.CLUSTER_NAME_SETTING.get(settings); pingResponses.add(new ZenPing.PingResponse(transportService.getLocalNode(), peersResponse.getMasterNode().orElse(null), clusterName, 0L)); peersResponse.getKnownPeers().forEach(dn -> pingResponses.add(new ZenPing.PingResponse(dn, null, clusterName, 0L))); channel.sendResponse(new UnicastZenPing.UnicastPingResponse(request.id, pingResponses.toArray(new ZenPing.PingResponse[0]))); }	maybe clusterstate.unknown_version instead of 0l?
private boolean matchByIP(String[] values, @Nullable String hostIp, @Nullable String publishIp) { for (String ipOrHost : values) { String value = InetAddresses.isInetAddress(ipOrHost) ? NetworkAddress.format(InetAddresses.forString(ipOrHost)) : ipOrHost; boolean matchIp = Regex.simpleMatch(value, hostIp) || Regex.simpleMatch(value, publishIp); if (matchIp) { return matchIp; } } return false; }	can you move this up to the top where the other class variables are please?
default AuthenticationFailureHandler getAuthenticationFailureHandler() { return null; } /** * Returns an ordered list of role providers that are used to resolve role names * to {@link RoleDescriptor} objects. Each provider is invoked in order to * resolve any role names not resolved by the reserved or native roles stores. * * Each role provider is represented as a {@link BiConsumer} which takes a set * of roles to resolve as the first parameter to consume and an {@link ActionListener} * as the second parameter to consume. The implementation of the role provider * should be asynchronous if the computation is lengthy or any disk and/or network * I/O is involved. The implementation is responsible for resolving whatever roles * it can into a set of {@link RoleDescriptor} instances. If successful, the * implementation must wrap the set of {@link RoleDescriptor} instances in a * {@link RoleRetrievalResult} using {@link RoleRetrievalResult#success(Set)} and then invoke * {@link ActionListener#onResponse(Object)}. If a failure was encountered, the * implementation should wrap the failure in a {@link RoleRetrievalResult} using * {@link RoleRetrievalResult#failure(Exception)} and then invoke * {@link ActionListener#onResponse(Object)} unless the failure needs to terminate the request, * in which case the implementation should invoke {@link ActionListener#onFailure(Exception)}	is this going to be breaking change? i see we are backporting to 6.5.0 if so is it ok?
private void roleDescriptors(Set<String> roleNames, ActionListener<RolesRetrievalResult> rolesResultListener) { final Set<String> filteredRoleNames = roleNames.stream().filter((s) -> { if (negativeLookupCache.get(s) != null) { logger.debug("Requested role [{}] does not exist (cached)", s); return false; } else { return true; } }).collect(Collectors.toSet()); final RolesRetrievalResult retrievalResult = new RolesRetrievalResult(); final Set<RoleDescriptor> builtInRoleDescriptors = getBuiltInRoleDescriptors(filteredRoleNames); retrievalResult.addDescriptors(builtInRoleDescriptors); Set<String> remainingRoleNames = difference(filteredRoleNames, builtInRoleDescriptors); if (remainingRoleNames.isEmpty()) { rolesResultListener.onResponse(retrievalResult); } else { final Set<RoleDescriptor> fileRoleDescriptors = getDescriptorsFromFileStore(remainingRoleNames); retrievalResult.addDescriptors(fileRoleDescriptors); remainingRoleNames = difference(remainingRoleNames, fileRoleDescriptors); if (remainingRoleNames.isEmpty()) { rolesResultListener.onResponse(retrievalResult); } else { loadNativeRoleDescriptors(retrievalResult, rolesResultListener, remainingRoleNames); } } }	there seems to be duplicate code, with your changes in iteratingactionlistener we could do something as follows and reuse the same logic as for iterating on custom roles provider: final rolesretrievalresult rolesretrievalresult = new rolesretrievalresult(); set<string> remainingrolenames = new hashset<>(filteredrolenames); list<biconsumer<set<string>, actionlistener<roleretrievalresult>>> rolesproviders = new arraylist<>(); // for reserved, file and native roles store rolesproviders.add(...); rolesproviders.add(...); rolesproviders.add(...); // if licensed, custom roles provider if (licensestate.iscustomroleprovidersallowed() && customrolesproviders.isempty() == false) { rolesproviders.addall(customrolesproviders); } ... ... new iteratingactionlistener<>(descriptorslistener, (rolesprovider, listener) -> { // try to resolve descriptors with role provider rolesprovider.accept(remainingrolenames, actionlistener.wrap(result -> { listener.onresponse(result); }, listener::onfailure)); }, rolesproviders, threadcontext, function.identity(), iterationpredicate).run(); may be we could refactor to have an interface common between reserved, files, native roles store, which would make this more simplified and easy to follow. wdyt?
public static boolean isPartialSearchableSnapshotIndex(Map<Setting<?>, Object> indexSettings) { assert indexSettings.containsKey(INDEX_STORE_TYPE_SETTING) : "must include store type in map"; assert indexSettings.get(SNAPSHOT_PARTIAL_SETTING) != null : "partial setting must be non-null in map (has default value)"; return SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOT_STORE_TYPE.equals(indexSettings.get(INDEX_STORE_TYPE_SETTING)) && (boolean) indexSettings.get(SNAPSHOT_PARTIAL_SETTING); }	this has been removed from x-pack core and placed into the searchable snapshot plugin instead.
public void testDefaultValueForPreference() { assertThat(DataTierAllocationDecider.INDEX_ROUTING_PREFER_SETTING.get(Settings.EMPTY), equalTo("")); Settings.Builder builder = Settings.builder(); builder.put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOT_STORE_TYPE); builder.put(SearchableSnapshotsConstants.SNAPSHOT_PARTIAL_SETTING.getKey(), true); Settings settings = builder.build(); assertThat(DataTierAllocationDecider.INDEX_ROUTING_PREFER_SETTING.get(settings), equalTo(DATA_FROZEN)); }	these seem to be used in 7.x still, backport beware :)
public void testListGet() { String s = Debugger.toString("def x = new ArrayList(); x.add(5); return x.get(0);"); assertEquals(5, exec("def x = new ArrayList(); x.add(5); return x.get(0);")); assertEquals(5, exec("def x = new ArrayList(); x.add(5); def index = 0; return x.get(index);")); }	did you mean to leave this?
* @param vectorBR - dense vector encoded in BytesRef */ public static float[] decodeDenseVector(Version indexVersion, BytesRef vectorBR) { if (vectorBR == null) { throw new IllegalArgumentException("A document doesn't have a value for a vector field!"); } int dimCount = indexVersion.onOrAfter(Version.V_7_4_0) ? (vectorBR.length - INT_BYTES) / INT_BYTES : vectorBR.length/ INT_BYTES; float[] vector = new float[dimCount]; ByteBuffer byteBuffer = ByteBuffer.wrap(vectorBR.bytes, vectorBR.offset, vectorBR.length); for (int dim = 0; dim < dimCount; dim++) { vector[dim] = byteBuffer.getFloat(); } return vector; }	@jtibshirani i was wondering if we plan to eventually switch to decodeanddotproduct? if not how about using floatbuffer instead? with floatbuffer we get all values in bulk instead of loop. from benchmarks: java static float[] decodewithfloatbuffer(bytesref vectorbr) { if (vectorbr == null) { throw new illegalargumentexception("a document doesn't have a value for a vector field!"); } bytebuffer bytebuffer = bytebuffer.wrap(vectorbr.bytes, vectorbr.offset, vectorbr.length); floatbuffer floatbuffer = bytebuffer.asfloatbuffer(); float[] vector = new float[vectorbr.length / int_bytes]; floatbuffer.get(vector); return vector; } vectorfunctionbenchmark.decodenoop avgt 30 42.340 +- 0.706 ns/op vectorfunctionbenchmark.decodewithbytebuffer avgt 30 91.239 +- 1.644 ns/op vectorfunctionbenchmark.decodewithfloatbuffer avgt 30 72.101 +- 1.872 ns/op a similar comment for decodesparsevector.
public static BytesRef encodeSparseVector(Version indexVersion, int[] dims, float[] values, int dimCount) { // 1. Sort dims and values sortSparseDimsValues(dims, values, dimCount); byte[] bytes = indexVersion.onOrAfter(Version.V_7_4_0) ? new byte[dimCount * (INT_BYTES + SHORT_BYTES) + INT_BYTES] : new byte[dimCount * (INT_BYTES + SHORT_BYTES)]; // 2. Encode dimensions // as each dimension is a positive value that doesn't exceed 65535, 2 bytes is enough for encoding it ByteBuffer byteBuffer = ByteBuffer.wrap(bytes); for (int dim = 0; dim < dimCount; dim++) { int dimValue = dims[dim]; byteBuffer.put((byte) (dimValue >> 8)); byteBuffer.put((byte) dimValue); } // 3. Encode values double dotProduct = 0.0f; for (int dim = 0; dim < dimCount; dim++) { float value = values[dim]; byteBuffer.putFloat(value); dotProduct += value * value; } // 4. Encode vector magnitude at the end if (indexVersion.onOrAfter(Version.V_7_4_0)) { float vectorMagnitude = (float) Math.sqrt(dotProduct); byteBuffer.putFloat(vectorMagnitude); } return new BytesRef(bytes); }	i wonder if bytebuffer.putshort((short) dimvalue)) would be better?
@Override public String toString() { ArrayList<String> outputsWithValue = new ArrayList<>(values.length); for (int i = 0; i < values.length; i++) { outputsWithValue.add(output.get(i) + "=" + values[i]); } return "Singleton" + NodeUtils.limitedToString(outputsWithValue); }	the initial implementation seems better - using a stringbuilder instead of a list. the extra "[]" can be added to the builder.
private static PrivateKey parsePKCS8(BufferedReader bReader) throws IOException, GeneralSecurityException { StringBuilder sb = new StringBuilder(); String line = bReader.readLine(); while (line != null) { if (PKCS8_FOOTER.equals(line.trim())) { break; } sb.append(line.trim()); line = bReader.readLine(); } if (null == line || PKCS8_FOOTER.equals(line.trim()) == false) { throw new KeyException("Malformed PEM file, PEM footer is invalid or missing"); } byte[] keyBytes = Base64.getDecoder().decode(sb.toString()); String keyAlgo = getKeyAlgorithmIdentifier(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance(keyAlgo); return keyFactory.generatePrivate(new PKCS8EncodedKeySpec(keyBytes)); } /** * Creates a {@link PrivateKey} from the contents of {@code bReader} that contains an EC private key encoded in * OpenSSL traditional format. * * @param bReader the {@link BufferedReader} containing the key file contents * @param passwordSupplier A password supplier for the potentially encrypted (password protected) key * @return {@link PrivateKey} * @throws IOException if the file can't be read * @throws GeneralSecurityException if the private key can't be generated from the {@link ECPrivateKeySpec}	this change was needed because readprivatekey no longer catches & wraps ioexception (which, in turn, was needed in order to have specific handling of nosuchfileexception)
public void testCcrRepositoryFetchesSnapshotShardSizeFromIndexShardStoreStats() throws Exception { final String leaderIndex = "leader"; final int numberOfShards = randomIntBetween(1, 5); assertAcked(leaderClient().admin().indices().prepareCreate(leaderIndex) .setSource(getIndexSettings(numberOfShards, 0, Map.of(Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), TimeValue.ZERO.getStringRep())), XContentType.JSON)); final int numDocs = scaledRandomIntBetween(0, 500); if (numDocs > 0) { final BulkRequestBuilder bulkRequest = leaderClient().prepareBulk(leaderIndex); for (int i = 0; i < numDocs; i++) { bulkRequest.add(new IndexRequest(leaderIndex).id(Integer.toString(i)).source("field", i)); } assertThat(bulkRequest.get().hasFailures(), is(false)); } final ForceMergeResponse forceMergeResponse = leaderClient().admin().indices().prepareForceMerge(leaderIndex) .setMaxNumSegments(1) .setFlush(true) .get(); assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards)); assertThat(forceMergeResponse.getFailedShards(), equalTo(0)); ensureLeaderGreen(leaderIndex); final IndexStats indexStats = leaderClient().admin().indices().prepareStats(leaderIndex) .clear() .setStore(true) .get() .getIndex(leaderIndex); assertThat(indexStats.getIndexShards(), notNullValue()); assertThat(indexStats.getIndexShards(), aMapWithSize(numberOfShards)); final String leaderCluster = CcrRepository.NAME_PREFIX + "leader_cluster"; final RepositoriesService repositoriesService = getFollowerCluster().getCurrentMasterNodeInstance(RepositoriesService.class); final Repository repository = repositoriesService.repository(leaderCluster); assertThat(repository.getMetadata().type(), equalTo(CcrRepository.TYPE)); assertThat(repository.getMetadata().name(), equalTo(leaderCluster)); for (int shardId = 0; shardId < numberOfShards; shardId++) { IndexShardSnapshotStatus.Copy indexShardSnapshotStatus = repository.getShardSnapshotStatus( new SnapshotId(CcrRepository.LATEST, CcrRepository.LATEST), new IndexId(indexStats.getIndex(), indexStats.getUuid()), new ShardId(new Index(indexStats.getIndex(), indexStats.getUuid()), shardId)).asCopy(); assertThat(indexShardSnapshotStatus, notNullValue()); assertThat(indexShardSnapshotStatus.getStage(), is(IndexShardSnapshotStatus.Stage.DONE)); assertThat(indexShardSnapshotStatus.getTotalSize(), equalTo(indexStats.getIndexShards().get(shardId).getPrimary().getStore().getSizeInBytes())); } final String followerIndex = "follower"; final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class); final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class); final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class); final Map<Integer, Long> fetchedSnapshotShardSizes = new ConcurrentHashMap<>(); final PlainActionFuture<Void> waitForRestoreInProgress = PlainActionFuture.newFuture(); final ClusterStateListener listener = event -> { RestoreInProgress restoreInProgress = event.state().custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY); if (restoreInProgress != null && restoreInProgress.isEmpty() == false && event.state().routingTable().hasIndex(followerIndex)) { final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(followerIndex); for (ShardRouting shardRouting : indexRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED)) { if (shardRouting.unassignedInfo().getLastAllocationStatus() == AllocationStatus.FETCHING_SHARD_DATA) { try { assertBusy(() -> { final Long snapshotShardSize = snapshotsInfoService.snapshotShardSizes().getShardSize(shardRouting); assertThat(snapshotShardSize, notNullValue()); fetchedSnapshotShardSizes.put(shardRouting.getId(), snapshotShardSize); }, 30L, TimeUnit.SECONDS); } catch (Exception e) { throw new AssertionError("Failed to retrieve snapshot shard size for shard " + shardRouting); } } } logger.info("[{}/{}] snapshot shard sizes fetched", fetchedSnapshotShardSizes.size(), numberOfShards); if (fetchedSnapshotShardSizes.size() == numberOfShards) { waitForRestoreInProgress.onResponse(null); } } }; clusterService.addListener(listener); final RestoreSnapshotRequest restoreRequest = new RestoreSnapshotRequest(leaderCluster, CcrRepository.LATEST) .indices(leaderIndex).indicesOptions(indicesOptions).renamePattern("^(.*)$") .renameReplacement(followerIndex) .masterNodeTimeout(TimeValue.MAX_VALUE) .indexSettings(Settings.builder() .put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, followerIndex) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true)); restoreService.restoreSnapshot(restoreRequest, PlainActionFuture.newFuture()); waitForRestoreInProgress.get(30L, TimeUnit.SECONDS); clusterService.removeListener(listener); ensureFollowerGreen(followerIndex); for (int shardId = 0; shardId < numberOfShards; shardId++) { assertThat("Snapshot shard size fetched for follower shard [" + shardId + "] does not match leader store size", fetchedSnapshotShardSizes.get(shardId), equalTo(indexStats.getIndexShards().get(shardId).getPrimary().getStore().getSizeInBytes())); } assertHitCount(followerClient().prepareSearch(followerIndex).setSize(0).get(), numDocs); assertAcked(followerClient().admin().indices().prepareDelete(followerIndex).setMasterNodeTimeout(TimeValue.MAX_VALUE)); }	could just make this a plainactionfuture<map<integer, long>> and return the map from it instead of having it in the outer scope? :) makes it a little clearer when the map is coming from and you don't have to reason about why there's a chm when debugging this.
public void testCcrRepositoryFetchesSnapshotShardSizeFromIndexShardStoreStats() throws Exception { final String leaderIndex = "leader"; final int numberOfShards = randomIntBetween(1, 5); assertAcked(leaderClient().admin().indices().prepareCreate(leaderIndex) .setSource(getIndexSettings(numberOfShards, 0, Map.of(Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), TimeValue.ZERO.getStringRep())), XContentType.JSON)); final int numDocs = scaledRandomIntBetween(0, 500); if (numDocs > 0) { final BulkRequestBuilder bulkRequest = leaderClient().prepareBulk(leaderIndex); for (int i = 0; i < numDocs; i++) { bulkRequest.add(new IndexRequest(leaderIndex).id(Integer.toString(i)).source("field", i)); } assertThat(bulkRequest.get().hasFailures(), is(false)); } final ForceMergeResponse forceMergeResponse = leaderClient().admin().indices().prepareForceMerge(leaderIndex) .setMaxNumSegments(1) .setFlush(true) .get(); assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards)); assertThat(forceMergeResponse.getFailedShards(), equalTo(0)); ensureLeaderGreen(leaderIndex); final IndexStats indexStats = leaderClient().admin().indices().prepareStats(leaderIndex) .clear() .setStore(true) .get() .getIndex(leaderIndex); assertThat(indexStats.getIndexShards(), notNullValue()); assertThat(indexStats.getIndexShards(), aMapWithSize(numberOfShards)); final String leaderCluster = CcrRepository.NAME_PREFIX + "leader_cluster"; final RepositoriesService repositoriesService = getFollowerCluster().getCurrentMasterNodeInstance(RepositoriesService.class); final Repository repository = repositoriesService.repository(leaderCluster); assertThat(repository.getMetadata().type(), equalTo(CcrRepository.TYPE)); assertThat(repository.getMetadata().name(), equalTo(leaderCluster)); for (int shardId = 0; shardId < numberOfShards; shardId++) { IndexShardSnapshotStatus.Copy indexShardSnapshotStatus = repository.getShardSnapshotStatus( new SnapshotId(CcrRepository.LATEST, CcrRepository.LATEST), new IndexId(indexStats.getIndex(), indexStats.getUuid()), new ShardId(new Index(indexStats.getIndex(), indexStats.getUuid()), shardId)).asCopy(); assertThat(indexShardSnapshotStatus, notNullValue()); assertThat(indexShardSnapshotStatus.getStage(), is(IndexShardSnapshotStatus.Stage.DONE)); assertThat(indexShardSnapshotStatus.getTotalSize(), equalTo(indexStats.getIndexShards().get(shardId).getPrimary().getStore().getSizeInBytes())); } final String followerIndex = "follower"; final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class); final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class); final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class); final Map<Integer, Long> fetchedSnapshotShardSizes = new ConcurrentHashMap<>(); final PlainActionFuture<Void> waitForRestoreInProgress = PlainActionFuture.newFuture(); final ClusterStateListener listener = event -> { RestoreInProgress restoreInProgress = event.state().custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY); if (restoreInProgress != null && restoreInProgress.isEmpty() == false && event.state().routingTable().hasIndex(followerIndex)) { final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(followerIndex); for (ShardRouting shardRouting : indexRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED)) { if (shardRouting.unassignedInfo().getLastAllocationStatus() == AllocationStatus.FETCHING_SHARD_DATA) { try { assertBusy(() -> { final Long snapshotShardSize = snapshotsInfoService.snapshotShardSizes().getShardSize(shardRouting); assertThat(snapshotShardSize, notNullValue()); fetchedSnapshotShardSizes.put(shardRouting.getId(), snapshotShardSize); }, 30L, TimeUnit.SECONDS); } catch (Exception e) { throw new AssertionError("Failed to retrieve snapshot shard size for shard " + shardRouting); } } } logger.info("[{}/{}] snapshot shard sizes fetched", fetchedSnapshotShardSizes.size(), numberOfShards); if (fetchedSnapshotShardSizes.size() == numberOfShards) { waitForRestoreInProgress.onResponse(null); } } }; clusterService.addListener(listener); final RestoreSnapshotRequest restoreRequest = new RestoreSnapshotRequest(leaderCluster, CcrRepository.LATEST) .indices(leaderIndex).indicesOptions(indicesOptions).renamePattern("^(.*)$") .renameReplacement(followerIndex) .masterNodeTimeout(TimeValue.MAX_VALUE) .indexSettings(Settings.builder() .put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, followerIndex) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true)); restoreService.restoreSnapshot(restoreRequest, PlainActionFuture.newFuture()); waitForRestoreInProgress.get(30L, TimeUnit.SECONDS); clusterService.removeListener(listener); ensureFollowerGreen(followerIndex); for (int shardId = 0; shardId < numberOfShards; shardId++) { assertThat("Snapshot shard size fetched for follower shard [" + shardId + "] does not match leader store size", fetchedSnapshotShardSizes.get(shardId), equalTo(indexStats.getIndexShards().get(shardId).getPrimary().getStore().getSizeInBytes())); } assertHitCount(followerClient().prepareSearch(followerIndex).setSize(0).get(), numDocs); assertAcked(followerClient().admin().indices().prepareDelete(followerIndex).setMasterNodeTimeout(TimeValue.MAX_VALUE)); }	should we remove this listener before we enter the busy assert loop? otherwise subsequent cs updates will run into it again in the background again and again? (technically probably not a problem because subsequent cs updates will prevent us from getting here but i'd still remove it to have one less thing to worry about when debugging this :)
public void testCcrRepositoryFetchesSnapshotShardSizeFromIndexShardStoreStats() throws Exception { final String leaderIndex = "leader"; final int numberOfShards = randomIntBetween(1, 5); assertAcked(leaderClient().admin().indices().prepareCreate(leaderIndex) .setSource(getIndexSettings(numberOfShards, 0, Map.of(Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), TimeValue.ZERO.getStringRep())), XContentType.JSON)); final int numDocs = scaledRandomIntBetween(0, 500); if (numDocs > 0) { final BulkRequestBuilder bulkRequest = leaderClient().prepareBulk(leaderIndex); for (int i = 0; i < numDocs; i++) { bulkRequest.add(new IndexRequest(leaderIndex).id(Integer.toString(i)).source("field", i)); } assertThat(bulkRequest.get().hasFailures(), is(false)); } final ForceMergeResponse forceMergeResponse = leaderClient().admin().indices().prepareForceMerge(leaderIndex) .setMaxNumSegments(1) .setFlush(true) .get(); assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards)); assertThat(forceMergeResponse.getFailedShards(), equalTo(0)); ensureLeaderGreen(leaderIndex); final IndexStats indexStats = leaderClient().admin().indices().prepareStats(leaderIndex) .clear() .setStore(true) .get() .getIndex(leaderIndex); assertThat(indexStats.getIndexShards(), notNullValue()); assertThat(indexStats.getIndexShards(), aMapWithSize(numberOfShards)); final String leaderCluster = CcrRepository.NAME_PREFIX + "leader_cluster"; final RepositoriesService repositoriesService = getFollowerCluster().getCurrentMasterNodeInstance(RepositoriesService.class); final Repository repository = repositoriesService.repository(leaderCluster); assertThat(repository.getMetadata().type(), equalTo(CcrRepository.TYPE)); assertThat(repository.getMetadata().name(), equalTo(leaderCluster)); for (int shardId = 0; shardId < numberOfShards; shardId++) { IndexShardSnapshotStatus.Copy indexShardSnapshotStatus = repository.getShardSnapshotStatus( new SnapshotId(CcrRepository.LATEST, CcrRepository.LATEST), new IndexId(indexStats.getIndex(), indexStats.getUuid()), new ShardId(new Index(indexStats.getIndex(), indexStats.getUuid()), shardId)).asCopy(); assertThat(indexShardSnapshotStatus, notNullValue()); assertThat(indexShardSnapshotStatus.getStage(), is(IndexShardSnapshotStatus.Stage.DONE)); assertThat(indexShardSnapshotStatus.getTotalSize(), equalTo(indexStats.getIndexShards().get(shardId).getPrimary().getStore().getSizeInBytes())); } final String followerIndex = "follower"; final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class); final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class); final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class); final Map<Integer, Long> fetchedSnapshotShardSizes = new ConcurrentHashMap<>(); final PlainActionFuture<Void> waitForRestoreInProgress = PlainActionFuture.newFuture(); final ClusterStateListener listener = event -> { RestoreInProgress restoreInProgress = event.state().custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY); if (restoreInProgress != null && restoreInProgress.isEmpty() == false && event.state().routingTable().hasIndex(followerIndex)) { final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(followerIndex); for (ShardRouting shardRouting : indexRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED)) { if (shardRouting.unassignedInfo().getLastAllocationStatus() == AllocationStatus.FETCHING_SHARD_DATA) { try { assertBusy(() -> { final Long snapshotShardSize = snapshotsInfoService.snapshotShardSizes().getShardSize(shardRouting); assertThat(snapshotShardSize, notNullValue()); fetchedSnapshotShardSizes.put(shardRouting.getId(), snapshotShardSize); }, 30L, TimeUnit.SECONDS); } catch (Exception e) { throw new AssertionError("Failed to retrieve snapshot shard size for shard " + shardRouting); } } } logger.info("[{}/{}] snapshot shard sizes fetched", fetchedSnapshotShardSizes.size(), numberOfShards); if (fetchedSnapshotShardSizes.size() == numberOfShards) { waitForRestoreInProgress.onResponse(null); } } }; clusterService.addListener(listener); final RestoreSnapshotRequest restoreRequest = new RestoreSnapshotRequest(leaderCluster, CcrRepository.LATEST) .indices(leaderIndex).indicesOptions(indicesOptions).renamePattern("^(.*)$") .renameReplacement(followerIndex) .masterNodeTimeout(TimeValue.MAX_VALUE) .indexSettings(Settings.builder() .put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, followerIndex) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true)); restoreService.restoreSnapshot(restoreRequest, PlainActionFuture.newFuture()); waitForRestoreInProgress.get(30L, TimeUnit.SECONDS); clusterService.removeListener(listener); ensureFollowerGreen(followerIndex); for (int shardId = 0; shardId < numberOfShards; shardId++) { assertThat("Snapshot shard size fetched for follower shard [" + shardId + "] does not match leader store size", fetchedSnapshotShardSizes.get(shardId), equalTo(indexStats.getIndexShards().get(shardId).getPrimary().getStore().getSizeInBytes())); } assertHitCount(followerClient().prepareSearch(followerIndex).setSize(0).get(), numDocs); assertAcked(followerClient().admin().indices().prepareDelete(followerIndex).setMasterNodeTimeout(TimeValue.MAX_VALUE)); }	let's add the exception to the assertionerror?
public void testCcrRepositoryFetchesSnapshotShardSizeFromIndexShardStoreStats() throws Exception { final String leaderIndex = "leader"; final int numberOfShards = randomIntBetween(1, 5); assertAcked(leaderClient().admin().indices().prepareCreate(leaderIndex) .setSource(getIndexSettings(numberOfShards, 0, Map.of(Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), TimeValue.ZERO.getStringRep())), XContentType.JSON)); final int numDocs = scaledRandomIntBetween(0, 500); if (numDocs > 0) { final BulkRequestBuilder bulkRequest = leaderClient().prepareBulk(leaderIndex); for (int i = 0; i < numDocs; i++) { bulkRequest.add(new IndexRequest(leaderIndex).id(Integer.toString(i)).source("field", i)); } assertThat(bulkRequest.get().hasFailures(), is(false)); } final ForceMergeResponse forceMergeResponse = leaderClient().admin().indices().prepareForceMerge(leaderIndex) .setMaxNumSegments(1) .setFlush(true) .get(); assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards)); assertThat(forceMergeResponse.getFailedShards(), equalTo(0)); ensureLeaderGreen(leaderIndex); final IndexStats indexStats = leaderClient().admin().indices().prepareStats(leaderIndex) .clear() .setStore(true) .get() .getIndex(leaderIndex); assertThat(indexStats.getIndexShards(), notNullValue()); assertThat(indexStats.getIndexShards(), aMapWithSize(numberOfShards)); final String leaderCluster = CcrRepository.NAME_PREFIX + "leader_cluster"; final RepositoriesService repositoriesService = getFollowerCluster().getCurrentMasterNodeInstance(RepositoriesService.class); final Repository repository = repositoriesService.repository(leaderCluster); assertThat(repository.getMetadata().type(), equalTo(CcrRepository.TYPE)); assertThat(repository.getMetadata().name(), equalTo(leaderCluster)); for (int shardId = 0; shardId < numberOfShards; shardId++) { IndexShardSnapshotStatus.Copy indexShardSnapshotStatus = repository.getShardSnapshotStatus( new SnapshotId(CcrRepository.LATEST, CcrRepository.LATEST), new IndexId(indexStats.getIndex(), indexStats.getUuid()), new ShardId(new Index(indexStats.getIndex(), indexStats.getUuid()), shardId)).asCopy(); assertThat(indexShardSnapshotStatus, notNullValue()); assertThat(indexShardSnapshotStatus.getStage(), is(IndexShardSnapshotStatus.Stage.DONE)); assertThat(indexShardSnapshotStatus.getTotalSize(), equalTo(indexStats.getIndexShards().get(shardId).getPrimary().getStore().getSizeInBytes())); } final String followerIndex = "follower"; final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class); final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class); final SnapshotsInfoService snapshotsInfoService = getFollowerCluster().getCurrentMasterNodeInstance(SnapshotsInfoService.class); final Map<Integer, Long> fetchedSnapshotShardSizes = new ConcurrentHashMap<>(); final PlainActionFuture<Void> waitForRestoreInProgress = PlainActionFuture.newFuture(); final ClusterStateListener listener = event -> { RestoreInProgress restoreInProgress = event.state().custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY); if (restoreInProgress != null && restoreInProgress.isEmpty() == false && event.state().routingTable().hasIndex(followerIndex)) { final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(followerIndex); for (ShardRouting shardRouting : indexRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED)) { if (shardRouting.unassignedInfo().getLastAllocationStatus() == AllocationStatus.FETCHING_SHARD_DATA) { try { assertBusy(() -> { final Long snapshotShardSize = snapshotsInfoService.snapshotShardSizes().getShardSize(shardRouting); assertThat(snapshotShardSize, notNullValue()); fetchedSnapshotShardSizes.put(shardRouting.getId(), snapshotShardSize); }, 30L, TimeUnit.SECONDS); } catch (Exception e) { throw new AssertionError("Failed to retrieve snapshot shard size for shard " + shardRouting); } } } logger.info("[{}/{}] snapshot shard sizes fetched", fetchedSnapshotShardSizes.size(), numberOfShards); if (fetchedSnapshotShardSizes.size() == numberOfShards) { waitForRestoreInProgress.onResponse(null); } } }; clusterService.addListener(listener); final RestoreSnapshotRequest restoreRequest = new RestoreSnapshotRequest(leaderCluster, CcrRepository.LATEST) .indices(leaderIndex).indicesOptions(indicesOptions).renamePattern("^(.*)$") .renameReplacement(followerIndex) .masterNodeTimeout(TimeValue.MAX_VALUE) .indexSettings(Settings.builder() .put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, followerIndex) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true)); restoreService.restoreSnapshot(restoreRequest, PlainActionFuture.newFuture()); waitForRestoreInProgress.get(30L, TimeUnit.SECONDS); clusterService.removeListener(listener); ensureFollowerGreen(followerIndex); for (int shardId = 0; shardId < numberOfShards; shardId++) { assertThat("Snapshot shard size fetched for follower shard [" + shardId + "] does not match leader store size", fetchedSnapshotShardSizes.get(shardId), equalTo(indexStats.getIndexShards().get(shardId).getPrimary().getStore().getSizeInBytes())); } assertHitCount(followerClient().prepareSearch(followerIndex).setSize(0).get(), numDocs); assertAcked(followerClient().admin().indices().prepareDelete(followerIndex).setMasterNodeTimeout(TimeValue.MAX_VALUE)); }	let's add a --> prefix to this logging like we have for other test log lines?
@Override protected void doStop() { boolean acquired = false; try { acquired = cacheSyncLock.tryLock(cacheSyncStopTimeout.duration(), cacheSyncStopTimeout.timeUnit()); if (acquired == false) { logger.warn("failed to acquire cache sync lock in [{}], cache might be partially persisted", cacheSyncStopTimeout); } cacheSyncTask.close(); cache.invalidateAll(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); logger.warn("interrupted while waiting for cache sync lock", e); } finally { if (acquired) { cacheSyncLock.unlock(); } } }	i think we need to also do cachesynctask.close() and cache.invalidateall() in this case? possibly better to surround the trylock with a separate try catch for interruptedexception.
private InetAddress[] resolve(String value) throws IOException { String gceMetadataPath; if (value.equals(GceAddressResolverType.GCE.configName)) { // We replace network placeholder with default network interface value: 0 gceMetadataPath = Strings.replace(GceAddressResolverType.GCE.gceName, "{{network}}", "0"); } else if (value.equals(GceAddressResolverType.PRIVATE_DNS.configName)) { gceMetadataPath = GceAddressResolverType.PRIVATE_DNS.gceName; } else if (value.startsWith(GceAddressResolverType.PRIVATE_IP.configName)) { // We extract the network interface from gce:privateIp:XX String network = "0"; String[] privateIpConfig = Strings.splitStringToArray(value, ':'); if (privateIpConfig != null && privateIpConfig.length == 3) { network = privateIpConfig[2]; } // We replace network placeholder with network interface value gceMetadataPath = Strings.replace(GceAddressResolverType.PRIVATE_IP.gceName, "{{network}}", network); } else { throw new IllegalArgumentException("[" + value + "] is not one of the supported GCE network.host setting. " + "Expecting _gce_, _gce:privateIp:X_, _gce:hostname_"); } try { String metadataResult = gceComputeService.metadata(gceMetadataPath); if (metadataResult == null || metadataResult.length() == 0) { throw new IOException("no gce metadata returned from [" + gceMetadataPath + "] for [" + value + "]"); } // really should be only one address: because we explicitly ask for only one via the Ec2HostnameType // but why do we even allow configuring this by hostname... return InetAddress.getAllByName(metadataResult); } catch (IOException e) { throw new IOException("IOException caught when fetching InetAddress from [" + gceMetadataPath + "]", e); } }	i totally agree with you. actually when i wrote the code, i did the same thing which was done for ec2 plugin. i think that we can totally remove that and only support ip address. it can be done in a follow up issue though. i'd add a todo here or open an issue.
@Override public RankFeatureFieldMapper build(BuilderContext context) { return new RankFeatureFieldMapper(name, new RankFeatureFieldType(buildFullName(context), meta.getValue(), positiveScoreImpact.getValue()), multiFieldsBuilder.build(this, context), copyTo.build(), positiveScoreImpact.getValue()); } } public static final TypeParser PARSER = new TypeParser((n, c) -> new Builder(n)); public static final class RankFeatureFieldType extends MappedFieldType { private final boolean positiveScoreImpact; public RankFeatureFieldType(String name, Map<String, String> meta, boolean positiveScoreImpact) { super(name, true, false, false, TextSearchInfo.NONE, meta); this.positiveScoreImpact = positiveScoreImpact; setIndexAnalyzer(Lucene.KEYWORD_ANALYZER); } @Override public String typeName() { return CONTENT_TYPE; } public boolean positiveScoreImpact() { return positiveScoreImpact; } @Override public Query existsQuery(QueryShardContext context) { return new TermQuery(new Term("_feature", name())); } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName, Supplier<SearchLookup> searchLookup) { throw new IllegalArgumentException("[rank_feature] fields do not support sorting, scripting or aggregating"); } @Override public ValueFetcher valueFetcher(QueryShardContext context, SearchLookup searchLookup, String format) { if (format != null) { throw new IllegalArgumentException("Field [" + name() + "] of type [" + typeName() + "] doesn't support formats."); } return new SourceValueFetcher(name(), context) { @Override protected Float parseSourceValue(Object value) { return objectToFloat(value); } }; } @Override public Query termQuery(Object value, QueryShardContext context) { throw new IllegalArgumentException("Queries on [rank_feature] fields are not supported"); } } private final boolean positiveScoreImpact; private RankFeatureFieldMapper(String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, boolean positiveScoreImpact) { super(simpleName, mappedFieldType, multiFields, copyTo); this.positiveScoreImpact = positiveScoreImpact; } @Override public RankFeatureFieldType fieldType() { return (RankFeatureFieldType) super.fieldType(); } @Override protected void parseCreateField(ParseContext context) throws IOException { float value; if (context.externalValueSet()) { Object v = context.externalValue(); value = objectToFloat(v); } else if (context.parser().currentToken() == Token.VALUE_NULL) { // skip return; } else { value = context.parser().floatValue(); } if (context.doc().getByKey(name()) != null) { throw new IllegalArgumentException("[rank_feature] fields do not support indexing multiple values for the same field [" + name() + "] in the same document"); } if (positiveScoreImpact == false) { value = 1 / value; } context.doc().addWithKey(name(), new FeatureField("_feature", name(), value)); } private static Float objectToFloat(Object value) { if (value instanceof Number) { return ((Number) value).floatValue(); } else { return Float.parseFloat(value.toString()); } } @Override protected String contentType() { return CONTENT_TYPE; }	if we're passing queryshardcontext i think that means we can avoid passing searchlookup here?
@Override public boolean equals(Object obj) { if (obj instanceof AutomatonQueryOnBinaryDv) { AutomatonQueryOnBinaryDv other = (AutomatonQueryOnBinaryDv) obj; return Objects.equals(field, other.field) && Objects.equals(matchPattern, other.matchPattern); } return false; }	i tend to stick with something like: if (obj == null || obj.getclass() != getclass()) { return false; } automatonqueryonbinarydv other =... that has the advantage of working "properly" on nulls and subclasses. and i do it from muscle memory....
@Override public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) { if (normalizer != null) { wildcardPattern = StringFieldType.normalizeWildcardPattern(name(), wildcardPattern, normalizer); } Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern)); //Unlike the doc values, ngram index has extra string-start and end characters. String ngramWildcardPattern = TOKEN_START_OR_END_CHAR + wildcardPattern + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR; Automaton ngramAutomaton = WildcardQuery.toAutomaton(new Term(name(), ngramWildcardPattern)); return automatonToQuery(wildcardPattern, wildcardPattern, ngramAutomaton, dvAutomaton); }	do you think it is worth catching the toocomplex exception every time you call automatontoquery? maybe it should be a checked exception (gasp!) and we always catch it and rewrite it to illegalargumentexcpetion with helpful words.
@Override public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) { if (normalizer != null) { wildcardPattern = StringFieldType.normalizeWildcardPattern(name(), wildcardPattern, normalizer); } Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern)); //Unlike the doc values, ngram index has extra string-start and end characters. String ngramWildcardPattern = TOKEN_START_OR_END_CHAR + wildcardPattern + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR; Automaton ngramAutomaton = WildcardQuery.toAutomaton(new Term(name(), ngramWildcardPattern)); return automatonToQuery(wildcardPattern, wildcardPattern, ngramAutomaton, dvAutomaton); }	it could also hurt speed. depending. :trollface:
@Override public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) { if (normalizer != null) { wildcardPattern = StringFieldType.normalizeWildcardPattern(name(), wildcardPattern, normalizer); } Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern)); //Unlike the doc values, ngram index has extra string-start and end characters. String ngramWildcardPattern = TOKEN_START_OR_END_CHAR + wildcardPattern + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR; Automaton ngramAutomaton = WildcardQuery.toAutomaton(new Term(name(), ngramWildcardPattern)); return automatonToQuery(wildcardPattern, wildcardPattern, ngramAutomaton, dvAutomaton); }	maybe we should just throw illegalargumentexception instead of catch and rethrow?
@Override public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) { if (normalizer != null) { wildcardPattern = StringFieldType.normalizeWildcardPattern(name(), wildcardPattern, normalizer); } Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern)); //Unlike the doc values, ngram index has extra string-start and end characters. String ngramWildcardPattern = TOKEN_START_OR_END_CHAR + wildcardPattern + TOKEN_START_OR_END_CHAR + TOKEN_START_OR_END_CHAR; Automaton ngramAutomaton = WildcardQuery.toAutomaton(new Term(name(), ngramWildcardPattern)); return automatonToQuery(wildcardPattern, wildcardPattern, ngramAutomaton, dvAutomaton); }	oh boy. um. i don't know the wildcard stuff well enough to answer that!
public StoredContext stashContext() { final ThreadContextStruct context = threadLocal.get(); /** * X-Opaque-ID should be preserved in a threadContext in order to propagate this across threads. * This is needed so the DeprecationLogger in another thread can see the value of X-Opaque-ID provided by a user. * The same is applied to Task.TRACE_ID. * Otherwise when context is stash, it should be empty. */ boolean hasHeadersToCopy = false; if (context.requestHeaders.isEmpty() == false) { for (String header : HEADERS_TO_COPY) { if (context.requestHeaders.containsKey(header)) { hasHeadersToCopy = true; break; } } } if (hasHeadersToCopy) { Map<String, String> map = headers(context); ThreadContextStruct threadContextStruct = DEFAULT_CONTEXT.putHeaders(map); threadLocal.set(threadContextStruct); } else { threadLocal.set(DEFAULT_CONTEXT); } return () -> { // If the node and thus the threadLocal get closed while this task // is still executing, we don't want this runnable to fail with an // uncaught exception threadLocal.set(context); }; }	java set really lacks containsany: context.requestheaders.keyset().containsany(headers_to_copy) :(
public void collect(int doc, long owningBucketOrd) throws IOException { if (values.advanceExact(doc)) { final HistogramValue sketch = values.histogram(); double previousKey = Double.NEGATIVE_INFINITY; while (sketch.next()) { final double value = sketch.value(); final int count = sketch.count(); double key = Math.floor((value - offset) / interval); assert key >= previousKey; if (hardBounds == null || hardBounds.contain(key * interval)) { long bucketOrd = bucketOrds.add(owningBucketOrd, Double.doubleToLongBits(key)); if (bucketOrd < 0) { // already seen bucketOrd = -1 - bucketOrd; collectExistingBucket(sub, doc, bucketOrd); } else { collectBucket(sub, doc, bucketOrd); } // We have added the document already and we have incremented bucket doc_count // by _doc_count times. To compensate for this, we should increment doc_count by // (count - _doc_count) so that we have added it count times. incrementBucketDocCount(bucketOrd, count - docCountProvider.getDocCount(doc)); } previousKey = key; } } }	no matter what _doc_count is, it was previously added to the bucket_count via the collectbucket methods. it could be any number. consequently, this incrementbucketdoccount may actually be decrementing the bucket_count to adjust for the difference. i think this is fine. the other potential solution is to override collectbucket... so the bucket count is not incremented. but that may prove too complicated. i think this is good solution for now somebody else from the aggs team should take a look as well.
private static Map<String, IndexAndMapping> mergeAcrossIndices( DataFrameAnalyticsSource source, ImmutableOpenMap<String, ImmutableOpenMap<String, MappingMetadata>> indexToMappings, MappingsType mappingsType) { Map<String, IndexAndMapping> mergedMappings = new HashMap<>(); Iterator<ObjectObjectCursor<String, ImmutableOpenMap<String, MappingMetadata>>> iterator = indexToMappings.iterator(); while (iterator.hasNext()) { ObjectObjectCursor<String, ImmutableOpenMap<String, MappingMetadata>> indexMappings = iterator.next(); Iterator<ObjectObjectCursor<String, MappingMetadata>> typeIterator = indexMappings.value.iterator(); while (typeIterator.hasNext()) { ObjectObjectCursor<String, MappingMetadata> typeMapping = typeIterator.next(); Map<String, Object> currentMappings = typeMapping.value.getSourceAsMap(); if (currentMappings.containsKey(mappingsType.type)) { @SuppressWarnings("unchecked") Map<String, Object> fieldMappings = (Map<String, Object>) currentMappings.get(mappingsType.type); for (Map.Entry<String, Object> fieldMapping : fieldMappings.entrySet()) { String field = fieldMapping.getKey(); if (source.isFieldExcluded(field) == false) { if (mergedMappings.containsKey(field)) { IndexAndMapping existingIndexAndMapping = mergedMappings.get(field); if (existingIndexAndMapping.mapping.equals(fieldMapping.getValue()) == false) { throw ExceptionsHelper.badRequestException( "cannot merge [{}] mappings because of differences for field [{}]; mapped as [{}] in index [{}]; " + "mapped as [{}] in index [{}]", mappingsType.type, field, fieldMapping.getValue(), indexMappings.key, existingIndexAndMapping.mapping, existingIndexAndMapping.index); } } else { mergedMappings.put(field, new IndexAndMapping(indexMappings.key, fieldMapping.getValue())); } } } } } } return mergedMappings; }	this was a tricky backport because of the difference between master and 7.x due to type removal. here we used to check the type names are all the same. i decided to simplify this bit by removing this validation. it is not really a meaningful validation. the reason is that in 7.x each index may only contain a single type. while it is possible for 6.x indices to have differently named types, as long as their mappings do not conflict we should be fine. @droberts195 as this is the main difference compared to the pr in master, could you verify this makes sense to you too?
public void testUpdatePolicy() { final AutoscalingPolicy policy = putRandomAutoscalingPolicy(); final AutoscalingPolicy updatedPolicy = new AutoscalingPolicy( policy.name(), new TreeSet<>(randomSubsetOf(randomIntBetween(1, 5), List.of("data", "data_content", "data_hot", "data_warm", "data_cold"))), mutateAutoscalingDeciders(policy.deciders()) ); putAutoscalingPolicy(updatedPolicy); final ClusterState state = client().admin().cluster().prepareState().get().getState(); final AutoscalingMetadata metadata = state.metadata().custom(AutoscalingMetadata.NAME); assertNotNull(metadata); assertThat(metadata.policies(), hasKey(policy.name())); assertThat(metadata.policies().get(policy.name()).policy(), equalTo(updatedPolicy)); }	this mutateautoscalingdeciders is: public static sortedmap<string, settings> mutateautoscalingdeciders(final sortedmap<string, settings> deciders) { if (deciders.size() == 0) { return randomautoscalingdeciders(); } else { // use a proper subset of the deciders return new treemap<>( randomsubsetof(randomintbetween(0, deciders.size() - 1), deciders.entryset()).stream() .collect(collectors.tomap(map.entry::getkey, map.entry::getvalue)) ); } } which seems like it could result in empty deciders. if this is ok, we can keep it.
public static SortedSet<String> randomRoles() { return randomSubsetOf(randomIntBetween(1, DiscoveryNode.getPossibleRoleNames().size() - 1), DiscoveryNode.getPossibleRoleNames()) .stream() .collect(Sets.toUnmodifiableSortedSet()); }	i think we can/should leave this out. randomroles is used in a number of tests and the empty set of roles is legal for many of those. the other change should be enough to fix this i think?
public void testSuccessfulSnapshotAndRestore() { setupTestCluster(randomFrom(1, 3, 5), randomIntBetween(2, 10)); String repoName = "repo"; String snapshotName = "snapshot"; final String index = "test"; final int shards = randomIntBetween(1, 10); final int documents = randomIntBetween(0, 100); TestClusterNode masterNode = testClusterNodes.currentMaster(testClusterNodes.nodes.values().iterator().next().clusterService.state()); final AtomicBoolean createdSnapshot = new AtomicBoolean(); final AtomicBoolean snapshotRestored = new AtomicBoolean(); final AtomicBoolean documentCountVerified = new AtomicBoolean(); masterNode.client.admin().cluster().preparePutRepository(repoName) .setType(FsRepository.TYPE).setSettings(Settings.builder().put("location", randomAlphaOfLength(10))) .execute( assertNoFailureListener( () -> masterNode.client.admin().indices().create( new CreateIndexRequest(index).waitForActiveShards(ActiveShardCount.ALL) .settings(defaultIndexSettings(shards)), assertNoFailureListener( () -> { final Runnable afterIndexing = () -> masterNode.client.admin().cluster().prepareCreateSnapshot(repoName, snapshotName) .setWaitForCompletion(true).execute(assertNoFailureListener(() -> { createdSnapshot.set(true); masterNode.client.admin().indices().delete( new DeleteIndexRequest(index), assertNoFailureListener(() -> masterNode.client.admin().cluster().restoreSnapshot( new RestoreSnapshotRequest(repoName, snapshotName).waitForCompletion(true), assertNoFailureListener(restoreSnapshotResponse -> { snapshotRestored.set(true); assertEquals(shards, restoreSnapshotResponse.getRestoreInfo().totalShards()); masterNode.client.search( new SearchRequest(index).source( new SearchSourceBuilder().size(0).trackTotalHits(true) ), assertNoFailureListener(r -> { assertEquals( (long) documents, Objects.requireNonNull(r.getHits().getTotalHits()).value ); documentCountVerified.set(true); })); }) ))); })); final AtomicInteger countdown = new AtomicInteger(documents); masterNode.client.admin().indices().putMapping( new PutMappingRequest(index).type("_doc").source("foo", "type=text"), assertNoFailureListener(r -> { for (int i = 0; i < documents; ++i) { masterNode.client.bulk( new BulkRequest().add(new IndexRequest(index).source( Collections.singletonMap("foo", "bar" + i))) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE), assertNoFailureListener( bulkResponse -> { assertFalse( "Failures in bulkresponse: " + bulkResponse.buildFailureMessage(), bulkResponse.hasFailures()); if (countdown.decrementAndGet() == 0) { afterIndexing.run(); } })); } if (documents == 0) { afterIndexing.run(); } } )); })))); runUntil( () -> createdSnapshot.get() && snapshotRestored.get() && documentCountVerified.get(), TimeUnit.MINUTES.toMillis(5L)); assertTrue(createdSnapshot.get()); assertTrue(snapshotRestored.get()); assertTrue(documentCountVerified.get()); SnapshotsInProgress finalSnapshotsInProgress = masterNode.clusterService.state().custom(SnapshotsInProgress.TYPE); assertFalse(finalSnapshotsInProgress.entries().stream().anyMatch(entry -> entry.state().completed() == false)); final Repository repository = masterNode.repositoriesService.repository(repoName); Collection<SnapshotId> snapshotIds = repository.getRepositoryData().getSnapshotIds(); assertThat(snapshotIds, hasSize(1)); final SnapshotInfo snapshotInfo = repository.getSnapshotInfo(snapshotIds.iterator().next()); assertEquals(SnapshotState.SUCCESS, snapshotInfo.state()); assertThat(snapshotInfo.indices(), containsInAnyOrder(index)); assertEquals(shards, snapshotInfo.successfulShards()); assertEquals(0, snapshotInfo.failedShards()); }	why do we need 3 different atomic variables? is not it enough just to have documentcountverified?
public void testSuccessfulSnapshotAndRestore() { setupTestCluster(randomFrom(1, 3, 5), randomIntBetween(2, 10)); String repoName = "repo"; String snapshotName = "snapshot"; final String index = "test"; final int shards = randomIntBetween(1, 10); final int documents = randomIntBetween(0, 100); TestClusterNode masterNode = testClusterNodes.currentMaster(testClusterNodes.nodes.values().iterator().next().clusterService.state()); final AtomicBoolean createdSnapshot = new AtomicBoolean(); final AtomicBoolean snapshotRestored = new AtomicBoolean(); final AtomicBoolean documentCountVerified = new AtomicBoolean(); masterNode.client.admin().cluster().preparePutRepository(repoName) .setType(FsRepository.TYPE).setSettings(Settings.builder().put("location", randomAlphaOfLength(10))) .execute( assertNoFailureListener( () -> masterNode.client.admin().indices().create( new CreateIndexRequest(index).waitForActiveShards(ActiveShardCount.ALL) .settings(defaultIndexSettings(shards)), assertNoFailureListener( () -> { final Runnable afterIndexing = () -> masterNode.client.admin().cluster().prepareCreateSnapshot(repoName, snapshotName) .setWaitForCompletion(true).execute(assertNoFailureListener(() -> { createdSnapshot.set(true); masterNode.client.admin().indices().delete( new DeleteIndexRequest(index), assertNoFailureListener(() -> masterNode.client.admin().cluster().restoreSnapshot( new RestoreSnapshotRequest(repoName, snapshotName).waitForCompletion(true), assertNoFailureListener(restoreSnapshotResponse -> { snapshotRestored.set(true); assertEquals(shards, restoreSnapshotResponse.getRestoreInfo().totalShards()); masterNode.client.search( new SearchRequest(index).source( new SearchSourceBuilder().size(0).trackTotalHits(true) ), assertNoFailureListener(r -> { assertEquals( (long) documents, Objects.requireNonNull(r.getHits().getTotalHits()).value ); documentCountVerified.set(true); })); }) ))); })); final AtomicInteger countdown = new AtomicInteger(documents); masterNode.client.admin().indices().putMapping( new PutMappingRequest(index).type("_doc").source("foo", "type=text"), assertNoFailureListener(r -> { for (int i = 0; i < documents; ++i) { masterNode.client.bulk( new BulkRequest().add(new IndexRequest(index).source( Collections.singletonMap("foo", "bar" + i))) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE), assertNoFailureListener( bulkResponse -> { assertFalse( "Failures in bulkresponse: " + bulkResponse.buildFailureMessage(), bulkResponse.hasFailures()); if (countdown.decrementAndGet() == 0) { afterIndexing.run(); } })); } if (documents == 0) { afterIndexing.run(); } } )); })))); runUntil( () -> createdSnapshot.get() && snapshotRestored.get() && documentCountVerified.get(), TimeUnit.MINUTES.toMillis(5L)); assertTrue(createdSnapshot.get()); assertTrue(snapshotRestored.get()); assertTrue(documentCountVerified.get()); SnapshotsInProgress finalSnapshotsInProgress = masterNode.clusterService.state().custom(SnapshotsInProgress.TYPE); assertFalse(finalSnapshotsInProgress.entries().stream().anyMatch(entry -> entry.state().completed() == false)); final Repository repository = masterNode.repositoriesService.repository(repoName); Collection<SnapshotId> snapshotIds = repository.getRepositoryData().getSnapshotIds(); assertThat(snapshotIds, hasSize(1)); final SnapshotInfo snapshotInfo = repository.getSnapshotInfo(snapshotIds.iterator().next()); assertEquals(SnapshotState.SUCCESS, snapshotInfo.state()); assertThat(snapshotInfo.indices(), containsInAnyOrder(index)); assertEquals(shards, snapshotInfo.successfulShards()); assertEquals(0, snapshotInfo.failedShards()); }	why does deterministictaskqueue.runallrunnabletasks(); no longer work for us?
@Override protected void assertEqualInstances(CloseIndexResponse expected, CloseIndexResponse actual) { assertNotSame(expected, actual); assertThat(actual.isAcknowledged(), equalTo(expected.isAcknowledged())); assertThat(actual.isShardsAcknowledged(), equalTo(expected.isShardsAcknowledged())); for (int i = 0; i < expected.getIndices().size(); i++) { CloseIndexResponse.IndexResult expectedIndexResult = expected.getIndices().get(i); CloseIndexResponse.IndexResult actualIndexResult = actual.getIndices().get(i); assertNotSame(expectedIndexResult, actualIndexResult); assertThat(actualIndexResult.getIndex(), equalTo(expectedIndexResult.getIndex())); assertThat(actualIndexResult.hasFailures(), equalTo(expectedIndexResult.hasFailures())); if (expectedIndexResult.hasFailures() == false) { assertThat(actualIndexResult.getException(), nullValue()); if (actualIndexResult.getShards() != null) { assertThat(Arrays.stream(actualIndexResult.getShards()) .allMatch(shardResult -> shardResult.hasFailures() == false), is(true)); } } if (expectedIndexResult.getException() != null) { assertThat(actualIndexResult.getShards(), nullValue()); assertThat(actualIndexResult.getException(), notNullValue()); assertThat(actualIndexResult.getException().getMessage(), equalTo(expectedIndexResult.getException().getMessage())); assertThat(actualIndexResult.getException().getClass(), equalTo(expectedIndexResult.getException().getClass())); assertArrayEquals(actualIndexResult.getException().getStackTrace(), expectedIndexResult.getException().getStackTrace()); } else { assertThat(actualIndexResult.getException(), nullValue()); } if (expectedIndexResult.getShards() != null) { assertThat(actualIndexResult.getShards().length, equalTo(expectedIndexResult.getShards().length)); for (int j = 0; j < expectedIndexResult.getShards().length; j++) { CloseIndexResponse.ShardResult expectedShardResult = expectedIndexResult.getShards()[j]; CloseIndexResponse.ShardResult actualShardResult = actualIndexResult.getShards()[j]; assertThat(actualShardResult.getId(), equalTo(expectedShardResult.getId())); assertThat(actualShardResult.hasFailures(), equalTo(expectedShardResult.hasFailures())); if (expectedShardResult.hasFailures()) { assertThat(actualShardResult.getFailures().length, equalTo(expectedShardResult.getFailures().length)); for (int k = 0; k < expectedShardResult.getFailures().length; k++) { CloseIndexResponse.ShardResult.Failure expectedFailure = expectedShardResult.getFailures()[k]; CloseIndexResponse.ShardResult.Failure actualFailure = actualShardResult.getFailures()[k]; assertThat(actualFailure.getNodeId(), equalTo(expectedFailure.getNodeId())); assertThat(actualFailure.index(), equalTo(expectedFailure.index())); assertThat(actualFailure.shardId(), equalTo(expectedFailure.shardId())); // Serialising and deserialising an exception seems to remove the "java.base/" part from the stack trace, // so these string replacements account for this. assertThat( actualFailure.reason().replace("java.base/", ""), equalTo(expectedFailure.reason().replace("java.base/", ""))); assertThat(actualFailure.getCause().getMessage(), equalTo(expectedFailure.getCause().getMessage())); assertThat(actualFailure.getCause().getClass(), equalTo(expectedFailure.getCause().getClass())); assertArrayEquals(actualFailure.getCause().getStackTrace(), expectedFailure.getCause().getStackTrace()); } } else { assertThat(actualShardResult.getFailures(), nullValue()); } } } else { assertThat(actualIndexResult.getShards(), nullValue()); } } }	can we check for a portion of the failure instead of an exact stack trace?
private static Map<String, MappedFieldType> parseRuntimeMappings(Map<String, Object> runtimeMappings, MapperService mapperService) { if (runtimeMappings.isEmpty()) { return Collections.emptyMap(); } Map<String, RuntimeFieldType> runtimeFields = RuntimeFieldType.parseRuntimeFields(new HashMap<>(runtimeMappings), mapperService.parserContext(), false); Map<String, MappedFieldType> runtimeFieldTypes = new HashMap<>(); for (RuntimeFieldType runtimeFieldType : runtimeFields.values()) { MappedFieldType fieldType = runtimeFieldType.asMappedFieldType(); runtimeFieldTypes.put(fieldType.name(), fieldType); } return Collections.unmodifiableMap(runtimeFieldTypes); }	i was thinking that maybe we should make fieldtypelookup public and use it in searchexecutioncontext instead of reproducing its behavour with the runtime fields defined in the search request
private Exception tryAcquireInFlightDocs(Operation operation, int addingDocs) { assert operation.origin() == Operation.Origin.PRIMARY : operation; assert operation.seqNo() == SequenceNumbers.UNASSIGNED_SEQ_NO : operation; assert addingDocs > 0 : addingDocs; //TODO: needs to be adapted to Lucene86? //indexWriter.getPendingNumDocs() + inFlightDocCount.addAndGet(addingDocs); final long totalDocs = inFlightDocCount.addAndGet(addingDocs); if (totalDocs > maxDocs) { releaseInFlightDocs(addingDocs); return new IllegalArgumentException("Number of documents in the index can't exceed [" + maxDocs + "]"); } else { return null; } }	@dnhatn in case we need to perform this action, i am not sure what we need to do here?
public ShardIterator onlyNodeSelectorActiveInitializingShardsIt(String nodeSelector,DiscoveryNodes discoveryNodes) { ArrayList<ShardRouting> ordered = new ArrayList<>(activeShards.size() + allInitializingShards.size()); List<String> selectedNodes = Arrays.asList(discoveryNodes.resolveNodesIds(nodeSelector)); // fill it in a randomized fashion for (int i = 0; i < activeShards.size(); i++) { ShardRouting shardRouting = activeShards.get(i); if (selectedNodes.contains(shardRouting.currentNodeId())) { ordered.add(shardRouting); } } for (int i = 0; i < allInitializingShards.size(); i++) { ShardRouting shardRouting = allInitializingShards.get(i); if ( selectedNodes.contains(shardRouting.currentNodeId())){ ordered.add(shardRouting); } } if ( ordered.size() == 0 ){ throw new IllegalArgumentException("No data node with for specification [" + nodeSelector + "] found"); } return new PlainShardIterator(shardId, ordered); }	can this be a set<string> instead i think we should not do linear access #contains calls
public ShardIterator onlyNodeSelectorActiveInitializingShardsIt(String nodeSelector,DiscoveryNodes discoveryNodes) { ArrayList<ShardRouting> ordered = new ArrayList<>(activeShards.size() + allInitializingShards.size()); List<String> selectedNodes = Arrays.asList(discoveryNodes.resolveNodesIds(nodeSelector)); // fill it in a randomized fashion for (int i = 0; i < activeShards.size(); i++) { ShardRouting shardRouting = activeShards.get(i); if (selectedNodes.contains(shardRouting.currentNodeId())) { ordered.add(shardRouting); } } for (int i = 0; i < allInitializingShards.size(); i++) { ShardRouting shardRouting = allInitializingShards.get(i); if ( selectedNodes.contains(shardRouting.currentNodeId())){ ordered.add(shardRouting); } } if ( ordered.size() == 0 ){ throw new IllegalArgumentException("No data node with for specification [" + nodeSelector + "] found"); } return new PlainShardIterator(shardId, ordered); }	is this comment a leftover?
public ShardIterator onlyNodeSelectorActiveInitializingShardsIt(String nodeSelector,DiscoveryNodes discoveryNodes) { ArrayList<ShardRouting> ordered = new ArrayList<>(activeShards.size() + allInitializingShards.size()); List<String> selectedNodes = Arrays.asList(discoveryNodes.resolveNodesIds(nodeSelector)); // fill it in a randomized fashion for (int i = 0; i < activeShards.size(); i++) { ShardRouting shardRouting = activeShards.get(i); if (selectedNodes.contains(shardRouting.currentNodeId())) { ordered.add(shardRouting); } } for (int i = 0; i < allInitializingShards.size(); i++) { ShardRouting shardRouting = allInitializingShards.get(i); if ( selectedNodes.contains(shardRouting.currentNodeId())){ ordered.add(shardRouting); } } if ( ordered.size() == 0 ){ throw new IllegalArgumentException("No data node with for specification [" + nodeSelector + "] found"); } return new PlainShardIterator(shardId, ordered); }	can we maybe have for each loops instead here?
public ShardIterator onlyNodeSelectorActiveInitializingShardsIt(String nodeSelector,DiscoveryNodes discoveryNodes) { ArrayList<ShardRouting> ordered = new ArrayList<>(activeShards.size() + allInitializingShards.size()); List<String> selectedNodes = Arrays.asList(discoveryNodes.resolveNodesIds(nodeSelector)); // fill it in a randomized fashion for (int i = 0; i < activeShards.size(); i++) { ShardRouting shardRouting = activeShards.get(i); if (selectedNodes.contains(shardRouting.currentNodeId())) { ordered.add(shardRouting); } } for (int i = 0; i < allInitializingShards.size(); i++) { ShardRouting shardRouting = allInitializingShards.get(i); if ( selectedNodes.contains(shardRouting.currentNodeId())){ ordered.add(shardRouting); } } if ( ordered.size() == 0 ){ throw new IllegalArgumentException("No data node with for specification [" + nodeSelector + "] found"); } return new PlainShardIterator(shardId, ordered); }	code format, no space after ( and before ) also use isempty() instead?
public void merge(Map<String, Map<String, Object>> mappings, MergeReason reason) { Map<String, CompressedXContent> mappingSourcesCompressed = new LinkedHashMap<>(mappings.size()); for (Map.Entry<String, Map<String, Object>> entry : mappings.entrySet()) { try { mappingSourcesCompressed.put(entry.getKey(), new CompressedXContent(Strings.toString( XContentFactory.jsonBuilder().map(entry.getValue())))); } catch (Exception e) { logger.info("problem", e); throw new MapperParsingException("Failed to parse mapping [{}]: {}", e, entry.getKey(), e.getMessage()); } } internalMerge(mappingSourcesCompressed, reason); }	is there a problem ? ;)
private void checkBulkActionIsProperlyClosed(XContentParser parser) throws IOException { XContentParser.Token token; try { token = parser.nextToken(); } catch (JsonEOFException ignore) { deprecationLogger.compatibleCritical( STRICT_ACTION_PARSING_WARNING_KEY, "A bulk action wasn't closed properly with the closing brace. Malformed objects are currently accepted but will be " + "rejected in a future version." ); return; } if (token != XContentParser.Token.END_OBJECT) { deprecationLogger.compatibleCritical( STRICT_ACTION_PARSING_WARNING_KEY, "A bulk action object contained multiple keys. Additional keys are currently ignored but will be rejected in a " + "future version." ); return; } if (parser.nextToken() != null) { deprecationLogger.compatibleCritical( STRICT_ACTION_PARSING_WARNING_KEY, "A bulk action contains trailing junk after the closing brace. It is currently ignored but will be rejected in a " + "future version." ); } }	rewording suggestion: suggestion "a bulk action contained trailing data after the closing brace. this is currently ignored but will be rejected in a "
@Override public void onRefreshSettings(Settings settings) { final boolean enable = settings.getAsBoolean(SETTING_CLUSTER_INDICES_CLOSE_ENABLE, this.closeIndexEnabled); if (enable != this.closeIndexEnabled) { logger.info("updating [{}] from [{}] to [{}]", SETTING_CLUSTER_INDICES_CLOSE_ENABLE, this.closeIndexEnabled, enable); this.closeIndexEnabled = enable; } }	should this be java + final boolean enable = settings.getasboolean(setting_cluster_indices_close_enable, true); ? settings can't really be cleared right now but if they could be this would stick the value to whatever it was set at before clearing. maybe it doesn't matter but i'm certainly more used to seeing the default value here.
public void testCloseAllRequiresName() { Settings clusterSettings = Settings.builder() .put(DestructiveOperations.REQUIRES_NAME, true) .build(); assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(clusterSettings)); createIndex("test1", "test2", "test3"); ClusterHealthResponse healthResponse = client().admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet(); assertThat(healthResponse.isTimedOut(), equalTo(false)); // Close all explicitly try { client().admin().indices().prepareClose("_all").execute().actionGet(); fail(); } catch (IllegalArgumentException e) { } // Close all wildcard try { client().admin().indices().prepareClose("*").execute().actionGet(); fail(); } catch (IllegalArgumentException e) { } // Close all wildcard try { client().admin().indices().prepareClose("test*").execute().actionGet(); fail(); } catch (IllegalArgumentException e) { } // Close all wildcard try { client().admin().indices().prepareClose("*", "-test1").execute().actionGet(); fail(); } catch (IllegalArgumentException e) { } // Close all wildcard try { client().admin().indices().prepareClose("*", "-test1", "+test1").execute().actionGet(); fail(); } catch (IllegalArgumentException e) { } CloseIndexResponse closeIndexResponse = client().admin().indices().prepareClose("test3", "test2").execute().actionGet(); assertThat(closeIndexResponse.isAcknowledged(), equalTo(true)); assertIndexIsClosed("test2", "test3"); // disable closing Client client = client(); createIndex("test_no_close"); healthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet(); assertThat(healthResponse.isTimedOut(), equalTo(false)); client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(TransportCloseIndexAction.SETTING_CLUSTER_INDICES_CLOSE_ENABLE, false)).get(); try { client.admin().indices().prepareClose("test_no_close").execute().actionGet(); fail("exception expected"); } catch (IllegalStateException ex) { assertEquals(ex.getMessage(), "closing indices is disabled - set [cluster.indices.close.enable: true] to enable it. NOTE: closed indices consume a significant amount of diskspace"); } }	note: closed indices still consume disk space?
@Override public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) { Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build(); StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP); StepKey checkNotWriteIndex = new StepKey(phase, NAME, CheckNotDataStreamWriteIndexStep.NAME); StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME); StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME); StepKey cleanupShrinkIndexKey = new StepKey(phase, NAME, CleanupShrinkIndexStep.NAME); StepKey generateShrinkIndexNameKey = new StepKey(phase, NAME, GenerateUniqueIndexNameStep.NAME); StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME); StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME); StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME); StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME); StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME); StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY); StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME); StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME); StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME); StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME); BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, checkNotWriteIndex, nextStepKey, (index, clusterState) -> { IndexMetadata indexMetadata = clusterState.getMetadata().index(index); if (numberOfShards != null && indexMetadata.getNumberOfShards() == numberOfShards) { return true; } if (indexMetadata.getSettings().get(LifecycleSettings.SNAPSHOT_INDEX_NAME) != null) { logger.warn("[{}] action is configured for index [{}] in policy [{}] which is mounted as searchable snapshot. " + "Skipping this action", ShrinkAction.NAME, indexMetadata.getIndex().getName(), LifecycleSettings.LIFECYCLE_NAME_SETTING.get(indexMetadata.getSettings())); return true; } return false; }); CheckNotDataStreamWriteIndexStep checkNotWriteIndexStep = new CheckNotDataStreamWriteIndexStep(checkNotWriteIndex, waitForNoFollowerStepKey); WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client); UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, cleanupShrinkIndexKey, client, readOnlySettings); CleanupShrinkIndexStep cleanupShrinkIndexStep = new CleanupShrinkIndexStep(cleanupShrinkIndexKey, generateShrinkIndexNameKey, client); GenerateUniqueIndexNameStep generateUniqueIndexNameStep = new GenerateUniqueIndexNameStep(generateShrinkIndexNameKey, setSingleNodeKey, SHRUNKEN_INDEX_PREFIX, (generatedIndexName, lifecycleStateBuilder) -> lifecycleStateBuilder.setShrinkIndexName(generatedIndexName)); SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client); ClusterStateWaitUntilThresholdStep checkShrinkReadyStep = new ClusterStateWaitUntilThresholdStep( new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey), cleanupShrinkIndexKey); ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, maxPrimaryShardSize, SHRUNKEN_INDEX_PREFIX); ClusterStateWaitUntilThresholdStep allocated = new ClusterStateWaitUntilThresholdStep( new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX), cleanupShrinkIndexKey); CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey, ShrinkAction::getShrunkIndexName, isShrunkIndexKey); // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted). // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to // the shrunken index and delete the source BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey, (index, clusterState) -> { IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName()); assert indexAbstraction != null : "invalid cluster metadata. index [" + index.getName() + "] was not found"; return indexAbstraction.getParentDataStream() != null; }); ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX); ReplaceDataStreamBackingIndexStep replaceDataStreamBackingIndex = new ReplaceDataStreamBackingIndexStep(replaceDataStreamIndexKey, deleteIndexKey, ShrinkAction::getShrunkIndexName); DeleteStep deleteSourceIndexStep = new DeleteStep(deleteIndexKey, isShrunkIndexKey, client); ShrunkenIndexCheckStep waitOnShrinkTakeover = new ShrunkenIndexCheckStep(isShrunkIndexKey, nextStepKey, SHRUNKEN_INDEX_PREFIX); return Arrays.asList(conditionalSkipShrinkStep, checkNotWriteIndexStep, waitForNoFollowersStep, readOnlyStep, cleanupShrinkIndexStep, generateUniqueIndexNameStep, setSingleNodeStep, checkShrinkReadyStep, shrink, allocated, copyMetadata, isDataStreamBranchingStep, aliasSwapAndDelete, waitOnShrinkTakeover, replaceDataStreamBackingIndex, deleteSourceIndexStep); }	can you comment the steps in this method so someone that comes in can follow the execution flow? i think it'd be helpful for others reading the code.
@Override public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) { Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build(); StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP); StepKey checkNotWriteIndex = new StepKey(phase, NAME, CheckNotDataStreamWriteIndexStep.NAME); StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME); StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME); StepKey cleanupShrinkIndexKey = new StepKey(phase, NAME, CleanupShrinkIndexStep.NAME); StepKey generateShrinkIndexNameKey = new StepKey(phase, NAME, GenerateUniqueIndexNameStep.NAME); StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME); StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME); StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME); StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME); StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME); StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY); StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME); StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME); StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME); StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME); BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, checkNotWriteIndex, nextStepKey, (index, clusterState) -> { IndexMetadata indexMetadata = clusterState.getMetadata().index(index); if (numberOfShards != null && indexMetadata.getNumberOfShards() == numberOfShards) { return true; } if (indexMetadata.getSettings().get(LifecycleSettings.SNAPSHOT_INDEX_NAME) != null) { logger.warn("[{}] action is configured for index [{}] in policy [{}] which is mounted as searchable snapshot. " + "Skipping this action", ShrinkAction.NAME, indexMetadata.getIndex().getName(), LifecycleSettings.LIFECYCLE_NAME_SETTING.get(indexMetadata.getSettings())); return true; } return false; }); CheckNotDataStreamWriteIndexStep checkNotWriteIndexStep = new CheckNotDataStreamWriteIndexStep(checkNotWriteIndex, waitForNoFollowerStepKey); WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client); UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, cleanupShrinkIndexKey, client, readOnlySettings); CleanupShrinkIndexStep cleanupShrinkIndexStep = new CleanupShrinkIndexStep(cleanupShrinkIndexKey, generateShrinkIndexNameKey, client); GenerateUniqueIndexNameStep generateUniqueIndexNameStep = new GenerateUniqueIndexNameStep(generateShrinkIndexNameKey, setSingleNodeKey, SHRUNKEN_INDEX_PREFIX, (generatedIndexName, lifecycleStateBuilder) -> lifecycleStateBuilder.setShrinkIndexName(generatedIndexName)); SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client); ClusterStateWaitUntilThresholdStep checkShrinkReadyStep = new ClusterStateWaitUntilThresholdStep( new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey), cleanupShrinkIndexKey); ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, maxPrimaryShardSize, SHRUNKEN_INDEX_PREFIX); ClusterStateWaitUntilThresholdStep allocated = new ClusterStateWaitUntilThresholdStep( new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX), cleanupShrinkIndexKey); CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey, ShrinkAction::getShrunkIndexName, isShrunkIndexKey); // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted). // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to // the shrunken index and delete the source BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey, (index, clusterState) -> { IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName()); assert indexAbstraction != null : "invalid cluster metadata. index [" + index.getName() + "] was not found"; return indexAbstraction.getParentDataStream() != null; }); ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX); ReplaceDataStreamBackingIndexStep replaceDataStreamBackingIndex = new ReplaceDataStreamBackingIndexStep(replaceDataStreamIndexKey, deleteIndexKey, ShrinkAction::getShrunkIndexName); DeleteStep deleteSourceIndexStep = new DeleteStep(deleteIndexKey, isShrunkIndexKey, client); ShrunkenIndexCheckStep waitOnShrinkTakeover = new ShrunkenIndexCheckStep(isShrunkIndexKey, nextStepKey, SHRUNKEN_INDEX_PREFIX); return Arrays.asList(conditionalSkipShrinkStep, checkNotWriteIndexStep, waitForNoFollowersStep, readOnlyStep, cleanupShrinkIndexStep, generateUniqueIndexNameStep, setSingleNodeStep, checkShrinkReadyStep, shrink, allocated, copyMetadata, isDataStreamBranchingStep, aliasSwapAndDelete, waitOnShrinkTakeover, replaceDataStreamBackingIndex, deleteSourceIndexStep); }	i *think* this only needs to rewind back to setsinglenodekey instead of cleanupshrinkindexkey, right? because this is the pre-step when the shrunken index hasn't actually been created yet. we only want to rewind and pick a new node if we can't allocate within the time frame, no need to delete anything.
@Override public Result isConditionMet(Index index, ClusterState clusterState) { IndexMetadata indexMetadata = clusterState.metadata().index(index); if (indexMetadata == null) { // Index must have been since deleted, ignore it logger.debug("[{}] lifecycle action for index [{}] executed but index no longer exists", getKey().getAction(), index.getName()); return new Result(false, null); } LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(indexMetadata); String shrunkenIndexName = lifecycleState.getShrinkIndexName(); if (shrunkenIndexName == null) { // this is for BWC reasons for polices that are in the middle of executing the shrink action when the update to generated // names happens shrunkenIndexName = shrunkIndexPrefix + index.getName(); } // We only want to make progress if all shards of the shrunk index are // active boolean indexExists = clusterState.metadata().index(shrunkenIndexName) != null; if (indexExists == false) { return new Result(false, new Info(false, -1, false)); } boolean allShardsActive = ActiveShardCount.ALL.enoughShardsActive(clusterState, shrunkenIndexName); int numShrunkIndexShards = clusterState.metadata().index(shrunkenIndexName).getNumberOfShards(); if (allShardsActive) { return new Result(true, null); } else { return new Result(false, new Info(true, numShrunkIndexShards, allShardsActive)); } }	it looks like we duplicate this enough places that maybe we want to put this in a static method somewhere, passing in the prefix and indexmetadata, what do you think?
private void putShrinkOnlyPolicy(RestClient client, String policyName) throws IOException { final XContentBuilder builder = jsonBuilder(); builder.startObject(); { builder.startObject("policy"); { builder.startObject("phases"); { builder.startObject("warm"); { builder.startObject("actions"); { builder.startObject("shrink"); { builder.field("number_of_shards", 1); } builder.endObject(); } builder.endObject(); } builder.endObject(); } builder.endObject(); } builder.endObject(); } builder.endObject(); final Request request = new Request("PUT", "_ilm/policy/" + policyName); request.setJsonEntity(Strings.toString(builder)); assertOK(client.performRequest(request)); }	why was this removed?
@Override public void visitConstant(ConstantNode irConstantNode, WriteScope writeScope) { MethodWriter methodWriter = writeScope.getMethodWriter(); Object constant = irConstantNode.getDecorationValue(IRDConstant.class); if (constant instanceof String) methodWriter.push((String)constant); else if (constant instanceof Double) methodWriter.push((double)constant); else if (constant instanceof Float) methodWriter.push((float)constant); else if (constant instanceof Long) methodWriter.push((long)constant); else if (constant instanceof Integer) methodWriter.push((int)constant); else if (constant instanceof Character) methodWriter.push((char)constant); else if (constant instanceof Short) methodWriter.push((short)constant); else if (constant instanceof Byte) methodWriter.push((byte)constant); else if (constant instanceof Boolean) methodWriter.push((boolean)constant); else { /* * The constant doesn't properly fit into the constant pool so * we should have made a static field for it. */ String fieldName = irConstantNode.getDecorationValue(IRDConstantFieldName.class); Type asmFieldType = MethodWriter.getType(irConstantNode.getDecorationValue(IRDExpressionType.class)); if (asmFieldType == null) { throw irConstantNode.getLocation() .createError(new IllegalStateException("Didn't attach constant to [" + irConstantNode + "]")); } methodWriter.getStatic(CLASS_TYPE, fieldName, asmFieldType); } }	is it correct to say this should never happen outside of bugs?
public void testInTernaryCondition() { assertEquals(true, exec("return /foo/.matcher('foo').matches() ? true : false")); assertEquals(1, exec("def i = 0; i += /foo/.matcher('foo').matches() ? 1 : 0; return i")); assertEquals(true, exec("return 'foo' ==~ /foo/ ? true : false")); assertEquals(1, exec("def i = 0; i += 'foo' ==~ /foo/ ? 1 : 0; return i")); }	this looks like a mistake.
@Override protected ConnectionProfile resolveConnectionProfile(ConnectionProfile connectionProfile) { ConnectionProfile connectionProfile1 = resolveConnectionProfile(connectionProfile, defaultConnectionProfile); if (connectionProfile1.getNumConnections() <= 3) { return connectionProfile1; } ConnectionProfile.Builder builder = new ConnectionProfile.Builder(); Set<TransportRequestOptions.Type> allTypesWithConnection = new HashSet<>(); Set<TransportRequestOptions.Type> allTypesWithoutConnection = new HashSet<>(); for (TransportRequestOptions.Type type :TransportRequestOptions.Type.values()) { int numConnections = connectionProfile1.getNumConnectionsPerType(type); if (numConnections > 0) { allTypesWithConnection.add(type); } else { allTypesWithoutConnection.add(type); } } // make sure we maintain at least the types that are supported by this profile even if we only use a single channel for them. builder.addConnections(3, allTypesWithConnection.toArray(new TransportRequestOptions.Type[0])); if (allTypesWithoutConnection.isEmpty() == false) { builder.addConnections(0, allTypesWithoutConnection.toArray(new TransportRequestOptions.Type[0])); } builder.setHandshakeTimeout(connectionProfile1.getHandshakeTimeout()); builder.setConnectTimeout(connectionProfile1.getConnectTimeout()); return builder.build(); }	can you give it a more meaningful name?
@Override protected ConnectionProfile resolveConnectionProfile(ConnectionProfile connectionProfile) { ConnectionProfile connectionProfile1 = resolveConnectionProfile(connectionProfile, defaultConnectionProfile); if (connectionProfile1.getNumConnections() <= 3) { return connectionProfile1; } ConnectionProfile.Builder builder = new ConnectionProfile.Builder(); Set<TransportRequestOptions.Type> allTypesWithConnection = new HashSet<>(); Set<TransportRequestOptions.Type> allTypesWithoutConnection = new HashSet<>(); for (TransportRequestOptions.Type type :TransportRequestOptions.Type.values()) { int numConnections = connectionProfile1.getNumConnectionsPerType(type); if (numConnections > 0) { allTypesWithConnection.add(type); } else { allTypesWithoutConnection.add(type); } } // make sure we maintain at least the types that are supported by this profile even if we only use a single channel for them. builder.addConnections(3, allTypesWithConnection.toArray(new TransportRequestOptions.Type[0])); if (allTypesWithoutConnection.isEmpty() == false) { builder.addConnections(0, allTypesWithoutConnection.toArray(new TransportRequestOptions.Type[0])); } builder.setHandshakeTimeout(connectionProfile1.getHandshakeTimeout()); builder.setConnectTimeout(connectionProfile1.getConnectTimeout()); return builder.build(); }	nit: space after the :
* @param pemWriter the writer for PEM objects * @param info the certificate authority information */ private static void writeCAInfoIfGenerated(ZipOutputStream outputStream, JcaPEMWriter pemWriter, CAInfo info) throws Exception { if (info.generated) { final String caDirName = "ca/"; ZipEntry zipEntry = new ZipEntry(caDirName); assert zipEntry.isDirectory(); outputStream.putNextEntry(zipEntry); outputStream.putNextEntry(new ZipEntry(caDirName + "ca.crt")); pemWriter.writeObject(info.caCert); pemWriter.flush(); outputStream.closeEntry(); outputStream.putNextEntry(new ZipEntry(caDirName + "ca.key")); if (info.password != null && info.password.length > 0) { try { PEMEncryptor encryptor = new JcePEMEncryptorBuilder("DES-EDE3-CBC").setProvider(BC_PROV). build(info.password); pemWriter.writeObject(info.privateKey, encryptor); } finally { // we can safely nuke the password chars now Arrays.fill(info.password, (char) 0); } } else { pemWriter.writeObject(info.privateKey); } pemWriter.flush(); outputStream.closeEntry(); } }	seems like an unnecessary line break.
public void testAutoExpandNumberOfReplicas0ToData() throws IOException { internalCluster().ensureAtMostNumDataNodes(2); logger.info("--> creating index test with auto expand replicas"); assertAcked(prepareCreate("test", 2, Settings.builder().put("auto_expand_replicas", "0-all"))); NumShards numShards = getNumShards("test"); logger.info("--> running cluster health"); ClusterHealthResponse clusterHealth = client().admin().cluster().prepareHealth() .setWaitForEvents(Priority.LANGUID) .setWaitForGreenStatus() .setWaitForActiveShards(numShards.numPrimaries * 2) .execute().actionGet(); logger.info("--> done cluster health, status {}", clusterHealth.getStatus()); assertThat(clusterHealth.isTimedOut(), equalTo(false)); assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.GREEN)); assertThat(clusterHealth.getIndices().get("test").getActivePrimaryShards(), equalTo(numShards.numPrimaries)); assertThat(clusterHealth.getIndices().get("test").getNumberOfReplicas(), equalTo(1)); assertThat(clusterHealth.getIndices().get("test").getActiveShards(), equalTo(numShards.numPrimaries * 2)); if (randomBoolean()) { client().admin().indices().prepareClose("test").setWaitForActiveShards(ActiveShardCount.ALL).get(); clusterHealth = client().admin().cluster().prepareHealth() .setWaitForEvents(Priority.LANGUID) .setWaitForGreenStatus() .setWaitForActiveShards(numShards.numPrimaries * 2) .execute().actionGet(); logger.info("--> done cluster health, status {}", clusterHealth.getStatus()); assertThat(clusterHealth.isTimedOut(), equalTo(false)); assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.GREEN)); assertThat(clusterHealth.getIndices().get("test").getActivePrimaryShards(), equalTo(numShards.numPrimaries)); assertThat(clusterHealth.getIndices().get("test").getNumberOfReplicas(), equalTo(1)); assertThat(clusterHealth.getIndices().get("test").getActiveShards(), equalTo(numShards.numPrimaries * 2)); } final long settingsVersion = client().admin().cluster().prepareState().get().getState().metaData().index("test").getSettingsVersion(); logger.info("--> add another node, should increase the number of replicas"); allowNodes("test", 3); logger.info("--> running cluster health"); clusterHealth = client().admin().cluster().prepareHealth() .setWaitForEvents(Priority.LANGUID) .setWaitForGreenStatus() .setWaitForActiveShards(numShards.numPrimaries * 3) .setWaitForNodes(">=3") .execute().actionGet(); logger.info("--> done cluster health, status {}", clusterHealth.getStatus()); assertThat(clusterHealth.isTimedOut(), equalTo(false)); assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.GREEN)); assertThat(clusterHealth.getIndices().get("test").getActivePrimaryShards(), equalTo(numShards.numPrimaries)); assertThat(clusterHealth.getIndices().get("test").getNumberOfReplicas(), equalTo(2)); assertThat(clusterHealth.getIndices().get("test").getActiveShards(), equalTo(numShards.numPrimaries * 3)); final long afterAddingOneNodeSettingsVersion = client().admin().cluster().prepareState().get().getState().metaData().index("test").getSettingsVersion(); assertThat(afterAddingOneNodeSettingsVersion, equalTo(1 + settingsVersion)); logger.info("--> closing one node"); internalCluster().ensureAtMostNumDataNodes(2); allowNodes("test", 2); logger.info("--> running cluster health"); clusterHealth = client().admin().cluster().prepareHealth() .setWaitForEvents(Priority.LANGUID) .setWaitForGreenStatus() .setWaitForActiveShards(numShards.numPrimaries * 2) .setWaitForNodes(">=2") .execute().actionGet(); logger.info("--> done cluster health, status {}", clusterHealth.getStatus()); assertThat(clusterHealth.isTimedOut(), equalTo(false)); assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.GREEN)); assertThat(clusterHealth.getIndices().get("test").getActivePrimaryShards(), equalTo(numShards.numPrimaries)); assertThat(clusterHealth.getIndices().get("test").getNumberOfReplicas(), equalTo(1)); assertThat(clusterHealth.getIndices().get("test").getActiveShards(), equalTo(numShards.numPrimaries * 2)); final long afterClosingOneNodeSettingsVersion = client().admin().cluster().prepareState().get().getState().metaData().index("test").getSettingsVersion(); assertThat(afterClosingOneNodeSettingsVersion, equalTo(1 + afterAddingOneNodeSettingsVersion)); logger.info("--> closing another node"); internalCluster().ensureAtMostNumDataNodes(1); allowNodes("test", 1); logger.info("--> running cluster health"); clusterHealth = client().admin().cluster().prepareHealth() .setWaitForEvents(Priority.LANGUID) .setWaitForGreenStatus() .setWaitForNodes(">=1") .setWaitForActiveShards(numShards.numPrimaries) .execute().actionGet(); logger.info("--> done cluster health, status {}", clusterHealth.getStatus()); assertThat(clusterHealth.isTimedOut(), equalTo(false)); assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.GREEN)); assertThat(clusterHealth.getIndices().get("test").getActivePrimaryShards(), equalTo(numShards.numPrimaries)); assertThat(clusterHealth.getIndices().get("test").getNumberOfReplicas(), equalTo(0)); assertThat(clusterHealth.getIndices().get("test").getActiveShards(), equalTo(numShards.numPrimaries)); final long afterClosingAnotherNodeSettingsVersion = client().admin().cluster().prepareState().get().getState().metaData().index("test").getSettingsVersion(); assertThat(afterClosingAnotherNodeSettingsVersion, equalTo(1 + afterClosingOneNodeSettingsVersion)); }	can we ack the close (or check that the index is effectively in close state after the following cluster health checks)?
public void addCustomExecutor(Executor executor, Info info) { if (executors.containsKey(info.getName())) { throw new IllegalArgumentException("Can't add custom executor, because the name is already used i"); } ExecutorHolder holder = new ExecutorHolder(executor, info); customExecutors = newMapBuilder(customExecutors).put(holder.info.getName(), holder).immutableMap(); }	same note here about syncing?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); version = Version.readVersion(in); build = Build.readBuild(in); if (in.readBoolean()) { totalIndexingBuffer = new ByteSizeValue(in.readLong()); } else { totalIndexingBuffer = null; } if (version.onOrBefore(Version.V_5_0_0_alpha4)) { // service attributes were removed if (in.readBoolean()) { int size = in.readVInt(); for (int i = 0; i < size; i++) { in.readString(); // key in.readString(); // value } } } if (in.readBoolean()) { settings = Settings.readSettingsFromStream(in); } if (in.readBoolean()) { os = OsInfo.readOsInfo(in); } if (in.readBoolean()) { process = ProcessInfo.readProcessInfo(in); } if (in.readBoolean()) { jvm = JvmInfo.readJvmInfo(in); } if (in.readBoolean()) { threadPool = ThreadPoolInfo.readThreadPoolInfo(in); } if (in.readBoolean()) { transport = TransportInfo.readTransportInfo(in); } if (in.readBoolean()) { http = HttpInfo.readHttpInfo(in); } if (in.readBoolean()) { plugins = new PluginsAndModules(); plugins.readFrom(in); } if (in.readBoolean()) { ingest = new IngestInfo(in); } }	do we need this bw comp layer?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject("nodes"); for (NodeInfo nodeInfo : getNodes()) { builder.startObject(nodeInfo.getNode().getId()); builder.field("name", nodeInfo.getNode().getName()); builder.field("transport_address", nodeInfo.getNode().getAddress().toString()); builder.field("host", nodeInfo.getNode().getHostName()); builder.field("ip", nodeInfo.getNode().getHostAddress()); builder.field("version", nodeInfo.getVersion()); builder.field("build_hash", nodeInfo.getBuild().shortHash()); if (nodeInfo.getTotalIndexingBuffer() != null) { builder.byteSizeField("total_indexing_buffer", "total_indexing_buffer_in_bytes", nodeInfo.getTotalIndexingBuffer()); } builder.startArray("roles"); for (DiscoveryNode.Role role : nodeInfo.getNode().getRoles()) { builder.value(role.getRoleName()); } builder.endArray(); if (!nodeInfo.getNode().getAttributes().isEmpty()) { builder.startObject("attributes"); for (Map.Entry<String, String> entry : nodeInfo.getNode().getAttributes().entrySet()) { builder.field(entry.getKey(), entry.getValue()); } builder.endObject(); } if (nodeInfo.getSettings() != null) { builder.startObject("settings"); Settings settings = nodeInfo.getSettings(); settings.toXContent(builder, params); builder.endObject(); } if (nodeInfo.getOs() != null) { nodeInfo.getOs().toXContent(builder, params); } if (nodeInfo.getProcess() != null) { nodeInfo.getProcess().toXContent(builder, params); } if (nodeInfo.getJvm() != null) { nodeInfo.getJvm().toXContent(builder, params); } if (nodeInfo.getThreadPool() != null) { nodeInfo.getThreadPool().toXContent(builder, params); } if (nodeInfo.getTransport() != null) { nodeInfo.getTransport().toXContent(builder, params); } if (nodeInfo.getHttp() != null) { nodeInfo.getHttp().toXContent(builder, params); } if (nodeInfo.getPlugins() != null) { nodeInfo.getPlugins().toXContent(builder, params); } if (nodeInfo.getIngest() != null) { nodeInfo.getIngest().toXContent(builder, params); } builder.endObject(); } builder.endObject(); return builder; }	i guess this piece of info that is not returned by nodes info was already returned in some other section of nodes info as well? i think we have a proper http section for it that is still there?
@Override protected void closeInternal() { ios.remove(sharedBytesPos, this); SharedBytes.this.decRef(); } } public static ByteSizeValue pageAligned(ByteSizeValue val) { if (val.getBytes() % PAGE_SIZE != 0L) { final long remainder = val.getBytes() % PAGE_SIZE; return ByteSizeValue.ofBytes(val.getBytes() + PAGE_SIZE - remainder); }	let's compute the remainder only once
public void testKeepTranslogAfterGlobalCheckpoint() throws Exception { IOUtils.close(engine, store); final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.UNASSIGNED_SEQ_NO); final BiFunction<EngineConfig, SeqNoStats, SequenceNumbersService> seqNoServiceSupplier = (config, seqNoStats) -> new SequenceNumbersService( config.getShardId(), config.getAllocationId(), config.getIndexSettings(), seqNoStats.getMaxSeqNo(), seqNoStats.getLocalCheckpoint(), seqNoStats.getGlobalCheckpoint()) { @Override public long getGlobalCheckpoint() { return globalCheckpoint.get(); } }; final IndexSettings indexSettings = new IndexSettings(defaultSettings.getIndexMetaData(), defaultSettings.getNodeSettings(), defaultSettings.getScopedSettings()); IndexMetaData.Builder builder = IndexMetaData.builder(indexSettings.getIndexMetaData()); builder.settings(Settings.builder().put(indexSettings.getSettings()) .put(IndexSettings.INDEX_TRANSLOG_RETENTION_AGE_SETTING.getKey(), randomFrom("-1", "10nanos", "500ms", "7s", "2m", "60m")) .put(IndexSettings.INDEX_TRANSLOG_RETENTION_SIZE_SETTING.getKey(), randomFrom("-1", "5b", "2kb", "1mb", "50gb")) ); indexSettings.updateIndexMetaData(builder.build()); final Path translogPath = createTempDir(); store = createStore(); try (InternalEngine engine = new InternalEngine(config(indexSettings, store, translogPath, NoMergePolicy.INSTANCE, null), seqNoServiceSupplier)) { int numDocs = scaledRandomIntBetween(10, 100); int uncommittedOps = 0; for (int i = 0; i < numDocs; i++) { ParseContext.Document document = testDocumentWithTextField(); document.add(new Field(SourceFieldMapper.NAME, BytesReference.toBytes(B_1), SourceFieldMapper.Defaults.FIELD_TYPE)); engine.index(indexForDoc(testParsedDocument(Integer.toString(i), null, document, B_1, null))); uncommittedOps++; if (frequently()) { globalCheckpoint.set(randomIntBetween( Math.toIntExact(engine.seqNoService().getGlobalCheckpoint()), Math.toIntExact(engine.seqNoService().getLocalCheckpoint()))); } if (frequently()) { engine.flush(randomBoolean(), true); uncommittedOps = 0; } if (rarely()) { engine.rollTranslogGeneration(); } try (Translog.Snapshot snapshot = engine.getTranslog().newSnapshot()) { long requiredOps = engine.seqNoService().getMaxSeqNo() - Math.max(0, engine.seqNoService().getGlobalCheckpoint()); assertThat("Should keep translog operations up to the global checkpoint", (long) snapshot.totalOperations(), greaterThanOrEqualTo(requiredOps)); } assertThat(engine.getTranslog().uncommittedOperations(), equalTo(uncommittedOps)); } engine.flush(randomBoolean(), true); } // Reopen engine to test onInit with existing index commits. try (InternalEngine engine = new InternalEngine(config(indexSettings, store, translogPath, NoMergePolicy.INSTANCE, null), seqNoServiceSupplier)) { final Set<Long> seqNoList = new HashSet<>(); try (Translog.Snapshot snapshot = engine.getTranslog().newSnapshot()) { Translog.Operation op; while ((op = snapshot.next()) != null) { seqNoList.add(op.seqNo()); } } for (long i = Math.max(0, globalCheckpoint.get() + 1); i <= engine.seqNoService().getLocalCheckpoint(); i++) { assertThat("Translog should keep op with seqno [" + i + "]", seqNoList, hasItem(i)); } } }	lol. over fanaticism. can we just have -1, 100ns and 60m? (ie., none , very likely to happen and never)
public void testMissingRootJvmOptions() throws IOException, JvmOptionsParser.JvmOptionsFileParserException { final Path config = newTempDir(); try { final JvmOptionsParser parser = new JvmOptionsParser(); parser.readJvmOptionsFiles(config); fail(); } catch (final NoSuchFileException expected) { // this is expected, the root JVM options file must exist } }	this is what is triggering the need for the temporary directory to exist.
public DateFieldType fieldType() { return (DateFieldType) super.fieldType(); }	nit: no need for these newlines?
public static SourceToParse source(String index, String type, String id, String routing, BytesReference source, XContentType contentType) { return new SourceToParse(index, type, id, routing, source, contentType); }	mark as @nullable both here and in constructor / getter?
public static SourceToParse source(String index, String type, String id, String routing, BytesReference source, XContentType contentType) { return new SourceToParse(index, type, id, routing, source, contentType); }	if we want to avoid sprinkling null all over the codebase, we can also have a second constructor here that does not take routing as parameter and simply delegates to this constructor here with routing set to null.
void getRemoteClusterState(final String remoteCluster, final long metadataVersion, final BiConsumer<ClusterStateResponse, Exception> handler) { final ClusterStateRequest request = new ClusterStateRequest(); request.clear(); request.metaData(true); request.routingTable(true); request.waitForMetaDataVersion(metadataVersion); // TODO: set non-compliant status on auto-follow coordination that can be viewed via a stats API ccrLicenseChecker.checkRemoteClusterLicenseAndFetchClusterState( client, remoteCluster, request, e -> handler.accept(null, e), remoteClusterStateRsp -> handler.accept(remoteClusterStateRsp, null)); }	same comment here about spelling out to response.
void autoFollowIndices() { final ClusterState followerClusterState = followerClusterStateSupplier.get(); final AutoFollowMetadata autoFollowMetadata = followerClusterState.metaData().custom(AutoFollowMetadata.TYPE); if (autoFollowMetadata == null) { LOGGER.info("AutoFollower for cluster [{}] has stopped, because there is no autofollow metadata", remoteCluster); return; } final List<String> patterns = autoFollowMetadata.getPatterns().entrySet().stream() .filter(entry -> entry.getValue().getRemoteCluster().equals(remoteCluster)) .map(Map.Entry::getKey) .collect(Collectors.toList()); if (patterns.isEmpty()) { LOGGER.info("AutoFollower for cluster [{}] has stopped, because there are no more patterns", remoteCluster); return; } this.autoFollowPatternsCountDown = new CountDown(patterns.size()); this.autoFollowResults = new AtomicArray<>(patterns.size()); getRemoteClusterState(remoteCluster, metadataVersion + 1, (leaderClusterStateRsp, e) -> { if (leaderClusterStateRsp != null) { assert e == null; if (leaderClusterStateRsp.isWaitForTimedOut()) { autoFollowIndices(); return; } ClusterState leaderClusterState = leaderClusterStateRsp.getState(); int i = 0; for (String autoFollowPatternName : patterns) { final int slot = i; AutoFollowPattern autoFollowPattern = autoFollowMetadata.getPatterns().get(autoFollowPatternName); Map<String, String> headers = autoFollowMetadata.getHeaders().get(autoFollowPatternName); List<String> followedIndices = autoFollowMetadata.getFollowedLeaderIndexUUIDs().get(autoFollowPatternName); final List<Index> leaderIndicesToFollow = getLeaderIndicesToFollow(autoFollowPattern, leaderClusterState, followerClusterState, followedIndices); if (leaderIndicesToFollow.isEmpty()) { finalise(slot, new AutoFollowResult(autoFollowPatternName)); } else { List<Tuple<String, AutoFollowPattern>> patternsForTheSameLeaderCluster = autoFollowMetadata.getPatterns() .entrySet().stream() .filter(item -> autoFollowPatternName.equals(item.getKey()) == false) .filter(item -> remoteCluster.equals(item.getValue().getRemoteCluster())) .map(item -> new Tuple<>(item.getKey(), item.getValue())) .collect(Collectors.toList()); Consumer<AutoFollowResult> resultHandler = result -> finalise(slot, result); checkAutoFollowPattern(autoFollowPatternName, remoteCluster, autoFollowPattern, leaderIndicesToFollow, headers, patternsForTheSameLeaderCluster, resultHandler); } i++; } } else { List<AutoFollowResult> results = new ArrayList<>(patterns.size()); for (String autoFollowPatternName : patterns) { results.add(new AutoFollowResult(autoFollowPatternName, e)); } statsUpdater.accept(results); } }); }	remote instead of leader.
public StoredContext stashContext() { final ThreadContextStruct context = threadLocal.get(); /** * X-Opaque-ID should be preserved in a threadContext in order to propagate this across threads. * This is needed so the DeprecationLogger in another thread can see the value of X-Opaque-ID provided by a user. * Otherwise when context is stash, it should be empty. */ if (context.requestHeaders.containsKey(Task.X_OPAQUE_ID) || context.requestHeaders.containsKey(Task.TRACE_PARENT)) { var map = new HashMap<>(Map.<String, String>of()); if (context.requestHeaders.containsKey(Task.X_OPAQUE_ID)) { map.put(Task.X_OPAQUE_ID, context.requestHeaders.get(Task.X_OPAQUE_ID)); } if (context.requestHeaders.containsKey(Task.TRACE_PARENT)) { map.put(Task.TRACE_PARENT, context.requestHeaders.get(Task.TRACE_PARENT)); } ThreadContextStruct threadContextStruct = DEFAULT_CONTEXT.putHeaders(map); threadLocal.set(threadContextStruct); } else { threadLocal.set(DEFAULT_CONTEXT); } return () -> { // If the node and thus the threadLocal get closed while this task // is still executing, we don't want this runnable to fail with an // uncaught exception threadLocal.set(context); }; }	avoids that the default table size of 16 is used when the map can only every hold 2 entries -> reduces allocations. suggestion map<string, string> map = new hashmap<>(2, 1);
@Override public void visit(QueryVisitor visitor) { if (visitor.acceptField(field)) { for (int i = 0; i < termArrays.size() - 1; i++) { visitor.consumeTerms(this, termArrays.get(i)); } for (Term term : termArrays.get(termArrays.size() - 1)) { visitor.consumeTermsMatching(this, field, () -> { Automaton a = PrefixQuery.toAutomaton(term.bytes()); return new CompiledAutomaton(a).runAutomaton; }); } visitor.visitLeaf(this); // TODO implement term visiting } }	i wonder if this automaton will respect maxexpansions parameter? or since we expect mostly use highlighter based on matches this is not important?
@Override public void visit(QueryVisitor visitor) { if (visitor.acceptField(field)) { for (int i = 0; i < termArrays.size() - 1; i++) { visitor.consumeTerms(this, termArrays.get(i)); } for (Term term : termArrays.get(termArrays.size() - 1)) { visitor.consumeTermsMatching(this, field, () -> { Automaton a = PrefixQuery.toAutomaton(term.bytes()); return new CompiledAutomaton(a).runAutomaton; }); } visitor.visitLeaf(this); // TODO implement term visiting } }	should we remove "// todo implement term visiting" on the next line?
public void testMultiPhrasePrefixQuery() throws Exception { final String[] inputs = { "The quick brown fox." }; final String[] outputs = { "The <b>quick brown fox</b>." }; MultiPhrasePrefixQuery query = new MultiPhrasePrefixQuery("text"); query.add(new Term("text", "quick")); query.add(new Term("text", "brown")); query.add(new Term("text", "fo")); assertHighlightOneDoc( "text", inputs, new StandardAnalyzer(), query, Locale.ROOT, BreakIterator.getSentenceInstance(Locale.ROOT), 0, outputs ); }	does this output change imply that we are dealing with breaking change?
public void checkForDeprecations(String id, NamedXContentRegistry namedXContentRegistry, Consumer<DeprecationIssue> onDeprecation) { LoggingDeprecationAccumulationHandler deprecationLogger = new LoggingDeprecationAccumulationHandler(); try { queryFromXContent(source, namedXContentRegistry, deprecationLogger); } catch (IOException e) { onDeprecation.accept( new DeprecationIssue( Level.CRITICAL, "Transform [" + id + "]: " + TransformMessages.LOG_TRANSFORM_CONFIGURATION_BAD_QUERY, TransformDeprecations.QUERY_BREAKING_CHANGES_URL, e.getMessage(), false, null ) ); } deprecationLogger.getDeprecations().forEach(deprecationMessage -> { onDeprecation.accept( new DeprecationIssue( Level.CRITICAL, "Transform [" + id + "] uses deprecated query options", TransformDeprecations.QUERY_BREAKING_CHANGES_URL, deprecationMessage, false, null ) ); }); }	ok, i like this way better than the way ml is doing it (these list of strings, etc.). good refactoring i think++
private static QueryBuilder queryFromXContent( Map<String, Object> source, NamedXContentRegistry namedXContentRegistry, DeprecationHandler deprecationHandler ) throws IOException { QueryBuilder query = null; XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().map(source); XContentParser sourceParser = XContentType.JSON.xContent() .createParser(namedXContentRegistry, deprecationHandler, BytesReference.bytes(xContentBuilder).streamInput()); query = AbstractQueryBuilder.parseInnerQueryBuilder(sourceParser); return query; }	is this local variable needed at all?
private static AggregatorFactories.Builder aggregationsFromXContent( Map<String, Object> source, NamedXContentRegistry namedXContentRegistry, DeprecationHandler deprecationHandler ) throws IOException { AggregatorFactories.Builder aggregations = null; XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().map(source); XContentParser sourceParser = XContentType.JSON.xContent() .createParser(namedXContentRegistry, deprecationHandler, BytesReference.bytes(xContentBuilder).streamInput()); sourceParser.nextToken(); aggregations = AggregatorFactories.parseAggregators(sourceParser); return aggregations; }	is this local variable needed?
* @param source snapshot to clone from * @param indices indices to clone * @param startTime start time * @param repositoryStateId repository state id that this clone is based on * @param version repository metadata version to write * @return snapshot clone entry */ public static Entry startClone(Snapshot snapshot, SnapshotId source, List<IndexId> indices, long startTime, long repositoryStateId, Version version) { return new SnapshotsInProgress.Entry(snapshot, true, false, State.STARTED, indices, Collections.emptyList(), Collections.emptyList(), startTime, repositoryStateId, ImmutableOpenMap.of(), null, Collections.emptyMap(), version, source, ImmutableOpenMap.of()); }	is this true? shouldn't this be taken from the original snapshot if it contained relevant system indices and the clone operation was set to clone those?
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Entry entry = (Entry) o; if (includeGlobalState != entry.includeGlobalState) return false; if (partial != entry.partial) return false; if (startTime != entry.startTime) return false; if (indices.equals(entry.indices) == false) return false; if (dataStreams.equals(entry.dataStreams) == false) return false; if (shards.equals(entry.shards) == false) return false; if (snapshot.equals(entry.snapshot) == false) return false; if (state != entry.state) return false; if (repositoryStateId != entry.repositoryStateId) return false; if (Objects.equals(failure, ((Entry) o).failure) == false) return false; if (Objects.equals(userMetadata, ((Entry) o).userMetadata) == false) return false; if (version.equals(entry.version) == false) return false; if (Objects.equals(source, ((Entry) o).source) == false) return false; if (clones.equals(((Entry) o).clones) == false) return false; if (!featureStates.equals(entry.featureStates)) return false; return true; }	we tend to use == false through the es codebase
public static Entry startedEntry(Snapshot snapshot, boolean includeGlobalState, boolean partial, List<IndexId> indices, List<String> dataStreams, List<SnapshotFeatureInfo> featureStates, long repositoryStateId, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards, Map<String, Object> userMetadata, Version version, long startTime) { return new SnapshotsInProgress.Entry(snapshot, includeGlobalState, partial, completed(shards.values()) ? State.SUCCESS : State.STARTED, indices, dataStreams, featureStates, startTime, repositoryStateId, shards, null, userMetadata, version); }	why switch places here, let's just put this as the last argument to keep noise down when backporting (there's some other ctors in 7.x i think that will be painful otherwise).
private void finalizeSnapshotEntry(SnapshotsInProgress.Entry entry, Metadata metadata, RepositoryData repositoryData) { assert currentlyFinalizing.contains(entry.repository()); try { final String failure = entry.failure(); final Snapshot snapshot = entry.snapshot(); logger.trace("[{}] finalizing snapshot in repository, state: [{}], failure[{}]", snapshot, entry.state(), failure); ArrayList<SnapshotShardFailure> shardFailures = new ArrayList<>(); for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardStatus : entry.shards()) { ShardId shardId = shardStatus.key; ShardSnapshotStatus status = shardStatus.value; final ShardState state = status.state(); if (state.failed()) { shardFailures.add(new SnapshotShardFailure(status.nodeId(), shardId, status.reason())); } else if (state.completed() == false) { shardFailures.add(new SnapshotShardFailure(status.nodeId(), shardId, "skipped")); } else { assert state == ShardState.SUCCESS; } } final ShardGenerations shardGenerations = buildGenerations(entry, metadata); final String repository = snapshot.getRepository(); final SnapshotInfo snapshotInfo = new SnapshotInfo(snapshot.getSnapshotId(), shardGenerations.indices().stream().map(IndexId::getName).collect(Collectors.toList()), entry.dataStreams(), entry.featureStates(), failure, threadPool.absoluteTimeInMillis(), entry.partial() ? shardGenerations.totalShards() : entry.shards().size(), shardFailures, entry.includeGlobalState(), entry.userMetadata(), entry.startTime() ); final StepListener<Metadata> metadataListener = new StepListener<>(); final Repository repo = repositoriesService.repository(snapshot.getRepository()); if (entry.isClone()) { threadPool.executor(ThreadPool.Names.SNAPSHOT).execute( ActionRunnable.supply(metadataListener, () -> { final Metadata existing = repo.getSnapshotGlobalMetadata(entry.source()); final Metadata.Builder metaBuilder = Metadata.builder(existing); final Set<Index> existingIndices = new HashSet<>(); for (IndexId index : entry.indices()) { final IndexMetadata indexMetadata = repo.getSnapshotIndexMetaData(repositoryData, entry.source(), index); existingIndices.add(indexMetadata.getIndex()); metaBuilder.put(indexMetadata, false); } // remove those data streams from metadata for which we are missing indices Map<String, DataStream> dataStreamsToCopy = new HashMap<>(); for (Map.Entry<String, DataStream> dataStreamEntry : existing.dataStreams().entrySet()) { if (existingIndices.containsAll(dataStreamEntry.getValue().getIndices())) { dataStreamsToCopy.put(dataStreamEntry.getKey(), dataStreamEntry.getValue()); } } metaBuilder.dataStreams(dataStreamsToCopy); return metaBuilder.build(); })); } else { metadataListener.onResponse(metadata); } metadataListener.whenComplete(meta -> repo.finalizeSnapshot( shardGenerations, repositoryData.getGenId(), metadataForSnapshot(entry, meta), snapshotInfo, entry.version(), state -> stateWithoutSnapshot(state, snapshot), ActionListener.wrap(newRepoData -> { completeListenersIgnoringException(endAndGetListenersToResolve(snapshot), Tuple.tuple(newRepoData, snapshotInfo)); logger.info("snapshot [{}] completed with state [{}]", snapshot, snapshotInfo.state()); runNextQueuedOperation(newRepoData, repository, true); }, e -> handleFinalizationFailure(e, entry, repositoryData))), e -> handleFinalizationFailure(e, entry, repositoryData)); } catch (Exception e) { assert false : new AssertionError(e); handleFinalizationFailure(e, entry, repositoryData); } } /** * Remove a snapshot from {@link #endingSnapshots}	is this even true for partial snapshots? we can concurrently delete the related system indices for partial snapshots and then not see them in the final snapshot can't we?
@Override protected void doRun() { try { String text = NlpTask.extractInput(processContext.modelInput.get(), doc); NlpTask.Processor processor = processContext.nlpTaskProcessor.get(); processor.validateInputs(text); Tuple<BytesReference, NlpTask.ResultProcessor> requestAndResult = processor .getRequestBuilder() .buildRequest(text, requestId); logger.trace(() -> "Inference Request "+ requestAndResult.v1().utf8ToString()); PyTorchResultProcessor.PendingResult pendingResult = processContext.resultProcessor.requestWritten(requestId); processContext.process.get().writeInferenceRequest(requestAndResult.v1()); waitForResult(processContext, pendingResult, requestId, timeout, requestAndResult.v2(), listener); } catch (IOException e) { logger.error(new ParameterizedMessage("[{}] error writing to process", processContext.modelId), e); onFailure(ExceptionsHelper.serverError("error writing to process", e)); } catch (Exception e) { onFailure(e); } finally { processContext.resultProcessor.requestAccepted(requestId); } }	@dimitris-athanasiou has made a bug fix here in #76719 you may need to resolve
private static EsField createField(DataTypeRegistry typeRegistry, String fieldName, String typeName, Map<String, EsField> props, boolean isAggregateable, boolean isAlias) { DataType esType = typeRegistry.fromEs(typeName); if (esType == TEXT) { return new TextEsField(fieldName, props, false, isAlias); } if (esType == KEYWORD) { int length = Short.MAX_VALUE; // TODO: to check whether isSearchable/isAggregateable takes into account the presence of the normalizer boolean normalized = false; return new KeywordEsField(fieldName, props, isAggregateable, length, normalized, isAlias); } if (esType == DATETIME) { return new DateEsField(fieldName, props, isAggregateable); } if (esType == UNSUPPORTED) { return new UnsupportedEsField(fieldName, typeName, null, props); } return new EsField(fieldName, esType, props, isAggregateable, isAlias); }	i think you can remove length and use its value directly in the keywordesfield consructor. i would say that normalized can also be removed, and still keep the todo in there?
@Override public ScriptTemplate asScript() { ScriptTemplate leftScript = asScript(left); ScriptTemplate rightScript = asScript(right == null ? Literal.NULL : right); return asScriptFrom(leftScript, rightScript); }	personal preference: i kinda liked more the method asoptionascript
private Tuple<Command, SqlSession> sql(String sql) { EsIndex test = new EsIndex("test", SqlTypesTests.loadMapping("mapping-multi-field-with-nested.json", true)); Analyzer analyzer = new Analyzer(SqlTestUtils.TEST_CFG, new FunctionRegistry(), IndexResolution.valid(test), null); Command cmd = (Command) analyzer.analyze(parser.createStatement(sql), false); IndexResolver resolver = mock(IndexResolver.class); SqlSession session = new SqlSession(SqlTestUtils.TEST_CFG, null, null, resolver, null, null, null, null, null); return new Tuple<>(cmd, session); }	could you please share the logic in this re-arrangement?
* @param transformConfig transform configuration * @return timestamp aligned with date histogram interval */ private static long alignTimestamp(long timestamp, TransformConfig transformConfig) { if (transformConfig.getPivotConfig() == null) { return timestamp; } if (transformConfig.getPivotConfig().getGroupConfig() == null) { return timestamp; } Map<String, SingleGroupSource> groups = transformConfig.getPivotConfig().getGroupConfig().getGroups(); if (groups == null || groups.isEmpty()) { return timestamp; } Map.Entry<String, SingleGroupSource> topLevelGroupEntry = groups.entrySet().iterator().next(); if (topLevelGroupEntry.getValue() instanceof DateHistogramGroupSource) { DateHistogramGroupSource dateHistogramGroupSource = (DateHistogramGroupSource) topLevelGroupEntry.getValue(); long interval = dateHistogramGroupSource.getInterval().getInterval().estimateMillis(); timestamp -= timestamp % interval; } return timestamp; }	there is a getrounding() method in datehistogramgroupsource for this purpose. sometimes rounding isn't trivial and has edge cases, i think this approach does not work for calendar intervals, but getrounding().round(...) will. (today we use this rounding for change detection)
public void testSearcherId() throws Exception { IOUtils.close(engine, store); AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED); try (Store store = createStore()) { final MergePolicy mergePolicy = new FilterMergePolicy(NoMergePolicy.INSTANCE) { @Override public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) { return true; } }; final EngineConfig config = config(defaultSettings, store, createTempDir(), mergePolicy, null, null, globalCheckpoint::get); String lastSearcherId; Map<String, Engine.Operation> latestOps = new HashMap<>(); try (InternalEngine engine = createEngine(null, null, null, config)) { lastSearcherId = ReadOnlyEngine.generateSearcherId(engine.getLastCommittedSegmentInfos()); assertNotNull(lastSearcherId); int iterations = randomIntBetween(0, 10); for (int i = 0; i < iterations; i++) { assertThat(ReadOnlyEngine.generateSearcherId(engine.getLastCommittedSegmentInfos()), equalTo(lastSearcherId)); final List<Engine.Operation> operations = generateHistoryOnReplica(between(1, 100), engine.getProcessedLocalCheckpoint() + 1L, false, randomBoolean(), randomBoolean()); boolean hasChanges = false; for (Engine.Operation op : operations) { if (op instanceof Engine.Index) { engine.index((Engine.Index) op); if (latestOps.containsKey(op.id()) == false || latestOps.get(op.id()).seqNo() < op.seqNo()) { latestOps.put(op.id(), op); hasChanges = true; } } else if (op instanceof Engine.Delete) { engine.delete((Engine.Delete) op); final Engine.Operation lastOp = latestOps.get(op.id()); hasChanges |= (lastOp instanceof Engine.Index && lastOp.seqNo() < op.seqNo()); if (lastOp == null || lastOp.seqNo() < op.seqNo()) { latestOps.put(op.id(), op); } } else { assertThat(op, instanceOf(Engine.NoOp.class)); engine.noOp((Engine.NoOp) op); } if (randomInt(100) < 10) { engine.refresh("test"); } if (rarely()) { engine.flush(false, true); } } engine.flush(randomBoolean(), true); final String newSearcherId = ReadOnlyEngine.generateSearcherId(engine.getLastCommittedSegmentInfos()); if (hasChanges || config.getIndexSettings().isSoftDeleteEnabled()) { assertThat(newSearcherId, not(equalTo(lastSearcherId))); } else { assertThat(newSearcherId, equalTo(lastSearcherId)); } if (randomBoolean()) { engine.flush(true, true); assertThat(ReadOnlyEngine.generateSearcherId(engine.getLastCommittedSegmentInfos()), equalTo(newSearcherId)); } lastSearcherId = newSearcherId; } globalCheckpoint.set(engine.getProcessedLocalCheckpoint()); } try (ReadOnlyEngine readOnlyEngine = new ReadOnlyEngine(config, null, null, true, Function.identity(), true)) { try (Engine.SearcherSupplier searcher = readOnlyEngine.acquireSearcherSupplier(Function.identity(), randomFrom(Engine.SearcherScope.values()))) { assertThat(searcher.getSearcherId(), equalTo(lastSearcherId)); } } } }	i struggle to see how this line can have any effect? if we have an index entry in the map, we must have already set haschanges = true above?
public long parse(String text, long now) { return parse(text, now, false, DateTimeZone.UTC); }	should you pass null here to make sure we are consistent with the previous behavior?
* @return tuple of BlobStoreIndexShardSnapshots and the last snapshot index generation */ private Tuple<BlobStoreIndexShardSnapshots, Long> buildBlobStoreIndexShardSnapshots(Set<String> blobs, BlobContainer shardContainer) throws IOException { long latest = latestGeneration(blobs); if (latest >= 0) { final BlobStoreIndexShardSnapshots shardSnapshots = indexShardSnapshotsFormat.read(shardContainer, Long.toString(latest)); return new Tuple<>(shardSnapshots, latest); } else if (blobs.stream().anyMatch(b -> b.startsWith(SNAPSHOT_PREFIX) || b.startsWith(INDEX_FILE_PREFIX) || b.startsWith(UPLOADED_DATA_BLOB_PREFIX))) { logger.warn("Could not find a readable index-N file in a non-empty shard snapshot directory [" + shardContainer.path() + "]"); } return new Tuple<>(BlobStoreIndexShardSnapshots.EMPTY, latest); }	i thought about whether this is ok or not quite a bit. i think we're not introducing any new risk qualitatively really. if we can't read what is in this directory it's broken, period. for snapshot creation: we can have some freak concurrency issues here if we aren't yet using shard generations and two data nodes try to write to the same path at the same time. but those we resolve either (on s3) by using the 3 minute wait or by not allowing overwrites for index-n on other types of repos. with shard generations being used, this is a non-issue anyway. for deletions we are in the clear also, because everything runs on the master.
private static String toLoggerName(final Class<?> cls) { String canonicalName = cls.getCanonicalName(); return canonicalName != null ? canonicalName : cls.getName(); } /** * Logs a message at the {@link #DEPRECATION}	btw do you think it would be worthy to make this method work in a "fluent" way? i initialliy thought that deprecation logger could be used like deprecationlogger.deprecate("msg") .compatiblewarning("msg2") the compatible logger is still not merged (going to be soon i hope), but maybe we could use the builder here? i am worried about the performance and we won't be creating too much objects here though.. wdyt?
static Query extractQuery(Query query) { if (query instanceof BoostQuery) { return extractQuery(((BoostQuery) query).getQuery()); } else if (query instanceof IndexOrDocValuesQuery) { return extractQuery(((IndexOrDocValuesQuery) query).getIndexQuery()); } else { return query; } }	maybe its possible to cover this execution branch in the tests as well.
@Override public void readFrom(StreamInput in) throws IOException { index = in.readString(); if (in.getVersion().onOrAfter(Version.V_5_4_0_UNRELEASED)) { shard = in.readInt(); } valid = in.readBoolean(); explanation = in.readOptionalString(); error = in.readOptionalString(); }	do you want to set it to -1 or something in the else arm so it is obviously bogus?
public void testExplainWithRewriteValidateQueryAllShards() throws Exception { client().admin().indices().prepareCreate("test") .addMapping("type1", "field", "type=text,analyzer=whitespace") .setSettings(SETTING_NUMBER_OF_SHARDS, 2).get(); client().prepareIndex("test", "type1", "1") .setSource("field", "quick lazy huge brown pidgin").get(); client().prepareIndex("test", "type1", "2") .setSource("field", "the quick brown fox").get(); client().prepareIndex("test", "type1", "3") .setSource("field", "the quick lazy huge brown fox jumps over the tree").get(); client().prepareIndex("test", "type1", "4") .setSource("field", "the lazy dog quacks like a duck").get(); refresh(); // prefix queries assertExplanations(QueryBuilders.matchPhrasePrefixQuery("field", "qu"), Arrays.asList( equalTo("field:quick"), allOf(containsString("field:quick"), containsString("field:quacks")) ), true, true); assertExplanations(QueryBuilders.matchPhrasePrefixQuery("field", "ju"), Arrays.asList( equalTo("field:jumps"), equalTo("+MatchNoDocsQuery(\\\\"empty MultiPhraseQuery\\\\") +MatchNoDocsQuery(\\\\"No " + "terms supplied for org.elasticsearch.common.lucene.search." + "MultiPhrasePrefixQuery\\\\")") ), true, true); }	i'd leave a comment here about how you are relying on specific routing behaviors for the result to be right.
public static InnerHitBuilder randomInnerHits(boolean recursive, boolean includeQueryTypeOrPath) { InnerHitBuilder innerHits = new InnerHitBuilder(); innerHits.setName(randomAlphaOfLengthBetween(1, 16)); innerHits.setFrom(randomIntBetween(0, 128)); innerHits.setSize(randomIntBetween(0, 128)); innerHits.setExplain(randomBoolean()); innerHits.setVersion(randomBoolean()); innerHits.setTrackScores(randomBoolean()); if (randomBoolean()) { innerHits.setStoredFieldNames(randomListStuff(16, () -> randomAlphaOfLengthBetween(1, 16))); } innerHits.setDocValueFields(randomListStuff(16, () -> randomAlphaOfLengthBetween(1, 16))); // Random script fields deduped on their field name. Map<String, SearchSourceBuilder.ScriptField> scriptFields = new HashMap<>(); for (SearchSourceBuilder.ScriptField field: randomListStuff(16, InnerHitBuilderTests::randomScript)) { scriptFields.put(field.fieldName(), field); } innerHits.setScriptFields(new HashSet<>(scriptFields.values())); FetchSourceContext randomFetchSourceContext; if (randomBoolean()) { randomFetchSourceContext = new FetchSourceContext(true, Strings.EMPTY_ARRAY, Strings.EMPTY_ARRAY); } else { randomFetchSourceContext = new FetchSourceContext(true, generateRandomStringArray(12, 16, false), generateRandomStringArray(12, 16, false) ); } innerHits.setFetchSourceContext(randomFetchSourceContext); if (randomBoolean()) { innerHits.setSorts(randomListStuff(16, () -> SortBuilders.fieldSort(randomAlphaOfLengthBetween(5, 20)).order(randomFrom(SortOrder.values()))) ); } innerHits.setHighlightBuilder(HighlightBuilderTests.randomHighlighterBuilder()); if (recursive && randomBoolean()) { int size = randomIntBetween(1, 16); for (int i = 0; i < size; i++) { innerHits.addChildInnerHit(randomInnerHits(false, includeQueryTypeOrPath)); } } if (includeQueryTypeOrPath) { QueryBuilder query = new MatchQueryBuilder(randomAlphaOfLengthBetween(1, 16), randomAlphaOfLengthBetween(1, 16)); if (randomBoolean()) { return new InnerHitBuilder(innerHits, randomAlphaOfLength(8), query, randomBoolean()); } else { return new InnerHitBuilder(innerHits, query, randomAlphaOfLength(8), randomBoolean()); } } else { return innerHits; } }	can you add more randomization here. you can build a random list of field names for the includes and the excludes for instance.
public void testIncompatibleElasticsearchVersion() throws Exception { PluginInfo info = new PluginInfo("my_plugin", "desc", "1.0", Version.fromId(6000099), "1.8", "FakePlugin", Collections.emptyList(), false); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> PluginsService.verifyCompatibility(info)); assertThat(e.getMessage(), containsString("was built for Elasticsearch version 6.0.0")); }	why is this better?
protected Settings createTestIndexSettings() { // we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually. Version indexVersionCreated = randomBoolean() ? Version.CURRENT : VersionUtils.randomVersionBetween(random(), Version.CURRENT.minimumIndexCompatibilityVersion(), Version.CURRENT); return Settings.builder() .put(IndexMetaData.SETTING_VERSION_CREATED, indexVersionCreated) .build(); }	i assume this was done before you made versionutils.randomindexcompatibleversion. should this be updated to use that?
public static long getStartingSeqNo(final RecoveryTarget recoveryTarget) { try { final long globalCheckpoint = Translog.readGlobalCheckpoint(recoveryTarget.translogLocation()); final SeqNoStats seqNoStats = recoveryTarget.store().loadSeqNoStats(globalCheckpoint); if (seqNoStats.getMaxSeqNo() <= seqNoStats.getGlobalCheckpoint()) { assert seqNoStats.getLocalCheckpoint() <= seqNoStats.getGlobalCheckpoint(); /* * Commit point is good for sequence-number based recovery as the maximum sequence number included in it is below the global * checkpoint (i.e., it excludes any operations that may not be on the primary). Recovery will start at the first operation * after the local checkpoint stored in the commit. */ return seqNoStats.getLocalCheckpoint() + 1; } else { return SequenceNumbersService.UNASSIGNED_SEQ_NO; } } catch (final IOException e) { /* * This can happen, for example, if a phase one of the recovery completed successfully, a network partition happens before the * translog on the recovery target is opened, the recovery enters a retry loop seeing now that the index files are on disk and * proceeds to attempt a sequence-number-based recovery. */ return SequenceNumbersService.UNASSIGNED_SEQ_NO; } }	can you move this up one line (i.e. outside of the if condition)?
public String concreteFunctionName(String alias) { String normalized = normalize(alias); return aliases.getOrDefault(normalized, alias.toUpperCase(Locale.ROOT)); }	i agree with the alias.touppercase() but i don't see why we normalize the received name. this means we accept charlength as a function name, but if the function name is incorrect (for example charlen) we tell the user what he meant (character_length, char_length). no mentioning of charlength. imo, given that it's impossible to guess where to put the underscore in case of a charlength or charlength, we should only accept straight names, without trying to normalize. otherwise, we won't be able to offer a consistent behavior to the user.
private void resolveConfig(AggregationContext aggregationContext) { config = new ValuesSourceConfig<>(ValuesSource.Bytes.WithOrdinals.ParentChild.class); DocumentMapper childDocMapper = aggregationContext.searchContext().mapperService().documentMapper(childType); if (childDocMapper != null) { ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper(); if (!parentFieldMapper.active()) { throw new SearchParseException(aggregationContext.searchContext(), "[children] no [_parent] field not configured that points to a parent type", null); // NOCOMMIT fix exception args } parentType = parentFieldMapper.type(); DocumentMapper parentDocMapper = aggregationContext.searchContext().mapperService().documentMapper(parentType); if (parentDocMapper != null) { // TODO: use the query API parentFilter = parentDocMapper.typeFilter(); childFilter = childDocMapper.typeFilter(); ParentChildIndexFieldData parentChildIndexFieldData = aggregationContext.searchContext().fieldData() .getForField(parentFieldMapper.fieldType()); config.fieldContext(new FieldContext(parentFieldMapper.fieldType().names().indexName(), parentChildIndexFieldData, parentFieldMapper.fieldType())); } else { config.unmapped(true); } } else { config.unmapped(true); } }	this comment is not relevant anymore as it seems to be using the query api already?
private void resolveValuesSourceConfigFromAncestors(String aggName, AggregatorFactory parent, Class<VS> requiredValuesSourceType) { ValuesSourceConfig config; while (parent != null) { if (parent instanceof ValuesSourceAggregatorFactory) { config = ((ValuesSourceAggregatorFactory) parent).config; if (config != null && config.valid()) { if (requiredValuesSourceType == null || requiredValuesSourceType.isAssignableFrom(config.valueSourceType)) { ValueFormat format = config.format; this.config = config; // if the user explicitly defined a format pattern, // we'll do our best to keep it even when we inherit the // value source form one of the ancestor aggregations if (this.config.formatPattern != null && format != null && format instanceof ValueFormat.Patternable) { this.config.format = ((ValueFormat.Patternable) format).create(this.config.formatPattern); } return; } } } parent = parent.parent(); } throw new AggregationExecutionException("could not find the appropriate value context to perform aggregation [" + aggName + "]"); }	should this nocommit block this pr?
public static IncidentEventContext parse(XContentParser parser) throws IOException { Type type = null; String href = null; String text = null; String src = null; String alt = null; String currentFieldName = null; XContentParser.Token token; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (Strings.hasLength(currentFieldName)) { if (XField.TYPE.match(currentFieldName, parser.getDeprecationHandler())) { try { type = Type.valueOf(parser.text().toUpperCase(Locale.ROOT)); } catch (IllegalArgumentException e) { String msg = "could not parse trigger incident event context. unknown context type [{}]"; throw new ElasticsearchParseException(msg, parser.text()); } } else { if (XField.HREF.match(currentFieldName, parser.getDeprecationHandler())) { href = parser.text(); } else if (XField.TEXT.match(currentFieldName, parser.getDeprecationHandler())) { text = parser.text(); } else if (XField.SRC.match(currentFieldName, parser.getDeprecationHandler())) { src = parser.text(); } else if (XField.ALT.match(currentFieldName, parser.getDeprecationHandler())) { alt = parser.text(); } else { String msg = "could not parse trigger incident event context. unknown field [{}]"; throw new ElasticsearchParseException(msg, currentFieldName); } } } } return createAndValidateTemplate(type, href, src, alt, text); }	these are straight copys of the template's parse method, but without the texttemplate mess. these will be used in the future when i finally finish removing the templates :)
public void testGetSnapshotsWithoutIndices() { createRepository("test-repo", "fs", randomRepoPath()); logger.info("--> snapshot"); final SnapshotInfo snapshotInfo = client().admin().cluster().prepareCreateSnapshot("test-repo", "test-snap") .setIndices().setWaitForCompletion(true).get().getSnapshotInfo(); assertThat(snapshotInfo.state(), is(SnapshotState.SUCCESS)); assertThat(snapshotInfo.totalShards(), is(0)); logger.info("--> verify that snapshot without index shows up in non-verbose listing"); final List<SnapshotInfo> snapshotInfos = client().admin().cluster().prepareGetSnapshots("test-repo").setVerbose(false).get().getSnapshots("test-repo"); assertThat(snapshotInfos, hasSize(1)); final SnapshotInfo found = snapshotInfos.get(0); assertThat(found.snapshotId(), is(snapshotInfo.snapshotId())); assertThat(found.state(), is(SnapshotState.SUCCESS)); }	can you add some javadocs to this method to describe what exact situation is being tested here? there are a large number of steps in the test, and it might be easier to have a high-level description of what's going on here.
public TimeValue throttleWaitTime(TimeValue lastBatchStartTime, TimeValue now, int lastBatchSize) { long earliestNextBatchStartTime = now.nanos() + (long) perfectlyThrottledBatchTime(lastBatchSize); long waitTime = min(MAX_THROTTLE_WAIT_TIME, max(0, earliestNextBatchStartTime - System.nanoTime())); return timeValueNanos(waitTime); }	i like limiting this here but think maybe we ought to also enforce it at request start and rethrottle time. we can work backwards from the batch size to reject requests that had a requests_per_second that is too small, i think.
public void testStdDevNaNAvg() { assertThat(MovingFunctions.stdDev(new double[] { 1.0, 2.0, 3.0 }, Double.NaN), equalTo(Double.NaN)); }	not sure if this test is useful since it cannot check the early termination of the method, but might be worth adding just to cover that code path, wdyt?
@Override public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException { final DocIdSet set = filter.getDocIdSet(context, null); final LeafCollector in = collector.getLeafCollector(context); final Bits bits = set == null ? null : set.bits(); if (bits != null) { // the filter supports random-access return new FilterLeafCollector(in) { public void collect(int doc) throws IOException { if (bits.get(doc)) { in.collect(doc); } } }; } // No random-access support, use the iterator and force in-order scoring final DocIdSetIterator iterator; if (DocIdSets.isEmpty(set)) { iterator = null; } else { iterator = set.iterator(); } return new FilterLeafCollector(in) { @Override public void collect(int doc) throws IOException { if (iterator == null) { return; } final int itDoc = iterator.docID(); if (itDoc > doc) { return; } else if (itDoc < doc) { if (iterator.advance(doc) == doc) { in.collect(doc); } } else { in.collect(doc); } } @Override public boolean acceptsDocsOutOfOrder() { // we only support iterating in order because the iterator can only advance return false; } }; }	you could avoid this null check by returning an alternate anon filterleafcollector above in the isempty case which has an empty collect method?
public void setInitialConfiguration(final VotingConfiguration votingConfiguration) { synchronized (mutex) { final ClusterState currentState = getStateForMasterService(); if (currentState.getLastAcceptedConfiguration().isEmpty() == false) { throw new CoordinationStateRejectedException("Cannot set initial configuration: configuration has already been set"); } assert currentState.term() == 0 : currentState; assert currentState.version() == 0 : currentState; if (mode != Mode.CANDIDATE) { throw new CoordinationStateRejectedException("Cannot set initial configuration in mode " + mode); } final List<DiscoveryNode> knownNodes = new ArrayList<>(); knownNodes.add(getLocalNode()); peerFinder.getFoundPeers().forEach(knownNodes::add); if (votingConfiguration.hasQuorum(knownNodes.stream().map(DiscoveryNode::getId).collect(Collectors.toList())) == false) { throw new CoordinationStateRejectedException("not enough nodes discovered to form a quorum in the initial configuration " + "[knownNodes=" + knownNodes + ", " + votingConfiguration + "]"); } logger.info("setting initial configuration to {}", votingConfiguration); final Builder builder = masterService.incrementVersion(currentState); builder.lastAcceptedConfiguration(votingConfiguration); builder.lastCommittedConfiguration(votingConfiguration); coordinationState.get().setInitialState(builder.build()); preVoteCollector.update(getPreVoteResponse(), null); // pick up the change to last-accepted version startElectionScheduler(); } }	nit: can you add a description for the votingconfiguration, i.e. initialconfig=, otherwise it's hard to depict from the exception message.
public List<Query> buildGroupedQueries(MultiMatchQueryBuilder.Type type, Map<String, Float> fieldNamesWithBoost, Object value, String minimumShouldMatch) throws IOException{ List<Query> queries = new ArrayList<>(); for (Map.Entry<String, Float> fieldNameWithBoost : fieldNamesWithBoost.entrySet()) { Float boostValue = fieldNameWithBoost.getValue(); Query query = parseGroup(type.matchQueryType(), fieldNameWithBoost.getKey(), boostValue, value, minimumShouldMatch); if (query != null) { queries.add(query); } } return queries; }	i looked up the usage of this method and found out the float is actually the boost for a field, which was not obvious before i looked it up. so i renamed it to this if that's okay. the code that calls this method also uses fieldnames without indicating the float is the boost of the field.
private static String differenceBetweenMapsIgnoringArrayOrder(String path, Map<String, Object> first, Map<String, Object> second) { if (first.size() != second.size()) { return path + ": sizes of the maps don't match: " + first.size() + " != " + second.size(); } for (Map.Entry<String, Object> clusterState : first.entrySet()) { String reason = differenceBetweenObjectsIgnoringArrayOrder(path + "/" + clusterState.getKey(), clusterState.getValue(), second.get(clusterState.getKey())); if (reason != null) { return reason; } } return null; }	looking up all the usages of this method, there are 2 code paths that end up here, both times this object is a clusterstate. should i rename this first and second to something like clusterstatea and clusterstateb? should i also rename them in the methods where they are passed along to this point in the code?
public void testInvalidTimestamp() throws Exception { assumeTrue("only test with fixture to have stable results", ENDPOINT != null); ClusterUpdateSettingsResponse settingsResponse = client().admin().cluster() .prepareUpdateSettings() .setPersistentSettings(Settings.builder() .put(GeoIpDownloaderTaskExecutor.ENABLED_SETTING.getKey(), true)) .get(); assertTrue(settingsResponse.isAcknowledged()); assertBusy(() -> { GeoIpTaskState state = getGeoIpTaskState(); assertEquals(Set.of("GeoLite2-ASN.mmdb", "GeoLite2-City.mmdb", "GeoLite2-Country.mmdb"), state.getDatabases().keySet()); }, 2, TimeUnit.MINUTES); putPipeline(); verifyUpdatedDatabase(); settingsResponse = client().admin().cluster() .prepareUpdateSettings() .setPersistentSettings(Settings.builder() .put("ingest.geoip.database_validity", TimeValue.timeValueMillis(1))) .get(); assertTrue(settingsResponse.isAcknowledged()); Thread.sleep(10); settingsResponse = client().admin().cluster() .prepareUpdateSettings() .setPersistentSettings(Settings.builder().put(GeoIpDownloader.POLL_INTERVAL_SETTING.getKey(), TimeValue.timeValueDays(2))) .get(); assertTrue(settingsResponse.isAcknowledged()); List<Path> geoIpTmpDirs = getGeoIpTmpDirs(); assertBusy(() -> { for (Path geoIpTmpDir : geoIpTmpDirs) { try (Stream<Path> files = Files.list(geoIpTmpDir)) { Set<String> names = files.map(f -> f.getFileName().toString()).collect(Collectors.toSet()); assertThat(names, not(hasItem("GeoLite2-ASN.mmdb"))); assertThat(names, not(hasItem("GeoLite2-City.mmdb"))); assertThat(names, not(hasItem("GeoLite2-Country.mmdb"))); } } }); putPipeline(); assertBusy(() -> { SimulateDocumentBaseResult result = simulatePipeline(); assertThat(result.getFailure(), nullValue()); assertTrue(result.getIngestDocument().hasField("tags")); @SuppressWarnings("unchecked") List<String> tags = result.getIngestDocument().getFieldValue("tags", List.class); assertThat(tags, contains("_geoip_expired_database")); assertFalse(result.getIngestDocument().hasField("ip-city")); assertFalse(result.getIngestDocument().hasField("ip-asn")); assertFalse(result.getIngestDocument().hasField("ip-country")); }); settingsResponse = client().admin().cluster() .prepareUpdateSettings() .setPersistentSettings(Settings.builder() .put("ingest.geoip.database_validity", (String) null)) .get(); assertTrue(settingsResponse.isAcknowledged()); assertBusy(() -> { for (Path geoIpTmpDir : geoIpTmpDirs) { try (Stream<Path> files = Files.list(geoIpTmpDir)) { Set<String> names = files.map(f -> f.getFileName().toString()).collect(Collectors.toSet()); assertThat(names, hasItems("GeoLite2-ASN.mmdb","GeoLite2-City.mmdb","GeoLite2-Country.mmdb")); } } }); }	minor nit: suggestion .putnull("ingest.geoip.database_validity")
public final void testFromXContent() throws IOException { AbstractXContentTestCase.testFromXContent( NUMBER_OF_TEST_RUNS, this::createTestInstance, supportsUnknownFields(), getShuffleFieldsExceptions(), getRandomFieldsExcludeFilter(), this::createParser, this::doParseInstance, this::assertEqualInstances, assertToXContentEquivalence(), getToXContentParams()); } /** * Parses to a new instance using the provided {@link XContentParser}	this is the key change, from true to use the asserttoxcontentequivalence hook. this is why i like long parameter lists on a single line per parameter. were this method already formatted on a single line per parameter, the diff would have made this change obvious: diff --git a/test/framework/src/main/java/org/elasticsearch/test/abstractserializingtestcase.java b/test/framework/src/main/java/org/elasticsearch/test/abstractserializingtestcase.java index 7e945a8414c..5aeb30bfdbd 100644 --- a/test/framework/src/main/java/org/elasticsearch/test/abstractserializingtestcase.java +++ b/test/framework/src/main/java/org/elasticsearch/test/abstractserializingtestcase.java @@ -44,7 +44,7 @@ public abstract class abstractserializingtestcase<t extends toxcontent & writeab this::createparser, this::doparseinstance, this::assertequalinstances, - true, + asserttoxcontentequivalence(), gettoxcontentparams()); }
public void testCacheRemoteShardFailed() throws Exception { final String index = "test"; setState(clusterService, ClusterStateCreationUtils.stateWithActivePrimary(index, true, randomInt(5))); ShardRouting failedShard = getRandomShardRouting(index); long primaryTerm = clusterService.state().metaData().index(index).primaryTerm(failedShard.id()); boolean markAsStale = randomBoolean(); int numListeners = between(1, 100); CountDownLatch latch = new CountDownLatch(numListeners); for (int i = 0; i < numListeners; i++) { shardStateAction.remoteShardFailed(failedShard.shardId(), failedShard.allocationId().getId(), primaryTerm + 1, markAsStale, "test", getSimulatedFailure(), new ShardStateAction.Listener() { @Override public void onSuccess() { latch.countDown(); } @Override public void onFailure(Exception e) { latch.countDown(); } }); } CapturingTransport.CapturedRequest[] capturedRequests = transport.getCapturedRequestsAndClear(); assertThat(capturedRequests, arrayWithSize(1)); transport.handleResponse(capturedRequests[0].requestId, TransportResponse.Empty.INSTANCE); latch.await(); assertThat(transport.capturedRequests(), arrayWithSize(0)); }	why primaryterm + 1? just a random but fixed value would do? (we never run this task)
public static void normalizePoint(double[] lonLat, boolean normLon, boolean normLat) { assert lonLat != null && lonLat.length == 2; normLat = normLat && (lonLat[1] > 90 || lonLat[1] < -90); normLon = normLon && (lonLat[0] > 180 || lonLat[0] < -180 || normLat); if (normLat) { lonLat[1] = centeredModulus(lonLat[1], 360); boolean shift = true; if (lonLat[1] < -90) { lonLat[1] = -180 - lonLat[1]; } else if (lonLat[1] > 90) { lonLat[1] = 180 - lonLat[1]; } else { // No need to shift the longitude, and the latitude is normalized shift = false; } if (shift) { if (normLon) { lonLat[0] += 180; } else { // Longitude won't be normalized, // keep it in the form x+k*360 (with x in ]-180;180]) // by only changing x, assuming k is meaningful for the user application. lonLat[0] += normalizeLon(lonLat[0]) > 0 ? -180 : 180; } } } if (normLon) { lonLat[0] = centeredModulus(lonLat[0], 360); } }	should there be a test reflecting this change in geoutilstests?
public double length(Line line) { double distance = 0; double[] prev = new double[]{line.getLon(0), line.getLat(0)}; GeoUtils.normalizePoint(prev, false, true); for (int i = 1; i < line.length(); i++) { double[] cur = new double[]{line.getLon(i), line.getLat(i)}; GeoUtils.normalizePoint(cur, false, true); distance += Math.sqrt((cur[0] - prev[0]) * (cur[0] - prev[0]) + (cur[1] - prev[1]) * (cur[1] - prev[1])); prev = cur; } return distance; }	is there a specific reason why 0.000001 was chosen here. or is it just a "close enough" difference
@Override protected TypeResolution resolveType() { TypeResolution resolution = isNumericOrDate(field(), "HISTOGRAM", ParamOrdinal.FIRST); if (resolution == TypeResolution.TYPE_RESOLVED) { // interval must be Literal interval if (field().dataType().isDateBased()) { resolution = isType(interval, d -> d.isInterval(), "(Date) HISTOGRAM", ParamOrdinal.SECOND, "interval"); } else { resolution = isNumeric(interval, "(Numeric) HISTOGRAM", ParamOrdinal.SECOND); } } return resolution; }	isn't it enough to just do datatype::isinterval ?
public DeprecationLoggerBuilder withDeprecation(String key, String msg, Object[] params) { String opaqueId = HeaderWarning.getXOpaqueId(); rateLimiter.limit(opaqueId + key, () -> { ESLogMessage deprecationMessage = DeprecatedMessage.of(opaqueId, msg, params); for (DeprecatedLogHandler handler : handlers) { handler.log(key, opaqueId, deprecationMessage); } for (DeprecatedLogHandler handler : additionalHandlers) { handler.log(key, opaqueId, deprecationMessage); } }); return this; }	as per comment above, i wonder if this could be single collection
public boolean hasNativeController() { return hasNativeController; } /** * Whether a license must be accepted before this plugin can be installed. * * @return {@code true}	nit: i just noticed this now, but it applies to the other prs as well. this comment seems no longer applicable since there is no longer an extra acceptance prompt.
private void ensureAllIndicesAreSupported(MetaData state) throws IOException { if (state.indices().isEmpty()) { /** * This is a fallback for nodes that used to be non-master nodes. Previously * nodes didn't write the index / global state. In that case we fall back to * walking the data directories to find the index shards and check them. */ final Settings settings = ImmutableSettings.builder().build(); // empty on purpose but bypassing the assertion for (String index : nodeEnv.findAllIndices()) { Set<ShardId> allShardIds = nodeEnv.findAllShardIds(new Index(index)); openShards(settings, allShardIds); } } else { for (IndexMetaData indexMetaData : state) { Set<ShardId> allShardIds = nodeEnv.findAllShardIds(new Index(indexMetaData.index())); openShards(indexMetaData.getSettings(), allShardIds); } } }	this is the way the check should work, but i'm afraid its not totally complete. imagine a 4.x commit point with some 3.x segments. i actually think lucene 5 does the wrong thing here right now and will throw some illegalargumentexception or spi error. i will look into it. in the future, this would be a cool test index for us to have cc @rjernst
private void ensureAllIndicesAreSupported(MetaData state) throws IOException { if (state.indices().isEmpty()) { /** * This is a fallback for nodes that used to be non-master nodes. Previously * nodes didn't write the index / global state. In that case we fall back to * walking the data directories to find the index shards and check them. */ final Settings settings = ImmutableSettings.builder().build(); // empty on purpose but bypassing the assertion for (String index : nodeEnv.findAllIndices()) { Set<ShardId> allShardIds = nodeEnv.findAllShardIds(new Index(index)); openShards(settings, allShardIds); } } else { for (IndexMetaData indexMetaData : state) { Set<ShardId> allShardIds = nodeEnv.findAllShardIds(new Index(indexMetaData.index())); openShards(indexMetaData.getSettings(), allShardIds); } } }	should this be a .closewhilehandlingexception, or do we want to make sure we can close them properly also?
*/ @Override public void format(LogEvent event, StringBuilder toAppendTo) { String traceId = getTraceId(); if (traceId != null) { toAppendTo.append(traceId); } }	what's the reason for this change? is it so that we can reuse the traceid formatter in ecsjsonlayout.java? i wonder if this changes behaviour a bit, previously if traceid was null the format would not append anything, but now we might get "trace.id:null". is that ok?
public BoxedQueryRequest keys(List<List<Object>> values) { List<QueryBuilder> newFilters; if (values == null || values.isEmpty()) { // no keys have been specified and none have been set if (keyFilters == null || keyFilters.isEmpty()) { return this; } newFilters = emptyList(); } else { // iterate on all possible values for a given key newFilters = new ArrayList<>(values.size()); for (int keyIndex = 0; keyIndex < keys.size(); keyIndex++) { String key = keys.get(keyIndex); boolean hasNullValue = false; Set<Object> keyValues = new HashSet<>(MAX_TERMS); // check the given keys but make sure to double check for // null as it translates to a different query (missing/not exists) for (List<Object> value : values) { Object keyValue = value.get(keyIndex); if (keyValue == null) { hasNullValue = true; } else { keyValues.add(keyValue); } } // too many unique terms, don't filter on the keys if (keyValues.size() > MAX_TERMS) { newFilters = emptyList(); break; } QueryBuilder query = null; if (keyValues.size() == 1) { query = termQuery(key, keyValues.iterator().next()); } else if (keyValues.size() > 1) { query = termsQuery(key, keyValues); } // if null values are present // make an OR call - either terms or null/missing values if (hasNullValue) { BoolQueryBuilder isMissing = boolQuery().mustNot(existsQuery(key)); if (query != null) { query = boolQuery() // terms query .should(query) // is missing .should(isMissing); } else { query = isMissing; } } newFilters.add(query); } } RuntimeUtils.replaceFilter(keyFilters, newFilters, searchSource); keyFilters = newFilters; return this; }	the key here is needed further down after the max_terms size check is performed on keyvalues size. you could move the key initialization further down after you initialize the query querybuilder with null.
static BytesRestResponse createSimpleErrorResponse(RestStatus status, String errorMessage) throws IOException { return new BytesRestResponse(status, JsonXContent.contentBuilder().startObject() .field("error", errorMessage) .endObject()); }	there is a small issue with this, could you please also print out the status?
public void testMinGenerationForSeqNo() throws IOException { final int operations = randomIntBetween(1, 4096); final List<Long> seqNos = LongStream.range(0, operations).boxed().collect(Collectors.toList()); Randomness.shuffle(seqNos); final Map<Long, List<Long>> generations = new HashMap<>(); for (int i = 0; i < operations; i++) { final Long seqNo = seqNos.get(i); translog.add(new Translog.NoOp(seqNo, 0, "test")); final List<Long> seqNoForGeneration = generations.computeIfAbsent( translog.currentFileGeneration(), g -> new ArrayList<>()); seqNoForGeneration.add(seqNo); if (rarely()) { translog.rollGeneration(); } } final Map<Long, Long> maxSeqNoByGeneration = generations .entrySet() .stream() .collect(Collectors.toMap( Map.Entry::getKey, e -> e.getValue().stream().max(Long::compareTo).orElse(Long.MAX_VALUE))); for (long seqNo = 0; seqNo < operations; seqNo++) { final long finalLongSeqNo = seqNo; final Long operand = maxSeqNoByGeneration .entrySet() .stream() .filter(e -> finalLongSeqNo <= e.getValue()) .map(Map.Entry::getKey).min(Long::compareTo) .orElse(Long.MIN_VALUE); assertThat(translog.getMinGenerationForSeqNo(seqNo).translogFileGeneration, equalTo(operand)); } }	it would be great to add duplicate seq# to this test. the real engine needs to deal with multiple versions of the same seq#, distinguished only by their associated term.
public void testMinGenerationForSeqNo() throws IOException { final int operations = randomIntBetween(1, 4096); final List<Long> seqNos = LongStream.range(0, operations).boxed().collect(Collectors.toList()); Randomness.shuffle(seqNos); final Map<Long, List<Long>> generations = new HashMap<>(); for (int i = 0; i < operations; i++) { final Long seqNo = seqNos.get(i); translog.add(new Translog.NoOp(seqNo, 0, "test")); final List<Long> seqNoForGeneration = generations.computeIfAbsent( translog.currentFileGeneration(), g -> new ArrayList<>()); seqNoForGeneration.add(seqNo); if (rarely()) { translog.rollGeneration(); } } final Map<Long, Long> maxSeqNoByGeneration = generations .entrySet() .stream() .collect(Collectors.toMap( Map.Entry::getKey, e -> e.getValue().stream().max(Long::compareTo).orElse(Long.MAX_VALUE))); for (long seqNo = 0; seqNo < operations; seqNo++) { final long finalLongSeqNo = seqNo; final Long operand = maxSeqNoByGeneration .entrySet() .stream() .filter(e -> finalLongSeqNo <= e.getValue()) .map(Map.Entry::getKey).min(Long::compareTo) .orElse(Long.MIN_VALUE); assertThat(translog.getMinGenerationForSeqNo(seqNo).translogFileGeneration, equalTo(operand)); } }	i'm wondering about this one. now it uses exactly the same logic as the code does - it searches for the generation whose max sequence number is below the desired one. should we not rely on our logic but rather hard test what we want - i.e., all operations above and including the seqno are present if you iterated on our generation->seqno from translog.getmingenerationforseqno(seqno).translogfilegeneration and upwards?
@Override public String toString() { return "TestResponse{" + "info='" + info + '\\\\'' + '}'; } } public void testSendRandomRequests() throws InterruptedException { TransportService serviceC = build( Settings.builder() .put("name", "TS_TEST") .put(TransportService.TRACE_LOG_INCLUDE_SETTING.getKey(), "") .put(TransportService.TRACE_LOG_EXCLUDE_SETTING.getKey(), "NOTHING") .build(), version0); DiscoveryNode nodeC = new DiscoveryNode("TS_C", "TS_C", serviceC.boundAddress().publishAddress(), emptyMap(), emptySet(), version0); serviceC.acceptIncomingRequests(); final CountDownLatch latch = new CountDownLatch(5); TransportConnectionListener waitForConnection = new TransportConnectionListener() { @Override public void onNodeConnected(DiscoveryNode node) { latch.countDown(); } @Override public void onNodeDisconnected(DiscoveryNode node) { fail("disconnect should not be called " + node); } }; serviceA.addConnectionListener(waitForConnection); serviceB.addConnectionListener(waitForConnection); serviceC.addConnectionListener(waitForConnection); serviceC.connectToNode(nodeA); serviceC.connectToNode(nodeB); serviceA.connectToNode(nodeC); serviceB.connectToNode(nodeC); serviceC.connectToNode(nodeC); latch.await(); serviceA.removeConnectionListener(waitForConnection); serviceB.removeConnectionListener(waitForConnection); serviceB.removeConnectionListener(waitForConnection); Map<TransportService, DiscoveryNode> toNodeMap = new HashMap<>(); toNodeMap.put(serviceA, nodeA); toNodeMap.put(serviceB, nodeB); toNodeMap.put(serviceC, nodeC); AtomicBoolean fail = new AtomicBoolean(false); class TRH implements TransportRequestHandler<TestRequest> { private final TransportService service; TRH(TransportService service) { this.service = service; } @Override public void messageReceived(TestRequest request, TransportChannel channel) throws Exception { if (randomBoolean()) { Thread.sleep(randomIntBetween(10, 50)); } if (fail.get()) { throw new IOException("forced failure"); } if (randomBoolean() && request.resendCount++ < 20) { DiscoveryNode node = randomFrom(nodeA, nodeB, nodeC); logger.debug("send secondary request from {} to {} - {}", toNodeMap.get(service), node, request.info); service.sendRequest(node, "action1", new TestRequest("secondary " + request.info), TransportRequestOptions.builder().withCompress(randomBoolean()).build(), new TransportResponseHandler<TestResponse>() { @Override public TestResponse newInstance() { return new TestResponse(); } @Override public void handleResponse(TestResponse response) { try { if (randomBoolean()) { Thread.sleep(randomIntBetween(10, 50)); } logger.debug("send secondary response {}", response.info); channel.sendResponse(response); } catch (Exception e) { throw new RuntimeException(e); } } @Override public void handleException(TransportException exp) { try { logger.debug("send secondary exception response for request {}", request.info); channel.sendResponse(exp); } catch (Exception e) { throw new RuntimeException(e); } } @Override public String executor() { return randomBoolean() ? ThreadPool.Names.SAME : ThreadPool.Names.GENERIC; } }); } else { logger.debug("send response for {}", request.info); channel.sendResponse(new TestResponse("Response for: " + request.info)); } } } serviceB.registerRequestHandler("action1", TestRequest::new, randomFrom(ThreadPool.Names.SAME, ThreadPool.Names.GENERIC), new TRH(serviceB)); serviceC.registerRequestHandler("action1", TestRequest::new, randomFrom(ThreadPool.Names.SAME, ThreadPool.Names.GENERIC), new TRH(serviceC)); serviceA.registerRequestHandler("action1", TestRequest::new, randomFrom(ThreadPool.Names.SAME, ThreadPool.Names.GENERIC), new TRH(serviceA)); int iters = randomIntBetween(30, 60); CountDownLatch allRequestsDone = new CountDownLatch(iters); class TRH2 implements TransportResponseHandler<TestResponse> { private final int id; public TRH2(int id) { this.id = id; } @Override public TestResponse newInstance() { return new TestResponse(); } @Override public void handleResponse(TestResponse response) { logger.debug("---> received response: {}", response.info); allRequestsDone.countDown(); } @Override public void handleException(TransportException exp) { logger.debug("---> received exception for id {}", exp, id); allRequestsDone.countDown(); Throwable unwrap = ExceptionsHelper.unwrap(exp, IOException.class); assertNotNull(unwrap); assertEquals(IOException.class, unwrap.getClass()); assertEquals("forced failure", unwrap.getMessage()); } @Override public String executor() { return randomBoolean() ? ThreadPool.Names.SAME : ThreadPool.Names.GENERIC; } } ; for (int i = 0; i < iters; i++) { TransportService service = randomFrom(serviceC, serviceB, serviceA); DiscoveryNode node = randomFrom(nodeC, nodeB, nodeA); logger.debug("send from {} to {}", toNodeMap.get(service), node); service.sendRequest(node, "action1", new TestRequest("REQ[" + i + "]"), TransportRequestOptions.builder().withCompress(randomBoolean()).build(), new TRH2(i)); } logger.debug("waiting for response"); fail.set(randomBoolean()); boolean await = allRequestsDone.await(5, TimeUnit.SECONDS); if (await == false) { logger.debug("now failing forcefully"); fail.set(true); assertTrue(allRequestsDone.await(5, TimeUnit.SECONDS)); }	a more meaningful name would be nice here too, for example testrequesthandler is okay.
public void testReturnSource() throws IOException { ScriptedMetricAggContexts.MapScript.Factory factory = scriptEngine.compile("test", "state._source = params._source", ScriptedMetricAggContexts.MapScript.CONTEXT, Collections.emptyMap()); Map<String, Object> params = new HashMap<>(); Map<String, Object> state = new HashMap<>(); MemoryIndex index = new MemoryIndex(); // we don't need a real index, just need to construct a LeafReaderContext which cannot be mocked LeafReaderContext leafReaderContext = index.createSearcher().getIndexReader().leaves().get(0); SearchLookup lookup = mock(SearchLookup.class); LeafSearchLookup leafLookup = mock(LeafSearchLookup.class); when(lookup.getLeafSearchLookup(leafReaderContext)).thenReturn(leafLookup); SourceLookup sourceLookup = mock(SourceLookup.class); when(leafLookup.asMap()).thenReturn(Collections.singletonMap("_source", sourceLookup)); when(sourceLookup.loadSourceIfNeeded()).thenReturn(Collections.singletonMap("test", 1)); ScriptedMetricAggContexts.MapScript.LeafFactory leafFactory = factory.newFactory(params, state, lookup); ScriptedMetricAggContexts.MapScript script = leafFactory.newInstance(leafReaderContext); script.execute(); assert(state.containsKey("_source")); assert(state.get("_source") instanceof Map && ((Map)state.get("_source")).containsKey("test")); assertEquals(1, ((Map)state.get("_source")).get("test")); }	i think assertthat with hamcrest matchers will give a nicer error message here if this fails.
static Request putScript(PutStoredScriptRequest putStoredScriptRequest) throws IOException { String endpoint = new EndpointBuilder().addPathPartAsIs("_scripts").addPathPart(putStoredScriptRequest.id()).build(); Request request = new Request(HttpPost.METHOD_NAME, endpoint); Params params = new Params(request); params.withTimeout(putStoredScriptRequest.timeout()); params.withMasterTimeout(putStoredScriptRequest.masterNodeTimeout()); if(Strings.hasText(putStoredScriptRequest.context())){ params.putParam("context", putStoredScriptRequest.context()); } request.setEntity(createEntity(putStoredScriptRequest, REQUEST_BODY_CONTENT_TYPE)); return request; }	minor - spaces between if and (strings, and space between ){ at the end of line
public void testPutScript() throws Exception { RestHighLevelClient client = highLevelClient(); { createIndex("index1", Settings.EMPTY); } { // tag::put-stored-script-request PutStoredScriptRequest request = new PutStoredScriptRequest(); request.id("id"); // <1> request.content(new BytesArray( "{\\\\n" + "\\\\"script\\\\": {\\\\n" + "\\\\"lang\\\\": \\\\"painless\\\\",\\\\n" + "\\\\"source\\\\": \\\\"Math.log(_score * 2) + params.multiplier\\\\"" + "}\\\\n" + "}\\\\n" ), XContentType.JSON); // <2> // end::put-stored-script-request // tag::put-stored-script-context request.context("context"); // <1> // end::put-stored-script-context // tag::put-stored-script-timeout request.timeout(TimeValue.timeValueMinutes(2)); // <1> request.timeout("2m"); // <2> // end::put-stored-script-timeout // tag::put-stored-script-masterTimeout request.masterNodeTimeout(TimeValue.timeValueMinutes(1)); // <1> request.masterNodeTimeout("1m"); // <2> // end::put-stored-script-masterTimeout } { PutStoredScriptRequest request = new PutStoredScriptRequest(); request.id("id"); // tag::put-stored-script-content-painless XContentBuilder builder = XContentFactory.jsonBuilder(); builder.startObject(); { builder.startObject("script"); { builder.field("lang", "painless"); builder.field("source", "Math.log(_score * 2) + params.multiplier"); } builder.endObject(); } builder.endObject(); request.content(BytesReference.bytes(builder), XContentType.JSON); // <1> // end::put-stored-script-content-painless // tag::put-stored-script-execute PutStoredScriptResponse putStoredScriptResponse = client.putScript(request, RequestOptions.DEFAULT); // end::put-stored-script-execute // tag::put-stored-script-response boolean acknowledged = putStoredScriptResponse.isAcknowledged(); // <1> // end::put-stored-script-response assertTrue(acknowledged); // tag::put-stored-script-execute-listener ActionListener<PutStoredScriptResponse> listener = new ActionListener<PutStoredScriptResponse>() { @Override public void onResponse(PutStoredScriptResponse response) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::put-stored-script-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::put-stored-script-execute-async client.putScriptAsync(request, RequestOptions.DEFAULT, listener); // <1> // end::put-stored-script-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } { PutStoredScriptRequest request = new PutStoredScriptRequest(); request.id("id"); // tag::put-stored-script-content-mustache XContentBuilder builder = XContentFactory.jsonBuilder(); builder.startObject(); { builder.startObject("script"); { builder.field("lang", "mustache"); builder.field("source", "{\\\\"query\\\\":{\\\\"match\\\\":{\\\\"title\\\\":\\\\"{{query_string}}\\\\"}}}"); } builder.endObject(); } builder.endObject(); request.content(BytesReference.bytes(builder), XContentType.JSON); // <1> // end::put-stored-script-content-mustache client.putScript(request, RequestOptions.DEFAULT); Map<String, Object> script = getAsMap("/_scripts/id"); assertThat(extractValue("script.lang", script), equalTo("mustache")); assertThat(extractValue("script.source", script), equalTo("{\\\\"query\\\\":{\\\\"match\\\\":{\\\\"title\\\\":\\\\"{{query_string}}\\\\"}}}")); } }	no need to create any index for stored scripts
public void testCompatibleFieldDeclarations() throws IOException { { // new_name is the only way to parse when compatibility is not set XContentParser parser = createParserWithCompatibilityFor(JsonXContent.jsonXContent, "{\\\\"new_name\\\\": 1}", RestApiCompatibleVersion.currentVersion()); StructWithCompatibleFields o = StructWithCompatibleFields.PARSER.parse(parser, null); assertEquals(1, o.intField); } { // old_name results with an exception when compatibility is not set XContentParser parser = createParserWithCompatibilityFor(JsonXContent.jsonXContent, "{\\\\"old_name\\\\": 1}", RestApiCompatibleVersion.currentVersion()); expectThrows(IllegalArgumentException.class, () -> StructWithCompatibleFields.PARSER.parse(parser, null)); } { // new_name is allowed to be parsed with compatibility XContentParser parser = createParserWithCompatibilityFor(JsonXContent.jsonXContent, "{\\\\"new_name\\\\": 1}", minimumSupported()); StructWithCompatibleFields o = StructWithCompatibleFields.PARSER.parse(parser, null); assertEquals(1, o.intField); } { // old_name is allowed to be parsed with compatibility, but results in deprecation XContentParser parser = createParserWithCompatibilityFor(JsonXContent.jsonXContent, "{\\\\"old_name\\\\": 1}", minimumSupported()); StructWithCompatibleFields o = StructWithCompatibleFields.PARSER.parse(parser, null); assertEquals(1, o.intField); assertWarnings(false, "[struct_with_compatible_fields][1:14] " + "Deprecated field [old_name] used, expected [new_name] instead"); } } // protected boolean enableWarningsCheck() { // return false; // } // // public void testFutureDeclaration() throws IOException { // // { // RestApiCompatibleVersion futureVersion = RestApiCompatibleVersion.V_9; //// Mockito.spy(RestApiCompatibleVersion.currentVersion()); //// Mockito.when(futureVersion.major).thenReturn((byte) (RestApiCompatibleVersion.currentVersion().major+1)); // // // new name is accessed in future versions // XContentParser parser = createParserWithCompatibilityFor(JsonXContent.jsonXContent, "{\\\\"new_name\\\\": 1}", // futureVersion); // StructWithCompatibleFields o = StructWithCompatibleFields.PARSER.parse(parser, null); // assertEquals(1, o.intField); //// assertWarnings(false, "[struct_with_compatible_fields][1:14] " + //// "Deprecated field [old_name] used, expected [new_name] instead"); // // // } // }	how do we test for future version? a class instead of enum would allow for spying, but would not look too great..
private Object unwrapMultiValue(Object values) { if (values == null) { return null; } if (dataType == DataType.GEO_POINT || dataType == DataType.GEO_SHAPE) { return extractGeoShape(values); } if (values instanceof List) { List<?> list = (List<?>) values; if (list.isEmpty()) { return null; } else { if (arrayLeniency || list.size() == 1) { return unwrapMultiValue(list.get(0)); } else { throw new SqlIllegalArgumentException("Arrays (returned by [{}]) are not supported", fieldName); } } } if (values instanceof Map) { throw new SqlIllegalArgumentException("Objects (returned by [{}]) are not supported", fieldName); } if (dataType == DataType.DATETIME) { if (values instanceof String) { return DateUtils.asDateTime(Long.parseLong(values.toString()), zoneId); } } if (values instanceof Long || values instanceof Double || values instanceof String || values instanceof Boolean) { return values; } throw new SqlIllegalArgumentException("Type {} (returned by [{}]) is not supported", values.getClass().getSimpleName(), fieldName); }	i think it's worth adding an isgeobased() method to datatype to perform this double check.
private Object extractGeoShape(Object values) { Object value; if (values instanceof List) { if (((List<?>) values).size() > 0) { value = ((List<?>) values).get(0); } else { return null; } } else { value = values; } if (dataType == DataType.GEO_SHAPE) { try { return new GeoShape(value); } catch (IOException ex) { throw new SqlIllegalArgumentException("Cannot read geo_shape value (returned by [{}])", fieldName); } } else { try { GeoPoint geoPoint = GeoUtils.parseGeoPoint(value, true); return new GeoShape(geoPoint.lon(), geoPoint.lat()); } catch (ElasticsearchParseException ex) { throw new SqlIllegalArgumentException("Cannot parse geo_point value (returned by [{}])", fieldName); } } } @SuppressWarnings({ "unchecked", "rawtypes" }	i don't think this is the correct approach here. if i'm not mistaken you are extracting here the first value from a field that has an array of geo shapes. sql has added recently a "multi field leniency" attribute to both rest and drivers support: https://www.elastic.co/guide/en/elasticsearch/reference/6.7/sql-rest.html#sql-rest-fields. code-wise, this leniency is handled [here](https://github.com/elastic/elasticsearch/blob/dd3e12275c70444e13461fe53da7c565e5f3db3e/x-pack/plugin/sql/src/main/java/org/elasticsearch/xpack/sql/execution/search/extractor/fieldhitextractor.java#l210) and [here](https://github.com/elastic/elasticsearch/blob/dd3e12275c70444e13461fe53da7c565e5f3db3e/x-pack/plugin/sql/src/main/java/org/elasticsearch/xpack/sql/execution/search/extractor/fieldhitextractor.java#l138) at extraction time. i think you need to include extractgeoshape() in the array leniency logic probably only in unwrapmultivalue method. also, would be good to see some tests with arrays of geo shapes and not only in one-level fields, but also multi nested fields (not the nested field type): field.subfield1.subfield2.my_geo_shapes_array.
public void testRewriteForUpdate() throws IOException { String pivotTransform = "{" + " \\\\"id\\\\" : \\\\"body_id\\\\"," + " \\\\"source\\\\" : {\\\\"index\\\\":\\\\"src\\\\"}," + " \\\\"dest\\\\" : {\\\\"index\\\\": \\\\"dest\\\\"}," + " \\\\"pivot\\\\" : {" + " \\\\"group_by\\\\": {" + " \\\\"id\\\\": {" + " \\\\"terms\\\\": {" + " \\\\"field\\\\": \\\\"id\\\\"" + "} } }," + " \\\\"aggs\\\\": {" + " \\\\"avg\\\\": {" + " \\\\"avg\\\\": {" + " \\\\"field\\\\": \\\\"points\\\\"" + "} } }," + " \\\\"max_page_search_size\\\\" : 111" + "}," + " \\\\"version\\\\" : \\\\"" + Version.V_7_6_0.toString() + "\\\\"" + "}"; TransformConfig transformConfig = createTransformConfigFromString(pivotTransform, "body_id", true); TransformConfig transformConfigRewritten = TransformConfig.rewriteForUpdate(transformConfig); assertNull(transformConfigRewritten.getPivotConfig().getMaxPageSearchSize()); assertNotNull(transformConfigRewritten.getSettings().getMaxPageSearchSize()); assertEquals(111L, transformConfigRewritten.getSettings().getMaxPageSearchSize().longValue()); assertTrue(transformConfigRewritten.getSettings().getDatesAsEpochMillis()); assertFalse(transformConfigRewritten.getSettings().getAlignCheckpoints()); assertWarnings(org.apache.logging.log4j.Level.WARN, TransformDeprecations.ACTION_MAX_PAGE_SEARCH_SIZE_IS_DEPRECATED); assertEquals(Version.CURRENT, transformConfigRewritten.getVersion()); }	any particular reason these are fully qualified?
@Override public void merge(Mapper mergeWith, MergeContext mergeContext) throws MergeMappingException { TTLFieldMapper ttlMergeWith = (TTLFieldMapper) mergeWith; if (ttlMergeWith.defaultTTL != -1) { this.defaultTTL = ttlMergeWith.defaultTTL; } if ((ttlMergeWith.enabledState != enabledState) &&!(ttlMergeWith.enabledState == Defaults.ENABLED_STATE)) { if (ttlMergeWith.enabledState != EnabledAttributeMapper.DISABLED) { this.enabledState = ttlMergeWith.enabledState; } else { mergeContext.addConflict("_ttl cannot be disabled once it was enabled."); } } }	i think you should not ignore it completely? the contract is that is simulate is true then conflicts are reported but changes are not performed.
@Override public void merge(Mapper mergeWith, MergeContext mergeContext) throws MergeMappingException { TTLFieldMapper ttlMergeWith = (TTLFieldMapper) mergeWith; if (ttlMergeWith.defaultTTL != -1) { this.defaultTTL = ttlMergeWith.defaultTTL; } if ((ttlMergeWith.enabledState != enabledState) &&!(ttlMergeWith.enabledState == Defaults.ENABLED_STATE)) { if (ttlMergeWith.enabledState != EnabledAttributeMapper.DISABLED) { this.enabledState = ttlMergeWith.enabledState; } else { mergeContext.addConflict("_ttl cannot be disabled once it was enabled."); } } }	i think this will report a conflict if _ttl was not explicitely enabled or disabled and then you try to disable it explicitely? which is not expected?
public void testTokenServiceCanRotateKeys() throws Exception { OAuth2Token response = createToken(TEST_USER_NAME, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING); String masterName = internalCluster().getMasterName(); TokenService masterTokenService = internalCluster().getInstance(TokenService.class, masterName); String activeKeyHash = masterTokenService.getActiveKeyHash(); for (TokenService tokenService : internalCluster().getInstances(TokenService.class)) { PlainActionFuture<UserToken> userTokenFuture = new PlainActionFuture<>(); tokenService.decodeToken(response.accessToken(), userTokenFuture); assertNotNull(userTokenFuture.actionGet()); assertEquals(activeKeyHash, tokenService.getActiveKeyHash()); } client().admin().cluster().prepareHealth().execute().get(); PlainActionFuture<AcknowledgedResponse> rotateActionFuture = new PlainActionFuture<>(); logger.info("rotate on master: {}", masterName); masterTokenService.rotateKeysOnMaster(rotateActionFuture); assertTrue(rotateActionFuture.actionGet().isAcknowledged()); assertNotEquals(activeKeyHash, masterTokenService.getActiveKeyHash()); for (TokenService tokenService : internalCluster().getInstances(TokenService.class)) { PlainActionFuture<UserToken> userTokenFuture = new PlainActionFuture<>(); tokenService.decodeToken(response.accessToken(), userTokenFuture); assertNotNull(userTokenFuture.actionGet()); assertNotEquals(activeKeyHash, tokenService.getActiveKeyHash()); } assertEquals(TEST_USER_NAME, response.principal()); }	nit: suggestion tokeninvalidation invalidateresponse = invalidateaccesstoken(accesstoken);
public void testCreatorRealmCaptureWillWorkWithClientRunAs() throws IOException { final String nativeTokenUsername = "native_token_user"; getSecurityClient().putUser(new User(nativeTokenUsername, "superuser"), SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING); // File realm user run-as a native realm user final Client runAsClient = client().filterWithHeader( Map.of( "Authorization", UsernamePasswordToken.basicAuthHeaderValue(ES_TEST_ROOT_USER, TEST_PASSWORD_SECURE_STRING.clone()), AuthenticationServiceField.RUN_AS_USER_HEADER, nativeTokenUsername ) ); // Create a token with client credentials and run-as, the token should be owned by the run-as user (native realm) var createTokenRequest1 = new org.elasticsearch.xpack.core.security.action.token.CreateTokenRequest(); createTokenRequest1.setGrantType("client_credentials"); final PlainActionFuture<CreateTokenResponse> future1 = new PlainActionFuture<>(); runAsClient.execute(CreateTokenAction.INSTANCE, createTokenRequest1, future1); final String accessToken = future1.actionGet().getTokenString(); // Token is usable final AuthenticateResponse authenticateResponse = client().filterWithHeader(Map.of("Authorization", "Bearer " + accessToken)) .execute(AuthenticateAction.INSTANCE, new AuthenticateRequest(nativeTokenUsername)) .actionGet(); assertThat(authenticateResponse.authentication().getUser().principal(), equalTo(nativeTokenUsername)); assertThat(authenticateResponse.authentication().getLookedUpBy().getName(), equalTo("index")); assertThat(authenticateResponse.authentication().getAuthenticatedBy().getName(), equalTo("file")); assertThat(authenticateResponse.authentication().getAuthenticationType(), is(Authentication.AuthenticationType.TOKEN)); // Invalidate tokens by realm and username respect the run-as user's username and realm var invalidateTokenRequest = new org.elasticsearch.xpack.core.security.action.token.InvalidateTokenRequest( null, null, "index", nativeTokenUsername ); final PlainActionFuture<org.elasticsearch.xpack.core.security.action.token.InvalidateTokenResponse> future2 = new PlainActionFuture<>(); client().execute(InvalidateTokenAction.INSTANCE, invalidateTokenRequest, future2); assertThat(future2.actionGet().getResult().getInvalidatedTokens().size(), equalTo(1)); // Create a token with password grant and run-as user (native realm) var createTokenRequest2 = new org.elasticsearch.xpack.core.security.action.token.CreateTokenRequest(); createTokenRequest2.setGrantType("password"); createTokenRequest2.setUsername(ES_TEST_ROOT_USER); createTokenRequest2.setPassword(TEST_PASSWORD_SECURE_STRING.clone()); final PlainActionFuture<CreateTokenResponse> future3 = new PlainActionFuture<>(); runAsClient.execute(CreateTokenAction.INSTANCE, createTokenRequest2, future3); final String refreshToken = future3.actionGet().getRefreshToken(); var createTokenRequest3 = new org.elasticsearch.xpack.core.security.action.token.CreateTokenRequest(); createTokenRequest3.setGrantType("refresh_token"); createTokenRequest3.setRefreshToken(refreshToken); final PlainActionFuture<CreateTokenResponse> future4 = new PlainActionFuture<>(); // Refresh token is bound to the original user that creates it. In this case, it is the run-as user // refresh without run-as should fail client().filterWithHeader( Map.of("Authorization", UsernamePasswordToken.basicAuthHeaderValue(ES_TEST_ROOT_USER, TEST_PASSWORD_SECURE_STRING.clone())) ).execute(RefreshTokenAction.INSTANCE, createTokenRequest3, future4); expectThrows(ElasticsearchSecurityException.class, future4::actionGet); // refresh with run-as should work final PlainActionFuture<CreateTokenResponse> future5 = new PlainActionFuture<>(); runAsClient.execute(RefreshTokenAction.INSTANCE, createTokenRequest3, future5); assertThat(future5.actionGet().getTokenString(), notNullValue()); }	this method should be rewritten to use the new convenient methods from testsecurityclient. i can follow up with a separate pr (since i wrote the original version).
private Environment getUpdatedEnvironment(Environment existingEnvironment){ if (inFipsSunJsseJvm()) { Settings additionalSettings = Settings.builder() .put(existingEnvironment.settings()) .put(XPackSettings.DIAGNOSE_TRUST_EXCEPTIONS_SETTING.getKey(), false) .build(); return new Environment(additionalSettings, existingEnvironment.configFile()); } return existingEnvironment; }	can't this be returnned from additionalsettings()?
private void executeCompletionListeners() { synchronized (this) { if (hasCompleted) { return; } hasCompleted = true; } // we don't need to restore the response headers, they should be included in the current // context since we are called by the search action listener. AsyncSearchResponse finalResponse = getResponse(false); for (Consumer<AsyncSearchResponse> listener : completionListeners.values()) { listener.accept(finalResponse); } completionListeners.clear(); } /** * Returns the current {@link AsyncSearchResponse}	i am not a fan of boolean flags, this method looks simple enough that we could split in two: getresponse and getresponsewithheaders or something along those lines?
public void testTestRunStartedSupportsClassInDefaultPackage() throws Exception { LoggingListener loggingListener = new LoggingListener(); Description description = Description.createTestDescription(Class.forName("Dummy"), "dummy"); loggingListener.testRunStarted(description); }	thanks for updating. would you mind pushing a comment explaining that this would throw an exception without your fix?
public RestResponse buildResponse(GetFieldMappingsResponse response, XContentBuilder builder) throws Exception { ImmutableMap<String, ImmutableMap<String, ImmutableMap<String, FieldMappingMetaData>>> mappingsByIndex = response.mappings(); boolean isPossibleSingleFieldRequest = indices.length == 1 && types.length == 1 && fields.length == 1; if (isPossibleSingleFieldRequest && isFieldMappingMissingField(mappingsByIndex)) { return new BytesRestResponse(OK, builder.startObject().endObject()); } RestStatus status = OK; if (mappingsByIndex.isEmpty() && fields.length > 0) { status = NOT_FOUND; } builder.startObject(); Map<String, String> param = Maps.newHashMap(); response.toXContent(builder, new ToXContent.DelegatingMapParams(param, request)); builder.endObject(); return new BytesRestResponse(status, builder); }	this can be simplified to response.toxcontent(builder,request);
private InputStream openBlobStream(int part, long pos, long length) throws IOException { final InputStream stream; if (fileInfo.metadata().hashEqualsContents() == false) { stream = blobContainer.readBlob(fileInfo.partName(part), pos, length); } else { // extract blob content from metadata hash final BytesRef data = fileInfo.metadata().hash(); if (part > 0) { assert fileInfo.numberOfParts() >= part; for (int i = 0; i < part; i++) { pos += fileInfo.partBytes(i); } } if ((pos < 0L) || (length < 0L) || (pos + length > data.bytes.length)) { throw new IllegalArgumentException("Invalid arguments (pos=" + pos + ", length=" + length + ") for hash content"); } stream = new ByteArrayInputStream(data.bytes, Math.toIntExact(pos), Math.toIntExact(length)); } return stream; }	let's include data.bytes.length in the message too, just in case.
@Override protected void doMasterOperation(ExplainLifecycleRequest request, String[] concreteIndices, ClusterState state, ActionListener<ExplainLifecycleResponse> listener) { Map<String, IndexLifecycleExplainResponse> indexReponses = new HashMap<>(); for (String index : concreteIndices) { IndexMetaData idxMetadata = state.metaData().index(index); Settings idxSettings = idxMetadata.getSettings(); LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(idxMetadata); String policyName = LifecycleSettings.LIFECYCLE_NAME_SETTING.get(idxSettings); String currentPhase = lifecycleState.getPhase(); String stepInfo = lifecycleState.getStepInfo(); BytesArray stepInfoBytes = null; if (stepInfo != null) { stepInfoBytes = new BytesArray(stepInfo); } // parse existing phase steps from the phase definition in the index settings String phaseDef = lifecycleState.getPhaseDefinition(); PhaseExecutionInfo phaseExecutionInfo = null; if (Strings.isNullOrEmpty(phaseDef) == false) { try (XContentParser parser = JsonXContent.jsonXContent.createParser(xContentRegistry, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, phaseDef)) { phaseExecutionInfo = PhaseExecutionInfo.parse(parser, currentPhase); } catch (IOException e) { listener.onFailure(new ElasticsearchParseException( "failed to parse phase definition for index [" + index + "]", e)); return; } } final IndexLifecycleExplainResponse indexResponse; if (Strings.hasLength(policyName) && Strings.hasLength(currentPhase)) { indexResponse = IndexLifecycleExplainResponse.newManagedIndexResponse(index, policyName, lifecycleState.getLifecycleDate(), lifecycleState.getPhase(), lifecycleState.getAction(), lifecycleState.getStep(), lifecycleState.getFailedStep(), lifecycleState.getPhaseTime(), lifecycleState.getActionTime(), lifecycleState.getStepTime(), stepInfoBytes, phaseExecutionInfo); } else { indexResponse = IndexLifecycleExplainResponse.newUnmanagedIndexResponse(index); } indexReponses.put(indexResponse.getIndex(), indexResponse); } listener.onResponse(new ExplainLifecycleResponse(indexReponses)); }	this was caught because there is a time window where the index has a policy name, but is not really managed by ilm just yet since it doesn't have its step info set. i thought it would be sufficient to keep this change small and add this extra check to verify that the lifecycle state exists. one way to check that it exists is to verify that there is at least one field set, the current phase.
static DeprecationIssue chainedMultiFieldsCheck(IndexMetadata indexMetadata) { List<String> issues = new ArrayList<>(); fieldLevelMappingIssue( indexMetadata, ((mappingMetadata, sourceAsMap) -> issues.addAll( findInPropertiesRecursively( mappingMetadata.type(), sourceAsMap, IndexDeprecationChecks::containsChainedMultiFields, IndexDeprecationChecks::formatField, "", "" ) )) ); if (issues.size() > 0) { return new DeprecationIssue( DeprecationIssue.Level.WARNING, "Defining multi-fields within multi-fields is deprecated", "https://ela.st/es-deprecation-7-chained-multi-fields", String.format( Locale.ROOT, "Remove chained multi-fields from the \\\\"%s\\\\" mapping%s. Multi-fields within multi-fields " + "are not supported in 8.0.", issues.stream().collect(Collectors.joining(",")), issues.size() > 1 ? "s" : "" ), false, null ); } return null; }	i don't see any test coverage of this method. was that intentional (there's definitely a lot of code reuse with the clusterdeprecationchecks methods)?
private static ChunkingConfig defaultChunkingConfig(@Nullable AggProvider aggProvider) { if (aggProvider == null || aggProvider.getParsedAggs() == null) { return ChunkingConfig.newAuto(); } else { AggregationBuilder histogram = ExtractorUtils.getHistogramAggregation(aggProvider.getParsedAggs().getAggregatorFactories()); if (histogram instanceof CompositeAggregationBuilder) { return ChunkingConfig.newOff(); } long histogramIntervalMillis = ExtractorUtils.getHistogramIntervalMillis(histogram); if (histogramIntervalMillis <= 0) { throw ExceptionsHelper.badRequestException(DATAFEED_AGGREGATIONS_INTERVAL_MUST_BE_GREATER_THAN_ZERO); } return ChunkingConfig.newManual(TimeValue.timeValueMillis(DEFAULT_AGGREGATION_CHUNKING_BUCKETS * histogramIntervalMillis)); } }	ok, after doing some experimentation, on an index with 14479391 docs, and grouping by 15m intervals + on a term field with ~851502, chunking provides a much better experience. i am thinking there should be an "auto", similar to what we do with scroll that takes into account the number of group_by fields, the date_histogram interval, min max time, and possibly the cardinality of the terms and maybe the composite agg paging size.
@Override protected void masterOperation(Task task, StartDatafeedAction.Request request, ClusterState state, ActionListener<NodeAcknowledgedResponse> listener) { StartDatafeedAction.DatafeedParams params = request.getParams(); if (licenseState.checkFeature(XPackLicenseState.Feature.MACHINE_LEARNING) == false) { listener.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)); return; } if (migrationEligibilityCheck.datafeedIsEligibleForMigration(request.getParams().getDatafeedId(), state)) { listener.onFailure(ExceptionsHelper.configHasNotBeenMigrated("start datafeed", request.getParams().getDatafeedId())); return; } AtomicReference<DatafeedConfig> datafeedConfigHolder = new AtomicReference<>(); PersistentTasksCustomMetadata tasks = state.getMetadata().custom(PersistentTasksCustomMetadata.TYPE); ActionListener<PersistentTasksCustomMetadata.PersistentTask<StartDatafeedAction.DatafeedParams>> waitForTaskListener = new ActionListener<PersistentTasksCustomMetadata.PersistentTask<StartDatafeedAction.DatafeedParams>>() { @Override public void onResponse(PersistentTasksCustomMetadata.PersistentTask<StartDatafeedAction.DatafeedParams> persistentTask) { waitForDatafeedStarted(persistentTask.getId(), params, listener); } @Override public void onFailure(Exception e) { if (ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) { logger.debug("datafeed already started", e); e = new ElasticsearchStatusException("cannot start datafeed [" + params.getDatafeedId() + "] because it has already been started", RestStatus.CONFLICT); } listener.onFailure(e); } }; // Verify data extractor factory can be created, then start persistent task Consumer<Job> createDataExtractor = job -> { final List<String> remoteIndices = RemoteClusterLicenseChecker.remoteIndices(params.getDatafeedIndices()); if (remoteIndices.isEmpty() == false) { final RemoteClusterLicenseChecker remoteClusterLicenseChecker = new RemoteClusterLicenseChecker(client, XPackLicenseState::isMachineLearningAllowedForOperationMode); remoteClusterLicenseChecker.checkRemoteClusterLicenses( RemoteClusterLicenseChecker.remoteClusterAliases( transportService.getRemoteClusterService().getRegisteredRemoteClusterNames(), params.getDatafeedIndices()), ActionListener.wrap( response -> { if (response.isSuccess() == false) { listener.onFailure(createUnlicensedError(params.getDatafeedId(), response)); } else if (remoteClusterClient == false) { listener.onFailure( ExceptionsHelper.badRequestException(Messages.getMessage( Messages.DATAFEED_NEEDS_REMOTE_CLUSTER_SEARCH, datafeedConfigHolder.get().getId(), RemoteClusterLicenseChecker.remoteIndices(datafeedConfigHolder.get().getIndices()), clusterService.getNodeName()))); } else { final RemoteClusterService remoteClusterService = transportService.getRemoteClusterService(); List<String> remoteAliases = RemoteClusterLicenseChecker.remoteClusterAliases( remoteClusterService.getRegisteredRemoteClusterNames(), remoteIndices ); checkRemoteClusterVersions( datafeedConfigHolder.get(), remoteAliases, (cn) -> remoteClusterService.getConnection(cn).getVersion() ); createDataExtractor(job, datafeedConfigHolder.get(), params, waitForTaskListener); } }, e -> listener.onFailure( createUnknownLicenseError( params.getDatafeedId(), RemoteClusterLicenseChecker.remoteIndices(params.getDatafeedIndices()), e)) ) ); } else { createDataExtractor(job, datafeedConfigHolder.get(), params, waitForTaskListener); } }; ActionListener<Job.Builder> jobListener = ActionListener.wrap( jobBuilder -> { try { Job job = jobBuilder.build(); validate(job, datafeedConfigHolder.get(), tasks, xContentRegistry); auditDeprecations(datafeedConfigHolder.get(), job, auditor, xContentRegistry); createDataExtractor.accept(job); } catch (Exception e) { listener.onFailure(e); } }, listener::onFailure ); ActionListener<DatafeedConfig.Builder> datafeedListener = ActionListener.wrap( datafeedBuilder -> { try { DatafeedConfig datafeedConfig = datafeedBuilder.build(); params.setDatafeedIndices(datafeedConfig.getIndices()); params.setJobId(datafeedConfig.getJobId()); params.setIndicesOptions(datafeedConfig.getIndicesOptions()); datafeedConfigHolder.set(datafeedConfig); if (datafeedConfig.hasCompositeAgg(xContentRegistry)) { if (state.nodes() .mastersFirstStream() .filter(MachineLearning::isMlNode) .map(DiscoveryNode::getVersion) .anyMatch(COMPOSITE_AGG_SUPPORT::after)) { listener.onFailure(ExceptionsHelper.badRequestException( "cannot start datafeed [{}] as [{}] requires all machine learning nodes to be at least version [{}]", datafeedConfig.getId(), "composite aggs", COMPOSITE_AGG_SUPPORT )); return; } } jobConfigProvider.getJob(datafeedConfig.getJobId(), jobListener); } catch (Exception e) { listener.onFailure(e); } }, listener::onFailure ); datafeedConfigProvider.getDatafeedConfig(params.getDatafeedId(), datafeedListener); }	should this be composite_agg_support::before?
static void create(Client client, DatafeedConfig datafeed, Job job, NamedXContentRegistry xContentRegistry, DatafeedTimingStatsReporter timingStatsReporter, ActionListener<DataExtractorFactory> listener) { boolean hasAggs = datafeed.hasAggregations(); boolean isComposite = hasAggs && datafeed.hasCompositeAgg(xContentRegistry); ActionListener<DataExtractorFactory> factoryHandler = ActionListener.wrap( factory -> listener.onResponse(datafeed.getChunkingConfig().isEnabled() ? new ChunkedDataExtractorFactory(client, datafeed, job, xContentRegistry, factory, timingStatsReporter) : factory) , listener::onFailure ); ActionListener<GetRollupIndexCapsAction.Response> getRollupIndexCapsActionHandler = ActionListener.wrap( response -> { if (response.getJobs().isEmpty() == false && datafeed.hasAggregations() == false) { listener.onFailure(new IllegalArgumentException("Aggregations are required when using Rollup indices")); return; } if (datafeed.hasAggregations() == false) { ScrollDataExtractorFactory.create(client, datafeed, job, xContentRegistry, timingStatsReporter, factoryHandler); return; } if (isComposite) { String[] indices = datafeed.getIndices().toArray(new String[0]); IndicesOptions indicesOptions = datafeed.getIndicesOptions(); AggregatedSearchRequestBuilder aggregatedSearchRequestBuilder = response.getJobs().isEmpty() ? AggregationDataExtractorFactory.requestBuilder(client, indices, indicesOptions) : RollupDataExtractorFactory.requestBuilder(client, indices, indicesOptions); factoryHandler.onResponse( new CompositeAggregationDataExtractorFactory( client, datafeed, job, xContentRegistry, timingStatsReporter, aggregatedSearchRequestBuilder ) ); return; } if (response.getJobs().isEmpty()) { // This means no rollup indexes are in the config factoryHandler.onResponse( new AggregationDataExtractorFactory(client, datafeed, job, xContentRegistry, timingStatsReporter)); } else { RollupDataExtractorFactory.create( client, datafeed, job, response.getJobs(), xContentRegistry, timingStatsReporter, factoryHandler); } }, e -> { Throwable cause = ExceptionsHelper.unwrapCause(e); if (cause instanceof IndexNotFoundException) { listener.onFailure(new ResourceNotFoundException("datafeed [" + datafeed.getId() + "] cannot retrieve data because index " + ((IndexNotFoundException) cause).getIndex() + " does not exist")); } else { listener.onFailure(e); } } ); if (RemoteClusterLicenseChecker.containsRemoteIndex(datafeed.getIndices())) { // If we have remote indices in the data feed, don't bother checking for rollup support // Rollups + CCS is not supported getRollupIndexCapsActionHandler.onResponse(new GetRollupIndexCapsAction.Response()); } else { ClientHelper.executeAsyncWithOrigin( client, ClientHelper.ML_ORIGIN, GetRollupIndexCapsAction.INSTANCE, new GetRollupIndexCapsAction.Request(datafeed.getIndices().toArray(new String[0]), datafeed.getIndicesOptions()), getRollupIndexCapsActionHandler); } }	could also extract a isrollup boolean. also, reuse local hasaggs
* @see org.elasticsearch.index.translog.Translog.Create * @see org.elasticsearch.index.translog.Translog.Index * @see org.elasticsearch.index.translog.Translog.Delete */ public Location add(Operation operation) throws TranslogException { ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(bigArrays); try { final BufferedChecksumStreamOutput checksumStreamOutput = new BufferedChecksumStreamOutput(out); final long start = out.position(); out.skip(RamUsageEstimator.NUM_BYTES_INT); writeOperationNoSize(checksumStreamOutput, operation); long end = out.position(); int operationSize = (int) (out.position() - RamUsageEstimator.NUM_BYTES_INT - start); out.seek(start); out.writeInt(operationSize); out.seek(end); ReleasablePagedBytesReference bytes = out.bytes(); try (ReleasableLock lock = readLock.acquire()) { Location location = current.add(bytes); if (config.isSyncOnEachOperation()) { current.sync(); } assert current.assertBytesAtLocation(location, bytes); return location; } } catch (Throwable e) { throw new TranslogException(shardId, "Failed to write operation [" + operation + "]", e); } finally { Releasables.close(out.bytes()); } }	you can use end instead of out.position() here.
* @throws IOException if shard state could not be persisted */ public void updateRoutingEntry(final ShardRouting newRouting) throws IOException { final ShardRouting currentRouting; synchronized (mutex) { currentRouting = this.shardRouting; if (!newRouting.shardId().equals(shardId())) { throw new IllegalArgumentException("Trying to set a routing entry with shardId " + newRouting.shardId() + " on a shard with shardId " + shardId() + ""); } if ((currentRouting == null || newRouting.isSameAllocation(currentRouting)) == false) { throw new IllegalArgumentException("Trying to set a routing entry with a different allocation. Current " + currentRouting + ", new " + newRouting); } if (currentRouting != null && currentRouting.primary() && newRouting.primary() == false) { throw new IllegalArgumentException("illegal state: trying to move shard from primary mode to replica mode. Current " + currentRouting + ", new " + newRouting); } if (state == IndexShardState.POST_RECOVERY && newRouting.active()) { assert currentRouting.active() == false : "we are in POST_RECOVERY, but our shard routing is active " + currentRouting; // we want to refresh *before* we move to internal STARTED state try { getEngine().refresh("cluster_state_started"); } catch (Exception e) { logger.debug("failed to refresh due to move to cluster wide started", e); } changeState(IndexShardState.STARTED, "global state is [" + newRouting.state() + "]"); } else if (state == IndexShardState.RELOCATED && (newRouting.relocating() == false || newRouting.equalsIgnoringMetaData(currentRouting) == false)) { // if the shard is marked as RELOCATED we have to fail when any changes in shard routing occur (e.g. due to recovery // failure / cancellation). The reason is that at the moment we cannot safely move back to STARTED without risking two // active primaries. throw new IndexShardRelocatedException(shardId(), "Shard is marked as relocated, cannot safely move to state " + newRouting.state()); } this.shardRouting = newRouting; persistMetadata(newRouting, currentRouting); } if (currentRouting != null && currentRouting.active() == false && newRouting.active()) { indexEventListener.afterIndexShardStarted(this); } if (newRouting.equals(currentRouting) == false) { indexEventListener.shardRoutingChanged(this, currentRouting, newRouting); } }	should we also synchronize the notification on the indexeventlistener maybe to ensure they are not called our of order?
static IndexResolution mergedMappings(String indexPattern, String[] indexNames, Map<String, Map<String, FieldCapabilities>> fieldCaps) { if (fieldCaps == null || fieldCaps.isEmpty()) { return IndexResolution.notFound(indexPattern); } // merge all indices onto the same one List<EsIndex> indices = buildIndices(indexNames, null, fieldCaps, i -> indexPattern, (n, types) -> { StringBuilder errorMessage = new StringBuilder(); boolean hasUnmapped = types.containsKey(UNMAPPED); if (types.size() > (hasUnmapped ? 2 : 1)) { // build the error message // and create a MultiTypeField for (Entry<String, FieldCapabilities> type : types.entrySet()) { // skip unmapped if (UNMAPPED.equals(type.getKey())) { continue; } if (errorMessage.length() > 0) { errorMessage.append(", "); } errorMessage.append("["); errorMessage.append(type.getKey()); errorMessage.append("] in "); errorMessage.append(Arrays.toString(type.getValue().indices())); } errorMessage.insert(0, "mapped as [" + (types.size() - (hasUnmapped ? 1 : 0)) + "] incompatible types: "); return new InvalidMappedField(n, errorMessage.toString()); } // type is okay, check aggregation else { FieldCapabilities fieldCap = types.values().iterator().next(); // validate search/agg-able if (fieldCap.isAggregatable() && fieldCap.nonAggregatableIndices() != null) { errorMessage.append("mapped as aggregatable except in "); errorMessage.append(Arrays.toString(fieldCap.nonAggregatableIndices())); } if (fieldCap.isSearchable() && fieldCap.nonSearchableIndices() != null) { if (errorMessage.length() > 0) { errorMessage.append(","); } errorMessage.append("mapped as searchable except in "); errorMessage.append(Arrays.toString(fieldCap.nonSearchableIndices())); } if (errorMessage.length() > 0) { return new InvalidMappedField(n, errorMessage.toString()); } } // everything checks return null; }); if (indices.size() != 1) { throw new SqlIllegalArgumentException("Incorrect merging of mappings (likely due to a bug) - expect 1 but found [{}]", indices.size()); } return IndexResolution.valid(indices.get(0)); }	you've already spotted it, but just a reminder to revert the formatting changes.
public static SearchRequest prepareRequest(Client client, SearchSourceBuilder source, TimeValue timeout, boolean includeFrozen, String... indices) { SearchRequest search = client.prepareSearch(indices) .setAllowPartialSearchResults(false) .setSource(source) .setTimeout(timeout) .setIndicesOptions( includeFrozen ? IndexResolver.FIELD_CAPS_FROZEN_INDICES_OPTIONS : IndexResolver.FIELD_CAPS_INDICES_OPTIONS) .request(); return search; }	why removing this one?
@Override public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null || getClass() != obj.getClass()) { return false; } QueryContainer other = (QueryContainer) obj; return Objects.equals(query, other.query) && Objects.equals(aggs, other.aggs) && Objects.equals(fields, other.fields) && Objects.equals(aliases, other.aliases) && Objects.equals(sort, other.sort) && Objects.equals(limit, other.limit) && Objects.equals(trackHits, other.trackHits) && Objects.equals(includeFrozen, other.includeFrozen); }	each equals on its own line, to have a consistent approach?
public void executed(IngestDocument ingestDocument) { if (ingestDocument.isModified()) { indexRequest.source(ingestDocument.getSource()); // uboness: don't we also need to update the metadata (index, id, etc...)? } indexRequest.putHeader(IngestPlugin.PIPELINE_ALREADY_PROCESSED, true); chain.proceed(action, indexRequest, listener); }	good point, we will once we have the processor that updates them, which is being worked on.
* @param reader the reader to invalidate the cache entry for * @param cacheKey the cache key to invalidate */ void invalidate(CacheEntity cacheEntity, DirectoryReader reader, BytesReference cacheKey) { if (reader.getReaderCacheHelper() != null) { cache.invalidate(new Key(cacheEntity, reader.getReaderCacheHelper().getKey(), cacheKey)); } }	why is it possible to have a null cache helper here ?
private void checkFieldNameLengthLimit(long limit) { Stream.concat(objectMappers.values().stream(), fieldMappers.values().stream()).forEach(mapper -> { String name = mapper.simpleName(); if (name.length() > limit) { throw new IllegalArgumentException("Field name [" + name + "] is longer than the limit of [" + limit + "] characters"); } }); }	could we just solve this with a normal loop (or two)? the stream stuff is just needless overhead here i think.
* @throws IOException in case there is a problem sending the request or parsing back the response */ public GetRollupCapsResponse getRollupCapabilities(GetRollupCapsRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequestAndParseEntity(request, RollupRequestConverters::getRollupCaps, options, GetRollupCapsResponse::fromXContent, Collections.emptySet()); } /** * Asynchronously Get the Rollup Capabilities of a target (non-rollup) index or pattern * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rollup-put-job.html"> * the docs</a> for more. * @param request the request * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	any reason these two methods have drastically different indentation? one of them is a newline every param and the async below is just 2 lines.
static Request putJob(final PutRollupJobRequest putRollupJobRequest) throws IOException { String endpoint = new RequestConverters.EndpointBuilder() .addPathPartAsIs("_xpack") .addPathPartAsIs("rollup") .addPathPartAsIs("job") .addPathPart(putRollupJobRequest.getConfig().getId()) .build(); Request request = new Request(HttpPut.METHOD_NAME, endpoint); request.setEntity(createEntity(putRollupJobRequest, REQUEST_BODY_CONTENT_TYPE)); return request; }	getrollupcapsrequest can be final
public void testGetRollupCaps() throws Exception { RestHighLevelClient client = highLevelClient(); DateHistogramGroupConfig dateHistogram = new DateHistogramGroupConfig("timestamp", DateHistogramInterval.HOUR, new DateHistogramInterval("7d"), "UTC"); // <1> TermsGroupConfig terms = new TermsGroupConfig("hostname", "datacenter"); HistogramGroupConfig histogram = new HistogramGroupConfig(5L, "load", "net_in", "net_out"); GroupConfig groups = new GroupConfig(dateHistogram, histogram, terms); List<MetricConfig> metrics = new ArrayList<>(); // <1> metrics.add(new MetricConfig("temperature", Arrays.asList("min", "max", "sum"))); metrics.add(new MetricConfig("voltage", Arrays.asList("avg", "value_count"))); //tag::x-pack-rollup-get-rollup-caps-setup final String indexPattern = "docs"; final String rollupIndexName = "rollup"; final String cron = "*/1 * * * * ?"; final int pageSize = 100; final TimeValue timeout = null; String id = "job_1"; RollupJobConfig config = new RollupJobConfig(id, indexPattern, rollupIndexName, cron, pageSize, groups, metrics, timeout); PutRollupJobRequest request = new PutRollupJobRequest(config); PutRollupJobResponse response = client.rollup().putRollupJob(request, RequestOptions.DEFAULT); boolean acknowledged = response.isAcknowledged(); //end::x-pack-rollup-get-rollup-caps-setup assertTrue(acknowledged); ClusterHealthRequest healthRequest = new ClusterHealthRequest(config.getRollupIndex()).waitForYellowStatus(); ClusterHealthResponse healthResponse = client.cluster().health(healthRequest, RequestOptions.DEFAULT); assertFalse(healthResponse.isTimedOut()); assertThat(healthResponse.getStatus(), isOneOf(ClusterHealthStatus.YELLOW, ClusterHealthStatus.GREEN)); // Now that the job is created, we should have a rollup index with metadata. // We can test out the caps API now. //tag::x-pack-rollup-get-rollup-caps-request GetRollupCapsRequest getRollupCapsRequest = new GetRollupCapsRequest("docs"); //end::x-pack-rollup-get-rollup-caps-request //tag::x-pack-rollup-get-rollup-caps-execute GetRollupCapsResponse capsResponse = client.rollup().getRollupCapabilities(getRollupCapsRequest, RequestOptions.DEFAULT); //end::x-pack-rollup-get-rollup-caps-execute //tag::x-pack-rollup-get-rollup-caps-response Map<String, RollableIndexCaps> rolledPatterns = capsResponse.getJobs(); RollableIndexCaps docsPattern = rolledPatterns.get("docs"); // indexName will be "docs" in this case... the index pattern that we rolled up String indexName = docsPattern.getIndexName(); // Each index pattern can have multiple jobs that rolled it up, so `getJobCaps()` // returns a list of jobs that rolled up the pattern List<RollupJobCaps> rollupJobs = docsPattern.getJobCaps(); RollupJobCaps jobCaps = rollupJobs.get(0); // jobID is the identifier we used when we created the job (e.g. `job1`) String jobID = jobCaps.getJobID(); // rollupIndex is the location that the job stored it's rollup docs (e.g. `rollup`) String rollupIndex = jobCaps.getRollupIndex(); // indexPattern is the same as the indexName that we retrieved earlier, redundant info assert jobCaps.getIndexPattern().equals(indexName); // Finally, fieldCaps are the capabilities of individual fields in the config // The key is the field name, and the value is a RollupFieldCaps object which // provides more info. Map<String, RollupJobCaps.RollupFieldCaps> fieldCaps = jobCaps.getFieldCaps(); // If we retrieve the "timestamp" field, it returns a list of maps. Each list // item represents a different aggregation that can be run against the "timestamp" // field, and any additional details specific to that agg (interval, etc) List<Map<String, Object>> timestampCaps = fieldCaps.get("timestamp").getAggs(); assert timestampCaps.get(0).toString().equals("{agg=date_histogram, delay=7d, interval=1h, time_zone=UTC}"); // In contrast to the timestamp field, the temperature field has multiple aggs configured List<Map<String, Object>> temperatureCaps = fieldCaps.get("temperature").getAggs(); assert temperatureCaps.toString().equals("[{agg=min}, {agg=max}, {agg=sum}]"); //end::x-pack-rollup-get-rollup-caps-response assertThat(indexName, equalTo("docs")); assertThat(jobID, equalTo("job_1")); assertThat(rollupIndex, equalTo("rollup")); assertThat(fieldCaps.size(), equalTo(8)); // tag::x-pack-rollup-get-rollup-caps-execute-listener ActionListener<GetRollupCapsResponse> listener = new ActionListener<GetRollupCapsResponse>() { @Override public void onResponse(GetRollupCapsResponse response) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::x-pack-rollup-get-rollup-caps-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::x-pack-rollup-get-rollup-caps-execute-async client.rollup().getRollupCapabilitiesAsync(getRollupCapsRequest, RequestOptions.DEFAULT, listener); // <1> // end::x-pack-rollup-get-rollup-caps-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); }	nit: missing new line
public PyTorchResult waitForResult( NativePyTorchProcess process, String requestId, PendingResult pendingResult, TimeValue timeout ) throws InterruptedException { if (process == null || process.isProcessAliveAfterWaiting() == false) { PyTorchResult storedResult = pendingResult.result.get(); return storedResult == null ? new PyTorchResult(requestId, null, null, "native process no longer started") : storedResult; } if (pendingResult.latch.await(timeout.millis(), TimeUnit.MILLISECONDS)) { return pendingResult.result.get(); } return null; }	it would be nice to avoid the call to isprocessaliveafterwaiting() as it contains a 45ms wait. it be sufficient to know that while loop in process() is still running by setting a boolean when the loop exits. that boolean would be tested here and if the while has finished we know there is no point waiting for a result that will never come.
@Override public void refresh() { SocketAccess.doPrivilegedVoid(credentialsProvider::refresh); } } /** * Customizes {@link com.amazonaws.auth.WebIdentityTokenCredentialsProvider} * * <ul> * <li>Reads the the location of the web identity token not from AWS_WEB_IDENTITY_TOKEN_FILE, but from a symlink * in the plugin directory, so we don't need to create a hardcoded read file permission for the plugin.</li> * <li>Supports customization of the STS endpoint via a system property, so we can test it against a test fixture.</li> * <li>Supports gracefully shutting down the provider and the STS client.</li> * </ul> */ static class CustomWebIdentityTokenCredentialsProvider implements AWSCredentialsProvider { private STSAssumeRoleWithWebIdentitySessionCredentialsProvider credentialsProvider; private AWSSecurityTokenService stsClient; CustomWebIdentityTokenCredentialsProvider(Environment environment) { // Check whether the original environment variable exists. If it doesn't, // the system doesn't support AWS web identity tokens if (System.getenv(AWS_WEB_IDENTITY_ENV_VAR) == null) { return; } // Make sure that a readable symlink to the token file exists in the plugin config directory Path webIdentityTokenFileSymlink = environment.configFile().resolve("repository-s3/aws-web-identity-token-file"); if (Files.exists(webIdentityTokenFileSymlink) == false) { throw new IllegalStateException( "A Web Identity Token symlink in the config directory doesn't exist: " + webIdentityTokenFileSymlink ); } if (Files.isReadable(webIdentityTokenFileSymlink) == false) { throw new IllegalStateException( "Unable to read a Web Identity Token symlink in the config directory: " + webIdentityTokenFileSymlink ); } String roleArn = System.getenv(AWS_ROLE_ARN_ENV_VAR); String roleSessionName = System.getenv(AWS_ROLE_SESSION_NAME_ENV_VAR); if (roleArn == null || roleSessionName == null) { LOGGER.warn( "Unable to use a web identity token for authentication. The AWS_WEB_IDENTITY_TOKEN_FILE environment " + "variable is set, but either AWS_ROLE_ARN or AWS_ROLE_SESSION_NAME are missing" ); return; } AWSSecurityTokenServiceClientBuilder stsClientBuilder = AWSSecurityTokenServiceClient.builder(); // Just for testing String customStsEndpoint = System.getProperty("com.amazonaws.sdk.stsMetadataServiceEndpointOverride"); if (customStsEndpoint != null) { stsClientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(customStsEndpoint, null)); } stsClientBuilder.withCredentials(new AWSStaticCredentialsProvider(new AnonymousAWSCredentials())); stsClient = SocketAccess.doPrivileged(stsClientBuilder::build); try { credentialsProvider = new STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder( roleArn, roleSessionName, webIdentityTokenFileSymlink.toString() ).withStsClient(stsClient).build(); } catch (Exception e) { stsClient.shutdown(); throw e; } } boolean isActive() { return credentialsProvider != null; } @Override public AWSCredentials getCredentials() { Objects.requireNonNull(credentialsProvider, "credentialsProvider is not set"); return credentialsProvider.getCredentials(); } @Override public void refresh() { if (credentialsProvider != null) { credentialsProvider.refresh(); } } public void shutdown() throws IOException { if (credentialsProvider != null) { IOUtils.close(credentialsProvider, () -> stsClient.shutdown()); } }	maybe we should log an error with the complete path, and only throw an exception indicating that the aws-web-identity-token-file link should exist (so that the complete config repository path does not appear in the rest api?)
public void testRegionCanBeSet() throws IOException { final String region = randomAlphaOfLength(5); final Map<String, S3ClientSettings> settings = S3ClientSettings.load( Settings.builder().put("s3.client.other.region", region).build() ); assertThat(settings.get("default").region, is("")); assertThat(settings.get("other").region, is(region)); try (S3Service s3Service = new S3Service(Mockito.mock(Environment.class))) { AmazonS3Client other = (AmazonS3Client) s3Service.buildClient(settings.get("other")); assertThat(other.getSignerRegionOverride(), is(region)); } }	nit: we have testenvironment.newenvironment(settings) if you don't really need to mock class behavior.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { String[] indices = Strings.splitStringByCommaToArray(request.param("index")); if (request.paramAsBoolean(INCLUDE_TYPE_NAME_PARAMETER, true)) { deprecationLogger.deprecatedAndMaybeLog("get_indices_with_types", TYPES_DEPRECATION_MESSAGE); } final GetIndexRequest getIndexRequest = new GetIndexRequest(); getIndexRequest.indices(indices); getIndexRequest.indicesOptions(IndicesOptions.fromRequest(request, getIndexRequest.indicesOptions())); getIndexRequest.local(request.paramAsBoolean("local", getIndexRequest.local())); getIndexRequest.masterNodeTimeout(request.paramAsTime("master_timeout", getIndexRequest.masterNodeTimeout())); getIndexRequest.humanReadable(request.paramAsBoolean("human", false)); getIndexRequest.includeDefaults(request.paramAsBoolean("include_defaults", false)); return channel -> client.admin().indices().getIndex(getIndexRequest, new RestToXContentListener<>(channel)); } /** * Parameters used for controlling the response and thus might not be consumed during * preparation of the request execution in {@link BaseRestHandler#prepareRequest(RestRequest, NodeClient)}	another candidate for referring to baseresthandler.default_include_type_name_policy but no big deal if you miss this
public static ImmutableOpenMap<String, MappingMetaData> createMappingsForIndex() { // rarely have no types int typeCount = rarely() ? 0 : scaledRandomIntBetween(1, 3); return createMappingsForIndex(typeCount); }	i'm not sure i understand when/why this would work? i can understand zero or one mappings for an index but shouldn't >1 be impossible/an error?
public static boolean isRealmTypeAvailable(XPackLicenseState licenseState, String type) { if (Security.ALL_REALMS_FEATURE.checkWithoutTracking(licenseState)) { return true; } else if (Security.STANDARD_REALMS_FEATURE.checkWithoutTracking(licenseState)) { return InternalRealms.isStandardRealm(type) || ReservedRealm.TYPE.equals(type); } else { return isBasicLicensedRealm(type); } }	this can be a separate pr: it would be great if we could consolidate these predicate methods, e.g. isbuiltinrealm, isstandardrealm and isbasiclicensedrealm, reservedrealm.type.equals(type). a consistent naming convention is also helpful.
public void init() throws Exception { concreteSecurityIndexName = randomFrom( RestrictedIndicesNames.INTERNAL_SECURITY_MAIN_INDEX_6, RestrictedIndicesNames.INTERNAL_SECURITY_MAIN_INDEX_7); token = mock(AuthenticationToken.class); when(token.principal()).thenReturn(randomAlphaOfLength(5)); transportRequest = new InternalRequest(); remoteAddress = new InetSocketAddress(InetAddress.getLocalHost(), 100); transportRequest.remoteAddress(new TransportAddress(remoteAddress)); restRequest = new FakeRestRequest.Builder(NamedXContentRegistry.EMPTY).withRemoteAddress(remoteAddress).build(); threadContext = new ThreadContext(Settings.EMPTY); firstRealm = mock(Realm.class); when(firstRealm.type()).thenReturn(FIRST_REALM_TYPE); when(firstRealm.name()).thenReturn(FIRST_REALM_NAME); when(firstRealm.toString()).thenReturn(FIRST_REALM_NAME + "/" + FIRST_REALM_TYPE); secondRealm = mock(Realm.class); when(secondRealm.type()).thenReturn(SECOND_REALM_TYPE); when(secondRealm.name()).thenReturn(SECOND_REALM_NAME); when(secondRealm.toString()).thenReturn(SECOND_REALM_NAME + "/" + SECOND_REALM_TYPE); Settings settings = Settings.builder() .put("path.home", createTempDir()) .put("node.name", "authc_test") .put(XPackSettings.TOKEN_SERVICE_ENABLED_SETTING.getKey(), true) .put(XPackSettings.API_KEY_SERVICE_ENABLED_SETTING.getKey(), true) .build(); MockLicenseState licenseState = mock(MockLicenseState.class); when(licenseState.isAllowed(Security.ALL_REALMS_FEATURE)).thenReturn(true); when(licenseState.isAllowed(Security.STANDARD_REALMS_FEATURE)).thenReturn(true); when(licenseState.checkFeature(Feature.SECURITY_TOKEN_SERVICE)).thenReturn(true); when(licenseState.copyCurrentLicenseState()).thenReturn(licenseState); when(licenseState.checkFeature(Feature.SECURITY_AUDITING)).thenReturn(true); when(licenseState.getOperationMode()).thenReturn(randomFrom(License.OperationMode.ENTERPRISE, License.OperationMode.PLATINUM)); ReservedRealm reservedRealm = mock(ReservedRealm.class); when(reservedRealm.type()).thenReturn("reserved"); when(reservedRealm.name()).thenReturn("reserved_realm"); realms = spy(new TestRealms(Settings.EMPTY, TestEnvironment.newEnvironment(settings), Map.of(FileRealmSettings.TYPE, config -> mock(FileRealm.class), NativeRealmSettings.TYPE, config -> mock(NativeRealm.class)), licenseState, threadContext, reservedRealm, Arrays.asList(firstRealm, secondRealm), Arrays.asList(firstRealm))); // Needed because this is calculated in the constructor, which means the override doesn't get called correctly realms.recomputeActiveRealms(); assertThat(realms.asList(), contains(firstRealm, secondRealm)); auditTrail = mock(AuditTrail.class); auditTrailService = new AuditTrailService(Collections.singletonList(auditTrail), licenseState); client = mock(Client.class); threadPool = new ThreadPool(settings, new FixedExecutorBuilder(settings, THREAD_POOL_NAME, 1, 1000, "xpack.security.authc.token.thread_pool", false), new FixedExecutorBuilder(Settings.EMPTY, SECURITY_CRYPTO_THREAD_POOL_NAME, 1, 1000, "xpack.security.crypto.thread_pool", false) ); threadContext = threadPool.getThreadContext(); when(client.threadPool()).thenReturn(threadPool); when(client.settings()).thenReturn(settings); when(client.prepareIndex(any(String.class))) .thenReturn(new IndexRequestBuilder(client, IndexAction.INSTANCE)); when(client.prepareUpdate(any(String.class), any(String.class))) .thenReturn(new UpdateRequestBuilder(client, UpdateAction.INSTANCE)); doAnswer(invocationOnMock -> { @SuppressWarnings("unchecked") ActionListener<IndexResponse> responseActionListener = (ActionListener<IndexResponse>) invocationOnMock.getArguments()[2]; responseActionListener.onResponse(new IndexResponse(new ShardId(".security", UUIDs.randomBase64UUID(), randomInt()), randomAlphaOfLength(4), randomNonNegativeLong(), randomNonNegativeLong(), randomNonNegativeLong(), true)); return null; }).when(client).execute(eq(IndexAction.INSTANCE), any(IndexRequest.class), anyActionListener()); doAnswer(invocationOnMock -> { GetRequestBuilder builder = new GetRequestBuilder(client, GetAction.INSTANCE); builder.setIndex((String) invocationOnMock.getArguments()[0]) .setId((String) invocationOnMock.getArguments()[1]); return builder; }).when(client).prepareGet(anyString(), anyString()); securityIndex = mock(SecurityIndexManager.class); doAnswer(invocationOnMock -> { Runnable runnable = (Runnable) invocationOnMock.getArguments()[1]; runnable.run(); return null; }).when(securityIndex).prepareIndexIfNeededThenExecute(anyConsumer(), any(Runnable.class)); doAnswer(invocationOnMock -> { Runnable runnable = (Runnable) invocationOnMock.getArguments()[1]; runnable.run(); return null; }).when(securityIndex).checkIndexVersionThenExecute(anyConsumer(), any(Runnable.class)); ClusterService clusterService = ClusterServiceUtils.createClusterService(threadPool); final SecurityContext securityContext = new SecurityContext(settings, threadContext); apiKeyService = new ApiKeyService(settings, Clock.systemUTC(), client, securityIndex, clusterService, mock(CacheInvalidatorRegistry.class), threadPool); tokenService = new TokenService(settings, Clock.systemUTC(), client, licenseState, securityContext, securityIndex, securityIndex, clusterService); serviceAccountService = mock(ServiceAccountService.class); doAnswer(invocationOnMock -> { @SuppressWarnings("unchecked") final ActionListener<Authentication> listener = (ActionListener<Authentication>) invocationOnMock.getArguments()[2]; listener.onResponse(null); return null; }).when(serviceAccountService).authenticateToken(any(), any(), any()); operatorPrivilegesService = mock(OperatorPrivileges.OperatorPrivilegesService.class); service = new AuthenticationService(settings, realms, auditTrailService, new DefaultAuthenticationFailureHandler(Collections.emptyMap()), threadPool, new AnonymousUser(settings), tokenService, apiKeyService, serviceAccountService, operatorPrivilegesService); }	this subclass and spy are getting rather complicated than their worth. what prevents us from replacing them with a simple mock?
public void testUnlicensedWithBasicRealmSettings() throws Exception { factories.put(LdapRealmSettings.LDAP_TYPE, config -> new DummyRealm(LdapRealmSettings.LDAP_TYPE, config)); final String type = randomFrom(FileRealmSettings.TYPE, NativeRealmSettings.TYPE); final String otherType = FileRealmSettings.TYPE.equals(type) ? NativeRealmSettings.TYPE : FileRealmSettings.TYPE; Settings.Builder builder = Settings.builder() .put("path.home", createTempDir()) .put("xpack.security.authc.realms.ldap.foo.order", "0") .put("xpack.security.authc.realms." + type + ".native.order", "1"); final boolean otherTypeDisabled = randomDisableRealm(builder, otherType); Settings settings = builder.build(); Environment env = TestEnvironment.newEnvironment(settings); Realms realms = new Realms(settings, env, factories, licenseState, threadContext, reservedRealm); Iterator<Realm> iter = realms.iterator(); assertThat(iter.hasNext(), is(true)); Realm realm = iter.next(); assertThat(realm, is(reservedRealm)); if (false == otherTypeDisabled) { assertThat(iter.hasNext(), is(true)); realm = iter.next(); assertThat(realm.type(), is(otherType)); } assertThat(iter.hasNext(), is(true)); realm = iter.next(); assertThat(realm.type(), is("ldap")); assertThat(iter.hasNext(), is(true)); realm = iter.next(); assertThat(realm.type(), is(type)); assertThat(iter.hasNext(), is(false)); assertThat(realms.getUnlicensedRealms(), empty()); // during init verify(licenseState, times(1)).addListener(Mockito.any(LicenseStateListener.class)); // each time the license changes ... verify(licenseState, times(1)).copyCurrentLicenseState(); verify(licenseState, times(1)).getOperationMode(); // Verify that we recorded licensed-feature use for each licensed realm (this is trigger on license load/change) verify(licenseState, times(1)).isAllowed(Security.STANDARD_REALMS_FEATURE); verify(licenseState).enableUsageTracking(Security.STANDARD_REALMS_FEATURE, "foo"); verifyNoMoreInteractions(licenseState); allowOnlyNativeRealms(); // because the license state changed ... verify(licenseState, times(2)).copyCurrentLicenseState(); verify(licenseState, times(2)).getOperationMode(); iter = realms.iterator(); assertThat(iter.hasNext(), is(true)); realm = iter.next(); assertThat(realm, is(reservedRealm)); if (false == otherTypeDisabled) { assertThat(iter.hasNext(), is(true)); realm = iter.next(); assertThat(realm.type(), is(otherType)); } assertThat(iter.hasNext(), is(true)); realm = iter.next(); assertThat(realm.type(), is(type)); assertThat(iter.hasNext(), is(false)); // Verify that we checked (a 2nd time) the license for the non-basic realm verify(licenseState, times(2)).isAllowed(Security.STANDARD_REALMS_FEATURE); // Verify that we stopped tracking use for realms which are no longer licensed verify(licenseState).disableUsageTracking(Security.STANDARD_REALMS_FEATURE, "foo"); verifyNoMoreInteractions(licenseState); assertThat(realms.getUnlicensedRealms(), iterableWithSize(1)); realm = realms.getUnlicensedRealms().get(0); assertThat(realm.type(), equalTo("ldap")); assertThat(realm.name(), equalTo("foo")); }	nit: these two calls are still part of "init". no change to license yet.
protected void doExecute(Task task, ValidateQueryRequest request, ActionListener<ValidateQueryResponse> listener) { request.nowInMillis = System.currentTimeMillis(); LongSupplier timeProvider = () -> request.nowInMillis; ActionListener<org.elasticsearch.index.query.QueryBuilder> rewriteListener = ActionListener.wrap(rewrittenQuery -> { request.query(rewrittenQuery); super.doExecute(task, request, listener); }, ex -> { if (ex instanceof IndexNotFoundException || ex instanceof IndexClosedException) { listener.onFailure(ex); } List<QueryExplanation> explanations = new ArrayList<>(); // TODO[PCS]: given that we can have multiple indices, what's the best way to include // these in a query explanation? explanations.add(new QueryExplanation("", QueryExplanation.RANDOM_SHARD, false, null, ex.getMessage())); listener.onResponse( new ValidateQueryResponse( false, explanations, // TODO[PCS]: is it correct to have 0 for total shards if the request is invalidated before // it makes it to the shards? I think the answer is probably no? 0, 0 , 0, null)); }); if (request.query() == null) { rewriteListener.onResponse(request.query()); } else { Rewriteable.rewriteAndFetch(request.query(), searchService.getRewriteContext(timeProvider), rewriteListener); } }	i assume this todo still needs to be resolved? i don't have a good solution here other than having something like "__coordinating_node_rewrite__" in place of the index name
protected void doExecute(Task task, ValidateQueryRequest request, ActionListener<ValidateQueryResponse> listener) { request.nowInMillis = System.currentTimeMillis(); LongSupplier timeProvider = () -> request.nowInMillis; ActionListener<org.elasticsearch.index.query.QueryBuilder> rewriteListener = ActionListener.wrap(rewrittenQuery -> { request.query(rewrittenQuery); super.doExecute(task, request, listener); }, ex -> { if (ex instanceof IndexNotFoundException || ex instanceof IndexClosedException) { listener.onFailure(ex); } List<QueryExplanation> explanations = new ArrayList<>(); // TODO[PCS]: given that we can have multiple indices, what's the best way to include // these in a query explanation? explanations.add(new QueryExplanation("", QueryExplanation.RANDOM_SHARD, false, null, ex.getMessage())); listener.onResponse( new ValidateQueryResponse( false, explanations, // TODO[PCS]: is it correct to have 0 for total shards if the request is invalidated before // it makes it to the shards? I think the answer is probably no? 0, 0 , 0, null)); }); if (request.query() == null) { rewriteListener.onResponse(request.query()); } else { Rewriteable.rewriteAndFetch(request.query(), searchService.getRewriteContext(timeProvider), rewriteListener); } }	i assume this todo still needs to be resolved? again i don't have a great solution here but given that it never made it onto any shard it is true that there were 0 total shards involved in the request, of which 0 succeeded and 0 failed so maybe this is ok?
@Override public void visitAssignment(EAssignment userAssignmentNode, ScriptScope scriptScope) { boolean read = scriptScope.getCondition(userAssignmentNode, Read.class); Class<?> compoundType = scriptScope.hasDecoration(userAssignmentNode, CompoundType.class) ? scriptScope.getDecoration(userAssignmentNode, CompoundType.class).getCompoundType() : null; ExpressionNode irAssignmentNode; // add a cast node if necessary for the value node for the assignment ExpressionNode irValueNode = injectCast(userAssignmentNode.getRightNode(), scriptScope); // handles a compound assignment using the stub generated from buildLoadStore if (compoundType != null) { boolean concatenate = userAssignmentNode.getOperation() == Operation.ADD && compoundType == String.class; scriptScope.setCondition(userAssignmentNode.getLeftNode(), Compound.class); UnaryNode irStoreNode = (UnaryNode) visit(userAssignmentNode.getLeftNode(), scriptScope); ExpressionNode irLoadNode = irStoreNode.getChildNode(); ExpressionNode irCompoundNode; // handles when the operation is a string concatenation if (concatenate) { StringConcatenationNode stringConcatenationNode = new StringConcatenationNode(irStoreNode.getLocation()); stringConcatenationNode.attachDecoration(new IRDExpressionType(String.class)); irCompoundNode = stringConcatenationNode; // must handle the StringBuilder case for java version <= 8 if (irLoadNode instanceof BinaryImplNode && WriterConstants.INDY_STRING_CONCAT_BOOTSTRAP_HANDLE == null) { ((DupNode) ((BinaryImplNode) irLoadNode).getLeftNode()).attachDecoration(new IRDDepth(1)); } // handles when the operation is mathematical } else { BinaryMathNode irBinaryMathNode = new BinaryMathNode(irStoreNode.getLocation()); irBinaryMathNode.setLeftNode(irLoadNode); irBinaryMathNode.attachDecoration(new IRDExpressionType(compoundType)); irBinaryMathNode.attachDecoration(new IRDOperation(userAssignmentNode.getOperation())); irBinaryMathNode.attachDecoration(new IRDBinaryType(compoundType)); // add a compound assignment flag to the binary math node irBinaryMathNode.attachDecoration(new IRDFlags(DefBootstrap.OPERATOR_COMPOUND_ASSIGNMENT)); irCompoundNode = irBinaryMathNode; } PainlessCast downcast = scriptScope.hasDecoration(userAssignmentNode, DowncastPainlessCast.class) ? scriptScope.getDecoration(userAssignmentNode, DowncastPainlessCast.class).getDowncastPainlessCast() : null; // no need to downcast so the binary math node is the value for the store node if (downcast == null) { irCompoundNode.attachDecoration(new IRDExpressionType(irStoreNode.getDecorationValue(IRDStoreType.class))); irStoreNode.setChildNode(irCompoundNode); // add a cast node to do a downcast as the value for the store node } else { CastNode irCastNode = new CastNode(irCompoundNode.getLocation()); irCastNode.attachDecoration(new IRDExpressionType(downcast.targetType)); irCastNode.attachDecoration(new IRDCast(downcast)); irCastNode.setChildNode(irCompoundNode); irStoreNode.setChildNode(irCastNode); } // the value is also read from this assignment if (read) { int accessDepth = scriptScope.getDecoration(userAssignmentNode.getLeftNode(), AccessDepth.class).getAccessDepth(); DupNode irDupNode; // the value is read from prior to assignment (post-increment) if (userAssignmentNode.postIfRead()) { int size = MethodWriter.getType(irLoadNode.getDecorationValue(IRDExpressionType.class)).getSize(); irDupNode = new DupNode(irLoadNode.getLocation()); irDupNode.attachDecoration(irLoadNode.getDecoration(IRDExpressionType.class)); irDupNode.attachDecoration(new IRDSize(size)); irDupNode.attachDecoration(new IRDDepth(accessDepth)); irDupNode.setChildNode(irLoadNode); irLoadNode = irDupNode; // the value is read from after the assignment (pre-increment/compound) } else { int size = MethodWriter.getType(irStoreNode.getDecorationValue(IRDExpressionType.class)).getSize(); irDupNode = new DupNode(irStoreNode.getLocation()); irDupNode.attachDecoration(new IRDExpressionType(irStoreNode.getDecorationValue(IRDStoreType.class))); irDupNode.attachDecoration(new IRDSize(size)); irDupNode.attachDecoration(new IRDDepth(accessDepth)); irDupNode.setChildNode(irStoreNode.getChildNode()); irStoreNode.setChildNode(irDupNode); } } PainlessCast upcast = scriptScope.hasDecoration(userAssignmentNode, UpcastPainlessCast.class) ? scriptScope.getDecoration(userAssignmentNode, UpcastPainlessCast.class).getUpcastPainlessCast() : null; // upcast the stored value if necessary if (upcast != null) { CastNode irCastNode = new CastNode(irLoadNode.getLocation()); irCastNode.attachDecoration(new IRDExpressionType(upcast.targetType)); irCastNode.attachDecoration(new IRDCast(upcast)); irCastNode.setChildNode(irLoadNode); irLoadNode = irCastNode; } if (concatenate) { StringConcatenationNode irStringConcatenationNode = (StringConcatenationNode) irCompoundNode; irStringConcatenationNode.addArgumentNode(irLoadNode); irStringConcatenationNode.addArgumentNode(irValueNode); } else { BinaryMathNode irBinaryMathNode = (BinaryMathNode) irCompoundNode; irBinaryMathNode.setLeftNode(irLoadNode); irBinaryMathNode.setRightNode(irValueNode); } irAssignmentNode = irStoreNode; // handles a standard assignment } else { irAssignmentNode = (ExpressionNode) visit(userAssignmentNode.getLeftNode(), scriptScope); // the value is read from after the assignment if (read) { int size = MethodWriter.getType(irValueNode.getDecorationValue(IRDExpressionType.class)).getSize(); int accessDepth = scriptScope.getDecoration(userAssignmentNode.getLeftNode(), AccessDepth.class).getAccessDepth(); DupNode irDupNode = new DupNode(irValueNode.getLocation()); irDupNode.attachDecoration(irValueNode.getDecoration(IRDExpressionType.class)); irDupNode.attachDecoration(new IRDSize(size)); irDupNode.attachDecoration(new IRDDepth(accessDepth)); irDupNode.setChildNode(irValueNode); irValueNode = irDupNode; } if (irAssignmentNode instanceof BinaryImplNode) { ((UnaryNode) ((BinaryImplNode) irAssignmentNode).getRightNode()).setChildNode(irValueNode); } else { ((UnaryNode) irAssignmentNode).setChildNode(irValueNode); } } scriptScope.putDecoration(userAssignmentNode, new IRNodeDecoration(irAssignmentNode)); }	oh no. i think i ran the formatter over this entire file. ooops.
static SecureSettings loadSecureSettings(Environment initialEnv, InputStream stdin) throws BootstrapException { try { return KeyStoreWrapper.bootstrap(initialEnv.configFile(), () -> readPassphrase(stdin, KeyStoreWrapper.MAX_PASSPHRASE_LENGTH)); } catch (Exception e) { throw new BootstrapException(e); } }	because the startup script writes to the node's keystore, it can also bootstrap the keystore.
*/ static PKCS10CertificationRequest generateCSR(KeyPair keyPair, X500Principal principal, GeneralNames sanList) throws IOException, OperatorCreationException { Objects.requireNonNull(keyPair, "Key-Pair must not be null"); Objects.requireNonNull(keyPair.getPublic(), "Public-Key must not be null"); Objects.requireNonNull(principal, "Principal must not be null"); JcaPKCS10CertificationRequestBuilder builder = new JcaPKCS10CertificationRequestBuilder(principal, keyPair.getPublic()); if (sanList != null) { ExtensionsGenerator extGen = new ExtensionsGenerator(); extGen.addExtension(Extension.subjectAlternativeName, false, sanList); builder.addAttribute(PKCSObjectIdentifiers.pkcs_9_at_extensionRequest, extGen.generate()); } return builder.build(new JcaContentSignerBuilder("SHA256withRSA").setProvider(CertGenUtils.BC_PROV).build(keyPair.getPrivate())); } /** * Gets a random serial for a certificate that is generated from a {@link SecureRandom}	maybe it is useful in test repro to have the same keys and certs (but the validity date changes for certs).
public void testDisassociateDeadNodes_givenAssignedPersistentTask() { DiscoveryNodes nodes = DiscoveryNodes.builder() .add(new DiscoveryNode("node1", new TransportAddress(InetAddress.getLoopbackAddress(), 9300), Version.CURRENT)) .localNodeId("node1") .masterNodeId("node1") .build(); String taskName = "test/task"; PersistentTasksCustomMetaData.Builder tasksBuilder = PersistentTasksCustomMetaData.builder() .addTask("task-id", taskName, emptyTaskParams(taskName), new PersistentTasksCustomMetaData.Assignment("node1", "test assignment")); ClusterState originalState = ClusterState.builder(new ClusterName("persistent-tasks-tests")) .nodes(nodes) .metaData(MetaData.builder().putCustom(PersistentTasksCustomMetaData.TYPE, tasksBuilder.build())) .build(); ClusterState returnedState = PersistentTasksCustomMetaData.deassociateDeadNodes(originalState); assertThat(originalState, sameInstance(returnedState)); PersistentTasksCustomMetaData originalTasks = PersistentTasksCustomMetaData.getPersistentTasksCustomMetaData(originalState); PersistentTasksCustomMetaData returnedTasks = PersistentTasksCustomMetaData.getPersistentTasksCustomMetaData(returnedState); assertEquals(originalTasks, returnedTasks); }	just use buildnewfaketransportaddress() (comes from estestcase)
public static void main(String args[]) throws Exception { System.out.println("checking for jar hell..."); checkJarHell(System.out::println); System.out.println("no jar hell found"); }	can you update the javadoc to explain this parameter?
public IgnoredFieldMapper build(BuilderContext context) { return new IgnoredFieldMapper(); } } public static class TypeParser implements MetadataFieldMapper.TypeParser { @Override public MetadataFieldMapper.Builder<?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { return new Builder(); } @Override public MetadataFieldMapper getDefault(ParserContext context) { return new IgnoredFieldMapper(); } } public static final class IgnoredFieldType extends StringFieldType { public static final IgnoredFieldType INSTANCE = new IgnoredFieldType(); private IgnoredFieldType() { super(NAME, true, true, TextSearchInfo.SIMPLE_MATCH_ONLY, Collections.emptyMap()); } @Override public String typeName() { return CONTENT_TYPE; } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { failIfNoDocValues(); return new SortedSetOrdinalsIndexFieldData.Builder(name(), CoreValuesSourceType.BYTES); } @Override public Query existsQuery(QueryShardContext context) { // This query is not performance sensitive, it only helps assess // quality of the data, so we may use a slow query. It shouldn't // be too slow in practice since the number of unique terms in this // field is bounded by the number of fields in the mappings. return new TermRangeQuery(name(), null, null, true, true); } } private IgnoredFieldMapper() { super(Defaults.FIELD_TYPE, IgnoredFieldType.INSTANCE); } @Override public void preParse(ParseContext context) throws IOException { } @Override public void postParse(ParseContext context) throws IOException { super.parse(context); } @Override public void parse(ParseContext context) throws IOException { // done in post-parse } @Override protected void parseCreateField(ParseContext context) throws IOException { for (String field : context.getIgnoredFields()) { context.doc().add(new Field(NAME, field, fieldType)); final BytesRef binaryValue = new BytesRef(field); if (fieldType().hasDocValues()) { context.doc().add(new SortedSetDocValuesField(fieldType().name(), binaryValue)); } } } @Override protected String contentType() { return CONTENT_TYPE; }	we should only set this boolean to true if the version is >= 7.10
@Override public void snapshotShard(Store store, MapperService mapperService, SnapshotId snapshotId, IndexId indexId, IndexCommit snapshotIndexCommit, IndexShardSnapshotStatus snapshotStatus) { if (mapperService.documentMapper() != null // if there is no mapping this is null && mapperService.documentMapper().sourceMapper().isComplete() == false) { throw new IllegalStateException("Can't snapshot _source only on an index that has incomplete source ie. has _source disabled " + "or filters the source"); } Directory unwrap = FilterDirectory.unwrap(store.directory()); if (unwrap instanceof FSDirectory == false) { throw new IllegalStateException("expected FSDirectory but got " + unwrap.toString()); } Path dataPath = ((FSDirectory)unwrap).getDirectory().getParent(); // TODO should we have a snapshot tmp directory per shard that is maintained by the system? Path snapPath = dataPath.resolve(SNAPSHOT_DIR_NAME); try (FSDirectory directory = new SimpleFSDirectory(snapPath)) { Store tempStore = new Store(store.shardId(), store.indexSettings(), directory, new ShardLock(store.shardId()) { @Override protected void closeInternal() { // do nothing; } }, Store.OnClose.EMPTY); Supplier<Query> querySupplier = mapperService.hasNested() ? Queries::newNestedFilter : null; // SourceOnlySnapshot will take care of soft- and hard-deletes no special casing needed here SourceOnlySnapshot snapshot = new SourceOnlySnapshot(tempStore.directory(), querySupplier); snapshot.syncSnapshot(snapshotIndexCommit); // we will use the lucene doc ID as the seq ID so we set the local checkpoint to maxDoc with a new index UUID SegmentInfos segmentInfos = tempStore.readLastCommittedSegmentsInfo(); final long maxDoc = segmentInfos.totalMaxDoc(); tempStore.bootstrapNewHistory(maxDoc, maxDoc); store.incRef(); try (DirectoryReader reader = DirectoryReader.open(tempStore.directory())) { IndexCommit indexCommit = reader.getIndexCommit(); super.snapshotShard(tempStore, mapperService, snapshotId, indexId, indexCommit, snapshotStatus); } finally { store.decRef(); } } catch (IOException e) { // why on earth does this super method not declare IOException throw new UncheckedIOException(e); } } /** * Returns an {@link EngineFactory}	maybe add an assert unwrap instanceof fsdirectory? we have a number of tests that allow for an exception in this code and it might not be the worst idea to lock this down for good? also, with that, maybe just let it die on a classcastexception below, that's probably good enough for debugging?
@Override public void snapshotShard(Store store, MapperService mapperService, SnapshotId snapshotId, IndexId indexId, IndexCommit snapshotIndexCommit, IndexShardSnapshotStatus snapshotStatus) { if (mapperService.documentMapper() != null // if there is no mapping this is null && mapperService.documentMapper().sourceMapper().isComplete() == false) { throw new IllegalStateException("Can't snapshot _source only on an index that has incomplete source ie. has _source disabled " + "or filters the source"); } Directory unwrap = FilterDirectory.unwrap(store.directory()); if (unwrap instanceof FSDirectory == false) { throw new IllegalStateException("expected FSDirectory but got " + unwrap.toString()); } Path dataPath = ((FSDirectory)unwrap).getDirectory().getParent(); // TODO should we have a snapshot tmp directory per shard that is maintained by the system? Path snapPath = dataPath.resolve(SNAPSHOT_DIR_NAME); try (FSDirectory directory = new SimpleFSDirectory(snapPath)) { Store tempStore = new Store(store.shardId(), store.indexSettings(), directory, new ShardLock(store.shardId()) { @Override protected void closeInternal() { // do nothing; } }, Store.OnClose.EMPTY); Supplier<Query> querySupplier = mapperService.hasNested() ? Queries::newNestedFilter : null; // SourceOnlySnapshot will take care of soft- and hard-deletes no special casing needed here SourceOnlySnapshot snapshot = new SourceOnlySnapshot(tempStore.directory(), querySupplier); snapshot.syncSnapshot(snapshotIndexCommit); // we will use the lucene doc ID as the seq ID so we set the local checkpoint to maxDoc with a new index UUID SegmentInfos segmentInfos = tempStore.readLastCommittedSegmentsInfo(); final long maxDoc = segmentInfos.totalMaxDoc(); tempStore.bootstrapNewHistory(maxDoc, maxDoc); store.incRef(); try (DirectoryReader reader = DirectoryReader.open(tempStore.directory())) { IndexCommit indexCommit = reader.getIndexCommit(); super.snapshotShard(tempStore, mapperService, snapshotId, indexId, indexCommit, snapshotStatus); } finally { store.decRef(); } } catch (IOException e) { // why on earth does this super method not declare IOException throw new UncheckedIOException(e); } } /** * Returns an {@link EngineFactory}	nit: formatting seems to be missing a space here ((fsdirectory) unwrap
@Override public void writeTo(StreamOutput out) throws IOException { out.writeVLong(insertOrder); Priority.writeTo(priority, out); out.writeText(source); if (out.getVersion().onOrAfter(Version.V_1_4_0)) { // timeInQueue is set to -1 when unknown and can be negative if time goes backwards out.writeLong(timeInQueue); } else { out.writeVLong(Math.max(0, timeInQueue)); } if (out.getVersion().onOrAfter(Version.V_1_3_0)) { out.writeBoolean(executing); } }	this is no longer needed. i'll fix this.
@Override protected QueryBuilder doRewrite(QueryRewriteContext queryRewriteContext) throws IOException { if ("_index".equals(fieldName)) { // Special-case optimisation for canMatch phase: // We can skip querying this shard if the index name doesn't match the value of this query on the "_index" field. QueryShardContext shardContext = queryRewriteContext.convertToShardContext(); if (shardContext != null && shardContext.indexMatches(BytesRefs.toString(value)) == false) { return new MatchNoneQueryBuilder(); } } return super.doRewrite(queryRewriteContext); }	bytesrefs.tostring is not needed ? suggestion if (shardcontext != null && shardcontext.indexmatches(value) == false) {
private Engine.Delete prepareDelete(String id, Term uid, long seqNo, long primaryTerm, long version, VersionType versionType, Engine.Operation.Origin origin, long ifSeqNo, long ifPrimaryTerm) { long startTime = threadPool.relativeTimeInNanos(); return new Engine.Delete(id, uid, seqNo, primaryTerm, version, versionType, origin, startTime, ifSeqNo, ifPrimaryTerm); }	this will conflict with internalengine's use of this value in e.g. where we still use the un-cached time: java deleteresult.settook(system.nanotime() - delete.starttime()); which will still throw of the internalindexingstats i think?
@Override public final Query termQuery(Object value, QueryShardContext context) { String pattern = valueToString(value); if (matches(pattern, context)) { if (context != null && context.getMapperService().hasNested()) { // type filters are expected not to match nested docs return Queries.newNonNestedFilter(); } return Queries.newMatchAllQuery(); } else { return new MatchNoDocsQuery(); } }	is this for backwards compatibility? nested fields are defined using a different field in 8x
@Override public MetadataFieldMapper getDefault(ParserContext context) { final IndexSettings indexSettings = context.mapperService().getIndexSettings(); return new TypeFieldMapper(indexSettings, defaultFieldType(indexSettings)); } } public static final class TypeFieldType extends ConstantFieldType { TypeFieldType() { } protected TypeFieldType(TypeFieldType ref) { super(ref); } @Override public MappedFieldType clone() { return new TypeFieldType(this); } @Override public String typeName() { return CONTENT_TYPE; } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { Function<MapperService, String> typeFunction = mapperService -> mapperService.documentMapper().type(); return new ConstantIndexFieldData.Builder(typeFunction); } @Override public ValuesSourceType getValuesSourceType() { return CoreValuesSourceType.BYTES; } @Override protected boolean matches(String pattern, QueryShardContext context) { if (pattern.contains("?") == false) { return Regex.simpleMatch(pattern, MapperService.SINGLE_MAPPING_NAME); } boolean matches = Pattern.matches(pattern.replace("?", "."), MapperService.SINGLE_MAPPING_NAME); return matches; } } /** * Specialization for a disjunction over many _type */ public static class TypesQuery extends Query { // Same threshold as TermInSetQuery private static final int BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD = 16; private final BytesRef[] types; public TypesQuery(BytesRef... types) { if (types == null) { throw new NullPointerException("types cannot be null."); } if (types.length == 0) { throw new IllegalArgumentException("types must contains at least one value."); } this.types = types; } public BytesRef[] getTerms() { return types; } @Override public Query rewrite(IndexReader reader) throws IOException { final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount()); if (types.length <= threshold) { Set<BytesRef> uniqueTypes = new HashSet<>(); BooleanQuery.Builder bq = new BooleanQuery.Builder(); int totalDocFreq = 0; for (BytesRef type : types) { if (uniqueTypes.add(type)) { Term term = new Term(CONTENT_TYPE, type); TermStates context = TermStates.build(reader.getContext(), term, true); if (context.docFreq() == 0) { // this _type is not present in the reader continue; } totalDocFreq += context.docFreq(); // strict equality should be enough ? if (totalDocFreq >= reader.maxDoc()) { assert totalDocFreq == reader.maxDoc(); // Matches all docs since _type is a single value field // Using a match_all query will help Lucene perform some optimizations // For instance, match_all queries as filter clauses are automatically removed return new MatchAllDocsQuery(); } bq.add(new TermQuery(term, context), BooleanClause.Occur.SHOULD); } } return new ConstantScoreQuery(bq.build()); } return new TermInSetQuery(CONTENT_TYPE, types); } @Override public boolean equals(Object obj) { if (sameClassAs(obj) == false) { return false; } TypesQuery that = (TypesQuery) obj; return Arrays.equals(types, that.types); } @Override public int hashCode() { return 31 * classHash() + Arrays.hashCode(types); } @Override public String toString(String field) { StringBuilder builder = new StringBuilder(); for (BytesRef type : types) { if (builder.length() > 0) { builder.append(' '); } builder.append(new Term(CONTENT_TYPE, type).toString()); } return builder.toString(); } } private TypeFieldMapper(IndexSettings indexSettings, MappedFieldType existing) { this(existing == null ? defaultFieldType(indexSettings) : existing.clone(), indexSettings); } private TypeFieldMapper(MappedFieldType fieldType, IndexSettings indexSettings) { super(NAME, fieldType, defaultFieldType(indexSettings), indexSettings.getSettings()); } private static MappedFieldType defaultFieldType(IndexSettings indexSettings) { MappedFieldType defaultFieldType = Defaults.FIELD_TYPE.clone(); defaultFieldType.setIndexOptions(IndexOptions.NONE); defaultFieldType.setHasDocValues(false); return defaultFieldType; } @Override public void preParse(ParseContext context) throws IOException { super.parse(context); } @Override public void parse(ParseContext context) throws IOException { // we parse in pre parse } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { if (fieldType().indexOptions() == IndexOptions.NONE && !fieldType().stored()) { return; } fields.add(new Field(fieldType().name(), MapperService.SINGLE_MAPPING_NAME, fieldType())); if (fieldType().hasDocValues()) { fields.add(new SortedSetDocValuesField(fieldType().name(), new BytesRef(MapperService.SINGLE_MAPPING_NAME))); } } @Override protected String contentType() { return CONTENT_TYPE; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { return builder; }	i think you can use context.getmapperservice().type() here, instead of single_mapping_name, and it will then backport easily.
protected final void doXContentAnalyzers(XContentBuilder builder, boolean includeDefaults) throws IOException { if (fieldType.tokenized() == false || fieldType().getTextSearchInfo().getSearchAnalyzer() == null) { return; } if (fieldType().indexAnalyzer() == null) { if (includeDefaults) { builder.field("analyzer", "default"); } } else { boolean hasDefaultIndexAnalyzer = fieldType().indexAnalyzer().name().equals("default"); final String searchAnalyzerName = fieldType().getTextSearchInfo().getSearchAnalyzer().name(); final String searchQuoteAnalyzerName = fieldType().getTextSearchInfo().getSearchQuoteAnalyzer() == null ? searchAnalyzerName : fieldType().getTextSearchInfo().getSearchQuoteAnalyzer().name(); boolean hasDifferentSearchAnalyzer = searchAnalyzerName.equals(fieldType().indexAnalyzer().name()) == false; boolean hasDifferentSearchQuoteAnalyzer = Objects.equals(searchAnalyzerName, searchQuoteAnalyzerName) == false; if (includeDefaults || hasDefaultIndexAnalyzer == false || hasDifferentSearchAnalyzer || hasDifferentSearchQuoteAnalyzer) { builder.field("analyzer", fieldType().indexAnalyzer().name()); if (includeDefaults || hasDifferentSearchAnalyzer || hasDifferentSearchQuoteAnalyzer) { builder.field("search_analyzer", searchAnalyzerName); if (includeDefaults || hasDifferentSearchQuoteAnalyzer) { builder.field("search_quote_analyzer", searchQuoteAnalyzerName); } } } } }	will this change the behavior when includedefaults is true and there isn't a search_analyzer? i'm not familiar enough with the default values of everything to know. i think it is a least worth a comment about it here.
@Override protected void mergeOptions(FieldMapper other, List<String> conflicts) { KeywordFieldMapper k = (KeywordFieldMapper) other; if (Objects.equals(fieldType().indexAnalyzer(), k.fieldType().indexAnalyzer()) == false) { conflicts.add("mapper [" + name() + "] has different [normalizer]"); } if (Objects.equals(mappedFieldType.getTextSearchInfo().getSimilarity(), k.fieldType().getTextSearchInfo().getSimilarity()) == false) { conflicts.add("mapper [" + name() + "] has different [similarity]"); } this.ignoreAbove = k.ignoreAbove; this.splitQueriesOnWhitespace = k.splitQueriesOnWhitespace; this.fieldType().setBoost(k.fieldType().boost()); }	should we be able to change the search_analyzer like this?
@Override public boolean equals(Object o) { if (o == null || getClass() != o.getClass()) { return false; } MappedFieldType fieldType = (MappedFieldType) o; return boost == fieldType.boost && docValues == fieldType.docValues && Objects.equals(name, fieldType.name) && Objects.equals(indexAnalyzer, fieldType.indexAnalyzer) && Objects.equals(eagerGlobalOrdinals, fieldType.eagerGlobalOrdinals) && Objects.equals(meta, fieldType.meta); }	should testsearchinfo be in equals and hashcode?
public void testBuild() throws IOException { for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) { SB suggestionBuilder = randomTestBuilder(); Settings indexSettings = Settings.builder().put(IndexMetadata.SETTING_VERSION_CREATED, Version.CURRENT).build(); IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(new Index(randomAlphaOfLengthBetween(1, 10), "_na_"), indexSettings); MapperService mapperService = mock(MapperService.class); ScriptService scriptService = mock(ScriptService.class); boolean fieldTypeSearchAnalyzerSet = randomBoolean(); MappedFieldType fieldType = mockFieldType(suggestionBuilder.field(), fieldTypeSearchAnalyzerSet); when(mapperService.searchAnalyzer()) .thenReturn(new NamedAnalyzer("mapperServiceSearchAnalyzer", AnalyzerScope.INDEX, new SimpleAnalyzer())); when(mapperService.fieldType(any(String.class))).thenReturn(fieldType); when(mapperService.getNamedAnalyzer(any(String.class))).then( invocation -> new NamedAnalyzer((String) invocation.getArguments()[0], AnalyzerScope.INDEX, new SimpleAnalyzer())); when(scriptService.compile(any(Script.class), any())).then(invocation -> new TestTemplateService.MockTemplateScript.Factory( ((Script) invocation.getArguments()[0]).getIdOrCode())); QueryShardContext mockShardContext = new QueryShardContext(0, idxSettings, BigArrays.NON_RECYCLING_INSTANCE, null, null, mapperService, null, scriptService, xContentRegistry(), namedWriteableRegistry, null, null, System::currentTimeMillis, null, null, () -> true, null); SuggestionContext suggestionContext = suggestionBuilder.build(mockShardContext); assertEquals(toBytesRef(suggestionBuilder.text()), suggestionContext.getText()); if (suggestionBuilder.text() != null && suggestionBuilder.prefix() == null) { assertEquals(toBytesRef(suggestionBuilder.text()), suggestionContext.getPrefix()); } else { assertEquals(toBytesRef(suggestionBuilder.prefix()), suggestionContext.getPrefix()); } assertEquals(toBytesRef(suggestionBuilder.regex()), suggestionContext.getRegex()); assertEquals(suggestionBuilder.field(), suggestionContext.getField()); int expectedSize = suggestionBuilder.size() != null ? suggestionBuilder.size : 5; assertEquals(expectedSize, suggestionContext.getSize()); Integer expectedShardSize = suggestionBuilder.shardSize != null ? suggestionBuilder.shardSize : Math.max(expectedSize, 5); assertEquals(expectedShardSize, suggestionContext.getShardSize()); assertSame(mockShardContext, suggestionContext.getShardContext()); if (suggestionBuilder.analyzer() != null) { assertEquals(suggestionBuilder.analyzer(), ((NamedAnalyzer) suggestionContext.getAnalyzer()).name()); } else if (fieldTypeSearchAnalyzerSet) { assertEquals("fieldSearchAnalyzer", ((NamedAnalyzer) suggestionContext.getAnalyzer()).name()); } else { assertEquals("mapperServiceSearchAnalyzer", ((NamedAnalyzer) suggestionContext.getAnalyzer()).name()); } assertSuggestionContext(suggestionBuilder, suggestionContext); } }	nice to see that gone!
public Aggregator createInternal(SearchContext searchContext, Aggregator parent, CardinalityUpperBound cardinality, Map<String, Object> metadata) throws IOException { HashMap<String, ValuesSource> valuesSources = new HashMap<>(); for (Map.Entry<String, ValuesSourceConfig> config : configs.entrySet()) { ValuesSource vs = config.getValue().toValuesSource(); if (vs != null) { valuesSources.put(config.getKey(), vs); } } if (valuesSources.isEmpty()) { return createUnmapped(searchContext, parent, metadata); } return doCreateInternal(valuesSources, searchContext, parent, cardinality, metadata); } /** * Create the {@linkplain Aggregator}	as noted on docreateinternal, it's more accurate to say this is used when we have no values.
public Aggregator createInternal(SearchContext searchContext, Aggregator parent, CardinalityUpperBound cardinality, Map<String, Object> metadata) throws IOException { HashMap<String, ValuesSource> valuesSources = new HashMap<>(); for (Map.Entry<String, ValuesSourceConfig> config : configs.entrySet()) { ValuesSource vs = config.getValue().toValuesSource(); if (vs != null) { valuesSources.put(config.getKey(), vs); } } if (valuesSources.isEmpty()) { return createUnmapped(searchContext, parent, metadata); } return doCreateInternal(valuesSources, searchContext, parent, cardinality, metadata); } /** * Create the {@linkplain Aggregator}	this isn't accurate. if the user specified a script, we might not have a mapped field at this point. or for that matter, if we couldn't map a field but the user provided a missing value. it would be accurate to say we are creating the aggregator for a non-empty values source. in fact, a large point of the values source abstraction is to not have to know at this point if we have a mapped field or not.
* @return shard failure information */ static SnapshotShardFailure readSnapshotShardFailure(StreamInput in) throws IOException { return new SnapshotShardFailure(in); }	while we're here... can we remove this, and its usage in snapshotinfo?
private static void checkHisto(HistogramAggregationBuilder source, List<RollupJobCaps> jobCaps, Set<RollupJobCaps> bestCaps) { ArrayList<RollupJobCaps> localCaps = new ArrayList<>(); for (RollupJobCaps cap : jobCaps) { RollupJobCaps.RollupFieldCaps fieldCaps = cap.getFieldCaps().get(source.field()); if (fieldCaps != null) { for (Map<String, Object> agg : fieldCaps.getAggs()) { if (agg.get(RollupField.AGG).equals(HistogramAggregationBuilder.NAME)) { Long interval = (long)agg.get(RollupField.INTERVAL); // query interval must be gte the configured interval, and a whole multiple if (interval <= source.interval() && source.interval() % interval == 0) { localCaps.add(cap); } break; } } } } if (localCaps.isEmpty()) { throw new IllegalArgumentException("There is not a rollup job that has a [" + source.getWriteableName() + "] agg on field [" + source.field() + "] which also satisfies all requirements of query."); } // We are a leaf, save our best caps if (source.getSubAggregations().size() == 0) { bestCaps.add(getTopEqualCaps(localCaps)); } else { // otherwise keep working down the tree source.getSubAggregations().forEach(sub -> doFindBestJobs(sub, localCaps, bestCaps)); } }	should we also turn this breaking change in a deprecation warning ?
private void setupSecurity(Settings settings, Environment environment) throws Exception { if (settings.getAsBoolean("security.manager.enabled", true)) { Security.configure(environment); } }	can we make "security.manager.enabled" a constant please
public void testRecoveryIsCancelledAfterDeletingTheIndex() throws Exception { updateSetting(INDICES_RECOVERY_MAX_CONCURRENT_SNAPSHOT_FILE_DOWNLOADS.getKey(), "1"); try { boolean seqNoRecovery = randomBoolean(); String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final Settings.Builder indexSettings = Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false) .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), "1s"); final List<String> dataNodes; if (seqNoRecovery) { dataNodes = internalCluster().startDataOnlyNodes(3); indexSettings.put("index.routing.allocation.include._name", String.join(",", dataNodes)); } else { dataNodes = internalCluster().startDataOnlyNodes(1); indexSettings.put("index.routing.allocation.require._name", dataNodes.get(0)); } createIndex(indexName, indexSettings.build()); int numDocs = randomIntBetween(300, 1000); indexDocs(indexName, numDocs, numDocs); if (seqNoRecovery) { // Flush to ensure that index_commit_seq_nos(replica) == index_commit_seq_nos(primary), // since the primary flushes the index before taking the snapshot. flush(indexName); } String repoName = "repo"; createRepo(repoName, "fs"); createSnapshot(repoName, "snap", Collections.singletonList(indexName)); final String targetNode; if (seqNoRecovery) { ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); IndexShardRoutingTable shardRoutingTable = clusterState.routingTable().index(indexName).shard(0); String primaryNodeName = clusterState.nodes().resolveNode(shardRoutingTable.primaryShard().currentNodeId()).getName(); String replicaNodeName = clusterState.nodes().resolveNode(shardRoutingTable.replicaShards().get(0).currentNodeId()).getName(); targetNode = dataNodes.stream() .filter(nodeName -> nodeName.equals(primaryNodeName) == false && nodeName.equals(replicaNodeName) == false) .findFirst() .get(); } else { targetNode = internalCluster().startDataOnlyNode(); } MockTransportService targetMockTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, targetNode); CountDownLatch recoverSnapshotFileRequestReceived = new CountDownLatch(1); CountDownLatch respondToRecoverSnapshotFile = new CountDownLatch(1); AtomicInteger numberOfRecoverSnapshotFileRequestsReceived = new AtomicInteger(); targetMockTransportService.addRequestHandlingBehavior(PeerRecoveryTargetService.Actions.RESTORE_FILE_FROM_SNAPSHOT, (handler, request, channel, task) -> { assertThat(numberOfRecoverSnapshotFileRequestsReceived.incrementAndGet(), is(equalTo(1))); recoverSnapshotFileRequestReceived.countDown(); respondToRecoverSnapshotFile.await(); handler.messageReceived(request, channel, task); } ); if (seqNoRecovery) { ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); IndexShardRoutingTable shardRoutingTable = clusterState.routingTable().index(indexName).shard(0); String primaryNodeName = clusterState.nodes().resolveNode(shardRoutingTable.primaryShard().currentNodeId()).getName(); assertThat(internalCluster().stopNode(primaryNodeName), is(equalTo(true))); } else { assertAcked( client().admin().indices().prepareUpdateSettings(indexName) .setSettings(Settings.builder() .put("index.routing.allocation.require._name", targetNode)).get() ); } recoverSnapshotFileRequestReceived.await(); assertAcked(client().admin().indices().prepareDelete(indexName).get()); respondToRecoverSnapshotFile.countDown(); assertThat(indexExists(indexName), is(equalTo(false))); } finally { updateSetting(INDICES_RECOVERY_MAX_CONCURRENT_SNAPSHOT_FILE_DOWNLOADS.getKey(), null); } }	did you intend to add a replica in this test for the seqnorecovery case? running the test failed for me when looking up the replica shard further down.
public void testRecoveryIsCancelledAfterDeletingTheIndex() throws Exception { updateSetting(INDICES_RECOVERY_MAX_CONCURRENT_SNAPSHOT_FILE_DOWNLOADS.getKey(), "1"); try { boolean seqNoRecovery = randomBoolean(); String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final Settings.Builder indexSettings = Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false) .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), "1s"); final List<String> dataNodes; if (seqNoRecovery) { dataNodes = internalCluster().startDataOnlyNodes(3); indexSettings.put("index.routing.allocation.include._name", String.join(",", dataNodes)); } else { dataNodes = internalCluster().startDataOnlyNodes(1); indexSettings.put("index.routing.allocation.require._name", dataNodes.get(0)); } createIndex(indexName, indexSettings.build()); int numDocs = randomIntBetween(300, 1000); indexDocs(indexName, numDocs, numDocs); if (seqNoRecovery) { // Flush to ensure that index_commit_seq_nos(replica) == index_commit_seq_nos(primary), // since the primary flushes the index before taking the snapshot. flush(indexName); } String repoName = "repo"; createRepo(repoName, "fs"); createSnapshot(repoName, "snap", Collections.singletonList(indexName)); final String targetNode; if (seqNoRecovery) { ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); IndexShardRoutingTable shardRoutingTable = clusterState.routingTable().index(indexName).shard(0); String primaryNodeName = clusterState.nodes().resolveNode(shardRoutingTable.primaryShard().currentNodeId()).getName(); String replicaNodeName = clusterState.nodes().resolveNode(shardRoutingTable.replicaShards().get(0).currentNodeId()).getName(); targetNode = dataNodes.stream() .filter(nodeName -> nodeName.equals(primaryNodeName) == false && nodeName.equals(replicaNodeName) == false) .findFirst() .get(); } else { targetNode = internalCluster().startDataOnlyNode(); } MockTransportService targetMockTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, targetNode); CountDownLatch recoverSnapshotFileRequestReceived = new CountDownLatch(1); CountDownLatch respondToRecoverSnapshotFile = new CountDownLatch(1); AtomicInteger numberOfRecoverSnapshotFileRequestsReceived = new AtomicInteger(); targetMockTransportService.addRequestHandlingBehavior(PeerRecoveryTargetService.Actions.RESTORE_FILE_FROM_SNAPSHOT, (handler, request, channel, task) -> { assertThat(numberOfRecoverSnapshotFileRequestsReceived.incrementAndGet(), is(equalTo(1))); recoverSnapshotFileRequestReceived.countDown(); respondToRecoverSnapshotFile.await(); handler.messageReceived(request, channel, task); } ); if (seqNoRecovery) { ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); IndexShardRoutingTable shardRoutingTable = clusterState.routingTable().index(indexName).shard(0); String primaryNodeName = clusterState.nodes().resolveNode(shardRoutingTable.primaryShard().currentNodeId()).getName(); assertThat(internalCluster().stopNode(primaryNodeName), is(equalTo(true))); } else { assertAcked( client().admin().indices().prepareUpdateSettings(indexName) .setSettings(Settings.builder() .put("index.routing.allocation.require._name", targetNode)).get() ); } recoverSnapshotFileRequestReceived.await(); assertAcked(client().admin().indices().prepareDelete(indexName).get()); respondToRecoverSnapshotFile.countDown(); assertThat(indexExists(indexName), is(equalTo(false))); } finally { updateSetting(INDICES_RECOVERY_MAX_CONCURRENT_SNAPSHOT_FILE_DOWNLOADS.getKey(), null); } }	i am not sure i follow this comment since at this time there is no replica?
private Store.MetadataSnapshot getMetadataSnapshot(String nodeName, String indexName, int globalCheckpoint) throws IOException { ClusterState clusterState = client().admin().cluster().prepareState().get().getState(); IndicesService indicesService = internalCluster().getInstance(IndicesService.class, nodeName); IndexService indexService = indicesService.indexService(clusterState.metadata().index(indexName).getIndex()); IndexShard shard = indexService.getShard(0); List<IndexCommit> commits = DirectoryReader.listCommits(shard.store().directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); assertThat(safeCommit, is(notNullValue())); return shard.store().getMetadata(safeCommit); }	can we use indexshard.acquiresafecommit() instead? need to release it then, but seems more straighforward.
@Override public boolean equals(Object other) { if (this == other) { return true; } if (other == null || getClass() != other.getClass()) { return false; } AnomalyRecord that = (AnomalyRecord) other; return Objects.equals(this.jobId, that.jobId) && this.detectorIndex == that.detectorIndex && this.bucketSpan == that.bucketSpan && this.probability == that.probability && this.impact == that.impact && this.recordScore == that.recordScore && this.initialRecordScore == that.initialRecordScore && Objects.deepEquals(this.typical, that.typical) && Objects.deepEquals(this.actual, that.actual) && Objects.equals(this.function, that.function) && Objects.equals(this.functionDescription, that.functionDescription) && Objects.equals(this.fieldName, that.fieldName) && Objects.equals(this.byFieldName, that.byFieldName) && Objects.equals(this.byFieldValue, that.byFieldValue) && Objects.equals(this.correlatedByFieldValue, that.correlatedByFieldValue) && Objects.equals(this.partitionFieldName, that.partitionFieldName) && Objects.equals(this.partitionFieldValue, that.partitionFieldValue) && Objects.equals(this.overFieldName, that.overFieldName) && Objects.equals(this.overFieldValue, that.overFieldValue) && Objects.equals(this.timestamp, that.timestamp) && Objects.equals(this.isInterim, that.isInterim) && Objects.equals(this.causes, that.causes) && Objects.equals(this.influences, that.influences); }	this will need changing to objects.equals() for type double. same in the server-side file.
public static AnomalyRecord createTestInstance(String jobId) { AnomalyRecord anomalyRecord = new AnomalyRecord(jobId, new Date(randomNonNegativeLong()), randomNonNegativeLong()); anomalyRecord.setActual(Collections.singletonList(randomDouble())); anomalyRecord.setTypical(Collections.singletonList(randomDouble())); anomalyRecord.setProbability(randomDouble()); anomalyRecord.setImpact(randomDouble()); anomalyRecord.setRecordScore(randomDouble()); anomalyRecord.setInitialRecordScore(randomDouble()); anomalyRecord.setInterim(randomBoolean()); if (randomBoolean()) { anomalyRecord.setFieldName(randomAlphaOfLength(12)); } if (randomBoolean()) { anomalyRecord.setByFieldName(randomAlphaOfLength(12)); anomalyRecord.setByFieldValue(randomAlphaOfLength(12)); } if (randomBoolean()) { anomalyRecord.setPartitionFieldName(randomAlphaOfLength(12)); anomalyRecord.setPartitionFieldValue(randomAlphaOfLength(12)); } if (randomBoolean()) { anomalyRecord.setOverFieldName(randomAlphaOfLength(12)); anomalyRecord.setOverFieldValue(randomAlphaOfLength(12)); } anomalyRecord.setFunction(randomAlphaOfLengthBetween(5, 20)); anomalyRecord.setFunctionDescription(randomAlphaOfLengthBetween(5, 20)); if (randomBoolean()) { anomalyRecord.setCorrelatedByFieldValue(randomAlphaOfLength(16)); } if (randomBoolean()) { int count = randomIntBetween(0, 9); List<Influence> influences = new ArrayList<>(); for (int i=0; i<count; i++) { influences.add(new Influence(randomAlphaOfLength(8), Collections.singletonList(randomAlphaOfLengthBetween(1, 28)))); } anomalyRecord.setInfluencers(influences); } if (randomBoolean()) { int count = randomIntBetween(0, 9); List<AnomalyCause> causes = new ArrayList<>(); for (int i=0; i<count; i++) { causes.add(new AnomalyCauseTests().createTestInstance()); } anomalyRecord.setCauses(causes); } return anomalyRecord; }	to confirm null works once the type is changed to double, surround this line with if (randomboolean()) {. same for the server-side file.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(jobId); out.writeInt(detectorIndex); out.writeDouble(probability); out.writeDouble(impact); out.writeOptionalString(byFieldName); out.writeOptionalString(byFieldValue); out.writeOptionalString(correlatedByFieldValue); out.writeOptionalString(partitionFieldName); out.writeOptionalString(partitionFieldValue); out.writeOptionalString(function); out.writeOptionalString(functionDescription); out.writeOptionalString(fieldName); out.writeOptionalString(overFieldName); out.writeOptionalString(overFieldValue); boolean hasTypical = typical != null; out.writeBoolean(hasTypical); if (hasTypical) { out.writeGenericValue(typical); } boolean hasActual = actual != null; out.writeBoolean(hasActual); if (hasActual) { out.writeGenericValue(actual); } out.writeBoolean(isInterim); boolean hasCauses = causes != null; out.writeBoolean(hasCauses); if (hasCauses) { out.writeList(causes); } out.writeDouble(recordScore); out.writeDouble(initialRecordScore); out.writeLong(timestamp.getTime()); out.writeLong(bucketSpan); boolean hasInfluencers = influences != null; out.writeBoolean(hasInfluencers); if (hasInfluencers) { out.writeList(influences); } }	to make this backwards compatible it needs to be: if (out.version().onorafter(version.v_6_5_0)) { out.writeoptionaldouble(impact); }
XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException { builder.field(Job.ID.getPreferredName(), jobId); builder.field(Result.RESULT_TYPE.getPreferredName(), RESULT_TYPE_VALUE); builder.field(PROBABILITY.getPreferredName(), probability); builder.field(IMPACT.getPreferredName(), impact); builder.field(RECORD_SCORE.getPreferredName(), recordScore); builder.field(INITIAL_RECORD_SCORE.getPreferredName(), initialRecordScore); builder.field(BUCKET_SPAN.getPreferredName(), bucketSpan); builder.field(Detector.DETECTOR_INDEX.getPreferredName(), detectorIndex); builder.field(Result.IS_INTERIM.getPreferredName(), isInterim); builder.timeField(Result.TIMESTAMP.getPreferredName(), Result.TIMESTAMP.getPreferredName() + "_string", timestamp.getTime()); if (byFieldName != null) { builder.field(BY_FIELD_NAME.getPreferredName(), byFieldName); } if (byFieldValue != null) { builder.field(BY_FIELD_VALUE.getPreferredName(), byFieldValue); } if (correlatedByFieldValue != null) { builder.field(CORRELATED_BY_FIELD_VALUE.getPreferredName(), correlatedByFieldValue); } if (partitionFieldName != null) { builder.field(PARTITION_FIELD_NAME.getPreferredName(), partitionFieldName); } if (partitionFieldValue != null) { builder.field(PARTITION_FIELD_VALUE.getPreferredName(), partitionFieldValue); } if (function != null) { builder.field(FUNCTION.getPreferredName(), function); } if (functionDescription != null) { builder.field(FUNCTION_DESCRIPTION.getPreferredName(), functionDescription); } if (typical != null) { builder.field(TYPICAL.getPreferredName(), typical); } if (actual != null) { builder.field(ACTUAL.getPreferredName(), actual); } if (fieldName != null) { builder.field(FIELD_NAME.getPreferredName(), fieldName); } if (overFieldName != null) { builder.field(OVER_FIELD_NAME.getPreferredName(), overFieldName); } if (overFieldValue != null) { builder.field(OVER_FIELD_VALUE.getPreferredName(), overFieldValue); } if (causes != null) { builder.field(CAUSES.getPreferredName(), causes); } if (influences != null) { builder.field(INFLUENCERS.getPreferredName(), influences); } Map<String, LinkedHashSet<String>> inputFields = inputFieldMap(); for (String fieldName : inputFields.keySet()) { builder.field(fieldName, inputFields.get(fieldName)); } return builder; }	when the type is changed to double this should be wrapped in a null check so we write nothing if it's null.
public Builder setChunkingConfig(ChunkingConfig chunkingConfig) { this.chunkingConfig = chunkingConfig; return this; } /** * This determines how far in the past we look for data being indexed too late for the datafeed to pick it up. * * We query the index to the latest finalized bucket from this TimeValue in the past looking to see if any data has been indexed * since the data was read with the Datafeed. * * The window must be larger than the {@link org.elasticsearch.client.ml.job.config.AnalysisConfig#bucketSpan}	nit missing full stop at the end of the sentence
public Collection<Object> createComponents(Client client, ClusterService clusterService, ThreadPool threadPool, ResourceWatcherService resourceWatcherService, ScriptService scriptService, NamedXContentRegistry xContentRegistry, Environment environment, NodeEnvironment nodeEnvironment, NamedWriteableRegistry namedWriteableRegistry) { settings = environment.settings(); enabled = ENABLED_SETTING.get(settings); if (enabled == false) { return List.of(); } SamlUtils.initialize(); CloudIdp idp = new CloudIdp(environment, settings); return List.of(); }	why is this repeated here?
private void deleteIndices(RepositoryData repositoryData, List<IndexId> indices, SnapshotId snapshotId, ActionListener<Void> listener) { if (indices.isEmpty()) { listener.onResponse(null); return; } final ActionListener<Void> groupedListener = new GroupedActionListener<>(ActionListener.map(listener, v -> null), indices.size()); for (IndexId indexId: indices) { threadPool.executor(ThreadPool.Names.SNAPSHOT).execute(new ActionRunnable<>(groupedListener) { @Override protected void doRun() { IndexMetaData indexMetaData = null; try { indexMetaData = getSnapshotIndexMetaData(snapshotId, indexId); } catch (Exception ex) { logger.warn(() -> new ParameterizedMessage("[{}] [{}] failed to read metadata for index", snapshotId, indexId.getName()), ex); } deleteIndexMetaDataBlobIgnoringErrors(snapshotId, indexId); if (indexMetaData != null) { for (int shardId = 0; shardId < indexMetaData.getNumberOfShards(); shardId++) { try { deleteShardSnapshot(repositoryData, indexId, new ShardId(indexMetaData.getIndex(), shardId), snapshotId); } catch (SnapshotException ex) { final int finalShardId = shardId; logger.warn(() -> new ParameterizedMessage("[{}] failed to delete shard data for shard [{}][{}]", snapshotId, indexId.getName(), finalShardId), ex); } } } groupedListener.onResponse(null); } }); } }	should we catch exception and not only snapshotexception? down the line we introduce final set<string> survivingsnapshotnames = repositorydata.getsnapshots(indexid).... that throws iae
public void writeUpdateProcessMessage(JobTask jobTask, UpdateParams updateParams, Consumer<Exception> handler) { AutodetectCommunicator communicator = getOpenAutodetectCommunicator(jobTask); if (communicator == null) { String message = "Cannot process update model debug config because job [" + jobTask.getJobId() + "] does not have a corresponding autodetect process"; logger.debug(message); handler.accept(ExceptionsHelper.conflictStatusException(message)); return; } UpdateProcessMessage.Builder updateProcessMessage = new UpdateProcessMessage.Builder(); updateProcessMessage.setModelPlotConfig(updateParams.getModelPlotConfig()); updateProcessMessage.setDetectorUpdates(updateParams.getDetectorUpdates()); // Step 3. Set scheduled events on message and write update process message ActionListener<QueryPage<ScheduledEvent>> eventsListener = ActionListener.wrap( events -> { updateProcessMessage.setScheduledEvents(events == null ? null : events.results()); communicator.writeUpdateProcessMessage(updateProcessMessage.build(), (aVoid, e) -> { if (e == null) { handler.accept(null); } else { handler.accept(e); } }); }, handler ); // Step 2. Set the filter on the message and get scheduled events ActionListener<MlFilter> filterListener = ActionListener.wrap( filter -> { updateProcessMessage.setFilter(filter); if (updateParams.isUpdateScheduledEvents()) { jobManager.getJob(jobTask.getJobId(), new ActionListener<Job>() { @Override public void onResponse(Job job) { DataCounts dataCounts = getStatistics(jobTask).get().v1(); ScheduledEventsQueryBuilder query = new ScheduledEventsQueryBuilder() .start(job.earliestValidTimestamp(dataCounts)); jobResultsProvider.scheduledEventsForJob(jobTask.getJobId(), job.getGroups(), query, eventsListener); } @Override public void onFailure(Exception e) { handler.accept(e); } }); } else { eventsListener.onResponse(null); } }, handler ); // Step 1. Get the filter if (updateParams.getFilter() == null) { filterListener.onResponse(null); } else { GetFiltersAction.Request getFilterRequest = new GetFiltersAction.Request(); getFilterRequest.setFilterId(updateParams.getFilter().getId()); executeAsyncWithOrigin(client, ML_ORIGIN, GetFiltersAction.INSTANCE, getFilterRequest, new ActionListener<GetFiltersAction.Response>() { @Override public void onResponse(GetFiltersAction.Response response) { filterListener.onResponse(response.getFilters().results().get(0)); } @Override public void onFailure(Exception e) { handler.accept(e); } }); } }	this message probably made sense once but it doesn't anymore. i'd suggest cannot update the job config because job...
@Override public synchronized void validateCurrentStage(Stage expected) { if (remoteTranslogSet == false) { super.validateCurrentStage(expected); } else { final Stage stage = getStage(); // For small indices it's possible that pre-warming finished shortly // after transitioning to FINALIZE stage if (stage != Stage.FINALIZE && stage != Stage.DONE) { assert false : "expected stage [" + Stage.FINALIZE + " || " + Stage.DONE + "]; but current stage is [" + stage + "]"; throw new IllegalStateException( "expected stage [" + Stage.FINALIZE + " || " + Stage.DONE + "]; " + "but current stage is [" + stage + "]" ); } } }	i think we need to also support going back to init in case the entire recovery fails and we end up retrying in recoveryresponsehandler.handleexception? at least i am not sure we cannot get into that situation. also, if we need the remotetranslogset boolean, we need to also reset that in that situation.
boolean dispatchRequest(final RestRequest request, final RestChannel channel, final NodeClient client, final Optional<RestHandler> mHandler) throws Exception { final int contentLength = request.hasContent() ? request.content().length() : 0; RestChannel responseChannel = channel; // Indicator of whether a response was sent or not boolean requestHandled; if (contentLength > 0 && mHandler.map(h -> hasContentType(request, h) == false).orElse(false)) { sendContentTypeErrorMessage(request, channel); requestHandled = true; } else if (contentLength > 0 && mHandler.map(h -> h.supportsContentStream()).orElse(false) && request.getXContentType() != XContentType.JSON && request.getXContentType() != XContentType.SMILE) { channel.sendResponse(BytesRestResponse.createSimpleErrorResponse(channel, RestStatus.NOT_ACCEPTABLE, "Content-Type [" + request.getXContentType() + "] does not support stream parsing. Use JSON or SMILE instead")); requestHandled = true; } else if (mHandler.isPresent()) { try { if (canTripCircuitBreaker(mHandler)) { inFlightRequestsBreaker(circuitBreakerService).addEstimateBytesAndMaybeBreak(contentLength, "<http_request>"); } else { inFlightRequestsBreaker(circuitBreakerService).addWithoutBreaking(contentLength); } // iff we could reserve bytes for the request we need to send the response also over this channel responseChannel = new ResourceHandlingHttpChannel(channel, circuitBreakerService, contentLength); final RestHandler wrappedHandler = mHandler.map(h -> handlerWrapper.apply(h)).get(); wrappedHandler.handleRequest(request, responseChannel, client); requestHandled = true; } catch (Exception e) { responseChannel.sendResponse(new BytesRestResponse(responseChannel, e)); // We "handled" the request by returning a response, even though it was an error requestHandled = true; } } else { // Get the map of matching handlers for a request, for the full set of HTTP methods. final Set<RestRequest.Method> validMethodSet = getValidHandlerMethodSet(request); try { // this will throws an exception if the method is invalid final RestRequest.Method requestMethod = request.method(); if (validMethodSet.size() > 0 && validMethodSet.contains(requestMethod) == false && requestMethod != RestRequest.Method.OPTIONS) { // If an alternative handler for an explicit path is registered to a // different HTTP method than the one supplied - return a 405 Method // Not Allowed error. handleUnsupportedHttpMethod(request, channel, validMethodSet, null); requestHandled = true; } else if (validMethodSet.contains(requestMethod) == false && (requestMethod == RestRequest.Method.OPTIONS)) { handleOptionsRequest(request, channel, validMethodSet); requestHandled = true; } else { requestHandled = false; } } catch (IllegalArgumentException e) { handleUnsupportedHttpMethod(request, channel, validMethodSet, e); requestHandled = true; } } // Return true if the request was handled, false otherwise. return requestHandled; } /** * If a request contains content, this method will return {@code true} if the {@code Content-Type} header is present, matches an * {@link XContentType}	more of an optional question/suggestion: can't we just do this a lot simpler by simply invoking this method upstream in org.elasticsearch.rest.restcontroller#tryallhandlers even if it means that we have to duplicate the call to getvalidhandlermethodset? (we'd save the whole change in this method and one level of indent)
public void testInvalidSelfCrossingPolygon() { PolygonBuilder builder = new PolygonBuilder(new CoordinatesBuilder() .coordinate(0, 0) .coordinate(0, 2) .coordinate(1, 1.9) .coordinate(0.5, 1.8) .coordinate(1.5, 1.8) .coordinate(1, 1.9) .coordinate(2, 2) .coordinate(2, 0) .coordinate(0, 0) ); Exception e = expectThrows(InvalidShapeException.class, () -> builder.close().buildS4J()); assertThat(e.getMessage(), containsString("Self-intersection at or near point [")); assertThat(e.getMessage(), not(containsString("NaN"))); }	this is already tested in geometryindexertests
public void testTokenize() { BertTokenizer tokenizer = BertTokenizer.builder( Arrays.asList("Elastic", "##search", "fun"), new BertTokenization(null, false, null) ).build(); TokenizationResult.Tokenization tokenization = tokenizer.tokenize("Elasticsearch fun"); assertThat(tokenization.getTokens(), arrayContaining("Elastic", "##search", "fun")); assertArrayEquals(new int[] {0, 1, 2}, tokenization.getTokenIds()); assertArrayEquals(new int[] {0, 0, 1}, tokenization.getTokenMap()); }	please add tests for the tokenize(string seq1, string seq2) overload
static Request putRepository(PutRepositoryRequest putRepositoryRequest) throws IOException { String endpoint = new EndpointBuilder().addPathPart("_snapshot").addPathPart(putRepositoryRequest.name()).build(); Request request = new Request(HttpPut.METHOD_NAME, endpoint); Params parameters = new Params(request); parameters.withMasterTimeout(putRepositoryRequest.masterNodeTimeout()); parameters.withTimeout(putRepositoryRequest.timeout()); request.setEntity(createEntity(putRepositoryRequest, REQUEST_BODY_CONTENT_TYPE)); return request; }	i see in the corresponding rest action that we also support the verify parameter. i guess we should support it here too?
public void testPutRepository() throws IOException { String repository = "repo"; PutRepositoryRequest putRepositoryRequest = new PutRepositoryRequest(repository); putRepositoryRequest.type(FsRepository.TYPE); putRepositoryRequest.settings("{\\\\"location\\\\": \\\\".\\\\"}", XContentType.JSON); Request request = RequestConverters.putRepository(putRepositoryRequest); assertThat("/_snapshot/" + repository, equalTo(request.getEndpoint())); assertThat(HttpPut.METHOD_NAME, equalTo(request.getMethod())); assertToXContentBody(putRepositoryRequest, request.getEntity()); }	would it be possible to randomize the values that the request holds like in other tests?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("name", name); builder.field("type", type); builder.startObject("settings"); settings.toXContent(builder, params); builder.endObject(); builder.endObject(); return builder; }	can you add a unit test for this method?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); writeAcknowledged(out); }	can you add a test for this class although very simple?
@Override public QB readFrom(StreamInput in) throws IOException { QB emptyBuilder = createBuilder(in.readString(), in.readGenericValue()); emptyBuilder.boost(in.readFloat()); emptyBuilder.queryName(in.readOptionalString()); return emptyBuilder; }	i wonder if we should use the setters in deserialization if we write the fields directly in the serialization. this works, but we already had cases with the internal conversion of bytesref in the terms/rangequery where this might be trappy. otherwise, should be also use getters in writeto()?
public void enterEveryRule(ParserRuleContext ctx) { if (inDetected(ctx)) { insideIn = true; } // Skip PrimaryExpressionContext for IN as it's not visited on exit due to // the grammar's peculiarity rule with "predicated" and "predicate". // Also skip the Identifiers as they are "cheap". if (ctx.getClass() != SqlBaseParser.UnquoteIdentifierContext.class && ctx.getClass() != SqlBaseParser.QuoteIdentifierContext.class && ctx.getClass() != SqlBaseParser.BackQuotedIdentifierContext.class && (insideIn == false || ctx.getClass() != SqlBaseParser.PrimaryExpressionContext.class)) { int currentDepth = depthCounts.putOrAdd(ctx.getClass().getSimpleName(), (short) 1, (short) 1); if (currentDepth > MAX_RULE_DEPTH) { throw new ParsingException(source(ctx), "SQL statement too large; " + "halt parsing to prevent memory errors (stopped at depth {})", MAX_RULE_DEPTH); } } super.enterEveryRule(ctx); }	can you import static all these classes to make the code a bit less crowded?
public void parse(ParseContext context) throws IOException { try { parseCreateField(context); } catch (Exception e) { String valuePreview = ""; try { XContentParser parser = context.parser(); Object complexValue = AbstractXContentParser.readValue(parser, HashMap::new); if (complexValue == null) { valuePreview = "null"; } else { valuePreview = complexValue.toString(); } } catch (Exception innerException) { throw new MapperParsingException("failed to parse field [{}] of type [{}] in document with id '{}'. " + "Could not parse field value preview,", e, fieldType().name(), fieldType().typeName(), context.sourceToParse().id()); } throw new MapperParsingException("failed to parse field [{}] of type [{}] in document with id '{}'. " + "Preview of field's value: '{}'", e, fieldType().name(), fieldType().typeName(), context.sourceToParse().id(), valuePreview); } multiFields.parse(this, context); }	i can see how having two methods is not fantastic, and why you had done it differently before. i had envisioned script as a member of fieldmapper directly, but we are going to see if that is possible once we add support for script to other mappers. the type of the consumer will make it possibly harder to share the impl but we'll see. i am happy though with the execute method, i find it much clearer than returning an executor like we had before, because it is evident what it does and easier to trace.
FieldTypeLookup indexTimeLookup() { return indexTimeLookup; }	it still bugs me slightly that we end up exposing this. thinking of alternatives, we could move a portion of parsecontext#executeindextimescripts to mappinglookup and rather expose a method that returns a map<string, consumer<leafreadercontext>> but i am not convinced this is a good idea either ;) maybe you have other ideas
@Override public CollapseType collapseType() { return CollapseType.NUMERIC; } } private final Builder builder; private final NumberType type; private final boolean indexed; private final boolean hasDocValues; private final boolean stored; private final Explicit<Boolean> ignoreMalformed; private final Explicit<Boolean> coerce; private final Number nullValue; private final FieldValues<Number> scriptValues; private final String onScriptError; private final boolean ignoreMalformedByDefault; private final boolean coerceByDefault; private NumberFieldMapper( String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder) { super(simpleName, mappedFieldType, multiFields, copyTo); this.type = builder.type; this.indexed = builder.indexed.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); this.stored = builder.stored.getValue(); this.ignoreMalformed = builder.ignoreMalformed.getValue(); this.coerce = builder.coerce.getValue(); this.nullValue = builder.nullValue.getValue(); this.ignoreMalformedByDefault = builder.ignoreMalformed.getDefaultValue().value(); this.coerceByDefault = builder.coerce.getDefaultValue().value(); this.scriptValues = builder.mapperScript(); this.onScriptError = builder.onScriptError.get(); this.builder = builder; } boolean coerce() { return coerce.value(); } boolean ignoreMalformed() { return ignoreMalformed.value(); } String onScriptError() { return onScriptError; } @Override public NumberFieldType fieldType() { return (NumberFieldType) super.fieldType(); } @Override protected String contentType() { return fieldType().type.typeName(); } @Override protected void parseCreateField(ParseContext context) throws IOException { if (this.scriptValues != null) { throw new IllegalArgumentException("Cannot index data directly into a field with a [script] parameter"); } XContentParser parser = context.parser(); Object value; Number numericValue = null; if (context.externalValueSet()) { value = context.externalValue(); } else if (parser.currentToken() == Token.VALUE_NULL) { value = null; } else if (coerce.value() && parser.currentToken() == Token.VALUE_STRING && parser.textLength() == 0) { value = null; } else { try { numericValue = fieldType().type.parse(parser, coerce.value()); } catch (InputCoercionException | IllegalArgumentException | JsonParseException e) { if (ignoreMalformed.value() && parser.currentToken().isValue()) { context.addIgnoredField(mappedFieldType.name()); return; } else { throw e; } } value = numericValue; } if (value == null) { value = nullValue; } if (value == null) { return; } if (numericValue == null) { numericValue = fieldType().type.parse(value, coerce.value()); } indexValue(context, numericValue); } private void indexValue(ParseContext context, Number numericValue) { context.doc().addAll(fieldType().type.createFields(fieldType().name(), numericValue, indexed, hasDocValues, stored)); if (hasDocValues == false && (stored || indexed)) { createFieldNamesField(context); } } @Override public boolean hasIndexTimeScript() { return this.scriptValues != null; } @Override public void executeIndexTimeScript(SearchLookup searchLookup, LeafReaderContext readerContext, ParseContext parseContext) { assert this.scriptValues != null; try { this.scriptValues.valuesForDoc(searchLookup, readerContext, 0, value -> indexValue(parseContext, value)); } catch (Exception e) { if ("ignore".equals(onScriptError)) { parseContext.addIgnoredField(name()); } else { throw new MapperParsingException("Error executing script on field [" + name() + "]", e); } } }	nit: i would add the docid as an argument here, even if it will always be zero, just because the other side is the one that knows about what type of reader there is and that it holds a single document.
@Override public FieldValues<Number> compile(String fieldName, Script script, ScriptCompiler compiler) { DoubleFieldScript.Factory scriptFactory = compiler.compile(script, DoubleFieldScript.CONTEXT); return (lookup, ctx, doc, consumer) -> { DoubleFieldScript s = scriptFactory .newFactory(fieldName, script.getParams(), lookup) .newInstance(ctx); s.runForDoc(doc); double[] vs = s.values(); for (int i = 0; i < s.count(); i++) { consumer.accept(vs[i]); } }; }	i asked before if the bit from runfordoc till the end can be a new method exposed by the script class, in the effort of consolidating how field script classes expose their values. do you have thoughts on this? we could also do it later
@Override public boolean equals(Object obj) { return super.equals(obj) && Objects.equals(cursor, ((SqlQueryRequest) obj).cursor) && Objects.equals(columnar, ((SqlQueryRequest) obj).columnar) && fieldMultiValueLeniency == ((SqlQueryRequest) obj).fieldMultiValueLeniency && indexIncludeFrozen == ((SqlQueryRequest) obj).indexIncludeFrozen && binaryCommunication == ((SqlQueryRequest) obj).binaryCommunication && Objects.equals(waitForCompletionTimeout, ((SqlQueryRequest) obj).waitForCompletionTimeout) && keepOnCompletion == ((SqlQueryRequest) obj).keepOnCompletion && Objects.equals(keepAlive, ((SqlQueryRequest) obj).keepAlive) && Objects.equals(allowPartialSearchResults, ((SqlQueryRequest) obj).allowPartialSearchResults); }	you can do direct comparison == and move this higher up (along with keeponcompletion) since they're cheaper to execute than a method invocation.
private boolean head(String path, long timeoutInMs) throws SQLException { ConnectionConfiguration pingCfg = new ConnectionConfiguration( cfg.baseUri(), cfg.connectionString(), cfg.validateProperties(), cfg.binaryCommunication(), cfg.connectTimeout(), timeoutInMs, cfg.queryTimeout(), cfg.pageTimeout(), cfg.pageSize(), cfg.authUser(), cfg.authPass(), cfg.sslConfig(), cfg.proxyConfig(), cfg.allowPartialSearchResults() ); try { return java.security.AccessController.doPrivileged( (PrivilegedAction<Boolean>) () -> JreHttpUrlConnection.http(path, "error_trace", pingCfg, JreHttpUrlConnection::head) ); } catch (ClientException ex) { throw new SQLException("Cannot ping server", ex); } } @SuppressWarnings({ "removal" }	no reason to do a partial request on a head request - that's used just to check if the server is alive; it doesn't send any params.
@Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } if (super.equals(o) == false) { return false; } SqlQueryRequest that = (SqlQueryRequest) o; return fetchSize == that.fetchSize && Objects.equals(query, that.query) && Objects.equals(params, that.params) && Objects.equals(zoneId, that.zoneId) && Objects.equals(catalog, that.catalog) && Objects.equals(requestTimeout, that.requestTimeout) && Objects.equals(pageTimeout, that.pageTimeout) && Objects.equals(columnar, that.columnar) && Objects.equals(cursor, that.cursor) && fieldMultiValueLeniency == that.fieldMultiValueLeniency && indexIncludeFrozen == that.indexIncludeFrozen && Objects.equals(binaryCommunication, that.binaryCommunication) && Objects.equals(waitForCompletionTimeout, that.waitForCompletionTimeout) && keepOnCompletion == that.keepOnCompletion && Objects.equals(keepAlive, that.keepAlive) && Objects.equals(allowPartialSearchResults, that.allowPartialSearchResults); }	same comment as before regarding direct comparisons on primitives.
private void checkResponseDestination(Response response) { final String asc = getSpConfiguration().getAscUrl(); if (response.isSigned() && asc.equals(response.getDestination()) == false) { throw samlException("SAML response " + response.getID() + " is for destination " + response.getDestination() + " but this realm uses " + asc); } }	i would prefer that we continue to reject _present-but-incorrect_ destinations even for unsigned messages. so this should be more like if (asc.equals(response.getdestination()) == false) { if(response.issigned() || strings.hastext(response.getdestination())) { // ...
public void cancelTimeout() { if (timeoutHandler != null) { timeoutHandler.cancel(); } } } static class DirectResponseChannel implements TransportChannel { final private String action; final private long requestId; final TransportServiceAdapter adapter; final ThreadPool threadPool; public DirectResponseChannel(String action, long requestId, TransportServiceAdapter adapter, ThreadPool threadPool) { this.action = action; this.requestId = requestId; this.adapter = adapter; this.threadPool = threadPool; } @Override public String action() { return action; } @Override public void sendResponse(TransportResponse response) throws IOException { sendResponse(response, TransportResponseOptions.EMPTY); } @Override public void sendResponse(final TransportResponse response, TransportResponseOptions options) throws IOException { final TransportResponseHandler handler = adapter.onResponseReceived(requestId); // ignore if its null, the adapter logs it if (handler != null) { final String executor = handler.executor(); if (ThreadPool.Names.SAME.equals(executor)) { processResponse(handler, response); } else { threadPool.executor(executor).execute(new Runnable() { @SuppressWarnings({"unchecked"}) @Override public void run() { processResponse(handler, response); } }); } } } protected void processResponse(TransportResponseHandler handler, TransportResponse response) { try { handler.handleResponse(response); } catch (Throwable e) { handler.handleException(new ResponseHandlerFailureTransportException(e)); } } @Override public void sendResponse(Throwable error) throws IOException { final TransportResponseHandler handler = adapter.onResponseReceived(requestId); // ignore if its null, the adapter logs it if (handler != null) { if (!(error instanceof RemoteTransportException)) { error = new RemoteTransportException(error.getMessage(), error); } final RemoteTransportException rtx = (RemoteTransportException) error; final String executor = handler.executor(); if (ThreadPool.Names.SAME.equals(executor)) { processException(handler, rtx); } else { threadPool.executor(handler.executor()).execute(new Runnable() { @SuppressWarnings({"unchecked"}) @Override public void run() { processException(handler, rtx); } }); } } } protected void processException(final TransportResponseHandler handler, final RemoteTransportException rtx) { try { handler.handleException(rtx); } catch (Throwable e) { handler.handleException(new ResponseHandlerFailureTransportException(e)); } }	can we pass to the channel the executor the transportrequesthandler is executing on, and on top of same, if its the same as the executor the request is executing on, we don't need to fork it again?
public void cancelTimeout() { if (timeoutHandler != null) { timeoutHandler.cancel(); } } } static class DirectResponseChannel implements TransportChannel { final private String action; final private long requestId; final TransportServiceAdapter adapter; final ThreadPool threadPool; public DirectResponseChannel(String action, long requestId, TransportServiceAdapter adapter, ThreadPool threadPool) { this.action = action; this.requestId = requestId; this.adapter = adapter; this.threadPool = threadPool; } @Override public String action() { return action; } @Override public void sendResponse(TransportResponse response) throws IOException { sendResponse(response, TransportResponseOptions.EMPTY); } @Override public void sendResponse(final TransportResponse response, TransportResponseOptions options) throws IOException { final TransportResponseHandler handler = adapter.onResponseReceived(requestId); // ignore if its null, the adapter logs it if (handler != null) { final String executor = handler.executor(); if (ThreadPool.Names.SAME.equals(executor)) { processResponse(handler, response); } else { threadPool.executor(executor).execute(new Runnable() { @SuppressWarnings({"unchecked"}) @Override public void run() { processResponse(handler, response); } }); } } } protected void processResponse(TransportResponseHandler handler, TransportResponse response) { try { handler.handleResponse(response); } catch (Throwable e) { handler.handleException(new ResponseHandlerFailureTransportException(e)); } } @Override public void sendResponse(Throwable error) throws IOException { final TransportResponseHandler handler = adapter.onResponseReceived(requestId); // ignore if its null, the adapter logs it if (handler != null) { if (!(error instanceof RemoteTransportException)) { error = new RemoteTransportException(error.getMessage(), error); } final RemoteTransportException rtx = (RemoteTransportException) error; final String executor = handler.executor(); if (ThreadPool.Names.SAME.equals(executor)) { processException(handler, rtx); } else { threadPool.executor(handler.executor()).execute(new Runnable() { @SuppressWarnings({"unchecked"}) @Override public void run() { processException(handler, rtx); } }); } } } protected void processException(final TransportResponseHandler handler, final RemoteTransportException rtx) { try { handler.handleException(rtx); } catch (Throwable e) { handler.handleException(new ResponseHandlerFailureTransportException(e)); } }	same as above for non exception case
private void runExpectHeaderTest( final Settings settings, final String expectation, final int contentLength, final HttpResponseStatus expectedStatus) throws InterruptedException { final HttpServerTransport.Dispatcher dispatcher = new HttpServerTransport.Dispatcher() { @Override public void dispatchRequest(RestRequest request, RestChannel channel, ThreadContext threadContext) { channel.sendResponse(new BytesRestResponse(OK, BytesRestResponse.TEXT_CONTENT_TYPE, new BytesArray("done"))); } @Override public void dispatchBadRequest(RestChannel channel, ThreadContext threadContext, Throwable cause) { logger.error(new ParameterizedMessage("Unexpected bad request [{}]", requestToString(channel.request())), cause); throw new AssertionError(); } }; try (Netty4HttpServerTransport transport = new Netty4HttpServerTransport(settings, networkService, bigArrays, threadPool, xContentRegistry(), dispatcher, clusterSettings, new SharedGroupFactory(settings))) { transport.start(); final TransportAddress remoteAddress = randomFrom(transport.boundAddress().boundAddresses()); try (Netty4HttpClient client = new Netty4HttpClient()) { final FullHttpRequest request = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.POST, "/"); request.headers().set(HttpHeaderNames.EXPECT, expectation); HttpUtil.setContentLength(request, contentLength); final FullHttpResponse response = client.post(remoteAddress.address(), request); try { assertThat(response.status(), equalTo(expectedStatus)); if (expectedStatus.equals(HttpResponseStatus.CONTINUE)) { final FullHttpRequest continuationRequest = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.POST, "/", Unpooled.EMPTY_BUFFER); final FullHttpResponse continuationResponse = client.post(remoteAddress.address(), continuationRequest); try { assertThat(continuationResponse.status(), is(HttpResponseStatus.OK)); assertThat( new String(ByteBufUtil.getBytes(continuationResponse.content()), StandardCharsets.UTF_8), is("done") ); } finally { continuationResponse.release(); } } } finally { response.release(); } } } }	maybe prefix the messages --> like we do for other test logging?
public static boolean isInetAddress(String ipString) { return ipStringToBytes(ipString) != null; }	let's add a comment here that explains that we are doing this check to filter out strings that end in % and have an empty scope id.
private static <T, E extends Exception> T withPassword(String description, char[] password, Terminal terminal, CheckedFunction<char[], T, E> body) throws E { if (password == null) { char[] promptedValue = terminal.readSecret("Enter password for " + description + " : "); boolean long_size = true; while(long_size){ if(promptedValue.length > MAX_SIZE_PW_BUFFER_OLDER_SSL){ if(terminal.promptYesNo("Your password exceeds a length of 50 characters. Versions of OpenSSL older than 1.1.0 will not be able to use this certificate. Do you want to continue?", true)) long_size = false; else{ promptedValue = terminal.readSecret("Enter password for " + description + " : "); if(promptedValue.length <= MAX_SIZE_PW_BUFFER_OLDER_SSL) long_size = false; } } else{ long_size = false; } }//close while try { return body.apply(promptedValue); } finally { Arrays.fill(promptedValue, (char) 0); } } else { return body.apply(password); } }	the formatting of thise code (spaces around brackets etc) is different to the way the rest of this source code is formatted. by the time we get to the end of this review process, we'll want to have it formatted in the same style as the surrounding code.
private static <T, E extends Exception> T withPassword(String description, char[] password, Terminal terminal, CheckedFunction<char[], T, E> body) throws E { if (password == null) { char[] promptedValue = terminal.readSecret("Enter password for " + description + " : "); boolean long_size = true; while(long_size){ if(promptedValue.length > MAX_SIZE_PW_BUFFER_OLDER_SSL){ if(terminal.promptYesNo("Your password exceeds a length of 50 characters. Versions of OpenSSL older than 1.1.0 will not be able to use this certificate. Do you want to continue?", true)) long_size = false; else{ promptedValue = terminal.readSecret("Enter password for " + description + " : "); if(promptedValue.length <= MAX_SIZE_PW_BUFFER_OLDER_SSL) long_size = false; } } else{ long_size = false; } }//close while try { return body.apply(promptedValue); } finally { Arrays.fill(promptedValue, (char) 0); } } else { return body.apply(password); } }	the withpassword method is called every time we need a password, even if it's being used to _read_ a certificate file. in that case we don't want to print this warning, because that would cause additional output (and an additional prompt) for simple things like reading a ca file that has a long password. we need to only perform this check/warning if the password is being applied to a new file. i'm ok if we want get rid of the promptyesno and just print out a warning, but we only want to do either of them when we know the password is being used to _write_ a file.
*/ public long waitForDocs(final long numDocs, @Nullable final BackgroundIndexer indexer) throws Exception { final AtomicLong lastKnownCount = new AtomicLong(-1); // indexing threads can wait for up to ~1m before retrying when they first try to index into a shard which is not STARTED. final long maxWaitTimeMs = Math.max(90 * 1000, 200 * numDocs); assertBusy( () -> { if (indexer != null) { lastKnownCount.set(indexer.totalIndexedDocs()); } // Have we managed to count enough docs yet? if (lastKnownCount.get() < numDocs) { try { long count = client().prepareSearch() .setTrackTotalHits(true) .setSize(0) .setQuery(matchAllQuery()) .get() .getHits().getTotalHits().value; if (count == lastKnownCount.get()) { // no progress - try to refresh for the next time client().admin().indices().prepareRefresh().get(); } lastKnownCount.set(count); } catch (Exception e) { // count now acts like search and barfs if all shards failed... logger.debug("failed to executed count", e); throw e; } } if (logger.isDebugEnabled()) { if (lastKnownCount.get() < numDocs) { logger.debug("[{}] docs indexed. waiting for [{}]", lastKnownCount.get(), numDocs); } else { logger.debug("[{}] docs visible for search (needed [{}])", lastKnownCount.get(), numDocs); } } assertThat(lastKnownCount.get(), greaterThanOrEqualTo(numDocs)); }, maxWaitTimeMs, TimeUnit.MILLISECONDS ); return lastKnownCount.get(); }	the test here was >= before and i couldn't see why that would be correct. please point out what i've missed.
public void testValidateShardLimit() { int nodesInCluster = randomIntBetween(2,90); ClusterShardLimitIT.ShardCounts counts = forDataNodeCount(nodesInCluster); Settings clusterSettings = Settings.builder() .put(MetaData.SETTING_CLUSTER_MAX_SHARDS_PER_NODE.getKey(), counts.getShardsPerNode()) .build(); ClusterState state = createClusterForShardLimitTest(nodesInCluster, counts.getFirstIndexShards(), counts.getFirstIndexReplicas(), counts.getFailingIndexShards(), counts.getFailingIndexReplicas(), clusterSettings); Index[] indices = Arrays.stream(state.metaData().indices().values().toArray(IndexMetaData.class)) .map(IndexMetaData::getIndex) .collect(Collectors.toList()) .toArray(new Index[2]); int totalShards = counts.getFailingIndexShards() * (1 + counts.getFailingIndexReplicas()); int currentShards = counts.getFirstIndexShards() * (1 + counts.getFirstIndexReplicas()); int maxShards = counts.getShardsPerNode() * nodesInCluster; ValidationException exception = expectThrows(ValidationException.class, () -> MetaDataIndexStateService.validateShardLimit(state, indices)); assertEquals("Validation Failed: 1: this action would add [" + totalShards + "] total shards, but this cluster currently has [" + currentShards + "]/[" + maxShards + "] maximum shards open;", exception.getMessage()); }	this change seems unrelated to types removal?
public Builder add(final ClusterPrivilege clusterPrivilege, final Set<String> allowedActionPatterns, final Set<String> excludeActionPatterns, final Predicate<TransportRequest> requestPredicate) { final Automaton allowedAutomaton = Automatons.patterns(allowedActionPatterns); final Automaton excludedAutomaton = Automatons.patterns(excludeActionPatterns); final Automaton actionAutomaton = Automatons.minusAndMinimize(allowedAutomaton, excludedAutomaton); return add(clusterPrivilege, new ActionRequestBasedPermissionCheck(clusterPrivilege, actionAutomaton, requestPredicate)); }	i'm curious about why we need the exclude here. excluding is a pretty rare case, and it complicates this api. is it really valuable?
public Builder add(final ClusterPrivilege clusterPrivilege, final Set<String> allowedActionPatterns, final Set<String> excludeActionPatterns, final Predicate<TransportRequest> requestPredicate) { final Automaton allowedAutomaton = Automatons.patterns(allowedActionPatterns); final Automaton excludedAutomaton = Automatons.patterns(excludeActionPatterns); final Automaton actionAutomaton = Automatons.minusAndMinimize(allowedAutomaton, excludedAutomaton); return add(clusterPrivilege, new ActionRequestBasedPermissionCheck(clusterPrivilege, actionAutomaton, requestPredicate)); }	if we do keep the exclude, can you add a special case for excludeactionpatterns being empty? it's a common case and will make the code more obviously correct (because automatons over empty patterns can have surprising behaviours).
public static Aggregator adaptIntoFiltersOrNull( String name, AggregatorFactories factories, ValuesSourceConfig valuesSourceConfig, InternalRange.Factory<?, ?> rangeFactory, Range[] ranges, boolean keyed, SearchContext context, Aggregator parent, CardinalityUpperBound cardinality, Map<String, Object> metadata ) throws IOException { if (valuesSourceConfig.fieldType() == null) { return null; } if (false == valuesSourceConfig.fieldType().isSearchable()) { return null; } if (valuesSourceConfig.missing() != null) { return null; } if (valuesSourceConfig.script() != null) { return null; } // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query? if (valuesSourceConfig.fieldType() instanceof DateFieldType && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) { // We don't generate sensible Queries for nanoseconds. return null; } String[] keys = new String[ranges.length]; Query[] filters = new Query[ranges.length]; for (int i = 0; i < ranges.length; i++) { keys[i] = Integer.toString(i); /* * Use the native format on the field rather than the one provided * on the valuesSourceConfig because the format on the field is what * we parse. With https://github.com/elastic/elasticsearch/pull/63692 * we can just cast to a long here and it'll be taken as millis. */ DocValueFormat format = valuesSourceConfig.fieldType().docValueFormat(null, null); filters[i] = valuesSourceConfig.fieldType() .rangeQuery( ranges[i].from == Double.NEGATIVE_INFINITY ? null : format.format(ranges[i].from), ranges[i].to == Double.POSITIVE_INFINITY ? null : format.format(ranges[i].to), true, false, ShapeRelation.CONTAINS, null, null, context.getQueryShardContext() ); } RangeAggregator.FromFilters<?> fromFilters = new RangeAggregator.FromFilters<>( parent, factories, subAggregators -> FiltersAggregator.build( name, subAggregators, keys, filters, false, null, context, parent, cardinality, metadata ), valuesSourceConfig.format(), ranges, keyed, rangeFactory ); if (false == ((FiltersAggregator) fromFilters.delegate()).collectsInFilterOrder()) { return null; } return fromFilters; }	just to say it out loud: we don't care that we just built and then threw away an aggregator here, because aggregator creation time is not highly sensitive to performance hits, correct?
@Override public int hashCode() { return location().hashCode(); }	extra new line :-)
public BitSet columnMask(List<Attribute> columns) { BitSet mask = new BitSet(fields.size()); for (int columnIndex = 0; columnIndex < columns.size(); columnIndex++) { Attribute column = columns.get(columnIndex); Attribute alias = aliases.get(column); // find the column index int index = -1; ExpressionId id = column instanceof AggregateFunctionAttribute ? ((AggregateFunctionAttribute) column).innerId() : column.id(); ExpressionId aliasId = alias != null ? (alias instanceof AggregateFunctionAttribute ? ((AggregateFunctionAttribute) alias) .innerId() : alias.id()) : null; for (int i = 0; i < fields.size(); i++) { Tuple<FieldExtraction, ExpressionId> tuple = fields.get(i); if (tuple.v2().equals(id) || (aliasId != null && tuple.v2().equals(aliasId))) { index = i; break; } } if (index > -1) { mask.set(index); } else { throw new SqlIllegalArgumentException("Cannot resolve field extractor index for column [{}]", column); } } return mask; }	why do you need this? i don't see columnindex to be used.
public static void validateSequentiallyOrderedParentAggs(AggregatorFactory<?> parent, String type, String name) { if (!(parent instanceof HistogramAggregatorFactory || parent instanceof DateHistogramAggregatorFactory || parent instanceof AutoDateHistogramAggregatorFactory)) { throw new IllegalStateException( type + " aggregation [" + name + "] must have a histogram, date_histogram or auto_date_histogram as parent"); } if (parent instanceof HistogramAggregatorFactory) { HistogramAggregatorFactory histoParent = (HistogramAggregatorFactory) parent; if (histoParent.minDocCount() != 0) { throw new IllegalStateException("parent histogram of " + type + " aggregation [" + name + "] must have min_doc_count of 0"); } } else if (parent instanceof DateHistogramAggregatorFactory) { DateHistogramAggregatorFactory histoParent = (DateHistogramAggregatorFactory) parent; if (histoParent.minDocCount() != 0) { throw new IllegalStateException("parent histogram of " + type + " aggregation [" + name + "] must have min_doc_count of 0"); } } }	++ this arrangement is cleaner than an empty if, thanks! one super tiny style nit: could you change it from a !(...) to an (...) == false? we try not to use the negation operator because it's easy to misread, whereas the more verbose == false is hard to miss.
void validateCacheSettings(Settings settings) { boolean useContext = SCRIPT_GENERAL_MAX_COMPILATIONS_RATE_SETTING.get(settings).equals(USE_CONTEXT_RATE_VALUE); if (useContext) { deprecationLogger.warn(DeprecationCategory.SCRIPTING, "scripting-context-cache", USE_CONTEXT_RATE_KEY_DEPRECATION_MESSAGE); } List<Setting.AffixSetting<?>> affixes = Arrays.asList(SCRIPT_MAX_COMPILATIONS_RATE_SETTING, SCRIPT_CACHE_EXPIRE_SETTING, SCRIPT_CACHE_SIZE_SETTING); List<String> customRates = new ArrayList<>(); List<String> keys = new ArrayList<>(); for (Setting.AffixSetting<?> affix: affixes) { for (String context: affix.getAsMap(settings).keySet()) { String s = affix.getConcreteSettingForNamespace(context).getKey(); if (contexts.containsKey(context) == false) { throw new IllegalArgumentException("Context [" + context + "] doesn't exist for setting [" + s + "]"); } keys.add(s); if (affix.equals(SCRIPT_MAX_COMPILATIONS_RATE_SETTING)) { customRates.add(s); } } } if (useContext == false && keys.isEmpty() == false) { keys.sort(Comparator.naturalOrder()); throw new IllegalArgumentException("Context cache settings [" + String.join(", ", keys) + "] requires [" + SCRIPT_GENERAL_MAX_COMPILATIONS_RATE_SETTING.getKey() + "] to be [" + USE_CONTEXT_RATE_KEY + "]"); } if (SCRIPT_DISABLE_MAX_COMPILATIONS_RATE_SETTING.get(settings)) { if (customRates.size() > 0) { customRates.sort(Comparator.naturalOrder()); throw new IllegalArgumentException("Cannot set custom context compilation rates [" + String.join(", ", customRates) + "] if compile rates disabled via [" + SCRIPT_DISABLE_MAX_COMPILATIONS_RATE_SETTING.getKey() + "]"); } if (useContext == false && SCRIPT_GENERAL_MAX_COMPILATIONS_RATE_SETTING.exists(settings)) { throw new IllegalArgumentException("Cannot set custom general compilation rates [" + SCRIPT_GENERAL_MAX_COMPILATIONS_RATE_SETTING.getKey() + "] to [" + SCRIPT_GENERAL_MAX_COMPILATIONS_RATE_SETTING.get(settings) + "] if compile rates disabled via [" + SCRIPT_DISABLE_MAX_COMPILATIONS_RATE_SETTING.getKey() + "]"); } } }	shouldn't this branch not be an error anymore? the setting is the general one, not per context?
static DeprecationIssue checkScriptContextCompilationsRateLimitSetting(final Settings settings, final PluginsAndModules pluginsAndModules, final ClusterState clusterState, final XPackLicenseState licenseState) { Setting.AffixSetting<?> maxSetting = ScriptService.SCRIPT_MAX_COMPILATIONS_RATE_SETTING; Set<String> contextCompilationRates = maxSetting.getAsMap(settings).keySet(); if (contextCompilationRates.isEmpty() == false) { String maxSettings = contextCompilationRates.stream().sorted().map( c -> maxSetting.getConcreteSettingForNamespace(c).getKey() ).collect(Collectors.joining(",")); return new DeprecationIssue(DeprecationIssue.Level.WARNING, String.format(Locale.ROOT, "settings [" + maxSettings + "] for context specific rate limits are deprecated and will" + " not be available in a future version." + " Use [" + ScriptService.SCRIPT_GENERAL_MAX_COMPILATIONS_RATE_SETTING.getKey() + "] to rate limit the compilation" + " of user scripts, system scripts are exempt.", maxSettings), "https://ela.st/es-deprecation-7-script-context-max-compile", String.format(Locale.ROOT, "[%s] is deprecated and will be removed in a future release", maxSettings), false, null); } return null; }	i'd edit slightly to align with the other deprecation messages: setting context-specific rate limits is deprecated. use [setting] to rate limit the compilation of user scripts. context-specific caches are no longer needed to prevent system scripts from triggering rate limits.
public void checkIdle(long inactiveTimeNS) { Engine engineOrNull = getEngineOrNull(); if (engineOrNull != null && System.nanoTime() - engineOrNull.getLastWriteNanos() >= inactiveTimeNS) { boolean wasActive = active.getAndSet(false); if (wasActive) { logger.debug("flushing shard on inactive"); threadPool.executor(ThreadPool.Names.FLUSH).execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { if (state != IndexShardState.CLOSED) { logger.warn("failed to flush shard on inactive", e); } } @Override protected void doRun() { periodicFlushMetric.inc(); flush(new FlushRequest().waitIfOngoing(false).force(false)); } }); } } }	nit: i think i prefer inc'ing the periodic flush metric before flush like you do here, but we should then do the same in afterwriteoperation for consistency. but it looks like the norm is to inc stats after the operation and therefore i think we should do so here too. it is of course only relevant if the operation fails for some reason.
public void testFlushOnInactive() throws Exception { final String indexName = "flush_on_inactive"; List<String> dataNodes = internalCluster().startDataOnlyNodes(2, Settings.builder() .put(IndexingMemoryController.SHARD_INACTIVE_TIME_SETTING.getKey(), randomTimeValue(2, 5, "s")).build()); assertAcked(client().admin().indices().prepareCreate(indexName).setSettings(Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), randomTimeValue(200, 500, "ms")) .put("index.routing.allocation.include._name", String.join(",", dataNodes)) .build())); ensureGreen(indexName); int numDocs = randomIntBetween(1, 10); for (int i = 0; i < numDocs; i++) { client().prepareIndex(indexName).setSource("f", "v").get(); } if (randomBoolean()) { internalCluster().restartNode(randomFrom(dataNodes), new InternalTestCluster.RestartCallback()); ensureGreen(indexName); } assertBusy(() -> { for (ShardStats shardStats : client().admin().indices().prepareStats(indexName).get().getShards()) { assertThat(shardStats.getStats().getTranslog().getUncommittedOperations(), equalTo(0)); } }, 30, TimeUnit.SECONDS); }	could we maybe make the setting smaller to ensure the test completes faster? maybe use: timevalue.timevaluemillis(scaledrandomintbetween(10, 2000)) instead?
public void findActiveTokensForRealm(String realmName, ActionListener<Collection<Tuple<UserToken, String>>> listener) { ensureEnabled(); final SecurityIndexManager frozenSecurityIndex = securityIndex.freeze(); if (Strings.isNullOrEmpty(realmName)) { listener.onFailure(new IllegalArgumentException("Realm name is required")); } else if (frozenSecurityIndex.indexExists() == false) { listener.onResponse(Collections.emptyList()); } else if (frozenSecurityIndex.isAvailable() == false) { listener.onFailure(frozenSecurityIndex.getUnavailableReason()); } else { final Instant now = clock.instant(); final BoolQueryBuilder boolQuery = QueryBuilders.boolQuery() .filter(QueryBuilders.termQuery("doc_type", "token")) .filter(QueryBuilders.termQuery("access_token.realm", realmName)) .filter(QueryBuilders.boolQuery() .should(QueryBuilders.boolQuery() .must(QueryBuilders.termQuery("access_token.invalidated", false)) .must(QueryBuilders.rangeQuery("access_token.user_token.expiration_time").gte(now.toEpochMilli())) ) .should(QueryBuilders.termQuery("refresh_token.invalidated", false)) ); final SearchRequest request = client.prepareSearch(SecurityIndexManager.SECURITY_INDEX_NAME) .setScroll(DEFAULT_KEEPALIVE_SETTING.get(settings)) .setQuery(boolQuery) .setVersion(false) .setSize(1000) .setFetchSource(true) .request(); final Supplier<ThreadContext.StoredContext> supplier = client.threadPool().getThreadContext().newRestorableContext(false); securityIndex.checkIndexVersionThenExecute(listener::onFailure, () -> ScrollHelper.fetchAllByEntity(client, request, new ContextPreservingActionListener<>(supplier, listener), this::parseHit)); } }	i see that scrollhelper.fetchallbyentity already wraps the listener inside a contextpreservingactionlistener.
public void checkIndexVersionThenExecute(final Consumer<Exception> consumer, final Runnable andThen) { final State indexState = this.indexState; // use a local copy so all checks execute against the same state! if (indexState.indexExists && indexState.isIndexUpToDate == false) { consumer.accept(new IllegalStateException( "Security index is not on the current version. Security features relying on the index will not be available until " + "the upgrade API is run on the security index")); } else { andThen.run(); } }	i am unsure about this method. i would have just copy-pasted the if branch to the others before the function call. just an opinion not a suggestion.
public String processCaptures( List<String> explanation, Map<String, Integer> fieldNameCountStore, Collection<String> snippets, Collection<String> prefaces, Collection<String> epilogues, Map<String, Object> mappings, Map<String, FieldStats> fieldStats, TimeoutChecker timeoutChecker ) { return super.processCaptures(explanation, fieldNameCountStore, snippets, prefaces, epilogues, null, fieldStats, timeoutChecker); } } /** * Used to check whether a single Grok pattern matches every sample message in its entirety. */ static class FullMatchGrokPatternCandidate { private final String grokPattern; private final String timeField; private final Grok grok; static FullMatchGrokPatternCandidate fromGrokPatternName(String grokPatternName, String timeField) { return new FullMatchGrokPatternCandidate("%{" + grokPatternName + "}", timeField, Grok.BUILTIN_PATTERNS); } static FullMatchGrokPatternCandidate fromGrokPatternName( String grokPatternName, String timeField, Map<String, String> grokPatternDefinitions ) { return new FullMatchGrokPatternCandidate("%{" + grokPatternName + "}", timeField, grokPatternDefinitions); } static FullMatchGrokPatternCandidate fromGrokPattern(String grokPattern, String timeField) { return new FullMatchGrokPatternCandidate(grokPattern, timeField, Grok.BUILTIN_PATTERNS); } static FullMatchGrokPatternCandidate fromGrokPattern( String grokPattern, String timeField, Map<String, String> grokPatternDefinitions ) { return new FullMatchGrokPatternCandidate(grokPattern, timeField, grokPatternDefinitions); } private FullMatchGrokPatternCandidate(String grokPattern, String timeField, Map<String, String> grokPatternDefinitions) { this.grokPattern = grokPattern; this.timeField = timeField; grok = new Grok(grokPatternDefinitions, grokPattern, TimeoutChecker.watchdog, logger::warn); } public String getTimeField() { return timeField; } public boolean matchesAll(Collection<String> sampleMessages, TimeoutChecker timeoutChecker) { for (String sampleMessage : sampleMessages) { if (grok.match(sampleMessage) == false) { return false; } timeoutChecker.check("full message Grok pattern matching"); } return true; } /** * This must only be called if {@link #matchesAll} returns <code>true</code>. * @return A tuple of (time field name, Grok string). */ public Tuple<String, String> processMatch( List<String> explanation, Collection<String> sampleMessages, Map<String, Object> mappings, Map<String, FieldStats> fieldStats, TimeoutChecker timeoutChecker ) { if (grokPattern.startsWith("%{") && grokPattern.endsWith("}")) { explanation.add( "A full message Grok pattern [" + grokPattern.substring(2, grokPattern.length() - 1) + "] looks appropriate" ); } if (mappings != null || fieldStats != null) { Map<String, Collection<String>> valuesPerField = new HashMap<>(); for (String sampleMessage : sampleMessages) { Map<String, Object> captures = timeoutChecker.grokCaptures( grok, sampleMessage, "full message Grok pattern field extraction" ); // If the pattern doesn't match then captures will be null if (captures == null) { throw new IllegalStateException("[" + grokPattern + "] does not match snippet [" + sampleMessage + "]"); } for (Map.Entry<String, Object> capture : captures.entrySet()) { String fieldName = capture.getKey(); String fieldValue = capture.getValue().toString(); valuesPerField.compute(fieldName, (k, v) -> { if (v == null) { return new ArrayList<>(Collections.singletonList(fieldValue)); } else { v.add(fieldValue); return v; } }); } } for (Map.Entry<String, Collection<String>> valuesForField : valuesPerField.entrySet()) { String fieldName = valuesForField.getKey(); Map<String, String> mapping = FileStructureUtils.guessScalarMapping( explanation, fieldName, valuesForField.getValue(), timeoutChecker ); timeoutChecker.check("mapping determination"); // Exclude the time field because that will be dropped and replaced with @timestamp if (mappings != null && fieldName.equals(timeField) == false) { mappings.put(fieldName, mapping); } if (fieldStats != null) { fieldStats.put( fieldName, FileStructureUtils.calculateFieldStats(mapping, valuesForField.getValue(), timeoutChecker) ); } } } return new Tuple<>(timeField, grokPattern); }	is it ever the case that this pattern contains regex with it as well as grok captures?
public void testManyToManyGeoPoints() throws ExecutionException, InterruptedException { /** * | q | d1 | d2 * | | | * | | | * | | | * |2 o| x | x * | | | * |1 o| x | x * |___________________________ * 1 2 3 4 5 6 7 */ assertAcked(prepareCreate("index") .addMapping("type", "{\\\\"type\\\\": {\\\\"properties\\\\": {\\\\"location\\\\": {\\\\"type\\\\": \\\\"geo_point\\\\"}}}}")); String d1 = randomBoolean() ? "{\\\\"location\\\\": [{\\\\"lat\\\\": \\\\"3\\\\", \\\\"lon\\\\": \\\\"2\\\\"}, {\\\\"lat\\\\": \\\\"4\\\\", \\\\"lon\\\\": \\\\"1\\\\"}]}" : "{\\\\"location\\\\": [{\\\\"lat\\\\": \\\\"4\\\\", \\\\"lon\\\\": \\\\"1\\\\"}, {\\\\"lat\\\\": \\\\"3\\\\", \\\\"lon\\\\": \\\\"2\\\\"}]}"; String d2 = randomBoolean() ? "{\\\\"location\\\\": [{\\\\"lat\\\\": \\\\"5\\\\", \\\\"lon\\\\": \\\\"1\\\\"}, {\\\\"lat\\\\": \\\\"6\\\\", \\\\"lon\\\\": \\\\"2\\\\"}]}" : "{\\\\"location\\\\": [{\\\\"lat\\\\": \\\\"6\\\\", \\\\"lon\\\\": \\\\"2\\\\"}, {\\\\"lat\\\\": \\\\"5\\\\", \\\\"lon\\\\": \\\\"1\\\\"}]}"; indexRandom(true, client().prepareIndex("index", "type", "d1").setSource(d1), client().prepareIndex("index", "type", "d2").setSource(d2)); ensureYellow(); GeoPoint[] q = new GeoPoint[2]; if (randomBoolean()) { q[0] = new GeoPoint(2, 1); q[1] = new GeoPoint(2, 2); } else { q[1] = new GeoPoint(2, 2); q[0] = new GeoPoint(2, 1); } SearchResponse searchResponse = client().prepareSearch() .setQuery(matchAllQuery()) .addSort(new GeoDistanceSortBuilder("location").points(q).sortMode("min").order(SortOrder.ASC).geoDistance(GeoDistance.PLANE).unit(DistanceUnit.KILOMETERS)) .execute().actionGet(); assertOrderedSearchHits(searchResponse, "d1", "d2"); assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], equalTo(GeoDistance.PLANE.calculate(2, 2, 3, 2, DistanceUnit.KILOMETERS))); assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], equalTo(GeoDistance.PLANE.calculate(2, 1, 5, 1, DistanceUnit.KILOMETERS))); searchResponse = client().prepareSearch() .setQuery(matchAllQuery()) .addSort(new GeoDistanceSortBuilder("location").points(q).sortMode("min").order(SortOrder.DESC).geoDistance(GeoDistance.PLANE).unit(DistanceUnit.KILOMETERS)) .execute().actionGet(); assertOrderedSearchHits(searchResponse, "d2", "d1"); assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], equalTo(GeoDistance.PLANE.calculate(2, 1, 5, 1, DistanceUnit.KILOMETERS))); assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], equalTo(GeoDistance.PLANE.calculate(2, 2, 3, 2, DistanceUnit.KILOMETERS))); searchResponse = client().prepareSearch() .setQuery(matchAllQuery()) .addSort(new GeoDistanceSortBuilder("location").points(q).sortMode("max").order(SortOrder.ASC).geoDistance(GeoDistance.PLANE).unit(DistanceUnit.KILOMETERS)) .execute().actionGet(); assertOrderedSearchHits(searchResponse, "d1", "d2"); assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], equalTo(GeoDistance.PLANE.calculate(2, 2, 4, 1, DistanceUnit.KILOMETERS))); assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], equalTo(GeoDistance.PLANE.calculate(2, 1, 6, 2, DistanceUnit.KILOMETERS))); searchResponse = client().prepareSearch() .setQuery(matchAllQuery()) .addSort(new GeoDistanceSortBuilder("location").points(q).sortMode("max").order(SortOrder.DESC).geoDistance(GeoDistance.PLANE).unit(DistanceUnit.KILOMETERS)) .execute().actionGet(); assertOrderedSearchHits(searchResponse, "d2", "d1"); assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], equalTo(GeoDistance.PLANE.calculate(2, 1, 6, 2, DistanceUnit.KILOMETERS))); assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], equalTo(GeoDistance.PLANE.calculate(2, 2, 4, 1, DistanceUnit.KILOMETERS))); }	or more simply: java .addmapping("type", "location", "type=geo_point")
@Override public RestChannelConsumer prepareRequest(RestRequest request, final NodeClient client) throws IOException { validateOpType(request.params().get("op_type")); request.params().put("op_type", "create"); return RestIndexAction.this.prepareRequest(request, client); }	maybe make this static and pkg private?
public Builder indexPrefixes(int minChars, int maxChars) { if (minChars > maxChars) throw new IllegalArgumentException("min_chars [" + minChars + "] must be less than max_chars [" + maxChars + "]"); if (minChars < 1) throw new IllegalArgumentException("min_chars [" + minChars + "] must be greater than zero"); this.prefixFieldType = new PrefixFieldType(name() + "._index_prefix", minChars, maxChars); fieldType().setPrefixFieldType(this.prefixFieldType); return this; }	not sure that we should let the user defines min and max or we should at least have a limit on maxchars ? we could also have sane default values (2 to 5 should be good enough for 99% of the case ?).
@Override public Query prefixQuery(String value, MultiTermQuery.RewriteMethod method, QueryShardContext context) { if (prefixFieldType == null || prefixFieldType.accept(value.length()) == false) { return super.prefixQuery(value, method, context); } return prefixFieldType.termQuery(value, context); }	should we use constant score as well if the rewrite method is null or constant_score ?
@Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { if (fielddata == false) { throw new IllegalArgumentException("Fielddata is disabled on text fields by default. Set fielddata=true on [" + name() + "] in order to load fielddata in memory by uninverting the inverted index. Note that this can however " + "use significant memory. Alternatively use a keyword field instead."); } return new PagedBytesIndexFieldData.Builder(fielddataMinFrequency, fielddataMaxFrequency, fielddataMinSegmentSize); } } private int positionIncrementGap; private PrefixFieldMapper prefixFieldMapper; protected TextFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, int positionIncrementGap, PrefixFieldMapper prefixFieldMapper, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); assert fieldType.tokenized(); assert fieldType.hasDocValues() == false; if (fieldType().indexOptions() == IndexOptions.NONE && fieldType().fielddata()) { throw new IllegalArgumentException("Cannot enable fielddata on a [text] field that is not indexed: [" + name() + "]"); } this.positionIncrementGap = positionIncrementGap; this.prefixFieldMapper = prefixFieldMapper; } @Override protected TextFieldMapper clone() { return (TextFieldMapper) super.clone(); } public int getPositionIncrementGap() { return this.positionIncrementGap; } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { final String value; if (context.externalValueSet()) { value = context.externalValue().toString(); } else { value = context.parser().textOrNull(); } if (value == null) { return; } if (fieldType().indexOptions() != IndexOptions.NONE || fieldType().stored()) { Field field = new Field(fieldType().name(), value, fieldType()); fields.add(field); if (fieldType().omitNorms()) { createFieldNamesField(context, fields); } if (prefixFieldMapper != null) { prefixFieldMapper.addField(value, fields); } } } @Override public Iterator<Mapper> iterator() { if (prefixFieldMapper == null) return super.iterator(); return Iterators.concat(super.iterator(), Collections.singleton(prefixFieldMapper).iterator()); } @Override protected String contentType() { return CONTENT_TYPE; } @Override protected void doMerge(Mapper mergeWith, boolean updateAllTypes) { super.doMerge(mergeWith, updateAllTypes); } @Override public TextFieldType fieldType() { return (TextFieldType) super.fieldType(); } @Override protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException { super.doXContentBody(builder, includeDefaults, params); doXContentAnalyzers(builder, includeDefaults); if (includeDefaults || positionIncrementGap != POSITION_INCREMENT_GAP_USE_ANALYZER) { builder.field("position_increment_gap", positionIncrementGap); } if (includeDefaults || fieldType().fielddata() != ((TextFieldType) defaultFieldType).fielddata()) { builder.field("fielddata", fieldType().fielddata()); } if (fieldType().fielddata()) { if (includeDefaults || fieldType().fielddataMinFrequency() != Defaults.FIELDDATA_MIN_FREQUENCY || fieldType().fielddataMaxFrequency() != Defaults.FIELDDATA_MAX_FREQUENCY || fieldType().fielddataMinSegmentSize() != Defaults.FIELDDATA_MIN_SEGMENT_SIZE) { builder.startObject("fielddata_frequency_filter"); if (includeDefaults || fieldType().fielddataMinFrequency() != Defaults.FIELDDATA_MIN_FREQUENCY) { builder.field("min", fieldType().fielddataMinFrequency()); } if (includeDefaults || fieldType().fielddataMaxFrequency() != Defaults.FIELDDATA_MAX_FREQUENCY) { builder.field("max", fieldType().fielddataMaxFrequency()); } if (includeDefaults || fieldType().fielddataMinSegmentSize() != Defaults.FIELDDATA_MIN_SEGMENT_SIZE) { builder.field("min_segment_size", fieldType().fielddataMinSegmentSize()); } builder.endObject(); } } if (fieldType().prefixFieldType != null) { fieldType().prefixFieldType.doXContent(builder); }	we usually use brackets even for single-line if/else statements
public void start(Settings settings, TransportService transportService, ClusterService clusterService, MetaDataIndexUpgradeService metaDataIndexUpgradeService, MetaDataUpgrader metaDataUpgrader, LucenePersistedStateFactory lucenePersistedStateFactory) { assert persistedState.get() == null : "should only start once, but already have " + persistedState.get(); if (DiscoveryNode.isMasterNode(settings) || DiscoveryNode.isDataNode(settings)) { try { PersistedState ps = lucenePersistedStateFactory.loadPersistedState((version, metadata) -> prepareInitialClusterState(transportService, clusterService, ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.get(settings)) .version(version) .metaData(upgradeMetaDataForMasterEligibleNode(metadata, metaDataIndexUpgradeService, metaDataUpgrader)) .build())); persistedState.set(ps); } catch (IOException e) { throw new ElasticsearchException("failed to load metadata", e); } } else { persistedState.set( new InMemoryPersistedState(0L, ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.get(settings)).build())); } }	nit: do we need to introduce this variable?
public void start(Settings settings, TransportService transportService, ClusterService clusterService, MetaDataIndexUpgradeService metaDataIndexUpgradeService, MetaDataUpgrader metaDataUpgrader, LucenePersistedStateFactory lucenePersistedStateFactory) { assert persistedState.get() == null : "should only start once, but already have " + persistedState.get(); if (DiscoveryNode.isMasterNode(settings) || DiscoveryNode.isDataNode(settings)) { try { PersistedState ps = lucenePersistedStateFactory.loadPersistedState((version, metadata) -> prepareInitialClusterState(transportService, clusterService, ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.get(settings)) .version(version) .metaData(upgradeMetaDataForMasterEligibleNode(metadata, metaDataIndexUpgradeService, metaDataUpgrader)) .build())); persistedState.set(ps); } catch (IOException e) { throw new ElasticsearchException("failed to load metadata", e); } } else { persistedState.set( new InMemoryPersistedState(0L, ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.get(settings)).build())); } }	upgrademetadataformastereligiblenode is also for data nodes now
private ReleasableDocument makeIndexMetaDataDocument(IndexMetaData indexMetaData) throws IOException { final ReleasableDocument indexMetaDataDocument = makeDocument(INDEX_TYPE_NAME, indexMetaData); boolean success = false; try { final String indexUUID = indexMetaData.getIndexUUID(); assert indexUUID.equals(IndexMetaData.INDEX_UUID_NA_VALUE) == false; indexMetaDataDocument.getDocument().add(new StringField(INDEX_UUID_FIELD_NAME, indexUUID, Field.Store.NO)); success = true; return indexMetaDataDocument; } finally { if (success == false) { IOUtils.closeWhileHandlingException(indexMetaDataDocument); } } }	i think this should be before the success = true; line in the try block since we should also close the lucenepersistedstate if cleaning the legacy data fails.
public void deleteAll() throws IOException { MANIFEST_FORMAT.cleanupOldFiles(Long.MAX_VALUE, nodeEnv.nodeDataPaths()); META_DATA_FORMAT.cleanupOldFiles(Long.MAX_VALUE, nodeEnv.nodeDataPaths()); for (String indexFolderName : nodeEnv.availableIndexFolders()) { INDEX_META_DATA_FORMAT.cleanupOldFiles(Long.MAX_VALUE, nodeEnv.resolveIndexFolder(indexFolderName)); } }	should we also delete the now-empty nodeenv.resolveindexfolder(indexfoldername).resolve("_state")?
public String getMessage() { return throwable != null ? throwable.getMessage() : null; }	would it be beneficial here to return an empty string instead of null? if not, maybe just annotate this with @nullable
@Override public long nextRoundingValue(long utcMillis) { long from = utcMillis + interval; // TODO this implementation makes me very upset while (from > utcMillis) { long rounded = round(from); if (rounded > utcMillis) { return rounded; } from += 60000; } throw new IllegalArgumentException("No rounding available after [" + utcMillis + "] in [" + timeZone + "]"); } } } static class OffsetRounding extends Rounding { static final byte ID = 3; private final Rounding delegate; private final long offset; OffsetRounding(Rounding delegate, long offset) { this.delegate = delegate; this.offset = offset; } OffsetRounding(StreamInput in) throws IOException { // Versions before 7.6.0 will never send this type of rounding. delegate = Rounding.read(in); offset = in.readZLong(); } @Override public void innerWriteTo(StreamOutput out) throws IOException { delegate.writeTo(out); out.writeZLong(offset); } @Override public byte id() { return ID; } @Override public Prepared prepare(long minUtcMillis, long maxUtcMillis) { return wrapPreparedRounding(delegate.prepare(minUtcMillis - offset, maxUtcMillis - offset)); } @Override public Prepared prepareForUnknown() { return wrapPreparedRounding(delegate.prepareForUnknown()); } @Override Prepared prepareJavaTime() { return wrapPreparedRounding(delegate.prepareJavaTime()); } private Prepared wrapPreparedRounding(Prepared delegatePrepared) { return new Prepared() { @Override public long round(long utcMillis) { return delegatePrepared.round(utcMillis - offset) + offset; } @Override public long nextRoundingValue(long utcMillis) { return delegatePrepared.nextRoundingValue(utcMillis - offset) + offset; } @Override public double roundingSize(long utcMillis, DateTimeUnit timeUnit) { return delegatePrepared.roundingSize(utcMillis, timeUnit); } }; } @Override public long offset() { return offset; } @Override public Rounding withoutOffset() { return delegate; } @Override public int hashCode() { return Objects.hash(delegate, offset); } @Override public boolean equals(Object obj) { if (obj == null || getClass() != obj.getClass()) { return false; } OffsetRounding other = (OffsetRounding) obj; return delegate.equals(other.delegate) && offset == other.offset; } @Override public String toString() { return delegate + " offset by " + offset; } } public static Rounding read(StreamInput in) throws IOException { byte id = in.readByte(); switch (id) { case TimeUnitRounding.ID: return new TimeUnitRounding(in); case TimeIntervalRounding.ID: return new TimeIntervalRounding(in); case OffsetRounding.ID: return new OffsetRounding(in); default: throw new ElasticsearchException("unknown rounding id [" + id + "]"); } } /** * Implementation of {@link Prepared} using pre-calculated "round down" points. */ private static class ArrayRounding implements Prepared { private final long[] values; private final int max; private final Prepared delegate; private ArrayRounding(long[] values, int max, Prepared delegate) { this.values = values; this.max = max; this.delegate = delegate; } @Override public long round(long utcMillis) { assert values[0] <= utcMillis : "utcMillis must be after " + values[0]; int idx = Arrays.binarySearch(values, 0, max, utcMillis); assert idx != -1 : "The insertion point is before the array! This should have tripped the assertion above."; assert -1 - idx <= values.length : "This insertion point is after the end of the array."; if (idx < 0) { idx = -2 - idx; } return values[idx]; } @Override public long nextRoundingValue(long utcMillis) { return delegate.nextRoundingValue(utcMillis); }	so i'm just glancing at this, and haven't read the whole class, but, it looks like the old implementation was using interval as a number of seconds, and this is using it as a number of milliseconds? is that correct? or am i misunderstanding something here?
public void testFoo() { Rounding rounding = new Rounding.TimeIntervalRounding(960000, ZoneId.of("Europe/Minsk")); long rounded = rounding.prepareForUnknown().round(877824908400L); long next = rounding.prepareForUnknown().nextRoundingValue(rounded); assertThat(next, greaterThan(rounded)); }	i'm not sure i need to keep these. they failed on some of my other attempts at implementing this method.
@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, ActionListener<Void> listener) { if (isReadOnly()) { listener.onFailure(new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository")); } else { SnapshotInfo snapshot = null; try { snapshot = getSnapshotInfo(snapshotId); } catch (SnapshotMissingException ex) { listener.onFailure(ex); return; } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) { logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex); } // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots final RepositoryData repositoryData; final RepositoryData updatedRepositoryData; try { repositoryData = getRepositoryData(); updatedRepositoryData = repositoryData.removeSnapshot(snapshotId); writeIndexGen(updatedRepositoryData, repositoryStateId); } catch (Exception ex) { listener.onFailure(new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex)); return; } final SnapshotInfo finalSnapshotInfo = snapshot; final Collection<IndexId> unreferencedIndices = Sets.newHashSet(repositoryData.getIndices().values()); unreferencedIndices.removeAll(updatedRepositoryData.getIndices().values()); final ActionListener<Void> afterDeleteIndices = unreferencedIndices.isEmpty() ? listener // if we don't have any newly unreferenced indices we move to the next step directly : ActionListener.wrap( vv -> deleteUnreferencedIndices(unreferencedIndices, listener), listener::onFailure); deleteSnapshotBlobs(snapshot, snapshotId, snapshot == null || snapshot.indices().isEmpty() ? afterDeleteIndices // if we don't have any indices to delete we move to the next step : ActionListener.wrap( v -> deleteIndices( finalSnapshotInfo.indices().stream().map(repositoryData::resolveIndexId).collect(Collectors.toList()), snapshotId, afterDeleteIndices), listener::onFailure)); } }	turned this into a chain of callbacks now and pulled up some of the logic here to make the exact flow of what gets delete when a little clearer (i hope) and avoid multiple callsites for the lister completions in downstream.
private void cleanUp( final RestClient client, final List<String> indices, final List<String> dataStreams, final List<String> autoFollowPatterns ) { for (String autoFollowPattern : autoFollowPatterns) { try { deleteAutoFollowPattern(client, autoFollowPattern); } catch (IOException e) { if (isNotFoundResponseException(e)) { continue; } logger.warn(() -> new ParameterizedMessage("failed to delete auto-follow pattern [{}] after test", autoFollowPattern), e); } } for (String dataStream : dataStreams) { try { deleteDataStream(client, dataStream); } catch (IOException e) { if (isNotFoundResponseException(e)) { continue; } logger.warn(() -> new ParameterizedMessage("failed to delete data stream [{}] after test", dataStream), e); } } for (String index : indices) { try { deleteIndex(client, index); deleteIndex(client, index); } catch (IOException e) { if (isNotFoundResponseException(e)) { continue; } logger.warn(() -> new ParameterizedMessage("failed to delete index [{}] after test", index), e); } } }	i think this was added by accident?
public void testFromJson() throws IOException { String json = "{\\\\n" + " \\\\"span_containing\\\\" : {\\\\n" + " \\\\"big\\\\" : {\\\\n" + " \\\\"span_near\\\\" : {\\\\n" + " \\\\"clauses\\\\" : [ {\\\\n" + " \\\\"span_term\\\\" : {\\\\n" + " \\\\"field1\\\\" : {\\\\n" + " \\\\"value\\\\" : \\\\"bar\\\\",\\\\n" + " \\\\"boost\\\\" : 1.0\\\\n" + " }\\\\n" + " }\\\\n" + " }, {\\\\n" + " \\\\"span_term\\\\" : {\\\\n" + " \\\\"field1\\\\" : {\\\\n" + " \\\\"value\\\\" : \\\\"baz\\\\",\\\\n" + " \\\\"boost\\\\" : 1.0\\\\n" + " }\\\\n" + " }\\\\n" + " } ],\\\\n" + " \\\\"slop\\\\" : 5,\\\\n" + " \\\\"in_order\\\\" : true,\\\\n" + " \\\\"boost\\\\" : 1.0\\\\n" + " }\\\\n" + " },\\\\n" + " \\\\"little\\\\" : {\\\\n" + " \\\\"span_term\\\\" : {\\\\n" + " \\\\"field1\\\\" : {\\\\n" + " \\\\"value\\\\" : \\\\"foo\\\\",\\\\n" + " \\\\"boost\\\\" : 1.0\\\\n" + " }\\\\n" + " }\\\\n" + " },\\\\n" + " \\\\"boost\\\\" : 2.0\\\\n" + " }\\\\n" + "}"; SpanContainingQueryBuilder parsed = (SpanContainingQueryBuilder) parseQuery(json); checkGeneratedJson(json, parsed); assertEquals(json, 2, ((SpanNearQueryBuilder) parsed.bigQuery()).clauses().size()); assertEquals(json, "foo", ((SpanTermQueryBuilder) parsed.littleQuery()).value()); assertEquals(json, 2.0, parsed.boost(), 0.0); }	can we not use "_" here, just a camel case!
public void testFromJson_withNonDefaultBoost_inBigQuery() { String json = "{\\\\n" + " \\\\"span_containing\\\\" : {\\\\n" + " \\\\"big\\\\" : {\\\\n" + " \\\\"span_near\\\\" : {\\\\n" + " \\\\"clauses\\\\" : [ {\\\\n" + " \\\\"span_term\\\\" : {\\\\n" + " \\\\"field1\\\\" : {\\\\n" + " \\\\"value\\\\" : \\\\"bar\\\\",\\\\n" + " \\\\"boost\\\\" : 1.0\\\\n" + " }\\\\n" + " }\\\\n" + " }, {\\\\n" + " \\\\"span_term\\\\" : {\\\\n" + " \\\\"field1\\\\" : {\\\\n" + " \\\\"value\\\\" : \\\\"baz\\\\",\\\\n" + " \\\\"boost\\\\" : 1.0\\\\n" + " }\\\\n" + " }\\\\n" + " } ],\\\\n" + " \\\\"slop\\\\" : 5,\\\\n" + " \\\\"in_order\\\\" : true,\\\\n" + " \\\\"boost\\\\" : 2.0\\\\n" + " }\\\\n" + " },\\\\n" + " \\\\"little\\\\" : {\\\\n" + " \\\\"span_term\\\\" : {\\\\n" + " \\\\"field1\\\\" : {\\\\n" + " \\\\"value\\\\" : \\\\"foo\\\\",\\\\n" + " \\\\"boost\\\\" : 1.0\\\\n" + " }\\\\n" + " }\\\\n" + " },\\\\n" + " \\\\"boost\\\\" : 1.0\\\\n" + " }\\\\n" + "}"; Exception exception = expectThrows(ParsingException.class, () -> parseQuery(json)); assertThat(exception.getMessage(), equalTo("span_containing [big] as a nested span clause can't have non-default boost value [2.0]")); }	the same thing just use camel case, without "_"
@Override public void writeTo(StreamOutput out) throws IOException { out.writeVLong(queryCount); out.writeVLong(queryTimeInMillis); out.writeVLong(queryCurrent); out.writeVLong(queryFailureCount); out.writeVLong(fetchCount); out.writeVLong(fetchTimeInMillis); out.writeVLong(fetchCurrent); out.writeVLong(fetchFailureCount); out.writeVLong(scrollCount); out.writeVLong(scrollTimeInMillis); out.writeVLong(scrollCurrent); out.writeVLong(suggestCount); out.writeVLong(suggestTimeInMillis); out.writeVLong(suggestCurrent); }	here you need to do the same backwards compatibility handling: if (out.getversion().onorafter(version.v_8_1_0)) { // write output }
@Override public void writeTo(StreamOutput out) throws IOException { out.writeVLong(queryCount); out.writeVLong(queryTimeInMillis); out.writeVLong(queryCurrent); out.writeVLong(queryFailureCount); out.writeVLong(fetchCount); out.writeVLong(fetchTimeInMillis); out.writeVLong(fetchCurrent); out.writeVLong(fetchFailureCount); out.writeVLong(scrollCount); out.writeVLong(scrollTimeInMillis); out.writeVLong(scrollCurrent); out.writeVLong(suggestCount); out.writeVLong(suggestTimeInMillis); out.writeVLong(suggestCurrent); }	backwards compatibility check here.
public void deleteIndexStore(String reason, IndexMetaData metaData) throws IOException { if (nodeEnv.hasNodeFile()) { synchronized (this) { String indexName = metaData.index(); if (indices.containsKey(indexName)) { String localUUid = indices.get(indexName).v1().indexUUID(); throw new ElasticsearchIllegalStateException("Can't delete index store for [" + indexName + "] - it's still part of the indices service [" + localUUid+ "] [" + metaData.getUUID() + "]"); } ClusterState clusterState = clusterService.state(); if (clusterState.metaData().hasIndex(indexName) && (clusterState.nodes().localNode().masterNode() == true)) { // we do not delete the store if it is a master eligible node and the index is still in the cluster state // because we want to keep the meta data for indices around even if no shards are left here final IndexMetaData index = clusterState.metaData().index(indexName); throw new ElasticsearchIllegalStateException("Can't delete closed index store for [" + indexName + "] - it's still part of the cluster state [" + index.getUUID() + "] [" + metaData.getUUID() + "]"); } } Index index = new Index(metaData.index()); final Settings indexSettings = buildIndexSettings(metaData); deleteIndexStore(reason, index, indexSettings); } }	in this case isn't the exception wrong? i mean shouldn't we just skip it in that case?
public ClusterState execute(ClusterState currentState) throws Exception { if (clusterState.getVersion() != currentState.getVersion()) { logger.trace("not deleting shard {}, the update task state version[{}] is not equal to cluster state before shard active api call [{}]", shardId, currentState.getVersion(), clusterState.getVersion()); return currentState; } IndexMetaData indexMeta = clusterState.getMetaData().indices().get(shardId.getIndex()); try { indicesService.deleteShardStore("no longer used", shardId, indexMeta); } catch (Throwable ex) { logger.debug("{} failed to delete unallocated shard, ignoring", ex, shardId); } // if the index doesn't exists anymore, delete its store as well, but only if its a non master node, since master // nodes keep the index metadata around if (indicesService.hasIndex(shardId.getIndex()) == false && currentState.nodes().localNode().masterNode() == false) { try { indicesService.deleteIndexStore("no longer used", indexMeta); } catch (Throwable ex) { logger.debug("{} failed to delete unallocated index, ignoring", ex, shardId.getIndex()); } } return currentState; }	just wondering why would master nodes want to keep the index directories around?
@Test public void testShardActiveElseWhere() throws Exception { boolean node1IsMasterEligible = randomBoolean(); boolean node2IsMasterEligible = !node1IsMasterEligible || randomBoolean(); Future<String> node_1_future = internalCluster().startNodeAsync(ImmutableSettings.builder().put("node.master", node1IsMasterEligible).build()); Future<String> node_2_future = internalCluster().startNodeAsync(ImmutableSettings.builder().put("node.master", node2IsMasterEligible).build()); final String node_1 = node_1_future.get(); final String node_2 = node_2_future.get(); final String node_1_id = internalCluster().getInstance(DiscoveryService.class, node_1).localNode().getId(); final String node_2_id = internalCluster().getInstance(DiscoveryService.class, node_2).localNode().getId(); logger.debug("node {} (node_1) is {}master eligible", node_1, node1IsMasterEligible ? "" : "not "); logger.debug("node {} (node_2) is {}master eligible", node_2, node2IsMasterEligible ? "" : "not "); logger.debug("node {} became master", internalCluster().getMasterName()); final int numShards = scaledRandomIntBetween(2, 20); assertAcked(prepareCreate("test") .setSettings(ImmutableSettings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0).put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, numShards)) ); ensureGreen("test"); waitNoPendingTasksOnAll(); ClusterStateResponse stateResponse = client().admin().cluster().prepareState().get(); RoutingNode routingNode = stateResponse.getState().routingNodes().node(node_2_id); final int[] node2Shards = new int[routingNode.numberOfOwningShards()]; int i = 0; for (MutableShardRouting mutableShardRouting : routingNode) { node2Shards[i] = mutableShardRouting.shardId().id(); i++; } logger.info("Node 2 has shards: {}", Arrays.toString(node2Shards)); final long shardVersions[] = new long[numShards]; final int shardIds[] = new int[numShards]; i=0; for (ShardRouting shardRouting : stateResponse.getState().getRoutingTable().allShards("test")) { shardVersions[i] = shardRouting.version(); shardIds[i] = shardRouting.getId(); i++; } internalCluster().getInstance(ClusterService.class, node_2).submitStateUpdateTask("test", Priority.IMMEDIATE, new ClusterStateUpdateTask() { @Override public ClusterState execute(ClusterState currentState) throws Exception { IndexRoutingTable.Builder indexRoutingTableBuilder = IndexRoutingTable.builder("test"); for (int i = 0; i < numShards; i++) { indexRoutingTableBuilder.addIndexShard( new IndexShardRoutingTable.Builder(new ShardId("test", i), false) .addShard(new ImmutableShardRouting("test", i, node_1_id, true, ShardRoutingState.STARTED, shardVersions[shardIds[i]])) .build() ); } return ClusterState.builder(currentState) .routingTable(RoutingTable.builder().add(indexRoutingTableBuilder).build()) .build(); } public boolean runOnlyOnMaster() { return false; } @Override public void onFailure(String source, Throwable t) { } }); waitNoPendingTasksOnAll(); logger.info("Checking if shards aren't removed"); for (int shard : node2Shards) { assertTrue(waitForShardDeletion(node_2, "test", shard)); } }	why is waitnopendingtasksonall() removed here?
public final int compare(ShardRouting o1, ShardRouting o2) { final Index o1Index = o1.index(); final Index o2Index = o2.index(); int cmp = 0; if (o1Index.equals(o2Index) == false) { final IndexMetadata metadata01 = getMetadata(o1Index); final IndexMetadata metadata02 = getMetadata(o2Index); cmp = Boolean.compare(metadata02.isSystem(), metadata01.isSystem()); if (cmp == 0) { cmp = Long.compare(metadata02.priority(), metadata01.priority()); if (cmp == 0) { cmp = Long.compare(metadata02.getCreationDate(), metadata01.getCreationDate()); if (cmp == 0) { cmp = o2Index.getName().compareTo(o1Index.getName()); } } } } return cmp; }	better compare the index here than just their names. if we have instance equality it's the same performance as comparing names. if we have different indices, the uuid strings will compare quicker than index names because there's no common prefixes.
synchronized void stop() { if (isCleanupRunning.compareAndSet(true, false)) { if (cancellable != null && cancellable.isCancelled() == false) { cancellable.cancel(); } } }	i am curious about these changesl: the newly added tests uncovered some bug?
public void clusterChanged(ClusterChangedEvent event) { final ClusterState state = event.state(); if (state.blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK)) { // Wait until the gateway has recovered from disk. return; } tryStartCleanup(state); }	i think that iscleanuprunning no longer needs to be atomic boolean given that it's only accessed from synchronized methods
@Override protected void onQueryFailure(int shardIndex, SearchShardTarget shardTarget, Exception exc) { // best effort to cancel expired tasks checkCancellation(); searchResponse.get().addShardFailure(shardIndex, new ShardSearchFailure(exc, shardTarget.getNodeIdText() != null ? shardTarget : null)); }	this one was causing npe? lovely to get npe as part of a null check :) maybe getnodeid should be changed to return null instead of npe when nodeid is null? it seems like a trappy behaviour.
private void onFinalResponse(CancellableTask submitTask, AsyncSearchTask searchTask, AsyncSearchResponse response, Runnable nextAction) { if (submitTask.isCancelled() || searchTask.isCancelled()) { // the task was cancelled so we ensure that there is nothing stored in the response index. store.deleteResponse(searchTask.getSearchId(), ActionListener.wrap( resp -> unregisterTaskAndMoveOn(searchTask, nextAction), exc -> { logger.error(() -> new ParameterizedMessage("failed to clean async-search [{}]", searchTask.getSearchId()), exc); unregisterTaskAndMoveOn(searchTask, nextAction); })); return; } try { store.storeFinalResponse(searchTask.getSearchId().getDocId(), threadContext.getResponseHeaders(),response, ActionListener.wrap(resp -> unregisterTaskAndMoveOn(searchTask, nextAction), exc -> { if (exc.getCause() instanceof DocumentMissingException == false && exc.getCause() instanceof VersionConflictEngineException == false) { logger.error(() -> new ParameterizedMessage("failed to store async-search [{}]", searchTask.getSearchId().getEncoded()), exc); } unregisterTaskAndMoveOn(searchTask, nextAction); })); } catch (Exception exc) { logger.error(() -> new ParameterizedMessage("failed to store async-search [{}]", searchTask.getSearchId().getEncoded()), exc); unregisterTaskAndMoveOn(searchTask, nextAction); } }	not related to your change, but i think this one should be changed like i did in other places to use exceptionhelper.unwrapcause , i forgot about this one. when can version conflict be thrown again?
public void setExpirationTime(long expirationTimeMillis) { this.expirationTimeMillis = expirationTimeMillis; }	i wonder if we could add a constructor that takes this, it seems possible given that we call decode in one place only and it could accept the expiration time too, and pass it to the constructor.
public void testUnassignRunningPersistentTask() throws Exception { PersistentTasksClusterService persistentTasksClusterService = internalCluster().getInstance(PersistentTasksClusterService.class, internalCluster().getMasterName()); // Speed up rechecks to a rate that is quicker than what settings would allow persistentTasksClusterService.setRecheckInterval(TimeValue.timeValueMillis(1)); PersistentTasksService persistentTasksService = internalCluster().getInstance(PersistentTasksService.class); PlainActionFuture<PersistentTask<TestParams>> future = new PlainActionFuture<>(); TestParams testParams = new TestParams("Blah"); testParams.setExecutorNodeAttr("test"); persistentTasksService.sendStartRequest(UUIDs.base64UUID(), TestPersistentTasksExecutor.NAME, testParams, future); PersistentTask<TestParams> task = future.get(); String taskId = task.getId(); Settings nodeSettings = Settings.builder().put(nodeSettings(0)).put("node.attr.test_attr", "test").build(); internalCluster().startNode(nodeSettings); waitForTaskToStart(); PlainActionFuture<PersistentTask<?>> unassignmentFuture = new PlainActionFuture<>(); // Disallow re-assignment after it is unallocated to verify master and node state TestPersistentTasksExecutor.setNonClusterStateCondition(false); persistentTasksClusterService.unassignPersistentTask(taskId, task.getAllocationId() + 1, "unassignment test", unassignmentFuture); PersistentTask<?> unassignedTask = unassignmentFuture.get(); assertThat(unassignedTask.getId(), equalTo(taskId)); assertThat(unassignedTask.getAssignment().getExplanation(), equalTo("unassignment test")); assertThat(unassignedTask.getAssignment().getExecutorNode(), is(nullValue())); assertBusy(() -> { // Verify that the task is NOT running on the node List<TaskInfo> tasks = client().admin().cluster().prepareListTasks().setActions(TestPersistentTasksExecutor.NAME + "[c]").get() .getTasks(); assertThat(tasks.size(), equalTo(0)); // Verify that the task is STILL in internal cluster state internalClusterHasSingleTask(taskId); }); // Allow it to be reassigned again to the same node TestPersistentTasksExecutor.setNonClusterStateCondition(true); // Verify it starts again waitForTaskToStart(); // Assert that we still have it in master state internalClusterHasSingleTask(taskId); // Complete or cancel the running task TaskInfo taskInfo = client().admin().cluster().prepareListTasks().setActions(TestPersistentTasksExecutor.NAME + "[c]") .get().getTasks().get(0); stopOrCancelTask(taskInfo.getTaskId()); }	nit: by master state, i'd expect the internalclusterhassingletask() method to check the cluster state on the master node, but in fact it checks using a random cluster service. so maybe just remove this comment?
public void testBulkWithDifferentContentTypes() throws IOException { { BulkRequest bulkRequest = new BulkRequest(); bulkRequest.add(new DeleteRequest("index", "type", "0")); bulkRequest.add(new UpdateRequest("index", "type", "1").script(mockScript("test"))); bulkRequest.add(new DeleteRequest("index", "type", "2")); Request request = RequestConverters.bulk(bulkRequest); assertEquals(XContentType.JSON.mediaTypeWithoutParameters(), request.getEntity().getContentType().getValue()); } { XContentType xContentType = randomFrom(XContentType.JSON, XContentType.SMILE); BulkRequest bulkRequest = new BulkRequest(); bulkRequest.add(new DeleteRequest("index", "type", "0")); bulkRequest.add(new IndexRequest("index", "type", "0").source(singletonMap("field", "value"), xContentType)); bulkRequest.add(new DeleteRequest("index", "type", "2")); Request request = RequestConverters.bulk(bulkRequest); assertEquals(xContentType.mediaTypeWithoutParameters(), request.getEntity().getContentType().getValue()); } { XContentType xContentType = randomFrom(XContentType.JSON, XContentType.SMILE); UpdateRequest updateRequest = new UpdateRequest("index", "type", "0"); if (randomBoolean()) { updateRequest.doc(new IndexRequest().source(singletonMap("field", "value"), xContentType)); } else { updateRequest.upsert(new IndexRequest().source(singletonMap("field", "value"), xContentType)); } Request request = RequestConverters.bulk(new BulkRequest().add(updateRequest)); assertEquals(xContentType.mediaTypeWithoutParameters(), request.getEntity().getContentType().getValue()); } { BulkRequest bulkRequest = new BulkRequest(); bulkRequest.add(new IndexRequest("index", "type", "0").source(singletonMap("field", "value"), XContentType.SMILE)); bulkRequest.add(new IndexRequest("index", "type", "1").source(singletonMap("field", "value"), XContentType.JSON)); IllegalArgumentException exception = expectThrows(IllegalArgumentException.class, () -> RequestConverters.bulk(bulkRequest)); assertEquals( "Mismatching content-type found for request with content-type [JSON], " + "previous requests have content-type [SMILE]", exception.getMessage()); } { BulkRequest bulkRequest = new BulkRequest(); bulkRequest.add(new IndexRequest("index", "type", "0").source(singletonMap("field", "value"), XContentType.JSON)); bulkRequest.add(new IndexRequest("index", "type", "1").source(singletonMap("field", "value"), XContentType.JSON)); bulkRequest.add(new UpdateRequest("index", "type", "2") .doc(new IndexRequest().source(singletonMap("field", "value"), XContentType.JSON)) .upsert(new IndexRequest().source(singletonMap("field", "value"), XContentType.SMILE))); IllegalArgumentException exception = expectThrows(IllegalArgumentException.class, () -> RequestConverters.bulk(bulkRequest)); assertEquals( "Mismatching content-type found for request with content-type [SMILE], " + "previous requests have content-type [JSON]", exception.getMessage()); } { XContentType xContentType = randomFrom(XContentType.CBOR, XContentType.YAML); BulkRequest bulkRequest = new BulkRequest(); bulkRequest.add(new DeleteRequest("index", "type", "0")); bulkRequest.add(new IndexRequest("index", "type", "1").source(singletonMap("field", "value"), XContentType.JSON)); bulkRequest.add(new DeleteRequest("index", "type", "2")); bulkRequest.add(new DeleteRequest("index", "type", "3")); bulkRequest.add(new IndexRequest("index", "type", "4").source(singletonMap("field", "value"), XContentType.JSON)); bulkRequest.add(new IndexRequest("index", "type", "1").source(singletonMap("field", "value"), xContentType)); IllegalArgumentException exception = expectThrows(IllegalArgumentException.class, () -> RequestConverters.bulk(bulkRequest)); assertEquals("Unsupported content-type found for request with content-type [" + xContentType + "], only JSON and SMILE are supported", exception.getMessage()); } }	pls dont mess with the existing spacing on this class, its causing a big diff :)
public void testCancellationOnClose() throws InterruptedException { final NetworkService networkService = new NetworkService(Collections.emptyList()); final CountDownLatch latch = new CountDownLatch(1); final CountDownLatch conditionLatch = new CountDownLatch(1); final Transport transport = new MockNioTransport( Settings.EMPTY, Version.CURRENT, threadPool, networkService, PageCacheRecycler.NON_RECYCLING_INSTANCE, new NamedWriteableRegistry(Collections.emptyList()), new NoneCircuitBreakerService()) { @Override public BoundTransportAddress boundAddress() { return new BoundTransportAddress( new TransportAddress[]{new TransportAddress(InetAddress.getLoopbackAddress(), 9500)}, new TransportAddress(InetAddress.getLoopbackAddress(), 9500) ); } @Override public TransportAddress[] addressesFromString(String address) throws UnknownHostException { if ("hostname1".equals(address)) { return new TransportAddress[]{new TransportAddress(TransportAddress.META_ADDRESS, 9300)}; } else if ("hostname2".equals(address)) { try { conditionLatch.countDown(); latch.await(); return new TransportAddress[]{new TransportAddress(TransportAddress.META_ADDRESS, 9300)}; } catch (InterruptedException e) { throw new RuntimeException(e); } } else { throw new UnknownHostException(address); } } }; closeables.push(transport); final TransportService transportService = new TransportService(Settings.EMPTY, transport, threadPool, TransportService.NOOP_TRANSPORT_INTERCEPTOR, x -> null, null, Collections.emptySet()); closeables.push(transportService); recreateSeedHostsResolver(transportService, Settings.builder().put(SeedHostsResolver.DISCOVERY_SEED_RESOLVER_TIMEOUT_SETTING.getKey(), "10m").build()); final PlainActionFuture<List<TransportAddress>> fut = new PlainActionFuture<>(); threadPool.generic().execute((() -> fut.onResponse(seedHostsResolver.resolveHosts(Arrays.asList("hostname1", "hostname2"))))); conditionLatch.await(); seedHostsResolver.stop(); assertThat(FutureUtils.get(fut, 10, TimeUnit.SECONDS), hasSize(0)); }	this is unreachable, right? suggestion throw new assertionerror("impossible");
private static void checkFilterOnAggs(LogicalPlan p, Set<Failure> localFailures) { if (p instanceof Filter) { Filter filter = (Filter) p; if ((filter.child() instanceof Aggregate) == false) { filter.condition().forEachDown(f -> { if (Functions.isAggregate(f)) { localFailures.add(fail(f, "Cannot use WHERE filtering on aggregate function [%s], use HAVING instead", Expressions.name(f))); } else if (Functions.isGrouping(f)) { localFailures.add(fail(f, "Cannot use WHERE filtering on grouping function [%s], use HAVING instead", Expressions.name(f))); } }, Function.class); } } }	maybe combine these two conditions into one similar with if (functions.isaggregate(f) || functions.isgrouping(f)) { localfailures.add(fail(f, "cannot use where filtering on " + functions.isaggregate(f) ? "aggregate" : "grouping" + "function [%s], use having instead", expressions.name(f))); ?
@Override public void sendResponse(Exception exception) throws IOException { try { outboundHandler.sendErrorResponse(version, channel, requestId, action, exception); } finally { Releasables.close(toRelease); release(true); } }	i would prefer to put this into the release method, which protects against double-releasing (see breaker)
public Mapper.Builder parse(String name, Map<String, Object> node, MappingParserContext parserContext) throws MapperParsingException { FieldMapper.Builder builder; boolean ignoreMalformedByDefault = IGNORE_MALFORMED_SETTING.get(parserContext.getSettings()); boolean coerceByDefault = COERCE_SETTING.get(parserContext.getSettings()); if (LegacyGeoShapeFieldMapper.containsDeprecatedParameter(node.keySet())) { if (parserContext.indexVersionCreated().onOrAfter(Version.V_8_0_0)) { Set<String> deprecatedParams = LegacyGeoShapeFieldMapper.getDeprecatedParameters(node.keySet()); throw new IllegalArgumentException("using deprecated parameters " + Arrays.toString(deprecatedParams.toArray()) + " in mapper [" + name + "] of type [geo_shape] is no longer allowed"); } builder = new LegacyGeoShapeFieldMapper.Builder( name, parserContext.indexVersionCreated(), ignoreMalformedByDefault, coerceByDefault); } else { builder = new GeoShapeWithDocValuesFieldMapper.Builder( name, parserContext.indexVersionCreated(), ignoreMalformedByDefault, coerceByDefault, geoFormatterFactory); } builder.parse(name, parserContext, node); return builder; } } private final Builder builder; private final GeoShapeIndexer indexer; public GeoShapeWithDocValuesFieldMapper(String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, GeoShapeIndexer indexer, GeoShapeParser parser, Builder builder) { super(simpleName, mappedFieldType, builder.ignoreMalformed.get(), builder.coerce.get(), builder.ignoreZValue.get(), builder.orientation.get(), multiFields, copyTo, parser); this.builder = builder; this.indexer = indexer; } @Override protected void index(DocumentParserContext context, Geometry geometry) throws IOException { if (geometry == null) { return; } geometry = indexer.prepareForIndexing(geometry); List<IndexableField> fields = indexer.indexShape(geometry); if (fieldType().isSearchable()) { context.doc().addAll(fields); } if (fieldType().hasDocValues()) { String name = fieldType().name(); BinaryGeoShapeDocValuesField docValuesField = (BinaryGeoShapeDocValuesField) context.doc().getByKey(name); if (docValuesField == null) { docValuesField = new BinaryGeoShapeDocValuesField(name); context.doc().addWithKey(name, docValuesField); } docValuesField.add(fields, geometry); } else if (fieldType().isSearchable()) { context.addToFieldNames(fieldType().name()); } } @Override protected String contentType() { return CONTENT_TYPE; } @Override public FieldMapper.Builder getMergeBuilder() { return new Builder( simpleName(), builder.version, builder.ignoreMalformed.getDefaultValue().value(), builder.coerce.getDefaultValue().value(), builder.geoFormatterFactory ).init(this); } @Override public GeoShapeWithDocValuesFieldType fieldType() { return (GeoShapeWithDocValuesFieldType) super.fieldType(); } @Override protected void checkIncomingMergeType(FieldMapper mergeWith) { if (mergeWith instanceof AbstractShapeGeometryFieldMapper<?> && (mergeWith instanceof GeoShapeWithDocValuesFieldMapper) == false) { throw new IllegalArgumentException("mapper [" + name() + "] of type [geo_shape] cannot change strategy from [BKD] to [recursive]"); }	these.second set of parentheses around (mergewith instanceof geoshapewithdocvaluesfieldmapper) is a bit confusing.
public MappedFieldType unmappedFieldType(String type) { if (type.equals("string")) { deprecationLogger.deprecated("[unmapped_type:string] should be replaced with [unmapped_type:keyword]"); type = "keyword"; } MappedFieldType fieldType = unmappedFieldTypes.get(type); if (fieldType == null) { final Mapper.TypeParser.ParserContext parserContext = documentMapperParser().parserContext(type); Mapper.TypeParser typeParser = parserContext.typeParser(type); if (typeParser == null) { throw new IllegalArgumentException("No mapper found for type [" + type + "]"); } final Mapper.Builder<?, ?> builder = typeParser.parse("__anonymous_" + type, new HashMap<>(), parserContext); final BuilderContext builderContext = new BuilderContext(indexSettings.getSettings(), new ContentPath(1)); fieldType = ((FieldMapper)builder.build(builderContext)).fieldType(); // There is no need to synchronize writes here. In the case of concurrent access, we could just // compute some mappers several times, which is not a big deal Map<String, MappedFieldType> newUnmappedFieldTypes = new HashMap<>(unmappedFieldTypes); newUnmappedFieldTypes.put(type, fieldType); unmappedFieldTypes = unmodifiableMap(newUnmappedFieldTypes); } return fieldType; }	@jimczi can you explain how this fixes the issue? just curious.
static Map<String, DatabaseReaderLazyLoader> loadDatabaseReaders(Path geoIpConfigDirectory, NodeCache cache) throws IOException { if (Files.exists(geoIpConfigDirectory) == false && Files.isDirectory(geoIpConfigDirectory)) { throw new IllegalStateException("the geoip directory [" + geoIpConfigDirectory + "] containing databases doesn't exist"); } boolean loadDatabaseOnHeap = Booleans.parseBoolean(System.getProperty("es.geoip.load_db_on_heap", "false")); Map<String, DatabaseReaderLazyLoader> databaseReaders = new HashMap<>(); try (Stream<Path> databaseFiles = Files.list(geoIpConfigDirectory)) { PathMatcher pathMatcher = geoIpConfigDirectory.getFileSystem().getPathMatcher("glob:**.mmdb"); // Use iterator instead of forEach otherwise IOException needs to be caught twice... Iterator<Path> iterator = databaseFiles.iterator(); while (iterator.hasNext()) { Path databasePath = iterator.next(); if (Files.isRegularFile(databasePath) && pathMatcher.matches(databasePath)) { String databaseFileName = databasePath.getFileName().toString(); DatabaseReaderLazyLoader holder = new DatabaseReaderLazyLoader(databaseFileName, () -> { DatabaseReader.Builder builder = createDatabaseBuilder(databasePath).withCache(cache); if (loadDatabaseOnHeap) { builder.fileMode(Reader.FileMode.MEMORY); } else { builder.fileMode(Reader.FileMode.MEMORY_MAPPED); } return builder.build(); }); databaseReaders.put(databaseFileName, holder); } } } return Collections.unmodifiableMap(databaseReaders); }	to this escape hatch
private static Geometry createGeometry(String type, List<Geometry> geometries, CoordinateNode coordinates, Boolean orientation, boolean defaultOrientation, boolean coerce, DistanceUnit.Distance radius) { ShapeType shapeType; if ("bbox".equals(type.toLowerCase(Locale.ROOT))) { shapeType = ShapeType.ENVELOPE; } else { shapeType = ShapeType.forName(type); } if (shapeType == ShapeType.GEOMETRYCOLLECTION) { if (geometries == null) { throw new ElasticsearchParseException("geometries not included"); } if (coordinates != null) { throw new ElasticsearchParseException("parameter coordinates is not supported for type " + type); } verifyNulls(type, null, orientation, radius); return new GeometryCollection<>(geometries); } // We expect to have coordinates for all the rest if (coordinates == null) { throw new ElasticsearchParseException("coordinates not included"); } switch (shapeType) { case CIRCLE: if (radius == null) { throw new ElasticsearchParseException("radius is not specified"); } verifyNulls(type, geometries, orientation, null); Point point = coordinates.asPoint(); return new Circle(point.getLat(), point.getLon(), point.getAlt(), radius.convert(DistanceUnit.METERS).value); case POINT: verifyNulls(type, geometries, orientation, radius); return coordinates.asPoint(); case MULTIPOINT: verifyNulls(type, geometries, orientation, radius); return coordinates.asMultiPoint(); case LINESTRING: verifyNulls(type, geometries, orientation, radius); return coordinates.asLineString(coerce); case MULTILINESTRING: verifyNulls(type, geometries, orientation, radius); return coordinates.asMultiLineString(coerce); case POLYGON: verifyNulls(type, geometries, null, radius); // handle possible null in orientation return coordinates.asPolygon(orientation != null ? orientation : defaultOrientation, coerce); case MULTIPOLYGON: verifyNulls(type, geometries, null, radius); // handle possible null in orientation return coordinates.asMultiPolygon(orientation != null ? orientation : defaultOrientation, coerce); case ENVELOPE: verifyNulls(type, geometries, orientation, radius); return coordinates.asRectangle(); default: throw new ElasticsearchParseException("unsuppoted shape type " + type); } }	should we just use equalsignorecase instead of transforming the type string tolowercase?
private static void checkEnabledFieldChange(ObjectMapper mergeWith, Mapper mergeWithMapper, Mapper mergeIntoMapper) { if (mergeIntoMapper instanceof ObjectMapper && mergeWithMapper instanceof ObjectMapper) { final ObjectMapper mergeIntoObjectMapper = (ObjectMapper) mergeIntoMapper; final ObjectMapper mergeWithObjectMapper = (ObjectMapper) mergeWithMapper; if (mergeIntoObjectMapper.isEnabled() != mergeWithObjectMapper.isEnabled()) { final String path = mergeWith.fullPath() + "." + mergeWithObjectMapper.simpleName() + ".enabled"; throw new MapperException("Can't update attribute for type [" + path + "] in index mapping"); } } }	nit: maybe delete this line
public void deleteResponse(AsyncExecutionId asyncExecutionId, ActionListener<DeleteResponse> listener) { try { DeleteRequest request = new DeleteRequest(index).id(asyncExecutionId.getDocId()); createIndexIfNecessary(ActionListener.wrap(v -> client.delete(request, listener), listener::onFailure)); } catch(Exception e) { listener.onFailure(e); } } /** * Returns the {@link AsyncTask} if the provided <code>asyncTaskId</code> * is registered in the task manager, <code>null</code> otherwise. * * This method throws a {@link ResourceNotFoundException}	do we need to create an index in this case or this is for consistency?
private static double calculateMedian(double[] sample) { final double[] sorted = Arrays.copyOf(sample, sample.length); Arrays.sort(sorted); final int halfway = (int) Math.ceil(sorted.length / 2d); final double median; if (sorted.length % 2 == 0) { // even median = (sorted[halfway - 1] + sorted[halfway]) / 2d; } else { // odd median = (sorted[halfway -1]); } return median; }	nit so small i feel bad even mentioning it - can you change the spacing to halfway - 1 so it's consistent
@Override public boolean equals(Object o) { if (sameClassAs(o) == false) { return false; } SliceQuery that = (SliceQuery) o; return id == that.id && max == that.max && doEquals(that); }	the semantics of two new methods doequals and dohashcode are not obvious (to me). maybe use sharddocsortfield.name for the field name in docidslicequery.java so we don't have to add these methods? but i am okay if you prefer to keep these.
@Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } Request other = (Request) obj; return Objects.equals(params, other.params); } } public static class DatafeedParams implements XPackPlugin.XPackPersistentTaskParams { public static ObjectParser<DatafeedParams, Void> PARSER = new ObjectParser<>(MlTasks.DATAFEED_TASK_NAME, true, DatafeedParams::new); static { PARSER.declareString((params, datafeedId) -> params.datafeedId = datafeedId, DatafeedConfig.ID); PARSER.declareString((params, startTime) -> params.startTime = parseDateOrThrow( startTime, START_TIME, System::currentTimeMillis), START_TIME); PARSER.declareString(DatafeedParams::setEndTime, END_TIME); PARSER.declareString((params, val) -> params.setTimeout(TimeValue.parseTimeValue(val, TIMEOUT.getPreferredName())), TIMEOUT); } static long parseDateOrThrow(String date, ParseField paramName, LongSupplier now) { DateMathParser dateMathParser = DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.toDateMathParser(); try { return dateMathParser.parse(date, now); } catch (Exception e) { String msg = Messages.getMessage(Messages.REST_INVALID_DATETIME_PARAMS, paramName.getPreferredName(), date); throw new ElasticsearchParseException(msg, e); } } public static DatafeedParams fromXContent(XContentParser parser) { return parseRequest(null, parser); } public static DatafeedParams parseRequest(String datafeedId, XContentParser parser) { DatafeedParams params = PARSER.apply(parser, null); if (datafeedId != null) { params.datafeedId = datafeedId; } return params; } public DatafeedParams(String datafeedId, long startTime) { this.datafeedId = ExceptionsHelper.requireNonNull(datafeedId, DatafeedConfig.ID.getPreferredName()); this.startTime = startTime; } public DatafeedParams(String datafeedId, String startTime) { this(datafeedId, parseDateOrThrow(startTime, START_TIME, System::currentTimeMillis)); } public DatafeedParams(StreamInput in) throws IOException { datafeedId = in.readString(); startTime = in.readVLong(); endTime = in.readOptionalLong(); timeout = TimeValue.timeValueMillis(in.readVLong()); if (in.getVersion().onOrAfter(Version.CURRENT)) { datafeedConfig = in.readOptionalWriteable(DatafeedConfig::new); job = in.readOptionalWriteable(Job::new); } } DatafeedParams() { } private String datafeedId; private long startTime; private Long endTime; private TimeValue timeout = TimeValue.timeValueSeconds(20); private DatafeedConfig datafeedConfig; private Job job; public String getDatafeedId() { return datafeedId; } public long getStartTime() { return startTime; } public Long getEndTime() { return endTime; } public void setEndTime(String endTime) { setEndTime(parseDateOrThrow(endTime, END_TIME, System::currentTimeMillis)); } public void setEndTime(Long endTime) { this.endTime = endTime; } public TimeValue getTimeout() { return timeout; } public void setTimeout(TimeValue timeout) { this.timeout = timeout; } public DatafeedConfig getDatafeedConfig() { return datafeedConfig; } public void setDatafeedConfig(DatafeedConfig datafeedConfig) { this.datafeedConfig = datafeedConfig; } public Job getJob() { return job; } public void setJob(Job job) { this.job = job; } @Override public String getWriteableName() { return MlTasks.DATAFEED_TASK_NAME; } @Override public Version getMinimalSupportedVersion() { return Version.CURRENT.minimumCompatibilityVersion(); } @Override public void writeTo(StreamOutput out) throws IOException { out.writeString(datafeedId); out.writeVLong(startTime); out.writeOptionalLong(endTime); out.writeVLong(timeout.millis()); if (out.getVersion().onOrAfter(Version.CURRENT)) { out.writeOptionalWriteable(datafeedConfig); out.writeOptionalWriteable(job); } } @Override public XContentBuilder toXContent(XContentBuilder builder, ToXContent.Params params) throws IOException { builder.startObject(); builder.field(DatafeedConfig.ID.getPreferredName(), datafeedId); builder.field(START_TIME.getPreferredName(), String.valueOf(startTime)); if (endTime != null) { builder.field(END_TIME.getPreferredName(), String.valueOf(endTime)); } builder.field(TIMEOUT.getPreferredName(), timeout.getStringRep()); builder.endObject(); return builder; } @Override public int hashCode() { return Objects.hash(datafeedId, startTime, endTime, timeout, datafeedConfig, job); } @Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } DatafeedParams other = (DatafeedParams) obj; return Objects.equals(datafeedId, other.datafeedId) && Objects.equals(startTime, other.startTime) && Objects.equals(endTime, other.endTime) && Objects.equals(timeout, other.timeout) && Objects.equals(datafeedConfig, other.datafeedConfig) && Objects.equals(job, other.job); } } static class RequestBuilder extends ActionRequestBuilder<Request, AcknowledgedResponse, RequestBuilder> { RequestBuilder(ElasticsearchClient client, StartDatafeedAction action) { super(client, action, new Request()); } }	it seems like this should be a specific version, not version.current. once we get to a mixed version 7.2/7.3 cluster surely it would be ok for the master node to be on 7.2?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(datafeedId); out.writeVLong(startTime); out.writeOptionalLong(endTime); out.writeVLong(timeout.millis()); if (out.getVersion().onOrAfter(Version.CURRENT)) { out.writeOptionalWriteable(datafeedConfig); out.writeOptionalWriteable(job); } }	as above, surely this should be a specific version?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(datafeedId); out.writeVLong(startTime); out.writeOptionalLong(endTime); out.writeVLong(timeout.millis()); if (out.getVersion().onOrAfter(Version.CURRENT)) { out.writeOptionalWriteable(datafeedConfig); out.writeOptionalWriteable(job); } }	it seems wrong that toxcontent() persists different information to writeto(). this means that the behaviour on full cluster restart is different to behaviour within a running cluster. if someone did a full cluster restart with started datafeeds i suspect that things wouldn't work on restart. (it should be possible to add a test in x-pack/qa/full-cluster-restart to confirm this.) however, i think the solution to this problem is not to put the whole datafeed config in the x-content representation. that creates the same problem we have now where search syntax that's been removed prevents reading the cluster state from disk. this leads to the conclusion that instead of storing the whole datafeed config in the task params we should store just enough fields from it to validate allocation, and definitely not the search or aggregations. then toxcontent() can be consistent with writeto().
Function<String, ZonedDateTime> getFunction(String format, ZoneId timezone, Locale locale) { return date -> Instant.ofEpochMilli((long) (Double.parseDouble(date) * 1000.0)).atZone(timezone); } }, UnixMs { @Override Function<String, ZonedDateTime> getFunction(String format, ZoneId timezone, Locale locale) { return date -> Instant.ofEpochMilli(Long.parseLong(date)).atZone(timezone); } }, Tai64n { @Override Function<String, ZonedDateTime> getFunction(String format, ZoneId timezone, Locale locale) { return date -> Instant.ofEpochMilli(parseMillis(date)).atZone(timezone); } private long parseMillis(String date) { if (date.startsWith("@")) { date = date.substring(1); } long base = Long.parseLong(date.substring(1, 16), 16); // 1356138046000 long rest = Long.parseLong(date.substring(16, 24), 16); return ((base * 1000) - 10000) + (rest/1000000); } }, Java { private final List<ChronoField> FIELDS = Arrays.asList(NANO_OF_SECOND, SECOND_OF_DAY, MINUTE_OF_DAY, HOUR_OF_DAY, DAY_OF_MONTH, MONTH_OF_YEAR); @Override Function<String, ZonedDateTime> getFunction(String format, ZoneId zoneId, Locale locale) { // support the 6.x BWC compatible way of parsing java 8 dates if (format.startsWith("8")) { format = format.substring(1); } DateFormatter dateFormatter = DateFormatter.forPattern(format) .withLocale(locale); final DateFormatter formatter = dateFormatter; return text -> { TemporalAccessor accessor = formatter.parse(text); // if there is no year nor year-of-era, we fall back to the current one and // fill the rest of the date up with the parsed date if (accessor.isSupported(ChronoField.YEAR) == false && accessor.isSupported(ChronoField.YEAR_OF_ERA) == false && accessor.isSupported(WeekFields.of(locale).weekBasedYear()) == false) { int year = LocalDate.now(ZoneOffset.UTC).getYear(); ZonedDateTime newTime = Instant.EPOCH.atZone(ZoneOffset.UTC).withYear(year); for (ChronoField field : FIELDS) { if (accessor.isSupported(field)) { newTime = newTime.with(field, accessor.get(field)); } } accessor = newTime.withZoneSameLocal(zoneId); } return DateFormatters.from(accessor, locale, zoneId) .withZoneSameInstant(zoneId); }; } }; abstract Function<String, ZonedDateTime> getFunction(String format, ZoneId timezone, Locale locale); static DateFormat fromString(String format) { switch (format) { case "ISO8601": return Iso8601; case "UNIX": return Unix; case "UNIX_MS": return UnixMs; case "TAI64N": return Tai64n; default: return Java; }	is this change unrelated to the timezone issue?
public void testRestoreMinmal() throws IOException { IndexShard shard = newStartedShard(true); int numInitialDocs = randomIntBetween(10, 100); for (int i = 0; i < numInitialDocs; i++) { final String id = Integer.toString(i); indexDoc(shard, id, randomDoc()); if (randomBoolean()) { shard.refresh("test"); } } for (int i = 0; i < numInitialDocs; i++) { final String id = Integer.toString(i); if (randomBoolean()) { if (rarely()) { deleteDoc(shard, id); } else { indexDoc(shard, id, randomDoc()); } } if (frequently()) { shard.refresh("test"); } } SnapshotId snapshotId = new SnapshotId("test", "test"); IndexId indexId = new IndexId(shard.shardId().getIndexName(), shard.shardId().getIndex().getUUID()); SourceOnlySnapshotRepository repository = new SourceOnlySnapshotRepository(createRepository()); repository.start(); try (Engine.IndexCommitRef snapshotRef = shard.acquireLastIndexCommit(true)) { IndexShardSnapshotStatus indexShardSnapshotStatus = IndexShardSnapshotStatus.newInitializing(null); final PlainActionFuture<String> future = PlainActionFuture.newFuture(); runAsSnapshot(shard.getThreadPool(), () -> { repository.snapshotShard(shard.store(), shard.mapperService(), snapshotId, indexId, snapshotRef.getIndexCommit(), null, indexShardSnapshotStatus, Version.CURRENT, Collections.emptyMap(), future); future.actionGet(); final PlainActionFuture<SnapshotInfo> finFuture = PlainActionFuture.newFuture(); repository.finalizeSnapshot(snapshotId, ShardGenerations.builder().put(indexId, 0, indexShardSnapshotStatus.generation()).build(), indexShardSnapshotStatus.asCopy().getStartTime(), null, 1, Collections.emptyList(), ESBlobStoreRepositoryIntegTestCase.getRepositoryData(repository).getGenId(), true, Metadata.builder().put(shard.indexSettings().getIndexMetadata(), false).build(), Collections.emptyMap(), Version.CURRENT, finFuture); finFuture.actionGet(); }); IndexShardSnapshotStatus.Copy copy = indexShardSnapshotStatus.asCopy(); assertEquals(copy.getTotalFileCount(), copy.getIncrementalFileCount()); assertEquals(copy.getStage(), IndexShardSnapshotStatus.Stage.DONE); } shard.refresh("test"); ShardRouting shardRouting = TestShardRouting.newShardRouting(new ShardId("index", "_na_", 0), randomAlphaOfLength(10), true, ShardRoutingState.INITIALIZING, new RecoverySource.SnapshotRecoverySource( UUIDs.randomBase64UUID(), new Snapshot("src_only", snapshotId), Version.CURRENT, indexId)); IndexMetadata metadata = runAsSnapshot(threadPool, () -> repository.getSnapshotIndexMetadata(snapshotId, indexId)); IndexShard restoredShard = newShard( shardRouting, metadata, null, SourceOnlySnapshotRepository.getEngineFactory(), () -> {}, RetentionLeaseSyncer.EMPTY); DiscoveryNode discoveryNode = new DiscoveryNode("node_g", buildNewFakeTransportAddress(), Version.CURRENT); restoredShard.markAsRecovering("test from snap", new RecoveryState(restoredShard.routingEntry(), discoveryNode, null)); runAsSnapshot(shard.getThreadPool(), () -> { final PlainActionFuture<Boolean> future = PlainActionFuture.newFuture(); restoredShard.restoreFromRepository(repository, future); assertTrue(future.actionGet()); }); assertEquals(restoredShard.recoveryState().getStage(), RecoveryState.Stage.DONE); assertEquals(restoredShard.recoveryState().getTranslog().recoveredOperations(), 0); assertEquals(IndexShardState.POST_RECOVERY, restoredShard.state()); restoredShard.refresh("test"); assertEquals(restoredShard.docStats().getCount(), shard.docStats().getCount()); EngineException engineException = expectThrows(EngineException.class, () -> restoredShard.get( new Engine.Get(false, false, Integer.toString(0), new Term("_id", Uid.encodeId(Integer.toString(0)))))); assertEquals(engineException.getCause().getMessage(), "_source only indices can't be searched or filtered"); SeqNoStats seqNoStats = restoredShard.seqNoStats(); assertEquals(seqNoStats.getMaxSeqNo(), seqNoStats.getLocalCheckpoint()); final IndexShard targetShard; try (Engine.Searcher searcher = restoredShard.acquireSearcher("test")) { assertEquals(searcher.getIndexReader().maxDoc(), seqNoStats.getLocalCheckpoint()); TopDocs search = searcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE); assertEquals(searcher.getIndexReader().numDocs(), search.totalHits.value); search = searcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE, new Sort(new SortField(SeqNoFieldMapper.NAME, SortField.Type.LONG)), false); assertEquals(searcher.getIndexReader().numDocs(), search.totalHits.value); long previous = -1; for (ScoreDoc doc : search.scoreDocs) { FieldDoc fieldDoc = (FieldDoc) doc; assertEquals(1, fieldDoc.fields.length); long current = (Long)fieldDoc.fields[0]; assertThat(previous, Matchers.lessThan(current)); previous = current; } expectThrows(UnsupportedOperationException.class, () -> searcher.search(new TermQuery(new Term("boom", "boom")), 1)); targetShard = reindex(searcher.getDirectoryReader(), new MappingMetadata("_doc", restoredShard.mapperService().documentMapper().meta())); } for (int i = 0; i < numInitialDocs; i++) { Engine.Get get = new Engine.Get(false, false, Integer.toString(i), new Term("_id", Uid.encodeId(Integer.toString(i)))); Engine.GetResult original = shard.get(get); Engine.GetResult restored = targetShard.get(get); assertEquals(original.exists(), restored.exists()); if (original.exists()) { Document document = original.docIdAndVersion().reader.document(original.docIdAndVersion().docId); Document restoredDocument = restored.docIdAndVersion().reader.document(restored.docIdAndVersion().docId); for (IndexableField field : document) { assertEquals(document.get(field.name()), restoredDocument.get(field.name())); } } IOUtils.close(original, restored); } closeShards(shard, restoredShard, targetShard); }	i'll double check with someone from the distributed team that this change is okay.
protected PutFilterAction.Response putMlFilter(MlFilter filter) { return client().execute(PutFilterAction.INSTANCE, new PutFilterAction.Request(filter)).actionGet(); }	why does this method have to be removed as part of this pr?
public void testCreatorRealmCaptureWillWorkWithClientRunAs() throws IOException { final String nativeTokenUsername = "native_token_user"; getSecurityClient().putUser(new User(nativeTokenUsername, "superuser"), SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING); final Client runAsClient = client().filterWithHeader( Map.of( "Authorization", UsernamePasswordToken.basicAuthHeaderValue(ES_TEST_ROOT_USER, TEST_PASSWORD_SECURE_STRING.clone()), AuthenticationServiceField.RUN_AS_USER_HEADER, nativeTokenUsername ) ); // Create a token with client credentials and run-as request header var createTokenRequest1 = new org.elasticsearch.xpack.core.security.action.token.CreateTokenRequest(); createTokenRequest1.setGrantType("client_credentials"); final PlainActionFuture<org.elasticsearch.xpack.core.security.action.token.CreateTokenResponse> future1 = new PlainActionFuture<>(); runAsClient.execute(CreateTokenAction.INSTANCE, createTokenRequest1, future1); final String accessToken = future1.actionGet().getTokenString(); var invalidateTokenRequest = new org.elasticsearch.xpack.core.security.action.token.InvalidateTokenRequest( null, null, "index", nativeTokenUsername ); final PlainActionFuture<org.elasticsearch.xpack.core.security.action.token.InvalidateTokenResponse> future2 = new PlainActionFuture<>(); client().execute(InvalidateTokenAction.INSTANCE, invalidateTokenRequest, future2); assertThat(future2.actionGet().getResult().getInvalidatedTokens().size(), equalTo(1)); // Create a token with password grant and run-as request header var createTokenRequest2 = new org.elasticsearch.xpack.core.security.action.token.CreateTokenRequest(); createTokenRequest2.setGrantType("password"); createTokenRequest2.setUsername(ES_TEST_ROOT_USER); createTokenRequest2.setPassword(TEST_PASSWORD_SECURE_STRING.clone()); final PlainActionFuture<org.elasticsearch.xpack.core.security.action.token.CreateTokenResponse> future3 = new PlainActionFuture<>(); runAsClient.execute(CreateTokenAction.INSTANCE, createTokenRequest2, future3); final String refreshToken = future3.actionGet().getRefreshToken(); var createTokenRequest3 = new org.elasticsearch.xpack.core.security.action.token.CreateTokenRequest(); createTokenRequest3.setGrantType("refresh_token"); createTokenRequest3.setRefreshToken(refreshToken); final PlainActionFuture<org.elasticsearch.xpack.core.security.action.token.CreateTokenResponse> future4 = new PlainActionFuture<>(); // refresh without run-as should fail client().filterWithHeader( Map.of("Authorization", UsernamePasswordToken.basicAuthHeaderValue(ES_TEST_ROOT_USER, TEST_PASSWORD_SECURE_STRING.clone())) ).execute(RefreshTokenAction.INSTANCE, createTokenRequest3, future4); expectThrows(ElasticsearchSecurityException.class, future4::actionGet); // refresh with run-as should work final PlainActionFuture<org.elasticsearch.xpack.core.security.action.token.CreateTokenResponse> future5 = new PlainActionFuture<>(); runAsClient.execute(RefreshTokenAction.INSTANCE, createTokenRequest3, future5); assertThat(future5.actionGet().getTokenString(), notNullValue()); }	should we authenticate with the token here? the test is checking that the token is created correctly with the right realm, and it feels like the best way to check both those things (token works, token is for correct user/realm) is via authenticate.
private DocumentMapper parse(String type, Map<String, Object> mapping, String defaultSource) throws MapperParsingException { if (type == null) { throw new MapperParsingException("Failed to derive type"); } if (defaultSource != null) { Tuple<String, Map<String, Object>> t = extractMapping(MapperService.DEFAULT_MAPPING, defaultSource); if (t.v2() != null) { XContentHelper.mergeDefaults(mapping, t.v2()); } } Mapper.TypeParser.ParserContext parserContext = parserContext(); // parse RootObjectMapper DocumentMapper.Builder docBuilder = doc(index.name(), indexSettings, (RootObjectMapper.Builder) rootObjectTypeParser.parse(type, mapping, parserContext)); Iterator<Map.Entry<String, Object>> iterator = mapping.entrySet().iterator(); // parse DocumentMapper while(iterator.hasNext()) { Map.Entry<String, Object> entry = iterator.next(); String fieldName = Strings.toUnderscoreCase(entry.getKey()); Object fieldNode = entry.getValue(); if ("index_analyzer".equals(fieldName)) { iterator.remove(); NamedAnalyzer analyzer = analysisService.analyzer(fieldNode.toString()); if (analyzer == null) { throw new MapperParsingException("Analyzer [" + fieldNode.toString() + "] not found for index_analyzer setting on root type [" + type + "]"); } docBuilder.indexAnalyzer(analyzer); } else if ("search_analyzer".equals(fieldName)) { iterator.remove(); NamedAnalyzer analyzer = analysisService.analyzer(fieldNode.toString()); if (analyzer == null) { throw new MapperParsingException("Analyzer [" + fieldNode.toString() + "] not found for search_analyzer setting on root type [" + type + "]"); } docBuilder.searchAnalyzer(analyzer); } else if ("search_quote_analyzer".equals(fieldName)) { iterator.remove(); NamedAnalyzer analyzer = analysisService.analyzer(fieldNode.toString()); if (analyzer == null) { throw new MapperParsingException("Analyzer [" + fieldNode.toString() + "] not found for search_analyzer setting on root type [" + type + "]"); } docBuilder.searchQuoteAnalyzer(analyzer); } else if ("analyzer".equals(fieldName)) { iterator.remove(); NamedAnalyzer analyzer = analysisService.analyzer(fieldNode.toString()); if (analyzer == null) { throw new MapperParsingException("Analyzer [" + fieldNode.toString() + "] not found for analyzer setting on root type [" + type + "]"); } docBuilder.indexAnalyzer(analyzer); docBuilder.searchAnalyzer(analyzer); } else { Mapper.TypeParser typeParser = rootTypeParsers.get(fieldName); if (typeParser != null) { iterator.remove(); docBuilder.put(typeParser.parse(fieldName, (Map<String, Object>) fieldNode, parserContext)); } } } ImmutableMap<String, Object> attributes = ImmutableMap.of(); if (mapping.containsKey("_meta")) { attributes = ImmutableMap.copyOf((Map<String, Object>) mapping.get("_meta")); mapping.remove("_meta"); } docBuilder.meta(attributes); if (!mapping.isEmpty()) { String remainingFields = ""; for (String key : mapping.keySet()) { remainingFields += " [" + key + " : " + mapping.get(key).toString() + "]"; } throw new MapperParsingException("Root type mapping not empty after parsing! Remaining fields:" + remainingFields); } if (!docBuilder.hasIndexAnalyzer()) { docBuilder.indexAnalyzer(analysisService.defaultIndexAnalyzer()); } if (!docBuilder.hasSearchAnalyzer()) { docBuilder.searchAnalyzer(analysisService.defaultSearchAnalyzer()); } if (!docBuilder.hasSearchQuoteAnalyzer()) { docBuilder.searchAnalyzer(analysisService.defaultSearchQuoteAnalyzer()); } DocumentMapper documentMapper = docBuilder.build(this); // update the source with the generated one documentMapper.refreshSource(); return documentMapper; } @SuppressWarnings({"unchecked"}	you can replace the call to get with a call to remove to not need to remove the _meta key on the next line
private DocumentMapper parse(String type, Map<String, Object> mapping, String defaultSource) throws MapperParsingException { if (type == null) { throw new MapperParsingException("Failed to derive type"); } if (defaultSource != null) { Tuple<String, Map<String, Object>> t = extractMapping(MapperService.DEFAULT_MAPPING, defaultSource); if (t.v2() != null) { XContentHelper.mergeDefaults(mapping, t.v2()); } } Mapper.TypeParser.ParserContext parserContext = parserContext(); // parse RootObjectMapper DocumentMapper.Builder docBuilder = doc(index.name(), indexSettings, (RootObjectMapper.Builder) rootObjectTypeParser.parse(type, mapping, parserContext)); Iterator<Map.Entry<String, Object>> iterator = mapping.entrySet().iterator(); // parse DocumentMapper while(iterator.hasNext()) { Map.Entry<String, Object> entry = iterator.next(); String fieldName = Strings.toUnderscoreCase(entry.getKey()); Object fieldNode = entry.getValue(); if ("index_analyzer".equals(fieldName)) { iterator.remove(); NamedAnalyzer analyzer = analysisService.analyzer(fieldNode.toString()); if (analyzer == null) { throw new MapperParsingException("Analyzer [" + fieldNode.toString() + "] not found for index_analyzer setting on root type [" + type + "]"); } docBuilder.indexAnalyzer(analyzer); } else if ("search_analyzer".equals(fieldName)) { iterator.remove(); NamedAnalyzer analyzer = analysisService.analyzer(fieldNode.toString()); if (analyzer == null) { throw new MapperParsingException("Analyzer [" + fieldNode.toString() + "] not found for search_analyzer setting on root type [" + type + "]"); } docBuilder.searchAnalyzer(analyzer); } else if ("search_quote_analyzer".equals(fieldName)) { iterator.remove(); NamedAnalyzer analyzer = analysisService.analyzer(fieldNode.toString()); if (analyzer == null) { throw new MapperParsingException("Analyzer [" + fieldNode.toString() + "] not found for search_analyzer setting on root type [" + type + "]"); } docBuilder.searchQuoteAnalyzer(analyzer); } else if ("analyzer".equals(fieldName)) { iterator.remove(); NamedAnalyzer analyzer = analysisService.analyzer(fieldNode.toString()); if (analyzer == null) { throw new MapperParsingException("Analyzer [" + fieldNode.toString() + "] not found for analyzer setting on root type [" + type + "]"); } docBuilder.indexAnalyzer(analyzer); docBuilder.searchAnalyzer(analyzer); } else { Mapper.TypeParser typeParser = rootTypeParsers.get(fieldName); if (typeParser != null) { iterator.remove(); docBuilder.put(typeParser.parse(fieldName, (Map<String, Object>) fieldNode, parserContext)); } } } ImmutableMap<String, Object> attributes = ImmutableMap.of(); if (mapping.containsKey("_meta")) { attributes = ImmutableMap.copyOf((Map<String, Object>) mapping.get("_meta")); mapping.remove("_meta"); } docBuilder.meta(attributes); if (!mapping.isEmpty()) { String remainingFields = ""; for (String key : mapping.keySet()) { remainingFields += " [" + key + " : " + mapping.get(key).toString() + "]"; } throw new MapperParsingException("Root type mapping not empty after parsing! Remaining fields:" + remainingFields); } if (!docBuilder.hasIndexAnalyzer()) { docBuilder.indexAnalyzer(analysisService.defaultIndexAnalyzer()); } if (!docBuilder.hasSearchAnalyzer()) { docBuilder.searchAnalyzer(analysisService.defaultSearchAnalyzer()); } if (!docBuilder.hasSearchQuoteAnalyzer()) { docBuilder.searchAnalyzer(analysisService.defaultSearchQuoteAnalyzer()); } DocumentMapper documentMapper = docBuilder.build(this); // update the source with the generated one documentMapper.refreshSource(); return documentMapper; } @SuppressWarnings({"unchecked"}	can you use a stringbuilder?
public Mapper.Builder parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { ObjectMapper.Builder builder = createBuilder(name); Iterator<Map.Entry<String, Object>> iterator = node.entrySet().iterator(); while (iterator.hasNext()) { Map.Entry<String, Object> entry = iterator.next(); String fieldName = Strings.toUnderscoreCase(entry.getKey()); Object fieldNode = entry.getValue(); parseObjectOrDocumentTyeProperties( fieldName, fieldNode, parserContext, builder); parseObjectProperties(name, fieldName, fieldNode, builder); } parseNested(name, node, builder); return builder; }	since you never call iterator.remove() in this loop, let's switch back to the for-each syntax?
@Override public Mapper.Builder parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException { ObjectMapper.Builder builder = createBuilder(name); Iterator<Map.Entry<String, Object>> iterator = node.entrySet().iterator(); while (iterator.hasNext()) { Map.Entry<String, Object> entry = iterator.next(); String fieldName = Strings.toUnderscoreCase(entry.getKey()); Object fieldNode = entry.getValue(); if (parseObjectOrDocumentTyeProperties( fieldName, fieldNode, parserContext, builder)) { iterator.remove(); } if (processField(builder,fieldName, fieldNode)) { iterator.remove(); } } return builder; }	or maybe directly: java if (parseobjectordocumenttyeproperties( fieldname, fieldnode, parsercontext, builder) || processfield(builder,fieldname, fieldnode)) { iterator.remove(); }
@Test(expected = MapperParsingException.class) public void testMisplacedTypeInRoot() throws IOException { String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/all/misplaced_type_in_root.json"); DocumentMapper docMapper = MapperTestUtils.newParser().parse("test", mapping); }	do you mean "related" instead of "unrelated"?
@Test public void testPercolationWithDynamicTemplates() throws Exception { assertAcked(prepareCreate("idx").addMapping("type", jsonBuilder().startObject().startObject("type") .field("dynamic", false) .startObject("properties") .startObject("custom") .field("dynamic", true) .field("type", "object") .field("incude_in_all", false) .endObject() .endObject() .startArray("dynamic_templates") .startObject() .startObject("custom_fields") .field("path_match", "custom.*") .startObject("mapping") .field("index", "not_analyzed") .endObject() .endObject() .endObject() .endArray() .endObject().endObject())); client().prepareIndex("idx", PercolatorService.TYPE_NAME, "1") .setSource(jsonBuilder().startObject().field("query", QueryBuilders.queryString("color:red")).endObject()) .get(); client().prepareIndex("idx", PercolatorService.TYPE_NAME, "2") .setSource(jsonBuilder().startObject().field("query", QueryBuilders.queryString("color:blue")).endObject()) .get(); PercolateResponse percolateResponse = client().preparePercolate().setDocumentType("type") .setPercolateDoc(new PercolateSourceBuilder.DocBuilder().setDoc(jsonBuilder().startObject().startObject("custom").field("color", "blue").endObject().endObject())) .get(); assertMatchCount(percolateResponse, 0l); assertThat(percolateResponse.getMatches(), arrayWithSize(0)); // wait until the mapping change has propagated from the percolate request client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).execute().actionGet(); awaitBusy(new Predicate<Object>() { @Override public boolean apply(Object input) { PendingClusterTasksResponse pendingTasks = client().admin().cluster().preparePendingClusterTasks().get(); return pendingTasks.pendingTasks().isEmpty(); } }); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).execute().actionGet(); // The previous percolate request introduced the custom.color field, so now we register the query again // and the field name `color` will be resolved to `custom.color` field in mapping via smart field mapping resolving. client().prepareIndex("idx", PercolatorService.TYPE_NAME, "2") .setSource(jsonBuilder().startObject().field("query", QueryBuilders.queryString("color:blue")).endObject()) .get(); // The second request will yield a match, since the query during the proper field during parsing. percolateResponse = client().preparePercolate().setDocumentType("type") .setPercolateDoc(new PercolateSourceBuilder.DocBuilder().setDoc(jsonBuilder().startObject().startObject("custom").field("color", "blue").endObject().endObject())) .get(); assertMatchCount(percolateResponse, 1l); assertThat(percolateResponse.getMatches()[0].getId().string(), equalTo("2")); }	nice that it found such issues!
private static void validateDataStreamsStillReferenced(ClusterState state, String templateName, ComposableIndexTemplate newTemplate) { final Set<String> dataStreams = state.metadata().dataStreams().keySet(); Function<Metadata, Set<String>> findUnreferencedDataStreams = meta -> { final Set<String> unreferenced = new HashSet<>(); // For each data stream that we have, see whether it's covered by a different // template (which is great), or whether it's now uncovered by any template for (String dataStream : dataStreams) { final String matchingTemplate = findV2Template(meta, dataStream, false); if (matchingTemplate == null) { unreferenced.add(dataStream); } else { // We found a template that still matches, great! Buuuuttt... check whether it // is a data stream template, as it's only useful if it has a data stream definition if (meta.templatesV2().get(matchingTemplate).getDataStreamTemplate() == null) { unreferenced.add(dataStream); } } } return unreferenced; }; // Find data streams that are currently unreferenced final Set<String> currentlyUnreferenced = findUnreferencedDataStreams.apply(state.metadata()); // Generate a metadata as if the new template were actually in the cluster state final Metadata updatedMetadata = Metadata.builder(state.metadata()).put(templateName, newTemplate).build(); // Find the data streams that would be unreferenced now that the template is updated/added final Set<String> newlyUnreferenced = findUnreferencedDataStreams.apply(updatedMetadata); // If we found any data streams that used to be covered, but will no longer be covered by // changing this template, then blow up with as much helpful information as we can muster if (newlyUnreferenced.size() > currentlyUnreferenced.size()) { throw new IllegalArgumentException("composable template [" + templateName + "] with index patterns " + newTemplate.indexPatterns() + ", priority [" + newTemplate.priority() + "] " + (newTemplate.getDataStreamTemplate() == null ? "and no data stream configuration " : "") + "would cause data streams " + newlyUnreferenced + " to no longer match a data stream template"); } }	if we read the data streams inside this function we can make it static. what do you think?
AmazonS3 buildClient(final S3ClientSettings clientSettings) { final AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard(); builder.withCredentials(buildCredentials(logger, clientSettings)); builder.withClientConfiguration(buildConfiguration(clientSettings)); final String endpoint = Strings.hasLength(clientSettings.endpoint) ? clientSettings.endpoint : Constants.S3_HOSTNAME; logger.debug("using endpoint [{}]", endpoint); // If the endpoint configuration isn't set on the builder then the default behaviour is to try // and work out what region we are in and use an appropriate endpoint - see AwsClientBuilder#setRegion. // In contrast, directly-constructed clients use s3.amazonaws.com unless otherwise instructed. We currently // use a directly-constructed client, and need to keep the existing behaviour to avoid a breaking change, // so to move to using the builder we must set it explicitly to keep the existing behaviour. // // We do this because directly constructing the client is deprecated (was already deprecated in 1.1.223 too) // so this change removes that usage of a deprecated API. builder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endpoint, null)); if (clientSettings.pathStyleAccess) { builder.enablePathStyleAccess(); } if (clientSettings.disableChunkedEncoding) { builder.withChunkedEncodingDisabled(true); } return builder.build(); }	slight preference for this, avoiding the boolean parameter: suggestion builder.disablechunkedencoding();
@Override boolean doPush(PointValues.Relation relation) { if (relation == PointValues.Relation.CELL_OUTSIDE_QUERY) { return false; } else if (relation == PointValues.Relation.CELL_INSIDE_QUERY) { answer = false; return false; } else { return true; } } } /** * within visitor stops as soon as there is one triangle that is not within the component */ private static class WithinVisitor extends Component2DVisitor { boolean answer; private WithinVisitor(Component2D component2D, CoordinateEncoder encoder) { super(component2D, encoder); answer = true; } @Override public boolean matches() { return answer; } @Override public void reset() { answer = true; } @Override void doVisitPoint(double x, double y) { answer = component2D.contains(x, y); } @Override void doVisitLine(double aX, double aY, double bX, double bY, byte metadata) { answer = component2D.containsLine(aX, aY, bX, bY); } @Override void doVisitTriangle(double aX, double aY, double bX, double bY, double cX, double cY, byte metadata) { answer = component2D.containsTriangle(aX, aY, bX, bY, cX, cY); } @Override public boolean push() { return answer; } @Override public boolean pushX(int minX) { answer = super.pushX(minX); return answer; } @Override public boolean pushY(int minY) { answer = super.pushY(minY); return answer; } @Override public boolean push(int maxX, int maxY) { answer = super.push(maxX, maxY); return answer; } @Override boolean doPush(PointValues.Relation relation) { if (relation == PointValues.Relation.CELL_OUTSIDE_QUERY) { answer = false; } return answer; } } /** * contains visitor stops as soon as there is one triangle that intersects the component * with an edge belonging to the original polygon. */ private static class ContainsVisitor extends Component2DVisitor { Component2D.WithinRelation answer; private ContainsVisitor(Component2D component2D, CoordinateEncoder encoder) { super(component2D, encoder); answer = Component2D.WithinRelation.DISJOINT; } @Override public boolean matches() { return answer == Component2D.WithinRelation.CANDIDATE; } @Override public void reset() { answer = Component2D.WithinRelation.DISJOINT; } @Override void doVisitPoint(double x, double y) { final Component2D.WithinRelation rel = component2D.withinPoint(x, y); if (rel != Component2D.WithinRelation.DISJOINT) { answer = rel; } } @Override void doVisitLine(double aX, double aY, double bX, double bY, byte metadata) { final boolean ab = (metadata & 1 << 4) == 1 << 4; final Component2D.WithinRelation rel = component2D.withinLine(aX, aY, ab, bX, bY); if (rel != Component2D.WithinRelation.DISJOINT) { answer = rel; } } @Override void doVisitTriangle(double aX, double aY, double bX, double bY, double cX, double cY, byte metadata) { final boolean ab = (metadata & 1 << 4) == 1 << 4; final boolean bc = (metadata & 1 << 5) == 1 << 5; final boolean ca = (metadata & 1 << 6) == 1 << 6; final Component2D.WithinRelation rel = component2D.withinTriangle(aX, aY, ab, bX, bY, bc, cX, cY, ca); if (rel != Component2D.WithinRelation.DISJOINT) { answer = rel; } } @Override public boolean push() { return answer != Component2D.WithinRelation.NOTWITHIN; } @Override boolean doPush(PointValues.Relation relation) { return relation == PointValues.Relation.CELL_CROSSES_QUERY; }	i think it would be nice to explain the rational behind each of these default answer values (here as well as the other visitors), especially since they are not the same for different visitors. it seems like the default value is assumed in some of the logic
public void apply(Project project) { final String url = System.getenv("SCA_URL"); final String token = System.getenv("SCA_TOKEN"); TaskProvider<DependenciesGraphTask> depsGraph = project.getTasks().register("dependenciesGraph", DependenciesGraphTask.class); depsGraph.configure(t -> { project.getPlugins().withType(JavaPlugin.class, p -> { t.setRuntimeConfiguration(project.getConfigurations().getByName(JavaPlugin.RUNTIME_CLASSPATH_CONFIGURATION_NAME)); t.setToken(token); t.setUrl(url); }); }); project.getGradle().getTaskGraph().whenReady(graph -> { if (graph.getAllTasks().stream().anyMatch(t -> t instanceof DependenciesGraphTask)) { if (url == null || token == null) { throw new GradleException("The environment variables SCA_URL and SCA_TOKEN need to be set before this task is run"); } } }); }	let's make the message "the environment variables sca_url and sca_token need to be set before task " + t.getpath() +" can run" as it won't be clear to the user what we mean by "this task" w/o that context.
public void disableMonitoring() throws Exception { final Settings settings = Settings.builder() .putNull("xpack.monitoring.collection.enabled") .putNull("xpack.monitoring.exporters._local.type") .putNull("xpack.monitoring.exporters._local.enabled") .putNull("cluster.metadata.display_name") .build(); assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings)); assertBusy(() -> assertThat("Exporters are not yet stopped", getMonitoringUsageExportersDefined(), is(false))); assertBusy(() -> { try { // now wait until Monitoring has actually stopped final NodesStatsResponse response = client().admin().cluster().prepareNodesStats().clear().setThreadPool(true).get(); for (final NodeStats nodeStats : response.getNodes()) { boolean foundBulkThreads = false; for(final ThreadPoolStats.Stats threadPoolStats : nodeStats.getThreadPool()) { if (WRITE.equals(threadPoolStats.getName())) { foundBulkThreads = true; assertThat("Still some active _bulk threads!", threadPoolStats.getActive(), equalTo(0)); break; } } assertThat("Could not find bulk thread pool", foundBulkThreads, is(true)); } } catch (Exception e) { throw new ElasticsearchException("Failed to wait for monitoring exporters to stop:", e); } }, 30L, TimeUnit.SECONDS); }	this is necessary since this test (and some others here) set the type via [node level setting](https://github.com/elastic/elasticsearch/pull/57399/files#diff-bab5e7d9636db9cadcd30fa5c91d4fc9r80) but the settings validation does not validate across node level vs. dynamic settings (they get merged after independent validation). it also looks like we don't support validation across persistent vs. transient settings either, which makes any of these cross settings dependencies validation a bit terse. this translates to a real change in behavior w.r.t monitoring, but cross level validation is a bigger issue then monitoring. the safety here (imo) outweighs the convenience of cross type setting validation (and already exists for all http type settings).
public void persistProgress(Runnable runnable) { persistProgress(client, taskParams.getId(), runnable); }	i think it would be beneficial to log the "persisted progress" object in the debug logger. this would probably require a setonce or something to grab the progresstostore object.
public static boolean isFastIterator(DocIdSet set) { return set instanceof FixedBitSet || set instanceof RoaringDocIdSet; } /** * Converts to a cacheable {@link DocIdSet} * <p/> * Note, we don't use {@link org.apache.lucene.search.DocIdSet#isCacheable()} because execution * might be expensive even if its cacheable (i.e. not going back to the reader to execute). We effectively * always either return an empty {@link DocIdSet} or {@link FixedBitSet}	the iterator in the roaringdocidset is at best o(1) and at worst o(logn) (n = present/absent docs), so i'd call it fast enough to meet the comment's criteria.
public void testFieldsParsing() throws Exception { byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/fieldstats/" + "fieldstats-index-constraints-request.json"); FieldStatsRequest request = new FieldStatsRequest(); request.source(new BytesArray(data)); assertThat(request.getFields().length, equalTo(5)); assertThat(request.getFields()[0], equalTo("field1")); assertThat(request.getFields()[1], equalTo("field2")); assertThat(request.getFields()[2], equalTo("field3")); assertThat(request.getFields()[3], equalTo("field4")); assertThat(request.getFields()[4], equalTo("field5")); assertThat(request.getIndexConstraints().length, equalTo(8)); assertThat(request.getIndexConstraints()[0].getField(), equalTo("field2")); assertThat(request.getIndexConstraints()[0].getValue(), equalTo("9")); assertThat(request.getIndexConstraints()[0].getProperty(), equalTo(MAX)); assertThat(request.getIndexConstraints()[0].getComparison(), equalTo(GTE)); assertThat(request.getIndexConstraints()[1].getField(), equalTo("field3")); assertThat(request.getIndexConstraints()[1].getValue(), equalTo("5")); assertThat(request.getIndexConstraints()[1].getProperty(), equalTo(MIN)); assertThat(request.getIndexConstraints()[1].getComparison(), equalTo(GT)); assertThat(request.getIndexConstraints()[2].getField(), equalTo("field4")); assertThat(request.getIndexConstraints()[2].getValue(), equalTo("a")); assertThat(request.getIndexConstraints()[2].getProperty(), equalTo(MIN)); assertThat(request.getIndexConstraints()[2].getComparison(), equalTo(GTE)); assertThat(request.getIndexConstraints()[3].getField(), equalTo("field4")); assertThat(request.getIndexConstraints()[3].getValue(), equalTo("g")); assertThat(request.getIndexConstraints()[3].getProperty(), equalTo(MAX)); assertThat(request.getIndexConstraints()[3].getComparison(), equalTo(LTE)); assertThat(request.getIndexConstraints()[4].getField(), equalTo("field5")); assertThat(request.getIndexConstraints()[4].getValue(), equalTo("2")); assertThat(request.getIndexConstraints()[4].getProperty(), equalTo(MIN)); assertThat(request.getIndexConstraints()[4].getComparison(), equalTo(GT)); assertThat(request.getIndexConstraints()[5].getField(), equalTo("field5")); assertThat(request.getIndexConstraints()[5].getValue(), equalTo("9")); assertThat(request.getIndexConstraints()[5].getProperty(), equalTo(MAX)); assertThat(request.getIndexConstraints()[5].getComparison(), equalTo(LT)); assertThat(request.getIndexConstraints()[6].getField(), equalTo("field1")); assertThat(request.getIndexConstraints()[6].getValue(), equalTo("2014-01-01")); assertThat(request.getIndexConstraints()[6].getProperty(), equalTo(MIN)); assertThat(request.getIndexConstraints()[6].getComparison(), equalTo(GTE)); assertThat(request.getIndexConstraints()[6].getOptionalFormat(), equalTo("date_optional_time")); assertThat(request.getIndexConstraints()[7].getField(), equalTo("field1")); assertThat(request.getIndexConstraints()[7].getValue(), equalTo("2015-01-01")); assertThat(request.getIndexConstraints()[7].getProperty(), equalTo(MAX)); assertThat(request.getIndexConstraints()[7].getComparison(), equalTo(LT)); assertThat(request.getIndexConstraints()[7].getOptionalFormat(), equalTo("date_optional_time")); }	for those following along at home, this is required because the example data had a duplicate key and @danielmitterdorfer switched it to min in the file.
public void testRepeatedConstructorParam() throws IOException { XContentParser parser = XContentType.JSON.xContent().createParser( "{\\\\n" + " \\\\"vegetable\\\\": 1,\\\\n" + " \\\\"vegetable\\\\": 2\\\\n" + "}"); Throwable e = expectThrows(ParsingException.class, () -> randomFrom(HasCtorArguments.ALL_PARSERS).apply(parser, MATCHER)); assertEquals("[has_required_arguments] failed to parse object", e.getMessage()); e = e.getCause(); assertThat(e, instanceOf(JsonParseException.class)); assertEquals("Duplicate field 'vegetable'", e.getMessage()); }	so this is one problem with doing this as a read-on-startup property. we can no longer test things like this. this test is fairly worthless unless the property is false and we have to manually perform the duplication detection. i think it'd be worth leaving a comment about that.
public void testTooManyQueriesInObject() throws IOException { String clauseType = randomFrom("must", "should", "must_not", "filter"); // should also throw error if invalid query is preceded by a valid one String query = "{\\\\n" + " \\\\"bool\\\\": {\\\\n" + " \\\\"" + clauseType + "\\\\": {\\\\n" + " \\\\"match\\\\": {\\\\n" + " \\\\"foo\\\\": \\\\"bar\\\\"\\\\n" + " },\\\\n" + " \\\\"match\\\\": {\\\\n" + " \\\\"baz\\\\": \\\\"buzz\\\\"\\\\n" + " }\\\\n" + " }\\\\n" + " }\\\\n" + "}"; JsonParseException ex = expectThrows(JsonParseException.class, () -> parseQuery(query, ParseFieldMatcher.EMPTY)); assertEquals("Duplicate field 'match'", ex.getMessage()); }	same here - this test is fairly out of date now.
public void testMultipleFilterElements() throws IOException { String queryString = "{ \\\\"" + ConstantScoreQueryBuilder.NAME + "\\\\" : {\\\\n" + "\\\\"filter\\\\" : { \\\\"term\\\\": { \\\\"foo\\\\": \\\\"a\\\\" } },\\\\n" + "\\\\"filter\\\\" : { \\\\"term\\\\": { \\\\"foo\\\\": \\\\"x\\\\" } },\\\\n" + "} }"; JsonParseException e = expectThrows(JsonParseException.class, () -> parseQuery(queryString)); assertThat(e.getMessage(), containsString("Duplicate field 'filter'")); } /** * test that "filter" does not accept an array of queries, throws {@link ParsingException}	same here. i wonder if it makes more sense to leave these tests as they are and skip them if the value of the duplicate check property is true. that way if we really want we can set it to false and run the tests again. and if they have a compile time reference to the duplicate check property when we remove the duplicate check property the test will stop compiling and we can use that as a reminder to remove that particular manual duplicate check.
@Override public final Query termQuery(Object value, QueryShardContext context) { String pattern = valueToString(value); if (matches(pattern, context)) { if (context != null && context.getMapperService().hasNested()) { // type filters are expected not to match nested docs return Queries.newNonNestedFilter(context.indexVersionCreated()); } return Queries.newMatchAllQuery(); } else { return new MatchNoDocsQuery(); } }	see comment in tests below, i wasn't sure if on 7.x we still need this so left it here for discussion.
public void testTermsQuery() throws Exception { QueryShardContext context = Mockito.mock(QueryShardContext.class); Version indexVersionCreated = VersionUtils.randomVersionBetween(random(), Version.V_6_0_0, Version.CURRENT); Settings indexSettings = Settings.builder() .put(IndexMetaData.SETTING_VERSION_CREATED, indexVersionCreated) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_INDEX_UUID, UUIDs.randomBase64UUID()).build(); IndexMetaData indexMetaData = IndexMetaData.builder(IndexMetaData.INDEX_UUID_NA_VALUE).settings(indexSettings).build(); IndexSettings mockSettings = new IndexSettings(indexMetaData, Settings.EMPTY); Mockito.when(context.getIndexSettings()).thenReturn(mockSettings); Mockito.when(context.indexVersionCreated()).thenReturn(indexVersionCreated); MapperService mapperService = Mockito.mock(MapperService.class); Mockito.when(mapperService.documentMapper()).thenReturn(null); Mockito.when(context.getMapperService()).thenReturn(mapperService); DocumentMapper mapper = Mockito.mock(DocumentMapper.class); Mockito.when(mapper.type()).thenReturn("_doc"); Mockito.when(mapperService.documentMapper()).thenReturn(mapper); TypeFieldMapper.TypeFieldType ft = new TypeFieldMapper.TypeFieldType(); ft.setName(TypeFieldMapper.NAME); Query query = ft.termQuery("_doc", context); assertEquals(new MatchAllDocsQuery(), query); query = ft.termQuery("other_type", context); assertEquals(new MatchNoDocsQuery(), query); Mockito.when(mapper.type()).thenReturn("other_type"); query = ft.termQuery("other_type", context); assertEquals(new MatchAllDocsQuery(), query); Mockito.when(mapperService.hasNested()).thenReturn(true); query = ft.termQuery("other_type", context); assertEquals(Queries.newNonNestedFilter(context.indexVersionCreated()), query); }	i left this test and added this special case in constantfieldtype#termquery because we might still need to support it on 7.x? happy to remove if its safe but i'm not too familiar with this area.
public void testInvalidGroupByNames() throws IOException { String invalidName = randomAlphaOfLengthBetween(0, 5) + ILLEGAL_FIELD_NAME_CHARACTERS[randomIntBetween(0,ILLEGAL_FIELD_NAME_CHARACTERS.length - 1)] + randomAlphaOfLengthBetween(0, 5); XContentBuilder source = JsonXContent.contentBuilder() .startObject() .startObject(invalidName) .startObject("terms") .field("field", "user") .endObject() .endObject() .endObject(); // lenient, passes but reports invalid try (XContentParser parser = createParser(source)) { GroupConfig groupConfig = GroupConfig.fromXContent(parser, true); assertFalse(groupConfig.isValid()); } // strict throws try (XContentParser parser = createParser(source)) { Exception e = expectThrows(ParsingException.class, () -> GroupConfig.fromXContent(parser, false)); assertTrue(e.getMessage().startsWith("Invalid group name")); } }	please put space after comma (",")
@Override public int hashCode() { return Objects.hash(confusionMatrix, otherActualClassCount); } } public static class ActualClass implements ToXContentObject { private static final ParseField ACTUAL_CLASS = new ParseField("actual_class"); private static final ParseField ACTUAL_CLASS_DOC_COUNT = new ParseField("actual_class_doc_count"); private static final ParseField PREDICTED_CLASSES = new ParseField("predicted_classes"); private static final ParseField OTHER_PREDICTED_CLASS_DOC_COUNT = new ParseField("other_predicted_class_doc_count"); @SuppressWarnings("unchecked") private static final ConstructingObjectParser<ActualClass, Void> PARSER = new ConstructingObjectParser<>( "multiclass_confusion_matrix_actual_class", true, a -> new ActualClass((String) a[0], (long) a[1], (List<PredictedClass>) a[2], (long) a[3])); static { PARSER.declareString(constructorArg(), ACTUAL_CLASS); PARSER.declareLong(constructorArg(), ACTUAL_CLASS_DOC_COUNT); PARSER.declareObjectArray(constructorArg(), PredictedClass.PARSER, PREDICTED_CLASSES); PARSER.declareLong(constructorArg(), OTHER_PREDICTED_CLASS_DOC_COUNT); } private final String actualClass; private final long actualClassDocCount; private final List<PredictedClass> predictedClasses; private final long otherPredictedClassDocCount; public ActualClass( String actualClass, long actualClassDocCount, List<PredictedClass> predictedClasses, long otherPredictedClassDocCount) { this.actualClass = actualClass; this.actualClassDocCount = actualClassDocCount; this.predictedClasses = Collections.unmodifiableList(predictedClasses); this.otherPredictedClassDocCount = otherPredictedClassDocCount; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(ACTUAL_CLASS.getPreferredName(), actualClass); builder.field(ACTUAL_CLASS_DOC_COUNT.getPreferredName(), actualClassDocCount); builder.field(PREDICTED_CLASSES.getPreferredName(), predictedClasses); builder.field(OTHER_PREDICTED_CLASS_DOC_COUNT.getPreferredName(), otherPredictedClassDocCount); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ActualClass that = (ActualClass) o; return Objects.equals(this.actualClass, that.actualClass) && this.actualClassDocCount == that.actualClassDocCount && Objects.equals(this.predictedClasses, that.predictedClasses) && this.otherPredictedClassDocCount == that.otherPredictedClassDocCount; } @Override public int hashCode() { return Objects.hash(actualClass, actualClassDocCount, predictedClasses, otherPredictedClassDocCount); } @Override public String toString() { return Strings.toString(this); } } public static class PredictedClass implements ToXContentObject { private static final ParseField PREDICTED_CLASS = new ParseField("predicted_class"); private static final ParseField COUNT = new ParseField("count"); @SuppressWarnings("unchecked") private static final ConstructingObjectParser<PredictedClass, Void> PARSER = new ConstructingObjectParser<>( "multiclass_confusion_matrix_predicted_class", true, a -> new PredictedClass((String) a[0], (long) a[1])); static { PARSER.declareString(constructorArg(), PREDICTED_CLASS); PARSER.declareLong(constructorArg(), COUNT); } private final String predictedClass; private final Long count; public PredictedClass(String predictedClass, Long count) { this.predictedClass = predictedClass; this.count = count; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(PREDICTED_CLASS.getPreferredName(), predictedClass); builder.field(COUNT.getPreferredName(), count); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; PredictedClass that = (PredictedClass) o; return Objects.equals(this.predictedClass, that.predictedClass) && Objects.equals(this.count, that.count); } @Override public int hashCode() { return Objects.hash(predictedClass, count); }	should this be nullable?
@Override public int hashCode() { return Objects.hash(confusionMatrix, otherActualClassCount); } } public static class ActualClass implements ToXContentObject { private static final ParseField ACTUAL_CLASS = new ParseField("actual_class"); private static final ParseField ACTUAL_CLASS_DOC_COUNT = new ParseField("actual_class_doc_count"); private static final ParseField PREDICTED_CLASSES = new ParseField("predicted_classes"); private static final ParseField OTHER_PREDICTED_CLASS_DOC_COUNT = new ParseField("other_predicted_class_doc_count"); @SuppressWarnings("unchecked") private static final ConstructingObjectParser<ActualClass, Void> PARSER = new ConstructingObjectParser<>( "multiclass_confusion_matrix_actual_class", true, a -> new ActualClass((String) a[0], (long) a[1], (List<PredictedClass>) a[2], (long) a[3])); static { PARSER.declareString(constructorArg(), ACTUAL_CLASS); PARSER.declareLong(constructorArg(), ACTUAL_CLASS_DOC_COUNT); PARSER.declareObjectArray(constructorArg(), PredictedClass.PARSER, PREDICTED_CLASSES); PARSER.declareLong(constructorArg(), OTHER_PREDICTED_CLASS_DOC_COUNT); } private final String actualClass; private final long actualClassDocCount; private final List<PredictedClass> predictedClasses; private final long otherPredictedClassDocCount; public ActualClass( String actualClass, long actualClassDocCount, List<PredictedClass> predictedClasses, long otherPredictedClassDocCount) { this.actualClass = actualClass; this.actualClassDocCount = actualClassDocCount; this.predictedClasses = Collections.unmodifiableList(predictedClasses); this.otherPredictedClassDocCount = otherPredictedClassDocCount; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(ACTUAL_CLASS.getPreferredName(), actualClass); builder.field(ACTUAL_CLASS_DOC_COUNT.getPreferredName(), actualClassDocCount); builder.field(PREDICTED_CLASSES.getPreferredName(), predictedClasses); builder.field(OTHER_PREDICTED_CLASS_DOC_COUNT.getPreferredName(), otherPredictedClassDocCount); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ActualClass that = (ActualClass) o; return Objects.equals(this.actualClass, that.actualClass) && this.actualClassDocCount == that.actualClassDocCount && Objects.equals(this.predictedClasses, that.predictedClasses) && this.otherPredictedClassDocCount == that.otherPredictedClassDocCount; } @Override public int hashCode() { return Objects.hash(actualClass, actualClassDocCount, predictedClasses, otherPredictedClassDocCount); } @Override public String toString() { return Strings.toString(this); } } public static class PredictedClass implements ToXContentObject { private static final ParseField PREDICTED_CLASS = new ParseField("predicted_class"); private static final ParseField COUNT = new ParseField("count"); @SuppressWarnings("unchecked") private static final ConstructingObjectParser<PredictedClass, Void> PARSER = new ConstructingObjectParser<>( "multiclass_confusion_matrix_predicted_class", true, a -> new PredictedClass((String) a[0], (long) a[1])); static { PARSER.declareString(constructorArg(), PREDICTED_CLASS); PARSER.declareLong(constructorArg(), COUNT); } private final String predictedClass; private final Long count; public PredictedClass(String predictedClass, Long count) { this.predictedClass = predictedClass; this.count = count; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(PREDICTED_CLASS.getPreferredName(), predictedClass); builder.field(COUNT.getPreferredName(), count); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; PredictedClass that = (PredictedClass) o; return Objects.equals(this.predictedClass, that.predictedClass) && Objects.equals(this.count, that.count); } @Override public int hashCode() { return Objects.hash(predictedClass, count); }	should this be nullable? i think it would throw if predictedclasses == null
@Override public int hashCode() { return Objects.hash(confusionMatrix, otherActualClassCount); } } public static class ActualClass implements ToXContentObject { private static final ParseField ACTUAL_CLASS = new ParseField("actual_class"); private static final ParseField ACTUAL_CLASS_DOC_COUNT = new ParseField("actual_class_doc_count"); private static final ParseField PREDICTED_CLASSES = new ParseField("predicted_classes"); private static final ParseField OTHER_PREDICTED_CLASS_DOC_COUNT = new ParseField("other_predicted_class_doc_count"); @SuppressWarnings("unchecked") private static final ConstructingObjectParser<ActualClass, Void> PARSER = new ConstructingObjectParser<>( "multiclass_confusion_matrix_actual_class", true, a -> new ActualClass((String) a[0], (long) a[1], (List<PredictedClass>) a[2], (long) a[3])); static { PARSER.declareString(constructorArg(), ACTUAL_CLASS); PARSER.declareLong(constructorArg(), ACTUAL_CLASS_DOC_COUNT); PARSER.declareObjectArray(constructorArg(), PredictedClass.PARSER, PREDICTED_CLASSES); PARSER.declareLong(constructorArg(), OTHER_PREDICTED_CLASS_DOC_COUNT); } private final String actualClass; private final long actualClassDocCount; private final List<PredictedClass> predictedClasses; private final long otherPredictedClassDocCount; public ActualClass( String actualClass, long actualClassDocCount, List<PredictedClass> predictedClasses, long otherPredictedClassDocCount) { this.actualClass = actualClass; this.actualClassDocCount = actualClassDocCount; this.predictedClasses = Collections.unmodifiableList(predictedClasses); this.otherPredictedClassDocCount = otherPredictedClassDocCount; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(ACTUAL_CLASS.getPreferredName(), actualClass); builder.field(ACTUAL_CLASS_DOC_COUNT.getPreferredName(), actualClassDocCount); builder.field(PREDICTED_CLASSES.getPreferredName(), predictedClasses); builder.field(OTHER_PREDICTED_CLASS_DOC_COUNT.getPreferredName(), otherPredictedClassDocCount); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ActualClass that = (ActualClass) o; return Objects.equals(this.actualClass, that.actualClass) && this.actualClassDocCount == that.actualClassDocCount && Objects.equals(this.predictedClasses, that.predictedClasses) && this.otherPredictedClassDocCount == that.otherPredictedClassDocCount; } @Override public int hashCode() { return Objects.hash(actualClass, actualClassDocCount, predictedClasses, otherPredictedClassDocCount); } @Override public String toString() { return Strings.toString(this); } } public static class PredictedClass implements ToXContentObject { private static final ParseField PREDICTED_CLASS = new ParseField("predicted_class"); private static final ParseField COUNT = new ParseField("count"); @SuppressWarnings("unchecked") private static final ConstructingObjectParser<PredictedClass, Void> PARSER = new ConstructingObjectParser<>( "multiclass_confusion_matrix_predicted_class", true, a -> new PredictedClass((String) a[0], (long) a[1])); static { PARSER.declareString(constructorArg(), PREDICTED_CLASS); PARSER.declareLong(constructorArg(), COUNT); } private final String predictedClass; private final Long count; public PredictedClass(String predictedClass, Long count) { this.predictedClass = predictedClass; this.count = count; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(PREDICTED_CLASS.getPreferredName(), predictedClass); builder.field(COUNT.getPreferredName(), count); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; PredictedClass that = (PredictedClass) o; return Objects.equals(this.predictedClass, that.predictedClass) && Objects.equals(this.count, that.count); } @Override public int hashCode() { return Objects.hash(predictedClass, count); }	according to the ctor, both of these are nullable.
public void accept(Iterable<DiscoveryNode> nodes) { final Set<DiscoveryNode> nodesSet = new LinkedHashSet<>(); nodesSet.add(localNode); nodes.forEach(nodesSet::add); logger.trace("discovered {}", nodesSet); if (checkWaitRequirements(request, nodesSet) && listenerNotified.compareAndSet(false, true)) { listenableFuture.onResponse(new GetDiscoveredNodesResponse(nodesSet)); } }	i think this needs a try {...} catch (exception e) { listenablefuture.onfailure(e); } around it, but the test to find this needs to interleave the steps of the other tests so is a bit ugly: java public void testthrowsexceptionifduplicatediscoveredlater() throws interruptedexception { new transportgetdiscoverednodesaction(settings.empty, empty_filters, transportservice, coordinator); // registers action transportservice.start(); transportservice.acceptincomingrequests(); coordinator.start(); coordinator.startinitialjoin(); final getdiscoverednodesrequest getdiscoverednodesrequest = new getdiscoverednodesrequest(); final string ip = localnode.getaddress().getaddress(); getdiscoverednodesrequest.setrequirednodes(collections.singletonlist(ip)); getdiscoverednodesrequest.setwaitfornodes(2); final countdownlatch countdownlatch = new countdownlatch(1); transportservice.sendrequest(localnode, getdiscoverednodesaction.name, getdiscoverednodesrequest, new responsehandler() { @override public void handleresponse(getdiscoverednodesresponse response) { throw new assertionerror("should not be called"); } @override public void handleexception(transportexception exp) { throwable t = exp.getrootcause(); assertthat(t, instanceof(illegalargumentexception.class)); assertthat(t.getmessage(), startswith('[' + ip + "] matches [")); countdownlatch.countdown(); } }); threadpool.generic().execute(() -> transportservice.sendrequest(localnode, request_peers_action_name, new peersrequest(othernode, emptylist()), new transportresponsehandler<peersresponse>() { @override public peersresponse read(streaminput in) throws ioexception { return new peersresponse(in); } @override public void handleresponse(peersresponse response) { logger.info("response: {}", response); } @override public void handleexception(transportexception exp) { logger.info("exception", exp); } @override public string executor() { return names.same; } })); asserttrue(countdownlatch.await(10, timeunit.seconds)); }
public void testTimeStampValidation() throws Exception { // Adding a template with a mapping without a timestamp field and expect data stream creation to fail. { PutComposableIndexTemplateAction.Request createTemplateRequest = new PutComposableIndexTemplateAction.Request("logs-foo"); createTemplateRequest.indexTemplate( new ComposableIndexTemplate( List.of("logs-*"), new Template(null, new CompressedXContent("{}"), null), null, null, null, null, new ComposableIndexTemplate.DataStreamTemplate("@timestamp")) ); client().execute(PutComposableIndexTemplateAction.INSTANCE, createTemplateRequest).actionGet(); Exception e = expectThrows(IllegalArgumentException.class, () -> client().index(new IndexRequest("logs-foobar").opType(CREATE).source("{}", XContentType.JSON)).actionGet()); assertThat(e.getMessage(), equalTo("expected timestamp field [@timestamp], " + "but found no timestamp field for backing index [logs-foobar-000001]")); } // Adding a template with a mapping with a timestamp field and expect data stream creation to succeed. { createIndexTemplate("logs-foo", "logs-*", "@timestamp"); client().index(new IndexRequest("logs-foobar").opType(CREATE).source("{}", XContentType.JSON)).actionGet(); GetDataStreamAction.Response getDataStreamResponse = client().admin().indices().getDataStreams(new GetDataStreamAction.Request("*")).actionGet(); getDataStreamResponse.getDataStreams().sort(Comparator.comparing(DataStream::getName)); assertThat(getDataStreamResponse.getDataStreams().size(), equalTo(1)); assertThat(getDataStreamResponse.getDataStreams().get(0).getName(), equalTo("logs-foobar")); assertThat(getDataStreamResponse.getDataStreams().get(0).getTimeStampField(), equalTo("@timestamp")); } // Adding a template with a mapping without a timestamp field and expect rollover to fail. { PutComposableIndexTemplateAction.Request createTemplateRequest = new PutComposableIndexTemplateAction.Request("logs-foo"); createTemplateRequest.indexTemplate( new ComposableIndexTemplate( List.of("logs-*"), new Template(null, new CompressedXContent("{}"), null), null, null, null, null, new ComposableIndexTemplate.DataStreamTemplate("@timestamp")) ); client().execute(PutComposableIndexTemplateAction.INSTANCE, createTemplateRequest).actionGet(); Exception e = expectThrows(IllegalArgumentException.class, () -> client().admin().indices().rolloverIndex(new RolloverRequest("logs-foobar", null)).actionGet()); assertThat(e.getMessage(), equalTo("expected timestamp field [@timestamp], " + "but found no timestamp field for backing index [logs-foobar-000002]")); } // Adding a template with a mapping with a timestamp field and expect rollover to succeed. { createIndexTemplate("logs-foo", "logs-*", "@timestamp"); RolloverResponse rolloverResponse = client().admin().indices().rolloverIndex(new RolloverRequest("logs-foobar", null)).actionGet(); assertThat(rolloverResponse.getNewIndex(), equalTo("logs-foobar-000002")); assertTrue(rolloverResponse.isRolledOver()); } }	i think these should each be separate test cases, instead of squeezing n into a single test. it makes for smaller more digestible tests, with clear names, whose failures are easier to understand.
public void testGetDocWithMultivaluedFields() throws Exception { String mapping1 = XContentFactory.jsonBuilder().startObject().startObject("type1") .startObject("properties") .startObject("field").field("type", "text").field("store", true).endObject() .endObject() .endObject().endObject().string(); String mapping2 = XContentFactory.jsonBuilder().startObject().startObject("type2") .startObject("properties") .startObject("field").field("type", "text").field("store", true).endObject() .endObject() .endObject().endObject().string(); assertAcked(prepareCreate("test") .addMapping("type1", mapping1, XContentType.JSON) .addMapping("type2", mapping2, XContentType.JSON) .setSettings("index.refresh_interval", -1, "index.version.created", Version.V_5_6_0.id)); // multi types in 5.6 ensureGreen(); GetResponse response = client().prepareGet("test", "type1", "1").get(); assertThat(response.isExists(), equalTo(false)); response = client().prepareGet("test", "type2", "1").get(); assertThat(response.isExists(), equalTo(false)); client().prepareIndex("test", "type1", "1") .setSource(jsonBuilder().startObject().array("field", "1", "2").endObject()).get(); client().prepareIndex("test", "type2", "1") .setSource(jsonBuilder().startObject().array("field", "1", "2").endObject()).get(); response = client().prepareGet("test", "type1", "1").setStoredFields("field").get(); assertThat(response.isExists(), equalTo(true)); assertThat(response.getId(), equalTo("1")); assertThat(response.getType(), equalTo("type1")); Set<String> fields = new HashSet<>(response.getFields().keySet()); assertThat(fields, equalTo(singleton("field"))); assertThat(response.getFields().get("field").getValues().size(), equalTo(2)); assertThat(response.getFields().get("field").getValues().get(0).toString(), equalTo("1")); assertThat(response.getFields().get("field").getValues().get(1).toString(), equalTo("2")); response = client().prepareGet("test", "type2", "1").setStoredFields("field").get(); assertThat(response.isExists(), equalTo(true)); assertThat(response.getType(), equalTo("type2")); assertThat(response.getId(), equalTo("1")); fields = new HashSet<>(response.getFields().keySet()); assertThat(fields, equalTo(singleton("field"))); assertThat(response.getFields().get("field").getValues().size(), equalTo(2)); assertThat(response.getFields().get("field").getValues().get(0).toString(), equalTo("1")); assertThat(response.getFields().get("field").getValues().get(1).toString(), equalTo("2")); // Now test values being fetched from stored fields. refresh(); response = client().prepareGet("test", "type1", "1").setStoredFields("field").get(); assertThat(response.isExists(), equalTo(true)); assertThat(response.getId(), equalTo("1")); fields = new HashSet<>(response.getFields().keySet()); assertThat(fields, equalTo(singleton("field"))); assertThat(response.getFields().get("field").getValues().size(), equalTo(2)); assertThat(response.getFields().get("field").getValues().get(0).toString(), equalTo("1")); assertThat(response.getFields().get("field").getValues().get(1).toString(), equalTo("2")); response = client().prepareGet("test", "type2", "1").setStoredFields("field").get(); assertThat(response.isExists(), equalTo(true)); assertThat(response.getId(), equalTo("1")); fields = new HashSet<>(response.getFields().keySet()); assertThat(fields, equalTo(singleton("field"))); assertThat(response.getFields().get("field").getValues().size(), equalTo(2)); assertThat(response.getFields().get("field").getValues().get(0).toString(), equalTo("1")); assertThat(response.getFields().get("field").getValues().get(1).toString(), equalTo("2")); }	if this is testing 5.x compat we should rename the test. it reads like it is testing stored fields with multiple values.
public void testGetFieldsMetaData() throws Exception { assertAcked(prepareCreate("test") .addMapping("parent") .addMapping("my-type1", "_parent", "type=parent", "field1", "type=keyword,store=true") .addAlias(new Alias("alias")) .setSettings("index.refresh_interval", -1, "index.version.created", Version.V_5_6_0.id)); // multi types in 5.6 client().prepareIndex("test", "my-type1", "1") .setRouting("1") .setParent("parent_1") .setSource(jsonBuilder().startObject().field("field1", "value").endObject()) .get(); GetResponse getResponse = client().prepareGet(indexOrAlias(), "my-type1", "1") .setRouting("1") .setStoredFields("field1") .get(); assertThat(getResponse.isExists(), equalTo(true)); assertThat(getResponse.getField("field1").isMetadataField(), equalTo(false)); assertThat(getResponse.getField("field1").getValue().toString(), equalTo("value")); assertThat(getResponse.getField("_routing").isMetadataField(), equalTo(true)); assertThat(getResponse.getField("_routing").getValue().toString(), equalTo("1")); assertThat(getResponse.getField("_parent").isMetadataField(), equalTo(true)); assertThat(getResponse.getField("_parent").getValue().toString(), equalTo("parent_1")); flush(); getResponse = client().prepareGet(indexOrAlias(), "my-type1", "1") .setStoredFields("field1") .setRouting("1") .get(); assertThat(getResponse.isExists(), equalTo(true)); assertThat(getResponse.getField("field1").isMetadataField(), equalTo(false)); assertThat(getResponse.getField("field1").getValue().toString(), equalTo("value")); assertThat(getResponse.getField("_routing").isMetadataField(), equalTo(true)); assertThat(getResponse.getField("_routing").getValue().toString(), equalTo("1")); assertThat(getResponse.getField("_parent").isMetadataField(), equalTo(true)); assertThat(getResponse.getField("_parent").getValue().toString(), equalTo("parent_1")); }	maybe we're better off testing old style parent/child metadata in its own test. that way we can remove the entire test when we no longer support it. maybe that'd be as simple as moving routing into another test and renaming the test.
public void testPutMapping() throws Exception { verify(client().admin().indices().preparePutMapping("foo").setType("type1").setSource("field", "type=text"), true); verify(client().admin().indices().preparePutMapping("_all").setType("type1").setSource("field", "type=text"), true); for (String index : Arrays.asList("foo", "foobar", "bar", "barbaz")) { assertAcked(prepareCreate(index).setSettings("index.version.created", Version.V_5_6_0.id)); // allows for multiple types } verify(client().admin().indices().preparePutMapping("foo").setType("type1").setSource("field", "type=text"), false); assertThat(client().admin().indices().prepareGetMappings("foo").get().mappings().get("foo").get("type1"), notNullValue()); verify(client().admin().indices().preparePutMapping("b*").setType("type1").setSource("field", "type=text"), false); assertThat(client().admin().indices().prepareGetMappings("bar").get().mappings().get("bar").get("type1"), notNullValue()); assertThat(client().admin().indices().prepareGetMappings("barbaz").get().mappings().get("barbaz").get("type1"), notNullValue()); verify(client().admin().indices().preparePutMapping("_all").setType("type2").setSource("field", "type=text"), false); assertThat(client().admin().indices().prepareGetMappings("foo").get().mappings().get("foo").get("type2"), notNullValue()); assertThat(client().admin().indices().prepareGetMappings("foobar").get().mappings().get("foobar").get("type2"), notNullValue()); assertThat(client().admin().indices().prepareGetMappings("bar").get().mappings().get("bar").get("type2"), notNullValue()); assertThat(client().admin().indices().prepareGetMappings("barbaz").get().mappings().get("barbaz").get("type2"), notNullValue()); verify(client().admin().indices().preparePutMapping().setType("type3").setSource("field", "type=text"), false); assertThat(client().admin().indices().prepareGetMappings("foo").get().mappings().get("foo").get("type3"), notNullValue()); assertThat(client().admin().indices().prepareGetMappings("foobar").get().mappings().get("foobar").get("type3"), notNullValue()); assertThat(client().admin().indices().prepareGetMappings("bar").get().mappings().get("bar").get("type3"), notNullValue()); assertThat(client().admin().indices().prepareGetMappings("barbaz").get().mappings().get("barbaz").get("type3"), notNullValue()); verify(client().admin().indices().preparePutMapping("c*").setType("type1").setSource("field", "type=text"), true); assertAcked(client().admin().indices().prepareClose("barbaz").get()); verify(client().admin().indices().preparePutMapping("barbaz").setType("type4").setSource("field", "type=text"), false); assertThat(client().admin().indices().prepareGetMappings("barbaz").get().mappings().get("barbaz").get("type4"), notNullValue()); }	should this test have a multi-type variant and a single type variant?
public void testDynamicUpdates() throws Exception { client().admin().indices().prepareCreate("test") .setSettings( Settings.builder() .put("index.number_of_shards", 1) .put("index.number_of_replicas", 0) .put(MapperService.INDEX_MAPPING_TOTAL_FIELDS_LIMIT_SETTING.getKey(), Long.MAX_VALUE) .put("index.version.created", Version.V_5_6_0) // for multiple types ).execute().actionGet(); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().execute().actionGet(); int recCount = randomIntBetween(200, 600); int numberOfTypes = randomIntBetween(1, 5); List<IndexRequestBuilder> indexRequests = new ArrayList<>(); for (int rec = 0; rec < recCount; rec++) { String type = "type" + (rec % numberOfTypes); String fieldName = "field_" + type + "_" + rec; indexRequests.add(client().prepareIndex("test", type, Integer.toString(rec)).setSource(fieldName, "some_value")); } indexRandom(true, indexRequests); logger.info("checking all the documents are there"); RefreshResponse refreshResponse = client().admin().indices().prepareRefresh().execute().actionGet(); assertThat(refreshResponse.getFailedShards(), equalTo(0)); SearchResponse response = client().prepareSearch("test").setSize(0).execute().actionGet(); assertThat(response.getHits().getTotalHits(), equalTo((long) recCount)); logger.info("checking all the fields are in the mappings"); for (int rec = 0; rec < recCount; rec++) { String type = "type" + (rec % numberOfTypes); String fieldName = "field_" + type + "_" + rec; assertConcreteMappingsOnAll("test", type, fieldName); } }	do we need a single type variant of this?
public void testMultiIndex() throws Exception { assertAcked(prepareCreate("test1").setSettings("index.version.created", Version.V_5_6_0.id)); createIndex("test2"); ensureGreen(); client().prepareIndex("test1", "type1", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test1", "type2", Integer.toString(1)).setSource("field", "value").execute().actionGet(); client().prepareIndex("test2", "type", Integer.toString(1)).setSource("field", "value").execute().actionGet(); refresh(); int numShards1 = getNumShards("test1").totalNumShards; int numShards2 = getNumShards("test2").totalNumShards; IndicesStatsRequestBuilder builder = client().admin().indices().prepareStats(); IndicesStatsResponse stats = builder.execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("_all").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("_all").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("*").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("test1").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1)); stats = builder.setIndices("test1", "test2").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards1 + numShards2)); stats = builder.setIndices("*2").execute().actionGet(); assertThat(stats.getTotalShards(), equalTo(numShards2)); }	this one asserts far fewer things about test2 then it does about test1 so it'll be annoying to untangle when we no longer support multiple types.
public void testFieldDataFieldsParam() throws Exception { assertAcked(client().admin().indices().prepareCreate("test1") .setSettings("index.version.created", Version.V_5_6_0.id) .addMapping("type", "bar", "type=text,fielddata=true", "baz", "type=text,fielddata=true").get()); ensureGreen(); client().prepareIndex("test1", "bar", Integer.toString(1)).setSource("{\\\\"bar\\\\":\\\\"bar\\\\",\\\\"baz\\\\":\\\\"baz\\\\"}", XContentType.JSON).get(); client().prepareIndex("test1", "baz", Integer.toString(1)).setSource("{\\\\"bar\\\\":\\\\"bar\\\\",\\\\"baz\\\\":\\\\"baz\\\\"}", XContentType.JSON).get(); refresh(); client().prepareSearch("_all").addSort("bar", SortOrder.ASC).addSort("baz", SortOrder.ASC).execute().actionGet(); IndicesStatsRequestBuilder builder = client().admin().indices().prepareStats(); IndicesStatsResponse stats = builder.execute().actionGet(); assertThat(stats.getTotal().fieldData.getMemorySizeInBytes(), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields(), is(nullValue())); stats = builder.setFieldDataFields("bar").execute().actionGet(); assertThat(stats.getTotal().fieldData.getMemorySizeInBytes(), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields().containsField("bar"), is(true)); assertThat(stats.getTotal().fieldData.getFields().get("bar"), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields().containsField("baz"), is(false)); stats = builder.setFieldDataFields("bar", "baz").execute().actionGet(); assertThat(stats.getTotal().fieldData.getMemorySizeInBytes(), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields().containsField("bar"), is(true)); assertThat(stats.getTotal().fieldData.getFields().get("bar"), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields().containsField("baz"), is(true)); assertThat(stats.getTotal().fieldData.getFields().get("baz"), greaterThan(0L)); stats = builder.setFieldDataFields("*").execute().actionGet(); assertThat(stats.getTotal().fieldData.getMemorySizeInBytes(), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields().containsField("bar"), is(true)); assertThat(stats.getTotal().fieldData.getFields().get("bar"), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields().containsField("baz"), is(true)); assertThat(stats.getTotal().fieldData.getFields().get("baz"), greaterThan(0L)); stats = builder.setFieldDataFields("*r").execute().actionGet(); assertThat(stats.getTotal().fieldData.getMemorySizeInBytes(), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields().containsField("bar"), is(true)); assertThat(stats.getTotal().fieldData.getFields().get("bar"), greaterThan(0L)); assertThat(stats.getTotal().fieldData.getFields().containsField("baz"), is(false)); }	is it worth adding an index with a single type to this?
public void testPhrasePrefix() throws IOException { Builder builder = Settings.builder() .put(indexSettings()) .put("index.analysis.analyzer.synonym.tokenizer", "whitespace") .putArray("index.analysis.analyzer.synonym.filter", "synonym", "lowercase") .put("index.analysis.filter.synonym.type", "synonym") .putArray("index.analysis.filter.synonym.synonyms", "quick => fast"); assertAcked(prepareCreate("test").setSettings(builder.build()).addMapping("type1", type1TermVectorMapping())); assertAcked(prepareCreate("test1").setSettings(builder.build()).addMapping("type2", "field4", "type=text,term_vector=with_positions_offsets,analyzer=synonym", "field3", "type=text,analyzer=synonym")); ensureGreen(); client().prepareIndex("test", "type1", "0").setSource( "field0", "The quick brown fox jumps over the lazy dog", "field1", "The quick brown fox jumps over the lazy dog").get(); client().prepareIndex("test", "type1", "1").setSource("field1", "The quick browse button is a fancy thing, right bro?").get(); refresh(); logger.info("--> highlighting and searching on field0"); SearchSourceBuilder source = searchSource() .query(matchPhrasePrefixQuery("field0", "bro")) .highlighter(highlight().field("field0").order("score").preTags("<x>").postTags("</x>")); SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet(); assertHighlight(searchResponse, 0, "field0", 0, 1, equalTo("The quick <x>brown</x> fox jumps over the lazy dog")); source = searchSource() .query(matchPhrasePrefixQuery("field0", "quick bro")) .highlighter(highlight().field("field0").order("score").preTags("<x>").postTags("</x>")); searchResponse = client().search(searchRequest("test").source(source)).actionGet(); assertHighlight(searchResponse, 0, "field0", 0, 1, equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog")); logger.info("--> highlighting and searching on field1"); source = searchSource() .query(boolQuery() .should(matchPhrasePrefixQuery("field1", "test")) .should(matchPhrasePrefixQuery("field1", "bro")) ) .highlighter(highlight().field("field1").order("score").preTags("<x>").postTags("</x>")); searchResponse = client().search(searchRequest("test").source(source)).actionGet(); assertThat(searchResponse.getHits().totalHits, equalTo(2L)); for (int i = 0; i < 2; i++) { assertHighlight(searchResponse, i, "field1", 0, 1, anyOf( equalTo("The quick <x>browse</x> button is a fancy thing, right <x>bro</x>?"), equalTo("The quick <x>brown</x> fox jumps over the lazy dog"))); } source = searchSource() .query(matchPhrasePrefixQuery("field1", "quick bro")) .highlighter(highlight().field("field1").order("score").preTags("<x>").postTags("</x>")); searchResponse = client().search(searchRequest("test").source(source)).actionGet(); assertHighlight(searchResponse, 0, "field1", 0, 1, anyOf( equalTo("The <x>quick</x> <x>browse</x> button is a fancy thing, right bro?"), equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog"))); assertHighlight(searchResponse, 1, "field1", 0, 1, anyOf( equalTo("The <x>quick</x> <x>browse</x> button is a fancy thing, right bro?"), equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog"))); // with synonyms client().prepareIndex("test1", "type2", "0").setSource( "type", "type2", "field4", "The quick brown fox jumps over the lazy dog", "field3", "The quick brown fox jumps over the lazy dog").get(); client().prepareIndex("test1", "type2", "1").setSource( "type", "type2", "field4", "The quick browse button is a fancy thing, right bro?").get(); client().prepareIndex("test1", "type2", "2").setSource( "type", "type2", "field4", "a quick fast blue car").get(); refresh(); source = searchSource().postFilter(termQuery("type", "type2")).query(matchPhrasePrefixQuery("field3", "fast bro")) .highlighter(highlight().field("field3").order("score").preTags("<x>").postTags("</x>")); searchResponse = client().search(searchRequest("test1").source(source)).actionGet(); assertHighlight(searchResponse, 0, "field3", 0, 1, equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog")); logger.info("--> highlighting and searching on field4"); source = searchSource().postFilter(termQuery("type", "type2")).query(matchPhrasePrefixQuery("field4", "the fast bro")) .highlighter(highlight().field("field4").order("score").preTags("<x>").postTags("</x>")); searchResponse = client().search(searchRequest("test1").source(source)).actionGet(); assertHighlight(searchResponse, 0, "field4", 0, 1, anyOf( equalTo("<x>The</x> <x>quick</x> <x>browse</x> button is a fancy thing, right bro?"), equalTo("<x>The</x> <x>quick</x> <x>brown</x> fox jumps over the lazy dog"))); assertHighlight(searchResponse, 1, "field4", 0, 1, anyOf( equalTo("<x>The</x> <x>quick</x> <x>browse</x> button is a fancy thing, right bro?"), equalTo("<x>The</x> <x>quick</x> <x>brown</x> fox jumps over the lazy dog"))); logger.info("--> highlighting and searching on field4"); source = searchSource().postFilter(termQuery("type", "type2")).query(matchPhrasePrefixQuery("field4", "a fast quick blue ca")) .highlighter(highlight().field("field4").order("score").preTags("<x>").postTags("</x>")); searchResponse = client().search(searchRequest("test1").source(source)).actionGet(); assertHighlight(searchResponse, 0, "field4", 0, 1, anyOf(equalTo("<x>a quick fast blue car</x>"), equalTo("<x>a</x> <x>quick</x> <x>fast</x> <x>blue</x> <x>car</x>"))); }	can you rename these indices? i know i'll get test and test1 mixed up. even test1 and test2 would be better.
public void testLoadMetadata() throws Exception { assertAcked(prepareCreate("test") .setSettings("index.version.created", Version.V_5_6_0.id) .addMapping("parent") .addMapping("my-type1", "_parent", "type=parent")); indexRandom(true, client().prepareIndex("test", "my-type1", "1") .setRouting("1") .setParent("parent_1") .setSource(jsonBuilder().startObject().field("field1", "value").endObject())); SearchResponse response = client().prepareSearch("test").addStoredField("field1").get(); assertSearchResponse(response); assertHitCount(response, 1); Map<String, SearchHitField> fields = response.getHits().getAt(0).getFields(); assertThat(fields.get("field1"), nullValue()); assertThat(fields.get("_routing").isMetadataField(), equalTo(true)); assertThat(fields.get("_routing").getValue().toString(), equalTo("1")); assertThat(fields.get("_parent").isMetadataField(), equalTo(true)); assertThat(fields.get("_parent").getValue().toString(), equalTo("parent_1")); }	if we have another test for routing i'd rename this one to testloadparent or something. then we can nuke it in 7.0 easilly.
public void testContextVariables() throws Exception { assertAcked(prepareCreate("test") .setSettings("index.version.created", Version.V_5_6_0.id) .addAlias(new Alias("alias")) .addMapping("type1", XContentFactory.jsonBuilder() .startObject() .startObject("type1") .endObject() .endObject()) .addMapping("subtype1", XContentFactory.jsonBuilder() .startObject() .startObject("subtype1") .startObject("_parent").field("type", "type1").endObject() .endObject() .endObject()) ); ensureGreen(); // Index some documents client().prepareIndex() .setIndex("test") .setType("type1") .setId("parentId1") .setSource("field1", 0, "content", "bar") .execute().actionGet(); client().prepareIndex() .setIndex("test") .setType("subtype1") .setId("id1") .setParent("parentId1") .setRouting("routing1") .setSource("field1", 1, "content", "foo") .execute().actionGet(); // Update the first object and note context variables values UpdateResponse updateResponse = client().prepareUpdate("test", "subtype1", "id1") .setRouting("routing1") .setScript(new Script(ScriptType.INLINE, UPDATE_SCRIPTS, EXTRACT_CTX_SCRIPT, Collections.emptyMap())) .execute().actionGet(); assertEquals(2, updateResponse.getVersion()); GetResponse getResponse = client().prepareGet("test", "subtype1", "id1").setRouting("routing1").execute().actionGet(); Map<String, Object> updateContext = (Map<String, Object>) getResponse.getSourceAsMap().get("update_context"); assertEquals("test", updateContext.get("_index")); assertEquals("subtype1", updateContext.get("_type")); assertEquals("id1", updateContext.get("_id")); assertEquals(1, updateContext.get("_version")); assertEquals("parentId1", updateContext.get("_parent")); assertEquals("routing1", updateContext.get("_routing")); // Idem with the second object updateResponse = client().prepareUpdate("test", "type1", "parentId1") .setScript(new Script(ScriptType.INLINE, UPDATE_SCRIPTS, EXTRACT_CTX_SCRIPT, Collections.emptyMap())) .execute().actionGet(); assertEquals(2, updateResponse.getVersion()); getResponse = client().prepareGet("test", "type1", "parentId1").execute().actionGet(); updateContext = (Map<String, Object>) getResponse.getSourceAsMap().get("update_context"); assertEquals("test", updateContext.get("_index")); assertEquals("type1", updateContext.get("_type")); assertEquals("parentId1", updateContext.get("_id")); assertEquals(1, updateContext.get("_version")); assertNull(updateContext.get("_parent")); assertNull(updateContext.get("_routing")); assertNull(updateContext.get("_ttl")); }	maybe worth leaving a comment that we can remove this setting, the parent, and the second document once we no longer support types.
public void testAPMSystemRole() { final TransportRequest request = mock(TransportRequest.class); final Authentication authentication = mock(Authentication.class); RoleDescriptor roleDescriptor = new ReservedRolesStore().roleDescriptor(APMSystemUser.ROLE_NAME); assertNotNull(roleDescriptor); assertThat(roleDescriptor.getMetadata(), hasEntry("_reserved", true)); Role APMSystemRole = Role.builder(roleDescriptor, null).build(); assertThat(APMSystemRole.cluster().check(ClusterHealthAction.NAME, request, authentication), is(true)); assertThat(APMSystemRole.cluster().check(ClusterStateAction.NAME, request, authentication), is(true)); assertThat(APMSystemRole.cluster().check(ClusterStatsAction.NAME, request, authentication), is(true)); assertThat(APMSystemRole.cluster().check(PutIndexTemplateAction.NAME, request, authentication), is(false)); assertThat(APMSystemRole.cluster().check(ClusterRerouteAction.NAME, request, authentication), is(false)); assertThat(APMSystemRole.cluster().check(ClusterUpdateSettingsAction.NAME, request, authentication), is(false)); assertThat(APMSystemRole.cluster().check(DelegatePkiAuthenticationAction.NAME, request, authentication), is(false)); assertThat(APMSystemRole.cluster().check(MonitoringBulkAction.NAME, request, authentication), is(true)); assertThat(APMSystemRole.runAs().check(randomAlphaOfLengthBetween(1, 30)), is(false)); assertThat(APMSystemRole.indices().allowedIndicesMatcher(IndexAction.NAME).test("foo"), is(false)); assertThat(APMSystemRole.indices().allowedIndicesMatcher(IndexAction.NAME).test(".reporting"), is(false)); assertThat(APMSystemRole.indices().allowedIndicesMatcher("indices:foo").test(randomAlphaOfLengthBetween(8, 24)), is(false)); final String index = ".monitoring-beats-" + randomIntBetween(10, 15);; logger.info("APM beats monitoring index name [{}]", index); assertThat(APMSystemRole.indices().allowedIndicesMatcher(CreateIndexAction.NAME).test(index), is(true)); assertThat(APMSystemRole.indices().allowedIndicesMatcher(IndexAction.NAME).test(index), is(true)); assertThat(APMSystemRole.indices().allowedIndicesMatcher(DeleteAction.NAME).test(index), is(false)); assertThat(APMSystemRole.indices().allowedIndicesMatcher(BulkAction.NAME).test(index), is(true)); assertNoAccessAllowed(APMSystemRole, RestrictedIndicesNames.RESTRICTED_NAMES); }	depending on whether we change this to use create_doc privilege, this will change to test implied_create_action is allowed and implied_index_action is denied https://github.com/elastic/elasticsearch/blob/fbda3a7e19a41d718260bdf6909b271e530cc5cb/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/authorizationservice.java#l96..l97
public Boolean getAsBoolean(String setting, Boolean defaultValue) { final Object found = settings.get(setting); if (found == null) { return defaultValue; } if (found instanceof Boolean) { return (Boolean) found; } final String asString = found.toString(); switch (asString) { case "true": return true; case "false": return false; } return Booleans.parseBoolean(asString, defaultValue); }	couldn't this optimizations exist inside booleans.parseboolean?
public static boolean isSearchableSnapshotStore(Settings indexSettings) { return SEARCHABLE_SNAPSHOT_STORE_TYPE.equals(indexSettings.get(INDEX_STORE_TYPE_SETTING.getKey())); }	if this is hot then we should move it to be directly represented on the indexsettings?
@Override protected void masterOperation(Request request, ClusterState clusterState, ActionListener<Response> listener) throws Exception { if (!licenseState.isDataFrameAllowed()) { listener.onFailure(LicenseUtils.newComplianceException(XPackField.DATA_FRAME)); return; } XPackPlugin.checkReadyForXPackCustomMetadata(clusterState); // set headers to run data frame transform as calling user Map<String, String> filteredHeaders = threadPool.getThreadContext().getHeaders().entrySet().stream() .filter(e -> ClientHelper.SECURITY_HEADER_FILTERS.contains(e.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); DataFrameTransformConfig config = request.getConfig(); config.setHeaders(filteredHeaders); String transformId = config.getId(); // quick check whether a transform has already been created under that name if (PersistentTasksCustomMetaData.getTaskWithId(clusterState, transformId) != null) { listener.onFailure(new ResourceAlreadyExistsException( DataFrameMessages.getMessage(DataFrameMessages.REST_PUT_DATA_FRAME_TRANSFORM_EXISTS, transformId))); return; } String[] dest = indexNameExpressionResolver.concreteIndexNames(clusterState, IndicesOptions.lenientExpandOpen(), config.getDestination()); if (dest.length > 0) { listener.onFailure(new ElasticsearchStatusException( DataFrameMessages.getMessage(DataFrameMessages.REST_PUT_DATA_FRAME_TARGET_INDEX_ALREADY_EXISTS, config.getDestination()), RestStatus.BAD_REQUEST)); return; } String[] src = indexNameExpressionResolver.concreteIndexNames(clusterState, IndicesOptions.lenientExpandOpen(), config.getSource()); if (src.length == 0) { listener.onFailure(new ElasticsearchStatusException( DataFrameMessages.getMessage(DataFrameMessages.REST_PUT_DATA_FRAME_SOURCE_INDEX_MISSING, config.getSource()), RestStatus.BAD_REQUEST)); return; } // Early check to verify that the user can create the destination index and can read from the source if (licenseState.isAuthAllowed()) { final String username = securityContext.getUser().principal(); RoleDescriptor.IndicesPrivileges sourceIndexPrivileges = RoleDescriptor.IndicesPrivileges.builder() .indices(config.getSource()) .privileges("read") .build(); RoleDescriptor.IndicesPrivileges destIndexPrivileges = RoleDescriptor.IndicesPrivileges.builder() .indices(config.getDestination()) .privileges("read", "index", "create_index") .build(); HasPrivilegesRequest privRequest = new HasPrivilegesRequest(); privRequest.applicationPrivileges(new RoleDescriptor.ApplicationResourcePrivileges[0]); privRequest.username(username); privRequest.clusterPrivileges(Strings.EMPTY_ARRAY); privRequest.indexPrivileges(sourceIndexPrivileges, destIndexPrivileges); ActionListener<HasPrivilegesResponse> privResponseListener = ActionListener.wrap( r -> handlePrivsResponse(username, config, r, listener), listener::onFailure); client.execute(HasPrivilegesAction.INSTANCE, privRequest, privResponseListener); } else { putDataFrame(config, listener); } }	nit: reading this is a bit counter-intuitive as i wondered, hey the other case doesn't call putdataframe, i now see it is wrapped. maybe add a comment here, e.g.: "no need to check access rights, go straight to creation"
* @param resultListener Listener to alert when request is completed */ public void getTransformConfigurations(String transformId, ActionListener<List<DataFrameTransformConfig>> resultListener) { BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery() .filter(QueryBuilders.termQuery("doc_type", DataFrameTransformConfig.NAME)); if (Strings.isAllOrWildcard(new String[]{transformId}) == false) { queryBuilder.filter(QueryBuilders.termQuery(DataFrameField.ID.getPreferredName(), transformId)); } SearchRequest request = client.prepareSearch(DataFrameInternalIndex.INDEX_NAME) .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN) .setTrackTotalHits(true) .setSize(10_000) // TODO support pagination .setQuery(queryBuilder) .request(); ClientHelper.executeAsyncWithOrigin(client.threadPool().getThreadContext(), DATA_FRAME_ORIGIN, request, ActionListener.<SearchResponse>wrap( searchResponse -> { List<DataFrameTransformConfig> configs = new ArrayList<>(searchResponse.getHits().getHits().length); for (SearchHit hit : searchResponse.getHits().getHits()) { DataFrameTransformConfig config = parseTransformLenientlyFromSourceSync(hit.getSourceRef(), resultListener::onFailure); if (config == null) { return; } configs.add(config); } if (configs.isEmpty() && (Strings.isAllOrWildcard(new String[]{transformId}) == false)) { resultListener.onFailure(new ResourceNotFoundException( DataFrameMessages.getMessage(DataFrameMessages.REST_DATA_FRAME_UNKNOWN_TRANSFORM, transformId))); return; } resultListener.onResponse(configs); }, resultListener::onFailure) , client::search); }	nit: add a bool , e.g. iswildcardquery, so you can reuse it below
* @param resultListener Listener to alert when request is completed */ public void getTransformConfigurations(String transformId, ActionListener<List<DataFrameTransformConfig>> resultListener) { BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery() .filter(QueryBuilders.termQuery("doc_type", DataFrameTransformConfig.NAME)); if (Strings.isAllOrWildcard(new String[]{transformId}) == false) { queryBuilder.filter(QueryBuilders.termQuery(DataFrameField.ID.getPreferredName(), transformId)); } SearchRequest request = client.prepareSearch(DataFrameInternalIndex.INDEX_NAME) .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN) .setTrackTotalHits(true) .setSize(10_000) // TODO support pagination .setQuery(queryBuilder) .request(); ClientHelper.executeAsyncWithOrigin(client.threadPool().getThreadContext(), DATA_FRAME_ORIGIN, request, ActionListener.<SearchResponse>wrap( searchResponse -> { List<DataFrameTransformConfig> configs = new ArrayList<>(searchResponse.getHits().getHits().length); for (SearchHit hit : searchResponse.getHits().getHits()) { DataFrameTransformConfig config = parseTransformLenientlyFromSourceSync(hit.getSourceRef(), resultListener::onFailure); if (config == null) { return; } configs.add(config); } if (configs.isEmpty() && (Strings.isAllOrWildcard(new String[]{transformId}) == false)) { resultListener.onFailure(new ResourceNotFoundException( DataFrameMessages.getMessage(DataFrameMessages.REST_DATA_FRAME_UNKNOWN_TRANSFORM, transformId))); return; } resultListener.onResponse(configs); }, resultListener::onFailure) , client::search); }	i am worried: what's the size of 10000 config objects? i would not be worried about the xcontent, but we parse all of those and create objects including nested objects like aggregationbuilders, querybuilders, etc. in addition: for get we parse every query but do not use it. what about having different methods? in the end we have 2 usecases: - getting exactly 1 config for the purpose of using it (indexer) - getting the configuration of 1 or more config for the purpose of returning the xcontent of it (get), if possible avoid object creations but at least do not keep 10k objects in a list
@Override protected long doUnsignedLongValue() throws IOException { JsonParser.NumberType numberType = parser.getNumberType(); if ((numberType == JsonParser.NumberType.INT) || (numberType == JsonParser.NumberType.LONG)) { long longValue = parser.getLongValue(); if (longValue < 0) { throw new IllegalArgumentException("Value [" + longValue + "] is out of range for unsigned long."); } return longValue; } else if (numberType == JsonParser.NumberType.BIG_INTEGER) { BigInteger bigIntegerValue = parser.getBigIntegerValue(); if (bigIntegerValue.compareTo(BIGINTEGER_MAX_UNSIGNED_LONG_VALUE) > 0 || bigIntegerValue.compareTo(BigInteger.ZERO) < 0) { throw new IllegalArgumentException("Value [" + bigIntegerValue + "] is out of range for unsigned long"); } return bigIntegerValue.longValue(); } else { // for all other value types including numbers with decimal parts throw new IllegalArgumentException("For input string: [" + parser.getValueAsString() + "]."); } }	do we really need to have support for unsigned longs at such a low level, or could we implement parsing of unsigned longs in the field mapper directly?
public List<ExecutorBuilder<?>> getExecutorBuilders(Settings settings) { if (SEARCHABLE_SNAPSHOTS_FEATURE_ENABLED) { return List.of(executorBuilder()); } else { return List.of(); } }	should we avoid even creating this threadpool if prewarming is disabled?
private synchronized void closeWithTragicEvent(final Exception ex) { assert ex != null; if (tragedy.compareAndSet(null, ex)) { try { close(); } catch (final IOException | RuntimeException e) { ex.addSuppressed(e); } } }	this case got lost
public void testXPackUserCannotAccessRestricitedIndices() { final String action = randomFrom(GetAction.NAME, SearchAction.NAME, IndexAction.NAME); final Predicate<String> predicate = XPackUser.ROLE.indices().allowedIndicesMatcher(action); assertThat(predicate.test(SecurityIndexManager.SECURITY_INDEX_NAME), Matchers.is(false)); assertThat(predicate.test(SecurityIndexManager.INTERNAL_SECURITY_INDEX), Matchers.is(false)); for (String index : RestrictedIndicesNames.NAMES_SET) { assertThat(predicate.test(index), Matchers.is(false)); } }	same commend about names_set vs security_indices
public void testAsyncSimulation() throws Exception { int numDocs = randomIntBetween(1, 64); List<IngestDocument> documents = new ArrayList<>(numDocs); for (int id = 0; id < numDocs; id++) { documents.add(new IngestDocument("_index", Integer.toString(id), null, 0L, VersionType.INTERNAL, new HashMap<>())); } Processor processor1 = new AbstractProcessor(null) { @Override public void execute(IngestDocument ingestDocument, BiConsumer<IngestDocument, Exception> handler) { threadPool.executor(ThreadPool.Names.GENERIC).execute(() -> { ingestDocument.setFieldValue("processed", true); handler.accept(ingestDocument, null); }); } @Override public IngestDocument execute(IngestDocument ingestDocument) throws Exception { throw new UnsupportedOperationException(); } @Override public String getType() { return "none-of-your-business"; } }; Pipeline pipeline = new Pipeline("_id", "_description", version, new CompoundProcessor(processor1)); SimulatePipelineRequest.Parsed request = new SimulatePipelineRequest.Parsed(pipeline, documents, false); AtomicReference<SimulatePipelineResponse> responseHolder = new AtomicReference<>(); AtomicReference<Exception> errorHolder = new AtomicReference<>(); CountDownLatch latch = new CountDownLatch(1); executionService.execute(request, ActionListener.wrap(response -> { responseHolder.set(response); latch.countDown(); }, e -> { errorHolder.set(e); latch.countDown(); })); latch.await(); assertThat(errorHolder.get(), nullValue()); SimulatePipelineResponse response = responseHolder.get(); assertThat(response, notNullValue()); assertThat(response.getResults().size(), equalTo(numDocs)); for (int id = 0; id < numDocs; id++) { SimulateDocumentBaseResult result = (SimulateDocumentBaseResult) response.getResults().get(id); assertThat(result.getIngestDocument().getMetadata().get(IngestDocument.MetaData.ID), equalTo(Integer.toString(id))); assertThat(result.getIngestDocument().getSourceAndMetadata().get("processed"), is(true)); } }	minor nit: add a timeout to not block?
@Override public void visitTypeCaptureReference(TypedCaptureReferenceNode irTypedCaptureReferenceNode, WriteScope writeScope) { MethodWriter methodWriter = writeScope.getMethodWriter(); methodWriter.writeDebugInfo(irTypedCaptureReferenceNode.getLocation()); Variable captured = writeScope.getVariable(irTypedCaptureReferenceNode.getCaptures().get(0)); methodWriter.visitVarInsn(captured.getAsmType().getOpcode(Opcodes.ILOAD), captured.getSlot()); Type methodType = Type.getMethodType(MethodWriter.getType( irTypedCaptureReferenceNode.getDecoration(IRDExpressionType.class).getType()), captured.getAsmType()); methodWriter.invokeDefCall(irTypedCaptureReferenceNode.getMethodName(), methodType, DefBootstrap.REFERENCE, irTypedCaptureReferenceNode.getDecoration(IRDExpressionType.class).getCanonicalTypeName()); }	pull into intermediate variable.
@Override public void visitTypeCaptureReference(TypedCaptureReferenceNode irTypedCaptureReferenceNode, WriteScope writeScope) { MethodWriter methodWriter = writeScope.getMethodWriter(); methodWriter.writeDebugInfo(irTypedCaptureReferenceNode.getLocation()); Variable captured = writeScope.getVariable(irTypedCaptureReferenceNode.getCaptures().get(0)); methodWriter.visitVarInsn(captured.getAsmType().getOpcode(Opcodes.ILOAD), captured.getSlot()); Type methodType = Type.getMethodType(MethodWriter.getType( irTypedCaptureReferenceNode.getDecoration(IRDExpressionType.class).getType()), captured.getAsmType()); methodWriter.invokeDefCall(irTypedCaptureReferenceNode.getMethodName(), methodType, DefBootstrap.REFERENCE, irTypedCaptureReferenceNode.getDecoration(IRDExpressionType.class).getCanonicalTypeName()); }	pull into intermediate variable.
@Override public void visitInstanceof(InstanceofNode irInstanceofNode, WriteScope writeScope) { MethodWriter methodWriter = writeScope.getMethodWriter(); ExpressionNode irChildNode = irInstanceofNode.getChildNode(); visit(irChildNode, writeScope); Class<?> instanceType = irInstanceofNode.getInstanceType(); Class<?> expressionType = irInstanceofNode.getDecoration(IRDExpressionType.class).getType(); if (irInstanceofNode.getInstanceType() == def.class) { methodWriter.writePop(MethodWriter.getType(expressionType).getSize()); methodWriter.push(true); } else if (irChildNode.getDecoration(IRDExpressionType.class).getType().isPrimitive()) { methodWriter.writePop(MethodWriter.getType(expressionType).getSize()); Class<?> boxedInstanceType = PainlessLookupUtility.typeToBoxedType(instanceType); Class<?> boxedExpressionType = PainlessLookupUtility.typeToBoxedType(irChildNode.getDecoration(IRDExpressionType.class).getType()); methodWriter.push(boxedInstanceType.isAssignableFrom(boxedExpressionType)); } else { methodWriter.instanceOf(MethodWriter.getType(PainlessLookupUtility.typeToBoxedType(instanceType))); } }	pull into intermediate variable.
@Override public void visitInstanceof(EInstanceof userInstanceofNode, ScriptScope scriptScope) { InstanceofNode irInstanceofNode = new InstanceofNode(userInstanceofNode.getLocation()); irInstanceofNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userInstanceofNode, ValueType.class).getValueType())); irInstanceofNode.setInstanceType(scriptScope.getDecoration(userInstanceofNode, InstanceType.class).getInstanceType()); irInstanceofNode.setChildNode((ExpressionNode)visit(userInstanceofNode.getExpressionNode(), scriptScope)); scriptScope.putDecoration(userInstanceofNode, new IRNodeDecoration(irInstanceofNode)); }	pull into intermediate variable.
@Override public void visitNewObj(ENewObj userNewObjectNode, ScriptScope scriptScope) { NewObjectNode irNewObjectNode = new NewObjectNode(userNewObjectNode.getLocation()); irNewObjectNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userNewObjectNode, ValueType.class).getValueType())); irNewObjectNode.setRead(scriptScope.getCondition(userNewObjectNode, Read.class)); irNewObjectNode.setConstructor( scriptScope.getDecoration(userNewObjectNode, StandardPainlessConstructor.class).getStandardPainlessConstructor()); for (AExpression userArgumentNode : userNewObjectNode.getArgumentNodes()) { irNewObjectNode.addArgumentNode(injectCast(userArgumentNode, scriptScope)); } scriptScope.putDecoration(userNewObjectNode, new IRNodeDecoration(irNewObjectNode)); }	pull into intermediate variable.
@Override public void visitCallLocal(ECallLocal callLocalNode, ScriptScope scriptScope) { InvokeCallMemberNode irInvokeCallMemberNode = new InvokeCallMemberNode(callLocalNode.getLocation()); if (scriptScope.hasDecoration(callLocalNode, StandardLocalFunction.class)) { irInvokeCallMemberNode.setLocalFunction( scriptScope.getDecoration(callLocalNode, StandardLocalFunction.class).getLocalFunction()); } else if (scriptScope.hasDecoration(callLocalNode, StandardPainlessMethod.class)) { irInvokeCallMemberNode.setImportedMethod( scriptScope.getDecoration(callLocalNode, StandardPainlessMethod.class).getStandardPainlessMethod()); } else if (scriptScope.hasDecoration(callLocalNode, StandardPainlessClassBinding.class)) { PainlessClassBinding painlessClassBinding = scriptScope.getDecoration(callLocalNode, StandardPainlessClassBinding.class).getPainlessClassBinding(); String bindingName = scriptScope.getNextSyntheticName("class_binding"); FieldNode irFieldNode = new FieldNode(callLocalNode.getLocation()); irFieldNode.setModifiers(Modifier.PRIVATE); irFieldNode.setFieldType(painlessClassBinding.javaConstructor.getDeclaringClass()); irFieldNode.setName(bindingName); irClassNode.addFieldNode(irFieldNode); irInvokeCallMemberNode.setClassBinding(painlessClassBinding); irInvokeCallMemberNode.setClassBindingOffset( (int)scriptScope.getDecoration(callLocalNode, StandardConstant.class).getStandardConstant()); irInvokeCallMemberNode.setBindingName(bindingName); } else if (scriptScope.hasDecoration(callLocalNode, StandardPainlessInstanceBinding.class)) { PainlessInstanceBinding painlessInstanceBinding = scriptScope.getDecoration(callLocalNode, StandardPainlessInstanceBinding.class).getPainlessInstanceBinding(); String bindingName = scriptScope.getNextSyntheticName("instance_binding"); FieldNode irFieldNode = new FieldNode(callLocalNode.getLocation()); irFieldNode.setModifiers(Modifier.PUBLIC | Modifier.STATIC); irFieldNode.setFieldType(painlessInstanceBinding.targetInstance.getClass()); irFieldNode.setName(bindingName); irClassNode.addFieldNode(irFieldNode); irInvokeCallMemberNode.setInstanceBinding(painlessInstanceBinding); irInvokeCallMemberNode.setBindingName(bindingName); scriptScope.addStaticConstant(bindingName, painlessInstanceBinding.targetInstance); } else { throw callLocalNode.createError(new IllegalStateException("illegal tree structure")); } for (AExpression userArgumentNode : callLocalNode.getArgumentNodes()) { irInvokeCallMemberNode.addArgumentNode(injectCast(userArgumentNode, scriptScope)); } irInvokeCallMemberNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(callLocalNode, ValueType.class).getValueType())); scriptScope.putDecoration(callLocalNode, new IRNodeDecoration(irInvokeCallMemberNode)); }	pull into intermediate variable.
@Override public void visitCallLocal(ECallLocal callLocalNode, ScriptScope scriptScope) { InvokeCallMemberNode irInvokeCallMemberNode = new InvokeCallMemberNode(callLocalNode.getLocation()); if (scriptScope.hasDecoration(callLocalNode, StandardLocalFunction.class)) { irInvokeCallMemberNode.setLocalFunction( scriptScope.getDecoration(callLocalNode, StandardLocalFunction.class).getLocalFunction()); } else if (scriptScope.hasDecoration(callLocalNode, StandardPainlessMethod.class)) { irInvokeCallMemberNode.setImportedMethod( scriptScope.getDecoration(callLocalNode, StandardPainlessMethod.class).getStandardPainlessMethod()); } else if (scriptScope.hasDecoration(callLocalNode, StandardPainlessClassBinding.class)) { PainlessClassBinding painlessClassBinding = scriptScope.getDecoration(callLocalNode, StandardPainlessClassBinding.class).getPainlessClassBinding(); String bindingName = scriptScope.getNextSyntheticName("class_binding"); FieldNode irFieldNode = new FieldNode(callLocalNode.getLocation()); irFieldNode.setModifiers(Modifier.PRIVATE); irFieldNode.setFieldType(painlessClassBinding.javaConstructor.getDeclaringClass()); irFieldNode.setName(bindingName); irClassNode.addFieldNode(irFieldNode); irInvokeCallMemberNode.setClassBinding(painlessClassBinding); irInvokeCallMemberNode.setClassBindingOffset( (int)scriptScope.getDecoration(callLocalNode, StandardConstant.class).getStandardConstant()); irInvokeCallMemberNode.setBindingName(bindingName); } else if (scriptScope.hasDecoration(callLocalNode, StandardPainlessInstanceBinding.class)) { PainlessInstanceBinding painlessInstanceBinding = scriptScope.getDecoration(callLocalNode, StandardPainlessInstanceBinding.class).getPainlessInstanceBinding(); String bindingName = scriptScope.getNextSyntheticName("instance_binding"); FieldNode irFieldNode = new FieldNode(callLocalNode.getLocation()); irFieldNode.setModifiers(Modifier.PUBLIC | Modifier.STATIC); irFieldNode.setFieldType(painlessInstanceBinding.targetInstance.getClass()); irFieldNode.setName(bindingName); irClassNode.addFieldNode(irFieldNode); irInvokeCallMemberNode.setInstanceBinding(painlessInstanceBinding); irInvokeCallMemberNode.setBindingName(bindingName); scriptScope.addStaticConstant(bindingName, painlessInstanceBinding.targetInstance); } else { throw callLocalNode.createError(new IllegalStateException("illegal tree structure")); } for (AExpression userArgumentNode : callLocalNode.getArgumentNodes()) { irInvokeCallMemberNode.addArgumentNode(injectCast(userArgumentNode, scriptScope)); } irInvokeCallMemberNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(callLocalNode, ValueType.class).getValueType())); scriptScope.putDecoration(callLocalNode, new IRNodeDecoration(irInvokeCallMemberNode)); }	pull into intermediate variable.
@Override public void visitNumeric(ENumeric userNumericNode, ScriptScope scriptScope) { ConstantNode irConstantNode = new ConstantNode(userNumericNode.getLocation()); irConstantNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userNumericNode, ValueType.class).getValueType())); irConstantNode.setConstant(scriptScope.getDecoration(userNumericNode, StandardConstant.class).getStandardConstant()); scriptScope.putDecoration(userNumericNode, new IRNodeDecoration(irConstantNode)); }	pull into intermediate variable.
@Override public void visitDecimal(EDecimal userDecimalNode, ScriptScope scriptScope) { ConstantNode irConstantNode = new ConstantNode(userDecimalNode.getLocation()); irConstantNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userDecimalNode, ValueType.class).getValueType())); irConstantNode.setConstant(scriptScope.getDecoration(userDecimalNode, StandardConstant.class).getStandardConstant()); scriptScope.putDecoration(userDecimalNode, new IRNodeDecoration(irConstantNode)); }	pull into intermediate variable.
@Override public void visitFunctionRef(EFunctionRef userFunctionRefNode, ScriptScope scriptScope) { ReferenceNode irReferenceNode; TargetType targetType = scriptScope.getDecoration(userFunctionRefNode, TargetType.class); CapturesDecoration capturesDecoration = scriptScope.getDecoration(userFunctionRefNode, CapturesDecoration.class); if (targetType == null) { DefInterfaceReferenceNode defInterfaceReferenceNode = new DefInterfaceReferenceNode(userFunctionRefNode.getLocation()); defInterfaceReferenceNode.setDefReferenceEncoding( scriptScope.getDecoration(userFunctionRefNode, EncodingDecoration.class).getEncoding()); irReferenceNode = defInterfaceReferenceNode; } else if (capturesDecoration != null && capturesDecoration.getCaptures().get(0).getType() == def.class) { TypedCaptureReferenceNode typedCaptureReferenceNode = new TypedCaptureReferenceNode(userFunctionRefNode.getLocation()); typedCaptureReferenceNode.setMethodName(userFunctionRefNode.getMethodName()); irReferenceNode = typedCaptureReferenceNode; } else { TypedInterfaceReferenceNode typedInterfaceReferenceNode = new TypedInterfaceReferenceNode(userFunctionRefNode.getLocation()); typedInterfaceReferenceNode.setReference( scriptScope.getDecoration(userFunctionRefNode, ReferenceDecoration.class).getReference()); irReferenceNode = typedInterfaceReferenceNode; } irReferenceNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userFunctionRefNode, ValueType.class).getValueType())); if (capturesDecoration != null) { irReferenceNode.addCapture(capturesDecoration.getCaptures().get(0).getName()); } scriptScope.putDecoration(userFunctionRefNode, new IRNodeDecoration(irReferenceNode)); }	pull into intermediate variable.
@Override public void visitBrace(EBrace userBraceNode, ScriptScope scriptScope) { boolean read = scriptScope.getCondition(userBraceNode, Read.class); boolean write = scriptScope.getCondition(userBraceNode, Write.class); boolean compound = scriptScope.getCondition(userBraceNode, Compound.class); Location location = userBraceNode.getLocation(); Class<?> valueType = scriptScope.getDecoration(userBraceNode, ValueType.class).getValueType(); Class<?> prefixValueType = scriptScope.getDecoration(userBraceNode.getPrefixNode(), ValueType.class).getValueType(); ExpressionNode irPrefixNode = (ExpressionNode)visit(userBraceNode.getPrefixNode(), scriptScope); ExpressionNode irIndexNode = injectCast(userBraceNode.getIndexNode(), scriptScope); StoreNode irStoreNode = null; ExpressionNode irLoadNode = null; if (prefixValueType.isArray()) { FlipArrayIndexNode irFlipArrayIndexNode = new FlipArrayIndexNode(userBraceNode.getIndexNode().getLocation()); irFlipArrayIndexNode.attachDecoration(new IRDExpressionType(int.class)); irFlipArrayIndexNode.setChildNode(irIndexNode); irIndexNode = irFlipArrayIndexNode; if (write || compound) { StoreBraceNode irStoreBraceNode = new StoreBraceNode(location); irStoreBraceNode.attachDecoration(new IRDExpressionType(read ? valueType : void.class)); irStoreBraceNode.setStoreType(valueType); irStoreNode = irStoreBraceNode; } if (write == false || compound) { LoadBraceNode irLoadBraceNode = new LoadBraceNode(location); irLoadBraceNode.attachDecoration(new IRDExpressionType(valueType)); irLoadNode = irLoadBraceNode; } } else if (prefixValueType == def.class) { Class<?> indexType = scriptScope.getDecoration(userBraceNode.getIndexNode(), ValueType.class).getValueType(); FlipDefIndexNode irFlipDefIndexNode = new FlipDefIndexNode(userBraceNode.getIndexNode().getLocation()); irFlipDefIndexNode.attachDecoration(new IRDExpressionType(indexType)); irFlipDefIndexNode.setChildNode(irIndexNode); irIndexNode = irFlipDefIndexNode; if (write || compound) { StoreBraceDefNode irStoreBraceNode = new StoreBraceDefNode(location); irStoreBraceNode.attachDecoration(new IRDExpressionType(read ? valueType : void.class)); irStoreBraceNode.setStoreType(valueType); irStoreBraceNode.setIndexType(indexType); irStoreNode = irStoreBraceNode; } if (write == false || compound) { LoadBraceDefNode irLoadBraceDefNode = new LoadBraceDefNode(location); irLoadBraceDefNode.attachDecoration(new IRDExpressionType(valueType)); irLoadBraceDefNode.setIndexType(indexType); irLoadNode = irLoadBraceDefNode; } } else if (scriptScope.getCondition(userBraceNode, MapShortcut.class)) { if (write || compound) { StoreMapShortcutNode irStoreMapShortcutNode = new StoreMapShortcutNode(location); irStoreMapShortcutNode.attachDecoration(new IRDExpressionType(read ? valueType : void.class)); irStoreMapShortcutNode.setStoreType(valueType); irStoreMapShortcutNode.setSetter( scriptScope.getDecoration(userBraceNode, SetterPainlessMethod.class).getSetterPainlessMethod()); irStoreNode = irStoreMapShortcutNode; } if (write == false || compound) { LoadMapShortcutNode irLoadMapShortcutNode = new LoadMapShortcutNode(location); irLoadMapShortcutNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userBraceNode, ValueType.class).getValueType())); irLoadMapShortcutNode.setGetter( scriptScope.getDecoration(userBraceNode, GetterPainlessMethod.class).getGetterPainlessMethod()); irLoadNode = irLoadMapShortcutNode; } } else if (scriptScope.getCondition(userBraceNode, ListShortcut.class)) { FlipCollectionIndexNode irFlipCollectionIndexNode = new FlipCollectionIndexNode(userBraceNode.getIndexNode().getLocation()); irFlipCollectionIndexNode.attachDecoration(new IRDExpressionType(int.class)); irFlipCollectionIndexNode.setChildNode(irIndexNode); irIndexNode = irFlipCollectionIndexNode; if (write || compound) { StoreListShortcutNode irStoreListShortcutNode = new StoreListShortcutNode(location); irStoreListShortcutNode.attachDecoration(new IRDExpressionType(read ? valueType : void.class)); irStoreListShortcutNode.setStoreType(valueType); irStoreListShortcutNode.setSetter( scriptScope.getDecoration(userBraceNode, SetterPainlessMethod.class).getSetterPainlessMethod()); irStoreNode = irStoreListShortcutNode; } if (write == false || compound) { LoadListShortcutNode irLoadListShortcutNode = new LoadListShortcutNode(location); irLoadListShortcutNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userBraceNode, ValueType.class).getValueType())); irLoadListShortcutNode.setGetter( scriptScope.getDecoration(userBraceNode, GetterPainlessMethod.class).getGetterPainlessMethod()); irLoadNode = irLoadListShortcutNode; } } else { throw userBraceNode.createError(new IllegalStateException("illegal tree structure")); } scriptScope.putDecoration(userBraceNode, new AccessDepth(2)); scriptScope.putDecoration(userBraceNode, new IRNodeDecoration( buildLoadStore(2, location, false, irPrefixNode, irIndexNode, irLoadNode, irStoreNode))); }	pull into intermediate variable.
@Override public void visitBrace(EBrace userBraceNode, ScriptScope scriptScope) { boolean read = scriptScope.getCondition(userBraceNode, Read.class); boolean write = scriptScope.getCondition(userBraceNode, Write.class); boolean compound = scriptScope.getCondition(userBraceNode, Compound.class); Location location = userBraceNode.getLocation(); Class<?> valueType = scriptScope.getDecoration(userBraceNode, ValueType.class).getValueType(); Class<?> prefixValueType = scriptScope.getDecoration(userBraceNode.getPrefixNode(), ValueType.class).getValueType(); ExpressionNode irPrefixNode = (ExpressionNode)visit(userBraceNode.getPrefixNode(), scriptScope); ExpressionNode irIndexNode = injectCast(userBraceNode.getIndexNode(), scriptScope); StoreNode irStoreNode = null; ExpressionNode irLoadNode = null; if (prefixValueType.isArray()) { FlipArrayIndexNode irFlipArrayIndexNode = new FlipArrayIndexNode(userBraceNode.getIndexNode().getLocation()); irFlipArrayIndexNode.attachDecoration(new IRDExpressionType(int.class)); irFlipArrayIndexNode.setChildNode(irIndexNode); irIndexNode = irFlipArrayIndexNode; if (write || compound) { StoreBraceNode irStoreBraceNode = new StoreBraceNode(location); irStoreBraceNode.attachDecoration(new IRDExpressionType(read ? valueType : void.class)); irStoreBraceNode.setStoreType(valueType); irStoreNode = irStoreBraceNode; } if (write == false || compound) { LoadBraceNode irLoadBraceNode = new LoadBraceNode(location); irLoadBraceNode.attachDecoration(new IRDExpressionType(valueType)); irLoadNode = irLoadBraceNode; } } else if (prefixValueType == def.class) { Class<?> indexType = scriptScope.getDecoration(userBraceNode.getIndexNode(), ValueType.class).getValueType(); FlipDefIndexNode irFlipDefIndexNode = new FlipDefIndexNode(userBraceNode.getIndexNode().getLocation()); irFlipDefIndexNode.attachDecoration(new IRDExpressionType(indexType)); irFlipDefIndexNode.setChildNode(irIndexNode); irIndexNode = irFlipDefIndexNode; if (write || compound) { StoreBraceDefNode irStoreBraceNode = new StoreBraceDefNode(location); irStoreBraceNode.attachDecoration(new IRDExpressionType(read ? valueType : void.class)); irStoreBraceNode.setStoreType(valueType); irStoreBraceNode.setIndexType(indexType); irStoreNode = irStoreBraceNode; } if (write == false || compound) { LoadBraceDefNode irLoadBraceDefNode = new LoadBraceDefNode(location); irLoadBraceDefNode.attachDecoration(new IRDExpressionType(valueType)); irLoadBraceDefNode.setIndexType(indexType); irLoadNode = irLoadBraceDefNode; } } else if (scriptScope.getCondition(userBraceNode, MapShortcut.class)) { if (write || compound) { StoreMapShortcutNode irStoreMapShortcutNode = new StoreMapShortcutNode(location); irStoreMapShortcutNode.attachDecoration(new IRDExpressionType(read ? valueType : void.class)); irStoreMapShortcutNode.setStoreType(valueType); irStoreMapShortcutNode.setSetter( scriptScope.getDecoration(userBraceNode, SetterPainlessMethod.class).getSetterPainlessMethod()); irStoreNode = irStoreMapShortcutNode; } if (write == false || compound) { LoadMapShortcutNode irLoadMapShortcutNode = new LoadMapShortcutNode(location); irLoadMapShortcutNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userBraceNode, ValueType.class).getValueType())); irLoadMapShortcutNode.setGetter( scriptScope.getDecoration(userBraceNode, GetterPainlessMethod.class).getGetterPainlessMethod()); irLoadNode = irLoadMapShortcutNode; } } else if (scriptScope.getCondition(userBraceNode, ListShortcut.class)) { FlipCollectionIndexNode irFlipCollectionIndexNode = new FlipCollectionIndexNode(userBraceNode.getIndexNode().getLocation()); irFlipCollectionIndexNode.attachDecoration(new IRDExpressionType(int.class)); irFlipCollectionIndexNode.setChildNode(irIndexNode); irIndexNode = irFlipCollectionIndexNode; if (write || compound) { StoreListShortcutNode irStoreListShortcutNode = new StoreListShortcutNode(location); irStoreListShortcutNode.attachDecoration(new IRDExpressionType(read ? valueType : void.class)); irStoreListShortcutNode.setStoreType(valueType); irStoreListShortcutNode.setSetter( scriptScope.getDecoration(userBraceNode, SetterPainlessMethod.class).getSetterPainlessMethod()); irStoreNode = irStoreListShortcutNode; } if (write == false || compound) { LoadListShortcutNode irLoadListShortcutNode = new LoadListShortcutNode(location); irLoadListShortcutNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userBraceNode, ValueType.class).getValueType())); irLoadListShortcutNode.setGetter( scriptScope.getDecoration(userBraceNode, GetterPainlessMethod.class).getGetterPainlessMethod()); irLoadNode = irLoadListShortcutNode; } } else { throw userBraceNode.createError(new IllegalStateException("illegal tree structure")); } scriptScope.putDecoration(userBraceNode, new AccessDepth(2)); scriptScope.putDecoration(userBraceNode, new IRNodeDecoration( buildLoadStore(2, location, false, irPrefixNode, irIndexNode, irLoadNode, irStoreNode))); }	pull into intermediate variable.
@Override public void visitCall(ECall userCallNode, ScriptScope scriptScope) { ExpressionNode irExpressionNode; ValueType prefixValueType = scriptScope.getDecoration(userCallNode.getPrefixNode(), ValueType.class); if (prefixValueType != null && prefixValueType.getValueType() == def.class) { InvokeCallDefNode irCallSubDefNode = new InvokeCallDefNode(userCallNode.getLocation()); for (AExpression userArgumentNode : userCallNode.getArgumentNodes()) { irCallSubDefNode.addArgumentNode((ExpressionNode)visit(userArgumentNode, scriptScope)); } irCallSubDefNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userCallNode, ValueType.class).getValueType())); irCallSubDefNode.setName(userCallNode.getMethodName()); irExpressionNode = irCallSubDefNode; } else { Class<?> boxType; if (prefixValueType != null) { boxType = prefixValueType.getValueType(); } else { boxType = scriptScope.getDecoration(userCallNode.getPrefixNode(), StaticType.class).getStaticType(); } InvokeCallNode irInvokeCallNode = new InvokeCallNode(userCallNode.getLocation()); PainlessMethod method = scriptScope.getDecoration(userCallNode, StandardPainlessMethod.class).getStandardPainlessMethod(); Object[] injections = PainlessLookupUtility.buildInjections(method, scriptScope.getCompilerSettings().asMap()); Class<?>[] parameterTypes = method.javaMethod.getParameterTypes(); int augmentedOffset = method.javaMethod.getDeclaringClass() == method.targetClass ? 0 : 1; for (int i = 0; i < injections.length; i++) { Object injection = injections[i]; Class<?> parameterType = parameterTypes[i + augmentedOffset]; if (parameterType != PainlessLookupUtility.typeToUnboxedType(injection.getClass())) { throw new IllegalStateException("illegal tree structure"); } ConstantNode constantNode = new ConstantNode(userCallNode.getLocation()); constantNode.attachDecoration(new IRDExpressionType(parameterType)); constantNode.setConstant(injection); irInvokeCallNode.addArgumentNode(constantNode); } for (AExpression userCallArgumentNode : userCallNode.getArgumentNodes()) { irInvokeCallNode.addArgumentNode(injectCast(userCallArgumentNode, scriptScope)); } irInvokeCallNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userCallNode, ValueType.class).getValueType())); irInvokeCallNode.setMethod(scriptScope.getDecoration(userCallNode, StandardPainlessMethod.class).getStandardPainlessMethod()); irInvokeCallNode.setBox(boxType); irExpressionNode = irInvokeCallNode; } if (userCallNode.isNullSafe()) { NullSafeSubNode irNullSafeSubNode = new NullSafeSubNode(irExpressionNode.getLocation()); irNullSafeSubNode.setChildNode(irExpressionNode); irNullSafeSubNode.copyDecorationFrom(irExpressionNode, IRDExpressionType.class); irExpressionNode = irNullSafeSubNode; } BinaryImplNode irBinaryImplNode = new BinaryImplNode(irExpressionNode.getLocation()); irBinaryImplNode.setLeftNode((ExpressionNode)visit(userCallNode.getPrefixNode(), scriptScope)); irBinaryImplNode.setRightNode(irExpressionNode); irBinaryImplNode.copyDecorationFrom(irExpressionNode, IRDExpressionType.class); scriptScope.putDecoration(userCallNode, new IRNodeDecoration(irBinaryImplNode)); }	pull into intermediate variable.
@Override public void visitCall(ECall userCallNode, ScriptScope scriptScope) { ExpressionNode irExpressionNode; ValueType prefixValueType = scriptScope.getDecoration(userCallNode.getPrefixNode(), ValueType.class); if (prefixValueType != null && prefixValueType.getValueType() == def.class) { InvokeCallDefNode irCallSubDefNode = new InvokeCallDefNode(userCallNode.getLocation()); for (AExpression userArgumentNode : userCallNode.getArgumentNodes()) { irCallSubDefNode.addArgumentNode((ExpressionNode)visit(userArgumentNode, scriptScope)); } irCallSubDefNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userCallNode, ValueType.class).getValueType())); irCallSubDefNode.setName(userCallNode.getMethodName()); irExpressionNode = irCallSubDefNode; } else { Class<?> boxType; if (prefixValueType != null) { boxType = prefixValueType.getValueType(); } else { boxType = scriptScope.getDecoration(userCallNode.getPrefixNode(), StaticType.class).getStaticType(); } InvokeCallNode irInvokeCallNode = new InvokeCallNode(userCallNode.getLocation()); PainlessMethod method = scriptScope.getDecoration(userCallNode, StandardPainlessMethod.class).getStandardPainlessMethod(); Object[] injections = PainlessLookupUtility.buildInjections(method, scriptScope.getCompilerSettings().asMap()); Class<?>[] parameterTypes = method.javaMethod.getParameterTypes(); int augmentedOffset = method.javaMethod.getDeclaringClass() == method.targetClass ? 0 : 1; for (int i = 0; i < injections.length; i++) { Object injection = injections[i]; Class<?> parameterType = parameterTypes[i + augmentedOffset]; if (parameterType != PainlessLookupUtility.typeToUnboxedType(injection.getClass())) { throw new IllegalStateException("illegal tree structure"); } ConstantNode constantNode = new ConstantNode(userCallNode.getLocation()); constantNode.attachDecoration(new IRDExpressionType(parameterType)); constantNode.setConstant(injection); irInvokeCallNode.addArgumentNode(constantNode); } for (AExpression userCallArgumentNode : userCallNode.getArgumentNodes()) { irInvokeCallNode.addArgumentNode(injectCast(userCallArgumentNode, scriptScope)); } irInvokeCallNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userCallNode, ValueType.class).getValueType())); irInvokeCallNode.setMethod(scriptScope.getDecoration(userCallNode, StandardPainlessMethod.class).getStandardPainlessMethod()); irInvokeCallNode.setBox(boxType); irExpressionNode = irInvokeCallNode; } if (userCallNode.isNullSafe()) { NullSafeSubNode irNullSafeSubNode = new NullSafeSubNode(irExpressionNode.getLocation()); irNullSafeSubNode.setChildNode(irExpressionNode); irNullSafeSubNode.copyDecorationFrom(irExpressionNode, IRDExpressionType.class); irExpressionNode = irNullSafeSubNode; } BinaryImplNode irBinaryImplNode = new BinaryImplNode(irExpressionNode.getLocation()); irBinaryImplNode.setLeftNode((ExpressionNode)visit(userCallNode.getPrefixNode(), scriptScope)); irBinaryImplNode.setRightNode(irExpressionNode); irBinaryImplNode.copyDecorationFrom(irExpressionNode, IRDExpressionType.class); scriptScope.putDecoration(userCallNode, new IRNodeDecoration(irBinaryImplNode)); }	pull into intermediate variable.
@Override public void visitBooleanComp(EBooleanComp userBooleanCompNode, ScriptScope scriptScope) { BooleanNode irBooleanNode = new BooleanNode(userBooleanCompNode.getLocation()); irBooleanNode.attachDecoration( new IRDExpressionType(scriptScope.getDecoration(userBooleanCompNode, ValueType.class).getValueType())); irBooleanNode.setOperation(userBooleanCompNode.getOperation()); irBooleanNode.setLeftNode(injectCast(userBooleanCompNode.getLeftNode(), scriptScope)); irBooleanNode.setRightNode(injectCast(userBooleanCompNode.getRightNode(), scriptScope)); scriptScope.putDecoration(userBooleanCompNode, new IRNodeDecoration(irBooleanNode)); }	pull into intermediate variable.
private void writeStandardTermVector(TermVectorResponse outResponse) throws IOException { Directory dir = FSDirectory.open(new File("/tmp/foo")); IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)); conf.setOpenMode(OpenMode.CREATE); IndexWriter writer = new IndexWriter(dir, conf); FieldType type = new FieldType(TextField.TYPE_STORED); type.setStoreTermVectorOffsets(true); type.setStoreTermVectorPayloads(false); type.setStoreTermVectorPositions(true); type.setStoreTermVectors(true); type.freeze(); Document d = new Document(); d.add(new Field("id", "abc", StringField.TYPE_STORED)); d.add(new Field("title", "the1 quick brown fox jumps over the1 lazy dog", type)); d.add(new Field("desc", "the1 quick brown fox jumps over the1 lazy dog", type)); writer.updateDocument(new Term("id", "abc"), d); writer.commit(); writer.close(); DirectoryReader dr = DirectoryReader.open(dir); IndexSearcher s = new IndexSearcher(dr); TopDocs search = s.search(new TermQuery(new Term("id", "abc")), 1); ScoreDoc[] scoreDocs = search.scoreDocs; int doc = scoreDocs[0].doc; Fields termVectors = dr.getTermVectors(doc); EnumSet<Flag> flags = EnumSet.of(Flag.Positions, Flag.Offsets); outResponse.setFields(termVectors, null, flags, termVectors); }	can we get away with ramdirectory or something like that? it's a shame to have to create something on disk. especially if this test is running on some server somewhere in a build system server.
private void writeStandardTermVector(TermVectorResponse outResponse) throws IOException { Directory dir = FSDirectory.open(new File("/tmp/foo")); IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)); conf.setOpenMode(OpenMode.CREATE); IndexWriter writer = new IndexWriter(dir, conf); FieldType type = new FieldType(TextField.TYPE_STORED); type.setStoreTermVectorOffsets(true); type.setStoreTermVectorPayloads(false); type.setStoreTermVectorPositions(true); type.setStoreTermVectors(true); type.freeze(); Document d = new Document(); d.add(new Field("id", "abc", StringField.TYPE_STORED)); d.add(new Field("title", "the1 quick brown fox jumps over the1 lazy dog", type)); d.add(new Field("desc", "the1 quick brown fox jumps over the1 lazy dog", type)); writer.updateDocument(new Term("id", "abc"), d); writer.commit(); writer.close(); DirectoryReader dr = DirectoryReader.open(dir); IndexSearcher s = new IndexSearcher(dr); TopDocs search = s.search(new TermQuery(new Term("id", "abc")), 1); ScoreDoc[] scoreDocs = search.scoreDocs; int doc = scoreDocs[0].doc; Fields termVectors = dr.getTermVectors(doc); EnumSet<Flag> flags = EnumSet.of(Flag.Positions, Flag.Offsets); outResponse.setFields(termVectors, null, flags, termVectors); }	according to my intellij it's not used anymore.. remove?
@Override public boolean equals(Object other) { if (other == null) { return false; } else if (other instanceof GetPipelineResponse){ GetPipelineResponse otherResponse = (GetPipelineResponse)other; if (pipelines == null) { return otherResponse.pipelines == null; } else { // We need a map here because order does not matter for equality Map<String, PipelineConfiguration> otherPipelineMap = new HashMap<>(); for (PipelineConfiguration pipeline: otherResponse.pipelines) { otherPipelineMap.put(pipeline.getId(), pipeline); } for (PipelineConfiguration pipeline: pipelines) { PipelineConfiguration otherPipeline = otherPipelineMap.get(pipeline.getId()); if (otherPipeline == null || !pipeline.getConfigAsMap().equals(otherPipeline.getConfigAsMap())) { return false; } } return true; } } else { return false; } }	shall we add equals and hashcode to pipelineconfiguration ?
public CompiledScript compile(Script script, ScriptContext scriptContext) { if (scriptContext == null) { throw new ElasticsearchIllegalArgumentException("The parameter scriptContext (ScriptContext) must not be null."); } String lang = script.lang(); if (lang == null) { lang = defaultLang; } ScriptEngineService scriptEngineService = getScriptEngineServiceForLang(lang); if (canExecuteScript(lang, scriptEngineService, script.type(), scriptContext) == false) { throw new ScriptException("scripts of type [" + script.type() + "], operation [" + scriptContext.getKey() + "] and lang [" + lang + "] are disabled"); } return compileInternal(script); }	i think we have to add a null check for script too
public CompiledScript compile(Script script, ScriptContext scriptContext) { if (scriptContext == null) { throw new ElasticsearchIllegalArgumentException("The parameter scriptContext (ScriptContext) must not be null."); } String lang = script.lang(); if (lang == null) { lang = defaultLang; } ScriptEngineService scriptEngineService = getScriptEngineServiceForLang(lang); if (canExecuteScript(lang, scriptEngineService, script.type(), scriptContext) == false) { throw new ScriptException("scripts of type [" + script.type() + "], operation [" + scriptContext.getKey() + "] and lang [" + lang + "] are disabled"); } return compileInternal(script); }	rename the argument to script?
public RestResponse buildResponse(SqlQueryResponse response) throws Exception { RestResponse restResponse; // XContent branch if (responseMediaType != null && responseMediaType instanceof XContentType) { XContentType type = (XContentType)responseMediaType; XContentBuilder builder = channel.newBuilder(request.getXContentType(), type, true); response.toXContent(builder, request); restResponse = new BytesRestResponse(RestStatus.OK, builder); } else {// TextFormat TextFormat type = (TextFormat)responseMediaType; final String data = type.format(request, response); restResponse = new BytesRestResponse(RestStatus.OK, type.contentType(request), data.getBytes(StandardCharsets.UTF_8)); if (response.hasCursor()) { restResponse.addHeader("Cursor", response.cursor()); } } restResponse.addHeader("Took-nanos", Long.toString(System.nanoTime() - startNanos)); return restResponse; }	nit: suggestion xcontenttype type = (xcontenttype) responsemediatype;
public RestResponse buildResponse(SqlQueryResponse response) throws Exception { RestResponse restResponse; // XContent branch if (responseMediaType != null && responseMediaType instanceof XContentType) { XContentType type = (XContentType)responseMediaType; XContentBuilder builder = channel.newBuilder(request.getXContentType(), type, true); response.toXContent(builder, request); restResponse = new BytesRestResponse(RestStatus.OK, builder); } else {// TextFormat TextFormat type = (TextFormat)responseMediaType; final String data = type.format(request, response); restResponse = new BytesRestResponse(RestStatus.OK, type.contentType(request), data.getBytes(StandardCharsets.UTF_8)); if (response.hasCursor()) { restResponse.addHeader("Cursor", response.cursor()); } } restResponse.addHeader("Took-nanos", Long.toString(System.nanoTime() - startNanos)); return restResponse; }	nit: suggestion } else { // textformat
static Settings additionalSettings(final Settings settings, final boolean enabled, final boolean transportClientMode) { if (enabled && transportClientMode) { final Settings.Builder builder = Settings.builder(); builder.put(SecuritySettings.addTransportSettings(settings)); builder.put(SecuritySettings.addUserSettings(settings)); return builder.build(); } else { return Settings.EMPTY; } }	i think these are redundant because the same four actions are already registered [in actionmodule](https://github.com/elastic/elasticsearch/blob/e639036ef1a8fd11c6c11baf7bd8b1a0c2a007eb/server/src/main/java/org/elasticsearch/action/actionmodule.java#l512).
*/ Boolean guessIsDayFirstFromMatches(@Nullable TimestampFormat onlyConsiderFormat) { BitSet firstIndeterminateNumbers = new BitSet(); BitSet secondIndeterminateNumbers = new BitSet(); for (TimestampMatch match : matches) { if (onlyConsiderFormat == null || onlyConsiderFormat.canMergeWith(match.timestampFormat)) { // Valid indetermine day/month numbers will be in the range 1 to 31. // -1 is used to mean "not present", and we ignore that here. if (match.firstIndeterminateDateNumber > 0) { assert match.firstIndeterminateDateNumber <= 31; if (match.firstIndeterminateDateNumber > 12) { explanation.add("Guessing day precedes month in timestamps as one sample had first number [" + match.firstIndeterminateDateNumber + "]"); return Boolean.TRUE; } firstIndeterminateNumbers.set(match.firstIndeterminateDateNumber); } if (match.secondIndeterminateDateNumber > 0) { assert match.secondIndeterminateDateNumber <= 31; if (match.secondIndeterminateDateNumber > 12) { explanation.add("Guessing month precedes day in timestamps as one sample had second number [" + match.secondIndeterminateDateNumber + "]"); return Boolean.FALSE; } secondIndeterminateNumbers.set(match.secondIndeterminateDateNumber); } } } // If there are many more values of one number than the other then assume that's the day final int ratioForResult = 3; int firstCardinality = firstIndeterminateNumbers.cardinality(); int secondCardinality = secondIndeterminateNumbers.cardinality(); if (secondCardinality == 0) { // This happens in the following cases: // - No indeterminate numbers (in which case the answer is irrelevant) // - Only one indeterminate number (in which case we favour month over day) return Boolean.FALSE; } // firstCardinality can be 0, but then secondCardinality should have been 0 too assert firstCardinality > 0; if (firstCardinality >= ratioForResult * secondCardinality) { explanation.add("Guessing day precedes month in timestamps as there were [" + firstCardinality + "] distinct values of the first number but only [" + secondCardinality + "] for the second"); return Boolean.TRUE; } if (secondCardinality >= ratioForResult * firstCardinality) { explanation.add("Guessing month precedes day in timestamps as there " + (firstCardinality == 1 ? "was" : "were") + " only [" + firstCardinality + "] distinct " + (firstCardinality == 1 ? "value" : "values") + " of the first number but [" + secondCardinality + "] for the second"); return Boolean.FALSE; } return null; }	typo: indeterminate; it is a confusing one to type :-)
public static double distance(GeoShape shape1, GeoShape shape2) { if (shape1.shapeBuilder instanceof PointBuilder == false) { throw new SqlIllegalArgumentException("distance calculation is only supported for points; received [{}]", shape1); } if (shape2.shapeBuilder instanceof PointBuilder == false) { throw new SqlIllegalArgumentException("distance calculation is only supported for points; received [{}]", shape1); } double srcLat = ((PointBuilder) shape1.shapeBuilder).latitude(); double srcLon = ((PointBuilder) shape1.shapeBuilder).longitude(); double dstLat = ((PointBuilder) shape2.shapeBuilder).latitude(); double dstLon = ((PointBuilder) shape2.shapeBuilder).longitude(); return GeoUtils.arcDistance(srcLat, srcLon, dstLat, dstLon); }	the error message here i think it should refer to shape2.
public static double distance(GeoShape shape1, GeoShape shape2) { if (shape1.shapeBuilder instanceof PointBuilder == false) { throw new SqlIllegalArgumentException("distance calculation is only supported for points; received [{}]", shape1); } if (shape2.shapeBuilder instanceof PointBuilder == false) { throw new SqlIllegalArgumentException("distance calculation is only supported for points; received [{}]", shape1); } double srcLat = ((PointBuilder) shape1.shapeBuilder).latitude(); double srcLon = ((PointBuilder) shape1.shapeBuilder).longitude(); double dstLat = ((PointBuilder) shape2.shapeBuilder).latitude(); double dstLon = ((PointBuilder) shape2.shapeBuilder).longitude(); return GeoUtils.arcDistance(srcLat, srcLon, dstLat, dstLon); }	can you use curly brackets for all these conditions, please? i think this is the overall style used in the other classes from sql. if (this == o) { return true; } if (o == null || getclass() != o.getclass()) { return false; }
public static Collection<? extends Entry> getNamedWriteables() { List<NamedWriteableRegistry.Entry> entries = new ArrayList<>(); entries.add(new Entry(IntervalDayTime.class, IntervalDayTime.NAME, IntervalDayTime::new)); entries.add(new Entry(IntervalYearMonth.class, IntervalYearMonth.NAME, IntervalYearMonth::new)); entries.add(new Entry(GeoShape.class, GeoShape.NAME, GeoShape::new)); return entries; }	since the new literal is a non interval, the getnamedwriteables could be extracted to a separate class literals.
protected void performReroute(String reason) { try { if (lifecycle.stopped()) { return; } if (rerouting.compareAndSet(false, true) == false) { logger.trace("already has pending reroute, ignoring {}", reason); return; } logger.trace("rerouting {}", reason); clusterService.submitStateUpdateTask(CLUSTER_UPDATE_TASK_SOURCE + "(" + reason + ")", new ClusterStateUpdateTask(Priority.HIGH) { @Override public ClusterState execute(ClusterState currentState) { rerouting.set(false); return allocationService.reroute(currentState, reason); } @Override public void onNoLongerMaster(String source) { rerouting.set(false); // no biggie } @Override public void onFailure(String source, Exception e) { rerouting.set(false); ClusterState state = clusterService.state(); if (logger.isTraceEnabled()) { logger.error((Supplier<?>) () -> new ParameterizedMessage("unexpected failure during [{}], current state:\\\\n{}", source, state), e); } else { logger.error((Supplier<?>) () -> new ParameterizedMessage("unexpected failure during [{}], current state version [{}]", source, state.version()), e); } } }); } catch (Exception e) { rerouting.set(false); ClusterState state = clusterService.state(); logger.warn((Supplier<?>) () -> new ParameterizedMessage("failed to reroute routing table, current state:\\\\n{}", state), e); } }	i wonder if this should be guarded by an iswarnenabled to avoid building the cluster state string if not.
private <Request extends AbstractSqlRequest, Response> Response post(String path, Request request, CheckedFunction<XContentParser, Response, IOException> responseParser) throws SQLException { byte[] requestBytes = toXContent(request); String query = "error_trace&mode=" + request.mode() + (request.clientId() != null ? "&clientid=" + request.clientId() : ""); Tuple<XContentType, byte[]> response = AccessController.doPrivileged((PrivilegedAction<ResponseOrException<Tuple<XContentType, byte[]>>>) () -> JreHttpUrlConnection.http(path, query, cfg, con -> con.request( (out) -> out.write(requestBytes), this::readFrom, "POST" ) )).getResponseOrThrowException(); return fromXContent(response.v1(), response.v2(), responseParser); }	use a constant for client id (by declaring it inside connectionconfiguration). to have consistent naming, use client.id instead of clientid
protected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException { SqlQueryRequest sqlRequest; try (XContentParser parser = request.contentOrSourceParamParser()) { String clientId = request.param("clientid"); if (clientId != null) { if (clientId.length() > CLIENT_ID_MAX_LENGTH) { clientId = clientId.substring(0, CLIENT_ID_MAX_LENGTH).toLowerCase(Locale.ROOT); } if (!clientId.equals(CLI) && !clientId.equals(CANVAS)) { clientId = null; } } sqlRequest = SqlQueryRequest.fromXContent(parser, new RequestInfo(Mode.fromString(request.param("mode")), clientId)); } /* * Since we support {@link TextFormat} <strong>and</strong> * {@link XContent} outputs we can't use {@link RestToXContentListener} * like everything else. We want to stick as closely as possible to * Elasticsearch's defaults though, while still layering in ways to * control the output more easilly. * * First we find the string that the user used to specify the response * format. If there is a {@code format} paramter we use that. If there * isn't but there is a {@code Accept} header then we use that. If there * isn't then we use the {@code Content-Type} header which is required. */ String accept = request.param("format"); if (accept == null) { accept = request.header("Accept"); if ("*/*".equals(accept)) { // */* means "I don't care" which we should treat like not specifying the header accept = null; } } if (accept == null) { accept = request.header("Content-Type"); } assert accept != null : "The Content-Type header is required"; /* * Second, we pick the actual content type to use by first parsing the * string from the previous step as an {@linkplain XContent} value. If * that doesn't parse we parse it as a {@linkplain TextFormat} value. If * that doesn't parse it'll throw an {@link IllegalArgumentException} * which we turn into a 400 error. */ XContentType xContentType = accept == null ? XContentType.JSON : XContentType.fromMediaTypeOrFormat(accept); if (xContentType != null) { return channel -> client.execute(SqlQueryAction.INSTANCE, sqlRequest, new RestResponseListener<SqlQueryResponse>(channel) { @Override public RestResponse buildResponse(SqlQueryResponse response) throws Exception { XContentBuilder builder = XContentBuilder.builder(xContentType.xContent()); response.toXContent(builder, request); return new BytesRestResponse(RestStatus.OK, builder); } }); } TextFormat textFormat = TextFormat.fromMediaTypeOrFormat(accept); long startNanos = System.nanoTime(); return channel -> client.execute(SqlQueryAction.INSTANCE, sqlRequest, new RestResponseListener<SqlQueryResponse>(channel) { @Override public RestResponse buildResponse(SqlQueryResponse response) throws Exception { Cursor cursor = Cursors.decodeFromString(sqlRequest.cursor()); final String data = textFormat.format(cursor, request, response); RestResponse restResponse = new BytesRestResponse(RestStatus.OK, textFormat.contentType(request), data.getBytes(StandardCharsets.UTF_8)); Cursor responseCursor = textFormat.wrapCursor(cursor, response); if (responseCursor != Cursor.EMPTY) { restResponse.addHeader("Cursor", Cursors.encodeToString(Version.CURRENT, responseCursor)); } restResponse.addHeader("Took-nanos", Long.toString(System.nanoTime() - startNanos)); return restResponse; } }); }	use constant for client.id instead of a string.
protected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException { SqlQueryRequest sqlRequest; try (XContentParser parser = request.contentOrSourceParamParser()) { String clientId = request.param("clientid"); if (clientId != null) { if (clientId.length() > CLIENT_ID_MAX_LENGTH) { clientId = clientId.substring(0, CLIENT_ID_MAX_LENGTH).toLowerCase(Locale.ROOT); } if (!clientId.equals(CLI) && !clientId.equals(CANVAS)) { clientId = null; } } sqlRequest = SqlQueryRequest.fromXContent(parser, new RequestInfo(Mode.fromString(request.param("mode")), clientId)); } /* * Since we support {@link TextFormat} <strong>and</strong> * {@link XContent} outputs we can't use {@link RestToXContentListener} * like everything else. We want to stick as closely as possible to * Elasticsearch's defaults though, while still layering in ways to * control the output more easilly. * * First we find the string that the user used to specify the response * format. If there is a {@code format} paramter we use that. If there * isn't but there is a {@code Accept} header then we use that. If there * isn't then we use the {@code Content-Type} header which is required. */ String accept = request.param("format"); if (accept == null) { accept = request.header("Accept"); if ("*/*".equals(accept)) { // */* means "I don't care" which we should treat like not specifying the header accept = null; } } if (accept == null) { accept = request.header("Content-Type"); } assert accept != null : "The Content-Type header is required"; /* * Second, we pick the actual content type to use by first parsing the * string from the previous step as an {@linkplain XContent} value. If * that doesn't parse we parse it as a {@linkplain TextFormat} value. If * that doesn't parse it'll throw an {@link IllegalArgumentException} * which we turn into a 400 error. */ XContentType xContentType = accept == null ? XContentType.JSON : XContentType.fromMediaTypeOrFormat(accept); if (xContentType != null) { return channel -> client.execute(SqlQueryAction.INSTANCE, sqlRequest, new RestResponseListener<SqlQueryResponse>(channel) { @Override public RestResponse buildResponse(SqlQueryResponse response) throws Exception { XContentBuilder builder = XContentBuilder.builder(xContentType.xContent()); response.toXContent(builder, request); return new BytesRestResponse(RestStatus.OK, builder); } }); } TextFormat textFormat = TextFormat.fromMediaTypeOrFormat(accept); long startNanos = System.nanoTime(); return channel -> client.execute(SqlQueryAction.INSTANCE, sqlRequest, new RestResponseListener<SqlQueryResponse>(channel) { @Override public RestResponse buildResponse(SqlQueryResponse response) throws Exception { Cursor cursor = Cursors.decodeFromString(sqlRequest.cursor()); final String data = textFormat.format(cursor, request, response); RestResponse restResponse = new BytesRestResponse(RestStatus.OK, textFormat.contentType(request), data.getBytes(StandardCharsets.UTF_8)); Cursor responseCursor = textFormat.wrapCursor(cursor, response); if (responseCursor != Cursor.EMPTY) { restResponse.addHeader("Cursor", Cursors.encodeToString(Version.CURRENT, responseCursor)); } restResponse.addHeader("Took-nanos", Long.toString(System.nanoTime() - startNanos)); return restResponse; } }); }	not sure what this block tries to achieve. if just cli and canvas are accepted, why bother checking the length?
private ZenPing.PingCollection pingAndWait(TimeValue timeout) { final ZenPing.PingCollection response = new ZenPing.PingCollection(); final CountDownLatch latch = new CountDownLatch(1); final AtomicBoolean counted = new AtomicBoolean(); try { pinger.ping(pings -> { response.addPings(pings); if (counted.compareAndSet(false, true)) { latch.countDown(); } }, timeout); } catch (Exception ex) { logger.warn("Ping execution failed", ex); if (counted.compareAndSet(false, true)) { latch.countDown(); } } try { latch.await(); return response; } catch (InterruptedException e) { logger.trace("pingAndWait interrupted"); return response; } }	nit: we can double countdown() the latch, so i don't think we need the counted boolean?
protected ZenPing newZenPing(Settings settings, ThreadPool threadPool, TransportService transportService, UnicastHostsProvider hostsProvider) { Logger logger = Loggers.getLogger(getClass(), settings); logger.info("Creating unicast zen ping"); return new UnicastZenPing(settings, threadPool, transportService, hostsProvider); }	i think we can remove this log line as it's normal?
@Override protected ZenPing newZenPing(Settings settings, ThreadPool threadPool, TransportService transportService, UnicastHostsProvider hostsProvider) { if (getPluginsService().filterPlugins(MockZenPing.TestPlugin.class).isEmpty()) { return super.newZenPing(settings, threadPool, transportService, hostsProvider); } else { return new MockZenPing(settings); } }	doesn't have to be part of this change, but i wonder if we should use a test only setting for this. might be simpler.
public static void tryVirtualLock() { Kernel32Library kernel = Kernel32Library.getInstance(); Pointer process = null; try { process = kernel.GetCurrentProcess(); // By default, Windows limits the number of pages that can be locked. // Thus, we need to first increase the working set size of the JVM by // the amount of memory we wish to lock, plus a small overhead (1MB). SizeT size = new SizeT(JvmInfo.jvmInfo().getMem().getHeapInit().getBytes() + (1024 * 1024)); if (!kernel.SetProcessWorkingSetSize(process, size, size)) { logger.warn("Unable to lock JVM memory. Failed to set working set size. Error code " + Native.getLastError()); } else { Kernel32Library.MemoryBasicInformation memInfo = new Kernel32Library.MemoryBasicInformation(); long address = 0; while (kernel.VirtualQueryEx(process, new Pointer(address), memInfo, memInfo.size()) != 0) { boolean lockable = memInfo.State.longValue() == Kernel32Library.MEM_COMMIT && (memInfo.Protect.longValue() & Kernel32Library.PAGE_NOACCESS) != Kernel32Library.PAGE_NOACCESS && (memInfo.Protect.longValue() & Kernel32Library.PAGE_GUARD) != Kernel32Library.PAGE_GUARD; if (lockable) { kernel.VirtualLock(memInfo.BaseAddress, new SizeT(memInfo.RegionSize.longValue())); } // Move to the next region address += memInfo.RegionSize.longValue(); } LOCAL_MLOCKALL = true; } } catch (UnsatisfiedLinkError e) { // this will have already been logged by Kernel32Library, no need to repeat it } finally { if (process != null) { kernel.CloseHandle(process); } } }	why does it need the overhead (might be good to document for future people)?
public Object value(Aggregation aggregation, Map<String, String> fieldTypeMap, String lookupFieldPrefix) { assert aggregation instanceof InternalAggregation && ((InternalAggregation) aggregation).getWriteableName() .equals(TransformAggregations.AggregationType.GEO_LINE.getName()) : "Unexpected type [" + aggregation.getClass().getName() + "] for aggregation [" + aggregation.getName() + "]"; try (XContentBuilder xContentBuilder = XContentFactory.jsonBuilder()) { xContentBuilder.startObject(); ((InternalAggregation) aggregation).doXContentBody(xContentBuilder, ToXContent.EMPTY_PARAMS); xContentBuilder.endObject(); Map<String, Object> aggMap = XContentHelper.convertToMap( BytesReference.bytes(xContentBuilder), false, XContentType.JSON ).v2(); // If `geometry` doesn't exist, then it should just be null return aggMap.get("geometry"); } catch (IOException ex) { throw new AggregationExtractionException( "failed to extract geo_line aggregation for aggregation [{}]", ex, aggregation.getName() ); } } } static class GeoTileBucketKeyExtractor implements BucketKeyExtractor { @Override public Object value(Object key, String type) { assert key instanceof String; Rectangle rectangle = GeoTileUtils.toBoundingBox(key.toString()); final Map<String, Object> geoShape = new HashMap<>(); geoShape.put(ShapeParser.FIELD_TYPE.getPreferredName(), PolygonBuilder.TYPE.shapeName()); geoShape.put( ShapeParser.FIELD_COORDINATES.getPreferredName(), Collections.singletonList( Arrays.asList( new Double[] { rectangle.getMaxLon(), rectangle.getMinLat() }, new Double[] { rectangle.getMinLon(), rectangle.getMinLat() }, new Double[] { rectangle.getMinLon(), rectangle.getMaxLat() }, new Double[] { rectangle.getMaxLon(), rectangle.getMaxLat() }, new Double[] { rectangle.getMaxLon(), rectangle.getMinLat() } ) ) ); return geoShape; } } static class DefaultBucketKeyExtractor implements BucketKeyExtractor { @Override public Object value(Object key, String type) { if (isNumericType(type) && key instanceof Double) { return dropFloatingPointComponentIfTypeRequiresIt(type, (Double) key); } else if ((DateFieldMapper.CONTENT_TYPE.equals(type) || DateFieldMapper.DATE_NANOS_CONTENT_TYPE.equals(type)) && key instanceof Long) { // date_histogram return bucket keys with milliseconds since epoch precision, therefore we don't need a // nanosecond formatter, for the parser on indexing side, time is optional (only the date part is mandatory) return DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.formatMillis((Long) key); } return key; } } static class DatesAsEpochBucketKeyExtractor implements BucketKeyExtractor { @Override public Object value(Object key, String type) { if (isNumericType(type) && key instanceof Double) { return dropFloatingPointComponentIfTypeRequiresIt(type, (Double) key); } return key; }	this is **rough** i would much rather have the internalgeoline agg in xpack core. that way we can cast the object and get stuff directly.
int read(byte[] b, int offset, int length) throws IOException { assert this.pos < maxPos : "should not try and read from a fully-read stream"; final int read = readFully(inputStream, b, offset, length, () -> {}); assert read <= length : read + " vs " + length; pos += read; return read; }	i'm wondering whether we should throw an eofexception if this hits eof too. do we need to behave differently here?
@Override public String validate(String setting, String value) { try { if (value == null) { return "the value of " + setting + " can not be null"; } if (!value.endsWith("%")) { return "the value of " + setting + " must end with %"; } final double asDouble = Double.parseDouble(value.substring(0, value.length() - 1)); if (asDouble < 0.0 || asDouble > 100.0) { return "the value of the setting " + setting + " must be a percentage between 0% and 100%"; } } catch (NumberFormatException ex) { return ex.getMessage(); } return null; }	can we include value's value in this error message?
*/ public static JavaVersion parse(String value) { Objects.requireNonNull(value); String prePart = null; if (!isValid(value)) { throw new IllegalArgumentException("value"); } List<Integer> version = new ArrayList<>(); String[] parts = value.split("-"); String[] numericComponents; if (parts.length == 1) { numericComponents = value.split("\\\\\\\\."); } else if (parts.length == 2) { numericComponents = parts[0].split("\\\\\\\\."); prePart = parts[1]; } else { throw new IllegalArgumentException("value"); } for (String component : numericComponents) { version.add(Integer.valueOf(component)); } return new JavaVersion(version, prePart); }	we should have a more descriptive failure message, which includes the actual value, not the literal string value. ;)
@Deprecated public void performRequestAsync(String method, String endpoint, Map<String, String> params, HttpEntity entity, ResponseListener responseListener, Header... headers) { Request request; try { request = new Request(method, endpoint); request.setParameters(params); request.setEntity(entity); request.setHeaders(headers); } catch (Exception e) { responseListener.onFailure(e); return; } performRequestAsync(request, responseListener); } /** * Sends a request to the Elasticsearch cluster that the client points to. The request is executed asynchronously * and the provided {@link ResponseListener} gets notified upon request completion or failure. * Selects a host out of the provided ones in a round-robin fashion. Failing hosts are marked dead and retried after a certain * amount of time (minimum 1 minute, maximum 30 minutes), depending on how many times they previously failed (the more failures, * the later they will be retried). In case of failures all of the alive nodes (or dead nodes that deserve a retry) are retried * until one responds or none of them does, in which case an {@link IOException} will be thrown. * * @param method the http method * @param endpoint the path of the request (without host and port) * @param params the query_string parameters * @param entity the body of the request, null if not applicable * @param httpAsyncResponseConsumerFactory the {@link HttpAsyncResponseConsumerFactory} used to create one * {@link HttpAsyncResponseConsumer} callback per retry. Controls how the response body gets streamed from a non-blocking HTTP * connection on the client side. * @param responseListener the {@link ResponseListener} to notify when the request is completed or fails * @param headers the optional request headers * @deprecated Prefer {@link #performRequestAsync(Request, ResponseListener)}	shall we have a small private method that allows to build the request given all the different params and incorporates the catch as well?
@Override public Query rangeQuery(Object lowerTerm, Object upperTerm, boolean includeLower, boolean includeUpper, QueryShardContext context) { if (lowerTerm == null || upperTerm == null) { throw new IllegalArgumentException("[range] queries on keyed [" + CONTENT_TYPE + "] fields must include both an upper and a lower bound."); } return super.rangeQuery(lowerTerm, upperTerm, includeLower, includeUpper, context); }	we could fix this instead of disallowing unbounded range queries, but i wasn't sure the extra complexity was worth it. it's not clear how useful this range functionality will be right now, especially without support for numerics.
public void testDynamicTemplates() throws Exception { String mapping = Strings.toString(XContentFactory.jsonBuilder() .startObject() .startObject(MapperService.SINGLE_MAPPING_NAME) .startArray("dynamic_templates") .startObject() .startObject("my_template") .field("match_mapping_type", "string") .startObject("mapping") .field("type", "keyword") .endObject() .endObject() .endObject() .endArray() .endObject() .endObject()); MapperService mapperService = createMapperService(mapping); assertEquals(mapping, mapperService.documentMapper().mappingSource().toString()); // no update if templates are not set explicitly String mapping2 = Strings.toString(XContentFactory.jsonBuilder() .startObject() .startObject(MapperService.SINGLE_MAPPING_NAME) .endObject() .endObject()); merge(mapperService, mapping2); assertEquals(mapping, mapperService.documentMapper().mappingSource().toString()); String mapping3 = Strings.toString(XContentFactory.jsonBuilder() .startObject() .startObject(MapperService.SINGLE_MAPPING_NAME) .field("dynamic_templates", Collections.emptyList()) .endObject() .endObject()); merge(mapperService, mapping3); assertEquals(mapping3, mapperService.documentMapper().mappingSource().toString()); }	this one got harder to read!
private void notifyListeners(final long globalCheckpoint, final IndexShardClosedException e) { assert Thread.holdsLock(this) : Thread.currentThread(); assertNotification(globalCheckpoint, e); // early return if there are no listeners if (listeners.isEmpty()) { return; } final Map<GlobalCheckpointListener, Tuple<Long, ScheduledFuture<?>>> listenersToNotify; if (globalCheckpoint != UNASSIGNED_SEQ_NO) { listenersToNotify = listeners .entrySet() .stream() .filter(entry -> entry.getValue().v1() <= globalCheckpoint) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); listenersToNotify.keySet().forEach(listeners::remove); } else { listenersToNotify = new HashMap<>(listeners); listeners.clear(); } if (listenersToNotify.isEmpty() == false) { listenersToNotify .forEach((listener, t) -> { /* * We do not want to interrupt any timeouts that fired, these will detect that the listener has been notified and not * trigger the timeout. */ FutureUtils.cancel(t.v2()); listener.executor().execute(() -> notifyListener(listener, globalCheckpoint, e)); }); } }	maybe move the wrapping listener.executor().execute(() ... into notifylistener to dry things up? we only ever invoke notifylistener from listener.executor().execute(() -> ... anyway.
public <T extends TransportResponse> void sendRequest(Transport.Connection connection, String action, TransportRequest request, TransportRequestOptions options, TransportResponseHandler<T> handler) { final boolean requireAuth = shouldRequireExistingAuthentication(); // the transport in core normally does this check, BUT since we are serializing to a string header we need to do it // ourselves otherwise we wind up using a version newer than what we can actually send final Version minVersion = Version.min(connection.getVersion(), Version.CURRENT); // Sometimes a system action gets executed like a internal create index request or update mappings request // which means that the user is copied over to system actions so we need to change the user if (AuthorizationUtils.shouldReplaceUserWithSystem(threadPool.getThreadContext(), action)) { securityContext.executeAsUser(SystemUser.INSTANCE, (original) -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original) , handler), sender, requireAuth), minVersion); } else if (AuthorizationUtils.shouldSetUserBasedOnActionOrigin(threadPool.getThreadContext())) { AuthorizationUtils.switchUserBasedOnActionOriginAndExecute(threadPool.getThreadContext(), securityContext, (original) -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original) , handler), sender, requireAuth)); } else if (securityContext.getAuthentication() != null && securityContext.getAuthentication().getVersion().equals(minVersion) == false) { // re-write the authentication since we want the authentication version to match the version of the connection securityContext.executeAfterRewritingAuthentication(original -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original), handler), sender, requireAuth), minVersion); } else { sendWithUser(connection, action, request, options, handler, sender, requireAuth); } }	do we still need to copy to a local variable that we use only once directly below ?
public <T extends TransportResponse> void sendRequest(Transport.Connection connection, String action, TransportRequest request, TransportRequestOptions options, TransportResponseHandler<T> handler) { final boolean requireAuth = shouldRequireExistingAuthentication(); // the transport in core normally does this check, BUT since we are serializing to a string header we need to do it // ourselves otherwise we wind up using a version newer than what we can actually send final Version minVersion = Version.min(connection.getVersion(), Version.CURRENT); // Sometimes a system action gets executed like a internal create index request or update mappings request // which means that the user is copied over to system actions so we need to change the user if (AuthorizationUtils.shouldReplaceUserWithSystem(threadPool.getThreadContext(), action)) { securityContext.executeAsUser(SystemUser.INSTANCE, (original) -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original) , handler), sender, requireAuth), minVersion); } else if (AuthorizationUtils.shouldSetUserBasedOnActionOrigin(threadPool.getThreadContext())) { AuthorizationUtils.switchUserBasedOnActionOriginAndExecute(threadPool.getThreadContext(), securityContext, (original) -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original) , handler), sender, requireAuth)); } else if (securityContext.getAuthentication() != null && securityContext.getAuthentication().getVersion().equals(minVersion) == false) { // re-write the authentication since we want the authentication version to match the version of the connection securityContext.executeAfterRewritingAuthentication(original -> sendWithUser(connection, action, request, options, new ContextRestoreResponseHandler<>(threadPool.getThreadContext().wrapRestorable(original), handler), sender, requireAuth), minVersion); } else { sendWithUser(connection, action, request, options, handler, sender, requireAuth); } }	this comment is not true anymore, right?
public static WritePrimaryResult<Request, Response> performOnPrimary(final Request request, final IndexShard primary, final Logger logger) { Objects.requireNonNull(request); Objects.requireNonNull(primary); // we flush to ensure that retention leases are committed flush(primary); return new WritePrimaryResult<>(request, new Response(), null, null, primary, logger); }	can we assert that we hold an operation permit here?
public static WriteReplicaResult<Request> performOnReplica(final Request request, final IndexShard replica, final Logger logger) { Objects.requireNonNull(request); Objects.requireNonNull(replica); replica.updateRetentionLeasesOnReplica(request.getRetentionLeases()); // we flush to ensure that retention leases are committed flush(replica); return new WriteReplicaResult<>(request, null, null, replica, logger); }	can we assert that we hold an operation permit here?
static RemoteInfo buildRemoteInfo(Map<String, Object> source) throws IOException { @SuppressWarnings("unchecked") Map<String, Object> remote = (Map<String, Object>) source.remove("remote"); if (remote == null) { return null; } String username = extractString(remote, "username"); String password = extractString(remote, "password"); String hostInRequest = requireNonNull(extractString(remote, "host"), "[host] must be specified to reindex from a remote cluster"); URI uri; try { uri = new URI(hostInRequest); // URI has less stringent URL parsing than our code. We want to fail if all values are not provided. if (uri.getPort() == -1) { throw new URISyntaxException(hostInRequest, "The port was not defined in the [host]"); } } catch (URISyntaxException ex) { throw new IllegalArgumentException("[host] must be of the form [scheme]://[host]:[port](/[pathPrefix])? but was [" + hostInRequest + "]", ex); } String scheme = uri.getScheme(); String host = uri.getHost(); int port = uri.getPort(); String pathPrefix = null; if (uri.getPath().isEmpty() == false) { pathPrefix = uri.getPath(); } Map<String, String> headers = extractStringStringMap(remote, "headers"); TimeValue socketTimeout = extractTimeValue(remote, "socket_timeout", RemoteInfo.DEFAULT_SOCKET_TIMEOUT); TimeValue connectTimeout = extractTimeValue(remote, "connect_timeout", RemoteInfo.DEFAULT_CONNECT_TIMEOUT); if (false == remote.isEmpty()) { throw new IllegalArgumentException( "Unsupported fields in [remote]: [" + Strings.collectionToCommaDelimitedString(remote.keySet()) + "]"); } return new RemoteInfo(scheme, host, port, pathPrefix, queryForRemote(source), username, password, headers, socketTimeout, connectTimeout); }	also check that scheme is not null in the above validation?
static RemoteInfo buildRemoteInfo(Map<String, Object> source) throws IOException { @SuppressWarnings("unchecked") Map<String, Object> remote = (Map<String, Object>) source.remove("remote"); if (remote == null) { return null; } String username = extractString(remote, "username"); String password = extractString(remote, "password"); String hostInRequest = requireNonNull(extractString(remote, "host"), "[host] must be specified to reindex from a remote cluster"); URI uri; try { uri = new URI(hostInRequest); // URI has less stringent URL parsing than our code. We want to fail if all values are not provided. if (uri.getPort() == -1) { throw new URISyntaxException(hostInRequest, "The port was not defined in the [host]"); } } catch (URISyntaxException ex) { throw new IllegalArgumentException("[host] must be of the form [scheme]://[host]:[port](/[pathPrefix])? but was [" + hostInRequest + "]", ex); } String scheme = uri.getScheme(); String host = uri.getHost(); int port = uri.getPort(); String pathPrefix = null; if (uri.getPath().isEmpty() == false) { pathPrefix = uri.getPath(); } Map<String, String> headers = extractStringStringMap(remote, "headers"); TimeValue socketTimeout = extractTimeValue(remote, "socket_timeout", RemoteInfo.DEFAULT_SOCKET_TIMEOUT); TimeValue connectTimeout = extractTimeValue(remote, "connect_timeout", RemoteInfo.DEFAULT_CONNECT_TIMEOUT); if (false == remote.isEmpty()) { throw new IllegalArgumentException( "Unsupported fields in [remote]: [" + Strings.collectionToCommaDelimitedString(remote.keySet()) + "]"); } return new RemoteInfo(scheme, host, port, pathPrefix, queryForRemote(source), username, password, headers, socketTimeout, connectTimeout); }	same for host, check that it's not null?
static RemoteInfo buildRemoteInfo(Map<String, Object> source) throws IOException { @SuppressWarnings("unchecked") Map<String, Object> remote = (Map<String, Object>) source.remove("remote"); if (remote == null) { return null; } String username = extractString(remote, "username"); String password = extractString(remote, "password"); String hostInRequest = requireNonNull(extractString(remote, "host"), "[host] must be specified to reindex from a remote cluster"); URI uri; try { uri = new URI(hostInRequest); // URI has less stringent URL parsing than our code. We want to fail if all values are not provided. if (uri.getPort() == -1) { throw new URISyntaxException(hostInRequest, "The port was not defined in the [host]"); } } catch (URISyntaxException ex) { throw new IllegalArgumentException("[host] must be of the form [scheme]://[host]:[port](/[pathPrefix])? but was [" + hostInRequest + "]", ex); } String scheme = uri.getScheme(); String host = uri.getHost(); int port = uri.getPort(); String pathPrefix = null; if (uri.getPath().isEmpty() == false) { pathPrefix = uri.getPath(); } Map<String, String> headers = extractStringStringMap(remote, "headers"); TimeValue socketTimeout = extractTimeValue(remote, "socket_timeout", RemoteInfo.DEFAULT_SOCKET_TIMEOUT); TimeValue connectTimeout = extractTimeValue(remote, "connect_timeout", RemoteInfo.DEFAULT_CONNECT_TIMEOUT); if (false == remote.isEmpty()) { throw new IllegalArgumentException( "Unsupported fields in [remote]: [" + Strings.collectionToCommaDelimitedString(remote.keySet()) + "]"); } return new RemoteInfo(scheme, host, port, pathPrefix, queryForRemote(source), username, password, headers, socketTimeout, connectTimeout); }	what if getpath returns null?
public void testFlushOnceUncommittedTranslogReachesMaxAge() throws Exception { final String indexName = "flush_on_old_translog"; List<String> dataNodes = internalCluster().startDataOnlyNodes(2, Settings.builder() .put(IndexingMemoryController.SHARD_UNCOMMITTED_TRANSLOG_MAX_AGE_SETTING.getKey(), TimeValue.timeValueSeconds(3)) .put(IndexingMemoryController.SHARD_INACTIVE_TIME_SETTING.getKey(), TimeValue.timeValueDays(1)).build()); ByteSizeValue translogGenerationThreshold = new ByteSizeValue(Translog.DEFAULT_HEADER_SIZE_IN_BYTES + 1, ByteSizeUnit.BYTES); assertAcked(client().admin().indices().prepareCreate(indexName).setSettings(Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexSettings.INDEX_TRANSLOG_SYNC_INTERVAL_SETTING.getKey(), randomTimeValue(200, 500, "ms")) .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), randomTimeValue(50, 200, "ms")) .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES)) .put(IndexSettings.INDEX_TRANSLOG_GENERATION_THRESHOLD_SIZE_SETTING.getKey(), translogGenerationThreshold) .put("index.routing.allocation.include._name", String.join(",", dataNodes)) .build())); ensureGreen(indexName); int numDocs = randomIntBetween(1, 50); for (int i = 0; i < numDocs; i++) { client().prepareIndex(indexName) .setSource("f", "v".repeat((int) translogGenerationThreshold.getBytes())).get(); // force multiple translog generations } if (randomBoolean()) { internalCluster().restartNode(randomFrom(dataNodes), new InternalTestCluster.RestartCallback()); ensureGreen(indexName); } assertBusy(() -> { for (ShardStats shardStats : client().admin().indices().prepareStats(indexName).get().getShards()) { assertThat(shardStats.getStats().getTranslog().getUncommittedOperations(), equalTo(0)); } }, 10, TimeUnit.SECONDS); }	perhaps reduce indices.memory.interval to make this test run faster.
public void testFlushOnceUncommittedTranslogReachesMaxAge() throws Exception { final String indexName = "flush_on_old_translog"; List<String> dataNodes = internalCluster().startDataOnlyNodes(2, Settings.builder() .put(IndexingMemoryController.SHARD_UNCOMMITTED_TRANSLOG_MAX_AGE_SETTING.getKey(), TimeValue.timeValueSeconds(3)) .put(IndexingMemoryController.SHARD_INACTIVE_TIME_SETTING.getKey(), TimeValue.timeValueDays(1)).build()); ByteSizeValue translogGenerationThreshold = new ByteSizeValue(Translog.DEFAULT_HEADER_SIZE_IN_BYTES + 1, ByteSizeUnit.BYTES); assertAcked(client().admin().indices().prepareCreate(indexName).setSettings(Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexSettings.INDEX_TRANSLOG_SYNC_INTERVAL_SETTING.getKey(), randomTimeValue(200, 500, "ms")) .put(IndexService.GLOBAL_CHECKPOINT_SYNC_INTERVAL_SETTING.getKey(), randomTimeValue(50, 200, "ms")) .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES)) .put(IndexSettings.INDEX_TRANSLOG_GENERATION_THRESHOLD_SIZE_SETTING.getKey(), translogGenerationThreshold) .put("index.routing.allocation.include._name", String.join(",", dataNodes)) .build())); ensureGreen(indexName); int numDocs = randomIntBetween(1, 50); for (int i = 0; i < numDocs; i++) { client().prepareIndex(indexName) .setSource("f", "v".repeat((int) translogGenerationThreshold.getBytes())).get(); // force multiple translog generations } if (randomBoolean()) { internalCluster().restartNode(randomFrom(dataNodes), new InternalTestCluster.RestartCallback()); ensureGreen(indexName); } assertBusy(() -> { for (ShardStats shardStats : client().admin().indices().prepareStats(indexName).get().getShards()) { assertThat(shardStats.getStats().getTranslog().getUncommittedOperations(), equalTo(0)); } }, 10, TimeUnit.SECONDS); }	do we need this condition?
public void flushOnIdle(long inactiveTimeNS) { Engine engineOrNull = getEngineOrNull(); if (engineOrNull != null && System.nanoTime() - engineOrNull.getLastWriteNanos() >= inactiveTimeNS) { flush(); } } /** * Called by {@link IndexingMemoryController}	i think we should rename this flush method to flushoninactive (or flushonidle) as it collides with flush(flushrequest).
private static Streamable tryCreateNewInstance(Streamable streamable) throws NoSuchMethodException, InstantiationException, IllegalAccessException, InvocationTargetException { try { Class<? extends Streamable> clazz = streamable.getClass(); Constructor<? extends Streamable> constructor = clazz.getConstructor(); assertThat(constructor, Matchers.notNullValue()); Streamable newInstance = constructor.newInstance(); return newInstance; } catch (Exception e) { return null; } } /** * This attemps to construct a new {@link Streamable} object that is in the process of * being converted from {@link Streamable} to {@link Writeable}. Assuming this constructs * the object successfully, #readFrom should not be called on the constructed object. * * @param streamable the object to retrieve the type of class to construct the new instance from * @param in the stream to read the object from * @return the newly constructed object from reading the stream * @throws NoSuchMethodException if constuctor cannot be found * @throws InstantiationException if the class represents an abstract class * @throws IllegalAccessException if this {@code Constructor}	i don't think you need this assertion. i think it just throws instead, right?
static AggValueExtractor getExtractor(Aggregation aggregation, String fieldType) { if (aggregation instanceof NumericMetricsAggregation.SingleValue) { return new SingleValueAggExtractor((SingleValue) aggregation, fieldType); } else if (aggregation instanceof ScriptedMetric) { return new ScriptedMetricAggExtractor((ScriptedMetric) aggregation); } else if (aggregation instanceof GeoCentroid) { return new GeoCentroidAggExtractor((GeoCentroid)aggregation); } else { // Execution should never reach this point! // Creating transforms with unsupported aggregations shall not be possible throw new AggregationExtractionException("unsupported aggregation [{}] with name [{}]", aggregation.getType(), aggregation.getName()); } }	_opinion_: this looks definitely nicer, but now we are creating 1 more object for every bucket result. the extractor object is created and extractor.value() gets immediately called on the next line (line 76, 77). so the object is very short-lived. it should be possible to return static instances and pass aggregation and fieldtype as part of value().
public Map<String, IndexStats> getIndices() { if (indicesStats != null) { return indicesStats; } final Map<String, IndexStatsBuilder> indexToIndexStatsBuilder = new HashMap<>(); for (ShardStats shard : shards) { Index index = shard.getShardRouting().index(); IndexStatsBuilder indexStatsBuilder = indexToIndexStatsBuilder.computeIfAbsent(index.getName(), k -> { ClusterHealthStatus health = null; IndexMetadata.State state = null; if (clusterState != null) { health = ClusterHealthStatus.fromClusterState(clusterState, index); IndexMetadata indexMetadata = clusterState.getMetadata().index(index); if (indexMetadata != null) { state = indexMetadata.getState(); } } return new IndexStatsBuilder(k, index.getUUID(), health, state); }); indexStatsBuilder.add(shard); } indicesStats = indexToIndexStatsBuilder.entrySet() .stream() .collect(Collectors.toMap(Map.Entry::getKey, entry -> entry.getValue().build())); return indicesStats; }	this is going to be often null since we don't serialize this anywhere in the class
public void testIds() { Map<Integer, Class<? extends ElasticsearchException>> ids = new HashMap<>(); ids.put(0, org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException.class); ids.put(1, org.elasticsearch.search.dfs.DfsPhaseExecutionException.class); ids.put(2, org.elasticsearch.common.util.CancellableThreads.ExecutionCancelledException.class); ids.put(3, org.elasticsearch.discovery.MasterNotDiscoveredException.class); ids.put(4, org.elasticsearch.ElasticsearchSecurityException.class); ids.put(5, org.elasticsearch.index.snapshots.IndexShardRestoreException.class); ids.put(6, org.elasticsearch.indices.IndexClosedException.class); ids.put(7, org.elasticsearch.http.BindHttpException.class); ids.put(8, org.elasticsearch.action.search.ReduceSearchPhaseException.class); ids.put(9, org.elasticsearch.node.NodeClosedException.class); ids.put(10, org.elasticsearch.index.engine.SnapshotFailedEngineException.class); ids.put(11, org.elasticsearch.index.shard.ShardNotFoundException.class); ids.put(12, org.elasticsearch.transport.ConnectTransportException.class); ids.put(13, org.elasticsearch.transport.NotSerializableTransportException.class); ids.put(14, org.elasticsearch.transport.ResponseHandlerFailureTransportException.class); ids.put(15, org.elasticsearch.indices.IndexCreationException.class); ids.put(16, org.elasticsearch.index.IndexNotFoundException.class); ids.put(17, org.elasticsearch.cluster.routing.IllegalShardRoutingStateException.class); ids.put(18, org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException.class); ids.put(19, org.elasticsearch.ResourceNotFoundException.class); ids.put(20, org.elasticsearch.transport.ActionTransportException.class); ids.put(21, org.elasticsearch.ElasticsearchGenerationException.class); ids.put(22, org.elasticsearch.index.engine.CreateFailedEngineException.class); ids.put(23, org.elasticsearch.index.shard.IndexShardStartedException.class); ids.put(24, org.elasticsearch.search.SearchContextMissingException.class); ids.put(25, org.elasticsearch.script.ScriptException.class); ids.put(26, org.elasticsearch.index.shard.TranslogRecoveryPerformer.BatchOperationException.class); ids.put(27, org.elasticsearch.snapshots.SnapshotCreationException.class); ids.put(28, org.elasticsearch.index.engine.DeleteFailedEngineException.class); ids.put(29, org.elasticsearch.index.engine.DocumentMissingException.class); ids.put(30, org.elasticsearch.snapshots.SnapshotException.class); ids.put(31, org.elasticsearch.indices.InvalidAliasNameException.class); ids.put(32, org.elasticsearch.indices.InvalidIndexNameException.class); ids.put(33, org.elasticsearch.indices.IndexPrimaryShardNotAllocatedException.class); ids.put(34, org.elasticsearch.transport.TransportException.class); ids.put(35, org.elasticsearch.ElasticsearchParseException.class); ids.put(36, org.elasticsearch.search.SearchException.class); ids.put(37, org.elasticsearch.index.mapper.MapperException.class); ids.put(38, org.elasticsearch.indices.InvalidTypeNameException.class); ids.put(39, org.elasticsearch.snapshots.SnapshotRestoreException.class); ids.put(40, org.elasticsearch.common.ParsingException.class); ids.put(41, org.elasticsearch.index.shard.IndexShardClosedException.class); ids.put(42, org.elasticsearch.indices.recovery.RecoverFilesRecoveryException.class); ids.put(43, org.elasticsearch.index.translog.TruncatedTranslogException.class); ids.put(44, org.elasticsearch.indices.recovery.RecoveryFailedException.class); ids.put(45, org.elasticsearch.index.shard.IndexShardRelocatedException.class); ids.put(46, org.elasticsearch.transport.NodeShouldNotConnectException.class); ids.put(47, org.elasticsearch.indices.IndexTemplateAlreadyExistsException.class); ids.put(48, org.elasticsearch.index.translog.TranslogCorruptedException.class); ids.put(49, org.elasticsearch.cluster.block.ClusterBlockException.class); ids.put(50, org.elasticsearch.search.fetch.FetchPhaseExecutionException.class); ids.put(51, org.elasticsearch.index.IndexShardAlreadyExistsException.class); ids.put(52, org.elasticsearch.index.engine.VersionConflictEngineException.class); ids.put(53, org.elasticsearch.index.engine.EngineException.class); ids.put(54, org.elasticsearch.index.engine.DocumentAlreadyExistsException.class); ids.put(55, org.elasticsearch.action.NoSuchNodeException.class); ids.put(56, org.elasticsearch.common.settings.SettingsException.class); ids.put(57, org.elasticsearch.indices.IndexTemplateMissingException.class); ids.put(58, org.elasticsearch.transport.SendRequestTransportException.class); ids.put(59, org.elasticsearch.common.util.concurrent.EsRejectedExecutionException.class); ids.put(60, org.elasticsearch.common.lucene.Lucene.EarlyTerminationException.class); ids.put(61, org.elasticsearch.cluster.routing.RoutingValidationException.class); ids.put(62, org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class); ids.put(63, org.elasticsearch.indices.AliasFilterParsingException.class); ids.put(64, org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class); ids.put(65, org.elasticsearch.gateway.GatewayException.class); ids.put(66, org.elasticsearch.index.shard.IndexShardNotRecoveringException.class); ids.put(67, org.elasticsearch.http.HttpException.class); ids.put(68, org.elasticsearch.ElasticsearchException.class); ids.put(69, org.elasticsearch.snapshots.SnapshotMissingException.class); ids.put(70, org.elasticsearch.action.PrimaryMissingActionException.class); ids.put(71, org.elasticsearch.action.FailedNodeException.class); ids.put(72, org.elasticsearch.search.SearchParseException.class); ids.put(73, org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException.class); ids.put(74, org.elasticsearch.common.blobstore.BlobStoreException.class); ids.put(75, org.elasticsearch.cluster.IncompatibleClusterStateVersionException.class); ids.put(76, org.elasticsearch.index.engine.RecoveryEngineException.class); ids.put(77, org.elasticsearch.common.util.concurrent.UncategorizedExecutionException.class); ids.put(78, org.elasticsearch.action.TimestampParsingException.class); ids.put(79, org.elasticsearch.action.RoutingMissingException.class); ids.put(80, org.elasticsearch.index.engine.IndexFailedEngineException.class); ids.put(81, org.elasticsearch.index.snapshots.IndexShardRestoreFailedException.class); ids.put(82, org.elasticsearch.repositories.RepositoryException.class); ids.put(83, org.elasticsearch.transport.ReceiveTimeoutTransportException.class); ids.put(84, org.elasticsearch.transport.NodeDisconnectedException.class); ids.put(85, org.elasticsearch.index.AlreadyExpiredException.class); ids.put(86, org.elasticsearch.search.aggregations.AggregationExecutionException.class); ids.put(87, org.elasticsearch.index.mapper.MergeMappingException.class); ids.put(88, org.elasticsearch.indices.InvalidIndexTemplateException.class); ids.put(89, org.elasticsearch.percolator.PercolateException.class); ids.put(90, org.elasticsearch.index.engine.RefreshFailedEngineException.class); ids.put(91, org.elasticsearch.search.aggregations.AggregationInitializationException.class); ids.put(92, org.elasticsearch.indices.recovery.DelayRecoveryException.class); ids.put(93, org.elasticsearch.search.warmer.IndexWarmerMissingException.class); ids.put(94, org.elasticsearch.client.transport.NoNodeAvailableException.class); ids.put(95, null); ids.put(96, org.elasticsearch.snapshots.InvalidSnapshotNameException.class); ids.put(97, org.elasticsearch.index.shard.IllegalIndexShardStateException.class); ids.put(98, org.elasticsearch.index.snapshots.IndexShardSnapshotException.class); ids.put(99, org.elasticsearch.index.shard.IndexShardNotStartedException.class); ids.put(100, org.elasticsearch.action.search.SearchPhaseExecutionException.class); ids.put(101, org.elasticsearch.transport.ActionNotFoundTransportException.class); ids.put(102, org.elasticsearch.transport.TransportSerializationException.class); ids.put(103, org.elasticsearch.transport.RemoteTransportException.class); ids.put(104, org.elasticsearch.index.engine.EngineCreationFailureException.class); ids.put(105, org.elasticsearch.cluster.routing.RoutingException.class); ids.put(106, org.elasticsearch.index.shard.IndexShardRecoveryException.class); ids.put(107, org.elasticsearch.repositories.RepositoryMissingException.class); ids.put(108, org.elasticsearch.index.percolator.PercolatorException.class); ids.put(109, org.elasticsearch.index.engine.DocumentSourceMissingException.class); ids.put(110, org.elasticsearch.index.engine.FlushNotAllowedEngineException.class); ids.put(111, org.elasticsearch.common.settings.NoClassSettingsException.class); ids.put(112, org.elasticsearch.transport.BindTransportException.class); ids.put(113, org.elasticsearch.rest.action.admin.indices.alias.delete.AliasesNotFoundException.class); ids.put(114, org.elasticsearch.index.shard.IndexShardRecoveringException.class); ids.put(115, org.elasticsearch.index.translog.TranslogException.class); ids.put(116, org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException.class); ids.put(117, org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnPrimaryException.class); ids.put(118, org.elasticsearch.ElasticsearchTimeoutException.class); ids.put(119, org.elasticsearch.search.query.QueryPhaseExecutionException.class); ids.put(120, org.elasticsearch.repositories.RepositoryVerificationException.class); ids.put(121, org.elasticsearch.search.aggregations.InvalidAggregationPathException.class); ids.put(122, null); ids.put(123, org.elasticsearch.indices.IndexAlreadyExistsException.class); ids.put(124, org.elasticsearch.script.Script.ScriptParseException.class); ids.put(125, org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.HttpOnTransportException.class); ids.put(126, org.elasticsearch.index.mapper.MapperParsingException.class); ids.put(127, org.elasticsearch.search.SearchContextException.class); ids.put(128, org.elasticsearch.search.builder.SearchSourceBuilderException.class); ids.put(129, org.elasticsearch.index.engine.EngineClosedException.class); ids.put(130, org.elasticsearch.action.NoShardAvailableActionException.class); ids.put(131, org.elasticsearch.action.UnavailableShardsException.class); ids.put(132, org.elasticsearch.index.engine.FlushFailedEngineException.class); ids.put(133, org.elasticsearch.common.breaker.CircuitBreakingException.class); ids.put(134, org.elasticsearch.transport.NodeNotConnectedException.class); ids.put(135, org.elasticsearch.index.mapper.StrictDynamicMappingException.class); ids.put(136, org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException.class); ids.put(137, org.elasticsearch.indices.TypeMissingException.class); ids.put(138, null); ids.put(139, null); ids.put(140, org.elasticsearch.discovery.Discovery.FailedToCommitClusterStateException.class); ids.put(141, org.elasticsearch.index.query.QueryShardException.class); Map<Class<? extends ElasticsearchException>, Integer> reverse = new HashMap<>(); for (Map.Entry<Integer, Class<? extends ElasticsearchException>> entry : ids.entrySet()) { if (entry.getValue() != null) { reverse.put(entry.getValue(), entry.getKey()); } } for (ElasticsearchException.ElasticsearchExceptionHandle handle : ElasticsearchException.ElasticsearchExceptionHandle.values()) { assertEquals((int)reverse.get(handle.exceptionClass), handle.id); } for (Map.Entry<Integer, Class<? extends ElasticsearchException>> entry : ids.entrySet()) { if (entry.getValue() != null) { assertEquals((int) entry.getKey(), ElasticsearchException.getId(entry.getValue())); } } }	should this really be null?
private static FieldCapabilities createFieldCapabilities(String field, String type) { return new FieldCapabilities(field, type, false, true, true, false, null, null, null, null, null, Collections.emptyMap()); }	maybe it'd be helpful to make a public static final fieldcapabilities simple(string name, string type, boolean searchable, boolean aggregable) { return new fieldcapabilities(field, type, false, searchable, aggregable, false, null, null, null, null, null, collections.emptymap()); } on fieldcapabilities or some test helper. it looks like a super common pattern.
* @return a warning value formatted according to RFC 7234 */ public static String formatWarning(final String s) { // Assume that the common scenario won't have a string to escape and encode. int length = WARNING_PREFIX.length() + s.length() + 3; final StringBuilder sb = new StringBuilder(length); sb.append(WARNING_PREFIX).append(" \\\\"").append(escapeAndEncode(s)).append("\\\\""); return sb.toString(); }	nit: remove empty line.
public static String format(final String prefix, final String messagePattern, final Object... argArray) { if (messagePattern == null) { return null; } if ((argArray == null) || (argArray.length == 0)) { if (prefix == null) { return messagePattern; } else { return prefix + messagePattern; } } int i = 0; int j; final StringBuilder sbuf = new StringBuilder(messagePattern.length() + 50); if (prefix != null) { sbuf.append(prefix); } for (int L = 0; L < argArray.length; L++) { j = messagePattern.indexOf(DELIM_STR, i); if (j == -1) { // no more variables if (i == 0) { // this is a simple string return messagePattern; } else { // add the tail string which contains no variables and return // the result. sbuf.append(messagePattern.substring(i, messagePattern.length())); return sbuf.toString(); } } else { if (isEscapedDelimiter(messagePattern, j)) { if (!isDoubleEscaped(messagePattern, j)) { L--; // DELIM_START was escaped, thus should not be incremented sbuf.append(messagePattern.substring(i, j - 1)); sbuf.append(DELIM_START); i = j + 1; } else { // The escape character preceding the delimiter start is // itself escaped: "abc x:\\\\\\\\{}" // we have to consume one backward slash sbuf.append(messagePattern.substring(i, j - 1)); deeplyAppendParameter(sbuf, argArray[L], new HashSet<Object[]>()); i = j + 2; } } else { // normal case sbuf.append(messagePattern.substring(i, j)); deeplyAppendParameter(sbuf, argArray[L], new HashSet<Object[]>()); i = j + 2; } } } // append the characters following the last {} pair. sbuf.append(messagePattern.substring(i, messagePattern.length())); return sbuf.toString(); }	nit: remove redundant parentheses.
public void testCloseCursor() throws SQLException, IOException { index("library", "1", builder -> { builder.field("name", "foo"); }); index("library", "2", builder -> { builder.field("name", "bar"); }); index("library", "3", builder -> { builder.field("name", "baz"); }); try (Connection connection = createConnection(connectionProperties())) { try (Statement statement = connection.createStatement()) { statement.setFetchSize(1); ResultSet results = statement.executeQuery(" SELECT name FROM library"); assertTrue(results.next()); results.close(); // force sending a cursor close since more pages are available assertTrue(results.isClosed()); } } }	since there doesn't seem to be existing test cases covering .close() could you also add one for the empty cursor and exhausted result set cases?
protected <Request extends ActionRequest, Response extends ActionResponse> void doExecute( ActionType<Response> action, Request request, ActionListener<Response> listener ) { if (ensureConnected == false) { try { remoteClusterService.getConnection(clusterAlias); } catch (NoSuchRemoteClusterException e) { listener.onFailure(e); // trigger another connection attempt, but don't wait for it to complete remoteClusterService.ensureConnected(clusterAlias, ActionListener.wrap(() -> {})); return; } } remoteClusterService.ensureConnected(clusterAlias, ActionListener.wrap(v -> { Transport.Connection connection; if (request instanceof RemoteClusterAwareRequest) { DiscoveryNode preferredTargetNode = ((RemoteClusterAwareRequest) request).getPreferredTargetNode(); connection = remoteClusterService.getConnection(preferredTargetNode, clusterAlias); } else { connection = remoteClusterService.getConnection(clusterAlias); } service.sendRequest( connection, action.name(), request, TransportRequestOptions.EMPTY, new ActionListenerResponseHandler<>(listener, action.getResponseReader()) ); }, listener::onFailure)); }	this still risks a wait in cases where we discovered the remote cluster was disconnected just after calling getconnection() above. i think we should avoid waiting on the result of ensureconnected() at all if this.ensureconnected is false.
private Object createGitRevisionResolver(final Project project) { return new Object() { private final AtomicReference<String> gitRevision = new AtomicReference<>(); @Override public String toString() { if (gitRevision.get() == null) { final ByteArrayOutputStream stdout = new ByteArrayOutputStream(); final ByteArrayOutputStream stderr = new ByteArrayOutputStream(); final ExecResult result = project.exec(spec -> { spec.setExecutable("git"); spec.setArgs(Arrays.asList("rev-parse", "HEAD")); spec.setStandardOutput(stdout); spec.setErrorOutput(stderr); spec.setIgnoreExitValue(true); }); final String revision; if (result.getExitValue() != 0) { revision = "unknown"; } else { revision = stdout.toString(UTF_8).trim(); } this.gitRevision.compareAndSet(null, revision); } return gitRevision.get(); } }; }	would it be better instead of making this an atomicreference to simply synchronize these calls so we can avoid to possibility shelling to git multiple times if this is called concurrently? i think as it is we could do that and simply the first one would win.
private void joinFieldResolveConfig(QueryShardContext queryShardContext, ValuesSourceConfig<WithOrdinals> config) { ParentJoinFieldMapper parentJoinFieldMapper = ParentJoinFieldMapper.getMapper(queryShardContext.getMapperService()); if (parentJoinFieldMapper != null) { config.unmapped(true); return; } ParentIdFieldMapper parentIdFieldMapper = parentJoinFieldMapper.getParentIdFieldMapper(childType, false); if (parentIdFieldMapper != null) { config.unmapped(true); return; } parentFilter = parentIdFieldMapper.getParentFilter(); childFilter = parentIdFieldMapper.getChildFilter(childType); MappedFieldType fieldType = parentIdFieldMapper.fieldType(); final SortedSetDVOrdinalsIndexFieldData fieldData = queryShardContext.getForField(fieldType); config.fieldContext(new FieldContext(fieldType.name(), fieldData, fieldType)); }	should it be: suggestion if (parentjoinfieldmapper == null) { ?
private void joinFieldResolveConfig(QueryShardContext queryShardContext, ValuesSourceConfig<WithOrdinals> config) { ParentJoinFieldMapper parentJoinFieldMapper = ParentJoinFieldMapper.getMapper(queryShardContext.getMapperService()); if (parentJoinFieldMapper != null) { config.unmapped(true); return; } ParentIdFieldMapper parentIdFieldMapper = parentJoinFieldMapper.getParentIdFieldMapper(childType, false); if (parentIdFieldMapper != null) { config.unmapped(true); return; } parentFilter = parentIdFieldMapper.getParentFilter(); childFilter = parentIdFieldMapper.getChildFilter(childType); MappedFieldType fieldType = parentIdFieldMapper.fieldType(); final SortedSetDVOrdinalsIndexFieldData fieldData = queryShardContext.getForField(fieldType); config.fieldContext(new FieldContext(fieldType.name(), fieldData, fieldType)); }	same here ? suggestion if (parentidfieldmapper == null) {
public void testGetProcedures() throws Exception { try (Connection h2 = LocalH2.anonymousDb(); Connection es = esJdbc()) { h2.createStatement().executeUpdate("RUNSCRIPT FROM 'classpath:/setup_mock_metadata_get_procedures.sql'"); ResultSet expected = h2.createStatement().executeQuery("SELECT * FROM mock"); assertResultSets(expected, es.getMetaData().getProcedures( randomBoolean() ? null : randomAlphaOfLength(5), randomBoolean() ? null : randomAlphaOfLength(5), randomBoolean() ? null : randomAlphaOfLength(5)), true); } } /** * We do not support procedures so we return an empty set for * {@link DatabaseMetaData#getProcedureColumns(String, String, String, String)}	see the comment below - the short assertresultsets signature should be kept so then test would only have to specify when they don't want to be lenient as oppose to all the time.
@Override public String toString() { if (sourceBuilder != null) { return sourceBuilder.toString(); } if (request.source() != null) { try { return XContentHelper.convertToJson(request.source().toBytesArray(), false, true); } catch(Exception e) { return "{ \\\\"error\\\\" : \\\\"" + ExceptionsHelper.detailedMessage(e) + "\\\\"}"; } } return new QuerySourceBuilder().toString(); }	this is printing an empty thing. java searchrequestbuilder builder = client().preparesearch("index").settypes("type"); system.out.println("builder = " + builder); gives builder = { } is that expected @javanna? or should we wait for #9962 for a fix?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("model_id", modelId); if (modelSize != null) { builder.field("model_size", modelSize); } if (state != null) { builder.field("state", state); } if (reason != null) { builder.field("reason", reason); } if (health != null) { builder.field("health", health); } builder.startArray("nodes"); for (NodeStats nodeStat : nodeStats){ nodeStat.toXContent(builder, params); } builder.endArray(); builder.endObject(); return builder; }	i would make health an object field, the existing allocationhealth enum can be one of the fields of this object. other fields can be allocation_count and target_allocation_count. i think in the future we would regret making this field a scalar, because it's more disruptive and harder to achieve bwc when converting a scalar to an object than it is when adding or removing fields from an existing object. and as we move away from the mvp of "allocate models to every node" we'll want to wait for more complex states than simply "on one node" and "on every node". also in the future for debugging cpu-based autoscaling it will be interesting to see what the stats consider the target allocation count to be. so i am pretty sure we'll end up wanting these fields in the future even though they seem redundant now.
void retry(Throwable failure) { assert failure != null; if (observer.isTimedOut()) { // we running as a last attempt after a timeout has happened. don't retry finishAsFailed(failure); return; } // make it threaded operation so we fork on the discovery listener thread internalRequest.request().operationThreaded(true); observer.waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { start(); } @Override public void onClusterServiceClose() { listener.onFailure(new NodeClosedException(clusterService.localNode())); } @Override public void onTimeout(TimeValue timeout) { // Try one more time... start(); } }); } /** upon success, finish the first phase and transfer responsibility to the {@link ReplicationPhase}	we should call finishasfailed() also in onclusterserviceclose() i think?
@Override public void handleException(TransportException exp) { onReplicaFailure(nodeId, exp); logger.trace("[{}] transport failure during replica request [{}] ", exp, node, replicaRequest); if (!ignoreReplicaException(exp)) { logger.warn("failed to perform " + actionName + " on remote replica " + node + shardIt.shardId(), exp); shardStateAction.shardFailed(shard, indexMetaData.getUUID(), "Failed to perform [" + actionName + "] on replica, message [" + ExceptionsHelper.detailedMessage(exp) + "]"); } }	can we remove this whole block and add a check before that the node id is not the local one while we are already at it or do it in a different pr?
public void testCloseRoutingTableRemovesRoutingTable() { final Set<Index> nonBlockedIndices = new HashSet<>(); final Map<Index, ClusterBlock> blockedIndices = new HashMap<>(); final Map<Index, AcknowledgedResponse> results = new HashMap<>(); final ClusterBlock closingBlock = MetaDataIndexStateService.createIndexClosingBlock(); ClusterState state = ClusterState.builder(new ClusterName("testCloseRoutingTableRemovesRoutingTable")).build(); for (int i = 0; i < randomIntBetween(1, 25); i++) { final String indexName = "index-" + i; if (randomBoolean()) { state = addOpenedIndex(indexName, randomIntBetween(1, 5), randomIntBetween(0, 5), state); nonBlockedIndices.add(state.metaData().index(indexName).getIndex()); } else { state = addBlockedIndex(indexName, randomIntBetween(1, 5), randomIntBetween(0, 5), state, closingBlock); blockedIndices.put(state.metaData().index(indexName).getIndex(), closingBlock); results.put(state.metaData().index(indexName).getIndex(), new AcknowledgedResponse(randomBoolean())); } } state = ClusterState.builder(state) .nodes(DiscoveryNodes.builder(state.nodes()) .add(new DiscoveryNode("old_node", buildNewFakeTransportAddress(), emptyMap(), new HashSet<>(Arrays.asList(DiscoveryNode.Role.values())), Version.V_7_1_0))) .build(); state = MetaDataIndexStateService.closeRoutingTable(state, blockedIndices, results); assertThat(state.metaData().indices().size(), equalTo(nonBlockedIndices.size() + blockedIndices.size())); for (Index nonBlockedIndex : nonBlockedIndices) { assertIsOpened(nonBlockedIndex.getName(), state); assertThat(state.blocks().hasIndexBlockWithId(nonBlockedIndex.getName(), INDEX_CLOSED_BLOCK_ID), is(false)); } for (Index blockedIndex : blockedIndices.keySet()) { if (results.get(blockedIndex).isAcknowledged()) { assertThat(state.metaData().index(blockedIndex).getState(), is(IndexMetaData.State.CLOSE)); assertThat(state.blocks().hasIndexBlock(blockedIndex.getName(), MetaDataIndexStateService.INDEX_CLOSED_BLOCK), is(true)); assertThat("Index must have only 1 block with [id=" + MetaDataIndexStateService.INDEX_CLOSED_BLOCK_ID + "]", state.blocks().indices().getOrDefault(blockedIndex.getName(), emptySet()).stream() .filter(clusterBlock -> clusterBlock.id() == MetaDataIndexStateService.INDEX_CLOSED_BLOCK_ID).count(), equalTo(1L)); assertThat("Index routing table should have been removed when closing the index on mixed cluster version", state.routingTable().index(blockedIndex), nullValue()); } else { assertIsOpened(blockedIndex.getName(), state); assertThat(state.blocks().hasIndexBlock(blockedIndex.getName(), closingBlock), is(true)); } } }	should we add one 8.0 node too to verify that the min version is used?
public void testAddIndexClosedBlocks() { final ClusterState initialState = ClusterState.builder(new ClusterName("testAddIndexClosedBlocks")).build(); { final Map<Index, ClusterBlock> blockedIndices = new HashMap<>(); Index[] indices = new Index[]{new Index("_name", "_uid")}; expectThrows(IndexNotFoundException.class, () -> MetaDataIndexStateService.addIndexClosedBlocks(indices, blockedIndices, initialState)); assertTrue(blockedIndices.isEmpty()); } { final Map<Index, ClusterBlock> blockedIndices = new HashMap<>(); Index[] indices = Index.EMPTY_ARRAY; ClusterState updatedState = MetaDataIndexStateService.addIndexClosedBlocks(indices, blockedIndices, initialState); assertSame(initialState, updatedState); assertTrue(blockedIndices.isEmpty()); } { final Map<Index, ClusterBlock> blockedIndices = new HashMap<>(); ClusterState state = addClosedIndex("closed", randomIntBetween(1, 3), randomIntBetween(0, 3), initialState); Index[] indices = new Index[]{state.metaData().index("closed").getIndex()}; ClusterState updatedState = MetaDataIndexStateService.addIndexClosedBlocks(indices, blockedIndices, state); assertSame(state, updatedState); assertTrue(blockedIndices.isEmpty()); } { final Map<Index, ClusterBlock> blockedIndices = new HashMap<>(); ClusterState state = addClosedIndex("closed", randomIntBetween(1, 3), randomIntBetween(0, 3), initialState); state = addOpenedIndex("opened", randomIntBetween(1, 3), randomIntBetween(0, 3), state); Index[] indices = new Index[]{state.metaData().index("opened").getIndex(), state.metaData().index("closed").getIndex()}; ClusterState updatedState = MetaDataIndexStateService.addIndexClosedBlocks(indices, blockedIndices, state); assertNotSame(state, updatedState); Index opened = updatedState.metaData().index("opened").getIndex(); assertTrue(blockedIndices.containsKey(opened)); assertHasBlock("opened", updatedState, blockedIndices.get(opened)); Index closed = updatedState.metaData().index("closed").getIndex(); assertFalse(blockedIndices.containsKey(closed)); } { IllegalArgumentException exception = expectThrows(IllegalArgumentException.class, () -> { ClusterState state = addRestoredIndex("restored", randomIntBetween(1, 3), randomIntBetween(0, 3), initialState); if (randomBoolean()) { state = addOpenedIndex("opened", randomIntBetween(1, 3), randomIntBetween(0, 3), state); } if (randomBoolean()) { state = addOpenedIndex("closed", randomIntBetween(1, 3), randomIntBetween(0, 3), state); } Index[] indices = new Index[]{state.metaData().index("restored").getIndex()}; MetaDataIndexStateService.addIndexClosedBlocks(indices, unmodifiableMap(emptyMap()), state); }); assertThat(exception.getMessage(), containsString("Cannot close indices that are being restored: [[restored]]")); } { SnapshotInProgressException exception = expectThrows(SnapshotInProgressException.class, () -> { ClusterState state = addSnapshotIndex("snapshotted", randomIntBetween(1, 3), randomIntBetween(0, 3), initialState); if (randomBoolean()) { state = addOpenedIndex("opened", randomIntBetween(1, 3), randomIntBetween(0, 3), state); } if (randomBoolean()) { state = addOpenedIndex("closed", randomIntBetween(1, 3), randomIntBetween(0, 3), state); } Index[] indices = new Index[]{state.metaData().index("snapshotted").getIndex()}; MetaDataIndexStateService.addIndexClosedBlocks(indices, unmodifiableMap(emptyMap()), state); }); assertThat(exception.getMessage(), containsString("Cannot close indices that are being snapshotted: [[snapshotted]]")); } { final Map<Index, ClusterBlock> blockedIndices = new HashMap<>(); ClusterState state = addOpenedIndex("index-1", randomIntBetween(1, 3), randomIntBetween(0, 3), initialState); state = addOpenedIndex("index-2", randomIntBetween(1, 3), randomIntBetween(0, 3), state); state = addOpenedIndex("index-3", randomIntBetween(1, 3), randomIntBetween(0, 3), state); Index index1 = state.metaData().index("index-1").getIndex(); Index index2 = state.metaData().index("index-2").getIndex(); Index index3 = state.metaData().index("index-3").getIndex(); Index[] indices = new Index[]{index1, index2, index3}; ClusterState updatedState = MetaDataIndexStateService.addIndexClosedBlocks(indices, blockedIndices, state); assertNotSame(state, updatedState); for (Index index : indices) { assertTrue(blockedIndices.containsKey(index)); assertHasBlock(index.getName(), updatedState, blockedIndices.get(index)); } } }	would it maybe make sense to still verify this works in the mixed cluster case (against 7.x)?
@Override protected void masterOperation(Task task, final ClusterStateRequest request, final ClusterState state, final ActionListener<ClusterStateResponse> listener) throws IOException { assert task instanceof CancellableTask : task + " not cancellable"; final CancellableTask cancellableTask = (CancellableTask) task; final Predicate<ClusterState> acceptableClusterStatePredicate = request.waitForMetadataVersion() == null ? clusterState -> true : clusterState -> clusterState.metadata().version() >= request.waitForMetadataVersion(); final Predicate<ClusterState> acceptableClusterStateOrFailedPredicate = request.local() ? acceptableClusterStatePredicate : acceptableClusterStatePredicate.or(clusterState -> cancellableTask.isCancelled() || clusterState.nodes().isLocalNodeElectedMaster() == false); if (acceptableClusterStatePredicate.test(state)) { ActionListener.completeWith(listener, () -> buildResponse(request, state)); } else { // It is possible that the task is cancelled after the predicate has been executed, therefore we should take // that into account as well for the assertion assert acceptableClusterStateOrFailedPredicate.test(state) == false || request.local() == false && cancellableTask.isCancelled(); new ClusterStateObserver(state, clusterService, request.waitForTimeout(), logger, threadPool.getThreadContext()) .waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState newState) { if (cancellableTask.isCancelled()) { listener.onFailure(new TaskCancelledException("task cancelled")); } else if (acceptableClusterStatePredicate.test(newState)) { ActionListener.completeWith(listener, () -> buildResponse(request, newState)); } else { listener.onFailure(new NotMasterException( "master stepped down waiting for metadata version " + request.waitForMetadataVersion())); } } @Override public void onClusterServiceClose() { listener.onFailure(new NodeClosedException(clusterService.localNode())); } @Override public void onTimeout(TimeValue timeout) { try { if (cancellableTask.isCancelled()) { listener.onFailure(new TaskCancelledException("task cancelled")); } else { listener.onResponse(new ClusterStateResponse(state.getClusterName(), null, true)); } } catch (Exception e) { listener.onFailure(e); } } }, acceptableClusterStateOrFailedPredicate); } }	i was expecting the cancellabletask.iscancelled() here but not the request.local() check. digging into why that's needed led me to conclude that there's a bug in #67413: local requests should also complete on the first cluster state update after cancellation.
public void testKerberosRealmWithInvalidKeytabPathConfigurations() throws IOException { final String keytabPathCase = randomFrom("keytabPathAsDirectory", "keytabFileDoesNotExist", "keytabPathWithNoReadPermissions"); final String expectedErrorMessage; final String keytabPath; switch (keytabPathCase) { case "keytabPathAsDirectory": final String dirName = randomAlphaOfLength(5); Files.createDirectory(dir.resolve(dirName)); keytabPath = dir.resolve(dirName).toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] is a directory"; break; case "keytabFileDoesNotExist": keytabPath = dir.resolve(randomAlphaOfLength(5) + ".keytab").toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] does not exist"; break; case "keytabPathWithNoReadPermissions": final String fileName = randomAlphaOfLength(5); final Path keytabFilePath = Files.createTempFile(dir, fileName, ".keytab"); Files.write(keytabFilePath, randomAlphaOfLength(5).getBytes(StandardCharsets.UTF_8)); final Set<String> supportedAttributes = keytabFilePath.getFileSystem().supportedFileAttributeViews(); if (supportedAttributes.contains("posix")) { final PosixFileAttributeView fileAttributeView = Files.getFileAttributeView(keytabFilePath, PosixFileAttributeView.class); fileAttributeView.setPermissions(PosixFilePermissions.fromString("---------")); } else if (supportedAttributes.contains("acl")) { final UserPrincipal principal = Files.getOwner(keytabFilePath); final AclFileAttributeView view = Files.getFileAttributeView(keytabFilePath, AclFileAttributeView.class); final AclEntry entry = AclEntry.newBuilder() .setType(AclEntryType.DENY) .setPrincipal(principal) .setPermissions(AclEntryPermission.READ_DATA, AclEntryPermission.READ_ATTRIBUTES).build(); final List<AclEntry> acl = view.getAcl(); acl.add(0, entry); view.setAcl(acl); } else { throw new UnsupportedOperationException( String.format(Locale.ROOT, "Don't know how to make file [%s] non-readable on a file system with attributes [%s]", keytabFilePath, supportedAttributes)); } keytabPath = keytabFilePath.toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] must have read permission"; break; default: throw new IllegalArgumentException("Unknown test case :" + keytabPathCase); } settings = KerberosTestCase.buildKerberosRealmSettings(keytabPath, 100, "10m", true, randomBoolean()); config = new RealmConfig("test-kerb-realm", settings, globalSettings, TestEnvironment.newEnvironment(globalSettings), new ThreadContext(globalSettings)); mockNativeRoleMappingStore = roleMappingStore(Arrays.asList("user")); mockKerberosTicketValidator = mock(KerberosTicketValidator.class); final IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () -> new KerberosRealm(config, mockNativeRoleMappingStore, mockKerberosTicketValidator, threadPool, null)); assertThat(iae.getMessage(), is(equalTo(expectedErrorMessage))); }	i don't understand why we are using randomization here as opposed to simply running every case each time. randomization is good for finding edge cases where we randomize some input data. i don't think it's good for poor man's test coverage to ensure all branches are covered, we really should be exercising those on every test run.
protected final void onChannelClosed(Channel channel) { try { final Optional<Long> first = pendingHandshakes.entrySet().stream() .filter((entry) -> entry.getValue().channel == channel).map((e) -> e.getKey()).findFirst(); if (first.isPresent()) { final Long requestId = first.get(); final HandshakeResponseHandler handler = pendingHandshakes.remove(requestId); if (handler != null) { // there might be a race removing this or this method might be called twice concurrently depending on how // the channel is closed ie. due to connection reset or broken pipes handler.handleException(new TransportException("connection reset")); } } } finally { disconnectFromNodeChannel(channel, "channel closed"); } }	question - why did you add this? it currently has a nasty side effect of spawning extra "disconnect" generic threads when we are already disconnecting. say we have an issue on one channel, we find the relevant nodechannels and close all other channels in it. those will end up here, spawning another disconnectfromnodechannel call, which will be a no-op (as we are cleaning it up), but we're creating all kind of concurrency issues. i'm wondering why we need this.
protected final void disconnectFromNodeChannel(final Channel channel, final String reason) { threadPool.generic().execute(() -> { try { if (isOpen(channel)) { closeChannels(Collections.singletonList(channel)); } } catch (IOException e) { logger.warn("failed to close channel", e); } finally { outer: { for (Map.Entry<DiscoveryNode, NodeChannels> entry : connectedNodes.entrySet()) { if (disconnectFromNode(entry.getKey(), channel, reason)) { // if we managed to find this channel and disconnect from it, then break, no need to check on // the rest of the nodes // Removing it here from open connections is paranoia since the close handler should deal with this. openConnections.remove(entry.getValue()); // we can only be connected and published to a single node with one connection. So if disconnectFromNode // returns true we can safely break out from here since we cleaned up everything needed break outer; } } // now if we haven't found the right connection in the connected nodes we have to go through the open connections // it might be that the channel belongs to a connection that is not published for (NodeChannels channels : openConnections) { if (channels.hasChannel(channel)) { IOUtils.closeWhileHandlingException(channels); break; } } } } }); }	question - why did you add this? it seems all channels has double closing protection?
protected final void disconnectFromNodeChannel(final Channel channel, final String reason) { threadPool.generic().execute(() -> { try { if (isOpen(channel)) { closeChannels(Collections.singletonList(channel)); } } catch (IOException e) { logger.warn("failed to close channel", e); } finally { outer: { for (Map.Entry<DiscoveryNode, NodeChannels> entry : connectedNodes.entrySet()) { if (disconnectFromNode(entry.getKey(), channel, reason)) { // if we managed to find this channel and disconnect from it, then break, no need to check on // the rest of the nodes // Removing it here from open connections is paranoia since the close handler should deal with this. openConnections.remove(entry.getValue()); // we can only be connected and published to a single node with one connection. So if disconnectFromNode // returns true we can safely break out from here since we cleaned up everything needed break outer; } } // now if we haven't found the right connection in the connected nodes we have to go through the open connections // it might be that the channel belongs to a connection that is not published for (NodeChannels channels : openConnections) { if (channels.hasChannel(channel)) { IOUtils.closeWhileHandlingException(channels); break; } } } } }); }	given the comment, should we make this an assertion?
protected final void disconnectFromNodeChannel(final Channel channel, final String reason) { threadPool.generic().execute(() -> { try { if (isOpen(channel)) { closeChannels(Collections.singletonList(channel)); } } catch (IOException e) { logger.warn("failed to close channel", e); } finally { outer: { for (Map.Entry<DiscoveryNode, NodeChannels> entry : connectedNodes.entrySet()) { if (disconnectFromNode(entry.getKey(), channel, reason)) { // if we managed to find this channel and disconnect from it, then break, no need to check on // the rest of the nodes // Removing it here from open connections is paranoia since the close handler should deal with this. openConnections.remove(entry.getValue()); // we can only be connected and published to a single node with one connection. So if disconnectFromNode // returns true we can safely break out from here since we cleaned up everything needed break outer; } } // now if we haven't found the right connection in the connected nodes we have to go through the open connections // it might be that the channel belongs to a connection that is not published for (NodeChannels channels : openConnections) { if (channels.hasChannel(channel)) { IOUtils.closeWhileHandlingException(channels); break; } } } } }); }	i think we need some kind of locking coordination here with the opening connections line: nodechannels = connecttochannels(node, connectionprofile); <-- from here we might get exceptions final channel channel = nodechannels.getchannels().get(0); // one channel is guaranteed by the connection profile final timevalue connecttimeout = connectionprofile.getconnecttimeout() == null ? defaultconnectionprofile.getconnecttimeout() : connectionprofile.getconnecttimeout(); final timevalue handshaketimeout = connectionprofile.gethandshaketimeout() == null ? connecttimeout : connectionprofile.gethandshaketimeout(); final version version = executehandshake(node, channel, handshaketimeout); nodechannels = new nodechannels(nodechannels, version); // clone the channels - we now have the correct version openconnections.add(nodechannels); <-- published transportserviceadapter.onconnectionopened(nodechannels); i guess the easiest would be to acquire the write lock as this is a very rare situation and only relevant to temporary connections. alternatively we would could introduce a dedicated lock.
public void testConcurrentDisconnectOnNonPublishedConnection() throws IOException, InterruptedException { MockTransportService serviceC = build(Settings.builder().put("name", "TS_TEST").build(), version0, null, true); CountDownLatch receivedLatch = new CountDownLatch(1); CountDownLatch sendResponseLatch = new CountDownLatch(1); serviceC.registerRequestHandler("action", TestRequest::new, ThreadPool.Names.SAME, (request, channel) -> { // don't block on a network thread here threadPool.generic().execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { try { channel.sendResponse(e); } catch (IOException e1) { throw new UncheckedIOException(e1); } } @Override protected void doRun() throws Exception { receivedLatch.countDown(); sendResponseLatch.await(); channel.sendResponse(TransportResponse.Empty.INSTANCE); } }); }); serviceC.start(); serviceC.acceptIncomingRequests(); CountDownLatch responseLatch = new CountDownLatch(1); TransportResponseHandler<TransportResponse> transportResponseHandler = new TransportResponseHandler<TransportResponse>() { @Override public TransportResponse newInstance() { return TransportResponse.Empty.INSTANCE; } @Override public void handleResponse(TransportResponse response) { responseLatch.countDown(); } @Override public void handleException(TransportException exp) { responseLatch.countDown(); } @Override public String executor() { return ThreadPool.Names.SAME; } }; ConnectionProfile.Builder builder = new ConnectionProfile.Builder(); builder.addConnections(1, TransportRequestOptions.Type.BULK, TransportRequestOptions.Type.PING, TransportRequestOptions.Type.RECOVERY, TransportRequestOptions.Type.REG, TransportRequestOptions.Type.STATE); try (Transport.Connection connection = serviceB.openConnection(serviceC.getLocalNode(), builder.build())) { serviceB.sendRequest(connection, "action", new TestRequest("hello world"), TransportRequestOptions.EMPTY, transportResponseHandler); receivedLatch.await(); assertPendingConnections(1, serviceB.getOriginalTransport()); serviceC.close(); assertPendingConnections(0, serviceC.getOriginalTransport()); sendResponseLatch.countDown(); responseLatch.await(); } assertPendingConnections(0, serviceC.getOriginalTransport()); }	i would expect the request to be cancelled as the connection closed on it mid flight. what is the role of sendresponselatch? servicec is close now?
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final String[] aliases = request.paramAsStringArrayOrEmptyIfAll("name"); final GetAliasesRequest getAliasesRequest = new GetAliasesRequest(aliases); final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); getAliasesRequest.indices(indices); getAliasesRequest.indicesOptions(IndicesOptions.fromRequest(request, getAliasesRequest.indicesOptions())); getAliasesRequest.local(request.paramAsBoolean("local", getAliasesRequest.local())); return channel -> client.admin().indices().getAliases(getAliasesRequest, new RestBuilderListener<GetAliasesResponse>(channel) { @Override public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final ImmutableOpenMap<String, List<AliasMetaData>> aliasMap = response.getAliases(); final Set<String> aliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : aliasMap) { for (final AliasMetaData aliasMetaData : cursor.value) { aliasNames.add(aliasMetaData.alias()); } } // first remove requested aliases that are exact matches final SortedSet<String> difference = Sets.sortedDifference(Arrays.stream(aliases).collect(Collectors.toSet()), aliasNames); // now remove requested aliases that contain wildcards that are simple matches final List<String> matches = new ArrayList<>(); outer: for (final String pattern : difference) { if (pattern.contains("*")) { for (final String aliasName : aliasNames) { if (Regex.simpleMatch(pattern, aliasName)) { matches.add(pattern); continue outer; } } } } difference.removeAll(matches); final RestStatus status; builder.startObject(); { if (!difference.isEmpty()) { status = RestStatus.NOT_FOUND; final String message; if (difference.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", toNamesString(difference.iterator().next())); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", toNamesString(difference.toArray(new String[0]))); } builder.field("error", message); builder.field("status", RestStatus.NOT_FOUND.getStatus()); } else { status = RestStatus.OK; } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } builder.endObject(); return new BytesRestResponse(status, builder); } }); }	you can probably use aliasmap.values() since you don't need the key for anything right? i don't think it's necessarily any better though, so either way is fine.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final String[] aliases = request.paramAsStringArrayOrEmptyIfAll("name"); final GetAliasesRequest getAliasesRequest = new GetAliasesRequest(aliases); final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); getAliasesRequest.indices(indices); getAliasesRequest.indicesOptions(IndicesOptions.fromRequest(request, getAliasesRequest.indicesOptions())); getAliasesRequest.local(request.paramAsBoolean("local", getAliasesRequest.local())); return channel -> client.admin().indices().getAliases(getAliasesRequest, new RestBuilderListener<GetAliasesResponse>(channel) { @Override public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final ImmutableOpenMap<String, List<AliasMetaData>> aliasMap = response.getAliases(); final Set<String> aliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : aliasMap) { for (final AliasMetaData aliasMetaData : cursor.value) { aliasNames.add(aliasMetaData.alias()); } } // first remove requested aliases that are exact matches final SortedSet<String> difference = Sets.sortedDifference(Arrays.stream(aliases).collect(Collectors.toSet()), aliasNames); // now remove requested aliases that contain wildcards that are simple matches final List<String> matches = new ArrayList<>(); outer: for (final String pattern : difference) { if (pattern.contains("*")) { for (final String aliasName : aliasNames) { if (Regex.simpleMatch(pattern, aliasName)) { matches.add(pattern); continue outer; } } } } difference.removeAll(matches); final RestStatus status; builder.startObject(); { if (!difference.isEmpty()) { status = RestStatus.NOT_FOUND; final String message; if (difference.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", toNamesString(difference.iterator().next())); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", toNamesString(difference.toArray(new String[0]))); } builder.field("error", message); builder.field("status", RestStatus.NOT_FOUND.getStatus()); } else { status = RestStatus.OK; } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } builder.endObject(); return new BytesRestResponse(status, builder); } }); }	we can remove the ! if we reverse this if statement, so java if (difference.isempty()) { status = reststatus.ok; } else { ... the error stuff ... }
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final String[] aliases = request.paramAsStringArrayOrEmptyIfAll("name"); final GetAliasesRequest getAliasesRequest = new GetAliasesRequest(aliases); final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); getAliasesRequest.indices(indices); getAliasesRequest.indicesOptions(IndicesOptions.fromRequest(request, getAliasesRequest.indicesOptions())); getAliasesRequest.local(request.paramAsBoolean("local", getAliasesRequest.local())); return channel -> client.admin().indices().getAliases(getAliasesRequest, new RestBuilderListener<GetAliasesResponse>(channel) { @Override public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final ImmutableOpenMap<String, List<AliasMetaData>> aliasMap = response.getAliases(); final Set<String> aliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : aliasMap) { for (final AliasMetaData aliasMetaData : cursor.value) { aliasNames.add(aliasMetaData.alias()); } } // first remove requested aliases that are exact matches final SortedSet<String> difference = Sets.sortedDifference(Arrays.stream(aliases).collect(Collectors.toSet()), aliasNames); // now remove requested aliases that contain wildcards that are simple matches final List<String> matches = new ArrayList<>(); outer: for (final String pattern : difference) { if (pattern.contains("*")) { for (final String aliasName : aliasNames) { if (Regex.simpleMatch(pattern, aliasName)) { matches.add(pattern); continue outer; } } } } difference.removeAll(matches); final RestStatus status; builder.startObject(); { if (!difference.isEmpty()) { status = RestStatus.NOT_FOUND; final String message; if (difference.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", toNamesString(difference.iterator().next())); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", toNamesString(difference.toArray(new String[0]))); } builder.field("error", message); builder.field("status", RestStatus.NOT_FOUND.getStatus()); } else { status = RestStatus.OK; } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } builder.endObject(); return new BytesRestResponse(status, builder); } }); }	personally i think we should return reststatus.partial_content (206) in the event that requested aliases were requested and *some* were found while some were not, but it sounds like this ship has already sailed as far as what to return.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final String[] aliases = request.paramAsStringArrayOrEmptyIfAll("name"); final GetAliasesRequest getAliasesRequest = new GetAliasesRequest(aliases); final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); getAliasesRequest.indices(indices); getAliasesRequest.indicesOptions(IndicesOptions.fromRequest(request, getAliasesRequest.indicesOptions())); getAliasesRequest.local(request.paramAsBoolean("local", getAliasesRequest.local())); return channel -> client.admin().indices().getAliases(getAliasesRequest, new RestBuilderListener<GetAliasesResponse>(channel) { @Override public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final ImmutableOpenMap<String, List<AliasMetaData>> aliasMap = response.getAliases(); final Set<String> aliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : aliasMap) { for (final AliasMetaData aliasMetaData : cursor.value) { aliasNames.add(aliasMetaData.alias()); } } // first remove requested aliases that are exact matches final SortedSet<String> difference = Sets.sortedDifference(Arrays.stream(aliases).collect(Collectors.toSet()), aliasNames); // now remove requested aliases that contain wildcards that are simple matches final List<String> matches = new ArrayList<>(); outer: for (final String pattern : difference) { if (pattern.contains("*")) { for (final String aliasName : aliasNames) { if (Regex.simpleMatch(pattern, aliasName)) { matches.add(pattern); continue outer; } } } } difference.removeAll(matches); final RestStatus status; builder.startObject(); { if (!difference.isEmpty()) { status = RestStatus.NOT_FOUND; final String message; if (difference.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", toNamesString(difference.iterator().next())); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", toNamesString(difference.toArray(new String[0]))); } builder.field("error", message); builder.field("status", RestStatus.NOT_FOUND.getStatus()); } else { status = RestStatus.OK; } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } builder.endObject(); return new BytesRestResponse(status, builder); } }); }	i think we should have a consistent message regardless of the number of aliases, i know it's bad form for people to write tests against error messages, but that doesn't mean people don't. so, i think we should stick with just aliases [foo] missing. what do you think? (i'm only +0 on the change)
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final String[] aliases = request.paramAsStringArrayOrEmptyIfAll("name"); final GetAliasesRequest getAliasesRequest = new GetAliasesRequest(aliases); final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); getAliasesRequest.indices(indices); getAliasesRequest.indicesOptions(IndicesOptions.fromRequest(request, getAliasesRequest.indicesOptions())); getAliasesRequest.local(request.paramAsBoolean("local", getAliasesRequest.local())); return channel -> client.admin().indices().getAliases(getAliasesRequest, new RestBuilderListener<GetAliasesResponse>(channel) { @Override public RestResponse buildResponse(GetAliasesResponse response, XContentBuilder builder) throws Exception { final ImmutableOpenMap<String, List<AliasMetaData>> aliasMap = response.getAliases(); final Set<String> aliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : aliasMap) { for (final AliasMetaData aliasMetaData : cursor.value) { aliasNames.add(aliasMetaData.alias()); } } // first remove requested aliases that are exact matches final SortedSet<String> difference = Sets.sortedDifference(Arrays.stream(aliases).collect(Collectors.toSet()), aliasNames); // now remove requested aliases that contain wildcards that are simple matches final List<String> matches = new ArrayList<>(); outer: for (final String pattern : difference) { if (pattern.contains("*")) { for (final String aliasName : aliasNames) { if (Regex.simpleMatch(pattern, aliasName)) { matches.add(pattern); continue outer; } } } } difference.removeAll(matches); final RestStatus status; builder.startObject(); { if (!difference.isEmpty()) { status = RestStatus.NOT_FOUND; final String message; if (difference.size() == 1) { message = String.format(Locale.ROOT, "alias [%s] missing", toNamesString(difference.iterator().next())); } else { message = String.format(Locale.ROOT, "aliases [%s] missing", toNamesString(difference.toArray(new String[0]))); } builder.field("error", message); builder.field("status", RestStatus.NOT_FOUND.getStatus()); } else { status = RestStatus.OK; } for (final ObjectObjectCursor<String, List<AliasMetaData>> entry : response.getAliases()) { builder.startObject(entry.key); { builder.startObject("aliases"); { for (final AliasMetaData alias : entry.value) { AliasMetaData.Builder.toXContent(alias, builder, ToXContent.EMPTY_PARAMS); } } builder.endObject(); } builder.endObject(); } } builder.endObject(); return new BytesRestResponse(status, builder); } }); }	could be status.getstatus() in the event we change the local variable above.
void clearActiveKeyCache() { this.keyCache.activeKeyCache.keyCache.invalidateAll(); }	can you say package private for testing?
public void testRefreshingMultipleTimesFails() throws Exception { Client client = client().filterWithHeader(Collections.singletonMap("Authorization", UsernamePasswordToken.basicAuthHeaderValue(SecuritySettingsSource.TEST_USER_NAME, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING))); SecurityClient securityClient = new SecurityClient(client); CreateTokenResponse createTokenResponse = securityClient.prepareCreateToken() .setGrantType("password") .setUsername(SecuritySettingsSource.TEST_USER_NAME) .setPassword(new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray())) .get(); assertNotNull(createTokenResponse.getRefreshToken()); CreateTokenResponse refreshResponse = securityClient.prepareRefreshToken(createTokenResponse.getRefreshToken()).get(); assertNotNull(refreshResponse); // We now have two documents, the original(now refreshed) token doc and the new one with the new access doc AtomicReference<String> docId = new AtomicReference<>(); assertBusy(() -> { SearchResponse searchResponse = client.prepareSearch(SecurityIndexManager.SECURITY_INDEX_NAME) .setSource(SearchSourceBuilder.searchSource() .query(QueryBuilders.boolQuery() .must(QueryBuilders.termQuery("doc_type", "token")) .must(QueryBuilders.termQuery("refresh_token.refreshed", "true")))) .setSize(1) .setTerminateAfter(1) .get(); assertThat(searchResponse.getHits().getTotalHits().value, equalTo(1L)); docId.set(searchResponse.getHits().getAt(0).getId()); }); // hack doc to modify the refresh time to 50 seconds ago so that we don't hit the lenient refresh case Instant refreshed = Instant.now(); Instant aWhileAgo = refreshed.minus(50L, ChronoUnit.SECONDS); assertTrue(Instant.now().isAfter(aWhileAgo)); UpdateResponse updateResponse = client.prepareUpdate(SecurityIndexManager.SECURITY_INDEX_NAME, "doc", docId.get()) .setDoc("refresh_token", Collections.singletonMap("refresh_time", aWhileAgo.toEpochMilli())) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) .setFetchSource("refresh_token", Strings.EMPTY_STRING) .get(); assertNotNull(updateResponse); Map<String, Object> refreshTokenMap = (Map<String, Object>) updateResponse.getGetResult().sourceAsMap().get("refresh_token"); assertTrue( Instant.ofEpochMilli((long) refreshTokenMap.get("refresh_time")).isBefore(Instant.now().minus(30L, ChronoUnit.SECONDS))); ElasticsearchSecurityException e = expectThrows(ElasticsearchSecurityException.class, () -> securityClient.prepareRefreshToken(createTokenResponse.getRefreshToken()).get()); assertEquals("invalid_grant", e.getMessage()); assertEquals(RestStatus.BAD_REQUEST, e.status()); assertEquals("token has already been refreshed more than 30 seconds in the past", e.getHeader("error_description").get(0)); }	sorry, i was not clear about what i wanted changed. i meant that we should add the code to countdown the latch and return in addition to the thread.currentthread().interrupt(). so it should be: java try { readylatch.await(); } catch (interruptedexception e) { thread.currentthread().interrupt(); completedlatch.countdown(); return; }
public static double defTodoubleImplicit(final Object value) { if (value instanceof Byte) { return (byte)value; } else if (value instanceof Short) { return (short)value; } else if (value instanceof Character) { return (char)value; } else if (value instanceof Integer) { return (int)value; } else if (value instanceof Long) { return (long)value; } else if (value instanceof Float) { return (float)value; } else if (value instanceof Double) { return (double)value; } else { throw new ClassCastException("cannot implicitly " + "cast def [" + PainlessLookupUtility.typeToUnboxedType(value.getClass()).getCanonicalName() + "] to double"); } }	aren't all the boxed types handled above? when would we hit this else case? i just wonder about using typotounboxedtype on a non-boxed type?
@Override protected boolean shouldMarkCoordinatingBytes(BulkShardRequest request) { return request.getParentTask().getNodeId().equals(clusterService.localNode().getId()) == false; }	shouldn't this be the default implementation in transportwriteaction? is there still a need to override this method here?
private void scheduleUpdate() { assert Thread.holdsLock(mutex); try { threadPoolExecutor.execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { logger.error("Exception occurred when storing new meta data", e); } @Override protected void doRun() { final Long term; final ClusterState clusterState; synchronized (mutex) { if (newCurrentTermQueued) { term = getCurrentTerm(); newCurrentTermQueued = false; } else { term = null; } if (newStateQueued) { clusterState = getLastAcceptedState(); newStateQueued = false; } else { clusterState = null; } } // write current term before last accepted state so that it is never below term in last accepted state if (term != null) { lucenePersistedState.setCurrentTerm(term); } if (clusterState != null) { lucenePersistedState.setLastAcceptedState(resetVotingConfiguration(clusterState)); } } }); } catch (EsRejectedExecutionException e) { // ignore cases where we are shutting down..., there is really nothing interesting to be done here... if (threadPoolExecutor.isShutdown() == false) { throw e; } } }	i think this is unexpected unless we're shutting down, so maybe better to assert threadpoolexecutor.isshutdown().
private void scheduleUpdate() { assert Thread.holdsLock(mutex); try { threadPoolExecutor.execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { logger.error("Exception occurred when storing new meta data", e); } @Override protected void doRun() { final Long term; final ClusterState clusterState; synchronized (mutex) { if (newCurrentTermQueued) { term = getCurrentTerm(); newCurrentTermQueued = false; } else { term = null; } if (newStateQueued) { clusterState = getLastAcceptedState(); newStateQueued = false; } else { clusterState = null; } } // write current term before last accepted state so that it is never below term in last accepted state if (term != null) { lucenePersistedState.setCurrentTerm(term); } if (clusterState != null) { lucenePersistedState.setLastAcceptedState(resetVotingConfiguration(clusterState)); } } }); } catch (EsRejectedExecutionException e) { // ignore cases where we are shutting down..., there is really nothing interesting to be done here... if (threadPoolExecutor.isShutdown() == false) { throw e; } } }	i see some slightly questionable synchronisation here. we construct the persistedclusterstateservice.writer using a simple arraylist of metadataindexwriters, and then iterate through that when closing it on (potentially) a different thread. unrelated to this pr, i just noticed it.
protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) throws Exception { try { BytesReference networkBytes = Netty4Utils.toBytesReference(in); int messageLength = TcpTransport.readMessageLength(networkBytes); int messageWithHeaderLength = messageLength + HEADER_SIZE; // If the message length is -1, we have not read a complete header. If the message length is // greater than the network bytes available, we have not read a complete frame. if (messageLength != -1 && messageWithHeaderLength <= networkBytes.length()) { final ByteBuf message = in.skipBytes(HEADER_SIZE); // 6 bytes would mean it is a ping. And we should ignore. if (messageWithHeaderLength != 6) { out.add(message); } } } catch (IllegalArgumentException ex) { throw new TooLongFrameException(ex); } }	i do not even think we should be making calculations with messagelength if it is equal to -1 so we should guard all of this in if (messagelength != -1).
public void testRecordCorrectSegmentCountsWithBackgroundMerges() throws Exception { final String repoName = "test-repo"; createRepository(repoName, "fs"); final String indexName = "test"; // disable merges assertAcked(prepareCreate(indexName).setSettings(indexSettingsNoReplicas(1).put(MergePolicyConfig.INDEX_MERGE_ENABLED, "false"))); // create an empty snapshot so that later snapshots run as quickly as possible createFullSnapshot(repoName, "empty"); // create a situation where we temporarily have a bunch of segments until the merges can catch up long id = 0; final int rounds = scaledRandomIntBetween(50, 300); for (int i = 0; i < rounds; ++i) { final int numDocs = scaledRandomIntBetween(100, 1000); BulkRequestBuilder request = client().prepareBulk(); for (int j = 0; j < numDocs; ++j) { request.add( Requests.indexRequest(indexName) .id(Long.toString(id++)) .source(jsonBuilder().startObject().field("l", randomLong()).endObject()) ); } assertNoFailures(request.get()); refresh(); } // snapshot with a bunch of unmerged segments final SnapshotInfo before = createFullSnapshot(repoName, "snapshot-before"); final SnapshotInfo.IndexSnapshotDetails beforeIndexDetails = before.indexSnapshotDetails().get(indexName); final long beforeSegmentCount = beforeIndexDetails.getMaxSegmentsPerShard(); // reactivate merges assertAcked(admin().indices().prepareClose(indexName).get()); assertAcked( admin().indices() .prepareUpdateSettings(indexName) .setSettings( Settings.builder() .put(MergePolicyConfig.INDEX_MERGE_POLICY_SEGMENTS_PER_TIER_SETTING.getKey(), "2") .put(MergePolicyConfig.INDEX_MERGE_ENABLED, "true") ) ); assertAcked(admin().indices().prepareOpen(indexName).get()); assertEquals(0, admin().indices().prepareForceMerge(indexName).setFlush(true).get().getFailedShards()); // wait for merges to reduce segment count assertBusy(() -> { IndicesStatsResponse stats = client().admin().indices().prepareStats(indexName).setSegments(true).get(); assertThat(stats.getIndex(indexName).getPrimaries().getSegments().getCount(), lessThan(beforeSegmentCount)); }, 30L, TimeUnit.SECONDS); final SnapshotInfo after = createFullSnapshot(repoName, "snapshot-after"); final int incrementalFileCount = clusterAdmin().prepareSnapshotStatus() .setRepository(repoName) .setSnapshots(after.snapshotId().getName()) .get() .getSnapshots() .get(0) .getStats() .getIncrementalFileCount(); assertEquals(0, incrementalFileCount); logger.info("--> no files have changed between snapshots, asserting that segment counts are constant as well"); final SnapshotInfo.IndexSnapshotDetails afterIndexDetails = after.indexSnapshotDetails().get(indexName); assertEquals(beforeSegmentCount, afterIndexDetails.getMaxSegmentsPerShard()); }	i think it'd be better to refresh on each bulk to force a segment to be written: suggestion bulkrequestbuilder request = client().preparebulk().setrefreshpolicy(writerequest.refreshpolicy.immediate); i suspect if we did that we wouldn't need so many rounds.
@Override public void deleteBlobsIgnoringIfNotExists(Iterator<String> blobNames) throws IOException { IOException ioe = null; while (blobNames.hasNext()) { try { IOUtils.rm(path.resolve(blobNames.next())); } catch (IOException e) { // track up to 10 delete exceptions and try to continue deleting on exceptions if (ioe == null) { ioe = e; } else if (ioe.getSuppressed().length < 10) { ioe.addSuppressed(e); } } } if (ioe != null) { throw ioe; } }	++ to carrying on, and also to a bound on the number of exceptions tracked, but i worry that there's a big difference between seeing 11/11 failures and 11/100k. could we report how many other exceptions we dropped after the first 11?
public void testThatTransportClientSettingCannotBeChanged() { Settings baseSettings = Settings.builder() .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) .build(); try (TransportClient client = new MockTransportClient(baseSettings)) { Settings settings = client.injector.getInstance(Settings.class); assertThat(Client.CLIENT_TYPE_SETTING_S.get(settings), is("transport")); } } /** * test that when plugins are provided that want to register * {@link NamedWriteable}, those are also made known to the * {@link NamedWriteableRegistry}	i think it'd be nice if this was in a unit test rather than an esintegtestcase. i think it'd work just fine in one, right? if so i think it is worth adding the extra test class so we can keep them separate. i just know _someone_ is going to come back through these tests one day looking to refactor something and wonder "does this need to be in an esintegtestcase?"
public void testThatTransportClientSettingCannotBeChanged() { Settings baseSettings = Settings.builder() .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) .build(); try (TransportClient client = new MockTransportClient(baseSettings)) { Settings settings = client.injector.getInstance(Settings.class); assertThat(Client.CLIENT_TYPE_SETTING_S.get(settings), is("transport")); } } /** * test that when plugins are provided that want to register * {@link NamedWriteable}, those are also made known to the * {@link NamedWriteableRegistry}	i'm not sure we should be adding more references to the injector. either we should make the namedwriteableregistry a package private member of the client or we should just serialize the thing somehow.
*/ protected List<ShardId> shards(Request request, ClusterState clusterState) { List<ShardId> shardIds = new ArrayList<>(); String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request); for (String index : concreteIndices) { IndexMetaData indexMetaData = clusterState.metaData().getIndices().get(index); if (indexMetaData != null) { for (IntObjectCursor<IndexShardRoutingTable> shardRouting : clusterState.getRoutingTable().indicesRouting().get(index).getShards()) { shardIds.add(shardRouting.value.shardId()); } } } return shardIds; }	i did this kind of thing as for (intobjectcursor<indexshardroutingtable> shardrouting : clusterstate.getroutingtable().indicesrouting().get(index).getshards()) { since i thought it made it more visually obvious that the second line was part of the for and not the code block below, but i could see either way.
private void perform(@Nullable final Exception currentFailure) { Exception lastFailure = this.lastFailure; if (lastFailure == null || TransportActions.isReadOverrideException(currentFailure)) { lastFailure = currentFailure; this.lastFailure = currentFailure; } final ShardRouting shardRouting = shardIt.nextOrNull(); if (shardRouting == null) { Exception failure = lastFailure; if (failure == null || isShardNotAvailableException(failure)) { failure = new NoShardAvailableActionException(null, LoggerMessageFormat.format("No shard available for [{}]", internalRequest.request()), failure); } else { logger.debug(() -> new ParameterizedMessage("{}: failed to execute [{}]", null, internalRequest.request()), failure); } listener.onFailure(failure); return; } DiscoveryNode node = nodes.get(shardRouting.currentNodeId()); if (node == null) { onFailure(shardRouting, new NoShardAvailableActionException(shardRouting.shardId())); } else { internalRequest.request().internalShardId = shardRouting.shardId(); if (logger.isTraceEnabled()) { logger.trace( "sending request [{}] to shard [{}] on node [{}]", internalRequest.request(), internalRequest.request().internalShardId, node ); } transportService.sendRequest(node, transportShardAction, internalRequest.request(), new TransportResponseHandler<Response>() { @Override public Response newInstance() { return newResponse(); } @Override public String executor() { return ThreadPool.Names.SAME; } @Override public void handleResponse(final Response response) { listener.onResponse(response); } @Override public void handleException(TransportException exp) { onFailure(shardRouting, exp); } }); } }	i think moving the whole new new transportresponsehandler<response>() { onto the next line is a little clearer.
public Map<String, Long> getPendingRequests(final String actionNamePrefix) { Map<String, Long> nodeCounts = new HashMap<>(); for (Map.Entry<Long, RequestHolder> entry : clientHandlers.entrySet()) { RequestHolder reqHolder = entry.getValue(); if (reqHolder.action().startsWith(actionNamePrefix)) { String nodeId = reqHolder.connection().getNode().getId(); nodeCounts.put(nodeId, nodeCounts.getOrDefault(nodeId, 0L) + 1); } } return nodeCounts; }	seems like you could iterate over values directly
public void testSetMaxFileSize() throws IOException { if (Constants.LINUX) { final List<String> lines = Files.readAllLines(PathUtils.get("/proc/self/limits")); if (!lines.isEmpty()) { for (final String line : lines) { if (line != null && line.startsWith("Max file size")) { final String[] fields = line.split("\\\\\\\\s+"); final String limit = fields[3]; assertThat( JNANatives.rlimitToString(JNANatives.MAX_FILE_SIZE), equalTo(limit)); return; } } } fail("should have read max file from /proc/self/limits"); } else if (Constants.MAC_OS_X) { assertThat( JNANatives.MAX_FILE_SIZE, anyOf(equalTo(Long.MIN_VALUE), greaterThanOrEqualTo(0L))); } else { assertThat(JNANatives.MAX_FILE_SIZE, equalTo(Long.MIN_VALUE)); } }	this if statement looks useless?
private static String buildMessage(Response response) throws IOException { String message = response.getRequestLine().getMethod() + " " + response.getHost() + response.getRequestLine().getUri() + ": " + response.getStatusLine().toString(); HttpEntity entity = response.getEntity(); if (entity != null) { if (entity.isRepeatable() == false) { entity = new BufferedHttpEntity(entity); response.getHttpResponse().setEntity(entity); } message += "\\\\n" + EntityUtils.toString(entity); } return message; } /** * Returns the {@link Response}	why make response non final with a package private constructor and leave responseexception final but add a public constructor to it? should we do the same with both?
protected void doExecute(Task task, SamlInitiateSingleSignOnRequest request, ActionListener<SamlInitiateSingleSignOnResponse> listener) { identityProvider.getRegisteredServiceProvider(request.getSpEntityId(), false, ActionListener.wrap( sp -> { try { if (null == sp) { final String message = "Service Provider with Entity ID [" + request.getSpEntityId() + "] is not registered with this Identity Provider"; logger.debug(message); listener.onFailure(new IllegalArgumentException(message)); return; } final SecondaryAuthentication secondaryAuthentication = SecondaryAuthentication.readFromContext(securityContext); if (secondaryAuthentication == null) { listener.onFailure( new ElasticsearchStatusException("Request is missing secondary authentication", RestStatus.FORBIDDEN)); return; } buildUserFromAuthentication(secondaryAuthentication, sp, ActionListener.wrap( user -> { if (user == null) { // TODO return SAML failure instead? listener.onFailure(new ElasticsearchSecurityException("User [{}] is not permitted to access service [{}]", secondaryAuthentication.getUser(), sp)); return; } final SuccessfulAuthenticationResponseMessageBuilder builder = new SuccessfulAuthenticationResponseMessageBuilder(samlFactory, Clock.systemUTC(), identityProvider); final Response response = builder.build(user, null); listener.onResponse(new SamlInitiateSingleSignOnResponse( user.getServiceProvider().getAssertionConsumerService().toString(), samlFactory.getXmlContent(response), user.getServiceProvider().getEntityId())); }, listener::onFailure )); } catch (IOException e) { listener.onFailure(new IllegalArgumentException(e.getMessage())); } }, listener::onFailure )); }	can we make it elasticsearchsecurityexception instead? suggestion new elasticsearchsecurityexception("request is missing secondary authentication", reststatus.forbidden));
public void testTokenServiceCanRotateKeys() throws Exception { final RestHighLevelClient restClient = new TestRestHighLevelClient(); CreateTokenResponse response = restClient.security().createToken(CreateTokenRequest.passwordGrant( SecuritySettingsSource.TEST_USER_NAME, SecuritySettingsSourceField.TEST_PASSWORD.toCharArray()), SECURITY_REQUEST_OPTIONS); String masterName = internalCluster().getMasterName(); TokenService masterTokenService = internalCluster().getInstance(TokenService.class, masterName); String activeKeyHash = masterTokenService.getActiveKeyHash(); for (TokenService tokenService : internalCluster().getInstances(TokenService.class)) { PlainActionFuture<UserToken> userTokenFuture = new PlainActionFuture<>(); tokenService.decodeToken(response.getAccessToken(), userTokenFuture); assertNotNull(userTokenFuture.actionGet()); assertEquals(activeKeyHash, tokenService.getActiveKeyHash()); } client().admin().cluster().prepareHealth().execute().get(); PlainActionFuture<ClusterStateUpdateResponse> rotateActionFuture = new PlainActionFuture<>(); logger.info("rotate on master: {}", masterName); masterTokenService.rotateKeysOnMaster(rotateActionFuture); assertTrue(rotateActionFuture.actionGet().isAcknowledged()); assertNotEquals(activeKeyHash, masterTokenService.getActiveKeyHash()); for (TokenService tokenService : internalCluster().getInstances(TokenService.class)) { PlainActionFuture<UserToken> userTokenFuture = new PlainActionFuture<>(); tokenService.decodeToken(response.getAccessToken(), userTokenFuture); assertNotNull(userTokenFuture.actionGet()); assertNotEquals(activeKeyHash, tokenService.getActiveKeyHash()); } assertNotNull(response.getAuthentication()); assertEquals(response.getAuthentication().getUser().getUsername(), SecuritySettingsSource.TEST_USER_NAME); }	the parameter order of assertequals should be first "expected", then "actual", i.e.: java assertequals(securitysettingssource.test_user_name, response.getauthentication().getuser().getusername());
@Override public void sendSearchResponse(InternalSearchResponse internalSearchResponse, String scrollId) { ShardSearchFailure[] failures = buildShardFailures(); Boolean allowPartialResults = request.allowPartialSearchResults(); assert allowPartialResults != null : "SearchRequest missing setting for allowPartialSearchResults"; if (allowPartialResults == false && failures.length > 0){ raisePhaseFailure(new SearchPhaseExecutionException("", "Shard failures", null, failures)); } else { listener.onResponse(buildSearchResponse(internalSearchResponse, scrollId, failures)); } }	line 330 looks to have a condition where listener.onfailure is invoked and shard failures.length ==0 (due to unavailable replicas for a shard group). it looks like that would be another scenario where the "success or fail only" listener guarantee would break?
* @param mapperService the shards mapper service * @param snapshotId snapshot id * @param indexId id for the index being snapshotted * @param snapshotIndexCommit commit point * @param snapshotStatus snapshot status */ void snapshotShard(Store store, MapperService mapperService, SnapshotId snapshotId, IndexId indexId, IndexCommit snapshotIndexCommit, IndexShardSnapshotStatus snapshotStatus); /** * Restores snapshot of the shard. * <p> * The index can be renamed on restore, hence different {@code shardId} and {@code snapshotShardId} are supplied. * @param shard the shard to restore the index into * @param store the store to restore the index into * @param snapshotId snapshot id * @param version version of elasticsearch that created this snapshot * @param indexId id of the index in the repository from which the restore is occurring * @param snapshotShardId shard id (in the snapshot) * @param recoveryState recovery state * @deprecated use {@link #restoreShard(Store, SnapshotId, Version, IndexId, ShardId, RecoveryState)}	i wonder if we even need this deprecation (instead of just outright changing the interface)? we will need to make changes to this interface anyway to fix https://github.com/elastic/elasticsearch/issues/41581 so maybe it's not worth being careful here when we have to break 3rd party implementations (not sure if there even are any) to fix functionality anyway?
public static void createStateIndexAndAliasIfNecessary(Client client, ClusterState state, final ActionListener<Boolean> finalListener) { if (state.getMetaData().getAliasAndIndexLookup().containsKey(jobStateIndexWriteAlias())) { finalListener.onResponse(false); return; } final ActionListener<Void> createAliasListener = ActionListener.wrap( r -> { final IndicesAliasesRequest request = client.admin() .indices() .prepareAliases() .addAlias(AnomalyDetectorsIndexFields.STATE_INDEX_PREFIX, jobStateIndexWriteAlias()) .request(); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request, ActionListener.<AcknowledgedResponse>wrap( resp -> finalListener.onResponse(resp.isAcknowledged()), finalListener::onFailure), client.admin().indices()::aliases); }, finalListener::onFailure ); // Only create the index or aliases if some other ML index exists - saves clutter if ML is never used. SortedMap<String, AliasOrIndex> mlLookup = state.getMetaData().getAliasAndIndexLookup().tailMap(".ml"); if (mlLookup.isEmpty() == false && mlLookup.firstKey().startsWith(".ml")) { if (mlLookup.containsKey(AnomalyDetectorsIndexFields.STATE_INDEX_PREFIX)) { createAliasListener.onResponse(null); } else { CreateIndexRequest createIndexRequest = client.admin() .indices() .prepareCreate(AnomalyDetectorsIndexFields.STATE_INDEX_PREFIX) .addAlias(new Alias(jobStateIndexWriteAlias())) .request(); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, createIndexRequest, ActionListener.<CreateIndexResponse>wrap( createIndexResponse -> finalListener.onResponse(true), createIndexFailure -> { // If it was created between our last check, and this request being handled, we should add the alias // Adding an alias that already exists is idempotent, so, no need to double check if the alias exists // as well. if (createIndexFailure instanceof ResourceAlreadyExistsException) { createAliasListener.onResponse(null); } else { finalListener.onFailure(createIndexFailure); } }), client.admin().indices()::create); } } }	we probably shouldn't do the clutter avoidance in this case. if this method is called we know we're intending to write to an ml index shortly even if we never have up to this time. the effect of not creating the alias is pretty bad - it results in creation of a concrete index called .ml-state-write, and that is hard to switch over to an alias of the same name.
public void snapshotMlMeta(MlMetadata mlMetadata, ActionListener<Boolean> listener) { if (tookConfigSnapshot.get()) { listener.onResponse(true); return; } if (mlMetadata.getJobs().isEmpty() && mlMetadata.getDatafeeds().isEmpty()) { listener.onResponse(true); return; } logger.debug("taking a snapshot of ml_metadata"); String documentId = "ml-config"; IndexRequestBuilder indexRequest = client.prepareIndex(AnomalyDetectorsIndex.jobStateIndexWriteAlias(), ElasticsearchMappings.DOC_TYPE, documentId) .setOpType(DocWriteRequest.OpType.CREATE); ToXContent.MapParams params = new ToXContent.MapParams(Collections.singletonMap(ToXContentParams.FOR_INTERNAL_STORAGE, "true")); try (XContentBuilder builder = XContentFactory.jsonBuilder()) { builder.startObject(); mlMetadata.toXContent(builder, params); builder.endObject(); indexRequest.setSource(builder); } catch (IOException e) { logger.error("failed to serialise ml_metadata", e); listener.onFailure(e); return; } executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, indexRequest.request(), ActionListener.<IndexResponse>wrap( indexResponse -> { listener.onResponse(indexResponse.getResult() == DocWriteResponse.Result.CREATED); }, listener::onFailure), client::index ); }	there's no guarantee that an autodetect process will have been started on the newer version of the product at the point when this call is made - if all ml jobs are closed prior to upgrading from 6.5 to 6.7 then that will definitely trigger this situation. so this method needs to call anomalydetectorsindex.createstateindexandaliasifnecessary() first.
public void persistQuantiles(Quantiles quantiles, WriteRequest.RefreshPolicy refreshPolicy, ActionListener<IndexResponse> listener) { Persistable persistable = new Persistable(quantiles.getJobId(), quantiles, Quantiles.documentId(quantiles.getJobId())); persistable.setRefreshPolicy(refreshPolicy); persistable.persist(AnomalyDetectorsIndex.jobStateIndexWriteAlias(), listener); }	this method can be called when reverting a model snapshot, and there's no guarantee that an autodetect process will have been started on the newer version of the product at the point when a model snapshot is reverted. the call chain is transportrevertmodelsnapshotaction.masteroperation() -> jobmanager.revertsnapshot() -> this method. so one of those two calls needs to call anomalydetectorsindex.createstateindexandaliasifnecessary() first.
@Test(expected = GradleException.class) public void testRepositoryURIThatUsesHttpScheme() throws URISyntaxException { final URI uri = new URI("http://s3.amazonaws.com/artifacts.elastic.co/maven"); BuildPlugin.assertRepositoryURIIsSecure("test", "test", uri); }	can we add a test for file: as well?
private String error(String query) { VerificationException e = expectThrows(VerificationException.class, () -> plan(query)); assertTrue(e.getMessage().startsWith("Found ")); final String header = "Found 1 problem\\\\nline "; return e.getMessage().substring(header.length()); }	i think this line fits inside the previous one.
private StreamInput decompressingStream(byte status, Version remoteVersion, StreamInput streamInput) throws IOException { if (TransportStatus.isCompress(status) && streamInput.available() > 0) { try { StreamInput decompressor = CompressorFactory.COMPRESSOR.streamInput(streamInput); decompressor.setVersion(remoteVersion); return decompressor; } catch (IllegalArgumentException e) { throw new IllegalStateException("stream marked as compressed, but is missing deflate header"); } } else { return streamInput; } }	this consumes some bytes from the input stream? do we need to mark/reset the stream?
BytesReference serialize(BytesStreamOutput bytesStream) throws IOException { storedContext.restore(); bytesStream.setVersion(version); bytesStream.skip(TcpHeader.headerSize(version)); // The compressible bytes stream will not close the underlying bytes stream BytesReference reference; int variableHeaderLength = -1; final long preHeaderPosition = bytesStream.position(); boolean needToWriteHeader = true; // TODO: Change to 7.6 after backport if (version.onOrAfter(Version.V_8_0_0)) { writeVariableHeader(bytesStream); variableHeaderLength = Math.toIntExact(bytesStream.position() - preHeaderPosition); needToWriteHeader = false; } try (CompressibleBytesOutputStream stream = new CompressibleBytesOutputStream(bytesStream, TransportStatus.isCompress(status))) { stream.setVersion(version); if (needToWriteHeader) { writeVariableHeader(stream); } reference = writeMessage(stream); } bytesStream.seek(0); final int contentSize = reference.length() - TcpHeader.headerSize(version); TcpHeader.writeHeader(bytesStream, requestId, status, version, contentSize, variableHeaderLength); return reference; }	should we just drop this boolean and instead check if (variableheaderlength != -1)?
private static String format(TcpChannel channel, BytesReference message, String event) throws IOException { final StringBuilder sb = new StringBuilder(); sb.append(channel); int messageLengthWithHeader = HEADER_SIZE + message.length(); // This is a ping if (message.length() == 0) { sb.append(" [ping]").append(' ').append(event).append(": ").append(messageLengthWithHeader).append('B'); } else { boolean success = false; StreamInput streamInput = message.streamInput(); try { final long requestId = streamInput.readLong(); final byte status = streamInput.readByte(); final boolean isRequest = TransportStatus.isRequest(status); final String type = isRequest ? "request" : "response"; Version version = Version.fromId(streamInput.readInt()); final String versionString = version.toString(); sb.append(" [length: ").append(messageLengthWithHeader); sb.append(", request id: ").append(requestId); sb.append(", type: ").append(type); sb.append(", version: ").append(versionString); // TODO: Change to 7.6 after backport if (streamInput.getVersion().onOrAfter(Version.V_8_0_0)) { sb.append(", header size: ").append(streamInput.readInt()).append('B'); } if (isRequest) { if (TransportStatus.isCompress(status)) { Compressor compressor; compressor = InboundMessage.getCompressor(streamInput); if (compressor == null) { throw new IllegalStateException(new NotCompressedException()); } streamInput = compressor.streamInput(streamInput); } // read and discard headers ThreadContext.readHeadersFromStream(streamInput); if (streamInput.getVersion().before(Version.V_8_0_0)) { // discard the features streamInput.readStringArray(); } sb.append(", action: ").append(streamInput.readString()); } sb.append(']'); sb.append(' ').append(event).append(": ").append(messageLengthWithHeader).append('B'); success = true; } finally { if (success) { IOUtils.close(streamInput); } else { IOUtils.closeWhileHandlingException(streamInput); } } } return sb.toString(); }	just drop this variable and use sb.append(", version: ").append(version) further down below
public void testSortsWWWAuthenticateHeaderValues() { final String basicAuthScheme = "Basic realm=\\\\"" + XPackField.SECURITY + "\\\\" charset=\\\\"UTF-8\\\\""; final String bearerAuthScheme = "Bearer realm=\\\\"" + XPackField.SECURITY + "\\\\""; final String negotiateAuthScheme = randomFrom("Negotiate", "Negotiate Ijoijksdk"); final Map<String, List<String>> failureResponeHeaders = new HashMap<>(); failureResponeHeaders.put("WWW-Authenticate", Arrays.asList(basicAuthScheme, bearerAuthScheme, negotiateAuthScheme)); final DefaultAuthenticationFailureHandler failuerHandler = new DefaultAuthenticationFailureHandler(failureResponeHeaders); final ElasticsearchSecurityException ese = failuerHandler.exceptionProcessingRequest(Mockito.mock(RestRequest.class), new Exception("other error"), new ThreadContext(Settings.builder().build())); assertThat(ese, is(notNullValue())); assertThat(ese.getHeader("WWW-Authenticate"), is(notNullValue())); assertThat(ese.getMessage(), equalTo("error attempting to authenticate request")); assertWWWAuthenticateWithSchemes(ese, basicAuthScheme, bearerAuthScheme, negotiateAuthScheme); }	can you make this explicit? ie there could be a bug in authschemepriority so we should validate negotiate first, then bearer, then basic
private InetSocketAddress bindToPort(final String name, final InetAddress hostAddress, String port) { PortsRange portsRange = new PortsRange(port); final AtomicReference<Exception> lastException = new AtomicReference<>(); final AtomicReference<InetSocketAddress> boundSocket = new AtomicReference<>(); closeLock.readLock().lock(); try { if (lifecycle.initialized() == false && lifecycle.started() == false) { throw new IllegalStateException("transport has been stopped"); } boolean success = portsRange.iterate(portNumber -> { try { TcpServerChannel channel = bind(name, new InetSocketAddress(hostAddress, portNumber)); synchronized (serverChannels) { List<TcpServerChannel> list = serverChannels.get(name); if (list == null) { list = new ArrayList<>(); serverChannels.put(name, list); } list.add(channel); boundSocket.set(channel.getLocalAddress()); } } catch (Exception e) { lastException.set(e); return false; } return true; }); if (!success) { throw new BindTransportException("Failed to bind to [" + port + "]", lastException.get()); } } finally { closeLock.readLock().unlock(); } if (logger.isDebugEnabled()) { logger.debug("Bound profile [{}] to address {{}}", name, NetworkAddress.format(boundSocket.get())); } return boundSocket.get(); }	@henningandersen @davecturner shouldn't we acquire the writelock here too to prevent this from running concurrently with the dostop (this method is only called from dostart anyway, so locking here shouldn't hurt anything since we're not concurrently)? otherwise, there isn't really anything that prevents a put to the serverchannels to run here if the shutdown happened after the above check for the lifecycle state passed is there? (which would lead us to still add to serverchannels after we're closed already)
void processResult(AutodetectResult result) { if (processKilled) { return; } Bucket bucket = result.getBucket(); if (bucket != null) { if (deleteInterimRequired) { // Delete any existing interim results generated by a Flush command // which have not been replaced or superseded by new results. LOGGER.trace("[{}] Deleting interim results", jobId); persister.deleteInterimResults(jobId); deleteInterimRequired = false; } // persist after deleting interim results in case the new // results are also interim timingStatsReporter.reportBucket(bucket); bulkResultsPersister.persistBucket(bucket).executeRequest(); ++currentRunBucketCount; } List<AnomalyRecord> records = result.getRecords(); if (records != null && !records.isEmpty()) { bulkResultsPersister.persistRecords(records); } List<Influencer> influencers = result.getInfluencers(); if (influencers != null && !influencers.isEmpty()) { bulkResultsPersister.persistInfluencers(influencers); } CategoryDefinition categoryDefinition = result.getCategoryDefinition(); if (categoryDefinition != null) { persister.persistCategoryDefinition(categoryDefinition, this::isAlive); } ModelPlot modelPlot = result.getModelPlot(); if (modelPlot != null) { bulkResultsPersister.persistModelPlot(modelPlot); } Annotation annotation = result.getAnnotation(); if (annotation != null) { annotationPersister.persistAnnotation(null, annotation); } Forecast forecast = result.getForecast(); if (forecast != null) { bulkResultsPersister.persistForecast(forecast); } ForecastRequestStats forecastRequestStats = result.getForecastRequestStats(); if (forecastRequestStats != null) { LOGGER.trace("Received Forecast Stats [{}]", forecastRequestStats.getId()); bulkResultsPersister.persistForecastRequestStats(forecastRequestStats); // execute the bulk request only in some cases or in doubt // otherwise rely on the count-based trigger switch (forecastRequestStats.getStatus()) { case OK: case STARTED: break; case FAILED: case SCHEDULED: case FINISHED: default: bulkResultsPersister.executeRequest(); } } ModelSizeStats modelSizeStats = result.getModelSizeStats(); if (modelSizeStats != null) { processModelSizeStats(modelSizeStats); } ModelSnapshot modelSnapshot = result.getModelSnapshot(); if (modelSnapshot != null) { // We need to refresh in order for the snapshot to be available when we try to update the job with it BulkResponse bulkResponse = persister.persistModelSnapshot(modelSnapshot, WriteRequest.RefreshPolicy.IMMEDIATE, this::isAlive); assert bulkResponse.getItems().length == 1; IndexResponse indexResponse = bulkResponse.getItems()[0].getResponse(); if (indexResponse.getResult() == DocWriteResponse.Result.CREATED) { updateModelSnapshotOnJob(modelSnapshot); } annotationPersister.persistAnnotation( ModelSnapshot.annotationDocumentId(modelSnapshot), createModelSnapshotAnnotation(modelSnapshot)); } Quantiles quantiles = result.getQuantiles(); if (quantiles != null) { LOGGER.debug("[{}] Parsed Quantiles with timestamp {}", jobId, quantiles.getTimestamp()); persister.persistQuantiles(quantiles, this::isAlive); bulkResultsPersister.executeRequest(); if (processKilled == false && renormalizer.isEnabled()) { // We need to make all results written up to these quantiles available for renormalization persister.commitResultWrites(jobId); LOGGER.debug("[{}] Quantiles queued for renormalization", jobId); renormalizer.renormalize(quantiles); } } FlushAcknowledgement flushAcknowledgement = result.getFlushAcknowledgement(); if (flushAcknowledgement != null) { LOGGER.debug("[{}] Flush acknowledgement parsed from output for ID {}", jobId, flushAcknowledgement.getId()); // Commit previous writes here, effectively continuing // the flush from the C++ autodetect process right // through to the data store Exception exception = null; try { bulkResultsPersister.executeRequest(); persister.commitResultWrites(jobId); LOGGER.debug("[{}] Flush acknowledgement sent to listener for ID {}", jobId, flushAcknowledgement.getId()); } catch (Exception e) { LOGGER.error( "[" + jobId + "] failed to bulk persist results and commit writes during flush acknowledgement for ID " + flushAcknowledgement.getId(), e); exception = e; throw e; } finally { flushListener.acknowledgeFlush(flushAcknowledgement, exception); } // Interim results may have been produced by the flush, // which need to be // deleted when the next finalized results come through deleteInterimRequired = true; } }	there is no batching here. i worry that this could be really slow if there is a job with 100000 split field values and we detect periodicity in all 100000 time series at the same time. to avoid the risk it needs to create bulk requests for these model change annotations, then execute these bulk requests when the next bucket result is seen, when the job is flushed, or when the results stream ends (basically wherever we currently call bulkresultspersister.executerequest()).
public void testInitialStatusIndexingOldID() { RollupJob job = new RollupJob(ConfigTestHelpers.randomRollupJobConfig(random()), Collections.emptyMap()); RollupJobStatus status = new RollupJobStatus(IndexerState.INDEXING, Collections.singletonMap("foo", "bar")); Client client = mock(Client.class); when(client.settings()).thenReturn(Settings.EMPTY); SchedulerEngine schedulerEngine = new SchedulerEngine(SETTINGS, Clock.systemUTC()); TaskId taskId = new TaskId("node", 123); RollupJobTask task = new RollupJobTask(1, "type", "action", taskId, job, status, client, schedulerEngine, pool, Collections.emptyMap()); task.init(null, mock(TaskManager.class), taskId.toString(),123); assertThat(((RollupJobStatus)task.getStatus()).getIndexerState(), equalTo(IndexerState.STARTED)); assertThat(((RollupJobStatus)task.getStatus()).getPosition().size(), equalTo(1)); assertTrue(((RollupJobStatus)task.getStatus()).getPosition().containsKey("foo")); }	i think we lost some spaces before 123 here and in a few other places below.
public void onModule(SettingsModule module) { module.registerSettingsFilterIfMissing(AwsS3Service.CLOUD_AWS.KEY); module.registerSettingsFilterIfMissing(AwsS3Service.CLOUD_AWS.SECRET); module.registerSettingsFilterIfMissing(AwsS3Service.CLOUD_AWS.PROXY_PASSWORD); module.registerSettingsFilterIfMissing(AwsS3Service.CLOUD_S3.KEY); module.registerSettingsFilterIfMissing(AwsS3Service.CLOUD_S3.SECRET); module.registerSettingsFilterIfMissing(AwsS3Service.CLOUD_S3.PROXY_PASSWORD); module.registerSettingsFilter("access_key"); // WTF is this? module.registerSettingsFilter("secret_key"); // WTF is this? }	i think it because you can define this per repository (so no prefix). unsure though. got the same q today while rewriting settings for the new infra.
@Override public IndexSettings getIndexSettings() { return indexSettings; } /** * Creates a new QueryShardContext. The context has not types set yet, if types are required set them via * {@link QueryShardContext#setTypes(String...)}. * * Passing a {@code null} {@link IndexReader} will return a valid context, however it won't be able to make * {@link IndexReader}	indentation was correct before
* @param globalCheckpoint the global checkpoint */ public void updateGlobalCheckpointForShard(final String allocationId, final long globalCheckpoint) { globalCheckpointTracker.updateGlobalCheckpointForShard(allocationId, globalCheckpoint); } /** * Called when the recovery process for a shard is ready to open the engine on the target shard. * See {@link GlobalCheckpointTracker#initiateTracking(String)}	should this also be called from recoverysourcehandler after calling recoverytarget.finalizerecovery(shard.getglobalcheckpoint())?
public void maybeSyncGlobalCheckpoint() { verifyPrimary(); verifyNotClosed(); final String allocationId = routingEntry().allocationId().getId(); final ObjectLongMap<String> globalCheckpoints = getEngine().seqNoService().getGlobalCheckpoints(allocationId); assert globalCheckpoints.containsKey(allocationId); final long globalCheckpoint = globalCheckpoints.get(allocationId); final boolean syncNeeded = StreamSupport.stream(globalCheckpoints.values().spliterator(), false).anyMatch(v -> v.value < globalCheckpoint); if (syncNeeded) { globalCheckpointSyncer.run(); } }	this check is very coarse. with the info available in globalcheckpointtracker, we could actually only care for global checkpoints of in-sync copies. in case of a recovering shard, we don't care about running the global checkpoint sync again and again.
@Override public final Query rangeQuery( Object lowerTerm, Object upperTerm, boolean includeLower, boolean includeUpper, ShapeRelation relation, ZoneId timeZone, DateMathParser parser, QueryShardContext context ) { if (relation == ShapeRelation.DISJOINT) { String message = "Field [%s] of type [%s] with runtime type [%s] does not support DISJOINT ranges"; throw new IllegalArgumentException(String.format(Locale.ROOT, message, name(), typeName(), runtimeType())); } return rangeQuery(lowerTerm, upperTerm, includeLower, includeUpper, timeZone, parser, context); }	i went with this rather funny way of writing the message because it made the formatter happy. most of the other options i tried caused the formatter to split the message over so many lines it wasn't readable. i don't generally like string.format for these but here it looks best, i think.
@BeforeClass public static void registerClients() throws Exception { try (CloseableHttpClient httpClient = HttpClients.createDefault()) { String codeClient = "{" + "\\\\"grant_types\\\\": [\\\\"authorization_code\\\\"]," + "\\\\"response_types\\\\": [\\\\"code\\\\"]," + "\\\\"preferred_client_id\\\\":\\\\"https://my.elasticsearch.org/rp\\\\"," + "\\\\"preferred_client_secret\\\\":\\\\"b07efb7a1cf6ec9462afe7b6d3ab55c6c7880262aa61ac28dded292aca47c9a2\\\\"," + "\\\\"redirect_uris\\\\": [\\\\"https://my.fantastic.rp/cb\\\\"]," + "\\\\"token_endpoint_auth_method\\\\":\\\\"client_secret_basic\\\\"" + "}"; String implicitClient = "{" + "\\\\"grant_types\\\\": [\\\\"implicit\\\\"]," + "\\\\"response_types\\\\": [\\\\"token id_token\\\\"]," + "\\\\"preferred_client_id\\\\":\\\\"elasticsearch-rp\\\\"," + "\\\\"preferred_client_secret\\\\":\\\\"b07efb7a1cf6ec9462afe7b6d3ab55c6c7880262aa61ac28dded292aca47c9a2\\\\"," + "\\\\"redirect_uris\\\\": [\\\\"https://my.fantastic.rp/cb\\\\"]" + "}"; String postClient = "{" + "\\\\"grant_types\\\\": [\\\\"authorization_code\\\\"]," + "\\\\"response_types\\\\": [\\\\"code\\\\"]," + "\\\\"preferred_client_id\\\\":\\\\"elasticsearch-post\\\\"," + "\\\\"preferred_client_secret\\\\":\\\\"b07efb7a1cf6ec9462afe7b6d3ab55c6c7880262aa61ac28dded292aca47c9a2\\\\"," + "\\\\"redirect_uris\\\\": [\\\\"https://my.fantastic.rp/cb\\\\"]," + "\\\\"token_endpoint_auth_method\\\\":\\\\"client_secret_post\\\\"" + "}"; String jwtClient = "{" + "\\\\"grant_types\\\\": [\\\\"authorization_code\\\\"]," + "\\\\"response_types\\\\": [\\\\"code\\\\"]," + "\\\\"preferred_client_id\\\\":\\\\"elasticsearch-post-jwt\\\\"," + "\\\\"preferred_client_secret\\\\":\\\\"b07efb7a1cf6ec9462afe7b6d3ab55c6c7880262aa61ac28dded292aca47c9a2\\\\"," + "\\\\"redirect_uris\\\\": [\\\\"https://my.fantastic.rp/cb\\\\"]," + "\\\\"token_endpoint_auth_method\\\\":\\\\"client_secret_jwt\\\\"" + "}"; HttpPost httpPost = new HttpPost(REGISTRATION_URL); final BasicHttpContext context = new BasicHttpContext(); httpPost.setEntity(new StringEntity(codeClient, ContentType.APPLICATION_JSON)); httpPost.setHeader("Accept", "application/json"); httpPost.setHeader("Content-type", "application/json"); httpPost.setHeader("Authorization", "Bearer 811fa888f3e0fdc9e01d4201bfeee46a"); HttpPost httpPost2 = new HttpPost(REGISTRATION_URL); httpPost2.setEntity(new StringEntity(implicitClient, ContentType.APPLICATION_JSON)); httpPost2.setHeader("Accept", "application/json"); httpPost2.setHeader("Content-type", "application/json"); httpPost2.setHeader("Authorization", "Bearer 811fa888f3e0fdc9e01d4201bfeee46a"); HttpPost httpPost3 = new HttpPost(REGISTRATION_URL); httpPost3.setEntity(new StringEntity(postClient, ContentType.APPLICATION_JSON)); httpPost3.setHeader("Accept", "application/json"); httpPost3.setHeader("Content-type", "application/json"); httpPost3.setHeader("Authorization", "Bearer 811fa888f3e0fdc9e01d4201bfeee46a"); HttpPost httpPost4 = new HttpPost(REGISTRATION_URL); httpPost4.setEntity(new StringEntity(jwtClient, ContentType.APPLICATION_JSON)); httpPost4.setHeader("Accept", "application/json"); httpPost4.setHeader("Content-type", "application/json"); httpPost4.setHeader("Authorization", "Bearer 811fa888f3e0fdc9e01d4201bfeee46a"); SocketAccess.doPrivileged(() -> { try (CloseableHttpResponse response = httpClient.execute(httpPost, context)) { assertThat(response.getStatusLine().getStatusCode(), equalTo(201)); } try (CloseableHttpResponse response2 = httpClient.execute(httpPost2, context)) { assertThat(response2.getStatusLine().getStatusCode(), equalTo(201)); } try (CloseableHttpResponse response3 = httpClient.execute(httpPost3, context)) { assertThat(response3.getStatusLine().getStatusCode(), equalTo(201)); } try (CloseableHttpResponse response4 = httpClient.execute(httpPost4, context)) { assertThat(response4.getStatusLine().getStatusCode(), equalTo(201)); } }); } }	not sure where this bearer token is picked from. i guess it has something to do with the docker image?
@Override public void readFrom(StreamInput in) throws IOException { index = in.readString(); type = in.readOptionalString(); id = in.readString(); seqNo = in.readZLong(); primaryTerm = in.readVLong(); version = in.readLong(); exists = in.readBoolean(); if (exists) { source = in.readBytesReference(); if (source.length() == 0) { source = null; } if (in.getVersion().onOrAfter(Version.V_8_0_0)) { documentFields = readFields(in); metaFields = readFields(in); } else { Map<String, DocumentField> fields = readFields(in); documentFields = new HashMap<>(); metaFields = new HashMap<>(); splitFieldsByMetadata(fields, documentFields, metaFields); } } }	i think this should be 7.3.0, since we intend to backport to 7.x?
private GetResult innerGetLoadFromStoredFields(String type, String id, String[] gFields, FetchSourceContext fetchSourceContext, Engine.GetResult get, MapperService mapperService) { Map<String, DocumentField> nonMetaDataFields = null; Map<String, DocumentField> metaDataFields = null; BytesReference source = null; DocIdAndVersion docIdAndVersion = get.docIdAndVersion(); FieldsVisitor fieldVisitor = buildFieldsVisitors(gFields, fetchSourceContext); if (fieldVisitor != null) { try { docIdAndVersion.reader.document(docIdAndVersion.docId, fieldVisitor); } catch (IOException e) { throw new ElasticsearchException("Failed to get type [" + type + "] and id [" + id + "]", e); } source = fieldVisitor.source(); if (!fieldVisitor.fields().isEmpty()) { fieldVisitor.postProcess(mapperService); nonMetaDataFields = new HashMap<>(); metaDataFields = new HashMap<>(); for (Map.Entry<String, List<Object>> entry : fieldVisitor.fields().entrySet()) { if (MapperService.isMetadataField(entry.getKey())) { metaDataFields.put(entry.getKey(), new DocumentField(entry.getKey(), entry.getValue())); } else { nonMetaDataFields.put(entry.getKey(), new DocumentField(entry.getKey(), entry.getValue())); } } } } DocumentMapper docMapper = mapperService.documentMapper(); if (gFields != null && gFields.length > 0) { for (String field : gFields) { Mapper fieldMapper = docMapper.mappers().getMapper(field); if (fieldMapper == null) { if (docMapper.objectMappers().get(field) != null) { // Only fail if we know it is a object field, missing paths / fields shouldn't fail. throw new IllegalArgumentException("field [" + field + "] isn't a leaf field"); } } } } if (!fetchSourceContext.fetchSource()) { source = null; } else if (fetchSourceContext.includes().length > 0 || fetchSourceContext.excludes().length > 0) { Map<String, Object> sourceAsMap; XContentType sourceContentType = null; // TODO: The source might parsed and available in the sourceLookup but that one uses unordered maps so different. Do we care? Tuple<XContentType, Map<String, Object>> typeMapTuple = XContentHelper.convertToMap(source, true); sourceContentType = typeMapTuple.v1(); sourceAsMap = typeMapTuple.v2(); sourceAsMap = XContentMapValues.filter(sourceAsMap, fetchSourceContext.includes(), fetchSourceContext.excludes()); try { source = BytesReference.bytes(XContentFactory.contentBuilder(sourceContentType).map(sourceAsMap)); } catch (IOException e) { throw new ElasticsearchException("Failed to get type [" + type + "] and id [" + id + "] with includes/excludes set", e); } } return new GetResult(shardId.getIndexName(), type, id, get.docIdAndVersion().seqNo, get.docIdAndVersion().primaryTerm, get.version(), get.exists(), source, nonMetaDataFields, metaDataFields); }	we should use documentfields for consistency
public void testDeprecationRouteThrottling() throws Exception { final Request deprecatedRequest = deprecatedRequest("GET", "xOpaqueId-testDeprecationRouteThrottling"); assertOK(client().performRequest(deprecatedRequest)); assertOK(client().performRequest(deprecatedRequest)); final Request postRequest = deprecatedRequest("POST", "xOpaqueId-testDeprecationRouteThrottling"); assertOK(client().performRequest(postRequest)); assertBusy(() -> { List<Map<String, Object>> documents = getIndexedDeprecations(); logger.warn(documents); assertThat(documents, hasSize(3)); assertThat( documents, containsInAnyOrder( allOf( hasEntry(KEY_FIELD_NAME, "deprecated_route_POST_/_test_cluster/deprecated_settings"), hasEntry("message", "[/_test_cluster/deprecated_settings] exists for deprecated tests") ), allOf( hasEntry(KEY_FIELD_NAME, "deprecated_route_GET_/_test_cluster/deprecated_settings"), hasEntry("message", "[/_test_cluster/deprecated_settings] exists for deprecated tests") ), allOf( hasEntry(KEY_FIELD_NAME, "deprecated_settings"), hasEntry("message", "[deprecated_settings] usage is deprecated. use [settings] instead") ) ) ); }, 30, TimeUnit.SECONDS); }	i wonder if we should be disabling this setting in an @after method?
private String format(final ChannelHandlerContext ctx, final String eventName, final ByteBuf arg) throws IOException { final int readableBytes = arg.readableBytes(); if (readableBytes == 0) { return super.format(ctx, eventName, arg); } else if (readableBytes >= 2) { final StringBuilder sb = new StringBuilder(); sb.append(ctx.channel().toString()); final int offset = arg.readerIndex(); // this might be an ES message, check the header if (arg.getByte(offset) == (byte) 'E' && arg.getByte(offset + 1) == (byte) 'S') { if (readableBytes == TcpHeader.MARKER_BYTES_SIZE + TcpHeader.MESSAGE_LENGTH_SIZE) { final int length = arg.getInt(offset + MESSAGE_LENGTH_OFFSET); if (length == TcpTransport.PING_DATA_SIZE) { sb.append(" [ping]").append(' ').append(eventName).append(": ").append(readableBytes).append('B'); return sb.toString(); } } else if (readableBytes >= TcpHeader.HEADER_SIZE) { // we are going to try to decode this as an ES message final int length = arg.getInt(offset + MESSAGE_LENGTH_OFFSET); final long requestId = arg.getLong(offset + REQUEST_ID_OFFSET); final byte status = arg.getByte(offset + STATUS_OFFSET); final boolean isRequest = TransportStatus.isRequest(status); final String type = isRequest ? "request" : "response"; final String version = Version.fromId(arg.getInt(offset + VERSION_ID_OFFSET)).toString(); sb.append(" [length: ").append(length); sb.append(", request id: ").append(requestId); sb.append(", type: ").append(type); sb.append(", version: ").append(version); if (isRequest) { // it looks like an ES request, try to decode the action final int remaining = readableBytes - ACTION_OFFSET; final ByteBuf slice = arg.slice(offset + ACTION_OFFSET, remaining); // the stream might be compressed try (StreamInput in = in(status, slice, remaining)) { // the first bytes in the message is the context headers try (ThreadContext context = new ThreadContext(Settings.EMPTY)) { context.readHeaders(in); } // now we decode the features in.readStringArray(); // now we can decode the action name sb.append(", action: ").append(in.readString()); } } sb.append(']'); sb.append(' ').append(eventName).append(": ").append(readableBytes).append('B'); return sb.toString(); } } } // we could not decode this as an ES message, use the default formatting return super.format(ctx, eventName, arg); }	shouldn't we have a version protection here?
public void setVersion(Version version) { this.version = version; }	can we have a java docs with some explanation of what the features are (or a link to where it's explained).
public void testSettingsContainsTransportClient() { final Settings baseSettings = Settings.builder() .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) .build(); try (TransportClient client = new MockTransportClient(baseSettings, Arrays.asList(MockPlugin.class))) { final Settings settings = TcpTransport.DEFAULT_FEATURES_SETTING.get(client.settings()); assertThat(settings.keySet(), hasItem("transport_client")); assertThat(settings.get("transport_client"), equalTo("true")); final ThreadContext threadContext = client.threadPool().getThreadContext(); assertEquals("true", threadContext.getHeader("test")); } }	what is this line testing? not relevant to this test method?
private boolean hasContentTypeOrCanAutoDetect(final RestRequest restRequest, final RestHandler restHandler) { if (restRequest.getXContentType() == null) { if (restHandler != null && restHandler.supportsPlainText()) { // content type of null with a handler that supports plain text gets through for now. Once we remove plain text this can // be removed! deprecationLogger.deprecated("Plain text request bodies are deprecated. Use request parameters or body " + "in a supported format."); } else if (isContentTypeRequired) { return false; } else { deprecationLogger.deprecated("Content type detection for rest requests is deprecated. Specify the content type using " + "the [Content-Type] header."); XContentType xContentType = XContentFactory.xContentType(restRequest.content()); if (xContentType == null) { return false; } else { restRequest.setXContentType(xContentType); } } } return true; }	i wondered if this was needed, it seems like it wasn't ;)
public void testExists() throws Exception { RestHighLevelClient client = highLevelClient(); // tag::exists-request GetRequest getRequest = new GetRequest( "posts", // <1> "doc", // <2> "1"); // <3> getRequest.fetchSourceContext(new FetchSourceContext(false)); // <4> getRequest.storedFields("_none_"); // <5> // end::exists-request { // tag::exists-execute boolean exists = client.exists(getRequest); // end::exists-execute assertFalse(exists); } { // tag::exists-execute-listener ActionListener<Boolean> listener = new ActionListener<Boolean>() { @Override public void onResponse(Boolean exists) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::exists-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::exists-execute-async client.existsAsync(getRequest, listener); // <1> // end::exists-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } }	would "_doc" be better here ?
void deprecated(final Set<ThreadContext> threadContexts, final String message, final boolean log, final Object... params) { threadContextLock.readLock().lock(); try{ final Iterator<ThreadContext> iterator = threadContexts.iterator(); if (iterator.hasNext()) { final String formattedMessage = LoggerMessageFormat.format(message, params); final String warningHeaderValue = formatWarning(formattedMessage); assert WARNING_HEADER_PATTERN.matcher(warningHeaderValue).matches(); assert extractWarningValueFromWarningHeader(warningHeaderValue).equals(escapeAndEncode(formattedMessage)); while (iterator.hasNext()) { final ThreadContext next = iterator.next(); next.addResponseHeader("Warning", warningHeaderValue); } } if (log) { AccessController.doPrivileged(new PrivilegedAction<Void>() { @SuppressLoggerChecks(reason = "safely delegates to logger") @Override public Void run() { /** * There should be only one threadContext (in prod env), @see DeprecationLogger#setThreadContext */ String opaqueId = getXOpaqueId(threadContexts); logger.warn(new DeprecatedMessage(message, opaqueId, params)); return null; } }); } }finally { threadContextLock.readLock().unlock(); } }	nit: space after try
void deprecated(final Set<ThreadContext> threadContexts, final String message, final boolean log, final Object... params) { threadContextLock.readLock().lock(); try{ final Iterator<ThreadContext> iterator = threadContexts.iterator(); if (iterator.hasNext()) { final String formattedMessage = LoggerMessageFormat.format(message, params); final String warningHeaderValue = formatWarning(formattedMessage); assert WARNING_HEADER_PATTERN.matcher(warningHeaderValue).matches(); assert extractWarningValueFromWarningHeader(warningHeaderValue).equals(escapeAndEncode(formattedMessage)); while (iterator.hasNext()) { final ThreadContext next = iterator.next(); next.addResponseHeader("Warning", warningHeaderValue); } } if (log) { AccessController.doPrivileged(new PrivilegedAction<Void>() { @SuppressLoggerChecks(reason = "safely delegates to logger") @Override public Void run() { /** * There should be only one threadContext (in prod env), @see DeprecationLogger#setThreadContext */ String opaqueId = getXOpaqueId(threadContexts); logger.warn(new DeprecatedMessage(message, opaqueId, params)); return null; } }); } }finally { threadContextLock.readLock().unlock(); } }	remove extra new line?
void deprecated(final Set<ThreadContext> threadContexts, final String message, final boolean log, final Object... params) { threadContextLock.readLock().lock(); try{ final Iterator<ThreadContext> iterator = threadContexts.iterator(); if (iterator.hasNext()) { final String formattedMessage = LoggerMessageFormat.format(message, params); final String warningHeaderValue = formatWarning(formattedMessage); assert WARNING_HEADER_PATTERN.matcher(warningHeaderValue).matches(); assert extractWarningValueFromWarningHeader(warningHeaderValue).equals(escapeAndEncode(formattedMessage)); while (iterator.hasNext()) { final ThreadContext next = iterator.next(); next.addResponseHeader("Warning", warningHeaderValue); } } if (log) { AccessController.doPrivileged(new PrivilegedAction<Void>() { @SuppressLoggerChecks(reason = "safely delegates to logger") @Override public Void run() { /** * There should be only one threadContext (in prod env), @see DeprecationLogger#setThreadContext */ String opaqueId = getXOpaqueId(threadContexts); logger.warn(new DeprecatedMessage(message, opaqueId, params)); return null; } }); } }finally { threadContextLock.readLock().unlock(); } }	you can remove the if here since it should never be closed at this point. maybe add an assert?
private ThreadContextStruct putResponse(final String key, final String value, final Function<String, String> uniqueValue, final int maxWarningHeaderCount, final long maxWarningHeaderSize) { assert value != null; long newWarningHeaderSize = warningHeadersSize; //check if we can add another warning header - if max size within limits if (key.equals("Warning")) { if (isWarningLimitReached) return this; // can't add warning headers - limit reached newWarningHeaderSize += "Warning".getBytes(StandardCharsets.UTF_8).length + value.getBytes(StandardCharsets.UTF_8).length; //if size is NOT unbounded AND limit is exceeded if ((maxWarningHeaderSize != -1) && (newWarningHeaderSize > maxWarningHeaderSize)) { logWarningsLimitReached(); return new ThreadContextStruct(requestHeaders, responseHeaders, transientHeaders, isSystemContext, newWarningHeaderSize, true); } } final Map<String, List<String>> newResponseHeaders = new HashMap<>(this.responseHeaders); final List<String> existingValues = newResponseHeaders.get(key); if (existingValues != null) { final Set<String> existingUniqueValues = existingValues.stream().map(uniqueValue).collect(Collectors.toSet()); assert existingValues.size() == existingUniqueValues.size(); if (existingUniqueValues.contains(uniqueValue.apply(value))) { return this; } final List<String> newValues = new ArrayList<>(existingValues); newValues.add(value); newResponseHeaders.put(key, Collections.unmodifiableList(newValues)); } else { newResponseHeaders.put(key, Collections.singletonList(value)); } //check if we can add another warning header - if max count within limits if ((key.equals("Warning")) && (maxWarningHeaderCount != -1)) { //if count is NOT unbounded, check its limits final int warningHeaderCount = newResponseHeaders.containsKey("Warning") ? newResponseHeaders.get("Warning").size() : 0; if (warningHeaderCount > maxWarningHeaderCount) { logWarningsLimitReached(); return new ThreadContextStruct(requestHeaders, responseHeaders, transientHeaders, isSystemContext, newWarningHeaderSize, true); } } return new ThreadContextStruct(requestHeaders, newResponseHeaders, transientHeaders, isSystemContext, newWarningHeaderSize, isWarningLimitReached); }	i think that we want to log *each* time that we drop a warning header, not only the first time for a given request. also we can be more precise than the current implementation which says one or the other condition is met, but we always know exactly which condition it is so we can help the user more by letting them know.
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); index = in.readOptionalString(); text = in.readString(); analyzer = in.readOptionalString(); tokenizer = in.readOptionalString(); int size = in.readVInt(); if (size > 0) { tokenFilters = new String[size]; for (int i = 0; i < size; i++) { tokenFilters[i] = in.readString(); } } if (in.getVersion().onOrAfter(Version.V_1_1_0)) { size = in.readVInt(); if (size > 0) { charFilters = new String[size]; for (int i = 0; i < size; i++) { charFilters[i] = in.readString(); } } } field = in.readOptionalString(); }	can't we sue streaminput#readstringarray() here?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeOptionalString(index); out.writeString(text); out.writeOptionalString(analyzer); out.writeOptionalString(tokenizer); if (tokenFilters == null) { out.writeVInt(0); } else { out.writeVInt(tokenFilters.length); for (String tokenFilter : tokenFilters) { out.writeString(tokenFilter); } } if (out.getVersion().onOrAfter(Version.V_1_1_0)) { if (charFilters == null) { out.writeVInt(0); } else { out.writeVInt(charFilters.length); for (String charFilter : charFilters) { out.writeString(charFilter); } } } out.writeOptionalString(field); }	see above we have a writestringarray as well
public void handleRequest(final RestRequest request, final RestChannel channel) { String text = request.param("text"); if (text == null && request.hasContent()) { text = request.content().toUtf8(); } if (text == null) { try { channel.sendResponse(new XContentThrowableRestResponse(request, new ElasticsearchIllegalArgumentException("text is missing"))); } catch (IOException e1) { logger.warn("Failed to send response", e1); } return; } AnalyzeRequest analyzeRequest = new AnalyzeRequest(request.param("index"), text); analyzeRequest.listenerThreaded(false); analyzeRequest.preferLocal(request.paramAsBoolean("prefer_local", analyzeRequest.preferLocalShard())); analyzeRequest.analyzer(request.param("analyzer")); analyzeRequest.field(request.param("field")); analyzeRequest.tokenizer(request.param("tokenizer")); analyzeRequest.tokenFilters(request.paramAsStringArray("token_filters", request.paramAsStringArray("filters", null))); analyzeRequest.charFilters(request.paramAsStringArray("char_filters", null)); client.admin().indices().analyze(analyzeRequest, new ActionListener<AnalyzeResponse>() { @Override public void onResponse(AnalyzeResponse response) { try { XContentBuilder builder = restContentBuilder(request, null); builder.startObject(); response.toXContent(builder, request); builder.endObject(); channel.sendResponse(new XContentRestResponse(request, OK, builder)); } catch (Throwable e) { onFailure(e); } } @Override public void onFailure(Throwable e) { try { channel.sendResponse(new XContentThrowableRestResponse(request, e)); } catch (IOException e1) { logger.error("Failed to send failure response", e1); } } }); }	can we make strings.empty_array the default?
@Test public void analyzeWithNoIndex() throws Exception { AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze("THIS IS A TEST").setAnalyzer("simple").execute().actionGet(); assertThat(analyzeResponse.getTokens().size(), equalTo(4)); analyzeResponse = client().admin().indices().prepareAnalyze("THIS IS A TEST").setTokenizer("keyword").setTokenFilters("lowercase").execute().actionGet(); assertThat(analyzeResponse.getTokens().size(), equalTo(1)); assertThat(analyzeResponse.getTokens().get(0).getTerm(), equalTo("this is a test")); }	i'd love to see a test that uses more than one char filter :)
public DocIdAndVersion lookupVersion(BytesRef id, LeafReaderContext context) throws IOException { assert context.reader().getCoreCacheHelper().getKey().equals(readerKey) : "context's reader is not the same as the reader class was initialized on."; int docID = getDocID(id, context.reader().getLiveDocs()); if (docID != DocIdSetIterator.NO_MORE_DOCS) { final NumericDocValues versions = context.reader().getNumericDocValues(VersionFieldMapper.NAME); if (versions == null) { throw new IllegalArgumentException("reader misses the [" + VersionFieldMapper.NAME + "] field"); } if (versions.advanceExact(docID) == false) { throw new IllegalArgumentException("Document [" + docID + "] misses the [" + VersionFieldMapper.NAME + "] field"); } return new DocIdAndVersion(docID, versions.longValue(), context.reader(), context.docBase); } else { return null; } } /** * returns the internal lucene doc id for the given id bytes. * {@link DocIdSetIterator#NO_MORE_DOCS}	can we use this method [here](https://github.com/elastic/elasticsearch/pull/29679/files#diff-5fc3777bafee17ea46815420b3dcfd1cl114) as well?
public static boolean hasHistoryInLucene(IndexReader reader, Term idTerm, long seqNo, long primaryTerm) throws IOException { final PerThreadIDVersionAndSeqNoLookup[] lookups = getLookupState(reader, idTerm.field()); final List<LeafReaderContext> leaves = reader.leaves(); // iterate backwards to optimize for the frequently updated documents which are likely to be in the last segments for (int i = leaves.size() - 1; i >= 0; i--) { final LeafReaderContext leaf = leaves.get(i); final PerThreadIDVersionAndSeqNoLookup lookup = lookups[leaf.ord]; final PostingsEnum postingsEnum = lookup.getPostingsOrNull(idTerm.bytes()); if (postingsEnum == null) { continue; } final NumericDocValues seqNoDV = leaf.reader().getNumericDocValues(SeqNoFieldMapper.NAME); final NumericDocValues primaryTermDV = leaf.reader().getNumericDocValues(SeqNoFieldMapper.PRIMARY_TERM_NAME); for (int docId = postingsEnum.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = postingsEnum.nextDoc()) { if (seqNoDV != null && seqNoDV.advanceExact(docId) && primaryTermDV != null && primaryTermDV.advanceExact(docId)) { if (seqNoDV.longValue() == seqNo && primaryTermDV.longValue() >= primaryTerm) { return true; } } } } return false; }	i think we can assert that both seqnodv and primarytermdv are non null. they are required at least for this code to work.
private OpVsLuceneDocStatus compareOpToLuceneDocBasedOnSeqNo(final Operation op) throws IOException { assert op.seqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO : "resolving ops based on seq# but no seqNo is found"; final OpVsLuceneDocStatus status; VersionValue versionValue = getVersionFromMap(op.uid().bytes()); assert incrementVersionLookup(); if (versionValue != null) { if (op.seqNo() > versionValue.seqNo || (op.seqNo() == versionValue.seqNo && op.primaryTerm() > versionValue.term)) { status = OpVsLuceneDocStatus.OP_NEWER; } else if (op.seqNo() == versionValue.seqNo && op.primaryTerm() == versionValue.term) { status = OpVsLuceneDocStatus.OP_STALE_HISTORY_EXISTED; } else { status = OpVsLuceneDocStatus.OP_STALE_HISTORY_NOT_FOUND; } } else { // load from index assert incrementIndexVersionLookup(); try (Searcher searcher = acquireSearcher("load_seq_no", SearcherScope.INTERNAL)) { DocIdAndSeqNo docAndSeqNo = VersionsAndSeqNoResolver.loadDocIdAndSeqNo(searcher.reader(), op.uid()); if (docAndSeqNo == null) { status = OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND; } else if (op.seqNo() > docAndSeqNo.seqNo) { status = OpVsLuceneDocStatus.OP_NEWER; } else if (op.seqNo() == docAndSeqNo.seqNo) { // load term to tie break final long existingTerm = VersionsAndSeqNoResolver.loadPrimaryTerm(docAndSeqNo, op.uid().field()); if (op.primaryTerm() > existingTerm) { status = OpVsLuceneDocStatus.OP_NEWER; } else { status = compareToLuceneHistory(op, searcher); } } else { status = compareToLuceneHistory(op, searcher); } } } return status; }	can you please add comments here in what cases we can get into these situations?
private void validateServerConfiguration(String prefix) { assert prefix.endsWith(".ssl"); SSLConfiguration configuration = getSSLConfiguration(prefix); final String enabledSetting = prefix + ".enabled"; if (settings.getAsBoolean(enabledSetting, false) == true) { // Client Authentication _should_ be required, but if someone turns it off, then this check is no longer relevant final SSLConfigurationSettings configurationSettings = SSLConfigurationSettings.withPrefix(prefix + "."); if (isConfigurationValidForServerUsage(configuration) == false) { throw new ElasticsearchSecurityException("invalid configuration for " + prefix + " - a server context requires a key and certificate, but these have not been configured; either [" + configurationSettings.x509KeyPair.keystorePath.getKey() + "] or [" + configurationSettings.x509KeyPair.certificatePath.getKey() + "] should be set"); } } else if (settings.hasValue(enabledSetting) == false) { final List<String> sslKeys = settings.keySet().stream() .filter(s -> s.startsWith(prefix)) .sorted() .collect(Collectors.toUnmodifiableList()); if (sslKeys.isEmpty() == false) { throw new ElasticsearchSecurityException("invalid configuration for " + prefix + " - [" + enabledSetting + "] is not set, but the following settings have been configured in elasticsearch.yml : [" + Strings.collectionToCommaDelimitedString(sslKeys) + "]"); } } }	unsure how clear "server context" is. we could try "server ssl context" or "server ssl configuration" ?
private boolean checkIfUserIsOwnerOfApiKeys(Authentication authentication, String apiKeyId, String username, String realmName, boolean ownedByAuthenticatedUser) { if (isCurrentAuthenticationUsingSameApiKeyIdFromRequest(authentication, apiKeyId)) { return true; } else { /* * TODO bizybot we need to think on how we can propagate appropriate error message to the end user when username, realm name * is missing. This is similar to the problem of propagating right error messages in case of access denied. */ if (authentication.getSourceRealm().getType().equals(API_KEY_REALM_TYPE)) { // API key cannot own any other API key so deny access return false; } else if (ownedByAuthenticatedUser) { return true; } else if (Strings.hasText(username) && Strings.hasText(realmName)) { final String authenticatedUserPrincipal = authentication.getUser().authenticatedUser().principal(); final String authenticatedUserRealm = authentication.getSourceRealm().getName(); return username.equals(authenticatedUserPrincipal) && realmName.equals(authenticatedUserRealm); } } return false; }	this is not strictly necessary since api key cannot impersonate another api key. but changed anyhow for consistency.
@Override protected IndexAnalyzers createIndexAnalyzers(IndexSettings indexSettings) { NamedAnalyzer dflt = new NamedAnalyzer( "default", AnalyzerScope.INDEX, new StandardAnalyzer(), TextFieldMapper.Defaults.POSITION_INCREMENT_GAP ); NamedAnalyzer standard = new NamedAnalyzer("standard", AnalyzerScope.INDEX, new StandardAnalyzer()); NamedAnalyzer keyword = new NamedAnalyzer("keyword", AnalyzerScope.INDEX, new KeywordAnalyzer()); NamedAnalyzer simple = new NamedAnalyzer("simple", AnalyzerScope.INDEX, new SimpleAnalyzer()); NamedAnalyzer whitespace = new NamedAnalyzer("whitespace", AnalyzerScope.INDEX, new WhitespaceAnalyzer()); return new IndexAnalyzers( Map.of("default", dflt, "standard", standard, "keyword", keyword, "simple", simple, "whitespace", whitespace), Map.of(), Map.of() ); }	this comment looks truncated?
@Override protected IndexAnalyzers createIndexAnalyzers(IndexSettings indexSettings) { NamedAnalyzer dflt = new NamedAnalyzer( "default", AnalyzerScope.INDEX, new StandardAnalyzer(), TextFieldMapper.Defaults.POSITION_INCREMENT_GAP ); NamedAnalyzer standard = new NamedAnalyzer("standard", AnalyzerScope.INDEX, new StandardAnalyzer()); NamedAnalyzer keyword = new NamedAnalyzer("keyword", AnalyzerScope.INDEX, new KeywordAnalyzer()); NamedAnalyzer simple = new NamedAnalyzer("simple", AnalyzerScope.INDEX, new SimpleAnalyzer()); NamedAnalyzer whitespace = new NamedAnalyzer("whitespace", AnalyzerScope.INDEX, new WhitespaceAnalyzer()); return new IndexAnalyzers( Map.of("default", dflt, "standard", standard, "keyword", keyword, "simple", simple, "whitespace", whitespace), Map.of(), Map.of() ); }	:heart: so much nicer
@Override protected ClusterBlockException checkBlock(VerifyRepositoryRequest request, ClusterState state) { return state.blocks().indexBlockedException(ClusterBlockLevel.METADATA_WRITE, ""); }	maybe i don't know what verify repo does, bu tit sounds like a read_only operation to me. why metadata_write and not metadata_read?
public static void disableIndexBlock(String index, String block) { disableIndexBlock(client(), index, block); } /** Disables an index block for the specified index using a given {@link Client}	is it important to provide a specific client here? isn't the previous method enough?
public static void assertAcked(DeleteIndexResponse response) { assertThat("Delete Index failed - not acked", response.isAcknowledged(), equalTo(true)); assertVersionSerializable(response); }	may i ask you to add javadocs here for people using our test-jar?
@Override public void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException { builder.field("type", contentType()); AbstractGeometryFieldType ft = (AbstractGeometryFieldType)fieldType(); if (includeDefaults || ft.orientation() != Defaults.ORIENTATION.value()) { builder.field(Names.ORIENTATION.getPreferredName(), ft.orientation()); } if (includeDefaults || coerce.explicit()) { builder.field(Names.COERCE.getPreferredName(), coerce.value()); } if (includeDefaults || ignoreMalformed.explicit()) { builder.field(IGNORE_MALFORMED, ignoreMalformed.value()); } if (includeDefaults || ignoreZValue.explicit()) { builder.field(GeoPointFieldMapper.Names.IGNORE_Z_VALUE.getPreferredName(), ignoreZValue.value()); } if (includeDefaults || docValues.explicit()) { builder.field(TypeParsers.DOC_VALUES, docValues.value()); } }	hi @jpountz, in a follow-up commit i've removed the explicit docvalues field from the mapper, but without it, this conditional is difficult to achieve (unless i'm missing something). it is difficult to check whether the value was set explicitly without access to docvaluesset which is in the field mapper's builder. not sure this is a big deal, but it does feel like the doc values field should only be serialized in this scenaior
@Override public String getType() { return NAME; }	this didn't do anything....
private SubSearchContext buildSubSearchContext( MapperService mapperService, QueryShardContext queryShardContext, BitsetFilterCache bitsetFilterCache ) { SearchContext ctx = mock(SearchContext.class); QueryCache queryCache = new DisabledQueryCache(mapperService.getIndexSettings()); QueryCachingPolicy queryCachingPolicy = new QueryCachingPolicy() { @Override public void onUse(Query query) { } @Override public boolean shouldCache(Query query) { // never cache a query return false; } }; try { when(ctx.searcher()).thenReturn( new ContextIndexSearcher( queryShardContext.searcher().getIndexReader(), queryShardContext.searcher().getSimilarity(), queryCache, queryCachingPolicy, false ) ); } catch (IOException e) { throw new RuntimeException(e); } when(ctx.fetchPhase()).thenReturn(new FetchPhase(Arrays.asList(new FetchSourcePhase(), new FetchDocValuesPhase()))); when(ctx.getQueryShardContext()).thenReturn(queryShardContext); NestedDocuments nestedDocuments = new NestedDocuments(mapperService, bitsetFilterCache::getBitSetProducer); when(ctx.getNestedDocuments()).thenReturn(nestedDocuments); IndexShard indexShard = mock(IndexShard.class); when(indexShard.shardId()).thenReturn(new ShardId("test", "test", 0)); when(ctx.indexShard()).thenReturn(indexShard); return new SubSearchContext(ctx); }	now all of this is contained to only be called for top_hits.
private void verifyRelocatingState() { final IndexShardState state = state(); if (state == IndexShardState.PROMOTING) { throw new IllegalIndexShardStateException(shardId, IndexShardState.STARTED, ": primary relocation is forbidden while primary-replica resync is in progress " + shardRouting); } if (state != IndexShardState.STARTED) { throw new IndexShardNotStartedException(shardId, state); } /* * If the master cancelled recovery, the target will be removed and the recovery will be cancelled. However, it is still possible * that we concurrently end up here and therefore have to protect that we do not mark the shard as relocated when its shard routing * says otherwise. */ if (shardRouting.relocating() == false) { throw new IllegalIndexShardStateException(shardId, IndexShardState.STARTED, ": shard is no longer relocating " + shardRouting); } }	this is already covered by the check if (state != indexshardstate.started) { below?
public static String snapshotUpgradeTaskId(String jobId, String snapshotId) { return JOB_SNAPSHOT_UPGRADE_TASK_ID_PREFIX + jobId + "-" + snapshotId; }	i don't think this is necessary as the place that uses it has access to the persistent task params.
public AutoscalingCapacity autoscalingCapacity(int maxMemoryPercent, boolean useAuto) { // We calculate the JVM size here first to ensure it stays the same given the rest of the calculations final Long jvmSize = useAuto ? Optional.ofNullable(this.jvmSize).orElse(dynamicallyCalculateJvmSizeFromNativeMemorySize(nodeMlNativeMemoryRequirement)) : null; // We first need to calculate the required node size given the required native ML memory size. // This way we can accurately determine the required node size AND what the overall memory percentage will be long requiredNodeSize = NativeMemoryCalculator.calculateApproxNecessaryNodeSize( nodeMlNativeMemoryRequirement, jvmSize, maxMemoryPercent, useAuto ); // We make the assumption that the JVM size is the same across the entire tier // This simplifies calculating the tier as it means that each node in the tier // will have the same dynamic memory calculation. And thus the tier is simply the sum of the memory necessary // times that scaling factor. // // Since this is a _minimum_ node size, the memory percent calculated here is not // necessarily what "auto" will imply after scaling. Because the JVM occupies a // smaller proportion of memory the bigger the node, the memory percent might be // higher than we calculate here, potentially resulting in a bigger ML tier than // required. The effect is most pronounced when the minimum node size is small and // the minimum tier size is a lot bigger. For example, if the minimum node size is // 1GB and the total native ML memory requirement for the tier is 32GB then the memory // percent will be 41%, implying a minimum ML tier size of 78GB. But in reality a // single 64GB ML node would have an auto memory percent of 90%, and 90% of 64GB is // plenty big enough for the 32GB of ML native memory required. // TODO: improve this in the next refactoring double memoryPercentForMl = NativeMemoryCalculator.modelMemoryPercent(requiredNodeSize, jvmSize, maxMemoryPercent, useAuto); double inverseScale = memoryPercentForMl <= 0 ? 0 : 100.0 / memoryPercentForMl; long requiredTierSize = Math.round(Math.ceil(tierMlNativeMemoryRequirement * inverseScale)); return new AutoscalingCapacity( // Tier should always be AT LEAST the largest node size. // This Math.max catches any strange rounding errors or weird input. new AutoscalingCapacity.AutoscalingResources(null, ByteSizeValue.ofBytes(Math.max(requiredTierSize, requiredNodeSize))), new AutoscalingCapacity.AutoscalingResources(null, ByteSizeValue.ofBytes(requiredNodeSize)) ); }	fixing this isn't what this pr is primarily about, but i noticed it while debugging the new test, so thought it worth documenting. the way all our sliding scales interact is very complex. we should refactor this once the formula for node size to jvm size is reversible. to be more accurate this method also needs to know the maximum ml node size and the principle that cloud always scales up before scaling out. maximum ml node size is not currently reliably set. but work is in progress to make it accurate, and this will take effect no later than the delegation of the jvm heap size to elasticsearch. so in the next refactor all the prerequisites will be in place to make this better.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(TYPE_FIELD.getPreferredName(), policy.getType().getWriteableName()); builder.field(POLICY.getPreferredName(), policy); builder.field(HEADERS.getPreferredName(), headers); builder.endObject(); return builder; }	it's weird that we don't read the type but we do write it out, are we trying to protect against bwc changes if/when we add another type?
protected LifecyclePolicy doParseInstance(XContentParser parser) { return LifecyclePolicy.parse(parser, lifecycleName, TestLifecycleType.INSTANCE); }	we should be able to rewrite the createtestinstance() and mutateinstance() methods so they use the timeseries lifecycle type so we don't have to have the parse method take the type here. the other tests in this suite can still use the testlifecycletype since they create the policy directly rather than parsing from json
private void joinFieldResolveConfig(SearchContext context, ValuesSourceConfig<WithOrdinals> config) { ParentJoinFieldMapper parentJoinFieldMapper = ParentJoinFieldMapper.getMapper(context.mapperService()); if(parentJoinFieldMapper != null){ ParentIdFieldMapper parentIdFieldMapper = parentJoinFieldMapper.getParentIdFieldMapper(childType, false); if (parentIdFieldMapper != null) { parentFilter = parentIdFieldMapper.getParentFilter(); childFilter = parentIdFieldMapper.getChildFilter(childType); MappedFieldType fieldType = parentIdFieldMapper.fieldType(); final SortedSetDVOrdinalsIndexFieldData fieldData = context.getForField(fieldType); config.fieldContext(new FieldContext(fieldType.name(), fieldData, fieldType)); } else { config.unmapped(true); } }else{ config.unmapped(true); } }	hmm, let's see if we can un-nest the if conditional? usually cleaner to read if they don't nest in my opinion. how about something like: java if (parentjoinfieldmapper == null) { config.unmapped(true); return; } parentidfieldmapper parentidfieldmapper = parentjoinfieldmapper.getparentidfieldmapper(childtype, false); if (parentidfieldmapper == null) { config.unmapped(true); return; } and then after that point we know both fields are not null, so we can set the config outside of a conditional. should flatten the whole thing a bit.
public static <T> T requireNonNull(T obj, ParseField paramName) { return requireNonNull(obj, paramName.getPreferredName()); }	what do you think of making this a boolean unwrappedinstanceof(throwable wrappedthrowable, class klass) or something?
private void handleFieldWildcards(TermVectorRequest request) { List<String> fieldNames = new ArrayList<>(); for (String pattern : request.selectedFields()) { if (Regex.isSimpleMatchPattern(pattern)) { for (String fieldName : indexShard.mapperService().simpleMatchToIndexNames(pattern)) { fieldNames.add(fieldName); } } else { fieldNames.add(pattern); } } request.selectedFields(fieldNames.toArray(Strings.EMPTY_ARRAY)); }	is the if necessary? it seems to me that the following should work? java for (string pattern : request.selectedfields()) { fieldnames.addall(indexshard.mapperservice().simplematchtoindexnames(pattern)); }
public void testToQuery() throws IOException { for (int runs = 0; runs < NUMBER_OF_TESTQUERIES; runs++) { QueryShardContext context = createShardContext(); context.setAllowUnmappedFields(true); QB firstQuery = createTestQueryBuilder(); QB controlQuery = copyQuery(firstQuery); setSearchContext(randomTypes, context); // only set search context for toQuery to be more realistic Query firstLuceneQuery = rewriteQuery(firstQuery, context).toQuery(context); assertNotNull("toQuery should not return null", firstLuceneQuery); assertLuceneQuery(firstQuery, firstLuceneQuery, context); SearchContext.removeCurrent(); // remove after assertLuceneQuery since the assertLuceneQuery impl might access the context as well assertTrue( "query is not equal to its copy after calling toQuery, firstQuery: " + firstQuery + ", secondQuery: " + controlQuery, firstQuery.equals(controlQuery)); assertTrue("equals is not symmetric after calling toQuery, firstQuery: " + firstQuery + ", secondQuery: " + controlQuery, controlQuery.equals(firstQuery)); assertThat("query copy's hashcode is different from original hashcode after calling toQuery, firstQuery: " + firstQuery + ", secondQuery: " + controlQuery, controlQuery.hashCode(), equalTo(firstQuery.hashCode())); QB secondQuery = copyQuery(firstQuery); // query _name never should affect the result of toQuery, we randomly set it to make sure if (randomBoolean()) { secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10)); } setSearchContext(randomTypes, context); Query secondLuceneQuery = rewriteQuery(secondQuery, context).toQuery(context); assertLuceneQuery(secondQuery, secondLuceneQuery, context); SearchContext.removeCurrent(); assertEquals("two equivalent query builders lead to different lucene queries", rewrite(secondLuceneQuery), rewrite(firstLuceneQuery)); // if the initial lucene query is null, changing its boost won't have any effect, we shouldn't test that if (firstLuceneQuery != null && supportsBoostAndQueryName()) { secondQuery.boost(firstQuery.boost() + 1f + randomFloat()); setSearchContext(randomTypes, context); Query thirdLuceneQuery = rewriteQuery(secondQuery, context).toQuery(context); SearchContext.removeCurrent(); assertNotEquals("modifying the boost doesn't affect the corresponding lucene query", rewrite(firstLuceneQuery), rewrite(thirdLuceneQuery)); } // check that context#isFilter is not changed by invoking toQuery/rewrite boolean filterFlag = randomBoolean(); context.setIsFilter(filterFlag); rewriteQuery(firstQuery, context).toQuery(context); assertEquals("isFilter should be unchanged", filterFlag, context.isFilter()); } }	shall we do the same with the second lucene query and other possible queries we have here?
void setEmptySeedHostsList() { seedHostsList = emptyList(); }	perhaps call this blackholerequestsfrom?
@Override public Query distanceFeatureQuery(Object origin, String pivot, SearchExecutionContext context) { failIfNotIndexedNorDocValuesFallback(context); long originLong = parseToLong(origin, true, null, null, context::nowInMillis); TimeValue pivotTime = TimeValue.parseTimeValue(pivot, "distance_feature.pivot"); long pivotLong = resolution.convert(pivotTime); // As we already apply boost in AbstractQueryBuilder::toQuery, we always passing a boost of 1.0 to distanceFeatureQuery if (isIndexed()) { return LongPoint.newDistanceFeatureQuery(name(), 1.0f, originLong, pivotLong); } else { return new LongScriptFieldDistanceFeatureQuery( new Script(""), ctx -> new SortedNumericDocValuesLongFieldScript(name(), context.lookup(), ctx), name(), originLong, pivotLong ); } }	were we silently not returning any result when the field was not indexed? or failing later with some other exception?
@Override public Query distanceFeatureQuery(Object origin, String pivot, SearchExecutionContext context) { failIfNotIndexedNorDocValuesFallback(context); long originLong = parseToLong(origin, true, null, null, context::nowInMillis); TimeValue pivotTime = TimeValue.parseTimeValue(pivot, "distance_feature.pivot"); long pivotLong = resolution.convert(pivotTime); // As we already apply boost in AbstractQueryBuilder::toQuery, we always passing a boost of 1.0 to distanceFeatureQuery if (isIndexed()) { return LongPoint.newDistanceFeatureQuery(name(), 1.0f, originLong, pivotLong); } else { return new LongScriptFieldDistanceFeatureQuery( new Script(""), ctx -> new SortedNumericDocValuesLongFieldScript(name(), context.lookup(), ctx), name(), originLong, pivotLong ); } }	this makes me a bit nervous. we use the original script only in equals/hashcode, and we use the empty script already for script-less runtime fields that load from _source: https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/mapper/abstractscriptfieldtype.java#l212 . i am not entirely sure what this may cause down the line.
public static int modelMemoryPercent(long machineMemory, Long jvmSize, int maxMemoryPercent, boolean useAuto) { if (useAuto) { jvmSize = jvmSize == null ? dynamicallyCalculateJvm(machineMemory) : jvmSize; if (machineMemory - jvmSize < OS_OVERHEAD || machineMemory == 0) { assert false: String.format( Locale.ROOT, "machine memory [%d] minus jvm [%d] is less than overhead [%d]", machineMemory, jvmSize, OS_OVERHEAD ); return maxMemoryPercent; } // This calculation is dynamic and designed to maximally take advantage of the underlying machine for machine learning // We only allow 200MB for the Operating system itself and take up to 90% of the underlying native memory left // Example calculations: // 1GB node -> 41% // 2GB node -> 66% // 16GB node -> 87% // 64GB node -> 90% return Math.min(90, (int)Math.ceil(((machineMemory - jvmSize - OS_OVERHEAD) / (double)machineMemory) * 100.0D)); } return maxMemoryPercent; }	the comment might be useful in the future. i can see that it hardcodes an assumption about the os overhead constant, but instead of deleting it it could be rephrased as "if the os overhead were 200mb then these are some example results".
private static OptionalLong allowedBytesForMl(String nodeBytes, String jvmBytes, int maxMemoryPercent, boolean useAuto) { if (nodeBytes == null) { return OptionalLong.empty(); } final long machineMemory; try { machineMemory = Long.parseLong(nodeBytes); } catch (NumberFormatException e) { return OptionalLong.empty(); } Long jvmMemory = null; try { if (Strings.isNullOrEmpty(jvmBytes) == false) { jvmMemory = Long.parseLong(jvmBytes); } } catch (NumberFormatException e) { return OptionalLong.empty(); } return OptionalLong.of(allowedBytesForMl(machineMemory, jvmMemory, maxMemoryPercent, useAuto)); }	there would be more symmetry in the code and it might be easier to reason about what's going on if you rename the existing dynamicallycalculatejvm method to dynamicallycalculatejvmsizefromnodesize and introduce a new method dynamicallycalculatejvmsizefrommlnativememory that does the bit below. then this line can become: jvmsize = jvmsize == null ? dynamicallycalculatejvmsizefrommlnativememory(nativemachinememory) : jvmsize; which matches the pattern that's been used elsewhere, and the names of the two utility methods make clear why they're different: one is taking total node memory as input while the other takes ml native memory requirement as input.
public static long calculateApproxNecessaryNodeSize(long nativeMachineMemory, Long jvmSize, int maxMemoryPercent, boolean useAuto) { if (nativeMachineMemory == 0) { return 0; } if (useAuto) { // TODO utilize official ergonomic JVM size calculations when available. final long unboxedJvm; // See dynamicallyCalculateJvm the following JVM calculations are arithmetic inverses of JVM calculation // // Example: For < 2GB node, the JVM is 0.4 * total_node_size. This means, the rest is 0.6 the node size. // So, the `nativeAndOverhead` is == 0.6 * total_node_size => total_node_size = (nativeAndOverHead / 0.6) // Consequently jvmSize = (nativeAndOverHead / 0.6)*0.4 = nativeAndOverHead * 2/3 if (jvmSize == null) { long nativeAndOverhead = nativeMachineMemory + OS_OVERHEAD; if (nativeAndOverhead < (ByteSizeValue.ofGb(2).getBytes() * 0.60)) { unboxedJvm = (long) Math.ceil(nativeAndOverhead * (2.0 / 3.0)); } else if (nativeAndOverhead < (ByteSizeValue.ofGb(8).getBytes() * 0.75)) { unboxedJvm = (long) Math.ceil(nativeAndOverhead / 3.0); } else { unboxedJvm = ByteSizeValue.ofGb(2).getBytes(); } } else { unboxedJvm = jvmSize; } // Due to rounding errors between calculating, we account for percentage inaccuracies so that our approximate node size // is always SLIGHTLY too large (or exactly correct). int modelMemoryPercent = modelMemoryPercent( nativeMachineMemory + unboxedJvm + OS_OVERHEAD, unboxedJvm, maxMemoryPercent, true) - AUTO_MEMORY_ERROR_RATE; // We calculate the inverse percentage of `nativeMachineMemory + OS_OVERHEAD` as `OS_OVERHEAD` is always present // on the native memory side and we need to account for it when we invert the model memory percentage return Math.max((long)Math.ceil((100.0/modelMemoryPercent) * (nativeMachineMemory + OS_OVERHEAD)), MINIMUM_AUTOMATIC_NODE_SIZE); } return (long) ((100.0/maxMemoryPercent) * nativeMachineMemory); }	i am not sure that having the 1 in a constant called auto_memory_error_rate makes this clearer. it's not really a rate. i don't see how the rounding errors in calculating the percentage should ever result in the rounded percentage being off by more than 1. the problem really is that the modelmemorypercent method does a math.ceil, when for this particular case we wanted a math.floor. so i would just hardcode 1 here and change the comment to say it's because modelmemorypercent uses math.ceil and we wanted math.floor. the alternatives would be: 1. add an extra argument to modelmemorypercent telling it whether to round up or down. 2. make modelmemorypercent return a double, and let the caller choose whether to round up or down. since this pr is quite urgent i'm happy to go with the low-churn solution of just subtracting 1 here and changing the comment.
static Mapping createDynamicUpdate(MappingLookup mappingLookup, List<Mapper> dynamicMappers, List<RuntimeField> dynamicRuntimeFields) { if (dynamicMappers.isEmpty() && dynamicRuntimeFields.isEmpty()) { return null; } RootObjectMapper root; if (dynamicMappers.isEmpty() == false) { root = createDynamicUpdate(mappingLookup, dynamicMappers); root.fixRedundantIncludes(); } else { root = mappingLookup.getMapping().getRoot().copyAndReset(); } root.addRuntimeFields(dynamicRuntimeFields); return mappingLookup.getMapping().mappingUpdate(root); }	is there a reason that we don't always do this when we create a root object mapper?
private static Set<Object> createSharedStatics(List<PainlessContextInfo> contextInfos) { Map<Object, Integer> staticInfoCounts = new HashMap<>(); for (PainlessContextInfo contextInfo : contextInfos) { for (PainlessContextMethodInfo methodInfo : contextInfo.getImportedMethods()) { staticInfoCounts.compute(methodInfo, (k, v) -> v == null ? 1 : v + 1); } for (PainlessContextClassBindingInfo classBindingInfo : contextInfo.getClassBindings()) { staticInfoCounts.compute(classBindingInfo, (k, v) -> v == null ? 1 : v + 1); } for (PainlessContextInstanceBindingInfo instanceBindingInfo : contextInfo.getInstanceBindings()) { staticInfoCounts.compute(instanceBindingInfo, (k, v) -> v == null ? 1 : v + 1); } } return staticInfoCounts.entrySet().stream().filter( e -> e.getValue() == contextInfos.size() ).map(Map.Entry::getKey).collect(Collectors.toSet()); }	i think this can be done more simply with merge: staticinfocounts.merge(methodinfo, 1, integer::sum)
@SuppressWarnings("unchecked") private void assertIndexAllocatedToNode(String index, String nodeId) throws IOException { Map<String, ?> state = entityAsMap(client().performRequest(new Request("GET", "/_cluster/state"))); Map<String, ?> metadata = (Map<String, Object>) XContentMapValues.extractValue("metadata.indices." + index, state); Map<String, ?> routingTable = (Map<String, Object>) XContentMapValues.extractValue("routing_table.indices." + index, state); Map<String, ?> settings = (Map<String, Object>) XContentMapValues.extractValue("settings", metadata); int numberOfShards = Integer.parseInt((String) XContentMapValues.extractValue("index.number_of_shards", settings)); for (int i = 0; i < numberOfShards; i++) { Collection<Map<String, ?>> shards = (Collection<Map<String, ?>>) XContentMapValues.extractValue("shards." + i, routingTable); Set<String> assignedNodes = new HashSet<>(); for (Map<String, ?> shard : shards) { assignedNodes.add((String) XContentMapValues.extractValue("node", shard)); } assertThat(nodeId, isIn(assignedNodes)); } }	given that we know the number of shards is 1, extracting this from cluster state seems unduly complex. would it be an idea to just assume one shard?
public IndexRequest ttl(String ttl) { this.ttl = TimeValue.parseTimeValue(ttl, null, "ttl"); return this; } /** * Sets the ttl as a {@link TimeValue}	can this be set/getpipeline?
@Override public synchronized void close() { if (lifecycle.started()) { stop(); } if (!lifecycle.moveToClosed()) { return; } ESLogger logger = Loggers.getLogger(Node.class, settings.get("name")); logger.info("closing ..."); StopWatch stopWatch = new StopWatch("node_close"); stopWatch.start("tribe"); injector.getInstance(TribeService.class).close(); stopWatch.stop().start("ingest_service"); try { injector.getInstance(NodeService.class).getIngestService().close(); } catch (IOException e) { logger.warn("IngestService close failed", e); } stopWatch.stop().start("http"); if (settings.getAsBoolean("http.enabled", true)) { injector.getInstance(HttpServer.class).close(); } stopWatch.stop().start("snapshot_service"); injector.getInstance(SnapshotsService.class).close(); injector.getInstance(SnapshotShardsService.class).close(); stopWatch.stop().start("client"); Releasables.close(injector.getInstance(Client.class)); stopWatch.stop().start("indices_cluster"); injector.getInstance(IndicesClusterStateService.class).close(); stopWatch.stop().start("indices"); injector.getInstance(IndicesTTLService.class).close(); injector.getInstance(IndicesService.class).close(); // close filter/fielddata caches after indices injector.getInstance(IndicesQueryCache.class).close(); injector.getInstance(IndicesFieldDataCache.class).close(); injector.getInstance(IndicesStore.class).close(); stopWatch.stop().start("routing"); injector.getInstance(RoutingService.class).close(); stopWatch.stop().start("cluster"); injector.getInstance(ClusterService.class).close(); stopWatch.stop().start("discovery"); injector.getInstance(DiscoveryService.class).close(); stopWatch.stop().start("monitor"); injector.getInstance(MonitorService.class).close(); stopWatch.stop().start("gateway"); injector.getInstance(GatewayService.class).close(); stopWatch.stop().start("search"); injector.getInstance(SearchService.class).close(); stopWatch.stop().start("rest"); injector.getInstance(RestController.class).close(); stopWatch.stop().start("transport"); injector.getInstance(TransportService.class).close(); stopWatch.stop().start("percolator_service"); injector.getInstance(PercolatorService.class).close(); for (Class<? extends LifecycleComponent> plugin : pluginsService.nodeServices()) { stopWatch.stop().start("plugin(" + plugin.getName() + ")"); injector.getInstance(plugin).close(); } stopWatch.stop().start("script"); try { injector.getInstance(ScriptService.class).close(); } catch(IOException e) { logger.warn("ScriptService close failed", e); } stopWatch.stop().start("thread_pool"); // TODO this should really use ThreadPool.terminate() injector.getInstance(ThreadPool.class).shutdown(); try { injector.getInstance(ThreadPool.class).awaitTermination(10, TimeUnit.SECONDS); } catch (InterruptedException e) { // ignore } stopWatch.stop().start("thread_pool_force_shutdown"); try { injector.getInstance(ThreadPool.class).shutdownNow(); } catch (Exception e) { // ignore } stopWatch.stop(); if (logger.isTraceEnabled()) { logger.trace("Close times for each service:\\\\n{}", stopWatch.prettyPrint()); } injector.getInstance(NodeEnvironment.class).close(); injector.getInstance(PageCacheRecycler.class).close(); logger.info("closed"); }	can we make nodeservice closeable and do the right thing there?
static Map<String, Object> createMappingsFromStringMap(Map<String, String> mappings) { Map<String, Object> fieldMappings = new HashMap<>(); for (Map.Entry<String, String> entry : mappings.entrySet()) { String[] parts = entry.getKey().split("\\\\\\\\."); Map<String, Object> current = fieldMappings; current = diveInto(current, parts[0]); for (int j = 1; j < parts.length; ++j) { current = diveInto(current, "fields"); current = diveInto(current, parts[j]); } current.put("type", entry.getValue()); } return fieldMappings; }	tests show this doesn't work when there are object fields mixed with multi-fields. i need to rethink this solution.
public DirectoryReader getDirectoryReader() { final IndexReader reader = getIndexReader(); assert reader instanceof DirectoryReader : "expected an instance of DirectoryReader, got " + reader.getClass(); return (DirectoryReader) reader; } /** * Wraps an {@link IndexReader}	let's call it cancellabledirectoryreader if it extends directoryreader.
@Override public CacheHelper getReaderCacheHelper() { return in.getReaderCacheHelper(); } } /** * Wraps a leaf reader with a cancellable task */ private static class CancellableLeafReader extends ExitableDirectoryReader.ExitableFilterAtomicReader { private CancellableLeafReader(LeafReader leafReader, Cancellable checkCancelled) { super(leafReader, checkCancelled); } @Override public NumericDocValues getNumericDocValues(String field) throws IOException { return in.getNumericDocValues(field); } @Override public BinaryDocValues getBinaryDocValues(String field) throws IOException { return in.getBinaryDocValues(field); } @Override public SortedDocValues getSortedDocValues(String field) throws IOException { return in.getSortedDocValues(field); } @Override public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException { return in.getSortedNumericDocValues(field); } @Override public SortedSetDocValues getSortedSetDocValues(String field) throws IOException { return in.getSortedSetDocValues(field); } } /** * Implementation of {@link QueryTimeout} with a Runnable task. */ private static class Cancellable implements QueryTimeout { private Supplier<Runnable> cancellable; public void setCancellable(Supplier<Runnable> cancellable) { this.cancellable = cancellable; } @Override public boolean shouldExit() { assert cancellable != null : "checkCancelled must be set immediately after the construction of CancellableIndexReader"; if (cancellable.get() == null) { return false; } cancellable.get().run(); return false; } @Override public boolean isTimeoutEnabled() { assert cancellable != null : "checkCancelled must be set immediately after the construction of CancellableIndexReader"; return cancellable.get() != null; }	this implementation feels a bit awkward, i'd rather like to fork exitabledirectoryreader entirely to not inherit from its querytimeout abstraction.
*/ private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata, @Nullable RepositoryData repositoryData) { final Snapshot snapshot = entry.snapshot(); final boolean newFinalization = endingSnapshots.add(snapshot); if (entry.isClone() && entry.state() == State.FAILED) { logger.debug("Removing failed snapshot clone [{}] from cluster state", entry); if (newFinalization) { removeFailedSnapshotFromClusterState(snapshot, new SnapshotException(snapshot, entry.failure()), null); } return; } final String repoName = snapshot.getRepository(); if (tryEnterRepoLoop(repoName)) { if (repositoryData == null) { repositoriesService.repository(repoName).getRepositoryData(new ActionListener<>() { @Override public void onResponse(RepositoryData repositoryData) { finalizeSnapshotEntry(snapshot, metadata, repositoryData); } @Override public void onFailure(Exception e) { clusterService.submitStateUpdateTask( "fail repo tasks for [" + repoName + "]", new FailPendingRepoTasksTask(repoName, e) ); } }); } else { finalizeSnapshotEntry(snapshot, metadata, repositoryData); } } else { if (newFinalization) { repositoryOperations.addFinalization(snapshot, metadata); } } } /** * Try starting to run a snapshot finalization or snapshot delete for the given repository. If this method returns * {@code true} then snapshot finalizations and deletions for the repo may be executed. Once no more operations are * ready for the repository {@link #leaveRepoLoop(String)}	do we have a test for this new condition? i deleted it and ran a few likely-looking suites and also all of :server:ictest but didn't see any failures.
*/ private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata, @Nullable RepositoryData repositoryData) { final Snapshot snapshot = entry.snapshot(); final boolean newFinalization = endingSnapshots.add(snapshot); if (entry.isClone() && entry.state() == State.FAILED) { logger.debug("Removing failed snapshot clone [{}] from cluster state", entry); if (newFinalization) { removeFailedSnapshotFromClusterState(snapshot, new SnapshotException(snapshot, entry.failure()), null); } return; } final String repoName = snapshot.getRepository(); if (tryEnterRepoLoop(repoName)) { if (repositoryData == null) { repositoriesService.repository(repoName).getRepositoryData(new ActionListener<>() { @Override public void onResponse(RepositoryData repositoryData) { finalizeSnapshotEntry(snapshot, metadata, repositoryData); } @Override public void onFailure(Exception e) { clusterService.submitStateUpdateTask( "fail repo tasks for [" + repoName + "]", new FailPendingRepoTasksTask(repoName, e) ); } }); } else { finalizeSnapshotEntry(snapshot, metadata, repositoryData); } } else { if (newFinalization) { repositoryOperations.addFinalization(snapshot, metadata); } } } /** * Try starting to run a snapshot finalization or snapshot delete for the given repository. If this method returns * {@code true} then snapshot finalizations and deletions for the repo may be executed. Once no more operations are * ready for the repository {@link #leaveRepoLoop(String)}	we could do a possible follow-up here to remove data from partially failed clones from the repo. since clones only really fail like this due to io exceptions and since io exceptions are really unlikely unless something is really broken i'm not sure it's worth the effort though because the cleanup will probably also fail (and would happen on a subsequent delete run). since we aren't writing a new index-n (so this isn't relevant to the scalability of the repo really) and clones by their very nature add almost no bytes to the repo i think for now this is good enough.
public void setBlockOnWriteShardLevelMeta() { assert blockAndFailOnWriteShardLevelMeta == false : "Either fail or wait after blocking on shard level metadata, not both"; blockOnWriteShardLevelMeta = true; }	not great to add yet another method to this, but this case turns out to be important i guess. reproducing this like we used to via blocking and then restarting master simply did not cover this obviously broken spot.
public String execute() { return propertyValue; } }; } } catch (Exception e) { throw ConfigurationUtils.newConfigurationException(processorType, processorTag, propertyName, e); } } private static void addMetadataToException(ElasticsearchException exception, String processorType, String processorTag, String propertyName) { if (processorType != null) { exception.addMetadata("es.processor_type", processorType); } if (processorTag != null) { exception.addMetadata("es.processor_tag", processorTag); } if (propertyName != null) { exception.addMetadata("es.property_name", propertyName); } } @SuppressWarnings("unchecked") public static Processor readProcessor(Map<String, Processor.Factory> processorFactories, ScriptService scriptService, String type, Object config) throws Exception { if (config instanceof Map) { return readProcessor(processorFactories, scriptService, type, (Map<String, Object>) config); } else if (config instanceof String && "script".equals(type)) { Map<String, Object> normalizedScript = new HashMap<>(1); normalizedScript.put(ScriptType.INLINE.getParseField().getPreferredName(), config); return readProcessor(processorFactories, scriptService, type, normalizedScript); } else { throw newConfigurationException(type, null, null, "property isn't a map, but of type [" + config.getClass().getName() + "]"); } } public static Processor readProcessor(Map<String, Processor.Factory> processorFactories, ScriptService scriptService, String type, Map<String, Object> config) throws Exception { String tag = ConfigurationUtils.readOptionalStringProperty(null, null, config, TAG_KEY); Script conditionalScript = maybeExtractConditional(config); Processor.Factory factory = processorFactories.get(type); if (factory != null) { boolean ignoreFailure = ConfigurationUtils.readBooleanProperty(null, null, config, "ignore_failure", false); List<Map<String, Object>> onFailureProcessorConfigs = ConfigurationUtils.readOptionalList(null, null, config, Pipeline.ON_FAILURE_KEY); List<Processor> onFailureProcessors = readProcessorConfigs(onFailureProcessorConfigs, scriptService, processorFactories); if (onFailureProcessorConfigs != null && onFailureProcessors.isEmpty()) { throw newConfigurationException(type, tag, Pipeline.ON_FAILURE_KEY, "processors list cannot be empty"); } try { Processor processor = factory.create(processorFactories, tag, config); if (config.isEmpty() == false) { throw new ElasticsearchParseException("processor [{}] doesn't support one or more provided configuration parameters {}", type, Arrays.toString(config.keySet().toArray())); } if (onFailureProcessors.size() > 0 || ignoreFailure) { processor = new CompoundProcessor(ignoreFailure, Collections.singletonList(processor), onFailureProcessors); } if (conditionalScript != null) { processor = new ConditionalProcessor(tag, conditionalScript, scriptService, processor); } return processor; } catch (Exception e) { throw newConfigurationException(type, tag, null, e); } } throw newConfigurationException(type, tag, null, "No processor type exists with name [" + type + "]"); } private static Script maybeExtractConditional(Map<String, Object> config) throws IOException { Object scriptSource = config.remove("if"); if (scriptSource != null) { try (XContentBuilder builder = XContentBuilder.builder(JsonXContent.jsonXContent) .map(normalizeScript(scriptSource)); InputStream stream = BytesReference.bytes(builder).streamInput(); XContentParser parser = XContentType.JSON.xContent().createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) { return Script.parse(parser); } } else { return null; } }	since you are returning the script, i think this can just be called extractconditional
public String execute() { return propertyValue; } }; } } catch (Exception e) { throw ConfigurationUtils.newConfigurationException(processorType, processorTag, propertyName, e); } } private static void addMetadataToException(ElasticsearchException exception, String processorType, String processorTag, String propertyName) { if (processorType != null) { exception.addMetadata("es.processor_type", processorType); } if (processorTag != null) { exception.addMetadata("es.processor_tag", processorTag); } if (propertyName != null) { exception.addMetadata("es.property_name", propertyName); } } @SuppressWarnings("unchecked") public static Processor readProcessor(Map<String, Processor.Factory> processorFactories, ScriptService scriptService, String type, Object config) throws Exception { if (config instanceof Map) { return readProcessor(processorFactories, scriptService, type, (Map<String, Object>) config); } else if (config instanceof String && "script".equals(type)) { Map<String, Object> normalizedScript = new HashMap<>(1); normalizedScript.put(ScriptType.INLINE.getParseField().getPreferredName(), config); return readProcessor(processorFactories, scriptService, type, normalizedScript); } else { throw newConfigurationException(type, null, null, "property isn't a map, but of type [" + config.getClass().getName() + "]"); } } public static Processor readProcessor(Map<String, Processor.Factory> processorFactories, ScriptService scriptService, String type, Map<String, Object> config) throws Exception { String tag = ConfigurationUtils.readOptionalStringProperty(null, null, config, TAG_KEY); Script conditionalScript = maybeExtractConditional(config); Processor.Factory factory = processorFactories.get(type); if (factory != null) { boolean ignoreFailure = ConfigurationUtils.readBooleanProperty(null, null, config, "ignore_failure", false); List<Map<String, Object>> onFailureProcessorConfigs = ConfigurationUtils.readOptionalList(null, null, config, Pipeline.ON_FAILURE_KEY); List<Processor> onFailureProcessors = readProcessorConfigs(onFailureProcessorConfigs, scriptService, processorFactories); if (onFailureProcessorConfigs != null && onFailureProcessors.isEmpty()) { throw newConfigurationException(type, tag, Pipeline.ON_FAILURE_KEY, "processors list cannot be empty"); } try { Processor processor = factory.create(processorFactories, tag, config); if (config.isEmpty() == false) { throw new ElasticsearchParseException("processor [{}] doesn't support one or more provided configuration parameters {}", type, Arrays.toString(config.keySet().toArray())); } if (onFailureProcessors.size() > 0 || ignoreFailure) { processor = new CompoundProcessor(ignoreFailure, Collections.singletonList(processor), onFailureProcessors); } if (conditionalScript != null) { processor = new ConditionalProcessor(tag, conditionalScript, scriptService, processor); } return processor; } catch (Exception e) { throw newConfigurationException(type, tag, null, e); } } throw newConfigurationException(type, tag, null, "No processor type exists with name [" + type + "]"); } private static Script maybeExtractConditional(Map<String, Object> config) throws IOException { Object scriptSource = config.remove("if"); if (scriptSource != null) { try (XContentBuilder builder = XContentBuilder.builder(JsonXContent.jsonXContent) .map(normalizeScript(scriptSource)); InputStream stream = BytesReference.bytes(builder).streamInput(); XContentParser parser = XContentType.JSON.xContent().createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) { return Script.parse(parser); } } else { return null; } }	no need for an else, it can just be outside the if
public Builder indexPrefixes(int minChars, int maxChars) { this.prefixFieldType = new PrefixFieldType(name() + "..prefix", minChars, maxChars); fieldType().setPrefixFieldType(this.prefixFieldType); return this; }	should we validate that minchars <= maxchars?
@Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName) { if (fielddata == false) { throw new IllegalArgumentException("Fielddata is disabled on text fields by default. Set fielddata=true on [" + name() + "] in order to load fielddata in memory by uninverting the inverted index. Note that this can however " + "use significant memory. Alternatively use a keyword field instead."); } return new PagedBytesIndexFieldData.Builder(fielddataMinFrequency, fielddataMaxFrequency, fielddataMinSegmentSize); } } private int positionIncrementGap; private PrefixFieldMapper prefixFieldMapper; protected TextFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, int positionIncrementGap, PrefixFieldMapper prefixFieldMapper, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); assert fieldType.tokenized(); assert fieldType.hasDocValues() == false; if (fieldType().indexOptions() == IndexOptions.NONE && fieldType().fielddata()) { throw new IllegalArgumentException("Cannot enable fielddata on a [text] field that is not indexed: [" + name() + "]"); } this.positionIncrementGap = positionIncrementGap; this.prefixFieldMapper = prefixFieldMapper; } @Override protected TextFieldMapper clone() { return (TextFieldMapper) super.clone(); } public int getPositionIncrementGap() { return this.positionIncrementGap; } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { final String value; if (context.externalValueSet()) { value = context.externalValue().toString(); } else { value = context.parser().textOrNull(); } if (value == null) { return; } if (fieldType().indexOptions() != IndexOptions.NONE || fieldType().stored()) { Field field = new Field(fieldType().name(), value, fieldType()); fields.add(field); if (fieldType().omitNorms()) { createFieldNamesField(context, fields); } if (prefixFieldMapper != null) { prefixFieldMapper.addField(value, fields); } } } @Override public Iterator<Mapper> iterator() { if (prefixFieldMapper == null) return super.iterator(); return Iterators.concat(multiFields.iterator(), Collections.singleton(prefixFieldMapper).iterator()); } @Override protected String contentType() { return CONTENT_TYPE; } @Override protected void doMerge(Mapper mergeWith, boolean updateAllTypes) { super.doMerge(mergeWith, updateAllTypes); } @Override public TextFieldType fieldType() { return (TextFieldType) super.fieldType(); } @Override protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException { super.doXContentBody(builder, includeDefaults, params); doXContentAnalyzers(builder, includeDefaults); if (includeDefaults || positionIncrementGap != POSITION_INCREMENT_GAP_USE_ANALYZER) { builder.field("position_increment_gap", positionIncrementGap); } if (includeDefaults || fieldType().fielddata() != ((TextFieldType) defaultFieldType).fielddata()) { builder.field("fielddata", fieldType().fielddata()); } if (fieldType().fielddata()) { if (includeDefaults || fieldType().fielddataMinFrequency() != Defaults.FIELDDATA_MIN_FREQUENCY || fieldType().fielddataMaxFrequency() != Defaults.FIELDDATA_MAX_FREQUENCY || fieldType().fielddataMinSegmentSize() != Defaults.FIELDDATA_MIN_SEGMENT_SIZE) { builder.startObject("fielddata_frequency_filter"); if (includeDefaults || fieldType().fielddataMinFrequency() != Defaults.FIELDDATA_MIN_FREQUENCY) { builder.field("min", fieldType().fielddataMinFrequency()); } if (includeDefaults || fieldType().fielddataMaxFrequency() != Defaults.FIELDDATA_MAX_FREQUENCY) { builder.field("max", fieldType().fielddataMaxFrequency()); } if (includeDefaults || fieldType().fielddataMinSegmentSize() != Defaults.FIELDDATA_MIN_SEGMENT_SIZE) { builder.field("min_segment_size", fieldType().fielddataMinSegmentSize()); } builder.endObject(); } } if (fieldType().prefixFieldType != null) { fieldType().prefixFieldType.doXContent(builder); }	maybe replace multifields.iterator() with super.iterator() to be more future-proof in case the super impl is updated in the future?
private ClusterState applyCreateIndexRequestWithV2Template(final ClusterState currentState, final CreateIndexClusterStateUpdateRequest request, final boolean silent, final String templateName, final BiConsumer<Metadata.Builder, IndexMetadata> metadataTransformer) throws Exception { logger.debug("applying create index request using the composable template [{}]", templateName); final Map<String, Object> mappings = resolveV2Mappings(request.mappings(), currentState, templateName, xContentRegistry); final Settings aggregatedIndexSettings = aggregateIndexSettings(currentState, request, MetadataIndexTemplateService.resolveSettings(currentState.metadata(), templateName), mappings, null, settings, indexScopedSettings); int routingNumShards = getIndexNumberOfRoutingShards(aggregatedIndexSettings, null); IndexMetadata tmpImd = buildAndValidateTemporaryIndexMetadata(currentState, aggregatedIndexSettings, request, routingNumShards); return applyCreateIndexWithTemporaryService(currentState, request, silent, null, tmpImd, mappings, indexService -> resolveAndValidateAliases(request.index(), request.aliases(), MetadataIndexTemplateService.resolveAliases(currentState.metadata(), templateName), currentState.metadata(), aliasValidator, // the context is only used for validation so it's fine to pass fake values for the // shard id and the current timestamp xContentRegistry, indexService.newQueryShardContext(0, null, () -> 0L, null)), Collections.singletonList(templateName), metadataTransformer); }	i think we can drop the "the" suggestion logger.debug("applying create index request using composable template [{}]", templatename);
public void testTranslogOpSerialization() throws Exception { BytesReference B_1 = new BytesArray(new byte[]{1}); SeqNoFieldMapper.SequenceID seqID = SeqNoFieldMapper.SequenceID.emptySeqID(); long randomSeqNum = randomBoolean() ? SequenceNumbersService.UNASSIGNED_SEQ_NO : randomNonNegativeLong(); long randomPrimaryTerm = randomBoolean() ? 0 : randomNonNegativeLong(); seqID.seqNo.setLongValue(randomSeqNum); seqID.seqNoDocValue.setLongValue(randomSeqNum); seqID.primaryTerm.setLongValue(randomPrimaryTerm); Field uidField = new Field("_uid", "1", UidFieldMapper.Defaults.FIELD_TYPE); Field versionField = new NumericDocValuesField("_version", 1); Document document = new Document(); document.add(new TextField("value", "test", Field.Store.YES)); document.add(uidField); document.add(versionField); document.add(seqID.seqNo); document.add(seqID.seqNoDocValue); document.add(seqID.primaryTerm); ParsedDocument doc = new ParsedDocument(versionField, seqID, "1", "type", null, Arrays.asList(document), B_1, null); Engine.Index eIndex = new Engine.Index(newUid("1"), doc, randomSeqNum, randomPrimaryTerm, 1, VersionType.INTERNAL, Origin.PRIMARY, 0, 0, false); Engine.IndexResult eIndexResult = new Engine.IndexResult(1, randomSeqNum, true); Translog.Index index = new Translog.Index(eIndex, eIndexResult); BytesStreamOutput out = new BytesStreamOutput(); index.writeTo(out); StreamInput in = out.bytes().streamInput(); Translog.Index serializedIndex = new Translog.Index(in); assertEquals(index, serializedIndex); Engine.Delete eDelete = new Engine.Delete("type", "1", newUid("1"), randomSeqNum, randomPrimaryTerm, 2, VersionType.INTERNAL, Origin.PRIMARY, 0); Engine.DeleteResult eDeleteResult = new Engine.DeleteResult(2, randomSeqNum, true); Translog.Delete delete = new Translog.Delete(eDelete, eDeleteResult); out = new BytesStreamOutput(); delete.writeTo(out); in = out.bytes().streamInput(); Translog.Delete serializedDelete = new Translog.Delete(in); assertEquals(delete, serializedDelete); }	can we add a comment this is only relevant for a bwc perspective and can be removed in es 7.0?
public void testAddUtf8String() throws Exception { KeyStoreWrapper.create().save(env.configFile(), new char[0]); byte[] bytes = new byte[randomIntBetween(8, 16)]; new Random().nextBytes(bytes); String secretValue = new String(bytes, StandardCharsets.UTF_8); setInput(secretValue); execute("-x", "foo"); assertSecureString("foo", secretValue); }	this won't guarantee any codepoints above 127. i think we should always ensure that, so that this test cannot randomly pass if the copying code you fixed is broken?
@Override public Map<String, Processor.Factory> getProcessors(Processor.Parameters parameters) { ingestService.set(parameters.ingestService); long cacheSize = CACHE_SIZE.get(parameters.env.settings()); GeoIpCache geoIpCache = new GeoIpCache(cacheSize); DatabaseRegistry registry = new DatabaseRegistry( parameters.env, parameters.client, geoIpCache, parameters.genericExecutor ); databaseRegistry.set(registry); return Map.of(GeoIpProcessor.TYPE, new GeoIpProcessor.Factory(registry)); }	nit: suggestion databaseregistry registry = new databaseregistry(parameters.env, parameters.client, geoipcache, parameters.genericexecutor);
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); final File[] files = fileDetails().toArray(new File[0]); out.writeVInt(files.length); for (File file : files) { file.writeTo(out); } out.writeLong(sourceThrottlingInNanos); out.writeLong(targetThrottleTimeInNanos); }	good catch. i think the right solution is to synchronize the method , just as all other methods are. can you also do it to the readfrom? this will also protect the out.writelong(sourcethrottlinginnanos); lines
@Test public void testConcurrentAccessToIndexFileDetails() throws InterruptedException { final Index index = new Index(); final CountDownLatch latch = new CountDownLatch(1); final BytesStreamOutput out = new BytesStreamOutput(); final AtomicBoolean stop = new AtomicBoolean(false); Thread writeThread = new Thread() { public void run() { try { latch.await(); } catch (InterruptedException e) { fail("interrupted while waiting to start write thread" + e.getMessage()); } while (stop.get() == false) { try { index.writeTo(out); } catch (IOException e) { fail("could not write index " + e.getMessage()); } } } }; Thread modifyThread = new Thread() { public void run() { try { latch.await(); } catch (InterruptedException e) { fail("interrupted while waiting to start modify thread" + e.getMessage()); } for (int i = 0; i < 100; i++) { index.addFileDetail(randomAsciiOfLength(10), 100, true); } stop.set(true); } }; writeThread.start(); modifyThread.start(); latch.countDown(); writeThread.join(); modifyThread.join(); }	i wonder why the testindex that has a concurrent reader didn't fail?
private static List<PhaseProgress> readProgressFromLegacy(DataFrameAnalyticsState state, StreamInput in) throws IOException { Integer legacyProgressPercent = in.readOptionalInt(); if (legacyProgressPercent == null) { return Collections.emptyList(); } int reindexingProgress = 0; int analyzingProgress = 0; switch (state) { case ANALYZING: analyzingProgress = legacyProgressPercent; break; case REINDEXING: reindexingProgress = legacyProgressPercent; break; case STARTED: case STOPPED: case STOPPING: default: return null; } return Arrays.asList( new PhaseProgress("reindexing", reindexingProgress), new PhaseProgress("loading_data", 100), new PhaseProgress("analyzing", analyzingProgress), new PhaseProgress("writing_results", 0)); }	#nit, it seems to me that loading_data would not be 100 unless it was currently in the analyzing state. though, this does not really matter too much.
void gatherStatsForStoppedTasks(List<String> expandedIds, GetDataFrameAnalyticsStatsAction.Response runningTasksResponse, ActionListener<GetDataFrameAnalyticsStatsAction.Response> listener) { List<String> stoppedTasksIds = determineStoppedTasksIds(expandedIds, runningTasksResponse.getResponse().results()); if (stoppedTasksIds.isEmpty()) { listener.onResponse(runningTasksResponse); return; } AtomicInteger counter = new AtomicInteger(stoppedTasksIds.size()); AtomicArray<Stats> stoppedStats = new AtomicArray<>(stoppedTasksIds.size()); for (int i = 0; i < stoppedTasksIds.size(); i++) { int slot = i; String analyticsId = stoppedTasksIds.get(i); searchStoredProgress(analyticsId, ActionListener.wrap( progress -> { stoppedStats.set(slot, buildStats(analyticsId, progress)); if (counter.decrementAndGet() == 0) { List<Stats> allTasksStats = new ArrayList<>(runningTasksResponse.getResponse().results()); allTasksStats.addAll(stoppedStats.asList()); Collections.sort(allTasksStats, Comparator.comparing(Stats::getId)); listener.onResponse(new GetDataFrameAnalyticsStatsAction.Response(new QueryPage<>( allTasksStats, allTasksStats.size(), GetDataFrameAnalyticsAction.Response.RESULTS_FIELD))); } }, listener::onFailure )); } }	i think it would be better to have a single query that gathers all the stored progresses for all the stopped analytics processes. the only concern would be query size, but i think we are restricted to 10k already given the id expansion logic.
void gatherStatsForStoppedTasks(List<String> expandedIds, GetDataFrameAnalyticsStatsAction.Response runningTasksResponse, ActionListener<GetDataFrameAnalyticsStatsAction.Response> listener) { List<String> stoppedTasksIds = determineStoppedTasksIds(expandedIds, runningTasksResponse.getResponse().results()); if (stoppedTasksIds.isEmpty()) { listener.onResponse(runningTasksResponse); return; } AtomicInteger counter = new AtomicInteger(stoppedTasksIds.size()); AtomicArray<Stats> stoppedStats = new AtomicArray<>(stoppedTasksIds.size()); for (int i = 0; i < stoppedTasksIds.size(); i++) { int slot = i; String analyticsId = stoppedTasksIds.get(i); searchStoredProgress(analyticsId, ActionListener.wrap( progress -> { stoppedStats.set(slot, buildStats(analyticsId, progress)); if (counter.decrementAndGet() == 0) { List<Stats> allTasksStats = new ArrayList<>(runningTasksResponse.getResponse().results()); allTasksStats.addAll(stoppedStats.asList()); Collections.sort(allTasksStats, Comparator.comparing(Stats::getId)); listener.onResponse(new GetDataFrameAnalyticsStatsAction.Response(new QueryPage<>( allTasksStats, allTasksStats.size(), GetDataFrameAnalyticsAction.Response.RESULTS_FIELD))); } }, listener::onFailure )); } }	as commented above, i think a single query for all progress docs would be preferable, you should be able to parse them back out correctly given each hit's doc id.
private void sendShardChangesRequest(long from, int maxOperationCount, long maxRequiredSeqNo, AtomicInteger retryCounter) { final long startTime = relativeTimeProvider.getAsLong(); synchronized (this) { lastFetchTime = startTime; } innerSendShardChangesRequest(from, maxOperationCount, response -> { synchronized (ShardFollowNodeTask.this) { // Always clear fetch exceptions: fetchExceptions.remove(from); if (response.getOperations().length > 0) { // do not count polls against fetch stats totalReadRemoteExecTimeMillis += response.getTookInMillis(); totalReadTimeMillis += TimeUnit.NANOSECONDS.toMillis(relativeTimeProvider.getAsLong() - startTime); successfulReadRequests++; operationsRead += response.getOperations().length; bytesRead += Arrays.stream(response.getOperations()).mapToLong(Translog.Operation::estimateSize).sum(); } } handleReadResponse(from, maxRequiredSeqNo, response); }, e -> { synchronized (ShardFollowNodeTask.this) { totalReadTimeMillis += TimeUnit.NANOSECONDS.toMillis(relativeTimeProvider.getAsLong() - startTime); failedReadRequests++; fetchExceptions.put(from, Tuple.tuple(retryCounter, ExceptionsHelper.convertToElastic(e))); } if (e instanceof ElasticsearchException) { ElasticsearchException elasticsearchException = (ElasticsearchException) e; if (elasticsearchException.getMetadataKeys().contains(Ccr.REQUESTED_OPS_MISSING_METADATA_KEY)) { handleFallenBehindLeaderShard(e, from, maxOperationCount, maxRequiredSeqNo, retryCounter); return; } } handleFailure(e, retryCounter, () -> sendShardChangesRequest(from, maxOperationCount, maxRequiredSeqNo, retryCounter)); }); }	i have been working on this pr locally with some other work of mine. when the listener is called, the exception is wrapped in a remotetransportexception. so the check if the metadata is present fails even if the underlying exception has the metadata. i think that this needs to be unwrapped further. blocking on a future unwraps the exception which is why your test passes (opposed to the listener infrastructure).
public TransformConfig createConfig() { TransformConfig.Builder transformConfigBuilder = new TransformConfig.Builder(); addCommonBuilderParameters(transformConfigBuilder); transformConfigBuilder.setSource(new SourceConfig(CONTINUOUS_EVENTS_SOURCE_INDEX)); transformConfigBuilder.setDest(new DestConfig(NAME, INGEST_PIPELINE)); transformConfigBuilder.setId(NAME); PivotConfig.Builder pivotConfigBuilder = new PivotConfig.Builder(); pivotConfigBuilder.setGroups( new GroupConfig.Builder().groupBy( "event", new TermsGroupSource.Builder().setField(termsField).setMissingBucket(missing).build() ).build() ); AggregatorFactories.Builder aggregations = new AggregatorFactories.Builder(); addCommonAggregations(aggregations); aggregations.addAggregator(AggregationBuilders.avg("metric.avg").field(metricField)); pivotConfigBuilder.setAggregations(aggregations); transformConfigBuilder.setPivotConfig(pivotConfigBuilder.build()); return transformConfigBuilder.build(); }	shouldn't this be termsfield?
public <Response> void execute(NodeListenerCallback<Response> callback, ActionListener<Response> listener) { List<DiscoveryNode> nodes; synchronized (mutex) { if (closed) { throw new IllegalStateException("transport client is closed"); } nodes = this.nodes; } ensureNodesAreAvailable(nodes); int index = getNodeNumber(); RetryListener<Response> retryListener = new RetryListener<>(callback, listener, nodes, index); DiscoveryNode node = nodes.get((index) % nodes.size()); try { callback.doWithNode(node, retryListener); } catch (Throwable t) { //this exception can't come from the TransportService as it doesn't throw exception at all listener.onFailure(t); } }	why do we need to sync on a mutex here? closed is volatile and nodes as well?
private IndexingStrategy planIndexingAsPrimary(Index index) throws IOException { assert index.origin() == Operation.Origin.PRIMARY : "planing as primary but origin isn't. got " + index.origin(); final IndexingStrategy plan; // resolve an external operation into an internal one which is safe to replay final boolean canOptimizeAddDocument = canOptimizeAddDocument(index); if (canOptimizeAddDocument && mayHaveBeenIndexedBefore(index) == false) { plan = IndexingStrategy.optimizedAppendOnly(1L); } else { versionMap.enforceSafeAccess(); // resolves incoming version final VersionValue versionValue = resolveDocVersion(index, index.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO); final long currentVersion; final boolean currentNotFoundOrDeleted; if (versionValue == null) { currentVersion = Versions.NOT_FOUND; currentNotFoundOrDeleted = true; } else { currentVersion = versionValue.version; currentNotFoundOrDeleted = versionValue.isDelete(); } if (index.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO && currentNotFoundOrDeleted) { final VersionConflictEngineException e = new VersionConflictEngineException(shardId, index.id(), index.getIfSeqNo(), index.getIfPrimaryTerm(), SequenceNumbers.UNASSIGNED_SEQ_NO, SequenceNumbers.UNASSIGNED_PRIMARY_TERM); plan = IndexingStrategy.skipDueToVersionConflict(e, true, currentVersion); } else if (index.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO && ( versionValue.seqNo != index.getIfSeqNo() || versionValue.term != index.getIfPrimaryTerm() )) { final VersionConflictEngineException e = new VersionConflictEngineException(shardId, index.id(), index.getIfSeqNo(), index.getIfPrimaryTerm(), versionValue.seqNo, versionValue.term); plan = IndexingStrategy.skipDueToVersionConflict(e, currentNotFoundOrDeleted, currentVersion); } else if (index.versionType().isVersionConflictForWrites( currentVersion, index.version(), currentNotFoundOrDeleted)) { final VersionConflictEngineException e = new VersionConflictEngineException(shardId, index, currentVersion, currentNotFoundOrDeleted); plan = IndexingStrategy.skipDueToVersionConflict(e, currentNotFoundOrDeleted, currentVersion); } else { plan = IndexingStrategy.processNormally(currentNotFoundOrDeleted, canOptimizeAddDocument ? 1L : index.versionType().updateVersion(currentVersion, index.version()) ); } } return plan; }	should we use versions.not_found instead of currentversion (line 1039) to avoid leaking the version of the deleted document?
* @param threadLocal whether this stream will only be used on the current thread or not * @return decompressing stream */ public static InputStream inputStream(InputStream in, boolean threadLocal) throws IOException { final byte[] headerBytes = new byte[HEADER.length]; int len = 0; while (len < headerBytes.length) { final int read = in.read(headerBytes, len, headerBytes.length - len); if (read == -1) { break; } len += read; } if (len != HEADER.length || Arrays.equals(headerBytes, HEADER) == false) { throw new IllegalArgumentException("Input stream is not compressed with DEFLATE!"); } final Releasable releasable; final Inflater inflater; if (threadLocal) { final ReleasableReference<Inflater> current = inflaterForStreamRef.get(); if (current.inUse) { // Nested de-compression streams should not happen but we still handle them safely by using a fresh Inflater inflater = new Inflater(true); releasable = inflater::end; } else { inflater = current.get(); releasable = current; } } else { inflater = new Inflater(true); releasable = inflater::end; } return new BufferedInputStream(new InflaterInputStream(in, inflater, BUFFER_SIZE) { @Override public void close() throws IOException { try { super.close(); } finally { // We are ensured to only call this once since we wrap this stream in a BufferedInputStream that will only close // its delegate once releasable.close(); } } }, BUFFER_SIZE); }	need this flag still and move the super.close into the flag check also due to a java8 bug: https://bugs.openjdk.java.net/browse/jdk-8054565 that otherwise forces another .finish on the underlying stream after the deflater has already been reset (thus adding another 5 bytes to it and breaking things).
public void testFollowNonExistingLeaderIndex() throws Exception { assumeFalse("Test should only run when both clusters are running", runningAgainstLeaderCluster); ResponseException e = expectThrows(ResponseException.class, () -> followIndex("leader_cluster:non-existing-index", "non-existing-index")); assertThat(e.getMessage(), containsString("no such index")); assertThat(e.getResponse().getStatusLine().getStatusCode(), equalTo(404)); e = expectThrows(ResponseException.class, () -> createAndFollowIndex("leader_cluster:non-existing-index", "non-existing-index")); assertThat(e.getMessage(), containsString("no such index")); assertThat(e.getResponse().getStatusLine().getStatusCode(), equalTo(404)); }	i would have preferred an unit test here, but the behavior between following a local / remote index is different and the ccrlicensechecker fetch the leader index metadata and then immediately fetch history uuids, which caused a npe if leader index metadata is null.
@Override public double value(String name) { if ("sum_of_squares".equals(name)) { return sumOfSqrs; } if ("variance".equals(name)) { return getVariance(); } if ("variance_population".equals(name)) { return getVariancePopulation(); } if ("variance_sampling".equals(name)) { return getVarianceSampling(); } if ("std_deviation".equals(name)) { return getStdDeviation(); } if ("std_deviation_population".equals(name)) { return getStdDeviationPopulation(); } if ("std_deviation_sampling".equals(name)) { return getStdDeviationSampling(); } if ("std_upper".equals(name)) { return getStdDeviationBound(Bounds.UPPER); } if ("std_lower".equals(name)) { return getStdDeviationBound(Bounds.LOWER); } if ("std_upper_population".equals(name)) { return getStdDeviationBound(Bounds.UPPER_POPULATION); } if ("std_lower_population".equals(name)) { return getStdDeviationBound(Bounds.LOWER_POPULATION); } if ("std_upper_sampling".equals(name)) { return getStdDeviationBound(Bounds.UPPER_SAMPLING); } if ("std_lower_sampling".equals(name)) { return getStdDeviationBound(Bounds.LOWER_SAMPLING); } return super.value(name); }	i'd feel better if we returned an immutable version or a copy here. or maybe just make metric_names immutable to begin with.
protected <T extends AggregationBuilder, V extends InternalAggregation> void verifyOutputFieldNames(T aggregationBuilder, V agg) throws IOException { if (aggregationBuilder.getOutputFieldNames().isEmpty()) { // aggregation does not support output field names yet return; } if (agg instanceof NumericMetricsAggregation.MultiValue) { NumericMetricsAggregation.MultiValue multiValueAgg = (NumericMetricsAggregation.MultiValue) agg; Set<String> valueNames = new HashSet<>(); for (String name : multiValueAgg.valueNames()) { valueNames.add(name); } assertEquals(aggregationBuilder.getOutputFieldNames().get(), valueNames); } } /** * Override to wrap the {@linkplain DirectoryReader} for aggs like * {@link NestedAggregationBuilder}	why do we need this instanceof check?
@Override protected void innerParseCreateField(ParseContext context, List<Field> fields) throws IOException { double value; float boost = this.boost; if (context.externalValueSet()) { Object externalValue = context.externalValue(); if (externalValue == null) { if (nullValue == null) { return; } value = nullValue; } else if (externalValue instanceof String) { String sExternalValue = (String) externalValue; if (sExternalValue.length() == 0) { if (nullValue == null) { return; } value = nullValue; } else { value = Double.parseDouble(sExternalValue); } } else { value = ((Number) externalValue).doubleValue(); } if (context.includeInAll(includeInAll, this)) { context.allEntries().addText(names.fullName(), Double.toString(value), boost); } } else { XContentParser parser = context.parser(); if (parser.currentToken() == XContentParser.Token.VALUE_NULL || (parser.currentToken() == XContentParser.Token.VALUE_STRING && parser.textLength() == 0)) { if (nullValue == null) { return; } value = nullValue; if (nullValueAsString != null && (context.includeInAll(includeInAll, this))) { context.allEntries().addText(names.fullName(), nullValueAsString, boost); } } else if (parser.currentToken() == XContentParser.Token.START_OBJECT) { XContentParser.Token token; String currentFieldName = null; Double objValue = nullValue; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else { if ("value".equals(currentFieldName) || "_value".equals(currentFieldName)) { if (parser.currentToken() != XContentParser.Token.VALUE_NULL) { objValue = parser.doubleValue(); } } else if ("boost".equals(currentFieldName) || "_boost".equals(currentFieldName)) { boost = parser.floatValue(); } else { throw new ElasticSearchIllegalArgumentException("unknown property [" + currentFieldName + "]"); } } } if (objValue == null) { // no value return; } value = objValue; } else { value = parser.doubleValue(); if (context.includeInAll(includeInAll, this)) { context.allEntries().addText(names.fullName(), parser.text(), boost); } } } if (fieldType.indexed() || fieldType.stored()) { CustomDoubleNumericField field = new CustomDoubleNumericField(this, value, fieldType); field.setBoost(boost); fields.add(field); } if (hasDocValues() && !Double.isNaN(value)) { CustomDoubleNumericDocValuesField field = context.doc().getField(names().indexName(), CustomDoubleNumericDocValuesField.class); if (field != null) { field.add(value); } else { field = new CustomDoubleNumericDocValuesField(names().indexName(), value); context.doc().add(field); } } }	hmm what happens if this is nan?
@Override public String numericAsString() { return Double.toString(number); } } public static class CustomDoubleNumericDocValuesField extends CustomNumericDocValuesField { public static final FieldType TYPE = new FieldType(); static { TYPE.setDocValueType(FieldInfo.DocValuesType.BINARY); TYPE.freeze(); } private final DoubleArrayList values; public CustomDoubleNumericDocValuesField(String name, double value) { super(name); values = new DoubleArrayList(); add(value); } public void add(double value) { values.add(value); } @Override public BytesRef binaryValue() { // sort new InPlaceMergeSorter() { @Override protected void swap(int i, int j) { final double tmp = values.get(i); values.set(i, values.get(j)); values.set(j, tmp); } @Override protected int compare(int i, int j) { return Double.compare(values.get(i), values.get(j)); } }.sort(0, values.size()); // deduplicate int numUniqueValues = values.isEmpty() ? 0 : 1; for (int i = 1; i < values.size(); ++i) { if (values.get(i) != values.get(i - 1)) { values.set(numUniqueValues++, values.get(i)); } } values.resize(numUniqueValues); final byte[] bytes = new byte[numUniqueValues * 8]; for (int i = 0; i < values.size(); ++i) { ByteUtils.writeDoubleLE(values.get(i), bytes, i * 8); } return new BytesRef(bytes); }	i wonder if those in place sorting could be in a utils class i can see that being used elsewhere and the code would look simpler here?
private TransformProgress getProgress(Function function, SearchRequest searchRequest) throws Exception { CountDownLatch latch = new CountDownLatch(1); final AtomicReference<TransformProgress> progressHolder = new AtomicReference<>(); try (RestHighLevelClient restClient = new TestRestHighLevelClient()) { SearchResponse response = restClient.search(searchRequest, RequestOptions.DEFAULT); function.getInitialProgressFromResponse( response, new LatchedActionListener<>( ActionListener.wrap(progressHolder::set, e -> { fail("got unexpected exception: " + e); }), latch ) ); } assertTrue("timed out after 20s", latch.await(20, TimeUnit.SECONDS)); return progressHolder.get(); }	if this is running in a different thread it will not fail the test as the exception does not propagate to the test runner. you need to set the exception to an atomic reference then check after the latch in the test thread.
@Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { if (waitForSnapshot) { logger.trace("adding snapshot completion listener to wait for deleted snapshot to finish"); addListener(snapshot, ActionListener.wrap( snapshotInfo -> { logger.debug("deleted snapshot completed - deleting files"); threadPool.executor(ThreadPool.Names.SNAPSHOT).execute(() -> { try { deleteSnapshot(snapshot.getRepository(), snapshot.getSnapshotId().getName(), listener, true); } catch (Exception ex) { logger.warn(() -> new ParameterizedMessage("[{}] failed to delete snapshot", snapshot), ex); } } ); }, e -> { if (abortedDuringInit) { logger.debug(() -> new ParameterizedMessage("Snapshot [{}] was aborted during INIT", snapshot), e); listener.onResponse(null); } else { if (ExceptionsHelper.unwrap(e, NotMasterException.class, FailedToCommitClusterStateException.class) != null) { logger.warn("master failover before deleted snapshot could complete", e); // Just pass the exception to the transport handler as is so it is retried on the new master listener.onFailure(e); } else { logger.warn("deleted snapshot failed", e); listener.onFailure( new SnapshotMissingException(snapshot.getRepository(), snapshot.getSnapshotId(), e)); } } } )); } else { logger.debug("deleted snapshot is not running - deleting files"); deleteSnapshotFromRepository(snapshot, listener, repositoryStateId, newState.nodes().getMinNodeVersion()); } } }); }	i think i would rather always bubble up the original exception, marking the deletion as failed (and have the client retry). this listener here can be called in a range of situations, and i don't think that in all cases it denotes that the snapshot has been deleted or fully aborted (especially because with waitforsnapshot we are supposed to wait until the snapshot has truly completed, whether exceptional or not).
@Override public int size() { return in.size(); } } public static void sort(final BytesRefArray bytes, final int[] indices) { sort(new BytesRefBuilder(), new BytesRefBuilder(), bytes, indices); } private static void sort(final BytesRefBuilder scratch, final BytesRefBuilder scratch1, final BytesRefArray bytes, final int[] indices) { final int numValues = bytes.size(); assert indices.length >= numValues; if (numValues > 1) { new InPlaceMergeSorter() { final Comparator<BytesRef> comparator = Comparator.naturalOrder(); @Override protected int compare(int i, int j) { return comparator.compare(bytes.get(scratch, indices[i]), bytes.get(scratch1, indices[j])); } @Override protected void swap(int i, int j) { int value_i = indices[i]; indices[i] = indices[j]; indices[j] = value_i; } }.sort(0, numValues); } } public static int sortAndDedup(final BytesRefArray bytes, final int[] indices) { final BytesRefBuilder scratch = new BytesRefBuilder(); final BytesRefBuilder scratch1 = new BytesRefBuilder(); final int numValues = bytes.size(); assert indices.length >= numValues; if (numValues <= 1) { return numValues; } sort(scratch, scratch1, bytes, indices); int uniqueCount = 1; BytesRefBuilder previous = scratch; BytesRefBuilder current = scratch1; bytes.get(previous, indices[0]); for (int i = 1; i < numValues; ++i) { bytes.get(current, indices[i]); if (!previous.get().equals(current.get())) { indices[uniqueCount++] = indices[i]; } BytesRefBuilder tmp = previous; previous = current; current = tmp; } return uniqueCount; } @SuppressWarnings("unchecked") public static <E> ArrayList<E> iterableAsArrayList(Iterable<? extends E> elements) { if (elements == null) { throw new NullPointerException("elements"); } if (elements instanceof Collection) { return new ArrayList<>((Collection) elements); } else { ArrayList<E> list = new ArrayList<>(); for (E element : elements) { list.add(element); } return list; } } @SafeVarargs @SuppressWarnings("varargs") public static <E> ArrayList<E> arrayAsArrayList(E... elements) { if (elements == null) { throw new NullPointerException("elements"); } return new ArrayList<>(Arrays.asList(elements)); } @SafeVarargs @SuppressWarnings("varargs") public static <E> ArrayList<E> asArrayList(E first, E... other) { if (other == null) { throw new NullPointerException("other"); } ArrayList<E> list = new ArrayList<>(1 + other.length); list.add(first); list.addAll(Arrays.asList(other)); return list; } @SafeVarargs @SuppressWarnings("varargs") public static<E> ArrayList<E> asArrayList(E first, E second, E... other) { if (other == null) { throw new NullPointerException("other"); } ArrayList<E> list = new ArrayList<>(1 + 1 + other.length); list.add(first); list.add(second); list.addAll(Arrays.asList(other)); return list; } /** * Creates a copy of the given collection with the given element appended. * * @param collection collection to copy * @param element element to append */ @SuppressWarnings("unchecked") public static <E> List<E> appendToCopy(Collection<E> collection, E element) { final int size = collection.size() + 1; final E[] array = collection.toArray((E[]) new Object[size]); array[size - 1] = element; return Arrays.asList(array); } public static <E> ArrayList<E> newSingletonArrayList(E element) { return new ArrayList<>(Collections.singletonList(element)); } public static <E> List<List<E>> eagerPartition(List<E> list, int size) { if (list == null) { throw new NullPointerException("list"); } if (size <= 0) { throw new IllegalArgumentException("size <= 0"); } List<List<E>> result = new ArrayList<>((int) Math.ceil(list.size() / size)); List<E> accumulator = new ArrayList<>(size); int count = 0; for (E element : list) { if (count == size) { result.add(accumulator); accumulator = new ArrayList<>(size); count = 0; } accumulator.add(element); count++; } if (count > 0) { result.add(accumulator); }	i think this can be written more succinctly and without the unchecked warning? return stream.concat(collection.stream(), stream.of(element)).collect(collectors.tolist())
public PutMappingRequest source(Map<String, ?> mappingSource) { try { XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON); builder.map(mappingSource); return source(BytesReference.bytes(builder), builder.contentType()); } catch (IOException e) { throw new ElasticsearchGenerationException("Failed to generate [" + mappingSource + "]", e); } }	this change does not hurt, but isn't it ok anyways given that we create the builder ourselves and we use json as content-type?
public void testShrinkThenSplitWithFailedNode() throws Exception { internalCluster().ensureAtLeastNumDataNodes(2); final String shrinkNode = internalCluster().startDataOnlyNode(); final int shardCount = between(2, 5); prepareCreate("original").setSettings(Settings.builder().put(indexSettings()) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, shardCount)).get(); client().admin().indices().prepareFlush("original").get(); ensureGreen(); client().admin().indices().prepareUpdateSettings("original") .setSettings(Settings.builder() .put(IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getConcreteSettingForNamespace("_name").getKey(), shrinkNode) .put(IndexMetaData.SETTING_BLOCKS_WRITE, true)).get(); ensureGreen(); assertAcked(client().admin().indices().prepareResizeIndex("original", "shrunk").setSettings(Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .putNull(IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getConcreteSettingForNamespace("_name").getKey()) .build()).setResizeType(ResizeType.SHRINK).get()); ensureGreen(); final int nodeCount = cluster().size(); internalCluster().stopRandomNode(InternalTestCluster.nameFilter(shrinkNode)); ensureStableCluster(nodeCount -1); // demonstrate that the index.routing.allocation.initial_recovery setting from the shrink doesn't carry over into the split index, // because this would cause the shrink to fail as the initial_recovery node is no longer present. logger.info("--> executing split"); assertAcked(client().admin().indices().prepareResizeIndex("shrunk", "splitagain").setSettings(Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, shardCount) .putNull(IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getConcreteSettingForNamespace("_name").getKey()) .build()).setResizeType(ResizeType.SPLIT)); ensureGreen("splitagain"); }	nit: formatting of the -1 is missing a space - 1 :)
public void testMismatch() throws IOException { GeoBoundsAggregationBuilder geoBoundsAggregationBuilder = new GeoBoundsAggregationBuilder("histo").field("bar"); DateHistogramAggregationBuilder histoBuilder = new DateHistogramAggregationBuilder("histo") .field("bar").interval(100); FilterAggregationBuilder filterBuilder = new FilterAggregationBuilder("filter", new TermQueryBuilder("foo", "bar")); filterBuilder.subAggregation(histoBuilder); MappedFieldType fieldType = new NumberFieldMapper.NumberFieldType(NumberFieldMapper.NumberType.LONG); List<InternalAggregation> responses = doQueries(new MatchAllDocsQuery(), iw -> { iw.addDocument(singleton(new NumericDocValuesField("number", 7))); iw.addDocument(singleton(new NumericDocValuesField("number", 2))); iw.addDocument(singleton(new NumericDocValuesField("number", 3))); }, geoBoundsAggregationBuilder, iw -> { iw.addDocument(singleton(new NumericDocValuesField("number", 7))); iw.addDocument(singleton(new NumericDocValuesField("number", 2))); iw.addDocument(singleton(new NumericDocValuesField("number", 3))); }, filterBuilder, new MappedFieldType[]{fieldType}, new MappedFieldType[]{fieldType}); // TODO SearchResponse.Clusters is not public, using null for now. Should fix upstream. MultiSearchResponse.Item unrolledItem = new MultiSearchResponse.Item(new SearchResponse( new InternalSearchResponse(null, new InternalAggregations(Collections.singletonList(responses.get(0))), null, null, false, false, 1), null, 1, 1, 0, 10, null, null), null); MultiSearchResponse.Item rolledItem = new MultiSearchResponse.Item(new SearchResponse( new InternalSearchResponse(null, new InternalAggregations(Collections.singletonList(responses.get(1))), null, null, false, false, 1), null, 1, 1, 0, 10, null, null), null); MultiSearchResponse.Item[] msearch = new MultiSearchResponse.Item[]{unrolledItem, rolledItem}; BigArrays bigArrays = new MockBigArrays(new MockPageCacheRecycler(Settings.EMPTY), new NoneCircuitBreakerService()); ScriptService scriptService = mock(ScriptService.class); InternalAggregation.ReduceContext reduceContext = new InternalAggregation.ReduceContext(bigArrays, scriptService, true); Exception e = expectThrows(RuntimeException.class, () -> RollupResponseTranslator.combineResponses(msearch, reduceContext)); assertThat(e.getMessage(), containsString("org.elasticsearch.search.aggregations.metrics.geobounds.InternalGeoBounds")); assertThat(e.getMessage(), containsString("org.elasticsearch.search.aggregations.InternalMultiBucketAggregation")); }	can this be more specific so that we at least check the expected exception type? by checking the exception message i think we at least used to expect a classcastexception or something alike. would be great if we can to that here now that the assertion on the exact message text needs to be relaxed a bit.
private static boolean alreadyContainsXPackCustomMetadata(ClusterState clusterState) { final Metadata metadata = clusterState.metadata(); return metadata.custom(LicensesMetadata.TYPE) != null || metadata.custom(MlMetadata.TYPE) != null || metadata.custom(WatcherMetadata.TYPE) != null || clusterState.custom(TokenMetadata.TYPE) != null || metadata.custom(TransformMetadata.TYPE) != null; }	any purpose of the extra lines here?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(name); out.writeCollection(values, StreamOutput::writeGenericValue); if (out.getVersion().onOrAfter(Version.V_7_16_0)) { out.writeCollection(ignoredValues, StreamOutput::writeGenericValue); } if (out.getVersion().onOrAfter(Version.V_8_1_0)) { out.writeCollection(lookupFields); } else { // We deliberately do not to fail CCS search requests when the remote clusters are on the new version, // and have lookup fields, but the local cluster is still on an old version. } }	should we throw an error in this case? otherwise the field could be (surprisingly) missing from the search hits? i think nowadays we prefer to "fail hard" on ccs incompatibilities instead of silently dropping data.
public void stopDeployment(TrainedModelDeploymentTask task) { ProcessContext processContext; synchronized (processContextByAllocation) { processContext = processContextByAllocation.get(task.getId()); } if (processContext != null) { logger.debug("[{}] Stopping deployment", task.getModelId()); processContext.stopProcess(); } else { logger.debug("[{}] No process context to stop", task.getModelId()); } }	the corresponding start deployment message is debug causing the odd situation where stopping was logged but starting not
public void infer(Map<String, Object> doc, InferenceConfigUpdate update, TimeValue timeout, ActionListener<InferenceResults> listener) { if (inferenceConfigHolder.get() == null) { listener.onFailure( // inference not possible against uninitialized model" ExceptionsHelper.conflictStatusException("Trained model [{}] is not initialized", params.getModelId()) ); return; } if (update.isSupported(inferenceConfigHolder.get()) == false) { listener.onFailure( new ElasticsearchStatusException( "Trained model [{}] is configured for task [{}] but called with task [{}]", RestStatus.FORBIDDEN, params.getModelId(), inferenceConfigHolder.get().getName(), update.getName() ) ); return; } trainedModelAllocationNodeService.infer(this, update.apply(inferenceConfigHolder.get()), doc, timeout, listener); }	is this comment necessary? if you think it is then please remove dangling double quote at the end
public StoredContext stashContext() { final ThreadContextStruct context = threadLocal.get(); /** * X-Opaque-ID should be preserved in a threadContext in order to propagate this across threads. * This is needed so the DeprecationLogger in another thread can see the value of X-Opaque-ID provided by a user. * Otherwise when context is stash, it should be empty. */ if (context.requestHeaders.containsKey(Task.X_OPAQUE_ID) || context.requestHeaders.containsKey(Task.TRACE_PARENT)) { var map = new HashMap<>(Map.<String, String>of()); if (context.requestHeaders.containsKey(Task.X_OPAQUE_ID)) { map.put(Task.X_OPAQUE_ID, context.requestHeaders.get(Task.X_OPAQUE_ID)); } if (context.requestHeaders.containsKey(Task.TRACE_PARENT)) { map.put(Task.TRACE_PARENT, context.requestHeaders.get(Task.TRACE_PARENT)); } ThreadContextStruct threadContextStruct = DEFAULT_CONTEXT.putHeaders(map); threadLocal.set(threadContextStruct); } else { threadLocal.set(DEFAULT_CONTEXT); } return () -> { // If the node and thus the threadLocal get closed while this task // is still executing, we don't want this runnable to fail with an // uncaught exception threadLocal.set(context); }; }	do we also need for trace_id ?
private void tryAllHandlers(final RestRequest request, final RestChannel channel, final ThreadContext threadContext) throws Exception { for (final RestHeaderDefinition restHeader : headersToCopy) { final String name = restHeader.getName(); final List<String> headerValues = request.getAllHeaderValues(name); if (headerValues != null && headerValues.isEmpty() == false) { final List<String> distinctHeaderValues = headerValues.stream().distinct().collect(Collectors.toList()); if (restHeader.isMultiValueAllowed() == false && distinctHeaderValues.size() > 1) { channel.sendResponse( BytesRestResponse. createSimpleErrorResponse(channel, BAD_REQUEST, "multiple values for single-valued header [" + name + "].")); return; } else if (name.equals(Task.TRACE_PARENT)) { String traceparent = distinctHeaderValues.get(0); // TODO rely on high performance `TraceContext` from apm-agent-java // This does little validation of the actual id values, since we only use them to persist in logging for now this // should be fine. Once we actually start instrumenting through apm-agent-java it will take care of validation. // Java has a fast path for single character regexes so this should not be as bad as its signature implies String[] tokens = traceparent.split("-"); if (tokens.length == 4) { threadContext.putTransient(Task.TRACE_ID, tokens[1]); } threadContext.putHeader(name, String.join(",", distinctHeaderValues)); } else { threadContext.putHeader(name, String.join(",", distinctHeaderValues)); } } } // error_trace cannot be used when we disable detailed errors // we consume the error_trace parameter first to ensure that it is always consumed if (request.paramAsBoolean("error_trace", false) && channel.detailedErrorsEnabled() == false) { channel.sendResponse( BytesRestResponse.createSimpleErrorResponse(channel, BAD_REQUEST, "error traces in responses are disabled.")); return; } final String rawPath = request.rawPath(); final String uri = request.uri(); final RestRequest.Method requestMethod; RestApiVersion restApiVersion = request.getRestApiVersion(); try { // Resolves the HTTP method and fails if the method is invalid requestMethod = request.method(); // Loop through all possible handlers, attempting to dispatch the request Iterator<MethodHandlers> allHandlers = getAllHandlers(request.params(), rawPath); while (allHandlers.hasNext()) { final RestHandler handler; final MethodHandlers handlers = allHandlers.next(); if (handlers == null) { handler = null; } else { handler = handlers.getHandler(requestMethod, restApiVersion); } if (handler == null) { if (handleNoHandlerFound(rawPath, requestMethod, uri, channel)) { return; } } else { dispatchRequest(request, channel, handler, restApiVersion); return; } } } catch (final IllegalArgumentException e) { handleUnsupportedHttpMethod(uri, null, channel, getValidHandlerMethodSet(rawPath), e); return; } // If request has not been handled, fallback to a bad request error. handleBadRequest(uri, requestMethod, channel); }	do you think we could extract the whole headers copy to a separate method?
public Builder cluster(Set<String> privilegeNames, Iterable<ConfigurableClusterPrivilege> configurableClusterPrivileges) { ClusterPermission.Builder builder = ClusterPermission.builder(); List<ClusterPermission> clusterPermissions = new ArrayList<>(); if (privilegeNames.isEmpty() == false) { for (String name : privilegeNames) { ClusterPrivilegeResolver.resolve(name).buildPermission(builder); } } for (ConfigurableClusterPrivilege ccp : configurableClusterPrivileges) { ccp.buildPermission(builder); } this.cluster = builder.build(); return this; }	probably builder = clusterprivilegeresolver.resolve(name).buildpermission(builder); i know from an implementation standpoint it does not matter but i believe it does given the interface. if we keep this as is, i suggest we change the interface.
public Builder cluster(Set<String> privilegeNames, Iterable<ConfigurableClusterPrivilege> configurableClusterPrivileges) { ClusterPermission.Builder builder = ClusterPermission.builder(); List<ClusterPermission> clusterPermissions = new ArrayList<>(); if (privilegeNames.isEmpty() == false) { for (String name : privilegeNames) { ClusterPrivilegeResolver.resolve(name).buildPermission(builder); } } for (ConfigurableClusterPrivilege ccp : configurableClusterPrivileges) { ccp.buildPermission(builder); } this.cluster = builder.build(); return this; }	probably builder = cpp.buildpermission(builder)
GetUserPrivilegesResponse buildUserPrivilegesResponseObject(Role userRole) { logger.trace(() -> new ParameterizedMessage("List privileges for role [{}]", arrayToCommaDelimitedString(userRole.names()))); // We use sorted sets for Strings because they will typically be small, and having a predictable order allows for simpler testing final Set<String> cluster = new TreeSet<>(); // But we don't have a meaningful ordering for objects like ConfigurableClusterPrivilege, so the tests work with "random" ordering final Set<ConfigurableClusterPrivilege> conditionalCluster = new HashSet<>(); for (ClusterPrivilege privilege : userRole.cluster().privileges()) { if (privilege instanceof ActionClusterPrivilege) { cluster.add(((ActionClusterPrivilege) privilege).name()); } else if (privilege instanceof ConfigurableClusterPrivilege) { conditionalCluster.add((ConfigurableClusterPrivilege) privilege); } else { logger.trace("found unsupported cluster privilege {}", privilege); } } final Set<GetUserPrivilegesResponse.Indices> indices = new LinkedHashSet<>(); for (IndicesPermission.Group group : userRole.indices().groups()) { final Set<BytesReference> queries = group.getQuery() == null ? Collections.emptySet() : group.getQuery(); final Set<FieldPermissionsDefinition.FieldGrantExcludeGroup> fieldSecurity = group.getFieldPermissions().hasFieldLevelSecurity() ? group.getFieldPermissions().getFieldPermissionsDefinition().getFieldGrantExcludeGroups() : Collections.emptySet(); indices.add(new GetUserPrivilegesResponse.Indices( Arrays.asList(group.indices()), group.privilege().name(), fieldSecurity, queries, group.allowRestrictedIndices() )); } final Set<RoleDescriptor.ApplicationResourcePrivileges> application = new LinkedHashSet<>(); for (String applicationName : userRole.application().getApplicationNames()) { for (ApplicationPrivilege privilege : userRole.application().getPrivileges(applicationName)) { final Set<String> resources = userRole.application().getResourcePatterns(privilege); if (resources.isEmpty()) { logger.trace("No resources defined in application privilege {}", privilege); } else { application.add(RoleDescriptor.ApplicationResourcePrivileges.builder() .application(applicationName) .privileges(privilege.name()) .resources(resources) .build()); } } } final Privilege runAsPrivilege = userRole.runAs().getPrivilege(); final Set<String> runAs; if (Operations.isEmpty(runAsPrivilege.getAutomaton())) { runAs = Collections.emptySet(); } else { runAs = runAsPrivilege.name(); } return new GetUserPrivilegesResponse(cluster, conditionalCluster, indices, application, runAs); }	probably better to code to thee interface nameable. shouldn't we outsource building the list of nameables to the clusterpermission?
private boolean shouldCheckForFallbackDeprecation(String name) { if (name.startsWith("xpack.security.authc.realms.")) { // try to see if this is actually using TLS Settings realm = settings.getByPrefix(name.substring(0, name.indexOf(".ssl"))); String type = realm.get("type"); // only check the types we know use ssl. custom realms may but we don't want to cause confusion if (LdapRealmSettings.LDAP_TYPE.equals(type) || LdapRealmSettings.AD_TYPE.equals(type)) { List<String> urls = realm.getAsList("url"); return urls.isEmpty() == false && urls.stream().anyMatch(s -> s.startsWith("ldaps://")); } else if (SamlRealmSettings.TYPE.equals(type)) { final String idpMetadataPath = SamlRealmSettings.IDP_METADATA_PATH.get(realm); return Strings.hasText(idpMetadataPath) && idpMetadataPath.startsWith("https://"); } } else if (name.startsWith("xpack.monitoring.exporters.")) { Settings exporterSettings = settings.getByPrefix(name.substring(0, name.indexOf(".ssl"))); List<String> hosts = exporterSettings.getAsList("host"); return hosts.stream().anyMatch(s -> s.startsWith("https")); } else if (name.equals(XPackSettings.HTTP_SSL_PREFIX) && XPackSettings.HTTP_SSL_ENABLED.get(settings)) { return true; } else if (name.equals("xpack.http.ssl") && XPackSettings.WATCHER_ENABLED.get(settings)) { return true; } return false; }	i think it might be nice to print the key missing from the actual config that falls back to the default value here instead of a descriptive name, but i'm not sure if it can be reliably computed and if it would actually make the deprecation log better, so i just bring it up for consideration
long getFreeBytes() { try { return Math.subtractExact(diskUsage.getFreeBytes(), relocatingShardSize); } catch (ArithmeticException e) { return Long.MAX_VALUE; } }	how could this overflow? isn't relocatingshardsize >= 0 and 0 <= diskusage.getfreebytes() <= long.max_value? is this to capture the case where diskusage.getfreebytes() == 0 && relocatingshardsize == long.max_value?
@Override public boolean equals(Object obj) { if (obj == null || (obj.getClass() != getClass())) { return false; } if (obj == this) { return true; } Request other = (Request) obj; return method.equals(other.method) && endpoint.equals(other.endpoint) && parameters.equals(other.parameters) && Objects.equals(entity, other.entity) && headers.equals(other.headers) && httpAsyncResponseConsumerFactory.equals(other.httpAsyncResponseConsumerFactory); }	did you do it so we'd have a nice equals and hashcode? maybe it'd be better to iterate by hand in those cases rather than make this. i'm not sure!
protected void injectStaticFieldsAndGetters() { Location internalLocation = new Location("$internal$ScriptInjectionPhase$injectStaticFieldsAndGetters", 0); int modifiers = Opcodes.ACC_PUBLIC | Opcodes.ACC_STATIC; FieldNode irFieldNode = new FieldNode(); irFieldNode.setLocation(internalLocation); irFieldNode.setModifiers(modifiers); irFieldNode.setFieldType(String.class); irFieldNode.setName("$NAME"); irClassNode.addFieldNode(irFieldNode); irFieldNode = new FieldNode(); irFieldNode.setLocation(internalLocation); irFieldNode.setModifiers(modifiers); irFieldNode.setFieldType(String.class); irFieldNode.setName("$SOURCE"); irClassNode.addFieldNode(irFieldNode); irFieldNode = new FieldNode(); irFieldNode.setLocation(internalLocation); irFieldNode.setModifiers(modifiers); irFieldNode.setFieldType(BitSet.class); irFieldNode.setName("$STATEMENTS"); irClassNode.addFieldNode(irFieldNode); FunctionNode irFunctionNode = new FunctionNode(); irFunctionNode.setLocation(internalLocation); irFunctionNode.setName("getName"); irFunctionNode.setReturnType(String.class); irFunctionNode.setStatic(false); irFunctionNode.setVarArgs(false); irFunctionNode.setSynthetic(true); irFunctionNode.setMaxLoopCounter(0); irClassNode.addFunctionNode(irFunctionNode); BlockNode irBlockNode = new BlockNode(); irBlockNode.setLocation(internalLocation); irBlockNode.setAllEscape(true); irFunctionNode.setBlockNode(irBlockNode); ReturnNode irReturnNode = new ReturnNode(); irReturnNode.setLocation(internalLocation); irBlockNode.addStatementNode(irReturnNode); LoadFieldMemberNode irLoadFieldMemberNode = new LoadFieldMemberNode(); irLoadFieldMemberNode.setLocation(internalLocation); irLoadFieldMemberNode.setExpressionType(String.class); irLoadFieldMemberNode.setName("$NAME"); irLoadFieldMemberNode.setStatic(true); // TODO(stu): add $COMPILER_INJECTS, add hash map and set it irReturnNode.setExpressionNode(irLoadFieldMemberNode); irFunctionNode = new FunctionNode(); irFunctionNode.setLocation(internalLocation); irFunctionNode.setName("getSource"); irFunctionNode.setReturnType(String.class); irFunctionNode.setStatic(false); irFunctionNode.setVarArgs(false); irFunctionNode.setSynthetic(true); irFunctionNode.setMaxLoopCounter(0); irClassNode.addFunctionNode(irFunctionNode); irBlockNode = new BlockNode(); irBlockNode.setLocation(internalLocation); irBlockNode.setAllEscape(true); irFunctionNode.setBlockNode(irBlockNode); irReturnNode = new ReturnNode(); irReturnNode.setLocation(internalLocation); irBlockNode.addStatementNode(irReturnNode); irLoadFieldMemberNode = new LoadFieldMemberNode(); irLoadFieldMemberNode.setLocation(internalLocation); irLoadFieldMemberNode.setExpressionType(String.class); irLoadFieldMemberNode.setName("$SOURCE"); irLoadFieldMemberNode.setStatic(true); irReturnNode.setExpressionNode(irLoadFieldMemberNode); irFunctionNode = new FunctionNode(); irFunctionNode.setLocation(internalLocation); irFunctionNode.setName("getStatements"); irFunctionNode.setReturnType(BitSet.class); irFunctionNode.setStatic(false); irFunctionNode.setVarArgs(false); irFunctionNode.setSynthetic(true); irFunctionNode.setMaxLoopCounter(0); irClassNode.addFunctionNode(irFunctionNode); irBlockNode = new BlockNode(); irBlockNode.setLocation(internalLocation); irBlockNode.setAllEscape(true); irFunctionNode.setBlockNode(irBlockNode); irReturnNode = new ReturnNode(); irReturnNode.setLocation(internalLocation); irBlockNode.addStatementNode(irReturnNode); irLoadFieldMemberNode = new LoadFieldMemberNode(); irLoadFieldMemberNode.setLocation(internalLocation); irLoadFieldMemberNode.setExpressionType(BitSet.class); irLoadFieldMemberNode.setName("$STATEMENTS"); irLoadFieldMemberNode.setStatic(true); irReturnNode.setExpressionNode(irLoadFieldMemberNode); }	what is this todo?
private static void performStateDirectoriesFsync(List<Tuple<Path, Directory>> stateDirectories) throws WriteStateException { for (int i = 0; i < stateDirectories.size(); i++) { try { stateDirectories.get(i).v2().syncMetaData(); } catch (IOException e) { throw new WriteStateException(true, "meta data directory fsync has failed " + stateDirectories.get(i).v1(), e); } } } /** * Writes the given state to the given directories. The state is written to a * state directory ({@value #STATE_DIR_NAME}) underneath each of the given file locations and is created if it * doesn't exist. The state is serialized to a temporary file in that directory and is then atomically moved to * it's target filename of the pattern {@code {prefix}{version}.st}. * If this method returns without exception there is a guarantee that state is persisted to the disk and loadLatestState will return * it.<br> * If <code>cleanup</code> is false, this method does not perform cleanup of old state files, * because one write could be a part of larger transaction. * If this write succeeds, but some further write fails, you may want to rollback the transaction and keep old file around. * After transaction is finished use {@link #cleanupOldFiles(long, Path[])} for the clean-up. * * @param state the state object to write * @param cleanup whether to perform auto cleanup. * @param locations the locations where the state should be written to. * @throws WriteStateException if some exception during writing state occurs. See also {@link WriteStateException#isDirty()}	i would prefer a separate method writeandcleanupoldfiles(), delegating to write(), over a boolean parameter here. bools make the call sites harder to understand because you have to remember what the true/false means.
* @param generation the generation to be loaded. * @param dataLocations the data-locations to try. * @return the state of asked generation or <code>null</code> if no state was found. */ public T loadGeneration(Logger logger, NamedXContentRegistry namedXContentRegistry, long generation, Path... dataLocations) { List<Path> stateFiles = findStateFilesByGeneration(generation, dataLocations); final List<Throwable> exceptions = new ArrayList<>(); for (Path stateFile : stateFiles) { try { T state = read(namedXContentRegistry, stateFile); logger.trace("state id [{}] read from [{}]", generation, stateFile.getFileName()); return state; } catch (Exception e) { exceptions.add(new IOException("failed to read " + stateFile.toAbsolutePath(), e)); logger.debug(() -> new ParameterizedMessage( "{}: failed to read [{}], ignoring...", stateFile.toAbsolutePath(), prefix), e); } } // if we reach this something went wrong ExceptionsHelper.maybeThrowRuntimeAndSuppress(exceptions); if (stateFiles.size() > 0) { // We have some state files but none of them gave us a usable state throw new IllegalStateException("Could not find a state file to recover from among " + stateFiles.stream().map(Path::toAbsolutePath).map(Object::toString).collect(Collectors.joining(", "))); } return null; }	nit: id -> generation
* @param dataLocations the data-locations to try. * @return tuple of the latest state and generation. (-1, null) of no state is found. */ public Tuple<T, Long> loadLatestStateWithGeneration(Logger logger, NamedXContentRegistry namedXContentRegistry, Path... dataLocations) throws IOException { long generation = findMaxStateId(prefix, dataLocations); T state = loadGeneration(logger, namedXContentRegistry, generation, dataLocations); if (generation > -1 && state == null) { throw new IllegalStateException("unable to find state files with state id " + generation + " returned by findMaxStateId function, in data folders [" + Arrays.stream(dataLocations).map(Path::toAbsolutePath). map(Object::toString).collect(Collectors.joining(", ")) + "], concurrent writes?"); } return Tuple.tuple(state, generation); }	can this happen (in production)? if so, i think we should have a comment describing how since it's not immediately obvious; if not, i think there should be an assertion here that this doesn't happen.
public static void corruptFile(Path file, Logger logger) throws IOException { Path fileToCorrupt = file; try (SimpleFSDirectory dir = new SimpleFSDirectory(fileToCorrupt.getParent())) { long checksumBeforeCorruption; try (IndexInput input = dir.openInput(fileToCorrupt.getFileName().toString(), IOContext.DEFAULT)) { checksumBeforeCorruption = CodecUtil.retrieveChecksum(input); } try (FileChannel raf = FileChannel.open(fileToCorrupt, StandardOpenOption.READ, StandardOpenOption.WRITE)) { raf.position(randomIntBetween(0, (int)Math.min(Integer.MAX_VALUE, raf.size()-1))); long filePointer = raf.position(); ByteBuffer bb = ByteBuffer.wrap(new byte[1]); raf.read(bb); bb.flip(); byte oldValue = bb.get(0); byte newValue = (byte) ~oldValue; bb.put(0, newValue); raf.write(bb, filePointer); logger.debug("Corrupting file {} -- flipping at position {} from {} to {} ", fileToCorrupt.getFileName().toString(), filePointer, Integer.toHexString(oldValue), Integer.toHexString(newValue)); } long checksumAfterCorruption; long actualChecksumAfterCorruption; try (ChecksumIndexInput input = dir.openChecksumInput(fileToCorrupt.getFileName().toString(), IOContext.DEFAULT)) { assertThat(input.getFilePointer(), is(0L)); input.seek(input.length() - 8); // one long is the checksum... 8 bytes checksumAfterCorruption = input.getChecksum(); actualChecksumAfterCorruption = input.readLong(); } StringBuilder msg = new StringBuilder(); msg.append("Checksum before: [").append(checksumBeforeCorruption).append("]"); msg.append(" after: [").append(checksumAfterCorruption).append("]"); msg.append(" checksum value after corruption: ").append(actualChecksumAfterCorruption).append("]"); msg.append(" file: ").append(fileToCorrupt.getFileName().toString()).append(" length: ") .append(dir.fileLength(fileToCorrupt.getFileName().toString())); logger.debug("{}", msg.toString()); assumeTrue("Checksum collision - " + msg.toString(), checksumAfterCorruption != checksumBeforeCorruption // collision || actualChecksumAfterCorruption != checksumBeforeCorruption); // checksum corrupted } }	in this test we can inline filetocorrupt = file (not something introduced here - i cannot comment on the line itself - but worth doing.)
public GeoPoint resetFromGeoHash(String geohash) { final long hash; try { hash = mortonEncode(geohash); } catch (IllegalArgumentException ex) { throw new ElasticsearchParseException(ex.getMessage(), ex); } return this.reset(GeoHashUtils.decodeLatitude(hash), GeoHashUtils.decodeLongitude(hash)); }	why don't we just use the iae?
* @return a new index searcher wrapping the provided index searcher or if no wrapping was performed * the provided index searcher */ protected IndexSearcher wrap(EngineConfig engineConfig, IndexSearcher searcher) throws IOException { return searcher; } /** * If there are configured {@link IndexSearcherWrapper} instances, the {@link IndexSearcher} of the provided engine searcher * gets wrapped and a new {@link Engine.Searcher} instances is returned, otherwise the provided {@link Engine.Searcher} is returned. * * This is invoked each time a {@link Engine.Searcher}	should we assert that reader.getcorecachekey() == enginesearcher.getdirectoryreader()? forcing the core cache key handling to be delegated to the inner reader could be trappy otherwise
public void testShouldPreFilterSearchShards() { int numIndices = randomIntBetween(2, 10); Index[] indices = new Index[numIndices]; for (int i = 0; i < numIndices; i++) { String indexName = randomAlphaOfLengthBetween(5, 10); indices[i] = new Index(indexName, indexName + "-uuid"); } ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).build(); { SearchRequest searchRequest = new SearchRequest(); assertFalse(TransportSearchAction.shouldPreFilterSearchShards(clusterState, searchRequest, indices, randomIntBetween(2, 128))); assertFalse(TransportSearchAction.shouldPreFilterSearchShards(clusterState, searchRequest, indices, randomIntBetween(129, 10000))); } { SearchRequest searchRequest = new SearchRequest() .source(new SearchSourceBuilder().query(QueryBuilders.rangeQuery("timestamp"))); assertFalse(TransportSearchAction.shouldPreFilterSearchShards(clusterState, searchRequest, indices, randomIntBetween(2, 128))); assertTrue(TransportSearchAction.shouldPreFilterSearchShards(clusterState, searchRequest, indices, randomIntBetween(129, 10000))); } { SearchRequest searchRequest = new SearchRequest() .source(new SearchSourceBuilder().sort(SortBuilders.fieldSort("timestamp"))); assertTrue(TransportSearchAction.shouldPreFilterSearchShards(clusterState, searchRequest, indices, randomIntBetween(2, 127))); assertTrue(TransportSearchAction.shouldPreFilterSearchShards(clusterState, searchRequest, indices, randomIntBetween(127, 10000))); } { SearchRequest searchRequest = new SearchRequest() .source(new SearchSourceBuilder().sort(SortBuilders.fieldSort("timestamp"))) .scroll("5m"); assertTrue(TransportSearchAction.shouldPreFilterSearchShards(clusterState, searchRequest, indices, randomIntBetween(2, 128))); assertTrue(TransportSearchAction.shouldPreFilterSearchShards(clusterState, searchRequest, indices, randomIntBetween(129, 10000))); } }	i'm not exactly sure whether this should be 129 or 128, the way i read the docs i would understand them to _not_ run pre-filter phase if <=128, but i might be wrong here. please let me know what you think is right.
public void registerSetting(Setting<?> setting) { if (setting.isFiltered()) { if (settingsFilterPattern.contains(setting.getKey()) == false) { registerSettingsFilter(setting.getKey()); } } if (setting.hasClusterScope()) { if (clusterSettings.containsKey(setting.getKey())) { throw new IllegalArgumentException("Cannot register setting [" + setting.getKey() + "] twice"); } clusterSettings.put(setting.getKey(), setting); } if (setting.hasIndexScope()) { if (indexSettings.containsKey(setting.getKey())) { throw new IllegalArgumentException("Cannot register setting [" + setting.getKey() + "] twice"); } indexSettings.put(setting.getKey(), setting); } }	ok i see we don't have a use for nodescope yet? can we remove it then?
public void registerSetting(Setting<?> setting) { if (setting.isFiltered()) { if (settingsFilterPattern.contains(setting.getKey()) == false) { registerSettingsFilter(setting.getKey()); } } if (setting.hasClusterScope()) { if (clusterSettings.containsKey(setting.getKey())) { throw new IllegalArgumentException("Cannot register setting [" + setting.getKey() + "] twice"); } clusterSettings.put(setting.getKey(), setting); } if (setting.hasIndexScope()) { if (indexSettings.containsKey(setting.getKey())) { throw new IllegalArgumentException("Cannot register setting [" + setting.getKey() + "] twice"); } indexSettings.put(setting.getKey(), setting); } }	hmm i think it should be an else if (setting.hasindexscope()) and then an } else { throw new iae("no scope?");}
@Override protected Settings nodeSettings() { Settings.Builder builder = Settings.builder(); if (Strings.isNullOrEmpty(System.getProperty("test.google.endpoint")) == false) { builder.put("gcs.client.default.endpoint", System.getProperty("test.google.endpoint")); } if (Strings.isNullOrEmpty(System.getProperty("test.google.tokenURI")) == false) { builder.put("gcs.client.default.token_uri", System.getProperty("test.google.tokenURI")); } return Settings.builder() .put(builder.build()) .setSecureSettings(credentials()) .build(); }	i think this should just be settings.builder builder = settings.builder().put(super.nodesettings()); and then we just return builder.build(); instead of duplicating the parent logic here.
private Version computePreviousFirstMinor() { List<Version> allVersions = DeclaredVersionsHolder.DECLARED_VERSIONS; for (int i = allVersions.size() - 1; i >= 0; i--) { Version v = allVersions.get(i); if (v.before(this) && (v.minor < this.minor || v.major < this.major)) { return fromId(v.major * 1000000 + v.minor * 10000 + 99); } } throw new IllegalArgumentException("couldn't find any released versions of the minor before [" + this + "]"); }	i see that this is only exercised by the ccs compatibility flag, which is fine. thanks for trying the approach of moving the logic to version. i was hoping it'd be easier. i don't have a strong opinion on where this bit should be. i wonder if it will be useful in the future, but it could also be misused if exposed at this level.up to you if you want to move it back or leave it here.
* @param sourceIndexMetadata the source index metadata * @param numTargetShards the total number of shards in the target index * @return a the source shard ID to split off from */ public static ShardId selectSplitShard(int shardId, IndexMetaData sourceIndexMetadata, int numTargetShards) { if (shardId >= numTargetShards) { throw new IllegalArgumentException("the number of target shards (" + numTargetShards + ") must be greater than the shard id: " + shardId); } int numSourceShards = sourceIndexMetadata.getNumberOfShards(); if (numSourceShards > numTargetShards) { throw new IllegalArgumentException("the number of source shards must be less that the number of target shards"); } int routingFactor = getRoutingFactor(numSourceShards, numTargetShards); return new ShardId(sourceIndexMetadata.getIndex(), shardId/routingFactor); }	can you add numsourceshards and numtargetshards to the message?
* @param sourceIndexMetadata the source index metadata * @param numTargetShards the total number of shards in the target index * @return a set of shard IDs to shrink into the given shard ID. */ public static Set<ShardId> selectShrinkShards(int shardId, IndexMetaData sourceIndexMetadata, int numTargetShards) { if (shardId >= numTargetShards) { throw new IllegalArgumentException("the number of target shards (" + numTargetShards + ") must be greater than the shard id: " + shardId); } if (sourceIndexMetadata.getNumberOfShards() < numTargetShards) { throw new IllegalArgumentException("the number of target shards must be less that the number of source shards"); } int routingFactor = getRoutingFactor(sourceIndexMetadata.getNumberOfShards(), numTargetShards); Set<ShardId> shards = new HashSet<>(routingFactor); for (int i = shardId * routingFactor; i < routingFactor*shardId + routingFactor; i++) { shards.add(new ShardId(sourceIndexMetadata.getIndex(), i)); } return shards; } /** * Returns the routing factor for and shrunk index with the given number of target shards. * This factor is used in the hash function in * {@link org.elasticsearch.cluster.routing.OperationRouting#generateShardId(IndexMetaData, String, String)}	add sourceindexmetadata.getnumberofshards() and numtargetshards to the message
* @return the routing factor for and shrunk index with the given number of target shards. * @throws IllegalArgumentException if the number of source shards is less than the number of target shards or if the source shards * are not divisible by the number of target shards. */ public static int getRoutingFactor(int sourceNumberOfShards, int targetNumberOfShards) { if (sourceNumberOfShards < targetNumberOfShards) { int spare = sourceNumberOfShards; sourceNumberOfShards = targetNumberOfShards; targetNumberOfShards = spare; } int factor = sourceNumberOfShards / targetNumberOfShards; if (factor * targetNumberOfShards != sourceNumberOfShards || factor <= 1) { if (sourceNumberOfShards < targetNumberOfShards) { throw new IllegalArgumentException("the number of source shards [" + sourceNumberOfShards + "] must be a must be a " + "factor of [" + targetNumberOfShards + "]"); } throw new IllegalArgumentException("the number of source shards [" + sourceNumberOfShards + "] must be a must be a " + "multiple of [" + targetNumberOfShards + "]"); } return factor; }	a bit weird to swap those around. especially as this results in source and target shards being swapped around in error reporting a few lines below: "the number of source shards [" + sourcenumberofshards + "] must be a must be a multiple ...
static void validateSplitIndex(ClusterState state, String sourceIndex, Set<String> targetIndexMappingsTypes, String targetIndexName, Settings targetIndexSettings) { IndexMetaData sourceMetaData = validateResize(state, sourceIndex, targetIndexMappingsTypes, targetIndexName, targetIndexSettings); if (IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.exists(targetIndexSettings)) { IndexMetaData.selectSplitShard(0, sourceMetaData, IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.get(targetIndexSettings)); } if (sourceMetaData.getCreationVersion().before(Version.V_6_0_0_alpha1)) { // ensure we have a single type. throw new IllegalStateException("source index created version is too old to apply a split operation"); } }	why is it important to have a single type for shard splitting?
static DateFormatter forPattern(String input) { if (Strings.hasLength(input) == false) { throw new IllegalArgumentException("No date pattern provided"); } List<DateFormatter> formatters = new ArrayList<>(); for (String pattern : Strings.delimitedListToStringArray(input, "||")) { if (Strings.hasLength(pattern) == false) { throw new IllegalArgumentException("Cannot have empty element in multi date format pattern: " + input); } if (pattern.startsWith("8")) { pattern = pattern.substring(1); } formatters.add(DateFormatters.forPattern(pattern)); } if (formatters.size() == 1) { return formatters.get(0); } return DateFormatters.merge(input, formatters); }	this looks odd and maybe warrant a comment?
public static ZoneId of(String zoneId) { String deprecatedId = DEPRECATED_SHORT_TIMEZONES.get(zoneId); if (deprecatedId != null) { deprecationLogger.deprecatedAndMaybeLog("timezone", "Use of short timezone id " + zoneId + " is deprecated. Use " + deprecatedId + " instead"); return ZoneId.of(deprecatedId); } return ZoneId.of(zoneId).normalized(); }	this makes sense to me intuitively but does this change any semantics?
@Override public DateMathParser toDateMathParser() { DateFormatter roundUpFormatter = this.parseDefaulting(ROUND_UP_BASE_FIELDS).withLocale(locale()); ZoneId zone = zone(); if (zone != null) { roundUpFormatter.withZone(zone); } return new JavaDateMathParser(this, roundUpFormatter); }	i think we need to reassign the return value here, i.e. roundupformatter = roundupformatter.withzone(zone); as #withzone() returns a copy.
public Instant parse(String text, LongSupplier now, boolean roundUp, ZoneId timeZone) { Instant time; String mathString; if (text.startsWith("now")) { try { // TODO only millisecond granularity here! time = Instant.ofEpochMilli(now.getAsLong()); } catch (Exception e) { throw new ElasticsearchParseException("could not read the current timestamp", e); } mathString = text.substring("now".length()); } else { int index = text.indexOf("||"); if (index == -1) { return parseDateTime(text, timeZone, roundUp); } time = parseDateTime(text.substring(0, index), timeZone, false); mathString = text.substring(index + 2); } return parseMath(mathString, time, roundUp, timeZone); }	does this need to be resolved?
@Override public boolean equals(Object o) { if (!super.equals(o)) return false; DateFieldType that = (DateFieldType) o; return Objects.equals(dateTimeFormatter, that.dateTimeFormatter); }	we never expect an instance of jodadateformatter here but i noticed that it lacks an implementation of #equals() and #hashcode() and i think we should add it anyway for consistency with javadateformatter.
@Override protected void setupFieldType(BuilderContext context) { super.setupFieldType(context); DateFormatter formatter = fieldType().dateTimeFormatter; if (fieldType().rangeType == RangeType.DATE) { boolean hasPatternChanged = Strings.hasLength(builder.pattern) && Objects.equals(builder.pattern, formatter.pattern()) == false; if (hasPatternChanged || Objects.equals(builder.locale, formatter.locale()) == false) { fieldType().setDateTimeFormatter(DateFormatter.forPattern(pattern).withLocale(locale)); } } else if (pattern != null) { throw new IllegalArgumentException("field [" + name() + "] of type [" + fieldType().rangeType + "] should not define a dateTimeFormatter unless it is a " + RangeType.DATE + " type"); } }	any reason the logic here is implemented slightly differently to datefieldmapper.builder#setupfieldtype()?
public RangeQueryBuilder timeZone(String timeZone) { if (timeZone == null) { throw new IllegalArgumentException("timezone cannot be null"); } try { this.timeZone = ZoneId.of(timeZone); } catch (ZoneRulesException e) { throw new IllegalArgumentException(e); } return this; }	zoneid#of() can also throw datetimeexception if the zone id is invalid. can we guarantee at this point that this is impossible or why are we only catching zonerulesexception?
ZoneId rewriteTimeZone(QueryShardContext context) throws IOException { final ZoneId tz = timeZone(); if (field() != null && tz != null && tz.getRules().isFixedOffset() == false && field() != null && script() == null) { final MappedFieldType ft = context.fieldMapper(field()); final IndexReader reader = context.getIndexReader(); if (ft != null && reader != null) { Long anyInstant = null; final IndexNumericFieldData fieldData = context.getForField(ft); for (LeafReaderContext ctx : reader.leaves()) { AtomicNumericFieldData leafFD = fieldData.load(ctx); SortedNumericDocValues values = leafFD.getLongValues(); if (values.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) { anyInstant = values.nextValue(); break; } } if (anyInstant != null) { Instant instant = Instant.ofEpochMilli(anyInstant); final long prevTransition = tz.getRules().previousTransition(instant).getInstant().toEpochMilli(); ZoneOffsetTransition nextOffsetTransition = tz.getRules().nextTransition(instant); final long nextTransition; if (nextOffsetTransition != null) { nextTransition = nextOffsetTransition.getInstant().toEpochMilli(); } else { nextTransition = instant.toEpochMilli(); } // We need all not only values but also rounded values to be within // [prevTransition, nextTransition]. final long low; Rounding.DateTimeUnit intervalAsUnit = getIntervalAsDateTimeUnit(); if (intervalAsUnit != null) { Rounding rounding = Rounding.builder(intervalAsUnit).timeZone(timeZone()).build(); low = rounding.nextRoundingValue(prevTransition); } else { final TimeValue intervalAsMillis = getIntervalAsTimeValue(); low = Math.addExact(prevTransition, intervalAsMillis.millis()); } // rounding rounds down, so 'nextTransition' is a good upper bound final long high = nextTransition; if (ft.isFieldWithinQuery(reader, low, high, true, false, ZoneOffset.UTC, EPOCH_MILLIS_PARSER, context) == Relation.WITHIN) { // All values in this reader have the same offset despite daylight saving times. // This is very common for location-based timezones such as Europe/Paris in // combination with time-based indices. return ZoneOffset.ofTotalSeconds(tz.getRules().getOffset(instant).getTotalSeconds()); } } } } return tz; }	can we guarantee that there is always a previous transition in our case? i am asking because that method might actually return null in that case. in my understanding the earliest instant that we support is 0 (in epoch millis) and intuitively i'd expect that there is a previous transition for any time zone but then time zones are not necessarily intuitive... .
public void testDuellingFormatsValidParsing() { assertSameDate("1522332219", "epoch_second"); assertSameDate("0", "epoch_second"); assertSameDate("1", "epoch_second"); assertSameDate("1522332219321", "epoch_millis"); assertSameDate("0", "epoch_millis"); assertSameDate("1", "epoch_millis"); assertSameDate("20181126", "basic_date"); assertSameDate("20181126T121212.123Z", "basic_date_time"); assertSameDate("20181126T121212.123+10:00", "basic_date_time"); assertSameDate("20181126T121212.123-0800", "basic_date_time"); assertSameDate("20181126T121212Z", "basic_date_time_no_millis"); assertSameDate("20181126T121212+01:00", "basic_date_time_no_millis"); assertSameDate("20181126T121212+0100", "basic_date_time_no_millis"); assertSameDate("2018363", "basic_ordinal_date"); assertSameDate("2018363T121212.123Z", "basic_ordinal_date_time"); assertSameDate("2018363T121212.123+0100", "basic_ordinal_date_time"); assertSameDate("2018363T121212.123+01:00", "basic_ordinal_date_time"); assertSameDate("2018363T121212Z", "basic_ordinal_date_time_no_millis"); assertSameDate("2018363T121212+0100", "basic_ordinal_date_time_no_millis"); assertSameDate("2018363T121212+01:00", "basic_ordinal_date_time_no_millis"); assertSameDate("121212.123Z", "basic_time"); assertSameDate("121212.123+0100", "basic_time"); assertSameDate("121212.123+01:00", "basic_time"); assertSameDate("121212Z", "basic_time_no_millis"); assertSameDate("121212+0100", "basic_time_no_millis"); assertSameDate("121212+01:00", "basic_time_no_millis"); assertSameDate("T121212.123Z", "basic_t_time"); assertSameDate("T121212.123+0100", "basic_t_time"); assertSameDate("T121212.123+01:00", "basic_t_time"); assertSameDate("T121212Z", "basic_t_time_no_millis"); assertSameDate("T121212+0100", "basic_t_time_no_millis"); assertSameDate("T121212+01:00", "basic_t_time_no_millis"); assertSameDate("2018W313", "basic_week_date"); assertSameDate("1W313", "basic_week_date"); assertSameDate("18W313", "basic_week_date"); assertSameDate("2018W313T121212.123Z", "basic_week_date_time"); assertSameDate("2018W313T121212.123+0100", "basic_week_date_time"); assertSameDate("2018W313T121212.123+01:00", "basic_week_date_time"); assertSameDate("2018W313T121212Z", "basic_week_date_time_no_millis"); assertSameDate("2018W313T121212+0100", "basic_week_date_time_no_millis"); assertSameDate("2018W313T121212+01:00", "basic_week_date_time_no_millis"); assertSameDate("2018-12-31", "date"); assertSameDate("18-5-6", "date"); assertSameDate("10000-5-6", "date"); assertSameDate("2018-12-31T12", "date_hour"); assertSameDate("2018-12-31T8", "date_hour"); assertSameDate("2018-12-31T12:12", "date_hour_minute"); assertSameDate("2018-12-31T8:3", "date_hour_minute"); assertSameDate("2018-12-31T12:12:12", "date_hour_minute_second"); assertSameDate("2018-12-31T12:12:1", "date_hour_minute_second"); assertSameDate("2018-12-31T12:12:12.123", "date_hour_minute_second_fraction"); assertSameDate("2018-12-31T12:12:12.123", "date_hour_minute_second_millis"); assertSameDate("2018-12-31T12:12:12.1", "date_hour_minute_second_millis"); assertSameDate("2018-12-31T12:12:12.1", "date_hour_minute_second_fraction"); // this is valid anymore when using java time, you need at least a month // assertSameDate("10000", "date_optional_time"); // assertSameDate("10000T", "date_optional_time"); // assertSameDate("2018", "date_optional_time"); // assertSameDate("2018T", "date_optional_time"); assertSameDate("2018-05", "date_optional_time"); assertSameDate("2018-05-30", "date_optional_time"); assertSameDate("2018-05-30T20", "date_optional_time"); assertSameDate("2018-05-30T20:21", "date_optional_time"); assertSameDate("2018-05-30T20:21:23", "date_optional_time"); assertSameDate("2018-05-30T20:21:23.123", "date_optional_time"); assertSameDate("2018-05-30T20:21:23.123Z", "date_optional_time"); assertSameDate("2018-05-30T20:21:23.123+0100", "date_optional_time"); assertSameDate("2018-05-30T20:21:23.123+01:00", "date_optional_time"); assertSameDate("2018-12-1", "date_optional_time"); assertSameDate("2018-12-31T10:15:30", "date_optional_time"); assertSameDate("2018-12-31T10:15:3", "date_optional_time"); assertSameDate("2018-12-31T10:5:30", "date_optional_time"); assertSameDate("2018-12-31T1:15:30", "date_optional_time"); assertSameDate("2018-12-31T10:15:30.123Z", "date_time"); assertSameDate("2018-12-31T10:15:30.123+0100", "date_time"); assertSameDate("2018-12-31T10:15:30.123+01:00", "date_time"); assertSameDate("2018-12-31T10:15:30.11Z", "date_time"); assertSameDate("2018-12-31T10:15:30.11+0100", "date_time"); assertSameDate("2018-12-31T10:15:30.11+01:00", "date_time"); assertSameDate("2018-12-31T10:15:3.123Z", "date_time"); assertSameDate("2018-12-31T10:15:3.123+0100", "date_time"); assertSameDate("2018-12-31T10:15:3.123+01:00", "date_time"); assertSameDate("2018-12-31T10:15:30Z", "date_time_no_millis"); assertSameDate("2018-12-31T10:15:30+0100", "date_time_no_millis"); assertSameDate("2018-12-31T10:15:30+01:00", "date_time_no_millis"); assertSameDate("2018-12-31T10:5:30Z", "date_time_no_millis"); assertSameDate("2018-12-31T10:5:30+0100", "date_time_no_millis"); assertSameDate("2018-12-31T10:5:30+01:00", "date_time_no_millis"); assertSameDate("2018-12-31T10:15:3Z", "date_time_no_millis"); assertSameDate("2018-12-31T10:15:3+0100", "date_time_no_millis"); assertSameDate("2018-12-31T10:15:3+01:00", "date_time_no_millis"); assertSameDate("2018-12-31T1:15:30Z", "date_time_no_millis"); assertSameDate("2018-12-31T1:15:30+0100", "date_time_no_millis"); assertSameDate("2018-12-31T1:15:30+01:00", "date_time_no_millis"); assertSameDate("12", "hour"); assertSameDate("01", "hour"); assertSameDate("1", "hour"); assertSameDate("12:12", "hour_minute"); assertSameDate("12:01", "hour_minute"); assertSameDate("12:1", "hour_minute"); assertSameDate("12:12:12", "hour_minute_second"); assertSameDate("12:12:01", "hour_minute_second"); assertSameDate("12:12:1", "hour_minute_second"); assertSameDate("12:12:12.123", "hour_minute_second_fraction"); assertSameDate("12:12:12.1", "hour_minute_second_fraction"); assertParseException("12:12:12", "hour_minute_second_fraction"); assertSameDate("12:12:12.123", "hour_minute_second_millis"); assertSameDate("12:12:12.1", "hour_minute_second_millis"); assertParseException("12:12:12", "hour_minute_second_millis"); assertSameDate("2018-128", "ordinal_date"); assertSameDate("2018-1", "ordinal_date"); assertSameDate("2018-128T10:15:30.123Z", "ordinal_date_time"); assertSameDate("2018-128T10:15:30.123+0100", "ordinal_date_time"); assertSameDate("2018-128T10:15:30.123+01:00", "ordinal_date_time"); assertSameDate("2018-1T10:15:30.123Z", "ordinal_date_time"); assertSameDate("2018-1T10:15:30.123+0100", "ordinal_date_time"); assertSameDate("2018-1T10:15:30.123+01:00", "ordinal_date_time"); assertSameDate("2018-128T10:15:30Z", "ordinal_date_time_no_millis"); assertSameDate("2018-128T10:15:30+0100", "ordinal_date_time_no_millis"); assertSameDate("2018-128T10:15:30+01:00", "ordinal_date_time_no_millis"); assertSameDate("2018-1T10:15:30Z", "ordinal_date_time_no_millis"); assertSameDate("2018-1T10:15:30+0100", "ordinal_date_time_no_millis"); assertSameDate("2018-1T10:15:30+01:00", "ordinal_date_time_no_millis"); assertSameDate("10:15:30.123Z", "time"); assertSameDate("10:15:30.123+0100", "time"); assertSameDate("10:15:30.123+01:00", "time"); assertSameDate("1:15:30.123Z", "time"); assertSameDate("1:15:30.123+0100", "time"); assertSameDate("1:15:30.123+01:00", "time"); assertSameDate("10:1:30.123Z", "time"); assertSameDate("10:1:30.123+0100", "time"); assertSameDate("10:1:30.123+01:00", "time"); assertSameDate("10:15:3.123Z", "time"); assertSameDate("10:15:3.123+0100", "time"); assertSameDate("10:15:3.123+01:00", "time"); assertParseException("10:15:3.1", "time"); assertParseException("10:15:3Z", "time"); assertSameDate("10:15:30Z", "time_no_millis"); assertSameDate("10:15:30+0100", "time_no_millis"); assertSameDate("10:15:30+01:00", "time_no_millis"); assertSameDate("01:15:30Z", "time_no_millis"); assertSameDate("01:15:30+0100", "time_no_millis"); assertSameDate("01:15:30+01:00", "time_no_millis"); assertSameDate("1:15:30Z", "time_no_millis"); assertSameDate("1:15:30+0100", "time_no_millis"); assertSameDate("1:15:30+01:00", "time_no_millis"); assertSameDate("10:5:30Z", "time_no_millis"); assertSameDate("10:5:30+0100", "time_no_millis"); assertSameDate("10:5:30+01:00", "time_no_millis"); assertSameDate("10:15:3Z", "time_no_millis"); assertSameDate("10:15:3+0100", "time_no_millis"); assertSameDate("10:15:3+01:00", "time_no_millis"); assertParseException("10:15:3", "time_no_millis"); assertSameDate("T10:15:30.123Z", "t_time"); assertSameDate("T10:15:30.123+0100", "t_time"); assertSameDate("T10:15:30.123+01:00", "t_time"); assertSameDate("T1:15:30.123Z", "t_time"); assertSameDate("T1:15:30.123+0100", "t_time"); assertSameDate("T1:15:30.123+01:00", "t_time"); assertSameDate("T10:1:30.123Z", "t_time"); assertSameDate("T10:1:30.123+0100", "t_time"); assertSameDate("T10:1:30.123+01:00", "t_time"); assertSameDate("T10:15:3.123Z", "t_time"); assertSameDate("T10:15:3.123+0100", "t_time"); assertSameDate("T10:15:3.123+01:00", "t_time"); assertParseException("T10:15:3.1", "t_time"); assertParseException("T10:15:3Z", "t_time"); assertSameDate("T10:15:30Z", "t_time_no_millis"); assertSameDate("T10:15:30+0100", "t_time_no_millis"); assertSameDate("T10:15:30+01:00", "t_time_no_millis"); assertSameDate("T1:15:30Z", "t_time_no_millis"); assertSameDate("T1:15:30+0100", "t_time_no_millis"); assertSameDate("T1:15:30+01:00", "t_time_no_millis"); assertSameDate("T10:1:30Z", "t_time_no_millis"); assertSameDate("T10:1:30+0100", "t_time_no_millis"); assertSameDate("T10:1:30+01:00", "t_time_no_millis"); assertSameDate("T10:15:3Z", "t_time_no_millis"); assertSameDate("T10:15:3+0100", "t_time_no_millis"); assertSameDate("T10:15:3+01:00", "t_time_no_millis"); assertParseException("T10:15:3", "t_time_no_millis"); assertSameDate("2012-W48-6", "week_date"); assertSameDate("2012-W01-6", "week_date"); assertSameDate("2012-W1-6", "week_date"); // joda comes up with a different exception message here, so we have to adapt assertJodaParseException("2012-W1-8", "week_date", "Cannot parse \\\\"2012-W1-8\\\\": Value 8 for dayOfWeek must be in the range [1,7]"); assertJavaTimeParseException("2012-W1-8", "week_date"); assertSameDate("2012-W48-6T10:15:30.123Z", "week_date_time"); assertSameDate("2012-W48-6T10:15:30.123+0100", "week_date_time"); assertSameDate("2012-W48-6T10:15:30.123+01:00", "week_date_time"); assertSameDate("2012-W1-6T10:15:30.123Z", "week_date_time"); assertSameDate("2012-W1-6T10:15:30.123+0100", "week_date_time"); assertSameDate("2012-W1-6T10:15:30.123+01:00", "week_date_time"); assertSameDate("2012-W48-6T10:15:30Z", "week_date_time_no_millis"); assertSameDate("2012-W48-6T10:15:30+0100", "week_date_time_no_millis"); assertSameDate("2012-W48-6T10:15:30+01:00", "week_date_time_no_millis"); assertSameDate("2012-W1-6T10:15:30Z", "week_date_time_no_millis"); assertSameDate("2012-W1-6T10:15:30+0100", "week_date_time_no_millis"); assertSameDate("2012-W1-6T10:15:30+01:00", "week_date_time_no_millis"); assertSameDate("2012", "year"); assertSameDate("1", "year"); assertSameDate("-2000", "year"); assertSameDate("2012-12", "yearMonth"); assertSameDate("1-1", "yearMonth"); assertSameDate("2012-12-31", "yearMonthDay"); assertSameDate("1-12-31", "yearMonthDay"); assertSameDate("2012-1-31", "yearMonthDay"); assertSameDate("2012-12-1", "yearMonthDay"); assertSameDate("2018", "week_year"); assertSameDate("1", "week_year"); assertSameDate("2017", "week_year"); assertSameDate("2018-W29", "weekyear_week"); assertSameDate("2018-W1", "weekyear_week"); assertSameDate("2012-W31-5", "weekyear_week_day"); assertSameDate("2012-W1-1", "weekyear_week_day"); }	should we then remove this?
public void testSerialization() throws Exception { org.elasticsearch.common.Rounding.DateTimeUnit randomDateTimeUnit = randomFrom(org.elasticsearch.common.Rounding.DateTimeUnit.values()); org.elasticsearch.common.Rounding rounding; if (randomBoolean()) { rounding = org.elasticsearch.common.Rounding.builder(randomDateTimeUnit).timeZone(ZoneOffset.UTC).build(); } else { rounding = org.elasticsearch.common.Rounding.builder(timeValue()).timeZone(ZoneOffset.UTC).build(); } BytesStreamOutput output = new BytesStreamOutput(); output.setVersion(Version.V_6_4_0); rounding.writeTo(output); Rounding roundingJoda = Rounding.Streams.read(output.bytes().streamInput()); org.elasticsearch.common.Rounding roundingJavaTime = org.elasticsearch.common.Rounding.read(output.bytes().streamInput()); int randomInt = randomIntBetween(1, 1_000_000_000); assertThat(roundingJoda.round(randomInt), is(roundingJavaTime.round(randomInt))); assertThat(roundingJoda.nextRoundingValue(randomInt), is(roundingJavaTime.nextRoundingValue(randomInt))); }	i guess this is to force the old behavior. does it make sense to add a comment here why this version has been chosen?
public void testValueFormat() { MappedFieldType ft = createDefaultFieldType(); long instant = DateFormatters.toZonedDateTime(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse("2015-10-12T14:10:55")) .toInstant().toEpochMilli(); assertEquals("2015-10-12T14:10:55.000Z", ft.docValueFormat(null, ZoneOffset.UTC).format(instant)); assertEquals("2015-10-12T15:10:55.000+01:00", ft.docValueFormat(null, ZoneOffset.ofHours(1)).format(instant)); assertEquals("2015", createDefaultFieldType().docValueFormat("YYYY", ZoneOffset.UTC).format(instant)); assertEquals(instant, ft.docValueFormat(null, ZoneOffset.UTC).parseLong("2015-10-12T14:10:55", false, null)); assertEquals(instant + 999, ft.docValueFormat(null, ZoneOffset.UTC).parseLong("2015-10-12T14:10:55", true, null)); long i = DateFormatters.toZonedDateTime(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse("2015-10-13")).toInstant().toEpochMilli(); assertEquals(i - 1, ft.docValueFormat(null, ZoneOffset.UTC).parseLong("2015-10-12||/d", true, null)); }	remove the commented line?
@Override protected void doAssertLuceneQuery(RangeQueryBuilder queryBuilder, Query query, SearchContext context) throws IOException { String expectedFieldName = expectedFieldName(queryBuilder.fieldName()); if (queryBuilder.from() == null && queryBuilder.to() == null) { final Query expectedQuery; if (context.mapperService().getIndexSettings().getIndexVersionCreated().onOrAfter(Version.V_6_1_0) && context.mapperService().fullName(queryBuilder.fieldName()).hasDocValues()) { expectedQuery = new ConstantScoreQuery(new DocValuesFieldExistsQuery(expectedFieldName)); } else if (context.mapperService().getIndexSettings().getIndexVersionCreated().onOrAfter(Version.V_6_1_0) && context.mapperService().fullName(queryBuilder.fieldName()).omitNorms() == false) { expectedQuery = new ConstantScoreQuery(new NormsFieldExistsQuery(expectedFieldName)); } else { expectedQuery = new ConstantScoreQuery(new TermQuery(new Term(FieldNamesFieldMapper.NAME, expectedFieldName))); } assertThat(query, equalTo(expectedQuery)); } else if (expectedFieldName.equals(DATE_FIELD_NAME) == false && expectedFieldName.equals(INT_FIELD_NAME) == false && expectedFieldName.equals(DATE_RANGE_FIELD_NAME) == false && expectedFieldName.equals(INT_RANGE_FIELD_NAME) == false) { assertThat(query, instanceOf(TermRangeQuery.class)); TermRangeQuery termRangeQuery = (TermRangeQuery) query; assertThat(termRangeQuery.getField(), equalTo(expectedFieldName)); assertThat(termRangeQuery.getLowerTerm(), equalTo(BytesRefs.toBytesRef(queryBuilder.from()))); assertThat(termRangeQuery.getUpperTerm(), equalTo(BytesRefs.toBytesRef(queryBuilder.to()))); assertThat(termRangeQuery.includesLower(), equalTo(queryBuilder.includeLower())); assertThat(termRangeQuery.includesUpper(), equalTo(queryBuilder.includeUpper())); } else if (expectedFieldName.equals(DATE_FIELD_NAME)) { assertThat(query, instanceOf(IndexOrDocValuesQuery.class)); query = ((IndexOrDocValuesQuery) query).getIndexQuery(); assertThat(query, instanceOf(PointRangeQuery.class)); MapperService mapperService = context.getQueryShardContext().getMapperService(); MappedFieldType mappedFieldType = mapperService.fullName(expectedFieldName); final Long fromInMillis; final Long toInMillis; // we have to normalize the incoming value into milliseconds since it could be literally anything if (mappedFieldType instanceof DateFieldMapper.DateFieldType) { fromInMillis = queryBuilder.from() == null ? null : ((DateFieldMapper.DateFieldType) mappedFieldType).parseToMilliseconds(queryBuilder.from(), !queryBuilder.includeLower(), queryBuilder.getDateTimeZone(), queryBuilder.getForceDateParser(), context.getQueryShardContext()); toInMillis = queryBuilder.to() == null ? null : ((DateFieldMapper.DateFieldType) mappedFieldType).parseToMilliseconds(queryBuilder.to(), queryBuilder.includeUpper(), queryBuilder.getDateTimeZone(), queryBuilder.getForceDateParser(), context.getQueryShardContext()); } else { fromInMillis = toInMillis = null; fail("unexpected mapped field type: [" + mappedFieldType.getClass() + "] " + mappedFieldType.toString()); } Long min = fromInMillis; Long max = toInMillis; long minLong, maxLong; if (min == null) { minLong = Long.MIN_VALUE; } else { minLong = min.longValue(); if (queryBuilder.includeLower() == false && minLong != Long.MAX_VALUE) { minLong++; } } if (max == null) { maxLong = Long.MAX_VALUE; } else { maxLong = max.longValue(); if (queryBuilder.includeUpper() == false && maxLong != Long.MIN_VALUE) { maxLong--; } } assertEquals(LongPoint.newRangeQuery(DATE_FIELD_NAME, minLong, maxLong), query); } else if (expectedFieldName.equals(INT_FIELD_NAME)) { assertThat(query, instanceOf(IndexOrDocValuesQuery.class)); query = ((IndexOrDocValuesQuery) query).getIndexQuery(); assertThat(query, instanceOf(PointRangeQuery.class)); Integer min = (Integer) queryBuilder.from(); Integer max = (Integer) queryBuilder.to(); int minInt, maxInt; if (min == null) { minInt = Integer.MIN_VALUE; } else { minInt = min.intValue(); if (queryBuilder.includeLower() == false && minInt != Integer.MAX_VALUE) { minInt++; } } if (max == null) { maxInt = Integer.MAX_VALUE; } else { maxInt = max.intValue(); if (queryBuilder.includeUpper() == false && maxInt != Integer.MIN_VALUE) { maxInt--; } } } else if (expectedFieldName.equals(DATE_RANGE_FIELD_NAME) || expectedFieldName.equals(INT_RANGE_FIELD_NAME)) { // todo can't check RangeFieldQuery because its currently package private (this will change) } else { throw new UnsupportedOperationException(); } }	i am not sure why this has changed?
static long parseDateOrThrow(String date, ParseField paramName, LongSupplier now) { DateMathParser dateMathParser = DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.toDateMathParser(); try { return dateMathParser.parse(date, now).toEpochMilli(); } catch (Exception e) { String msg = Messages.getMessage(Messages.REST_INVALID_DATETIME_PARAMS, paramName.getPreferredName(), date); throw new ElasticsearchParseException(msg, e); } }	i wonder whether it makes sense to switch to instant internally instead of operating on raw longs? (not in this pr but maybe this would be worth pursuing in the future)
@Override protected AutodetectResult createTestInstance() { Bucket bucket; List<AnomalyRecord> records = null; List<Influencer> influencers = null; Quantiles quantiles; ModelSnapshot modelSnapshot; ModelSizeStats.Builder modelSizeStats; ModelPlot modelPlot; Forecast forecast; ForecastRequestStats forecastRequestStats; CategoryDefinition categoryDefinition; FlushAcknowledgement flushAcknowledgement; String jobId = "foo"; if (randomBoolean()) { bucket = new Bucket(jobId, new Date(randomLongBetween(0, 3000000000000L)), randomNonNegativeLong()); } else { bucket = null; } if (randomBoolean()) { int size = randomInt(10); records = new ArrayList<>(size); for (int i = 0; i < size; i++) { AnomalyRecord record = new AnomalyRecord(jobId, new Date(randomLongBetween(0, 3000000000000L)), randomNonNegativeLong()); record.setProbability(randomDoubleBetween(0.0, 1.0, true)); records.add(record); } } if (randomBoolean()) { int size = randomInt(10); influencers = new ArrayList<>(size); for (int i = 0; i < size; i++) { Influencer influencer = new Influencer(jobId, randomAlphaOfLength(10), randomAlphaOfLength(10), new Date(randomLongBetween(0, 3000000000000L)), randomNonNegativeLong()); influencer.setProbability(randomDoubleBetween(0.0, 1.0, true)); influencers.add(influencer); } } if (randomBoolean()) { quantiles = QuantilesTests.createRandomized(); } else { quantiles = null; } if (randomBoolean()) { modelSnapshot = ModelSnapshotTests.createRandomized(); } else { modelSnapshot = null; } if (randomBoolean()) { modelSizeStats = new ModelSizeStats.Builder(jobId); modelSizeStats.setModelBytes(randomNonNegativeLong()); } else { modelSizeStats = null; } if (randomBoolean()) { modelPlot = new ModelPlot(jobId, new Date(randomLongBetween(0, 3000000000000L)), randomNonNegativeLong(), randomInt()); } else { modelPlot = null; } if (randomBoolean()) { forecast = new Forecast(jobId, randomAlphaOfLength(20), new Date(randomLongBetween(0, 3000000000000L)), randomNonNegativeLong(), randomInt()); } else { forecast = null; } if (randomBoolean()) { forecastRequestStats = new ForecastRequestStats(jobId, randomAlphaOfLength(20)); } else { forecastRequestStats = null; } if (randomBoolean()) { categoryDefinition = new CategoryDefinition(jobId); categoryDefinition.setCategoryId(randomLong()); } else { categoryDefinition = null; } if (randomBoolean()) { flushAcknowledgement = new FlushAcknowledgement(randomAlphaOfLengthBetween(1, 20), new Date(randomLongBetween(0, 3000000000000L))); } else { flushAcknowledgement = null; } return new AutodetectResult(bucket, records, influencers, quantiles, modelSnapshot, modelSizeStats == null ? null : modelSizeStats.build(), modelPlot, forecast, forecastRequestStats, categoryDefinition, flushAcknowledgement); }	would it make sense to introduce a #randomdate() method here as well similarly to autodetectresultprocessorit?
public Bucket createTestInstance(String jobId) { Bucket bucket = new Bucket(jobId, new Date(randomLongBetween(0, 3000000000000L)), randomNonNegativeLong()); if (randomBoolean()) { bucket.setAnomalyScore(randomDouble()); } if (randomBoolean()) { int size = randomInt(10); List<BucketInfluencer> bucketInfluencers = new ArrayList<>(size); for (int i = 0; i < size; i++) { BucketInfluencer bucketInfluencer = new BucketInfluencer(jobId, new Date(), 600); bucketInfluencer.setAnomalyScore(randomDouble()); bucketInfluencer.setInfluencerFieldName(randomAlphaOfLengthBetween(1, 20)); bucketInfluencer.setInitialAnomalyScore(randomDouble()); bucketInfluencer.setProbability(randomDouble()); bucketInfluencer.setRawAnomalyScore(randomDouble()); bucketInfluencers.add(bucketInfluencer); } bucket.setBucketInfluencers(bucketInfluencers); } if (randomBoolean()) { bucket.setEventCount(randomNonNegativeLong()); } if (randomBoolean()) { bucket.setInitialAnomalyScore(randomDouble()); } if (randomBoolean()) { bucket.setInterim(randomBoolean()); } if (randomBoolean()) { bucket.setProcessingTimeMs(randomLong()); } if (randomBoolean()) { int size = randomInt(10); List<AnomalyRecord> records = new ArrayList<>(size); for (int i = 0; i < size; i++) { AnomalyRecord anomalyRecord = new AnomalyRecordTests().createTestInstance(jobId); records.add(anomalyRecord); } bucket.setRecords(records); } if (randomBoolean()) { int size = randomInt(10); List<String> scheduledEvents = new ArrayList<>(size); IntStream.range(0, size).forEach(i -> scheduledEvents.add(randomAlphaOfLength(20))); bucket.setScheduledEvents(scheduledEvents); } return bucket; }	would it make sense to also introduce a #randomdate() method similarly to autodetectresultprocessorit?
public ModelPlot createTestInstance(String jobId) { ModelPlot modelPlot = new ModelPlot(jobId, new Date(randomLongBetween(0, 3000000000000L)), randomNonNegativeLong(), randomInt()); if (randomBoolean()) { modelPlot.setByFieldName(randomAlphaOfLengthBetween(1, 20)); } if (randomBoolean()) { modelPlot.setByFieldValue(randomAlphaOfLengthBetween(1, 20)); } if (randomBoolean()) { modelPlot.setPartitionFieldName(randomAlphaOfLengthBetween(1, 20)); } if (randomBoolean()) { modelPlot.setPartitionFieldValue(randomAlphaOfLengthBetween(1, 20)); } if (randomBoolean()) { modelPlot.setModelFeature(randomAlphaOfLengthBetween(1, 20)); } if (randomBoolean()) { modelPlot.setModelLower(randomDouble()); } if (randomBoolean()) { modelPlot.setModelUpper(randomDouble()); } if (randomBoolean()) { modelPlot.setModelMedian(randomDouble()); } if (randomBoolean()) { modelPlot.setActual(randomDouble()); } return modelPlot; }	would it make sense to also introduce a #randomdate() method similarly to autodetectresultprocessorit?
public static final long mortonEncode(final long geoHashLong) { final int level = (int)(geoHashLong&15); final short odd = (short)(level & 1); return BitUtil.flipFlop(((geoHashLong >>> 4) << odd) << (((12 - level) * 5) + (MORTON_OFFSET - odd))); }	what's the reason for deprecating this instead of just removing? it is completely internal right?
@Override public Filter parse(QueryParseContext parseContext) throws IOException, QueryParsingException { XContentParser parser = parseContext.parser(); Boolean cache = null; CacheKeyFilter.Key cacheKey = null; String fieldName = null; Object from = null; Object to = null; boolean includeLower = true; boolean includeUpper = true; String execution = "index"; String filterName = null; String currentFieldName = null; XContentParser.Token token; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_OBJECT) { fieldName = currentFieldName; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else { if ("from".equals(currentFieldName)) { from = parser.objectBytes(); } else if ("to".equals(currentFieldName)) { to = parser.objectBytes(); } else if ("include_lower".equals(currentFieldName) || "includeLower".equals(currentFieldName)) { includeLower = parser.booleanValue(); } else if ("include_upper".equals(currentFieldName) || "includeUpper".equals(currentFieldName)) { includeUpper = parser.booleanValue(); } else if ("gt".equals(currentFieldName)) { from = parser.objectBytes(); includeLower = false; } else if ("gte".equals(currentFieldName) || "ge".equals(currentFieldName)) { from = parser.objectBytes(); includeLower = true; } else if ("lt".equals(currentFieldName)) { to = parser.objectBytes(); includeUpper = false; } else if ("lte".equals(currentFieldName) || "le".equals(currentFieldName)) { to = parser.objectBytes(); includeUpper = true; } else { throw new QueryParsingException(parseContext.index(), "[range] filter does not support [" + currentFieldName + "]"); } } } } else if (token.isValue()) { if ("_name".equals(currentFieldName)) { filterName = parser.text(); } else if ("_cache".equals(currentFieldName)) { cache = parser.booleanValue(); } else if ("_cache_key".equals(currentFieldName) || "_cacheKey".equals(currentFieldName)) { cacheKey = new CacheKeyFilter.Key(parser.text()); } else if ("execution".equals(currentFieldName)) { execution = parser.text(); } else { throw new QueryParsingException(parseContext.index(), "[range] filter does not support [" + currentFieldName + "]"); } } } if (fieldName == null) { throw new QueryParsingException(parseContext.index(), "[range] filter no field specified for range filter"); } Filter filter = null; MapperService.SmartNameFieldMappers smartNameFieldMappers = parseContext.smartFieldMappers(fieldName); if (smartNameFieldMappers != null) { if (smartNameFieldMappers.hasMapper()) { if (execution.equals("index")) { if (cache == null) { cache = true; } filter = smartNameFieldMappers.mapper().rangeFilter(from, to, includeLower, includeUpper, parseContext); } else if ("fielddata".equals(execution)) { if (cache == null) { cache = false; } FieldMapper mapper = smartNameFieldMappers.mapper(); if (!(mapper instanceof NumberFieldMapper)) { throw new QueryParsingException(parseContext.index(), "[range] filter field [" + fieldName + "] is not a numeric type"); } filter = ((NumberFieldMapper) mapper).rangeFilter(parseContext.fieldData(), from, to, includeLower, includeUpper, parseContext); } else { throw new QueryParsingException(parseContext.index(), "[range] filter doesn't support [" + execution + "] execution"); } } } if (filter == null) { if (cache == null) { cache = true; } filter = new TermRangeFilter(fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper); } if (cache) { filter = parseContext.cacheFilter(filter, cacheKey); } filter = wrapSmartNameFilter(filter, smartNameFieldMappers, parseContext); if (filterName != null) { parseContext.addNamedFilter(filterName, filter); } return filter; }	cache flag might come after / before execution, so you might be setting the flag when it has been set by the user..., i would check it after the parsing
public void setDocument(int docId) { script.setDocument(docId); final Object value = script.run(); if (value == null) { resize(0); } else if (value instanceof Number) { resize(1); values[0] = ((Number) value).doubleValue(); } else if (value instanceof ReadableInstant) { resize(1); values[0] = ((ReadableInstant) value).getMillis(); } else if (value.getClass().isArray()) { resize(Array.getLength(value)); for (int i = 0; i < count(); ++i) { values[i] = toDoubleValue(Array.get(value, i)); } } else if (value instanceof Collection) { resize(((Collection<?>) value).size()); int i = 0; for (Object v : (Collection<?>) value) { values[i++] = toDoubleValue(v); } assert i == count(); } else { resize(1); values[0] = toDoubleValue(value); } sort(); }	this is just a bonus fix, unrelated to the date change right?
public void testLimitNestedDocsDefaultSettings() throws Exception{ Settings settings = Settings.builder().build(); MapperService mapperService = createIndex("test1", settings).mapperService(); String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties") .startObject("nested1").field("type", "nested").endObject() .endObject().endObject().endObject().string(); DocumentMapper docMapper = mapperService.documentMapperParser().parse("type", new CompressedXContent(mapping)); long defaultMaxNoNestedDocs = MapperService.INDEX_MAPPING_NESTED_DOCS_LIMIT_SETTING.get(settings); // parsing a doc with No. nested objects > defaultMaxNoNestedDocs fails XContentBuilder docBuilder = XContentFactory.jsonBuilder(); docBuilder.startObject(); { docBuilder.startArray("nested1"); { for(int i=0; i<=defaultMaxNoNestedDocs; i++) { docBuilder.startObject().field("f", i).endObject(); } } docBuilder.endArray(); } docBuilder.endObject(); SourceToParse source1 = SourceToParse.source("test1", "type", "1", docBuilder.bytes(), XContentType.JSON); MapperParsingException e = expectThrows(MapperParsingException.class, () -> docMapper.parse(source1)); assertEquals( "The number of nested documents has exceeded the allowed limit of [" + defaultMaxNoNestedDocs + "]. This limit can be set by changing the [" + MapperService.INDEX_MAPPING_NESTED_DOCS_LIMIT_SETTING.getKey() + "] index level setting.", e.getMessage() ); }	nit: some whitespaces around the operators etc... would be nice
public void testLimitNestedDocsDefaultSettings() throws Exception{ Settings settings = Settings.builder().build(); MapperService mapperService = createIndex("test1", settings).mapperService(); String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties") .startObject("nested1").field("type", "nested").endObject() .endObject().endObject().endObject().string(); DocumentMapper docMapper = mapperService.documentMapperParser().parse("type", new CompressedXContent(mapping)); long defaultMaxNoNestedDocs = MapperService.INDEX_MAPPING_NESTED_DOCS_LIMIT_SETTING.get(settings); // parsing a doc with No. nested objects > defaultMaxNoNestedDocs fails XContentBuilder docBuilder = XContentFactory.jsonBuilder(); docBuilder.startObject(); { docBuilder.startArray("nested1"); { for(int i=0; i<=defaultMaxNoNestedDocs; i++) { docBuilder.startObject().field("f", i).endObject(); } } docBuilder.endArray(); } docBuilder.endObject(); SourceToParse source1 = SourceToParse.source("test1", "type", "1", docBuilder.bytes(), XContentType.JSON); MapperParsingException e = expectThrows(MapperParsingException.class, () -> docMapper.parse(source1)); assertEquals( "The number of nested documents has exceeded the allowed limit of [" + defaultMaxNoNestedDocs + "]. This limit can be set by changing the [" + MapperService.INDEX_MAPPING_NESTED_DOCS_LIMIT_SETTING.getKey() + "] index level setting.", e.getMessage() ); }	i'm suprised this triggers the no. fields warnings since "f" is not the field thats defined with the "nested" type above? i would have expected this to not trigger the limit but the exception tested below seems to indicate it triggers the setting. what am i missing?
public void testLimitNestedDocsDefaultSettings() throws Exception{ Settings settings = Settings.builder().build(); MapperService mapperService = createIndex("test1", settings).mapperService(); String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").startObject("properties") .startObject("nested1").field("type", "nested").endObject() .endObject().endObject().endObject().string(); DocumentMapper docMapper = mapperService.documentMapperParser().parse("type", new CompressedXContent(mapping)); long defaultMaxNoNestedDocs = MapperService.INDEX_MAPPING_NESTED_DOCS_LIMIT_SETTING.get(settings); // parsing a doc with No. nested objects > defaultMaxNoNestedDocs fails XContentBuilder docBuilder = XContentFactory.jsonBuilder(); docBuilder.startObject(); { docBuilder.startArray("nested1"); { for(int i=0; i<=defaultMaxNoNestedDocs; i++) { docBuilder.startObject().field("f", i).endObject(); } } docBuilder.endArray(); } docBuilder.endObject(); SourceToParse source1 = SourceToParse.source("test1", "type", "1", docBuilder.bytes(), XContentType.JSON); MapperParsingException e = expectThrows(MapperParsingException.class, () -> docMapper.parse(source1)); assertEquals( "The number of nested documents has exceeded the allowed limit of [" + defaultMaxNoNestedDocs + "]. This limit can be set by changing the [" + MapperService.INDEX_MAPPING_NESTED_DOCS_LIMIT_SETTING.getKey() + "] index level setting.", e.getMessage() ); }	just as feedback, nothing to change really, but i liked the previous variable name better ;-)
@Override public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null || getClass() != obj.getClass()) { return false; } DateTimeParseProcessor other = (DateTimeParseProcessor) obj; return Objects.equals(parser, other.parser) && Objects.equals(left(), other.left()) && Objects.equals(right(), other.right()); }	is this used? if so it shouldn't contain only the parser.tostring() but also left & and right.
@Override public void readFrom(StreamInput in) throws IOException { this.name = in.readString(); this.description = in.readString(); this.site = in.readBoolean(); this.jvm = in.readBoolean(); if (in.getVersion().onOrAfter(Version.V_1_0_0_RC2)) { this.version = in.readString(); } }	for this to work absolutely correctly you need to do: if (in.getversion().onorafter(version.v_1_0_0_rc2)) { this.version = in.readstring(); } else { this.version = default_version; } since if this instance was used before it might not be initialised with default_version which you are expecting.
private ImmutableList<Tuple<PluginInfo,Plugin>> loadSitePlugins() { ImmutableList.Builder<Tuple<PluginInfo, Plugin>> sitePlugins = ImmutableList.builder(); List<String> loadedJvmPlugins = new ArrayList<String>(); // Already known jvm plugins are ignored for(Tuple<PluginInfo, Plugin> tuple : plugins) { if (tuple.v1().isSite()) { loadedJvmPlugins.add(tuple.v1().getName()); } } // Let's try to find all _site plugins we did not already found File pluginsFile = environment.pluginsFile(); if (!pluginsFile.exists() || !pluginsFile.isDirectory()) { return sitePlugins.build(); } for (File pluginFile : pluginsFile.listFiles()) { if (!loadedJvmPlugins.contains(pluginFile.getName())) { File sitePluginDir = new File(pluginFile, "_site"); if (sitePluginDir.exists()) { // We have a _site plugin. Let's try to get more information on it String name = pluginFile.getName(); String version = PluginInfo.DEFAULT_VERSION; String description = PluginInfo.DEFAULT_DESCRIPTION; // We check if es-plugin.properties exists in plugin/_site dir File pluginPropFile = new File(sitePluginDir, ES_PLUGIN_PROPERTIES); if (pluginPropFile.exists()) { Properties pluginProps = new Properties(); InputStream is = null; try { is = new FileInputStream(pluginPropFile.getAbsolutePath()); pluginProps.load(is); description = pluginProps.getProperty("description", PluginInfo.DEFAULT_DESCRIPTION); version = pluginProps.getProperty("version", PluginInfo.DEFAULT_VERSION); } catch (Exception e) { // Can not load properties for this site plugin. Ignoring. } finally { if (is != null) { try { is.close(); } catch (IOException e) { // ignore } } } } if (logger.isTraceEnabled()) { logger.trace("found a site plugin name [{}], version [{}], description [{}]", name, version, description); } sitePlugins.add(new Tuple<PluginInfo, Plugin>(new PluginInfo(name, description, true, false, version), null)); } } } return sitePlugins.build(); }	maybe you wanna log that as well? also do you maybe wanna catch runtimeexception as well?
private ImmutableList<Tuple<PluginInfo,Plugin>> loadSitePlugins() { ImmutableList.Builder<Tuple<PluginInfo, Plugin>> sitePlugins = ImmutableList.builder(); List<String> loadedJvmPlugins = new ArrayList<String>(); // Already known jvm plugins are ignored for(Tuple<PluginInfo, Plugin> tuple : plugins) { if (tuple.v1().isSite()) { loadedJvmPlugins.add(tuple.v1().getName()); } } // Let's try to find all _site plugins we did not already found File pluginsFile = environment.pluginsFile(); if (!pluginsFile.exists() || !pluginsFile.isDirectory()) { return sitePlugins.build(); } for (File pluginFile : pluginsFile.listFiles()) { if (!loadedJvmPlugins.contains(pluginFile.getName())) { File sitePluginDir = new File(pluginFile, "_site"); if (sitePluginDir.exists()) { // We have a _site plugin. Let's try to get more information on it String name = pluginFile.getName(); String version = PluginInfo.DEFAULT_VERSION; String description = PluginInfo.DEFAULT_DESCRIPTION; // We check if es-plugin.properties exists in plugin/_site dir File pluginPropFile = new File(sitePluginDir, ES_PLUGIN_PROPERTIES); if (pluginPropFile.exists()) { Properties pluginProps = new Properties(); InputStream is = null; try { is = new FileInputStream(pluginPropFile.getAbsolutePath()); pluginProps.load(is); description = pluginProps.getProperty("description", PluginInfo.DEFAULT_DESCRIPTION); version = pluginProps.getProperty("version", PluginInfo.DEFAULT_VERSION); } catch (Exception e) { // Can not load properties for this site plugin. Ignoring. } finally { if (is != null) { try { is.close(); } catch (IOException e) { // ignore } } } } if (logger.isTraceEnabled()) { logger.trace("found a site plugin name [{}], version [{}], description [{}]", name, version, description); } sitePlugins.add(new Tuple<PluginInfo, Plugin>(new PluginInfo(name, description, true, false, version), null)); } } } return sitePlugins.build(); }	just use ioutils.closewhilehandlingexception(is) instead of the 6 lines in the finally block
@Override public Map<String, Processor.Factory> getProcessors(Processor.Parameters parameters) { if (this.enabled == false) { return Collections.emptyMap(); } InferenceProcessor.Factory inferenceFactory = new InferenceProcessor.Factory(parameters.client, parameters.ingestService.getClusterService(), this.settings, parameters.ingestService, getLicenseState()); getLicenseState().addListener(inferenceFactory); parameters.ingestService.addIngestClusterStateListener(inferenceFactory); return Collections.singletonMap(InferenceProcessor.TYPE, inferenceFactory); }	this needs some due diligence. if there was an ingest pipeline in the cluster state containing an inference ingest processor and the cluster was restarted with xpack.ml.enabled: false what would happen? the tentative plan for restoring full cluster snapshots in cloud in the future is to disable all x-pack plugins during the snapshot restore, which will lead to this exact situation. if there's any doubt about what will happen it might be safer to allow the ingest processors to exist but just have them fail on every document they process (via the failure response from the infer model action) if the license is invalid.
public void testMinVersionSerialization() throws IOException { PersistentTasksCustomMetaData.Builder tasks = PersistentTasksCustomMetaData.builder(); Version minVersion = getFirstVersion(); final Version streamVersion = randomVersionBetween(random(), minVersion, getPreviousVersion(Version.CURRENT)); tasks.addTask("test_compatible_version", TestPersistentTasksExecutor.NAME, new TestParams(null, randomVersionBetween(random(), minVersion, streamVersion), randomBoolean() ? Optional.empty() : Optional.of("test")), randomAssignment()); tasks.addTask("test_incompatible_version", TestPersistentTasksExecutor.NAME, new TestParams(null, randomVersionBetween(random(), getNextVersion(streamVersion), Version.CURRENT), randomBoolean() ? Optional.empty() : Optional.of("test")), randomAssignment()); final BytesStreamOutput out = new BytesStreamOutput(); out.setVersion(streamVersion); Set<String> features = new HashSet<>(); if (randomBoolean()) { features.add("test"); } if (randomBoolean()) { features.add(TransportClient.TRANSPORT_CLIENT_FEATURE); } tasks.build().writeTo(out); final StreamInput input = out.bytes().streamInput(); input.setVersion(streamVersion); PersistentTasksCustomMetaData read = new PersistentTasksCustomMetaData(new NamedWriteableAwareStreamInput(input, getNamedWriteableRegistry())); Set<String> expectedIds = new HashSet<>(tasks.getCurrentTaskIds()); expectedIds.remove("test_incompatible_version"); assertThat(read.taskMap().keySet(), equalTo(expectedIds)); }	the features are never set on the stream.
public void testMinVersionSerialization() throws IOException { PersistentTasksCustomMetaData.Builder tasks = PersistentTasksCustomMetaData.builder(); Version minVersion = getFirstVersion(); final Version streamVersion = randomVersionBetween(random(), minVersion, getPreviousVersion(Version.CURRENT)); tasks.addTask("test_compatible_version", TestPersistentTasksExecutor.NAME, new TestParams(null, randomVersionBetween(random(), minVersion, streamVersion), randomBoolean() ? Optional.empty() : Optional.of("test")), randomAssignment()); tasks.addTask("test_incompatible_version", TestPersistentTasksExecutor.NAME, new TestParams(null, randomVersionBetween(random(), getNextVersion(streamVersion), Version.CURRENT), randomBoolean() ? Optional.empty() : Optional.of("test")), randomAssignment()); final BytesStreamOutput out = new BytesStreamOutput(); out.setVersion(streamVersion); Set<String> features = new HashSet<>(); if (randomBoolean()) { features.add("test"); } if (randomBoolean()) { features.add(TransportClient.TRANSPORT_CLIENT_FEATURE); } tasks.build().writeTo(out); final StreamInput input = out.bytes().streamInput(); input.setVersion(streamVersion); PersistentTasksCustomMetaData read = new PersistentTasksCustomMetaData(new NamedWriteableAwareStreamInput(input, getNamedWriteableRegistry())); Set<String> expectedIds = new HashSet<>(tasks.getCurrentTaskIds()); expectedIds.remove("test_incompatible_version"); assertThat(read.taskMap().keySet(), equalTo(expectedIds)); }	this is indirect. why not collections.singleton("test_compatible_version")? then the local is not even needed and it saves a few lines too.
public void testMinVersionSerialization() throws IOException { PersistentTasksCustomMetaData.Builder tasks = PersistentTasksCustomMetaData.builder(); Version minVersion = getFirstVersion(); final Version streamVersion = randomVersionBetween(random(), minVersion, getPreviousVersion(Version.CURRENT)); tasks.addTask("test_compatible_version", TestPersistentTasksExecutor.NAME, new TestParams(null, randomVersionBetween(random(), minVersion, streamVersion), randomBoolean() ? Optional.empty() : Optional.of("test")), randomAssignment()); tasks.addTask("test_incompatible_version", TestPersistentTasksExecutor.NAME, new TestParams(null, randomVersionBetween(random(), getNextVersion(streamVersion), Version.CURRENT), randomBoolean() ? Optional.empty() : Optional.of("test")), randomAssignment()); final BytesStreamOutput out = new BytesStreamOutput(); out.setVersion(streamVersion); Set<String> features = new HashSet<>(); if (randomBoolean()) { features.add("test"); } if (randomBoolean()) { features.add(TransportClient.TRANSPORT_CLIENT_FEATURE); } tasks.build().writeTo(out); final StreamInput input = out.bytes().streamInput(); input.setVersion(streamVersion); PersistentTasksCustomMetaData read = new PersistentTasksCustomMetaData(new NamedWriteableAwareStreamInput(input, getNamedWriteableRegistry())); Set<String> expectedIds = new HashSet<>(tasks.getCurrentTaskIds()); expectedIds.remove("test_incompatible_version"); assertThat(read.taskMap().keySet(), equalTo(expectedIds)); }	same comment on the indirectness.
public static Version getNextVersion(Version version) { for (int i = 0; i < ALL_VERSIONS.size(); i++) { Version v = ALL_VERSIONS.get(i); if (v.after(version)) { return v; } } throw new IllegalArgumentException("couldn't find any released versions after [" + version + "]"); } /** * Get the released version before {@link Version#CURRENT}	this should loop over released_versions.
@Override public String toString() { return Strings.toString(this); } } public static class JobParams implements PersistentTaskParams { /** TODO Remove in 7.0.0 */ public static final ParseField IGNORE_DOWNTIME = new ParseField("ignore_downtime"); public static final ParseField TIMEOUT = new ParseField("timeout"); public static ObjectParser<JobParams, Void> PARSER = new ObjectParser<>(TASK_NAME, JobParams::new); static { PARSER.declareString(JobParams::setJobId, Job.ID); PARSER.declareBoolean((p, v) -> {}, IGNORE_DOWNTIME); PARSER.declareString((params, val) -> params.setTimeout(TimeValue.parseTimeValue(val, TIMEOUT.getPreferredName())), TIMEOUT); } public static JobParams fromXContent(XContentParser parser) { return parseRequest(null, parser); } public static JobParams parseRequest(String jobId, XContentParser parser) { JobParams params = PARSER.apply(parser, null); if (jobId != null) { params.jobId = jobId; } return params; } private String jobId; // A big state can take a while to restore. For symmetry with the _close endpoint any // changes here should be reflected there too. private TimeValue timeout = MachineLearningField.STATE_PERSIST_RESTORE_TIMEOUT; JobParams() { } public JobParams(String jobId) { this.jobId = ExceptionsHelper.requireNonNull(jobId, Job.ID.getPreferredName()); } public JobParams(StreamInput in) throws IOException { jobId = in.readString(); if (in.getVersion().onOrBefore(Version.V_5_5_0)) { // Read `ignoreDowntime` in.readBoolean(); } timeout = TimeValue.timeValueMillis(in.readVLong()); } public String getJobId() { return jobId; } public void setJobId(String jobId) { this.jobId = jobId; } public TimeValue getTimeout() { return timeout; } public void setTimeout(TimeValue timeout) { this.timeout = timeout; } @Override public String getWriteableName() { return TASK_NAME; } @Override public void writeTo(StreamOutput out) throws IOException { out.writeString(jobId); if (out.getVersion().onOrBefore(Version.V_5_5_0)) { // Write `ignoreDowntime` - true by default out.writeBoolean(true); } out.writeVLong(timeout.millis()); } @Override public XContentBuilder toXContent(XContentBuilder builder, ToXContent.Params params) throws IOException { builder.startObject(); builder.field(Job.ID.getPreferredName(), jobId); builder.field(TIMEOUT.getPreferredName(), timeout.getStringRep()); builder.endObject(); return builder; } @Override public int hashCode() { return Objects.hash(jobId, timeout); } @Override public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null || obj.getClass() != getClass()) { return false; } OpenJobAction.JobParams other = (OpenJobAction.JobParams) obj; return Objects.equals(jobId, other.jobId) && Objects.equals(timeout, other.timeout); } @Override public String toString() { return Strings.toString(this); } @Override public Version getMinimalSupportedVersion() { return Version.V_5_4_0; } @Override public Optional<String> getRequiredFeature() { return XPackClientPlugin.X_PACK_FEATURE; } } public static class Response extends AcknowledgedResponse { public Response() { super(); } public Response(boolean acknowledged) { super(acknowledged); } @Override public void readFrom(StreamInput in) throws IOException { readAcknowledged(in); } @Override public void writeTo(StreamOutput out) throws IOException { writeAcknowledged(out); } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; AcknowledgedResponse that = (AcknowledgedResponse) o; return isAcknowledged() == that.isAcknowledged(); } @Override public int hashCode() { return Objects.hash(isAcknowledged()); } } public interface JobTaskMatcher { static boolean match(Task task, String expectedJobId) { String expectedDescription = "job-" + expectedJobId; return task instanceof JobTaskMatcher && expectedDescription.equals(task.getDescription()); } } static class RequestBuilder extends ActionRequestBuilder<Request, Response> { RequestBuilder(ElasticsearchClient client, OpenJobAction action) { super(client, action, new Request()); }	this can come from xpackpersistenttaskparams.
@Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } Request other = (Request) obj; return Objects.equals(params, other.params); } } public static class DatafeedParams implements PersistentTaskParams { public static ObjectParser<DatafeedParams, Void> PARSER = new ObjectParser<>(TASK_NAME, DatafeedParams::new); static { PARSER.declareString((params, datafeedId) -> params.datafeedId = datafeedId, DatafeedConfig.ID); PARSER.declareString((params, startTime) -> params.startTime = parseDateOrThrow( startTime, START_TIME, System::currentTimeMillis), START_TIME); PARSER.declareString(DatafeedParams::setEndTime, END_TIME); PARSER.declareString((params, val) -> params.setTimeout(TimeValue.parseTimeValue(val, TIMEOUT.getPreferredName())), TIMEOUT); } static long parseDateOrThrow(String date, ParseField paramName, LongSupplier now) { DateMathParser dateMathParser = new DateMathParser(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER); try { return dateMathParser.parse(date, now); } catch (Exception e) { String msg = Messages.getMessage(Messages.REST_INVALID_DATETIME_PARAMS, paramName.getPreferredName(), date); throw new ElasticsearchParseException(msg, e); } } public static DatafeedParams fromXContent(XContentParser parser) { return parseRequest(null, parser); } public static DatafeedParams parseRequest(String datafeedId, XContentParser parser) { DatafeedParams params = PARSER.apply(parser, null); if (datafeedId != null) { params.datafeedId = datafeedId; } return params; } public DatafeedParams(String datafeedId, long startTime) { this.datafeedId = ExceptionsHelper.requireNonNull(datafeedId, DatafeedConfig.ID.getPreferredName()); this.startTime = startTime; } public DatafeedParams(String datafeedId, String startTime) { this(datafeedId, parseDateOrThrow(startTime, START_TIME, System::currentTimeMillis)); } public DatafeedParams(StreamInput in) throws IOException { datafeedId = in.readString(); startTime = in.readVLong(); endTime = in.readOptionalLong(); timeout = TimeValue.timeValueMillis(in.readVLong()); } DatafeedParams() { } private String datafeedId; private long startTime; private Long endTime; private TimeValue timeout = TimeValue.timeValueSeconds(20); public String getDatafeedId() { return datafeedId; } public long getStartTime() { return startTime; } public Long getEndTime() { return endTime; } public void setEndTime(String endTime) { setEndTime(parseDateOrThrow(endTime, END_TIME, System::currentTimeMillis)); } public void setEndTime(Long endTime) { this.endTime = endTime; } public TimeValue getTimeout() { return timeout; } public void setTimeout(TimeValue timeout) { this.timeout = timeout; } @Override public String getWriteableName() { return TASK_NAME; } @Override public Version getMinimalSupportedVersion() { return Version.V_5_4_0; } @Override public Optional<String> getRequiredFeature() { return XPackPlugin.X_PACK_FEATURE; } @Override public void writeTo(StreamOutput out) throws IOException { out.writeString(datafeedId); out.writeVLong(startTime); out.writeOptionalLong(endTime); out.writeVLong(timeout.millis()); } @Override public XContentBuilder toXContent(XContentBuilder builder, ToXContent.Params params) throws IOException { builder.startObject(); builder.field(DatafeedConfig.ID.getPreferredName(), datafeedId); builder.field(START_TIME.getPreferredName(), String.valueOf(startTime)); if (endTime != null) { builder.field(END_TIME.getPreferredName(), String.valueOf(endTime)); } builder.field(TIMEOUT.getPreferredName(), timeout.getStringRep()); builder.endObject(); return builder; } @Override public int hashCode() { return Objects.hash(datafeedId, startTime, endTime, timeout); } @Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } DatafeedParams other = (DatafeedParams) obj; return Objects.equals(datafeedId, other.datafeedId) && Objects.equals(startTime, other.startTime) && Objects.equals(endTime, other.endTime) && Objects.equals(timeout, other.timeout); } } public static class Response extends AcknowledgedResponse { public Response() { super(); } public Response(boolean acknowledged) { super(acknowledged); } @Override public void readFrom(StreamInput in) throws IOException { readAcknowledged(in); } @Override public void writeTo(StreamOutput out) throws IOException { writeAcknowledged(out); } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; AcknowledgedResponse that = (AcknowledgedResponse) o; return isAcknowledged() == that.isAcknowledged(); } @Override public int hashCode() { return Objects.hash(isAcknowledged()); } } static class RequestBuilder extends ActionRequestBuilder<Request, Response> { RequestBuilder(ElasticsearchClient client, StartDatafeedAction action) { super(client, action, new Request()); } }	this can come from xpackpersistenttaskparams.
private void validateRecoveryStatus(RecoveryStatus onGoingRecovery, ShardId shardId) { if (onGoingRecovery == null) { // shard is getting closed on us throw new IndexShardClosedException(shardId); } if (onGoingRecovery.indexShard.state() == IndexShardState.CLOSED) { // mark sentCanceledToSource after cancel recovery, o.w. cancelRecovery will do nothing removeAndCleanOnGoingRecovery(onGoingRecovery); onGoingRecovery.sentCanceledToSource = true; throw new IndexShardClosedException(shardId); } if (onGoingRecovery.isCanceled()) { onGoingRecovery.sentCanceledToSource = true; throw new IndexShardClosedException(shardId); } }	comment not relevant anymore, right?
private void maybeLogSlowMessage(boolean success) { final long logThreshold = slowLogThresholdMs; if (logThreshold > 0) { final long took = threadPool.relativeTimeInMillis() - startTime; handlingTimeTracker.addHandlingTime(took); if (took > logThreshold) { logger.warn( "sending transport message [{}] of size [{}] on [{}] took [{}ms] which is above the warn " + "threshold of [{}ms] with success [{}]", message, messageSize, channel, took, logThreshold, success ); } } } }); } catch (RuntimeException ex) { listener.onFailure(ex); CloseableChannel.closeChannel(channel); throw ex; } }	this may be spurious since it counts time spent waiting for the channel to become writeable (cf https://github.com/elastic/elasticsearch/issues/77838). should we track it separately from the inbound time tracking?
protected final List<?> fetchFromDocValues(MapperService mapperService, MappedFieldType ft, DocValueFormat format, Object sourceValue) throws IOException { BiFunction<MappedFieldType, Supplier<SearchLookup>, IndexFieldData<?>> fieldDataLookup = (mft, lookupSource) -> mft .fielddataBuilder("test", () -> { throw new UnsupportedOperationException(); }) .build(new IndexFieldDataCache.None(), new NoneCircuitBreakerService(), mapperService); SetOnce<List<?>> result = new SetOnce<>(); withLuceneIndex(mapperService, iw -> { iw.addDocument(mapperService.documentMapper().parse(source(b -> b.field(ft.name(), sourceValue))).rootDoc()); }, iw -> { IndexSearcher indexSearcher = newSearcher(iw); SearchLookup lookup = new SearchLookup(mapperService, fieldDataLookup); ValueFetcher valueFetcher = new DocValueFetcher(format, lookup.doc().getForField(ft)); indexSearcher.search(new MatchAllDocsQuery(), new Collector() { @Override public ScoreMode scoreMode() { return ScoreMode.COMPLETE_NO_SCORES; } @Override public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException { valueFetcher.setNextReader(context); return new LeafCollector() { @Override public void setScorer(Scorable scorer) throws IOException {} @Override public void collect(int doc) throws IOException { lookup.source().setSegmentAndDocument(context, doc); result.set(valueFetcher.fetchValues(lookup.source())); } }; } }); }); return result.get(); }	could we just access the leaf reader directly? this is pretty common when you're just testing a single document: leafreadercontext context = searcher.getindexreader().leaves().get(0);
private static void logSearchResponse(SearchResponse response, Logger logger) { List<Aggregation> aggs = Collections.emptyList(); if (response.getAggregations() != null) { aggs = response.getAggregations().asList(); } StringBuilder aggsNames = new StringBuilder(); for (int i = 0; i < aggs.size(); i++) { aggsNames.append(aggs.get(i).getName() + (i + 1 == aggs.size() ? "" : ", ")); } SearchHit[] hits = response.getHits().getHits(); int count = hits != null ? hits.length : 0; logger.trace("Got search response [hits {}, {} aggregations: [{}], {} failed shards, {} skipped shards, " + "{} successful shards, {} total shards, took {}, timed out [{}]]", count, aggs.size(), aggsNames, response.getFailedShards(), response.getSkippedShards(), response.getSuccessfulShards(), response.getTotalShards(), response.getTook(), response.isTimedOut()); }	why gettotalhits() wasn't the best option?
public void testSqlDefaults() { XPackLicenseState licenseState = TestUtils.newTestLicenseState(); assertThat(licenseState.checkFeature(XPackLicenseState.Feature.JDBC), is(true)); }	nit: these testsqlxxx now has nothing to do with sql. should we call them testjdbcxxx instead?
protected void masterOperation( Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener ) { listener.onResponse(new XPackUsageFeatureResponse(new EnrichFeatureSetUsage(true))); }	for these basic features, i think it is better to drop the available parameter in its constructor similar to the enabled parameter, i.e. have something like: java public enrichfeaturesetusage() { super(xpackfield.enrich, true, true); }
protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener) { Metadata metadata = state.metadata(); IndexLifecycleMetadata lifecycleMetadata = metadata.custom(IndexLifecycleMetadata.TYPE); final IndexLifecycleFeatureSetUsage usage; if (lifecycleMetadata != null) { Map<String, Integer> policyUsage = new HashMap<>(); metadata.indices().forEach(entry -> { String policyName = LifecycleSettings.LIFECYCLE_NAME_SETTING.get(entry.value.getSettings()); Integer indicesManaged = policyUsage.get(policyName); if (indicesManaged == null) { indicesManaged = 1; } else { indicesManaged = indicesManaged + 1; } policyUsage.put(policyName, indicesManaged); }); List<IndexLifecycleFeatureSetUsage.PolicyStats> policyStats = lifecycleMetadata.getPolicies().values().stream().map(policy -> { Map<String, IndexLifecycleFeatureSetUsage.PhaseStats> phaseStats = policy.getPhases().values().stream().map(phase -> { String[] actionNames = phase.getActions().keySet().toArray(new String[phase.getActions().size()]); return new Tuple<>(phase.getName(), new IndexLifecycleFeatureSetUsage.PhaseStats(phase.getMinimumAge(), actionNames)); }).collect(Collectors.toMap(Tuple::v1, Tuple::v2)); return new IndexLifecycleFeatureSetUsage.PolicyStats(phaseStats, policyUsage.getOrDefault(policy.getName(), 0)); }).collect(Collectors.toList()); usage = new IndexLifecycleFeatureSetUsage(true, policyStats); } else { usage = new IndexLifecycleFeatureSetUsage(true); } listener.onResponse(new XPackUsageFeatureResponse(usage)); }	same here, the available parameter can be dropped from the constructor
protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener) { final SnapshotLifecycleMetadata slmMeta = state.metadata().custom(SnapshotLifecycleMetadata.TYPE); final SLMFeatureSetUsage usage = new SLMFeatureSetUsage(true, slmMeta == null ? null : slmMeta.getStats()); listener.onResponse(new XPackUsageFeatureResponse(usage)); }	here as well for the available parameter.
protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener) { final boolean collectionEnabled = monitoringService != null && monitoringService.isMonitoringActive(); var usage = new MonitoringFeatureSetUsage(true, collectionEnabled, exportersUsage(exporters)); listener.onResponse(new XPackUsageFeatureResponse(usage)); }	again, the available parameter can be removed.
protected boolean shouldCollect(final boolean isElectedMaster) { // this can only run when monitoring is allowed and CCR is enabled and allowed, but also only on the elected master node return isElectedMaster && XPackSettings.CCR_ENABLED_SETTING.get(settings) && licenseState.checkFeature(XPackLicenseState.Feature.CCR); }	i'd leave this untouched. the change assumes the superclass call always return true, which is the case with changes of this pr. but strictly speaking, the logic is more foolproof to keep this call so that any future changes will be covered.
protected boolean shouldCollect(final boolean isElectedMaster) { // this can only run when monitoring is allowed and CCR is enabled and allowed, but also only on the elected master node return isElectedMaster && XPackSettings.CCR_ENABLED_SETTING.get(settings) && licenseState.checkFeature(XPackLicenseState.Feature.CCR); }	similarly, i'd leave this method call untouched.
protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener) { Map<String, Object> sslUsage = sslUsage(settings); Map<String, Object> tokenServiceUsage = tokenServiceUsage(settings); Map<String, Object> apiKeyServiceUsage = apiKeyServiceUsage(settings); Map<String, Object> auditUsage = auditUsage(settings); Map<String, Object> ipFilterUsage = ipFilterUsage(ipFilter); Map<String, Object> anonymousUsage = singletonMap("enabled", AnonymousUser.isAnonymousEnabled(settings)); Map<String, Object> fips140Usage = fips140Usage(settings); final AtomicReference<Map<String, Object>> rolesUsageRef = new AtomicReference<>(); final AtomicReference<Map<String, Object>> roleMappingUsageRef = new AtomicReference<>(); final AtomicReference<Map<String, Object>> realmsUsageRef = new AtomicReference<>(); final boolean enabled = licenseState.isSecurityEnabled(); final CountDown countDown = new CountDown(3); final Runnable doCountDown = () -> { if (countDown.countDown()) { var usage = new SecurityFeatureSetUsage(true, enabled, realmsUsageRef.get(), rolesUsageRef.get(), roleMappingUsageRef.get(), sslUsage, auditUsage, ipFilterUsage, anonymousUsage, tokenServiceUsage, apiKeyServiceUsage, fips140Usage); listener.onResponse(new XPackUsageFeatureResponse(usage)); } }; final ActionListener<Map<String, Object>> rolesStoreUsageListener = ActionListener.wrap(rolesStoreUsage -> { rolesUsageRef.set(rolesStoreUsage); doCountDown.run(); }, listener::onFailure); final ActionListener<Map<String, Object>> roleMappingStoreUsageListener = ActionListener.wrap(nativeRoleMappingStoreUsage -> { Map<String, Object> usage = singletonMap("native", nativeRoleMappingStoreUsage); roleMappingUsageRef.set(usage); doCountDown.run(); }, listener::onFailure); final ActionListener<Map<String, Object>> realmsUsageListener = ActionListener.wrap(realmsUsage -> { realmsUsageRef.set(realmsUsage); doCountDown.run(); }, listener::onFailure); if (rolesStore == null || enabled == false) { rolesStoreUsageListener.onResponse(Collections.emptyMap()); } else { rolesStore.usageStats(rolesStoreUsageListener); } if (roleMappingStore == null || enabled == false) { roleMappingStoreUsageListener.onResponse(Collections.emptyMap()); } else { roleMappingStore.usageStats(roleMappingStoreUsageListener); } if (realms == null || enabled == false) { realmsUsageListener.onResponse(Collections.emptyMap()); } else { realms.usageStats(realmsUsageListener); } }	another one for the available parameter that can be dropped.
public <Request extends ActionRequest, Response extends ActionResponse> void apply(Task task, String action, Request request, ActionListener<Response> listener, ActionFilterChain<Request, Response> chain) { /* A functional requirement - when the license of security is disabled (invalid/expires), security will continue to operate normally, except all read operations will be blocked. */ if (licenseState.isActive() == false && LICENSE_EXPIRATION_ACTION_MATCHER.test(action)) { logger.error("blocking [{}] operation due to expired license. Cluster health, cluster stats and indices stats \\\\n" + "operations are blocked on license expiration. All data operations (read and write) continue to work. \\\\n" + "If you have a new license, please update it. Otherwise, please reach out to your support contact.", action); throw LicenseUtils.newComplianceException(XPackField.SECURITY); } if (licenseState.isSecurityEnabled()) { final ActionListener<Response> contextPreservingListener = ContextPreservingActionListener.wrapPreservingContext(listener, threadContext); ActionListener<Void> authenticatedListener = ActionListener.wrap( (aVoid) -> chain.proceed(task, action, request, contextPreservingListener), contextPreservingListener::onFailure); final boolean useSystemUser = AuthorizationUtils.shouldReplaceUserWithSystem(threadContext, action); try { if (useSystemUser) { securityContext.executeAsUser(SystemUser.INSTANCE, (original) -> { try { applyInternal(action, request, authenticatedListener); } catch (IOException e) { listener.onFailure(e); } }, Version.CURRENT); } else if (AuthorizationUtils.shouldSetUserBasedOnActionOrigin(threadContext)) { AuthorizationUtils.switchUserBasedOnActionOriginAndExecute(threadContext, securityContext, (original) -> { try { applyInternal(action, request, authenticatedListener); } catch (IOException e) { listener.onFailure(e); } }); } else { try (ThreadContext.StoredContext ignore = threadContext.newStoredContext(true)) { applyInternal(action, request, authenticatedListener); } } } catch (Exception e) { listener.onFailure(e); } } else if (SECURITY_ACTION_MATCHER.test(action)) { if (licenseState.isSecurityEnabled() == false) { listener.onFailure(new ElasticsearchException("Security must be explicitly enabled when using a [" + licenseState.getOperationMode().description() + "] license. " + "Enable security by setting [xpack.security.enabled] to [true] in the elasticsearch.yml file " + "and restart the node.")); } else { listener.onFailure(LicenseUtils.newComplianceException(XPackField.SECURITY)); } } else { chain.proceed(task, action, request, listener); } }	i understand this is an existing comment. but the wording of except all read operations will be blocked feels wrong or confusing at least. maybe it was the previous intention and is no longer accurate. this really just blocks "cluster:monitor/health*", "cluster:monitor/stats*", "indices:monitor/stats*", "cluster:monitor/nodes/stats*" and we could be clear about it in the comment.
public void testBodyConsumed() throws Exception { final XPackLicenseState licenseState = mock(XPackLicenseState.class); when(licenseState.isSecurityEnabled()).thenReturn(true); final RestHasPrivilegesAction action = new RestHasPrivilegesAction(Settings.EMPTY, mock(SecurityContext.class), licenseState); try (XContentBuilder bodyBuilder = JsonXContent.contentBuilder().startObject().endObject(); NodeClient client = new NoOpNodeClient(this.getTestName())) { final RestRequest request = new FakeRestRequest.Builder(xContentRegistry()) .withPath("/_security/user/_has_privileges/") .withContent(new BytesArray(bodyBuilder.toString()), XContentType.JSON) .build(); final RestChannel channel = new FakeRestChannel(request, true, 1); ElasticsearchSecurityException e = expectThrows(ElasticsearchSecurityException.class, () -> action.handleRequest(request, channel, client)); assertThat(e.getMessage(), equalTo("there is no authenticated user")); } }	to retain the existing behaviour of this test, should this mock return false instead of true so that action.handlerequest(...) does not throw exception but returns a response about security is not enabled (currently it returns security not available due to license)?
protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener) { SqlStatsRequest sqlRequest = new SqlStatsRequest(); sqlRequest.includeStats(true); sqlRequest.setParentTask(clusterService.localNode().getId(), task.getId()); client.execute(SqlStatsAction.INSTANCE, sqlRequest, ActionListener.wrap(r -> { List<Counters> countersPerNode = r.getNodes() .stream() .map(SqlStatsResponse.NodeStatsResponse::getStats) .filter(Objects::nonNull) .collect(Collectors.toList()); Counters mergedCounters = Counters.merge(countersPerNode); SqlFeatureSetUsage usage = new SqlFeatureSetUsage(true, mergedCounters.toNestedMap()); listener.onResponse(new XPackUsageFeatureResponse(usage)); }, listener::onFailure)); }	yet another available parameter.
protected void masterOperation( Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener ) { PersistentTasksCustomMetadata taskMetadata = PersistentTasksCustomMetadata.getPersistentTasksCustomMetadata(state); Collection<PersistentTasksCustomMetadata.PersistentTask<?>> transformTasks = taskMetadata == null ? Collections.emptyList() : taskMetadata.findTasks(TransformTaskParams.NAME, (t) -> true); final int taskCount = transformTasks.size(); final Map<String, Long> transformsCountByState = new HashMap<>(); for (PersistentTasksCustomMetadata.PersistentTask<?> transformTask : transformTasks) { TransformState transformState = (TransformState) transformTask.getState(); TransformTaskState taskState = transformState.getTaskState(); if (taskState != null) { transformsCountByState.merge(taskState.value(), 1L, Long::sum); } } ActionListener<TransformIndexerStats> totalStatsListener = ActionListener.wrap(statSummations -> { var usage = new TransformFeatureSetUsage(true, transformsCountByState, statSummations); listener.onResponse(new XPackUsageFeatureResponse(usage)); }, listener::onFailure); ActionListener<SearchResponse> totalTransformCountListener = ActionListener.wrap(transformCountSuccess -> { if (transformCountSuccess.getShardFailures().length > 0) { logger.error( "total transform count search returned shard failures: {}", Arrays.toString(transformCountSuccess.getShardFailures()) ); } long totalTransforms = transformCountSuccess.getHits().getTotalHits().value; if (totalTransforms == 0) { var usage = new TransformFeatureSetUsage(true, transformsCountByState, new TransformIndexerStats()); listener.onResponse(new XPackUsageFeatureResponse(usage)); return; } transformsCountByState.merge(TransformTaskState.STOPPED.value(), totalTransforms - taskCount, Long::sum); TransformInfoTransportAction.getStatisticSummations(client, totalStatsListener); }, transformCountFailure -> { if (transformCountFailure instanceof ResourceNotFoundException) { TransformInfoTransportAction.getStatisticSummations(client, totalStatsListener); } else { listener.onFailure(transformCountFailure); } }); SearchRequest totalTransformCount = client.prepareSearch( TransformInternalIndexConstants.INDEX_NAME_PATTERN, TransformInternalIndexConstants.INDEX_NAME_PATTERN_DEPRECATED ) .setTrackTotalHits(true) .setQuery( QueryBuilders.constantScoreQuery( QueryBuilders.boolQuery() .filter(QueryBuilders.termQuery(TransformField.INDEX_DOC_TYPE.getPreferredName(), TransformConfig.NAME)) ) ) .request(); ClientHelper.executeAsyncWithOrigin( client.threadPool().getThreadContext(), ClientHelper.TRANSFORM_ORIGIN, totalTransformCount, totalTransformCountListener, client::search ); }	here again, the available parameter.
protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener) { int numDenseVectorFields = 0; int numSparseVectorFields = 0; int avgDenseVectorDims = 0; if (state != null) { for (IndexMetadata indexMetadata : state.metadata()) { MappingMetadata mappingMetadata = indexMetadata.mapping(); if (mappingMetadata != null) { Map<String, Object> mappings = mappingMetadata.getSourceAsMap(); if (mappings.containsKey("properties")) { @SuppressWarnings("unchecked") Map<String, Map<String, Object>> fieldMappings = (Map<String, Map<String, Object>>) mappings.get("properties"); for (Map<String, Object> typeDefinition : fieldMappings.values()) { String fieldType = (String) typeDefinition.get("type"); if (fieldType != null) { if (fieldType.equals(DenseVectorFieldMapper.CONTENT_TYPE)) { numDenseVectorFields++; int dims = (Integer) typeDefinition.get("dims"); avgDenseVectorDims += dims; } else if (fieldType.equals(SparseVectorFieldMapper.CONTENT_TYPE)) { numSparseVectorFields++; } } } } } } if (numDenseVectorFields > 0) { avgDenseVectorDims = avgDenseVectorDims / numDenseVectorFields; } } VectorsFeatureSetUsage usage = new VectorsFeatureSetUsage(true, numDenseVectorFields, avgDenseVectorDims); listener.onResponse(new XPackUsageFeatureResponse(usage)); }	the available parameter again.
@Override protected void masterOperation(Task task, XPackUsageRequest request, ClusterState state, ActionListener<XPackUsageFeatureResponse> listener) { final VotingOnlyNodeFeatureSetUsage usage = new VotingOnlyNodeFeatureSetUsage(true); listener.onResponse(new XPackUsageFeatureResponse(usage)); } } public static class UsageInfoAction extends XPackInfoFeatureTransportAction { @Inject public UsageInfoAction(TransportService transportService, ActionFilters actionFilters) { super(XPackInfoFeatureAction.VOTING_ONLY.name(), transportService, actionFilters); } @Override protected String name() { return XPackField.VOTING_ONLY; } @Override protected boolean available() { return true; } @Override protected boolean enabled() { return true; }	i believe this is the last occurrence of the available parameter.
private static void checkClass(Map<String, Path> clazzes, String clazz, Path jarpath) { if (clazz.equals("module-info") || clazz.endsWith(".module-info")) { // Ignore jigsaw module descriptions return; } Path previous = clazzes.put(clazz, jarpath); if (previous != null) { if (previous.equals(jarpath)) { if (clazz.startsWith("org.apache.xmlbeans")) { return; // https://issues.apache.org/jira/browse/XMLBEANS-499 } // throw a better exception in this ridiculous case. // unfortunately the zip file format allows this buggy possibility // UweSays: It can, but should be considered as bug :-) throw new IllegalStateException("jar hell!" + System.lineSeparator() + "class: " + clazz + System.lineSeparator() + "exists multiple times in jar: " + jarpath + " !!!!!!!!!"); } else { throw new IllegalStateException("jar hell!" + System.lineSeparator() + "class: " + clazz + System.lineSeparator() + "jar1: " + previous + System.lineSeparator() + "jar2: " + jarpath); } } }	why are there 2 variants? shouldn't the class name always be module-info?
public static ESLogMessage of(String key, String xOpaqueId, String messagePattern, Object... args){ if (Strings.isNullOrEmpty(xOpaqueId)) { return new ESLogMessage(messagePattern, args).field("key", key); } Object value = new Object() { @Override public String toString() { return ParameterizedMessage.format(messagePattern, args); } }; return new ESLogMessage(messagePattern, args) .field("data_stream.type", "logs") .field("data_stream.datatype", "deprecation") .field("data_stream.namespace", "elasticsearch") .field("ecs.version", ECS_VERSION) .field("key", key) .field("message", value) .field(X_OPAQUE_ID_FIELD_NAME, xOpaqueId); }	@pugnascotia this should be data_stream.dataset to be aligned with the indexing strategy. i would also propose to keep the namespace as default and use deprecation.elasticsearch as the dataset name. only important thing is that the dataset does not contain a -.
public static ESLogMessage of(String key, String xOpaqueId, String messagePattern, Object... args){ if (Strings.isNullOrEmpty(xOpaqueId)) { return new ESLogMessage(messagePattern, args).field("key", key); } Object value = new Object() { @Override public String toString() { return ParameterizedMessage.format(messagePattern, args); } }; return new ESLogMessage(messagePattern, args) .field("data_stream.type", "logs") .field("data_stream.datatype", "deprecation") .field("data_stream.namespace", "elasticsearch") .field("ecs.version", ECS_VERSION) .field("key", key) .field("message", value) .field(X_OPAQUE_ID_FIELD_NAME, xOpaqueId); }	@jakelandis how do these look to you?
public static void resolveConfig(Environment env, final ImmutableSettings.Builder settingsBuilder) { try { Path startingPath; final String esLoggingDir = System.getProperty("es.logging"); if (Strings.hasText(esLoggingDir)) { final URL url = env.resolveConfig(esLoggingDir); startingPath = new File(url.toURI()).toPath(); } else { startingPath = env.configFile().toPath(); } Files.walkFileTree(startingPath, EnumSet.of(FileVisitOption.FOLLOW_LINKS), Integer.MAX_VALUE, new SimpleFileVisitor<Path>() { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { String fileName = file.getFileName().toString(); if (fileName.startsWith("logging.")) { for (String allowedSuffix : ALLOWED_SUFFIXES) { if (fileName.endsWith(allowedSuffix)) { loadConfig(file, settingsBuilder); break; } } } return FileVisitResult.CONTINUE; } }); } catch (IOException | URISyntaxException e) { throw new ElasticsearchException("Failed to load logging configuration", e); } }	can we extract es.logging to a constant with package private visibility, so we can also use it in the test instead of copy pasting the same string?
public static void resolveConfig(Environment env, final ImmutableSettings.Builder settingsBuilder) { try { Path startingPath; final String esLoggingDir = System.getProperty("es.logging"); if (Strings.hasText(esLoggingDir)) { final URL url = env.resolveConfig(esLoggingDir); startingPath = new File(url.toURI()).toPath(); } else { startingPath = env.configFile().toPath(); } Files.walkFileTree(startingPath, EnumSet.of(FileVisitOption.FOLLOW_LINKS), Integer.MAX_VALUE, new SimpleFileVisitor<Path>() { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { String fileName = file.getFileName().toString(); if (fileName.startsWith("logging.")) { for (String allowedSuffix : ALLOWED_SUFFIXES) { if (fileName.endsWith(allowedSuffix)) { loadConfig(file, settingsBuilder); break; } } } return FileVisitResult.CONTINUE; } }); } catch (IOException | URISyntaxException e) { throw new ElasticsearchException("Failed to load logging configuration", e); } }	can you remove this empty line please?
public static void resolveConfig(Environment env, final ImmutableSettings.Builder settingsBuilder) { try { Path startingPath; final String esLoggingDir = System.getProperty("es.logging"); if (Strings.hasText(esLoggingDir)) { final URL url = env.resolveConfig(esLoggingDir); startingPath = new File(url.toURI()).toPath(); } else { startingPath = env.configFile().toPath(); } Files.walkFileTree(startingPath, EnumSet.of(FileVisitOption.FOLLOW_LINKS), Integer.MAX_VALUE, new SimpleFileVisitor<Path>() { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { String fileName = file.getFileName().toString(); if (fileName.startsWith("logging.")) { for (String allowedSuffix : ALLOWED_SUFFIXES) { if (fileName.endsWith(allowedSuffix)) { loadConfig(file, settingsBuilder); break; } } } return FileVisitResult.CONTINUE; } }); } catch (IOException | URISyntaxException e) { throw new ElasticsearchException("Failed to load logging configuration", e); } }	can you remove this empty line please?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeString(scriptLang); out.writeOptionalString(id); out.writeBytesReference(script); if (out.getVersion().onOrAfter(Version.V_5_3_0_UNRELEASED)) { if (xContentType == null) { out.writeBoolean(false); } else { out.writeBoolean(true); xContentType.writeTo(out); } } }	can you clarify that this if is only needed for bw comp and should go away once the bw comp layer is removed?
public BytesReference script() { return script; }	shall we add javadocs with the motivation for the deprecation and what it is replaced with? i assume that these methods can be removed in master once this pr is backported to 5.x?
@SuppressWarnings("unchecked") public PutMappingRequest source(Map mappingSource) { try { XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON); builder.map(mappingSource); return source(builder.string(), XContentType.JSON); } catch (IOException e) { throw new ElasticsearchGenerationException("Failed to generate [" + mappingSource + "]", e); } } /** * The mapping source definition. * @deprecated use {@link #source(String, XContentType)}	not something that you changed but maybe something to fix as a followup, we have 4 xcontenttypes, but given that source is a string, we only support yaml or json here. i wonder if source should rather be a bytesreference? i never thought we'd support yaml here though :) who knows if anybody is using that format. i also wonder if instead of converting here we should keep around the xcontenttype and carry it around all the way to documentmapperparser#parse where we actually parse it. that may be a bigger effort but sounds cleaner to me.
public Builder loadFromSource(String source) { SettingsLoader settingsLoader = SettingsLoaderFactory.loaderFromSource(source); try { Map<String, String> loadedSettings = settingsLoader.load(source); put(loadedSettings); } catch (Exception e) { throw new SettingsException("Failed to load settings from [" + source + "]", e); } return this; } /** * Loads settings from the actual string content that represents them using the * {@link SettingsLoaderFactory#loaderFromXContentType(XContentType)}	do we need to deprecate the other variant of this method that doesn't take the xcontenttype?
public Builder loadFromSource(String source, XContentType xContentType) { SettingsLoader settingsLoader = SettingsLoaderFactory.loaderFromXContentType(xContentType); try { Map<String, String> loadedSettings = settingsLoader.load(source); put(loadedSettings); } catch (Exception e) { throw new SettingsException("Failed to load settings from [" + source + "]", e); } return this; } /** * Loads settings from a url that represents them using the * {@link SettingsLoaderFactory#loaderFromSource(String)}	here the contract is that we support only json or yaml rather than all of the 4 content types that we usually support, shall we make that clearer? or shall we even create an enum that is a subset of xcontenttype, let's say xcontenttypestring or something along those lines? it seems like that could help, but i am not sure how much it would complicate things at the same time.
* @return A settings loader. */ public static SettingsLoader loaderFromXContentType(XContentType xContentType) { if (xContentType == XContentType.JSON) { return new JsonSettingsLoader(true); } else if (xContentType == XContentType.YAML) { return new YamlSettingsLoader(true); } else { throw new IllegalArgumentException("unsupported content type [" + xContentType + "]"); } }	can we deprecate the auto-detection from this class? loaderfromsource i mean.
public XContentBuilder rawField(String name, InputStream value, XContentType contentType) throws IOException { generator.writeRawField(name, value, contentType); return this; }	do these raw methods that don't take the contenttype need to be deprecated?
@Deprecated public static Tuple<XContentType, Map<String, Object>> convertToMap(BytesReference bytes, boolean ordered) throws ElasticsearchParseException { return convertToMap(bytes, ordered, null); }	add some simple javadocs?
public static Tuple<XContentType, Map<String, Object>> convertToMap(BytesReference bytes, boolean ordered, XContentType xContentType) throws ElasticsearchParseException { try { XContentType contentType = xContentType; InputStream input; Compressor compressor = CompressorFactory.compressor(bytes); if (compressor != null) { InputStream compressedStreamInput = compressor.streamInput(bytes.streamInput()); if (compressedStreamInput.markSupported() == false) { compressedStreamInput = new BufferedInputStream(compressedStreamInput); } if (contentType == null) { contentType = XContentFactory.xContentType(compressedStreamInput); } input = compressedStreamInput; } else { if (contentType == null) { contentType = XContentFactory.xContentType(bytes); } input = bytes.streamInput(); } return new Tuple<>(contentType, convertToMap(XContentFactory.xContent(contentType), input, ordered)); } catch (IOException e) { throw new ElasticsearchParseException("Failed to parse content to map", e); } } /** * Convert a string in some {@link XContent} format to a {@link Map}. Throws an {@link ElasticsearchParseException}	maybe there is a way to not accept null? for instance have a third private method that takes a checkedfunction<bytesreference, xcontenttype> which either returns the provided one or does the auto-detection? just an idea though.
@Override public XContent xContent() { return YamlXContent.yamlXContent; } }, /** * A CBOR based content type. */ CBOR(3) { @Override public String mediaTypeWithoutParameters() { return "application/cbor"; } @Override public String shortName() { return "cbor"; } @Override public XContent xContent() { return CborXContent.cborXContent; } }; public static XContentType fromMediaTypeOrFormat(String mediaType) { if (mediaType == null) { return null; } for (XContentType type : values()) { if (isSameMediaTypeAs(mediaType, type)) { return type; } } if(mediaType.toLowerCase(Locale.ROOT).startsWith("application/*")) { return JSON; } return null; } public static XContentType fromMediaTypeStrict(String mediaType) { final String lowercaseMediaType = mediaType.toLowerCase(Locale.ROOT); for (XContentType type : values()) { if (type.mediaTypeWithoutParameters().equals(mediaType)) { return type; } } return null; } private static boolean isSameMediaTypeAs(String stringType, XContentType type) { return type.mediaTypeWithoutParameters().equalsIgnoreCase(stringType) || stringType.toLowerCase(Locale.ROOT).startsWith(type.mediaTypeWithoutParameters().toLowerCase(Locale.ROOT) + ";") || type.shortName().equalsIgnoreCase(stringType); } private int index; XContentType(int index) { this.index = index; } public int index() { return index; } public String mediaType() { return mediaTypeWithoutParameters(); } public abstract String shortName(); public abstract XContent xContent(); public abstract String mediaTypeWithoutParameters(); public static XContentType readFrom(StreamInput in) throws IOException { int index = in.readVInt(); for (XContentType contentType : values()) { if (index == contentType.index) { return contentType; } } throw new IllegalStateException("Unknown XContentType with index [" + index + "]"); }	add javadocs? reading strict i would expect to never return null. what is the difference between this one and the existing method?
public void dispatchRequest(RestRequest request, RestChannel channel, ThreadContext threadContext) { if (request.rawPath().equals("/favicon.ico")) { handleFavicon(request, channel); return; } RestChannel responseChannel = channel; try { final int contentLength = request.hasContent() ? request.content().length() : 0; if (checkContentType(request, responseChannel, contentLength)) { if (canTripCircuitBreaker(request)) { inFlightRequestsBreaker(circuitBreakerService).addEstimateBytesAndMaybeBreak(contentLength, "<http_request>"); } else { inFlightRequestsBreaker(circuitBreakerService).addWithoutBreaking(contentLength); } // iff we could reserve bytes for the request we need to send the response also over this channel responseChannel = new ResourceHandlingHttpChannel(channel, circuitBreakerService, contentLength); dispatchRequest(request, responseChannel, client, threadContext); } } catch (Exception e) { try { responseChannel.sendResponse(new BytesRestResponse(channel, e)); } catch (Exception inner) { inner.addSuppressed(e); logger.error((Supplier<?>) () -> new ParameterizedMessage("failed to send failure response for uri [{}]", request.uri()), inner); } } }	i find the method name here a bit confusing, not sure when it returns true and when it returns false.
boolean checkContentType(final RestRequest restRequest, final RestChannel channel, final int contentLength) { if (contentLength > 0) { if (restRequest.getXContentType() == null && restRequest.isPlainText() == false) { try { XContentBuilder builder = channel.newErrorBuilder(); final List<String> contentTypeHeader = restRequest.getHeader("Content-Type"); final String errorMessage; if (contentTypeHeader == null) { errorMessage = "Content-Type header is missing"; } else { errorMessage = "Content-Type header [" + Strings.collectionToCommaDelimitedString(restRequest.getHeader("Content-Type")) + "] is not supported"; } builder.startObject().field("error", errorMessage).endObject(); RestResponse response = new BytesRestResponse(BAD_REQUEST, builder); response.addHeader("Content-Type", builder.contentType().mediaType()); channel.sendResponse(response); } catch (IOException e) { logger.warn("Failed to send response", e); } return false; } } return true; }	can you explain what we are doing here? seems that content-type header is required only in some specific situation?
boolean checkContentType(final RestRequest restRequest, final RestChannel channel, final int contentLength) { if (contentLength > 0) { if (restRequest.getXContentType() == null && restRequest.isPlainText() == false) { try { XContentBuilder builder = channel.newErrorBuilder(); final List<String> contentTypeHeader = restRequest.getHeader("Content-Type"); final String errorMessage; if (contentTypeHeader == null) { errorMessage = "Content-Type header is missing"; } else { errorMessage = "Content-Type header [" + Strings.collectionToCommaDelimitedString(restRequest.getHeader("Content-Type")) + "] is not supported"; } builder.startObject().field("error", errorMessage).endObject(); RestResponse response = new BytesRestResponse(BAD_REQUEST, builder); response.addHeader("Content-Type", builder.contentType().mediaType()); channel.sendResponse(response); } catch (IOException e) { logger.warn("Failed to send response", e); } return false; } } return true; }	can we not return some response body here and rather use bytesrestresponse for this? i am worried about parsing back responses and i would like to centralize the code that writes errors out as much as possible.
boolean checkContentType(final RestRequest restRequest, final RestChannel channel, final int contentLength) { if (contentLength > 0) { if (restRequest.getXContentType() == null && restRequest.isPlainText() == false) { try { XContentBuilder builder = channel.newErrorBuilder(); final List<String> contentTypeHeader = restRequest.getHeader("Content-Type"); final String errorMessage; if (contentTypeHeader == null) { errorMessage = "Content-Type header is missing"; } else { errorMessage = "Content-Type header [" + Strings.collectionToCommaDelimitedString(restRequest.getHeader("Content-Type")) + "] is not supported"; } builder.startObject().field("error", errorMessage).endObject(); RestResponse response = new BytesRestResponse(BAD_REQUEST, builder); response.addHeader("Content-Type", builder.contentType().mediaType()); channel.sendResponse(response); } catch (IOException e) { logger.warn("Failed to send response", e); } return false; } } return true; }	returning false always means that a response has been sent back?
public abstract BytesReference content(); /** * Get the value of the header or {@code null} if not found. This method only retrieves the first header value if multiple values are * sent. Use of {@link #getHeader(String)}	i think this makes headers case-sensitive but they shouldn't be?
public final Tuple<XContentType, BytesReference> contentOrSourceParam() { if (hasContent()) { if (xContentType == null) { throw new IllegalStateException("Content-Type must be provided"); } return new Tuple<>(xContentType, content()); } String source = param("source"); if (source != null) { BytesArray bytes = new BytesArray(source); String typeParam = param("source_type"); final XContentType xContentType; if (typeParam != null) { xContentType = parseContentType(Collections.singletonList(typeParam)); } else { xContentType = XContentFactory.xContentType(bytes); } if (xContentType == null) { throw new IllegalStateException("could not determine source content type"); } return new Tuple<>(xContentType, bytes); } return new Tuple<>(XContentType.JSON, BytesArray.EMPTY); } /** * Call a consumer with the parser for the contents of this request if it has contents, otherwise with a parser for the {@code source} * parameter if there is one, otherwise with {@code null}. Use {@link #contentOrSourceParamParser()}	in this case we auto-detect rather than assuming json? i think this is json in the vast majority of the cases?
public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { String scrollIds = request.param("scroll_id"); ClearScrollRequest clearRequest = new ClearScrollRequest(); clearRequest.setScrollIds(Arrays.asList(splitScrollIds(scrollIds))); request.withContentOrSourceParamParserOrNullLenient((xContentParser -> { if (xContentParser == null) { if (request.hasContent() && request.isPlainText()) { // TODO: why do we accept this plain text value? maybe we can just use the scroll params? BytesReference body = request.content(); String bodyScrollIds = body.utf8ToString(); clearRequest.setScrollIds(Arrays.asList(splitScrollIds(bodyScrollIds))); } } else { // NOTE: if rest request with xcontent body has request parameters, these parameters does not override xcontent value clearRequest.setScrollIds(null); try { buildFromContent(xContentParser, clearRequest); } catch (IOException e) { throw new IllegalArgumentException("Failed to parse request body", e); } } })); return channel -> client.clearScroll(clearRequest, new RestStatusToXContentListener<>(channel)); }	i bumped into this as well in the past. we can't even test this in the rest tests i think :/ i would love to get rid of it to be honest. open an issue?
public static void parseMultiLineRequest(RestRequest request, IndicesOptions indicesOptions, boolean allowExplicitIndex, BiConsumer<SearchRequest, XContentParser> consumer) throws IOException { String[] indices = Strings.splitStringByCommaToArray(request.param("index")); String[] types = Strings.splitStringByCommaToArray(request.param("type")); String searchType = request.param("search_type"); String routing = request.param("routing"); final Tuple<XContentType, BytesReference> sourceTuple = request.contentOrSourceParam(); final XContent xContent = sourceTuple.v1().xContent(); final BytesReference data = sourceTuple.v2(); int from = 0; int length = data.length(); byte marker = xContent.streamSeparator(); while (true) { int nextMarker = findNextMarker(marker, from, data, length); if (nextMarker == -1) { break; } // support first line with \\\\n if (nextMarker == 0) { from = nextMarker + 1; continue; } SearchRequest searchRequest = new SearchRequest(); if (indices != null) { searchRequest.indices(indices); } if (indicesOptions != null) { searchRequest.indicesOptions(indicesOptions); } if (types != null && types.length > 0) { searchRequest.types(types); } if (routing != null) { searchRequest.routing(routing); } if (searchType != null) { searchRequest.searchType(searchType); } IndicesOptions defaultOptions = IndicesOptions.strictExpandOpenAndForbidClosed(); // now parse the action if (nextMarker - from > 0) { try (XContentParser parser = xContent.createParser(request.getXContentRegistry(), data.slice(from, nextMarker - from))) { Map<String, Object> source = parser.map(); for (Map.Entry<String, Object> entry : source.entrySet()) { Object value = entry.getValue(); if ("index".equals(entry.getKey()) || "indices".equals(entry.getKey())) { if (!allowExplicitIndex) { throw new IllegalArgumentException("explicit index in multi search is not allowed"); } searchRequest.indices(nodeStringArrayValue(value)); } else if ("type".equals(entry.getKey()) || "types".equals(entry.getKey())) { searchRequest.types(nodeStringArrayValue(value)); } else if ("search_type".equals(entry.getKey()) || "searchType".equals(entry.getKey())) { searchRequest.searchType(nodeStringValue(value, null)); } else if ("request_cache".equals(entry.getKey()) || "requestCache".equals(entry.getKey())) { searchRequest.requestCache(nodeBooleanValue(value, entry.getKey())); } else if ("preference".equals(entry.getKey())) { searchRequest.preference(nodeStringValue(value, null)); } else if ("routing".equals(entry.getKey())) { searchRequest.routing(nodeStringValue(value, null)); } } defaultOptions = IndicesOptions.fromMap(source, defaultOptions); } } searchRequest.indicesOptions(defaultOptions); // move pointers from = nextMarker + 1; // now for the body nextMarker = findNextMarker(marker, from, data, length); if (nextMarker == -1) { break; } BytesReference bytes = data.slice(from, nextMarker - from); try (XContentParser parser = xContent.createParser(request.getXContentRegistry(), bytes)) { consumer.accept(searchRequest, parser); } // move pointers from = nextMarker + 1; } }	in msearch, do we support other formats besides json? maybe we do, i just never realized that.
public Channel getChannel() { return channel; } /** * A wrapper of {@link HttpHeaders} that implements a map to prevent copying unnecessarily. This class does not support modifications * and due to the underlying implementation, it performs case insensitive lookups of key to values. * * It is important to note that this implementation does have some downsides in that each invocation of the * {@link #values()} and {@link #entrySet()}	oh nice some other comment of mine above may be wrong then, seems like you have made headers case-insensitive. so does it mean that they we case-sensitive before?
public void testExactMatchRLike() throws Exception { RLikePattern pattern = new RLikePattern("abc"); FieldAttribute fa = getFieldAttribute(); RLike l = new RLike(EMPTY, fa, pattern); Expression e = new ReplaceRegexMatch().rule(l); assertEquals(Equals.class, e.getClass()); Equals eq = (Equals) e; assertEquals(fa, eq.left()); assertEquals("abc", eq.right().fold()); }	typo. suggestion assertequals(or, e);
public void testExactMatchRLike() throws Exception { RLikePattern pattern = new RLikePattern("abc"); FieldAttribute fa = getFieldAttribute(); RLike l = new RLike(EMPTY, fa, pattern); Expression e = new ReplaceRegexMatch().rule(l); assertEquals(Equals.class, e.getClass()); Equals eq = (Equals) e; assertEquals(fa, eq.left()); assertEquals("abc", eq.right().fold()); }	super nit-picky. would just test that there are no changes: suggestion assertequals(or.right(), firstor.right());
public void testStart_DoNotDetectCrashWhenNoInputPipeProvided() throws Exception { when(processPipes.getProcessInStream()).thenReturn(Optional.empty()); try (AbstractNativeProcess process = new TestNativeProcess()) { process.start(executorService); mockNativeProcessLoggingStreamEnds.countDown(); // Not detecting a crash is confirmed in terminateExecutorService() } }	i think this could be made even better by nesting the finally so that we count down before close whether the test passes or fails. try (abstractnativeprocess process = new testnativeprocess()) { try { process.start(executorservice); } finally { mocknativeprocessloggingstreamends.countdown(); // not detecting a crash is confirmed in terminateexecutorservice() } } (and obviously the same pattern for all the other places too.) or to avoid nested try blocks it could be done like we're already doing it in teststart_donotdetectcrashwhenprocessisbeingkilled: abstractnativeprocess process = new testnativeprocess()) { try { process.start(executorservice); } finally { mocknativeprocessloggingstreamends.countdown(); // not detecting a crash is confirmed in terminateexecutorservice() process.close(); }
* @param clazz the Processor class to look for * @return True if the pipeline contains an instance of the Processor class passed in */ public boolean hasProcessor(String pipelineId, Class<? extends Processor> clazz) { Pipeline pipeline = getPipeline(pipelineId); if (pipeline == null) { return false; } for (Processor processor: pipeline.flattenAllProcessors()) { if (processor instanceof WrappedProcessor) { if (clazz.isAssignableFrom(((WrappedProcessor) processor).getInnerProcessor().getClass())) { return true; } } else { if (clazz.isAssignableFrom(processor.getClass())) { return true; } } } return false; }	i think this method does not work as expected in the following cases: 1) hasprocessor("_id", foreachprocessor.class/conditionalprocessor.class) invocations, because both classes implemented wrappedprocessor, we never end up checking whether processor is assignable from clazz and straight check the inner processor. 2) in the case that a wrapped processor is wrapped by another wrapped processor (for example: foreach processor wraps a conditional processor) then we never check the 2nd wrapped processor or any processor below that. i think something like the following fixes the above cases: if (clazz.isassignablefrom(processor.getclass())) { return true; } while (processor instanceof wrappedprocessor) { wrappedprocessor wrappedprocessor = (wrappedprocessor) processor; if (clazz.isassignablefrom(wrappedprocessor.getinnerprocessor().getclass())) { return true; } processor = wrappedprocessor.getinnerprocessor(); if (wrappedprocessor == processor) { break; } } (replaces the body of the for loop)
public String toDelimitedString(char delimiter) { StringBuilder sb = new StringBuilder(); settings.forEach((key, value) -> sb.append(key).append("=").append(value).append(delimiter)); return sb.toString(); }	this appears to be an unrelated formatting change. i'm also doubting it, since now we have those lambda allocations in a loop. i've personally never been a fan of using foreach to mutate, i've felt it should be side-effect free (because it looks like an expression, not a statement). either way, it's unrelated to our main story.
void setupOptions(Terminal terminal, OptionSet options, Environment env) throws Exception { keyStoreWrapper = keyStoreFunction.apply(env); try (SecureString keystorePassword = keyStoreWrapper.hasPassword() ? new SecureString(terminal.readSecret("Enter the password for elasticsearch.keystore: ")) : new SecureString(new char[0])) { keyStoreWrapper.decrypt(keystorePassword.getChars()); } catch (SecurityException e) { if (e.getCause() instanceof AEADBadTagException) { terminal.println(""); terminal.println("Failed to decrypt elasticsearch.keystore with the provided password."); terminal.println(""); throw new UserException(ExitCodes.DATA_ERROR, "Wrong password for elasticsearch.keystore"); } } Settings.Builder settingsBuilder = Settings.builder(); settingsBuilder.put(env.settings(), true); if (settingsBuilder.getSecureSettings() == null) { settingsBuilder.setSecureSettings(keyStoreWrapper); } Settings settings = settingsBuilder.build(); elasticUserPassword = ReservedRealm.BOOTSTRAP_ELASTIC_PASSWORD.get(settings); client = clientFunction.apply(env, settings); String providedUrl = urlOption.value(options); url = new URL(providedUrl == null ? client.getDefaultURL() : providedUrl); setShouldPrompt(options); }	since we don't need to write the keystore back out in these tools, can we have a utility method somewhere to read the keystore, instead of copying this code to each tool?
@Override public RestChannelConsumer innerPrepareRequest(RestRequest request, NodeClient client) throws IOException { final String scope = request.param("application"); final String[] privileges = request.paramAsStringArray("privilege", Strings.EMPTY_ARRAY); final GetPrivilegesRequestBuilder requestBuilder = new GetPrivilegesRequestBuilder(client); // Application names cannot start with `_`, so we use this for built-in names if ("_cluster".equals(scope)) { requestBuilder.privilegeTypes(GetPrivilegesRequest.PrivilegeType.CLUSTER); } else if ("_index".equals(scope)) { requestBuilder.privilegeTypes(GetPrivilegesRequest.PrivilegeType.INDEX); } else if ("_application".equals(scope)) { requestBuilder.privilegeTypes(GetPrivilegesRequest.PrivilegeType.APPLICATION); } else if (Strings.hasText(scope)) { requestBuilder.privilegeTypes(GetPrivilegesRequest.PrivilegeType.APPLICATION).application(scope).privileges(privileges); } return channel -> requestBuilder.execute(new RestBuilderListener<>(channel) { @Override public RestResponse buildResponse(GetPrivilegesResponse response, XContentBuilder builder) throws Exception { builder.startObject(); outputArrayIfExists(builder, "_cluster", response.getClusterPrivileges()); outputArrayIfExists(builder, "_index", response.getIndexPrivileges()); final Map<String, Set<ApplicationPrivilegeDescriptor>> appPrivs = groupByApplicationName(response.applicationPrivileges()); for (String app : appPrivs.keySet()) { builder.startObject(app); for (ApplicationPrivilegeDescriptor privilege : appPrivs.get(app)) { builder.field(privilege.getName(), privilege); } builder.endObject(); } builder.endObject(); // if the user asked for specific privileges, but none of them were found // we'll return an empty result and 404 status code if (privileges.length != 0 && response.isEmpty()) { return new BytesRestResponse(RestStatus.NOT_FOUND, builder); } // either the user asked for all privileges, or at least one of the privileges // was found return new BytesRestResponse(RestStatus.OK, builder); } }); }	can we change the parameter name to scope in 8.0.0 and add a small comment here to explain why we map application to a scope when backporting in 7.x ?
@Override public RestChannelConsumer innerPrepareRequest(RestRequest request, NodeClient client) throws IOException { final String scope = request.param("application"); final String[] privileges = request.paramAsStringArray("privilege", Strings.EMPTY_ARRAY); final GetPrivilegesRequestBuilder requestBuilder = new GetPrivilegesRequestBuilder(client); // Application names cannot start with `_`, so we use this for built-in names if ("_cluster".equals(scope)) { requestBuilder.privilegeTypes(GetPrivilegesRequest.PrivilegeType.CLUSTER); } else if ("_index".equals(scope)) { requestBuilder.privilegeTypes(GetPrivilegesRequest.PrivilegeType.INDEX); } else if ("_application".equals(scope)) { requestBuilder.privilegeTypes(GetPrivilegesRequest.PrivilegeType.APPLICATION); } else if (Strings.hasText(scope)) { requestBuilder.privilegeTypes(GetPrivilegesRequest.PrivilegeType.APPLICATION).application(scope).privileges(privileges); } return channel -> requestBuilder.execute(new RestBuilderListener<>(channel) { @Override public RestResponse buildResponse(GetPrivilegesResponse response, XContentBuilder builder) throws Exception { builder.startObject(); outputArrayIfExists(builder, "_cluster", response.getClusterPrivileges()); outputArrayIfExists(builder, "_index", response.getIndexPrivileges()); final Map<String, Set<ApplicationPrivilegeDescriptor>> appPrivs = groupByApplicationName(response.applicationPrivileges()); for (String app : appPrivs.keySet()) { builder.startObject(app); for (ApplicationPrivilegeDescriptor privilege : appPrivs.get(app)) { builder.field(privilege.getName(), privilege); } builder.endObject(); } builder.endObject(); // if the user asked for specific privileges, but none of them were found // we'll return an empty result and 404 status code if (privileges.length != 0 && response.isEmpty()) { return new BytesRestResponse(RestStatus.NOT_FOUND, builder); } // either the user asked for all privileges, or at least one of the privileges // was found return new BytesRestResponse(RestStatus.OK, builder); } }); }	suggestion: extract _cluster , _index , _application to class vars ?
private void innerUpdateSnapshotState(final UpdateIndexShardSnapshotStatusRequest request, ActionListener<UpdateIndexShardSnapshotStatusResponse> listener) { logger.trace("received updated snapshot restore state [{}]", request); clusterService.submitStateUpdateTask( "update snapshot state", request, ClusterStateTaskConfig.build(Priority.NORMAL), snapshotStateExecutor, new ClusterStateTaskListener() { @Override public void onFailure(String source, Exception e) { listener.onFailure(e); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { try { listener.onResponse(new UpdateIndexShardSnapshotStatusResponse()); } finally { // Maybe this state update completed the snapshot. If we are not already ending it because of a concurrent // state update we check if its state is completed and end it if it is. if (endingSnapshots.contains(request.snapshot()) == false) { final SnapshotsInProgress snapshotsInProgress = newState.custom(SnapshotsInProgress.TYPE); final SnapshotsInProgress.Entry updatedEntry = snapshotsInProgress.snapshot(request.snapshot()); // If the entry is still in the cluster state and is completed, try finalizing the snapshot in the repo if (updatedEntry != null && updatedEntry.state().completed()) { endSnapshot(updatedEntry, newState.metadata()); } } } } }); }	this was a bug introduced in #56669 , we could npe here if a data node were to resend a state update for an already finished snapshot (master fail-over or node disconnect can easily lead to this) causing an npe here needlessly.
*/ private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata) { if (endingSnapshots.add(entry.snapshot()) == false) { return; } final Snapshot snapshot = entry.snapshot(); threadPool.executor(ThreadPool.Names.SNAPSHOT).execute(new AbstractRunnable() { @Override protected void doRun() { final Repository repository = repositoriesService.repository(snapshot.getRepository()); final String failure = entry.failure(); logger.trace("[{}] finalizing snapshot in repository, state: [{}], failure[{}]", snapshot, entry.state(), failure); ArrayList<SnapshotShardFailure> shardFailures = new ArrayList<>(); for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardStatus : entry.shards()) { ShardId shardId = shardStatus.key; ShardSnapshotStatus status = shardStatus.value; final ShardState state = status.state(); if (state.failed()) { shardFailures.add(new SnapshotShardFailure(status.nodeId(), shardId, status.reason())); } else if (state.completed() == false) { shardFailures.add(new SnapshotShardFailure(status.nodeId(), shardId, "skipped")); } else { assert state == ShardState.SUCCESS; } } final ShardGenerations shardGenerations = buildGenerations(entry, metadata); repository.finalizeSnapshot( snapshot.getSnapshotId(), shardGenerations, entry.startTime(), failure, entry.partial() ? shardGenerations.totalShards() : entry.shards().size(), unmodifiableList(shardFailures), entry.repositoryStateId(), entry.includeGlobalState(), metadataForSnapshot(entry, metadata), entry.userMetadata(), entry.version(), state -> stateWithoutSnapshot(state, snapshot), ActionListener.wrap(result -> { final List<ActionListener<Tuple<RepositoryData, SnapshotInfo>>> completionListeners = snapshotCompletionListeners.remove(snapshot); if (completionListeners != null) { try { ActionListener.onResponse(completionListeners, result); } catch (Exception e) { logger.warn("Failed to notify listeners", e); } } endingSnapshots.remove(snapshot); logger.info("snapshot [{}] completed with state [{}]", snapshot, result.v2().state()); }, this::onFailure)); } @Override public void onFailure(final Exception e) { Snapshot snapshot = entry.snapshot(); if (ExceptionsHelper.unwrap(e, NotMasterException.class, FailedToCommitClusterStateException.class) != null) { // Failure due to not being master any more, don't try to remove snapshot from cluster state the next master // will try ending this snapshot again logger.debug(() -> new ParameterizedMessage( "[{}] failed to update cluster state during snapshot finalization", snapshot), e); failSnapshotCompletionListeners(snapshot, new SnapshotException(snapshot, "Failed to update cluster state during snapshot finalization", e)); } else { logger.warn(() -> new ParameterizedMessage("[{}] failed to finalize snapshot", snapshot), e); removeSnapshotFromClusterState(snapshot, e); } } }); }	this is unnecessary now, we will not pass a snapshot with unknown_repo_gen to this method any longer, we just remove it from the cluster state in processwaitingshardsandremovednodes and are done with it.
protected void recomputeActiveRealms() { final XPackLicenseState licenseStateSnapshot = licenseState.copyCurrentLicenseState(); final List<Realm> licensedRealms = calculateLicensedRealms(licenseStateSnapshot); logger.info( "license mode is [{}], currently licensed security realms are [{}]", licenseStateSnapshot.getOperationMode().description(), Strings.collectionToCommaDelimitedString(licensedRealms) ); // Stop license-tracking for any previously-active realms that are no longer allowed if (activeRealms != null) { activeRealms.stream().filter(r -> licensedRealms.contains(r) == false).forEach(realm -> { final LicensedFeature.Persistent feature = getLicensedFeatureForRealm(realm.type()); if (feature != null) { feature.stopTracking(licenseStateSnapshot, realm.name()); } }); } activeRealms = licensedRealms; }	is this sorta a bug for 7.x? if the cluster begins with the basic license and the user configures a single non-basic realm, say ldap. because license is incompatible, licensedrealms contains file and native realms because they are implicit fallback. the license then gets upgraded to gold, licensedrealms is re-computed and now contains ldap but not file or native. in this case, security.standard_realms_feature.stoptracking will be invoked with both file and native realms. but it seems to be an effective no-op because underlyingly it uses concurrenthaspmap.replace(key, oldvalue, newvalue) which does nothing since oldvalue does not match. realms is complex and 7.x has non-trivial difference. so my understanding could be off. anyway, changes in this pr get rid of the problem entirely.
protected void recomputeActiveRealms() { final XPackLicenseState licenseStateSnapshot = licenseState.copyCurrentLicenseState(); final List<Realm> licensedRealms = calculateLicensedRealms(licenseStateSnapshot); logger.info( "license mode is [{}], currently licensed security realms are [{}]", licenseStateSnapshot.getOperationMode().description(), Strings.collectionToCommaDelimitedString(licensedRealms) ); // Stop license-tracking for any previously-active realms that are no longer allowed if (activeRealms != null) { activeRealms.stream().filter(r -> licensedRealms.contains(r) == false).forEach(realm -> { final LicensedFeature.Persistent feature = getLicensedFeatureForRealm(realm.type()); if (feature != null) { feature.stopTracking(licenseStateSnapshot, realm.name()); } }); } activeRealms = licensedRealms; }	when is null possible? shouldn't it always have a feature object since unknown should fallback to custom?
public void testLicenseLevels() { for (String type : InternalRealms.getConfigurableRealmsTypes()) { final LicensedFeature.Persistent feature = InternalRealms.getLicensedFeature(type); if (InternalRealms.isBuiltinRealm(type)) { assertThat(feature, nullValue()); } else if (InternalRealms.isStandardRealm(type)) { assertThat(feature, notNullValue()); // In theory this could be "standard" too, but there aren't any realms on that license level assertThat(feature.getMinimumOperationMode(), is(License.OperationMode.GOLD)); } else { assertThat(feature, notNullValue()); // In theory this could be "enterprise" too, but there aren't any realms on that license level assertThat(feature.getMinimumOperationMode(), is(License.OperationMode.PLATINUM)); } } }	i don't quite get what this comment try to say. could you please elaborate?
public void testInitRealmsFailsForMultipleKerberosRealms() throws IOException { final Settings.Builder builder = Settings.builder().put("path.home", createTempDir()); builder.put("xpack.security.authc.realms.kerberos.realm_1.order", 1); builder.put("xpack.security.authc.realms.kerberos.realm_2.order", 2); final Settings settings = builder.build(); Environment env = TestEnvironment.newEnvironment(settings); final IllegalArgumentException iae = expectThrows( IllegalArgumentException.class, () -> new Realms(settings, env, factories, licenseState, threadContext, reservedRealm) ); assertThat( iae.getMessage(), is( equalTo( "multiple realms [realm_1, realm_2] configured of type [kerberos], [kerberos] can only have one such realm configured" ) ) ); }	is this spotless format change?
synchronized void updateCheckpointOnReplica(long globalCheckpoint) { if (this.globalCheckpoint <= globalCheckpoint) { this.globalCheckpoint = globalCheckpoint; logger.trace("global checkpoint updated from primary to [{}]", globalCheckpoint); } else { throw new IllegalArgumentException("global checkpoint from primary should never decrease. current [" + this.globalCheckpoint + "], got [" + globalCheckpoint + "]"); } }	nit: removed -> removed.
* @throws IOException if shard state could not be persisted */ void updateRoutingEntry(ShardRouting shardRouting) throws IOException; /** * Notifies the service of the current allocation ids in the cluster state. * see {@link GlobalCheckpointService#updateAllocationIdsFromMaster(Set, Set)}	nit: see -> see
private void syncGlobalCheckpoint() { PlainActionFuture<ReplicationResponse> listner = new PlainActionFuture<>(); try { new GlobalCheckpointSync(listner, this).execute(); listner.get(); } catch (Exception e) { throw new AssertionError(e); } }	nit: listner -> listener
public void testInheritMaxValidAutoIDTimestampOnRecovery() throws Exception { try (ReplicationGroup shards = createGroup(0)) { shards.startAll(); final IndexRequest indexRequest = new IndexRequest(index.getName(), "type").source("{}"); indexRequest.onRetry(); // force an update of the timestamp final IndexResponse response = shards.index(indexRequest); assertEquals(DocWriteResponse.Result.CREATED, response.getResult()); if (randomBoolean()) { // lets check if that also happens if no translog record is replicated shards.flush(); } IndexShard replica = shards.addReplica(); shards.recoverReplica(replica); SegmentsStats segmentsStats = replica.segmentStats(false); SegmentsStats primarySegmentStats = shards.getPrimary().segmentStats(false); assertNotEquals(IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, primarySegmentStats.getMaxUnsafeAutoIdTimestamp()); assertEquals(primarySegmentStats.getMaxUnsafeAutoIdTimestamp(), segmentsStats.getMaxUnsafeAutoIdTimestamp()); assertNotEquals(Long.MAX_VALUE, segmentsStats.getMaxUnsafeAutoIdTimestamp()); } }	hello, it's nice to have you here.
public void testAppendWhileRecovering() throws Exception { try (ReplicationGroup shards = createGroup(0)) { shards.startAll(); IndexShard replica = shards.addReplica(); CountDownLatch latch = new CountDownLatch(2); int numDocs = randomIntBetween(100, 200); shards.appendDocs(1);// just append one to the translog so we can assert below Thread thread = new Thread() { @Override public void run() { try { latch.countDown(); latch.await(); shards.appendDocs(numDocs - 1); } catch (Exception e) { throw new AssertionError(e); } } }; thread.start(); Future<Void> future = shards.asyncRecoverReplica(replica, (indexShard, node) -> new RecoveryTarget(indexShard, node, recoveryListener, version -> { }) { @Override public void cleanFiles(int totalTranslogOps, Store.MetadataSnapshot sourceMetaData) throws IOException { super.cleanFiles(totalTranslogOps, sourceMetaData); latch.countDown(); try { latch.await(); } catch (InterruptedException e) { throw new AssertionError(e); } } }); future.get(); thread.join(); shards.assertAllEqual(numDocs); Engine engine = IndexShardTests.getEngineFromShard(replica); assertEquals("expected at no version lookups ", InternalEngineTests.getNumVersionLookups((InternalEngine) engine), 0); for (IndexShard shard : shards) { engine = IndexShardTests.getEngineFromShard(shard); assertEquals(0, InternalEngineTests.getNumIndexVersionsLookups((InternalEngine) engine)); assertEquals(0, InternalEngineTests.getNumVersionLookups((InternalEngine) engine)); } } }	this line break is annoying, can you revert it?
public void testMissingActiveIdsPreventAdvance() { final Map<String, Long> active = randomAllocations(1, 5); final Map<String, Long> initializing = randomAllocations(0, 5); final Map<String, Long> assigned = new HashMap<>(); assigned.putAll(active); assigned.putAll(initializing); checkpointService.updateAllocationIdsFromMaster( new HashSet<>(randomSubsetOf(randomInt(active.size()-1), active.keySet())), initializing.keySet()); randomSubsetOf(initializing.keySet()).forEach(checkpointService::markAllocationIdAsInSync); assigned.forEach(checkpointService::updateLocalCheckpoint); // now mark all active shards checkpointService.updateAllocationIdsFromMaster(active.keySet(), initializing.keySet()); // global checkpoint can't be advanced, but we need a sync assertTrue(checkpointService.updateCheckpointOnPrimary()); assertThat(checkpointService.getCheckpoint(), equalTo(UNASSIGNED_SEQ_NO)); // update again assigned.forEach(checkpointService::updateLocalCheckpoint); assertTrue(checkpointService.updateCheckpointOnPrimary()); assertThat(checkpointService.getCheckpoint(), not(equalTo(UNASSIGNED_SEQ_NO))); }	nit: active.size()-1 -> active.size() - 1
public void testMissingActiveIdsPreventAdvance() { final Map<String, Long> active = randomAllocations(1, 5); final Map<String, Long> initializing = randomAllocations(0, 5); final Map<String, Long> assigned = new HashMap<>(); assigned.putAll(active); assigned.putAll(initializing); checkpointService.updateAllocationIdsFromMaster( new HashSet<>(randomSubsetOf(randomInt(active.size()-1), active.keySet())), initializing.keySet()); randomSubsetOf(initializing.keySet()).forEach(checkpointService::markAllocationIdAsInSync); assigned.forEach(checkpointService::updateLocalCheckpoint); // now mark all active shards checkpointService.updateAllocationIdsFromMaster(active.keySet(), initializing.keySet()); // global checkpoint can't be advanced, but we need a sync assertTrue(checkpointService.updateCheckpointOnPrimary()); assertThat(checkpointService.getCheckpoint(), equalTo(UNASSIGNED_SEQ_NO)); // update again assigned.forEach(checkpointService::updateLocalCheckpoint); assertTrue(checkpointService.updateCheckpointOnPrimary()); assertThat(checkpointService.getCheckpoint(), not(equalTo(UNASSIGNED_SEQ_NO))); }	nit: initializing.size()-1 -> initializing.size() - 1
@Override protected void masterOperation(final ClusterRerouteRequest request, final ClusterState state, final ActionListener<ClusterRerouteResponse> listener) { // Gather all stale primary allocation commands into a map indexed by the index name they correspond to // so we can check if the nodes they correspond to actually have any data for the shard Map<String, List<AbstractAllocateAllocationCommand>> stalePrimaryAllocations = null; for (AllocationCommand command : request.getCommands().commands()) { if (command instanceof AllocateStalePrimaryAllocationCommand) { if (stalePrimaryAllocations == null) { stalePrimaryAllocations = new HashMap<>(); } final AllocateStalePrimaryAllocationCommand cmd = (AllocateStalePrimaryAllocationCommand) command; stalePrimaryAllocations.computeIfAbsent(cmd.index(), k -> new ArrayList<>()).add(cmd); } } if (stalePrimaryAllocations == null) { // We don't have any stale primary allocations, we simply execute the state update task for the requested allocations submitStateUpdate(request, listener); } else { // We get the index shard store status for indices that we want to allocate stale primaries on first to fail requests // where there's no data for a given shard on a given node. verifyThenSubmitUpdate(request, listener, stalePrimaryAllocations); } }	i think we can afford to make this hashmap eagerly and avoid the noise in the loop.
private void putFakeCorruptionMarker(IndexSettings indexSettings, ShardId shardId, Path indexPath) throws IOException { try(Store store = new Store(shardId, indexSettings, new SimpleFSDirectory(indexPath), new DummyShardLock(shardId))) { store.markStoreCorrupted(new IOException("fake ioexception")); } }	sorry for these two noisy cleanups that snuck into this file
public void testFailedAllocationOfStalePrimaryToDataNodeWithNoData() throws Exception { String dataNodeWithShardCopy = internalCluster().startNode(); logger.info("--> create single shard index"); assertAcked(client().admin().indices().prepareCreate("test").setSettings(Settings.builder() .put("index.number_of_shards", 1).put("index.number_of_replicas", 0)).get()); ensureGreen("test"); String dataNodeWithNoShardCopy = internalCluster().startNode(); ensureStableCluster(2); internalCluster().stopRandomNode(InternalTestCluster.nameFilter(dataNodeWithShardCopy)); ensureStableCluster(1); assertThat(client().admin().cluster().prepareState().get().getState().getRoutingTable().index("test") .getShards().get(0).primaryShard().unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.NODE_LEFT)); logger.info("--> force allocation of stale copy to node that does not have shard copy"); Throwable iae = expectThrows( IllegalArgumentException.class, () -> client().admin().cluster().prepareReroute().add(new AllocateStalePrimaryAllocationCommand("test", 0, dataNodeWithNoShardCopy, true)).get()); assertThat(iae.getMessage(), equalTo("No data for shard [0] of index [test] found on node [" + dataNodeWithNoShardCopy + ']')); logger.info("--> wait until shard is failed and becomes unassigned again"); assertBusy(() -> assertTrue(client().admin().cluster().prepareState().get().getState().toString(), client().admin().cluster().prepareState().get().getState().getRoutingTable().index("test").allPrimaryShardsUnassigned())); assertThat(client().admin().cluster().prepareState().get().getState().getRoutingTable().index("test") .getShards().get(0).primaryShard().unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.NODE_LEFT)); }	i think we no longer want this to be an assertbusy() - we expect that the previous reroute didn't do anything, so it should still be in this state from beforehand.
private SSLContext getSSLContext() throws Exception { String certPath = "/org/elasticsearch/xpack/security/transport/ssl/certs/simple/testclient.crt"; String keyPath = "/org/elasticsearch/xpack/security/transport/ssl/certs/simple/testclient.pem"; SSLContext sslContext; TrustManager tm = CertParsingUtils.trustManager(CertParsingUtils.readCertificates(Collections.singletonList(getDataPath (certPath)))); KeyManager km = CertParsingUtils.keyManager(CertParsingUtils.readCertificates(Collections.singletonList(getDataPath (certPath))), PemUtils.readPrivateKey(getDataPath(keyPath), "testclient"::toCharArray), "testclient".toCharArray()); sslContext = SSLContext.getInstance("TLSv1.2"); sslContext.init(new KeyManager[]{km}, new TrustManager[]{tm}, new SecureRandom()); return sslContext; }	indentation is off after other changes
private SSLContext getSSLContext() { try { String certPath = "/org/elasticsearch/xpack/security/transport/ssl/certs/simple/testclient.crt"; String nodeCertPath = "/org/elasticsearch/xpack/security/transport/ssl/certs/simple/testnode.crt"; String keyPath = "/org/elasticsearch/xpack/security/transport/ssl/certs/simple/testclient.pem"; TrustManager tm = CertParsingUtils.trustManager(CertParsingUtils.readCertificates(Arrays.asList(getDataPath (certPath), getDataPath(nodeCertPath)))); KeyManager km = CertParsingUtils.keyManager(CertParsingUtils.readCertificates(Collections.singletonList(getDataPath (certPath))), PemUtils.readPrivateKey(getDataPath(keyPath), "testclient"::toCharArray), "testclient".toCharArray()); SSLContext context = SSLContext.getInstance("TLSv1.2"); context.init(new KeyManager[]{km}, new TrustManager[]{tm}, new SecureRandom()); return context; } catch (Exception e) { throw new ElasticsearchException("failed to initialize SSLContext", e); } }	can you add spaces? new keymanager[] { km }, new trustmanager[] { tm }
void maybePutIndexUnderLock(BytesRef uid, IndexVersionValue version) { assert assertKeyedLockHeldByCurrentThread(uid); Maps maps = this.maps; if (maps.isSafeAccessMode()) { putIndexUnderLock(uid, version); } else { // The existing delete tombstone is no longer the latest version (.i.e stale) // when a newer version is processed regardless of the safe mode of the version map. // Therefore, it's better to always prune that tombstone to avoid accidental accesses. removeTombstoneUnderLock(uid); maps.current.markAsUnsafe(); assert putAssertionMap(uid, version); } }	how about: // even though we don't store a record of the indexing operation (and mark as unsafe) we should still remove any previous delete for this uuid. note that this is expected to not hurt performance because the tombstone is small (if not empty) when usafe is relevant.
private static void checkNestedUsedInGroupByOrHavingOrWhereOrOrderBy(LogicalPlan p, Set<Failure> localFailures, AttributeMap<Expression> attributeRefs) { List<FieldAttribute> nested = new ArrayList<>(); Consumer<FieldAttribute> matchNested = fa -> { if (fa.isNested()) { nested.add(fa); } }; Consumer<Expression> checkForNested = e -> attributeRefs.getOrDefault(e, e).forEachUp(matchNested, FieldAttribute.class); Consumer<ScalarFunction> checkForNestedInFcuntion = f -> f.arguments().forEach( arg -> arg.forEachUp(matchNested, FieldAttribute.class)); // nested fields shouldn't be used in aggregates or having (yet) p.forEachDown(a -> a.groupings().forEach(agg -> agg.forEachUp(checkForNested)), Aggregate.class); if (!nested.isEmpty()) { localFailures.add( fail(nested.get(0), "Grouping isn't (yet) compatible with nested fields " + new AttributeSet(nested).names())); nested.clear(); } // check in having p.forEachDown(f -> f.forEachDown(a -> f.condition().forEachUp(checkForNested), Aggregate.class), Filter.class); if (!nested.isEmpty()) { localFailures.add( fail(nested.get(0), "HAVING isn't (yet) compatible with nested fields " + new AttributeSet(nested).names())); nested.clear(); } // check in where (scalars not allowed) p.forEachDown(f -> f.condition().forEachUp(e -> attributeRefs.getOrDefault(e, e).forEachUp(sf -> { if (sf instanceof BinaryComparison == false && sf instanceof IsNull == false && sf instanceof IsNotNull == false && sf instanceof Not == false && sf instanceof BinaryLogic== false) { checkForNestedInFcuntion.accept(sf); }}, ScalarFunction.class) ), Filter.class); if (!nested.isEmpty()) { localFailures.add( fail(nested.get(0), "WHERE isn't (yet) compatible with scalar functions on nested fields " + new AttributeSet(nested).names())); nested.clear(); } // check in order by (scalars not allowed) p.forEachDown(ob -> ob.order().forEach(o -> o.forEachUp(e -> attributeRefs.getOrDefault(e, e).forEachUp(checkForNestedInFcuntion, ScalarFunction.class) )), OrderBy.class); if (!nested.isEmpty()) { localFailures.add( fail(nested.get(0), "ORDER BY isn't (yet) compatible with scalar functions on nested fields " + new AttributeSet(nested).names())); } }	typo in the variable name: checkfornestedinfcuntion .
private static Map<String, RoleDescriptor> initializeReservedRoles() { return MapBuilder.<String, RoleDescriptor>newMapBuilder() .put("superuser", SUPERUSER_ROLE_DESCRIPTOR) .put("transport_client", new RoleDescriptor("transport_client", new String[] { "transport_client" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_user", new RoleDescriptor("kibana_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("manage", "read", "index", "delete") .build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("all").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("monitoring_user", new RoleDescriptor("monitoring_user", new String[] { "cluster:monitor/main" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_agent", new RoleDescriptor("remote_monitoring_agent", new String[] { "manage_index_templates", "manage_ingest_pipelines", "monitor", "cluster:monitor/xpack/watcher/watch/get", "cluster:admin/xpack/watcher/watch/put", "cluster:admin/xpack/watcher/watch/delete", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices("metricbeat-*").privileges("index", "create_index").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_collector", new RoleDescriptor( "remote_monitoring_collector", new String[] { "monitor" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("monitor").build(), RoleDescriptor.IndicesPrivileges.builder().indices(SystemIndicesNames.indexNames().toArray(new String[0])) .privileges("monitor").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("read").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null )) .put("ingest_admin", new RoleDescriptor("ingest_admin", new String[] { "manage_index_templates", "manage_pipeline" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) // reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role .put("reporting_user", new RoleDescriptor("reporting_user", null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_dashboard_only_user", new RoleDescriptor( "kibana_dashboard_only_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*").privileges("read", "view_index_metadata").build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put(KibanaUser.ROLE_NAME, new RoleDescriptor(KibanaUser.ROLE_NAME, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", "manage_token" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*", ".reporting-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build() }, null, new ConditionalClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("logstash_system", new RoleDescriptor("logstash_system", new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("beats_admin", new RoleDescriptor("beats_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.BEATS_ROLE, new RoleDescriptor(UsernamesField.BEATS_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.APM_ROLE, new RoleDescriptor(UsernamesField.APM_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_user", new RoleDescriptor("machine_learning_user", new String[] { "monitor_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-notifications*") .privileges("view_index_metadata", "read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*") .privileges("view_index_metadata", "read", "write").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_admin", new RoleDescriptor("machine_learning_admin", new String[] { "manage_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".ml-anomalies*", ".ml-notifications*", ".ml-state*", ".ml-meta*") .privileges("view_index_metadata", "read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*") .privileges("view_index_metadata", "read", "write").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_admin", new RoleDescriptor("watcher_admin", new String[] { "manage_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX, TriggeredWatchStoreField.INDEX_NAME, HistoryStoreField.INDEX_PREFIX + "*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_user", new RoleDescriptor("watcher_user", new String[] { "monitor_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX) .privileges("read") .build(), RoleDescriptor.IndicesPrivileges.builder().indices(HistoryStoreField.INDEX_PREFIX + "*") .privileges("read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("logstash_admin", new RoleDescriptor("logstash_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".logstash*") .privileges("create", "delete", "index", "manage", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_user", new RoleDescriptor("rollup_user", new String[] { "monitor_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_admin", new RoleDescriptor("rollup_admin", new String[] { "manage_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .immutableMap(); }	this should have allowrestrictedindices(true)
public DoubleValues getValues(LeafReaderContext leaf, DoubleValues scores) { LeafGeoPointFieldData leafData = (LeafGeoPointFieldData) fieldData.load(leaf); final MultiGeoPointValues values = leafData.getGeoPointValues(); return new DoubleValues() { int doc = -1; @Override public double doubleValue() throws IOException { if (values.advanceExact(doc)) { return values.nextValue().getLat(); } return 0; } @Override public boolean advanceExact(int doc) throws IOException { this.doc = doc; return true; } }; }	i don't think this is a correct implementation. if the correct impl has different perf characteristics we need to fix them but lets keep it correct. i don't think it should return 0 if there is nothing to return we should rather throw an exception instead? another option would be to assign the value in advanceexact instead of the doc and then return the value only in doublevalue and assert that we never do it unless it's actually advanced to this doc?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeVInt(entries.size()); for (Entry entry : entries) { entry.snapshot().writeTo(out); out.writeBoolean(entry.includeGlobalState()); out.writeBoolean(entry.partial()); out.writeByte(entry.state().value()); out.writeVInt(entry.indices().size()); for (IndexId index : entry.indices()) { index.writeTo(out); } out.writeLong(entry.startTime()); out.writeVInt(entry.shards().size()); for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : entry.shards()) { shardEntry.key.writeTo(out); shardEntry.value.writeTo(out); } out.writeLong(entry.repositoryStateId); out.writeOptionalString(entry.failure); out.writeMap(entry.userMetadata); if (out.getVersion().onOrAfter(VERSION_IN_SNAPSHOT_VERSION)) { Version.writeVersion(entry.version, out); } else { out.writeBoolean(SnapshotsService.useShardGenerations(entry.version)); } if (out.getVersion().onOrAfter(DATA_STREAMS_IN_SNAPSHOT)) { out.writeStringArray(entry.dataStreams.toArray(new String[0])); } } }	we should assert that we don't have any data streams if we're writing to an older version and must make sure to not start snapshots that include data streams as long as there's old version nodes in the cluster that don't understand data stream snapshots.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeVInt(entries.size()); for (Entry entry : entries) { entry.snapshot().writeTo(out); out.writeBoolean(entry.includeGlobalState()); out.writeBoolean(entry.partial()); out.writeByte(entry.state().value()); out.writeVInt(entry.indices().size()); for (IndexId index : entry.indices()) { index.writeTo(out); } out.writeLong(entry.startTime()); out.writeVInt(entry.shards().size()); for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : entry.shards()) { shardEntry.key.writeTo(out); shardEntry.value.writeTo(out); } out.writeLong(entry.repositoryStateId); out.writeOptionalString(entry.failure); out.writeMap(entry.userMetadata); if (out.getVersion().onOrAfter(VERSION_IN_SNAPSHOT_VERSION)) { Version.writeVersion(entry.version, out); } else { out.writeBoolean(SnapshotsService.useShardGenerations(entry.version)); } if (out.getVersion().onOrAfter(DATA_STREAMS_IN_SNAPSHOT)) { out.writeStringArray(entry.dataStreams.toArray(new String[0])); } } }	you can just use writestringcollection no need to go through the array round trip.
@Override public List<Route> routes() { List<Route> routes = new ArrayList<>(); String pathPrefix = getPathPrefix(); routes.add(new Route(GET, pathPrefix)); if (healthService != null) { Map<String, List<String>> componentsToIndicators = healthService.getComponentToIndicatorMapping(); List<Route> componentRoutes = componentsToIndicators.keySet() .stream() .map(componentName -> new Route(GET, pathPrefix + "/" + componentName)) .collect(toList()); List<Route> indicatorRoutes = componentsToIndicators.entrySet() .stream() .flatMap( entry -> entry.getValue().stream().map(indicator -> new Route(GET, pathPrefix + "/" + entry.getKey() + "/" + indicator)) ) .collect(toList()); routes.addAll(componentRoutes); routes.addAll(indicatorRoutes); } return routes; }	you might be able to use a request parameter within the routes that are returned here. might make this code much easier to read? this way you can just pick out the component id and indicator ids if they exist and forward them on to the transport action? new route(get, /_internal/_health/{component_id}); .... request.param("component_id")
public RangeFieldType fieldType() { return (RangeFieldType)fieldType; }	note: we think this is a bug, i'll open a separate pr fixing this
@Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, HISTOGRAM { @Override public ValuesSource getEmpty() { // TODO: Is this the correct exception type here? throw new IllegalArgumentException("Can't deal with unmapped ValuesSource type " + this.value()); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { throw new AggregationExecutionException("value source of type [" + this.value() + "] is not supported by scripts"); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { final IndexFieldData<?> indexFieldData = fieldContext.indexFieldData(); if (!(indexFieldData instanceof IndexHistogramFieldData)) { throw new IllegalArgumentException("Expected histogram type on field [" + fieldContext.field() + "], but got [" + fieldContext.fieldType().typeName() + "]"); } return new ValuesSource.Histogram.Fielddata((IndexHistogramFieldData) indexFieldData); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, // TODO: Ordinal Numbering sync with types from master IP{ @Override public ValuesSource getEmpty() { return BYTES.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return BYTES.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return BYTES.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return BYTES.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } }, DATE { @Override public ValuesSource getEmpty() { return NUMERIC.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return NUMERIC.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return NUMERIC.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return NUMERIC.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } }, BOOLEAN { @Override public ValuesSource getEmpty() { return NUMERIC.getEmpty(); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { return NUMERIC.getScript(script, scriptValueType); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { return NUMERIC.getField(fieldContext, script); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { return NUMERIC.replaceMissing(valuesSource, rawMissing, docValueFormat, now); } }; public static ValuesSourceType fromString(String name) { return valueOf(name.trim().toUpperCase(Locale.ROOT)); } public static ValuesSourceType fromStream(StreamInput in) throws IOException { return in.readEnum(CoreValuesSourceType.class); } @Override public void writeTo(StreamOutput out) throws IOException { CoreValuesSourceType state = this; out.writeEnum(state); }	i'm not sure how i feel about pulling the specialized values source types here into master without the code that uses them. that seems potentially confusing without context - e.g. we have an ip type now, but vsconfig will still resolve ip fields to bytes because this isn't wired up. at a minimum, we should leave a comment in big letters that this is test only until the refactor lands (which hopefully is soon, but still)
public void testSupportedFieldTypes() throws IOException { MapperRegistry mapperRegistry = new IndicesModule(Collections.emptyList()).getMapperRegistry(); Settings settings = Settings.builder().put("index.version.created", Version.CURRENT.id).build(); String fieldName = "typeTestFieldName"; List<ValuesSourceType> supportedVSTypes = getSupportedValuesSourceTypes(); if (supportedVSTypes.isEmpty()) { // If the test says it doesn't support any VStypes, it has not been converted yet so skip return; } else if (supportedVSTypes.contains(CoreValuesSourceType.ANY)) { throw new IllegalArgumentException("Tests should not specify CoreValuesSourceType.ANY as a supported ValuesSourceType, " + "but should instead list the concrete ValuesSourceTypes that are supported"); } for (Map.Entry<String, Mapper.TypeParser> mappedType : mapperRegistry.getMapperParsers().entrySet()) { if (TYPE_TEST_BLACKLIST.contains(mappedType.getKey())) { continue; } if (mappedType.getKey().equals(ObjectMapper.CONTENT_TYPE)) { // Cannot aggregate objects continue; } Map<String, Object> source = new HashMap<>(); source.put("type", mappedType.getKey()); source.put("doc_values", "true"); FieldMapper mapper = null; try { Mapper.Builder builder = mappedType.getValue().parse(fieldName, source, new MockParserContext()); mapper = (FieldMapper) builder.build(new BuilderContext(settings, new ContentPath())); } catch (Exception e) { fail(); } MappedFieldType fieldType = mapper.fieldType(); try (Directory directory = newDirectory()) { RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory); writeTestDoc(fieldType, fieldName, indexWriter); indexWriter.close(); try (IndexReader indexReader = DirectoryReader.open(directory)) { IndexSearcher indexSearcher = newSearcher(indexReader, true, true); AggregationBuilder aggregationBuilder = createAggBuilderForTypeTest(fieldType, fieldName); try { searchAndReduce(indexSearcher, new MatchAllDocsQuery(), aggregationBuilder, fieldType); if (supportedVSTypes.contains(fieldType.getValuesSourceType()) == false) { fail("Aggregator [" + aggregationBuilder.getType() + "] should not support field type [" + fieldType.typeName() + "] but executing against the field did not throw an excetion"); } } catch (Exception e) { if (supportedVSTypes.contains(fieldType.getValuesSourceType())) { fail("Aggregator [" + aggregationBuilder.getType() + "] supports field type [" + fieldType.typeName() + "] but executing against the field threw an exception: [" + e.getMessage() + "]"); } } } } } }	i think you missed removing this check when adding the blacklist?
public void testSupportedFieldTypes() throws IOException { MapperRegistry mapperRegistry = new IndicesModule(Collections.emptyList()).getMapperRegistry(); Settings settings = Settings.builder().put("index.version.created", Version.CURRENT.id).build(); String fieldName = "typeTestFieldName"; List<ValuesSourceType> supportedVSTypes = getSupportedValuesSourceTypes(); if (supportedVSTypes.isEmpty()) { // If the test says it doesn't support any VStypes, it has not been converted yet so skip return; } else if (supportedVSTypes.contains(CoreValuesSourceType.ANY)) { throw new IllegalArgumentException("Tests should not specify CoreValuesSourceType.ANY as a supported ValuesSourceType, " + "but should instead list the concrete ValuesSourceTypes that are supported"); } for (Map.Entry<String, Mapper.TypeParser> mappedType : mapperRegistry.getMapperParsers().entrySet()) { if (TYPE_TEST_BLACKLIST.contains(mappedType.getKey())) { continue; } if (mappedType.getKey().equals(ObjectMapper.CONTENT_TYPE)) { // Cannot aggregate objects continue; } Map<String, Object> source = new HashMap<>(); source.put("type", mappedType.getKey()); source.put("doc_values", "true"); FieldMapper mapper = null; try { Mapper.Builder builder = mappedType.getValue().parse(fieldName, source, new MockParserContext()); mapper = (FieldMapper) builder.build(new BuilderContext(settings, new ContentPath())); } catch (Exception e) { fail(); } MappedFieldType fieldType = mapper.fieldType(); try (Directory directory = newDirectory()) { RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory); writeTestDoc(fieldType, fieldName, indexWriter); indexWriter.close(); try (IndexReader indexReader = DirectoryReader.open(directory)) { IndexSearcher indexSearcher = newSearcher(indexReader, true, true); AggregationBuilder aggregationBuilder = createAggBuilderForTypeTest(fieldType, fieldName); try { searchAndReduce(indexSearcher, new MatchAllDocsQuery(), aggregationBuilder, fieldType); if (supportedVSTypes.contains(fieldType.getValuesSourceType()) == false) { fail("Aggregator [" + aggregationBuilder.getType() + "] should not support field type [" + fieldType.typeName() + "] but executing against the field did not throw an excetion"); } } catch (Exception e) { if (supportedVSTypes.contains(fieldType.getValuesSourceType())) { fail("Aggregator [" + aggregationBuilder.getType() + "] supports field type [" + fieldType.typeName() + "] but executing against the field threw an exception: [" + e.getMessage() + "]"); } } } } } }	why not just let the exception happen here? the test will still fail, and the exception is likely to be clearer as to what went wrong than just a call to fail()
public void testSupportedFieldTypes() throws IOException { MapperRegistry mapperRegistry = new IndicesModule(Collections.emptyList()).getMapperRegistry(); Settings settings = Settings.builder().put("index.version.created", Version.CURRENT.id).build(); String fieldName = "typeTestFieldName"; List<ValuesSourceType> supportedVSTypes = getSupportedValuesSourceTypes(); if (supportedVSTypes.isEmpty()) { // If the test says it doesn't support any VStypes, it has not been converted yet so skip return; } else if (supportedVSTypes.contains(CoreValuesSourceType.ANY)) { throw new IllegalArgumentException("Tests should not specify CoreValuesSourceType.ANY as a supported ValuesSourceType, " + "but should instead list the concrete ValuesSourceTypes that are supported"); } for (Map.Entry<String, Mapper.TypeParser> mappedType : mapperRegistry.getMapperParsers().entrySet()) { if (TYPE_TEST_BLACKLIST.contains(mappedType.getKey())) { continue; } if (mappedType.getKey().equals(ObjectMapper.CONTENT_TYPE)) { // Cannot aggregate objects continue; } Map<String, Object> source = new HashMap<>(); source.put("type", mappedType.getKey()); source.put("doc_values", "true"); FieldMapper mapper = null; try { Mapper.Builder builder = mappedType.getValue().parse(fieldName, source, new MockParserContext()); mapper = (FieldMapper) builder.build(new BuilderContext(settings, new ContentPath())); } catch (Exception e) { fail(); } MappedFieldType fieldType = mapper.fieldType(); try (Directory directory = newDirectory()) { RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory); writeTestDoc(fieldType, fieldName, indexWriter); indexWriter.close(); try (IndexReader indexReader = DirectoryReader.open(directory)) { IndexSearcher indexSearcher = newSearcher(indexReader, true, true); AggregationBuilder aggregationBuilder = createAggBuilderForTypeTest(fieldType, fieldName); try { searchAndReduce(indexSearcher, new MatchAllDocsQuery(), aggregationBuilder, fieldType); if (supportedVSTypes.contains(fieldType.getValuesSourceType()) == false) { fail("Aggregator [" + aggregationBuilder.getType() + "] should not support field type [" + fieldType.typeName() + "] but executing against the field did not throw an excetion"); } } catch (Exception e) { if (supportedVSTypes.contains(fieldType.getValuesSourceType())) { fail("Aggregator [" + aggregationBuilder.getType() + "] supports field type [" + fieldType.typeName() + "] but executing against the field threw an exception: [" + e.getMessage() + "]"); } } } } } }	once we are running these tests against aggregations using the vs registry, we can say reliably what exception should be thrown. at that point, we should invert this test - if the vstype is not supported, we should use expectthrows to confirm we get the right exception, and otherwise fail on any exception. probably can't do that now though since our exceptions are a bit inconsistent in master.
public void testSupportedFieldTypes() throws IOException { MapperRegistry mapperRegistry = new IndicesModule(Collections.emptyList()).getMapperRegistry(); Settings settings = Settings.builder().put("index.version.created", Version.CURRENT.id).build(); String fieldName = "typeTestFieldName"; List<ValuesSourceType> supportedVSTypes = getSupportedValuesSourceTypes(); if (supportedVSTypes.isEmpty()) { // If the test says it doesn't support any VStypes, it has not been converted yet so skip return; } else if (supportedVSTypes.contains(CoreValuesSourceType.ANY)) { throw new IllegalArgumentException("Tests should not specify CoreValuesSourceType.ANY as a supported ValuesSourceType, " + "but should instead list the concrete ValuesSourceTypes that are supported"); } for (Map.Entry<String, Mapper.TypeParser> mappedType : mapperRegistry.getMapperParsers().entrySet()) { if (TYPE_TEST_BLACKLIST.contains(mappedType.getKey())) { continue; } if (mappedType.getKey().equals(ObjectMapper.CONTENT_TYPE)) { // Cannot aggregate objects continue; } Map<String, Object> source = new HashMap<>(); source.put("type", mappedType.getKey()); source.put("doc_values", "true"); FieldMapper mapper = null; try { Mapper.Builder builder = mappedType.getValue().parse(fieldName, source, new MockParserContext()); mapper = (FieldMapper) builder.build(new BuilderContext(settings, new ContentPath())); } catch (Exception e) { fail(); } MappedFieldType fieldType = mapper.fieldType(); try (Directory directory = newDirectory()) { RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory); writeTestDoc(fieldType, fieldName, indexWriter); indexWriter.close(); try (IndexReader indexReader = DirectoryReader.open(directory)) { IndexSearcher indexSearcher = newSearcher(indexReader, true, true); AggregationBuilder aggregationBuilder = createAggBuilderForTypeTest(fieldType, fieldName); try { searchAndReduce(indexSearcher, new MatchAllDocsQuery(), aggregationBuilder, fieldType); if (supportedVSTypes.contains(fieldType.getValuesSourceType()) == false) { fail("Aggregator [" + aggregationBuilder.getType() + "] should not support field type [" + fieldType.typeName() + "] but executing against the field did not throw an excetion"); } } catch (Exception e) { if (supportedVSTypes.contains(fieldType.getValuesSourceType())) { fail("Aggregator [" + aggregationBuilder.getType() + "] supports field type [" + fieldType.typeName() + "] but executing against the field threw an exception: [" + e.getMessage() + "]"); } } } } } }	i'm concerned we might introduce some instability by trying to aggregate over arbitrarily large ranges, which isn't really supported (i.e. it eventually trips the circuit breaker). i'd suggest just just setting the end value to the next value after the start. also, you can use rangetype.nextup(object) for all cases, it's always well defined.
public void testSupportedFieldTypes() throws IOException { MapperRegistry mapperRegistry = new IndicesModule(Collections.emptyList()).getMapperRegistry(); Settings settings = Settings.builder().put("index.version.created", Version.CURRENT.id).build(); String fieldName = "typeTestFieldName"; List<ValuesSourceType> supportedVSTypes = getSupportedValuesSourceTypes(); if (supportedVSTypes.isEmpty()) { // If the test says it doesn't support any VStypes, it has not been converted yet so skip return; } else if (supportedVSTypes.contains(CoreValuesSourceType.ANY)) { throw new IllegalArgumentException("Tests should not specify CoreValuesSourceType.ANY as a supported ValuesSourceType, " + "but should instead list the concrete ValuesSourceTypes that are supported"); } for (Map.Entry<String, Mapper.TypeParser> mappedType : mapperRegistry.getMapperParsers().entrySet()) { if (TYPE_TEST_BLACKLIST.contains(mappedType.getKey())) { continue; } if (mappedType.getKey().equals(ObjectMapper.CONTENT_TYPE)) { // Cannot aggregate objects continue; } Map<String, Object> source = new HashMap<>(); source.put("type", mappedType.getKey()); source.put("doc_values", "true"); FieldMapper mapper = null; try { Mapper.Builder builder = mappedType.getValue().parse(fieldName, source, new MockParserContext()); mapper = (FieldMapper) builder.build(new BuilderContext(settings, new ContentPath())); } catch (Exception e) { fail(); } MappedFieldType fieldType = mapper.fieldType(); try (Directory directory = newDirectory()) { RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory); writeTestDoc(fieldType, fieldName, indexWriter); indexWriter.close(); try (IndexReader indexReader = DirectoryReader.open(directory)) { IndexSearcher indexSearcher = newSearcher(indexReader, true, true); AggregationBuilder aggregationBuilder = createAggBuilderForTypeTest(fieldType, fieldName); try { searchAndReduce(indexSearcher, new MatchAllDocsQuery(), aggregationBuilder, fieldType); if (supportedVSTypes.contains(fieldType.getValuesSourceType()) == false) { fail("Aggregator [" + aggregationBuilder.getType() + "] should not support field type [" + fieldType.typeName() + "] but executing against the field did not throw an excetion"); } } catch (Exception e) { if (supportedVSTypes.contains(fieldType.getValuesSourceType())) { fail("Aggregator [" + aggregationBuilder.getType() + "] supports field type [" + fieldType.typeName() + "] but executing against the field threw an exception: [" + e.getMessage() + "]"); } } } } } }	can this generate an invalid range if end sorts before start?
public void testSupportedFieldTypes() throws IOException { MapperRegistry mapperRegistry = new IndicesModule(Collections.emptyList()).getMapperRegistry(); Settings settings = Settings.builder().put("index.version.created", Version.CURRENT.id).build(); String fieldName = "typeTestFieldName"; List<ValuesSourceType> supportedVSTypes = getSupportedValuesSourceTypes(); if (supportedVSTypes.isEmpty()) { // If the test says it doesn't support any VStypes, it has not been converted yet so skip return; } else if (supportedVSTypes.contains(CoreValuesSourceType.ANY)) { throw new IllegalArgumentException("Tests should not specify CoreValuesSourceType.ANY as a supported ValuesSourceType, " + "but should instead list the concrete ValuesSourceTypes that are supported"); } for (Map.Entry<String, Mapper.TypeParser> mappedType : mapperRegistry.getMapperParsers().entrySet()) { if (TYPE_TEST_BLACKLIST.contains(mappedType.getKey())) { continue; } if (mappedType.getKey().equals(ObjectMapper.CONTENT_TYPE)) { // Cannot aggregate objects continue; } Map<String, Object> source = new HashMap<>(); source.put("type", mappedType.getKey()); source.put("doc_values", "true"); FieldMapper mapper = null; try { Mapper.Builder builder = mappedType.getValue().parse(fieldName, source, new MockParserContext()); mapper = (FieldMapper) builder.build(new BuilderContext(settings, new ContentPath())); } catch (Exception e) { fail(); } MappedFieldType fieldType = mapper.fieldType(); try (Directory directory = newDirectory()) { RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory); writeTestDoc(fieldType, fieldName, indexWriter); indexWriter.close(); try (IndexReader indexReader = DirectoryReader.open(directory)) { IndexSearcher indexSearcher = newSearcher(indexReader, true, true); AggregationBuilder aggregationBuilder = createAggBuilderForTypeTest(fieldType, fieldName); try { searchAndReduce(indexSearcher, new MatchAllDocsQuery(), aggregationBuilder, fieldType); if (supportedVSTypes.contains(fieldType.getValuesSourceType()) == false) { fail("Aggregator [" + aggregationBuilder.getType() + "] should not support field type [" + fieldType.typeName() + "] but executing against the field did not throw an excetion"); } } catch (Exception e) { if (supportedVSTypes.contains(fieldType.getValuesSourceType())) { fail("Aggregator [" + aggregationBuilder.getType() + "] supports field type [" + fieldType.typeName() + "] but executing against the field threw an exception: [" + e.getMessage() + "]"); } } } } } }	we've got copies of this same function in iprangetests and iprangeaggregatortests, at least. maybe put it up in estestcase and have all three use the same version?
@Override public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException { ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalHistogram.TYPE, context) .requiresSortedValues(true) .targetValueType(ValueType.NUMERIC) .formattable(true) .build(); boolean keyed = false; long minDocCount = 1; InternalOrder order = (InternalOrder) InternalOrder.KEY_ASC; long interval = -1; ExtendedBounds extendedBounds = null; XContentParser.Token token; String currentFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (vsParser.token(currentFieldName, token, parser)) { continue; } else if (token.isValue()) { if ("interval".equals(currentFieldName)) { interval = parser.longValue(); } else if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) { minDocCount = parser.longValue(); } else if ("keyed".equals(currentFieldName)) { keyed = parser.booleanValue(); } else { throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "]."); } } else if (token == XContentParser.Token.START_OBJECT) { if ("order".equals(currentFieldName)) { while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.VALUE_STRING) { String dir = parser.text(); boolean asc = "asc".equals(dir); if (!asc && !"desc".equals(dir)) { throw new SearchParseException(context, "Unknown order direction [" + dir + "] in aggregation [" + aggregationName + "]. Should be either [asc] or [desc]"); } order = resolveOrder(currentFieldName, asc); } } } else if (EXTENDED_BOUNDS.match(currentFieldName)) { extendedBounds = new ExtendedBounds(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token.isValue()) { if ("min".equals(currentFieldName)) { extendedBounds.min = parser.longValue(true); } else if ("max".equals(currentFieldName)) { extendedBounds.max = parser.longValue(true); } else { throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "]."); } } } } else { throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "]."); } } else { throw new SearchParseException(context, "Unexpected token " + token + " in aggregation [" + aggregationName + "]."); } } if (interval < 0) { throw new SearchParseException(context, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]"); } Rounding rounding = new Rounding.Interval(interval); if (extendedBounds != null) { // with numeric histogram, we can process here and fail fast if the bounds are invalid extendedBounds.processAndValidate(aggregationName, context, ValueParser.RAW); } return new HistogramAggregator.Factory(aggregationName, vsParser.config(), rounding, order, keyed, minDocCount, extendedBounds, InternalHistogram.FACTORY); }	to coerce, should be: parser.longvalue(true);
@Override public void close() throws IOException { if (this.closeCalled.compareAndSet(false, true) == false) { throw new IllegalStateException("DummySubCommand already closed"); } if (throwsExceptionOnClose) { throw new IOException("Error occurred while closing DummySubCommand"); } } } DummyMultiCommand multiCommand; @Before public void setupCommand() { multiCommand = new DummyMultiCommand(); } @Override protected Command newCommand() { return multiCommand; } public void testNoCommandsConfigured() throws Exception { IllegalStateException e = expectThrows(IllegalStateException.class, () -> { execute(); }); assertEquals("No subcommands configured", e.getMessage()); } public void testUnknownCommand() throws Exception { multiCommand.subcommands.put("something", new DummySubCommand()); UserException e = expectThrows(UserException.class, () -> { execute("somethingelse"); }); assertEquals(ExitCodes.USAGE, e.exitCode); assertEquals("Unknown command [somethingelse]", e.getMessage()); } public void testMissingCommand() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); UserException e = expectThrows(UserException.class, () -> { execute(); }); assertEquals(ExitCodes.USAGE, e.exitCode); assertEquals("Missing command", e.getMessage()); } public void testHelp() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); multiCommand.subcommands.put("command2", new DummySubCommand()); execute("-h"); String output = terminal.getOutput(); assertTrue(output, output.contains("command1")); assertTrue(output, output.contains("command2")); } public void testSubcommandHelp() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); multiCommand.subcommands.put("command2", new DummySubCommand()); execute("command2", "-h"); String output = terminal.getOutput(); assertFalse(output, output.contains("command1")); assertTrue(output, output.contains("A dummy subcommand")); } public void testSubcommandArguments() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); execute("command1", "foo", "bar"); String output = terminal.getOutput(); assertFalse(output, output.contains("command1")); assertTrue(output, output.contains("Arguments: [foo, bar]")); } public void testClose() throws Exception { DummySubCommand subCommand1 = new DummySubCommand(); DummySubCommand subCommand2 = new DummySubCommand(); multiCommand.subcommands.put("command1", subCommand1); multiCommand.subcommands.put("command2", subCommand2); multiCommand.close(); assertTrue("MultiCommand was not closed when close method is invoked", multiCommand.closed.get()); assertTrue("SubCommand1 was not closed when close method is invoked", subCommand1.closeCalled.get()); assertTrue("SubCommand2 was not closed when close method is invoked", subCommand2.closeCalled.get()); } public void testCloseWhenSubCommandCloseThrowsException() throws Exception { final boolean command1Throws = randomBoolean(); final boolean command2Throws = randomBoolean(); final DummySubCommand subCommand1 = new DummySubCommand(command1Throws); final DummySubCommand subCommand2 = new DummySubCommand(command2Throws); multiCommand.subcommands.put("command1", subCommand1); multiCommand.subcommands.put("command2", subCommand2); if (command1Throws || command2Throws) { // verify exception is thrown, as well as other non failed sub-commands closed // properly. IOException ioe = expectThrows(IOException.class, multiCommand::close); assertEquals("Error occurred while closing DummySubCommand", ioe.getMessage()); if (command1Throws && command2Throws) { assertEquals(1, ioe.getSuppressed().length); assertTrue("Missing suppressed exceptions", ioe.getSuppressed()[0] instanceof IOException); assertEquals("Error occurred while closing DummySubCommand", ioe.getSuppressed()[0].getMessage()); } } else { multiCommand.close(); } assertTrue("SubCommand1 was not closed when close method is invoked", subCommand1.closeCalled.get()); assertTrue("SubCommand2 was not closed when close method is invoked", subCommand2.closeCalled.get()); } // Tests for multicommand error logging static class ErrorHandlingMultiCommand extends MultiCommand { ErrorHandlingMultiCommand() { super("error catching", () -> {}); } @Override protected boolean addShutdownHook() { return false; } } static class ErrorThrowingSubCommand extends Command { ErrorThrowingSubCommand() { super("error throwing", () -> {}); } @Override protected void execute(Terminal terminal, OptionSet options) throws Exception { throw new UserException(1, "Dummy error"); } @Override protected boolean addShutdownHook() { return false; } } public void testErrorDisplayedWithDefault() throws Exception { MockTerminal terminal = new MockTerminal(); MultiCommand mc = new ErrorHandlingMultiCommand(); mc.subcommands.put("throw", new ErrorThrowingSubCommand()); mc.main(new String[]{"throw", "--silent"}, terminal); assertThat(terminal.getOutput(), is(emptyString())); assertThat(terminal.getErrorOutput(), equalTo("ERROR: Dummy error\\\\n")); } public void testNoErrorDisplayedWithOverride() throws Exception { MockTerminal terminal = new MockTerminal(); MultiCommand mc = new ErrorHandlingMultiCommand(); mc.subcommands.put("throw", new ErrorThrowingSubCommand() { @Override protected void execute(Terminal terminal, OptionSet options) throws Exception { terminal.errorPrintln(Terminal.Verbosity.NORMAL, "Dummy error"); throw new UserException(1, null); } }); mc.main(new String[]{"throw", "--silent"}	what we're really testing here is that a null user exception prints nothing; the errorprintln line mimics an implementation detail from another class. i should remove that line and rename the test.
@Override public String nodeDescription() { return clusterName.value() + "/" + localNode(); }	this will append discoverynode#tostring but i think that you still want discoverynode#id?
* a listener to resolve once it finished * @param listener listener to resolve once this method actions including executing {@code consumer} in the non-failure case complete * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation * @throws IllegalStateException if the relocation target is no longer part of the replication group */ public void relocated(final String targetAllocationId, final BiConsumer<ReplicationTracker.PrimaryContext, ActionListener<Void>> consumer, final ActionListener<Void> listener) throws IllegalIndexShardStateException, IllegalStateException { assert shardRouting.primary() : "only primaries can be marked as relocated: " + shardRouting; try (Releasable forceRefreshes = refreshListeners.forceRefreshes()) { indexShardOperationPermits.asyncBlockOperations(new ActionListener<>() { @Override public void onResponse(Releasable releasable) { boolean success = false; try { forceRefreshes.close(); // no shard operation permits are being held here, move state from started to relocated assert indexShardOperationPermits.getActiveOperationsCount() == OPERATIONS_BLOCKED : "in-flight operations in progress while moving shard state to relocated"; /* * We should not invoke the runnable under the mutex as the expected implementation is to handoff the primary * context via a network operation. Doing this under the mutex can implicitly block the cluster state update thread * on network operations. */ verifyRelocatingState(); final ReplicationTracker.PrimaryContext primaryContext = replicationTracker.startRelocationHandoff(targetAllocationId); // make sure we release all permits before we resolve the final listener final ActionListener<Void> wrappedInnerListener = ActionListener.runBefore(listener, releasable::close); final ActionListener<Void> wrappedListener = new ActionListener<>() { @Override public void onResponse(Void unused) { try { // make changes to primaryMode and relocated flag only under mutex synchronized (mutex) { verifyRelocatingState(); replicationTracker.completeRelocationHandoff(); } wrappedInnerListener.onResponse(null); } catch (Exception e) { onFailure(e); } } @Override public void onFailure(Exception e) { try { replicationTracker.abortRelocationHandoff(); } catch (final Exception inner) { e.addSuppressed(inner); } wrappedInnerListener.onFailure(e); } }; try { consumer.accept(primaryContext, wrappedListener); } catch (final Exception e) { wrappedListener.onFailure(e); } success = true; } catch (Exception e) { listener.onFailure(e); } finally { if (success == false) { releasable.close(); } } } @Override public void onFailure(Exception e) { if (e instanceof TimeoutException) { logger.warn("timed out waiting for relocation hand-off to complete"); // This is really bad as ongoing replication operations are preventing this shard from completing relocation // hand-off. // Fail primary relocation source and target shards. failShard("timed out waiting for relocation hand-off to complete", null); listener.onFailure( new IndexShardClosedException(shardId(), "timed out waiting for relocation hand-off to complete")); } else { listener.onFailure(e); } } }, 30L, TimeUnit.SECONDS); } }	this forks a new thread which blocks, pending acquiring all the permits, but that means we are now blocking outside the scope of the cancellablethreads so will no longer interrupt this wait if the recovery is cancelled.
* a listener to resolve once it finished * @param listener listener to resolve once this method actions including executing {@code consumer} in the non-failure case complete * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation * @throws IllegalStateException if the relocation target is no longer part of the replication group */ public void relocated(final String targetAllocationId, final BiConsumer<ReplicationTracker.PrimaryContext, ActionListener<Void>> consumer, final ActionListener<Void> listener) throws IllegalIndexShardStateException, IllegalStateException { assert shardRouting.primary() : "only primaries can be marked as relocated: " + shardRouting; try (Releasable forceRefreshes = refreshListeners.forceRefreshes()) { indexShardOperationPermits.asyncBlockOperations(new ActionListener<>() { @Override public void onResponse(Releasable releasable) { boolean success = false; try { forceRefreshes.close(); // no shard operation permits are being held here, move state from started to relocated assert indexShardOperationPermits.getActiveOperationsCount() == OPERATIONS_BLOCKED : "in-flight operations in progress while moving shard state to relocated"; /* * We should not invoke the runnable under the mutex as the expected implementation is to handoff the primary * context via a network operation. Doing this under the mutex can implicitly block the cluster state update thread * on network operations. */ verifyRelocatingState(); final ReplicationTracker.PrimaryContext primaryContext = replicationTracker.startRelocationHandoff(targetAllocationId); // make sure we release all permits before we resolve the final listener final ActionListener<Void> wrappedInnerListener = ActionListener.runBefore(listener, releasable::close); final ActionListener<Void> wrappedListener = new ActionListener<>() { @Override public void onResponse(Void unused) { try { // make changes to primaryMode and relocated flag only under mutex synchronized (mutex) { verifyRelocatingState(); replicationTracker.completeRelocationHandoff(); } wrappedInnerListener.onResponse(null); } catch (Exception e) { onFailure(e); } } @Override public void onFailure(Exception e) { try { replicationTracker.abortRelocationHandoff(); } catch (final Exception inner) { e.addSuppressed(inner); } wrappedInnerListener.onFailure(e); } }; try { consumer.accept(primaryContext, wrappedListener); } catch (final Exception e) { wrappedListener.onFailure(e); } success = true; } catch (Exception e) { listener.onFailure(e); } finally { if (success == false) { releasable.close(); } } } @Override public void onFailure(Exception e) { if (e instanceof TimeoutException) { logger.warn("timed out waiting for relocation hand-off to complete"); // This is really bad as ongoing replication operations are preventing this shard from completing relocation // hand-off. // Fail primary relocation source and target shards. failShard("timed out waiting for relocation hand-off to complete", null); listener.onFailure( new IndexShardClosedException(shardId(), "timed out waiting for relocation hand-off to complete")); } else { listener.onFailure(e); } } }, 30L, TimeUnit.SECONDS); } }	if listener.onresponse(null) throws an exception in here (e.g. the recovery was cancelled) then we've already closed the releasable, but calling onfailure() in the following catch clause will close it again.
* a listener to resolve once it finished * @param listener listener to resolve once this method actions including executing {@code consumer} in the non-failure case complete * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation * @throws IllegalStateException if the relocation target is no longer part of the replication group */ public void relocated(final String targetAllocationId, final BiConsumer<ReplicationTracker.PrimaryContext, ActionListener<Void>> consumer, final ActionListener<Void> listener) throws IllegalIndexShardStateException, IllegalStateException { assert shardRouting.primary() : "only primaries can be marked as relocated: " + shardRouting; try (Releasable forceRefreshes = refreshListeners.forceRefreshes()) { indexShardOperationPermits.asyncBlockOperations(new ActionListener<>() { @Override public void onResponse(Releasable releasable) { boolean success = false; try { forceRefreshes.close(); // no shard operation permits are being held here, move state from started to relocated assert indexShardOperationPermits.getActiveOperationsCount() == OPERATIONS_BLOCKED : "in-flight operations in progress while moving shard state to relocated"; /* * We should not invoke the runnable under the mutex as the expected implementation is to handoff the primary * context via a network operation. Doing this under the mutex can implicitly block the cluster state update thread * on network operations. */ verifyRelocatingState(); final ReplicationTracker.PrimaryContext primaryContext = replicationTracker.startRelocationHandoff(targetAllocationId); // make sure we release all permits before we resolve the final listener final ActionListener<Void> wrappedInnerListener = ActionListener.runBefore(listener, releasable::close); final ActionListener<Void> wrappedListener = new ActionListener<>() { @Override public void onResponse(Void unused) { try { // make changes to primaryMode and relocated flag only under mutex synchronized (mutex) { verifyRelocatingState(); replicationTracker.completeRelocationHandoff(); } wrappedInnerListener.onResponse(null); } catch (Exception e) { onFailure(e); } } @Override public void onFailure(Exception e) { try { replicationTracker.abortRelocationHandoff(); } catch (final Exception inner) { e.addSuppressed(inner); } wrappedInnerListener.onFailure(e); } }; try { consumer.accept(primaryContext, wrappedListener); } catch (final Exception e) { wrappedListener.onFailure(e); } success = true; } catch (Exception e) { listener.onFailure(e); } finally { if (success == false) { releasable.close(); } } } @Override public void onFailure(Exception e) { if (e instanceof TimeoutException) { logger.warn("timed out waiting for relocation hand-off to complete"); // This is really bad as ongoing replication operations are preventing this shard from completing relocation // hand-off. // Fail primary relocation source and target shards. failShard("timed out waiting for relocation hand-off to complete", null); listener.onFailure( new IndexShardClosedException(shardId(), "timed out waiting for relocation hand-off to complete")); } else { listener.onFailure(e); } } }, 30L, TimeUnit.SECONDS); } }	we used to wait for 30 _minutes_ to acquire all the permits -- i don't think 30 seconds is enough in general.
@Override public void handoffPrimaryContext(ReplicationTracker.PrimaryContext primaryContext, ActionListener<Void> listener) { target.handoffPrimaryContext(primaryContext, listener); }	wondering why this doesn't use the executor like the other methods in this class.
static ClusterState moveClusterStateToNextStep(Index index, ClusterState clusterState, StepKey currentStep, StepKey nextStep, LongSupplier nowSupplier) { IndexMetaData idxMeta = clusterState.getMetaData().index(index); Settings.Builder indexSettings = moveIndexSettingsToNextStep(idxMeta.getSettings(), currentStep, nextStep, nowSupplier); ClusterState.Builder newClusterStateBuilder = newClusterStateWithIndexSettings(idxMeta.getIndex(), clusterState, indexSettings); return newClusterStateBuilder.build(); }	im curious: why does this need to change?
@Override public ClusterState execute(ClusterState currentState) { Settings indexSettings = currentState.getMetaData().index(index).getSettings(); String indexPolicySetting = LifecycleSettings.LIFECYCLE_NAME_SETTING.get(indexSettings); // policy could be updated in-between execution if (policy.equals(indexPolicySetting) == false) { throw new IllegalArgumentException("policy [" + policy + "] does not match " + LifecycleSettings.LIFECYCLE_NAME + " [" + indexPolicySetting + "]"); } if (currentStepKey.equals(IndexLifecycleRunner.getCurrentStepKey(indexSettings)) == false) { throw new IllegalArgumentException("index [" + index.getName() + "] is not on current step [" + currentStepKey + "]"); } try { policyStepsRegistry.getStep(policy, nextStepKey); } catch (IllegalStateException e) { throw new IllegalArgumentException(e.getMessage()); } return IndexLifecycleRunner.moveClusterStateToNextStep(index, currentState, currentStepKey, nextStepKey, nowSupplier); }	i'm not sure about throwing exceptions here during the normal execution, i want to think about this a bit more
private void onLastReadForInitialWrite() { if (earlyReadNodes.isEmpty() == false) { if (logger.isTraceEnabled()) { logger.trace("sending read request to [{}] for [{}] before write complete", earlyReadNodes, request.getDescription()); } readOnNodes(earlyReadNodes, true); } if (request.getAbortWrite()) { throw new BlobWriteAbortedException(); } }	i'm not sure this would reproduce the issue we fixed for gcp. since we already read through all the bytes once to compute the md5 there, we'd throw while computing the checksum and never while running the actual upload. without the fix this would have reproduced the issue easily but with the fix in place it seems we wouldn't catch it if e.g. say the gcp behavior was broken?
@Override public int hashCode() { return Objects.hash(deploymentId, allowNoMatch, expandedIds); } } public static class Response extends BaseTasksResponse implements ToXContentObject { public static final ParseField DEPLOYMENT_STATS = new ParseField("deployment_stats"); private final QueryPage<AllocationStats> stats; public Response( List<TaskOperationFailure> taskFailures, List<? extends ElasticsearchException> nodeFailures, List<AllocationStats> stats, long count ) { super(taskFailures, nodeFailures); this.stats = new QueryPage<>(stats, count, DEPLOYMENT_STATS); } public Response(StreamInput in) throws IOException { super(in); stats = new QueryPage<>(in, AllocationStats::new); } public QueryPage<AllocationStats> getStats() { return stats; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); stats.doXContentBody(builder, params); toXContentCommon(builder, params); builder.endObject(); return builder; } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); stats.writeTo(out); } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; if (super.equals(o) == false) return false; Response response = (Response) o; return Objects.equals(stats, response.stats); } @Override public int hashCode() { return Objects.hash(super.hashCode(), stats); }	this is is used by two transport actions now, i opted to move it to its own class. it is pretty big regardless and probably needed its own class.
public HasChildQueryBuilder minMaxChildren(int minChildren, int maxChildren) { if (minChildren < 0) { throw new IllegalArgumentException("[" + NAME + "] requires non-negative 'min_children' field"); } if (minChildren == 0) { deprecationLogger.deprecatedAndMaybeLog("min_children", MIN_CHILDREN_0_DEPRECATION_MESSAGE); } if (maxChildren < 0) { throw new IllegalArgumentException("[" + NAME + "] requires non-negative 'max_children' field"); } if (maxChildren < minChildren) { throw new IllegalArgumentException("[" + NAME + "] 'max_children' is less than 'min_children'"); } this.minChildren = minChildren; this.maxChildren = maxChildren; return this; }	since maxchildren == 0 is also disallowed in 8.0, that should get a warning too.
private TaskProvider<LoggedExec> createRunBwcGradleTask(Project project, String name, Action<LoggedExec> configAction) { return project.getTasks().register(name, LoggedExec.class, loggedExec -> { loggedExec.dependsOn("checkoutBwcBranch"); loggedExec.usesService(bwcTaskThrottleProvider); loggedExec.setSpoolOutput(true); loggedExec.setWorkingDir(checkoutDir.get()); loggedExec.doFirst(new Action<Task>() { @Override public void execute(Task t) { // Execution time so that the checkouts are available String compilerVersionInfoPath = minimumCompilerVersionPath(unreleasedVersionInfo.get().version); String minimumCompilerVersion = readFromFile(new File(checkoutDir.get(), compilerVersionInfoPath)); loggedExec.environment("JAVA_HOME", getJavaHome(Integer.parseInt(minimumCompilerVersion))); } }); if (Os.isFamily(Os.FAMILY_WINDOWS)) { loggedExec.executable("cmd"); loggedExec.args("/C", "call", new File(checkoutDir.get(), "gradlew").toString()); } else { loggedExec.executable(new File(checkoutDir.get(), "gradlew").toString()); } if (project.getGradle().getStartParameter().isOffline()) { loggedExec.args("--offline"); } // TODO resolve String buildCacheUrl = System.getProperty("org.elasticsearch.build.cache.url"); if (buildCacheUrl != null) { loggedExec.args("-Dorg.elasticsearch.build.cache.url=" + buildCacheUrl); } loggedExec.args("-Dbuild.snapshot=true"); loggedExec.args("-Dscan.tag.NESTED"); final LogLevel logLevel = project.getGradle().getStartParameter().getLogLevel(); List<LogLevel> nonDefaultLogLevels = Arrays.asList(LogLevel.QUIET, LogLevel.WARN, LogLevel.INFO, LogLevel.DEBUG); if (nonDefaultLogLevels.contains(logLevel)) { loggedExec.args("--" + logLevel.name().toLowerCase(Locale.ENGLISH)); } final String showStacktraceName = project.getGradle().getStartParameter().getShowStacktrace().name(); assert Arrays.asList("INTERNAL_EXCEPTIONS", "ALWAYS", "ALWAYS_FULL").contains(showStacktraceName); if (showStacktraceName.equals("ALWAYS")) { loggedExec.args("--stacktrace"); } else if (showStacktraceName.equals("ALWAYS_FULL")) { loggedExec.args("--full-stacktrace"); } if (project.getGradle().getStartParameter().isParallelProjectExecutionEnabled()) { loggedExec.args("--parallel"); } loggedExec.setStandardOutput(new IndentingOutputStream(System.out, unreleasedVersionInfo.get().version)); loggedExec.setErrorOutput(new IndentingOutputStream(System.err, unreleasedVersionInfo.get().version)); configAction.execute(loggedExec); }); }	lambdas as task actions are now deprecated. therefore we do not need a comment here as the build would fail to signal this anyhow
@Override protected void masterOperation(PutEnrichPolicyAction.Request request, ClusterState state, ActionListener<AcknowledgedResponse> listener) { if (licenseState.isAuthAllowed()) { RoleDescriptor.IndicesPrivileges privileges = RoleDescriptor.IndicesPrivileges.builder() .indices(request.getPolicy().getIndices()) .privileges("read") .build(); String username = securityContext.getUser().principal(); HasPrivilegesRequest privRequest = new HasPrivilegesRequest(); privRequest.applicationPrivileges(new RoleDescriptor.ApplicationResourcePrivileges[0]); privRequest.username(username); privRequest.clusterPrivileges(Strings.EMPTY_ARRAY); privRequest.indexPrivileges(privileges); ActionListener<HasPrivilegesResponse> wrappedListener = ActionListener.wrap( r -> { if (r.isCompleteMatch()) { putPolicy(request, listener); } else { listener.onFailure(Exceptions.authorizationError("Could not store policy because an index specified {} did not" + " exist or user {} lacks permission to access it.", request.getPolicy().getIndices(), username)); } }, listener::onFailure); client.execute(HasPrivilegesAction.INSTANCE, privRequest, wrappedListener); } else { putPolicy(request, listener); } }	maybe: unable to store policy, because no indices match with the specified index patterns [{}] i don't think we have to include the fact that the user has no privileges to read specific indices, because the user is unable to verify the existence of indices matching with the specified patterns.
public void validateMappings(Map<String, Map<String, FieldCapabilities>> fieldCapsResponse, ActionRequestValidationException validationException) { Map<String, FieldCapabilities> fieldCaps = fieldCapsResponse.get(field); if (fieldCaps != null && fieldCaps.isEmpty() == false) { fieldCaps.forEach((key, value) -> { if (RollupField.NUMERIC_FIELD_MAPPER_TYPES.contains(key) || RollupField.DATE_FIELD_MAPPER_TYPE.equals(key)) { if (value.isAggregatable() == false) { validationException.addValidationError("The field [" + field + "] must be aggregatable across all indices, " + "but is not."); } if (RollupField.DATE_FIELD_MAPPER_TYPE.equals(key) && RollupField.SUPPORTED_DATE_METRICS.containsAll(metrics) == false) { List<String> unsupportedMetrics = new ArrayList<>(metrics); unsupportedMetrics.removeAll(RollupField.SUPPORTED_DATE_METRICS); validationException.addValidationError("Only the metrics " + RollupField.SUPPORTED_DATE_METRICS.toString() + " are supported for [date] types, but unsupported metrics " + unsupportedMetrics + " supplied for field [" + field + "]"); } } else { validationException.addValidationError("The field referenced by a metric group must be a [numeric] or [date] type, " + "but found " + fieldCaps.keySet().toString() + " for field [" + field + "]"); } }); } else { validationException.addValidationError("Could not find a [numeric] or [date] field with name [" + field + "] in any of the " + "indices matching the index pattern."); } }	now that there are two cases, i wonder if we should push the isaggregatable() check out of the type conditionals (e.g. apply to everything), then have an if...if else...else for the types? that way we won't need another conditional inside the first that specializes for the date type. not sure how that would look, so the current way is fine if that ends up being messier. :) has a nice side effect that the validation error will include both a message about non-aggregatable field as well as the metric missing at the same time.
public void validateMappings(Map<String, Map<String, FieldCapabilities>> fieldCapsResponse, ActionRequestValidationException validationException) { Map<String, FieldCapabilities> fieldCaps = fieldCapsResponse.get(field); if (fieldCaps != null && fieldCaps.isEmpty() == false) { fieldCaps.forEach((key, value) -> { if (RollupField.NUMERIC_FIELD_MAPPER_TYPES.contains(key) || RollupField.DATE_FIELD_MAPPER_TYPE.equals(key)) { if (value.isAggregatable() == false) { validationException.addValidationError("The field [" + field + "] must be aggregatable across all indices, " + "but is not."); } if (RollupField.DATE_FIELD_MAPPER_TYPE.equals(key) && RollupField.SUPPORTED_DATE_METRICS.containsAll(metrics) == false) { List<String> unsupportedMetrics = new ArrayList<>(metrics); unsupportedMetrics.removeAll(RollupField.SUPPORTED_DATE_METRICS); validationException.addValidationError("Only the metrics " + RollupField.SUPPORTED_DATE_METRICS.toString() + " are supported for [date] types, but unsupported metrics " + unsupportedMetrics + " supplied for field [" + field + "]"); } } else { validationException.addValidationError("The field referenced by a metric group must be a [numeric] or [date] type, " + "but found " + fieldCaps.keySet().toString() + " for field [" + field + "]"); } }); } else { validationException.addValidationError("Could not find a [numeric] or [date] field with name [" + field + "] in any of the " + "indices matching the index pattern."); } }	super tiny nit: could we wrap the supported_date_metrics with brackets ([...])? makes it easier for the user to see the difference in message and dynamic values. ditto to below with unsupportedmetrics
protected static void addNoticeGeneration(final Project project, PluginPropertiesExtension extension) { final var licenseFile = extension.getLicenseFile(); var tasks = project.getTasks(); CopySpec bundleSpec = extension.getBundleSpec(); if (licenseFile != null) { bundleSpec.from(licenseFile.getParentFile(), s -> { s.include(licenseFile.getName()); s.rename(f -> "LICENSE.txt"); }); } final var noticeFile = extension.getNoticeFile(); if (noticeFile != null) { final var generateNotice = tasks.register("generateNotice", NoticeTask.class, noticeTask -> { noticeTask.setInputFile(noticeFile); noticeTask.source(Util.getJavaMainSourceSet(project).get().getAllJava()); }); bundleSpec.from(generateNotice); } extension.setBundleSpec(bundleSpec); }	is this bit necessary? aren't we mutating the original spec?
private static TaskProvider<Zip> createBundleTasks(final Project project, PluginPropertiesExtension extension) { final var pluginMetadata = project.file("src/main/plugin-metadata"); final var templateFile = new File(project.getBuildDir(), "templates/plugin-descriptor.properties"); // create tasks to build the properties file for this plugin final var copyPluginPropertiesTemplate = project.getTasks().register("copyPluginPropertiesTemplate", new Action<Task>() { @Override public void execute(Task task) { task.getOutputs().file(templateFile); // intentionally an Action and not a lambda to avoid issues with up-to-date check task.doLast(new Action<Task>() { @Override public void execute(Task task) { InputStream resourceTemplate = PluginBuildPlugin.class.getResourceAsStream("/" + templateFile.getName()); try { String templateText = IOUtils.toString(resourceTemplate, StandardCharsets.UTF_8.name()); FileUtils.write(templateFile, templateText, "UTF-8"); } catch (IOException e) { throw new GradleException("Unable to copy plugin properties", e); } } }); } }); final var buildProperties = project.getTasks().register("pluginProperties", Copy.class, copy -> { copy.dependsOn(copyPluginPropertiesTemplate); copy.from(templateFile); copy.into(new File(project.getBuildDir(), "generated-resources")); }); // add the plugin properties and metadata to test resources, so unit tests can // know about the plugin (used by test security code to statically initialize the plugin in unit tests) var testSourceSet = project.getExtensions().getByType(SourceSetContainer.class).getByName("test"); Map<String, Object> map = Map.of("builtBy", buildProperties); testSourceSet.getOutput().dir(map, new File(project.getBuildDir(), "generated-resources")); testSourceSet.getResources().srcDir(pluginMetadata); var bundleSpec = createBundleSpec(project, pluginMetadata, buildProperties); extension.setBundleSpec(bundleSpec); // create the actual bundle task, which zips up all the files for the plugin final var bundle = project.getTasks().register("bundlePlugin", Zip.class, zip -> zip.with(bundleSpec)); project.getTasks().named(BasePlugin.ASSEMBLE_TASK_NAME).configure(task -> task.dependsOn(bundle)); // also make the zip available as a configuration (used when depending on this project) var configuration = project.getConfigurations().create("zip"); configuration.getAttributes().attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.ZIP_TYPE); project.getArtifacts().add("zip", bundle); TaskProvider<Copy> explodedBundle = project.getTasks().register("explodedBundlePlugin", Copy.class, copy -> { copy.with(bundleSpec); copy.into(new File(project.getBuildDir(), "explodedBundle")); }); // also make the exploded bundle available as a configuration (used when depending on this project) var explodedBundleZip = project.getConfigurations().create(EXPLODED_BUNDLE_CONFIG); explodedBundleZip.setCanBeResolved(false); explodedBundleZip.setCanBeConsumed(true); explodedBundleZip.getAttributes().attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.DIRECTORY_TYPE); project.getArtifacts().add(EXPLODED_BUNDLE_CONFIG, explodedBundle); return bundle; }	should we use a sync task instead for reliability?
public void testConcurrentOperationsTryingToCreateSecurityIndexAndAlias() throws Exception { assertSecurityIndexActive(); final int processors = Runtime.getRuntime().availableProcessors(); final int numThreads = Math.min(50, scaledRandomIntBetween((processors + 1) / 2, 4 * processors)); // up to 50 threads final int maxNumRequests = 50 / numThreads; // bound to a maximum of 50 requests final int numRequests = scaledRandomIntBetween(Math.min(4, maxNumRequests), maxNumRequests); logger.info("creating users with [{}] threads, each sending [{}] requests", numThreads, numRequests); final List<ActionFuture<PutUserResponse>> futures = new CopyOnWriteArrayList<>(); final List<Exception> exceptions = new CopyOnWriteArrayList<>(); final Thread[] threads = new Thread[numThreads]; final CyclicBarrier barrier = new CyclicBarrier(threads.length); final AtomicInteger userNumber = new AtomicInteger(0); for (int i = 0; i < threads.length; i++) { threads[i] = new Thread(new AbstractRunnable() { @Override public void onFailure(Exception e) { exceptions.add(e); } @Override protected void doRun() throws Exception { final List<PutUserRequestBuilder> requests = new ArrayList<>(numRequests); final SecureString password = new SecureString("test-user-password".toCharArray()); for (int i = 0; i < numRequests; i++) { requests.add(new PutUserRequestBuilder(client()) .username("user" + userNumber.getAndIncrement()) .password(password, getFastStoredHashAlgoForTests()) .roles(randomAlphaOfLengthBetween(1, 16))); } barrier.await(10L, TimeUnit.SECONDS); for (PutUserRequestBuilder request : requests) { futures.add(request.execute()); } } }, "create_users_thread" + i); threads[i].start(); } for (Thread thread : threads) { thread.join(); } assertThat(exceptions, Matchers.empty()); assertEquals(futures.size(), numRequests * numThreads); for (ActionFuture<PutUserResponse> future : futures) { assertTrue(future.actionGet().created()); } }	we could also choose to reduce the concurrency only when the test is infipsjvm since the build stats show that all failures so far are for fips jobs.
public Decision canRemain(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { return Decision.ALWAYS; } /** * Returns a {@link Decision} whether the given shard routing can be allocated at all at this state of the * {@link RoutingAllocation}. The default is {@link Decision#ALWAYS}	i think we should reverse this, such that deciders have to opt-out, just like for canforceallocateprimary. that means that we do not inadvertently introduce a new decider that breaks replace/vacate. it should respect throttle though.
@Override public Decision canForceDuringVacate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { if (replacementFromSourceToTarget(allocation.metadata(), shardRouting.currentNodeId(), node.nodeId())) { // Allow overriding this decider during node replacement // TODO: double check for 100% disk usage with estimated sizes to avoid hitting disk limits return Decision.single(Decision.Type.YES, NAME, "overriding disk watermark limits during node replacement of [%s] with [%s]", shardRouting.currentNodeId(), node.nodeId()); } else { return Decision.NO; } }	i think the decider does not need to check if a replacement is in action, that is the callers responsibility (i.e., balancedshardsallocator).
@Override public Decision canForceDuringVacate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { if (replacementFromSourceToTarget(allocation.metadata(), shardRouting.currentNodeId(), node.nodeId())) { // Allow overriding this decider during node replacement return Decision.single(Decision.Type.YES, NAME, "overriding allocation filters during node replacement of [%s] with [%s]", shardRouting.currentNodeId(), node.nodeId()); } else { return Decision.NO; } }	also callers responsibility to only call this when a replace is ongoing between source and target.
@Override public Decision canForceDuringVacate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { if (replacementFromSourceToTarget(allocation.metadata(), shardRouting.currentNodeId(), node.nodeId())) { // Allow overriding this decider during node replacement return Decision.single(Decision.Type.YES, NAME, "overriding allocation filters during node replacement of [%s] with [%s]", shardRouting.currentNodeId(), node.nodeId()); } else { return Decision.NO; } }	a detail, perhaps we want to let the caller pass in part of the reason or append it afterwards, "node replacement of [a] with [b]"? would be nice to have a consistent message.
public XContentParserConfiguration withFiltering(Set<String> includeStrings, Set<String> excludeStrings) { return new XContentParserConfiguration( registry, deprecationHandler, restApiVersion, FilterPath.compile(includeStrings), FilterPath.compile(excludeStrings), supportDotInFieldName ); }	i like the name i used for this variable more. and i like forcing folks to configure it when configuring filtering. that way then they build the filter they have to think about it.
* @param nextFilters nextFilters is a List, used to check the inner property of name * @return true if the name equal a final node, otherwise return false */ boolean matches(String name, List<FilterPath> nextFilters) { return matches(name, nextFilters, false); } /** * Similar to {@link #matches(String, List)}	i don't think we need to keep the two argument version of this method.
* @param nextFilters nextFilters is a List, used to check the inner property of name * @param supportDotInFieldName support dot in field name or not * @return true if the name equal a final node, otherwise return false */ boolean matches(String name, List<FilterPath> nextFilters, boolean supportDotInFieldName) { if (nextFilters == null) { return false; } FilterPath termNode = termsChildren.get(name); if (termNode != null) { if (termNode.isFinalNode()) { return true; } else { nextFilters.add(termNode); } } for (FilterPath wildcardNode : wildcardChildren) { String wildcardPattern = wildcardNode.getPattern(); if (Glob.globMatch(wildcardPattern, name)) { if (wildcardNode.isFinalNode()) { return true; } else { nextFilters.add(wildcardNode); } } } if (isDoubleWildcard) { nextFilters.add(this); } // support dot in path if (supportDotInFieldName && nextFilters.isEmpty()) { // contains dot and not the first or last char int dotIndex = name.indexOf('.'); if ((dotIndex != -1) && (dotIndex != 0) && (dotIndex != name.length() - 1)) { String prefixName = name.substring(0, dotIndex); String subName = name.substring(dotIndex + 1); termNode = termsChildren.get(prefixName); if (termNode != null) { if (termNode.isFinalNode) { return true; } else { return termNode.matches(subName, nextFilters, true); } } } } return false; }	i haven't really wrapped my head around when nextfilters is empty. that's why i did mine in the caller. could you explain what's up here?
public FilterPathBasedFilter supportDotInFieldName(boolean supportDotInFieldName) { this.supportDotInFieldName = supportDotInFieldName; return this; }	i'd prefer adding another boolean parameter to the ctor.
public void testDotInFieldName() { // FilterPath match FilterPath[] filterPaths = FilterPath.compile(singleton("foo")); List<FilterPath> nextFilters = new ArrayList<>(); assertTrue(filterPaths[0].matches("foo.bar", nextFilters, true)); assertEquals(nextFilters.size(), 0); // FilterPath not match filterPaths = FilterPath.compile(singleton("bar")); assertFalse(filterPaths[0].matches("foo.bar", nextFilters, true)); assertEquals(nextFilters.size(), 0); // FilterPath equals to fieldName filterPaths = FilterPath.compile(singleton("foo.bar")); assertTrue(filterPaths[0].matches("foo.bar", nextFilters, true)); assertEquals(nextFilters.size(), 0); // FilterPath longer than fieldName filterPaths = FilterPath.compile(singleton("foo.bar.test")); assertFalse(filterPaths[0].matches("foo.bar", nextFilters, true)); assertEquals(nextFilters.size(), 1); nextFilters.clear(); // partial match filterPaths = FilterPath.compile(singleton("foo.bar.test")); assertFalse(filterPaths[0].matches("foo.bar.text", nextFilters, true)); assertEquals(nextFilters.size(), 0); }	i think it's worth taking some of the tests from [my pr](https://github.com/elastic/elasticsearch/pull/83148/files#diff-f1ab9e67867791ff2768917c0f445128aaac5886ee3c8eb15322db370b5f39e7) into this one. just to test the filtering "above" this layer.
@Override public void process(String[] row, Runnable incrementTrainingDocs, Runnable incrementTestDocs) { if (canBeUsedForTraining(row) == false) { incrementTestDocs.run(); return; } String classValue = row[dependentVariableIndex]; ClassSample sample = classSamples.get(classValue); if (sample == null) { throw new IllegalStateException("Unknown class [" + classValue + "]; expected one of " + classSamples.keySet()); } // We use ensure the target sample count is at least 1 as if the cardinality // is too low we might get a target of zero and, thus, no samples of the whole class double targetSampleCount = Math.max(1.0, samplingRatio * sample.cardinality); // The idea here is that the probability increases as the chances we have to get the target proportion // for a class decreases. double p = (targetSampleCount - sample.training) / (sample.cardinality - sample.observed); boolean isTraining = random.nextDouble() <= p; sample.observed++; if (isTraining) { sample.training++; incrementTrainingDocs.run(); } else { row[dependentVariableIndex] = DataFrameDataExtractor.NULL_VALUE; incrementTestDocs.run(); } }	is it guaranteed that sample.cardinality is at least 1? if it was 0, then targetsamplecount would be 1 but maybe that's not what we'd wanted?
protected boolean isGetLicenseTest() { String testName = getTestName(); return testName != null && (testName.contains("/get-license/") || testName.contains("\\\\\\\\get-license\\\\\\\\")); } /** * Compares the results of running two analyzers against many random * strings. The goal is to figure out if two anlayzers are "the same" by * comparing their results. This is far from perfect but should be fairly * accurate, especially for gross things like missing {@code decimal_digit}	why does this need to support \\\\\\\\ but ccr doesn't?
protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException { builder.field("type", contentType()); if (includeDefaults || fieldType().boost() != 1.0f) { builder.field("boost", fieldType().boost()); } boolean indexed = fieldType().indexOptions() != IndexOptions.NONE; boolean defaultIndexed = defaultFieldType.indexOptions() != IndexOptions.NONE; if (includeDefaults || indexed != defaultIndexed) { builder.field("index", indexed); } if (includeDefaults || fieldType().stored() != defaultFieldType.stored()) { builder.field("store", fieldType().stored()); } doXContentDocValues(builder, includeDefaults); if (includeDefaults || fieldType().storeTermVectors() != defaultFieldType.storeTermVectors()) { builder.field("term_vector", termVectorOptionsToString(fieldType())); } if (includeDefaults || fieldType().omitNorms() != defaultFieldType.omitNorms()) { builder.field("norms", fieldType().omitNorms() == false); } if (indexed && (includeDefaults || fieldType().indexOptions() != defaultFieldType.indexOptions())) { builder.field("index_options", indexOptionToString(fieldType().indexOptions())); } if (includeDefaults || fieldType().eagerGlobalOrdinals() != defaultFieldType.eagerGlobalOrdinals()) { builder.field("eager_global_ordinals", fieldType().eagerGlobalOrdinals()); } if (fieldType().similarity() != null) { builder.field("similarity", fieldType().similarity().name()); } else if (includeDefaults) { builder.field("similarity", SimilarityService.DEFAULT_SIMILARITY); } multiFields.toXContent(builder, params); copyTo.toXContent(builder, params); }	this is another tokenized() method than the one deleted above, no? i don't know enough about the details of how mappings work, but how are they different? how is this removal in the if-clause related to the rest of this change?
public void checkCompatibility(MappedFieldType other, List<String> conflicts) { checkTypeName(other); boolean indexed = indexOptions() != IndexOptions.NONE; boolean mergeWithIndexed = other.indexOptions() != IndexOptions.NONE; // TODO: should be validating if index options go "up" (but "down" is ok) if (indexed != mergeWithIndexed) { conflicts.add("mapper [" + name() + "] has different [index] values"); } if (stored() != other.stored()) { conflicts.add("mapper [" + name() + "] has different [store] values"); } if (hasDocValues() != other.hasDocValues()) { conflicts.add("mapper [" + name() + "] has different [doc_values] values"); } if (omitNorms() && !other.omitNorms()) { conflicts.add("mapper [" + name() + "] has different [norms] values, cannot change from disable to enabled"); } if (storeTermVectors() != other.storeTermVectors()) { conflicts.add("mapper [" + name() + "] has different [store_term_vector] values"); } if (storeTermVectorOffsets() != other.storeTermVectorOffsets()) { conflicts.add("mapper [" + name() + "] has different [store_term_vector_offsets] values"); } if (storeTermVectorPositions() != other.storeTermVectorPositions()) { conflicts.add("mapper [" + name() + "] has different [store_term_vector_positions] values"); } if (storeTermVectorPayloads() != other.storeTermVectorPayloads()) { conflicts.add("mapper [" + name() + "] has different [store_term_vector_payloads] values"); } // null and "default"-named index analyzers both mean the default is used if (indexAnalyzer() == null || "default".equals(indexAnalyzer().name())) { if (other.indexAnalyzer() != null && "default".equals(other.indexAnalyzer().name()) == false) { conflicts.add("mapper [" + name() + "] has different [analyzer]"); } } else if (other.indexAnalyzer() == null || "default".equals(other.indexAnalyzer().name())) { conflicts.add("mapper [" + name() + "] has different [analyzer]"); } else if (indexAnalyzer().name().equals(other.indexAnalyzer().name()) == false) { conflicts.add("mapper [" + name() + "] has different [analyzer]"); } if (Objects.equals(similarity(), other.similarity()) == false) { conflicts.add("mapper [" + name() + "] has different [similarity]"); } }	same here, this it the field type tokenized flag, no? not sure if this matters.
@Override public void refresh() { SocketAccess.doPrivilegedVoid(credentialsProvider::refresh); } } /** * Customizes {@link com.amazonaws.auth.WebIdentityTokenCredentialsProvider} * * <ul> * <li>Reads the the location of the web identity token not from AWS_WEB_IDENTITY_TOKEN_FILE, but from a symlink * in the plugin directory, so we don't need to create a hardcoded read file permission for the plugin.</li> * <li>Supports customization of the STS endpoint via a system property, so we can test it against a test fixture.</li> * <li>Supports gracefully shutting down the provider and the STS client.</li> * </ul> */ static class CustomWebIdentityTokenCredentialsProvider implements AWSCredentialsProvider { private STSAssumeRoleWithWebIdentitySessionCredentialsProvider credentialsProvider; private AWSSecurityTokenService stsClient; CustomWebIdentityTokenCredentialsProvider(Environment environment) { // Check whether the original environment variable exists. If it doesn't, // the system doesn't support AWS web identity tokens if (System.getenv(AWS_WEB_IDENTITY_ENV_VAR) == null) { return; } // Make sure that a readable symlink to the token file exists in the plugin config directory Path webIdentityTokenFileSymlink = environment.configFile().resolve("repository-s3/aws-web-identity-token-file"); if (Files.exists(webIdentityTokenFileSymlink) == false) { throw new IllegalStateException( "A Web Identity Token symlink in the config directory doesn't exist: " + webIdentityTokenFileSymlink ); } if (Files.isReadable(webIdentityTokenFileSymlink) == false) { throw new IllegalStateException( "Unable to read a Web Identity Token symlink in the config directory: " + webIdentityTokenFileSymlink ); } String roleArn = System.getenv(AWS_ROLE_ARN_ENV_VAR); String roleSessionName = System.getenv(AWS_ROLE_SESSION_NAME_ENV_VAR); if (roleArn == null || roleSessionName == null) { return; } AWSSecurityTokenServiceClientBuilder stsClientBuilder = AWSSecurityTokenServiceClient.builder(); // Just for testing String customStsEndpoint = System.getProperty("com.amazonaws.sdk.stsMetadataServiceEndpointOverride"); if (customStsEndpoint != null) { stsClientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(customStsEndpoint, null)); } stsClientBuilder.withCredentials(new AWSStaticCredentialsProvider(new AnonymousAWSCredentials())); stsClient = SocketAccess.doPrivileged(stsClientBuilder::build); credentialsProvider = new STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder( roleArn, roleSessionName, webIdentityTokenFileSymlink.toString() ).withStsClient(stsClient).build(); } boolean isActive() { return credentialsProvider != null; } @Override public AWSCredentials getCredentials() { return credentialsProvider != null ? credentialsProvider.getCredentials() : new AnonymousAWSCredentials(); } @Override public void refresh() { if (credentialsProvider != null) { credentialsProvider.refresh(); } } public void shutdown() { if (credentialsProvider != null) { credentialsProvider.close(); stsClient.shutdown(); } }	just to be sure to understand - this client is shared among all repositories or should we have one instance per repository? should we also be able to configure its s3 client configuration (timeouts, retries, max connections etc)?
@Override public void refresh() { SocketAccess.doPrivilegedVoid(credentialsProvider::refresh); } } /** * Customizes {@link com.amazonaws.auth.WebIdentityTokenCredentialsProvider} * * <ul> * <li>Reads the the location of the web identity token not from AWS_WEB_IDENTITY_TOKEN_FILE, but from a symlink * in the plugin directory, so we don't need to create a hardcoded read file permission for the plugin.</li> * <li>Supports customization of the STS endpoint via a system property, so we can test it against a test fixture.</li> * <li>Supports gracefully shutting down the provider and the STS client.</li> * </ul> */ static class CustomWebIdentityTokenCredentialsProvider implements AWSCredentialsProvider { private STSAssumeRoleWithWebIdentitySessionCredentialsProvider credentialsProvider; private AWSSecurityTokenService stsClient; CustomWebIdentityTokenCredentialsProvider(Environment environment) { // Check whether the original environment variable exists. If it doesn't, // the system doesn't support AWS web identity tokens if (System.getenv(AWS_WEB_IDENTITY_ENV_VAR) == null) { return; } // Make sure that a readable symlink to the token file exists in the plugin config directory Path webIdentityTokenFileSymlink = environment.configFile().resolve("repository-s3/aws-web-identity-token-file"); if (Files.exists(webIdentityTokenFileSymlink) == false) { throw new IllegalStateException( "A Web Identity Token symlink in the config directory doesn't exist: " + webIdentityTokenFileSymlink ); } if (Files.isReadable(webIdentityTokenFileSymlink) == false) { throw new IllegalStateException( "Unable to read a Web Identity Token symlink in the config directory: " + webIdentityTokenFileSymlink ); } String roleArn = System.getenv(AWS_ROLE_ARN_ENV_VAR); String roleSessionName = System.getenv(AWS_ROLE_SESSION_NAME_ENV_VAR); if (roleArn == null || roleSessionName == null) { return; } AWSSecurityTokenServiceClientBuilder stsClientBuilder = AWSSecurityTokenServiceClient.builder(); // Just for testing String customStsEndpoint = System.getProperty("com.amazonaws.sdk.stsMetadataServiceEndpointOverride"); if (customStsEndpoint != null) { stsClientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(customStsEndpoint, null)); } stsClientBuilder.withCredentials(new AWSStaticCredentialsProvider(new AnonymousAWSCredentials())); stsClient = SocketAccess.doPrivileged(stsClientBuilder::build); credentialsProvider = new STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder( roleArn, roleSessionName, webIdentityTokenFileSymlink.toString() ).withStsClient(stsClient).build(); } boolean isActive() { return credentialsProvider != null; } @Override public AWSCredentials getCredentials() { return credentialsProvider != null ? credentialsProvider.getCredentials() : new AnonymousAWSCredentials(); } @Override public void refresh() { if (credentialsProvider != null) { credentialsProvider.refresh(); } } public void shutdown() { if (credentialsProvider != null) { credentialsProvider.close(); stsClient.shutdown(); } }	should we close this instance in case something goes wrong when building the stsassumerolewithwebidentitysessioncredentialsprovider
@Override public void refresh() { SocketAccess.doPrivilegedVoid(credentialsProvider::refresh); } } /** * Customizes {@link com.amazonaws.auth.WebIdentityTokenCredentialsProvider} * * <ul> * <li>Reads the the location of the web identity token not from AWS_WEB_IDENTITY_TOKEN_FILE, but from a symlink * in the plugin directory, so we don't need to create a hardcoded read file permission for the plugin.</li> * <li>Supports customization of the STS endpoint via a system property, so we can test it against a test fixture.</li> * <li>Supports gracefully shutting down the provider and the STS client.</li> * </ul> */ static class CustomWebIdentityTokenCredentialsProvider implements AWSCredentialsProvider { private STSAssumeRoleWithWebIdentitySessionCredentialsProvider credentialsProvider; private AWSSecurityTokenService stsClient; CustomWebIdentityTokenCredentialsProvider(Environment environment) { // Check whether the original environment variable exists. If it doesn't, // the system doesn't support AWS web identity tokens if (System.getenv(AWS_WEB_IDENTITY_ENV_VAR) == null) { return; } // Make sure that a readable symlink to the token file exists in the plugin config directory Path webIdentityTokenFileSymlink = environment.configFile().resolve("repository-s3/aws-web-identity-token-file"); if (Files.exists(webIdentityTokenFileSymlink) == false) { throw new IllegalStateException( "A Web Identity Token symlink in the config directory doesn't exist: " + webIdentityTokenFileSymlink ); } if (Files.isReadable(webIdentityTokenFileSymlink) == false) { throw new IllegalStateException( "Unable to read a Web Identity Token symlink in the config directory: " + webIdentityTokenFileSymlink ); } String roleArn = System.getenv(AWS_ROLE_ARN_ENV_VAR); String roleSessionName = System.getenv(AWS_ROLE_SESSION_NAME_ENV_VAR); if (roleArn == null || roleSessionName == null) { return; } AWSSecurityTokenServiceClientBuilder stsClientBuilder = AWSSecurityTokenServiceClient.builder(); // Just for testing String customStsEndpoint = System.getProperty("com.amazonaws.sdk.stsMetadataServiceEndpointOverride"); if (customStsEndpoint != null) { stsClientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(customStsEndpoint, null)); } stsClientBuilder.withCredentials(new AWSStaticCredentialsProvider(new AnonymousAWSCredentials())); stsClient = SocketAccess.doPrivileged(stsClientBuilder::build); credentialsProvider = new STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder( roleArn, roleSessionName, webIdentityTokenFileSymlink.toString() ).withStsClient(stsClient).build(); } boolean isActive() { return credentialsProvider != null; } @Override public AWSCredentials getCredentials() { return credentialsProvider != null ? credentialsProvider.getCredentials() : new AnonymousAWSCredentials(); } @Override public void refresh() { if (credentialsProvider != null) { credentialsProvider.refresh(); } } public void shutdown() { if (credentialsProvider != null) { credentialsProvider.close(); stsClient.shutdown(); } }	i find this confusing, is it possible that this method gets called when credentialsprovider is null?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeStringArrayNullable(indices); indicesOptions.writeIndicesOptions(out); out.writeOptionalNamedWriteable(filter); out.writeString(timestampField); out.writeOptionalString(tiebreakerField); out.writeString(eventCategoryField); out.writeVInt(size); out.writeVInt(fetchSize); out.writeString(query); if (out.getVersion().onOrAfter(Version.V_7_15_0)) { out.writeBoolean(ccsMinimizeRoundtrips); } if (out.getVersion().onOrAfter(Version.V_8_0_0)) { // TODO: Remove after backport out.writeOptionalTimeValue(waitForCompletionTimeout); out.writeOptionalTimeValue(keepAlive); out.writeBoolean(keepOnCompletion); } if (out.getVersion().onOrAfter(Version.V_7_10_0)) { out.writeString(resultPosition); } if (out.getVersion().onOrAfter(Version.V_7_13_0)) { out.writeBoolean(fetchFields != null); if (fetchFields != null) { out.writeList(fetchFields); } out.writeMap(runtimeMappings); } }	not part of sql, but removes a leftover comment only.
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ShardFollowTask that = (ShardFollowTask) o; return Objects.equals(leaderClusterAlias, that.leaderClusterAlias) && Objects.equals(followShardId, that.followShardId) && Objects.equals(leaderShardId, that.leaderShardId) && maxChunkSize == that.maxChunkSize && numConcurrentChunks == that.numConcurrentChunks && processorMaxTranslogBytes == that.processorMaxTranslogBytes && Objects.equals(headers, that.headers); }	do you have equals/hashcode unit tests? if not can you add some and ensure the headers aspect is appropriately tested?
@SuppressWarnings("try") static Client wrapClient(Client client, ShardFollowTask shardFollowTask) { if (shardFollowTask.getHeaders().isEmpty()) { return client; } else { final ThreadContext threadContext = client.threadPool().getThreadContext(); Map<String, String> filteredHeaders = shardFollowTask.getHeaders().entrySet().stream() .filter(e -> ShardFollowTask.HEADER_FILTERS.contains(e.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); return new FilterClient(client) { @Override protected <Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>> void doExecute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener) { final Supplier<ThreadContext.StoredContext> supplier = threadContext.newRestorableContext(false); try (ThreadContext.StoredContext ignore = stashWithHeaders(threadContext, filteredHeaders)) { super.doExecute(action, request, new ContextPreservingActionListener<>(supplier, listener)); } } }; } }	this is really difficult to read. how about: diff diff --git a/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/shardfollowtasksexecutor.java b/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/shardfollowtasksexecutor.java index 37f248040ee..084b37eaa12 100644 --- a/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/shardfollowtasksexecutor.java +++ b/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/shardfollowtasksexecutor.java @@ -385,9 +385,11 @@ public class shardfollowtasksexecutor extends persistenttasksexecutor<shardfollo .collect(collectors.tomap(map.entry::getkey, map.entry::getvalue)); return new filterclient(client) { @override - protected <request extends actionrequest, response extends actionresponse, requestbuilder extends - actionrequestbuilder<request, response, requestbuilder>> void doexecute(action<request, response, - requestbuilder> action, request request, actionlistener<response> listener) { + protected < + request extends actionrequest, + response extends actionresponse, + requestbuilder extends actionrequestbuilder<request, response, requestbuilder>> + void doexecute(action<request, response, requestbuilder> action, request request, actionlistener<response> listener) { final supplier<threadcontext.storedcontext> supplier = threadcontext.newrestorablecontext(false); try (threadcontext.storedcontext ignore = stashwithheaders(threadcontext, filteredheaders)) { super.doexecute(action, request, new contextpreservingactionlistener<>(supplier, listener));
public void merge(Mapping mapping, boolean updateAllTypes) { try (ReleasableLock lock = mappingWriteLock.acquire()) { // first simulate to only check for conflicts MergeResult mergeResult = new MergeResult(true, updateAllTypes); this.mapping.merge(mapping, mergeResult); if (mergeResult.hasConflicts()) { throw new MergeMappingException(mergeResult.buildConflicts()); } // then check conflicts with other types // if there is a single type then we are implicitly updating all types mapperService.checkMappersCompatibility(mapping, updateAllTypes || mapperService.types().size() == 1); // finally apply the mapping update for real mergeResult = new MergeResult(false, updateAllTypes); assert mergeResult.hasConflicts() == false; // we already simulated this.mapping.merge(mapping, mergeResult); addMappers(mergeResult.getNewObjectMappers(), mergeResult.getNewFieldMappers(), updateAllTypes); refreshSource(); } }	why do we need to merge this again since we are still holding on to the lock? i don't necessarily understand why this is helping us as well but that might just be because i don't know this code very well.
private void checkUniqueness(Collection<ObjectMapper> objectMappers, Collection<FieldMapper> fieldMappers) { Map<String, Mapper> alreadySeen = new HashMap<>(); for (ObjectMapper mapper : objectMappers) { final Mapper removed = alreadySeen.put(mapper.fullPath(), mapper); if (removed != null) { throw new IllegalArgumentException("Two fields are defined for path [" + mapper.fullPath() + "]: " + Arrays.asList(mapper, removed)); } } for (FieldMapper mapper : fieldMappers) { final Mapper removed = alreadySeen.put(mapper.name(), mapper); if (removed != null && (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_2_0_0) || mapper != removed)) { // we need the 'removed != mapper' condition because some metadata mappers used to be registered both as a metadata mapper and // as a sub mapper of the root object mapper throw new IllegalArgumentException("Two fields are defined for path [" + mapper.name() + "]: " + Arrays.asList(mapper, removed)); } } }	can we fix this when upgrading the mappings, so we don't have to keep this leniency in the check?
public void testTurnOffTranslogRetentionAfterUpgraded() throws Exception { if (isRunningAgainstOldCluster()) { createIndex(index, Settings.builder() .put(IndexMetadata.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1) .put(IndexMetadata.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 1) .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true).build()); ensureGreen(index); int numDocs = randomIntBetween(10, 100); for (int i = 0; i < numDocs; i++) { indexDocument(Integer.toString(randomIntBetween(1, 100))); if (rarely()) { flush(index, randomBoolean()); } } } else { ensureGreen(index); flush(index, true); assertEmptyTranslog(index); ensurePeerRecoveryRetentionLeasesRenewedAndSynced(index); } }	why have we combined two tests into one, where before they were using assumetrue / assumefalse?
public void testSystemIndexResolutionWhenAllowed() { Settings settings = Settings.builder().build(); Metadata.Builder mdBuilder = Metadata.builder() .put(indexBuilder(".ml-meta", settings).state(State.OPEN).system(true)) .put(indexBuilder(".watches", settings).state(State.OPEN).system(true)) .put(indexBuilder(".ml-stuff", settings).state(State.OPEN).system(true)) .put(indexBuilder("some-other-index").state(State.OPEN)); ClusterState state = ClusterState.builder(new ClusterName("_name")).metadata(mdBuilder).build(); // Single name { SearchRequest request = new SearchRequest(".ml-meta"); List<String> indexNames = Arrays .stream(indexNameExpressionResolver.concreteIndices(state, request)) .map(i -> i.getName()) .collect(Collectors.toList()); assertThat(indexNames, containsInAnyOrder(".ml-meta")); } // Wildcard that should match multiple { SearchRequest request = new SearchRequest(".ml-*"); List<String> indexNames = Arrays .stream(indexNameExpressionResolver.concreteIndices(state, request)) .map(i -> i.getName()) .collect(Collectors.toList()); assertThat(indexNames, containsInAnyOrder(".ml-meta", ".ml-stuff")); } // Wildcard that just matches one { SearchRequest request = new SearchRequest(".w*"); List<String> indexNames = Arrays .stream(indexNameExpressionResolver.concreteIndices(state, request)) .map(i -> i.getName()) .collect(Collectors.toList()); assertThat(indexNames, containsInAnyOrder(".watches")); } // Full wildcard { SearchRequest request = new SearchRequest(randomFrom("*", "_all")); List<String> indexNames = Arrays .stream(indexNameExpressionResolver.concreteIndices(state, request)) .map(i -> i.getName()) .collect(Collectors.toList()); assertThat(indexNames, containsInAnyOrder("some-other-index", ".ml-stuff", ".ml-meta", ".watches")); } }	what do you think about defining a helper method, to cut down on the ceremony here? something like: java private void getconcreteindexnames(clusterstate state, searchrequest request) { return list<string> indexnames = arrays .stream(indexnameexpressionresolver.concreteindices(state, request)) .map(i -> i.getname()) .collect(collectors.tolist()); }
* @param allowSystemIndexWarnings Whether deprecation warnings for system index access should be allowed/expected. * @throws IOException Yes */ @SuppressWarnings("unchecked") public static void assertLegacyTemplateMatchesIndexMappings(RestClient client, String templateName, String indexName, boolean notAnErrorIfIndexDoesNotExist, Set<String> exceptions, boolean allowSystemIndexWarnings) throws IOException { Request getTemplate = new Request("GET", "_template/" + templateName); Response templateResponse = client.performRequest(getTemplate); assertEquals("missing template [" + templateName + "]", 200, templateResponse.getStatusLine().getStatusCode()); Map<String, Object> templateMappings = (Map<String, Object>) XContentMapValues.extractValue( ESRestTestCase.entityAsMap(templateResponse), templateName, "mappings"); assertNotNull(templateMappings); Request getIndexMapping = new Request("GET", indexName + "/_mapping"); if (allowSystemIndexWarnings) { final String systemIndexWarning = "this request accesses system indices: [" + indexName + "], but in a future major version, " + "direct access to system indices will be prevented by default"; getIndexMapping.setOptions(ESRestTestCase.expectVersionSpecificWarnings(v -> { v.current(systemIndexWarning); v.compatible(systemIndexWarning); })); } Response indexMappingResponse; try { indexMappingResponse = client.performRequest(getIndexMapping); } catch (ResponseException e) { if (e.getResponse().getStatusLine().getStatusCode() == 404 && notAnErrorIfIndexDoesNotExist) { return; } else { throw e; } } assertEquals("error getting mappings for index [" + indexName + "]", 200, indexMappingResponse.getStatusLine().getStatusCode()); Map<String, Object> indexMappings = (Map<String, Object>) XContentMapValues.extractValue( ESRestTestCase.entityAsMap(indexMappingResponse), indexName, "mappings"); assertNotNull(indexMappings); // ignore the _meta field indexMappings.remove("_meta"); templateMappings.remove("_meta"); // We cannot do a simple comparison of mappings e.g // Objects.equals(indexMappings, templateMappings) because some // templates use strings for the boolean values - "true" and "false" // which are automatically converted to Booleans causing the equality // to fail. boolean mappingsAreTheSame = true; // flatten the map of maps Map<String, Object> flatTemplateMap = flattenMap(templateMappings); Map<String, Object> flatIndexMap = flattenMap(indexMappings); SortedSet<String> keysInTemplateMissingFromIndex = new TreeSet<>(flatTemplateMap.keySet()); keysInTemplateMissingFromIndex.removeAll(flatIndexMap.keySet()); SortedSet<String> keysInIndexMissingFromTemplate = new TreeSet<>(flatIndexMap.keySet()); keysInIndexMissingFromTemplate.removeAll(flatTemplateMap.keySet()); // In the case of object fields the 'type: object' mapping is set by default. // If this does not explicitly appear in the template it is not an error // as ES has added the default to the index mappings keysInIndexMissingFromTemplate.removeIf(key -> key.endsWith("type") && "object".equals(flatIndexMap.get(key))); // Remove the exceptions keysInIndexMissingFromTemplate.removeAll(exceptions); StringBuilder errorMesssage = new StringBuilder("Error the template mappings [") .append(templateName) .append("] and index mappings [") .append(indexName) .append("] are not the same") .append(System.lineSeparator()); if (keysInTemplateMissingFromIndex.isEmpty() == false) { mappingsAreTheSame = false; errorMesssage.append("Keys in the template missing from the index mapping: ") .append(keysInTemplateMissingFromIndex) .append(System.lineSeparator()); } if (keysInIndexMissingFromTemplate.isEmpty() == false) { mappingsAreTheSame = false; errorMesssage.append("Keys in the index missing from the template mapping: ") .append(keysInIndexMissingFromTemplate) .append(System.lineSeparator()); } // find values that are different for the same key Set<String> commonKeys = new TreeSet<>(flatIndexMap.keySet()); commonKeys.retainAll(flatTemplateMap.keySet()); for (String key : commonKeys) { Object template = flatTemplateMap.get(key); Object index = flatIndexMap.get(key); if (Objects.equals(template, index) == false) { // Both maybe be booleans but different representations if (areBooleanObjectsAndEqual(index, template)) { continue; } mappingsAreTheSame = false; errorMesssage.append("Values for key [").append(key).append("] are different").append(System.lineSeparator()); errorMesssage.append(" template value [").append(template).append("] ").append(template.getClass().getSimpleName()) .append(System.lineSeparator()); errorMesssage.append(" index value [").append(index).append("] ").append(index.getClass().getSimpleName()) .append(System.lineSeparator()); } } if (mappingsAreTheSame == false) { fail(errorMesssage.toString()); } }	i can't see this new parameter being used. am i missing something?
* @param key the key for the setting * @param defaultValue the default value for this setting * @param properties properties properties for this setting like scope, filtering... * @param <T> the generics type parameter reflecting the actual type of the enum * @return the setting object */ public static <T extends Enum<T>> Setting<T> enumSetting(Class<T> clazz, String key, T defaultValue, Property... properties) { return new Setting<>(key, defaultValue.toString(), e -> Enum.valueOf(clazz, e.toUpperCase(Locale.ROOT)), properties); }	nit: looks like properties is included twice in the param description
public void testThrowsIllegalArgumentExceptionOnInvalidEnumSetting() { Setting setting = Setting.enumSetting(TestEnumSetting.class, "foo", TestEnumSetting.ON, Property.Filtered); final Settings settings = Settings.builder().put("foo", "bar").build(); final IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> setting.get(settings)); assertNull(e.getCause()); }	this seems like an odd test, to check that the exception has no cause. can we instead check part of the message to be sure it in the iae we expect?
public void testNgramHighlightingWithBrokenPositions() throws IOException { assertAcked(prepareCreate("test") .addMapping("test", jsonBuilder() .startObject() .startObject("test") .startObject("properties") .startObject("name") .field("type", "text") .startObject("fields") .startObject("autocomplete") .field("type", "text") .field("analyzer", "autocomplete") .field("search_analyzer", "search_autocomplete") .field("term_vector", "with_positions_offsets") .endObject() .endObject() .endObject() .endObject() .endObject() .endObject()) .setSettings(Settings.builder() .put(indexSettings()) .put(IndexSettings.MAX_NGRAM_DIFF_SETTING.getKey(), 19) .put("analysis.tokenizer.autocomplete.max_gram", 20) .put("analysis.tokenizer.autocomplete.min_gram", 1) .put("analysis.tokenizer.autocomplete.token_chars", "letter,digit") .put("analysis.tokenizer.autocomplete.type", "ngram") .put("analysis.filter.wordDelimiter.type", "word_delimiter") .putList("analysis.filter.wordDelimiter.type_table", "& => ALPHANUM", "| => ALPHANUM", "! => ALPHANUM", "? => ALPHANUM", ". => ALPHANUM", "- => ALPHANUM", "# => ALPHANUM", "% => ALPHANUM", "+ => ALPHANUM", ", => ALPHANUM", "~ => ALPHANUM", ": => ALPHANUM", "/ => ALPHANUM", "^ => ALPHANUM", "$ => ALPHANUM", "@ => ALPHANUM", ") => ALPHANUM", "( => ALPHANUM", "] => ALPHANUM", "[ => ALPHANUM", "} => ALPHANUM", "{ => ALPHANUM") .put("analysis.filter.wordDelimiter.type.split_on_numerics", false) .put("analysis.filter.wordDelimiter.generate_word_parts", true) .put("analysis.filter.wordDelimiter.generate_number_parts", false) .put("analysis.filter.wordDelimiter.catenate_words", true) .put("analysis.filter.wordDelimiter.catenate_numbers", true) .put("analysis.filter.wordDelimiter.catenate_all", false) .put("analysis.analyzer.autocomplete.tokenizer", "autocomplete") .putList("analysis.analyzer.autocomplete.filter", "lowercase", "wordDelimiter") .put("analysis.analyzer.search_autocomplete.tokenizer", "whitespace") .putList("analysis.analyzer.search_autocomplete.filter", "lowercase", "wordDelimiter"))); client().prepareIndex("test", "test", "1") .setSource("name", "ARCOTEL Hotels Deutschland").get(); refresh(); SearchResponse search = client().prepareSearch("test").setTypes("test") .setQuery(matchQuery("name.autocomplete", "deut tel").operator(Operator.OR)) .highlighter(new HighlightBuilder().field("name.autocomplete")).get(); assertHighlight(search, 0, "name.autocomplete", 0, equalTo("ARCO<em>TEL</em> Ho<em>tel</em>s <em>Deut</em>schland")); }	this is about the tokenizer, not the filter, but i think we also shouldn't use the camel case version of that one anymore.
public static IndexMetadata legacyFromXContent(XContentParser parser) throws IOException { if (parser.currentToken() == null) { // fresh parser? move to the first token parser.nextToken(); } if (parser.currentToken() == XContentParser.Token.START_OBJECT) { // on a start object move to next token parser.nextToken(); } XContentParserUtils.ensureExpectedToken(XContentParser.Token.FIELD_NAME, parser.currentToken(), parser); Builder builder = new Builder(parser.currentName()); String currentFieldName = null; XContentParser.Token token = parser.nextToken(); XContentParserUtils.ensureExpectedToken(XContentParser.Token.START_OBJECT, token, parser); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token == XContentParser.Token.START_OBJECT) { if ("settings".equals(currentFieldName)) { Settings settings = Settings.fromXContent(parser); if (SETTING_INDEX_VERSION_CREATED.get(settings).onOrAfter(Version.CURRENT.minimumIndexCompatibilityVersion())) { throw new IllegalStateException("this method should only be used to parse older index metadata versions " + "but got " + SETTING_INDEX_VERSION_CREATED.get(settings)); } builder.settings(settings); } else if ("mappings".equals(currentFieldName)) { // don't try to parse these for now parser.skipChildren(); } else { // assume it's custom index metadata parser.skipChildren(); } } else if (token == XContentParser.Token.START_ARRAY) { if ("mappings".equals(currentFieldName)) { // don't try to parse these for now parser.skipChildren(); } else { parser.skipChildren(); } } else if (token.isValue()) { if ("state".equals(currentFieldName)) { builder.state(State.fromString(parser.text())); } else if ("version".equals(currentFieldName)) { builder.version(parser.longValue()); } else if ("mapping_version".equals(currentFieldName)) { builder.mappingVersion(parser.longValue()); } else if ("settings_version".equals(currentFieldName)) { builder.settingsVersion(parser.longValue()); } else if ("routing_num_shards".equals(currentFieldName)) { builder.setRoutingNumShards(parser.intValue()); } else { // unknown, ignore } } else { throw new IllegalArgumentException("Unexpected token " + token); } } XContentParserUtils.ensureExpectedToken(XContentParser.Token.END_OBJECT, parser.nextToken(), parser); IndexMetadata indexMetadata = builder.build(); assert indexMetadata.getCreationVersion().before(Version.CURRENT.minimumIndexCompatibilityVersion()); return indexMetadata; } } /** * Return the {@link Version}	nit: could use org.elasticsearch.common.xcontent.xcontentparserutils#throwunknowntoken for nicer exceptions.
*/ public RandomScoreFunctionBuilder seed(int seed) { this.seed = seed; return this; } /** * seed variant taking a long value. * @see {@link #seed(int)}	this javadoc is wrong
* @param parser the outer parser * @return The 3D array of doubles * @throws IOException */ public static double[][][] parseArrayOfArraysOfArraysOfDoubles(String fieldName, XContentParser parser) throws IOException { if (parser.currentToken() != XContentParser.Token.START_ARRAY) { throw new IllegalArgumentException("unexpected token [" + parser.currentToken() + "] for [" + fieldName + "]"); } List<List<List<Double>>> values = new ArrayList<>(); while(parser.nextToken() != XContentParser.Token.END_ARRAY) { if (parser.currentToken() != XContentParser.Token.START_ARRAY) { throw new IllegalArgumentException("unexpected token [" + parser.currentToken() + "] for [" + fieldName + "]"); } List<List<Double>> innerList = new ArrayList<>(); while(parser.nextToken() != XContentParser.Token.END_ARRAY) { if (parser.currentToken() != XContentParser.Token.START_ARRAY) { throw new IllegalArgumentException("unexpected token [" + parser.currentToken() + "] for [" + fieldName + "]"); } if (parser.currentToken() != XContentParser.Token.START_ARRAY) { throw new IllegalArgumentException("unexpected token [" + parser.currentToken() + "] for [" + fieldName + "]"); } List<Double> innerInner = new ArrayList<>(); while (parser.nextToken() != XContentParser.Token.END_ARRAY) { if (parser.currentToken().isValue() == false) { throw new IllegalStateException("expected non-null value but got [" + parser.currentToken() + "] " + "for [" + fieldName + "]"); } innerInner.add(parser.doubleValue()); } innerList.add(innerInner); } values.add(innerList); } double [][][] val = new double[values.size()][values.get(0).size()][values.get(0).get(0).size()]; for (int i = 0; i < val.length; i++) { for (int j = 0; j < val[0].length; j++) { double[] doubles = values.get(i).get(j).stream().mapToDouble(d -> d).toArray(); System.arraycopy(doubles, 0, val[i][j], 0, doubles.length); } } return val; }	should we check we have doubles specifically? otherwise, parser.doublevalue() below might throw an ioexception.
static BytesReference jsonRequest(TokenizationResult tokenization, int padToken, String requestId) throws IOException { XContentBuilder builder = XContentFactory.jsonBuilder(); builder.startObject(); builder.field(REQUEST_ID, requestId); NlpTask.RequestBuilder.writePaddedTokens(TOKENS, tokenization, padToken, (tokens, i) -> tokens.getTokenIds()[i], builder); NlpTask.RequestBuilder.writePaddedTokens(ARG1, tokenization, padToken, (tokens, i) -> 1, builder); int numRequests = tokenization.getTokenizations().size(); NlpTask.RequestBuilder.writeNonPaddedIds(ARG2, numRequests, tokenization.getLongestSequenceLength(), i -> 0, builder); NlpTask.RequestBuilder.writeNonPaddedIds(ARG3, numRequests, tokenization.getLongestSequenceLength(), i -> i, builder); builder.endObject(); // BytesReference.bytes closes the builder return BytesReference.bytes(builder); }	rename to batchsize to avoid overloading request term?
InferenceResults processResult(TokenizationResult tokenization, PyTorchResult pyTorchResult) { if (tokenization.getTokenizations().isEmpty() || tokenization.getTokenizations().get(0).getTokens().isEmpty()) { return new FillMaskResults(Collections.emptyList()); } int maskTokenIndex = tokenization.getTokenizations().get(0).getTokens().indexOf(BertTokenizer.MASK_TOKEN); double[] normalizedScores = NlpHelpers.convertToProbabilitiesBySoftMax(pyTorchResult.getInferenceResult()[0][maskTokenIndex]); NlpHelpers.ScoreAndIndex[] scoreAndIndices = NlpHelpers.topK(NUM_RESULTS, normalizedScores); List<FillMaskResults.Prediction> results = new ArrayList<>(NUM_RESULTS); for (NlpHelpers.ScoreAndIndex scoreAndIndex : scoreAndIndices) { String predictedToken = tokenization.getFromVocab(scoreAndIndex.index); String sequence = tokenization.getTokenizations().get(0).getInput().replace(BertTokenizer.MASK_TOKEN, predictedToken); results.add(new FillMaskResults.Prediction(predictedToken, scoreAndIndex.score, sequence)); } return new FillMaskResults(results); }	right. it took me a bit to understand why we only process the first array here. i think it is because, currently, c++ writes back a separate result for each input in a batch. correct me if i'm wrong. i think we should try to capture this somehow. at least a comment would do. might also be worth adding an assertion.
@Override public void parse(ParseContext context) throws IOException { AbstractGeometryFieldType fieldType = (AbstractGeometryFieldType)fieldType(); @SuppressWarnings("unchecked") Indexer<Parsed, Processed> geometryIndexer = fieldType.geometryIndexer(); @SuppressWarnings("unchecked") Parser<Parsed> geometryParser = fieldType.geometryParser(); try { Processed shape = context.parseExternalValue(geometryIndexer.processedClass()); if (shape == null) { Parsed geometry = geometryParser.parse(context.parser(), this); if (geometry == null) { return; } shape = geometryIndexer.prepareForIndexing(geometry); } List<IndexableField> fields = new ArrayList<>(geometryIndexer.indexShape(context, shape)); final byte[] scratch = new byte[7 * Integer.BYTES]; if (fieldType().hasDocValues()) { // doc values are generated from the indexed fields. ShapeField.DecodedTriangle[] triangles = new ShapeField.DecodedTriangle[fields.size()]; for (int i =0; i < fields.size(); i++) { BytesRef bytesRef = fields.get(i).binaryValue(); assert bytesRef.length == 7 * Integer.BYTES; System.arraycopy(bytesRef.bytes, bytesRef.offset, scratch, 0, 7 * Integer.BYTES); ShapeField.decodeTriangle(scratch, triangles[i] = new ShapeField.DecodedTriangle()); } geometryIndexer.indexDocValueField(context, triangles, new CentroidCalculator((Geometry) shape)); } createFieldNamesField(context, fields); for (IndexableField field : fields) { context.doc().add(field); } } catch (Exception e) { if (ignoreMalformed.value() == false) { throw new MapperParsingException("failed to parse field [{}] of type [{}]", e, fieldType().name(), fieldType().typeName()); } context.addIgnoredField(fieldType().name()); } }	nit suggestion for (int i = 0; i < fields.size(); i++) {
@Override public boolean advanceExact(int docId) throws IOException { if (geoValues.advanceExact(docId)) { resize(geoValues.docValueCount()); for (int i = 0; i < docValueCount(); ++i) { MultiGeoValues.GeoValue target = geoValues.nextValue(); values[i] = tiler.encode(target.lon(), target.lat(), precision); } sort(); return true; } else { return false; } } } /** Sorted numeric doc values for precision 0 */ protected static class AllCellValues extends AbstractSortingNumericDocValues { private MultiGeoValues geoValues; protected AllCellValues(MultiGeoValues geoValues, GeoGridTiler tiler) { this.geoValues = geoValues; resize(1); values[0] = tiler.encode(0, 0, 0); } // for testing protected long[] getValues() { return values; } @Override public boolean advanceExact(int docId) throws IOException { resize(1); return geoValues.advanceExact(docId); }	do we plan to handle this before merging or can it wait?
public void testUnfollow() { final long settingsVersion = randomNonNegativeLong(); IndexMetaData.Builder followerIndex = IndexMetaData.builder("follow_index") .settings(settings(Version.CURRENT).put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true)) .settingsVersion(settingsVersion) .numberOfShards(1) .numberOfReplicas(0) .state(IndexMetaData.State.CLOSE) .putCustom(Ccr.CCR_CUSTOM_METADATA_KEY, new HashMap<>()); ClusterState current = ClusterState.builder(new ClusterName("cluster_name")) .metaData(MetaData.builder() .put(followerIndex) .build()) .build(); ClusterState result = TransportUnfollowAction.unfollow("follow_index", current); IndexMetaData resultIMD = result.metaData().index("follow_index"); assertThat(resultIMD.getSettings().get(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey()), nullValue()); assertThat(resultIMD.getCustomData(Ccr.CCR_CUSTOM_METADATA_KEY), nullValue()); assertThat(resultIMD.getSettingsVersion(), equalTo(settingsVersion+ 1)); }	nit on the formatting here (whitespace in front of the + please).
@Override public Query fuzzyQuery(Object value, Fuzziness fuzziness, int prefixLength, int maxExpansions, boolean transpositions) { throw new UnsupportedOperationException("[fuzzy] queries are not currently supported on keyed " + "[" + CONTENT_TYPE + "] fields."); }	i don't know what is your plan on this but it should be possible to allow this type of query by adding the field prefix in the fuzzy query and change the prefixlength to be prefixlength + field.length + 1 ?
@Override public String toString() { return functionName() + "(" + field().toString() + " AS " + to().sqlName() + ")#" + id(); }	isn't super.name + "as " + to().sqlname() enough? what's the last character in super.name() that needs removing? lgtm otherwise.
*/ @Deprecated public static XContentType xContentType(BytesReference bytes) { BytesRef br = bytes.toBytesRef(); return XContentFactory.xContentType(br.bytes, br.offset, br.length); }	this is in xcontenthelper rather than directly on xcontentparser because we want to use bytesreference here, which is in server due to its lucene dependency :(
public static BytesReference childBytes(XContentParser parser) throws IOException { if (parser.currentToken() == null) { parser.nextToken(); } XContentBuilder builder = XContentBuilder.builder(parser.contentType().xContent()); int depth = 0; do { switch (parser.nextToken()) { case START_OBJECT: depth++; builder.startObject(); break; case END_OBJECT: depth--; builder.endObject(); break; case START_ARRAY: builder.startArray(); break; case END_ARRAY: builder.endArray(); break; case FIELD_NAME: builder.field(parser.currentName()); break; case VALUE_STRING: builder.value(parser.text()); break; case VALUE_NUMBER: builder.value(parser.numberValue()); break; case VALUE_BOOLEAN: builder.value(parser.booleanValue()); break; case VALUE_EMBEDDED_OBJECT: builder.value(parser.binaryValue()); break; case VALUE_NULL: builder.nullValue(); break; default: throw new XContentParseException(parser.getTokenLocation(), "Unknown token type [" + parser.currentToken() + "]"); } } while (depth > 0); return BytesReference.bytes(builder); }	i'm not sure that this is correct, and i have a todo in the corresponding test because i'm not sure under what circumstances we actually get an embedded object in xcontent.
@Override protected void masterOperation(Request request, ClusterState clusterState, ActionListener<Response> listener) throws Exception { if (!licenseState.isDataFrameAllowed()) { listener.onFailure(LicenseUtils.newComplianceException(XPackField.DATA_FRAME)); return; } XPackPlugin.checkReadyForXPackCustomMetadata(clusterState); String jobId = request.getConfig().getId(); // quick check whether a job has already been created under that name if (PersistentTasksCustomMetaData.getTaskWithId(clusterState, jobId) != null) { listener.onFailure(new ResourceAlreadyExistsException( DataFrameMessages.getMessage(DataFrameMessages.REST_PUT_DATA_FRAME_JOB_EXISTS, jobId))); return; } // create the job, note the non-state creating steps are done first, so we minimize the chance to end up with orphaned state // job validation JobValidator jobCreator = new JobValidator(request.getConfig(), client); jobCreator.validate(ActionListener.wrap(validationResult -> { // deduce target mappings jobCreator.deduceMappings(ActionListener.wrap(mappings -> { // create the destination index DataframeIndex.createDestinationIndex(client, request.getConfig(), mappings, ActionListener.wrap(createIndexResult -> { DataFrameJob job = createDataFrameJob(jobId, threadPool); // create the job configuration and store it in the internal index dataFrameJobConfigManager.putJobConfiguration(request.getConfig(), false, ActionListener.wrap(r -> { // finally start the persistent task persistentTasksService.sendStartRequest(job.getId(), DataFrameJob.NAME, job, ActionListener.wrap(persistentTask -> { listener.onResponse(new PutDataFrameJobAction.Response(true)); }, startPersistentTaskException -> { // delete the otherwise orphaned job configuration, for now we do not delete the destination index dataFrameJobConfigManager.deleteJobConfiguration(jobId, ActionListener.wrap(r2 -> { logger.debug("Deleted data frame job [{}] configuration from data frame configuration index", jobId); listener.onFailure( new RuntimeException( DataFrameMessages.getMessage( DataFrameMessages.REST_PUT_DATA_FRAME_FAILED_TO_START_PERSISTENT_TASK, r2), startPersistentTaskException)); }, deleteJobFromIndexException -> { logger.error("Failed to cleanup orphaned data frame job [{}] configuration", jobId); listener.onFailure( new RuntimeException( DataFrameMessages.getMessage( DataFrameMessages.REST_PUT_DATA_FRAME_FAILED_TO_START_PERSISTENT_TASK, false), startPersistentTaskException)); })); })); }, pubJobIntoIndexException -> { if (pubJobIntoIndexException instanceof VersionConflictEngineException) { // the job already exists although we checked before, can happen if requests come in simultaneously listener.onFailure(new ResourceAlreadyExistsException(DataFrameMessages .getMessage(DataFrameMessages.REST_PUT_DATA_FRAME_JOB_EXISTS, jobId))); } else { listener.onFailure(new RuntimeException(DataFrameMessages.REST_PUT_DATA_FRAME_FAILED_PERSIST_JOB_CONFIGURATION, pubJobIntoIndexException)); } })); }, createDestinationIndexException -> { listener.onFailure(new RuntimeException(DataFrameMessages.REST_PUT_DATA_FRAME_FAILED_TO_CREATE_TARGET_INDEX, createDestinationIndexException)); })); }, deduceTargetMappingsException -> { listener.onFailure(new RuntimeException(DataFrameMessages.REST_PUT_DATA_FRAME_FAILED_TO_DEDUCE_TARGET_MAPPINGS, deduceTargetMappingsException)); })); }, validationException -> { listener.onFailure(new RuntimeException(DataFrameMessages.REST_PUT_DATA_FRAME_FAILED_TO_VALIDATE_DATA_FRAME_CONFIGURATION, validationException)); })); }	we don't log or return these exception anywhere. do we at least want to log it?
@Override public synchronized void onCancelled() { logger.info( "Received cancellation request for data frame job [" + job.getId() + "], state: [" + indexer.getState() + "]"); if (indexer.abort()) { // there is no background job running, we can shutdown safely shutdown(); } }	i am not sure how this latch is ever counted down. maybe you should use a latchedactionlistener?
@Override public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; final Scroll scroll = scroll(); if (source != null && source.trackTotalHits() == false && scroll != null) { validationException = addValidationError("disabling [track_total_hits] is not allowed in a scroll context", validationException); } if (source != null && source.from() > 0 && scroll != null) { validationException = addValidationError("using [from] is not allowed in a scroll context", validationException); } if (requestCache != null && requestCache && scroll != null) { validationException = addValidationError("[request_cache] cannot be used in a scroll context", validationException); } if (source != null && source.size() == 0 && scroll != null) { validationException = addValidationError("[size] cannot be [0] in a scroll context", validationException); } if (source != null && source.rescores() != null && !source.rescores().isEmpty() && scroll != null) { validationException = addValidationError("using [rescore] is not allowed in a scroll context", validationException); } return validationException; }	nit: we tend to prefer source.rescores().isempty() == false in our codebase
*/ OnDiskState loadOnDiskState(boolean checkClean) throws IOException { OnDiskState onDiskState = OnDiskState.NO_ON_DISK_STATE; final Path indexPath = dataPath.resolve(METADATA_DIRECTORY_NAME); if (Files.exists(indexPath)) { try (Directory directory = createDirectory(indexPath)) { if (checkClean) { try (BytesStreamOutput outputStream = new BytesStreamOutput()) { final boolean isClean; try (PrintStream printStream = new PrintStream(outputStream, true, StandardCharsets.UTF_8); CheckIndex checkIndex = new CheckIndex(directory)) { checkIndex.setInfoStream(printStream); checkIndex.setChecksumsOnly(true); isClean = checkIndex.checkIndex().clean; } if (isClean == false) { if (logger.isErrorEnabled()) { outputStream.bytes().utf8ToString().lines().forEach(l -> logger.error("checkIndex: {}", l)); } throw new IllegalStateException("metadata index at [" + dataPath + "] has been changed by an external force after it was last written by Elasticsearch and is now unreadable"); } } } try (DirectoryReader directoryReader = DirectoryReader.open(directory)) { onDiskState = loadOnDiskState(dataPath, directoryReader); if (nodeId.equals(onDiskState.nodeId) == false) { throw new IllegalStateException("unexpected node ID in metadata, found [" + onDiskState.nodeId + "] in [" + dataPath + "] but expected [" + nodeId + "]"); } } } catch (IndexNotFoundException e) { logger.debug(new ParameterizedMessage("no on-disk state at {}", indexPath), e); } } return onDiskState; }	we often refer to this as either cluster state or global state. i wonder if we could call it global state metadata index, just to be sure this message is interpreted correctly by operators?
*/ OnDiskState loadOnDiskState(boolean checkClean) throws IOException { OnDiskState onDiskState = OnDiskState.NO_ON_DISK_STATE; final Path indexPath = dataPath.resolve(METADATA_DIRECTORY_NAME); if (Files.exists(indexPath)) { try (Directory directory = createDirectory(indexPath)) { if (checkClean) { try (BytesStreamOutput outputStream = new BytesStreamOutput()) { final boolean isClean; try (PrintStream printStream = new PrintStream(outputStream, true, StandardCharsets.UTF_8); CheckIndex checkIndex = new CheckIndex(directory)) { checkIndex.setInfoStream(printStream); checkIndex.setChecksumsOnly(true); isClean = checkIndex.checkIndex().clean; } if (isClean == false) { if (logger.isErrorEnabled()) { outputStream.bytes().utf8ToString().lines().forEach(l -> logger.error("checkIndex: {}", l)); } throw new IllegalStateException("metadata index at [" + dataPath + "] has been changed by an external force after it was last written by Elasticsearch and is now unreadable"); } } } try (DirectoryReader directoryReader = DirectoryReader.open(directory)) { onDiskState = loadOnDiskState(dataPath, directoryReader); if (nodeId.equals(onDiskState.nodeId) == false) { throw new IllegalStateException("unexpected node ID in metadata, found [" + onDiskState.nodeId + "] in [" + dataPath + "] but expected [" + nodeId + "]"); } } } catch (IndexNotFoundException e) { logger.debug(new ParameterizedMessage("no on-disk state at {}", indexPath), e); } } return onDiskState; }	same comment on metadata as above, perhaps global state metadata?
private void verify(SortedNumericDocValues values, int maxDoc) { for (long missingValue : new long[] { 0, randomLong() }) { for (MultiValueMode mode : MultiValueMode.values()) { if (skipModes.contains(mode)) { continue; } final NumericDocValues selected = mode.select(values, missingValue); for (int i = 0; i < maxDoc; ++i) { final long actual = selected.get(i); long expected; values.setDocument(i); int numValues = values.count(); if (numValues == 0) { expected = missingValue; } else { expected = mode.startLong(); for (int j = 0; j < numValues; ++j) { expected = mode.apply(expected, values.valueAt(j)); } expected = mode.reduce(expected, numValues); } assertEquals(mode.toString() + " docId=" + i, expected, actual); } } } }	can we have tests for the new modes added somehow?
* @param keepAlive the extended time to live for the search context */ public SearchRequestBuilder setSearchContext(String searchContextId, TimeValue keepAlive) { sourceBuilder().pointInTimeBuilder(new SearchSourceBuilder.PointInTimeBuilder(searchContextId).setKeepAlive(keepAlive)); return this; }	should this be renamed to setpointintime or setpointintimebuilder ?
public static PointInTimeBuilder fromXContent(XContentParser parser) throws IOException { final XContentParams params = PARSER.parse(parser, null); if (params.id == null) { throw new IllegalArgumentException("point int time id is not provided"); } return new PointInTimeBuilder(params.id).setKeepAlive(params.keepAlive); }	can you add javadoc
@Override public void addRefreshListener(RefreshListener listener) { requireNonNull(listener, "listener cannot be null"); Translog.Location listenerLocation = requireNonNull(listener.location(), "listener's location cannot be null"); Translog.Location lastRefresh = lastRefreshedLocation; if (lastRefresh != null && lastRefresh.compareTo(listenerLocation) >= 0) { // Location already visible, just call the listener listener.refreshed(false); return; } if (refreshListenersEstimatedSize < engineConfig.getIndexSettings().getMaxRefreshListeners()) { // We have a free slot so register the listener refreshListeners.add(listener); refreshListenersEstimatedSize++; return; } /* * No free slot so force a refresh and call the listener in this thread. Do so outside of the synchronized block so any other * attempts to add a listener can continue. */ refresh("too_many_listeners"); listener.refreshed(true); }	can we just use a non-concurrent datastructure and make this synchronized. maybe we can just use a simple arraylist with preallocated space and be done with it? i don't think we need this estimated size here we can just make this simple and move into a synced block?
public void testTooManyRefreshListeners() throws Exception { try (Store store = createStore(); Engine engine = createEngine(defaultSettings, store, createTempDir(), NoMergePolicy.INSTANCE)) { ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null); Engine.Index index = new Engine.Index(newUid("1"), doc); engine.index(index); // Fill the listener slots List<DummyRefreshListener> nonForcedListeners = new ArrayList<>(INDEX_SETTINGS.getMaxRefreshListeners()); for (int i = 0; i < defaultSettings.getMaxRefreshListeners(); i++) { DummyRefreshListener listener = new DummyRefreshListener(index.getTranslogLocation()); nonForcedListeners.add(listener); engine.addRefreshListener(listener); } // We shouldn't have called any of them for (DummyRefreshListener listener : nonForcedListeners) { assertNull("Called listener too early!", listener.forcedRefresh.get()); } // Add one more listener which should cause a refresh. In this thread, no less. DummyRefreshListener forcingListener = new DummyRefreshListener(index.getTranslogLocation()); engine.addRefreshListener(forcingListener); assertTrue("Forced listener wasn't forced?", forcingListener.forcedRefresh.get()); // That forces all the listeners through. On the listener thread pool so give them some time with assertBusy. for (DummyRefreshListener listener : nonForcedListeners) { assertBusy( () -> assertEquals("Expected listener called with unforced refresh!", Boolean.FALSE, listener.forcedRefresh.get())); } } }	50 is too much :) 10 is ok?
public void testTooManyRefreshListeners() throws Exception { try (Store store = createStore(); Engine engine = createEngine(defaultSettings, store, createTempDir(), NoMergePolicy.INSTANCE)) { ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null); Engine.Index index = new Engine.Index(newUid("1"), doc); engine.index(index); // Fill the listener slots List<DummyRefreshListener> nonForcedListeners = new ArrayList<>(INDEX_SETTINGS.getMaxRefreshListeners()); for (int i = 0; i < defaultSettings.getMaxRefreshListeners(); i++) { DummyRefreshListener listener = new DummyRefreshListener(index.getTranslogLocation()); nonForcedListeners.add(listener); engine.addRefreshListener(listener); } // We shouldn't have called any of them for (DummyRefreshListener listener : nonForcedListeners) { assertNull("Called listener too early!", listener.forcedRefresh.get()); } // Add one more listener which should cause a refresh. In this thread, no less. DummyRefreshListener forcingListener = new DummyRefreshListener(index.getTranslogLocation()); engine.addRefreshListener(forcingListener); assertTrue("Forced listener wasn't forced?", forcingListener.forcedRefresh.get()); // That forces all the listeners through. On the listener thread pool so give them some time with assertBusy. for (DummyRefreshListener listener : nonForcedListeners) { assertBusy( () -> assertEquals("Expected listener called with unforced refresh!", Boolean.FALSE, listener.forcedRefresh.get())); } } }	please don't do this. express the runtime in anything else but not time. use operations or something like this. time is going to suck very much.
public void testTooManyRefreshListeners() throws Exception { try (Store store = createStore(); Engine engine = createEngine(defaultSettings, store, createTempDir(), NoMergePolicy.INSTANCE)) { ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null); Engine.Index index = new Engine.Index(newUid("1"), doc); engine.index(index); // Fill the listener slots List<DummyRefreshListener> nonForcedListeners = new ArrayList<>(INDEX_SETTINGS.getMaxRefreshListeners()); for (int i = 0; i < defaultSettings.getMaxRefreshListeners(); i++) { DummyRefreshListener listener = new DummyRefreshListener(index.getTranslogLocation()); nonForcedListeners.add(listener); engine.addRefreshListener(listener); } // We shouldn't have called any of them for (DummyRefreshListener listener : nonForcedListeners) { assertNull("Called listener too early!", listener.forcedRefresh.get()); } // Add one more listener which should cause a refresh. In this thread, no less. DummyRefreshListener forcingListener = new DummyRefreshListener(index.getTranslogLocation()); engine.addRefreshListener(forcingListener); assertTrue("Forced listener wasn't forced?", forcingListener.forcedRefresh.get()); // That forces all the listeners through. On the listener thread pool so give them some time with assertBusy. for (DummyRefreshListener listener : nonForcedListeners) { assertBusy( () -> assertEquals("Expected listener called with unforced refresh!", Boolean.FALSE, listener.forcedRefresh.get())); } } }	don't sleep here please use something like a latch or so that you can reset to resting and use operations
public void testTooManyRefreshListeners() throws Exception { try (Store store = createStore(); Engine engine = createEngine(defaultSettings, store, createTempDir(), NoMergePolicy.INSTANCE)) { ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null); Engine.Index index = new Engine.Index(newUid("1"), doc); engine.index(index); // Fill the listener slots List<DummyRefreshListener> nonForcedListeners = new ArrayList<>(INDEX_SETTINGS.getMaxRefreshListeners()); for (int i = 0; i < defaultSettings.getMaxRefreshListeners(); i++) { DummyRefreshListener listener = new DummyRefreshListener(index.getTranslogLocation()); nonForcedListeners.add(listener); engine.addRefreshListener(listener); } // We shouldn't have called any of them for (DummyRefreshListener listener : nonForcedListeners) { assertNull("Called listener too early!", listener.forcedRefresh.get()); } // Add one more listener which should cause a refresh. In this thread, no less. DummyRefreshListener forcingListener = new DummyRefreshListener(index.getTranslogLocation()); engine.addRefreshListener(forcingListener); assertTrue("Forced listener wasn't forced?", forcingListener.forcedRefresh.get()); // That forces all the listeners through. On the listener thread pool so give them some time with assertBusy. for (DummyRefreshListener listener : nonForcedListeners) { assertBusy( () -> assertEquals("Expected listener called with unforced refresh!", Boolean.FALSE, listener.forcedRefresh.get())); } } }	same here - don't sleep
@Override public Map<String, DirectoryFactory> getDirectoryFactories() { return Map.of(SearchableSnapshotRepository.SNAPSHOT_DIRECTORY_FACTORY_KEY, SearchableSnapshotRepository.newDirectoryFactory(repositoriesService::get, cacheService::get, System::nanoTime)); }	using system::nanotime since we need finer resolution than threadpool::relativetimeinnanos offers.
public static NodeExplanation calculateNodeExplanation(ShardRouting shard, IndexMetaData indexMetaData, DiscoveryNode node, Decision nodeDecision, Float nodeWeight, IndicesShardStoresResponse.StoreStatus storeStatus, String assignedNodeId, Set<String> activeAllocationIds) { final ClusterAllocationExplanation.FinalDecision finalDecision; final ClusterAllocationExplanation.StoreCopy storeCopy; final String finalExplanation; if (storeStatus == null) { // No copies of the data storeCopy = ClusterAllocationExplanation.StoreCopy.NONE; } else { final Throwable storeErr = storeStatus.getStoreException(); if (storeErr != null) { if (ExceptionsHelper.unwrapCause(storeErr) instanceof CorruptIndexException) { storeCopy = ClusterAllocationExplanation.StoreCopy.CORRUPT; } else { storeCopy = ClusterAllocationExplanation.StoreCopy.IO_ERROR; } } else if (activeAllocationIds.isEmpty()) { // The ids are only empty if dealing with a legacy index // TODO: fetch the shard state versions and display here? storeCopy = ClusterAllocationExplanation.StoreCopy.UNKNOWN; } else if (activeAllocationIds.contains(storeStatus.getAllocationId())) { storeCopy = ClusterAllocationExplanation.StoreCopy.AVAILABLE; } else { // Otherwise, this is a stale copy of the data (allocation ids don't match) storeCopy = ClusterAllocationExplanation.StoreCopy.STALE; } } if (node.getId().equals(assignedNodeId)) { finalDecision = ClusterAllocationExplanation.FinalDecision.ALREADY_ASSIGNED; finalExplanation = "the shard is already assigned to this node"; } else if (shard.primary() && shard.unassigned() && storeCopy == ClusterAllocationExplanation.StoreCopy.STALE) { finalExplanation = "the copy of the shard is stale, allocation ids do not match"; finalDecision = ClusterAllocationExplanation.FinalDecision.NO; } else if (shard.primary() && shard.unassigned() && storeCopy == ClusterAllocationExplanation.StoreCopy.CORRUPT) { finalExplanation = "the copy of the shard is corrupt"; finalDecision = ClusterAllocationExplanation.FinalDecision.NO; } else if (shard.primary() && shard.unassigned() && storeCopy == ClusterAllocationExplanation.StoreCopy.IO_ERROR) { finalExplanation = "the copy of the shard cannot be read"; finalDecision = ClusterAllocationExplanation.FinalDecision.NO; } else { if (nodeDecision.type() == Decision.Type.NO) { finalDecision = ClusterAllocationExplanation.FinalDecision.NO; finalExplanation = "the shard cannot be assigned because one or more allocation decider returns a 'NO' decision"; } else { finalDecision = ClusterAllocationExplanation.FinalDecision.YES; if (storeCopy == ClusterAllocationExplanation.StoreCopy.AVAILABLE) { finalExplanation = "the shard can be assigned and the node contains a valid copy of the shard data"; } else { finalExplanation = "the shard can be assigned"; } } } return new NodeExplanation(node, nodeDecision, nodeWeight, storeStatus, finalDecision, finalExplanation, storeCopy); } /** * For the given {@code ShardRouting}, return the explanation of the allocation for that shard on all nodes. If {@code * includeYesDecisions}	my (hopefully) last corrections here (in this case i'm correcting my earlier self ): - we should also add allocatedpostindexcreate() to be more in line with primaryshardallocator. this means that this condition and the subsequent two become shard.primary() && shard.unassigned() && shard.allocatedpostindexcreate(indexmetadata) && storecopy == clusterallocationexplanation.storecopy.stale - we should also handle storecopy.none in the same way. in case of assigning an unassigned primary, having no shard data is also a no.
@Override public LogicalPlan visitEventFilter(EventFilterContext ctx) { Source source = source(ctx); Expression condition = expression(ctx.expression()); if (ctx.event != null) { Source eventSource = source(ctx.event); String eventName = visitIdentifier(ctx.event); Literal eventValue = new Literal(eventSource, eventName, DataTypes.KEYWORD); UnresolvedAttribute eventField = new UnresolvedAttribute(eventSource, params.fieldEventCategory()); Expression eventMatch = new Equals(eventSource, eventField, eventValue, params.zoneId()); condition = new And(source, eventMatch, condition); } Filter filter = new Filter(source, RELATION, condition); // add implicit sorting - when pipes are added, this would better sit there (as a default pipe) Order order = new Order(source, fieldTimestamp(), Order.OrderDirection.ASC, Order.NullsPosition.FIRST); OrderBy orderBy = new OrderBy(source, filter, singletonList(order)); return orderBy; }	maybe extract this to a static method so that the string is also more "static".
public void testRelocatedClosedIndexIssue() throws Exception { final String indexName = "closed-index"; final List<String> dataNodes = internalCluster().startDataOnlyNodes(2); // allocate shard to first data node createIndex(indexName, Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put("index.routing.allocation.include._name", dataNodes.get(0)) .build()); indexRandom(randomBoolean(), randomBoolean(), randomBoolean(), IntStream.range(0, randomIntBetween(0, 50)) .mapToObj(n -> client().prepareIndex(indexName, "_doc").setSource("num", n)).collect(toList())); assertAcked(client().admin().indices().prepareClose(indexName)); // move single shard to second node client().admin().indices().prepareUpdateSettings(indexName).setSettings(Settings.builder() .put("index.routing.allocation.include._name", dataNodes.get(1))).get(); ensureGreen(indexName); internalCluster().fullRestart(); assertIndexIsClosed(indexName); ensureGreen(indexName); }	don't forget the waitforactiveshards when backporting :) (i always forgot it)
public void testScriptFieldUsingSource() throws Exception { createIndex("test"); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet(); client().prepareIndex("test", "type1", "1") .setSource(jsonBuilder().startObject() .startObject("obj1").field("test", "something").endObject() .startObject("obj2").startArray("arr2").value("arr_value1").value("arr_value2").endArray().endObject() .startArray("arr3").startObject().field("arr3_field1", "arr3_value1").endObject().endArray() .endObject()) .execute().actionGet(); client().admin().indices().refresh(refreshRequest()).actionGet(); SearchResponse response = client().prepareSearch().setQuery(matchAllQuery()).addScriptField("s_obj1", new Script("_source.obj1")) .addScriptField("s_obj1_test", new Script("_source.obj1.test")).addScriptField("s_obj2", new Script("_source.obj2")) .addScriptField("s_obj2_arr2", new Script("_source.obj2.arr2")).addScriptField("s_arr3", new Script("_source.arr3")) .execute().actionGet(); assertThat("Failures " + Arrays.toString(response.getShardFailures()), response.getShardFailures().length, equalTo(0)); assertThat(response.getHits().getAt(0).field("s_obj1_test").value().toString(), equalTo("something")); Map<String, Object> sObj1 = response.getHits().getAt(0).field("s_obj1").value(); assertThat(sObj1.get("test").toString(), equalTo("something")); assertThat(response.getHits().getAt(0).field("s_obj1_test").value().toString(), equalTo("something")); Map<String, Object> sObj2 = response.getHits().getAt(0).field("s_obj2").value(); List<?> sObj2Arr2 = (List<?>) sObj2.get("arr2"); assertThat(sObj2Arr2.size(), equalTo(2)); assertThat(sObj2Arr2.get(0).toString(), equalTo("arr_value1")); assertThat(sObj2Arr2.get(1).toString(), equalTo("arr_value2")); sObj2Arr2 = response.getHits().getAt(0).field("s_obj2_arr2").values(); assertThat(sObj2Arr2.size(), equalTo(2)); assertThat(sObj2Arr2.get(0).toString(), equalTo("arr_value1")); assertThat(sObj2Arr2.get(1).toString(), equalTo("arr_value2")); List<?> sObj2Arr3 = response.getHits().getAt(0).field("s_arr3").values(); assertThat(((Map<?, ?>) sObj2Arr3.get(0)).get("arr3_field1").toString(), equalTo("arr3_value1")); }	you can skip these two and just create the index. it ought to just automatically create.
public void testScriptFieldUsingSource() throws Exception { createIndex("test"); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet(); client().prepareIndex("test", "type1", "1") .setSource(jsonBuilder().startObject() .startObject("obj1").field("test", "something").endObject() .startObject("obj2").startArray("arr2").value("arr_value1").value("arr_value2").endArray().endObject() .startArray("arr3").startObject().field("arr3_field1", "arr3_value1").endObject().endArray() .endObject()) .execute().actionGet(); client().admin().indices().refresh(refreshRequest()).actionGet(); SearchResponse response = client().prepareSearch().setQuery(matchAllQuery()).addScriptField("s_obj1", new Script("_source.obj1")) .addScriptField("s_obj1_test", new Script("_source.obj1.test")).addScriptField("s_obj2", new Script("_source.obj2")) .addScriptField("s_obj2_arr2", new Script("_source.obj2.arr2")).addScriptField("s_arr3", new Script("_source.arr3")) .execute().actionGet(); assertThat("Failures " + Arrays.toString(response.getShardFailures()), response.getShardFailures().length, equalTo(0)); assertThat(response.getHits().getAt(0).field("s_obj1_test").value().toString(), equalTo("something")); Map<String, Object> sObj1 = response.getHits().getAt(0).field("s_obj1").value(); assertThat(sObj1.get("test").toString(), equalTo("something")); assertThat(response.getHits().getAt(0).field("s_obj1_test").value().toString(), equalTo("something")); Map<String, Object> sObj2 = response.getHits().getAt(0).field("s_obj2").value(); List<?> sObj2Arr2 = (List<?>) sObj2.get("arr2"); assertThat(sObj2Arr2.size(), equalTo(2)); assertThat(sObj2Arr2.get(0).toString(), equalTo("arr_value1")); assertThat(sObj2Arr2.get(1).toString(), equalTo("arr_value2")); sObj2Arr2 = response.getHits().getAt(0).field("s_obj2_arr2").values(); assertThat(sObj2Arr2.size(), equalTo(2)); assertThat(sObj2Arr2.get(0).toString(), equalTo("arr_value1")); assertThat(sObj2Arr2.get(1).toString(), equalTo("arr_value2")); List<?> sObj2Arr3 = response.getHits().getAt(0).field("s_arr3").values(); assertThat(((Map<?, ?>) sObj2Arr3.get(0)).get("arr3_field1").toString(), equalTo("arr3_value1")); }	since you don't care about the body of the source maybe use something like setsource("foo", "bar").
public void testScriptFieldsForNullReturn() throws Exception { createIndex("test"); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet(); client().prepareIndex("test", "type1", "1") .setSource(jsonBuilder().startObject().startObject("obj1").field("test", "something").endObject()) .execute().actionGet(); client().admin().indices().refresh(refreshRequest()).actionGet(); SearchResponse response = client().prepareSearch().setQuery(matchAllQuery()) .addScriptField("test_script_1", new Script("return null")) .execute().actionGet(); assertThat("Failures " + Arrays.toString(response.getShardFailures()), response.getShardFailures().length, equalTo(0)); SearchHitField fieldObj = response.getHits().getAt(0).field("test_script_1"); assertThat(fieldObj, notNullValue()); List<?> fieldValues = fieldObj.values(); assertThat(fieldValues.size(), equalTo(1)); assertThat(fieldValues.get(0), nullValue()); }	i'd set refresh to true on the index request instead of using a separate refresh.
public void testScriptFieldsForNullReturn() throws Exception { createIndex("test"); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet(); client().prepareIndex("test", "type1", "1") .setSource(jsonBuilder().startObject().startObject("obj1").field("test", "something").endObject()) .execute().actionGet(); client().admin().indices().refresh(refreshRequest()).actionGet(); SearchResponse response = client().prepareSearch().setQuery(matchAllQuery()) .addScriptField("test_script_1", new Script("return null")) .execute().actionGet(); assertThat("Failures " + Arrays.toString(response.getShardFailures()), response.getShardFailures().length, equalTo(0)); SearchHitField fieldObj = response.getHits().getAt(0).field("test_script_1"); assertThat(fieldObj, notNullValue()); List<?> fieldValues = fieldObj.values(); assertThat(fieldValues.size(), equalTo(1)); assertThat(fieldValues.get(0), nullValue()); }	it is more common to just call .get() now. this is a pretty old test!
public void testScriptFieldsForNullReturn() throws Exception { createIndex("test"); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet(); client().prepareIndex("test", "type1", "1") .setSource(jsonBuilder().startObject().startObject("obj1").field("test", "something").endObject()) .execute().actionGet(); client().admin().indices().refresh(refreshRequest()).actionGet(); SearchResponse response = client().prepareSearch().setQuery(matchAllQuery()) .addScriptField("test_script_1", new Script("return null")) .execute().actionGet(); assertThat("Failures " + Arrays.toString(response.getShardFailures()), response.getShardFailures().length, equalTo(0)); SearchHitField fieldObj = response.getHits().getAt(0).field("test_script_1"); assertThat(fieldObj, notNullValue()); List<?> fieldValues = fieldObj.values(); assertThat(fieldValues.size(), equalTo(1)); assertThat(fieldValues.get(0), nullValue()); }	assertnofailures is more common in newer tests and much shorter.
public void testScriptFieldsForNullReturn() throws Exception { createIndex("test"); client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet(); client().prepareIndex("test", "type1", "1") .setSource(jsonBuilder().startObject().startObject("obj1").field("test", "something").endObject()) .execute().actionGet(); client().admin().indices().refresh(refreshRequest()).actionGet(); SearchResponse response = client().prepareSearch().setQuery(matchAllQuery()) .addScriptField("test_script_1", new Script("return null")) .execute().actionGet(); assertThat("Failures " + Arrays.toString(response.getShardFailures()), response.getShardFailures().length, equalTo(0)); SearchHitField fieldObj = response.getHits().getAt(0).field("test_script_1"); assertThat(fieldObj, notNullValue()); List<?> fieldValues = fieldObj.values(); assertThat(fieldValues.size(), equalTo(1)); assertThat(fieldValues.get(0), nullValue()); }	i like hassize(1) for this kind of thing because it makes a nicer error message.
private void innerRefresh(String tokenDocId, Authentication userAuth, ActionListener<Tuple<UserToken, String>> listener, AtomicInteger attemptCount) { if (attemptCount.getAndIncrement() > MAX_RETRY_ATTEMPTS) { logger.warn("Failed to refresh token for doc [{}] after [{}] attempts", tokenDocId, attemptCount.get()); listener.onFailure(invalidGrantException("could not refresh the requested token")); } else { Consumer<Exception> onFailure = ex -> listener.onFailure(traceLog("refresh token", tokenDocId, ex)); GetRequest getRequest = client.prepareGet(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE, tokenDocId).request(); executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, getRequest, ActionListener.<GetResponse>wrap(response -> { if (response.isExists()) { final Map<String, Object> source = response.getSource(); final Optional<ElasticsearchSecurityException> invalidSource = checkTokenDocForRefresh(source, userAuth); if (invalidSource.isPresent()) { onFailure.accept(invalidSource.get()); } else { if (eligibleForMultiRefresh(source)) { final Map<String, Object> refreshTokenSrc = (Map<String, Object>) source.get("refresh_token"); final String supersedingTokenDocId = (String) refreshTokenSrc.get("superseded_by"); logger.debug("Token document [{}] was recently refreshed, attempting to reuse [{}] for returning an " + "access token and refresh token", tokenDocId, supersedingTokenDocId); GetRequest supersedingTokenGetRequest = client.prepareGet(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE, supersedingTokenDocId).request(); executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, supersedingTokenGetRequest, ActionListener.<GetResponse>wrap(supersedingTokenResponse -> { if (supersedingTokenResponse.isExists()) { final Map<String, Object> supersedingTokenSource = supersedingTokenResponse.getSource(); final Map<String, Object> supersedingUserTokenSource = (Map<String, Object>) ((Map<String, Object>) supersedingTokenSource.get("access_token")).get("user_token"); final Map<String, Object> supersedingRefreshTokenSrc = (Map<String, Object>) supersedingTokenSource.get("refresh_token"); final String supersedingRefreshTokenValue = (String) supersedingRefreshTokenSrc.get("token"); reIssueTokens(supersedingUserTokenSource, supersedingRefreshTokenValue, listener); } else { logger.info("could not find token document [{}] for refresh", supersedingTokenGetRequest); onFailure.accept(invalidGrantException("could not refresh the requested token")); } }, e -> { logger.info("could not find token document [{}] for refresh", supersedingTokenGetRequest); onFailure.accept(invalidGrantException("could not refresh the requested token")); }), client::get); } else { final Map<String, Object> userTokenSource = (Map<String, Object>) ((Map<String, Object>) source.get("access_token")).get("user_token"); final String authString = (String) userTokenSource.get("authentication"); final Integer version = (Integer) userTokenSource.get("version"); final Map<String, Object> metadata = (Map<String, Object>) userTokenSource.get("metadata"); Version authVersion = Version.fromId(version); try (StreamInput in = StreamInput.wrap(Base64.getDecoder().decode(authString))) { in.setVersion(authVersion); Authentication authentication = new Authentication(in); final String newUserTokenId = UUIDs.randomBase64UUID(); final Instant refreshTime = clock.instant(); Map<String, Object> updateMap = new HashMap<>(); updateMap.put("refreshed", true); updateMap.put("refresh_time", refreshTime.toEpochMilli()); updateMap.put("superseded_by", getTokenDocumentId(newUserTokenId)); UpdateRequestBuilder updateRequest = client.prepareUpdate(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE, tokenDocId) .setDoc("refresh_token", updateMap) .setRefreshPolicy(RefreshPolicy.WAIT_UNTIL); updateRequest.setIfSeqNo(response.getSeqNo()); updateRequest.setIfPrimaryTerm(response.getPrimaryTerm()); executeAsyncWithOrigin(client.threadPool().getThreadContext(), SECURITY_ORIGIN, updateRequest.request(), ActionListener.<UpdateResponse>wrap( updateResponse -> createUserToken(newUserTokenId, authentication, userAuth, listener, metadata, true), e -> { Throwable cause = ExceptionsHelper.unwrapCause(e); if (cause instanceof VersionConflictEngineException || isShardNotAvailableException(e)) { innerRefresh(tokenDocId, userAuth, listener, attemptCount); } else { onFailure.accept(e); } }), client::update); } } } } else { logger.info("could not find token document [{}] for refresh", tokenDocId); onFailure.accept(invalidGrantException("could not refresh the requested token")); } }, e -> { if (isShardNotAvailableException(e)) { innerRefresh(tokenDocId, userAuth, listener, attemptCount); } else { listener.onFailure(e); } }), client::get); } } /** * Performs checks on the retrieved source and returns an {@link Optional}	i guess we should add an error log here.
public void testRefreshingMultipleTimesFails() throws Exception { Client client = client().filterWithHeader(Collections.singletonMap("Authorization", UsernamePasswordToken.basicAuthHeaderValue(SecuritySettingsSource.TEST_USER_NAME, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING))); SecurityClient securityClient = new SecurityClient(client); CreateTokenResponse createTokenResponse = securityClient.prepareCreateToken() .setGrantType("password") .setUsername(SecuritySettingsSource.TEST_USER_NAME) .setPassword(new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray())) .get(); assertNotNull(createTokenResponse.getRefreshToken()); CreateTokenResponse refreshResponse = securityClient.prepareRefreshToken(createTokenResponse.getRefreshToken()).get(); assertNotNull(refreshResponse); // We now have two documents, the original(now refreshed) token doc and the new one with the new access doc AtomicReference<String> docId = new AtomicReference<>(); assertBusy(() -> { SearchResponse searchResponse = client.prepareSearch(SecurityIndexManager.SECURITY_INDEX_NAME) .setSource(SearchSourceBuilder.searchSource() .query(QueryBuilders.boolQuery() .must(QueryBuilders.termQuery("doc_type", "token")) .must(QueryBuilders.termQuery("refresh_token.refreshed", "true")))) .setSize(1) .setTerminateAfter(1) .get(); assertThat(searchResponse.getHits().getTotalHits().value, equalTo(1L)); docId.set(searchResponse.getHits().getAt(0).getId()); }); // hack doc to modify the refresh time to 10 seconds ago so that we don't hit the lenient refresh case Instant refreshed = Instant.now(); Instant aWhileAgo = refreshed.minus(10L, ChronoUnit.SECONDS); assertTrue(Instant.now().isAfter(aWhileAgo)); client.prepareUpdate(SecurityIndexManager.SECURITY_INDEX_NAME, "doc", docId.get()) .setDoc("refresh_token", Collections.singletonMap("refresh_time", aWhileAgo.toEpochMilli())) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) .get(); ElasticsearchSecurityException e = expectThrows(ElasticsearchSecurityException.class, () -> securityClient.prepareRefreshToken(createTokenResponse.getRefreshToken()).get()); assertEquals("invalid_grant", e.getMessage()); assertEquals(RestStatus.BAD_REQUEST, e.status()); assertEquals("token has already been refreshed", e.getHeader("error_description").get(0)); }	let's verify the update response here as that might help with debugging in case of test failure.
public void testRefreshingMultipleTimesWithinWindowSucceeds() throws Exception { Client client = client().filterWithHeader(Collections.singletonMap("Authorization", UsernamePasswordToken.basicAuthHeaderValue(SecuritySettingsSource.TEST_USER_NAME, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING))); SecurityClient securityClient = new SecurityClient(client); CreateTokenResponse createTokenResponse = securityClient.prepareCreateToken() .setGrantType("password") .setUsername(SecuritySettingsSource.TEST_USER_NAME) .setPassword(new SecureString(SecuritySettingsSourceField.TEST_PASSWORD.toCharArray())) .get(); assertNotNull(createTokenResponse.getRefreshToken()); CreateTokenResponse refreshResponse = securityClient.prepareRefreshToken(createTokenResponse.getRefreshToken()).get(); assertNotNull(refreshResponse); assertNotNull(refreshResponse.getRefreshToken()); assertNotEquals(refreshResponse.getRefreshToken(), createTokenResponse.getRefreshToken()); assertNotEquals(refreshResponse.getTokenString(), createTokenResponse.getTokenString()); assertNoTimeout(client().filterWithHeader(Collections.singletonMap("Authorization", "Bearer " + refreshResponse.getTokenString())) .admin().cluster().prepareHealth().get()); CreateTokenResponse secondRefreshResponse = securityClient.prepareRefreshToken(createTokenResponse.getRefreshToken()).get(); assertNotNull(secondRefreshResponse); assertNotNull(secondRefreshResponse.getRefreshToken()); assertThat(secondRefreshResponse.getRefreshToken(), equalTo(refreshResponse.getRefreshToken())); assertThat(secondRefreshResponse.getTokenString(), equalTo(refreshResponse.getTokenString())); assertNoTimeout( client().filterWithHeader(Collections.singletonMap("Authorization", "Bearer " + secondRefreshResponse.getTokenString())) .admin().cluster().prepareHealth().get()); }	i think we should fire a small number of requests in parallel and then check the responses have the same token string.
public static GetResult fromXContentEmbedded(XContentParser parser, String index, String type, String id) throws IOException { XContentParser.Token token = parser.currentToken(); ensureExpectedToken(XContentParser.Token.FIELD_NAME, token, parser::getTokenLocation); String currentFieldName = parser.currentName(); long version = -1; long seqNo = UNASSIGNED_SEQ_NO; long primaryTerm = UNASSIGNED_PRIMARY_TERM; Boolean found = null; BytesReference source = null; Map<String, DocumentField> fields = new HashMap<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (token.isValue()) { if (_INDEX.equals(currentFieldName)) { index = parser.text(); } else if (_TYPE.equals(currentFieldName)) { type = parser.text(); } else if (_ID.equals(currentFieldName)) { id = parser.text(); } else if (_VERSION.equals(currentFieldName)) { version = parser.longValue(); } else if (_SEQ_NO.equals(currentFieldName)) { seqNo = parser.longValue(); } else if (_PRIMARY_TERM.equals(currentFieldName)) { primaryTerm = parser.longValue(); } else if (FOUND.equals(currentFieldName)) { found = parser.booleanValue(); } else { // This fields is in metadata area of the xContent, thus should be treated as metadata fields.put(currentFieldName, new DocumentField(currentFieldName, Collections.singletonList(parser.objectText()), true)); } } else if (token == XContentParser.Token.START_OBJECT) { if (SourceFieldMapper.NAME.equals(currentFieldName)) { try (XContentBuilder builder = XContentBuilder.builder(parser.contentType().xContent())) { //the original document gets slightly modified: whitespaces or pretty printing are not preserved, //it all depends on the current builder settings builder.copyCurrentStructure(parser); source = BytesReference.bytes(builder); } } else if (FIELDS.equals(currentFieldName)) { while(parser.nextToken() != XContentParser.Token.END_OBJECT) { DocumentField getField = DocumentField.fromXContent(parser, false); fields.put(getField.getName(), getField); } } else { parser.skipChildren(); // skip potential inner objects for forward compatibility } } else if (token == XContentParser.Token.START_ARRAY) { if (IgnoredFieldMapper.NAME.equals(currentFieldName)) { fields.put(currentFieldName, new DocumentField(currentFieldName, parser.list(), false)); } else { parser.skipChildren(); // skip potential inner arrays for forward compatibility } } } return new GetResult(index, type, id, seqNo, primaryTerm, version, found, source, fields); }	@rjernst this documentfield is not inside of _source, but i assumed that it is not a metadata field based on the way the fields component is filled during serialization https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/get/getresult.java#l277-l282
@Override protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); if (field != null) { builder.field("field", field); } if (bucketCountThresholds.getRequiredSize().explicit()) { builder.field(AbstractTermsParametersParser.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), bucketCountThresholds.getRequiredSize().value()); } if (bucketCountThresholds.getShardSize().explicit()) { builder.field(AbstractTermsParametersParser.SHARD_SIZE_FIELD_NAME.getPreferredName(), bucketCountThresholds.getShardSize().value()); } if (bucketCountThresholds.getMinDocCount().explicit()) { builder.field(AbstractTermsParametersParser.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), bucketCountThresholds.getMinDocCount().value()); } if (bucketCountThresholds.getShardMinDocCount().explicit()) { builder.field(AbstractTermsParametersParser.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), bucketCountThresholds.getShardMinDocCount().value()); } if (executionHint != null) { builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint); } if (includePattern != null) { if (includeFlags == 0) { builder.field("include", includePattern); } else { builder.startObject("include") .field("pattern", includePattern) .field("flags", includeFlags) .endObject(); } } if (excludePattern != null) { if (excludeFlags == 0) { builder.field("exclude", excludePattern); } else { builder.startObject("exclude") .field("pattern", excludePattern) .field("flags", excludeFlags) .endObject(); } } if (filterBuilder != null) { builder.field(SignificantTermsParametersParser.BACKGROUND_FILTER.getPreferredName()); filterBuilder.toXContent(builder, params); } return builder.endObject(); }	i think it would be nicer to have this logic in a bucketcountthresholds.toxcontent method? this way, the explicit objects could remain private to the bucketcountthresholds class that would only expose plain longs?
private static Aggregator createAndRegisterContextAware(AggregationContext context, AggregatorFactory factory, Aggregator parent, boolean collectsSingleBucket) throws IOException { final Aggregator aggregator = factory.create(context, parent, collectsSingleBucket); if (aggregator.shouldCollect()) { context.registerReaderContextAware(aggregator); } // Once the aggregator is fully constructed perform any initialisation - // can't do everything in constructors if Aggregator base class needs // to delegate to subclasses as part of construction. aggregator.preCollection(); return aggregator; }	sorry to comment on the same thing again but now i see it i'm wondering if collectsfromsinglebucket would be better since collectssinglebucket seems a bit ambiguous as to whether it collects from a single parent bucket or only collects one bucket in total (which the terms agg doesn't)
private boolean waitForRelocationsToStart(final String index, TimeValue timeout) throws InterruptedException { return awaitBusy(() -> client().admin().cluster().prepareHealth(index).execute().actionGet().getRelocatingShards() > 0, timeout.millis(), TimeUnit.MILLISECONDS); }	i removed this test as it uses the blockingclusterstatelistener which does not work with the new infrastructure. i will open a pr so that snapshotshardsservice.innerupdatesnapshotstate uses the default clusterservice-based batching mechanism, making this test obsolete.
public void testScalingThreadPoolRejectAfterShutdown() throws Exception { final boolean rejectAfterShutdown = randomBoolean(); final int min = randomIntBetween(1, 4); final int max = randomIntBetween(min, 16); final EsThreadPoolExecutor scalingExecutor = EsExecutors.newScaling( getTestName().toLowerCase(Locale.ROOT), min, max, randomLongBetween(0, 100), TimeUnit.MILLISECONDS, rejectAfterShutdown, EsExecutors.daemonThreadFactory(getTestName().toLowerCase(Locale.ROOT)), new ThreadContext(Settings.EMPTY) ); try { final AtomicLong executed = new AtomicLong(); final AtomicLong rejected = new AtomicLong(); final AtomicLong failed = new AtomicLong(); final CountDownLatch latch = new CountDownLatch(max); final CountDownLatch block = new CountDownLatch(1); for (int i = 0; i < max; i++) { execute(scalingExecutor, () -> { try { latch.countDown(); block.await(); } catch (InterruptedException e) { fail(e.toString()); } }, executed, rejected, failed); } latch.await(); assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); final int queued = randomIntBetween(1, 100); for (int i = 0; i < queued; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(queued)); scalingExecutor.shutdown(); final int queuedAfterShutdown = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterShutdown; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(queued) : equalTo(queued + queuedAfterShutdown)); block.countDown(); assertBusy(() -> assertTrue(scalingExecutor.isTerminated())); assertThat(scalingExecutor.getActiveCount(), equalTo(0)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); assertThat(failed.get(), equalTo(0L)); final Matcher<Long> executionsMatcher = rejectAfterShutdown ? equalTo((long) max + queued) : greaterThanOrEqualTo((long) max + queued); assertThat(scalingExecutor.getCompletedTaskCount(), executionsMatcher); assertThat(executed.get(), executionsMatcher); final EsRejectedExecutionHandler handler = (EsRejectedExecutionHandler) scalingExecutor.getRejectedExecutionHandler(); Matcher<Long> rejectionsMatcher = rejectAfterShutdown ? equalTo((long) queuedAfterShutdown) : equalTo(0L); assertThat(handler.rejected(), rejectionsMatcher); assertThat(rejected.get(), rejectionsMatcher); final int queuedAfterTermination = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterTermination; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), executionsMatcher); assertThat(executed.get(), executionsMatcher); rejectionsMatcher = rejectAfterShutdown ? equalTo((long) queuedAfterShutdown + queuedAfterTermination) : equalTo(0L); assertThat(handler.rejected(), rejectionsMatcher); assertThat(rejected.get(), rejectionsMatcher); assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(0) : equalTo(queuedAfterTermination)); assertThat(failed.get(), equalTo(0L)); } finally { ThreadPool.terminate(scalingExecutor, 10, TimeUnit.SECONDS); } }	can we also check that it is lessthanorequalto(max + queued + queuedaftershutdown)?
public void testScalingThreadPoolRejectAfterShutdown() throws Exception { final boolean rejectAfterShutdown = randomBoolean(); final int min = randomIntBetween(1, 4); final int max = randomIntBetween(min, 16); final EsThreadPoolExecutor scalingExecutor = EsExecutors.newScaling( getTestName().toLowerCase(Locale.ROOT), min, max, randomLongBetween(0, 100), TimeUnit.MILLISECONDS, rejectAfterShutdown, EsExecutors.daemonThreadFactory(getTestName().toLowerCase(Locale.ROOT)), new ThreadContext(Settings.EMPTY) ); try { final AtomicLong executed = new AtomicLong(); final AtomicLong rejected = new AtomicLong(); final AtomicLong failed = new AtomicLong(); final CountDownLatch latch = new CountDownLatch(max); final CountDownLatch block = new CountDownLatch(1); for (int i = 0; i < max; i++) { execute(scalingExecutor, () -> { try { latch.countDown(); block.await(); } catch (InterruptedException e) { fail(e.toString()); } }, executed, rejected, failed); } latch.await(); assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); final int queued = randomIntBetween(1, 100); for (int i = 0; i < queued; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(queued)); scalingExecutor.shutdown(); final int queuedAfterShutdown = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterShutdown; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(queued) : equalTo(queued + queuedAfterShutdown)); block.countDown(); assertBusy(() -> assertTrue(scalingExecutor.isTerminated())); assertThat(scalingExecutor.getActiveCount(), equalTo(0)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); assertThat(failed.get(), equalTo(0L)); final Matcher<Long> executionsMatcher = rejectAfterShutdown ? equalTo((long) max + queued) : greaterThanOrEqualTo((long) max + queued); assertThat(scalingExecutor.getCompletedTaskCount(), executionsMatcher); assertThat(executed.get(), executionsMatcher); final EsRejectedExecutionHandler handler = (EsRejectedExecutionHandler) scalingExecutor.getRejectedExecutionHandler(); Matcher<Long> rejectionsMatcher = rejectAfterShutdown ? equalTo((long) queuedAfterShutdown) : equalTo(0L); assertThat(handler.rejected(), rejectionsMatcher); assertThat(rejected.get(), rejectionsMatcher); final int queuedAfterTermination = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterTermination; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), executionsMatcher); assertThat(executed.get(), executionsMatcher); rejectionsMatcher = rejectAfterShutdown ? equalTo((long) queuedAfterShutdown + queuedAfterTermination) : equalTo(0L); assertThat(handler.rejected(), rejectionsMatcher); assertThat(rejected.get(), rejectionsMatcher); assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(0) : equalTo(queuedAfterTermination)); assertThat(failed.get(), equalTo(0L)); } finally { ThreadPool.terminate(scalingExecutor, 10, TimeUnit.SECONDS); } }	also here can we check less than or equal to (max + barrier.getparties() - 1)?
public void testScalingThreadPoolRejectAfterShutdown() throws Exception { final boolean rejectAfterShutdown = randomBoolean(); final int min = randomIntBetween(1, 4); final int max = randomIntBetween(min, 16); final EsThreadPoolExecutor scalingExecutor = EsExecutors.newScaling( getTestName().toLowerCase(Locale.ROOT), min, max, randomLongBetween(0, 100), TimeUnit.MILLISECONDS, rejectAfterShutdown, EsExecutors.daemonThreadFactory(getTestName().toLowerCase(Locale.ROOT)), new ThreadContext(Settings.EMPTY) ); try { final AtomicLong executed = new AtomicLong(); final AtomicLong rejected = new AtomicLong(); final AtomicLong failed = new AtomicLong(); final CountDownLatch latch = new CountDownLatch(max); final CountDownLatch block = new CountDownLatch(1); for (int i = 0; i < max; i++) { execute(scalingExecutor, () -> { try { latch.countDown(); block.await(); } catch (InterruptedException e) { fail(e.toString()); } }, executed, rejected, failed); } latch.await(); assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); final int queued = randomIntBetween(1, 100); for (int i = 0; i < queued; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), equalTo(0L)); assertThat(scalingExecutor.getActiveCount(), equalTo(max)); assertThat(scalingExecutor.getQueue().size(), equalTo(queued)); scalingExecutor.shutdown(); final int queuedAfterShutdown = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterShutdown; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(queued) : equalTo(queued + queuedAfterShutdown)); block.countDown(); assertBusy(() -> assertTrue(scalingExecutor.isTerminated())); assertThat(scalingExecutor.getActiveCount(), equalTo(0)); assertThat(scalingExecutor.getQueue().size(), equalTo(0)); assertThat(failed.get(), equalTo(0L)); final Matcher<Long> executionsMatcher = rejectAfterShutdown ? equalTo((long) max + queued) : greaterThanOrEqualTo((long) max + queued); assertThat(scalingExecutor.getCompletedTaskCount(), executionsMatcher); assertThat(executed.get(), executionsMatcher); final EsRejectedExecutionHandler handler = (EsRejectedExecutionHandler) scalingExecutor.getRejectedExecutionHandler(); Matcher<Long> rejectionsMatcher = rejectAfterShutdown ? equalTo((long) queuedAfterShutdown) : equalTo(0L); assertThat(handler.rejected(), rejectionsMatcher); assertThat(rejected.get(), rejectionsMatcher); final int queuedAfterTermination = randomIntBetween(1, 100); for (int i = 0; i < queuedAfterTermination; i++) { execute(scalingExecutor, () -> {}, executed, rejected, failed); } assertThat(scalingExecutor.getCompletedTaskCount(), executionsMatcher); assertThat(executed.get(), executionsMatcher); rejectionsMatcher = rejectAfterShutdown ? equalTo((long) queuedAfterShutdown + queuedAfterTermination) : equalTo(0L); assertThat(handler.rejected(), rejectionsMatcher); assertThat(rejected.get(), rejectionsMatcher); assertThat(scalingExecutor.getQueue().size(), rejectAfterShutdown ? equalTo(0) : equalTo(queuedAfterTermination)); assertThat(failed.get(), equalTo(0L)); } finally { ThreadPool.terminate(scalingExecutor, 10, TimeUnit.SECONDS); } }	can we check that scalingexecutor.getcompletedtaskcount() + rejected.get() == max + barrier.getparties() - 1? to ensure every request is accounted for exactly once.
@Override public void restoreShard( Store store, SnapshotId snapshotId, IndexId indexId, ShardId snapshotShardId, RecoveryState recoveryState, ActionListener<Void> listener ) { final ShardId shardId = store.shardId(); final ActionListener<Void> restoreListener = listener.delegateResponse( (l, e) -> l.onFailure(new IndexShardRestoreFailedException(shardId, "failed to restore snapshot [" + snapshotId + "]", e)) ); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT); final BlobContainer container = shardContainer(indexId, snapshotShardId); synchronized (ongoingRestores) { if (store.isClosing()) { restoreListener.onFailure(new AlreadyClosedException("store is closing")); return; } if (lifecycle.started() == false) { restoreListener.onFailure(new AlreadyClosedException("repository [" + metadata.name() + "] closed")); return; } final boolean added = ongoingRestores.add(shardId); assert added; } executor.execute(ActionRunnable.wrap(ActionListener.runAfter(restoreListener, () -> { final List<ActionListener<Void>> onEmptyListeners; synchronized (ongoingRestores) { if (ongoingRestores.remove(shardId) && ongoingRestores.isEmpty() && emptyListeners != null) { onEmptyListeners = emptyListeners; emptyListeners = null; } else { return; } } ActionListener.onResponse(onEmptyListeners, null); }), l -> { final BlobStoreIndexShardSnapshot snapshot = loadShardSnapshot(container, snapshotId); final SnapshotFiles snapshotFiles = new SnapshotFiles(snapshot.snapshot(), snapshot.indexFiles(), null); new FileRestoreContext(metadata.name(), shardId, snapshotId, recoveryState) { @Override protected void restoreFiles( List<BlobStoreIndexShardSnapshot.FileInfo> filesToRecover, Store store, ActionListener<Void> listener ) { if (filesToRecover.isEmpty()) { listener.onResponse(null); } else { // Start as many workers as fit into the snapshot pool at once at the most final int workers = Math.min( threadPool.info(ThreadPool.Names.SNAPSHOT).getMax(), snapshotFiles.indexFiles().size() ); final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> files = new LinkedBlockingQueue<>(filesToRecover); final ActionListener<Void> allFilesListener = fileQueueListener(files, workers, listener.map(v -> null)); // restore the files from the snapshot to the Lucene store for (int i = 0; i < workers; ++i) { try { executeOneFileRestore(files, allFilesListener); } catch (Exception e) { allFilesListener.onFailure(e); } } } } private void executeOneFileRestore( BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> files, ActionListener<Void> allFilesListener ) throws InterruptedException { final BlobStoreIndexShardSnapshot.FileInfo fileToRecover = files.poll(0L, TimeUnit.MILLISECONDS); if (fileToRecover == null) { allFilesListener.onResponse(null); } else { executor.execute(ActionRunnable.wrap(allFilesListener, filesListener -> { store.incRef(); try { restoreFile(fileToRecover, store); } finally { store.decRef(); } executeOneFileRestore(files, filesListener); })); } } private void restoreFile(BlobStoreIndexShardSnapshot.FileInfo fileInfo, Store store) throws IOException { ensureNotClosing(store); logger.trace(() -> new ParameterizedMessage("[{}] restoring [{}] to [{}]", metadata.name(), fileInfo, store)); boolean success = false; try ( IndexOutput indexOutput = store.createVerifyingOutput( fileInfo.physicalName(), fileInfo.metadata(), IOContext.DEFAULT ) ) { if (fileInfo.name().startsWith(VIRTUAL_DATA_BLOB_PREFIX)) { final BytesRef hash = fileInfo.metadata().hash(); indexOutput.writeBytes(hash.bytes, hash.offset, hash.length); recoveryState.getIndex().addRecoveredBytesToFile(fileInfo.physicalName(), hash.length); } else { try (InputStream stream = maybeRateLimitRestores(new SlicedInputStream(fileInfo.numberOfParts()) { @Override protected InputStream openSlice(int slice) throws IOException { ensureNotClosing(store); return container.readBlob(fileInfo.partName(slice)); } })) { final byte[] buffer = new byte[Math.toIntExact(Math.min(bufferSize, fileInfo.length()))]; int length; while ((length = stream.read(buffer)) > 0) { ensureNotClosing(store); indexOutput.writeBytes(buffer, 0, length); recoveryState.getIndex().addRecoveredBytesToFile(fileInfo.physicalName(), length); } } } Store.verify(indexOutput); indexOutput.close(); store.directory().sync(Collections.singleton(fileInfo.physicalName())); success = true; } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) { try { store.markStoreCorrupted(ex); } catch (IOException e) { logger.warn("store cannot be marked as corrupted", e); } throw ex; } finally { if (success == false) { store.deleteQuiet(fileInfo.physicalName()); } } } void ensureNotClosing(final Store store) throws AlreadyClosedException { assert store.refCount() > 0; if (store.isClosing()) { throw new AlreadyClosedException("store is closing"); } if (lifecycle.started() == false) { throw new AlreadyClosedException("repository [" + metadata.name() + "] closed"); } } }.restore(snapshotFiles, store, l); })); }	maybe add some context?
void ensureNotClosing(final Store store) throws AlreadyClosedException { assert store.refCount() > 0; if (store.isClosing()) { throw new AlreadyClosedException("store is closing"); } if (lifecycle.started() == false) { throw new AlreadyClosedException("repository [" + metadata.name() + "] closed"); } }	added this here as well since we close the repositories service before the indices service and would otherwise have to wait for the restores to actually complete in some cases i think.
public void testForgetFollower() throws IOException { final String forgetLeader = "forget-leader"; final String forgetFollower = "forget-follower"; if ("leader".equals(targetCluster)) { logger.info("running against leader cluster"); final Settings indexSettings = Settings.builder() .put("index.number_of_replicas", 0) .put("index.number_of_shards", 1) .put("index.soft_deletes.enabled", true) .build(); createIndex(forgetLeader, indexSettings); } else { logger.info("running against follower cluster"); followIndex(client(), "leader_cluster", forgetLeader, forgetFollower); final Response response = client().performRequest(new Request("GET", "/" + forgetFollower + "/_stats")); final String followerIndexUUID = ObjectPath.createFromResponse(response).evaluate("indices." + forgetFollower + ".uuid"); assertOK(client().performRequest(new Request("POST", "/" + forgetFollower + "/_ccr/pause_follow"))); try (RestClient leaderClient = buildLeaderClient(restClientSettings())) { final Request request = new Request("POST", "/" + forgetLeader + "/_ccr/forget_follower"); final String requestBody = "{" + "\\\\"follower_cluster\\\\":\\\\"follow-cluster\\\\"," + "\\\\"follower_index\\\\":\\\\"" + forgetFollower + "\\\\"," + "\\\\"follower_index_uuid\\\\":\\\\"" + followerIndexUUID + "\\\\"," + "\\\\"leader_remote_cluster\\\\":\\\\"leader_cluster\\\\"" + "}"; request.setJsonEntity(requestBody); logger.info(requestBody); assertOK(leaderClient.performRequest(request)); final Request retentionLeasesRequest = new Request("GET", "/" + forgetLeader + "/_stats"); retentionLeasesRequest.addParameter("level", "shards"); final Response retentionLeasesResponse = leaderClient.performRequest(retentionLeasesRequest); final ArrayList<Object> shardsStats = ObjectPath.createFromResponse(retentionLeasesResponse).evaluate("indices." + forgetLeader + ".shards.0"); assertThat(shardsStats, hasSize(1)); @SuppressWarnings("unchecked") final Map<String, Object> shardStatsAsMap = (Map<String, Object>)shardsStats.get(0); @SuppressWarnings("unchecked") final Map<String, Object> retentionLeasesStats = (Map<String, Object>) shardStatsAsMap.get("retention_leases"); @SuppressWarnings("unchecked") final ArrayList<Object> leases = (ArrayList<Object>)retentionLeasesStats.get("leases"); assertThat(leases, empty()); } } }	since no unfollow has happened, the shard follow task may add a new lease, right?
*/ public long recoverLocallyUpToGlobalCheckpoint() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } assert recoveryState.getStage() == RecoveryState.Stage.INDEX : "unexpected recovery stage [" + recoveryState.getStage() + "]"; assert routingEntry().recoverySource().getType() == RecoverySource.Type.PEER : "not a peer recovery [" + routingEntry() + "]"; final Optional<SequenceNumbers.CommitInfo> safeCommit; final long globalCheckpoint; try { final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); safeCommit = store.findSafeIndexCommit(globalCheckpoint); } catch (org.apache.lucene.index.IndexNotFoundException e) { logger.trace("skip local recovery as no index commit found"); return UNASSIGNED_SEQ_NO; } catch (Exception e) { logger.debug("skip local recovery as failed to find the safe commit", e); return UNASSIGNED_SEQ_NO; } if (safeCommit.isPresent() == false) { logger.trace("skip local recovery as no safe commit found"); return UNASSIGNED_SEQ_NO; } assert safeCommit.get().localCheckpoint <= globalCheckpoint : safeCommit.get().localCheckpoint + " > " + globalCheckpoint; try { maybeCheckIndex(); // check index here and won't do it again if ops-based recovery occurs recoveryState.setStage(RecoveryState.Stage.TRANSLOG); if (safeCommit.get().localCheckpoint == globalCheckpoint) { logger.trace("skip local recovery as the safe commit is up to date; safe commit {} global checkpoint {}", safeCommit.get(), globalCheckpoint); recoveryState.getTranslog().totalLocal(0); return globalCheckpoint + 1; } try { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (engine, snapshot) -> { recoveryState.getTranslog().totalLocal(snapshot.totalOperations()); final int recoveredOps = runTranslogRecovery(engine, snapshot, Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY, recoveryState.getTranslog()::incrementRecoveredOperations); recoveryState.getTranslog().totalLocal(recoveredOps); // adjust the total local to reflect the actual count return recoveredOps; }; innerOpenEngineAndTranslog(); getEngine().recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); logger.trace("shard locally recovered up to {}", getEngine().getSeqNoStats(globalCheckpoint)); } finally { synchronized (mutex) { IOUtils.close(currentEngineReference.getAndSet(null)); } } } catch (Exception e) { logger.debug(new ParameterizedMessage("failed to recover shard locally up to global checkpoint {}", globalCheckpoint), e); return UNASSIGNED_SEQ_NO; } try { // we need to find the safe commit again as we should have created a new one during the local recovery final Optional<SequenceNumbers.CommitInfo> newSafeCommit = store.findSafeIndexCommit(globalCheckpoint); assert newSafeCommit.isPresent() : "no safe commit found after local recovery"; return newSafeCommit.get().localCheckpoint + 1; } catch (Exception e) { if (Assertions.ENABLED) { throw new AssertionError( "failed to find the safe commit after recovering shard locally up to global checkpoint " + globalCheckpoint, e); } logger.debug(new ParameterizedMessage( "failed to find the safe commit after recovering shard locally up to global checkpoint {}", globalCheckpoint), e); return UNASSIGNED_SEQ_NO; } }	unless there are failures during recovery, this should be equal to snapshot.totaloperations(), no? if so, i would rather not adjust this here.
public void testUpdateableSynonymsRejectedAtIndexTime() throws FileNotFoundException, IOException { String synonymsFileName = "synonyms.txt"; Path configDir = node().getEnvironment().configFile(); if (Files.exists(configDir) == false) { Files.createDirectory(configDir); } Path synonymsFile = configDir.resolve(synonymsFileName); if (Files.exists(synonymsFile) == false) { Files.createFile(synonymsFile); } try (PrintWriter out = new PrintWriter( new OutputStreamWriter(Files.newOutputStream(synonymsFile, StandardOpenOption.WRITE), StandardCharsets.UTF_8))) { out.println("foo, baz"); } final String indexName = "test"; final String analyzerName = "my_synonym_analyzer"; MapperException ex = expectThrows(MapperException.class, () -> client().admin().indices().prepareCreate(indexName).setSettings(Settings.builder().put("index.number_of_shards", 5) .put("index.number_of_replicas", 0).put("analysis.analyzer." + analyzerName + ".tokenizer", "standard") .putList("analysis.analyzer." + analyzerName + ".filter", "lowercase", "synonym_filter") .put("analysis.filter.synonym_filter.type", "synonym").put("analysis.filter.synonym_filter.updateable", "true") .put("analysis.filter.synonym_filter.synonyms_path", synonymsFileName)) .addMapping("_doc", "field", "type=text,analyzer=" + analyzerName).get()); assertEquals("Failed to parse mapping: analyzer [my_synonym_analyzer] " + "contains filters [synonym_filter] that are not allowed to run in all mode.", ex.getMessage()); }	given that this is reused in three different tests now, i wonder if it's worth extracting it as a separate method?
public void testSynonymsInMultiplexerUpdateable() throws FileNotFoundException, IOException { String synonymsFileName = "synonyms.txt"; Path configDir = node().getEnvironment().configFile(); if (Files.exists(configDir) == false) { Files.createDirectory(configDir); } Path synonymsFile = configDir.resolve(synonymsFileName); if (Files.exists(synonymsFile) == false) { Files.createFile(synonymsFile); } try (PrintWriter out = new PrintWriter( new OutputStreamWriter(Files.newOutputStream(synonymsFile, StandardOpenOption.WRITE), StandardCharsets.UTF_8))) { out.println("foo, baz"); } final String indexName = "test"; final String synonymAnalyzerName = "synonym_in_multiplexer_analyzer"; assertAcked(client().admin().indices().prepareCreate(indexName) .setSettings(Settings.builder().put("index.number_of_shards", 5).put("index.number_of_replicas", 0) .put("analysis.analyzer." + synonymAnalyzerName + ".tokenizer", "whitespace") .putList("analysis.analyzer." + synonymAnalyzerName + ".filter", "my_multiplexer") .put("analysis.filter.synonym_filter.type", "synonym").put("analysis.filter.synonym_filter.updateable", "true") .put("analysis.filter.synonym_filter.synonyms_path", synonymsFileName) .put("analysis.filter.my_multiplexer.type", "multiplexer") .putList("analysis.filter.my_multiplexer.filters", "synonym_filter")) .addMapping("_doc", "field", "type=text,analyzer=standard,search_analyzer=" + synonymAnalyzerName)); client().prepareIndex(indexName).setId("1").setSource("field", "foo").get(); assertNoFailures(client().admin().indices().prepareRefresh(indexName).execute().actionGet()); SearchResponse response = client().prepareSearch(indexName).setQuery(QueryBuilders.matchQuery("field", "baz")).get(); assertHitCount(response, 1L); response = client().prepareSearch(indexName).setQuery(QueryBuilders.matchQuery("field", "buzz")).get(); assertHitCount(response, 0L); Response analyzeResponse = client().admin().indices().prepareAnalyze(indexName, "foo").setAnalyzer(synonymAnalyzerName).get(); assertEquals(2, analyzeResponse.getTokens().size()); final Set<String> tokens = new HashSet<>(); analyzeResponse.getTokens().stream().map(AnalyzeToken::getTerm).forEach(t -> tokens.add(t)); assertTrue(tokens.contains("foo")); assertTrue(tokens.contains("baz")); // now update synonyms file and trigger reloading try (PrintWriter out = new PrintWriter( new OutputStreamWriter(Files.newOutputStream(synonymsFile, StandardOpenOption.WRITE), StandardCharsets.UTF_8))) { out.println("foo, baz, buzz"); } ReloadAnalyzersResponse reloadResponse = client().execute(ReloadAnalyzerAction.INSTANCE, new ReloadAnalyzersRequest(indexName)) .actionGet(); assertNoFailures(reloadResponse); Set<String> reloadedAnalyzers = reloadResponse.getReloadDetails().get(indexName).getReloadedAnalyzers(); assertEquals(1, reloadedAnalyzers.size()); assertTrue(reloadedAnalyzers.contains(synonymAnalyzerName)); analyzeResponse = client().admin().indices().prepareAnalyze(indexName, "foo").setAnalyzer(synonymAnalyzerName).get(); assertEquals(3, analyzeResponse.getTokens().size()); tokens.clear(); analyzeResponse.getTokens().stream().map(AnalyzeToken::getTerm).forEach(t -> tokens.add(t)); assertTrue(tokens.contains("foo")); assertTrue(tokens.contains("baz")); assertTrue(tokens.contains("buzz")); response = client().prepareSearch(indexName).setQuery(QueryBuilders.matchQuery("field", "baz")).get(); assertHitCount(response, 1L); response = client().prepareSearch(indexName).setQuery(QueryBuilders.matchQuery("field", "buzz")).get(); assertHitCount(response, 1L); }	maybe also add a test asserting that a multiplexer containing updateable synonyms is rejected as an index-time analyzer as well?
public void testUpdateableSynonymsRejectedAtIndexTime() throws FileNotFoundException, IOException { String synonymsFileName = "synonyms.txt"; Path configDir = node().getEnvironment().configFile(); if (Files.exists(configDir) == false) { Files.createDirectory(configDir); } Path synonymsFile = configDir.resolve(synonymsFileName); if (Files.exists(synonymsFile) == false) { Files.createFile(synonymsFile); } try (PrintWriter out = new PrintWriter( new OutputStreamWriter(Files.newOutputStream(synonymsFile, StandardOpenOption.WRITE), StandardCharsets.UTF_8))) { out.println("foo, baz"); } final String indexName = "test"; final String analyzerName = "my_synonym_analyzer"; MapperException ex = expectThrows(MapperException.class, () -> client().admin().indices().prepareCreate(indexName).setSettings(Settings.builder().put("index.number_of_shards", 5) .put("index.number_of_replicas", 0).put("analysis.analyzer." + analyzerName + ".tokenizer", "standard") .putList("analysis.analyzer." + analyzerName + ".filter", "lowercase", "synonym_filter") .put("analysis.filter.synonym_filter.type", "synonym").put("analysis.filter.synonym_filter.updateable", "true") .put("analysis.filter.synonym_filter.synonyms_path", synonymsFileName)) .addMapping("_doc", "field", "type=text,analyzer=" + analyzerName).get()); assertEquals("Failed to parse mapping: analyzer [my_synonym_analyzer] " + "contains filters [synonym_filter] that are not allowed to run in all mode.", ex.getMessage()); }	this compression of multiple settings onto single lines seems to me to make test more difficult to follow, can we keep the indentation as it was before?
private String basicTemplate(Function function) { if (function.dataType().name().equals("DATE") || function.dataType() == DATETIME || // Aggregations on date_nanos are returned as string (function instanceof AggregateFunction && ((AggregateFunction) function).field().dataType() == DATETIME)) { return "{sql}.asDateTime({})"; } else if (function instanceof AggregateFunction) { DataType dt = function.dataType(); if (dt.isInteger()) { // MAX, MIN, SUM need to retain field's data type, so that possible operations on integral types (like division) work // correctly -> perform a cast // SQL function classes not available in QL: filter by name String fn = function.functionName(); if ("MAX".equals(fn) || "MIN".equals(fn)) { return "(" + (dt == INTEGER ? "int" : dt.esType()) + "){}"; } else if ("SUM".equals(fn)) { return "(long){}"; } } } return "{}"; }	same here: please, let's have a comment for the future to show that this part with the cast is needed for the bucket selector so the case of having.
private String basicTemplate(Function function) { if (function.dataType().name().equals("DATE") || function.dataType() == DATETIME || // Aggregations on date_nanos are returned as string (function instanceof AggregateFunction && ((AggregateFunction) function).field().dataType() == DATETIME)) { return "{sql}.asDateTime({})"; } else if (function instanceof AggregateFunction) { DataType dt = function.dataType(); if (dt.isInteger()) { // MAX, MIN, SUM need to retain field's data type, so that possible operations on integral types (like division) work // correctly -> perform a cast // SQL function classes not available in QL: filter by name String fn = function.functionName(); if ("MAX".equals(fn) || "MIN".equals(fn)) { return "(" + (dt == INTEGER ? "int" : dt.esType()) + "){}"; } else if ("SUM".equals(fn)) { return "(long){}"; } } } return "{}"; }	i don't think this casting is safe enough - it converts the results into primitive which not only causes autoboxing for objects, but fails in case of nulls.
private String basicTemplate(Function function) { if (function.dataType().name().equals("DATE") || function.dataType() == DATETIME || // Aggregations on date_nanos are returned as string (function instanceof AggregateFunction && ((AggregateFunction) function).field().dataType() == DATETIME)) { return "{sql}.asDateTime({})"; } else if (function instanceof AggregateFunction) { DataType dt = function.dataType(); if (dt.isInteger()) { // MAX, MIN, SUM need to retain field's data type, so that possible operations on integral types (like division) work // correctly -> perform a cast // SQL function classes not available in QL: filter by name String fn = function.functionName(); if ("MAX".equals(fn) || "MIN".equals(fn)) { return "(" + (dt == INTEGER ? "int" : dt.esType()) + "){}"; } else if ("SUM".equals(fn)) { return "(long){}"; } } } return "{}"; }	not arguing whether this is correct or not - just pointing out that this is a breaking change.
public void writeTo(StreamOutput out) throws IOException { out.writeString(name); out.writeString(property); out.writeOptionalString(innerKey); out.writeOptionalString(dataType == null ? null : dataType.name()); }	this section also needs to be backwards compatible - if it's after 7_13, write the datatype, otherwise keep writing the bool.
public void testNeverLeaveStaleDeleteTombstone() throws Exception { LiveVersionMap versionMap = new LiveVersionMap(); BytesRef uid = uid("1"); long versions = between(10, 1000); for (long version = 1; version <= versions; version++) { if (randomBoolean()) { versionMap.beforeRefresh(); versionMap.afterRefresh(randomBoolean()); } if (randomBoolean()) { versionMap.enforceSafeAccess(); } try (Releasable ignore = versionMap.acquireLock(uid)) { if (randomBoolean()) { versionMap.putDeleteUnderLock(uid, new DeleteVersionValue(version, 1, 1, 1)); } else { versionMap.maybePutIndexUnderLock(uid, new IndexVersionValue(randomTranslogLocation(), version, 1, 1)); } VersionValue storedValue = versionMap.getUnderLock(uid); if (storedValue != null) { assertThat("Keeping a stale version value", storedValue.version, equalTo(version)); } } } }	can we also check that this return value correlates with what we expect? i.e., if the doc was just deleted it should be deleted, if it was indexed it should never be marked as deleted? (and found if we're in safe mode). also can you check this property holds also not directly after indexing/deleting? (i.e., after refresh, enforcesafe access etc). said differently - maybe also randomly do nothing under lock?
* @param needActive Whether current license needs to be active * @param allowTrial Whether the feature is allowed for trial license * * @return true if feature is allowed, otherwise false */ private synchronized boolean isAllowedByLicenseAndSecurity( OperationMode minimumMode, boolean needSecurity, boolean needActive, boolean allowTrial) { final Status localStatus = status; if (needSecurity && false == isSecurityEnabled(localStatus.mode, isSecurityExplicitlyEnabled, isSecurityEnabled)) { return false; } if (needActive && false == localStatus.active) { return false; } return isAllowedByOperationMode(localStatus.mode, minimumMode, allowTrial); }	i agree with your reasoning that since status is not volatile, we can remove the local variables from here and other places ( i don't think there was an explicit reason this was not done in #33396 )
public void testLookbackOnly() throws Exception { client().admin().indices().prepareCreate("data-1") .setMapping("time", "type=date") .get(); long numDocs = randomIntBetween(32, 2048); long now = System.currentTimeMillis(); long oneWeekAgo = now - 604800000; long twoWeeksAgo = oneWeekAgo - 604800000; indexDocs(logger, "data-1", numDocs, twoWeeksAgo, oneWeekAgo); client().admin().indices().prepareCreate("data-2") .setMapping("time", "type=date") .get(); client().admin().cluster().prepareHealth("data-1", "data-2").setWaitForYellowStatus().get(); long numDocs2 = randomIntBetween(32, 2048); indexDocs(logger, "data-2", numDocs2, oneWeekAgo, now); Job.Builder job = createScheduledJob("lookback-job"); registerJob(job); PutJobAction.Response putJobResponse = putJob(job); assertThat(putJobResponse.getResponse().getJobVersion(), equalTo(Version.CURRENT)); openJob(job.getId()); assertBusy(() -> assertEquals(getJobStats(job.getId()).get(0).getState(), JobState.OPENED)); List<String> t = new ArrayList<>(2); t.add("data-1"); t.add("data-2"); DatafeedConfig datafeedConfig = createDatafeed(job.getId() + "-datafeed", job.getId(), t); registerDatafeed(datafeedConfig); putDatafeed(datafeedConfig); startDatafeed(datafeedConfig.getId(), 0L, now); assertBusy(() -> { DataCounts dataCounts = getDataCounts(job.getId()); assertThat(dataCounts.getProcessedRecordCount(), equalTo(numDocs + numDocs2)); assertThat(dataCounts.getOutOfOrderTimeStampCount(), equalTo(0L)); GetDatafeedsStatsAction.Request request = new GetDatafeedsStatsAction.Request(datafeedConfig.getId()); GetDatafeedsStatsAction.Response response = client().execute(GetDatafeedsStatsAction.INSTANCE, request).actionGet(); assertThat(response.getResponse().results().get(0).getDatafeedState(), equalTo(DatafeedState.STOPPED)); }, 60, TimeUnit.SECONDS); waitUntilJobIsClosed(job.getId()); }	does this need an assume too?
public void checkIndexStateThenExecute(final Consumer<Exception> consumer, final Runnable andThen) { final State indexState = this.indexState; // use a local copy so all checks execute against the same state! try { // TODO we should improve this so we don't fire off a bunch of requests to do the same thing (create or update mappings) if (indexState == State.UNRECOVERED_STATE) { throw new ElasticsearchStatusException( "Cluster state has not been recovered yet, cannot write to the [" + indexState.concreteIndexName + "] index", RestStatus.SERVICE_UNAVAILABLE); } else if (indexState.indexExists() && indexState.isIndexUpToDate == false) { throw new IllegalStateException("Index [" + indexState.concreteIndexName + "] is not on the current version." + "Security features relying on the index will not be available until the upgrade API is run on the index"); } else { andThen.run(); } } catch (Exception e) { consumer.accept(e); } }	i think we still need to handle the case where the master node doesn't know about the security system index, so for bwc we should maintain this code somehow.
@Override protected void taskOperation(CancelTasksRequest request, CancellableTask cancellableTask, ActionListener<TaskInfo> listener) { String nodeId = clusterService.localNode().getId(); if (cancellableTask.shouldCancelChildrenOnCancellation()) { StepListener<Void> completedListener = new StepListener<>(); GroupedActionListener<Void> groupedListener = new GroupedActionListener<>(ActionListener.map(completedListener, r -> null), 3); Collection<DiscoveryNode> childrenNodes = taskManager.startBanOnChildrenNodes(cancellableTask.getId(), () -> groupedListener.onResponse(null)); boolean cancelled = taskManager.cancel(cancellableTask, request.getReason(), () -> groupedListener.onResponse(null)); if (cancelled == false) { throw new IllegalStateException("task with id " + cancellableTask.getId() + " is already cancelled"); } StepListener<Void> banOnNodesListener = new StepListener<>(); setBanOnNodes(request.getReason(), cancellableTask, childrenNodes, banOnNodesListener); banOnNodesListener.whenComplete(groupedListener::onResponse, groupedListener::onFailure); // We remove bans after all child tasks are completed although in theory we can do it on a per-node basic. completedListener.whenComplete( r -> removeBanOnNodes(cancellableTask, childrenNodes), e -> removeBanOnNodes(cancellableTask, childrenNodes)); // if wait_for_child_tasks is true, then only return when (1) bans are placed on child nodes, (2) child tasks are // completed or failed, (3) the main task is cancelled. Otherwise, return after bans are placed on child nodes. if (request.waitForChildTasks()) { completedListener.whenComplete(r -> listener.onResponse(cancellableTask.taskInfo(nodeId, false)), listener::onFailure); } else { banOnNodesListener.whenComplete(r -> listener.onResponse(cancellableTask.taskInfo(nodeId, false)), listener::onFailure); } } else { logger.trace("task {} doesn't have any children that should be cancelled", cancellableTask.getId()); final boolean cancelled = taskManager.cancel(cancellableTask, request.getReason(), () -> listener.onResponse(cancellableTask.taskInfo(nodeId, false))); if (cancelled == false) { throw new IllegalStateException("task with id " + cancellableTask.getId() + " is already cancelled"); } } }	i wonder if this message is misleading. the task could have been cancelled, but it could also have completed successfully before this request came in.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final String[] nodesIds = Strings.splitStringByCommaToArray(request.param("nodes")); final TaskId taskId = new TaskId(request.param("task_id")); final String[] actions = Strings.splitStringByCommaToArray(request.param("actions")); final TaskId parentTaskId = new TaskId(request.param("parent_task_id")); final String groupBy = request.param("group_by", "nodes"); CancelTasksRequest cancelTasksRequest = new CancelTasksRequest(); cancelTasksRequest.setTaskId(taskId); cancelTasksRequest.setNodes(nodesIds); cancelTasksRequest.setActions(actions); cancelTasksRequest.setParentTaskId(parentTaskId); cancelTasksRequest.setWaitForChildTasks(request.paramAsBoolean("wait_for_child_tasks", true)); return channel -> client.admin().cluster().cancelTasks(cancelTasksRequest, listTasksResponseListener(nodesInCluster, groupBy, channel)); }	shouldn't we get the default value from the canceltasksrequest so we don't duplicate it?
void updateRefreshedCheckpoint(long checkpoint) { refreshedCheckpoint.updateAndGet(curr -> Math.max(curr, checkpoint)); assert refreshedCheckpoint.get() >= checkpoint : refreshedCheckpoint.get() + " < " + checkpoint; } } @Override public final long getMaxSeenAutoIdTimestamp() { return maxSeenAutoIdTimestamp.get(); } @Override public final void updateMaxUnsafeAutoIdTimestamp(long newTimestamp) { updateAutoIdTimestamp(newTimestamp, true); } private void updateAutoIdTimestamp(long newTimestamp, boolean unsafe) { assert newTimestamp >= -1 : "invalid timestamp [" + newTimestamp + "]"; maxSeenAutoIdTimestamp.updateAndGet(curr -> Math.max(curr, newTimestamp)); assert maxSeenAutoIdTimestamp.get() >= newTimestamp; if (unsafe) { maxUnsafeAutoIdTimestamp.updateAndGet(curr -> Math.max(curr, newTimestamp)); assert maxUnsafeAutoIdTimestamp.get() >= newTimestamp; }	i don't think this add much value, we just did a max operation with it.
@Override public long indexTranslogOperations(List<Translog.Operation> operations, int totalTranslogOps, long maxSeenAutoIdTimestampOnPrimary) throws IOException { final RecoveryState.Translog translog = state().getTranslog(); translog.totalOperations(totalTranslogOps); assert indexShard().recoveryState() == state(); if (indexShard().state() != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, indexShard().state()); } indexShard().updateMaxUnsafeAutoIdTimestamp(maxSeenAutoIdTimestampOnPrimary); for (Translog.Operation operation : operations) { Engine.Result result = indexShard().applyTranslogOperation(operation, Engine.Operation.Origin.PEER_RECOVERY); if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) { throw new MapperException("mapping updates are not allowed [" + operation + "]"); } assert result.getFailure() == null: "unexpected failure while replicating translog entry: " + result.getFailure(); ExceptionsHelper.reThrowIfNotNull(result.getFailure()); } // update stats only after all operations completed (to ensure that mapping updates don't mess with stats) translog.incrementRecoveredOperations(operations.size()); indexShard().sync(); // roll over / flush / trim if needed indexShard().afterWriteOperation(); return indexShard().getLocalCheckpoint(); }	can you add a comment as to why we set this and how don't know what timestamp is associated with the operation so we use an upper bound?
* @return number of "used" bytes so far */ @Override public double addEstimateBytesAndMaybeBreak(long bytes, String label) throws CircuitBreakingException { final LimitAndOverhead limitAndOverhead = this.limitAndOverhead; final long memoryBytesLimit = limitAndOverhead.limit; final double overheadConstant = limitAndOverhead.overhead; // short-circuit on no data allowed, immediately throwing an exception if (memoryBytesLimit == 0) { circuitBreak(label, bytes); } long newUsed; // If there is no limit (-1), we can optimize a bit by using // .addAndGet() instead of looping (because we don't have to check a // limit), which makes the RamAccountingTermsEnum case faster. if (memoryBytesLimit == -1) { newUsed = noLimit(bytes, label); } else { newUsed = limit(bytes, label, overheadConstant, memoryBytesLimit); } // Additionally, we need to check that we haven't exceeded the parent's limit try { parent.checkParentLimit((long) (bytes * overheadConstant), label); } catch (CircuitBreakingException e) { // If the parent breaker is tripped, this breaker has to be // adjusted back down because the allocation is "blocked" but the // breaker has already been incremented this.addWithoutBreaking(-bytes); throw e; } return newUsed; }	i am doing this so there is only one volatile read in this function.
* @return number of "used" bytes so far */ @Override public double addEstimateBytesAndMaybeBreak(long bytes, String label) throws CircuitBreakingException { final LimitAndOverhead limitAndOverhead = this.limitAndOverhead; final long memoryBytesLimit = limitAndOverhead.limit; final double overheadConstant = limitAndOverhead.overhead; // short-circuit on no data allowed, immediately throwing an exception if (memoryBytesLimit == 0) { circuitBreak(label, bytes); } long newUsed; // If there is no limit (-1), we can optimize a bit by using // .addAndGet() instead of looping (because we don't have to check a // limit), which makes the RamAccountingTermsEnum case faster. if (memoryBytesLimit == -1) { newUsed = noLimit(bytes, label); } else { newUsed = limit(bytes, label, overheadConstant, memoryBytesLimit); } // Additionally, we need to check that we haven't exceeded the parent's limit try { parent.checkParentLimit((long) (bytes * overheadConstant), label); } catch (CircuitBreakingException e) { // If the parent breaker is tripped, this breaker has to be // adjusted back down because the allocation is "blocked" but the // breaker has already been incremented this.addWithoutBreaking(-bytes); throw e; } return newUsed; }	i am passing in the already read settings as we don't care about updates while we are checking the limit. this is effectively as it was before. additionally, it also prevents another unnecessary volatile read.
@Override public ValueFetcher valueFetcher(SearchExecutionContext context, String format) { DateFormatter defaultFormatter = dateTimeFormatter(); DateFormatter formatter = format != null ? DateFormatter.forPattern(format).withLocale(defaultFormatter.locale()) : defaultFormatter; return new SourceValueFetcher(name(), context, nullValue) { @Override public String parseSourceValue(Object value) { String date = value instanceof Number ? NUMBER_FORMAT.format(value) : value.toString(); // TODO can we emit a warning if we're losing precision here? I'm not sure we can. long timestamp = parse(date); ZonedDateTime dateTime = resolution().toInstant(timestamp).atZone(ZoneOffset.UTC); return formatter.format(dateTime); } }; }	this will be used per each document in request, so just using headerwarning.addwarning(formattedmessage); would probably spam response headers. i am not sure we have a convenient way of doing this. the code could look like this... static logger l = logmanager.getlogger("datefieldmapperheader"); static { final loggercontext context = (loggercontext) logmanager.getcontext(false); final configuration configuration = context.getconfiguration(); ratelimitingfilter ratelimitingfilter = new ratelimitingfilter(); headerwarningappender datefieldmapperheaderappender = new headerwarningappender("datefieldmapperheaderappender", ratelimitingfilter); loggers.addappender(logmanager.getlogger("datefieldmapperheader"), datefieldmapperheaderappender); } and then a usage public string parsesourcevalue(object value) { l.info(new eslogmessage("somemessage").with(deprecatedmessage.key_field_name, somecleversearchrequestuuid));
public long getLength() { return length; }	i understand why you did this and it's nice, but sadly it can't really work in 100% cases. we use a sparse file to store cached data but there is no way to ensure that the filesystem we're running on supports sparse files (thought most os do support it). in that case the sparsefiletracker would report only the cached data but the file on disk would really fill enough blocks to store the full length of file. can we revert this?
private static void ensureSnapshotIsLoaded(IndexShard indexShard) { final SearchableSnapshotDirectory directory = SearchableSnapshotDirectory.unwrapDirectory(indexShard.store().directory()); assert directory != null; OnDemandRecoveryState.Index index = (OnDemandRecoveryState.Index) indexShard.recoveryState().getIndex(); final boolean success = directory.loadSnapshot(index); assert directory.listAll().length > 0 : "expecting directory listing to be non-empty"; assert success || indexShard.routingEntry() .recoverySource() .getType() == RecoverySource.Type.PEER : "loading snapshot must not be called twice unless we are retrying a peer recovery"; }	i think we can pass recoverystate directly and cast the index instead in the loadsnapshot() method
public void testInvalidAliasName() throws Exception { final String[] invalidAliasNames = new String[] { "-alias1", "+alias2", "_alias3", "a#lias", "al:ias", ".", ".." }; setupRequestAlias(new Alias(randomFrom(invalidAliasNames))); expectThrows(InvalidAliasNameException.class, this::executeTask); }	this is testing that aliases with names with leading '-' cannot indeed be created so that -* can be safely used as an exclude wildcard (as it presently is).
public void testResolveAliasesAllGetAliasesRequestNoAuthorizedIndices() { GetAliasesRequest request = new GetAliasesRequest(); if (randomBoolean()) { request.aliases("_all"); } request.indices("non_existing"); //current user is not authorized for any index, aliases are replaced with the no-aliases-expression ResolvedIndices resolvedIndices = resolveIndices(request, buildAuthorizedIndices(userNoIndices, GetAliasesAction.NAME)); assertThat(resolvedIndices.getLocal(), contains("non_existing")); assertThat(Arrays.asList(request.indices()), contains("non_existing")); assertThat(request.aliases(), arrayContaining(IndicesAndAliasesResolver.NO_INDICES_OR_ALIASES_ARRAY)); }	fixing these previous 3 tests : testresolvealiaseswildcardsindicesaliasesrequestdeleteactionsnoauthorizedindices, testresolvealiaseswildcardsgetaliasesrequestnoauthorizedindices, testresolvealiasesexclusionwildcardsgetaliasesrequest also test for the changes introduced herein, so no additional test is required.
public <T> void declareFieldArray(BiConsumer<Value, List<T>> consumer, ContextParser<Context, T> itemParser, ParseField field, ValueType type) { declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type); } /** * Declares a set of fields that are required for parsing to succeed. Only one of the values * provided per String[] must be matched. E.g. * * declareRequiredFieldSet(new String[]{"foo", "bar"}); means "foo" or "bar" (or both) fields must be * specified by the user. If neither of those fields are configured, an exception will be thrown. * * Multiple required sets can be configured: * * declareRequiredFieldSet(new String[]{"foo", "bar"}); * declareRequiredFieldSet(new String[]{"bizz", "buzz"}); * * requires that ("foo" OR "bar") AND ("bizz" OR "buzz") are configured by the user. * In JSON, it means any of these combinations are acceptable * * - {"foo":"...", "bizz": "..."} * - {"bar":"...", "bizz": "..."} * - {"foo":"...", "buzz": "..."} * - {"bar":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "..."} * - {"foo":"...", "bar":"...", "buzz": "..."} * - {"foo":"...", "bizz":"...", "buzz": "..."} * - {"bar":"...", "bizz":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "...", "buzz": "..."} * * The following would however be rejected: * * - {"foo":"..."} Missing (bizz OR buzz) * - {"bar":"..."} Missing (bizz OR buzz) * - {"bizz": "..."} Missing (foo OR bar) * - {"buzz": "..."} Missing (foo OR bar) * - {"foo":"...", "bar": "..."} Missing (bizz OR buzz) * - {"bizz":"...", "buzz": "..."} Missing (foo OR bar) * - {"unrelated":"..."}	the first sentence of javadocs are the "description", and separated from the rest, so i would move the e.g. down to the next paragraph.
public <T> void declareFieldArray(BiConsumer<Value, List<T>> consumer, ContextParser<Context, T> itemParser, ParseField field, ValueType type) { declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type); } /** * Declares a set of fields that are required for parsing to succeed. Only one of the values * provided per String[] must be matched. E.g. * * declareRequiredFieldSet(new String[]{"foo", "bar"}); means "foo" or "bar" (or both) fields must be * specified by the user. If neither of those fields are configured, an exception will be thrown. * * Multiple required sets can be configured: * * declareRequiredFieldSet(new String[]{"foo", "bar"}); * declareRequiredFieldSet(new String[]{"bizz", "buzz"}); * * requires that ("foo" OR "bar") AND ("bizz" OR "buzz") are configured by the user. * In JSON, it means any of these combinations are acceptable * * - {"foo":"...", "bizz": "..."} * - {"bar":"...", "bizz": "..."} * - {"foo":"...", "buzz": "..."} * - {"bar":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "..."} * - {"foo":"...", "bar":"...", "buzz": "..."} * - {"foo":"...", "bizz":"...", "buzz": "..."} * - {"bar":"...", "bizz":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "...", "buzz": "..."} * * The following would however be rejected: * * - {"foo":"..."} Missing (bizz OR buzz) * - {"bar":"..."} Missing (bizz OR buzz) * - {"bizz": "..."} Missing (foo OR bar) * - {"buzz": "..."} Missing (foo OR bar) * - {"foo":"...", "bar": "..."} Missing (bizz OR buzz) * - {"bizz":"...", "buzz": "..."} Missing (foo OR bar) * - {"unrelated":"..."}	instead of "or both", maybe say "at least one of"?
public <T> void declareFieldArray(BiConsumer<Value, List<T>> consumer, ContextParser<Context, T> itemParser, ParseField field, ValueType type) { declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type); } /** * Declares a set of fields that are required for parsing to succeed. Only one of the values * provided per String[] must be matched. E.g. * * declareRequiredFieldSet(new String[]{"foo", "bar"}); means "foo" or "bar" (or both) fields must be * specified by the user. If neither of those fields are configured, an exception will be thrown. * * Multiple required sets can be configured: * * declareRequiredFieldSet(new String[]{"foo", "bar"}); * declareRequiredFieldSet(new String[]{"bizz", "buzz"}); * * requires that ("foo" OR "bar") AND ("bizz" OR "buzz") are configured by the user. * In JSON, it means any of these combinations are acceptable * * - {"foo":"...", "bizz": "..."} * - {"bar":"...", "bizz": "..."} * - {"foo":"...", "buzz": "..."} * - {"bar":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "..."} * - {"foo":"...", "bar":"...", "buzz": "..."} * - {"foo":"...", "bizz":"...", "buzz": "..."} * - {"bar":"...", "bizz":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "...", "buzz": "..."} * * The following would however be rejected: * * - {"foo":"..."} Missing (bizz OR buzz) * - {"bar":"..."} Missing (bizz OR buzz) * - {"bizz": "..."} Missing (foo OR bar) * - {"buzz": "..."} Missing (foo OR bar) * - {"foo":"...", "bar": "..."} Missing (bizz OR buzz) * - {"bizz":"...", "buzz": "..."} Missing (foo OR bar) * - {"unrelated":"..."}	can you describe this in prose instead of boolean logic? here it looks like a query, so it is a little confusing to understand. i would also use something like "keys are present" instead of "configured"?
public <T> void declareFieldArray(BiConsumer<Value, List<T>> consumer, ContextParser<Context, T> itemParser, ParseField field, ValueType type) { declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type); } /** * Declares a set of fields that are required for parsing to succeed. Only one of the values * provided per String[] must be matched. E.g. * * declareRequiredFieldSet(new String[]{"foo", "bar"}); means "foo" or "bar" (or both) fields must be * specified by the user. If neither of those fields are configured, an exception will be thrown. * * Multiple required sets can be configured: * * declareRequiredFieldSet(new String[]{"foo", "bar"}); * declareRequiredFieldSet(new String[]{"bizz", "buzz"}); * * requires that ("foo" OR "bar") AND ("bizz" OR "buzz") are configured by the user. * In JSON, it means any of these combinations are acceptable * * - {"foo":"...", "bizz": "..."} * - {"bar":"...", "bizz": "..."} * - {"foo":"...", "buzz": "..."} * - {"bar":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "..."} * - {"foo":"...", "bar":"...", "buzz": "..."} * - {"foo":"...", "bizz":"...", "buzz": "..."} * - {"bar":"...", "bizz":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "...", "buzz": "..."} * * The following would however be rejected: * * - {"foo":"..."} Missing (bizz OR buzz) * - {"bar":"..."} Missing (bizz OR buzz) * - {"bizz": "..."} Missing (foo OR bar) * - {"buzz": "..."} Missing (foo OR bar) * - {"foo":"...", "bar": "..."} Missing (bizz OR buzz) * - {"bizz":"...", "buzz": "..."} Missing (foo OR bar) * - {"unrelated":"..."}	this isn't going to render properly. we should use actual lists, along with {@code ...} and proper escapes for the curlies.
public <T> void declareFieldArray(BiConsumer<Value, List<T>> consumer, ContextParser<Context, T> itemParser, ParseField field, ValueType type) { declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type); } /** * Declares a set of fields that are required for parsing to succeed. Only one of the values * provided per String[] must be matched. E.g. * * declareRequiredFieldSet(new String[]{"foo", "bar"}); means "foo" or "bar" (or both) fields must be * specified by the user. If neither of those fields are configured, an exception will be thrown. * * Multiple required sets can be configured: * * declareRequiredFieldSet(new String[]{"foo", "bar"}); * declareRequiredFieldSet(new String[]{"bizz", "buzz"}); * * requires that ("foo" OR "bar") AND ("bizz" OR "buzz") are configured by the user. * In JSON, it means any of these combinations are acceptable * * - {"foo":"...", "bizz": "..."} * - {"bar":"...", "bizz": "..."} * - {"foo":"...", "buzz": "..."} * - {"bar":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "..."} * - {"foo":"...", "bar":"...", "buzz": "..."} * - {"foo":"...", "bizz":"...", "buzz": "..."} * - {"bar":"...", "bizz":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "...", "buzz": "..."} * * The following would however be rejected: * * - {"foo":"..."} Missing (bizz OR buzz) * - {"bar":"..."} Missing (bizz OR buzz) * - {"bizz": "..."} Missing (foo OR bar) * - {"buzz": "..."} Missing (foo OR bar) * - {"foo":"...", "bar": "..."} Missing (bizz OR buzz) * - {"bizz":"...", "buzz": "..."} Missing (foo OR bar) * - {"unrelated":"..."}	maybe use prose? missing on of "bizz" or "buzz" key
public <T> void declareFieldArray(BiConsumer<Value, List<T>> consumer, ContextParser<Context, T> itemParser, ParseField field, ValueType type) { declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type); } /** * Declares a set of fields that are required for parsing to succeed. Only one of the values * provided per String[] must be matched. E.g. * * declareRequiredFieldSet(new String[]{"foo", "bar"}); means "foo" or "bar" (or both) fields must be * specified by the user. If neither of those fields are configured, an exception will be thrown. * * Multiple required sets can be configured: * * declareRequiredFieldSet(new String[]{"foo", "bar"}); * declareRequiredFieldSet(new String[]{"bizz", "buzz"}); * * requires that ("foo" OR "bar") AND ("bizz" OR "buzz") are configured by the user. * In JSON, it means any of these combinations are acceptable * * - {"foo":"...", "bizz": "..."} * - {"bar":"...", "bizz": "..."} * - {"foo":"...", "buzz": "..."} * - {"bar":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "..."} * - {"foo":"...", "bar":"...", "buzz": "..."} * - {"foo":"...", "bizz":"...", "buzz": "..."} * - {"bar":"...", "bizz":"...", "buzz": "..."} * - {"foo":"...", "bar":"...", "bizz": "...", "buzz": "..."} * * The following would however be rejected: * * - {"foo":"..."} Missing (bizz OR buzz) * - {"bar":"..."} Missing (bizz OR buzz) * - {"bizz": "..."} Missing (foo OR bar) * - {"buzz": "..."} Missing (foo OR bar) * - {"foo":"...", "bar": "..."} Missing (bizz OR buzz) * - {"bizz":"...", "buzz": "..."} Missing (foo OR bar) * - {"unrelated":"..."}	seems like we want a table here.
public void handle(boolean httpRedirect, String payload, Collection<String> allowedSamlRequestIds) { final ParsedQueryString parsed = parseQueryStringAndValidateSignature(payload, "SAMLResponse"); if (httpRedirect && parsed.hasSignature == false) { throw samlException("URL is not signed, but is required for HTTP-Redirect binding"); } else if (httpRedirect == false && parsed.hasSignature) { throw samlException("URL is signed, but binding is HTTP-POST"); } final Element root; if (httpRedirect) { logger.info("Process SAML LogoutResponse with HTTP-Redirect binding"); root = parseSamlMessage(inflate(decodeBase64(parsed.samlMessage))); } else { logger.info("Process SAML LogoutResponse with HTTP-POST binding"); root = parseSamlMessage(decodeBase64(parsed.samlMessage)); } if (LOGOUT_RESPONSE_TAG_NAME.equals(root.getLocalName()) && SAML_NAMESPACE.equals(root.getNamespaceURI())) { final LogoutResponse logoutResponse = buildXmlObject(root, LogoutResponse.class); if (logoutResponse == null) { throw samlException("Cannot convert element {} into LogoutResponse object", root); } if (httpRedirect == false && logoutResponse.getSignature() == null) { throw samlException("LogoutResponse is not signed, but a signature is required for HTTP-Post binding"); } else if (httpRedirect == false) { validateSignature(logoutResponse.getSignature()); } checkInResponseTo(logoutResponse, allowedSamlRequestIds); checkStatus(logoutResponse.getStatus()); checkIssuer(logoutResponse.getIssuer(), logoutResponse); checkResponseDestination(logoutResponse, getSpConfiguration().getLogoutUrl()); } else { throw samlException("SAML content [{}] should have a root element of Namespace=[{}] Tag=[{}]", root, SAML_NAMESPACE, LOGOUT_RESPONSE_TAG_NAME); } }	given that i had 4 comments in 10 lines ( woke up picky today, apologies :pray: ) think we could do this is a simpler way, something like java final element root; if (httpredirect) { logger.debug("process saml logoutresponse with http-redirect binding"); final parsedquerystring parsed = parsequerystringandvalidatesignature(payload, "samlresponse"); if (parsed.hassignature == false){ throw samlexception("saml logoutresponse messages must be signed"); } root = parsesamlmessage(inflate(decodebase64(parsed.samlmessage))); } else { logger.info("process saml logoutresponse with http-post binding"); root = parsesamlmessage(decodebase64(payload)); }
public void handle(boolean httpRedirect, String payload, Collection<String> allowedSamlRequestIds) { final ParsedQueryString parsed = parseQueryStringAndValidateSignature(payload, "SAMLResponse"); if (httpRedirect && parsed.hasSignature == false) { throw samlException("URL is not signed, but is required for HTTP-Redirect binding"); } else if (httpRedirect == false && parsed.hasSignature) { throw samlException("URL is signed, but binding is HTTP-POST"); } final Element root; if (httpRedirect) { logger.info("Process SAML LogoutResponse with HTTP-Redirect binding"); root = parseSamlMessage(inflate(decodeBase64(parsed.samlMessage))); } else { logger.info("Process SAML LogoutResponse with HTTP-POST binding"); root = parseSamlMessage(decodeBase64(parsed.samlMessage)); } if (LOGOUT_RESPONSE_TAG_NAME.equals(root.getLocalName()) && SAML_NAMESPACE.equals(root.getNamespaceURI())) { final LogoutResponse logoutResponse = buildXmlObject(root, LogoutResponse.class); if (logoutResponse == null) { throw samlException("Cannot convert element {} into LogoutResponse object", root); } if (httpRedirect == false && logoutResponse.getSignature() == null) { throw samlException("LogoutResponse is not signed, but a signature is required for HTTP-Post binding"); } else if (httpRedirect == false) { validateSignature(logoutResponse.getSignature()); } checkInResponseTo(logoutResponse, allowedSamlRequestIds); checkStatus(logoutResponse.getStatus()); checkIssuer(logoutResponse.getIssuer(), logoutResponse); checkResponseDestination(logoutResponse, getSpConfiguration().getLogoutUrl()); } else { throw samlException("SAML content [{}] should have a root element of Namespace=[{}] Tag=[{}]", root, SAML_NAMESPACE, LOGOUT_RESPONSE_TAG_NAME); } }	buildxmlobject doesn't return null so i think we can remove this "check and throw"
private static String sslContextAlgorithm(List<String> supportedProtocols) { if (supportedProtocols.isEmpty()) { throw new IllegalArgumentException("no SSL/TLS protocols have been configured"); } for (Entry<String, String> entry : ORDERED_PROTOCOL_ALGORITHM_MAP.entrySet()) { if (supportedProtocols.contains(entry.getKey())) { return entry.getValue(); } } throw new IllegalArgumentException("no supported SSL/TLS protocol was found in the configured supported protocols: " + supportedProtocols); }	i really like we got rid of this switch :)
public StringMatcher build() { if (allText.isEmpty()) { return MATCH_NOTHING; } final String description = describe(allText); if (nonExactMatch.contains("*")) { return new StringMatcher(description, s -> true); } if (exactMatch.isEmpty()) { return new StringMatcher(description, buildAutomataPredicate(nonExactMatch)); } if (nonExactMatch.isEmpty()) { return new StringMatcher(description, buildExactMatchPredicate(exactMatch)); } final Predicate<String> predicate = buildExactMatchPredicate(exactMatch).or(buildAutomataPredicate(nonExactMatch)); return new StringMatcher(description, predicate); }	since we are trying to squeeze every bit for performance, i wonder whether it could help a tiny bit if we perform this check in the include(string pattern) method. it already checks for wildcard and should be easy to tweak it to flag for seeing a match-all pattern.
public interface Factory extends ScriptFactory { LeafFactory newFactory(String fieldName, Map<String, Object> params, SearchLookup searchLookup); } public interface LeafFactory { GeoPointFieldScript newInstance(LeafReaderContext ctx); } public static final Factory PARSE_FROM_SOURCE = (field, params, lookup) -> (LeafFactory) ctx -> new GeoPointFieldScript( field, params, lookup, ctx ) { private final GeoPoint scratch = new GeoPoint(); @Override public void execute() { Object v = XContentMapValues.extractValue(field, leafSearchLookup.source().loadSourceIfNeeded()); GeoUtils.parseGeoPoint(v, scratch, true); emit(scratch.lat(), scratch.lon()); } }; public GeoPointFieldScript(String fieldName, Map<String, Object> params, SearchLookup searchLookup, LeafReaderContext ctx) { super(fieldName, params, searchLookup, ctx); } protected void emit(double lat, double lon) { int latitudeEncoded = encodeLatitude(lat); int longitudeEncoded = encodeLongitude(lon); emit(Long.valueOf((((long) latitudeEncoded) << 32) | (longitudeEncoded & 0xFFFFFFFFL))); } public static class Emit { private final GeoPointFieldScript script; public Emit(GeoPointFieldScript script) { this.script = script; } public void emit(double lat, double lon) { script.emit(lat, lon); } }	i have another question, is that ok? not sure if execute method needs to be thread safe.
@Override protected boolean matches(String pattern, boolean caseInsensitive, SearchExecutionContext context) { if (caseInsensitive) { pattern = Strings.toLowercaseAscii(pattern); } String tierPreference = DataTierAllocationDecider.INDEX_ROUTING_PREFER_SETTING.get(context.getIndexSettings().getSettings()); if (Strings.hasText(tierPreference) == false) { return false; } // Tier preference can be a comma-delimited list of tiers, ordered by preference // It was decided we should only test the first of these potentially multiple preferences. String firstPreference = tierPreference.split(",")[0].trim(); return Regex.simpleMatch(pattern, firstPreference); }	this check is a little unusual, but it matches how we test for the setting in other places like datatierallocationdecider.
@Override public void clusterChanged(ClusterChangedEvent event) { if (!event.routingTableChanged()) { return; } if (event.state().blocks().disableStatePersistence()) { return; } for (IndexRoutingTable indexRoutingTable : event.state().routingTable()) { IndexSettings indexSettings = new IndexSettings(event.state().getMetaData().index(indexRoutingTable.index()), settings, Collections.EMPTY_LIST); // Note, closed indices will not have any routing information, so won't be deleted for (IndexShardRoutingTable indexShardRoutingTable : indexRoutingTable) { if (shardCanBeDeleted(event.state(), indexShardRoutingTable)) { ShardId shardId = indexShardRoutingTable.shardId(); if (indicesService.canDeleteShardContent(shardId, indexSettings)) { deleteShardIfExistElseWhere(event.state(), indexShardRoutingTable); } } } } }	nit: collections.emptylist() to avoid unchecked assignment warnings.
public UnaryOperator<Map<String, IndexTemplateMetaData>> getIndexTemplateMetaDataUpgrader() { return templates -> { templates.keySet().removeIf(OLD_LOGSTASH_INDEX_NAME::equals); TemplateUtils.loadTemplateIntoMap("/" + LOGSTASH_TEMPLATE_FILE_NAME + ".json", templates, LOGSTASH_INDEX_NAME, Version.CURRENT.toString(), TEMPLATE_VERSION_PATTERN, LogManager.getLogger(Logstash.class)); return templates; }; }	this might be a bad idea because i could easily see someone creating a template named this, but i'm not sure if we want to leave it hanging around either.
@Override protected void nodeOperation(AllocatedPersistentTask task, StartDataFrameAnalyticsAction.TaskParams params, PersistentTaskState state) { LOGGER.info("[{}] Starting data frame analytics", params.getId()); DataFrameAnalyticsTaskState analyticsTaskState = (DataFrameAnalyticsTaskState) state; // If we are "stopping" there is nothing to do if (analyticsTaskState != null && analyticsTaskState.getState() == DataFrameAnalyticsState.STOPPING) { return; } if (analyticsTaskState == null) { DataFrameAnalyticsTaskState startedState = new DataFrameAnalyticsTaskState(DataFrameAnalyticsState.STARTED, task.getAllocationId()); task.updatePersistentTaskState(startedState, ActionListener.wrap( response -> manager.execute((DataFrameAnalyticsTask) task, DataFrameAnalyticsState.STARTED), task::markAsFailed)); } else { manager.execute((DataFrameAnalyticsTask)task, analyticsTaskState.getState()); } }	from what i can tell, nothing is setting the state to stopping. however, it makes sense to me that we should not attempt to execute if we are in the middle of stopping.
* @param config The config from which to create the extractor factory * @param listener The listener to notify on creation or failure */ public static void create(Client client, DataFrameAnalyticsConfig config, ActionListener<DataFrameDataExtractorFactory> listener) { List<DataFrameAnalysis> analyses = DataFrameAnalysesUtils.readAnalyses(config.getAnalyses()); Set<String> resultFields = analyses.stream().flatMap(analysis -> analysis.getResultFields().stream()).collect(Collectors.toSet()); validateIndexAndExtractFields(client, config.getHeaders(), config.getDest(), config.getAnalysesFields(), resultFields, ActionListener.wrap( extractedFields -> listener.onResponse( new DataFrameDataExtractorFactory(client, config.getId(), config.getDest(), extractedFields, config.getHeaders())), listener::onFailure )); }	we could extract those 2 lines in a method resolveresultfields and reuse it further down.
private boolean shouldUseOrdinals(Aggregator parent, ValuesSource valuesSource, AggregationContext context) { // if there is a parent bucket aggregator the number of instances of this aggregator is going to be unbounded and most instances // may only aggregate few documents, so don't use ordinals if (hasParentBucketAggregator(parent)) { return false; } // be defensive: if the number of unique values is unknown, don't use ordinals final long maxNumUniqueValues = valuesSource.metaData().maxAtomicUniqueValuesCount(); if (maxNumUniqueValues == -1) { return false; } // if the number of unique values is high compared to the document count, then ordinals are only going to make things slower int maxDoc = 0; for (AtomicReaderContext ctx : context.searchContext().searcher().getTopReaderContext().reader().leaves()) { maxDoc = Math.max(maxDoc, ctx.reader().maxDoc()); } if (maxNumUniqueValues > (maxDoc >>> 4)) { return false; } return true; }	maybe 'maxdoc >>> 4' should be configurable?
protected void handleIncomingClusterStateRequest(BytesTransportRequest request, TransportChannel channel) throws IOException { Compressor compressor = CompressorFactory.compressor(request.bytes()); StreamInput in = request.bytes().streamInput(); try { if (compressor != null) { in = compressor.streamInput(in); } in = new NamedWriteableAwareStreamInput(in, namedWriteableRegistry); in.setVersion(request.version()); synchronized (lastSeenClusterStateMutex) { final ClusterState incomingState; // If true we received full cluster state - otherwise diffs if (in.readBoolean()) { incomingState = ClusterState.readFrom(in, transportService.getLocalNode()); fullClusterStateReceivedCount.incrementAndGet(); logger.debug("received full cluster state version [{}] with size [{}]", incomingState.version(), request.bytes().length()); } else if (lastSeenClusterState != null) { Diff<ClusterState> diff = ClusterState.readDiffFrom(in, lastSeenClusterState.nodes().getLocalNode()); incomingState = diff.apply(lastSeenClusterState); compatibleClusterStateDiffReceivedCount.incrementAndGet(); logger.debug("received diff cluster state version [{}] with uuid [{}], diff size [{}]", incomingState.version(), incomingState.stateUUID(), request.bytes().length()); } else { logger.debug("received diff for but don't have any local cluster state - requesting full state"); throw new IncompatibleClusterStateVersionException("have no local cluster state"); } incomingClusterStateListener.onIncomingClusterState(incomingState); lastSeenClusterState = incomingState; } } catch (IncompatibleClusterStateVersionException e) { incompatibleClusterStateDiffReceivedCount.incrementAndGet(); throw e; } catch (Exception e) { logger.warn("unexpected error while deserializing an incoming cluster state", e); throw e; } finally { IOUtils.close(in); } channel.sendResponse(TransportResponse.Empty.INSTANCE); }	this will also catch exceptions when calling incomingclusterstatelistener.onincomingclusterstate, which in return calls zendiscovery.validateincomingstate. i think it's confusing to catch these as well here and erroneously reporting them here as deserialization errors. maybe it would be better to reduce the scope of the catch clause...
public void testTranslogRecoveryWithMultipleGenerations() throws IOException { final int docs = randomIntBetween(1, 4096); final List<Long> seqNos = LongStream.range(0, docs).boxed().collect(Collectors.toList()); Collections.shuffle(seqNos); engine.close(); Engine initialEngine = null; try { final AtomicInteger counter = new AtomicInteger(); initialEngine = new InternalEngine(copy(engine.config(), EngineConfig.OpenMode.CREATE_INDEX_AND_TRANSLOG)) { @Override public SequenceNumbersService seqNoService() { return new SequenceNumbersService( engine.shardId, engine.config().getIndexSettings(), SequenceNumbersService.NO_OPS_PERFORMED, SequenceNumbersService.NO_OPS_PERFORMED, SequenceNumbersService.UNASSIGNED_SEQ_NO) { @Override public long generateSeqNo() { return seqNos.get(counter.getAndIncrement()); } }; } }; for (int i = 0; i < docs; i++) { final String id = Integer.toString(i); final ParsedDocument doc = testParsedDocument(id, "test", null, testDocumentWithTextField(), SOURCE, null); initialEngine.index(indexForDoc(doc)); if (rarely()) { initialEngine.getTranslog().rollGeneration(); } else if (rarely()) { initialEngine.flush(); } } } finally { IOUtils.close(initialEngine); } Engine recoveringEngine = null; try { recoveringEngine = new InternalEngine(copy(initialEngine.config(), EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG)); recoveringEngine.recoverFromTranslog(); try (Engine.Searcher searcher = recoveringEngine.acquireSearcher("test")) { TopDocs topDocs = searcher.searcher().search(new MatchAllDocsQuery(), docs); assertEquals(docs, topDocs.totalHits); } } finally { IOUtils.close(recoveringEngine); } }	would be great to assert on the number of docs recovered. it's hard but i think it's good to keep a tight control of this one - it's complicated code and we need to know it keeps on doing what we expect it to.
public void testIgnoreMalformedValues() throws IOException { DocumentMapper ignoreMapper = createDocumentMapper(fieldMapping(b -> { b.field("type", "geo_shape"); b.field("ignore_malformed", true); })); DocumentMapper failMapper = createDocumentMapper(fieldMapping(b -> { b.field("type", "geo_shape"); b.field("ignore_malformed", false); })); { BytesReference arrayedDoc = BytesReference.bytes( XContentFactory.jsonBuilder().startObject().field("field", "Bad shape").endObject() ); SourceToParse sourceToParse = new SourceToParse("1", arrayedDoc, XContentType.JSON); ParsedDocument document = ignoreMapper.parse(sourceToParse); assertThat(document.docs().get(0).getFields("field").length, equalTo(0)); MapperParsingException exception = expectThrows(MapperParsingException.class, () -> failMapper.parse(sourceToParse)); assertThat(exception.getCause().getMessage(), containsString("Unknown geometry type: bad")); } { BytesReference arrayedDoc = BytesReference.bytes( XContentFactory.jsonBuilder() .startObject() .field( "field", "POLYGON ((18.9401790919516 -33.9681188869036, 18.9401790919516 -33.9681188869036, 18.9401790919517 " + "-33.9681188869036, 18.9401790919517 -33.9681188869036, 18.9401790919516 -33.9681188869036))" ) .endObject() ); SourceToParse sourceToParse = new SourceToParse("1", arrayedDoc, XContentType.JSON); ParsedDocument document = ignoreMapper.parse(sourceToParse); assertThat(document.docs().get(0).getFields("field").length, equalTo(0)); MapperParsingException exception = expectThrows(MapperParsingException.class, () -> failMapper.parse(sourceToParse)); assertThat(exception.getCause().getMessage(), containsString("Unable to Tessellate shape")); } }	since the pr description seems to indicate this change leads to better error messages, i wonder if there is a possible test that could be written to show that case? the test here seems to have a less useful error message for the end-user.
@Override public void shardStarted(ShardRouting initializingShard, ShardRouting startedShard) { assert Objects.equals(initializingShard.allocationId().getId(), startedShard.allocationId().getId()) : "initializingShard.allocationId [" + initializingShard.allocationId().getId() + "] and startedShard.allocationId [" + startedShard.allocationId().getId() + "] have to have the same"; changes(startedShard.shardId()).addedAllocationIds.add(startedShard.allocationId().getId()); if (startedShard.primary() // started shard has to have null recoverySource; have to pick up recoverySource from its initializing state && (initializingShard.recoverySource() == RecoverySource.ExistingStoreRecoverySource.FORCE_STALE_PRIMARY_INSTANCE || initializingShard.recoverySource() instanceof RecoverySource.SnapshotRecoverySource)) { Updates updates = changes(startedShard.shardId()); updates.removedAllocationIds.add(RecoverySource.ExistingStoreRecoverySource.FORCED_ALLOCATION_ID); } }	i think this is a left over? we're separating the s&r part to another pr.
@Override public void shardStarted(ShardRouting initializingShard, ShardRouting startedShard) { assert Objects.equals(initializingShard.allocationId().getId(), startedShard.allocationId().getId()) : "initializingShard.allocationId [" + initializingShard.allocationId().getId() + "] and startedShard.allocationId [" + startedShard.allocationId().getId() + "] have to have the same"; changes(startedShard.shardId()).addedAllocationIds.add(startedShard.allocationId().getId()); if (startedShard.primary() // started shard has to have null recoverySource; have to pick up recoverySource from its initializing state && (initializingShard.recoverySource() == RecoverySource.ExistingStoreRecoverySource.FORCE_STALE_PRIMARY_INSTANCE || initializingShard.recoverySource() instanceof RecoverySource.SnapshotRecoverySource)) { Updates updates = changes(startedShard.shardId()); updates.removedAllocationIds.add(RecoverySource.ExistingStoreRecoverySource.FORCED_ALLOCATION_ID); } }	we can capture this one earlier and use it in line 75?
@Override public void handleException(TransportException exp) { logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, transportReplicaAction); if (ignoreReplicaException(exp)) { onReplicaFailure(nodeId, exp); } else { String message = String.format(Locale.ROOT, "failed to perform %s on replica on node %s", transportReplicaAction, node); logger.warn("[{}] {}", exp, shardId, message); shardStateAction.shardFailed( shard, indexShardReference.routingEntry(), message, exp, new ShardStateAction.Listener() { @Override public void onSuccess() { onReplicaFailure(nodeId, exp); } @Override public void onFailure(Throwable shardFailedError) { logger.error("[{}] catastrophic error while failing replica shard [{}] for [{}]", shardFailedError, shardId, shard, exp); if (shardFailedError instanceof ShardStateAction.NoLongerPrimaryShardException) { // we are no longer the primary, fail ourselves and start over ShardRouting primaryShard = indexShardReference.routingEntry(); String message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard [%s] for [%s]", primaryShard, shard, exp); shardStateAction.shardFailed(primaryShard, primaryShard, message, shardFailedError, new ShardStateAction.Listener() { @Override public void onSuccess() { if (clusterService.state().version() == clusterStateVersion) { new ClusterStateObserver(clusterService, null, logger, threadPool.getThreadContext()).waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { // nothing to do, we just needed a new cluster state if (logger.isTraceEnabled()) { logger.trace("previous cluster state version [{}], new cluster state [{}]", clusterStateVersion, state.prettyPrint()); } } @Override public void onClusterServiceClose() { logger.warn("[{}] node [{}] shutdown while waiting to fail demoted primary shard [{}] subsequent to [{}] while failing replica shard [{}]", shardId, clusterService.localNode(), primaryShard, exp, shard); forceFinishAsFailed(new NodeClosedException(clusterService.localNode())); } @Override public void onTimeout(TimeValue timeout) { // we wait indefinitely } }); } new ReroutePhase(null, request, new ActionListener<Response>() { @Override public void onResponse(Response response) { Releasables.close(indexShardReference); try { channel.sendResponse(response); } catch (IOException channelException) { logger.warn("[{}] failed to send response back to client for action [" + transportReplicaAction + "] after failing demoted primary shard [{}]", channelException, shardId, response, primaryShard); } } @Override public void onFailure(Throwable reroutePhaseError) { logger.error("[{}] reroute phase failed while retrying request after failing demoted primary shard [{}] subsequent to failing shard [{}] caused by [{}]", reroutePhaseError, shardId, primaryShard, shard, exp); forceFinishAsFailed(reroutePhaseError); } }).run(); } @Override public void onFailure(Throwable primaryShardFailedError) { logger.error("[{}] failed to fail demoted primary shard [{}] subsequent to failing shard [{}] caused by [{}]", primaryShardFailedError, shardId, primaryShard, shard, exp); forceFinishAsFailed(primaryShardFailedError); } }); } else { forceFinishAsFailed(shardFailedError); } } } ); } }	this is already logged in the shard state action
@Override public void handleException(TransportException exp) { logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, transportReplicaAction); if (ignoreReplicaException(exp)) { onReplicaFailure(nodeId, exp); } else { String message = String.format(Locale.ROOT, "failed to perform %s on replica on node %s", transportReplicaAction, node); logger.warn("[{}] {}", exp, shardId, message); shardStateAction.shardFailed( shard, indexShardReference.routingEntry(), message, exp, new ShardStateAction.Listener() { @Override public void onSuccess() { onReplicaFailure(nodeId, exp); } @Override public void onFailure(Throwable shardFailedError) { logger.error("[{}] catastrophic error while failing replica shard [{}] for [{}]", shardFailedError, shardId, shard, exp); if (shardFailedError instanceof ShardStateAction.NoLongerPrimaryShardException) { // we are no longer the primary, fail ourselves and start over ShardRouting primaryShard = indexShardReference.routingEntry(); String message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard [%s] for [%s]", primaryShard, shard, exp); shardStateAction.shardFailed(primaryShard, primaryShard, message, shardFailedError, new ShardStateAction.Listener() { @Override public void onSuccess() { if (clusterService.state().version() == clusterStateVersion) { new ClusterStateObserver(clusterService, null, logger, threadPool.getThreadContext()).waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { // nothing to do, we just needed a new cluster state if (logger.isTraceEnabled()) { logger.trace("previous cluster state version [{}], new cluster state [{}]", clusterStateVersion, state.prettyPrint()); } } @Override public void onClusterServiceClose() { logger.warn("[{}] node [{}] shutdown while waiting to fail demoted primary shard [{}] subsequent to [{}] while failing replica shard [{}]", shardId, clusterService.localNode(), primaryShard, exp, shard); forceFinishAsFailed(new NodeClosedException(clusterService.localNode())); } @Override public void onTimeout(TimeValue timeout) { // we wait indefinitely } }); } new ReroutePhase(null, request, new ActionListener<Response>() { @Override public void onResponse(Response response) { Releasables.close(indexShardReference); try { channel.sendResponse(response); } catch (IOException channelException) { logger.warn("[{}] failed to send response back to client for action [" + transportReplicaAction + "] after failing demoted primary shard [{}]", channelException, shardId, response, primaryShard); } } @Override public void onFailure(Throwable reroutePhaseError) { logger.error("[{}] reroute phase failed while retrying request after failing demoted primary shard [{}] subsequent to failing shard [{}] caused by [{}]", reroutePhaseError, shardId, primaryShard, shard, exp); forceFinishAsFailed(reroutePhaseError); } }).run(); } @Override public void onFailure(Throwable primaryShardFailedError) { logger.error("[{}] failed to fail demoted primary shard [{}] subsequent to failing shard [{}] caused by [{}]", primaryShardFailedError, shardId, primaryShard, shard, exp); forceFinishAsFailed(primaryShardFailedError); } }); } else { forceFinishAsFailed(shardFailedError); } } } ); } }	i think we should fail the local shard on any unexpected error. seems like the "safe" thing to do..
@Override public void onFailure(Throwable shardFailedError) { logger.error("[{}] catastrophic error while failing replica shard [{}] for [{}]", shardFailedError, shardId, shard, exp); if (shardFailedError instanceof ShardStateAction.NoLongerPrimaryShardException) { // we are no longer the primary, fail ourselves and start over ShardRouting primaryShard = indexShardReference.routingEntry(); String message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard [%s] for [%s]", primaryShard, shard, exp); shardStateAction.shardFailed(primaryShard, primaryShard, message, shardFailedError, new ShardStateAction.Listener() { @Override public void onSuccess() { if (clusterService.state().version() == clusterStateVersion) { new ClusterStateObserver(clusterService, null, logger, threadPool.getThreadContext()).waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { // nothing to do, we just needed a new cluster state if (logger.isTraceEnabled()) { logger.trace("previous cluster state version [{}], new cluster state [{}]", clusterStateVersion, state.prettyPrint()); } } @Override public void onClusterServiceClose() { logger.warn("[{}] node [{}] shutdown while waiting to fail demoted primary shard [{}] subsequent to [{}] while failing replica shard [{}]", shardId, clusterService.localNode(), primaryShard, exp, shard); forceFinishAsFailed(new NodeClosedException(clusterService.localNode())); } @Override public void onTimeout(TimeValue timeout) { // we wait indefinitely } }); } new ReroutePhase(null, request, new ActionListener<Response>() { @Override public void onResponse(Response response) { Releasables.close(indexShardReference); try { channel.sendResponse(response); } catch (IOException channelException) { logger.warn("[{}] failed to send response back to client for action [" + transportReplicaAction + "] after failing demoted primary shard [{}]", channelException, shardId, response, primaryShard); } } @Override public void onFailure(Throwable reroutePhaseError) { logger.error("[{}] reroute phase failed while retrying request after failing demoted primary shard [{}] subsequent to failing shard [{}] caused by [{}]", reroutePhaseError, shardId, primaryShard, shard, exp); forceFinishAsFailed(reroutePhaseError); } }).run(); } @Override public void onFailure(Throwable primaryShardFailedError) { logger.error("[{}] failed to fail demoted primary shard [{}] subsequent to failing shard [{}] caused by [{}]", primaryShardFailedError, shardId, primaryShard, shard, exp); forceFinishAsFailed(primaryShardFailedError); } }); } else { forceFinishAsFailed(shardFailedError); } }	i think this should be done via indexshard#failshard (which can be exposed via indexshardreference ). will be cleaner and faster (it's local fail first, then notify the master)
@Override public void onFailure(Throwable shardFailedError) { logger.error("[{}] catastrophic error while failing replica shard [{}] for [{}]", shardFailedError, shardId, shard, exp); if (shardFailedError instanceof ShardStateAction.NoLongerPrimaryShardException) { // we are no longer the primary, fail ourselves and start over ShardRouting primaryShard = indexShardReference.routingEntry(); String message = String.format(Locale.ROOT, "primary shard [%s] was demoted while failing replica shard [%s] for [%s]", primaryShard, shard, exp); shardStateAction.shardFailed(primaryShard, primaryShard, message, shardFailedError, new ShardStateAction.Listener() { @Override public void onSuccess() { if (clusterService.state().version() == clusterStateVersion) { new ClusterStateObserver(clusterService, null, logger, threadPool.getThreadContext()).waitForNextChange(new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState state) { // nothing to do, we just needed a new cluster state if (logger.isTraceEnabled()) { logger.trace("previous cluster state version [{}], new cluster state [{}]", clusterStateVersion, state.prettyPrint()); } } @Override public void onClusterServiceClose() { logger.warn("[{}] node [{}] shutdown while waiting to fail demoted primary shard [{}] subsequent to [{}] while failing replica shard [{}]", shardId, clusterService.localNode(), primaryShard, exp, shard); forceFinishAsFailed(new NodeClosedException(clusterService.localNode())); } @Override public void onTimeout(TimeValue timeout) { // we wait indefinitely } }); } new ReroutePhase(null, request, new ActionListener<Response>() { @Override public void onResponse(Response response) { Releasables.close(indexShardReference); try { channel.sendResponse(response); } catch (IOException channelException) { logger.warn("[{}] failed to send response back to client for action [" + transportReplicaAction + "] after failing demoted primary shard [{}]", channelException, shardId, response, primaryShard); } } @Override public void onFailure(Throwable reroutePhaseError) { logger.error("[{}] reroute phase failed while retrying request after failing demoted primary shard [{}] subsequent to failing shard [{}] caused by [{}]", reroutePhaseError, shardId, primaryShard, shard, exp); forceFinishAsFailed(reroutePhaseError); } }).run(); } @Override public void onFailure(Throwable primaryShardFailedError) { logger.error("[{}] failed to fail demoted primary shard [{}] subsequent to failing shard [{}] caused by [{}]", primaryShardFailedError, shardId, primaryShard, shard, exp); forceFinishAsFailed(primaryShardFailedError); } }); } else { forceFinishAsFailed(shardFailedError); } }	i think we can do simpler by just returned a retryable exception to the reroute phase that started all of this. it will do the same thing. also - i miss the annoying "request" reset (we should open an issue to remove it). i guess it's still coming..
public void testDateTimes() throws IOException { assertQuery("SELECT CAST('2019-01-14T12:29:25.000Z' AS DATETIME)", "CAST('2019-01-14T12:29:25.000Z' AS DATETIME)", "datetime", "2019-01-14T12:29:25.000Z", 24); assertQuery("SELECT CAST(-26853765751000 AS DATETIME)", "CAST(-26853765751000 AS DATETIME)", "datetime", "1119-01-15T12:37:29.000Z", 24); assertQuery("SELECT CAST(CAST('-26853765751000' AS BIGINT) AS DATETIME)", "CAST(CAST('-26853765751000' AS BIGINT) AS DATETIME)", "datetime", "1119-01-15T12:37:29.000Z", 24); assertQuery("SELECT CAST('2019-01-14' AS DATE)", "CAST('2019-01-14' AS DATE)", "date", "2019-01-14T00:00:00.000Z", 24); assertQuery("SELECT CAST(-26853765751000 AS DATE)", "CAST(-26853765751000 AS DATE)", "date", "1119-01-15T00:00:00.000Z", 24); assertQuery("SELECT CAST('12:29:25.123Z' AS TIME)", "CAST('12:29:25.123Z' AS TIME)", "time", "12:29:25.123Z", 24); assertQuery("SELECT CAST(-26853765751000 AS TIME)", "CAST(-26853765751000 AS TIME)", "time", "12:37:29.000Z", 24); }	the tests' values don't reflect the maximum precision of the time type.
@SuppressForbidden(reason = "test reads from jar") public static InputStream readFromJarUrl(URL source) throws IOException { URLConnection con = source.openConnection(); con.setDefaultUseCaches(false); return con.getInputStream(); }	could you add a comment why this is necessary?
public void resolveAsSeparateMappings(String indexWildcard, String javaRegex, ActionListener<List<EsIndex>> listener) { GetIndexRequest getIndexRequest = createGetIndexRequest(indexWildcard); client.admin().indices().getIndex(getIndexRequest, ActionListener.wrap(getIndexResponse -> { ImmutableOpenMap<String, ImmutableOpenMap<String, MappingMetaData>> mappings = getIndexResponse.getMappings(); ImmutableOpenMap<String, List<AliasMetaData>> aliases = getIndexResponse.getAliases(); List<EsIndex> results = new ArrayList<>(mappings.size()); Pattern pattern = javaRegex != null ? Pattern.compile(javaRegex) : null; for (ObjectObjectCursor<String, ImmutableOpenMap<String, MappingMetaData>> indexMappings : mappings) { /* * We support wildcard expressions here, and it's only for commands that only perform the get index call. * We can and simply have to use the concrete index name and show that to users. * Get index against an alias with security enabled, where the user has only access to get mappings for the alias * and not the concrete index: there is a well known information leak of the concrete index name in the response. */ String concreteIndex = indexMappings.key; // take into account aliases List<AliasMetaData> aliasMetadata = aliases.get(concreteIndex); boolean matchesAlias = false; if (pattern != null && aliasMetadata != null) { for (AliasMetaData aliasMeta : aliasMetadata) { matchesAlias |= pattern.matcher(aliasMeta.alias()).matches(); } } if (pattern == null || matchesAlias || pattern.matcher(concreteIndex).matches()) { IndexResolution getIndexResult = buildGetIndexResult(concreteIndex, concreteIndex, indexMappings.value); if (getIndexResult.isValid()) { results.add(getIndexResult.get()); } } } results.sort(Comparator.comparing(EsIndex::name)); listener.onResponse(results); }, listener::onFailure)); }	shouldn't we exit on first occurrence of true?
@Override @SuppressWarnings("unchecked") protected <T> T compileScript(Script script, ScriptContext<T> context) { if (script.getIdOrCode().equals("serializer_test")) { return (T)serializableScript(); } if (script.getIdOrCode().equals("throws")) { return (T)errorThrowingScript(); } if (script.getIdOrCode().equals("single-valued")) { return (T)singleValueScript(); } if (script.getIdOrCode().equals("multi-valued")) { return (T)multipleValuesScript(); } throw new UnsupportedOperationException("Unknown script " + script.getIdOrCode()); }	nit: i definitely prefer a space here...
private void authenticateWithCache(UsernamePasswordToken token, ActionListener<AuthenticationResult> listener) { try { final AtomicBoolean createdAndStartedFuture = new AtomicBoolean(false); final ListenableFuture<Tuple<AuthenticationResult, UserWithHash>> future = cache.computeIfAbsent(token.principal(), k -> { final ListenableFuture<Tuple<AuthenticationResult, UserWithHash>> created = new ListenableFuture<>(); if (createdAndStartedFuture.compareAndSet(false, true) == false) { throw new IllegalStateException("something else already started this. how?"); } return created; }); if (createdAndStartedFuture.get()) { doAuthenticate(token, ActionListener.wrap(result -> { if (result.isAuthenticated()) { final User user = result.getUser(); final UserWithHash userWithHash = new UserWithHash(user, token.credentials(), hasher); future.onResponse(new Tuple<>(result, userWithHash)); } else { future.onResponse(new Tuple<>(result, null)); } }, future::onFailure)); } future.addListener(ActionListener.wrap(tuple -> { if (tuple != null) { handleResult(future, createdAndStartedFuture.get(), token, tuple, listener); } else { handleFailure(future, createdAndStartedFuture.get(), token, new IllegalStateException("unknown error authenticating"), listener); } }, e -> handleFailure(future, createdAndStartedFuture.get(), token, e, listener)), threadPool.executor(ThreadPool.Names.GENERIC)); } catch (ExecutionException e) { listener.onResponse(AuthenticationResult.unsuccessful("", e)); } }	the use of createdandstartedfuture here worries me. we need to handle the two cases (directly authenticated vs loaded from cache) differently, but this variable is a couple of steps removed from that and i fear that something like a race condition somewhere in the cache could mean that this is true, but the user was not actually authenticated with the credentials from token. perhaps we could set an atomicreference to user in the listener for doauthenticate and check whether the user being pulled from the cache matches. then were no longer testing "did we create the future" but "did we actually authenticate _this_ user object". e.g. doauthenticate(token, actionlistener.wrap(result -> { if (result.isauthenticated()) { final user user = result.getuser(); authenticateduser.set(user); final boolean wasauthenticatedwiththistoken = tuple.v2() == authenticateduser.get(); handleresult(future, wasauthenticatedwiththistoken, token, tuple, listener);
public static Object doProcess(Object input, Object substring, Object start, boolean caseInsensitive) { if (input == null) { return null; } if (input instanceof String == false && input instanceof Character == false) { throw new EqlIllegalArgumentException("A string/char is required; received [{}]", input); } if (substring == null) { return null; } if (substring instanceof String == false && substring instanceof Character == false) { throw new EqlIllegalArgumentException("A string/char is required; received [{}]", substring); } if (start != null && start instanceof Number == false) { throw new EqlIllegalArgumentException("A number is required; received [{}]", start); } int startIndex = start == null ? 0 : ((Number) start).intValue(); int result; if (caseInsensitive == false) { result = input.toString().indexOf(substring.toString(), startIndex); } else { result = input.toString().toLowerCase(Locale.ROOT).indexOf(substring.toString().toLowerCase(Locale.ROOT), startIndex); } return result < 0 ? null : result; }	flip the if to avoid double negation.
public static BetweenFunctionPipe randomBetweenFunctionPipe() { return (BetweenFunctionPipe) (new Between(randomSource(), randomStringLiteral(), randomStringLiteral(), randomStringLiteral(), randomStringLiteral(), randomBoolean()) .makePipe()); }	minor: why did the greedy param change to randomstringliteral?
@Override public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null || getClass() != obj.getClass()) { return false; } DateTimeParseProcessor other = (DateTimeParseProcessor) obj; return Objects.equals(left(), other.left()) && Objects.equals(right(), other.right()) && Objects.equals(zoneId(), other.zoneId()) && Objects.equals(parser, other.parser); }	please use super.equals() and only check the equality on parser here.
private void processMemoryUsage(MemoryUsage memoryUsage) { statsHolder.setMemoryUsage(memoryUsage); statsPersister.persistWithRetry(memoryUsage, memoryUsage::documentId); }	i think current and latest mean the same thing here. could you use the same word for method name and variable name?
@Override public DataFrameDataExtractor.Row next() { DataFrameDataExtractor.Row row = null; while (shouldHaveMatch(row) == false && hasNext()) { advanceToNextBatchIfNecessary(); row = currentDataFrameRows.get(currentDataFrameRowsIndex++); } if (shouldHaveMatch(row) == false) { throw ExceptionsHelper.serverError("no more data frame rows could be found while joining results"); } return row; }	imo the logic would be simpler (less comparisons with false) if this method was negated.
public CrossValidationSplitter create() { if (config.getAnalysis() instanceof Regression) { return createSingleClassSplitter((Regression) config.getAnalysis()); } if (config.getAnalysis() instanceof Classification) { return createStratifiedSplitter((Classification) config.getAnalysis()); } return (row) -> true; }	[nit] you can drop the parentheses here
*/ @Override public Deque<T> next() { if (!hasNext()) { throw new NoSuchElementException(); } SearchResponse searchResponse = doSearch(searchAfterFields()); if (trackTotalHits && totalHits.get() == 0) { totalHits.set(searchResponse.getHits().getTotalHits().value); } return mapHits(searchResponse); }	whats the behaviour here if the documents in the index change between batches e.g. if more are added or removed? will the value gettotalhits() change?
public void testProcess_GivenSingleBatchWithTestRows() throws IOException { givenClientHasNoFailures(); String dataDoc = "{\\\\"f_1\\\\": \\\\"foo\\\\", \\\\"f_2\\\\": 42.0}"; String[] dataValues = {"42.0"}; DataFrameDataExtractor.Row testRow = newTestRow(newHit(dataDoc), dataValues, 1); DataFrameDataExtractor.Row normalRow = newTrainingRow(newHit(dataDoc), dataValues, 2); givenDataFrameBatches(List.of(Arrays.asList(testRow, normalRow))); Map<String, Object> resultFields = new HashMap<>(); resultFields.put("a", "1"); resultFields.put("b", "2"); RowResults result = new RowResults(2, resultFields); givenProcessResults(Arrays.asList(result)); List<BulkRequest> capturedBulkRequests = bulkRequestCaptor.getAllValues(); assertThat(capturedBulkRequests.size(), equalTo(1)); BulkRequest capturedBulkRequest = capturedBulkRequests.get(0); assertThat(capturedBulkRequest.numberOfActions(), equalTo(1)); IndexRequest indexRequest = (IndexRequest) capturedBulkRequest.requests().get(0); Map<String, Object> indexedDocSource = indexRequest.sourceAsMap(); assertThat(indexedDocSource.size(), equalTo(4)); assertThat(indexedDocSource.get("f_1"), equalTo("foo")); assertThat(indexedDocSource.get("f_2"), equalTo(42.0)); assertThat(indexedDocSource.get("a"), equalTo("1")); assertThat(indexedDocSource.get("b"), equalTo("2")); }	i think there is no need to mix list.of and arrays.aslist here. could you choose one of them (probably arrays.aslist as the code below suggests you want to make it backportable to 7.x)?
public void testUpgradeDataFolder() throws IOException, InterruptedException { String node = internalCluster().startNode(); prepareCreate("test").get(); indexRandom(true, client().prepareIndex("test", "type1", "1").setSource("{}", XContentType.JSON)); String nodeId = client().admin().cluster().prepareState().get().getState().nodes().getMasterNodeId(); final Settings dataPathSettings = internalCluster().dataPathSettings(node); internalCluster().stopRandomDataNode(); // simulate older data path layout by moving data under "nodes/0" folder final List<Path> dataPaths = Environment.PATH_DATA_SETTING.get(dataPathSettings) .stream().map(PathUtils::get).collect(Collectors.toList()); dataPaths.forEach(path -> { final Path targetPath = path.resolve("nodes").resolve("0"); try { Files.createDirectories(targetPath); try (DirectoryStream<Path> stream = Files.newDirectoryStream(path)) { for (Path subPath : stream) { String fileName = subPath.getFileName().toString(); Path targetSubPath = targetPath.resolve(fileName); if (fileName.equals("nodes") == false) { Files.move(subPath, targetSubPath, StandardCopyOption.ATOMIC_MOVE); } } } IOUtils.fsync(targetPath, true); IOUtils.fsync(path, true); } catch (IOException e) { throw new UncheckedIOException(e); } }); dataPaths.forEach(path -> assertTrue(Files.exists(path.resolve("nodes")))); // create extra file/folder, and check that upgrade fails if (dataPaths.isEmpty() == false) { final Path badFile = Files.createTempFile(randomFrom(dataPaths).resolve("nodes").resolve("0"), "bad", "file"); IllegalStateException ise = expectThrows(IllegalStateException.class, () -> internalCluster().startNode(dataPathSettings)); assertThat(ise.getMessage(), containsString("unexpected file/folder encountered during data folder upgrade")); Files.delete(badFile); final Path badFolder = Files.createDirectories(randomFrom(dataPaths).resolve("nodes").resolve("0").resolve("bad-folder")); ise = expectThrows(IllegalStateException.class, () -> internalCluster().startNode(dataPathSettings)); assertThat(ise.getMessage(), containsString("unexpected folder encountered during data folder upgrad")); Files.delete(badFolder); final Path conflictingFolder = randomFrom(dataPaths).resolve("indices"); if (Files.exists(conflictingFolder) == false) { Files.createDirectories(conflictingFolder); ise = expectThrows(IllegalStateException.class, () -> internalCluster().startNode(dataPathSettings)); assertThat(ise.getMessage(), containsString("target folder already exists during data folder upgrade")); Files.delete(conflictingFolder); } } // check that upgrade works dataPaths.forEach(path -> assertTrue(Files.exists(path.resolve("nodes")))); internalCluster().startNode(dataPathSettings); dataPaths.forEach(path -> assertFalse(Files.exists(path.resolve("nodes")))); assertEquals(nodeId, client().admin().cluster().prepareState().get().getState().nodes().getMasterNodeId()); assertTrue(client().admin().indices().prepareExists("test").get().isExists()); ensureYellow("test"); assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1L); }	i don't think we care about fsyncs here.
public void testSslTrustIsReloaded() throws Exception { assumeFalse( "NPE thrown in BCFIPS JSSE - addressed in https://github.com/bcgit/bc-java/commit/" + "5aed687e17a3cd63f34373cafe92699b90076fb6#diff-8e5d8089bc0d504d93194a1e484d3950R179", inFipsJvm() ); InMemoryDirectoryServer ldapServer = randomFrom(ldapServers); InetAddress listenAddress = ldapServer.getListenAddress("ldaps"); if (listenAddress == null) { listenAddress = InetAddress.getLoopbackAddress(); } String ldapUrl = new LDAPURL( "ldaps", NetworkAddress.format(listenAddress), ldapServer.getListenPort("ldaps"), null, null, null, null ).toString(); String groupSearchBase = "o=sevenSeas"; String userTemplates = "cn={0},ou=people,o=sevenSeas"; Settings settings = Settings.builder() .put(globalSettings) .put(buildLdapSettings(ldapUrl, userTemplates, groupSearchBase, LdapSearchScope.SUB_TREE)) .build(); final Path realCa = getDataPath("/org/elasticsearch/xpack/security/authc/ldap/support/ldap-ca.crt"); final Path fakeCa = getDataPath("/org/elasticsearch/xpack/security/authc/ldap/support/smb_ca.crt"); final Environment environment = TestEnvironment.newEnvironment(settings); RealmConfig config = new RealmConfig(REALM_IDENTIFIER, settings, environment, new ThreadContext(settings)); LdapSessionFactory sessionFactory = new LdapSessionFactory(config, sslService, threadPool); String user = "Horatio Hornblower"; SecureString userPass = new SecureString("pass"); try (ResourceWatcherService resourceWatcher = new ResourceWatcherService(settings, threadPool)) { new SSLConfigurationReloader(resourceWatcher, SSLService.getSSLConfigurations(environment).values()).setSSLService(sslService); final FileTime oldModifiedTime = Files.getLastModifiedTime(ldapCaPath); Files.copy(fakeCa, ldapCaPath, StandardCopyOption.REPLACE_EXISTING); // Force the modified file to have a different modified time // Depending on the granularity of the filesystem it could otherwise be possible the newly copied file looks identical to the // old file (certificates commonly have the same file size) final FileTime newModifiedTime = Files.getLastModifiedTime(ldapCaPath); if (oldModifiedTime.equals(newModifiedTime)) { Files.setLastModifiedTime(ldapCaPath, FileTime.fromMillis(oldModifiedTime.toMillis() + 5_000)); } resourceWatcher.notifyNow(ResourceWatcherService.Frequency.HIGH); UncategorizedExecutionException e = expectThrows( UncategorizedExecutionException.class, () -> session(sessionFactory, user, userPass) ); final Throwable immediateCause = e.getCause(); assertThat(immediateCause, instanceOf(ExecutionException.class)); final Throwable sdkCause = immediateCause.getCause(); assertThat(sdkCause, instanceOf(LDAPException.class)); @SuppressWarnings(value = { "unchecked", "rawtypes" }) final Class<? extends Exception>[] expectedCause = new Class[] { SSLException.class, GeneralSecurityException.class }; final Throwable underlyingCause = ExceptionsHelper.unwrap(sdkCause, expectedCause); if (underlyingCause == null) { // This is the easiest way to have a JUnit test failure with a clear exception chain throw new AssertionError( "Unexpected root cause - expected one of " + Strings.arrayToCommaDelimitedString(expectedCause), sdkCause ); } // It's ok if an upgrade to the LDAP-SDK or JDK causes this message to change (and we need to update the test) // but we want to check that we failed for a reason we're expecting and not some unexpected network error. assertThat( underlyingCause, throwableWithMessage(anyOf(containsString("PKIX path validation failed"), containsString("peer not authenticated"))) ); Files.copy(realCa, ldapCaPath, StandardCopyOption.REPLACE_EXISTING); resourceWatcher.notifyNow(ResourceWatcherService.Frequency.HIGH); // Occasionally the reload doesn't take immediate effect so the next connection fails. assertBusy(() -> { final LdapSession session = session(sessionFactory, user, userPass); assertThat(session.userDn(), is("cn=Horatio Hornblower,ou=people,o=sevenSeas")); session.close(); }, 3, TimeUnit.SECONDS); } }	nit: why bother with the if? the comment makes it clear why this is here, and it shouldn't make a difference to just always do it.
@Override public String toString() { StringBuilder sb = new StringBuilder(); if (nodeName.length() > 0) { sb.append('{').append(nodeName).append('}'); } if (nodeId != null) { sb.append('{').append(nodeId).append('}'); } if (Strings.hasLength(hostName)) { sb.append('{').append(hostName).append('}'); } if (address != null) { sb.append('{').append(address).append('}'); } if (!attributes.isEmpty()) { sb.append(attributes); } return sb.toString(); }	why is this change? did we decide to change our usage of [] to {}?
private void bindServerBootstrap(final String name, final InetAddress hostAddress, Settings settings) { String port = settings.get("port"); PortsRange portsRange = new PortsRange(port); final AtomicReference<Exception> lastException = new AtomicReference<>(); final AtomicReference<InetSocketAddress> boundSocket = new AtomicReference<>(); boolean success = portsRange.iterate(new PortsRange.PortCallback() { @Override public boolean onPortNumber(int portNumber) { try { Channel channel = serverBootstraps.get(name).bind(new InetSocketAddress(hostAddress, portNumber)); synchronized (serverChannels) { List<Channel> list = serverChannels.get(name); if (list == null) { list = new ArrayList<>(); serverChannels.put(name, list); } list.add(channel); boundSocket.set((InetSocketAddress)channel.getLocalAddress()); } } catch (Exception e) { lastException.set(e); return false; } return true; } }); if (!success) { throw new BindTransportException("Failed to bind to [" + port + "]", lastException.get()); } if (!DEFAULT_PROFILE.equals(name)) { InetSocketAddress boundAddress = boundSocket.get(); int publishPort = settings.getAsInt("publish_port", boundAddress.getPort()); String publishHost = settings.get("publish_host", boundAddress.getHostString()); InetSocketAddress publishAddress = createPublishAddress(publishHost, publishPort); // TODO: support real multihoming with publishing. Today we use putIfAbsent so only the prioritized address is published profileBoundAddresses.putIfAbsent(name, new BoundTransportAddress(new InetSocketTransportAddress(boundAddress), new InetSocketTransportAddress(publishAddress))); } logger.info("Bound profile [{}] to address {{}}", name, NetworkAddress.format(boundSocket.get())); }	same question about the {} wrappers.
@Override public long bytesToPreallocate() { return 0; } } /** * A test aggregator that extends {@link Aggregator} instead of {@link AggregatorBase} * to avoid tripping the circuit breaker when executing on a shard. */ private static class TestAggregator extends Aggregator { private final String name; private final Aggregator parent; private TestAggregator(String name, Aggregator parent) { this.name = name; this.parent = parent; } @Override public String name() { return name; } @Override public Aggregator parent() { return parent; } @Override public Aggregator subAggregator(String name) { return null; } @Override public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException { return new InternalAggregation[] { buildEmptyAggregation() }; } @Override public InternalAggregation buildEmptyAggregation() { return new InternalMax(name(), Double.NaN, DocValueFormat.RAW, null); } @Override public void close() {} @Override public LeafBucketCollector getLeafCollector(LeafReaderContext ctx) throws IOException { throw new CollectionTerminatedException(); } @Override public ScoreMode scoreMode() { return ScoreMode.COMPLETE_NO_SCORES; } @Override public void preCollection() throws IOException {} @Override public Aggregator[] subAggregators() { throw new UnsupportedOperationException(); }	why do we change this to a null here?
void processBulkIndexIngestRequest(Task task, BulkRequest original, ActionListener<BulkResponse> listener) { long ingestStartTimeInNanos = System.nanoTime(); BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(original); ingestService.getPipelineExecutionService().executeBulkRequest(() -> bulkRequestModifier, (indexRequest, exception) -> { logger.debug((Supplier<?>) () -> new ParameterizedMessage("failed to execute pipeline [{}] for document [{}/{}/{}]", indexRequest.getPipeline(), indexRequest.index(), indexRequest.type(), indexRequest.id()), exception); bulkRequestModifier.markCurrentItemAsFailed(exception); }, (exception) -> { if (exception != null) { logger.error("failed to execute pipeline for a bulk request", exception); listener.onFailure(exception); } else { long ingestTookInMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - ingestStartTimeInNanos); BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest(); ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(ingestTookInMillis, listener); if (bulkRequest.requests().isEmpty()) { // at this stage, the transport bulk action can't deal with a bulk request with no requests, // so we stop and send an empty response back to the client. // (this will happen if pre-processing all items in the bulk failed) actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0)); } else { doExecute(task, bulkRequest, actionListener); } } }); }	would it make sense to use a bitset here instead?
void processBulkIndexIngestRequest(Task task, BulkRequest original, ActionListener<BulkResponse> listener) { long ingestStartTimeInNanos = System.nanoTime(); BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(original); ingestService.getPipelineExecutionService().executeBulkRequest(() -> bulkRequestModifier, (indexRequest, exception) -> { logger.debug((Supplier<?>) () -> new ParameterizedMessage("failed to execute pipeline [{}] for document [{}/{}/{}]", indexRequest.getPipeline(), indexRequest.index(), indexRequest.type(), indexRequest.id()), exception); bulkRequestModifier.markCurrentItemAsFailed(exception); }, (exception) -> { if (exception != null) { logger.error("failed to execute pipeline for a bulk request", exception); listener.onFailure(exception); } else { long ingestTookInMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - ingestStartTimeInNanos); BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest(); ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(ingestTookInMillis, listener); if (bulkRequest.requests().isEmpty()) { // at this stage, the transport bulk action can't deal with a bulk request with no requests, // so we stop and send an empty response back to the client. // (this will happen if pre-processing all items in the bulk failed) actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0)); } else { doExecute(task, bulkRequest, actionListener); } } }); }	can we assign bulkrequest.requests() to a local var?
void processBulkIndexIngestRequest(Task task, BulkRequest original, ActionListener<BulkResponse> listener) { long ingestStartTimeInNanos = System.nanoTime(); BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(original); ingestService.getPipelineExecutionService().executeBulkRequest(() -> bulkRequestModifier, (indexRequest, exception) -> { logger.debug((Supplier<?>) () -> new ParameterizedMessage("failed to execute pipeline [{}] for document [{}/{}/{}]", indexRequest.getPipeline(), indexRequest.index(), indexRequest.type(), indexRequest.id()), exception); bulkRequestModifier.markCurrentItemAsFailed(exception); }, (exception) -> { if (exception != null) { logger.error("failed to execute pipeline for a bulk request", exception); listener.onFailure(exception); } else { long ingestTookInMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - ingestStartTimeInNanos); BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest(); ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(ingestTookInMillis, listener); if (bulkRequest.requests().isEmpty()) { // at this stage, the transport bulk action can't deal with a bulk request with no requests, // so we stop and send an empty response back to the client. // (this will happen if pre-processing all items in the bulk failed) actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0)); } else { doExecute(task, bulkRequest, actionListener); } } }); }	actionlistener#wrap can simplify this?
void processBulkIndexIngestRequest(Task task, BulkRequest original, ActionListener<BulkResponse> listener) { long ingestStartTimeInNanos = System.nanoTime(); BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(original); ingestService.getPipelineExecutionService().executeBulkRequest(() -> bulkRequestModifier, (indexRequest, exception) -> { logger.debug((Supplier<?>) () -> new ParameterizedMessage("failed to execute pipeline [{}] for document [{}/{}/{}]", indexRequest.getPipeline(), indexRequest.index(), indexRequest.type(), indexRequest.id()), exception); bulkRequestModifier.markCurrentItemAsFailed(exception); }, (exception) -> { if (exception != null) { logger.error("failed to execute pipeline for a bulk request", exception); listener.onFailure(exception); } else { long ingestTookInMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - ingestStartTimeInNanos); BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest(); ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(ingestTookInMillis, listener); if (bulkRequest.requests().isEmpty()) { // at this stage, the transport bulk action can't deal with a bulk request with no requests, // so we stop and send an empty response back to the client. // (this will happen if pre-processing all items in the bulk failed) actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0)); } else { doExecute(task, bulkRequest, actionListener); } } }); }	can we assign response.getitems() to a local var?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject() .field("state", state()) .field("primary", primary()) .field("node", currentNodeId()) .field("relocating_node", relocatingNodeId()) .field("shard", shardId().id()) .field("index", shardId().index().name()) .field("version", version); if (restoreSource() != null) { builder.field("restore_source"); restoreSource().toXContent(builder, params); } if (allocationId != null) { allocationId.toXContent(builder, params); } if (unassignedInfo != null) { unassignedInfo.toXContent(builder, params); } return builder.endObject(); }	why did you add it? i mean, it is very internal...., it will mean cluster state api will become so much more noisy
private boolean applyStartedShards(RoutingNodes routingNodes, Iterable<? extends ShardRouting> startedShardEntries) { boolean dirty = false; // apply shards might be called several times with the same shard, ignore it STARTED_SHARDS_LOOP: for (ShardRouting startedShard : startedShardEntries) { assert startedShard.initializing(); // validate index still exists. strictly speaking this is not needed but it gives clearer logs if (routingNodes.routingTable().index(startedShard.index()) == null) { logger.debug("{} ignoring shard started, unknown index (routing: {})", startedShard.shardId(), startedShard); continue; } RoutingNodes.RoutingNodeIterator currentRoutingNode = routingNodes.routingNodeIter(startedShard.currentNodeId()); if (currentRoutingNode == null) { logger.debug("{} failed to find shard in order to start it [failed to find node], ignoring (routing: {})", startedShard.shardId(), startedShard); continue; } for (ShardRouting shard : currentRoutingNode) { if (shard.allocationId().getId().equals(startedShard.allocationId().getId())) { if (shard.active()) { logger.trace("{} shard is already started, ignoring (routing: {})", startedShard.shardId(), startedShard); } else { dirty = true; // override started shard with the latest copy. Capture it now , before starting the shard destroys it... startedShard = new ShardRouting(shard); routingNodes.started(shard); logger.trace("{} marked shard as started (routing: {})", startedShard.shardId(), startedShard); break; } } } // startedShard is the current state of the shard (post relocation for example) // this means that after relocation, the state will be started and the currentNodeId will be // the node we relocated to if (startedShard.relocatingNodeId() == null) { continue; } RoutingNodes.RoutingNodeIterator sourceRoutingNode = routingNodes.routingNodeIter(startedShard.relocatingNodeId()); if (sourceRoutingNode != null) { while (sourceRoutingNode.hasNext()) { ShardRouting shard = sourceRoutingNode.next(); if (shard.allocationId().getId().equals(startedShard.allocationId().getRelocationId())) { assert shard.relocating() : "source shard for relocation is not marked as relocating. source " + shard + ", started target " + startedShard; dirty = true; sourceRoutingNode.remove(); break; } } } } return dirty; }	should the break be outside of the if/else? we found match
protected void doExecute(Task task, SamlInitiateSingleSignOnRequest request, ActionListener<SamlInitiateSingleSignOnResponse> listener) { final SamlAuthenticationState authenticationState = request.getSamlAuthenticationState(); if (authenticationState != null) { final ValidationException validationException = authenticationState.validate(); if (validationException != null) { listener.onFailure(validationException); return; } } identityProvider.getRegisteredServiceProvider(request.getSpEntityId(), ActionListener.wrap( sp -> { if (null == sp) { final String message = "Service Provider with Entity ID [" + request.getSpEntityId() + "] is not registered with this Identity Provider"; logger.debug(message); listener.onFailure(new IllegalArgumentException(message)); return; } final SecondaryAuthentication secondaryAuthentication = SecondaryAuthentication.readFromContext(securityContext); if (secondaryAuthentication == null) { if (authenticationState != null) { final FailedAuthenticationResponseMessageBuilder builder = new FailedAuthenticationResponseMessageBuilder(samlFactory, Clock.systemUTC(), identityProvider) .setInResponseTo(authenticationState.getAuthnRequestId()) .setAcsUrl(authenticationState.getRequestedAcsUrl()) .setPrimaryStatusCode(StatusCode.REQUESTER) .setSecondaryStatusCode(StatusCode.AUTHN_FAILED); final Response response = builder.build(); listener.onResponse(new SamlInitiateSingleSignOnResponse( authenticationState.getRequestedAcsUrl(), samlFactory.getXmlContent(response), authenticationState.getEntityId())); return; } else { listener.onFailure(new IllegalStateException("Request is missing secondary authentication")); return; } } final UserServiceAuthentication user = buildUserFromAuthentication(secondaryAuthentication.getAuthentication(), sp); final SuccessfulAuthenticationResponseMessageBuilder builder = new SuccessfulAuthenticationResponseMessageBuilder( samlFactory, Clock.systemUTC(), identityProvider); final Response response = builder.build(user, request.getSamlAuthenticationState()); listener.onResponse(new SamlInitiateSingleSignOnResponse( user.getServiceProvider().getAssertionConsumerService().toString(), samlFactory.getXmlContent(response), user.getServiceProvider().getEntityId())); }, e -> { if (authenticationState != null) { final FailedAuthenticationResponseMessageBuilder builder = new FailedAuthenticationResponseMessageBuilder(samlFactory, Clock.systemUTC(), identityProvider) .setInResponseTo(authenticationState.getAuthnRequestId()) .setAcsUrl(authenticationState.getRequestedAcsUrl()) .setPrimaryStatusCode(StatusCode.RESPONDER); final Response response = builder.build(); listener.onResponse(new SamlInitiateSingleSignOnResponse( authenticationState.getRequestedAcsUrl(), samlFactory.getXmlContent(response), authenticationState.getEntityId())); } else { listener.onFailure(e); } } )); }	can't this be handled in the validation on the request? why would we do it inside the action?
protected void doExecute(Task task, SamlInitiateSingleSignOnRequest request, ActionListener<SamlInitiateSingleSignOnResponse> listener) { final SamlAuthenticationState authenticationState = request.getSamlAuthenticationState(); if (authenticationState != null) { final ValidationException validationException = authenticationState.validate(); if (validationException != null) { listener.onFailure(validationException); return; } } identityProvider.getRegisteredServiceProvider(request.getSpEntityId(), ActionListener.wrap( sp -> { if (null == sp) { final String message = "Service Provider with Entity ID [" + request.getSpEntityId() + "] is not registered with this Identity Provider"; logger.debug(message); listener.onFailure(new IllegalArgumentException(message)); return; } final SecondaryAuthentication secondaryAuthentication = SecondaryAuthentication.readFromContext(securityContext); if (secondaryAuthentication == null) { if (authenticationState != null) { final FailedAuthenticationResponseMessageBuilder builder = new FailedAuthenticationResponseMessageBuilder(samlFactory, Clock.systemUTC(), identityProvider) .setInResponseTo(authenticationState.getAuthnRequestId()) .setAcsUrl(authenticationState.getRequestedAcsUrl()) .setPrimaryStatusCode(StatusCode.REQUESTER) .setSecondaryStatusCode(StatusCode.AUTHN_FAILED); final Response response = builder.build(); listener.onResponse(new SamlInitiateSingleSignOnResponse( authenticationState.getRequestedAcsUrl(), samlFactory.getXmlContent(response), authenticationState.getEntityId())); return; } else { listener.onFailure(new IllegalStateException("Request is missing secondary authentication")); return; } } final UserServiceAuthentication user = buildUserFromAuthentication(secondaryAuthentication.getAuthentication(), sp); final SuccessfulAuthenticationResponseMessageBuilder builder = new SuccessfulAuthenticationResponseMessageBuilder( samlFactory, Clock.systemUTC(), identityProvider); final Response response = builder.build(user, request.getSamlAuthenticationState()); listener.onResponse(new SamlInitiateSingleSignOnResponse( user.getServiceProvider().getAssertionConsumerService().toString(), samlFactory.getXmlContent(response), user.getServiceProvider().getEntityId())); }, e -> { if (authenticationState != null) { final FailedAuthenticationResponseMessageBuilder builder = new FailedAuthenticationResponseMessageBuilder(samlFactory, Clock.systemUTC(), identityProvider) .setInResponseTo(authenticationState.getAuthnRequestId()) .setAcsUrl(authenticationState.getRequestedAcsUrl()) .setPrimaryStatusCode(StatusCode.RESPONDER); final Response response = builder.build(); listener.onResponse(new SamlInitiateSingleSignOnResponse( authenticationState.getRequestedAcsUrl(), samlFactory.getXmlContent(response), authenticationState.getEntityId())); } else { listener.onFailure(e); } } )); }	i feel like it would be good (in a follow up probably) to actually indicate to the caller (in the json response) that this a failure rather than success.
private void updateMaxTermSeen(final long term) { final long updatedMaxTermSeen = maxTermSeen.updateAndGet(oldMaxTerm -> Math.max(oldMaxTerm, term)); synchronized (mutex) { long currentTerm = getCurrentTerm(); if (mode == Mode.LEADER && updatedMaxTermSeen > currentTerm) { if (publicationInProgress() == false) { // Bump our term. However if there is a publication in flight then doing so would cancel the publication, so don't do that // since we check whether a term bump is needed at the end of the publication too. ensureTermAtLeast(getLocalNode(), updatedMaxTermSeen); startElection(); } else if (term == updatedMaxTermSeen) { logger.debug("updateMaxTermSeen: updatedMaxTermSeen = {} > currentTerm = {}, enqueueing term bump", updatedMaxTermSeen, currentTerm); } } } }	maybe move that comment before the if (publicationinprogress() == false) check
private void updateMaxTermSeen(final long term) { final long updatedMaxTermSeen = maxTermSeen.updateAndGet(oldMaxTerm -> Math.max(oldMaxTerm, term)); synchronized (mutex) { long currentTerm = getCurrentTerm(); if (mode == Mode.LEADER && updatedMaxTermSeen > currentTerm) { if (publicationInProgress() == false) { // Bump our term. However if there is a publication in flight then doing so would cancel the publication, so don't do that // since we check whether a term bump is needed at the end of the publication too. ensureTermAtLeast(getLocalNode(), updatedMaxTermSeen); startElection(); } else if (term == updatedMaxTermSeen) { logger.debug("updateMaxTermSeen: updatedMaxTermSeen = {} > currentTerm = {}, enqueueing term bump", updatedMaxTermSeen, currentTerm); } } } }	i know this is for concurrency reasons. just wondering: would it make sense to move the update of the term under the mutex? in that case, this condition would not need to be checked here.
public static Fuzziness build(Object fuzziness) { if (fuzziness instanceof Fuzziness) { return (Fuzziness) fuzziness; } String string = fuzziness.toString(); if (AUTO.asString().equalsIgnoreCase(string)) { return AUTO; } else if (string.toUpperCase(Locale.ROOT).startsWith(AUTO.asString() + ":")) { return parseCustomAuto(string); } return new Fuzziness(string); }	can you add an assert string.touppercase(locale.root).startswith(auto.asstring() + ":") on top of that line? just to make sure tests would catch if we mistakenly start to call this method without performing that check first
private static Fuzziness parseCustomAuto( final String string) { String[] fuzzinessLimit = string.substring(AUTO.asString().length() + 1).split(","); if (fuzzinessLimit.length == 2) { try { int lowerLimit = Integer.parseInt(fuzzinessLimit[0]); int highLimit = Integer.parseInt(fuzzinessLimit[1]); return new Fuzziness("AUTO", lowerLimit, highLimit); } catch (NumberFormatException e) { throw new ElasticsearchParseException("failed to parse [{}] as a \\\\"auto:int,int\\\\"", e, string); } } else { throw new ElasticsearchParseException("Auto fuzziness wrongly configured"); } }	can you make the message more specific and say that you were looking for 2 integers, but got arrays.tostring(fuzzinesslimit)?
@Override public void readFrom(StreamInput in) throws IOException { int version = in.readVInt(); // version id = in.readString(); type = in.readString(); source = in.readBytesReference(); try { if (version >= 1) { if (in.readBoolean()) { routing = in.readString(); } } if (version >= 2) { if (in.readBoolean()) { parent = in.readString(); } } if (version >= 3) { this.version = in.readLong(); } if (version >= 4) { this.timestamp = in.readLong(); } if (version >= 5) { this.ttl = in.readLong(); } if (version >= 6) { this.versionType = VersionType.fromValue(in.readByte()); } } catch (Exception e) { if (e instanceof InterruptedException) { Thread.currentThread().interrupt(); } throw new ElasticsearchException("failed to read [" + type + "][" + id + "]", e); } assert versionType.validateVersionForWrites(version); }	i think interruptedexception is maybe not needed, because its checked and not in the signatures anywhere. but exception is much better than throwable. thanks! the main issue was: it hides outofmemoryerror and this confuses users and makes reporting tools for jvm problems useless.
@Override protected Collection<Class<? extends Plugin>> getPlugins() { return List.of(MapperExtrasPlugin.class, TestGeoShapeFieldMapperPlugin.class); }	i think there is room to pull the testgeoshapefieldmapperplugin to the parent test class and inherited by those that need it or need to extend to include more plugins. i could see it stay either way since it is fairly low impact and the dependency is expected to go away
public void testIncorrectCompatibleHandlersDoNotDispatch() { final byte version = (byte) (Version.CURRENT.major - 1); final String mimeType = randomFrom("application/vnd.elasticsearch+json;compatible-with="+version); String content = randomAlphaOfLength((int) Math.round(BREAKER_LIMIT.getBytes() / inFlightRequestsBreaker.getOverhead())); final List<String> contentTypeHeader = Collections.singletonList(mimeType); // request to compatible api with body requires a content-type header. // this one does no have it, making it non compatiblegit FakeRestRequest fakeRestRequest = new FakeRestRequest.Builder(NamedXContentRegistry.EMPTY) .withContent(new BytesArray(content), RestRequest.parseContentType(contentTypeHeader)).withPath("/foo") .withHeaders(Map.of("Accept", contentTypeHeader)) .build(); restController.registerHandler(RestRequest.Method.GET, "/foo", new RestHandler() { @Override public void handleRequest(RestRequest request, RestChannel channel, NodeClient client) throws Exception { Assert.fail(); } @Override public boolean supportsContentStream() { return true; } @Override public boolean compatibilityRequired() { return true; } }); AssertingChannel channel = new AssertingChannel(fakeRestRequest, false, RestStatus.BAD_REQUEST); assertFalse(channel.getSendResponseCalled()); restController.dispatchRequest(fakeRestRequest, channel, new ThreadContext(Settings.EMPTY)); assertTrue(channel.getSendResponseCalled()); }	don't think you need the randomfrom
public void testIncorrectCompatibleHandlersDoNotDispatch() { final byte version = (byte) (Version.CURRENT.major - 1); final String mimeType = randomFrom("application/vnd.elasticsearch+json;compatible-with="+version); String content = randomAlphaOfLength((int) Math.round(BREAKER_LIMIT.getBytes() / inFlightRequestsBreaker.getOverhead())); final List<String> contentTypeHeader = Collections.singletonList(mimeType); // request to compatible api with body requires a content-type header. // this one does no have it, making it non compatiblegit FakeRestRequest fakeRestRequest = new FakeRestRequest.Builder(NamedXContentRegistry.EMPTY) .withContent(new BytesArray(content), RestRequest.parseContentType(contentTypeHeader)).withPath("/foo") .withHeaders(Map.of("Accept", contentTypeHeader)) .build(); restController.registerHandler(RestRequest.Method.GET, "/foo", new RestHandler() { @Override public void handleRequest(RestRequest request, RestChannel channel, NodeClient client) throws Exception { Assert.fail(); } @Override public boolean supportsContentStream() { return true; } @Override public boolean compatibilityRequired() { return true; } }); AssertingChannel channel = new AssertingChannel(fakeRestRequest, false, RestStatus.BAD_REQUEST); assertFalse(channel.getSendResponseCalled()); restController.dispatchRequest(fakeRestRequest, channel, new ThreadContext(Settings.EMPTY)); assertTrue(channel.getSendResponseCalled()); }	nit: git :) (i do that all the time!)
public void testIncorrectCompatibleHandlersDoNotDispatch() { final byte version = (byte) (Version.CURRENT.major - 1); final String mimeType = randomFrom("application/vnd.elasticsearch+json;compatible-with="+version); String content = randomAlphaOfLength((int) Math.round(BREAKER_LIMIT.getBytes() / inFlightRequestsBreaker.getOverhead())); final List<String> contentTypeHeader = Collections.singletonList(mimeType); // request to compatible api with body requires a content-type header. // this one does no have it, making it non compatiblegit FakeRestRequest fakeRestRequest = new FakeRestRequest.Builder(NamedXContentRegistry.EMPTY) .withContent(new BytesArray(content), RestRequest.parseContentType(contentTypeHeader)).withPath("/foo") .withHeaders(Map.of("Accept", contentTypeHeader)) .build(); restController.registerHandler(RestRequest.Method.GET, "/foo", new RestHandler() { @Override public void handleRequest(RestRequest request, RestChannel channel, NodeClient client) throws Exception { Assert.fail(); } @Override public boolean supportsContentStream() { return true; } @Override public boolean compatibilityRequired() { return true; } }); AssertingChannel channel = new AssertingChannel(fakeRestRequest, false, RestStatus.BAD_REQUEST); assertFalse(channel.getSendResponseCalled()); restController.dispatchRequest(fakeRestRequest, channel, new ThreadContext(Settings.EMPTY)); assertTrue(channel.getSendResponseCalled()); }	doesn't this request have content-type and accept header and a body ? (the body is withcontent right?) so with a body, matching accept and content-type headers , shouldn't this be an example of a good request ?
* @param blobSize expected size of the blob to be written */ void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException { SocketAccess.doPrivilegedVoidIOException(() -> { InputStreamContent stream = new InputStreamContent(null, inputStream); stream.setLength(blobSize); Storage.Objects.Insert insert = client.objects().insert(bucket, null, stream); insert.setIfGenerationMatch(0L); // ensures that the file does not already exist insert.setName(blobName); try { insert.execute(); } catch (GoogleJsonResponseException e) { GoogleJsonError error = e.getDetails(); if ((e.getStatusCode() == HTTP_PRECON_FAILED) || ((error != null) && (error.getCode() == HTTP_PRECON_FAILED))) { throw new FileAlreadyExistsException(blobName, null, e.getMessage()); } } }); }	exception must be thrown again
@Override public void onFailure(final Exception e) { onFailure.accept(unknownLicense.apply(e)); } }); } /** * Fetches the history UUIDs for leader index on per shard basis using the specified leaderClient. * * @param leaderClient the leader client * @param leaderIndexMetaData the leader index metadata * @param onFailure the failure consumer * @param historyUUIDConsumer the leader index history uuid and consumer */ // NOTE: Placed this method here; in order to avoid duplication of logic for fetching history UUIDs // in case of following a local or a remote cluster. public void fetchLeaderHistoryUUIDs( final Client leaderClient, final IndexMetaData leaderIndexMetaData, final Consumer<Exception> onFailure, final Consumer<String[]> historyUUIDConsumer) { String leaderIndex = leaderIndexMetaData.getIndex().getName(); CheckedConsumer<IndicesStatsResponse, Exception> indicesStatsHandler = indicesStatsResponse -> { IndexStats indexStats = indicesStatsResponse.getIndices().get(leaderIndex); String[] historyUUIDs = new String[leaderIndexMetaData.getNumberOfShards()]; for (IndexShardStats indexShardStats : indexStats) { for (ShardStats shardStats : indexShardStats) { CommitStats commitStats = shardStats.getCommitStats(); String historyUUID = commitStats.getUserData().get(Engine.HISTORY_UUID_KEY); ShardId shardId = shardStats.getShardRouting().shardId(); historyUUIDs[shardId.id()] = historyUUID; } } historyUUIDConsumer.accept(historyUUIDs); }; IndicesStatsRequest request = new IndicesStatsRequest(); request.indices(leaderIndex); leaderClient.admin().indices().stats(request, ActionListener.wrap(indicesStatsHandler, onFailure)); } private static ElasticsearchStatusException indexMetadataNonCompliantRemoteLicense( final String leaderIndex, final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) { final String clusterAlias = licenseCheck.remoteClusterLicenseInfo().clusterAlias(); final String message = String.format( Locale.ROOT, "can not fetch remote index [%s:%s] metadata as the remote cluster [%s] is not licensed for [ccr]; %s", clusterAlias, leaderIndex, clusterAlias, RemoteClusterLicenseChecker.buildErrorMessage( "ccr", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)); return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST); } private static ElasticsearchStatusException clusterStateNonCompliantRemoteLicense( final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) { final String clusterAlias = licenseCheck.remoteClusterLicenseInfo().clusterAlias(); final String message = String.format( Locale.ROOT, "can not fetch remote cluster state as the remote cluster [%s] is not licensed for [ccr]; %s", clusterAlias, RemoteClusterLicenseChecker.buildErrorMessage( "ccr", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)); return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST); }	commitstats might be null if a shard is (being) closed.
@Override public void onFailure(final Exception e) { onFailure.accept(unknownLicense.apply(e)); } }); } /** * Fetches the history UUIDs for leader index on per shard basis using the specified leaderClient. * * @param leaderClient the leader client * @param leaderIndexMetaData the leader index metadata * @param onFailure the failure consumer * @param historyUUIDConsumer the leader index history uuid and consumer */ // NOTE: Placed this method here; in order to avoid duplication of logic for fetching history UUIDs // in case of following a local or a remote cluster. public void fetchLeaderHistoryUUIDs( final Client leaderClient, final IndexMetaData leaderIndexMetaData, final Consumer<Exception> onFailure, final Consumer<String[]> historyUUIDConsumer) { String leaderIndex = leaderIndexMetaData.getIndex().getName(); CheckedConsumer<IndicesStatsResponse, Exception> indicesStatsHandler = indicesStatsResponse -> { IndexStats indexStats = indicesStatsResponse.getIndices().get(leaderIndex); String[] historyUUIDs = new String[leaderIndexMetaData.getNumberOfShards()]; for (IndexShardStats indexShardStats : indexStats) { for (ShardStats shardStats : indexShardStats) { CommitStats commitStats = shardStats.getCommitStats(); String historyUUID = commitStats.getUserData().get(Engine.HISTORY_UUID_KEY); ShardId shardId = shardStats.getShardRouting().shardId(); historyUUIDs[shardId.id()] = historyUUID; } } historyUUIDConsumer.accept(historyUUIDs); }; IndicesStatsRequest request = new IndicesStatsRequest(); request.indices(leaderIndex); leaderClient.admin().indices().stats(request, ActionListener.wrap(indicesStatsHandler, onFailure)); } private static ElasticsearchStatusException indexMetadataNonCompliantRemoteLicense( final String leaderIndex, final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) { final String clusterAlias = licenseCheck.remoteClusterLicenseInfo().clusterAlias(); final String message = String.format( Locale.ROOT, "can not fetch remote index [%s:%s] metadata as the remote cluster [%s] is not licensed for [ccr]; %s", clusterAlias, leaderIndex, clusterAlias, RemoteClusterLicenseChecker.buildErrorMessage( "ccr", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)); return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST); } private static ElasticsearchStatusException clusterStateNonCompliantRemoteLicense( final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) { final String clusterAlias = licenseCheck.remoteClusterLicenseInfo().clusterAlias(); final String message = String.format( Locale.ROOT, "can not fetch remote cluster state as the remote cluster [%s] is not licensed for [ccr]; %s", clusterAlias, RemoteClusterLicenseChecker.buildErrorMessage( "ccr", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)); return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST); }	you might have missed boaz's comment: > also - can we assert that the history uuids of all the shard copies that we got are identical moreover, not every shard is allocated or associated with a historyuuid. should we fail if there is no historyuuid for a shardid?
@Override public void onFailure(final Exception e) { onFailure.accept(unknownLicense.apply(e)); } }); } /** * Fetches the history UUIDs for leader index on per shard basis using the specified leaderClient. * * @param leaderClient the leader client * @param leaderIndexMetaData the leader index metadata * @param onFailure the failure consumer * @param historyUUIDConsumer the leader index history uuid and consumer */ // NOTE: Placed this method here; in order to avoid duplication of logic for fetching history UUIDs // in case of following a local or a remote cluster. public void fetchLeaderHistoryUUIDs( final Client leaderClient, final IndexMetaData leaderIndexMetaData, final Consumer<Exception> onFailure, final Consumer<String[]> historyUUIDConsumer) { String leaderIndex = leaderIndexMetaData.getIndex().getName(); CheckedConsumer<IndicesStatsResponse, Exception> indicesStatsHandler = indicesStatsResponse -> { IndexStats indexStats = indicesStatsResponse.getIndices().get(leaderIndex); String[] historyUUIDs = new String[leaderIndexMetaData.getNumberOfShards()]; for (IndexShardStats indexShardStats : indexStats) { for (ShardStats shardStats : indexShardStats) { CommitStats commitStats = shardStats.getCommitStats(); String historyUUID = commitStats.getUserData().get(Engine.HISTORY_UUID_KEY); ShardId shardId = shardStats.getShardRouting().shardId(); historyUUIDs[shardId.id()] = historyUUID; } } historyUUIDConsumer.accept(historyUUIDs); }; IndicesStatsRequest request = new IndicesStatsRequest(); request.indices(leaderIndex); leaderClient.admin().indices().stats(request, ActionListener.wrap(indicesStatsHandler, onFailure)); } private static ElasticsearchStatusException indexMetadataNonCompliantRemoteLicense( final String leaderIndex, final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) { final String clusterAlias = licenseCheck.remoteClusterLicenseInfo().clusterAlias(); final String message = String.format( Locale.ROOT, "can not fetch remote index [%s:%s] metadata as the remote cluster [%s] is not licensed for [ccr]; %s", clusterAlias, leaderIndex, clusterAlias, RemoteClusterLicenseChecker.buildErrorMessage( "ccr", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)); return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST); } private static ElasticsearchStatusException clusterStateNonCompliantRemoteLicense( final RemoteClusterLicenseChecker.LicenseCheck licenseCheck) { final String clusterAlias = licenseCheck.remoteClusterLicenseInfo().clusterAlias(); final String message = String.format( Locale.ROOT, "can not fetch remote cluster state as the remote cluster [%s] is not licensed for [ccr]; %s", clusterAlias, RemoteClusterLicenseChecker.buildErrorMessage( "ccr", licenseCheck.remoteClusterLicenseInfo(), RemoteClusterLicenseChecker::isLicensePlatinumOrTrial)); return new ElasticsearchStatusException(message, RestStatus.BAD_REQUEST); }	we can "clear" all flags to reduce this stat request.
* @param clusterAlias the remote cluster alias * @param leaderIndex the name of the leader index * @param onFailure the failure consumer * @param consumer the consumer for supplying the leader index metadata and historyUUIDs of all leader shards * @param <T> the type of response the listener is waiting for */ public <T> void checkRemoteClusterLicenseAndFetchLeaderIndexMetadataAndHistoryUUIDs( final Client client, final String clusterAlias, final String leaderIndex, final Consumer<Exception> onFailure, final BiConsumer<String[], IndexMetaData> consumer) { final ClusterStateRequest request = new ClusterStateRequest(); request.clear(); request.metaData(true); request.indices(leaderIndex); checkRemoteClusterLicenseAndFetchClusterState( client, clusterAlias, request, onFailure, leaderClusterState -> { IndexMetaData leaderIndexMetaData = leaderClusterState.getMetaData().index(leaderIndex); final Client leaderClient = client.getRemoteClusterClient(clusterAlias); fetchLeaderHistoryUUIDs(leaderClient, leaderIndexMetaData, onFailure, historyUUIDs -> { consumer.accept(historyUUIDs, leaderIndexMetaData); }); }, licenseCheck -> indexMetadataNonCompliantRemoteLicense(leaderIndex, licenseCheck), e -> indexMetadataUnknownRemoteLicense(leaderIndex, clusterAlias, e)); } /** * Fetches the leader cluster state from the remote cluster by the specified cluster state request. Before fetching the cluster state, * the remote cluster is checked for license compliance with CCR. If the remote cluster is not licensed for CCR, * the {@code onFailure}	we can re-indent the javadocs.
static void validate(Request request, IndexMetaData leaderIndex, IndexMetaData followIndex, String[] leaderIndexHistoryUUID, MapperService followerMapperService) { if (leaderIndex == null) { throw new IllegalArgumentException("leader index [" + request.leaderIndex + "] does not exist"); } if (followIndex == null) { throw new IllegalArgumentException("follow index [" + request.followerIndex + "] does not exist"); } String[] recordedHistoryUUIDs = extractIndexShardHistoryUUIDs(followIndex); assert recordedHistoryUUIDs.length == leaderIndexHistoryUUID.length; for (int i = 0; i < leaderIndexHistoryUUID.length; i++) { String recordedLeaderIndexHistoryUUID = recordedHistoryUUIDs[i]; String actualLeaderIndexHistoryUUID = leaderIndexHistoryUUID[i]; if (recordedLeaderIndexHistoryUUID.equals(actualLeaderIndexHistoryUUID) == false) { throw new IllegalArgumentException("follow index [" + request.followerIndex + "] should reference [" + recordedLeaderIndexHistoryUUID + "] as history uuid but instead reference [" + actualLeaderIndexHistoryUUID + "] as history uuid"); } } if (leaderIndex.getSettings().getAsBoolean(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), false) == false) { throw new IllegalArgumentException("leader index [" + request.leaderIndex + "] does not have soft deletes enabled"); } if (leaderIndex.getNumberOfShards() != followIndex.getNumberOfShards()) { throw new IllegalArgumentException("leader index primary shards [" + leaderIndex.getNumberOfShards() + "] does not match with the number of shards of the follow index [" + followIndex.getNumberOfShards() + "]"); } if (leaderIndex.getRoutingNumShards() != followIndex.getRoutingNumShards()) { throw new IllegalArgumentException("leader index number_of_routing_shards [" + leaderIndex.getRoutingNumShards() + "] does not match with the number_of_routing_shards of the follow index [" + followIndex.getRoutingNumShards() + "]"); } if (leaderIndex.getState() != IndexMetaData.State.OPEN || followIndex.getState() != IndexMetaData.State.OPEN) { throw new IllegalArgumentException("leader and follow index must be open"); } if (CcrSettings.CCR_FOLLOWING_INDEX_SETTING.get(followIndex.getSettings()) == false) { throw new IllegalArgumentException("the following index [" + request.followerIndex + "] is not ready " + "to follow; the setting [" + CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey() + "] must be enabled."); } // Make a copy, remove settings that are allowed to be different and then compare if the settings are equal. Settings leaderSettings = filter(leaderIndex.getSettings()); Settings followerSettings = filter(followIndex.getSettings()); if (leaderSettings.equals(followerSettings) == false) { throw new IllegalArgumentException("the leader and follower index settings must be identical"); } // Validates if the current follower mapping is mergable with the leader mapping. // This also validates for example whether specific mapper plugins have been installed followerMapperService.merge(leaderIndex, MapperService.MergeReason.MAPPING_RECOVERY); }	should we just use arrays.equals instead?
void runPolicyAfterStateChange(String policy, IndexMetadata indexMetadata) { String index = indexMetadata.getIndex().getName(); LifecycleExecutionState lifecycleState = LifecycleExecutionState.fromIndexMetadata(indexMetadata); final StepKey currentStepKey = LifecycleExecutionState.getCurrentStepKey(lifecycleState); if (TerminalPolicyStep.KEY.equals(currentStepKey)) { logger.debug("policy [{}] for index [{}] complete, skipping execution", policy, index); return; } if (currentStepKey != null) { String currentStepName = currentStepKey.getName(); if (ErrorStep.NAME.equals(currentStepName)) { logger.debug("policy [{}] for index [{}] on an error step, skipping execution", policy, index); return; } if (ClusterStateSteps.NAMES.contains(currentStepName) == false) { logger.debug("step [{}] for index [{}] is not cluster state step, skipping execution", currentStepKey, index); return; } } if (busyIndices.contains(Tuple.tuple(indexMetadata.getIndex(), currentStepKey))) { // try later again, already doing work for this index at this step, no need to check for more work yet return; } final Step currentStep; try { currentStep = getCurrentStep(stepRegistry, policy, indexMetadata, lifecycleState); } catch (Exception e) { markPolicyRetrievalError(policy, indexMetadata.getIndex(), lifecycleState, e); return; } if (currentStep == null) { if (stepRegistry.policyExists(policy) == false) { markPolicyDoesNotExist(policy, indexMetadata.getIndex(), lifecycleState); } else { logger.error("current step [{}] for index [{}] with policy [{}] is not recognized", currentStepKey, index, policy); } return; } logger.trace("[{}] maybe running step ({}) after state change: {}", index, currentStep.getClass().getSimpleName(), currentStep.getKey()); if (currentStep instanceof PhaseCompleteStep) { if (currentStep.getNextStepKey() == null) { logger.debug("[{}] stopping in the current phase ({}) as there are no more steps in the policy", index, currentStep.getKey().getPhase()); return; } // Only proceed to the next step if enough time has elapsed to go into the next phase if (isReadyToTransitionToThisPhase(policy, indexMetadata, currentStep.getNextStepKey().getPhase())) { moveToStep(indexMetadata.getIndex(), policy, currentStep.getKey(), currentStep.getNextStepKey()); } } else if (currentStep instanceof ClusterStateActionStep || currentStep instanceof ClusterStateWaitStep) { logger.debug("[{}] running policy with current-step [{}]", indexMetadata.getIndex().getName(), currentStep.getKey()); submitUnlessAlreadyQueued(String.format(Locale.ROOT, "ilm-execute-cluster-state-steps [%s]", currentStep), new ExecuteStepsUpdateTask(policy, indexMetadata.getIndex(), currentStep, stepRegistry, this, nowSupplier)); } else { logger.trace("[{}] ignoring step execution from cluster state change event [{}]", index, currentStep.getKey()); } } /** * Move the index to the given {@code newStepKey}, always checks to ensure that the index's * current step matches the {@code currentStepKey}	isn't it a massive bug to enter this condition? also, we're changing behavior now, previously i think we would have called: markpolicydoesnotexist(policy, indexmetadata.getindex(), lifecyclestate); in this case?
public void expandIds(String idExpression, boolean allowNoResources, PageParams pageParams, Set<String> tags, ActionListener<Tuple<Long, Set<String>>> idsListener) { String[] tokens = Strings.tokenizeToStringArray(idExpression, ","); Set<String> foundResourceIds = new HashSet<>(); if (tags.isEmpty()) { foundResourceIds.addAll(matchedResourceIds(tokens)); } else { for(String resourceId : matchedResourceIds(tokens)) { // Does the model as a resource have all the tags? if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) { foundResourceIds.add(resourceId); } } } SearchSourceBuilder sourceBuilder = new SearchSourceBuilder() .sort(SortBuilders.fieldSort(TrainedModelConfig.MODEL_ID.getPreferredName()) // If there are no resources, there might be no mapping for the id field. // This makes sure we don't get an error if that happens. .unmappedType("long")) .query(buildExpandIdsQuery(tokens, tags)) // We "buffer" the from and size to take into account models stored as resources. // This is so we handle the edge cases when the model that is stored as a resource is at the start/end of // a page. .from(Math.max(0, pageParams.getFrom() - foundResourceIds.size())) .size(Math.min(10_000, pageParams.getSize() + foundResourceIds.size())); sourceBuilder.trackTotalHits(true) // we only care about the item id's .fetchSource(TrainedModelConfig.MODEL_ID.getPreferredName(), null); IndicesOptions indicesOptions = SearchRequest.DEFAULT_INDICES_OPTIONS; SearchRequest searchRequest = new SearchRequest(InferenceIndexConstants.INDEX_PATTERN) .indicesOptions(IndicesOptions.fromOptions(true, indicesOptions.allowNoIndices(), indicesOptions.expandWildcardsOpen(), indicesOptions.expandWildcardsClosed(), indicesOptions)) .source(sourceBuilder); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest, ActionListener.<SearchResponse>wrap( response -> { long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size(); Set<String> foundFromDocs = new HashSet<>(); for (SearchHit hit : response.getHits().getHits()) { Map<String, Object> docSource = hit.getSourceAsMap(); if (docSource == null) { continue; } Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName()); if (idValue instanceof String) { foundFromDocs.add(idValue.toString()); } } Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount); ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources); requiredMatches.filterMatchedIds(allFoundIds); if (requiredMatches.hasUnmatchedIds()) { idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString())); } else { idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds)); } }, idsListener::onFailure ), client::search); }	unrelated nit: can models_stored_as_resource be a collections.unmodifiableset then line 520 return new hashset<>(models_stored_as_resource); in matchedresourceids wouldn't have to wrap a set in a set
public void expandIds(String idExpression, boolean allowNoResources, PageParams pageParams, Set<String> tags, ActionListener<Tuple<Long, Set<String>>> idsListener) { String[] tokens = Strings.tokenizeToStringArray(idExpression, ","); Set<String> foundResourceIds = new HashSet<>(); if (tags.isEmpty()) { foundResourceIds.addAll(matchedResourceIds(tokens)); } else { for(String resourceId : matchedResourceIds(tokens)) { // Does the model as a resource have all the tags? if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) { foundResourceIds.add(resourceId); } } } SearchSourceBuilder sourceBuilder = new SearchSourceBuilder() .sort(SortBuilders.fieldSort(TrainedModelConfig.MODEL_ID.getPreferredName()) // If there are no resources, there might be no mapping for the id field. // This makes sure we don't get an error if that happens. .unmappedType("long")) .query(buildExpandIdsQuery(tokens, tags)) // We "buffer" the from and size to take into account models stored as resources. // This is so we handle the edge cases when the model that is stored as a resource is at the start/end of // a page. .from(Math.max(0, pageParams.getFrom() - foundResourceIds.size())) .size(Math.min(10_000, pageParams.getSize() + foundResourceIds.size())); sourceBuilder.trackTotalHits(true) // we only care about the item id's .fetchSource(TrainedModelConfig.MODEL_ID.getPreferredName(), null); IndicesOptions indicesOptions = SearchRequest.DEFAULT_INDICES_OPTIONS; SearchRequest searchRequest = new SearchRequest(InferenceIndexConstants.INDEX_PATTERN) .indicesOptions(IndicesOptions.fromOptions(true, indicesOptions.allowNoIndices(), indicesOptions.expandWildcardsOpen(), indicesOptions.expandWildcardsClosed(), indicesOptions)) .source(sourceBuilder); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest, ActionListener.<SearchResponse>wrap( response -> { long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size(); Set<String> foundFromDocs = new HashSet<>(); for (SearchHit hit : response.getHits().getHits()) { Map<String, Object> docSource = hit.getSourceAsMap(); if (docSource == null) { continue; } Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName()); if (idValue instanceof String) { foundFromDocs.add(idValue.toString()); } } Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount); ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources); requiredMatches.filterMatchedIds(allFoundIds); if (requiredMatches.hasUnmatchedIds()) { idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString())); } else { idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds)); } }, idsListener::onFailure ), client::search); }	so the strategy is to get extra then figure out where the resource model ids would fit in that sorted list
static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) { TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs); allFoundIds.addAll(foundFromResources); int from = pageParams.getFrom(); int bufferedFrom = Math.min(foundFromResources.size(), from); // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from); // Last page this means that if we "buffered" the from pagination due to resources we should clear that out // We only clear from the front as that would include buffered IDs that fall on the previous page if (from + sizeLimit >= totalMatchedIds) { while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) { allFoundIds.remove(allFoundIds.first()); bufferedFrom--; } } // Systematically remove items while we are above the limit while (allFoundIds.size() > sizeLimit) { // If we are still over limit, and have buffered items, that means the first ids belong on the previous page if (bufferedFrom > 0) { allFoundIds.remove(allFoundIds.first()); bufferedFrom--; } else { // If we have removed all items belonging on the previous page, but are still over sized, this means we should // remove items that belong on the next page. allFoundIds.remove(allFoundIds.last()); } } return allFoundIds; }	my way of looking at it is this: we have a request for 10 ids and 2 models stored as resource. we query for 14 ids so we can figure out where the resource model ids would fit. we have a result like aaoooooooooobb where the as and bs are the padding. if the model resource ids come before the as then they are outside the ordering and all the os are returned. same if the model resource ids come after the bs. if they are inserted middle (ids m & n) we have an ordering like aaooooomooonoobb we we take the first 10 after the as. same for aamoooooonoooobb mnaaoooooooooobb amaoooooonoooobb aaoooooooooobbmn i think we have to track the insertion position of the model resource ids. collections.binarysearch() would give the insert position. there is a further complication when the search does not return size hits and you have aaoooooooooo or aaoooo
public void failDestructive(String[] aliasesOrIndices) { if (!destructiveRequiresName) { return; } if (aliasesOrIndices == null || aliasesOrIndices.length == 0) { throw new IllegalArgumentException("Wildcard expressions or all indices are not allowed"); } else if (aliasesOrIndices.length == 1) { if (hasWildcardUsage(aliasesOrIndices[0])) { throw new IllegalArgumentException("Wildcard expressions or all indices are not allowed"); } } else if (Arrays.equals(aliasesOrIndices, new String[]{"*", "-*"})) { // do nothing, allowing the use of the "matchNone" pattern, "*,-*", // which will never actually be destructive as it operates on no indices } else { for (String aliasesOrIndex : aliasesOrIndices) { if (hasWildcardUsage(aliasesOrIndex)) { throw new IllegalArgumentException("Wildcard expressions or all indices are not allowed"); } } } }	can you make the string[] a static final variable?
*/ void deleteBlobs(Collection<String> blobNames) throws IOException { if (blobNames == null || blobNames.isEmpty()) { return; } if (blobNames.size() == 1) { deleteBlob(blobNames.iterator().next()); return; } final List<Storage.Objects.Delete> deletions = new ArrayList<>(blobNames.size()); final Iterator<String> blobs = blobNames.iterator(); SocketAccess.doPrivilegedVoidIOException(() -> { while (blobs.hasNext()) { // Create a delete request for each blob to delete deletions.add(client.objects().delete(bucket, blobs.next())); if (blobs.hasNext() == false || deletions.size() == MAX_BATCHING_REQUESTS) { try { // Deletions are executed using a batch request BatchRequest batch = client.batch(); // Used to track successful deletions CountDown countDown = new CountDown(deletions.size()); for (Storage.Objects.Delete delete : deletions) { // Queue the delete request in batch delete.queue(batch, new JsonBatchCallback<Void>() { @Override public void onFailure(GoogleJsonError e, HttpHeaders responseHeaders) throws IOException { logger.error("failed to delete blob [{}] in bucket [{}]: {}", delete.getObject(), delete.getBucket(), e .getMessage()); } @Override public void onSuccess(Void aVoid, HttpHeaders responseHeaders) throws IOException { countDown.countDown(); } }); } batch.execute(); if (countDown.isCountedDown() == false) { throw new IOException("Failed to delete all [" + deletions.size() + "] blobs"); } } finally { deletions.clear(); } } } }); }	this is going to oversize if blobnames.size() > max_batching_requests i think.
@Deprecated public static Logger getLogger(Class<?> clazz) { return ESLoggerFactory.getLogger(null, clazz); }	null looked weird, but then i saw the comment on esloggerfactory that this will be cleaned up at some point, so it looks okay.
public void internalAddListener(ActionListener<T> listener) { listener = new ThreadedActionListener<>(logger, threadPool, ThreadPool.Names.LISTENER, listener, false); boolean executeImmediate = false; synchronized (this) { if (executedListeners) { executeImmediate = true; } else { Object listeners = this.listeners; if (listeners == null) { listeners = listener; } else if (listeners instanceof List) { ((List) this.listeners).add(listener); } else { Object orig = listeners; listeners = new ArrayList<>(2); ((List) listeners).add(orig); ((List) listeners).add(listener); } this.listeners = listeners; } } if (executeImmediate) { executeListener(listener); } }	i don't think this indenting is intended.
@Override public List<String> filteredWarnings() { return Stream.concat(super.filteredWarnings().stream(), List.of("setting [path.shared_data] is deprecated and will be removed in a future release", "Configuring multiple [path.data] paths is deprecated. Use RAID or other system level features for utilizing " + "multiple disks. This feature will be removed in 8.0.").stream()).collect(Collectors.toList()); }	iirc path.shared_data is randomly set by the test framework. i think in that case this should be in the estestcase filtered warnings?
void setExpireAfterAccess(long expireAfterAccess) { if (expireAfterAccess <= 0) { throw new IllegalArgumentException("expireAfterAccess <= 0"); } this.expireAfterAccess = expireAfterAccess; this.entriesExpireAfterAccess = true; }	maybe these fields should be renamed, too, to add nanos?
void setExpireAfterWrite(long expireAfterWrite) { if (expireAfterWrite <= 0) { throw new IllegalArgumentException("expireAfterWrite <= 0"); } this.expireAfterWrite = expireAfterWrite; this.entriesExpireAfterWrite = true; }	maybe these fields should be renamed, too, to add nanos?
public CacheBuilder<K, V> setExpireAfterAccess(TimeValue expireAfterAccess) { Objects.requireNonNull(expireAfterAccess); final long expireAfterAccessNanos = expireAfterAccess.getNanos(); if (expireAfterAccessNanos <= 0) { throw new IllegalArgumentException("expireAfterAccess <= 0"); } this.expireAfterAccess = expireAfterAccessNanos; return this; }	can we rename the field too?
public CacheBuilder<K, V> setExpireAfterAccess(TimeValue expireAfterAccess) { Objects.requireNonNull(expireAfterAccess); final long expireAfterAccessNanos = expireAfterAccess.getNanos(); if (expireAfterAccessNanos <= 0) { throw new IllegalArgumentException("expireAfterAccess <= 0"); } this.expireAfterAccess = expireAfterAccessNanos; return this; }	echoing @nik9000, a javadoc would be nice here too.
public CacheBuilder<K, V> setExpireAfterAccess(TimeValue expireAfterAccess) { Objects.requireNonNull(expireAfterAccess); final long expireAfterAccessNanos = expireAfterAccess.getNanos(); if (expireAfterAccessNanos <= 0) { throw new IllegalArgumentException("expireAfterAccess <= 0"); } this.expireAfterAccess = expireAfterAccessNanos; return this; }	can we rename the field too?
public final MergeScheduler newMergeScheduler() { MergeScheduler scheduler = buildMergeScheduler(); // an internal settings, that would allow us to disable this behavior if really needed if (indexSettings.getAsBoolean("index.merge.force_async_merge", true)) { scheduler = new EnableMergeScheduler(scheduler); } return scheduler; }	can we make this a public static final string and maybe randomize it once in a while?
private Map<String, Mapper.TypeParser> getMappers(List<MapperPlugin> mapperPlugins) { Map<String, Mapper.TypeParser> mappers = new LinkedHashMap<>(); // builtin mappers for (NumberFieldMapper.NumberType type : NumberFieldMapper.NumberType.values()) { mappers.put(type.typeName(), new NumberFieldMapper.TypeParser(type)); } mappers.put(BooleanFieldMapper.CONTENT_TYPE, new BooleanFieldMapper.TypeParser()); mappers.put(BinaryFieldMapper.CONTENT_TYPE, new BinaryFieldMapper.TypeParser()); mappers.put(DateFieldMapper.CONTENT_TYPE, new DateFieldMapper.TypeParser()); mappers.put(IpFieldMapper.CONTENT_TYPE, new IpFieldMapper.TypeParser()); mappers.put(StringFieldMapper.CONTENT_TYPE, new StringFieldMapper.TypeParser()); mappers.put(TextFieldMapper.CONTENT_TYPE, new TextFieldMapper.TypeParser()); mappers.put(KeywordFieldMapper.CONTENT_TYPE, new KeywordFieldMapper.TypeParser()); mappers.put(TokenCountFieldMapper.CONTENT_TYPE, new TokenCountFieldMapper.TypeParser()); mappers.put(ObjectMapper.CONTENT_TYPE, new ObjectMapper.TypeParser()); mappers.put(ObjectMapper.NESTED_CONTENT_TYPE, new ObjectMapper.TypeParser()); mappers.put(CompletionFieldMapper.CONTENT_TYPE, new CompletionFieldMapper.TypeParser()); mappers.put(GeoPointFieldMapper.CONTENT_TYPE, new GeoPointFieldMapper.TypeParser()); if (ShapesAvailability.JTS_AVAILABLE && ShapesAvailability.SPATIAL4J_AVAILABLE) { mappers.put(GeoShapeFieldMapper.CONTENT_TYPE, new GeoShapeFieldMapper.TypeParser()); } for (MapperPlugin mapperPlugin : mapperPlugins) { mappers.putAll(mapperPlugin.getMappers()); } return Collections.unmodifiableMap(mappers); }	should we make sure that they have an empty intersection? sets.hasemptyintersection(mappers.keyset(), mapperplugin.getmappers().keyset()?
public void testCluster_GivenAnomalyDetectionJobAndTrainedModelDeployment_ShouldNotAllocateBothOnSameNode() throws Exception { // This test starts 2 ML nodes and then starts an anomaly detection job and a // trained model deployment that do not both fit in one node. We then proceed // to stop both ML nodes and start a single ML node back up. We should see // that both the job and the model cannot be allocated on that node. // At the moment there is a bug that prevents that and they both end up // assigned on the same node so the test makes this clear but we should change this // test when the bug is fixed. internalCluster().ensureAtMostNumDataNodes(0); logger.info("Starting dedicated master node..."); internalCluster().startMasterOnlyNode(); logger.info("Starting ml and data node..."); internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.ML_ROLE))); logger.info("Starting another ml and data node..."); internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.ML_ROLE))); ensureStableCluster(); MlMemoryAction.Response memoryStats = client().execute(MlMemoryAction.INSTANCE, new MlMemoryAction.Request("ml:true")).actionGet(); long maxNativeBytesPerNode = 0; for (MlMemoryAction.Response.MlMemoryStats stats : memoryStats.getNodes()) { maxNativeBytesPerNode = stats.getMlMax().getBytes(); } String jobId = "test-node-goes-down-while-running-job"; Job.Builder job = createJob(jobId, ByteSizeValue.ofBytes((long) (0.8 * maxNativeBytesPerNode))); PutJobAction.Request putJobRequest = new PutJobAction.Request(job); client().execute(PutJobAction.INSTANCE, putJobRequest).actionGet(); client().execute(OpenJobAction.INSTANCE, new OpenJobAction.Request(job.getId())).actionGet(); TrainedModelConfig model = TrainedModelConfig.builder() .setModelId("test_model") .setModelType(TrainedModelType.PYTORCH) .setModelSize((long) (0.3 * maxNativeBytesPerNode)) .setInferenceConfig(new PassThroughConfig(new VocabularyConfig(InferenceIndexConstants.nativeDefinitionStore()), null, null)) .setLocation(new IndexLocation(InferenceIndexConstants.nativeDefinitionStore())) .build(); TrainedModelDefinitionDoc modelDefinitionDoc = new TrainedModelDefinitionDoc( new BytesArray(""), model.getModelId(), 0, model.getModelSize(), model.getModelSize(), 1, true ); try (XContentBuilder builder = JsonXContent.contentBuilder()) { modelDefinitionDoc.toXContent(builder, null); client().execute( IndexAction.INSTANCE, new IndexRequest(InferenceIndexConstants.nativeDefinitionStore()).source(builder) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) ).actionGet(); } client().execute(PutTrainedModelAction.INSTANCE, new PutTrainedModelAction.Request(model, true)).actionGet(); client().execute( PutTrainedModelVocabularyAction.INSTANCE, new PutTrainedModelVocabularyAction.Request( model.getModelId(), List.of("these", "are", "my", "words", "[SEP]", "[CLS]", BertTokenizer.UNKNOWN_TOKEN, BertTokenizer.PAD_TOKEN), List.of() ) ).actionGet(); client().execute(StartTrainedModelDeploymentAction.INSTANCE, new StartTrainedModelDeploymentAction.Request(model.getModelId())) .actionGet(); setMlIndicesDelayedNodeLeftTimeoutToZero(); String jobNode = client().execute(GetJobsStatsAction.INSTANCE, new GetJobsStatsAction.Request(job.getId())) .actionGet() .getResponse() .results() .get(0) .getNode() .getId(); String modelNode = client().execute( GetTrainedModelsStatsAction.INSTANCE, new GetTrainedModelsStatsAction.Request(model.getModelId()) ).actionGet().getResources().results().get(0).getDeploymentStats().getNodeStats().get(0).getNode().getId(); // Assert the job and model were assigned to different nodes as they would not fit in the same node assertThat(jobNode, not(equalTo(modelNode))); // Stop both ML nodes internalCluster().stopNode(jobNode); internalCluster().stopNode(modelNode); // Start a new ML node internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.ML_ROLE))); assertBusy(() -> { GetJobsStatsAction.Response jobStats = client().execute( GetJobsStatsAction.INSTANCE, new GetJobsStatsAction.Request(job.getId()) ).actionGet(); assertThat(jobStats.getResponse().results().get(0).getState(), equalTo(JobState.OPENED)); }); assertBusy(() -> { GetTrainedModelsStatsAction.Response modelStats = client().execute( GetTrainedModelsStatsAction.INSTANCE, new GetTrainedModelsStatsAction.Request(model.getModelId()) ).actionGet(); assertThat(modelStats.getResources().results().get(0).getDeploymentStats().getState(), equalTo(AllocationState.STARTED)); }); // Clean up client().execute(CloseJobAction.INSTANCE, new CloseJobAction.Request(jobId).setForce(true)).actionGet(); client().execute(StopTrainedModelDeploymentAction.INSTANCE, new StopTrainedModelDeploymentAction.Request(model.getModelId())) .actionGet(); }	even after the code is changed to do the right thing these assertions could still pass. there's a subtle difference between them that might be worth calling out in comments: - a job that was opened but whose node left the cluster and is not currently assigned is still opened - a trained model that is not allocated anywhere because the nodes it was on left the cluster is no longer started but starting so these assertions will still pass in the future if the trained model allocation takes precedence over persistent task assignment.
public void testCluster_GivenAnomalyDetectionJobAndTrainedModelDeployment_ShouldNotAllocateBothOnSameNode() throws Exception { // This test starts 2 ML nodes and then starts an anomaly detection job and a // trained model deployment that do not both fit in one node. We then proceed // to stop both ML nodes and start a single ML node back up. We should see // that both the job and the model cannot be allocated on that node. // At the moment there is a bug that prevents that and they both end up // assigned on the same node so the test makes this clear but we should change this // test when the bug is fixed. internalCluster().ensureAtMostNumDataNodes(0); logger.info("Starting dedicated master node..."); internalCluster().startMasterOnlyNode(); logger.info("Starting ml and data node..."); internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.ML_ROLE))); logger.info("Starting another ml and data node..."); internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.ML_ROLE))); ensureStableCluster(); MlMemoryAction.Response memoryStats = client().execute(MlMemoryAction.INSTANCE, new MlMemoryAction.Request("ml:true")).actionGet(); long maxNativeBytesPerNode = 0; for (MlMemoryAction.Response.MlMemoryStats stats : memoryStats.getNodes()) { maxNativeBytesPerNode = stats.getMlMax().getBytes(); } String jobId = "test-node-goes-down-while-running-job"; Job.Builder job = createJob(jobId, ByteSizeValue.ofBytes((long) (0.8 * maxNativeBytesPerNode))); PutJobAction.Request putJobRequest = new PutJobAction.Request(job); client().execute(PutJobAction.INSTANCE, putJobRequest).actionGet(); client().execute(OpenJobAction.INSTANCE, new OpenJobAction.Request(job.getId())).actionGet(); TrainedModelConfig model = TrainedModelConfig.builder() .setModelId("test_model") .setModelType(TrainedModelType.PYTORCH) .setModelSize((long) (0.3 * maxNativeBytesPerNode)) .setInferenceConfig(new PassThroughConfig(new VocabularyConfig(InferenceIndexConstants.nativeDefinitionStore()), null, null)) .setLocation(new IndexLocation(InferenceIndexConstants.nativeDefinitionStore())) .build(); TrainedModelDefinitionDoc modelDefinitionDoc = new TrainedModelDefinitionDoc( new BytesArray(""), model.getModelId(), 0, model.getModelSize(), model.getModelSize(), 1, true ); try (XContentBuilder builder = JsonXContent.contentBuilder()) { modelDefinitionDoc.toXContent(builder, null); client().execute( IndexAction.INSTANCE, new IndexRequest(InferenceIndexConstants.nativeDefinitionStore()).source(builder) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) ).actionGet(); } client().execute(PutTrainedModelAction.INSTANCE, new PutTrainedModelAction.Request(model, true)).actionGet(); client().execute( PutTrainedModelVocabularyAction.INSTANCE, new PutTrainedModelVocabularyAction.Request( model.getModelId(), List.of("these", "are", "my", "words", "[SEP]", "[CLS]", BertTokenizer.UNKNOWN_TOKEN, BertTokenizer.PAD_TOKEN), List.of() ) ).actionGet(); client().execute(StartTrainedModelDeploymentAction.INSTANCE, new StartTrainedModelDeploymentAction.Request(model.getModelId())) .actionGet(); setMlIndicesDelayedNodeLeftTimeoutToZero(); String jobNode = client().execute(GetJobsStatsAction.INSTANCE, new GetJobsStatsAction.Request(job.getId())) .actionGet() .getResponse() .results() .get(0) .getNode() .getId(); String modelNode = client().execute( GetTrainedModelsStatsAction.INSTANCE, new GetTrainedModelsStatsAction.Request(model.getModelId()) ).actionGet().getResources().results().get(0).getDeploymentStats().getNodeStats().get(0).getNode().getId(); // Assert the job and model were assigned to different nodes as they would not fit in the same node assertThat(jobNode, not(equalTo(modelNode))); // Stop both ML nodes internalCluster().stopNode(jobNode); internalCluster().stopNode(modelNode); // Start a new ML node internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.ML_ROLE))); assertBusy(() -> { GetJobsStatsAction.Response jobStats = client().execute( GetJobsStatsAction.INSTANCE, new GetJobsStatsAction.Request(job.getId()) ).actionGet(); assertThat(jobStats.getResponse().results().get(0).getState(), equalTo(JobState.OPENED)); }); assertBusy(() -> { GetTrainedModelsStatsAction.Response modelStats = client().execute( GetTrainedModelsStatsAction.INSTANCE, new GetTrainedModelsStatsAction.Request(model.getModelId()) ).actionGet(); assertThat(modelStats.getResources().results().get(0).getDeploymentStats().getState(), equalTo(AllocationState.STARTED)); }); // Clean up client().execute(CloseJobAction.INSTANCE, new CloseJobAction.Request(jobId).setForce(true)).actionGet(); client().execute(StopTrainedModelDeploymentAction.INSTANCE, new StopTrainedModelDeploymentAction.Request(model.getModelId())) .actionGet(); }	i think it's worth adding a todo here to say that this is where we should wait for _either_ the job or model to get assigned to the new node, then after that check a few times that the other is not assigned anywhere. it will be easy to forget as time goes by exactly what doesn't work.
public static Tree buildRandomTree(List<String> featureNames, int depth) { int numFeatures = featureNames.size(); Tree.Builder builder = Tree.builder(); builder.setFeatureNames(featureNames); TreeNode.Builder node = builder.addJunction(0, randomInt(numFeatures), true, randomDouble()); List<Integer> childNodes = List.of(node.getLeftChild(), node.getRightChild()); for (int i = 0; i < depth -1; i++) { List<Integer> nextNodes = new ArrayList<>(); for (int nodeId : childNodes) { if (i == depth -2) { builder.addLeaf(nodeId, randomDouble()); } else { TreeNode.Builder childNode = builder.addJunction(nodeId, randomInt(numFeatures), true, randomDouble()); nextNodes.add(childNode.getLeftChild()); nextNodes.add(childNode.getRightChild()); } } childNodes = nextNodes; } List<String> categoryLabels = null; if (randomBoolean()) { categoryLabels = Arrays.asList(generateRandomStringArray(randomIntBetween(1, 10), randomIntBetween(1, 10), false, false)); } return builder.setClassificationLabels(categoryLabels) .setTargetType(randomFrom(TargetType.REGRESSION, TargetType.CLASSIFICATION)) .build(); }	how about randomfrom(targettype.values())? it is more future-proof.
public void testInvalidatedApiKeysDeletedByRemover() throws Exception { Client client = client().filterWithHeader(Collections.singletonMap("Authorization", UsernamePasswordToken .basicAuthHeaderValue(SecuritySettingsSource.TEST_SUPERUSER, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING))); awaitBusySoThatExpiredApiKeysRemoverCanBeTriggered(client); List<CreateApiKeyResponse> createdApiKeys = createApiKeys(2, null); SecurityClient securityClient = new SecurityClient(client); PlainActionFuture<InvalidateApiKeyResponse> listener = new PlainActionFuture<>(); securityClient.invalidateApiKey(InvalidateApiKeyRequest.usingApiKeyId(createdApiKeys.get(0).getId()), listener); InvalidateApiKeyResponse invalidateResponse = listener.get(); assertThat(invalidateResponse.getInvalidatedApiKeys().size(), equalTo(1)); assertThat(invalidateResponse.getPreviouslyInvalidatedApiKeys().size(), equalTo(0)); assertThat(invalidateResponse.getErrors().size(), equalTo(0)); PlainActionFuture<GetApiKeyResponse> getApiKeyResponseListener = new PlainActionFuture<>(); securityClient.getApiKey(GetApiKeyRequest.usingRealmName("file"), getApiKeyResponseListener); assertThat(getApiKeyResponseListener.get().getApiKeyInfos().length, is(2)); awaitBusySoThatExpiredApiKeysRemoverCanBeTriggered(client); // invalidate API key to trigger remover listener = new PlainActionFuture<>(); securityClient.invalidateApiKey(InvalidateApiKeyRequest.usingApiKeyId(createdApiKeys.get(1).getId()), listener); assertThat(listener.get().getInvalidatedApiKeys().size(), is(1)); awaitApiKeysRemoverCompletion(); refreshSecurityIndex(); // Verify that 1st invalidated API key is deleted whereas the next one is not getApiKeyResponseListener = new PlainActionFuture<>(); securityClient.getApiKey(GetApiKeyRequest.usingRealmName("file"), getApiKeyResponseListener); assertThat(getApiKeyResponseListener.get().getApiKeyInfos().length, is(1)); ApiKey apiKey = getApiKeyResponseListener.get().getApiKeyInfos()[0]; assertThat(apiKey.getId(), is(createdApiKeys.get(1).getId())); assertThat(apiKey.isInvalidated(), is(true)); }	i think this is much simpler and easier to read: java final long lastrunmillis = streamsupport.stream(internalcluster().getinstances(apikeyservice.class).spliterator(), false) .maptolong(apikeyservice::lasttimewhenapikeysremoverwastriggered) .max() .orelsethrow();
public void testInvalidatedApiKeysDeletedByRemover() throws Exception { Client client = client().filterWithHeader(Collections.singletonMap("Authorization", UsernamePasswordToken .basicAuthHeaderValue(SecuritySettingsSource.TEST_SUPERUSER, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING))); awaitBusySoThatExpiredApiKeysRemoverCanBeTriggered(client); List<CreateApiKeyResponse> createdApiKeys = createApiKeys(2, null); SecurityClient securityClient = new SecurityClient(client); PlainActionFuture<InvalidateApiKeyResponse> listener = new PlainActionFuture<>(); securityClient.invalidateApiKey(InvalidateApiKeyRequest.usingApiKeyId(createdApiKeys.get(0).getId()), listener); InvalidateApiKeyResponse invalidateResponse = listener.get(); assertThat(invalidateResponse.getInvalidatedApiKeys().size(), equalTo(1)); assertThat(invalidateResponse.getPreviouslyInvalidatedApiKeys().size(), equalTo(0)); assertThat(invalidateResponse.getErrors().size(), equalTo(0)); PlainActionFuture<GetApiKeyResponse> getApiKeyResponseListener = new PlainActionFuture<>(); securityClient.getApiKey(GetApiKeyRequest.usingRealmName("file"), getApiKeyResponseListener); assertThat(getApiKeyResponseListener.get().getApiKeyInfos().length, is(2)); awaitBusySoThatExpiredApiKeysRemoverCanBeTriggered(client); // invalidate API key to trigger remover listener = new PlainActionFuture<>(); securityClient.invalidateApiKey(InvalidateApiKeyRequest.usingApiKeyId(createdApiKeys.get(1).getId()), listener); assertThat(listener.get().getInvalidatedApiKeys().size(), is(1)); awaitApiKeysRemoverCompletion(); refreshSecurityIndex(); // Verify that 1st invalidated API key is deleted whereas the next one is not getApiKeyResponseListener = new PlainActionFuture<>(); securityClient.getApiKey(GetApiKeyRequest.usingRealmName("file"), getApiKeyResponseListener); assertThat(getApiKeyResponseListener.get().getApiKeyInfos().length, is(1)); ApiKey apiKey = getApiKeyResponseListener.get().getApiKeyInfos()[0]; assertThat(apiKey.getId(), is(createdApiKeys.get(1).getId())); assertThat(apiKey.isInvalidated(), is(true)); }	there is no guarantee that this thread pools relative time in millis is going to be the same as the one on the node the api key service came from. also you need to check the return value of awaitbusy! i think you need to do something like: java final string[] nodenames = internalcluster().getnodenames(); string nodewithmostrecentrun = null; long apikeylasttrigger = -1l; for (string nodename : nodenames) { apikeyservice apikeyservice = internalcluster().getinstance(apikeyservice.class, nodename); if (apikeyservice.lasttimewhenapikeysremoverwastriggered() > apikeylasttrigger) { nodewithmostrecentrun = nodename; apikeylasttrigger = apikeyservice.lasttimewhenapikeysremoverwastriggered(); } } final threadpool threadpool = internalcluster().getinstance(threadpool.class, nodewithmostrecentrun); final long lastruntime = apikeylasttrigger; assertbusy(() -> { assertthat(threadpool.relativetimeinmillis() - lastruntime, greaterthan(delete_interval_millis)); });
public void testExpiredApiKeysBehaviorWhenKeysExpired1WeekBeforeAnd1DayBefore() throws Exception { Client client = client().filterWithHeader(Collections.singletonMap("Authorization", UsernamePasswordToken .basicAuthHeaderValue(SecuritySettingsSource.TEST_SUPERUSER, SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING))); awaitBusySoThatExpiredApiKeysRemoverCanBeTriggered(client); int noOfKeys = 4; List<CreateApiKeyResponse> createdApiKeys = createApiKeys(noOfKeys, null); Instant created = Instant.now(); SecurityClient securityClient = new SecurityClient(client); PlainActionFuture<GetApiKeyResponse> getApiKeyResponseListener = new PlainActionFuture<>(); securityClient.getApiKey(GetApiKeyRequest.usingRealmName("file"), getApiKeyResponseListener); assertThat(getApiKeyResponseListener.get().getApiKeyInfos().length, is(noOfKeys)); // Expire the 1st key such that it cannot be deleted by the remover // hack doc to modify the expiration time to a day before Instant dayBefore = created.minus(1L, ChronoUnit.DAYS); assertTrue(Instant.now().isAfter(dayBefore)); UpdateResponse expirationDateUpdatedResponse = client .prepareUpdate(SecurityIndexManager.SECURITY_INDEX_NAME, "doc", createdApiKeys.get(0).getId()) .setDoc("expiration_time", dayBefore.toEpochMilli()).setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE).get(); assertThat(expirationDateUpdatedResponse.getResult(), is(DocWriteResponse.Result.UPDATED)); // Expire the 2nd key such that it cannot be deleted by the remover // hack doc to modify the expiration time to the week before Instant weekBefore = created.minus(8L, ChronoUnit.DAYS); assertTrue(Instant.now().isAfter(weekBefore)); expirationDateUpdatedResponse = client .prepareUpdate(SecurityIndexManager.SECURITY_INDEX_NAME, "doc", createdApiKeys.get(1).getId()) .setDoc("expiration_time", weekBefore.toEpochMilli()).setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE).get(); assertThat(expirationDateUpdatedResponse.getResult(), is(DocWriteResponse.Result.UPDATED)); // Invalidate to trigger the remover PlainActionFuture<InvalidateApiKeyResponse> listener = new PlainActionFuture<>(); securityClient.invalidateApiKey(InvalidateApiKeyRequest.usingApiKeyId(createdApiKeys.get(2).getId()), listener); assertThat(listener.get().getInvalidatedApiKeys().size(), is(1)); awaitApiKeysRemoverCompletion(); refreshSecurityIndex(); // Verify get API keys does not return expired and deleted key getApiKeyResponseListener = new PlainActionFuture<>(); securityClient.getApiKey(GetApiKeyRequest.usingRealmName("file"), getApiKeyResponseListener); assertThat(getApiKeyResponseListener.get().getApiKeyInfos().length, is(3)); Set<String> expectedKeyIds = Sets.newHashSet(createdApiKeys.get(0).getId(), createdApiKeys.get(2).getId(), createdApiKeys.get(3).getId()); for (ApiKey apiKey : getApiKeyResponseListener.get().getApiKeyInfos()) { assertThat(apiKey.getId(), isIn(expectedKeyIds)); if (apiKey.getId().equals(createdApiKeys.get(0).getId())) { // has been expired, not invalidated assertTrue(apiKey.getExpiration().isBefore(Instant.now())); assertThat(apiKey.isInvalidated(), is(false)); } else if (apiKey.getId().equals(createdApiKeys.get(2).getId())) { // has not been expired as no expiration, but invalidated assertThat(apiKey.getExpiration(), is(nullValue())); assertThat(apiKey.isInvalidated(), is(true)); } else if (apiKey.getId().equals(createdApiKeys.get(3).getId())) { // has not been expired as no expiration, not invalidated assertThat(apiKey.getExpiration(), is(nullValue())); assertThat(apiKey.isInvalidated(), is(false)); } else { fail("unexpected API key " + apiKey); } } }	i would do this in an assertbusy and the entire method will just become: java assertbusy(() -> { final refreshresponse refreshresponse = client().admin().indices().preparerefresh(securityindexmanager.security_index_name).get(); assertthat(refreshresponse.getfailedshards(), is(0)); });
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeInt(maxNumSegments); out.writeBoolean(onlyExpungeDeletes); out.writeBoolean(flush); if (out.getVersion().onOrAfter(FORCE_MERGE_UUID_VERSION)) { out.writeString(forceMergeUUID == null ? FORCE_MERGE_UUID_NA_VALUE : forceMergeUUID); } }	i would just use readoptionalstring and writeoptionalstring, no need for the extra force_merge_uuid_na_value. it simplifies the code and no need for the extra value. sending an extra byte on the wire does not hurt either.
public IndexFieldData<?> build(IndexSettings indexSettings, MappedFieldType fieldType, IndexFieldDataCache cache, CircuitBreakerService breakerService, MapperService mapperService) { if (mapperService.isIdFieldDataEnabled() == false) { throw new IllegalArgumentException("Fielddata access on the _id field is disallowed, " + "you can re-enable it by updating the dynamic cluster setting: " + IndicesService.INDICES_ID_FIELD_DATA_ENABLED_SETTING.getKey()); } deprecationLogger.deprecatedAndMaybeLog("id_field_data", ID_FIELD_DATA_DEPRECATION_MESSAGE); final IndexFieldData<?> fieldData = fieldDataBuilder.build(indexSettings, fieldType, cache, breakerService, mapperService); return new IndexFieldData<AtomicFieldData>() { @Override public Index index() { return fieldData.index(); } @Override public String getFieldName() { return fieldData.getFieldName(); } @Override public AtomicFieldData load(LeafReaderContext context) { return wrap(fieldData.load(context)); } @Override public AtomicFieldData loadDirect(LeafReaderContext context) throws Exception { return wrap(fieldData.loadDirect(context)); } @Override public SortField sortField(Object missingValue, MultiValueMode sortMode, Nested nested, boolean reverse) { XFieldComparatorSource source = new BytesRefFieldComparatorSource(this, missingValue, sortMode, nested); return new SortField(getFieldName(), source, reverse); } @Override public BucketedSort newBucketedSort(BigArrays bigArrays, Object missingValue, MultiValueMode sortMode, Nested nested, SortOrder sortOrder, DocValueFormat format) { throw new UnsupportedOperationException("can't sort on the [" + CONTENT_TYPE + "] field"); } @Override public void clear() { fieldData.clear(); } }; } }; } } private static AtomicFieldData wrap(AtomicFieldData in) { return new AtomicFieldData() { @Override public void close() { in.close(); } @Override public long ramBytesUsed() { return in.ramBytesUsed(); } @Override public ScriptDocValues<?> getScriptValues() { return new ScriptDocValues.Strings(getBytesValues()); } @Override public SortedBinaryDocValues getBytesValues() { SortedBinaryDocValues inValues = in.getBytesValues(); return new SortedBinaryDocValues() { @Override public BytesRef nextValue() throws IOException { BytesRef encoded = inValues.nextValue(); return new BytesRef(Uid.decodeId( Arrays.copyOfRange(encoded.bytes, encoded.offset, encoded.offset + encoded.length))); } @Override public int docValueCount() { final int count = inValues.docValueCount(); // If the count is not 1 then the impl is not correct as the binary representation // does not preserve order. But id fields only have one value per doc so we are good. assert count == 1; return inValues.docValueCount(); } @Override public boolean advanceExact(int doc) throws IOException { return inValues.advanceExact(doc); } }; } }; } static MappedFieldType defaultFieldType(IndexSettings indexSettings) { MappedFieldType defaultFieldType = Defaults.FIELD_TYPE.clone(); defaultFieldType.setIndexOptions(IndexOptions.DOCS); defaultFieldType.setStored(true); return defaultFieldType; } private IdFieldMapper(IndexSettings indexSettings, MappedFieldType existing) { this(existing == null ? defaultFieldType(indexSettings) : existing, indexSettings); } private IdFieldMapper(MappedFieldType fieldType, IndexSettings indexSettings) { super(NAME, fieldType, defaultFieldType(indexSettings), indexSettings.getSettings()); } @Override public void preParse(ParseContext context) throws IOException { super.parse(context); } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { if (fieldType.indexOptions() != IndexOptions.NONE || fieldType.stored()) { BytesRef id = Uid.encodeId(context.sourceToParse().id()); fields.add(new Field(NAME, id, fieldType)); } }	curiosity question: the other exceptions just say "can only sort on numerics", but this one is formatted a bit different, is there a reason the content_type is helpful here but not elsewhere?
public void testOnlyOneNodeRunsTemplateUpdates() { TemplateUpgradeService service = new TemplateUpgradeService(Settings.EMPTY, null, clusterService, null, Collections.emptyList()); for (int i = 0; i < NODE_TEST_ITERS; i++) { int nodesCount = randomIntBetween(1, 10); int clientNodesCount = randomIntBetween(0, 4); DiscoveryNodes nodes = randomNodes(nodesCount, clientNodesCount); int updaterNode = -1; for (int j = 0; j < nodesCount; j++) { DiscoveryNodes localNodes = DiscoveryNodes.builder(nodes).localNodeId(nodes.resolveNode("node_" + j).getId()).build(); if (service.shouldLocalNodeUpdateTemplates(localNodes)) { assertThat("Expected only one node to update template, found " + updaterNode + " and " + j, updaterNode, lessThan(0)); updaterNode = j; } } assertThat("Expected one node to update template", updaterNode, greaterThanOrEqualTo(0)); } }	that should not be doesnotruntemplateupgrade i assume
@Override public DateFieldMapper build(BuilderContext context) { fieldType.setOmitNorms(fieldType.omitNorms() && boost == 1.0f); // index age check if (dateTimeFormatter.equals(Defaults.DATE_TIME_FORMATTER) && context.indexCreatedVersion().before(Version.V_2_0_0)) { dateTimeFormatter = Defaults.DATE_TIME_FORMATTER_BEFORE_2_0; } if (!locale.equals(dateTimeFormatter.locale())) { dateTimeFormatter = new FormatDateTimeFormatter(dateTimeFormatter.format(), dateTimeFormatter.parser(), dateTimeFormatter.printer(), locale); } DateFieldMapper fieldMapper = new DateFieldMapper(buildNames(context), dateTimeFormatter, fieldType.numericPrecisionStep(), boost, fieldType, docValues, nullValue, timeUnit, ignoreMalformed(context), coerce(context), similarity, normsLoading, fieldDataSettings, context.indexSettings(), multiFieldsBuilder.build(this, context), copyTo); fieldMapper.includeInAll(includeInAll); return fieldMapper; }	nit picky - can we move this to use something similar to getdatetimeformatter in timestampfieldmapper ?
public void testThatOlderIndicesAllowNonStrictDates() throws Exception { String mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("date_field").field("type", "date").endObject().endObject() .endObject().endObject().string(); String indexName = "test-" + randomAsciiOfLength(10).toLowerCase(Locale.ROOT); IndexService index = createIndex(indexName, settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_5_2_ID).build()); client().admin().indices().preparePutMapping(indexName).setType("type").setSource(mapping).get(); DocumentMapper defaultMapper = index.mapperService().documentMapper("type"); defaultMapper.parse("type", "1", XContentFactory.jsonBuilder() .startObject() .field("date_field", "1-1-1T00:00:44.000Z") .endObject() .bytes()); }	it sucks we can do this, but it helps here :)
public void testThatOlderIndicesAllowNonStrictDates() throws Exception { String mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("date_field").field("type", "date").endObject().endObject() .endObject().endObject().string(); String indexName = "test-" + randomAsciiOfLength(10).toLowerCase(Locale.ROOT); IndexService index = createIndex(indexName, settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_5_2_ID).build()); client().admin().indices().preparePutMapping(indexName).setType("type").setSource(mapping).get(); DocumentMapper defaultMapper = index.mapperService().documentMapper("type"); defaultMapper.parse("type", "1", XContentFactory.jsonBuilder() .startObject() .field("date_field", "1-1-1T00:00:44.000Z") .endObject() .bytes()); }	can we also check we can parse a proper full formatted date here? i always like to test both "good" and "bas"
public void decrypt(char[] password) throws GeneralSecurityException, IOException { if (entries.get() != null) { throw new IllegalStateException("Keystore has already been decrypted"); } if (formatVersion <= 2) { decryptLegacyEntries(); assert password.length == 0; return; } final byte[] salt; final byte[] iv; final byte[] encryptedBytes; try (ByteArrayInputStream bytesStream = new ByteArrayInputStream(dataBytes); DataInputStream input = new DataInputStream(bytesStream)) { int saltLen = input.readInt(); salt = new byte[saltLen]; input.readFully(salt); int ivLen = input.readInt(); iv = new byte[ivLen]; input.readFully(iv); int encryptedLen = input.readInt(); encryptedBytes = new byte[encryptedLen]; input.readFully(encryptedBytes); } catch (EOFException e) { throw new SecurityException("Keystore has been corrupted or tampered with"); } Cipher cipher = createCipher(Cipher.DECRYPT_MODE, password, salt, iv); try (ByteArrayInputStream bytesStream = new ByteArrayInputStream(encryptedBytes); CipherInputStream cipherStream = new CipherInputStream(bytesStream, cipher); DataInputStream input = new DataInputStream(cipherStream)) { entries.set(new HashMap<>()); int numEntries = input.readInt(); while (numEntries-- > 0) { String setting = input.readUTF(); EntryType entryType = EntryType.valueOf(input.readUTF()); int entrySize = input.readInt(); byte[] entryBytes = new byte[entrySize]; input.readFully(entryBytes); entries.get().put(setting, new Entry(entryType, entryBytes)); } if (input.read() != -1) { throw new SecurityException("Keystore has been corrupted or tampered with"); } } catch (EOFException e) { throw new SecurityException("Keystore has been corrupted or tampered with"); } }	while the exception message was the same in all cases before, we still would have been able to tell where the failed read happened (ie where in the bytes the corrupt/tamped data occurs). are we sure we want to drop the root cause exception here? i think that at least warrants a comment here as to why.
public void decrypt(char[] password) throws GeneralSecurityException, IOException { if (entries.get() != null) { throw new IllegalStateException("Keystore has already been decrypted"); } if (formatVersion <= 2) { decryptLegacyEntries(); assert password.length == 0; return; } final byte[] salt; final byte[] iv; final byte[] encryptedBytes; try (ByteArrayInputStream bytesStream = new ByteArrayInputStream(dataBytes); DataInputStream input = new DataInputStream(bytesStream)) { int saltLen = input.readInt(); salt = new byte[saltLen]; input.readFully(salt); int ivLen = input.readInt(); iv = new byte[ivLen]; input.readFully(iv); int encryptedLen = input.readInt(); encryptedBytes = new byte[encryptedLen]; input.readFully(encryptedBytes); } catch (EOFException e) { throw new SecurityException("Keystore has been corrupted or tampered with"); } Cipher cipher = createCipher(Cipher.DECRYPT_MODE, password, salt, iv); try (ByteArrayInputStream bytesStream = new ByteArrayInputStream(encryptedBytes); CipherInputStream cipherStream = new CipherInputStream(bytesStream, cipher); DataInputStream input = new DataInputStream(cipherStream)) { entries.set(new HashMap<>()); int numEntries = input.readInt(); while (numEntries-- > 0) { String setting = input.readUTF(); EntryType entryType = EntryType.valueOf(input.readUTF()); int entrySize = input.readInt(); byte[] entryBytes = new byte[entrySize]; input.readFully(entryBytes); entries.get().put(setting, new Entry(entryType, entryBytes)); } if (input.read() != -1) { throw new SecurityException("Keystore has been corrupted or tampered with"); } } catch (EOFException e) { throw new SecurityException("Keystore has been corrupted or tampered with"); } }	same comment about dropping the root cause as above.
public void testUpgradeNoop() throws Exception { KeyStoreWrapper keystore = KeyStoreWrapper.create(); SecureString seed = keystore.getString(KeyStoreWrapper.SEED_SETTING.getKey()); keystore.save(env.configFile(), new char[0]); // upgrade does not overwrite seed KeyStoreWrapper.upgrade(keystore, env.configFile(), new char[0]); assertEquals(seed.toString(), keystore.getString(KeyStoreWrapper.SEED_SETTING.getKey()).toString()); keystore = KeyStoreWrapper.load(env.configFile()); keystore.decrypt(new char[0]); assertEquals(seed.toString(), keystore.getString(KeyStoreWrapper.SEED_SETTING.getKey()).toString()); }	this seems to test just one case. can it be duplicated/refactored so it tests each of the readfully calls?
@Override final public void clear() { throw new UnsupportedOperationException(); } } /** * Store terms as a {@link BytesReference}. * <p> * When users send a query contain a lot of terms, A {@link BytesReference} can help * gc and reduce the cost of {@link #doWriteTo}, which can be slow for lots of terms. */ private static class ValuesAfterV8 extends Values { private final BytesReference valueRef; private final int size; private ValuesAfterV8(StreamInput in) throws IOException { this(in.readBytesReference()); } private ValuesAfterV8(Iterable<?> values, boolean convert) { this(serialize(values, convert)); } private ValuesAfterV8(BytesReference bytesRef) { this.valueRef = bytesRef; try (StreamInput in = valueRef.streamInput()) { size = consumerHeadersAndGetListSize(in); } catch (IOException e) { throw new UncheckedIOException(e); } } @Override public int size() { return size; } @Override public Iterator iterator() { return new Iterator<>() { private final StreamInput in; private int pos = 0; { try { in = valueRef.streamInput(); consumerHeadersAndGetListSize(in); } catch (IOException e) { throw new UncheckedIOException("failed to deserialize TermsQueryBuilder", e); } } @Override public boolean hasNext() { return pos < size; } @Override public Object next() { try { pos++; return in.readGenericValue(); } catch (IOException e) { throw new UncheckedIOException("failed to deserialize TermsQueryBuilder", e); } } }; } @Override public void writeTo(StreamOutput out) throws IOException { if (out.getVersion().onOrAfter(Version.V_8_0_0)) { out.writeBytesReference(valueRef); } else { valueRef.writeTo(out); } } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ValuesAfterV8 that = (ValuesAfterV8) o; return Objects.equals(valueRef, that.valueRef); } @Override public int hashCode() { return Objects.hash(valueRef); } private int consumerHeadersAndGetListSize(StreamInput in) throws IOException { byte genericSign = in.readByte(); assert genericSign == 7; return in.readVInt(); } } /** * package private for testing purpose * * This is for lower version requests compatible. * <p> * If we do not keep this, it could be expensive when receiving a request from * lower version. * We have to read the value list by {@link StreamInput#readGenericValue}, * serialize it into {@link BytesReference}, and then deserialize it again when * {@link #doToQuery} called}. * <p> * * TODO: remove in 9.0.0 */ static class ValuesBeforeV8 extends Values { private final List<?> values; private ValuesBeforeV8(List<?> values) throws IOException { this.values = values; } @Override public int size() { return values.size(); } @Override public boolean contains(Object o) { return values.contains(o); } @Override public Iterator iterator() { return values.iterator(); } @Override public Object[] toArray() { return values.toArray(); } @Override public Object[] toArray(Object[] a) { return values.toArray(a); } @Override public Object[] toArray(IntFunction generator) { return values.toArray(generator); } @Override public void writeTo(StreamOutput out) throws IOException { if (out.getVersion().onOrAfter(Version.V_8_0_0)) { BytesReference bytesRef = serialize(values, false); out.writeBytesReference(bytesRef); } else { out.writeGenericValue(values); } } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ValuesBeforeV8 that = (ValuesBeforeV8) o; return Objects.equals(values, that.values); }	same here, should be private too.
@Override protected QueryBuilder parseQuery(XContentParser parser) throws IOException { QueryBuilder query = super.parseQuery(parser); assertThat(query, CoreMatchers.instanceOf(TermsQueryBuilder.class)); return query; }	that seems fragile since the serialization could change in lower version after this pr so we can diverge. it's a good thing to test serialization in a unit test but it should be limited to the code that is implemented in this branch (without termsquerybuilderbeforev8).
public void testSerializationFromLowerVersion() throws IOException { { TermsQueryBuilderBeforeV8 builder = new TermsQueryBuilderBeforeV8("foo", Arrays.asList(1, 3, 4), null); TermsQueryBuilder copy = (TermsQueryBuilder) serializeAndDeserialize(Version.V_7_12_0, builder, Version.V_8_0_0, TermsQueryBuilder::new); List<Object> values = copy.values(); assertEquals(Arrays.asList(1L, 3L, 4L), values); assertTrue(copy.getValues() instanceof TermsQueryBuilder.ValuesBeforeV8); } { TermsQueryBuilderBeforeV8 builder = new TermsQueryBuilderBeforeV8("foo", Arrays.asList(1L, 3L, 4L), null); TermsQueryBuilder copy = (TermsQueryBuilder) serializeAndDeserialize(Version.V_7_12_0, builder, Version.V_8_0_0, TermsQueryBuilder::new); List<Object> values = copy.values(); assertEquals(Arrays.asList(1L, 3L, 4L), values); assertTrue(copy.getValues() instanceof TermsQueryBuilder.ValuesBeforeV8); } { TermsQueryBuilderBeforeV8 builder = new TermsQueryBuilderBeforeV8("foo", Arrays.asList("a", "b", "c"), null); TermsQueryBuilder copy = (TermsQueryBuilder) serializeAndDeserialize(Version.V_7_12_0, builder, Version.V_8_0_0, TermsQueryBuilder::new); List<Object> values = copy.values(); assertEquals(Arrays.asList("a", "b", "c"), values); assertTrue(copy.getValues() instanceof TermsQueryBuilder.ValuesBeforeV8); } { TermsQueryBuilderBeforeV8 builder = new TermsQueryBuilderBeforeV8("foo", Arrays.asList(new BytesRef("a"), new BytesRef("b"), new BytesRef("c")), null); TermsQueryBuilder copy = (TermsQueryBuilder) serializeAndDeserialize(Version.V_7_12_0, builder, Version.V_8_0_0, TermsQueryBuilder::new); List<Object> values = copy.values(); assertEquals(Arrays.asList("a", "b", "c"), values); assertTrue(copy.getValues() instanceof TermsQueryBuilder.ValuesBeforeV8); } }	it would be easier to maintain if the version are randomized. you can use versionutils#randomcompatibleversion(random, version.current) to pick compatible versions randomly. otherwise we need to change the checks on every backport/new branch.
public void deleteFiles(String path) throws URISyntaxException, StorageException { this.client.deleteFiles(this.clientName, this.locMode, container, path); }	i don't think this method is used anymore. so, maybe you should clean it up as well while you are at it.
public static void deleteAllWatcherData() throws IOException { var queryWatchesRequest = new Request("GET", "/_watcher/_query/watches"); var response = ObjectPath.createFromResponse(ESRestTestCase.adminClient().performRequest(queryWatchesRequest)); int totalCount = response.evaluate("count"); List<Map<?, ?>> watches = response.evaluate("watches"); assertThat("Less watches requested than exist in total", watches.size(), equalTo(totalCount)); for (Map<?, ?> watch : watches) { String id = (String) watch.get("_id"); var deleteWatchRequest = new Request("DELETE", "/_watcher/watch/" + id); assertOK(ESRestTestCase.adminClient().performRequest(deleteWatchRequest)); } var deleteWatchHistoryRequest = new Request("DELETE", ".watcher-history-*"); deleteWatchHistoryRequest.addParameter("ignore_unavailable", "true"); ESRestTestCase.adminClient().performRequest(deleteWatchHistoryRequest); }	nit: i assume this would be a programmers error, if so can this just be a java assert instead of an assertthat to help differentiate between something that is being tested and a self documenting sanity check. also, why less ? should be it be not equal ?
@Override public ClusterState execute(ClusterState currentState) throws Exception { final String indexName = indexNameExpressionResolver.resolveDateMathExpression(request.index()); // This will throw an exception if the index or data stream does not exist and creating it is prohibited. final boolean shouldAutoCreate = autoCreateIndex.shouldAutoCreate(indexName, currentState); if (shouldAutoCreate == false) { // The index or data stream already exists. return currentState; } final DataStreamTemplate dataStreamTemplate = resolveAutoCreateDataStream(request, currentState.metadata()); if (dataStreamTemplate != null) { CreateDataStreamClusterStateUpdateRequest createRequest = new CreateDataStreamClusterStateUpdateRequest( request.index(), request.masterNodeTimeout(), request.timeout()); ClusterState clusterState = metadataCreateDataStreamService.createDataStream(createRequest, currentState); indexNameRef.set(clusterState.metadata().dataStreams().get(request.index()).getIndices().get(0).getName()); return clusterState; } else { indexNameRef.set(indexName); CreateIndexClusterStateUpdateRequest updateRequest = new CreateIndexClusterStateUpdateRequest(request.cause(), indexName, request.index()) .ackTimeout(request.timeout()).masterNodeTimeout(request.masterNodeTimeout()); return createIndexService.applyCreateIndexRequest(currentState, updateRequest, false); } }	moving this call here has the side effect of also applying action.auto_create_index to data streams if the template does not have allow_auto_create set, which i don't think is what we wanted to do.
* @return null if no external value has been set or the value */ public <T> T parseExternalValue(Class<T> clazz) { if (!externalValueSet() || externalValue() == null) { return null; } if (!(externalValue().getClass().isAssignableFrom(clazz))) { throw new ElasticsearchIllegalArgumentException("illegal external value class [" + externalValue().getClass().getName() + "]. Should be " + clazz.getName()); } return (T) externalValue(); }	@s1monw actually this does not work for com.spatial4j.core.shape.shape as shape is an interface i guess. thanks to the tests i'm working on :-)
@Override public MetaData getSnapshotGlobalMetaData(SnapshotId snapshotId) { assert SNAPSHOT_ID.equals(snapshotId) : "RemoteClusterRepository only supports " + SNAPSHOT_ID + " as the SnapshotId"; Client remoteClient = client.getRemoteClusterClient(remoteClusterAlias); ClusterStateRequest clusterStateRequest = CcrRequests.clusterStateRequest("dummy_index_name"); ClusterStateResponse clusterState = remoteClient.admin().cluster().state(clusterStateRequest).actionGet(); return clusterState.getState().metaData(); }	you lost get comment about "dummy_index_name"
public final boolean registerListener(CancellationListener listener) { if (isCancelled) { return false; } return listeners.add(listener); }	i think i would expect registering a listener on a cancelled task to complete the listener straight away, similarly to how things like completablefuture#whencomplete and listenableactionfuture#addlistener work. it's probably ok as it's used here but i worry this will trap a future developer. it's possible that the task is cancelled after we check the iscancelled flag but before the listener is added, which would mean this method returns true but the listener is never completed.
@Override public void execute(SearchContext context) { if (context.aggregations() == null) { context.queryResult().aggregations(null); return; } if (context.queryResult().hasAggs()) { // no need to compute the aggs twice, they should be computed on a per context basis return; } Aggregator[] aggregators = context.aggregations().aggregators(); List<Aggregator> globals = new ArrayList<>(); for (int i = 0; i < aggregators.length; i++) { if (aggregators[i] instanceof GlobalAggregator) { globals.add(aggregators[i]); } } // optimize the global collector based execution if (!globals.isEmpty()) { BucketCollector globalsCollector = MultiBucketCollector.wrap(globals); Query query = context.buildFilteredQuery(Queries.newMatchAllQuery()); try { final Collector collector; if (context.getProfilers() == null) { collector = globalsCollector; } else { InternalProfileCollector profileCollector = new InternalProfileCollector( globalsCollector, CollectorResult.REASON_AGGREGATION_GLOBAL, // TODO: report on sub collectors Collections.emptyList()); collector = profileCollector; // start a new profile with this collector context.getProfilers().addQueryProfiler().setCollector(profileCollector); } globalsCollector.preCollection(); context.searcher().search(query, collector); } catch (Exception e) { throw new QueryPhaseExecutionException(context.shardTarget(), "Failed to execute global aggregators", e); } finally { context.clearReleasables(SearchContext.Lifetime.COLLECTION); } } List<InternalAggregation> aggregations = new ArrayList<>(aggregators.length); context.aggregations().resetBucketMultiConsumer(); for (Aggregator aggregator : context.aggregations().aggregators()) { try { aggregator.postCollection(); aggregations.add(aggregator.buildAggregations(new long[] {0})[0]); } catch (IOException e) { throw new AggregationExecutionException("Failed to build aggregation [" + aggregator.name() + "]", e); } } context.queryResult().aggregations(new InternalAggregations(aggregations)); // disable aggregations so that they don't run on next pages in case of scrolling context.aggregations(null); context.queryCollectors().remove(AggregationPhase.class); }	can we use aggregator.buildtoplevel() ?
private void runJob(JobTask jobTask, JobState jobState, OpenJobAction.JobParams params) { // If the job is closing, simply stop and return if (JobState.CLOSING.equals(jobState)) { // Mark as completed instead of using `stop` as stop assumes native processes have started logger.info("[{}] job got reassigned while stopping. Marking as completed", params.getJobId()); jobTask.markAsCompleted(); return; } // If the job is failed then the Persistent Task Service will // try to restart it on a node restart. Exiting here leaves the // job in the failed state and it must be force closed. if (JobState.FAILED.equals(jobState)) { return; } ActionListener<Boolean> hasRunningDatafeedTaskListener = ActionListener.wrap( hasRunningDatafeed -> { if (hasRunningDatafeed) { // This job has a running datafeed attached to it. // In order to prevent gaps in the model we revert to the current snapshot deleting intervening results. String modelSnapshotId = params.getJob().getModelSnapshotId() == null ? ModelSnapshot.EMPTY_SNAPSHOT_ID : params.getJob().getModelSnapshotId(); logger.info("[{}] job has running datafeed task; performing recovery", jobTask.getJobId()); revertToSnapshot(jobTask.getJobId(), modelSnapshotId, ActionListener.wrap( response -> openJob(jobTask), jobTask::markAsFailed )); } else { openJob(jobTask); } }, jobTask::markAsFailed ); hasRunningDatafeedTask(jobTask.getJobId(), hasRunningDatafeedTaskListener); }	can you change "performing recovery" to something like "reverting to last good model snapshot [x]" but only if modelsnapshotid != modelsnapshot.empty_snapshot_id
public FetchSubPhaseProcessor getProcessor(FetchContext context) throws IOException { if (context.fetchScores() == false) { return null; } final IndexSearcher searcher = context.searcher(); final Weight weight = searcher.createWeight(searcher.rewrite(context.rewriteQuery()), ScoreMode.COMPLETE, 1); return new FetchSubPhaseProcessor() { Scorer scorer; @Override public void setNextReader(LeafReaderContext readerContext) throws IOException { ScorerSupplier scorerSupplier = weight.scorerSupplier(readerContext); if (scorerSupplier == null) { throw new IllegalStateException("Can't compute score on document as it doesn't match the query"); } scorer = scorerSupplier.get(1L); // random-access } @Override public void process(HitContext hitContext) throws IOException { if (scorer == null || scorer.iterator().advance(hitContext.docId()) != hitContext.docId()) { throw new IllegalStateException("Can't compute score on document " + hitContext + " as it doesn't match the query"); } hitContext.hit().score(scorer.score()); } }; }	we don't need the searcher.rewrite() here, i think?
public static final boolean indexExists(final Directory directory) throws IOException { return DirectoryReader.indexExists(directory); } /** * Wait for an index to exist for up to {@code timeLimitMillis}. Returns * true if the index eventually exists, false if not. * * Will retry the directory every second for at least {@code timeLimitMillis}	nit pick, can we have a dedicated test for this class ? simple as the one in the shadowenginetests
public static final boolean indexExists(final Directory directory) throws IOException { return DirectoryReader.indexExists(directory); } /** * Wait for an index to exist for up to {@code timeLimitMillis}. Returns * true if the index eventually exists, false if not. * * Will retry the directory every second for at least {@code timeLimitMillis}	i think we should just catch interruptedexception here set the interrupted flag and return false.
@SuppressWarnings("unchecked") public void testSimpleGetFieldMappingsWithDefaults() throws Exception { assertAcked(prepareCreate("test").addMapping("type", getMappingForType("type"))); client().prepareIndex("test", "type", "1").setSource("num", 1).get(); // we need to await busily for the mapping because we don't require acking on the dynamic mapping of an index request. assertBusy(() -> { GetFieldMappingsResponse response = client().admin().indices().prepareGetFieldMappings() .setFields("num", "field1", "obj.subfield").includeDefaults(true).get(); assertNotNull(response.fieldMappings("test", "type", "num")); assertThat((Map<String, Object>) response.fieldMappings("test", "type", "num").sourceAsMap().get("num"), hasEntry("index", Boolean.TRUE)); assertThat((Map<String, Object>) response.fieldMappings("test", "type", "num").sourceAsMap().get("num"), hasEntry("type", "long")); assertNotNull(response.fieldMappings("test", "type", "field1")); assertThat((Map<String, Object>) response.fieldMappings("test", "type", "field1").sourceAsMap().get("field1"), hasEntry("index", Boolean.TRUE)); assertThat((Map<String, Object>) response.fieldMappings("test", "type", "field1").sourceAsMap().get("field1"), hasEntry("type", "text")); assertNotNull(response.fieldMappings("test", "type", "obj.subfield")); assertThat((Map<String, Object>) response.fieldMappings("test", "type", "obj.subfield").sourceAsMap().get("subfield"), hasEntry("type", "keyword")); }); }	i was wondering if the whole code block needs to be in the assertbusy ?
public static IndexAction parse(String watchId, String actionId, XContentParser parser) throws IOException { String index = null; String docId = null; DocWriteRequest.OpType opType = null; String executionTimeField = null; TimeValue timeout = null; ZoneId dynamicNameTimeZone = null; RefreshPolicy refreshPolicy = null; String currentFieldName = null; XContentParser.Token token; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (Field.INDEX.match(currentFieldName, parser.getDeprecationHandler())) { try { index = parser.text(); } catch (ElasticsearchParseException pe) { throw new ElasticsearchParseException("could not parse [{}] action [{}/{}]. failed to parse index name value for " + "field [{}]", pe, TYPE, watchId, actionId, currentFieldName); } } else if (token == XContentParser.Token.VALUE_NUMBER) { if (Field.TIMEOUT.match(currentFieldName, parser.getDeprecationHandler())) { timeout = timeValueMillis(parser.longValue()); } else { throw new ElasticsearchParseException("could not parse [{}] action [{}/{}]. unexpected number field [{}]", TYPE, watchId, actionId, currentFieldName); } } else if (token == XContentParser.Token.VALUE_STRING) { if (Field.DOC_ID.match(currentFieldName, parser.getDeprecationHandler())) { docId = parser.text(); } else if (Field.OP_TYPE.match(currentFieldName, parser.getDeprecationHandler())) { try { opType = DocWriteRequest.OpType.fromString(parser.text()); } catch (IllegalArgumentException e) { throw new ElasticsearchParseException("could not parse [{}] action [{}/{}]. failed to parse op_type value for " + "field [{}]", TYPE, watchId, actionId, currentFieldName); } } else if (Field.EXECUTION_TIME_FIELD.match(currentFieldName, parser.getDeprecationHandler())) { executionTimeField = parser.text(); } else if (Field.TIMEOUT_HUMAN.match(currentFieldName, parser.getDeprecationHandler())) { // Parser for human specified timeouts and 2.x compatibility timeout = WatcherDateTimeUtils.parseTimeValue(parser, Field.TIMEOUT_HUMAN.toString()); } else if (Field.DYNAMIC_NAME_TIMEZONE.match(currentFieldName, parser.getDeprecationHandler())) { if (token == XContentParser.Token.VALUE_STRING) { dynamicNameTimeZone = DateUtils.of(parser.text()); } else { throw new ElasticsearchParseException("could not parse [{}] action for watch [{}]. failed to parse [{}]. must be " + "a string value (e.g. 'UTC' or '+01:00').", TYPE, watchId, currentFieldName); } } else if (Field.REFRESH.match(currentFieldName, parser.getDeprecationHandler())) { refreshPolicy = RefreshPolicy.parse(parser.text()); } else { throw new ElasticsearchParseException("could not parse [{}] action [{}/{}]. unexpected string field [{}]", TYPE, watchId, actionId, currentFieldName); } } else { throw new ElasticsearchParseException("could not parse [{}] action [{}/{}]. unexpected token [{}]", TYPE, watchId, actionId, token); } } return new IndexAction(index, docId, opType, executionTimeField, timeout, dynamicNameTimeZone, refreshPolicy); }	maybe validate that only create or index op type can be used?
@Override public void afterBulk(long executionId, BulkRequest request, BulkResponse response) { long items = request.numberOfActions(); if (logger.isTraceEnabled()) { logger.trace("indexed [{}] items into ILM history index [{}], items: {}", items, Arrays.stream(response.getItems()) .map(BulkItemResponse::getIndex) .distinct() .collect(Collectors.joining(",")), request.requests().stream() .map(dwr -> ((IndexRequest) dwr).sourceAsMap()) .map(Objects::toString) .collect(Collectors.joining(","))); } if (response.hasFailures()) { IllegalStateException e = new IllegalStateException("failures during bulk request"); Arrays.stream(response.getItems()) .filter(BulkItemResponse::isFailed) .map(r->r.getFailure().getCause()) .forEach(e::addSuppressed); logger.error("failures:", e); } }	i'm not sure we should change the way we report errors. eg. could the failure cause be null ever? if so addsuppressed would fail. i think we should keep the bulkitemresponse#getfailuremessage as the reported error. what do you think?
* @param listener Called after the data stream has been created. `onResponse` called with `true` if the data stream was created, * `false` if it already existed. */ static void ensureHistoryDataStream(Client client, ClusterState state, ActionListener<Boolean> listener) { if (state.getMetadata().dataStreams().containsKey(ILM_HISTORY_ALIAS)) { listener.onResponse(false); } else { logger.debug("creating ILM history data stream [{}]", ILM_HISTORY_ALIAS); client.execute(CreateDataStreamAction.INSTANCE, new CreateDataStreamAction.Request(ILM_HISTORY_ALIAS), new ActionListener<AcknowledgedResponse>() { @Override public void onResponse(AcknowledgedResponse acknowledgedResponse) { listener.onResponse(true); } @Override public void onFailure(Exception e) { if(e instanceof IllegalArgumentException && e.getMessage().contains("exists")){ listener.onResponse(false); logger.debug( "data stream [{}] was created after checking for its existence, likely due to a concurrent call", ILM_HISTORY_ALIAS); } else { listener.onFailure(e); } } }); } }	should we maybe probe the error message to be an exact match? ie. data_stream [ilm-history-3] already exists (with corresponding parameterized data stream name) i wonder why we're not throwing resourcealreadyexists in the data stream case too.
public void refreshMapping(final String index, final String indexUUID, final String... types) { final long insertOrder; synchronized (refreshOrUpdateMutex) { insertOrder = ++refreshOrUpdateInsertOrder; refreshOrUpdateQueue.add(new RefreshTask(index, indexUUID, types)); } clusterService.submitStateUpdateTask("refresh-mapping [" + index + "][" + Arrays.toString(types) + "]", Priority.HIGH, new ProcessedClusterStateUpdateTask() { private volatile List<MappingTask> allTasks; @Override public void onFailure(String source, Throwable t) { logger.warn("failure during [{}]", t, source); } @Override public ClusterState execute(ClusterState currentState) throws Exception { Tuple<ClusterState, List<MappingTask>> tuple = executeRefreshOrUpdate(currentState, insertOrder); this.allTasks = tuple.v2(); return tuple.v1(); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { if (allTasks == null) { return; } for (Object task : allTasks) { if (task instanceof UpdateTask) { UpdateTask uTask = (UpdateTask) task; ClusterStateUpdateResponse response = new ClusterStateUpdateResponse(true); uTask.listener.onResponse(response); } } } }); }	you should really do this in a try / catch block -- we should make sure that all listeners are all in even if one is bogus / throws and exception?
public void updateMapping(final String index, final String indexUUID, final String type, final CompressedString mappingSource, final long order, final String nodeId, final ActionListener<ClusterStateUpdateResponse> listener) { final long insertOrder; synchronized (refreshOrUpdateMutex) { insertOrder = ++refreshOrUpdateInsertOrder; refreshOrUpdateQueue.add(new UpdateTask(index, indexUUID, type, mappingSource, order, nodeId, listener)); } clusterService.submitStateUpdateTask("update-mapping [" + index + "][" + type + "] / node [" + nodeId + "], order [" + order + "]", Priority.HIGH, new ProcessedClusterStateUpdateTask() { private volatile List<MappingTask> allTasks; public void onFailure(String source, Throwable t) { listener.onFailure(t); } @Override public ClusterState execute(final ClusterState currentState) throws Exception { Tuple<ClusterState, List<MappingTask>> tuple = executeRefreshOrUpdate(currentState, insertOrder); this.allTasks = tuple.v2(); return tuple.v1(); } @Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { if (allTasks == null) { return; } for (Object task : allTasks) { if (task instanceof UpdateTask) { UpdateTask uTask = (UpdateTask) task; ClusterStateUpdateResponse response = new ClusterStateUpdateResponse(true); try { uTask.listener.onResponse(response); } catch (Throwable t) { logger.debug("failed ot ping back on response of mapping processing for task [{}]", t, uTask.listener); } } } } }); }	i wonder if we should spawn this to a background thread as this is still being run on the cluster state processing thread. just be on the safe side.
@Override public AllocateEmptyPrimaryAllocationCommand build() { validate(); return new AllocateEmptyPrimaryAllocationCommand(index, shard, node, acceptDataLoss); } } @Override public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting shardRouting = null; RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { shardRouting = shard; break; } } if (shardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "primary [" + index + "][" + shardId + "] is already assigned"); } if (shardRouting.recoverySource().getType() != RecoverySource.Type.EMPTY_STORE && acceptDataLoss == false) { String dataLossWarning = "allocating an empty primary for [" + index + "][" + shardId + "] can result in data loss. Please confirm by setting the accept_data_loss parameter to true"; return explainOrThrowRejectedCommand(explain, allocation, dataLossWarning); } UnassignedInfo unassignedInfoToUpdate = null; if (shardRouting.unassignedInfo().getReason() != UnassignedInfo.Reason.FORCED_EMPTY_PRIMARY) { String unassignedInfoMessage = "force empty allocation from previous reason " + shardRouting.unassignedInfo().getReason() + ", " + shardRouting.unassignedInfo().getMessage(); unassignedInfoToUpdate = new UnassignedInfo(UnassignedInfo.Reason.FORCED_EMPTY_PRIMARY, unassignedInfoMessage, shardRouting.unassignedInfo().getFailure(), 0, System.nanoTime(), System.currentTimeMillis(), false, shardRouting.unassignedInfo().getLastAllocationStatus()); }	i think the behaviour of this block of code is clear without a comment.
@Override public AllocateEmptyPrimaryAllocationCommand build() { validate(); return new AllocateEmptyPrimaryAllocationCommand(index, shard, node, acceptDataLoss); } } @Override public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting shardRouting = null; RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { shardRouting = shard; break; } } if (shardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "primary [" + index + "][" + shardId + "] is already assigned"); } if (shardRouting.recoverySource().getType() != RecoverySource.Type.EMPTY_STORE && acceptDataLoss == false) { String dataLossWarning = "allocating an empty primary for [" + index + "][" + shardId + "] can result in data loss. Please confirm by setting the accept_data_loss parameter to true"; return explainOrThrowRejectedCommand(explain, allocation, dataLossWarning); } UnassignedInfo unassignedInfoToUpdate = null; if (shardRouting.unassignedInfo().getReason() != UnassignedInfo.Reason.FORCED_EMPTY_PRIMARY) { String unassignedInfoMessage = "force empty allocation from previous reason " + shardRouting.unassignedInfo().getReason() + ", " + shardRouting.unassignedInfo().getMessage(); unassignedInfoToUpdate = new UnassignedInfo(UnassignedInfo.Reason.FORCED_EMPTY_PRIMARY, unassignedInfoMessage, shardRouting.unassignedInfo().getFailure(), 0, System.nanoTime(), System.currentTimeMillis(), false, shardRouting.unassignedInfo().getLastAllocationStatus()); }	nit: needs a space before the colon.
public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting primaryShardRouting = null; for (RoutingNode node: allocation.routingNodes()) { for (ShardRouting shard: node) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { primaryShardRouting = shard; break; } } } if (primaryShardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate a replica shard [" + index + "][" + shardId + "], while corresponding primary shard is still unassigned"); } List<ShardRouting> replicaShardRoutings = new ArrayList<>(); RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && !shard.primary()) { replicaShardRoutings.add(shard); } } ShardRouting shardRouting; if (replicaShardRoutings.isEmpty()) { return explainOrThrowRejectedCommand(explain, allocation, "all copies of [" + index + "][" + shardId + "] are already assigned. Use the move allocation command instead"); } else { shardRouting = replicaShardRoutings.get(0); } Decision decision = allocation.deciders().canAllocate(shardRouting, routingNode, allocation); if (decision.type() == Decision.Type.NO) { // don't use explainOrThrowRejectedCommand to keep the original "NO" decision if (explain) { return new RerouteExplanation(this, decision); } throw new IllegalArgumentException("[" + name() + "] allocation of [" + index + "][" + shardId + "] on node " + discoNode + " is not allowed, reason: " + decision); } initializeUnassignedShard(allocation, routingNodes, routingNode, shardRouting); return new RerouteExplanation(this, decision); }	i think the behaviour of this block of code is clear without a comment.
public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting primaryShardRouting = null; for (RoutingNode node: allocation.routingNodes()) { for (ShardRouting shard: node) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { primaryShardRouting = shard; break; } } } if (primaryShardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate a replica shard [" + index + "][" + shardId + "], while corresponding primary shard is still unassigned"); } List<ShardRouting> replicaShardRoutings = new ArrayList<>(); RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && !shard.primary()) { replicaShardRoutings.add(shard); } } ShardRouting shardRouting; if (replicaShardRoutings.isEmpty()) { return explainOrThrowRejectedCommand(explain, allocation, "all copies of [" + index + "][" + shardId + "] are already assigned. Use the move allocation command instead"); } else { shardRouting = replicaShardRoutings.get(0); } Decision decision = allocation.deciders().canAllocate(shardRouting, routingNode, allocation); if (decision.type() == Decision.Type.NO) { // don't use explainOrThrowRejectedCommand to keep the original "NO" decision if (explain) { return new RerouteExplanation(this, decision); } throw new IllegalArgumentException("[" + name() + "] allocation of [" + index + "][" + shardId + "] on node " + discoNode + " is not allowed, reason: " + decision); } initializeUnassignedShard(allocation, routingNodes, routingNode, shardRouting); return new RerouteExplanation(this, decision); }	nit: space before colon
public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting primaryShardRouting = null; for (RoutingNode node: allocation.routingNodes()) { for (ShardRouting shard: node) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { primaryShardRouting = shard; break; } } } if (primaryShardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate a replica shard [" + index + "][" + shardId + "], while corresponding primary shard is still unassigned"); } List<ShardRouting> replicaShardRoutings = new ArrayList<>(); RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && !shard.primary()) { replicaShardRoutings.add(shard); } } ShardRouting shardRouting; if (replicaShardRoutings.isEmpty()) { return explainOrThrowRejectedCommand(explain, allocation, "all copies of [" + index + "][" + shardId + "] are already assigned. Use the move allocation command instead"); } else { shardRouting = replicaShardRoutings.get(0); } Decision decision = allocation.deciders().canAllocate(shardRouting, routingNode, allocation); if (decision.type() == Decision.Type.NO) { // don't use explainOrThrowRejectedCommand to keep the original "NO" decision if (explain) { return new RerouteExplanation(this, decision); } throw new IllegalArgumentException("[" + name() + "] allocation of [" + index + "][" + shardId + "] on node " + discoNode + " is not allowed, reason: " + decision); } initializeUnassignedShard(allocation, routingNodes, routingNode, shardRouting); return new RerouteExplanation(this, decision); }	nit: space before colon
public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting primaryShardRouting = null; for (RoutingNode node: allocation.routingNodes()) { for (ShardRouting shard: node) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { primaryShardRouting = shard; break; } } } if (primaryShardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate a replica shard [" + index + "][" + shardId + "], while corresponding primary shard is still unassigned"); } List<ShardRouting> replicaShardRoutings = new ArrayList<>(); RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && !shard.primary()) { replicaShardRoutings.add(shard); } } ShardRouting shardRouting; if (replicaShardRoutings.isEmpty()) { return explainOrThrowRejectedCommand(explain, allocation, "all copies of [" + index + "][" + shardId + "] are already assigned. Use the move allocation command instead"); } else { shardRouting = replicaShardRoutings.get(0); } Decision decision = allocation.deciders().canAllocate(shardRouting, routingNode, allocation); if (decision.type() == Decision.Type.NO) { // don't use explainOrThrowRejectedCommand to keep the original "NO" decision if (explain) { return new RerouteExplanation(this, decision); } throw new IllegalArgumentException("[" + name() + "] allocation of [" + index + "][" + shardId + "] on node " + discoNode + " is not allowed, reason: " + decision); } initializeUnassignedShard(allocation, routingNodes, routingNode, shardRouting); return new RerouteExplanation(this, decision); }	nit: space before colon
public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting primaryShardRouting = null; for (RoutingNode node: allocation.routingNodes()) { for (ShardRouting shard: node) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { primaryShardRouting = shard; break; } } } if (primaryShardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate a replica shard [" + index + "][" + shardId + "], while corresponding primary shard is still unassigned"); } List<ShardRouting> replicaShardRoutings = new ArrayList<>(); RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && !shard.primary()) { replicaShardRoutings.add(shard); } } ShardRouting shardRouting; if (replicaShardRoutings.isEmpty()) { return explainOrThrowRejectedCommand(explain, allocation, "all copies of [" + index + "][" + shardId + "] are already assigned. Use the move allocation command instead"); } else { shardRouting = replicaShardRoutings.get(0); } Decision decision = allocation.deciders().canAllocate(shardRouting, routingNode, allocation); if (decision.type() == Decision.Type.NO) { // don't use explainOrThrowRejectedCommand to keep the original "NO" decision if (explain) { return new RerouteExplanation(this, decision); } throw new IllegalArgumentException("[" + name() + "] allocation of [" + index + "][" + shardId + "] on node " + discoNode + " is not allowed, reason: " + decision); } initializeUnassignedShard(allocation, routingNodes, routingNode, shardRouting); return new RerouteExplanation(this, decision); }	the house style is to use == false instead of !.
@Override public AllocateStalePrimaryAllocationCommand build() { validate(); return new AllocateStalePrimaryAllocationCommand(index, shard, node, acceptDataLoss); } } @Override public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting shardRouting = null; RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { shardRouting = shard; break; } } if (shardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "primary [" + index + "][" + shardId + "] is already assigned"); } if (acceptDataLoss == false) { String dataLossWarning = "allocating an empty primary for [" + index + "][" + shardId + "] can result in data loss. Please " + "confirm by setting the accept_data_loss parameter to true"; return explainOrThrowRejectedCommand(explain, allocation, dataLossWarning); } if (shardRouting.recoverySource().getType() != RecoverySource.Type.EXISTING_STORE) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate an existing primary shard [" + index + "][" + shardId + "], while no such shard has ever been active"); }	i think the behaviour of this block of code is clear without a comment.
@Override public AllocateStalePrimaryAllocationCommand build() { validate(); return new AllocateStalePrimaryAllocationCommand(index, shard, node, acceptDataLoss); } } @Override public RerouteExplanation execute(RoutingAllocation allocation, boolean explain) { final DiscoveryNode discoNode; try { discoNode = allocation.nodes().resolveNode(node); } catch (IllegalArgumentException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } final RoutingNodes routingNodes = allocation.routingNodes(); RoutingNode routingNode = routingNodes.node(discoNode.getId()); if (routingNode == null) { return explainOrThrowMissingRoutingNode(allocation, explain, discoNode); } // Validate input shard and index values try { allocation.routingTable().shardRoutingTable(index, shardId).primaryShard(); } catch (IndexNotFoundException | ShardNotFoundException e) { return explainOrThrowRejectedCommand(explain, allocation, e); } ShardRouting shardRouting = null; RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned(); for (ShardRouting shard: unassigned) { if (shard.getIndexName().equals(index) && shard.getId() == shardId && shard.primary()) { shardRouting = shard; break; } } if (shardRouting == null) { return explainOrThrowRejectedCommand(explain, allocation, "primary [" + index + "][" + shardId + "] is already assigned"); } if (acceptDataLoss == false) { String dataLossWarning = "allocating an empty primary for [" + index + "][" + shardId + "] can result in data loss. Please " + "confirm by setting the accept_data_loss parameter to true"; return explainOrThrowRejectedCommand(explain, allocation, dataLossWarning); } if (shardRouting.recoverySource().getType() != RecoverySource.Type.EXISTING_STORE) { return explainOrThrowRejectedCommand(explain, allocation, "trying to allocate an existing primary shard [" + index + "][" + shardId + "], while no such shard has ever been active"); }	nit: space before colon
@Override public synchronized Status getStatus() { final String leaderIndex; if (params.getLeaderClusterAlias() != null) { leaderIndex = params.getLeaderClusterAlias() + ":" + params.getLeaderShardId().getIndexName(); } else { leaderIndex = params.getLeaderShardId().getIndexName(); } final long timeSinceLastFetchMillis; if (lastFetchTime != 0) { timeSinceLastFetchMillis = TimeUnit.NANOSECONDS.toMillis(relativeTimeProvider.getAsLong() - lastFetchTime); } else { // To avoid confusion when ccr didn't yet execute a fetch: timeSinceLastFetchMillis = 0; } return new Status( leaderIndex, getFollowShardId().getId(), leaderGlobalCheckpoint, leaderMaxSeqNo, followerGlobalCheckpoint, followerMaxSeqNo, lastRequestedSeqNo, numConcurrentReads, numConcurrentWrites, buffer.size(), currentIndexMetadataVersion, totalFetchTimeMillis, numberOfSuccessfulFetches, numberOfFailedFetches, operationsReceived, totalTransferredBytes, totalIndexTimeMillis, numberOfSuccessfulBulkOperations, numberOfFailedBulkOperations, numberOfOperationsIndexed, new TreeMap<>(fetchExceptions), timeSinceLastFetchMillis); }	i don't think it should be 0, i think it should be -1 so that we can distinguish a fetch just happened less than 1ms ago from a fetch just happened.
@Override public byte readByte() throws IOException { try { return buffer.readByte(); } catch (IndexOutOfBoundsException ex) { throw new EOFException(); } }	can you put the ex as a cause in there?
@Override public long ramBytesUsed() { return buffer.capacity(); } } private static class ByteBufStreamInput extends StreamInput { private final ByteBuf buffer; private final int endIndex; ByteBufStreamInput(ByteBuf buffer, int length) { if (length > buffer.readableBytes()) { throw new IndexOutOfBoundsException(); } this.buffer = buffer; int startIndex = buffer.readerIndex(); endIndex = startIndex + length; buffer.markReaderIndex(); } @Override public BytesReference readBytesReference(int length) throws IOException { // NOTE: It is unsafe to share a reference of the internal structure, so we // use the default implementation which will copy the bytes. It is unsafe because // a netty ByteBuf might be pooled which requires a manual release to prevent // memory leaks. return super.readBytesReference(length); } @Override public BytesRef readBytesRef(int length) throws IOException { // NOTE: It is unsafe to share a reference of the internal structure, so we // use the default implementation which will copy the bytes. It is unsafe because // a netty ByteBuf might be pooled which requires a manual release to prevent // memory leaks. return super.readBytesRef(length); } @Override public int available() throws IOException { return endIndex - buffer.readerIndex(); } @Override protected void ensureCanReadBytes(int length) throws EOFException { int bytesAvailable = endIndex - buffer.readerIndex(); if (bytesAvailable < length) { throw new EOFException("tried to read: " + length + " bytes but only " + bytesAvailable + " remaining"); } } @Override public void mark(int readlimit) { buffer.markReaderIndex(); } @Override public boolean markSupported() { return true; } @Override public int read() throws IOException { if (available() == 0) { return -1; } return buffer.readByte() & 0xff; } @Override public int read(byte[] b, int off, int len) throws IOException { if (len == 0) { return 0; } int available = available(); if (available == 0) { return -1; } len = Math.min(available, len); buffer.readBytes(b, off, len); return len; } @Override public void reset() throws IOException { buffer.resetReaderIndex(); } @Override public long skip(long n) throws IOException { if (n > Integer.MAX_VALUE) { return skipBytes(Integer.MAX_VALUE); } else { return skipBytes((int) n); } } public int skipBytes(int n) throws IOException { int nBytes = Math.min(available(), n); buffer.skipBytes(nBytes); return nBytes; } @Override public byte readByte() throws IOException { try { return buffer.readByte(); } catch (IndexOutOfBoundsException ex) { throw new EOFException(); } } @Override public void readBytes(byte[] b, int offset, int len) throws IOException { int read = read(b, offset, len); if (read < len) { throw new IndexOutOfBoundsException(); } } @Override public void close() throws IOException { // nothing to do here }	can you put the ex as a cause in there?
public void datafeedTimingStats(List<String> jobIds, Consumer<Map<String, DatafeedTimingStats>> handler, Consumer<Exception> errorHandler) { if (jobIds.isEmpty()) { handler.accept(Map.of()); return; } MultiSearchRequestBuilder msearchRequestBuilder = client.prepareMultiSearch(); for (String jobId : jobIds) { String indexName = AnomalyDetectorsIndex.jobResultsAliasedName(jobId); msearchRequestBuilder.add(createLatestDatafeedTimingStatsSearch(indexName, jobId)); } MultiSearchRequest msearchRequest = msearchRequestBuilder.request(); executeAsyncWithOrigin( client.threadPool().getThreadContext(), ML_ORIGIN, msearchRequest, ActionListener.<MultiSearchResponse>wrap( msearchResponse -> { Map<String, DatafeedTimingStats> timingStatsByJobId = new HashMap<>(); for (int i = 0; i < msearchResponse.getResponses().length; i++) { String jobId = jobIds.get(i); MultiSearchResponse.Item itemResponse = msearchResponse.getResponses()[i]; if (itemResponse.isFailure()) { errorHandler.accept(itemResponse.getFailure()); } else { SearchResponse searchResponse = itemResponse.getResponse(); ShardSearchFailure[] shardFailures = searchResponse.getShardFailures(); int unavailableShards = searchResponse.getTotalShards() - searchResponse.getSuccessfulShards(); if (shardFailures != null && shardFailures.length > 0) { LOGGER.error("[{}] Search request returned shard failures: {}", jobId, Arrays.toString(shardFailures)); errorHandler.accept( new ElasticsearchException(ExceptionsHelper.shardFailuresToErrorMsg(jobId, shardFailures))); } else if (unavailableShards > 0) { errorHandler.accept( new ElasticsearchException( "[" + jobId + "] Search request encountered [" + unavailableShards + "] unavailable shards")); } else { SearchHits hits = searchResponse.getHits(); long hitsCount = hits.getHits().length; if (hitsCount == 0) { SearchRequest searchRequest = msearchRequest.requests().get(i); LOGGER.debug("Found 0 hits for [{}]", new Object[]{searchRequest.indices()}); } else if (hitsCount > 1) { SearchRequest searchRequest = msearchRequest.requests().get(i); LOGGER.debug("Found multiple hits for [{}]", new Object[]{searchRequest.indices()}); } else { assert hitsCount == 1; SearchHit hit = hits.getHits()[0]; DatafeedTimingStats timingStats = parseSearchHit(hit, DatafeedTimingStats.PARSER, errorHandler); timingStatsByJobId.put(jobId, timingStats); } } } } handler.accept(timingStatsByJobId); }, errorHandler ), client::multiSearch); }	i don't understand how this is latest. i thought total_search_time_ms was a summation of the individual search times. ah, i see, the assumption that it is always monotonically increasing. do we account for the data feed being deleted, and a new one being created? either we should be keeping the previous data feeds search time total, or we should restart the summation back at zero. if we want an actual summation of search time for the lifetime of the job, then continuing to increase the stats from a previously deleted datafeed makes sense. if we want an accurate search time per data feed, then this search would fail as we could restart the search time statistics.
private static Client startClient(Path tempDir, TransportAddress... transportAddresses) { logger.info("--> Starting Elasticsearch Java TransportClient {}, {}", transportAddresses, tempDir); Settings.Builder clientSettingsBuilder = Settings.builder() .put("cluster.name", "qa_migrate_tests_" + counter.getAndIncrement()) .put("client.transport.ignore_cluster_name", true) .put("path.home", tempDir) .put(SecurityField.USER_SETTING.getKey(), "transport_user:x-pack-test-password"); if (Boolean.parseBoolean(System.getProperty(FIPS_SYSPROP)) && JavaVersion.current().getVersion().get(0) == 8) { clientSettingsBuilder.put(XPackSettings.DIAGNOSE_TRUST_EXCEPTIONS_SETTING.getKey(), false); } TransportClient client = new PreBuiltXPackTransportClient(clientSettingsBuilder.build()).addTransportAddresses(transportAddresses); Exception clientException = null; try { logger.info("--> Elasticsearch Java TransportClient started"); ClusterHealthResponse health = client.admin().cluster().prepareHealth().get(); logger.info("--> connected to [{}] cluster which is running [{}] node(s).", health.getClusterName(), health.getNumberOfNodes()); } catch (Exception e) { clientException = e; } assumeNoException("Sounds like your cluster is not running at " + clusterAddresses, clientException); return client; }	can't you just do this? i tried the import against your branch and it was ok. suggestion if (infipssunjssejvm()) {
private static Client startClient(Path tempDir, TransportAddress... transportAddresses) { Settings.Builder builder = Settings.builder() .put("node.name", "qa_xpack_smoke_client_" + counter.getAndIncrement()) .put("client.transport.ignore_cluster_name", true) .put("xpack.security.enabled", false) .put(Environment.PATH_HOME_SETTING.getKey(), tempDir); if (Boolean.parseBoolean(System.getProperty(FIPS_SYSPROP)) && JavaVersion.current().getVersion().get(0) == 8) { builder.put(XPackSettings.DIAGNOSE_TRUST_EXCEPTIONS_SETTING.getKey(), false); } TransportClient client = new PreBuiltXPackTransportClient(builder.build()) .addTransportAddresses(transportAddresses); logger.info("--> Elasticsearch Java TransportClient started"); Exception clientException = null; try { ClusterHealthResponse health = client.admin().cluster().prepareHealth().get(); logger.info("--> connected to [{}] cluster which is running [{}] node(s).", health.getClusterName(), health.getNumberOfNodes()); } catch (Exception e) { logger.error("Error getting cluster health", e); clientException = e; } assumeNoException("Sounds like your cluster is not running at " + clusterAddresses, clientException); return client; }	same here? suggestion if (infipssunjssejvm()) {
* and the expression that can be resolved to an alias or an index name. * @throws IllegalArgumentException if the index resolution does not lead to an index, or leads to more than one index * @return the write index obtained as a result of the index resolution */ public Index concreteWriteIndex(ClusterState state, IndicesRequest request) { String indexExpression = request.indices() != null && request.indices().length > 0 ? request.indices()[0] : null; Context context = new Context(state, request.indicesOptions(), false, true); Index[] indices = concreteIndices(context, indexExpression); // concreteIndices will throw its own exception when checking for a write index. Assert here for good measure. assert indices.length == 1 : "The index/alias and options provided did not point to a write-index"; return indices[0]; }	i don't think null is good hear (it translates to an all).
* and the expression that can be resolved to an alias or an index name. * @throws IllegalArgumentException if the index resolution does not lead to an index, or leads to more than one index * @return the write index obtained as a result of the index resolution */ public Index concreteWriteIndex(ClusterState state, IndicesRequest request) { String indexExpression = request.indices() != null && request.indices().length > 0 ? request.indices()[0] : null; Context context = new Context(state, request.indicesOptions(), false, true); Index[] indices = concreteIndices(context, indexExpression); // concreteIndices will throw its own exception when checking for a write index. Assert here for good measure. assert indices.length == 1 : "The index/alias and options provided did not point to a write-index"; return indices[0]; }	i think this needs to be stronger - the expression resolvers can in theory give you multiple expressions / indices. this needs to be a hard exception (and we should test it)
public String resolveIndexRouting(@Nullable String routing, String aliasOrIndex) { if (aliasOrIndex == null) { return routing; } AliasOrIndex result = getAliasAndIndexLookup().get(aliasOrIndex); if (result == null || result.isAlias() == false) { return routing; } AliasOrIndex.Alias alias = (AliasOrIndex.Alias) result; List<String> indexReferencesWithRouting = new ArrayList<>(); for (Tuple<String, AliasMetaData> tuple : alias.getConcreteIndexAndAliasMetaDatas()) { if (tuple.v2().indexRouting() != null) { indexReferencesWithRouting.add(tuple.v1()); } } if (indexReferencesWithRouting.isEmpty()) { return routing; } if (result.getIndices().size() > 1) { throw new IllegalArgumentException("Alias [" + alias.getAliasName() + "] references multiple indices and provides an index" + " routing for index [" + Strings.collectionToDelimitedString(indexReferencesWithRouting, ",") + "]"); } AliasMetaData aliasMd = alias.getFirstAliasMetaData(); if (aliasMd.indexRouting().indexOf(',') != -1) { throw new IllegalArgumentException("index/alias [" + aliasOrIndex + "] provided with routing value [" + aliasMd.getIndexRouting() + "] that resolved to several routing values, rejecting operation"); } if (routing != null) { if (!routing.equals(aliasMd.indexRouting())) { throw new IllegalArgumentException("Alias [" + aliasOrIndex + "] has index routing associated with it [" + aliasMd.indexRouting() + "], and was provided with routing value [" + routing + "], rejecting operation"); } } // Alias routing overrides the parent routing (if any). return aliasMd.indexRouting(); }	can we separate this into a different change, so we can discuss it?
public void testAliases() throws Exception { logger.info("--> creating index [test]"); createIndex("test"); ensureGreen(); logger.info("--> aliasing index [test] with [alias1]"); assertAcked(admin().indices().prepareAliases().addAlias("test", "alias1", false)); logger.info("--> indexing against [alias1], should fail now"); IllegalArgumentException exception = expectThrows(IllegalArgumentException.class, () -> client().index(indexRequest("alias1").type("type1").id("1").source(source("2", "test"), XContentType.JSON)).actionGet()); assertThat(exception.getMessage(), equalTo("Alias [alias1] points to an index [test] with [is_write_index=false]")); logger.info("--> aliasing index [test] with [alias1]"); assertAcked(admin().indices().prepareAliases().addAlias("test", "alias1")); logger.info("--> indexing against [alias1], should work now"); IndexResponse indexResponse = client().index(indexRequest("alias1").type("type1").id("1") .source(source("1", "test"), XContentType.JSON)).actionGet(); assertThat(indexResponse.getIndex(), equalTo("test")); logger.info("--> creating index [test_x]"); createIndex("test_x"); ensureGreen(); logger.info("--> add index [test_x] with [alias1]"); assertAcked(admin().indices().prepareAliases().addAlias("test_x", "alias1")); logger.info("--> indexing against [alias1], should fail now"); exception = expectThrows(IllegalArgumentException.class, () -> client().index(indexRequest("alias1").type("type1").id("1").source(source("2", "test"), XContentType.JSON)).actionGet()); assertThat(exception.getMessage(), equalTo("Alias [alias1] points to multiple indices with none set as a write-index [is_write_index=true]")); logger.info("--> deleting against [alias1], should fail now"); exception = expectThrows(IllegalArgumentException.class, () -> client().delete(deleteRequest("alias1").type("type1").id("1")).actionGet()); assertThat(exception.getMessage(), equalTo("Alias [alias1] points to multiple indices with none set as a write-index [is_write_index=true]")); logger.info("--> add index [test_x] with [alias1] as write-index"); assertAcked(admin().indices().prepareAliases().addAlias("test_x", "alias1", true)); logger.info("--> indexing against [alias1], should work now"); indexResponse = client().index(indexRequest("alias1").type("type1").id("1") .source(source("1", "test"), XContentType.JSON)).actionGet(); assertThat(indexResponse.getIndex(), equalTo("test_x")); logger.info("--> deleting against [alias1], should fail now"); DeleteResponse deleteResponse = client().delete(deleteRequest("alias1").type("type1").id("1")).actionGet(); assertThat(deleteResponse.getIndex(), equalTo("test_x")); logger.info("--> remove [alias1], Aliasing index [test_x] with [alias1]"); assertAcked(admin().indices().prepareAliases().removeAlias("test", "alias1").addAlias("test_x", "alias1")); logger.info("--> indexing against [alias1], should work against [test_x]"); indexResponse = client().index(indexRequest("alias1").type("type1").id("1") .source(source("1", "test"), XContentType.JSON)).actionGet(); assertThat(indexResponse.getIndex(), equalTo("test_x")); }	we now lost the part of the test that goes from two aliases without a flag to one without a flag.
*/ public String[] ignoreIndexSettings() { return ignoredIndexSettings; } /** * @return whether the local copy of the snapshot is partial ({@code true}) or complete ({@code false}	maybe drop (or adjust the doc) here?
public void testProcess_GivenDataFrameRowsJoinerFails() { givenDataFrameRows(2); RowResults rowResults1 = mock(RowResults.class); RowResults rowResults2 = mock(RowResults.class); givenProcessResults(Arrays.asList(new AnalyticsResult(rowResults1, 50, null), new AnalyticsResult(rowResults2, 100, null))); doThrow(new RuntimeException("some failure")).when(dataFrameRowsJoiner).processRowResults(any(RowResults.class)); AnalyticsResultProcessor resultProcessor = createResultProcessor(); resultProcessor.process(process); resultProcessor.awaitForCompletion(); assertThat(resultProcessor.getFailure(), equalTo("error processing results; some failure")); ArgumentCaptor<String> auditCaptor = ArgumentCaptor.forClass(String.class); verify(auditor).error(eq(JOB_ID), auditCaptor.capture()); assertThat(auditCaptor.getValue(), containsString("Error processing results; some failure")); assertThat(progressTracker.writingResultsPercent.get(), equalTo(0)); }	is it possible to replace auditcaptor.capture() with containsstring("error processing results; some failure")?
LogEntryBuilder withAuthentication(Authentication authentication) { logEntry.with(PRINCIPAL_FIELD_NAME, authentication.getUser().principal()); logEntry.with(AUTHENTICATION_TYPE_FIELD_NAME, authentication.getAuthenticationType().toString()); if (Authentication.AuthenticationType.API_KEY == authentication.getAuthenticationType()) { logEntry.with(API_KEY_ID_FIELD_NAME, (String) authentication.getMetadata().get(ApiKeyService.API_KEY_ID_KEY)); String apiKeyName = (String) authentication.getMetadata().get(ApiKeyService.API_KEY_NAME_KEY); if (apiKeyName != null) { logEntry.with(API_KEY_NAME_FIELD_NAME, (String) authentication.getMetadata().get(ApiKeyService.API_KEY_NAME_KEY)); } String creatorRealmName = (String) authentication.getMetadata().get(ApiKeyService.API_KEY_CREATOR_REALM_NAME); if (creatorRealmName != null) { // can be null for API keys created before version 7.7 logEntry.with(PRINCIPAL_REALM_FIELD_NAME, creatorRealmName); } } else { if (authentication.getUser().isRunAs()) { logEntry.with(PRINCIPAL_REALM_FIELD_NAME, authentication.getLookedUpBy().getName()) .with(PRINCIPAL_RUN_BY_FIELD_NAME, authentication.getUser().authenticatedUser().principal()) .with(PRINCIPAL_RUN_BY_REALM_FIELD_NAME, authentication.getAuthenticatedBy().getName()); } else { logEntry.with(PRINCIPAL_REALM_FIELD_NAME, authentication.getAuthenticatedBy().getName()); } } return this; }	this can be just java logentry.with(api_key_name_field_name, apikeyname);
*/ public boolean isConfigurationValidForServerUsage(SSLConfiguration sslConfiguration) { Objects.requireNonNull(sslConfiguration, "SSLConfiguration cannot be null"); return sslConfiguration.keyConfig() != KeyConfig.NONE; } /** * Indicates whether client authentication is enabled for a particular configuration * @param settings the settings used to identify the ssl configuration, typically under a *.ssl. prefix. The global configuration * will be used for fallback * @deprecated Use {@link #isSSLClientAuthEnabled(SSLConfiguration)} with {@link #getSSLConfiguration(String)}	can we handle this as part of this change or do you want it to be separate?
Map<SSLConfiguration, SSLContextHolder> loadSSLConfigurations() { Map<SSLConfiguration, SSLContextHolder> sslContextHolders = new HashMap<>(); sslContextHolders.put(globalSSLConfiguration, createSslContext(globalSSLConfiguration)); this.sslConfigurations.put("_global", globalSSLConfiguration); final Settings transportSSLSettings = settings.getByPrefix(XPackSettings.TRANSPORT_SSL_PREFIX); Map<String, Settings> sslSettingsMap = new HashMap<>(); sslSettingsMap.put(XPackSettings.HTTP_SSL_PREFIX, getHttpTransportSSLSettings(settings)); sslSettingsMap.put("xpack.http.ssl", settings.getByPrefix("xpack.http.ssl.")); sslSettingsMap.putAll(getRealmsSSLSettings(settings)); sslSettingsMap.putAll(getMonitoringExporterSettings(settings)); sslSettingsMap.forEach((key, sslSettings) -> { if (sslSettings.isEmpty()) { storeSslConfiguration(key, globalSSLConfiguration); } else { final SSLConfiguration configuration = new SSLConfiguration(sslSettings, globalSSLConfiguration); storeSslConfiguration(key, configuration); sslContextHolders.computeIfAbsent(configuration, this::createSslContext); } }); // transport is special because we want to use a auto-generated key when there isn't one final SSLConfiguration transportSSLConfiguration = new SSLConfiguration(transportSSLSettings, globalSSLConfiguration); this.transportSSLConfiguration.set(transportSSLConfiguration); this.sslConfigurations.put("_transport", transportSSLConfiguration); Map<String, Settings> profileSettings = getTransportProfileSSLSettings(settings); sslContextHolders.computeIfAbsent(transportSSLConfiguration, this::createSslContext); profileSettings.forEach((key, profileSetting) -> { final SSLConfiguration configuration = new SSLConfiguration(profileSetting, transportSSLConfiguration); storeSslConfiguration(key, configuration); sslContextHolders.computeIfAbsent(configuration, this::createSslContext); }); return Collections.unmodifiableMap(sslContextHolders); }	can you do me a favor and delete this comment? it is a left over from me that is no longer relevant
Map<SSLConfiguration, SSLContextHolder> loadSSLConfigurations() { Map<SSLConfiguration, SSLContextHolder> sslContextHolders = new HashMap<>(); sslContextHolders.put(globalSSLConfiguration, createSslContext(globalSSLConfiguration)); this.sslConfigurations.put("_global", globalSSLConfiguration); final Settings transportSSLSettings = settings.getByPrefix(XPackSettings.TRANSPORT_SSL_PREFIX); Map<String, Settings> sslSettingsMap = new HashMap<>(); sslSettingsMap.put(XPackSettings.HTTP_SSL_PREFIX, getHttpTransportSSLSettings(settings)); sslSettingsMap.put("xpack.http.ssl", settings.getByPrefix("xpack.http.ssl.")); sslSettingsMap.putAll(getRealmsSSLSettings(settings)); sslSettingsMap.putAll(getMonitoringExporterSettings(settings)); sslSettingsMap.forEach((key, sslSettings) -> { if (sslSettings.isEmpty()) { storeSslConfiguration(key, globalSSLConfiguration); } else { final SSLConfiguration configuration = new SSLConfiguration(sslSettings, globalSSLConfiguration); storeSslConfiguration(key, configuration); sslContextHolders.computeIfAbsent(configuration, this::createSslContext); } }); // transport is special because we want to use a auto-generated key when there isn't one final SSLConfiguration transportSSLConfiguration = new SSLConfiguration(transportSSLSettings, globalSSLConfiguration); this.transportSSLConfiguration.set(transportSSLConfiguration); this.sslConfigurations.put("_transport", transportSSLConfiguration); Map<String, Settings> profileSettings = getTransportProfileSSLSettings(settings); sslContextHolders.computeIfAbsent(transportSSLConfiguration, this::createSslContext); profileSettings.forEach((key, profileSetting) -> { final SSLConfiguration configuration = new SSLConfiguration(profileSetting, transportSSLConfiguration); storeSslConfiguration(key, configuration); sslContextHolders.computeIfAbsent(configuration, this::createSslContext); }); return Collections.unmodifiableMap(sslContextHolders); }	the use of _ as our prefix to avoid collisions or to make something special concerns me. in the ipfilter we use .http for http to avoid collisions since it is much harder to create a setting/name that would result in that as the key. maybe we should use . instead of _ here?
private static Map<String, Settings> getMonitoringExporterSettings(Settings settings) { Map<String, Settings> sslSettings = new HashMap<>(); Map<String, Settings> exportersSettings = settings.getGroups("xpack.monitoring.exporters."); for (Entry<String, Settings> entry : exportersSettings.entrySet()) { Settings exporterSSLSettings = entry.getValue().getByPrefix("ssl."); // Put this even if empty, so that the name will be mapped to the global SSL configuration sslSettings.put("xpack.monitoring.exporters." + entry.getKey() + ".ssl", exporterSSLSettings); } return sslSettings; }	derp istraceenabled means the warn log message only gets logged when trace is enabled
protected void doStart(ClusterState clusterState) { try { final Predicate<ClusterState> masterChangePredicate = MasterNodeChangePredicate.build(clusterState); final DiscoveryNodes nodes = clusterState.nodes(); if (nodes.isLocalNodeElectedMaster() || localExecute(request)) { // check for block, if blocked, retry, else, execute locally final ClusterBlockException blockException = checkBlock(request, clusterState); if (blockException != null) { if (!blockException.retryable()) { listener.onFailure(blockException); } else { logger.trace("can't execute due to a cluster block, retrying", blockException); retry(blockException, newState -> { try { ClusterBlockException newException = checkBlock(request, newState); return (newException == null || !newException.retryable()); } catch (Exception e) { // accept state as block will be rechecked by doStart() and listener.onFailure() then called logger.trace("exception occurred during cluster block checking, accepting state", e); return true; } }); } } else { ActionListener<Response> delegate = new ActionListener<Response>() { @Override public void onResponse(Response response) { listener.onResponse(response); } @Override public void onFailure(Exception t) { if (t instanceof Discovery.FailedToCommitClusterStateException || (t instanceof NotMasterException)) { logger.debug(() -> new ParameterizedMessage("master could not publish cluster state or stepped down before publishing action [{}], scheduling a retry", actionName), t); retry(t, masterChangePredicate); } else { listener.onFailure(t); } } }; threadPool.executor(executor).execute(new ActionRunnable(delegate) { @Override protected void doRun() throws Exception { masterOperation(task, request, clusterState, delegate); } }); } } else { if (nodes.getMasterNode() == null) { logger.debug("no known master node, scheduling a retry"); retry(null, masterChangePredicate); } else { DiscoveryNode masterNode = nodes.getMasterNode(); final String actionName = getMasterActionName(masterNode); transportService.sendRequest(masterNode, actionName, request, new ActionListenerResponseHandler<Response>(listener, TransportMasterNodeAction.this::newResponse) { @Override public void handleException(final TransportException exp) { Throwable cause = exp.unwrapCause(); if (cause instanceof ConnectTransportException) { // we want to retry here a bit to see if a new master is elected logger.debug("connection exception while trying to forward request with action name [{}] to master node [{}], scheduling a retry. Error: [{}]", actionName, nodes.getMasterNode(), exp.getDetailedMessage()); retry(cause, masterChangePredicate); } else { listener.onFailure(exp); } } }); } } } catch (Exception e) { listener.onFailure(e); } }	while it might be tricky to see in this diff what's actually changed: the three lines above here have been added.
protected void doStart(ClusterState clusterState) { try { final Predicate<ClusterState> masterChangePredicate = MasterNodeChangePredicate.build(clusterState); final DiscoveryNodes nodes = clusterState.nodes(); if (nodes.isLocalNodeElectedMaster() || localExecute(request)) { // check for block, if blocked, retry, else, execute locally final ClusterBlockException blockException = checkBlock(request, clusterState); if (blockException != null) { if (!blockException.retryable()) { listener.onFailure(blockException); } else { logger.trace("can't execute due to a cluster block, retrying", blockException); retry(blockException, newState -> { try { ClusterBlockException newException = checkBlock(request, newState); return (newException == null || !newException.retryable()); } catch (Exception e) { // accept state as block will be rechecked by doStart() and listener.onFailure() then called logger.trace("exception occurred during cluster block checking, accepting state", e); return true; } }); } } else { ActionListener<Response> delegate = new ActionListener<Response>() { @Override public void onResponse(Response response) { listener.onResponse(response); } @Override public void onFailure(Exception t) { if (t instanceof Discovery.FailedToCommitClusterStateException || (t instanceof NotMasterException)) { logger.debug(() -> new ParameterizedMessage("master could not publish cluster state or stepped down before publishing action [{}], scheduling a retry", actionName), t); retry(t, masterChangePredicate); } else { listener.onFailure(t); } } }; threadPool.executor(executor).execute(new ActionRunnable(delegate) { @Override protected void doRun() throws Exception { masterOperation(task, request, clusterState, delegate); } }); } } else { if (nodes.getMasterNode() == null) { logger.debug("no known master node, scheduling a retry"); retry(null, masterChangePredicate); } else { DiscoveryNode masterNode = nodes.getMasterNode(); final String actionName = getMasterActionName(masterNode); transportService.sendRequest(masterNode, actionName, request, new ActionListenerResponseHandler<Response>(listener, TransportMasterNodeAction.this::newResponse) { @Override public void handleException(final TransportException exp) { Throwable cause = exp.unwrapCause(); if (cause instanceof ConnectTransportException) { // we want to retry here a bit to see if a new master is elected logger.debug("connection exception while trying to forward request with action name [{}] to master node [{}], scheduling a retry. Error: [{}]", actionName, nodes.getMasterNode(), exp.getDetailedMessage()); retry(cause, masterChangePredicate); } else { listener.onFailure(exp); } } }); } } } catch (Exception e) { listener.onFailure(e); } }	while it might be tricky to see in this diff what's actually changed: the two lines above here have been added.
public XContentBuilder field(ParseField field, String value) throws IOException { return field(field.getPreferredName(), value); }	nit: i wonder what these method buy us, my concern is that we have already a lot of methods in xcontentbuilder. after all callers only have to call the other variant of these methods and pass in field.getpreferredname(). i am a bit on the fence on this. maybe let's try and do it separately and see what others think about it.
private Stream<IndexRequest> processBucketsToIndexRequests(CompositeAggregation agg) { String indexName = job.getConfig().getDestinationIndex(); List<CompositeValuesSourceBuilder<?>> sources = job.getConfig().getSourceConfig().getSources(); Collection<AggregationBuilder> aggregationBuilders = job.getConfig().getAggregationConfig().getAggregatorFactories(); return AggregationResultUtils.extractCompositeAggregationResults(agg, sources, aggregationBuilders).map(document -> { XContentBuilder builder; try { builder = jsonBuilder(); builder.startObject(); builder.map(document); builder.endObject(); } catch (IOException e) { throw new UncheckedIOException(e); } IndexRequest request = new IndexRequest(indexName, DOC_TYPE).source(builder); return request; }); }	as mentioned above, i think whatever object we return should know how to take a builder and add itself appropriately.
public void testSendSnapshotSendsOps() throws IOException { final RecoverySettings recoverySettings = new RecoverySettings(Settings.EMPTY, service); final int fileChunkSizeInBytes = recoverySettings.getChunkSize().bytesAsInt(); final StartRecoveryRequest request = getStartRecoveryRequest(); final IndexShard shard = mock(IndexShard.class); when(shard.state()).thenReturn(IndexShardState.STARTED); final RecoveryTargetHandler recoveryTarget = mock(RecoveryTargetHandler.class); final RecoverySourceHandler handler = new RecoverySourceHandler(shard, recoveryTarget, request, fileChunkSizeInBytes, Settings.EMPTY); final List<Translog.Operation> operations = new ArrayList<>(); final int initialNumberOfDocs = randomIntBetween(16, 64); for (int i = 0; i < initialNumberOfDocs; i++) { final Engine.Index index = getIndex(Integer.toString(i)); operations.add(new Translog.Index(index, new Engine.IndexResult(1, SequenceNumbers.UNASSIGNED_SEQ_NO, true))); } final int numberOfDocsWithValidSequenceNumbers = randomIntBetween(16, 64); for (int i = initialNumberOfDocs; i < initialNumberOfDocs + numberOfDocsWithValidSequenceNumbers; i++) { final Engine.Index index = getIndex(Integer.toString(i)); operations.add(new Translog.Index(index, new Engine.IndexResult(1, i - initialNumberOfDocs, true))); } operations.add(null); final long startingSeqNo = randomIntBetween(0, numberOfDocsWithValidSequenceNumbers - 1); final long requiredStartingSeqNo = randomIntBetween((int) startingSeqNo, numberOfDocsWithValidSequenceNumbers - 1); final long endingSeqNo = randomIntBetween((int) requiredStartingSeqNo, numberOfDocsWithValidSequenceNumbers - 1); // todo add proper tests RecoverySourceHandler.SendSnapshotResult result = handler.sendSnapshot(startingSeqNo, requiredStartingSeqNo, endingSeqNo, new Translog.Snapshot() { @Override public void close() { } private int counter = 0; @Override public int totalOperations() { return operations.size() - 1; } @Override public Translog.Operation next() throws IOException { return operations.get(counter++); } }); final int expectedOps = (int) (endingSeqNo - startingSeqNo + 1); assertThat(result.totalOperations, equalTo(expectedOps)); final ArgumentCaptor<List> shippedOpsCaptor = ArgumentCaptor.forClass(List.class); if (expectedOps > 0) { verify(recoveryTarget).indexTranslogOperations(shippedOpsCaptor.capture(), ArgumentCaptor.forClass(Integer.class).capture()); List<Translog.Operation> shippedOps = shippedOpsCaptor.getAllValues().stream() .flatMap(List::stream).map(o -> (Translog.Operation) o).collect(Collectors.toList()); shippedOps.sort(Comparator.comparing(Translog.Operation::seqNo)); assertThat(shippedOps.size(), equalTo(expectedOps)); for (int i = 0; i < shippedOps.size(); i++) { assertThat(shippedOps.get(i), equalTo(operations.get(i + (int) startingSeqNo + initialNumberOfDocs))); } } else { verify(recoveryTarget, never()).indexTranslogOperations(null, 0); } if (endingSeqNo >= requiredStartingSeqNo + 1) { // check that missing ops blows up List<Translog.Operation> requiredOps = operations.subList(0, operations.size() - 1).stream() // remove last null marker .filter(o -> o.seqNo() >= requiredStartingSeqNo && o.seqNo() <= endingSeqNo).collect(Collectors.toList()); List<Translog.Operation> opsToSkip = randomSubsetOf(randomIntBetween(1, requiredOps.size()), requiredOps); expectThrows(IllegalStateException.class, () -> handler.sendSnapshot(startingSeqNo, requiredStartingSeqNo, endingSeqNo, new Translog.Snapshot() { @Override public void close() { } private int counter = 0; @Override public int totalOperations() { return operations.size() - 1 - opsToSkip.size(); } @Override public Translog.Operation next() throws IOException { Translog.Operation op; do { op = operations.get(counter++); } while (op != null && opsToSkip.contains(op)); return op; } })); } }	it can start from requiredstartingseqno - 1?
public void testSendSnapshotSendsOps() throws IOException { final RecoverySettings recoverySettings = new RecoverySettings(Settings.EMPTY, service); final int fileChunkSizeInBytes = recoverySettings.getChunkSize().bytesAsInt(); final StartRecoveryRequest request = getStartRecoveryRequest(); final IndexShard shard = mock(IndexShard.class); when(shard.state()).thenReturn(IndexShardState.STARTED); final RecoveryTargetHandler recoveryTarget = mock(RecoveryTargetHandler.class); final RecoverySourceHandler handler = new RecoverySourceHandler(shard, recoveryTarget, request, fileChunkSizeInBytes, Settings.EMPTY); final List<Translog.Operation> operations = new ArrayList<>(); final int initialNumberOfDocs = randomIntBetween(16, 64); for (int i = 0; i < initialNumberOfDocs; i++) { final Engine.Index index = getIndex(Integer.toString(i)); operations.add(new Translog.Index(index, new Engine.IndexResult(1, SequenceNumbers.UNASSIGNED_SEQ_NO, true))); } final int numberOfDocsWithValidSequenceNumbers = randomIntBetween(16, 64); for (int i = initialNumberOfDocs; i < initialNumberOfDocs + numberOfDocsWithValidSequenceNumbers; i++) { final Engine.Index index = getIndex(Integer.toString(i)); operations.add(new Translog.Index(index, new Engine.IndexResult(1, i - initialNumberOfDocs, true))); } operations.add(null); final long startingSeqNo = randomIntBetween(0, numberOfDocsWithValidSequenceNumbers - 1); final long requiredStartingSeqNo = randomIntBetween((int) startingSeqNo, numberOfDocsWithValidSequenceNumbers - 1); final long endingSeqNo = randomIntBetween((int) requiredStartingSeqNo, numberOfDocsWithValidSequenceNumbers - 1); // todo add proper tests RecoverySourceHandler.SendSnapshotResult result = handler.sendSnapshot(startingSeqNo, requiredStartingSeqNo, endingSeqNo, new Translog.Snapshot() { @Override public void close() { } private int counter = 0; @Override public int totalOperations() { return operations.size() - 1; } @Override public Translog.Operation next() throws IOException { return operations.get(counter++); } }); final int expectedOps = (int) (endingSeqNo - startingSeqNo + 1); assertThat(result.totalOperations, equalTo(expectedOps)); final ArgumentCaptor<List> shippedOpsCaptor = ArgumentCaptor.forClass(List.class); if (expectedOps > 0) { verify(recoveryTarget).indexTranslogOperations(shippedOpsCaptor.capture(), ArgumentCaptor.forClass(Integer.class).capture()); List<Translog.Operation> shippedOps = shippedOpsCaptor.getAllValues().stream() .flatMap(List::stream).map(o -> (Translog.Operation) o).collect(Collectors.toList()); shippedOps.sort(Comparator.comparing(Translog.Operation::seqNo)); assertThat(shippedOps.size(), equalTo(expectedOps)); for (int i = 0; i < shippedOps.size(); i++) { assertThat(shippedOps.get(i), equalTo(operations.get(i + (int) startingSeqNo + initialNumberOfDocs))); } } else { verify(recoveryTarget, never()).indexTranslogOperations(null, 0); } if (endingSeqNo >= requiredStartingSeqNo + 1) { // check that missing ops blows up List<Translog.Operation> requiredOps = operations.subList(0, operations.size() - 1).stream() // remove last null marker .filter(o -> o.seqNo() >= requiredStartingSeqNo && o.seqNo() <= endingSeqNo).collect(Collectors.toList()); List<Translog.Operation> opsToSkip = randomSubsetOf(randomIntBetween(1, requiredOps.size()), requiredOps); expectThrows(IllegalStateException.class, () -> handler.sendSnapshot(startingSeqNo, requiredStartingSeqNo, endingSeqNo, new Translog.Snapshot() { @Override public void close() { } private int counter = 0; @Override public int totalOperations() { return operations.size() - 1 - opsToSkip.size(); } @Override public Translog.Operation next() throws IOException { Translog.Operation op; do { op = operations.get(counter++); } while (op != null && opsToSkip.contains(op)); return op; } })); } }	is this proper enough now?
public void testSendSnapshotSendsOps() throws IOException { final RecoverySettings recoverySettings = new RecoverySettings(Settings.EMPTY, service); final int fileChunkSizeInBytes = recoverySettings.getChunkSize().bytesAsInt(); final StartRecoveryRequest request = getStartRecoveryRequest(); final IndexShard shard = mock(IndexShard.class); when(shard.state()).thenReturn(IndexShardState.STARTED); final RecoveryTargetHandler recoveryTarget = mock(RecoveryTargetHandler.class); final RecoverySourceHandler handler = new RecoverySourceHandler(shard, recoveryTarget, request, fileChunkSizeInBytes, Settings.EMPTY); final List<Translog.Operation> operations = new ArrayList<>(); final int initialNumberOfDocs = randomIntBetween(16, 64); for (int i = 0; i < initialNumberOfDocs; i++) { final Engine.Index index = getIndex(Integer.toString(i)); operations.add(new Translog.Index(index, new Engine.IndexResult(1, SequenceNumbers.UNASSIGNED_SEQ_NO, true))); } final int numberOfDocsWithValidSequenceNumbers = randomIntBetween(16, 64); for (int i = initialNumberOfDocs; i < initialNumberOfDocs + numberOfDocsWithValidSequenceNumbers; i++) { final Engine.Index index = getIndex(Integer.toString(i)); operations.add(new Translog.Index(index, new Engine.IndexResult(1, i - initialNumberOfDocs, true))); } operations.add(null); final long startingSeqNo = randomIntBetween(0, numberOfDocsWithValidSequenceNumbers - 1); final long requiredStartingSeqNo = randomIntBetween((int) startingSeqNo, numberOfDocsWithValidSequenceNumbers - 1); final long endingSeqNo = randomIntBetween((int) requiredStartingSeqNo, numberOfDocsWithValidSequenceNumbers - 1); // todo add proper tests RecoverySourceHandler.SendSnapshotResult result = handler.sendSnapshot(startingSeqNo, requiredStartingSeqNo, endingSeqNo, new Translog.Snapshot() { @Override public void close() { } private int counter = 0; @Override public int totalOperations() { return operations.size() - 1; } @Override public Translog.Operation next() throws IOException { return operations.get(counter++); } }); final int expectedOps = (int) (endingSeqNo - startingSeqNo + 1); assertThat(result.totalOperations, equalTo(expectedOps)); final ArgumentCaptor<List> shippedOpsCaptor = ArgumentCaptor.forClass(List.class); if (expectedOps > 0) { verify(recoveryTarget).indexTranslogOperations(shippedOpsCaptor.capture(), ArgumentCaptor.forClass(Integer.class).capture()); List<Translog.Operation> shippedOps = shippedOpsCaptor.getAllValues().stream() .flatMap(List::stream).map(o -> (Translog.Operation) o).collect(Collectors.toList()); shippedOps.sort(Comparator.comparing(Translog.Operation::seqNo)); assertThat(shippedOps.size(), equalTo(expectedOps)); for (int i = 0; i < shippedOps.size(); i++) { assertThat(shippedOps.get(i), equalTo(operations.get(i + (int) startingSeqNo + initialNumberOfDocs))); } } else { verify(recoveryTarget, never()).indexTranslogOperations(null, 0); } if (endingSeqNo >= requiredStartingSeqNo + 1) { // check that missing ops blows up List<Translog.Operation> requiredOps = operations.subList(0, operations.size() - 1).stream() // remove last null marker .filter(o -> o.seqNo() >= requiredStartingSeqNo && o.seqNo() <= endingSeqNo).collect(Collectors.toList()); List<Translog.Operation> opsToSkip = randomSubsetOf(randomIntBetween(1, requiredOps.size()), requiredOps); expectThrows(IllegalStateException.class, () -> handler.sendSnapshot(startingSeqNo, requiredStartingSeqNo, endingSeqNo, new Translog.Snapshot() { @Override public void close() { } private int counter = 0; @Override public int totalOperations() { return operations.size() - 1 - opsToSkip.size(); } @Override public Translog.Operation next() throws IOException { Translog.Operation op; do { op = operations.get(counter++); } while (op != null && opsToSkip.contains(op)); return op; } })); } }	i think this does not verify the right thing. you check that it's never called with the arguments null and 0, whereas you want to check that it's never called with any arguments.
public void testSendSnapshotSendsOps() throws IOException { final RecoverySettings recoverySettings = new RecoverySettings(Settings.EMPTY, service); final int fileChunkSizeInBytes = recoverySettings.getChunkSize().bytesAsInt(); final StartRecoveryRequest request = getStartRecoveryRequest(); final IndexShard shard = mock(IndexShard.class); when(shard.state()).thenReturn(IndexShardState.STARTED); final RecoveryTargetHandler recoveryTarget = mock(RecoveryTargetHandler.class); final RecoverySourceHandler handler = new RecoverySourceHandler(shard, recoveryTarget, request, fileChunkSizeInBytes, Settings.EMPTY); final List<Translog.Operation> operations = new ArrayList<>(); final int initialNumberOfDocs = randomIntBetween(16, 64); for (int i = 0; i < initialNumberOfDocs; i++) { final Engine.Index index = getIndex(Integer.toString(i)); operations.add(new Translog.Index(index, new Engine.IndexResult(1, SequenceNumbers.UNASSIGNED_SEQ_NO, true))); } final int numberOfDocsWithValidSequenceNumbers = randomIntBetween(16, 64); for (int i = initialNumberOfDocs; i < initialNumberOfDocs + numberOfDocsWithValidSequenceNumbers; i++) { final Engine.Index index = getIndex(Integer.toString(i)); operations.add(new Translog.Index(index, new Engine.IndexResult(1, i - initialNumberOfDocs, true))); } operations.add(null); final long startingSeqNo = randomIntBetween(0, numberOfDocsWithValidSequenceNumbers - 1); final long requiredStartingSeqNo = randomIntBetween((int) startingSeqNo, numberOfDocsWithValidSequenceNumbers - 1); final long endingSeqNo = randomIntBetween((int) requiredStartingSeqNo, numberOfDocsWithValidSequenceNumbers - 1); // todo add proper tests RecoverySourceHandler.SendSnapshotResult result = handler.sendSnapshot(startingSeqNo, requiredStartingSeqNo, endingSeqNo, new Translog.Snapshot() { @Override public void close() { } private int counter = 0; @Override public int totalOperations() { return operations.size() - 1; } @Override public Translog.Operation next() throws IOException { return operations.get(counter++); } }); final int expectedOps = (int) (endingSeqNo - startingSeqNo + 1); assertThat(result.totalOperations, equalTo(expectedOps)); final ArgumentCaptor<List> shippedOpsCaptor = ArgumentCaptor.forClass(List.class); if (expectedOps > 0) { verify(recoveryTarget).indexTranslogOperations(shippedOpsCaptor.capture(), ArgumentCaptor.forClass(Integer.class).capture()); List<Translog.Operation> shippedOps = shippedOpsCaptor.getAllValues().stream() .flatMap(List::stream).map(o -> (Translog.Operation) o).collect(Collectors.toList()); shippedOps.sort(Comparator.comparing(Translog.Operation::seqNo)); assertThat(shippedOps.size(), equalTo(expectedOps)); for (int i = 0; i < shippedOps.size(); i++) { assertThat(shippedOps.get(i), equalTo(operations.get(i + (int) startingSeqNo + initialNumberOfDocs))); } } else { verify(recoveryTarget, never()).indexTranslogOperations(null, 0); } if (endingSeqNo >= requiredStartingSeqNo + 1) { // check that missing ops blows up List<Translog.Operation> requiredOps = operations.subList(0, operations.size() - 1).stream() // remove last null marker .filter(o -> o.seqNo() >= requiredStartingSeqNo && o.seqNo() <= endingSeqNo).collect(Collectors.toList()); List<Translog.Operation> opsToSkip = randomSubsetOf(randomIntBetween(1, requiredOps.size()), requiredOps); expectThrows(IllegalStateException.class, () -> handler.sendSnapshot(startingSeqNo, requiredStartingSeqNo, endingSeqNo, new Translog.Snapshot() { @Override public void close() { } private int counter = 0; @Override public int totalOperations() { return operations.size() - 1 - opsToSkip.size(); } @Override public Translog.Operation next() throws IOException { Translog.Operation op; do { op = operations.get(counter++); } while (op != null && opsToSkip.contains(op)); return op; } })); } }	formatting nit: can you move the while behind the closing bracket }, not in a new line?
*/ public static SearchResponse combineResponses(MultiSearchResponse.Item[] msearchResponses, InternalAggregation.ReduceContext reduceContext) throws Exception { assert msearchResponses.length >= 2; boolean first = true; SearchResponse liveResponse = null; List<SearchResponse> rolledResponses = new ArrayList<>(); for (MultiSearchResponse.Item item : msearchResponses) { if (item.isFailure()) { Exception e = item.getFailure(); // We can tolerate missing indices, assuming there is at least one to process at the end if (e instanceof IndexNotFoundException) { String targetIndex = first ? "\\\\"Live\\\\"" : "Rollup"; logger.warn(targetIndex + " index not found during rollup search.", e); } else { throw e; } } else { // No error, add to responses if (first) { liveResponse = item.getResponse(); } else { rolledResponses.add(item.getResponse()); } } first = false; } // If we only have a live index left, just return it directly. We know it can't be an error already if (rolledResponses.isEmpty() && liveResponse != null) { return liveResponse; } else if (rolledResponses.isEmpty()) { throw new ResourceNotFoundException("No indices (live or rollup) found during rollup search"); } return doCombineResponse(liveResponse, rolledResponses, reduceContext); }	not sure we should log anything here, shouldn't this be the normal case when the live index is removed ? i also wonder if it is possible to get an indexnotfoundexception on the rollup search since we validate that the index exists before sending the query ?
public static SearchResponse translateResponse(MultiSearchResponse.Item[] rolledMsearch, InternalAggregation.ReduceContext reduceContext) throws Exception { assert rolledMsearch.length > 0; List<SearchResponse> responses = new ArrayList<>(); for (MultiSearchResponse.Item item : rolledMsearch) { if (item.isFailure()) { Exception e = item.getFailure(); // We can tolerate missing indices, assuming there is at least one to process at the end if (e instanceof IndexNotFoundException) { logger.warn("Rollup index not found during rollup search.", e); } else { throw e; } } else { // No error, add to responses responses.add(item.getResponse()); } } // If we have no responses here, all the responses were index-not-found, so we need to fail if (responses.isEmpty()) { throw new ResourceNotFoundException("Rollup index not found during rollup search", rolledMsearch[0].getFailure()); } return doCombineResponse(null, responses, reduceContext); } /** * Combines an msearch with rollup + live aggregations into a SearchResponse * representing the union of the two responses. The response format is identical to * a non-rollup search response (aka a "normal aggregation" response). * * If the MSearch Response returns the following: * * <pre>{@code * [ * { * "took":228, * "timed_out":false, * "_shards":{...}, * "hits":{...}, * "aggregations":{ * "histo":{ * "buckets":[ * { * "key_as_string":"2017-05-15T00:00:00.000Z", * "key":1494806400000, * "doc_count":1, * "the_max":{ * "value":1.0 * } * } * ] * } * } * }, * { * "took":205, * "timed_out":false, * "_shards":{...}, * "hits":{...}, * "aggregations":{ * "filter_histo":{ * "doc_count":1, * "histo":{ * "buckets":[ * { * "key_as_string":"2017-05-14T00:00:00.000Z", * "key":1494720000000, * "doc_count":1, * "the_max":{ * "value":19995.0 * }, * "histo._count":{ * "value":1.0E9 * } * } * ] * } * } * } * } * }</pre> * * It would be collapsed into: * * <pre>{@code * { * "took": 228, * "timed_out": false, * "_shards": {...}, * "hits": {...}, * "aggregations": { * "histo": { * "buckets": [ * { * "key_as_string": "2017-05-14T00:00:00.000Z", * "key": 1494720000000, * "doc_count": 1000000000, * "the_max": { * "value": 19995 * } * }, * { * "key_as_string": "2017-05-15T00:00:00.000Z", * "key": 1494806400000, * "doc_count": 1, * "the_max": { * "value": 1 * } * } * ] * } * } * } * }</pre> * * It essentially takes the conventions listed in {@link RollupRequestTranslator}	i wonder how it is possible to get an indexnotfoundexception since we validate that the indices exist before sending the request ? not sure that logging a warning would help here, if it's not a normal case we should think of a way to convey this information in the response.
public void run() { SearchContext context = null; try { long now = System.nanoTime(); ShardSearchRequest request = new ShardSearchRequest(indexShard.shardId().index().name(), indexShard.shardId().id(), indexMetaData.numberOfShards(), SearchType.QUERY_THEN_FETCH) .source(entry.source()) .types(entry.types()) .queryCache(entry.queryCache()); context = createContext(request, warmerContext.searcher()); // if we use sort, we need to do query to sort on it and load relevant field data // if not, we might as well use COUNT (and cache if needed) if (context.sort() == null) { context.searchType(SearchType.COUNT); } boolean canCache = indicesQueryCache.canCache(request, context); // early terminate when we can cache, since we can only do proper caching on top level searcher // also, if we can't cache, and its top, we don't need to execute it, since we already did when its not top if ((canCache && !top) || (!canCache && top)) { return; } if (canCache) { indicesQueryCache.load(request, context, queryPhase); } else { queryPhase.execute(context); } long took = System.nanoTime() - now; if (indexShard.warmerService().logger().isTraceEnabled()) { indexShard.warmerService().logger().trace("warmed [{}], took [{}]", entry.name(), TimeValue.timeValueNanos(took)); } } catch (Throwable t) { indexShard.warmerService().logger().warn("warmer [{}] failed", t, entry.name()); } finally { try { if (context != null) { freeContext(context); cleanContext(context); } } finally { latch.countDown(); } } }	this looks like a complicated way to say if (cancache != top)?
CreateIndexRequestBuilder createIndex(String indexName, String type, String fieldName) throws Exception { return client().admin().indices().prepareCreate(indexName).setSettings(ImmutableSettings.builder().put(SINGLE_SHARD_NO_REPLICA).put(SearchService.NORMS_LOADING_KEY, Loading.LAZY_VALUE)).addMapping(type, JsonXContent.contentBuilder() .startObject() .startObject(type) .startObject("properties") .startObject(fieldName) .field("type", "string") .startObject("norms") .field("loading", Loading.EAGER_VALUE) .endObject() .endObject() .endObject() .endObject() .endObject() ); }	i tend to find it easier to read when indented?
@Override public Double doubleValue(Object endpointValue) { assert endpointValue instanceof Double; return (Double)endpointValue; }	style nit: should have space between cast and variable
@Override protected Aggregator doCreateInternal(ValuesSource valuesSource, Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException { if (collectsFromSingleBucket == false) { return asMultiBucketAggregator(this, context, parent); } if (valuesSource instanceof ValuesSource.Numeric) { return new NumericHistogramAggregator(name, factories, interval, offset, order, keyed, minDocCount, minBound, maxBound, (ValuesSource.Numeric) valuesSource, config.format(), context, parent, pipelineAggregators, metaData); } else if (valuesSource instanceof ValuesSource.Bytes.FieldData.RangeFieldData) { ValuesSource.Bytes.FieldData.RangeFieldData rangeValueSource = (ValuesSource.Bytes.FieldData.RangeFieldData) valuesSource; if (rangeValueSource.rangeType().isNumeric() == false) { throw new IllegalArgumentException("Expected numeric range type but found non-numeric range [" + rangeValueSource.rangeType().name + "]"); } return new RangeHistogramAggregator(name, factories, interval, offset, order, keyed, minDocCount, minBound, maxBound, (ValuesSource.Bytes.FieldData.RangeFieldData) valuesSource, config.format(), context, parent, pipelineAggregators, metaData); } else { throw new IllegalArgumentException("Expected one of [Numeric, RangeFieldData] values source, found [" + valuesSource.toString() + "]"); } }	style nit: else if/else should go next to last clause's bracket
@Override protected Aggregator doCreateInternal(ValuesSource valuesSource, Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException { if (collectsFromSingleBucket == false) { return asMultiBucketAggregator(this, context, parent); } if (valuesSource instanceof ValuesSource.Numeric) { return new NumericHistogramAggregator(name, factories, interval, offset, order, keyed, minDocCount, minBound, maxBound, (ValuesSource.Numeric) valuesSource, config.format(), context, parent, pipelineAggregators, metaData); } else if (valuesSource instanceof ValuesSource.Bytes.FieldData.RangeFieldData) { ValuesSource.Bytes.FieldData.RangeFieldData rangeValueSource = (ValuesSource.Bytes.FieldData.RangeFieldData) valuesSource; if (rangeValueSource.rangeType().isNumeric() == false) { throw new IllegalArgumentException("Expected numeric range type but found non-numeric range [" + rangeValueSource.rangeType().name + "]"); } return new RangeHistogramAggregator(name, factories, interval, offset, order, keyed, minDocCount, minBound, maxBound, (ValuesSource.Bytes.FieldData.RangeFieldData) valuesSource, config.format(), context, parent, pipelineAggregators, metaData); } else { throw new IllegalArgumentException("Expected one of [Numeric, RangeFieldData] values source, found [" + valuesSource.toString() + "]"); } }	i'm almost positive this is the wrong thing to do here, but i'm not sure what the right thing to do is.
public static <VS extends ValuesSource> ValuesSourceConfig<VS> resolve( QueryShardContext context, ValueType valueType, String field, Script script, Object missing, ZoneId timeZone, String format) { if (field == null) { if (script == null) { ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(ValuesSourceType.ANY); config.format(resolveFormat(null, valueType, timeZone)); return config; } ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : ValuesSourceType.ANY; if (valuesSourceType == ValuesSourceType.ANY) { // the specific value source type is undefined, but for scripts, // we need to have a specific value source // type to know how to handle the script values, so we fallback // on Bytes valuesSourceType = ValuesSourceType.BYTES; } ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType); config.missing(missing); config.timezone(timeZone); config.format(resolveFormat(format, valueType, timeZone)); config.script(createScript(script, context)); config.scriptValueType(valueType); return config; } MappedFieldType fieldType = context.fieldMapper(field); if (fieldType == null) { ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : ValuesSourceType.ANY; ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType); config.missing(missing); config.timezone(timeZone); config.format(resolveFormat(format, valueType, timeZone)); config.unmapped(true); if (valueType != null) { // todo do we really need this for unmapped? config.scriptValueType(valueType); } return config; } IndexFieldData<?> indexFieldData = context.getForField(fieldType); ValuesSourceConfig<VS> config; if (valueType == null) { if (indexFieldData instanceof IndexNumericFieldData) { config = new ValuesSourceConfig<>(ValuesSourceType.NUMERIC); } else if (indexFieldData instanceof IndexGeoPointFieldData) { config = new ValuesSourceConfig<>(ValuesSourceType.GEOPOINT); } else { if (fieldType instanceof RangeFieldMapper.RangeFieldType) { config = new ValuesSourceConfig<>(ValuesSourceType.RANGE); } else { config = new ValuesSourceConfig<>(ValuesSourceType.BYTES); } } } else { config = new ValuesSourceConfig<>(valueType.getValuesSourceType()); } config.fieldContext(new FieldContext(field, indexFieldData, fieldType)); config.missing(missing); config.timezone(timeZone); config.script(createScript(script, context)); config.format(fieldType.docValueFormat(format, timeZone)); return config; }	maybe promote this up to an else if? not sure if it'd be cleaner or not, but it's nice to reduce nested if where possible.
private VS originalValuesSource() { if (fieldContext() == null) { if (valueSourceType() == ValuesSourceType.NUMERIC) { return (VS) numericScript(); } if (valueSourceType() == ValuesSourceType.BYTES) { return (VS) bytesScript(); } // TODO: Do we need a range script case? throw new AggregationExecutionException("value source of type [" + valueSourceType().name() + "] is not supported by scripts"); } if (valueSourceType() == ValuesSourceType.NUMERIC) { return (VS) numericField(); } if (valueSourceType() == ValuesSourceType.GEOPOINT) { return (VS) geoPointField(); } if (valueSourceType() == ValuesSourceType.RANGE) { return (VS) rangeField(); } // falling back to bytes values return (VS) bytesField(); }	probably, eventually. i suspect it's non-trivial work though, so ok marking that as a limitation for first pass and we can revisit.
protected WritePrimaryResult<BulkShardOperationsRequest, BulkShardOperationsResponse> shardOperationOnPrimary( final BulkShardOperationsRequest request, final IndexShard primary) throws Exception { return shardOperationOnPrimary(request.shardId(), request.getOperations(), primary, logger); }	i am fine with this.
public static Connection csvConnection(CsvTestCase csvTest) throws IOException, SQLException { Properties csvProperties = new Properties(); csvProperties.setProperty("charset", "UTF-8"); csvProperties.setProperty("separator", "|"); csvProperties.setProperty("trimValues", "true"); // Converter when type is java.sql.Time use this property csvProperties.setProperty("timeFormat", "HH:mm:ss.SSSX"); Tuple<String, String> resultsAndTypes = extractColumnTypesAndStripCli(csvTest.earlySchema, csvTest.expectedResults); csvProperties.setProperty("columnTypes", resultsAndTypes.v2()); Reader reader = new StringReader(resultsAndTypes.v1()); TableReader tableReader = new TableReader() { @Override public Reader getReader(Statement statement, String tableName) throws SQLException { return reader; } @Override public List<String> getTableNames(Connection connection) throws SQLException { throw new UnsupportedOperationException(); } }; return new CsvConnection(tableReader, csvProperties, "") { }; }	i don't understand this comment.
public void testGetSnapshotsWithSnapshotInProgress() throws Exception { createRepository("test-repo", "mock", Settings.builder().put("location", randomRepoPath()).put("block_on_data", true)); String indexName = "test-idx-1"; createIndexWithContent(indexName, indexSettingsNoReplicas(randomIntBetween(1, 10)).build()); ensureGreen(); ActionFuture<CreateSnapshotResponse> createSnapshotResponseActionFuture = startFullSnapshot("test-repo", "test-snap"); logger.info("--> wait for data nodes to get blocked"); waitForBlockOnAnyDataNode("test-repo"); awaitNumberOfSnapshotsInProgress(1); GetSnapshotsResponse response1 = client().admin() .cluster() .prepareGetSnapshots("test-repo") .setSnapshots("test-snap") .setIgnoreUnavailable(true) .get(); List<SnapshotInfo> snapshotInfoList = response1.getSnapshots(); assertEquals(1, snapshotInfoList.size()); SnapshotInfo snapshotInfo = snapshotInfoList.get(0); assertEquals(SnapshotState.IN_PROGRESS, snapshotInfo.state()); SnapshotStatus snapshotStatus = client().admin().cluster().prepareSnapshotStatus().execute().actionGet().getSnapshots().get(0); assertThat(snapshotInfo.totalShards(), equalTo(snapshotStatus.getIndices().get(indexName).getShardsStats().getTotalShards())); assertThat( snapshotInfo.successfulShards(), lessThanOrEqualTo(snapshotStatus.getIndices().get(indexName).getShardsStats().getDoneShards()) ); assertThat(snapshotInfo.shardFailures().size(), equalTo(0)); String notExistedSnapshotName = "snapshot_not_exist"; GetSnapshotsResponse response2 = client().admin() .cluster() .prepareGetSnapshots("test-repo") .setSnapshots(notExistedSnapshotName) .setIgnoreUnavailable(true) .get(); assertEquals(0, response2.getSnapshots().size()); expectThrows( SnapshotMissingException.class, () -> client().admin() .cluster() .prepareGetSnapshots("test-repo") .setSnapshots(notExistedSnapshotName) .setIgnoreUnavailable(false) .execute() .actionGet() ); logger.info("--> unblock all data nodes"); unblockAllDataNodes("test-repo"); assertSuccessful(createSnapshotResponseActionFuture); }	same here: i had to relax the condition to lessthanorequalto from equalto, because the getsnapshot and snapshotstatus api calls happen with a delay, so we can't guarantee that successfulshards and doneshards are actually the same
private static void checkBooleanFiltering(LogicalPlan p, Set<Failure> localFailures) { if (p instanceof Filter) { Expression condition = ((Filter) p).condition(); if (condition.resolved() && condition.dataType() != BOOLEAN) { localFailures.add(fail(condition, "Cannot filter by non-boolean expression of type [{}]", condition.dataType())); } } }	no need to check if the condition is resolved, this has already been checked before - these checks get applied only if the plan is fully resolved.
private static void checkBooleanFiltering(LogicalPlan p, Set<Failure> localFailures) { if (p instanceof Filter) { Expression condition = ((Filter) p).condition(); if (condition.resolved() && condition.dataType() != BOOLEAN) { localFailures.add(fail(condition, "Cannot filter by non-boolean expression of type [{}]", condition.dataType())); } } }	maybe condition expression needs to be boolean, found [{}]. remove the notion of filter which is not something the user declares.
private static boolean isAllowedTypeForClaim(Object o) { return (o instanceof String || o instanceof Boolean || o instanceof Number || (o instanceof Collection && ((Collection) o).stream() .allMatch(c -> c instanceof String || c instanceof Boolean || c instanceof Number))); }	i think we probably want to ensure that these are homogenous collections. i don't know how we would make sense of [ "1", true, 1.0 ]
private static void validateClaimType(Object claimValueObject, String settingKey) { if (claimValueObject instanceof String == false && claimValueObject instanceof Collection && ((Collection) claimValueObject).stream().allMatch(c -> c instanceof String) == false) { throw new SettingsException("Setting [ " + settingKey + " expects a claim with String or a String Array value but found a " + claimValueObject.getClass().getName()); } }	i'd prefer a more descriptive method name if possible. the current feels like it will validate the claim value is a certain specified type. but it really just asserts the value is either: * a string or * if it is a collection, it must be a collection of string now i am not sure if this is what we really want? is it ok if the value is a number?
static ClaimParser forSetting(Logger logger, OpenIdConnectRealmSettings.ClaimSetting setting, RealmConfig realmConfig, boolean required) { if (realmConfig.hasSetting(setting.getClaim())) { String claimName = realmConfig.getSetting(setting.getClaim()); if (realmConfig.hasSetting(setting.getPattern())) { Pattern regex = Pattern.compile(realmConfig.getSetting(setting.getPattern())); return new ClaimParser( "OpenID Connect Claim [" + claimName + "] with pattern [" + regex.pattern() + "] for [" + setting.name(realmConfig) + "]", claims -> { Object claimValueObject = claims.getClaim(claimName); validateClaimType(claimValueObject, RealmSettings.getFullSettingKey(realmConfig, setting.getClaim())); List<String> values; if (claimValueObject == null) { values = List.of(); } else if (claimValueObject instanceof String) { values = List.of((String) claimValueObject); } else if (claimValueObject instanceof Collection) { validateClaimType(claimValueObject, RealmSettings.getFullSettingKey(realmConfig, setting.getClaim())); values = (List<String>) claimValueObject; } else { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + " expects a claim with String or a String Array value but found a " + claimValueObject.getClass().getName()); } return values.stream().map(s -> { if (s == null) { logger.debug("OpenID Connect Claim [{}] is null", claimName); return null; } final Matcher matcher = regex.matcher(s); if (matcher.find() == false) { logger.debug("OpenID Connect Claim [{}] is [{}], which does not match [{}]", claimName, s, regex.pattern()); return null; } final String value = matcher.group(1); if (Strings.isNullOrEmpty(value)) { logger.debug("OpenID Connect Claim [{}] is [{}], which does match [{}] but group(1) is empty", claimName, s, regex.pattern()); return null; } return value; }).filter(Objects::nonNull).collect(Collectors.toUnmodifiableList()); }); } else { return new ClaimParser( "OpenID Connect Claim [" + claimName + "] for [" + setting.name(realmConfig) + "]", claims -> { Object claimValueObject = claims.getClaim(claimName); validateClaimType(claimValueObject, RealmSettings.getFullSettingKey(realmConfig, setting.getClaim())); if (claimValueObject == null) { return List.of(); } else if (claimValueObject instanceof String) { return List.of((String) claimValueObject); } return ((List<String>) claimValueObject).stream() .filter(Objects::nonNull) .collect(Collectors.toUnmodifiableList()); }); } } else if (required) { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + "] is required"); } else if (realmConfig.hasSetting(setting.getPattern())) { throw new SettingsException("Setting [" + RealmSettings.getFullSettingKey(realmConfig, setting.getPattern()) + "] cannot be set unless [" + RealmSettings.getFullSettingKey(realmConfig, setting.getClaim()) + "] is also set"); } else { return new ClaimParser("No OpenID Connect Claim for [" + setting.name(realmConfig) + "]", attributes -> List.of()); } }	the new validateclaimtype method does not assert claimvalueobject is a list. in theory it could be a set or a number.
private void assertCount(RestClient client, int count) throws IOException { Map<String, Object> expected = new HashMap<>(); String mode = randomMode(); expected.put("columns", singletonList(columnInfo(mode, "COUNT(*)", "long", JDBCType.BIGINT, 20))); expected.put("rows", singletonList(singletonList(count))); Request request = new Request("POST", SQL_QUERY_REST_ENDPOINT); request.setJsonEntity("{\\\\"query\\\\": \\\\"SELECT COUNT(*) FROM test\\\\"" + mode(mode) + version(mode) + "}"); Map<String, Object> actual = toMap(client.performRequest(request), mode); if (false == expected.equals(actual)) { NotEqualMessageBuilder message = new NotEqualMessageBuilder(); message.compareMaps(actual, expected); fail("Response does not match:\\\\n" + message.toString()); } }	it's worth adding an utility (or two) to handle the query wrapping: 1. one to add mode() and version at the end as parameters 2. another to wrap the given string in a query with the {\\\\"query\\\\", escaped quotes and everything else.
@Override public void expectScrollMatchesAdmin(String adminSql, String user, String userSql) throws Exception { String mode = randomMode(); Map<String, Object> adminResponse = runSql(null, new StringEntity("{\\\\"query\\\\": \\\\"" + adminSql + "\\\\", \\\\"fetch_size\\\\": 1" + mode(mode) + version(mode) + "}", ContentType.APPLICATION_JSON), mode); Map<String, Object> otherResponse = runSql(user, new StringEntity("{\\\\"query\\\\": \\\\"" + adminSql + "\\\\", \\\\"fetch_size\\\\": 1" + mode(mode) + version(mode) + "}", ContentType.APPLICATION_JSON), mode); String adminCursor = (String) adminResponse.remove("cursor"); String otherCursor = (String) otherResponse.remove("cursor"); assertNotNull(adminCursor); assertNotNull(otherCursor); assertResponse(adminResponse, otherResponse); while (true) { adminResponse = runSql(null, new StringEntity("{\\\\"cursor\\\\": \\\\"" + adminCursor + "\\\\"" + mode(mode) + version(mode) + "}", ContentType.APPLICATION_JSON), mode); otherResponse = runSql(user, new StringEntity("{\\\\"cursor\\\\": \\\\"" + otherCursor + "\\\\"" + mode(mode) + version(mode) + "}", ContentType.APPLICATION_JSON), mode); adminCursor = (String) adminResponse.remove("cursor"); otherCursor = (String) otherResponse.remove("cursor"); assertResponse(adminResponse, otherResponse); if (adminCursor == null) { assertNull(otherCursor); return; } assertNotNull(otherCursor); } }	same comment as above, it applies in other places but i won't repeat it.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (query != null) { builder.field("query", query); } builder.field("mode", mode().toString()); if (clientId() != null) { builder.field("client_id", clientId()); } if (version() != null) { builder.field("client_version", version().toString()); } if (this.params != null && this.params.isEmpty() == false) { builder.startArray("params"); for (SqlTypedParamValue val : this.params) { val.toXContent(builder, params); } builder.endArray(); } if (zoneId != null) { builder.field("time_zone", zoneId.getId()); } if (fetchSize != Protocol.FETCH_SIZE) { builder.field("fetch_size", fetchSize); } if (requestTimeout != Protocol.REQUEST_TIMEOUT) { builder.field("request_timeout", requestTimeout.getStringRep()); } if (pageTimeout != Protocol.PAGE_TIMEOUT) { builder.field("page_timeout", pageTimeout.getStringRep()); } if (filter != null) { builder.field("filter"); filter.toXContent(builder, params); } if (columnar != null) { builder.field("columnar", columnar); } if (fieldMultiValueLeniency) { builder.field("field_multi_value_leniency", fieldMultiValueLeniency); } if (indexIncludeFrozen) { builder.field("index_include_frozen", indexIncludeFrozen); } if (binaryCommunication != null) { builder.field("binary_format", binaryCommunication); } if (cursor != null) { builder.field("cursor", cursor); } return builder; }	some constants for these strings would be nice.
public void testIndexPrefixMapping() throws IOException { QueryShardContext queryShardContext = indexService.newQueryShardContext( randomInt(20), null, () -> { throw new UnsupportedOperationException(); }, null); { String mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field") .field("type", "text") .field("analyzer", "standard") .startObject("index_prefixes") .field("min_chars", 2) .field("max_chars", 10) .endObject() .endObject().endObject() .endObject().endObject()); DocumentMapper mapper = parser.parse("type", new CompressedXContent(mapping)); assertEquals(mapping, mapper.mappingSource().toString()); assertThat(mapper.mappers().getMapper("field._index_prefix").toString(), containsString("prefixChars=2:10")); FieldMapper fieldMapper = (FieldMapper) mapper.mappers().getMapper("field"); MappedFieldType fieldType = fieldMapper.fieldType; Query q = fieldType.prefixQuery("goin", CONSTANT_SCORE_REWRITE, queryShardContext); assertEquals(new ConstantScoreQuery(new TermQuery(new Term("field._index_prefix", "goin"))), q); q = fieldType.prefixQuery("internationalisatio", CONSTANT_SCORE_REWRITE, queryShardContext); assertEquals(new PrefixQuery(new Term("field", "internationalisatio")), q); q = fieldType.prefixQuery("g", CONSTANT_SCORE_REWRITE, queryShardContext); Automaton automaton = Operations.concatenate(Arrays.asList(Automata.makeChar('g'), Automata.makeAnyChar())); assertEquals(new ConstantScoreQuery(new AutomatonQuery(new Term("field._index_prefix", "g*"), automaton)), q); ParsedDocument doc = mapper.parse(SourceToParse.source("test", "type", "1", BytesReference .bytes(XContentFactory.jsonBuilder() .startObject() .field("field", "Some English text that is going to be very useful") .endObject()), XContentType.JSON)); IndexableField[] fields = doc.rootDoc().getFields("field._index_prefix"); assertEquals(1, fields.length); } { String mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field") .field("type", "text") .field("analyzer", "standard") .startObject("index_prefixes").endObject() .endObject().endObject() .endObject().endObject()); CompressedXContent json = new CompressedXContent(mapping); DocumentMapper mapper = parser.parse("type", json); FieldMapper fieldMapper = (FieldMapper) mapper.mappers().getMapper("field"); MappedFieldType fieldType = fieldMapper.fieldType; Query q1 = fieldType.prefixQuery("g", CONSTANT_SCORE_REWRITE, queryShardContext); assertThat(q1, instanceOf(ConstantScoreQuery.class)); assertThat(((ConstantScoreQuery)q1).getQuery(), instanceOf(AutomatonQuery.class)); Query q2 = fieldType.prefixQuery("go", CONSTANT_SCORE_REWRITE, queryShardContext); assertThat(q2, instanceOf(ConstantScoreQuery.class)); Query q5 = fieldType.prefixQuery("going", CONSTANT_SCORE_REWRITE, queryShardContext); assertThat(q5, instanceOf(ConstantScoreQuery.class)); Query q6 = fieldType.prefixQuery("goings", CONSTANT_SCORE_REWRITE, queryShardContext); assertThat(q6, instanceOf(PrefixQuery.class)); } { String illegalMapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field") .field("type", "text") .field("analyzer", "standard") .startObject("index_prefixes") .field("min_chars", 1) .field("max_chars", 10) .endObject() .startObject("fields") .startObject("_index_prefix").field("type", "text").endObject() .endObject() .endObject().endObject() .endObject().endObject()); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> { indexService.mapperService() .merge("type", new CompressedXContent(illegalMapping), MergeReason.MAPPING_UPDATE); }); assertThat(e.getMessage(), containsString("Field [field._index_prefix] is defined twice in [type]")); } { String badConfigMapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field") .field("type", "text") .field("analyzer", "standard") .startObject("index_prefixes") .field("min_chars", 11) .field("max_chars", 10) .endObject() .endObject().endObject() .endObject().endObject()); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> parser.parse("type", new CompressedXContent(badConfigMapping)) ); assertThat(e.getMessage(), containsString("min_chars [11] must be less than max_chars [10]")); } { String badConfigMapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field") .field("type", "text") .field("analyzer", "standard") .startObject("index_prefixes") .field("min_chars", 0) .field("max_chars", 10) .endObject() .endObject().endObject() .endObject().endObject()); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> parser.parse("type", new CompressedXContent(badConfigMapping)) ); assertThat(e.getMessage(), containsString("min_chars [0] must be greater than zero")); } { String badConfigMapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field") .field("type", "text") .field("analyzer", "standard") .startObject("index_prefixes") .field("min_chars", 1) .field("max_chars", 25) .endObject() .endObject().endObject() .endObject().endObject()); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> parser.parse("type", new CompressedXContent(badConfigMapping)) ); assertThat(e.getMessage(), containsString("max_chars [25] must be less than 20")); } { String badConfigMapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field") .field("type", "text") .field("analyzer", "standard") .field("index_prefixes", (String) null) .endObject().endObject() .endObject().endObject()); MapperParsingException e = expectThrows(MapperParsingException.class, () -> parser.parse("type", new CompressedXContent(badConfigMapping)) ); assertThat(e.getMessage(), containsString("[index_prefixes] must not have a [null] value")); } { String badConfigMapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties").startObject("field") .field("type", "text") .field("index", "false") .startObject("index_prefixes").endObject() .endObject().endObject() .endObject().endObject()); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> parser.parse("type", new CompressedXContent(badConfigMapping)) ); assertThat(e.getMessage(), containsString("Cannot set index_prefixes on unindexed field [field]")); } }	it seems like these detailed query construction tests would fit better in a unit test like textfieldtypetests.
* @param setting setting to use to configure rate limiter * @param defaultRate default limiting rate * @return rate limiter or null of no throttling is needed */ private RateLimiter getRateLimiter(Settings repositorySettings, String setting, ByteSizeValue defaultRate) { ByteSizeValue maxSnapshotBytesPerSec = repositorySettings.getAsBytesSize(setting, defaultRate); if (maxSnapshotBytesPerSec.getBytes() <= 0) { return null; } else { return new RateLimiter.SimpleRateLimiter(maxSnapshotBytesPerSec.getMbFrac()); } }	this was the mistake that underpins this entire change. the use of settings.getasbytesize(setting, defaultrate) is useless here. settings here is node-level settings, and these can never contain the throttle rates since they are not registered as node-level settings. so we always end up falling back to the default in the case the repository settings do not have a throttle defined. once we remove this usage, we can remove settings from being a field in this class, and then the entire rest of the change set falls out.
@Override public InternalAggregation reduce(List<InternalAggregation> aggregations, AggregationReduceContext reduceContext) { long docCount = 0L; List<InternalAggregations> subAggregationsList = new ArrayList<>(aggregations.size()); for (InternalAggregation aggregation : aggregations) { assert aggregation.getName().equals(getName()); docCount += ((InternalSingleBucketAggregation) aggregation).getDocCount(); subAggregationsList.add(((InternalSingleBucketAggregation) aggregation).getAggregations()); } InternalAggregations aggs = InternalAggregations.reduce(subAggregationsList, reduceContext); if (reduceContext.isFinalReduce()) { SamplingContext context = buildContext(); aggs = InternalAggregations.from( aggs.asList().stream().map(agg -> ((InternalAggregation) agg).finalizeSampling(context)).collect(Collectors.toList()) ); } return newAggregation(getName(), docCount, aggs); }	i don't think i understand what's being asserted here
* @param listener The BulkProcessor listener that gets called on bulk events * @param name The name of this processor, e.g. to identify the scheduler threads * @return the builder for BulkProcessor */ public static Builder builder(BiConsumer<BulkRequest, ActionListener<BulkResponse>> consumer, Listener listener, String name) { Objects.requireNonNull(consumer, "consumer"); Objects.requireNonNull(listener, "listener"); final ScheduledThreadPoolExecutor flushScheduler = Scheduler.initScheduler(Settings.EMPTY, name + FLUSH_SCHEDULER_NAME_SUFFIX); final ScheduledThreadPoolExecutor retryScheduler = Scheduler.initScheduler(Settings.EMPTY, name + RETRY_SCHEDULER_NAME_SUFFIX); return new Builder(consumer, listener, buildScheduler(flushScheduler), buildScheduler(retryScheduler), () -> { Scheduler.terminate(flushScheduler, 10, TimeUnit.SECONDS); Scheduler.terminate(retryScheduler, 10, TimeUnit.SECONDS); }); }	since this class is the [documented approach](https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-document-bulk.html#java-rest-high-document-bulk-processor) for clients ingestion, can we preserve binary compatibility here ? (i.e. preserve the original method signature and pass an empty/hard-coded string)
@Override public void validate(String value, Map<Setting<?>, Object> settings) { if (Strings.hasText(value)) { if (SearchableSnapshotsConstants.isPartialSearchableSnapshotIndex(settings)) { if (value.equals(DATA_FROZEN) == false) { throw new IllegalArgumentException("only the [" + DATA_FROZEN + "] tier preference may be used for partial searchable snapshots"); } } else { String[] split = value.split(","); if (Arrays.stream(split).anyMatch(DATA_FROZEN::equals)) { throw new IllegalArgumentException("[" + DATA_FROZEN + "] tier can only be used for partial searchable snapshots"); } } } }	i think we need to either make _tier_preference validate that partial searchable snapshots always have the value data_frozen (i.e., must be set) or else make that the default when unset. ideally, we should remove support for include/exclude/require now that we have the chance too? can be a separate pr if that is easier.
public Bucket get(String bucket, BucketGetOption... options) { if (bucketName.equals(bucket)) { return StorageTestUtils.createBucket(this, bucketName); } else { return null; } }	api change in the dependency made overriding this necessary. since we don't use it yet, i added just a dummy override.
public void testRetrieveCardinality() { final int p = randomIntBetween(MIN_PRECISION, MAX_PRECISION); final HyperLogLogPlusPlus counts = new HyperLogLogPlusPlus(p, BigArrays.NON_RECYCLING_INSTANCE, 1); int bucket = randomInt(100); counts.collect(bucket, -8688952809613614893L); for (int i = 0; i < 1000; i++) { int cardinality = bucket == i ? 1 : 0; assertEquals(cardinality, counts.cardinality(i)); } }	probably worth a comment about why this particular random number. if it is to trip the problem, can we somehow assert that it tripped with this number? i feel like we could easily end up in a position where the test doesn't do anything if we don't have some package private method to check to see if this thing happened *somehow*.
@Override public void readFrom(StreamInput in) throws IOException { int size = in.readVInt(); ImmutableMap.Builder<String, StoreFileMetaData> builder = ImmutableMap.builder(); for (int i = 0; i < size; i++) { StoreFileMetaData meta = StoreFileMetaData.readStoreFileMetaData(in); builder.put(meta.name(), meta); logger.info("read {} {}", meta.name(), meta); } this.metadata = builder.build(); assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles(); commitUserData = new HashMap<>(); int num = in.readVInt(); for (int i = num; i > 0; i--) { commitUserData.put(in.readString(), in.readOptionalString()); } }	left over debugging tool?
@Override public void readFrom(StreamInput in) throws IOException { int size = in.readVInt(); ImmutableMap.Builder<String, StoreFileMetaData> builder = ImmutableMap.builder(); for (int i = 0; i < size; i++) { StoreFileMetaData meta = StoreFileMetaData.readStoreFileMetaData(in); builder.put(meta.name(), meta); logger.info("read {} {}", meta.name(), meta); } this.metadata = builder.build(); assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles(); commitUserData = new HashMap<>(); int num = in.readVInt(); for (int i = num; i > 0; i--) { commitUserData.put(in.readString(), in.readOptionalString()); } }	we don't need the optional string here as we found out :)
@Override public void writeTo(StreamOutput out) throws IOException { out.writeVInt(this.metadata.size()); for (StoreFileMetaData meta : this) { meta.writeTo(out); } out.writeVInt(commitUserData.size()); for (Map.Entry<String, String> entry : commitUserData.entrySet()) { out.writeString(entry.getKey()); out.writeOptionalString(entry.getValue()); } }	same here - no optional
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeLong(recoveryId); shardId.writeTo(out); sourceNode.writeTo(out); targetNode.writeTo(out); out.writeBoolean(markAsRelocated); metadataSnapshot.writeTo(out); out.writeByte(recoveryType.id()); }	can we put this next to the other getters?
@Test public void testRecoveryDiffWithLegacyCommit() { Map<String, StoreFileMetaData> metaDataMap = new HashMap<>(); metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[] {1}))); metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef())); Store.MetadataSnapshot first = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP); Store.MetadataSnapshot second = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP); Store.RecoveryDiff recoveryDiff = first.recoveryDiff(second); assertEquals(recoveryDiff.toString(), recoveryDiff.different.size(), 2); }	can we add a test somewhere here (or extend an existing one) to see we read commit data correctly?
private void assertDocumentExists(RestClient client, String index, String id) throws IOException { Request request = new Request("HEAD", "/" + index + "/_doc/" + id); Response response; try { response = client.performRequest(request); if (response.getStatusLine().getStatusCode() != 200) { logger.error(EntityUtils.toString(response.getEntity())); fail("HTTP response code expected to be [200] but was [" + response.getStatusLine().getStatusCode() + "]"); } } catch (ResponseException ex) { logger.error(EntityUtils.toString(ex.getResponse().getEntity())); fail("HTTP response code expected to be [200] but was [" + ex.getResponse().getStatusLine().getStatusCode() + "]"); } }	maybe also include the stacktrace just in case? suggestion logger.error(entityutils.tostring(ex.getresponse().getentity()), ex);
public synchronized AutoFollowStats getStats() { final Map<String, AutoFollower> autoFollowers = this.autoFollowers; final TreeMap<String, AutoFollowedCluster> timesSinceLastAutoFollowPerRemoteCluster = new TreeMap<>(); for (Map.Entry<String, AutoFollower> entry : autoFollowers.entrySet()) { long lastAutoFollowTimeInNanos = entry.getValue().lastAutoFollowTimeInNanos; long lastSeenMetadataVersion = entry.getValue().metadataVersion; if (lastAutoFollowTimeInNanos != -1) { long timeSinceLastAutoFollowInMillis = TimeUnit.NANOSECONDS.toMillis(relativeNanoTimeProvider.getAsLong() - lastAutoFollowTimeInNanos); timesSinceLastAutoFollowPerRemoteCluster.put(entry.getKey(), new AutoFollowedCluster(timeSinceLastAutoFollowInMillis, lastSeenMetadataVersion)); } else { timesSinceLastAutoFollowPerRemoteCluster.put(entry.getKey(), new AutoFollowedCluster(-1L, lastSeenMetadataVersion)); } } return new AutoFollowStats( numberOfFailedIndicesAutoFollowed, numberOfFailedRemoteClusterStateRequests, numberOfSuccessfulIndicesAutoFollowed, new TreeMap<>(recentAutoFollowErrors), timesSinceLastAutoFollowPerRemoteCluster ); }	since the time unit is ms, we should remove this conversion.
static DeprecationIssue checkRemovedSetting(final Settings settings, final Setting<?> removedSetting, final String url) { if (removedSetting.exists(settings) == false) { return null; } final String removedSettingKey = removedSetting.getKey(); final String value = removedSetting.get(settings).toString(); final String message = String.format(Locale.ROOT, "setting [%s] is deprecated and will be removed in the next major version", removedSettingKey); final String details = String.format(Locale.ROOT, "the setting [%s] is currently set to [%s], remove this setting", removedSettingKey, value); return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, message, url, details); }	should this be warning instead of critical ? if the setting is removed, won't it be be "archived" allowing the server to continue to function ? i think we are trying to reserve critical for things items that will prevent the server from running.
public void testNegativeZero() { assertEquals( NumberType.DOUBLE.rangeQuery("field", null, -0d, true, true), NumberType.DOUBLE.rangeQuery("field", null, +0d, true, false)); assertEquals( NumberType.FLOAT.rangeQuery("field", null, -0f, true, true), NumberType.FLOAT.rangeQuery("field", null, +0f, true, false)); assertEquals( NumberType.HALF_FLOAT.rangeQuery("field", null, -0f, true, true), NumberType.HALF_FLOAT.rangeQuery("field", null, +0f, true, false)); assertFalse(NumberType.DOUBLE.termQuery("field", -0d).equals(NumberType.DOUBLE.termQuery("field", +0d))); assertFalse(NumberType.FLOAT.termQuery("field", -0d).equals(NumberType.FLOAT.termQuery("field", +0d))); assertFalse(NumberType.HALF_FLOAT.termQuery("field", -0d).equals(NumberType.HALF_FLOAT.termQuery("field", +0d))); }	you should also test nextup(-0f) ?: assertequals( numberfieldmapper.numbertype.double.rangequery("field", -0f, null, false, false), numberfieldmapper.numbertype.double.rangequery("field", +0f, null, true, false));
@Override protected void taskOperation(StopRollupJobAction.Request request, RollupJobTask jobTask, ActionListener<StopRollupJobAction.Response> listener) { if (jobTask.getConfig().getId().equals(request.getId())) { jobTask.stop(listener); waitForStopped(request, jobTask, listener); } else { listener.onFailure(new RuntimeException("ID of rollup task [" + jobTask.getConfig().getId() + "] does not match request's ID [" + request.getId() + "]")); } }	we should not call the listener if waitforstopped is set.
@Override protected void taskOperation(StopRollupJobAction.Request request, RollupJobTask jobTask, ActionListener<StopRollupJobAction.Response> listener) { if (jobTask.getConfig().getId().equals(request.getId())) { jobTask.stop(listener); waitForStopped(request, jobTask, listener); } else { listener.onFailure(new RuntimeException("ID of rollup task [" + jobTask.getConfig().getId() + "] does not match request's ID [" + request.getId() + "]")); } }	we shouldn't block the current thread. i think we can use the generic thread pool here in order to make sure that we don't block on important threads (network threads). something like: threadpool.generic().execute(new abstractrunnable() { @override protected void dorun() throws exception { if (waitforstopped(request, jobtask)) { listener.onresponse(new stop...); } else { listener.onfailure(new stop...); } } @override public void onfailure(exception e) { listener.onfailure(e); }
public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; final Scroll scroll = scroll(); if (source != null && source.trackTotalHits() == false && scroll != null) { validationException = addValidationError("disabling [track_total_hits] is not allowed in a scroll context", validationException); } if (source != null && source.from() > 0 && scroll != null) { validationException = addValidationError("using [from] is not allowed in a scroll context", validationException); } if (requestCache != null && requestCache && scroll != null) { DEPRECATION_LOGGER.deprecated("Explicitly set [request_cache] for a scroll query is deprecated and will return a 400 " + "error in future versions"); } if (source != null && source.size() == 0 && scroll != null) { validationException = addValidationError("[size] cannot be [0] in a scroll context", validationException); } if (source != null && source.rescores() != null && source.rescores().isEmpty() == false && scroll != null) { DEPRECATION_LOGGER.deprecated("Using [rescore] for a scroll query is deprecated and will return a 400 error in future " + "versions"); } return validationException; }	it would be nice to mention that the rescore will be ignored, as that may not be clear to the user only from the fact it's deprecated.
public static Metadata readFrom(StreamInput in) throws IOException { Builder builder = new Builder(); builder.version = in.readLong(); builder.clusterUUID = in.readString(); builder.clusterUUIDCommitted = in.readBoolean(); builder.coordinationMetadata(new CoordinationMetadata(in)); builder.transientSettings(readSettingsFromStream(in)); builder.persistentSettings(readSettingsFromStream(in)); if (in.getVersion().onOrAfter(Version.V_7_3_0)) { builder.hashesOfConsistentSettings(DiffableStringMap.readFrom(in)); } final Function<String, MappingMetadata> mappingLookup; if (in.getVersion().onOrAfter(MAPPINGS_AS_HASH_VERSION)) { final int mappings = in.readVInt(); if (mappings > 0) { final Map<String, MappingMetadata> mappingMetadataMap = new HashMap<>(mappings); for (int i = 0; i < mappings; i++) { final MappingMetadata m = new MappingMetadata(in); mappingMetadataMap.put(m.getSha256(), m); } mappingLookup = mappingMetadataMap::get; } else { mappingLookup = null; } } else { mappingLookup = null; } int size = in.readVInt(); for (int i = 0; i < size; i++) { builder.put(IndexMetadata.readFrom(in, mappingLookup), false); } size = in.readVInt(); for (int i = 0; i < size; i++) { builder.put(IndexTemplateMetadata.readFrom(in)); } int customSize = in.readVInt(); for (int i = 0; i < customSize; i++) { Custom customIndexMetadata = in.readNamedWriteable(Custom.class); builder.putCustom(customIndexMetadata.getWriteableName(), customIndexMetadata); } return builder.build(); }	the hashmap constructors accepts the capacity, not the expected amount of elements. it needs to be sized a bit higher than mappings, otherwise it will need to be resized/rehashed. see https://github.com/google/guava/blob/master/guava/src/com/google/common/collect/maps.java#l273
public void testLiteralIntegerInvalid() throws Exception { ParsingException ex = expectThrows(ParsingException.class, () -> parser.createExpression("123456789098765432101")); assertEquals(ex.getErrorMessage(), "Cannot parse number [123456789098765432101]"); }	shouldn't this be a "number x is too large" situation?
static Request getIndex(GetIndexRequest getIndexRequest) { String[] indices = getIndexRequest.indices() == null ? Strings.EMPTY_ARRAY : getIndexRequest.indices(); String endpoint = endpoint(indices); Request request = new Request(HttpGet.METHOD_NAME, endpoint); Params params = new Params(request); params.withIndicesOptions(getIndexRequest.indicesOptions()); params.withLocal(getIndexRequest.local()); params.withIncludeDefaults(getIndexRequest.includeDefaults()); params.withHuman(getIndexRequest.humanReadable()); params.withMasterTimeout(getIndexRequest.masterNodeTimeout()); return request; }	can you also add this parameter to the corresponding rest spec? we are missing it there i think.
MappedFieldType asMappedFieldType(); /** * For runtime fields the {@link RuntimeField.Parser} returns directly the {@link MappedFieldType}	this is also a lie at this point :)
public void testMultiFieldsIsNotSupported() throws IOException { XContentBuilder mapping = runtimeFieldMapping(b -> { minimalMapping(b); b.startObject("fields").startObject("test").field("type", "keyword").endObject().endObject(); }); MapperParsingException exception = expectThrows(MapperParsingException.class, () -> createMapperService(mapping)); assertThat(exception.getMessage(), containsString("unknown parameter [fields] on mapper")); }	if we do duplicate the code, i wonder if the message could be tailored to runtime fields, like it used to be. if we decide not to duplicate the code , i guess there is no way around it. in general, i am surprised that we call this "mapper": do users even know what a mapper is?
static Request getUsers(GetUsersRequest getUsersRequest) { RequestConverters.EndpointBuilder builder = new RequestConverters.EndpointBuilder() .addPathPartAsIs("_xpack/security/user"); if (getUsersRequest.getUsernames().size() > 0) { builder.addPathPart(Strings.collectionToCommaDelimitedString(getUsersRequest.getUsernames())); } return new Request(HttpGet.METHOD_NAME, builder.build()); }	/_xpack/security/user - > /_security/user since https://github.com/elastic/elasticsearch/pull/36293 and https://github.com/elastic/elasticsearch/pull/36379 are merged
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeOptionalString(realm); out.writeOptionalString(user); if (out.getVersion().onOrAfter(Version.V_7_10_0)) { if (Strings.hasText(apiKeyId)) { out.writeOptionalStringArray(new String[] { apiKeyId }); } else { out.writeOptionalStringArray(null); } } else { out.writeOptionalString(apiKeyId); } out.writeOptionalString(apiKeyName); out.writeOptionalBoolean(ownedByAuthenticatedUser); } } String[][] inputs = new String[][]{ {randomNullOrEmptyString(), randomNullOrEmptyString(), randomNullOrEmptyString(), randomNullOrEmptyString(), "false"}, {randomNullOrEmptyString(), "user", "api-kid", "api-kname", "false"}, {"realm", randomNullOrEmptyString(), "api-kid", "api-kname", "false"}, {"realm", "user", "api-kid", randomNullOrEmptyString(), "false"}, {randomNullOrEmptyString(), randomNullOrEmptyString(), "api-kid", "api-kname", "false"}, {"realm", randomNullOrEmptyString(), randomNullOrEmptyString(), randomNullOrEmptyString(), "true"}, {randomNullOrEmptyString(), "user", randomNullOrEmptyString(), randomNullOrEmptyString(), "true"}, }; String[][] expectedErrorMessages = new String[][]{ {"One of [api key id, api key name, username, realm name] must be specified if [owner] flag is false"}, {"username or realm name must not be specified when the api key id or api key name is specified", "only one of [api key id, api key name] can be specified"}, {"username or realm name must not be specified when the api key id or api key name is specified", "only one of [api key id, api key name] can be specified"}, {"username or realm name must not be specified when the api key id or api key name is specified"}, {"only one of [api key id, api key name] can be specified"}, {"neither username nor realm-name may be specified when invalidating owned API keys"}, {"neither username nor realm-name may be specified when invalidating owned API keys"} }; for (int caseNo = 0; caseNo < inputs.length; caseNo++) { try (ByteArrayOutputStream bos = new ByteArrayOutputStream(); OutputStreamStreamOutput osso = new OutputStreamStreamOutput(bos)) { final Version streamVersion = randomVersionBetween(random(), Version.V_7_4_0, getPreviousVersion(Version.V_7_10_0)); Dummy d = new Dummy(inputs[caseNo]); osso.setVersion(streamVersion); d.writeTo(osso); ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); InputStreamStreamInput issi = new InputStreamStreamInput(bis); issi.setVersion(streamVersion); InvalidateApiKeyRequest request = new InvalidateApiKeyRequest(issi); ActionRequestValidationException ve = request.validate(); assertNotNull(ve.getMessage(), ve); assertEquals(expectedErrorMessages[caseNo].length, ve.validationErrors().size()); assertThat(ve.validationErrors(), containsInAnyOrder(expectedErrorMessages[caseNo])); } } } public void testSerialization() throws IOException { final String apiKeyId = randomAlphaOfLength(5); final boolean ownedByAuthenticatedUser = true; InvalidateApiKeyRequest invalidateApiKeyRequest = InvalidateApiKeyRequest.usingApiKeyId(apiKeyId, ownedByAuthenticatedUser); { ByteArrayOutputStream outBuffer = new ByteArrayOutputStream(); OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer); out.setVersion(randomVersionBetween(random(), Version.V_7_0_0, Version.V_7_3_0)); invalidateApiKeyRequest.writeTo(out); InputStreamStreamInput inputStreamStreamInput = new InputStreamStreamInput(new ByteArrayInputStream(outBuffer.toByteArray())); inputStreamStreamInput.setVersion(randomVersionBetween(random(), Version.V_7_0_0, Version.V_7_3_0)); InvalidateApiKeyRequest requestFromInputStream = new InvalidateApiKeyRequest(inputStreamStreamInput); assertThat(requestFromInputStream.getIds(), equalTo(invalidateApiKeyRequest.getIds())); // old version so the default for `ownedByAuthenticatedUser` is false assertThat(requestFromInputStream.ownedByAuthenticatedUser(), is(false)); } { ByteArrayOutputStream outBuffer = new ByteArrayOutputStream(); OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer); out.setVersion(randomVersionBetween(random(), Version.V_7_4_0, Version.CURRENT)); invalidateApiKeyRequest.writeTo(out); InputStreamStreamInput inputStreamStreamInput = new InputStreamStreamInput(new ByteArrayInputStream(outBuffer.toByteArray())); inputStreamStreamInput.setVersion(randomVersionBetween(random(), Version.V_7_4_0, Version.CURRENT)); InvalidateApiKeyRequest requestFromInputStream = new InvalidateApiKeyRequest(inputStreamStreamInput); assertThat(requestFromInputStream, equalTo(invalidateApiKeyRequest)); } } public void testSerializationWillThrowWhenMultipleIdsAndOldVersionStream() { final InvalidateApiKeyRequest invalidateApiKeyRequest = new InvalidateApiKeyRequest( randomFrom(randomNullOrEmptyString(), randomAlphaOfLength(8)), randomFrom(randomNullOrEmptyString(), randomAlphaOfLength(8)), null, randomFrom(randomNullOrEmptyString(), randomAlphaOfLength(8)), false, new String[] { randomAlphaOfLength(12), randomAlphaOfLength(12) }); ByteArrayOutputStream outBuffer = new ByteArrayOutputStream(); OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer); out.setVersion(randomVersionBetween(random(), Version.V_7_4_0, getPreviousVersion(Version.V_7_10_0))); final IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> invalidateApiKeyRequest.writeTo(out)); assertThat(e.getMessage(), containsString("a request with multi-valued field [ids] cannot be sent to an older version")); }	oh wow. i can't believe we need to have this. we definitely need to look at removing this class in a follow up pr.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); { for (final String index : indices) { builder.startObject(index); { builder.startObject("aliases"); List<AliasMetaData> indexAliases = aliases.get(index); if (indexAliases != null) { for (final AliasMetaData alias : indexAliases) { AliasMetaData.Builder.toXContent(alias, builder, params); } } builder.endObject(); ImmutableOpenMap<String, MappingMetaData> indexMappings = mappings.get(index); boolean includeTypeName = params.paramAsBoolean(INCLUDE_TYPE_NAME_PARAMETER, false); if (includeTypeName) { builder.startObject("mappings"); if (indexMappings != null) { for (final ObjectObjectCursor<String, MappingMetaData> typeEntry : indexMappings) { builder.field(typeEntry.key); builder.map(typeEntry.value.sourceAsMap()); } } builder.endObject(); } else { if (indexMappings != null && indexMappings.size() > 0) { builder.field("mappings"); for (final ObjectObjectCursor<String, MappingMetaData> typeEntry : indexMappings) { builder.map(typeEntry.value.sourceAsMap()); } } else { // we always want to output a mappings object, even if empty builder.startObject("mappings"); builder.endObject(); } } builder.startObject("settings"); Settings indexSettings = settings.get(index); if (indexSettings != null) { indexSettings.toXContent(builder, params); } builder.endObject(); Settings defaultIndexSettings = defaultSettings.get(index); if (defaultIndexSettings != null && defaultIndexSettings.isEmpty() == false) { builder.startObject("defaults"); defaultIndexSettings.toXContent(builder, params); builder.endObject(); } } builder.endObject(); } } builder.endObject(); return builder; }	i think we should ensure that we're returning the singleton document mapping, as opposed to a _default_ mapping. here is an example of how this looks for getmappingsresponse: https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/getmappingsresponse.java#l132. i also wonder if it's worth unifying the logic between this class and getmappingsresponse.
private static ImmutableOpenMap<String, MappingMetaData> parseMappings(XContentParser parser) throws IOException { ImmutableOpenMap.Builder<String, MappingMetaData> indexMappings = ImmutableOpenMap.builder(); // We start at START_OBJECT since parseIndexEntry ensures that Map<String, Object> map = parser.map(); if (map.isEmpty() == false) { indexMappings.put(MapperService.SINGLE_MAPPING_NAME, new MappingMetaData(MapperService.SINGLE_MAPPING_NAME, map)); } return indexMappings.build(); }	i think this comment can be removed?
public static GetIndexResponse fromXContent(XContentParser parser, boolean legacyWithTypes) throws IOException { ImmutableOpenMap.Builder<String, List<AliasMetaData>> aliases = ImmutableOpenMap.builder(); ImmutableOpenMap.Builder<String, ImmutableOpenMap<String, MappingMetaData>> mappings = ImmutableOpenMap.builder(); ImmutableOpenMap.Builder<String, Settings> settings = ImmutableOpenMap.builder(); ImmutableOpenMap.Builder<String, Settings> defaultSettings = ImmutableOpenMap.builder(); List<String> indices = new ArrayList<>(); if (parser.currentToken() == null) { parser.nextToken(); } ensureExpectedToken(Token.START_OBJECT, parser.currentToken(), parser::getTokenLocation); parser.nextToken(); while (!parser.isClosed()) { if (parser.currentToken() == Token.START_OBJECT) { // we assume this is an index entry String indexName = parser.currentName(); indices.add(indexName); IndexEntry indexEntry = parseIndexEntry(parser, legacyWithTypes); // make the order deterministic CollectionUtil.timSort(indexEntry.indexAliases, Comparator.comparing(AliasMetaData::alias)); aliases.put(indexName, Collections.unmodifiableList(indexEntry.indexAliases)); mappings.put(indexName, indexEntry.indexMappings); settings.put(indexName, indexEntry.indexSettings); if (indexEntry.indexDefaultSettings.isEmpty() == false) { defaultSettings.put(indexName, indexEntry.indexDefaultSettings); } } else if (parser.currentToken() == Token.START_ARRAY) { parser.skipChildren(); } else { parser.nextToken(); } } return new GetIndexResponse( indices.toArray(new String[0]), mappings.build(), aliases.build(), settings.build(), defaultSettings.build() ); }	to check i understand, are we only retaining this method so that we can test the serialization of getindexresponse? it looks like we do not support include_type_name = true in the get indices hlrc call.
private Translog openTranslog(EngineConfig engineConfig, IndexWriter writer) throws IOException { final TranslogConfig translogConfig = engineConfig.getTranslogConfig(); translogConfig.setTranslogGeneration(null); if (openMode == EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG) { final Translog.TranslogGeneration generation = loadTranslogIdFromCommit(writer); // We expect that this shard already exists, so it must already have an existing translog else something is badly wrong! if (generation == null) { throw new IllegalStateException("no translog generation present in commit data but translog is expected to exist"); } translogConfig.setTranslogGeneration(generation); if (generation != null && generation.translogUUID == null) { throw new IndexFormatTooOldException("trasnlog", "translog has no generation nor a UUID - this might be an index from a previous version consider upgrading to N-1 first"); } } final Translog translog = new Translog(translogConfig); final Translog.TranslogGeneration generation = translogConfig.getTranslogGeneration(); if (generation == null || generation.translogUUID == null) { assert openMode != EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG : "OpenMode must not be " + EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG; if (generation == null) { logger.debug("no translog ID present in the current generation - creating one"); } else if (generation.translogUUID == null) { logger.debug("upgraded translog to pre 2.0 format, associating translog with index - writing translog UUID"); } boolean success = false; try { commitIndexWriter(writer, translog, openMode == EngineConfig.OpenMode.OPEN_INDEX_CREATE_TRANSLOG ? writer.getCommitData().get(SYNC_COMMIT_ID) : null); success = true; } finally { if (success == false) { IOUtils.closeWhileHandlingException(translog); } } } return translog; }	can we add what the openmode is ?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(featureName); if (out.getVersion().onOrAfter(Version.V_8_0_0)) { out.writeOptionalDouble(importance); } else { out.writeDouble(importance); } out.writeBoolean(classImportance != null); if (classImportance != null) { if (out.getVersion().before(Version.V_7_10_0)) { out.writeMap(ClassImportance.toMap(classImportance), StreamOutput::writeString, StreamOutput::writeDouble); } else { out.writeList(classImportance); } } }	couldn't this blow up? we need to somehow require that importance here is not null. i think the way to do that is to run classimportance.stream().maptodouble(classimportance::getimportance).map(math::abs).sum() if it is null. if classimportance is null && importance is null, then maybe just return a 0.0
@Override public Query existsQuery(QueryShardContext context) { if (hasDocValues()) { return new DocValuesFieldExistsQuery(name()); } else { //TODO this field differs from all other fields as it never goes into _field_names, so the default impl would not work //though changing this is problematic for existing indices that don't have this field in _field_names? throw new QueryShardException(context, "field " + name() + " of type [" + CONTENT_TYPE + "] " + "has no doc values and cannot be searched"); } }	this looks like a bug in histogramfieldmapper, can you open a separate issue for it? i wonder if we should revisit how we build the field names field, and maybe add something to mappertestcase to ensure that it is always added properly.
@Override public Query existsQuery(QueryShardContext context) { //TODO how can this work if this field never has doc_values? return new DocValuesFieldExistsQuery(name()); }	the vector is stored in a binarydocvaluesfield, so i think this is fine.
public UpdateRequestBuilder setDoc(XContentType xContentType, Object... source) { request.doc(xContentType, source); return this; } /** * Sets the index request to be used if the document does not exists. Otherwise, a * {@link org.elasticsearch.index.engine.DocumentMissingException}	i think you can import this class and this'll be easier to read.
public void testUpdateSetting() { AtomicReference<Float> index = new AtomicReference<>(); AtomicReference<Float> shard = new AtomicReference<>(); ClusterState.Builder builder = ClusterState.builder(new ClusterName("foo")); ClusterSettings settingsService = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS); settingsService.addSettingsUpdateConsumer(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING, index::set); settingsService.addSettingsUpdateConsumer(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING, shard::set); SettingsUpdater updater = new SettingsUpdater(settingsService); MetaData.Builder metaData = MetaData.builder() .persistentSettings(Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 1.5) .put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 2.5).build()) .transientSettings(Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 3.5) .put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 4.5).build()); ClusterState build = builder.metaData(metaData).build(); ClusterState clusterState = updater.updateSettings(build, Settings.builder() .put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 0.5).build(), Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 0.4).build(), logger); assertNotSame(clusterState, build); assertEquals(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 0.4, 0.1); assertEquals(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 2.5, 0.1); assertEquals(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.get(clusterState.metaData().transientSettings()), 0.5, 0.1); assertEquals(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.get(clusterState.metaData().transientSettings()), 4.5, 0.1); clusterState = updater.updateSettings(clusterState, Settings.builder().putNull("cluster.routing.*").build(), Settings.EMPTY, logger); assertEquals(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 0.4, 0.1); assertEquals(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 2.5, 0.1); assertFalse(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().transientSettings())); assertFalse(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().transientSettings())); clusterState = updater.updateSettings(clusterState, Settings.EMPTY, Settings.builder().putNull("cluster.routing.*") .put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 10.0).build(), logger); assertEquals(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 10.0, 0.1); assertFalse(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().persistentSettings())); assertFalse(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().transientSettings())); assertFalse(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().transientSettings())); assertNull("updater only does a dryRun", index.get()); assertNull("updater only does a dryRun", shard.get()); }	the old way was more readable i think.
@Override public void finalizeSnapshot(final SnapshotId snapshotId, final List<IndexId> indices, final long startTime, final String failure, final int totalShards, final List<SnapshotShardFailure> shardFailures, final long repositoryStateId, final boolean includeGlobalState, final MetaData clusterMetaData, final Map<String, Object> userMetadata, final ActionListener<SnapshotInfo> listener) { final StepListener<Void> afterMetaWrites = new StepListener<>(); // We upload one meta blob for each index, one for the cluster-state and one snap-${uuid}.dat blob final GroupedActionListener<Void> allMetaListener = new GroupedActionListener<>(ActionListener.map(afterMetaWrites, v -> null), 2 + indices.size()); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT); // We ignore all FileAlreadyExistsException when writing metadata since otherwise a master failover while in this method will // mean that no snap-${uuid}.dat blob is ever written for this snapshot. This is safe because any updated version of the // index or global metadata will be compatible with the segments written in this snapshot as well. // Failing on an already existing index-${repoGeneration} below ensures that the index.latest blob is not updated in a way // that decrements the generation it points at // Write Global MetaData executor.execute(ActionRunnable.wrap(allMetaListener, l -> { globalMetaDataFormat.write(clusterMetaData, blobContainer(), snapshotId.getUUID(), false); l.onResponse(null); })); // write the index metadata for each index in the snapshot for (IndexId index : indices) { executor.execute(ActionRunnable.wrap(allMetaListener, l -> { indexMetaDataFormat.write(clusterMetaData.index(index.getName()), indexContainer(index), snapshotId.getUUID(), false); l.onResponse(null); })); } // We add the whenComplete action to the metadata written listener on the SNAPSHOT pool to ensure we always execute the last // step on the SNAPSHOT pool executor.execute(ActionRunnable.wrap(listener, finalListener -> { final SnapshotInfo snapshotInfo = new SnapshotInfo(snapshotId, indices.stream().map(IndexId::getName).collect(Collectors.toList()), startTime, failure, threadPool.absoluteTimeInMillis(), totalShards, shardFailures, includeGlobalState, userMetadata); executor.execute(ActionRunnable.wrap(allMetaListener, l -> { snapshotFormat.write(snapshotInfo, blobContainer(), snapshotId.getUUID(), false); l.onResponse(null); })); afterMetaWrites.whenComplete(v -> { writeIndexGen(getRepositoryData().addSnapshot(snapshotId, snapshotInfo.state(), indices), repositoryStateId); finalListener.onResponse(snapshotInfo); }, ex -> finalListener.onFailure( new SnapshotException(metadata.name(), snapshotId, "failed to update snapshot in repository", ex))); })); }	i removed this specific rethrow because we write the index meta in parallel to the root level snap- blob with this change anyway so throwing with a separate message here seemed pointless.
@Override public void finalizeSnapshot(final SnapshotId snapshotId, final List<IndexId> indices, final long startTime, final String failure, final int totalShards, final List<SnapshotShardFailure> shardFailures, final long repositoryStateId, final boolean includeGlobalState, final MetaData clusterMetaData, final Map<String, Object> userMetadata, final ActionListener<SnapshotInfo> listener) { final StepListener<Void> afterMetaWrites = new StepListener<>(); // We upload one meta blob for each index, one for the cluster-state and one snap-${uuid}.dat blob final GroupedActionListener<Void> allMetaListener = new GroupedActionListener<>(ActionListener.map(afterMetaWrites, v -> null), 2 + indices.size()); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT); // We ignore all FileAlreadyExistsException when writing metadata since otherwise a master failover while in this method will // mean that no snap-${uuid}.dat blob is ever written for this snapshot. This is safe because any updated version of the // index or global metadata will be compatible with the segments written in this snapshot as well. // Failing on an already existing index-${repoGeneration} below ensures that the index.latest blob is not updated in a way // that decrements the generation it points at // Write Global MetaData executor.execute(ActionRunnable.wrap(allMetaListener, l -> { globalMetaDataFormat.write(clusterMetaData, blobContainer(), snapshotId.getUUID(), false); l.onResponse(null); })); // write the index metadata for each index in the snapshot for (IndexId index : indices) { executor.execute(ActionRunnable.wrap(allMetaListener, l -> { indexMetaDataFormat.write(clusterMetaData.index(index.getName()), indexContainer(index), snapshotId.getUUID(), false); l.onResponse(null); })); } // We add the whenComplete action to the metadata written listener on the SNAPSHOT pool to ensure we always execute the last // step on the SNAPSHOT pool executor.execute(ActionRunnable.wrap(listener, finalListener -> { final SnapshotInfo snapshotInfo = new SnapshotInfo(snapshotId, indices.stream().map(IndexId::getName).collect(Collectors.toList()), startTime, failure, threadPool.absoluteTimeInMillis(), totalShards, shardFailures, includeGlobalState, userMetadata); executor.execute(ActionRunnable.wrap(allMetaListener, l -> { snapshotFormat.write(snapshotInfo, blobContainer(), snapshotId.getUUID(), false); l.onResponse(null); })); afterMetaWrites.whenComplete(v -> { writeIndexGen(getRepositoryData().addSnapshot(snapshotId, snapshotInfo.state(), indices), repositoryStateId); finalListener.onResponse(snapshotInfo); }, ex -> finalListener.onFailure( new SnapshotException(metadata.name(), snapshotId, "failed to update snapshot in repository", ex))); })); }	this catch is gone now, it was dead code because we don't do the exists check for this blob anymore in the line above where we write the snap- blob.
@Override public void finalizeSnapshot(final SnapshotId snapshotId, final List<IndexId> indices, final long startTime, final String failure, final int totalShards, final List<SnapshotShardFailure> shardFailures, final long repositoryStateId, final boolean includeGlobalState, final MetaData clusterMetaData, final Map<String, Object> userMetadata, final ActionListener<SnapshotInfo> listener) { final StepListener<Void> afterMetaWrites = new StepListener<>(); // We upload one meta blob for each index, one for the cluster-state and one snap-${uuid}.dat blob final GroupedActionListener<Void> allMetaListener = new GroupedActionListener<>(ActionListener.map(afterMetaWrites, v -> null), 2 + indices.size()); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT); // We ignore all FileAlreadyExistsException when writing metadata since otherwise a master failover while in this method will // mean that no snap-${uuid}.dat blob is ever written for this snapshot. This is safe because any updated version of the // index or global metadata will be compatible with the segments written in this snapshot as well. // Failing on an already existing index-${repoGeneration} below ensures that the index.latest blob is not updated in a way // that decrements the generation it points at // Write Global MetaData executor.execute(ActionRunnable.wrap(allMetaListener, l -> { globalMetaDataFormat.write(clusterMetaData, blobContainer(), snapshotId.getUUID(), false); l.onResponse(null); })); // write the index metadata for each index in the snapshot for (IndexId index : indices) { executor.execute(ActionRunnable.wrap(allMetaListener, l -> { indexMetaDataFormat.write(clusterMetaData.index(index.getName()), indexContainer(index), snapshotId.getUUID(), false); l.onResponse(null); })); } // We add the whenComplete action to the metadata written listener on the SNAPSHOT pool to ensure we always execute the last // step on the SNAPSHOT pool executor.execute(ActionRunnable.wrap(listener, finalListener -> { final SnapshotInfo snapshotInfo = new SnapshotInfo(snapshotId, indices.stream().map(IndexId::getName).collect(Collectors.toList()), startTime, failure, threadPool.absoluteTimeInMillis(), totalShards, shardFailures, includeGlobalState, userMetadata); executor.execute(ActionRunnable.wrap(allMetaListener, l -> { snapshotFormat.write(snapshotInfo, blobContainer(), snapshotId.getUUID(), false); l.onResponse(null); })); afterMetaWrites.whenComplete(v -> { writeIndexGen(getRepositoryData().addSnapshot(snapshotId, snapshotInfo.state(), indices), repositoryStateId); finalListener.onResponse(snapshotInfo); }, ex -> finalListener.onFailure( new SnapshotException(metadata.name(), snapshotId, "failed to update snapshot in repository", ex))); })); }	can we extract the snapshot metadata upload at the same level as global metadata/indices metadata?
@Override public void finalizeSnapshot(SnapshotId snapshotId, List<IndexId> indices, long startTime, String failure, int totalShards, List<SnapshotShardFailure> shardFailures, long repositoryStateId, boolean includeGlobalState, MetaData metaData, Map<String, Object> userMetadata, ActionListener<SnapshotInfo> listener) { // we process the index metadata at snapshot time. This means if somebody tries to restore // a _source only snapshot with a plain repository it will be just fine since we already set the // required engine, that the index is read-only and the mapping to a default mapping try { super.finalizeSnapshot(snapshotId, indices, startTime, failure, totalShards, shardFailures, repositoryStateId, includeGlobalState, metadataToSnapshot(indices, metaData), userMetadata, listener); } catch (IOException ex) { listener.onFailure(ex); } }	i'm not sure it can happen at all? or maybe it's for extra safety?
public boolean match(String path, String name, String hint, XContentFieldType xcontentFieldType) { if (pathMatch != null && matchType.matches(pathMatch, path) == false) { return false; } if (match != null && matchType.matches(match, name) == false) { return false; } if (pathUnmatch != null && matchType.matches(pathUnmatch, path)) { return false; } if (unmatch != null && matchType.matches(unmatch, name)) { return false; } if (this.xcontentFieldType != null && this.xcontentFieldType != xcontentFieldType) { return false; } if (runtimeMapping && xcontentFieldType.supportsRuntimeField() == false) { return false; } if (Objects.equals(hint, matchHint) == false) { return false; } return true; }	call the argument mappinghint?
protected void serverAcceptedChannel(HttpChannel httpChannel) { boolean addedOnThisCall = httpChannels.add(httpChannel); assert addedOnThisCall : "Channel should only be added to http channel set once"; totalChannelsAccepted.incrementAndGet(); httpChannelStats.put( HttpStats.ClientStats.getChannelKey(httpChannel), new HttpStats.ClientStats(threadPool.absoluteTimeInMillis()) ); httpChannel.addCloseListener(ActionListener.wrap(() -> { try { httpChannels.remove(httpChannel); HttpStats.ClientStats clientStats = httpChannelStats.get(HttpStats.ClientStats.getChannelKey(httpChannel)); if (clientStats != null) { clientStats.closedTimeMillis = threadPool.absoluteTimeInMillis(); } } catch (Exception e) { } })); pruneClientStats(true); logger.trace(() -> new ParameterizedMessage("Http channel accepted: {}", httpChannel)); }	can you add a comment and maybe trace logging here, just in case we need it in the future? i don't think we should swallow exceptions without ever making them apparent elsewhere.
* @param out the OutputStream to copy to * @throws IOException in case of I/O errors */ public static void copy(byte[] in, OutputStream out) throws IOException { Objects.requireNonNull(in, "No input byte array specified"); Objects.requireNonNull(out, "No OutputStream specified"); try (OutputStream out2 = out) { out.write(in); } }	similarly, shouldn't we use out2within the block?
* @param out the Writer to copy to * @return the number of characters copied * @throws IOException in case of I/O errors */ public static int copy(Reader in, Writer out) throws IOException { Objects.requireNonNull(in, "No Reader specified"); Objects.requireNonNull(out, "No Writer specified"); // Leverage try-with-resources to close in and out so that exceptions in close() are either propagated or added as suppressed // exceptions to the main exception try (Reader in2 = in; Writer out2 = out) { int byteCount = 0; char[] buffer = new char[BUFFER_SIZE]; int bytesRead; while ((bytesRead = in.read(buffer)) != -1) { out.write(buffer, 0, bytesRead); byteCount += bytesRead; } out.flush(); return byteCount; } }	similarly, shouldn't we use in2 and out2 within the block?
* @param out the Writer to copy to * @throws IOException in case of I/O errors */ public static void copy(String in, Writer out) throws IOException { Objects.requireNonNull(in, "No input String specified"); Objects.requireNonNull(out, "No Writer specified"); try (Writer out2 = out) { out.write(in); } }	similarly, shouldn't we use out2within the block?
* @param out the stream to copy to * @return the number of bytes copied * @throws IOException in case of I/O errors */ public static long copy(InputStream in, OutputStream out, byte[] buffer) throws IOException { Objects.requireNonNull(in, "No InputStream specified"); Objects.requireNonNull(out, "No OutputStream specified"); // Leverage try-with-resources to close in and out so that exceptions in close() are either propagated or added as suppressed // exceptions to the main exception try (InputStream in2 = in; OutputStream out2 = out) { long byteCount = 0; int bytesRead; while ((bytesRead = in.read(buffer)) != -1) { out.write(buffer, 0, bytesRead); byteCount += bytesRead; } out.flush(); return byteCount; } }	shouldn't we use in2 and out2 in the block instead of in and out?
public ClusterState execute(ClusterState currentState) { if (newState.version() > currentState.version()) { logger.warn("received cluster state from [{}] which is also master but with a newer cluster_state, rejoining to cluster...", newState.nodes().masterNode()); return rejoin(currentState, "zen-disco-master_receive_cluster_state_from_another_master [" + newState.nodes().masterNode() + "]"); } else { logger.warn("received cluster state from [{}] which is also master but with an older cluster_state, telling [{}] to rejoin the cluster", newState.nodes().masterNode(), newState.nodes().masterNode()); try { // make sure we're connect to this node (connect to node exists if we already are transportService.connectToNode(newState.nodes().masterNode()); transportService.sendRequest(newState.nodes().masterNode(), RejoinClusterRequestHandler.ACTION, new RejoinClusterRequest(currentState.nodes().localNodeId()), new EmptyTransportResponseHandler(ThreadPool.Names.SAME) { @Override public void handleException(TransportException exp) { logger.warn("failed to send rejoin request to [{}]", exp, newState.nodes().masterNode()); } }); } catch (Exception e) { logger.warn("failed to send rejoin request to [{}]", e, newState.nodes().masterNode()); } return currentState; } }	the comment is not properly phrase? maybe mention that the node might got disconnected?
@Override protected TypeResolution resolveType() { if (!childrenResolved()) { return new TypeResolution("Unresolved children"); } TypeResolution resolution; int index = 0; List<Integer> nonFoldables = new ArrayList<>(2); for (Expression e : children()) { // Currently we have limited enum for ordinal numbers // So just using default here for error messaging ParamOrdinal paramOrd = ParamOrdinal.fromIndex(index); resolution = isStringAndExact(e, sourceText(), paramOrd); if (resolution.unresolved()) { return resolution; } resolution = isFoldable(e, sourceText(), paramOrd); if (resolution.unresolved()) { nonFoldables.add(index); } index++; } if (nonFoldables.size() == 2) { StringBuilder sb = new StringBuilder(format(null, "only one argument of [{}] must be non-constant but multiple found: ", sourceText())); for (Integer i : nonFoldables) { ParamOrdinal pOrd = ParamOrdinal.fromIndex(i); sb.append(format(null, "{}argument: [{}]", pOrd == ParamOrdinal.DEFAULT ? "" : pOrd.name().toLowerCase(Locale.ROOT) + " ", Expressions.name(children().get(i)))); sb.append(", "); } sb.delete(sb.length() - 2, sb.length()); return new TypeResolution(sb.toString()); } return TypeResolution.TYPE_RESOLVED; }	i would emphasize the two "versions" of match function, rather than mentioning that one argument needs to be a field. something more aligned with the (to be updated) documentation. for example "either first or last argument of match must be a non-constant" (or around the same idea).
protected LogicalPlan rule(Filter filter) { return filter.transformExpressionsUp(e -> { if (e instanceof Match) { Match m = (Match) e; if (m.children().get(0).foldable()) { int size = m.children().size(); List<Expression> newChildren = new ArrayList<>(size); Expression field = null; for (Expression c : m.children()) { if (c.foldable() == false) { field = c; } else { newChildren.add(c); } } newChildren.add(0, field); return m.replaceChildren(newChildren); } } return e; }); }	why isn't this done inside the main match constructor or even better inside makesubstitute?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeStringArray(indices); if (out.getVersion().before(Version.V_8_0_0)) { out.writeStringArray(Strings.EMPTY_ARRAY); } indicesOptions.writeIndicesOptions(out); if (out.getVersion().before(Version.V_8_0_0)) { out.writeBoolean(true); } out.writeStringArray(fields); out.writeBoolean(includeDefaults); }	same here as well, not needed if the request never goes across nodes.
@Override public Info info(String name) { return infos.computeIfAbsent(name, n -> new Info(n, ThreadPoolType.FIXED, random.nextInt(10) + 1)); }	motivation for the map complication here: by using a random size for this pool here we still get the same kind of upload-ordering coverage we had before in the snapshotresiliencytests
@Override public final Query toQuery(QueryShardContext context) throws IOException { Query query = doToQuery(context); if (query != null) { if (boost != DEFAULT_BOOST) { if (query instanceof SpanQuery) { query = new SpanBoostQuery((SpanQuery) query, boost); } else { query = new BoostQuery(query, boost); } } if (queryName != null) { context.setMatchNamedQueries(true); if (query instanceof SpanQuery) { query = new NamedSpanQuery(queryName, (SpanQuery)query); } else { query = new NamedQuery(queryName, query); } } } return query; }	why not use lucene's namedmatches#wrapquery ?
* @param clusterAlias the remote cluster alias * @param leaderIndex the name of the leader index * @param onFailure the failure consumer * @param consumer the consumer for supplying the leader index metadata and historyUUIDs of all leader shards * @param <T> the type of response the listener is waiting for */ public <T> void checkRemoteClusterLicenseAndFetchLeaderIndexMetadataAndHistoryUUIDs( final Client client, final String clusterAlias, final String leaderIndex, final Consumer<Exception> onFailure, final BiConsumer<String[], IndexMetaData> consumer) { final ClusterStateRequest request = new ClusterStateRequest(); request.clear(); request.metaData(true); request.indices(leaderIndex); checkRemoteClusterLicenseAndFetchClusterState( client, Collections.emptyMap(), clusterAlias, request, onFailure, leaderClusterState -> { IndexMetaData leaderIndexMetaData = leaderClusterState.getMetaData().index(leaderIndex); if (leaderIndexMetaData == null) { onFailure.accept(new IndexNotFoundException(leaderIndex)); return; } final Client leaderClient = client.getRemoteClusterClient(clusterAlias); hasPrivilegesToFollowIndices(leaderClient, new String[] {leaderIndex}, e -> { if (e == null) { fetchHistoryUUIDs(leaderClient, leaderIndexMetaData, onFailure, historyUUIDs -> { consumer.accept(historyUUIDs, leaderIndexMetaData); }); } else { onFailure.accept(e); } }); }, licenseCheck -> indexMetadataNonCompliantRemoteLicense(leaderIndex, licenseCheck), e -> indexMetadataUnknownRemoteLicense(leaderIndex, clusterAlias, e)); } /** * Fetches the leader cluster state from the remote cluster by the specified cluster state request. Before fetching the cluster state, * the remote cluster is checked for license compliance with CCR. If the remote cluster is not licensed for CCR, * the {@code onFailure}	this change is unneeded now?
@Override protected AllocatedPersistentTask createTask(long id, String type, String action, TaskId parentTaskId, PersistentTasksCustomMetaData.PersistentTask<ShardFollowTask> taskInProgress, Map<String, String> headers) { ShardFollowTask params = taskInProgress.getParams(); final Client leaderClient; if (params.getLeaderClusterAlias() != null) { leaderClient = wrapClient(client.getRemoteClusterClient(params.getLeaderClusterAlias()), params.getHeaders()); } else { leaderClient = wrapClient(client, params.getHeaders()); } Client followerClient = wrapClient(client, params.getHeaders()); BiConsumer<TimeValue, Runnable> scheduler = (delay, command) -> { try { threadPool.schedule(delay, Ccr.CCR_THREAD_POOL_NAME, command); } catch (EsRejectedExecutionException e) { if (e.isExecutorShutdown()) { logger.debug("couldn't schedule command, executor is shutting down", e); } else { throw e; } } }; IndexMetaData followIndexMetaData = clusterService.state().metaData().index(params.getFollowShardId().getIndex()); Map<String, String> ccrIndexMetadata = followIndexMetaData.getCustomData(Ccr.CCR_CUSTOM_METADATA_KEY); String[] recordedLeaderShardHistoryUUIDs = extractLeaderShardHistoryUUIDs(ccrIndexMetadata); final String recordedLeaderShardHistoryUUID = recordedLeaderShardHistoryUUIDs[params.getLeaderShardId().id()]; return new ShardFollowNodeTask(id, type, action, getDescription(taskInProgress), parentTaskId, headers, params, scheduler, System::nanoTime) { @Override protected void innerUpdateMapping(LongConsumer handler, Consumer<Exception> errorHandler) { Index leaderIndex = params.getLeaderShardId().getIndex(); Index followIndex = params.getFollowShardId().getIndex(); ClusterStateRequest clusterStateRequest = new ClusterStateRequest(); clusterStateRequest.clear(); clusterStateRequest.metaData(true); clusterStateRequest.indices(leaderIndex.getName()); leaderClient.admin().cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> { IndexMetaData indexMetaData = clusterStateResponse.getState().metaData().getIndexSafe(leaderIndex); if (indexMetaData.getMappings().isEmpty()) { assert indexMetaData.getMappingVersion() == 1; handler.accept(indexMetaData.getMappingVersion()); return; } assert indexMetaData.getMappings().size() == 1 : "expected exactly one mapping, but got [" + indexMetaData.getMappings().size() + "]"; MappingMetaData mappingMetaData = indexMetaData.getMappings().iterator().next().value; PutMappingRequest putMappingRequest = new PutMappingRequest(followIndex.getName()); putMappingRequest.type(mappingMetaData.type()); putMappingRequest.source(mappingMetaData.source().string(), XContentType.JSON); followerClient.admin().indices().putMapping(putMappingRequest, ActionListener.wrap( putMappingResponse -> handler.accept(indexMetaData.getMappingVersion()), errorHandler)); }, errorHandler)); } @Override protected void innerSendBulkShardOperationsRequest( final String followerHistoryUUID, final List<Translog.Operation> operations, final long maxSeqNoOfUpdatesOrDeletes, final Consumer<BulkShardOperationsResponse> handler, final Consumer<Exception> errorHandler) { final BulkShardOperationsRequest request = new BulkShardOperationsRequest(params.getFollowShardId(), followerHistoryUUID, operations, maxSeqNoOfUpdatesOrDeletes); followerClient.execute(BulkShardOperationsAction.INSTANCE, request, ActionListener.wrap(response -> handler.accept(response), errorHandler)); } @Override protected void innerSendShardChangesRequest(long from, int maxOperationCount, Consumer<ShardChangesAction.Response> handler, Consumer<Exception> errorHandler) { ShardChangesAction.Request request = new ShardChangesAction.Request(params.getLeaderShardId(), recordedLeaderShardHistoryUUID); request.setFromSeqNo(from); request.setMaxOperationCount(maxOperationCount); request.setMaxBatchSize(params.getMaxBatchSize()); request.setPollTimeout(params.getPollTimeout()); leaderClient.execute(ShardChangesAction.INSTANCE, request, ActionListener.wrap(handler::accept, errorHandler)); } }; }	nit - maybe : final string leaderhistoryuuid = getleadershardhistoryuuid(ccindexmetadata][params.getleadershardid().id())] instead of these 3 lines?
public void testUpdateRepository() { final InternalTestCluster cluster = internalCluster(); final String repositoryName = "test-repo"; final Client client = client(); final RepositoriesService repositoriesService = cluster.getDataOrMasterNodeInstances(RepositoriesService.class).iterator().next(); final Settings.Builder repoSettings = Settings.builder().put("location", randomRepoPath()); assertAcked( client.admin().cluster().preparePutRepository(repositoryName).setType(FsRepository.TYPE).setSettings(repoSettings).get() ); final GetRepositoriesResponse originalGetRepositoriesResponse = client.admin() .cluster() .prepareGetRepositories(repositoryName) .get(); assertThat(originalGetRepositoriesResponse.repositories(), hasSize(1)); RepositoryMetadata originalRepositoryMetadata = originalGetRepositoriesResponse.repositories().get(0); assertThat(originalRepositoryMetadata.type(), equalTo(FsRepository.TYPE)); final Repository originalRepository = repositoriesService.repository(repositoryName); assertThat(originalRepository, instanceOf(FsRepository.class)); final boolean updated = randomBoolean(); final String updatedRepositoryType = updated ? "mock" : FsRepository.TYPE; assertAcked( client.admin().cluster().preparePutRepository(repositoryName).setType(updatedRepositoryType).setSettings(repoSettings).get() ); final GetRepositoriesResponse updatedGetRepositoriesResponse = client.admin() .cluster() .prepareGetRepositories(repositoryName) .get(); assertThat(updatedGetRepositoriesResponse.repositories(), hasSize(1)); final RepositoryMetadata updatedRepositoryMetadata = updatedGetRepositoriesResponse.repositories().get(0); assertThat(updatedRepositoryMetadata.type(), equalTo(updatedRepositoryType)); final Repository updatedRepository = repositoriesService.repository(repositoryName); assertThat(updatedRepository, updated ? not(sameInstance(originalRepository)) : sameInstance(originalRepository)); // check that a noop update does not verify. internalCluster().startDataOnlyNode(Settings.builder().put(Environment.PATH_REPO_SETTING.getKey(), createTempDir()).build()); assertAcked( client.admin().cluster().preparePutRepository(repositoryName).setType(updatedRepositoryType).setSettings(repoSettings).get() ); }	how does this verify that no verification takes place?
public ClusterState execute(ClusterState currentState) { Metadata metadata = currentState.metadata(); Metadata.Builder mdBuilder = Metadata.builder(currentState.metadata()); RepositoriesMetadata repositories = metadata.custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY); List<RepositoryMetadata> repositoriesMetadata = new ArrayList<>(repositories.repositories().size() + 1); for (RepositoryMetadata repositoryMetadata : repositories.repositories()) { if (repositoryMetadata.name().equals(newRepositoryMetadata.name())) { if (newRepositoryMetadata.isNoopUpdate(repositoryMetadata)) { // Previous version is the same as this one no update is needed. return currentState; } Repository existing = RepositoriesService.this.repositories.get(request.name()); if (existing == null) { existing = RepositoriesService.this.internalRepositories.get(request.name()); } assert existing != null : "repository [" + newRepositoryMetadata.name() + "] must exist"; assert existing.getMetadata() == repositoryMetadata; final RepositoryMetadata updatedMetadata; if (canUpdateInPlace(newRepositoryMetadata, existing)) { // we're updating in place so the updated metadata must point at the same uuid and generations updatedMetadata = repositoryMetadata.withSettings(newRepositoryMetadata.settings()); } else { ensureRepositoryNotInUse(currentState, request.name()); updatedMetadata = newRepositoryMetadata; } found = true; repositoriesMetadata.add(updatedMetadata); } else { repositoriesMetadata.add(repositoryMetadata); } } if (found == false) { repositoriesMetadata.add(new RepositoryMetadata(request.name(), request.type(), request.settings())); } repositories = new RepositoriesMetadata(repositoriesMetadata); mdBuilder.putCustom(RepositoriesMetadata.TYPE, repositories); changed = true; return ClusterState.builder(currentState).metadata(mdBuilder).build(); }	somewhat optional but: i wonder, can't we just check equality of settings below in this if and break out there to keep it simple? i find it confusing that we would add a third "equals" method to repositorymetadata that only differs through the uuid check from equalsignoregenerations and that redundantly checks name and type equality. if (canupdateinplace(newrepositorymetadata, existing)) { // we're updating in place so the updated metadata must point at the same uuid and generations updatedmetadata = repositorymetadata.withsettings(newrepositorymetadata.settings());
private ThreadContextStruct putHeaders(Map<String, String> headers) { if (headers.isEmpty()) { return this; } else { final Map<String, String> newHeaders = new HashMap<>(this.requestHeaders); for (Map.Entry<String, String> entry : headers.entrySet()) { putSingleHeader(entry.getKey(), entry.getValue(), newHeaders); } return new ThreadContextStruct(newHeaders, responseHeaders, transientHeaders, isSystemContext); } }	i just realized i'm unsure about this fix. the putsingleheader below will fail if the key already exists, so anything in requestheaders can then not be overriden. i think what we want instead is a loop over requestheaders after the headers loop, but which uses putifabsent?
public void testPutHeaders() { Settings build = Settings.builder().put("request.headers.default", "1").build(); ThreadContext threadContext = new ThreadContext(build); threadContext.putHeader(Collections.<String, String>emptyMap()); threadContext.putHeader(Collections.<String, String>singletonMap("foo", "bar")); assertEquals("bar", threadContext.getHeader("foo")); try { threadContext.putHeader(Collections.<String, String>singletonMap("foo", "boom")); } catch (IllegalArgumentException e) { assertEquals("value for key [foo] already present", e.getMessage()); } }	you should use expectthrows(...) instead of try/catch.
public DataFrameAnalyticsConfig build() { applyMaxModelMemoryLimit(); return new DataFrameAnalyticsConfig(id, source, dest, analysis, headers, modelMemoryLimit, analyzedFields, createTime, version); } /** * Builds {@link DataFrameAnalyticsConfig} object for the purpose of performing memory estimation. * Some fields (i.e. "id", "dest") may not be present, therefore we overrite them here to make {@link DataFrameAnalyticsConfig}	i'm not sure this is a good idea. it means that if the user specifies a model memory limit in the config they supply when asking for an estimate then there is a chance that instead of giving them an answer we tell them that the arbitrary number they had in this config was too high. does the model memory limit supplied at this stage affect the estimation in any way? if so how? and if not, maybe in the constructor call immediately below we should pass min_model_memory_limit instead of the supplied value.
private void verifyModelPruneWindow() { if (modelPruneWindow == null) { return; } long modelPruneWindowSecs = modelPruneWindow.seconds(); long bucketSpanSecs = bucketSpan.seconds(); if (modelPruneWindowSecs % bucketSpanSecs != 0) { throw ExceptionsHelper.badRequestException(MODEL_PRUNE_WINDOW.getPreferredName() + " [" + modelPruneWindow.toString() + "]" + " must be a multiple of " + BUCKET_SPAN.getPreferredName() + " [" + bucketSpan.toString() + "]"); } if (modelPruneWindowSecs / bucketSpanSecs < MINIMUM_MODEL_PRUNE_WINDOW_BUCKETS) { throw ExceptionsHelper.badRequestException(MODEL_PRUNE_WINDOW.getPreferredName() + " [" + modelPruneWindow.toString() + "]" + " must be at least " + MINIMUM_MODEL_PRUNE_WINDOW_BUCKETS + " times greater than " + BUCKET_SPAN.getPreferredName() + " [" + bucketSpan.toString() + "]"); } }	if this is a thing we are enforcing, it makes sense to me to make this model_prune_window_buckets and take a whole, positive value indicating the number of buckets in the past to look at. forcing somebody to supply a timevalue when we require that value to always be a bucket multiple doesn't make sense to me
public DataStream rollover(Index newWriteIndex) { assert newWriteIndex.getName().equals(getBackingIndexName(name, generation + 1)); List<Index> backingIndices = new ArrayList<>(indices); backingIndices.add(newWriteIndex); return new DataStream(name, timeStampField, backingIndices, generation + 1); } /** * Removes the specified backing index and returns a new {@code DataStream} instance with * the remaining backing indices. * * @param index the backing index to remove * @return new {@code DataStream}	would be good to assert that backingindices.size() == indices.size() - 1?
public void testDateHistogramPivot() throws Exception { String transformId = "simpleDateHistogramPivot"; String dataFrameIndex = "pivot_reviews_via_date_histogram"; final Request createDataframeTransformRequest = new Request("PUT", DATAFRAME_ENDPOINT + transformId); String config = "{" + " \\\\"source\\\\": \\\\"reviews\\\\"," + " \\\\"dest\\\\": \\\\"" + dataFrameIndex + "\\\\","; config += " \\\\"pivot\\\\": {" + " \\\\"group_by\\\\": [ {" + " \\\\"by_day\\\\": {" + " \\\\"date_histogram\\\\": {" + " \\\\"interval\\\\": \\\\"1d\\\\",\\\\"field\\\\":\\\\"timestamp\\\\",\\\\"format\\\\":\\\\"yyyy-MM-DD\\\\"" + " } } } ]," + " \\\\"aggregations\\\\": {" + " \\\\"avg_rating\\\\": {" + " \\\\"avg\\\\": {" + " \\\\"field\\\\": \\\\"stars\\\\"" + " } } } }" + "}"; createDataframeTransformRequest.setJsonEntity(config); Map<String, Object> createDataframeTransformResponse = entityAsMap(client().performRequest(createDataframeTransformRequest)); assertThat(createDataframeTransformResponse.get("acknowledged"), equalTo(Boolean.TRUE)); assertTrue(indexExists(dataFrameIndex)); startAndWaitForTransform(transformId, dataFrameIndex); // we expect 17 documents as there shall be 21 days worth of docs Map<String, Object> indexStats = getAsMap(dataFrameIndex + "/_stats"); assertEquals(21, XContentMapValues.extractValue("_all.total.docs.count", indexStats)); assertOnePivotValue(dataFrameIndex + "/_search?q=by_day:2017-01-15", 3.82); }	nit: comment says 17
public void testSearcherId() throws Exception { final String indexName = "test_commit_id"; final int numberOfShards = randomIntBetween(1, 5); createIndex(indexName, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, numberOfShards) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .build()); indexRandom(randomBoolean(), randomBoolean(), randomBoolean(), IntStream.range(0, randomIntBetween(0, 50)) .mapToObj(n -> client().prepareIndex(indexName).setSource("num", n)).collect(toList())); ensureGreen(indexName); assertAcked(client().admin().indices().prepareClose(indexName)); assertIndexIsClosed(indexName); ensureGreen(indexName); if (randomBoolean()) { assertAcked(client().admin().indices().prepareUpdateSettings(indexName) .setSettings(Settings.builder().put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1))); internalCluster().ensureAtLeastNumDataNodes(2); ensureGreen(indexName); } String[] searcherIds = new String[numberOfShards]; Set<String> allocatedNodes = internalCluster().nodesInclude(indexName); for (String node : allocatedNodes) { IndexService indexService = internalCluster().getInstance(IndicesService.class, node).indexServiceSafe(resolveIndex(indexName)); for (IndexShard shard : indexService) { try (Engine.SearcherSupplier searcher = shard.acquireSearcherSupplier()) { assertNotNull(searcher.getCommitId()); if (searcherIds[shard.shardId().id()] != null) { assertThat(searcher.getCommitId(), equalTo(searcherIds[shard.shardId().id()])); } else { searcherIds[shard.shardId().id()] = searcher.getCommitId(); } } } } for (String node : allocatedNodes) { if (randomBoolean()) { internalCluster().restartNode(node); } } ensureGreen(indexName); allocatedNodes = internalCluster().nodesInclude(indexName); for (String node : allocatedNodes) { IndexService indexService = internalCluster().getInstance(IndicesService.class, node).indexServiceSafe(resolveIndex(indexName)); for (IndexShard shard : indexService) { try (Engine.SearcherSupplier searcher = shard.acquireSearcherSupplier()) { assertNotNull(searcher.getCommitId()); assertThat(searcher.getCommitId(), equalTo(searcherIds[shard.shardId().id()])); } } } }	i wonder if we should rename the method to getsearcherid(), just like you did for the test case? since it is no longer a commit-id, it might be a confusing name.
public void testSearcherId() throws Exception { IOUtils.close(engine, store); AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED); try (Store store = createStore()) { final EngineConfig config = config(defaultSettings, store, createTempDir(), NoMergePolicy.INSTANCE, null, null, globalCheckpoint::get); String lastCommitId; try (InternalEngine engine = createEngine(config)) { lastCommitId = ReadOnlyEngine.generateSearcherId(engine.getLastCommittedSegmentInfos()); assertNotNull(lastCommitId); int iterations = randomIntBetween(0, 10); for (int i = 0; i < iterations; i++) { assertThat(ReadOnlyEngine.generateSearcherId(engine.getLastCommittedSegmentInfos()), equalTo(lastCommitId)); final List<Engine.Operation> operations = generateHistoryOnReplica(between(1, 100), engine.getProcessedLocalCheckpoint() + 1L, false, randomBoolean(), randomBoolean()); applyOperations(engine, operations); engine.flush(randomBoolean(), true); final String newCommitId = ReadOnlyEngine.generateSearcherId(engine.getLastCommittedSegmentInfos()); assertThat(newCommitId, not(equalTo(lastCommitId))); if (randomBoolean()) { engine.flush(true, true); assertThat(ReadOnlyEngine.generateSearcherId(engine.getLastCommittedSegmentInfos()), equalTo(newCommitId)); } lastCommitId = newCommitId; } globalCheckpoint.set(engine.getProcessedLocalCheckpoint()); } try (ReadOnlyEngine readOnlyEngine = new ReadOnlyEngine(config, null, null, true, Function.identity(), true)) { try (Engine.SearcherSupplier searcher = readOnlyEngine.acquireSearcherSupplier(Function.identity(), randomFrom(Engine.SearcherScope.values()))) { assertThat(searcher.getCommitId(), equalTo(lastCommitId)); } } } }	maybe name this lastsearcherid?
public void testSearcherId() throws Exception { final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); final int numberOfShards = between(1, 5); assertAcked( client().admin() .indices() .prepareCreate(indexName) .setSettings(Settings.builder().put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, numberOfShards).build()) .setMapping("{\\\\"properties\\\\":{\\\\"created_date\\\\":{\\\\"type\\\\": \\\\"date\\\\", \\\\"format\\\\": \\\\"yyyy-MM-dd\\\\"}}}") ); final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>(); final int docCount = between(0, 100); for (int i = 0; i < docCount; i++) { indexRequestBuilders.add(client().prepareIndex(indexName).setSource("created_date", "2011-02-02")); } indexRandom(true, false, indexRequestBuilders); assertThat( client().admin().indices().prepareForceMerge(indexName).setOnlyExpungeDeletes(true).setFlush(true).get().getFailedShards(), equalTo(0) ); refresh(indexName); forceMerge(); final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT); createRepository(repositoryName, "fs"); final SnapshotId snapshotOne = createSnapshot(repositoryName, "snapshot-1", List.of(indexName)).snapshotId(); assertAcked(client().admin().indices().prepareDelete(indexName)); mountSnapshot(repositoryName, snapshotOne.getName(), indexName, indexName, Settings.EMPTY); ensureGreen(indexName); final String[] searcherIds = new String[numberOfShards]; Set<String> allocatedNodes = internalCluster().nodesInclude(indexName); for (String node : allocatedNodes) { IndexService indexService = internalCluster().getInstance(IndicesService.class, node).indexServiceSafe(resolveIndex(indexName)); for (IndexShard indexShard : indexService) { try (Engine.SearcherSupplier searcher = indexShard.acquireSearcherSupplier()) { assertNotNull(searcher.getCommitId()); if (searcherIds[indexShard.shardId().id()] != null) { assertThat(searcher.getCommitId(), equalTo(searcherIds[indexShard.shardId().id()])); } else { searcherIds[indexShard.shardId().id()] = searcher.getCommitId(); } } } } for (String allocatedNode : allocatedNodes) { if (randomBoolean()) { internalCluster().restartNode(allocatedNode); } } ensureGreen(indexName); allocatedNodes = internalCluster().nodesInclude(indexName); for (String node : allocatedNodes) { IndexService indexService = internalCluster().getInstance(IndicesService.class, node).indexServiceSafe(resolveIndex(indexName)); for (IndexShard indexShard : indexService) { try (Engine.SearcherSupplier searcher = indexShard.acquireSearcherSupplier()) { assertNotNull(searcher.getCommitId()); assertThat(searcher.getCommitId(), equalTo(searcherIds[indexShard.shardId().id()])); } } } }	does this assume that the snapshot is mounted with replicas? otherwise i am not sure we will hit this line. notice that searchable snapshots by default mount with 0 replicas. perhaps randomly add indexmetadata.setting_number_of_replicas, 1 to settings when mounting?
private void runTestDiscoveryNodeIsRemoteClusterClient(final Settings settings, final boolean expected) { final DiscoveryNode node = DiscoveryNode.createLocal(settings, new TransportAddress(TransportAddress.META_ADDRESS, 9200), "node"); assertThat(node.isRemoteClusterClient(), equalTo(expected)); if (expected) { assertThat(node.getRoles(), hasItem(DiscoveryNodeRole.REMOTE_CLUSTER_CLIENT_ROLE)); } else { assertThat(node.getRoles(), not(hasItem(DiscoveryNodeRole.REMOTE_CLUSTER_CLIENT_ROLE))); } }	i'm not sure about this. the assertion will hold if the roles collection has at least one item equal to the specifed role. that's not the same as it being the only role in the collection.
private static String getRoleSuffix(Settings settings) { if (NodeRoleSettings.NODE_ROLES_SETTING.exists(settings) == false) { return ""; } final List<DiscoveryNodeRole> roles = NodeRoleSettings.NODE_ROLES_SETTING.get(settings); if (roles.isEmpty()) { return "c"; } return roles.stream().sorted().map(DiscoveryNodeRole::roleNameAbbreviation).collect(Collectors.joining()); }	this is super-nit-picky, but does it make sense to map and then sort, in case the abbreviation, for some reason, ends up making the result look unsorted? it's actually what discoverynode#tostring() does too. suggestion return roles.stream().map(discoverynoderole::rolenameabbreviation).sorted().collect(collectors.joining());
public void testMaxConcurrentJobAllocations() throws Exception { int numMlNodes = 2; internalCluster().ensureAtMostNumDataNodes(0); // start non ml node, but that will hold the indices logger.info("Start non ml node:"); String nonMlNode = internalCluster().startNode(removeRoles(Set.of(MachineLearning.ML_ROLE))); logger.info("Starting ml nodes"); internalCluster().startNodes(numMlNodes, onlyRole(MachineLearning.ML_ROLE)); ensureStableCluster(numMlNodes + 1); int maxConcurrentJobAllocations = randomIntBetween(1, 4); client().admin().cluster().prepareUpdateSettings() .setTransientSettings(Settings.builder() .put(MachineLearning.CONCURRENT_JOB_ALLOCATIONS.getKey(), maxConcurrentJobAllocations)) .get(); // Sample each cs update and keep track each time a node holds more than `maxConcurrentJobAllocations` opening jobs. List<String> violations = new CopyOnWriteArrayList<>(); internalCluster().clusterService(nonMlNode).addListener(event -> { PersistentTasksCustomMetadata tasks = event.state().metadata().custom(PersistentTasksCustomMetadata.TYPE); if (tasks == null) { return; } for (DiscoveryNode node : event.state().nodes()) { Collection<PersistentTask<?>> foundTasks = tasks.findTasks(MlTasks.JOB_TASK_NAME, task -> { JobTaskState jobTaskState = (JobTaskState) task.getState(); return node.getId().equals(task.getExecutorNode()) && (jobTaskState == null || jobTaskState.isStatusStale(task)); }); int count = foundTasks.size(); if (count > maxConcurrentJobAllocations) { violations.add("Observed node [" + node.getName() + "] with [" + count + "] opening jobs on cluster state version [" + event.state().version() + "]"); } } }); ensureYellow(); // at least the primary shards of the indices a job uses should be started int numJobs = numMlNodes * 10; for (int i = 0; i < numJobs; i++) { Job.Builder job = createJob(Integer.toString(i), new ByteSizeValue(2, ByteSizeUnit.MB)); PutJobAction.Request putJobRequest = new PutJobAction.Request(job); client().execute(PutJobAction.INSTANCE, putJobRequest).actionGet(); OpenJobAction.Request openJobRequest = new OpenJobAction.Request(job.getId()); client().execute(OpenJobAction.INSTANCE, openJobRequest).actionGet(); } assertBusy(checkAllJobsAreAssignedAndOpened(numJobs)); logger.info("stopping ml nodes"); for (int i = 0; i < numMlNodes; i++) { // fork so stopping all ml nodes proceeds quicker: Runnable r = () -> { try { internalCluster() .stopRandomNode(settings -> DiscoveryNode.hasRole(settings, MachineLearning.ML_ROLE) == false); } catch (IOException e) { logger.error("error stopping node", e); } }; new Thread(r).start(); } ensureStableCluster(1, nonMlNode); assertBusy(() -> { ClusterState state = client(nonMlNode).admin().cluster().prepareState().get().getState(); PersistentTasksCustomMetadata tasks = state.metadata().custom(PersistentTasksCustomMetadata.TYPE); assertEquals(numJobs, tasks.taskMap().size()); for (PersistentTask<?> task : tasks.taskMap().values()) { assertNull(task.getExecutorNode()); } }); logger.info("re-starting ml nodes"); internalCluster().startNodes(numMlNodes, onlyRole(MachineLearning.ML_ROLE)); ensureStableCluster(1 + numMlNodes); assertBusy(checkAllJobsAreAssignedAndOpened(numJobs), 30, TimeUnit.SECONDS); assertEquals("Expected no violations, but got [" + violations + "]", 0, violations.size()); }	i think this line is supposed to match nodes where ml *is* enabled, no? the preceding comment says "stopping ml nodes".
public void testCloseUnassignedJobAndDatafeed() throws Exception { internalCluster().ensureAtMostNumDataNodes(0); logger.info("Starting dedicated master node..."); internalCluster().startMasterOnlyNode(); logger.info("Starting ml and data node..."); internalCluster().startNode(onlyRoles(Set.of(DiscoveryNodeRole.DATA_ROLE, DiscoveryNodeRole.MASTER_ROLE))); ensureStableClusterOnAllNodes(2); // index some datafeed data client().admin().indices().prepareCreate("data") .setMapping("time", "type=date") .get(); long numDocs1 = randomIntBetween(32, 2048); long now = System.currentTimeMillis(); long weekAgo = now - 604800000; long twoWeeksAgo = weekAgo - 604800000; indexDocs(logger, "data", numDocs1, twoWeeksAgo, weekAgo); String jobId = "test-lose-ml-node"; String datafeedId = jobId + "-datafeed"; setupJobAndDatafeed(jobId, datafeedId, TimeValue.timeValueHours(1)); waitForDatafeed(jobId, numDocs1); // stop the only ML node ensureGreen(); // replicas must be assigned, otherwise we could lose a whole index internalCluster().stopRandomNonMasterNode(); ensureStableCluster(1); // Job state is opened but the job is not assigned to a node (because we just killed the only ML node) GetJobsStatsAction.Request jobStatsRequest = new GetJobsStatsAction.Request(jobId); GetJobsStatsAction.Response jobStatsResponse = client().execute(GetJobsStatsAction.INSTANCE, jobStatsRequest).actionGet(); assertEquals(JobState.OPENED, jobStatsResponse.getResponse().results().get(0).getState()); GetDatafeedsStatsAction.Request datafeedStatsRequest = new GetDatafeedsStatsAction.Request(datafeedId); GetDatafeedsStatsAction.Response datafeedStatsResponse = client().execute(GetDatafeedsStatsAction.INSTANCE, datafeedStatsRequest).actionGet(); assertEquals(DatafeedState.STARTED, datafeedStatsResponse.getResponse().results().get(0).getDatafeedState()); // An unassigned datafeed can be stopped either normally or by force StopDatafeedAction.Request stopDatafeedRequest = new StopDatafeedAction.Request(datafeedId); stopDatafeedRequest.setForce(randomBoolean()); StopDatafeedAction.Response stopDatafeedResponse = client().execute(StopDatafeedAction.INSTANCE, stopDatafeedRequest).actionGet(); assertTrue(stopDatafeedResponse.isStopped()); // Since 7.5 we can also stop an unassigned job either normally or by force CloseJobAction.Request closeJobRequest = new CloseJobAction.Request(jobId); closeJobRequest.setForce(randomBoolean()); CloseJobAction.Response closeJobResponse = client().execute(CloseJobAction.INSTANCE, closeJobRequest).actionGet(); assertTrue(closeJobResponse.isClosed()); }	the code used to enable master and data here, was that a mistake in the old code?
protected void parseInternalRequest(Request internal, RestRequest restRequest, Map<String, Consumer<Object>> bodyConsumers) throws IOException { assert internal != null : "Request should not be null"; assert restRequest != null : "RestRequest should not be null"; SearchRequest searchRequest = internal.getSearchRequest(); int scrollSize = searchRequest.source().size(); // searchRequest.source().size(SIZE_ALL_MATCHES); try (XContentParser parser = extractRequestSpecificFields(restRequest, bodyConsumers)) { RestSearchAction.parseSearchRequest(searchRequest, restRequest, parser); } internal.setSize(searchRequest.source().size()); searchRequest.source().size(restRequest.paramAsInt("scroll_size", scrollSize)); String conflicts = restRequest.param("conflicts"); if (conflicts != null) { internal.setConflicts(conflicts); } // Let the requester set search timeout. It is probably only going to be useful for testing but who knows. if (restRequest.hasParam("search_timeout")) { searchRequest.source().timeout(restRequest.paramAsTime("search_timeout", null)); } }	i think you should just remove this line entirely.
@Override public Y build(BuilderContext context) { if (pathType != ContentPath.Type.FULL && context.indexCreatedVersion().onOrAfter(Version.V_1_4_0)) { throw new MapperParsingException("`path: just_name` is not supported on indices created on or after Elasticsearch 1.4.0"); } ContentPath.Type origPathType = context.path().pathType(); context.path().pathType(pathType); context.path().add(name); Map<String, Mapper> mappers = new HashMap<>(); for (Mapper.Builder builder : mappersBuilders) { Mapper mapper = builder.build(context); mappers.put(mapper.name(), mapper); } context.path().pathType(origPathType); context.path().remove(); ObjectMapper objectMapper = createMapper(name, context.path().fullPathAsText(name), enabled, nested, dynamic, pathType, mappers, context.indexSettings()); objectMapper.includeInAllIfNotSet(includeInAll); return (Y) objectMapper; }	i think that we should do path == contentpath.type.just_name instead of pathtype != contentpath.type.full in case in the future we want to add a new option...
public void testPathJustName() { IndexService indexService = createIndex("test"); Settings settings = indexService.settingsService().getSettings(); DocumentMapperParser mapperParser = indexService.mapperService().documentMapperParser(); // fails on the current version try { doc("test", settings, rootObject("person").add(object("name").pathType(ContentPath.Type.JUST_NAME).add(stringField("first").indexName("last")))).build(mapperParser); fail("path: just_name is not supported anymore"); } catch (ElasticsearchIllegalArgumentException e) { // expected } // but succeeds on < 1.4.0 Version v = Version.V_1_4_0; while (v.onOrAfter(Version.V_1_4_0)) { v = randomFrom(randomVersion()); } settings = ImmutableSettings.builder().put(settings).put(IndexMetaData.SETTING_VERSION_CREATED, v).build(); doc("test", settings, rootObject("person").add(object("name").pathType(ContentPath.Type.JUST_NAME).add(stringField("first").indexName("last")))).build(mapperParser); }	also check for mapperparsingexception as root cause here?
public void testPathJustName() { IndexService indexService = createIndex("test"); Settings settings = indexService.settingsService().getSettings(); DocumentMapperParser mapperParser = indexService.mapperService().documentMapperParser(); // fails on the current version try { doc("test", settings, rootObject("person").add(object("name").pathType(ContentPath.Type.JUST_NAME).add(stringField("first").indexName("last")))).build(mapperParser); fail("path: just_name is not supported anymore"); } catch (ElasticsearchIllegalArgumentException e) { // expected } // but succeeds on < 1.4.0 Version v = Version.V_1_4_0; while (v.onOrAfter(Version.V_1_4_0)) { v = randomFrom(randomVersion()); } settings = ImmutableSettings.builder().put(settings).put(IndexMetaData.SETTING_VERSION_CREATED, v).build(); doc("test", settings, rootObject("person").add(object("name").pathType(ContentPath.Type.JUST_NAME).add(stringField("first").indexName("last")))).build(mapperParser); }	maybe also add an test that verifies behaviour if version >= 1.4.0?
public void testCustomIndexName() { IndexService indexService = createIndex("test"); Settings settings = indexService.settingsService().getSettings(); DocumentMapperParser mapperParser = indexService.mapperService().documentMapperParser(); // fails on current version try { doc("test", settings, rootObject("person").add(stringField("first").indexName("last"))).build(mapperParser); fail("custom index_name is not supported anymore"); } catch (ElasticsearchIllegalArgumentException e) { // expected } // but succeeds if version <= 1.3.0 Version v = Version.V_1_4_0; while (v.onOrAfter(Version.V_1_4_0)) { v = randomFrom(randomVersion()); } settings = ImmutableSettings.builder().put(settings).put(IndexMetaData.SETTING_VERSION_CREATED, v).build(); doc("test", settings, rootObject("person").add(stringField("first").indexName("last"))).build(mapperParser); }	should we check for the mapperparsingexception as root cause? (that is what is thrown if index_name is used on and after 1.4.0)
public ClusterState deleteIndices(ClusterState currentState, Set<Index> indices) { final Metadata meta = currentState.metadata(); final Set<Index> indicesToDelete = new HashSet<>(); final Map<Index, DataStream> backingIndices = new HashMap<>(); for (Index index : indices) { IndexMetadata im = meta.getIndexSafe(index); IndexAbstraction.DataStream parent = meta.getIndicesLookup().get(im.getIndex().getName()).getParentDataStream(); if (parent != null) { if (parent.getWriteIndex().equals(im)) { throw new IllegalArgumentException( "index [" + index.getName() + "] is the write index for data stream [" + parent.getName() + "] and cannot be deleted" ); } else { backingIndices.put(index, parent.getDataStream()); } } indicesToDelete.add(im.getIndex()); } // Check if index deletion conflicts with any running snapshots Set<Index> snapshottingIndices = SnapshotsService.snapshottingIndices(currentState, indicesToDelete); if (snapshottingIndices.isEmpty() == false) { throw new SnapshotInProgressException( "Cannot delete indices that are being snapshotted: " + snapshottingIndices + ". Try again after snapshot finishes or cancel the currently running snapshot." ); } RoutingTable.Builder routingTableBuilder = RoutingTable.builder(currentState.routingTable()); Metadata.Builder metadataBuilder = Metadata.builder(meta); ClusterBlocks.Builder clusterBlocksBuilder = ClusterBlocks.builder().blocks(currentState.blocks()); final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metadataBuilder.indexGraveyard()); final int previousGraveyardSize = graveyardBuilder.tombstones().size(); for (final Index index : indices) { String indexName = index.getName(); logger.info("{} deleting index", index); routingTableBuilder.remove(indexName); clusterBlocksBuilder.removeIndexBlocks(indexName); metadataBuilder.remove(indexName); if (backingIndices.containsKey(index)) { DataStream parent = metadataBuilder.dataStream(backingIndices.get(index).getName()); metadataBuilder.put(parent.removeBackingIndex(index)); } } // add tombstones to the cluster state for each deleted index final IndexGraveyard currentGraveyard = graveyardBuilder.addTombstones(indices).build(settings); metadataBuilder.indexGraveyard(currentGraveyard); // the new graveyard set on the metadata logger.trace( "{} tombstones purged from the cluster state. Previous tombstone size: {}. Current tombstone size: {}.", graveyardBuilder.getNumPurged(), previousGraveyardSize, currentGraveyard.getTombstones().size() ); // add snapshot(s) marked as to delete to the cluster state final Map<String, Set<SnapshotId>> snapshotsToDelete = listOfSnapshotsToDelete(currentState, indicesToDelete); if (snapshotsToDelete.isEmpty() == false) { RepositoriesMetadata repositories = currentState.metadata().custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY); for (Map.Entry<String, Set<SnapshotId>> snapshotToDelete : snapshotsToDelete.entrySet()) { repositories = repositories.addSnapshotsToDelete(snapshotToDelete.getKey(), snapshotToDelete.getValue()); } metadataBuilder.putCustom(RepositoriesMetadata.TYPE, repositories); } Metadata newMetadata = metadataBuilder.build(); ClusterBlocks blocks = clusterBlocksBuilder.build(); // update snapshot restore entries ImmutableOpenMap<String, ClusterState.Custom> customs = currentState.getCustoms(); final RestoreInProgress restoreInProgress = currentState.custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY); RestoreInProgress updatedRestoreInProgress = RestoreService.updateRestoreStateWithDeletedIndices(restoreInProgress, indices); if (updatedRestoreInProgress != restoreInProgress) { ImmutableOpenMap.Builder<String, ClusterState.Custom> builder = ImmutableOpenMap.builder(customs); builder.put(RestoreInProgress.TYPE, updatedRestoreInProgress); customs = builder.build(); } return allocationService.reroute( ClusterState.builder(currentState) .routingTable(routingTableBuilder.build()) .metadata(newMetadata) .blocks(blocks) .customs(customs) .build(), "deleted indices [" + indices + "]" ); }	i think we risk getting an illegalargumentexception from repositoriesmetadata.withupdate when the repo has been deleted while deleting the index.
private static Map<String, Set<SnapshotId>> listOfSnapshotsToDelete(final ClusterState currentState, final Set<Index> indicesToDelete) { final Map<String, Set<SnapshotId>> snapshotsToDelete = new HashMap<>(); for (Index indexToDelete : indicesToDelete) { final Settings indexSettings = currentState.metadata().getIndexSafe(indexToDelete).getSettings(); if (SearchableSnapshotsSettings.isSearchableSnapshotIndexWithSnapshotDeletion(indexSettings) == false) { continue; } final String repositoryName = repositoryNameFromIndexSettings(currentState, indexSettings); final String snapshotName = indexSettings.get(SEARCHABLE_SNAPSHOTS_SNAPSHOT_NAME_SETTING_KEY); final String snapshotUuid = indexSettings.get(SEARCHABLE_SNAPSHOTS_SNAPSHOT_UUID_SETTING_KEY); boolean canDeleteSnapshot = true; // TODO change this to an assertion once it becomes impossible to delete a snapshot that is mounted as an index if (currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY) .getEntries() .stream() .anyMatch(entry -> entry.getSnapshots().contains(new SnapshotId(snapshotName, snapshotUuid)))) { continue; // this snapshot is part of an existing snapshot deletion in progress, nothing to do } for (IndexMetadata other : currentState.metadata()) { if (indicesToDelete.contains(other.getIndex())) { continue; // do not check indices that are going to be deleted } final Settings otherSettings = other.getSettings(); if (SearchableSnapshotsSettings.isSearchableSnapshotStore(otherSettings) == false) { continue; // other index is not a searchable snapshot index, skip } final String otherSnapshotUuid = otherSettings.get(SEARCHABLE_SNAPSHOTS_SNAPSHOT_UUID_SETTING_KEY); if (Objects.equals(snapshotUuid, otherSnapshotUuid) == false) { continue; // other index is backed by a different snapshot, skip } assert otherSettings.getAsBoolean(SEARCHABLE_SNAPSHOTS_DELETE_SNAPSHOT_ON_INDEX_DELETION, false) : other; canDeleteSnapshot = false; // another index is using the same snapshot, do not delete break; } if (canDeleteSnapshot) { snapshotsToDelete.computeIfAbsent(repositoryName, r -> new HashSet<>()) .add(new SnapshotId(indexSettings.get(SEARCHABLE_SNAPSHOTS_SNAPSHOT_NAME_SETTING_KEY), snapshotUuid)); } } return snapshotsToDelete; }	i wonder if we should return null in case there is a repo-uuid on the index but no such repo was found? this works differently from repositoriesservice.indexsettingsmatchrepositorymetadata
public ClusterState execute(ClusterState currentState) { Metadata metadata = currentState.metadata(); Metadata.Builder mdBuilder = Metadata.builder(currentState.metadata()); RepositoriesMetadata repositories = metadata.custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY); List<RepositoryMetadata> repositoriesMetadata = new ArrayList<>(repositories.repositories().size() + 1); for (RepositoryMetadata repositoryMetadata : repositories.repositories()) { if (repositoryMetadata.name().equals(newRepositoryMetadata.name())) { Repository existing = RepositoriesService.this.repositories.get(request.name()); if (existing == null) { existing = RepositoriesService.this.internalRepositories.get(request.name()); } assert existing != null : "repository [" + newRepositoryMetadata.name() + "] must exist"; assert existing.getMetadata() == repositoryMetadata; final RepositoryMetadata updatedMetadata; if (canUpdateInPlace(newRepositoryMetadata, existing)) { if (repositoryMetadata.settings().equals(newRepositoryMetadata.settings())) { // Previous version is the same as this one no update is needed. return currentState; } // we're updating in place so the updated metadata must point at the same uuid and generations updatedMetadata = repositoryMetadata.withSettings(newRepositoryMetadata.settings()); } else { ensureRepositoryNotInUse(currentState, request.name()); updatedMetadata = newRepositoryMetadata.withSnapshotsToDelete(repositoryMetadata.snapshotsToDelete()); } found = true; repositoriesMetadata.add(updatedMetadata); } else { repositoriesMetadata.add(repositoryMetadata); } } if (found == false) { repositoriesMetadata.add(new RepositoryMetadata(request.name(), request.type(), request.settings())); } repositories = new RepositoriesMetadata(repositoriesMetadata); mdBuilder.putCustom(RepositoriesMetadata.TYPE, repositories); changed = true; return ClusterState.builder(currentState).metadata(mdBuilder).build(); }	this could update the repo-reference to point to a completely different repo. i wonder if we should not registere the repository uuid together with the snapshot to delete - and then remove the snapshots to delete if the uuid does not match when it is assigned later? also relates to david's comment in repositorymetadata, keeping the list of snapshots to delete separate from the repo-registration sort of makes sense i think.
* @param listener listener */ private void deleteSnapshotsByUuid(final DeleteSnapshotRequest request, final ActionListener<Void> listener) { deleteSnapshots(SnapshotId::getUUID, request, listener); } /** * Deletes snapshots from the repository. In-progress snapshots matched by the delete will be aborted before deleting them. * Snapshots to delete are identified by converting their {@link SnapshotId} to a {@link String} using the mapping function * {@code mapping}; the resulting string is then compared to the snapshots names/uuids/patterns to match against. * * @param mapping the mapping function used to match the {@link SnapshotId} against the given snapshotNamesOrUuids * @param request the {@link DeleteSnapshotRequest}	i would prefer to let this method have the raw parameters rather than use the request, since in the uuid case it is sort of bending the original purpose of deletesnapshotrequest.
public ClusterState execute(ClusterState currentState) { ensureRepositoryExists(repositoryName, currentState); ensureSnapshotNameAvailableInRepo(repositoryData, snapshotName, repository); ensureNoCleanupInProgress(currentState, repositoryName, snapshotName, "clone snapshot"); final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries(); ensureSnapshotNameNotRunning(runningSnapshots, repositoryName, snapshotName); validate(repositoryName, snapshotName, currentState); final SnapshotId sourceSnapshotId = repositoryData.getSnapshotIds() .stream() .filter(src -> src.getName().equals(request.source())) .findAny() .orElseThrow(() -> new SnapshotMissingException(repositoryName, request.source())); final SnapshotDeletionsInProgress deletionsInProgress = currentState.custom( SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY ); if (deletionsInProgress.getEntries().stream().anyMatch(entry -> entry.getSnapshots().contains(sourceSnapshotId))) { throw new ConcurrentSnapshotExecutionException( repositoryName, sourceSnapshotId.getName(), "cannot clone from snapshot that is being deleted" ); } final RepositoryMetadata repositoryMetadata = currentState.metadata() .custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY) .repository(repositoryName); if (repositoryMetadata != null && repositoryMetadata.snapshotsToDelete().contains(sourceSnapshotId)) { throw new ConcurrentSnapshotExecutionException( repositoryName, sourceSnapshotId.getName(), "cannot clone a snapshot that is marked as deleted" ); } ensureBelowConcurrencyLimit(repositoryName, snapshotName, snapshots, deletionsInProgress); final List<String> indicesForSnapshot = new ArrayList<>(); for (IndexId indexId : repositoryData.getIndices().values()) { if (repositoryData.getSnapshots(indexId).contains(sourceSnapshotId)) { indicesForSnapshot.add(indexId.getName()); } } final List<String> matchingIndices = SnapshotUtils.filterIndices( indicesForSnapshot, request.indices(), request.indicesOptions() ); if (matchingIndices.isEmpty()) { throw new SnapshotException( new Snapshot(repositoryName, sourceSnapshotId), "No indices in the source snapshot [" + sourceSnapshotId + "] matched requested pattern [" + Strings.arrayToCommaDelimitedString(request.indices()) + "]" ); } newEntry = SnapshotsInProgress.startClone( snapshot, sourceSnapshotId, repositoryData.resolveIndices(matchingIndices), threadPool.absoluteTimeInMillis(), repositoryData.getGenId(), minCompatibleVersion(currentState.nodes().getMinNodeVersion(), repositoryData, null) ); return ClusterState.builder(currentState) .putCustom(SnapshotsInProgress.TYPE, SnapshotsInProgress.of(CollectionUtils.appendToCopy(runningSnapshots, newEntry))) .build(); }	++, i wonder if we want similar protection in transportmountsearchablesnapshotaction?
public void testDeleteIndexWithSnapshotDeletion() { final boolean deleteSnapshot = randomBoolean(); final IndexMetadata indexMetadata = IndexMetadata.builder("test") .settings(Settings.builder() .put("index.version.created", VersionUtils.randomVersion(random())) .put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOT_STORE_TYPE) .put(SEARCHABLE_SNAPSHOTS_REPOSITORY_NAME_SETTING_KEY, "repo_name") .put(SEARCHABLE_SNAPSHOTS_REPOSITORY_UUID_SETTING_KEY, randomBoolean() ? null : "repo_uuid") .put(SEARCHABLE_SNAPSHOTS_SNAPSHOT_NAME_SETTING_KEY, "snap_name") .put(SEARCHABLE_SNAPSHOTS_SNAPSHOT_UUID_SETTING_KEY, "snap_uuid") .put(SearchableSnapshotsSettings.SEARCHABLE_SNAPSHOTS_DELETE_SNAPSHOT_ON_INDEX_DELETION, deleteSnapshot) .build()) .numberOfShards(1) .numberOfReplicas(1) .build(); final ClusterState initialState = ClusterState.builder(ClusterName.DEFAULT) .metadata(Metadata.builder() .put(indexMetadata, false) .putCustom(RepositoriesMetadata.TYPE, new RepositoriesMetadata( List.of(new RepositoryMetadata("repo_name", "fs", Settings.EMPTY).withUuid("repo_uuid"))))) .routingTable(RoutingTable.builder().addAsNew(indexMetadata).build()) .blocks(ClusterBlocks.builder().addBlocks(indexMetadata)) .build(); final ClusterState updatedState = service.deleteIndices(initialState, Set.of(indexMetadata.getIndex())); assertThat(updatedState.metadata().getIndices().get("test"), nullValue()); assertThat(updatedState.blocks().indices().get("test"), nullValue()); assertThat(updatedState.routingTable().index("test"), nullValue()); final RepositoriesMetadata updatedRepos = updatedState.metadata().custom(RepositoriesMetadata.TYPE, RepositoriesMetadata.EMPTY); assertThat(updatedRepos.repository("repo_name"), notNullValue()); if (deleteSnapshot) { assertThat(updatedRepos.repository("repo_name").hasSnapshotsToDelete(), equalTo(true)); assertThat(updatedRepos.repository("repo_name").snapshotsToDelete(), hasSize(1)); assertThat(updatedRepos.repository("repo_name").snapshotsToDelete(), hasItem(new SnapshotId("snap_name", "snap_uuid"))); } else { assertThat(updatedRepos.repository("repo_name").hasSnapshotsToDelete(), equalTo(false)); assertThat(updatedRepos.repository("repo_name").snapshotsToDelete(), hasSize(0)); } }	can this be: assertthat(updatedrepos.repository("repo_name").snapshotstodelete(), equalto(list.of(new snapshotid("snap_name", "snap_uuid"))));
private static XContentBuilder mappings() { try { return jsonBuilder() .startObject() .startObject(SINGLE_MAPPING_NAME) .startObject("_meta") .field("version", Version.CURRENT) .endObject() .field("dynamic", "strict") .startObject("properties") .startObject("name") .field("type", "keyword") .endObject() .startObject("chunk") .field("type", "integer") .endObject() .startObject("timestamp") .field("type", "long") .endObject() .startObject("data") .field("type", "binary") .endObject() .endObject() .endObject() .endObject(); } catch (IOException e) { throw new UncheckedIOException("Failed to build mappings for " + DATABASES_INDEX, e); } }	maybe use type date? this still accepts time in ms since epoch and treats values as date.
public void innerToXContent(XContentBuilder builder, Params params) throws IOException { if (from != -1) { builder.field(FROM_FIELD.getPreferredName(), from); } if (size != -1) { builder.field(SIZE_FIELD.getPreferredName(), size); } if (timeout != null && !timeout.equals(TimeValue.MINUS_ONE)) { builder.field(TIMEOUT_FIELD.getPreferredName(), timeout.getStringRep()); } if (terminateAfter != SearchContext.DEFAULT_TERMINATE_AFTER) { builder.field(TERMINATE_AFTER_FIELD.getPreferredName(), terminateAfter); } if (queryBuilder != null) { builder.field(QUERY_FIELD.getPreferredName(), queryBuilder); } if (postQueryBuilder != null) { builder.field(POST_FILTER_FIELD.getPreferredName(), postQueryBuilder); } if (minScore != null) { builder.field(MIN_SCORE_FIELD.getPreferredName(), minScore); } if (version != null) { builder.field(VERSION_FIELD.getPreferredName(), version); } if (explain != null) { builder.field(EXPLAIN_FIELD.getPreferredName(), explain); } if (profile) { builder.field("profile", true); } if (fetchSourceContext != null) { builder.field(_SOURCE_FIELD.getPreferredName(), fetchSourceContext); } if (storedFieldsContext != null) { storedFieldsContext.toXContent(STORED_FIELDS_FIELD.getPreferredName(), builder); } if (docValueFields != null) { builder.startArray(DOCVALUE_FIELDS_FIELD.getPreferredName()); for (String fieldDataField : docValueFields) { builder.value(fieldDataField); } builder.endArray(); } if (scriptFields != null) { builder.startObject(SCRIPT_FIELDS_FIELD.getPreferredName()); for (ScriptField scriptField : scriptFields) { scriptField.toXContent(builder, params); } builder.endObject(); } if (sorts != null) { builder.startArray(SORT_FIELD.getPreferredName()); for (SortBuilder<?> sort : sorts) { sort.toXContent(builder, params); } builder.endArray(); } if (trackScores) { builder.field(TRACK_SCORES_FIELD.getPreferredName(), true); } if (searchAfterBuilder != null) { builder.array(SEARCH_AFTER.getPreferredName(), searchAfterBuilder.getSortValues()); } if (sliceBuilder != null) { builder.field(SLICE.getPreferredName(), sliceBuilder); } if (!indexBoosts.isEmpty()) { builder.startArray(INDICES_BOOST_FIELD.getPreferredName()); for (IndexBoost ib : indexBoosts) { builder.startObject(); builder.field(ib.index, ib.boost); builder.endObject(); } builder.endArray(); } if (aggregations != null) { builder.field(AGGREGATIONS_FIELD.getPreferredName(), aggregations); } if (highlightBuilder != null) { builder.field(HIGHLIGHT_FIELD.getPreferredName(), highlightBuilder); } if (suggestBuilder != null) { builder.field(SUGGEST_FIELD.getPreferredName(), suggestBuilder); } if (rescoreBuilders != null) { builder.startArray(RESCORE_FIELD.getPreferredName()); for (RescoreBuilder<?> rescoreBuilder : rescoreBuilders) { rescoreBuilder.toXContent(builder, params); } builder.endArray(); } if (stats != null) { builder.field(STATS_FIELD.getPreferredName(), stats); } if (extBuilders != null && extBuilders.isEmpty() == false) { builder.startObject(EXT_FIELD.getPreferredName()); for (SearchExtBuilder extBuilder : extBuilders) { extBuilder.toXContent(builder, params); } builder.endObject(); } if (collapse != null) { builder.field(COLLAPSE.getPreferredName(), collapse); } }	can you merge the toxcontent and innertoxcontent() methods?
public double dotProductSparse(VectorScriptDocValues.SparseVectorScriptDocValues dvs) { BytesRef value = dvs.getEncodedValue(); int[] docDims = VectorEncoderDecoder.decodeSparseVectorDims(value); float[] docValues = VectorEncoderDecoder.decodeSparseVector(value); return intDotProductSparse(queryValues, queryDims, docValues, docDims); } } /** * Calculate cosine similarity between a query's sparse vector and documents' sparse vectors * * CosineSimilaritySparse is implemented as a class to use * painless script caching to prepare queryVector and calculate queryVectorMagnitude * only once per script execution for all documents. * A user will call `cosineSimilaritySparse(params.queryVector, doc['my_vector'])` */ public static final class CosineSimilaritySparse extends VectorSparseFunctions { final double queryVectorMagnitude; public CosineSimilaritySparse(Map<String, Number> queryVector) { super(queryVector); double dotProduct = 0; for (int i = 0; i< queryDims.length; i++) { dotProduct += queryValues[i] * queryValues[i]; } this.queryVectorMagnitude = Math.sqrt(dotProduct); } public double cosineSimilaritySparse(VectorScriptDocValues.SparseVectorScriptDocValues dvs) { BytesRef value = dvs.getEncodedValue(); int[] docDims = VectorEncoderDecoder.decodeSparseVectorDims(value); float[] docValues = VectorEncoderDecoder.decodeSparseVector(value); // calculate docVector magnitude double dotProduct = 0; for (float docValue : docValues) { dotProduct += (double) docValue * docValue; } final double docVectorMagnitude = Math.sqrt(dotProduct); double docQueryDotProduct = intDotProductSparse(queryValues, queryDims, docValues, docDims); return docQueryDotProduct / (docVectorMagnitude * queryVectorMagnitude); } } private static double intDotProductSparse(double[] v1Values, int[] v1Dims, float[] v2Values, int[] v2Dims) { double v1v2DotProduct = 0; int v1Index = 0; int v2Index = 0; // find common dimensions among vectors v1 and v2 and calculate dotProduct based on common dimensions while (v1Index < v1Values.length && v2Index < v2Values.length) { if (v1Dims[v1Index] == v2Dims[v2Index]) { v1v2DotProduct += v1Values[v1Index] * v2Values[v2Index]; v1Index++; v2Index++; } else if (v1Dims[v1Index] > v2Dims[v2Index]) { v2Index++; } else { v1Index++; } }	just curious about the deletions here, whats the relation of this to adding the l1/l2 norm? or is this part of a different change?
public void testSparseVectorFunctions() { int[] docVectorDims = {2, 10, 50, 113, 4545}; float[] docVectorValues = {230.0f, 300.33f, -34.8988f, 15.555f, -200.0f}; BytesRef encodedDocVector = VectorEncoderDecoder.encodeSparseVector(docVectorDims, docVectorValues, docVectorDims.length); VectorScriptDocValues.SparseVectorScriptDocValues dvs = mock(VectorScriptDocValues.SparseVectorScriptDocValues.class); when(dvs.getEncodedValue()).thenReturn(encodedDocVector); Map<String, Number> queryVector = new HashMap<String, Number>() {{ put("2", 0.5); put("10", 111.3); put("50", -13.0); put("113", 14.8); put("4545", -156.0); }}; // test dotProduct DotProductSparse docProductSparse = new DotProductSparse(queryVector); double result = docProductSparse.dotProductSparse(dvs); assertEquals("dotProductSparse result is not equal to the expected value!", 65425.62, result, 0.1); // test cosineSimilarity CosineSimilaritySparse cosineSimilaritySparse = new CosineSimilaritySparse(queryVector); double result2 = cosineSimilaritySparse.cosineSimilaritySparse(dvs); assertEquals("cosineSimilaritySparse result is not equal to the expected value!", 0.78, result2, 0.1); // test l1norm L1NormSparse l1Norm = new L1NormSparse(queryVector); double result3 = l1Norm.l1normSparse(dvs); assertEquals("l1normSparse result is not equal to the expected value!", 485.18, result3, 0.1); // test l2norm L2NormSparse l2Norm = new L2NormSparse(queryVector); double result4 = l2Norm.l2normSparse(dvs); assertEquals("l2normSparse result is not equal to the expected value!", 301.36, result4, 0.1); }	i know if might be paranoid, but can you add something so the last doc dimension is larger than the biggest query dimention to trigger the while-loops at the end of the function? and the other way around, we'd probably need a second case where the last queryvector dim > highest doc dim. sorry, might be a bit paranoid but those are code paths i don't see covered otherwise.
public void testSnapshotAndRestoreAllDataStreamsInPlace() throws Exception { CreateSnapshotResponse createSnapshotResponse = client.admin() .cluster() .prepareCreateSnapshot(REPO, SNAPSHOT) .setWaitForCompletion(true) .setIndices("ds") .setIncludeGlobalState(false) .get(); RestStatus status = createSnapshotResponse.getSnapshotInfo().status(); assertEquals(RestStatus.OK, status); assertEquals(Collections.singletonList(DS_BACKING_INDEX_NAME), getSnapshot(REPO, SNAPSHOT).indices()); // Close all indices: CloseIndexRequest closeIndexRequest = new CloseIndexRequest("*"); closeIndexRequest.indicesOptions(IndicesOptions.strictExpandHidden()); assertAcked(client.admin().indices().close(closeIndexRequest).actionGet()); RestoreSnapshotResponse restoreSnapshotResponse = client.admin() .cluster() .prepareRestoreSnapshot(REPO, SNAPSHOT) .setWaitForCompletion(true) .setIndices("ds") .get(); assertEquals(1, restoreSnapshotResponse.getRestoreInfo().successfulShards()); assertEquals(DOCUMENT_SOURCE, client.prepareGet(DS_BACKING_INDEX_NAME, id).get().getSourceAsMap()); SearchHit[] hits = client.prepareSearch("ds").get().getHits().getHits(); assertEquals(1, hits.length); assertEquals(DOCUMENT_SOURCE, hits[0].getSourceAsMap()); GetDataStreamAction.Request getDataSteamRequest = new GetDataStreamAction.Request(new String[]{"*"}); GetDataStreamAction.Response ds = client.execute(GetDataStreamAction.INSTANCE, getDataSteamRequest).get(); assertThat(ds.getDataStreams().stream().map(e -> e.getDataStream().getName()).collect(Collectors.toList()), contains(equalTo("ds"), equalTo("other-ds"))); List<Index> backingIndices = ds.getDataStreams().get(0).getDataStream().getIndices(); assertThat(backingIndices.stream().map(Index::getName).collect(Collectors.toList()), contains(DS_BACKING_INDEX_NAME)); backingIndices = ds.getDataStreams().get(1).getDataStream().getIndices(); String expectedBackingIndexName = DataStream.getDefaultBackingIndexName("other-ds", 1); assertThat(backingIndices.stream().map(Index::getName).collect(Collectors.toList()), contains(expectedBackingIndexName)); }	this test will pas when #70934 is merged. (the restore fails, because after a snapshot was taken, a rollover happened, which created a new backing index, when restoring the version of the data stream where this didn't happen, the restore fails because a unrelated index is in the data stream's namespace)
private void toggleFrozenSettings(final Index[] concreteIndices, final FreezeRequest request, final ActionListener<FreezeResponse> listener) { clusterService.submitStateUpdateTask("toggle-frozen-settings", new AckedClusterStateUpdateTask(Priority.URGENT, request, listener.delegateFailure((delegate, acknowledgedResponse) -> { OpenIndexClusterStateUpdateRequest updateRequest = new OpenIndexClusterStateUpdateRequest() .ackTimeout(request.timeout()).masterNodeTimeout(request.masterNodeTimeout()) .indices(concreteIndices).waitForActiveShards(request.waitForActiveShards()); indexStateService.openIndex(updateRequest, delegate.delegateFailure((l, openIndexClusterStateUpdateResponse) -> l.onResponse(new FreezeResponse(openIndexClusterStateUpdateResponse.isAcknowledged(), openIndexClusterStateUpdateResponse.isShardsAcknowledged())))); })) { @Override public ClusterState execute(ClusterState currentState) { List<String> writeIndices = new ArrayList<>(); SortedMap<String, IndexAbstraction> lookup = currentState.metadata().getIndicesLookup(); for (Index index : concreteIndices) { IndexAbstraction ia = lookup.get(index.getName()); if (ia != null && ia.getParentDataStream() != null && ia.getParentDataStream().getWriteIndex().getIndex().equals(index)) { writeIndices.add(index.getName()); } } if (writeIndices.size() > 0) { throw new IllegalArgumentException("cannot freeze the following data stream write indices [" + Strings.collectionToCommaDelimitedString(writeIndices) + "]"); } final Metadata.Builder builder = Metadata.builder(currentState.metadata()); ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks()); for (Index index : concreteIndices) { IndexMetadata meta = currentState.metadata().getIndexSafe(index); if (meta.getState() != IndexMetadata.State.CLOSE) { throw new IllegalStateException("index [" + index.getName() + "] is not closed"); } final IndexMetadata.Builder imdBuilder = IndexMetadata.builder(meta); imdBuilder.settingsVersion(meta.getSettingsVersion() + 1); final Settings.Builder settingsBuilder = Settings.builder() .put(currentState.metadata().index(index).getSettings()) .put(FrozenEngine.INDEX_FROZEN.getKey(), request.freeze()) .put(IndexSettings.INDEX_SEARCH_THROTTLED.getKey(), request.freeze()); if (request.freeze()) { settingsBuilder.put("index.blocks.write", true); blocks.addIndexBlock(index.getName(), IndexMetadata.INDEX_WRITE_BLOCK); } else { settingsBuilder.remove("index.blocks.write"); blocks.removeIndexBlock(index.getName(), IndexMetadata.INDEX_WRITE_BLOCK); } imdBuilder.settings(settingsBuilder); builder.put(imdBuilder.build(), true); } return ClusterState.builder(currentState).blocks(blocks).metadata(builder).build(); } }); }	i didn't add a unit test for this, because currently for this code no unit tests exist. the cannot freeze write index for data stream yaml test does test this.
public <Request extends ReplicatedWriteRequest<Request>, Response extends ReplicationResponse & WriteResponse> WritePrimaryResult<Request, Response> executeSingleItemBulkRequestOnPrimary( Request request, IndexShard primary) throws Exception { BulkItemRequest[] itemRequests = new BulkItemRequest[1]; WriteRequest.RefreshPolicy refreshPolicy = request.getRefreshPolicy(); request.setRefreshPolicy(WriteRequest.RefreshPolicy.NONE); itemRequests[0] = new BulkItemRequest(0, ((DocWriteRequest) request)); BulkShardRequest bulkShardRequest = new BulkShardRequest(request.shardId(), refreshPolicy, itemRequests); WritePrimaryResult<BulkShardRequest, BulkShardResponse> result = shardOperationOnPrimary(bulkShardRequest, primary); BulkShardResponse bulkShardResponse = result.finalResponseIfSuccessful; assert bulkShardResponse.getResponses().length == 1: "expected only one bulk shard response"; BulkItemResponse itemResponse = bulkShardResponse.getResponses()[0]; final Response response; final Exception failure; if (itemResponse.isFailed()) { failure = itemResponse.getFailure().getCause(); response = null; } else { response = (Response) itemResponse.getResponse(); failure = null; } return new WritePrimaryResult<>(request, response, result.location, failure, primary, logger); }	feels weird to mutate the incoming request here. why is it needed?
protected void doExecute(Task task, final IndexRequest request, final ActionListener<IndexResponse> listener) { BulkRequest bulkRequest = new BulkRequest(); bulkRequest.add(request); bulkRequest.setRefreshPolicy(request.getRefreshPolicy()); bulkRequest.timeout(request.timeout()); bulkRequest.waitForActiveShards(request.waitForActiveShards()); request.setRefreshPolicy(WriteRequest.RefreshPolicy.NONE); bulkAction.execute(task, bulkRequest, new ActionListener<BulkResponse>() { @Override public void onResponse(BulkResponse bulkItemResponses) { assert bulkItemResponses.getItems().length == 1: "expected only one item in bulk request"; BulkItemResponse bulkItemResponse = bulkItemResponses.getItems()[0]; if (bulkItemResponse.isFailed() == false) { IndexResponse response = bulkItemResponse.getResponse(); listener.onResponse(response); } else { listener.onFailure(bulkItemResponse.getFailure().getCause()); } } @Override public void onFailure(Exception e) { listener.onFailure(e); } }); }	since this is also the same code here as in delete, i wonder if we should add a generic bwc class that will do all this shared munging (and the code from the bulk action as well).
public DataFrameTransformConfig getConfig() { return config; }	why is it sometimes spelled singular ("validation") and sometimes plural ("validations")? is it deliberate?
public Boolean getDeferValidations() { return deferValidations; } /** * Indicates if deferrable validations be skipped until the transform starts * * @param deferValidations {@code true}	s/be skipped/should be skipped/ ?
public Boolean getDeferValidations() { return deferValidations; } /** * Indicates if deferrable validations be skipped until the transform starts * * @param deferValidations {@code true}	i think the parameter type can be "boolean" here as when you call "setdefervalidations" you know whether you want "true" or "false".
public void testPutDataFrameTransform() throws IOException { PutDataFrameTransformRequest putRequest = new PutDataFrameTransformRequest( DataFrameTransformConfigTests.randomDataFrameTransformConfig()); Request request = DataFrameRequestConverters.putDataFrameTransform(putRequest); assertFalse(request.getParameters().containsKey("defer_validation")); assertEquals(HttpPut.METHOD_NAME, request.getMethod()); assertThat(request.getEndpoint(), equalTo("/_data_frame/transforms/" + putRequest.getConfig().getId())); try (XContentParser parser = createParser(JsonXContent.jsonXContent, request.getEntity().getContent())) { DataFrameTransformConfig parsedConfig = DataFrameTransformConfig.PARSER.apply(parser, null); assertThat(parsedConfig, equalTo(putRequest.getConfig())); } putRequest.setDeferValidations(true); request = DataFrameRequestConverters.putDataFrameTransform(putRequest); assertThat(request.getParameters().get("defer_validation"), equalTo(Boolean.toString(putRequest.getDeferValidations()))); }	i'd rewrite it to: assertthat(request.getparameters(), not(haskey("defer_validation")); this has the advantage of spitting better message when the assertion fails.
public void testPutDataFrameTransform() throws IOException { PutDataFrameTransformRequest putRequest = new PutDataFrameTransformRequest( DataFrameTransformConfigTests.randomDataFrameTransformConfig()); Request request = DataFrameRequestConverters.putDataFrameTransform(putRequest); assertFalse(request.getParameters().containsKey("defer_validation")); assertEquals(HttpPut.METHOD_NAME, request.getMethod()); assertThat(request.getEndpoint(), equalTo("/_data_frame/transforms/" + putRequest.getConfig().getId())); try (XContentParser parser = createParser(JsonXContent.jsonXContent, request.getEntity().getContent())) { DataFrameTransformConfig parsedConfig = DataFrameTransformConfig.PARSER.apply(parser, null); assertThat(parsedConfig, equalTo(putRequest.getConfig())); } putRequest.setDeferValidations(true); request = DataFrameRequestConverters.putDataFrameTransform(putRequest); assertThat(request.getParameters().get("defer_validation"), equalTo(Boolean.toString(putRequest.getDeferValidations()))); }	i'd rewrite it to: assertthat(request.getparameters(), hasentry("defer_validation", boolean.tostring(putrequest.getdefervalidations()));
@Override public Query wrapFunctionQuery(ScalarFunction sf, Expression field, Supplier<Query> querySupplier) { if (field instanceof StDistance && querySupplier instanceof GeoDistanceQuery) { return ExpressionTranslator.wrapIfNested(querySupplier.get(), ((StDistance) field).left()); } if (field instanceof FieldAttribute) { return ExpressionTranslator.wrapIfNested(querySupplier.get(), field); } return new ScriptQuery(sf.source(), sf.asScript()); }	shouldn't it be: querysupplier.get()?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); recoveryId = in.readLong(); shardId = ShardId.readShardId(in); targetAllocationId = in.readString(); sourceNode = new DiscoveryNode(in); targetNode = new DiscoveryNode(in); metadataSnapshot = new Store.MetadataSnapshot(in); primaryRelocation = in.readBoolean(); if (in.getVersion().onOrAfter(Version.V_6_0_0_alpha1_UNRELEASED)) { startingSeqNo = in.readLong(); } else { startingSeqNo = SequenceNumbersService.UNASSIGNED_SEQ_NO; } }	this needs some version conditioned reads / writes?!
@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, ActionListener<Void> listener) { if (isReadOnly()) { listener.onFailure(new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository")); } else { SnapshotInfo snapshot = null; try { snapshot = getSnapshotInfo(snapshotId); } catch (SnapshotMissingException ex) { listener.onFailure(ex); return; } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) { logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex); } // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots final RepositoryData updatedRepositoryData; final Map<String, BlobContainer> foundIndices; final Map<String, BlobMetaData> rootBlobs; try { rootBlobs = blobContainer().listBlobs(); final RepositoryData repositoryData = getRepositoryData(latestGeneration(rootBlobs.keySet())); updatedRepositoryData = repositoryData.removeSnapshot(snapshotId); // Cache the indices that were found before writing out the new index-N blob so that a stuck master will never // delete an index that was created by another master node after writing this index-N blob. foundIndices = blobStore().blobContainer(indicesPath()).children(); writeIndexGen(updatedRepositoryData, repositoryStateId); } catch (Exception ex) { listener.onFailure(new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex)); return; } final List<String> snapMetaFilesToDelete = Arrays.asList(snapshotFormat.blobName(snapshotId.getUUID()), globalMetaDataFormat.blobName(snapshotId.getUUID())); try { blobContainer().deleteBlobsIgnoringIfNotExists(snapMetaFilesToDelete); } catch (Exception e) { logger.warn(() -> new ParameterizedMessage("[{}] Unable to delete global metadata files", snapshotId), e); } final var survivingIndices = updatedRepositoryData.getIndices(); deleteIndices( updatedRepositoryData, Optional.ofNullable(snapshot) .map(info -> info.indices().stream().filter(survivingIndices::containsKey) .map(updatedRepositoryData::resolveIndexId).collect(Collectors.toList())) .orElse(Collections.emptyList()), snapshotId, ActionListener.delegateFailure(listener, (l, v) -> cleanupStaleBlobs(foundIndices, Sets.difference(rootBlobs.keySet(), new HashSet<>(snapMetaFilesToDelete)).stream().collect( Collectors.toMap(Function.identity(), rootBlobs::get)), updatedRepositoryData, ActionListener.map(l, ignored -> null)))); } } /** * Cleans up stale blobs directly under the repository root as well as all indices paths that aren't referenced by any existing * snapshots. This method is only to be called directly after a new {@link RepositoryData} was written to the repository and with * parameters {@code foundIndices}, {@code rootBlobs} * * @param foundIndices all indices blob containers found in the repository before {@code newRepoData} was written * @param rootBlobs all blobs found directly under the repository root * @param newRepoData new repository data that was just written * @param listener listener to invoke with the combined {@link DeleteResult}	didn't you just make this change unnecessary by handling all exceptions and re-throwing them as ioexception?
public static Translog.Location performOnReplica(BulkShardRequest request, IndexShard replica) throws Exception { Translog.Location location = null; final long primaryTerm = request.primaryTerm(); for (int i = 0; i < request.items().length; i++) { BulkItemRequest item = request.items()[i]; final Engine.Result operationResult; DocWriteRequest docWriteRequest = item.request(); try { switch (replicaItemExecutionMode(item, i)) { case NORMAL: final DocWriteResponse primaryResponse = item.getPrimaryResponse().getResponse(); operationResult = performNormalOpOnReplica(primaryResponse, docWriteRequest, primaryTerm, replica); assert operationResult != null : "operation result must never be null when primary response has no failure"; location = syncOperationResultOrThrow(operationResult, location); break; case NOOP: break; case FAILURE: final BulkItemResponse.Failure failure = item.getPrimaryResponse().getFailure(); assert failure.getSeqNo() != SequenceNumbersService.UNASSIGNED_SEQ_NO : "seq no must be assigned"; operationResult = replica.applyNoOpOnReplica(failure.getSeqNo(), primaryTerm, failure.getMessage()); assert operationResult != null : "operation result must never be null when primary response has no failure"; location = syncOperationResultOrThrow(operationResult, location); break; default: throw new IllegalStateException("illegal replica item execution mode for: " + docWriteRequest); } } catch (Exception e) { // if its not an ignore replica failure, we need to make sure to bubble up the failure // so we will fail the shard if (!TransportActions.isShardNotAvailableException(e)) { throw e; } } } return location; }	nit: i personally like the replica.markseqnoasnoop name better. applynoop is a strange notion (why do you have to apply it if there's nothing to do). also, it conflicts with the noop execution mode one case above it.
static Engine.IndexResult executeIndexRequestOnPrimary(IndexRequest request, IndexShard primary, MappingUpdatePerformer mappingUpdater) throws Exception { final SourceToParse sourceToParse = SourceToParse.source(request.index(), request.type(), request.id(), request.source(), request.getContentType()) .routing(request.routing()).parent(request.parent()); try { return primary.applyIndexOperationOnPrimary(request.version(), request.versionType(), sourceToParse, request.getAutoGeneratedTimestamp(), request.isRetry(), update -> { mappingUpdater.updateMappings(update, primary.shardId(), sourceToParse.type()); throw new ReplicationOperation.RetryOnPrimaryException(primary.shardId(), "Mapping updated"); }); } catch (ReplicationOperation.RetryOnPrimaryException e) { return primary.applyIndexOperationOnPrimary(request.version(), request.versionType(), sourceToParse, request.getAutoGeneratedTimestamp(), request.isRetry(), update -> mappingUpdater.verifyMappings(update, primary.shardId())); } }	throwing a retryonprimaryexception feels ugly. i see why you did it and i can't come up with something better. on top of that, this made me realize that retryonprimaryexception has serious problems. i'll reach out to discuss. to be clear - this shouldn't stop this pr as it is an existing situation.
private Engine.IndexResult applyIndexOperation(long seqNo, long opPrimaryTerm, long version, VersionType versionType, long autoGeneratedTimeStamp, boolean isRetry, Engine.Operation.Origin origin, SourceToParse sourceToParse, Consumer<Mapping> onMappingUpdate) throws IOException { assert opPrimaryTerm <= this.primaryTerm : "op term [ " + opPrimaryTerm + " ] > shard term [" + this.primaryTerm + "]"; assert versionType.validateVersionForWrites(version); ensureWriteAllowed(origin); Engine.Index operation; try { operation = prepareIndex(docMapper(sourceToParse.type()), sourceToParse, seqNo, opPrimaryTerm, version, versionType, origin, autoGeneratedTimeStamp, isRetry); Mapping update = operation.parsedDoc().dynamicMappingsUpdate(); if (update != null) { onMappingUpdate.accept(update); } } catch (MapperParsingException | IllegalArgumentException e) { return new Engine.IndexResult(e, version, seqNo); } catch (Exception e) { verifyNotClosed(e); throw e; } return index(getEngine(), operation); }	can we put this catch clause around the prepareindex call alone? illegalargumentexception is fairly generic and may come of onmappingupdate as well, but i don't think we want to support that.
@Override protected QueryBuilder doRewrite(QueryRewriteContext queryRewriteContext) throws IOException { if (supplier != null) { return supplier.get() == null ? this : new GeoShapeQueryBuilder(this.fieldName, supplier.get()).relation(relation).strategy (strategy); } else if (this.shape == null) { AtomicReference<ShapeBuilder> supplier = new AtomicReference<>(); queryRewriteContext.registerAsyncAction((client, listener) -> { GetRequest getRequest = new GetRequest(indexedShapeIndex, indexedShapeType, indexedShapeId); fetch(client, getRequest, indexedShapePath, ActionListener.wrap(builder-> { supplier.set(builder); listener.onResponse(null); }, listener::onFailure)); }); return new GeoShapeQueryBuilder(this.fieldName, supplier::get, this.indexedShapeId, this.indexedShapeType).relation(relation) .strategy(strategy); } return this; }	i wonder if atomicreference is needed here given that we write from a single thread. it's just a visibility problem hence setonce would be a good fit, which has also the advantage of checking that we do set it only once (thanks for the suggestion!)
private void rewriteShardRequest(ShardSearchRequest request, ActionListener<ShardSearchRequest> listener) { Rewriteable.rewriteAndFetch(request.getRewriteable(), indicesService.getRewriteContext(request::nowInMillis), ActionListener.wrap(r -> threadPool.executor(Names.SEARCH).execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { listener.onFailure(e); } @Override protected void doRun() throws Exception { listener.onResponse(request); } }), listener::onFailure)); }	would be nice to address the unchecked warning on this line. to be honest i tried that myself and i didn't succeed :)
public void testTransportClient() throws URISyntaxException, IOException { try (CloseableHttpClient client = HttpClientBuilder.create().build()) { final String str = String.format( Locale.ROOT, "%s/employees/1", System.getProperty("tests.jboss.home") ); logger.info("Connecting to uri: " + str); final HttpPut put = new HttpPut(new URI(str)); final String body; try (XContentBuilder builder = jsonBuilder()) { builder.startObject(); { builder.field("first_name", "John"); builder.field("last_name", "Smith"); builder.field("age", 25); builder.field("about", "I love to go rock climbing"); builder.startArray("interests"); { builder.value("sports"); builder.value("music"); } builder.endArray(); } builder.endObject(); body = Strings.toString(builder); } put.setEntity(new StringEntity(body, ContentType.APPLICATION_JSON)); try (CloseableHttpResponse response = client.execute(put)) { int status = response.getStatusLine().getStatusCode(); assertThat("expected a 201 response but got: " + status + " - body: " + EntityUtils.toString(response.getEntity()), status, equalTo(201)); } final HttpGet get = new HttpGet(new URI(str)); try ( CloseableHttpResponse response = client.execute(get); XContentParser parser = JsonXContent.jsonXContent.createParser( new NamedXContentRegistry(ClusterModule.getNamedXWriteables()), DeprecationHandler.THROW_UNSUPPORTED_OPERATION, response.getEntity().getContent())) { final Map<String, Object> map = parser.map(); assertThat(map.get("first_name"), equalTo("John")); assertThat(map.get("last_name"), equalTo("Smith")); assertThat(map.get("age"), equalTo(25)); assertThat(map.get("about"), equalTo("I love to go rock climbing")); final Object interests = map.get("interests"); assertThat(interests, instanceOf(List.class)); @SuppressWarnings("unchecked") final List<String> interestsAsList = (List<String>) interests; assertThat(interestsAsList, containsInAnyOrder("sports", "music")); } } }	i think "home" means something else in this context. maybe test.wildfly.root?
public static void writeBuild(Build build, StreamOutput out) throws IOException { if (out.getVersion().onOrAfter(Version.V_6_3_0)) { out.writeString(build.flavor().displayName()); } if (out.getVersion().onOrAfter(Version.V_6_3_0)) { out.writeString(build.type().displayName()); } out.writeString(build.shortHash()); out.writeString(build.date()); out.writeBoolean(build.isSnapshot()); if (out.getVersion().onOrAfter(Version.V_7_0_0)) { out.writeString(build.getQualifiedVersion()); } }	could you add javadoc to this?
static void warnIfPreRelease(final Build build,final Logger logger) { if (build.isProductionRelease() == false) { logger.warn( "version [{}] is a pre-release version of Elasticsearch and is not suitable for production", Build.CURRENT.getQualifiedVersion()); } }	i think maybe this isn't worth being its own method any more. now that we're testing isproductionrelease instead.
public static Nullable and(Nullable... args) { if (args.length == 0) { return UNKNOWN; } Nullable returnValue = args[0]; // UKNOWN AND <anything> => UKNOWN // NEVER AND NEVER => NEVER // POSSIBLE AND NEVER/POSSIBLE => POSSIBLE for (int i = 1; i < args.length; i++) { if (returnValue == UNKNOWN || args[i] == UNKNOWN) { returnValue = UNKNOWN; } else if (returnValue == POSSIBLY || args[i] == POSSIBLY) { returnValue = POSSIBLY; } } return returnValue; }	the never case doesn't seem to be handled. also it might make sense to assign some bitmasks to the enum to make the comparison simpler and use just one accumulator. that is, instead of checking the return value and the next value, it would be easier to just 'combine' the current value (returnvalue) with the next value and be done with. (an oo way would be to define a method on the enum but that adds a virtual call for minimum gain). further more since unknown trumps everything, the loop could be skipped if this value is found: java nullability value = null; for (nullability n: nullables) { switch (n) { case unknown: return unknown; case possible: value = n; case never: if (value == null) { value = n; } } } return value;
private void processAggs(long docCount, List<Aggregation> aggregations) throws IOException { if (aggregations.isEmpty()) { // This means we reached a bucket aggregation without sub-aggs. Thus, we can flush the path written so far. queueDocToWrite(keyValuePairs, docCount); return; } List<Aggregation> leafAggregations = new ArrayList<>(); List<MultiBucketsAggregation> bucketAggregations = new ArrayList<>(); List<SingleBucketAggregation> singleBucketAggregations = new ArrayList<>(); // Sort into leaf and bucket aggregations. // The leaf aggregations will be processed first. for (Aggregation agg : aggregations) { if (agg instanceof MultiBucketsAggregation) { bucketAggregations.add((MultiBucketsAggregation)agg); } else if (agg instanceof SingleBucketAggregation){ // Skip a level down for single bucket aggs, if they have a sub-agg that is not // a bucketed agg we should treat it like a leaf in this bucket SingleBucketAggregation singleBucketAggregation = (SingleBucketAggregation)agg; for (Aggregation subAgg : singleBucketAggregation.getAggregations()) { if (subAgg instanceof MultiBucketsAggregation || subAgg instanceof SingleBucketAggregation) { singleBucketAggregations.add(singleBucketAggregation); } else { leafAggregations.add(subAgg); } } } else { leafAggregations.add(agg); } } // If on the current level (indicated via bucketAggregations) or one of the next levels (singleBucketAggregations) // we have more than 1 `MultiBucketsAggregation`, we should error out. // We need to make the check in this way as each of the items in `singleBucketAggregations` is treated as a separate branch // in the recursive handling of this method. int bucketAggLevelCount = Math.max(bucketAggregations.size(), (int)singleBucketAggregations.stream() .flatMap(s -> asList(s.getAggregations()).stream()) .filter(MultiBucketsAggregation.class::isInstance) .count()); if (bucketAggLevelCount > 1) { throw new IllegalArgumentException("Multiple bucket aggregations at the same level are not supported"); } List<String> addedLeafKeys = new ArrayList<>(); for (Aggregation leafAgg : leafAggregations) { if (timeField.equals(leafAgg.getName())) { processTimeField(leafAgg); } else if (fields.contains(leafAgg.getName())) { boolean leafAdded = processLeaf(leafAgg); if (leafAdded) { addedLeafKeys.add(leafAgg.getName()); } } } boolean noMoreBucketsToProcess = bucketAggregations.isEmpty(); if (noMoreBucketsToProcess == false) { MultiBucketsAggregation bucketAgg = bucketAggregations.get(0); if (bucketAgg instanceof Histogram) { processDateHistogram((Histogram) bucketAgg); } else { // Ignore bucket aggregations that don't contain a field we // are interested in. This avoids problems with pipeline bucket // aggregations where we would create extra docs traversing a // bucket agg that isn't used but is required for the pipeline agg. if (bucketAggContainsRequiredAgg(bucketAgg)) { processBucket(bucketAgg, fields.contains(bucketAgg.getName())); } else { noMoreBucketsToProcess = true; } } } noMoreBucketsToProcess = singleBucketAggregations.isEmpty() && noMoreBucketsToProcess; // we support more than one `SingleBucketAggregation` at each level // However, we only want to recurse with multi/single bucket aggs. // Non-bucketed sub-aggregations were handle as leaf aggregations at this level for (SingleBucketAggregation singleBucketAggregation : singleBucketAggregations) { processAggs(singleBucketAggregation.getDocCount(), asList(singleBucketAggregation.getAggregations()) .stream() .filter( aggregation -> (aggregation instanceof MultiBucketsAggregation || aggregation instanceof SingleBucketAggregation)) .collect(Collectors.toList())); } // If there are no more bucket aggregations to process we've reached the end // and it's time to write the doc if (noMoreBucketsToProcess) { queueDocToWrite(keyValuePairs, docCount); } addedLeafKeys.forEach(k -> keyValuePairs.remove(k)); }	nit: don't need the aslist
private void processAggs(long docCount, List<Aggregation> aggregations) throws IOException { if (aggregations.isEmpty()) { // This means we reached a bucket aggregation without sub-aggs. Thus, we can flush the path written so far. queueDocToWrite(keyValuePairs, docCount); return; } List<Aggregation> leafAggregations = new ArrayList<>(); List<MultiBucketsAggregation> bucketAggregations = new ArrayList<>(); List<SingleBucketAggregation> singleBucketAggregations = new ArrayList<>(); // Sort into leaf and bucket aggregations. // The leaf aggregations will be processed first. for (Aggregation agg : aggregations) { if (agg instanceof MultiBucketsAggregation) { bucketAggregations.add((MultiBucketsAggregation)agg); } else if (agg instanceof SingleBucketAggregation){ // Skip a level down for single bucket aggs, if they have a sub-agg that is not // a bucketed agg we should treat it like a leaf in this bucket SingleBucketAggregation singleBucketAggregation = (SingleBucketAggregation)agg; for (Aggregation subAgg : singleBucketAggregation.getAggregations()) { if (subAgg instanceof MultiBucketsAggregation || subAgg instanceof SingleBucketAggregation) { singleBucketAggregations.add(singleBucketAggregation); } else { leafAggregations.add(subAgg); } } } else { leafAggregations.add(agg); } } // If on the current level (indicated via bucketAggregations) or one of the next levels (singleBucketAggregations) // we have more than 1 `MultiBucketsAggregation`, we should error out. // We need to make the check in this way as each of the items in `singleBucketAggregations` is treated as a separate branch // in the recursive handling of this method. int bucketAggLevelCount = Math.max(bucketAggregations.size(), (int)singleBucketAggregations.stream() .flatMap(s -> asList(s.getAggregations()).stream()) .filter(MultiBucketsAggregation.class::isInstance) .count()); if (bucketAggLevelCount > 1) { throw new IllegalArgumentException("Multiple bucket aggregations at the same level are not supported"); } List<String> addedLeafKeys = new ArrayList<>(); for (Aggregation leafAgg : leafAggregations) { if (timeField.equals(leafAgg.getName())) { processTimeField(leafAgg); } else if (fields.contains(leafAgg.getName())) { boolean leafAdded = processLeaf(leafAgg); if (leafAdded) { addedLeafKeys.add(leafAgg.getName()); } } } boolean noMoreBucketsToProcess = bucketAggregations.isEmpty(); if (noMoreBucketsToProcess == false) { MultiBucketsAggregation bucketAgg = bucketAggregations.get(0); if (bucketAgg instanceof Histogram) { processDateHistogram((Histogram) bucketAgg); } else { // Ignore bucket aggregations that don't contain a field we // are interested in. This avoids problems with pipeline bucket // aggregations where we would create extra docs traversing a // bucket agg that isn't used but is required for the pipeline agg. if (bucketAggContainsRequiredAgg(bucketAgg)) { processBucket(bucketAgg, fields.contains(bucketAgg.getName())); } else { noMoreBucketsToProcess = true; } } } noMoreBucketsToProcess = singleBucketAggregations.isEmpty() && noMoreBucketsToProcess; // we support more than one `SingleBucketAggregation` at each level // However, we only want to recurse with multi/single bucket aggs. // Non-bucketed sub-aggregations were handle as leaf aggregations at this level for (SingleBucketAggregation singleBucketAggregation : singleBucketAggregations) { processAggs(singleBucketAggregation.getDocCount(), asList(singleBucketAggregation.getAggregations()) .stream() .filter( aggregation -> (aggregation instanceof MultiBucketsAggregation || aggregation instanceof SingleBucketAggregation)) .collect(Collectors.toList())); } // If there are no more bucket aggregations to process we've reached the end // and it's time to write the doc if (noMoreBucketsToProcess) { queueDocToWrite(keyValuePairs, docCount); } addedLeafKeys.forEach(k -> keyValuePairs.remove(k)); }	now that singlebucketaggregations only has multi or single bucket aggs added does this calculation not simplify to bucketaggregations.size() + singlebucketaggregations.size()
@Override public Query termQuery(Object value, QueryShardContext context) { throw new QueryShardException(context, "Murmur3 fields are not searchable: " + this.nameForMessages()); } } protected Murmur3FieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType, Settings indexSettings, MultiFields multiFields, CopyTo copyTo) { super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo); } @Override protected String contentType() { return CONTENT_TYPE; } @Override protected void parseCreateField(ParseContext context, List<IndexableField> fields) throws IOException { final Object value; if (context.externalValueSet()) { value = context.externalValue(); } else { value = context.parser().textOrNull(); } if (value != null) { final BytesRef bytes = new BytesRef(value.toString()); final long hash = MurmurHash3.hash128(bytes.bytes, bytes.offset, bytes.length, 0, new MurmurHash3.Hash128()).h1; fields.add(new SortedNumericDocValuesField(fieldType().name(), hash)); if (fieldType().stored()) { fields.add(new StoredField(name(), hash)); } }	this.nameformessages auto includes the surround brackets. see other comments where i explain why this method was introduced.
public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType fieldType) { Objects.requireNonNull(fieldType, "fieldType is null"); return (IFD) indexFieldDataService.apply(fieldType.fieldTypeForIndex(), fullyQualifiedIndexName); }	if a non alias field type will simply return its name, for aliases it will return the name of the alias target, and the rest will just work!
protected Field[] getFields(IndexReader reader, int docId, String fieldName) throws IOException { // $fieldName is ignored as we ask mapper.fieldType which may or may not be an alias. // we know its low level reader, and matching docId, since that's how we call the highlighter with SourceLookup sourceLookup = searchContext.lookup().source(); sourceLookup.setSegmentAndDocument((LeafReaderContext) reader.getContext(), docId); final String fieldInIndex = mapper.fieldType().nameForIndex(); List<Object> values = sourceLookup.extractRawValues(fieldInIndex); if (values.isEmpty()) { return EMPTY_FIELDS; } Field[] fields = new Field[values.size()]; for (int i = 0; i < values.size(); i++) { fields[i] = new Field(fieldInIndex, values.get(i).toString(), TextField.TYPE_NOT_STORED); } return fields; }	does the right thing if an alias...
<Req extends ActionRequest, Resp> void performRequestAsync(Req request, CheckedFunction<Req, Request, IOException> requestConverter, CheckedFunction<Response, Resp, IOException> responseConverter, ActionListener<Resp> listener, Set<Integer> ignores, Header... headers) { ActionRequestValidationException validationException = request.validate(); if (validationException != null) { listener.onFailure(validationException); return; } try { Request req = requestConverter.apply(request); ResponseListener responseListener = wrapResponseListener(responseConverter, listener, ignores); client.performRequestAsync(req.method, req.endpoint, req.params, req.entity, responseListener, headers); } catch (Exception e) { listener.onFailure(e); } }	i am not sure here, i think for correctness we should only wrap the requestconverter call? the wrapresponselistener and performrequestasync should really never throw exception. wdyt?
public interface Factory extends ScriptFactory { LeafFactory newFactory(String fieldName, Map<String, Object> params, SearchLookup searchLookup); } public interface LeafFactory { GeoPointFieldScript newInstance(LeafReaderContext ctx); } public GeoPointFieldScript(String fieldName, Map<String, Object> params, SearchLookup searchLookup, LeafReaderContext ctx) { super(fieldName, params, searchLookup, ctx); } /** * Consumers must copy the emitted GeoPoint(s) if stored. */ public void runGeoPointForDoc(int doc, Consumer<GeoPoint> consumer) { runForDoc(doc); GeoPoint point = new GeoPoint(); for (int i = 0; i < count(); i++) { final int lat = (int) (values()[i] >>> 32); final int lon = (int) (values()[i] & 0xFFFFFFFF); point.reset(GeoEncodingUtils.decodeLatitude(lat), GeoEncodingUtils.decodeLongitude(lon)); consumer.accept(point); } } @Override protected List<Object> extractFromSource(String path) { Object value = XContentMapValues.extractValue(path, leafSearchLookup.source().source()); if (value instanceof List<?>) { @SuppressWarnings("unchecked") List<Object> list = (List<Object>) value; if (list.size() > 0 && list.get(0) instanceof Number) { //[2, 1]: two values but one single point, return it as a list or each value will be seen as a different geopoint. return Collections.singletonList(list); } //e.g. [ [2,1], {lat:2, lon:1} ] return list; } //e.g. {lat: 2, lon: 1} return Collections.singletonList(value); } @Override protected void emitFromObject(Object value) { if (value instanceof List<?>) { List<?> values = (List<?>) value; if (values.size() > 0 && values.get(0) instanceof Number) { emitPoint(value); } else { for (Object point : values) { emitPoint(point); } } } else { emitPoint(value); } } private void emitPoint(Object point) { if (point != null) { final GeoPoint scratch = new GeoPoint(); try { GeoUtils.parseGeoPoint(point, scratch, true); } catch(Exception e) { //ignore } emit(scratch.lat(), scratch.lon()); } } protected final void emit(double lat, double lon) { int latitudeEncoded = encodeLatitude(lat); int longitudeEncoded = encodeLongitude(lon); emit(Long.valueOf((((long) latitudeEncoded) << 32) | (longitudeEncoded & 0xFFFFFFFFL))); } public static class Emit { private final GeoPointFieldScript script; public Emit(GeoPointFieldScript script) { this.script = script; } public void emit(double lat, double lon) { script.emit(lat, lon); } }	we were avoiding allocating a new scratch geopoint for every emit() beforehand, is there a way of sharing this somehow? scripts are run on a single thread so we don't need to worry about thread safety.
private static EncryptedPrivateKeyInfo getEncryptedPrivateKeyInfo(byte[] keyBytes) throws IOException, GeneralSecurityException { try { return new EncryptedPrivateKeyInfo(keyBytes); } catch (IOException e) { // The Sun JCE provider can't handle non-AES PBES2 data (but it can handle PBES1 DES data - go figure) // It's not worth our effort to try and decrypt it ourselves, but we can detect it and give a good error message DerParser parser = new DerParser(keyBytes); final DerParser.Asn1Object rootSeq = parser.readAsn1Object(DerParser.Type.SEQUENCE); parser = rootSeq.getParser(); final DerParser.Asn1Object algSeq = parser.readAsn1Object(DerParser.Type.SEQUENCE); parser = algSeq.getParser(); final String algId = parser.readAsn1Object(DerParser.Type.OBJECT_OID).getOid(); if (PBES2_OID.equals(algId)) { final DerParser.Asn1Object algData = parser.readAsn1Object(DerParser.Type.SEQUENCE); parser = algData.getParser(); final DerParser.Asn1Object ignoreKdf = parser.readAsn1Object(DerParser.Type.SEQUENCE); final DerParser.Asn1Object cryptSeq = parser.readAsn1Object(DerParser.Type.SEQUENCE); parser = cryptSeq.getParser(); final String encryptionId = parser.readAsn1Object(DerParser.Type.OBJECT_OID).getOid(); if (encryptionId.startsWith(AES_OID) == false) { final String name = getAlgorithmNameFromOid(encryptionId); throw new GeneralSecurityException( "PKCS#8 Private Key is encrypted with unsupported PBES2 algorithm [" + encryptionId + "]" + (name == null ? "" : " (" + name + ")"), e ); } if (JavaVersion.current().compareTo(JavaVersion.parse("11.0.0")) < 0) { // PBES2 appears to be supported on Oracle 8, but not OpenJDK8 // We don't both clarifying that here because it is supported on the bundled JDK, and that's what people should use throw new GeneralSecurityException( "PKCS#8 Private Key is encrypted with PBES2 which is not supported on this JDK [" + JavaVersion.current() + "]", e ); } } throw e; } }	both -> bother ( in case you need to make another commit for another reason)
private static EncryptedPrivateKeyInfo getEncryptedPrivateKeyInfo(byte[] keyBytes) throws IOException, GeneralSecurityException { try { return new EncryptedPrivateKeyInfo(keyBytes); } catch (IOException e) { // The Sun JCE provider can't handle non-AES PBES2 data (but it can handle PBES1 DES data - go figure) // It's not worth our effort to try and decrypt it ourselves, but we can detect it and give a good error message DerParser parser = new DerParser(keyBytes); final DerParser.Asn1Object rootSeq = parser.readAsn1Object(DerParser.Type.SEQUENCE); parser = rootSeq.getParser(); final DerParser.Asn1Object algSeq = parser.readAsn1Object(DerParser.Type.SEQUENCE); parser = algSeq.getParser(); final String algId = parser.readAsn1Object(DerParser.Type.OBJECT_OID).getOid(); if (PBES2_OID.equals(algId)) { final DerParser.Asn1Object algData = parser.readAsn1Object(DerParser.Type.SEQUENCE); parser = algData.getParser(); final DerParser.Asn1Object ignoreKdf = parser.readAsn1Object(DerParser.Type.SEQUENCE); final DerParser.Asn1Object cryptSeq = parser.readAsn1Object(DerParser.Type.SEQUENCE); parser = cryptSeq.getParser(); final String encryptionId = parser.readAsn1Object(DerParser.Type.OBJECT_OID).getOid(); if (encryptionId.startsWith(AES_OID) == false) { final String name = getAlgorithmNameFromOid(encryptionId); throw new GeneralSecurityException( "PKCS#8 Private Key is encrypted with unsupported PBES2 algorithm [" + encryptionId + "]" + (name == null ? "" : " (" + name + ")"), e ); } if (JavaVersion.current().compareTo(JavaVersion.parse("11.0.0")) < 0) { // PBES2 appears to be supported on Oracle 8, but not OpenJDK8 // We don't both clarifying that here because it is supported on the bundled JDK, and that's what people should use throw new GeneralSecurityException( "PKCS#8 Private Key is encrypted with PBES2 which is not supported on this JDK [" + JavaVersion.current() + "]", e ); } } throw e; } }	maybe add the word version "on this jdk version [" . or a "consider upgrading to jdk11" or something similar ? we do still support 8 ( both oracle and {adopt}openjdk ) so "that's what people should use" reads a little too absolute?. i don't think it's worth trying to figure out if this is oracle 8 (even if that would be relatively simple) given the low frequency this comes up with, but maybe a helpful hint in the error message will help those who will encounter it.
public static List<String> buildCategorizeCommand(Environment env, Settings settings, Job job, Logger logger) { List<String> command = new ArrayList<>(); command.add(CATEGORIZE_PATH); String jobId = JOB_ID_ARG + job.getId(); command.add(jobId); AnalysisConfig analysisConfig = job.getAnalysisConfig(); if (analysisConfig != null) { addIfNotNull(analysisConfig.getCategorizationFieldName(), CATEGORIZATION_FIELD_ARG, command); } // Input is always length encoded command.add(LENGTH_ENCODED_INPUT_ARG); int intervalStagger = calculateStaggeringInterval(job.getId()); logger.debug("Periodic operations staggered by " + intervalStagger +" seconds for job '" + job.getId() + "'"); // Supply a URL for persisting/restoring model state unless model // persistence has been explicitly disabled. if (DONT_PERSIST_MODEL_STATE_SETTING.get(settings)) { logger.info("Will not persist model state - " + DONT_PERSIST_MODEL_STATE_SETTING + " setting was set"); } else { // Persist model state every few hours even if the job isn't closed long persistInterval = (job.getBackgroundPersistInterval() == null) ? (DEFAULT_BASE_PERSIST_INTERVAL + intervalStagger) : job.getBackgroundPersistInterval().getSeconds(); command.add(PERSIST_INTERVAL_ARG + persistInterval); } return command; }	url? out of date comment i think perhaps 'set the model state persist interval unless...'
private static <T> void setRandomIndicesOptions(Function<IndicesOptions, T> setter, Supplier<IndicesOptions> getter, Map<String, String> expectedParams) { if (randomBoolean()) { setter.apply(IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), randomBoolean(), randomBoolean())); } expectedParams.put("ignore_unavailable", Boolean.toString(getter.get().ignoreUnavailable())); expectedParams.put("allow_no_indices", Boolean.toString(getter.get().allowNoIndices())); if (getter.get().expandWildcardsOpen() && getter.get().expandWildcardsClosed()) { expectedParams.put("expand_wildcards", "open,closed"); } else if (getter.get().expandWildcardsOpen()) { expectedParams.put("expand_wildcards", "open"); } else if (getter.get().expandWildcardsClosed()) { expectedParams.put("expand_wildcards", "closed"); } else { expectedParams.put("expand_wildcards", "none"); } }	here and in setrandomindicesoptions, you could replace the function with a consumer. that works and makes the generics go away.
private static String parseLegacySnapshotUUID(XContentParser parser) throws IOException { String uuid = null; while (parser.nextToken() != XContentParser.Token.END_OBJECT) { String currentFieldName = parser.currentName(); parser.nextToken(); if (UUID.equals(currentFieldName)) { uuid = parser.text(); } } return uuid; } /** * A few details of an individual snapshot stored in the top-level index blob, so they are readily accessible without having to load * the corresponding {@link SnapshotInfo}	i actually think never at this point. this is only relevant for very old repository data (pre-5.4ish) if i'm not mistaken. we can probably remove this because we don't fully support reading all repository metadata from repositories that old at this point anyway.
public static <T> List<T> randomSubsetOf(int size, T... values) { List<T> originalList = newArrayList(values); if (size > originalList.size()) { throw new IllegalArgumentException("Can\\\\'t pick " + size + " random objects from a list of " + originalList.size() + " objects"); } List<T> list = newArrayList(); for(int i = 0; i < size; i++) { list.add(originalList.remove(randomInt(originalList.size() - 1))); } return list; }	can we just shufffle the list and then return sublist(0, size)?
static Map<String, IndexTemplateV2.DataStreamTemplate> resolveAutoCreateDataStreams(Metadata metadata, Set<String> autoCreateIndices) { Map<String, IndexTemplateV2.DataStreamTemplate> autoCreateDataStreams = new HashMap<>(); Iterator<String> autoCreateIndicesIterator = autoCreateIndices.iterator(); while (autoCreateIndicesIterator.hasNext()) { String indexName = autoCreateIndicesIterator.next(); String v2Template = MetadataIndexTemplateService.findV2Template(metadata, indexName, false); if (v2Template != null) { IndexTemplateV2 indexTemplateV2 = metadata.templatesV2().get(v2Template); if (indexTemplateV2.getDataStreamTemplate() != null) { autoCreateIndicesIterator.remove(); autoCreateDataStreams.put(indexName, indexTemplateV2.getDataStreamTemplate()); } } } return autoCreateDataStreams; }	i think this means that if you have both v1 and v2 templates matching the same pattern and the v2 one is a data stream, it takes precedence even when prefer_v2_templates=false. i think this will cause some confusion and i would prefer to stick to respecting the option.
void autoCreateDataStreams(final Task task, final BulkRequest bulkRequest, final long startTimeNanos, final ActionListener<BulkResponse> listener, final AtomicArray<BulkItemResponse> responses, final Map<String, IndexTemplateV2.DataStreamTemplate> autoCreateDataStreams, final Map<String, IndexNotFoundException> indicesThatCannotBeCreated) { final AtomicInteger counter = new AtomicInteger(autoCreateDataStreams.size()); for (Map.Entry<String, IndexTemplateV2.DataStreamTemplate> entry : autoCreateDataStreams.entrySet()) { final String name = entry.getKey(); CreateDataStreamAction.Request request = new CreateDataStreamAction.Request(name); request.setTimestampFieldName(entry.getValue().getTimestampField()); CheckedConsumer<AcknowledgedResponse, ? extends Exception> onResponse = response -> { if (counter.decrementAndGet() == 0) { threadPool.executor(ThreadPool.Names.WRITE).execute( () -> executeBulk(task, bulkRequest, startTimeNanos, listener, responses, indicesThatCannotBeCreated)); } }; Consumer<Exception> onFailure = e -> { if (!(ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException)) { // fail all requests involving this index, if create didn't work for (int i = 0; i < bulkRequest.requests.size(); i++) { DocWriteRequest<?> item = bulkRequest.requests.get(i); if (item != null && setResponseFailureIfIndexMatches(responses, i, item, name, e)) { bulkRequest.requests.set(i, null); } } } if (counter.decrementAndGet() == 0) { executeBulk(task, bulkRequest, startTimeNanos, ActionListener.wrap(listener::onResponse, inner -> { inner.addSuppressed(e); listener.onFailure(inner); }), responses, indicesThatCannotBeCreated); } }; client.admin().indices().createDataStream(request, ActionListener.wrap(onResponse, onFailure)); } }	duplicates code in the onfailure handler in the doexecute method. might be worth consolidating.
public void testResolveAutoCreateDataStreams() { Metadata metadata; { Metadata.Builder mdBuilder = new Metadata.Builder(); DataStreamTemplate dataStreamTemplate = new DataStreamTemplate("@timestamp"); mdBuilder.put("1", new IndexTemplateV2(List.of("legacy-logs-*"), null, null, 10L, null, null, null)); mdBuilder.put("2", new IndexTemplateV2(List.of("logs-*"), null, null, 20L, null, null, dataStreamTemplate)); mdBuilder.put("3", new IndexTemplateV2(List.of("logs-foobar"), null, null, 30L, null, null, dataStreamTemplate)); metadata = mdBuilder.build(); } Map<String, DataStreamTemplate> result = TransportBulkAction.resolveAutoCreateDataStreams(metadata, Set.of()); assertThat(result, anEmptyMap()); Set<String> autoCreateIndices = new HashSet<>(Set.of("logs-foobar", "logs-barbaz")); result = TransportBulkAction.resolveAutoCreateDataStreams(metadata, autoCreateIndices); assertThat(autoCreateIndices, empty()); assertThat(result, aMapWithSize(2)); assertThat(result, hasKey("logs-foobar")); assertThat(result, hasKey("logs-barbaz")); // An index that matches with a template without a data steam definition autoCreateIndices = new HashSet<>(Set.of("logs-foobar", "logs-barbaz", "legacy-logs-foobaz")); result = TransportBulkAction.resolveAutoCreateDataStreams(metadata, autoCreateIndices); assertThat(autoCreateIndices, hasSize(1)); assertThat(autoCreateIndices, hasItem("legacy-logs-foobaz")); assertThat(result, aMapWithSize(2)); assertThat(result, hasKey("logs-foobar")); assertThat(result, hasKey("logs-barbaz")); // An index that doesn't match with an index template autoCreateIndices = new HashSet<>(Set.of("logs-foobar", "logs-barbaz", "my-index")); result = TransportBulkAction.resolveAutoCreateDataStreams(metadata, autoCreateIndices); assertThat(autoCreateIndices, hasSize(1)); assertThat(autoCreateIndices, hasItem("my-index")); assertThat(result, aMapWithSize(2)); assertThat(result, hasKey("logs-foobar")); assertThat(result, hasKey("logs-barbaz")); }	would this variable be better named autocreatedatastreams?
public static Tuple<XContentType, Map<String, Object>> convertToMap(BytesReference bytes, boolean ordered, XContentType xContentType) throws ElasticsearchParseException { try { final XContentType contentType; InputStream input; Compressor compressor = CompressorFactory.compressor(bytes); if (compressor != null) { InputStream compressedStreamInput = compressor.streamInput(bytes.streamInput()); if (compressedStreamInput.markSupported() == false) { compressedStreamInput = new BufferedInputStream(compressedStreamInput); } input = compressedStreamInput; } else { input = bytes.streamInput(); } contentType = xContentType != null ? xContentType : XContentFactory.xContentType(input); try { return new Tuple<>(Objects.requireNonNull(contentType), convertToMap(XContentFactory.xContent(contentType), input, ordered)); } finally { input.close(); } } catch (IOException e) { throw new ElasticsearchParseException("Failed to parse content to map", e); } } /** * Convert a string in some {@link XContent} format to a {@link Map}. Throws an {@link ElasticsearchParseException}	why does try/with not work here?
@Override public void newDynamicBooleanField(ParseContext context, String name) throws IOException { createDynamicField(new BooleanFieldMapper.Builder(name, null), context); }	i wonder if instead of null we should have some impl of scriptcompiler that throws unsupportedoperationexception when used.
public final void runForDoc(int docId, Consumer<Boolean> consumer) { runForDoc(docId); int count = trues + falses; for (int i = 0; i < count; i++) { consumer.accept(i >= falses); } } /** * How many {@code true}	i think we need to emit true trues times and then false falses times here? otherwise we're collapsing things down to a single value where there may be multiple values in the source.
* @throws IOException in case there is a problem sending the request or parsing back the response */ public final BulkByScrollResponse reindex(ReindexRequest reindexRequest, RequestOptions options) throws IOException { return performRequestAndParseEntity( reindexRequest, RequestConverters::reindex, options, BulkByScrollResponse::fromXContent, emptySet() ); } /** * Submits a reindex task. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html">Reindex API on elastic.co</a> * @param reindexRequest the request * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	i wonder if there's any value to returning a reindexsubmissionresponse rather than just a taskid here?
public Settings archiveUnknownOrInvalidSettings( final Settings settings, final Consumer<Map.Entry<String, String>> unknownConsumer, final BiConsumer<Map.Entry<String, String>, IllegalArgumentException> invalidConsumer) { Settings.Builder builder = Settings.builder(); boolean changed = false; for (Map.Entry<String, String> entry : settings.getAsMap().entrySet()) { try { Setting<?> setting = get(entry.getKey()); if (setting != null) { try { setting.get(settings); } catch (IllegalArgumentException ex) { if (setting.isMandatory()) { throw new IllegalStateException("can't archive mandatory setting [" + setting.getKey() + "]", ex); } throw ex; } builder.put(entry.getKey(), entry.getValue()); } else { if (entry.getKey().startsWith(ARCHIVED_SETTINGS_PREFIX) || isPrivateSetting(entry.getKey())) { builder.put(entry.getKey(), entry.getValue()); } else { changed = true; unknownConsumer.accept(entry); /* * We put them back in here such that tools can check from the outside if there are any indices with invalid * settings. The setting can remain there but we want users to be aware that some of their setting are invalid and * they can research why and what they need to do to replace them. */ builder.put(ARCHIVED_SETTINGS_PREFIX + entry.getKey(), entry.getValue()); } } } catch (IllegalArgumentException ex) { changed = true; invalidConsumer.accept(entry, ex); /* * We put them back in here such that tools can check from the outside if there are any indices with invalid settings. The * setting can remain there but we want users to be aware that some of their setting are invalid and they can research why * and what they need to do to replace them. */ builder.put(ARCHIVED_SETTINGS_PREFIX + entry.getKey(), entry.getValue()); } } if (changed) { return builder.build(); } else { return settings; } }	nit: i think we need to renamed the method + docs - it now also rejects mandatory and invalid settings.
public Settings archiveUnknownOrInvalidSettings( final Settings settings, final Consumer<Map.Entry<String, String>> unknownConsumer, final BiConsumer<Map.Entry<String, String>, IllegalArgumentException> invalidConsumer) { Settings.Builder builder = Settings.builder(); boolean changed = false; for (Map.Entry<String, String> entry : settings.getAsMap().entrySet()) { try { Setting<?> setting = get(entry.getKey()); if (setting != null) { try { setting.get(settings); } catch (IllegalArgumentException ex) { if (setting.isMandatory()) { throw new IllegalStateException("can't archive mandatory setting [" + setting.getKey() + "]", ex); } throw ex; } builder.put(entry.getKey(), entry.getValue()); } else { if (entry.getKey().startsWith(ARCHIVED_SETTINGS_PREFIX) || isPrivateSetting(entry.getKey())) { builder.put(entry.getKey(), entry.getValue()); } else { changed = true; unknownConsumer.accept(entry); /* * We put them back in here such that tools can check from the outside if there are any indices with invalid * settings. The setting can remain there but we want users to be aware that some of their setting are invalid and * they can research why and what they need to do to replace them. */ builder.put(ARCHIVED_SETTINGS_PREFIX + entry.getKey(), entry.getValue()); } } } catch (IllegalArgumentException ex) { changed = true; invalidConsumer.accept(entry, ex); /* * We put them back in here such that tools can check from the outside if there are any indices with invalid settings. The * setting can remain there but we want users to be aware that some of their setting are invalid and they can research why * and what they need to do to replace them. */ builder.put(ARCHIVED_SETTINGS_PREFIX + entry.getKey(), entry.getValue()); } } if (changed) { return builder.build(); } else { return settings; } }	nit: how about: "setting [..] is invalid and mandatory"?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); type = in.readString(); id = in.readString(); routing = in.readOptionalString(); if (in.getVersion().before(Version.V_7_0_0)) { in.readOptionalString(); // _parent } version = in.readLong(); versionType = VersionType.fromValue(in.readByte()); if (in.getVersion().onOrAfter(Version.V_7_0_0)) { ifSeqNoMatch = in.readZLong(); ifPrimaryTermMatch = in.readVLong(); } else { ifSeqNoMatch = SequenceNumbers.UNASSIGNED_SEQ_NO; ifPrimaryTermMatch = 0; } }	can it be negative at this point? if not can we assert?
* The id is optional, if it is not provided, one will be generated automatically. */ default IndexRequestBuilder prepareIndex() { return new IndexRequestBuilder(this, IndexAction.INSTANCE, null); }	we've discussed this previously in #18468 and do not agree that these should be default methods on client.
public void createApiKey(Authentication authentication, CreateApiKeyRequest request, ActionListener<CreateApiKeyResponse> listener) { ensureEnabled(); if (authentication == null) { listener.onFailure(new IllegalArgumentException("authentication must be provided")); } else { final Instant created = clock.instant(); final Instant expiration = getApiKeyExpiration(created, request); final SecureString apiKey = UUIDs.randomBase64UUIDSecureString(); final Version version = clusterService.state().nodes().getMinNodeVersion(); if (version.before(Version.V_7_0_0)) { // TODO(jaymode) change to V6_6_0 on backport! logger.warn("nodes prior to the minimum supported version for api keys {} exist in the cluster; these nodes will not be " + "able to use api keys", Version.V_7_0_0); } final char[] keyHash = hasher.hash(apiKey); try (XContentBuilder builder = XContentFactory.jsonBuilder()) { builder.startObject() .field("doc_type", "api_key") .field("creation_time", created.toEpochMilli()) .field("expiration_time", expiration == null ? null : expiration.toEpochMilli()) .field("api_key_invalidated", false); byte[] utf8Bytes = null; try { utf8Bytes = CharArrays.toUtf8Bytes(keyHash); builder.field("api_key_hash").utf8Value(utf8Bytes, 0, utf8Bytes.length); } finally { if (utf8Bytes != null) { Arrays.fill(utf8Bytes, (byte) 0); } } builder.startObject("role_descriptors"); for (RoleDescriptor descriptor : request.getRoleDescriptors()) { builder.field(descriptor.getName(), (contentBuilder, params) -> descriptor.toXContent(contentBuilder, params, true)); } builder.endObject(); builder.field("name", request.getName()) .field("version", version.id) .startObject("creator") .field("principal", authentication.getUser().principal()) .field("metadata", authentication.getUser().metadata()) .field("realm", authentication.getLookedUpBy() == null ? authentication.getAuthenticatedBy().getName() : authentication.getLookedUpBy().getName()) .endObject() .endObject(); final IndexRequest indexRequest = client.prepareIndex(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE) .setSource(builder) .setRefreshPolicy(request.getRefreshPolicy()) .request(); securityIndex.prepareIndexIfNeededThenExecute(listener::onFailure, () -> executeAsyncWithOrigin(client, SECURITY_ORIGIN, IndexAction.INSTANCE, indexRequest, ActionListener.wrap(indexResponse -> listener.onResponse(new CreateApiKeyResponse(request.getName(), indexResponse.getId(), apiKey, expiration)), listener::onFailure))); } catch (IOException e) { listener.onFailure(e); } finally { Arrays.fill(keyHash, (char) 0); } } } /** * Checks for the presence of a {@code Authorization} header with a value that starts with * {@code ApiKey }	maybe invalidated since we already know it's the api_key this would refer to?
public void invalidateApiKeysForRealmAndUser(String realmName, String userName, ActionListener<InvalidateApiKeyResponse> invalidateListener) { ensureEnabled(); if (Strings.hasText(realmName) == false && Strings.hasText(userName) == false) { logger.trace("No realm name or username provided"); invalidateListener.onFailure(new IllegalArgumentException("realm name or username must be provided")); } else { findActiveApiKeysForUserAndRealm(userName, realmName, ActionListener.wrap(apiKeyIds -> { if (apiKeyIds.isEmpty()) { logger.warn("No api keys to invalidate for realm [{}] and username [{}]", realmName, userName); invalidateListener.onResponse(InvalidateApiKeyResponse.emptyResponse()); } else { invalidateAllApiKeys(apiKeyIds, invalidateListener); } }, invalidateListener::onFailure)); } }	there can only be one api key for a given id, right?
public static boolean isAllOrWildcard(String[] data) { return CollectionUtils.isEmpty(data) || data.length == 1 && ("_all".equals(data[0]) || "*".equals(data[0])); }	please add a comment and unit test.
@Override AzureStorageService createAzureStoreService(final Settings settings) { return new AzureStorageService(settings) { @Override RetryPolicyFactory createRetryPolicy(final AzureStorageSettings azureStorageSettings) { return new RetryExponentialRetry(1, 100, 500, azureStorageSettings.getMaxRetries()); } @Override BlobRequestOptions getBlobRequestOptionsForWriteBlob() { BlobRequestOptions options = new BlobRequestOptions(); options.setSingleBlobPutThresholdInBytes(Math.toIntExact(ByteSizeUnit.MB.toBytes(1))); return options; } }; } } /** * Minimal HTTP handler that acts as an Azure compliant server */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an Azure endpoint") private static class InternalHttpHandler implements HttpHandler { private final Map<String, BytesReference> blobs = new ConcurrentHashMap<>(); @Override public void handle(final HttpExchange exchange) throws IOException { final String request = exchange.getRequestMethod() + " " + exchange.getRequestURI().toString(); try { if (Regex.simpleMatch("PUT /container/*blockid=*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); final String blockId = params.get("blockid"); blobs.put(blockId, Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("PUT /container/*comp=blocklist*", request)) { final String blockList = Streams.copyToString(new InputStreamReader(exchange.getRequestBody(), StandardCharsets.UTF_8)); final List<String> blockIds = Arrays.stream(blockList.split("<Latest>")) .filter(line -> line.contains("</Latest>")) .map(line -> line.substring(0, line.indexOf("</Latest>"))) .collect(Collectors.toList()); final ByteArrayOutputStream blob = new ByteArrayOutputStream(); for (String blockId : blockIds) { BytesReference block = blobs.remove(blockId); assert block != null; block.writeTo(blob); } blobs.put(exchange.getRequestURI().getPath(), new BytesArray(blob.toByteArray())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("PUT /container/*", request)) { blobs.put(exchange.getRequestURI().getPath(), Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("HEAD /container/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().getPath()); if (blob == null) { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); return; } exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(blob.length())); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1); } else if (Regex.simpleMatch("GET /container/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().getPath()); if (blob == null) { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); return; } final String range = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.STORAGE_RANGE_HEADER); final Matcher matcher = Pattern.compile("^bytes=([0-9]+)-([0-9]+)$").matcher(range); assert matcher.matches(); final int start = Integer.parseInt(matcher.group(1)); final int length = Integer.parseInt(matcher.group(2)) - start + 1; exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(length)); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), length); exchange.getResponseBody().write(blob.toBytesRef().bytes, start, length); } else if (Regex.simpleMatch("DELETE /container/*", request)) { Streams.readFully(exchange.getRequestBody()); blobs.entrySet().removeIf(blob -> blob.getKey().startsWith(exchange.getRequestURI().getPath())); exchange.sendResponseHeaders(RestStatus.ACCEPTED.getStatus(), -1); } else if (Regex.simpleMatch("GET /container?restype=container&comp=list*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); final StringBuilder list = new StringBuilder(); list.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); list.append("<EnumerationResults>"); final String prefix = params.get("prefix"); list.append("<Blobs>"); for (Map.Entry<String, BytesReference> blob : blobs.entrySet()) { if (prefix == null || blob.getKey().startsWith("/container/" + prefix)) { list.append("<Blob><Name>").append(blob.getKey().replace("/container/", "")).append("</Name>"); list.append("<Properties><Content-Length>").append(blob.getValue().length()).append("</Content-Length>"); list.append("<BlobType>BlockBlob</BlobType></Properties></Blob>"); } } list.append("</Blobs>"); list.append("</EnumerationResults>"); byte[] response = list.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else { exchange.sendResponseHeaders(RestStatus.BAD_REQUEST.getStatus(), -1); } } finally { exchange.close(); } } } /** * HTTP handler that injects random Azure service errors * * Note: it is not a good idea to allow this handler to simulate too many errors as it would * slow down the test suite. */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an Azure endpoint") private static class AzureErroneousHttpHandler extends ErroneousHttpHandler { AzureErroneousHttpHandler(final HttpHandler delegate, final int maxErrorsPerRequest) { super(delegate, maxErrorsPerRequest); } @Override protected String requestUniqueId(final HttpExchange exchange) { final String requestId = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.CLIENT_REQUEST_ID_HEADER); final String range = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.STORAGE_RANGE_HEADER); return exchange.getRequestMethod() + " " + requestId + (range != null ? " " + range : ""); }	this allows to lower the threshold between single put blob request and put blocks + put blocks list requests.
public void handle(final HttpExchange exchange) throws IOException { final String request = exchange.getRequestMethod() + " " + exchange.getRequestURI().toString(); try { if (Regex.simpleMatch("PUT /container/*blockid=*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); final String blockId = params.get("blockid"); blobs.put(blockId, Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("PUT /container/*comp=blocklist*", request)) { final String blockList = Streams.copyToString(new InputStreamReader(exchange.getRequestBody(), StandardCharsets.UTF_8)); final List<String> blockIds = Arrays.stream(blockList.split("<Latest>")) .filter(line -> line.contains("</Latest>")) .map(line -> line.substring(0, line.indexOf("</Latest>"))) .collect(Collectors.toList()); final ByteArrayOutputStream blob = new ByteArrayOutputStream(); for (String blockId : blockIds) { BytesReference block = blobs.remove(blockId); assert block != null; block.writeTo(blob); } blobs.put(exchange.getRequestURI().getPath(), new BytesArray(blob.toByteArray())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("PUT /container/*", request)) { blobs.put(exchange.getRequestURI().getPath(), Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("HEAD /container/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().getPath()); if (blob == null) { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); return; } exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(blob.length())); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1); } else if (Regex.simpleMatch("GET /container/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().getPath()); if (blob == null) { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); return; } final String range = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.STORAGE_RANGE_HEADER); final Matcher matcher = Pattern.compile("^bytes=([0-9]+)-([0-9]+)$").matcher(range); assert matcher.matches(); final int start = Integer.parseInt(matcher.group(1)); final int length = Integer.parseInt(matcher.group(2)) - start + 1; exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(length)); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), length); exchange.getResponseBody().write(blob.toBytesRef().bytes, start, length); } else if (Regex.simpleMatch("DELETE /container/*", request)) { Streams.readFully(exchange.getRequestBody()); blobs.entrySet().removeIf(blob -> blob.getKey().startsWith(exchange.getRequestURI().getPath())); exchange.sendResponseHeaders(RestStatus.ACCEPTED.getStatus(), -1); } else if (Regex.simpleMatch("GET /container?restype=container&comp=list*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); final StringBuilder list = new StringBuilder(); list.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); list.append("<EnumerationResults>"); final String prefix = params.get("prefix"); list.append("<Blobs>"); for (Map.Entry<String, BytesReference> blob : blobs.entrySet()) { if (prefix == null || blob.getKey().startsWith("/container/" + prefix)) { list.append("<Blob><Name>").append(blob.getKey().replace("/container/", "")).append("</Name>"); list.append("<Properties><Content-Length>").append(blob.getValue().length()).append("</Content-Length>"); list.append("<BlobType>BlockBlob</BlobType></Properties></Blob>"); } } list.append("</Blobs>"); list.append("</EnumerationResults>"); byte[] response = list.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else { exchange.sendResponseHeaders(RestStatus.BAD_REQUEST.getStatus(), -1); } } finally { exchange.close(); } } } /** * HTTP handler that injects random Azure service errors * * Note: it is not a good idea to allow this handler to simulate too many errors as it would * slow down the test suite. */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an Azure endpoint") private static class AzureErroneousHttpHandler extends ErroneousHttpHandler { AzureErroneousHttpHandler(final HttpHandler delegate, final int maxErrorsPerRequest) { super(delegate, maxErrorsPerRequest); } @Override protected String requestUniqueId(final HttpExchange exchange) { final String requestId = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.CLIENT_REQUEST_ID_HEADER); final String range = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.STORAGE_RANGE_HEADER); return exchange.getRequestMethod() + " " + requestId + (range != null ? " " + range : ""); }	this was wrong (ie, query parameters were part of the blob name)
@Override AzureStorageService createAzureStoreService(final Settings settings) { return new AzureStorageService(settings) { @Override RetryPolicyFactory createRetryPolicy(final AzureStorageSettings azureStorageSettings) { return new RetryExponentialRetry(1, 100, 500, azureStorageSettings.getMaxRetries()); } @Override BlobRequestOptions getBlobRequestOptionsForWriteBlob() { BlobRequestOptions options = new BlobRequestOptions(); options.setSingleBlobPutThresholdInBytes(Math.toIntExact(ByteSizeUnit.MB.toBytes(1))); return options; } }; } } /** * Minimal HTTP handler that acts as an Azure compliant server */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an Azure endpoint") private static class InternalHttpHandler implements HttpHandler { private final Map<String, BytesReference> blobs = new ConcurrentHashMap<>(); @Override public void handle(final HttpExchange exchange) throws IOException { final String request = exchange.getRequestMethod() + " " + exchange.getRequestURI().toString(); try { if (Regex.simpleMatch("PUT /container/*blockid=*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); final String blockId = params.get("blockid"); blobs.put(blockId, Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("PUT /container/*comp=blocklist*", request)) { final String blockList = Streams.copyToString(new InputStreamReader(exchange.getRequestBody(), StandardCharsets.UTF_8)); final List<String> blockIds = Arrays.stream(blockList.split("<Latest>")) .filter(line -> line.contains("</Latest>")) .map(line -> line.substring(0, line.indexOf("</Latest>"))) .collect(Collectors.toList()); final ByteArrayOutputStream blob = new ByteArrayOutputStream(); for (String blockId : blockIds) { BytesReference block = blobs.remove(blockId); assert block != null; block.writeTo(blob); } blobs.put(exchange.getRequestURI().getPath(), new BytesArray(blob.toByteArray())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("PUT /container/*", request)) { blobs.put(exchange.getRequestURI().getPath(), Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("HEAD /container/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().getPath()); if (blob == null) { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); return; } exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(blob.length())); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1); } else if (Regex.simpleMatch("GET /container/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().getPath()); if (blob == null) { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); return; } final String range = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.STORAGE_RANGE_HEADER); final Matcher matcher = Pattern.compile("^bytes=([0-9]+)-([0-9]+)$").matcher(range); assert matcher.matches(); final int start = Integer.parseInt(matcher.group(1)); final int length = Integer.parseInt(matcher.group(2)) - start + 1; exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(length)); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), length); exchange.getResponseBody().write(blob.toBytesRef().bytes, start, length); } else if (Regex.simpleMatch("DELETE /container/*", request)) { Streams.readFully(exchange.getRequestBody()); blobs.entrySet().removeIf(blob -> blob.getKey().startsWith(exchange.getRequestURI().getPath())); exchange.sendResponseHeaders(RestStatus.ACCEPTED.getStatus(), -1); } else if (Regex.simpleMatch("GET /container?restype=container&comp=list*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); final StringBuilder list = new StringBuilder(); list.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); list.append("<EnumerationResults>"); final String prefix = params.get("prefix"); list.append("<Blobs>"); for (Map.Entry<String, BytesReference> blob : blobs.entrySet()) { if (prefix == null || blob.getKey().startsWith("/container/" + prefix)) { list.append("<Blob><Name>").append(blob.getKey().replace("/container/", "")).append("</Name>"); list.append("<Properties><Content-Length>").append(blob.getValue().length()).append("</Content-Length>"); list.append("<BlobType>BlockBlob</BlobType></Properties></Blob>"); } } list.append("</Blobs>"); list.append("</EnumerationResults>"); byte[] response = list.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else { exchange.sendResponseHeaders(RestStatus.BAD_REQUEST.getStatus(), -1); } } finally { exchange.close(); } } } /** * HTTP handler that injects random Azure service errors * * Note: it is not a good idea to allow this handler to simulate too many errors as it would * slow down the test suite. */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an Azure endpoint") private static class AzureErroneousHttpHandler extends ErroneousHttpHandler { AzureErroneousHttpHandler(final HttpHandler delegate, final int maxErrorsPerRequest) { super(delegate, maxErrorsPerRequest); } @Override protected String requestUniqueId(final HttpExchange exchange) { final String requestId = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.CLIENT_REQUEST_ID_HEADER); final String range = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.STORAGE_RANGE_HEADER); return exchange.getRequestMethod() + " " + requestId + (range != null ? " " + range : ""); }	nit: suggestion asserttrue(matcher.matches()); ? it's weird to run into a normal assert here, especially when it has side-effects.
@Override AzureStorageService createAzureStoreService(final Settings settings) { return new AzureStorageService(settings) { @Override RetryPolicyFactory createRetryPolicy(final AzureStorageSettings azureStorageSettings) { return new RetryExponentialRetry(1, 100, 500, azureStorageSettings.getMaxRetries()); } @Override BlobRequestOptions getBlobRequestOptionsForWriteBlob() { BlobRequestOptions options = new BlobRequestOptions(); options.setSingleBlobPutThresholdInBytes(Math.toIntExact(ByteSizeUnit.MB.toBytes(1))); return options; } }; } } /** * Minimal HTTP handler that acts as an Azure compliant server */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an Azure endpoint") private static class InternalHttpHandler implements HttpHandler { private final Map<String, BytesReference> blobs = new ConcurrentHashMap<>(); @Override public void handle(final HttpExchange exchange) throws IOException { final String request = exchange.getRequestMethod() + " " + exchange.getRequestURI().toString(); try { if (Regex.simpleMatch("PUT /container/*blockid=*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); final String blockId = params.get("blockid"); blobs.put(blockId, Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("PUT /container/*comp=blocklist*", request)) { final String blockList = Streams.copyToString(new InputStreamReader(exchange.getRequestBody(), StandardCharsets.UTF_8)); final List<String> blockIds = Arrays.stream(blockList.split("<Latest>")) .filter(line -> line.contains("</Latest>")) .map(line -> line.substring(0, line.indexOf("</Latest>"))) .collect(Collectors.toList()); final ByteArrayOutputStream blob = new ByteArrayOutputStream(); for (String blockId : blockIds) { BytesReference block = blobs.remove(blockId); assert block != null; block.writeTo(blob); } blobs.put(exchange.getRequestURI().getPath(), new BytesArray(blob.toByteArray())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("PUT /container/*", request)) { blobs.put(exchange.getRequestURI().getPath(), Streams.readFully(exchange.getRequestBody())); exchange.sendResponseHeaders(RestStatus.CREATED.getStatus(), -1); } else if (Regex.simpleMatch("HEAD /container/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().getPath()); if (blob == null) { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); return; } exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(blob.length())); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1); } else if (Regex.simpleMatch("GET /container/*", request)) { final BytesReference blob = blobs.get(exchange.getRequestURI().getPath()); if (blob == null) { exchange.sendResponseHeaders(RestStatus.NOT_FOUND.getStatus(), -1); return; } final String range = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.STORAGE_RANGE_HEADER); final Matcher matcher = Pattern.compile("^bytes=([0-9]+)-([0-9]+)$").matcher(range); assert matcher.matches(); final int start = Integer.parseInt(matcher.group(1)); final int length = Integer.parseInt(matcher.group(2)) - start + 1; exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(length)); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), length); exchange.getResponseBody().write(blob.toBytesRef().bytes, start, length); } else if (Regex.simpleMatch("DELETE /container/*", request)) { Streams.readFully(exchange.getRequestBody()); blobs.entrySet().removeIf(blob -> blob.getKey().startsWith(exchange.getRequestURI().getPath())); exchange.sendResponseHeaders(RestStatus.ACCEPTED.getStatus(), -1); } else if (Regex.simpleMatch("GET /container?restype=container&comp=list*", request)) { final Map<String, String> params = new HashMap<>(); RestUtils.decodeQueryString(exchange.getRequestURI().getQuery(), 0, params); final StringBuilder list = new StringBuilder(); list.append("<?xml version=\\\\"1.0\\\\" encoding=\\\\"UTF-8\\\\"?>"); list.append("<EnumerationResults>"); final String prefix = params.get("prefix"); list.append("<Blobs>"); for (Map.Entry<String, BytesReference> blob : blobs.entrySet()) { if (prefix == null || blob.getKey().startsWith("/container/" + prefix)) { list.append("<Blob><Name>").append(blob.getKey().replace("/container/", "")).append("</Name>"); list.append("<Properties><Content-Length>").append(blob.getValue().length()).append("</Content-Length>"); list.append("<BlobType>BlockBlob</BlobType></Properties></Blob>"); } } list.append("</Blobs>"); list.append("</EnumerationResults>"); byte[] response = list.toString().getBytes(StandardCharsets.UTF_8); exchange.getResponseHeaders().add("Content-Type", "application/xml"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), response.length); exchange.getResponseBody().write(response); } else { exchange.sendResponseHeaders(RestStatus.BAD_REQUEST.getStatus(), -1); } } finally { exchange.close(); } } } /** * HTTP handler that injects random Azure service errors * * Note: it is not a good idea to allow this handler to simulate too many errors as it would * slow down the test suite. */ @SuppressForbidden(reason = "this test uses a HttpServer to emulate an Azure endpoint") private static class AzureErroneousHttpHandler extends ErroneousHttpHandler { AzureErroneousHttpHandler(final HttpHandler delegate, final int maxErrorsPerRequest) { super(delegate, maxErrorsPerRequest); } @Override protected String requestUniqueId(final HttpExchange exchange) { final String requestId = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.CLIENT_REQUEST_ID_HEADER); final String range = exchange.getRequestHeaders().getFirst(Constants.HeaderConstants.STORAGE_RANGE_HEADER); return exchange.getRequestMethod() + " " + requestId + (range != null ? " " + range : ""); }	i did not notice it but head and get blob share the same request id. when reading a blob from azure using input stream, the sdk first execute a head request to know the size of the object to read and then retrieve the content using 1 or more requests, all with the same request id.
private void sync(final Consumer<IndexShard> sync, final String source) { for (final IndexShard shard : this.shards.values()) { if (shard.routingEntry().active() && shard.routingEntry().primary()) { switch (shard.state()) { case CLOSED: case CREATED: case RECOVERING: continue; case POST_RECOVERY: assert false : "shard " + shard.shardId() + " is in post-recovery but marked as active"; continue; case STARTED: try { shard.runUnderPrimaryPermit( () -> { if (shard.isRelocatedPrimary() == false) { sync.accept(shard); } }, e -> { if (e instanceof AlreadyClosedException == false && e instanceof IndexShardClosedException == false) { logger.warn( new ParameterizedMessage( "{} failed to execute {} sync", shard.shardId(), source), e); } }, ThreadPool.Names.SAME, source + " sync"); } catch (final AlreadyClosedException | IndexShardClosedException e) { // the shard was closed concurrently, continue } continue; default: throw new IllegalStateException("unknown state [" + shard.state() + "]"); } } } }	i think this could potentially still fail if the shardrouting is updated between checking in line 821 and asserting inside rununderprimarypermit. at least i cannot find the protection against that. if others agree, this could be something for a follow-up.
private static Version getIndexVersionFromIndex(String index, MetaData metaData) { if (metaData.getIndices().containsKey(index)) { return metaData.getIndices().get(index).settings().getAsVersion(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT); } return Version.CURRENT; }	why not use version.indexecreated? how is it possible to be looking up created version for an index that doesn't exist?
public void setTimeUnit(TimeUnit timeUnit) { checkIfFrozen(); this.timeUnit = timeUnit; this.dateMathParser = new DateMathParser(dateTimeFormatter); }	i've tried keeping this kind of 2x stuff out of the field types. instead, could we have a 2x formatter that wraps the other formatter, and does this fallback? then we can only add the wrapper when version < 2x (which we have when building).
@Override public String toString() { return LoggerMessageFormat.format(null, "{{}={}}", prefix(), value); }	why use the null prefix -and not use alternative method?
@Override protected void doExecute(Task task, NodesReloadSecureSettingsRequest request, ActionListener<NodesReloadSecureSettingsResponse> listener) { if (request.hasPassword() && isNodeLocal(request) == false && isNodeTransportTLSEnabled() == false) { listener.onFailure( new IllegalStateException("Secure settings cannot be updated cluster wide when TLS for the transport layer" + " is not enabled. Enable TLS or use the API with a `_local` filter on each node.")); } else { super.doExecute(task, request, listener); } }	i tend to useillegalstateexception as a form of assert in production code. in this case i would be using a general elasticsearchexception (i could not find a more specialized one - this case is very particular, it is not merely a request validation)
@Override public RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException { final String[] nodesIds = Strings.splitStringByCommaToArray(request.param("nodeId")); final NodesReloadSecureSettingsRequestBuilder nodesRequestBuilder = client.admin() .cluster() .prepareReloadSecureSettings() .setTimeout(request.param("timeout")) .setNodesIds(nodesIds); if (request.hasContent()) { nodesRequestBuilder.source(request.content(), request.getXContentType()); } final NodesReloadSecureSettingsRequest nodesRequest = nodesRequestBuilder.request(); return channel -> nodesRequestBuilder .execute(new RestBuilderListener<NodesReloadSecureSettingsResponse>(channel) { @Override public RestResponse buildResponse(NodesReloadSecureSettingsResponse response, XContentBuilder builder) throws Exception { builder.startObject(); RestActions.buildNodesHeader(builder, channel.request(), response); builder.field("cluster_name", response.getClusterName().value()); response.toXContent(builder, channel.request()); builder.endObject(); // clear password for the original request nodesRequest.closePassword(); return new BytesRestResponse(RestStatus.OK, builder); } }); }	make sure you clear the password in the listener's failure handle too.
public void usedDeprecatedName(String usedName, String modernName) { LoggingDeprecationHandler.INSTANCE.usedDeprecatedName(usedName, modernName); deprecations.add(LoggerMessageFormat.format("Deprecated field [{}] used, expected [{}] instead", (Object)usedName, modernName)); }	this confused me until i realised it was the var args argument. is new object[] {usedname, modernname} any clearer or confusing in a different way
public void testTreeClassificationProbability() { // Build a tree with 2 nodes and 3 leaves using 2 features // The leaves have unique values 0.1, 0.2, 0.3 Tree.Builder builder = Tree.builder().setTargetType(TargetType.CLASSIFICATION); TreeNode.Builder rootNode = builder.addJunction(0, 0, true, 0.5); builder.addLeaf(rootNode.getRightChild(), 1.0); TreeNode.Builder leftChildNode = builder.addJunction(rootNode.getLeftChild(), 1, true, 0.8); builder.addLeaf(leftChildNode.getLeftChild(), 1.0); builder.addLeaf(leftChildNode.getRightChild(), 0.0); List<String> featureNames = Arrays.asList("foo", "bar"); Tree tree = builder.setFeatureNames(featureNames).build(); // This feature vector should hit the right child of the root node List<Double> featureVector = Arrays.asList(0.6, 0.0); Map<String, Object> featureMap = zipObjMap(featureNames, featureVector); assertEquals(Arrays.asList(0.0, 1.0), tree.classificationProbability(featureMap)); // This should hit the left child of the left child of the root node // i.e. it takes the path left, left featureVector = Arrays.asList(0.3, 0.7); featureMap = zipObjMap(featureNames, featureVector); assertEquals(Arrays.asList(0.0, 1.0), tree.classificationProbability(featureMap)); // This should hit the right child of the left child of the root node // i.e. it takes the path left, right featureVector = Arrays.asList(0.3, 0.9); featureMap = zipObjMap(featureNames, featureVector); assertEquals(Arrays.asList(1.0, 0.0), tree.classificationProbability(featureMap)); }	this could be written as: assertthat(tree.classificationprobability(featuremap), contains(0.0, 1.0));
public static Tree buildRandomTree(List<String> featureNames, int depth) { Tree.Builder builder = Tree.builder(); int numFeatures = featureNames.size() - 1; builder.setFeatureNames(featureNames); TreeNode.Builder node = builder.addJunction(0, randomInt(numFeatures), true, randomDouble()); List<Integer> childNodes = List.of(node.getLeftChild(), node.getRightChild()); for (int i = 0; i < depth -1; i++) { List<Integer> nextNodes = new ArrayList<>(); for (int nodeId : childNodes) { if (i == depth -2) { builder.addLeaf(nodeId, randomDouble()); } else { TreeNode.Builder childNode = builder.addJunction(nodeId, randomInt(numFeatures), true, randomDouble()); nextNodes.add(childNode.getLeftChild()); nextNodes.add(childNode.getRightChild()); } } childNodes = nextNodes; } List<String> categoryLabels = null; if (randomBoolean()) { categoryLabels = Arrays.asList(generateRandomStringArray(randomIntBetween(1, 10), randomIntBetween(1, 10), false, false)); } return builder.setTargetType(randomFrom(TargetType.values())) .setClassificationLabels(categoryLabels) .build(); }	will this fail validation if labels are set but target type is regression?
public Email.Builder render(TextTemplateEngine engine, Map<String, Object> model, HtmlSanitizer htmlSanitizer, Map<String, Attachment> attachments) throws AddressException { Email.Builder builder = Email.builder(); if (from != null) { builder.from(engine.render(from, model)); } if (replyTo != null) { Email.AddressList addresses = templatesToAddressList(engine, replyTo, model); builder.replyTo(addresses); } if (priority != null) { builder.priority(Email.Priority.resolve(engine.render(priority, model))); } if (to != null) { Email.AddressList addresses = templatesToAddressList(engine, to, model); builder.to(addresses); } if (cc != null) { Email.AddressList addresses = templatesToAddressList(engine, cc, model); builder.cc(addresses); } if (bcc != null) { Email.AddressList addresses = templatesToAddressList(engine, bcc, model); builder.bcc(addresses); } if (subject != null) { builder.subject(engine.render(subject, model)); } Set<String> warnings = new HashSet<>(1); if (attachments != null) { for (Attachment attachment : attachments.values()) { builder.attach(attachment); warnings.addAll(attachment.getWarnings()); } } String htmlWarnings = ""; String textWarnings = ""; if(warnings.isEmpty() == false){ StringBuilder textWarningBuilder = new StringBuilder(); StringBuilder htmlWarningBuilder = new StringBuilder(); warnings.forEach(w -> { if(Strings.isNullOrEmpty(w) == false) { textWarningBuilder.append(w).append("\\\\n"); htmlWarningBuilder.append(w).append("<br>"); } }); textWarningBuilder.append("\\\\n"); htmlWarningBuilder.append("<br>"); htmlWarnings = htmlWarningBuilder.toString(); textWarnings = textWarningBuilder.toString(); } if (textBody != null) { builder.textBody(textWarnings + engine.render(textBody, model)); } if (htmlBody != null) { String renderedHtml = htmlWarnings + engine.render(htmlBody, model); renderedHtml = htmlSanitizer.sanitize(renderedHtml); builder.htmlBody(renderedHtml); } return builder; }	i just realized, that despite the current documenation, body is not actually required, will need to adjust this to always emit a warning even if the body is not defined.
private Exception randomRetryPrimaryException(ShardId shardId) { return randomFrom( new ShardNotFoundException(shardId), new IndexNotFoundException(shardId.getIndex()), new IndexShardClosedException(shardId), new AlreadyClosedException("primary is closed"), new ReplicationOperation.RetryOnPrimaryException(shardId, "hello") ); }	maybe put the shard id into the message just for completeness?
public void withBackoff(BiConsumer<BulkRequest, ActionListener<BulkResponse>> consumer, BulkRequest bulkRequest, ActionListener<BulkResponse> listener) { RetryHandler r = new RetryHandler(backoffPolicy, consumer, listener, scheduler); r.execute(bulkRequest); } /** * Invokes #accept(BulkRequest, ActionListener). Backs off on the provided exception and delegates results to the * provided listener. Retries will be scheduled using the class's thread pool. * @param consumer The consumer to which apply the request and listener * @param bulkRequest The bulk request that should be executed. * @param listener A listener that is invoked when the bulk request finishes or completes with an exception. The listener is not * @param settings settings * @deprecated Prefer {@link #withBackoff(BiConsumer, BulkRequest, ActionListener)}. The {@link Settings}	since this is actively part of the public api i'm deprecating the old way before removing it in master.
public static void blockDataNode(String repository, String nodeName) { AbstractSnapshotIntegTestCase.<MockRepository>getRepositoryOnNode(repository, nodeName).blockOnDataFiles(); }	the fact that we didn't have this logic revealed an unfortunate lack of test coverage. we have a number of tests that simulate data-node failure but they're all based on blocking the data-node via the existing block-and-wait and then shutting down the blocked data nodes which triggers a very different code path on master.
public void testGetParams_GivenDefaults() { OutlierDetection outlierDetection = new OutlierDetection.Builder().build(); Map<String, Object> params = outlierDetection.getParams(); assertThat(params.size(), equalTo(3)); assertThat(params.containsKey("compute_feature_influence"), is(true)); assertThat(params.get("compute_feature_influence"), is(true)); assertThat(params.containsKey("outlier_fraction"), is(true)); assertThat((double) params.get("outlier_fraction"), closeTo(0.05, 0.0001)); assertThat(params.containsKey("standardization_enabled"), is(true)); assertThat(params.get("standardization_enabled"), is(true)); }	fyi: there are haskey and hasentry methods in org.hamcrest.matchers which you could use here.
public AnalyzeRequest tokenFilters(String... tokenFilters) { assert tokenFilters != null: "token filters must not be null"; this.tokenFilters = tokenFilters; return this; }	imo i think this should be an if statement that throws an illegalarguementexception instead of an assert since a user can easily pass in null at runtime here. that way they will get an exception when its set rather than a weird exception happening when we try to use the variable. same goes for charfilters and attributes below
public LogicalPlan apply(LogicalPlan plan) { if (metrics() != null) { plan.forEachDown(p -> { if (p instanceof SubQueryAlias) { metrics().inc(SUBSELECT); } }); plan.forEachExpressionsDown(expr -> { if (expr instanceof SubQueryExpression) { metrics().inc(SUBSELECT); } }); } return plan; }	i think here we will count double if the subselect is also aliased or not?
protected void doExecute(Task task, SqlTranslateRequest request, ActionListener<SqlTranslateResponse> listener) { sqlLicenseChecker.checkIfSqlAllowed(request.mode()); counter.inc(); Configuration cfg = new Configuration(request.timeZone(), request.fetchSize(), request.requestTimeout(), request.pageTimeout(), request.filter()); planExecutor.searchSource(cfg, request.query(), request.params(), ActionListener.wrap( searchSourceBuilder -> listener.onResponse(new SqlTranslateResponse(searchSourceBuilder)), listener::onFailure)); }	i would use a static string for the key.
public void testConcurrentSnapshotRestoreAndDeleteOther() { setupTestCluster(randomFrom(1, 3, 5), randomIntBetween(2, 10)); String repoName = "repo"; String snapshotName = "snapshot"; final String index = "test"; final int shards = randomIntBetween(1, 10); TestClusterNodes.TestClusterNode masterNode = testClusterNodes.currentMaster(testClusterNodes.nodes.values().iterator().next().clusterService.state()); final StepListener<CreateSnapshotResponse> createSnapshotResponseStepListener = new StepListener<>(); continueOrDie(createRepoAndIndex(repoName, index, shards), createIndexResponse -> client().admin().cluster().prepareCreateSnapshot(repoName, snapshotName) .setWaitForCompletion(true).execute(createSnapshotResponseStepListener)); final StepListener<CreateSnapshotResponse> createOtherSnapshotResponseStepListener = new StepListener<>(); continueOrDie(createSnapshotResponseStepListener, createSnapshotResponse -> client().admin().cluster().prepareCreateSnapshot(repoName, "snapshot-2") .setWaitForCompletion(true) .execute(createOtherSnapshotResponseStepListener)); final StepListener<AcknowledgedResponse> deleteSnapshotStepListener = new StepListener<>(); final StepListener<RestoreSnapshotResponse> restoreSnapshotResponseListener = new StepListener<>(); continueOrDie(createOtherSnapshotResponseStepListener, createSnapshotResponse -> { scheduleNow( () -> client().admin().cluster().prepareDeleteSnapshot(repoName, snapshotName).execute(deleteSnapshotStepListener)); scheduleNow(() -> client().admin().cluster().restoreSnapshot( new RestoreSnapshotRequest(repoName, "snapshot-2").waitForCompletion(true) .renamePattern("(.+)").renameReplacement("restored_$1"), restoreSnapshotResponseListener)); }); deterministicTaskQueue.runAllRunnableTasks(); assertThat(deleteSnapshotStepListener.result().isAcknowledged(), is(true)); assertThat(restoreSnapshotResponseListener.result().getRestoreInfo().failedShards(), is(0)); final Repository repository = masterNode.repositoriesService.repository(repoName); Collection<SnapshotId> snapshotIds = getRepositoryData(repository).getSnapshotIds(); assertThat(snapshotIds, contains(createOtherSnapshotResponseStepListener.result().getSnapshotInfo().snapshotId())); for (SnapshotId snapshotId : snapshotIds) { final SnapshotInfo snapshotInfo = repository.getSnapshotInfo(snapshotId); assertEquals(SnapshotState.SUCCESS, snapshotInfo.state()); assertThat(snapshotInfo.indices(), containsInAnyOrder(index)); assertEquals(shards, snapshotInfo.successfulShards()); assertEquals(0, snapshotInfo.failedShards()); } }	should we also index some docs (mostly to generate more snapshot files) before and in-between snapshots, and then run a query?
@Override public Set<CharSequence> parseContext(Document document) { Set<CharSequence> values = null; if (fieldName != null) { IndexableField[] fields = document.getFields(fieldName); values = new HashSet<>(fields.length); for (IndexableField field : fields) { if (field.fieldType() instanceof KeywordFieldMapper.KeywordFieldType) { values.add(field.binaryValue().utf8ToString()); } else if (field.fieldType() instanceof StringFieldType) { values.add(field.stringValue()); } else { // ignore doc_values field assert field.fieldType() instanceof SortedDocValuesField || field.fieldType() instanceof SortedSetDocValuesField; } } } return (values == null) ? Collections.emptySet() : values; }	can we make it a real exception? as well as fail if none of the fields provided a value so that we fail on numeric fields, or unindexed keyword fields?
@Override protected void masterOperation( final CreateAndFollowIndexAction.Request request, final ClusterState state, final ActionListener<CreateAndFollowIndexAction.Response> listener) throws Exception { if (ccrLicenseChecker.isCcrAllowed() == false) { listener.onFailure(LicenseUtils.newComplianceException("ccr")); return; } final String[] indices = new String[]{request.getFollowRequest().getLeaderIndex()}; validateClusterAlias(remoteClusterService.getRemoteClusterNames(), request.getFollowRequest().getLeaderIndex()); final Map<String, List<String>> remoteClusterIndices = remoteClusterService.groupClusterIndices(indices, s -> false); if (remoteClusterIndices.containsKey(RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY)) { createFollowerIndexAndFollowLocalIndex(request, state, listener); } else { assert remoteClusterIndices.size() == 1; final Map.Entry<String, List<String>> entry = remoteClusterIndices.entrySet().iterator().next(); assert entry.getValue().size() == 1; final String clusterAlias = entry.getKey(); final String leaderIndex = entry.getValue().get(0); createFollowerIndexAndFollowRemoteIndex(request, clusterAlias, leaderIndex, listener); } }	i thought that the idea was to end up not using groupclusterindices completely, but maybe i misunderstood. can't you do what you need without calling groupclusterindices?
static void validateClusterAlias(Set<String> remoteClusterNames, String leaderIndex) { int indexOf = leaderIndex.indexOf(':'); if (indexOf != -1) { String clusterAlias = leaderIndex.substring(0, indexOf); if (remoteClusterNames.contains(clusterAlias) == false) { throw new IllegalArgumentException("unknown cluster alias [" + clusterAlias + "]"); } } }	nit: we have a constant for ":"
* @param logger the logger to */ static void check( final boolean enforceLimits, final List<BootstrapCheck> checks, final Logger logger) throws NodeValidationException { final List<String> errors = new ArrayList<>(); final List<String> ignoredErrors = new ArrayList<>(); final String esEnforceBootstrapChecks = System.getProperty(ES_ENFORCE_BOOTSTRAP_CHECKS); final boolean enforceBootstrapChecks; if (esEnforceBootstrapChecks == null) { enforceBootstrapChecks = false; } else if (Boolean.TRUE.toString().equals(esEnforceBootstrapChecks)) { enforceBootstrapChecks = true; } else { final String message = String.format( Locale.ROOT, "[%s] must be [true] but was [%s]", ES_ENFORCE_BOOTSTRAP_CHECKS, esEnforceBootstrapChecks); throw new IllegalArgumentException(message); } if (enforceLimits) { logger.info("bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks"); } else if (enforceBootstrapChecks) { logger.info("explicitly enforcing bootstrap checks"); } for (final BootstrapCheck check : checks) { if (check.check()) { if (!(enforceLimits || enforceBootstrapChecks) && !check.alwaysEnforce()) { ignoredErrors.add(check.errorMessage()); } else { errors.add(check.errorMessage()); } } } if (!ignoredErrors.isEmpty()) { ignoredErrors.forEach(error -> log(logger, error)); } if (!errors.isEmpty()) { final List<String> messages = new ArrayList<>(1 + errors.size()); messages.add("bootstrap checks failed"); messages.addAll(errors); final NodeValidationException ne = new NodeValidationException(String.join("\\\\n", messages)); errors.stream().map(IllegalStateException::new).forEach(ne::addSuppressed); throw ne; } }	commenting here as this is the first line i can comment on. nit: the java docs of enforcelimits needs to be adapted imo
@Override public boolean test(PersistentTasksCustomMetadata.PersistentTask<?> persistentTask) { JobState jobState = JobState.CLOSED; String reason = "__unknown__"; if (persistentTask != null) { JobTaskState jobTaskState = (JobTaskState) persistentTask.getState(); jobState = jobTaskState == null ? JobState.OPENING : jobTaskState.getState(); reason = jobTaskState != null ? jobTaskState.getReason() : reason; PersistentTasksCustomMetadata.Assignment assignment = persistentTask.getAssignment(); // This means we are awaiting a new node to be spun up, ok to return back to the user to await node creation if (assignment != null && assignment.equals(JobNodeSelector.AWAITING_LAZY_ASSIGNMENT)) { return true; } // This logic is only appropriate when opening a job, not when reallocating following a failure, // and this is why this class must only be used when opening a job if (assignment != null && assignment.equals(PersistentTasksCustomMetadata.INITIAL_ASSIGNMENT) == false && assignment.isAssigned() == false) { OpenJobAction.JobParams params = (OpenJobAction.JobParams) persistentTask.getParams(); // Assignment has failed on the master node despite passing our "fast fail" validation if (assignment.equals(AWAITING_UPGRADE)) { exception = makeCurrentlyBeingUpgradedException(logger, params.getJobId(), assignment.getExplanation()); } else if (assignment.getExplanation().contains("[" + EnableAssignmentDecider.ALLOCATION_NONE_EXPLANATION + "]")) { exception = makeAssignmentsNotAllowedException(logger, params.getJobId()); } else { exception = makeNoSuitableNodesException(logger, params.getJobId(), assignment.getExplanation()); } // The persistent task should be cancelled so that the observed outcome is the // same as if the "fast fail" validation on the coordinating node had failed shouldCancel = true; return true; } } switch (jobState) { // The OPENING case here is expected to be incredibly short-lived, just occurring during the // time period when a job has successfully been assigned to a node but the request to update // its task state is still in-flight. (The long-lived OPENING case when a lazy node needs to // be added to the cluster to accommodate the job was dealt with higher up this method when the // magic AWAITING_LAZY_ASSIGNMENT assignment was checked for.) case OPENING: case CLOSED: return false; case OPENED: node = persistentTask.getExecutorNode(); return true; case CLOSING: exception = ExceptionsHelper.conflictStatusException( "The job has been {} while waiting to be {}", JobState.CLOSED, JobState.OPENED); return true; case FAILED: default: // Default http status is SERVER ERROR exception = ExceptionsHelper.serverError( "Unexpected job state [{}] with reason [{}] while waiting for job to be {}", jobState, reason, JobState.OPENED ); return true; } }	can you change the message to include reason only if it is known please. "unexpected job state [xx] with reason [__unknown__]... " is a message that can only frustrate
private void buildScriptService(Settings additionalSettings) throws IOException { Settings finalSettings = Settings.builder().put(baseSettings).put(additionalSettings).build(); Environment environment = new Environment(finalSettings); // TODO: scriptService = new ScriptService(finalSettings, environment, resourceWatcherService, scriptEngineRegistry, scriptContextRegistry, scriptSettings) { @Override String getScriptFromClusterState(String scriptLang, String id) { //mock the script that gets retrieved from an index return "100"; } }; }	missing comment after todo?
protected GetAutoFollowPatternAction.Response createServerTestInstance(XContentType xContentType) { int numPatterns = randomIntBetween(0, 16); NavigableMap<String, AutoFollowMetadata.AutoFollowPattern> patterns = new TreeMap<>(); for (int i = 0; i < numPatterns; i++) { String remoteCluster = randomAlphaOfLength(4); List<String> leaderIndexPatterns = Collections.singletonList(randomAlphaOfLength(4)); List<String> leaderIndexExclusionsPatterns = Collections.singletonList(randomAlphaOfLength(4)); String followIndexNamePattern = randomAlphaOfLength(4); final Settings settings = Settings.builder().put(IndexMetadata.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), randomIntBetween(0, 4)).build(); boolean active = randomBoolean(); Integer maxOutstandingReadRequests = null; if (randomBoolean()) { maxOutstandingReadRequests = randomIntBetween(0, Integer.MAX_VALUE); } Integer maxOutstandingWriteRequests = null; if (randomBoolean()) { maxOutstandingWriteRequests = randomIntBetween(0, Integer.MAX_VALUE); } Integer maxReadRequestOperationCount = null; if (randomBoolean()) { maxReadRequestOperationCount = randomIntBetween(0, Integer.MAX_VALUE); } ByteSizeValue maxReadRequestSize = null; if (randomBoolean()) { maxReadRequestSize = new ByteSizeValue(randomNonNegativeLong()); } Integer maxWriteBufferCount = null; if (randomBoolean()) { maxWriteBufferCount = randomIntBetween(0, Integer.MAX_VALUE); } ByteSizeValue maxWriteBufferSize = null; if (randomBoolean()) { maxWriteBufferSize = new ByteSizeValue(randomNonNegativeLong()); } Integer maxWriteRequestOperationCount = null; if (randomBoolean()) { maxWriteRequestOperationCount = randomIntBetween(0, Integer.MAX_VALUE); } ByteSizeValue maxWriteRequestSize = null; if (randomBoolean()) { maxWriteRequestSize = new ByteSizeValue(randomNonNegativeLong()); } TimeValue maxRetryDelay = null; if (randomBoolean()) { maxRetryDelay = new TimeValue(randomNonNegativeLong()); } TimeValue readPollTimeout = null; if (randomBoolean()) { readPollTimeout = new TimeValue(randomNonNegativeLong()); } patterns.put( randomAlphaOfLength(4), new AutoFollowMetadata.AutoFollowPattern( remoteCluster, leaderIndexPatterns, leaderIndexExclusionsPatterns, followIndexNamePattern, settings, active, maxReadRequestOperationCount, maxWriteRequestOperationCount, maxOutstandingReadRequests, maxOutstandingWriteRequests, maxReadRequestSize, maxWriteRequestSize, maxWriteBufferCount, maxWriteBufferSize, maxRetryDelay, readPollTimeout ) ); } return new GetAutoFollowPatternAction.Response(patterns); }	maybe randomize with empty, single and multiple exclusions patterns?
private static void warnIfFollowedIndicesExcludedWithNewPatterns(String autoFollowName, List<String> newLeaderPatterns, List<String> newLeaderIndexExclusionPatterns, Metadata remoteMetadata, Metadata localMetadata, AutoFollowPattern previousPattern, List<String> followedIndexUUIDS) { final boolean hasNewExclusionPatterns = newLeaderIndexExclusionPatterns .stream() .anyMatch(p -> previousPattern.getLeaderIndexExclusionPatterns().contains(p) == false); if (hasNewExclusionPatterns == false) { return; } for (IndexMetadata localIndexMetadata : localMetadata) { final Map<String, String> ccrMetadata = localIndexMetadata.getCustomData(Ccr.CCR_CUSTOM_METADATA_KEY); if (ccrMetadata != null && followedIndexUUIDS.contains(ccrMetadata.get(Ccr.CCR_CUSTOM_METADATA_LEADER_INDEX_UUID_KEY))) { final String leaderIndexName = ccrMetadata.get(Ccr.CCR_CUSTOM_METADATA_LEADER_INDEX_NAME_KEY); IndexAbstraction indexAbstraction = remoteMetadata.getIndicesLookup().get(leaderIndexName); final IndexMetadata leaderIndexMetadata = remoteMetadata.index(leaderIndexName); if (AutoFollowPattern.match(newLeaderPatterns, newLeaderIndexExclusionPatterns, indexAbstraction) == false) { logger.warn("The follower index {} for leader index {} does not match against the updated auto follow " + "pattern with name {}, follow patterns {} and exclusion patterns {}", localIndexMetadata.getIndex(), leaderIndexMetadata.getIndex(), autoFollowName, newLeaderPatterns, newLeaderIndexExclusionPatterns); } } } }	i'm a bit torn about this - i wonder if we should make it more explicit to the user by returning an error. the same question applies when inclusion patterns are updated - a follower index might previously match a pattern but not anymore. maybe we should prevent such update and require either a new pattern to be created or the follower indices to be unfollowed first?
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	getrecoverysource.gettype() == type.peer avoid the cast here
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	technically, learning about new primary also does this ;)
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	this logic is the same as for getstartingseqno. perhaps it makes sense to share it?
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	what about frozen indices?
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	do we need this here? we don't do this for other steps in peer recovery either?
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	why use this instead of local_translog_recovery as origin? i'm not sure which of the two is more appropriate, but was wondering about what motivated your choice here
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	can we assert that this compareandset must succeed?
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	could this throw an exception? should we just catch that and move on?
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	why do we need to set init here? perhaps we need a proper stage transition here
public void prepareShardForPeerRecovery() { if (state != IndexShardState.RECOVERING) { throw new IndexShardNotRecoveringException(shardId, state); } if (recoveryState.getRecoverySource() instanceof RecoverySource.PeerRecoverySource == false) { assert false : "only peer recovery needs to recover locally up to the global checkpoint"; throw new IllegalStateException("only peer recovery needs to recover locally up to the global checkpoint"); } try { final long globalCheckpoint; final Engine engine; synchronized (mutex) { String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID); List<IndexCommit> commits = DirectoryReader.listCommits(store.directory()); IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint); SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet()); boolean shouldRecoverLocally = commitInfo.maxSeqNo <= globalCheckpoint && commitInfo.localCheckpoint < globalCheckpoint; if (shouldRecoverLocally && indexSettings.getIndexMetaData().getState() == IndexMetaData.State.OPEN) { engine = innerOpenEngineAndTranslog(); } else { engine = null; } } if (engine != null) { try { indexShardOperationPermits.blockOperations(-1, TimeUnit.SECONDS, () -> { final Engine.TranslogRecoveryRunner translogRecoveryRunner = (e, snapshot) -> runTranslogRecovery(e, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> { }); engine.recoverFromTranslog(translogRecoveryRunner, globalCheckpoint); }); engine.flush(true, true); } finally { synchronized (mutex) { currentEngineReference.compareAndSet(engine, null); IOUtils.close(engine); } recoveryState.setStage(RecoveryState.Stage.INIT); recoveryState.setStage(RecoveryState.Stage.INDEX); } } } catch (Exception ignored) { // We can hit exception if the store is empty or corrupted or incomplete (due to the abort of the previous peer recovery). // It's fine if this step failed as the peer recovery can use whatever outcome correctly. We can safely ignore exception here. } }	we should at least debug log this. we could also distinguish between expected and unexpected cases here and log with even higher log level in those unexpected cases (perhaps even add an assert false).
static ClusterState createDataStream(MetadataCreateIndexService metadataCreateIndexService, ClusterState currentState, CreateDataStreamClusterStateUpdateRequest request) throws Exception { if (currentState.nodes().getMinNodeVersion().compareTo(Version.V_8_0_0) < 0) { throw new IllegalStateException("data streams require minimum node version of " + Version.V_8_0_0); } if (currentState.metadata().dataStreams().containsKey(request.name)) { throw new IllegalArgumentException("data_stream [" + request.name + "] already exists"); } MetadataCreateIndexService.validateIndexOrAliasName(request.name, (s1, s2) -> new IllegalArgumentException("data_stream [" + s1 + "] " + s2)); if (request.name.toLowerCase(Locale.ROOT).equals(request.name) == false) { throw new IllegalArgumentException("data_stream [" + request.name + "] must be lowercase"); } if (request.name.startsWith(".")) { throw new IllegalArgumentException("data_stream [" + request.name + "] must not start with '.'"); } ComposableIndexTemplate template = lookupTemplateForDataStream(request.name, currentState.metadata()); String firstBackingIndexName = DataStream.getDefaultBackingIndexName(request.name, 1); CreateIndexClusterStateUpdateRequest createIndexRequest = new CreateIndexClusterStateUpdateRequest("initialize_data_stream", firstBackingIndexName, firstBackingIndexName) .dataStreamName(request.name) .settings(Settings.builder().put("index.hidden", true).build()); currentState = metadataCreateIndexService.applyCreateIndexRequest(currentState, createIndexRequest, false); IndexMetadata firstBackingIndex = currentState.metadata().index(firstBackingIndexName); assert firstBackingIndex != null; Metadata.Builder builder = Metadata.builder(currentState.metadata()).put( new DataStream(request.name, template.getDataStreamTemplate().getTimestampField(), List.of(firstBackingIndex.getIndex()))); logger.info("adding data stream [{}]", request.name); return ClusterState.builder(currentState).metadata(builder).build(); }	i think check is a bit clearer: currentstate.nodes().getminnodeversion().before(version.v_8_0_0)
public static PluginInfo readFromProperties(Path dir) throws IOException { Path descriptor = dir.resolve(ES_PLUGIN_PROPERTIES); Properties props = new Properties(); try (InputStream stream = Files.newInputStream(descriptor)) { props.load(stream); } String name = dir.getFileName().toString(); String description = props.getProperty("description"); if (description == null) { throw new IllegalArgumentException("Property [description] is missing for plugin [" + name + "]"); } String version = props.getProperty("version"); if (version == null) { throw new IllegalArgumentException("Property [version] is missing for plugin [" + name + "]"); } boolean jvm = Boolean.parseBoolean(props.getProperty("jvm")); boolean site = Boolean.parseBoolean(props.getProperty("site")); if (jvm == false && site == false) { throw new IllegalArgumentException("Plugin [" + name + "] must be at least a jvm or site plugin"); } boolean isolated = true; String classname = "NA"; if (jvm) { String esVersionString = props.getProperty("elasticsearch.version"); if (esVersionString == null) { throw new IllegalArgumentException("Property [elasticsearch.version] is missing for jvm plugin [" + name + "]"); } Version esVersion = Version.fromString(esVersionString); if (esVersion.equals(Version.CURRENT) == false) { throw new IllegalArgumentException("Elasticsearch version [" + esVersionString + "] is too old for plugin [" + name + "]"); } isolated = Boolean.parseBoolean(props.getProperty("isolated", "true")); classname = props.getProperty("classname"); if (classname == null) { throw new IllegalArgumentException("Property [classname] is missing for jvm plugin [" + name + "]"); } } if (site) { if (!Files.exists(dir.resolve("_site"))) { throw new IllegalArgumentException("Plugin [" + name + "] is a site plugin but has no _site"); } } return new PluginInfo(name, description, site, version, jvm, classname, isolated); }	maybe add directory or dir at the end of the string
public AnalysisConfig build() { TimeUtils.checkPositiveMultiple(bucketSpan, TimeUnit.SECONDS, BUCKET_SPAN); if (modelPruneWindow != null) { TimeUtils.checkNonNegativeMultiple(modelPruneWindow, TimeUnit.SECONDS, MODEL_PRUNE_WINDOW); } if (latency != null) { TimeUtils.checkNonNegativeMultiple(latency, TimeUnit.SECONDS, LATENCY); } verifyDetectorAreDefined(); Detector.Builder.verifyFieldName(summaryCountFieldName); Detector.Builder.verifyFieldName(categorizationFieldName); verifyMlCategoryIsUsedWhenCategorizationFieldNameIsSet(); verifyCategorizationAnalyzer(); verifyCategorizationFilters(); verifyConfigConsistentWithPerPartitionCategorization(); verifyNoMetricFunctionsWhenSummaryCountFieldNameIsSet(); verifyNoInconsistentNestedFieldNames(); return new AnalysisConfig(bucketSpan, categorizationFieldName, categorizationFilters, categorizationAnalyzerConfig, perPartitionCategorizationConfig, latency, summaryCountFieldName, detectors, influencers, multivariateByFields, modelPruneWindow); }	shouldn't this always be greater than the bucketspan? i see that the c++ side secretly does a math.max(bucketspan, modelprunewindow type of thing. i think it would be better to not allow a window to be too small.
private static AnalysisConfig.Builder createValidConfig() { List<Detector> detectors = new ArrayList<>(); Detector detector = new Detector.Builder("count", null).build(); detectors.add(detector); AnalysisConfig.Builder analysisConfig = new AnalysisConfig.Builder(detectors); analysisConfig.setBucketSpan(TimeValue.timeValueHours(1)); analysisConfig.setLatency(TimeValue.ZERO); analysisConfig.setModelPruneWindow(TimeValue.ZERO); return analysisConfig; }	zero shouldn't be valid, as it's null that means "don't prune after a fixed time".
private static AnalysisConfig.Builder createValidCategorizationConfig() { Detector.Builder detector = new Detector.Builder("count", null); detector.setByFieldName("mlcategory"); AnalysisConfig.Builder analysisConfig = new AnalysisConfig.Builder(Collections.singletonList(detector.build())); analysisConfig.setBucketSpan(TimeValue.timeValueHours(1)); analysisConfig.setLatency(TimeValue.ZERO); analysisConfig.setModelPruneWindow(TimeValue.ZERO); analysisConfig.setCategorizationFieldName("msg"); return analysisConfig; }	zero shouldn't be valid, as it's null that means "don't prune after a fixed time".
public void syncRetentionLeases() { assert assertPrimaryMode(); verifyNotClosed(); replicationTracker.renewPeerRecoveryRetentionLeases(); final Tuple<Boolean, RetentionLeases> retentionLeases = getRetentionLeases(true); logger.trace("background syncing retention leases [{}] after expiration check", retentionLeases.v2()); retentionLeaseSyncer.backgroundSync( shardId, shardRouting.allocationId().getId(), getPendingPrimaryTerm(), retentionLeases.v2()); }	with this change, i think we no longer need to return a tuple in getretentionleases(true)?
public CreateIndexRequest settings(Map<String, ?> source) { try { XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON); builder.map(source); settings(Strings.toString(builder), XContentType.JSON); } catch (IOException e) { throw new ElasticsearchGenerationException("Failed to generate [" + source + "]", e); } return this; } /** * Set the mapping for this index * * The mapping should be in the form of a JSON string, with an outer _doc key * <pre> * .mapping("{\\\\"_doc\\\\":{\\\\"properties\\\\": ... }}	one the client side we are [not nesting the mapping definition under the type](https://github.com/elastic/elasticsearch/blob/master/client/rest-high-level/src/main/java/org/elasticsearch/client/indices/createindexrequest.java#l151) . are we ok with this difference?
public static NGram createRandom() { int length = randomIntBetween(1, 10); return new NGram(randomAlphaOfLength(10), IntStream.range(1, Math.min(5, length)).limit(5).boxed().collect(Collectors.toList()), randomBoolean() ? null : randomIntBetween(0, 10), randomBoolean() ? null : length, randomBoolean() ? null : randomBoolean(), randomBoolean() ? null : randomAlphaOfLength(10)); }	i think this can occasionally fail. if length is 1, then intstream.range(1, 1) will result to an empty list. looking into this more, it seems we don't validate against empty ngrams. should we?
protected NodeConfigurationSource getNodeConfigSource() { Settings.Builder initialNodeSettings = Settings.builder(); Settings.Builder initialTransportClientSettings = Settings.builder(); if (addMockTransportService()) { initialNodeSettings.put(NetworkModule.TRANSPORT_TYPE_KEY, getTestTransportType()); initialTransportClientSettings.put(NetworkModule.TRANSPORT_TYPE_KEY, getTestTransportType()); } if (addTestZenDiscovery() && getUseZen2() == false) { initialNodeSettings.put(TestZenDiscovery.USE_ZEN2.getKey(), false); } return new NodeConfigurationSource() { @Override public Settings nodeSettings(int nodeOrdinal) { return Settings.builder() .put(initialNodeSettings.build()) .put(ESIntegTestCase.this.nodeSettings(nodeOrdinal)).build(); } @Override public List<Settings> addExtraClusterBootstrapSettings(List<Settings> allNodesSettings) { return ESIntegTestCase.this.addExtraClusterBootstrapSettings(allNodesSettings); } @Override public Path nodeConfigPath(int nodeOrdinal) { return ESIntegTestCase.this.nodeConfigPath(nodeOrdinal); } @Override public Collection<Class<? extends Plugin>> nodePlugins() { return ESIntegTestCase.this.nodePlugins(); } @Override public Settings transportClientSettings() { return Settings.builder().put(initialTransportClientSettings.build()) .put(ESIntegTestCase.this.transportClientSettings()).build(); } @Override public Collection<Class<? extends Plugin>> transportClientPlugins() { Collection<Class<? extends Plugin>> plugins = ESIntegTestCase.this.transportClientPlugins(); if (plugins.contains(getTestTransportPlugin()) == false) { plugins = new ArrayList<>(plugins); plugins.add(getTestTransportPlugin()); } return Collections.unmodifiableCollection(plugins); } }; } /** * Performs cluster bootstrap when {@link #bootstrapMasterNodeId} is started with the names of all * previously started master-eligible nodes. * If {@link #bootstrapMasterNodeId}	s/previously started/existing and new/?
* If {@link #bootstrapMasterNodeId} is -1 (default), this method does nothing. */ protected List<Settings> bootstrapMasterNodeWithSpecifiedId(List<Settings> allNodesSettings) { if (bootstrapMasterNodeId == -1) { // fast-path return allNodesSettings; } int currentNodeId = internalCluster().numMasterNodes(); List<Settings> newSettings = new ArrayList<>(); for (Settings settings : allNodesSettings) { if (org.elasticsearch.node.Node.NODE_MASTER_SETTING.get(settings) == false) { newSettings.add(settings); } else { currentNodeId++; if (currentNodeId != bootstrapMasterNodeId) { newSettings.add(settings); } else { List<String> nodeNames = new ArrayList<>(); for (Settings nodeSettings : internalCluster().getDataOrMasterNodeInstances(Settings.class)) { if (org.elasticsearch.node.Node.NODE_MASTER_SETTING.get(nodeSettings)) { nodeNames.add(org.elasticsearch.node.Node.NODE_NAME_SETTING.get(nodeSettings)); } } for (Settings nodeSettings : allNodesSettings) { if (org.elasticsearch.node.Node.NODE_MASTER_SETTING.get(nodeSettings)) { nodeNames.add(org.elasticsearch.node.Node.NODE_NAME_SETTING.get(nodeSettings)); } } newSettings.add(Settings.builder().put(settings) .putList(ClusterBootstrapService.INITIAL_MASTER_NODES_SETTING.getKey(), nodeNames) .build()); } } } return newSettings; } /** * This method is called before starting a collection of nodes. * At this point the test has a holistic view on all nodes settings and might perform settings adjustments as needed. * For instance, the test could retrieve master node names and fill in * {@link org.elasticsearch.cluster.coordination.ClusterBootstrapService#INITIAL_MASTER_NODES_SETTING} setting. * By default, this method delegates to {@link #bootstrapMasterNodeWithSpecifiedId(List)}	i think this is never overridden, so can be inlined.
@Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } Response response = (Response) o; return clusterName.equals(response.clusterName) && status == response.status && components.equals(response.components); }	interestingly we use a list<healthcomponentresult> here and that's ok because healthcomponentresult is a record which has a list of healthindicatorresult which is also a record but that last one has a healthindicatordetails member and i'm not sure how we can guarantee that the equals method does not use objects references?
@Override public int hashCode() { return Objects.hash(dataType, value); }	changed the order of hashing to encourage caching / similarities of values that have the same datatype.
@Override protected Expression canonicalize() { if (field() instanceof Negatable) { return ((Negatable) field()).negate().canonical(); } return super.canonicalize(); }	canonicalize after negation to avoid recreating the trees twice since canonical already handles normalization and ordering.
protected final void innerWriteTo(StreamOutput out, boolean asKey) throws IOException { shardId.writeTo(out); out.writeByte(searchType.id()); if (!asKey) { out.writeVInt(numberOfShards); } out.writeOptionalWriteable(scroll); out.writeOptionalWriteable(source); if (out.getVersion().before(Version.V_8_0_0)) { // types not supported so send an empty array to previous versions out.writeStringArray(Strings.EMPTY_ARRAY); } aliasFilter.writeTo(out); out.writeFloat(indexBoost); if (asKey == false) { out.writeVLong(nowInMillis); } out.writeOptionalBoolean(requestCache); out.writeOptionalString(clusterAlias); out.writeBoolean(allowPartialSearchResults); if (asKey == false) { out.writeStringArray(indexRoutings); out.writeOptionalString(preference); } if (out.getVersion().onOrAfter(Version.V_7_7_0)) { if (asKey == false) { out.writeBoolean(canReturnNullResponseIfMatchNoDocs); } out.writeOptionalWriteable(bottomSortValues); } }	@jimczi i was wondering if should also do this for bottomsortvalues as well? as the result from the request should not be dependent on bottomsortvalues. that's not relevant for indicesrequestcacheit but overall
CloudBlobClient getSelectedClient(String account, LocationMode mode) { logger.trace("selecting a client for account [{}], mode [{}]", account, mode.name()); AzureStorageSettings azureStorageSettings = null; if (this.primaryStorageSettings == null) { throw new IllegalArgumentException("No primary azure storage can be found. Check your elasticsearch.yml."); } if (account != null) { azureStorageSettings = this.secondariesStorageSettings.get(account); } // if account is not secondary, it's the primary if (azureStorageSettings == null) { if (account == null || primaryStorageSettings.getName() == null || account.equals(primaryStorageSettings.getName())) { azureStorageSettings = primaryStorageSettings; } } if (azureStorageSettings == null) { // We did not get an account. That's bad. throw new IllegalArgumentException("Can not find azure account [" + account + "]. Check your elasticsearch.yml."); } CloudBlobClient client = this.clients.get(azureStorageSettings.getAccount()); if (client == null) { throw new IllegalArgumentException("Can not find an azure client for account [" + account + "]"); } // NOTE: for now, just set the location mode in case it is different; // only one mode per storage account can be active at a time client.getDefaultRequestOptions().setLocationMode(mode); // Set timeout option. Defaults to 5mn. See cloud.azure.storage.timeout or cloud.azure.storage.xxx.timeout try { int timeout = (int) azureStorageSettings.getTimeout().getMillis(); client.getDefaultRequestOptions().setTimeoutIntervalInMs(timeout); } catch (ClassCastException e) { throw new IllegalArgumentException("Can not cast [" + azureStorageSettings.getTimeout() + "] to int."); } return client; }	java || this.secondariesstoragesettings.isempty() :crying_cat_face:
*/ public static void rm(final Path... locations) throws IOException { final LinkedHashMap<Path,Throwable> unremoved = rm(new LinkedHashMap<>(), locations); if (!unremoved.isEmpty()) { final IOException ioException = new IOException("could not remove all files"); for (final Map.Entry<Path,Throwable> kv : unremoved.entrySet()) { ioException.addSuppressed(new IOException(kv.getKey().toAbsolutePath().toString(), kv.getValue())); } throw ioException; } }	can you explain the benefits of this change?
private void closeShardInjector(String reason, ShardId sId, Injector shardInjector, IndexShard indexShard) { final int shardId = sId.id(); try { try { indicesLifecycle.beforeIndexShardClosed(sId, indexShard, indexSettings); } finally { // close everything else even if the beforeIndexShardClosed threw an exception for (Class<? extends Closeable> closeable : pluginsService.shardServices()) { try { shardInjector.getInstance(closeable).close(); } catch (Throwable e) { logger.debug("[{}] failed to clean plugin shard service [{}]", e, shardId, closeable); } } // now we can close the translog service, we need to close it before the we close the shard closeInjectorResource(sId, shardInjector, TranslogService.class); // this logic is tricky, we want to close the engine so we rollback the changes done to it // and close the shard so no operations are allowed to it if (indexShard != null) { try { final boolean flushEngine = deleted.get() == false && closed.get(); // only flush we are we closed (closed index or shutdown) and if we are not deleted indexShard.close(reason, flushEngine); indexShard.waitForEngineClose(); } catch (Throwable e) { logger.debug("[{}] failed to close index shard", e, shardId); // ignore } } closeInjectorResource(sId, shardInjector, MergeSchedulerProvider.class, MergePolicyProvider.class, IndexShardGatewayService.class, Translog.class, PercolatorQueriesRegistry.class); // call this before we close the store, so we can release resources for it indicesLifecycle.afterIndexShardClosed(sId, indexShard, indexSettings); } } finally { try { shardInjector.getInstance(Store.class).close(); } catch (Throwable e) { logger.warn("[{}] failed to close store on shard removal (reason: [{}])", e, shardId, reason); } } }	can we have a unittest that this actually works?
private final EngineConfig newEngineConfig() { final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(mapperService, mapperAnalyzer, queryParserService, indexAliasesService, indexCache) { @Override protected void operationProcessed() { assert recoveryState != null; recoveryState.getTranslog().incrementRecoveredOperations(); } }; return new EngineConfig(shardId, threadPool, indexingService, indexSettingsService, warmer, store, deletionPolicy, translog, mergePolicyProvider, mergeScheduler, mapperAnalyzer, similarityService.similarity(), codecService, failedEngineListener, translogRecoveryPerformer); }	is there a better name than "foo"
private final EngineConfig newEngineConfig() { final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(mapperService, mapperAnalyzer, queryParserService, indexAliasesService, indexCache) { @Override protected void operationProcessed() { assert recoveryState != null; recoveryState.getTranslog().incrementRecoveredOperations(); } }; return new EngineConfig(shardId, threadPool, indexingService, indexSettingsService, warmer, store, deletionPolicy, translog, mergePolicyProvider, mergeScheduler, mapperAnalyzer, similarityService.similarity(), codecService, failedEngineListener, translogRecoveryPerformer); }	this can go away?
public void close(String reason, boolean flushEngine) throws IOException { synchronized (mutex) { try { indexSettingsService.removeListener(applyRefreshSettings); if (state != IndexShardState.CLOSED) { FutureUtils.cancel(refreshScheduledFuture); refreshScheduledFuture = null; FutureUtils.cancel(mergeScheduleFuture); mergeScheduleFuture = null; } changeState(IndexShardState.CLOSED, reason); } finally { // we can set the flushOnClose to this value because the only other thing that would update it is the settings refresh // and we removed the listener for setting changes above this.flushOnClose = (flushEngine && this.flushOnClose); decRef(); } } }	why are we not closing the engine here anymore? i think this is a bug
public void close(String reason, boolean flushEngine) throws IOException { synchronized (mutex) { try { indexSettingsService.removeListener(applyRefreshSettings); if (state != IndexShardState.CLOSED) { FutureUtils.cancel(refreshScheduledFuture); refreshScheduledFuture = null; FutureUtils.cancel(mergeScheduleFuture); mergeScheduleFuture = null; } changeState(IndexShardState.CLOSED, reason); } finally { // we can set the flushOnClose to this value because the only other thing that would update it is the settings refresh // and we removed the listener for setting changes above this.flushOnClose = (flushEngine && this.flushOnClose); decRef(); } } }	this should be public to be useful no? i also think we need javadocs what this does and maybe a timeout?
public Builder setFeatureImportance(List<TotalFeatureImportance> totalFeatureImportance) { if (totalFeatureImportance == null) { return this; } if (this.metadata == null) { this.metadata = new HashMap<>(); } this.metadata.put(TOTAL_FEATURE_IMPORTANCE, totalFeatureImportance.stream().map(TotalFeatureImportance::asMap).collect(Collectors.toList())); return this; }	make a copy of the unmodifiable map here
public void getTrainedModel(final String modelId, final boolean includeDefinition, final boolean includeTotalFeatureImportance, final ActionListener<TrainedModelConfig> finalListener) { if (MODELS_STORED_AS_RESOURCE.contains(modelId)) { try { finalListener.onResponse(loadModelFromResource(modelId, includeDefinition == false).build()); return; } catch (ElasticsearchException ex) { finalListener.onFailure(ex); return; } } ActionListener<TrainedModelConfig.Builder> getTrainedModelListener = ActionListener.wrap( modelBuilder -> { if (includeTotalFeatureImportance == false) { finalListener.onResponse(modelBuilder.build()); return; } this.getTrainedModelMetadata(Collections.singletonList(modelId), ActionListener.wrap( metadata -> { TrainedModelMetadata modelMetadata = metadata.get(modelId); if (modelMetadata != null) { modelBuilder.setFeatureImportance(modelMetadata.getTotalFeatureImportances()); } finalListener.onResponse(modelBuilder.build()); }, failure -> { // total feature importance is not necessary for a model to be valid // we shouldn't fail if it is not found if (ExceptionsHelper.unwrapCause(failure) instanceof ResourceNotFoundException) { finalListener.onResponse(modelBuilder.build()); return; } finalListener.onFailure(failure); } )); }, finalListener::onFailure ); QueryBuilder queryBuilder = QueryBuilders.constantScoreQuery(QueryBuilders .idsQuery() .addIds(modelId)); MultiSearchRequestBuilder multiSearchRequestBuilder = client.prepareMultiSearch() .add(client.prepareSearch(InferenceIndexConstants.INDEX_PATTERN) .setQuery(queryBuilder) // use sort to get the last .addSort("_index", SortOrder.DESC) .setSize(1) .request()); if (includeDefinition) { multiSearchRequestBuilder.add(client.prepareSearch(InferenceIndexConstants.INDEX_PATTERN) .setQuery(QueryBuilders.constantScoreQuery(QueryBuilders .boolQuery() .filter(QueryBuilders.termQuery(TrainedModelConfig.MODEL_ID.getPreferredName(), modelId)) .filter(QueryBuilders.termQuery(InferenceIndexConstants.DOC_TYPE.getPreferredName(), TrainedModelDefinitionDoc.NAME)))) // There should be AT MOST these many docs. There might be more if definitions have been reindex to newer indices // If this ends up getting duplicate groups of definition documents, the parsing logic will throw away any doc that // is in a different index than the first index seen. .setSize(MAX_NUM_DEFINITION_DOCS) // First find the latest index .addSort("_index", SortOrder.DESC) // Then, sort by doc_num .addSort(SortBuilders.fieldSort(TrainedModelDefinitionDoc.DOC_NUM.getPreferredName()) .order(SortOrder.ASC) // We need this for the search not to fail when there are no mappings yet in the index .unmappedType("long")) .request()); } ActionListener<MultiSearchResponse> multiSearchResponseActionListener = ActionListener.wrap( multiSearchResponse -> { TrainedModelConfig.Builder builder; try { builder = handleSearchItem(multiSearchResponse.getResponses()[0], modelId, this::parseInferenceDocLenientlyFromSource); } catch (ResourceNotFoundException ex) { getTrainedModelListener.onFailure(new ResourceNotFoundException( Messages.getMessage(Messages.INFERENCE_NOT_FOUND, modelId))); return; } catch (Exception ex) { getTrainedModelListener.onFailure(ex); return; } if (includeDefinition) { try { List<TrainedModelDefinitionDoc> docs = handleSearchItems(multiSearchResponse.getResponses()[1], modelId, this::parseModelDefinitionDocLenientlyFromSource); try { String compressedString = getDefinitionFromDocs(docs, modelId); builder.setDefinitionFromString(compressedString); } catch (ElasticsearchException elasticsearchException) { getTrainedModelListener.onFailure(elasticsearchException); return; } } catch (ResourceNotFoundException ex) { getTrainedModelListener.onFailure(new ResourceNotFoundException( Messages.getMessage(Messages.MODEL_DEFINITION_NOT_FOUND, modelId))); return; } catch (Exception ex) { getTrainedModelListener.onFailure(ex); return; } } getTrainedModelListener.onResponse(builder); }, getTrainedModelListener::onFailure ); executeAsyncWithOrigin(client, ML_ORIGIN, MultiSearchAction.INSTANCE, multiSearchRequestBuilder.request(), multiSearchResponseActionListener); }	nit: it is clearer if finallistener is used for these onfailure calls.
public void getTrainedModels(Set<String> modelIds, boolean allowNoResources, boolean includeTotalFeatureImportance, final ActionListener<List<TrainedModelConfig>> finalListener) { QueryBuilder queryBuilder = QueryBuilders.constantScoreQuery(QueryBuilders.idsQuery().addIds(modelIds.toArray(new String[0]))); SearchRequest searchRequest = client.prepareSearch(InferenceIndexConstants.INDEX_PATTERN) .addSort(TrainedModelConfig.MODEL_ID.getPreferredName(), SortOrder.ASC) .addSort("_index", SortOrder.DESC) .setQuery(queryBuilder) .setSize(modelIds.size()) .request(); List<TrainedModelConfig.Builder> configs = new ArrayList<>(modelIds.size()); Set<String> modelsInIndex = Sets.difference(modelIds, MODELS_STORED_AS_RESOURCE); Set<String> modelsAsResource = Sets.intersection(MODELS_STORED_AS_RESOURCE, modelIds); for(String modelId : modelsAsResource) { try { configs.add(loadModelFromResource(modelId, true)); } catch (ElasticsearchException ex) { finalListener.onFailure(ex); return; } } if (modelsInIndex.isEmpty()) { finalListener.onResponse(configs.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } ActionListener<List<TrainedModelConfig.Builder>> getTrainedModelListener = ActionListener.wrap( modelBuilders -> { if (includeTotalFeatureImportance == false) { finalListener.onResponse(modelBuilders.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } this.getTrainedModelMetadata(modelIds, ActionListener.wrap( metadata -> finalListener.onResponse(modelBuilders.stream() .map(builder -> { TrainedModelMetadata modelMetadata = metadata.get(builder.getModelId()); if (modelMetadata != null) { builder.setFeatureImportance(modelMetadata.getTotalFeatureImportances()); } return builder.build(); }) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())), failure -> { // total feature importance is not necessary for a model to be valid // we shouldn't fail if it is not found if (ExceptionsHelper.unwrapCause(failure) instanceof ResourceNotFoundException) { finalListener.onResponse(modelBuilders.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } finalListener.onFailure(failure); } )); }, finalListener::onFailure ); ActionListener<SearchResponse> configSearchHandler = ActionListener.wrap( searchResponse -> { Set<String> observedIds = new HashSet<>( searchResponse.getHits().getHits().length + modelsAsResource.size(), 1.0f); observedIds.addAll(modelsAsResource); for(SearchHit searchHit : searchResponse.getHits().getHits()) { try { if (observedIds.contains(searchHit.getId()) == false) { configs.add( parseInferenceDocLenientlyFromSource(searchHit.getSourceRef(), searchHit.getId()) ); observedIds.add(searchHit.getId()); } } catch (IOException ex) { getTrainedModelListener.onFailure( ExceptionsHelper.serverError(INFERENCE_FAILED_TO_DESERIALIZE, ex, searchHit.getId())); return; } } // We previously expanded the IDs. // If the config has gone missing between then and now we should throw if allowNoResources is false // Otherwise, treat it as if it was never expanded to begin with. Set<String> missingConfigs = Sets.difference(modelIds, observedIds); if (missingConfigs.isEmpty() == false && allowNoResources == false) { getTrainedModelListener.onFailure(new ResourceNotFoundException(Messages.INFERENCE_NOT_FOUND_MULTIPLE, missingConfigs)); return; } // Ensure sorted even with the injection of locally resourced models getTrainedModelListener.onResponse(configs); }, getTrainedModelListener::onFailure ); executeAsyncWithOrigin(client, ML_ORIGIN, SearchAction.INSTANCE, searchRequest, configSearchHandler); }	configs are still sorted by the search aren't they? sorting here shouldn't be necessary
public void getTrainedModels(Set<String> modelIds, boolean allowNoResources, boolean includeTotalFeatureImportance, final ActionListener<List<TrainedModelConfig>> finalListener) { QueryBuilder queryBuilder = QueryBuilders.constantScoreQuery(QueryBuilders.idsQuery().addIds(modelIds.toArray(new String[0]))); SearchRequest searchRequest = client.prepareSearch(InferenceIndexConstants.INDEX_PATTERN) .addSort(TrainedModelConfig.MODEL_ID.getPreferredName(), SortOrder.ASC) .addSort("_index", SortOrder.DESC) .setQuery(queryBuilder) .setSize(modelIds.size()) .request(); List<TrainedModelConfig.Builder> configs = new ArrayList<>(modelIds.size()); Set<String> modelsInIndex = Sets.difference(modelIds, MODELS_STORED_AS_RESOURCE); Set<String> modelsAsResource = Sets.intersection(MODELS_STORED_AS_RESOURCE, modelIds); for(String modelId : modelsAsResource) { try { configs.add(loadModelFromResource(modelId, true)); } catch (ElasticsearchException ex) { finalListener.onFailure(ex); return; } } if (modelsInIndex.isEmpty()) { finalListener.onResponse(configs.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } ActionListener<List<TrainedModelConfig.Builder>> getTrainedModelListener = ActionListener.wrap( modelBuilders -> { if (includeTotalFeatureImportance == false) { finalListener.onResponse(modelBuilders.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } this.getTrainedModelMetadata(modelIds, ActionListener.wrap( metadata -> finalListener.onResponse(modelBuilders.stream() .map(builder -> { TrainedModelMetadata modelMetadata = metadata.get(builder.getModelId()); if (modelMetadata != null) { builder.setFeatureImportance(modelMetadata.getTotalFeatureImportances()); } return builder.build(); }) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())), failure -> { // total feature importance is not necessary for a model to be valid // we shouldn't fail if it is not found if (ExceptionsHelper.unwrapCause(failure) instanceof ResourceNotFoundException) { finalListener.onResponse(modelBuilders.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } finalListener.onFailure(failure); } )); }, finalListener::onFailure ); ActionListener<SearchResponse> configSearchHandler = ActionListener.wrap( searchResponse -> { Set<String> observedIds = new HashSet<>( searchResponse.getHits().getHits().length + modelsAsResource.size(), 1.0f); observedIds.addAll(modelsAsResource); for(SearchHit searchHit : searchResponse.getHits().getHits()) { try { if (observedIds.contains(searchHit.getId()) == false) { configs.add( parseInferenceDocLenientlyFromSource(searchHit.getSourceRef(), searchHit.getId()) ); observedIds.add(searchHit.getId()); } } catch (IOException ex) { getTrainedModelListener.onFailure( ExceptionsHelper.serverError(INFERENCE_FAILED_TO_DESERIALIZE, ex, searchHit.getId())); return; } } // We previously expanded the IDs. // If the config has gone missing between then and now we should throw if allowNoResources is false // Otherwise, treat it as if it was never expanded to begin with. Set<String> missingConfigs = Sets.difference(modelIds, observedIds); if (missingConfigs.isEmpty() == false && allowNoResources == false) { getTrainedModelListener.onFailure(new ResourceNotFoundException(Messages.INFERENCE_NOT_FOUND_MULTIPLE, missingConfigs)); return; } // Ensure sorted even with the injection of locally resourced models getTrainedModelListener.onResponse(configs); }, getTrainedModelListener::onFailure ); executeAsyncWithOrigin(client, ML_ORIGIN, SearchAction.INSTANCE, searchRequest, configSearchHandler); }	makes sense is it the case that newer models after version 7.10 will always have the total feature importance?
public void getTrainedModels(Set<String> modelIds, boolean allowNoResources, boolean includeTotalFeatureImportance, final ActionListener<List<TrainedModelConfig>> finalListener) { QueryBuilder queryBuilder = QueryBuilders.constantScoreQuery(QueryBuilders.idsQuery().addIds(modelIds.toArray(new String[0]))); SearchRequest searchRequest = client.prepareSearch(InferenceIndexConstants.INDEX_PATTERN) .addSort(TrainedModelConfig.MODEL_ID.getPreferredName(), SortOrder.ASC) .addSort("_index", SortOrder.DESC) .setQuery(queryBuilder) .setSize(modelIds.size()) .request(); List<TrainedModelConfig.Builder> configs = new ArrayList<>(modelIds.size()); Set<String> modelsInIndex = Sets.difference(modelIds, MODELS_STORED_AS_RESOURCE); Set<String> modelsAsResource = Sets.intersection(MODELS_STORED_AS_RESOURCE, modelIds); for(String modelId : modelsAsResource) { try { configs.add(loadModelFromResource(modelId, true)); } catch (ElasticsearchException ex) { finalListener.onFailure(ex); return; } } if (modelsInIndex.isEmpty()) { finalListener.onResponse(configs.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } ActionListener<List<TrainedModelConfig.Builder>> getTrainedModelListener = ActionListener.wrap( modelBuilders -> { if (includeTotalFeatureImportance == false) { finalListener.onResponse(modelBuilders.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } this.getTrainedModelMetadata(modelIds, ActionListener.wrap( metadata -> finalListener.onResponse(modelBuilders.stream() .map(builder -> { TrainedModelMetadata modelMetadata = metadata.get(builder.getModelId()); if (modelMetadata != null) { builder.setFeatureImportance(modelMetadata.getTotalFeatureImportances()); } return builder.build(); }) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())), failure -> { // total feature importance is not necessary for a model to be valid // we shouldn't fail if it is not found if (ExceptionsHelper.unwrapCause(failure) instanceof ResourceNotFoundException) { finalListener.onResponse(modelBuilders.stream() .map(TrainedModelConfig.Builder::build) .sorted(Comparator.comparing(TrainedModelConfig::getModelId)) .collect(Collectors.toList())); return; } finalListener.onFailure(failure); } )); }, finalListener::onFailure ); ActionListener<SearchResponse> configSearchHandler = ActionListener.wrap( searchResponse -> { Set<String> observedIds = new HashSet<>( searchResponse.getHits().getHits().length + modelsAsResource.size(), 1.0f); observedIds.addAll(modelsAsResource); for(SearchHit searchHit : searchResponse.getHits().getHits()) { try { if (observedIds.contains(searchHit.getId()) == false) { configs.add( parseInferenceDocLenientlyFromSource(searchHit.getSourceRef(), searchHit.getId()) ); observedIds.add(searchHit.getId()); } } catch (IOException ex) { getTrainedModelListener.onFailure( ExceptionsHelper.serverError(INFERENCE_FAILED_TO_DESERIALIZE, ex, searchHit.getId())); return; } } // We previously expanded the IDs. // If the config has gone missing between then and now we should throw if allowNoResources is false // Otherwise, treat it as if it was never expanded to begin with. Set<String> missingConfigs = Sets.difference(modelIds, observedIds); if (missingConfigs.isEmpty() == false && allowNoResources == false) { getTrainedModelListener.onFailure(new ResourceNotFoundException(Messages.INFERENCE_NOT_FOUND_MULTIPLE, missingConfigs)); return; } // Ensure sorted even with the injection of locally resourced models getTrainedModelListener.onResponse(configs); }, getTrainedModelListener::onFailure ); executeAsyncWithOrigin(client, ML_ORIGIN, SearchAction.INSTANCE, searchRequest, configSearchHandler); }	nit: delete extra blank line
private void setMaxShingleDiff(int maxShingleDiff) { this.maxShingleDiff = maxShingleDiff; }	same here, gethighlightmaxanalyzedoffset ?
@Override public void prepareSelectedBuckets(long... selectedBuckets) throws IOException { if (finished == false) { throw new IllegalStateException("Cannot replay yet, collection is not finished: postCollect() has not been called"); } if (this.selectedBuckets != null) { throw new IllegalStateException("Already been replayed"); } final LongHash hash = new LongHash(selectedBuckets.length, BigArrays.NON_RECYCLING_INSTANCE); for (long bucket : selectedBuckets) { hash.add(bucket); } this.selectedBuckets = hash; boolean needsScores = scoreMode().needsScores(); Weight weight = null; if (needsScores) { Query query = isGlobal ? new MatchAllDocsQuery() : searchContext.query(); weight = searchContext.searcher().createWeight(searchContext.searcher().rewrite(query), ScoreMode.COMPLETE, 1f); } for (Entry entry : entries) { final LeafBucketCollector leafCollector = collector.getLeafCollector(entry.context); DocIdSetIterator scoreIt = null; if (needsScores) { Scorer scorer = weight.scorer(entry.context); // We don't need to check if the scorer is null // since we are sure that there are documents to replay (entry.docDeltas it not empty). scoreIt = scorer.iterator(); leafCollector.setScorer(scorer); } final PackedLongValues.Iterator docDeltaIterator = entry.docDeltas.iterator(); final PackedLongValues.Iterator buckets = entry.buckets.iterator(); int doc = 0; for (long i = 0, end = entry.docDeltas.size(); i < end; ++i) { doc += docDeltaIterator.next(); final long bucket = buckets.next(); final long rebasedBucket = hash.find(bucket); if (rebasedBucket != -1) { if (needsScores) { if (scoreIt.docID() < doc) { scoreIt.advance(doc); } // aggregations should only be replayed on matching documents assert scoreIt.docID() == doc; } leafCollector.collect(doc, rebasedBucket); } } } collector.postCollection(); }	i'm assuming that since we're lazily creating these structures, we should never have an entry with empty docdeltasbuilder? should we put an assert here to make sure?
private void handleSkipTerms(XMoreLikeThis mlt, String[] unlikeText, Fields[] unlikeFields) throws IOException { Set<Term> skipTerms = new HashSet<>(); // handle like text if (unlikeText != null) { for (String text : unlikeText) { // only use the first field to be consistent String fieldName = moreLikeFields[0]; try (TokenStream ts = analyzer.tokenStream(fieldName, new FastStringReader(text))) { CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class); ts.reset(); while (ts.incrementToken()) { skipTerms.add(new Term(fieldName, termAtt.toString())); } ts.end(); } } } // handle like fields if (unlikeFields != null) { for (Fields fields : unlikeFields) { for (String fieldName : fields) { Terms terms = fields.terms(fieldName); final TermsEnum termsEnum = terms.iterator(null); BytesRef text; while ((text = termsEnum.next()) != null) { skipTerms.add(new Term(fieldName, text.utf8ToString())); } } } } if (!skipTerms.isEmpty()) { mlt.setSkipTerms(skipTerms); } }	no need to wrap into a faststringreader, there is already analyzer.tokenstream(string,string)?
public MoreLikeThisQueryBuilder unlike(String... likeText) { this.unlike_docs = new ArrayList<>(); for (String text : likeText) { this.unlike_docs.add(new Item(text)); } return this; }	can you add javadocs since it is a public api?
@Override public void handleException(TransportException exp) { if (isClosed.get()) { logger.debug("closed check scheduler received a response, doing nothing"); return; } if (exp instanceof ConnectTransportException || exp.getCause() instanceof ConnectTransportException) { logger.debug(new ParameterizedMessage( "leader [{}] disconnected during check, restarting discovery", leader), exp); leaderFailed(new ConnectTransportException(leader, "disconnected during check", exp)); return; } long failureCount = failureCountSinceLastSuccess.incrementAndGet(); if (failureCount >= leaderCheckRetryCount) { logger.debug(new ParameterizedMessage( "leader [{}] has failed {} consecutive checks (limit [{}] is {}), restarting discovery; last failure was:", leader, failureCount, LEADER_CHECK_RETRY_COUNT_SETTING.getKey(), leaderCheckRetryCount), exp); leaderFailed(new ElasticsearchException( "node [" + leader + "] failed [" + failureCount + "] consecutive checks", exp)); return; } logger.debug(new ParameterizedMessage("{} consecutive failures (limit [{}] is {}) with leader [{}]", failureCount, LEADER_CHECK_RETRY_COUNT_SETTING.getKey(), leaderCheckRetryCount, leader), exp); scheduleNextWakeUp(); }	restarting discovery does not belong to a message in this class (it assumes stuff about calling context). perhaps just leave that part out
static void registerAggregators(ValuesSourceRegistry valuesSourceRegistry) { valuesSourceRegistry.register(BoxplotAggregationBuilder.NAME, CoreValuesSourceType.NUMERIC, new BoxplotAggregatorSupplier() { @Override public Aggregator build(String name, ValuesSource valuesSource, DocValueFormat formatter, double compression, SearchContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException { return new BoxplotAggregator(name, valuesSource, formatter, compression, context, parent, pipelineAggregators, metaData); } }); }	@talevy showed me that in cases like this, we can use a lambda with a type cast. since boxplotaggregator already just takes a valuessource directly (as opposed to needing a cast), i think we can just say (boxplotaggregationsupplier) boxplotaggregator::new here. we'll also need to add the @functionalinterface annotation to boxplotaggregationsupplier.
*/ static Settings aggregateIndexSettings(ClusterState currentState, CreateIndexClusterStateUpdateRequest request, Settings combinedTemplateSettings, @Nullable IndexMetadata sourceMetadata, Settings settings, IndexScopedSettings indexScopedSettings, ShardLimitValidator shardLimitValidator, Set<ExplicitIndexSettingProvider> explicitIndexSettingProviders) { // Create builders for the template and request settings. We transform these into builders // because we may want settings to be "removed" from these prior to being set on the new // index (see more comments below) final Settings.Builder templateSettings = Settings.builder().put(combinedTemplateSettings); final Settings.Builder requestSettings = Settings.builder().put(request.settings()); final Settings.Builder indexSettingsBuilder = Settings.builder(); if (sourceMetadata == null) { final Settings.Builder explicitDefaultSettings = Settings.builder(); final Settings templateAndRequestSettings = Settings.builder() .put(combinedTemplateSettings) .put(request.settings()) .build(); // Loop through all the explicit index setting providers, adding them to the // explicitDefaultSettings map for (ExplicitIndexSettingProvider listener : explicitIndexSettingProviders) { try { explicitDefaultSettings.put(listener.getExplicitIndexSettings(request.index(), templateAndRequestSettings)); } catch (Exception e) { logger.warn(new ParameterizedMessage("failed invoking explicit setting provider for creation of [{}] index", request.index()), e); } } // For all the explicit settings, we go through the template and request level settings // and see if either a template or the request has "cancelled out" an explicit default // setting. For example, if a plugin had as an explicit setting: // "index.mysetting": "blah // And either a template or create index request had: // "index.mysetting": null // We want to remove the explicit setting not only from the explicitly set settings, but // also from the template and request settings, so that from the newly create index's // perspective it is as though the setting has not been set at all (using the default // value). for (String explicitSetting : explicitDefaultSettings.keys()) { if (templateSettings.keys().contains(explicitSetting) && templateSettings.get(explicitSetting) == null) { explicitDefaultSettings.remove(explicitSetting); templateSettings.remove(explicitSetting); } if (requestSettings.keys().contains(explicitSetting) && requestSettings.get(explicitSetting) == null) { explicitDefaultSettings.remove(explicitSetting); requestSettings.remove(explicitSetting); } } // Finally, we actually add the explicit defaults prior to the template settings and the // request settings, so that the precedence goes: // Explicit Defaults -> Template -> Request -> Necessary Settings (# of shards, uuid, etc) indexSettingsBuilder.put(explicitDefaultSettings.build()); indexSettingsBuilder.put(templateSettings.build()); } // now, put the request settings, so they override templates indexSettingsBuilder.put(requestSettings.build()); if (indexSettingsBuilder.get(IndexMetadata.SETTING_INDEX_VERSION_CREATED.getKey()) == null) { final DiscoveryNodes nodes = currentState.nodes(); final Version createdVersion = Version.min(Version.CURRENT, nodes.getSmallestNonClientNodeVersion()); indexSettingsBuilder.put(IndexMetadata.SETTING_INDEX_VERSION_CREATED.getKey(), createdVersion); } if (INDEX_NUMBER_OF_SHARDS_SETTING.exists(indexSettingsBuilder) == false) { indexSettingsBuilder.put(SETTING_NUMBER_OF_SHARDS, INDEX_NUMBER_OF_SHARDS_SETTING.get(settings)); } if (INDEX_NUMBER_OF_REPLICAS_SETTING.exists(indexSettingsBuilder) == false) { indexSettingsBuilder.put(SETTING_NUMBER_OF_REPLICAS, INDEX_NUMBER_OF_REPLICAS_SETTING.get(settings)); } if (settings.get(SETTING_AUTO_EXPAND_REPLICAS) != null && indexSettingsBuilder.get(SETTING_AUTO_EXPAND_REPLICAS) == null) { indexSettingsBuilder.put(SETTING_AUTO_EXPAND_REPLICAS, settings.get(SETTING_AUTO_EXPAND_REPLICAS)); } if (indexSettingsBuilder.get(SETTING_CREATION_DATE) == null) { indexSettingsBuilder.put(SETTING_CREATION_DATE, Instant.now().toEpochMilli()); } indexSettingsBuilder.put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, request.getProvidedName()); indexSettingsBuilder.put(SETTING_INDEX_UUID, UUIDs.randomBase64UUID()); if (sourceMetadata != null) { assert request.resizeType() != null; prepareResizeIndexSettings( currentState, indexSettingsBuilder, request.recoverFrom(), request.index(), request.resizeType(), request.copySettings(), indexScopedSettings); } Settings indexSettings = indexSettingsBuilder.build(); /* * We can not validate settings until we have applied templates, otherwise we do not know the actual settings * that will be used to create this index. */ shardLimitValidator.validateShardLimit(indexSettings, currentState); validateSoftDeleteSettings(indexSettings); validateTranslogRetentionSettings(indexSettings); return indexSettings; }	if we expected exceptions here, we should document it on the interface. however, i don't think we should. i think the contract should be that a provider provides all valid settings, or settings.empty if none can be satisfied. removing the catch here and any exception thrown by a provider impl would result in a 500 and no index created, which i think is slightly better then settings that _should_ have been applied but were not.
*/ static Settings aggregateIndexSettings(ClusterState currentState, CreateIndexClusterStateUpdateRequest request, Settings combinedTemplateSettings, @Nullable IndexMetadata sourceMetadata, Settings settings, IndexScopedSettings indexScopedSettings, ShardLimitValidator shardLimitValidator, Set<ExplicitIndexSettingProvider> explicitIndexSettingProviders) { // Create builders for the template and request settings. We transform these into builders // because we may want settings to be "removed" from these prior to being set on the new // index (see more comments below) final Settings.Builder templateSettings = Settings.builder().put(combinedTemplateSettings); final Settings.Builder requestSettings = Settings.builder().put(request.settings()); final Settings.Builder indexSettingsBuilder = Settings.builder(); if (sourceMetadata == null) { final Settings.Builder explicitDefaultSettings = Settings.builder(); final Settings templateAndRequestSettings = Settings.builder() .put(combinedTemplateSettings) .put(request.settings()) .build(); // Loop through all the explicit index setting providers, adding them to the // explicitDefaultSettings map for (ExplicitIndexSettingProvider listener : explicitIndexSettingProviders) { try { explicitDefaultSettings.put(listener.getExplicitIndexSettings(request.index(), templateAndRequestSettings)); } catch (Exception e) { logger.warn(new ParameterizedMessage("failed invoking explicit setting provider for creation of [{}] index", request.index()), e); } } // For all the explicit settings, we go through the template and request level settings // and see if either a template or the request has "cancelled out" an explicit default // setting. For example, if a plugin had as an explicit setting: // "index.mysetting": "blah // And either a template or create index request had: // "index.mysetting": null // We want to remove the explicit setting not only from the explicitly set settings, but // also from the template and request settings, so that from the newly create index's // perspective it is as though the setting has not been set at all (using the default // value). for (String explicitSetting : explicitDefaultSettings.keys()) { if (templateSettings.keys().contains(explicitSetting) && templateSettings.get(explicitSetting) == null) { explicitDefaultSettings.remove(explicitSetting); templateSettings.remove(explicitSetting); } if (requestSettings.keys().contains(explicitSetting) && requestSettings.get(explicitSetting) == null) { explicitDefaultSettings.remove(explicitSetting); requestSettings.remove(explicitSetting); } } // Finally, we actually add the explicit defaults prior to the template settings and the // request settings, so that the precedence goes: // Explicit Defaults -> Template -> Request -> Necessary Settings (# of shards, uuid, etc) indexSettingsBuilder.put(explicitDefaultSettings.build()); indexSettingsBuilder.put(templateSettings.build()); } // now, put the request settings, so they override templates indexSettingsBuilder.put(requestSettings.build()); if (indexSettingsBuilder.get(IndexMetadata.SETTING_INDEX_VERSION_CREATED.getKey()) == null) { final DiscoveryNodes nodes = currentState.nodes(); final Version createdVersion = Version.min(Version.CURRENT, nodes.getSmallestNonClientNodeVersion()); indexSettingsBuilder.put(IndexMetadata.SETTING_INDEX_VERSION_CREATED.getKey(), createdVersion); } if (INDEX_NUMBER_OF_SHARDS_SETTING.exists(indexSettingsBuilder) == false) { indexSettingsBuilder.put(SETTING_NUMBER_OF_SHARDS, INDEX_NUMBER_OF_SHARDS_SETTING.get(settings)); } if (INDEX_NUMBER_OF_REPLICAS_SETTING.exists(indexSettingsBuilder) == false) { indexSettingsBuilder.put(SETTING_NUMBER_OF_REPLICAS, INDEX_NUMBER_OF_REPLICAS_SETTING.get(settings)); } if (settings.get(SETTING_AUTO_EXPAND_REPLICAS) != null && indexSettingsBuilder.get(SETTING_AUTO_EXPAND_REPLICAS) == null) { indexSettingsBuilder.put(SETTING_AUTO_EXPAND_REPLICAS, settings.get(SETTING_AUTO_EXPAND_REPLICAS)); } if (indexSettingsBuilder.get(SETTING_CREATION_DATE) == null) { indexSettingsBuilder.put(SETTING_CREATION_DATE, Instant.now().toEpochMilli()); } indexSettingsBuilder.put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, request.getProvidedName()); indexSettingsBuilder.put(SETTING_INDEX_UUID, UUIDs.randomBase64UUID()); if (sourceMetadata != null) { assert request.resizeType() != null; prepareResizeIndexSettings( currentState, indexSettingsBuilder, request.recoverFrom(), request.index(), request.resizeType(), request.copySettings(), indexScopedSettings); } Settings indexSettings = indexSettingsBuilder.build(); /* * We can not validate settings until we have applied templates, otherwise we do not know the actual settings * that will be used to create this index. */ shardLimitValidator.validateShardLimit(indexSettings, currentState); validateSoftDeleteSettings(indexSettings); validateTranslogRetentionSettings(indexSettings); return indexSettings; }	mabye a debug message here to help with future troubleshooting ?
*/ static Settings aggregateIndexSettings(ClusterState currentState, CreateIndexClusterStateUpdateRequest request, Settings combinedTemplateSettings, @Nullable IndexMetadata sourceMetadata, Settings settings, IndexScopedSettings indexScopedSettings, ShardLimitValidator shardLimitValidator, Set<ExplicitIndexSettingProvider> explicitIndexSettingProviders) { // Create builders for the template and request settings. We transform these into builders // because we may want settings to be "removed" from these prior to being set on the new // index (see more comments below) final Settings.Builder templateSettings = Settings.builder().put(combinedTemplateSettings); final Settings.Builder requestSettings = Settings.builder().put(request.settings()); final Settings.Builder indexSettingsBuilder = Settings.builder(); if (sourceMetadata == null) { final Settings.Builder explicitDefaultSettings = Settings.builder(); final Settings templateAndRequestSettings = Settings.builder() .put(combinedTemplateSettings) .put(request.settings()) .build(); // Loop through all the explicit index setting providers, adding them to the // explicitDefaultSettings map for (ExplicitIndexSettingProvider listener : explicitIndexSettingProviders) { try { explicitDefaultSettings.put(listener.getExplicitIndexSettings(request.index(), templateAndRequestSettings)); } catch (Exception e) { logger.warn(new ParameterizedMessage("failed invoking explicit setting provider for creation of [{}] index", request.index()), e); } } // For all the explicit settings, we go through the template and request level settings // and see if either a template or the request has "cancelled out" an explicit default // setting. For example, if a plugin had as an explicit setting: // "index.mysetting": "blah // And either a template or create index request had: // "index.mysetting": null // We want to remove the explicit setting not only from the explicitly set settings, but // also from the template and request settings, so that from the newly create index's // perspective it is as though the setting has not been set at all (using the default // value). for (String explicitSetting : explicitDefaultSettings.keys()) { if (templateSettings.keys().contains(explicitSetting) && templateSettings.get(explicitSetting) == null) { explicitDefaultSettings.remove(explicitSetting); templateSettings.remove(explicitSetting); } if (requestSettings.keys().contains(explicitSetting) && requestSettings.get(explicitSetting) == null) { explicitDefaultSettings.remove(explicitSetting); requestSettings.remove(explicitSetting); } } // Finally, we actually add the explicit defaults prior to the template settings and the // request settings, so that the precedence goes: // Explicit Defaults -> Template -> Request -> Necessary Settings (# of shards, uuid, etc) indexSettingsBuilder.put(explicitDefaultSettings.build()); indexSettingsBuilder.put(templateSettings.build()); } // now, put the request settings, so they override templates indexSettingsBuilder.put(requestSettings.build()); if (indexSettingsBuilder.get(IndexMetadata.SETTING_INDEX_VERSION_CREATED.getKey()) == null) { final DiscoveryNodes nodes = currentState.nodes(); final Version createdVersion = Version.min(Version.CURRENT, nodes.getSmallestNonClientNodeVersion()); indexSettingsBuilder.put(IndexMetadata.SETTING_INDEX_VERSION_CREATED.getKey(), createdVersion); } if (INDEX_NUMBER_OF_SHARDS_SETTING.exists(indexSettingsBuilder) == false) { indexSettingsBuilder.put(SETTING_NUMBER_OF_SHARDS, INDEX_NUMBER_OF_SHARDS_SETTING.get(settings)); } if (INDEX_NUMBER_OF_REPLICAS_SETTING.exists(indexSettingsBuilder) == false) { indexSettingsBuilder.put(SETTING_NUMBER_OF_REPLICAS, INDEX_NUMBER_OF_REPLICAS_SETTING.get(settings)); } if (settings.get(SETTING_AUTO_EXPAND_REPLICAS) != null && indexSettingsBuilder.get(SETTING_AUTO_EXPAND_REPLICAS) == null) { indexSettingsBuilder.put(SETTING_AUTO_EXPAND_REPLICAS, settings.get(SETTING_AUTO_EXPAND_REPLICAS)); } if (indexSettingsBuilder.get(SETTING_CREATION_DATE) == null) { indexSettingsBuilder.put(SETTING_CREATION_DATE, Instant.now().toEpochMilli()); } indexSettingsBuilder.put(IndexMetadata.SETTING_INDEX_PROVIDED_NAME, request.getProvidedName()); indexSettingsBuilder.put(SETTING_INDEX_UUID, UUIDs.randomBase64UUID()); if (sourceMetadata != null) { assert request.resizeType() != null; prepareResizeIndexSettings( currentState, indexSettingsBuilder, request.recoverFrom(), request.index(), request.resizeType(), request.copySettings(), indexScopedSettings); } Settings indexSettings = indexSettingsBuilder.build(); /* * We can not validate settings until we have applied templates, otherwise we do not know the actual settings * that will be used to create this index. */ shardLimitValidator.validateShardLimit(indexSettings, currentState); validateSoftDeleteSettings(indexSettings); validateTranslogRetentionSettings(indexSettings); return indexSettings; }	mabye a debug message here to help with future troubleshooting ?
public static boolean isFrozenNode(DiscoveryNode discoveryNode) { return discoveryNode.getRoles().contains(DATA_FROZEN_NODE_ROLE) || discoveryNode.getRoles().contains(DiscoveryNodeRole.DATA_ROLE); } /** * This listener injects the setting allocating all newly created indices with * {@code index.routing.allocation.include._tier: "data_hot"}	nit: the listener ? not sure i follow what that is in reference to..
public RiverClusterState execute(RiverClusterState currentState) { return updateRiverClusterState(source, currentState, event.state(), countDown); }	why is this method synced? i don't see a reason though...
private void setFlushAfterMergeThresholdSize(ByteSizeValue byteSizeValue) { this.flushAfterMergeThresholdSize = byteSizeValue; }	should we keep this kind of logic if index_soft_deletes_setting.get(settings) && indexmetadata.setting_index_version_created.get(settings).onorafter(version.v_7_4_0)? might be nice.
public synchronized boolean hasAllPeerRecoveryRetentionLeases() { return hasAllPeerRecoveryRetentionLeases; } /** * Create any required peer-recovery retention leases that do not currently exist because we just did a rolling upgrade from a version * prior to {@link Version#V_7_4_0}	currently, hasallpeerrecoveryretentionleases is initially defined as this.hasallpeerrecoveryretentionleases = indexsettings.getindexversioncreated().onorafter(version.v_7_4_0) i guess we need to add && softdeletesenabled?
@Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } Request other = (Request) obj; return Objects.equals(id, other.id) && Objects.equals(pageParams, other.pageParams); } } public static class Response extends BaseTasksResponse implements ToXContentObject { private List<DataFrameTransformStateAndStats> transformsStateAndStats; private long totalCount; public Response(List<DataFrameTransformStateAndStats> transformsStateAndStats) { super(Collections.emptyList(), Collections.emptyList()); this.transformsStateAndStats = transformsStateAndStats; } public Response(List<DataFrameTransformStateAndStats> transformsStateAndStats, List<TaskOperationFailure> taskFailures, List<? extends ElasticsearchException> nodeFailures) { super(taskFailures, nodeFailures); this.transformsStateAndStats = transformsStateAndStats; } public Response(StreamInput in) throws IOException { super(in); transformsStateAndStats = in.readList(DataFrameTransformStateAndStats::new); if (in.getVersion().onOrAfter(Version.CURRENT)) { totalCount = in.readLong(); } else { totalCount = transformsStateAndStats.size(); } } public Response setTotalCount(long totalCount) { this.totalCount = totalCount; return this; } public List<DataFrameTransformStateAndStats> getTransformsStateAndStats() { return transformsStateAndStats; } public long getTotalCount() { return totalCount; } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeList(transformsStateAndStats); if (out.getVersion().onOrAfter(Version.CURRENT)) { out.writeLong(totalCount); } } @Override public void readFrom(StreamInput in) { throw new UnsupportedOperationException("usage of Streamable is to be replaced by Writeable"); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); toXContentCommon(builder, params); builder.field(DataFrameField.COUNT.getPreferredName(), totalCount == 0 ? transformsStateAndStats.size() : totalCount); builder.field(DataFrameField.TRANSFORMS.getPreferredName(), transformsStateAndStats); builder.endObject(); return builder; } @Override public int hashCode() { return Objects.hash(transformsStateAndStats, totalCount); } @Override public boolean equals(Object other) { if (this == other) { return true; } if (other == null || getClass() != other.getClass()) { return false; } final Response that = (Response) other; return Objects.equals(this.transformsStateAndStats, that.transformsStateAndStats) && this.totalCount == that.totalCount; } @Override public final String toString() { return Strings.toString(this); }	how does this work when you don't provide a concrete version (e.g. 7.3.0) but "current"?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); toXContentCommon(builder, params); builder.field(DataFrameField.COUNT.getPreferredName(), totalCount == 0 ? transformsStateAndStats.size() : totalCount); builder.field(DataFrameField.TRANSFORMS.getPreferredName(), transformsStateAndStats); builder.endObject(); return builder; }	can "0" be a valid value of totalcount? if so, totalcount should likely be nullable.
protected void doExecute(Task task, StopDataFrameTransformAction.Request request, ActionListener<StopDataFrameTransformAction.Response> listener) { final ClusterState state = clusterService.state(); final DiscoveryNodes nodes = state.nodes(); if (nodes.isLocalNodeElectedMaster() == false) { // Delegates stop data frame to elected master node so it becomes the coordinating node. if (nodes.getMasterNode() == null) { listener.onFailure(new MasterNotDiscoveredException("no known master node")); } else { transportService.sendRequest(nodes.getMasterNode(), actionName, request, new ActionListenerResponseHandler<>(listener, StopDataFrameTransformAction.Response::new)); } } else { final ActionListener<StopDataFrameTransformAction.Response> finalListener; if (request.waitForCompletion()) { finalListener = waitForStopListener(request, listener); } else { finalListener = listener; } dataFrameTransformsConfigManager.expandTransformIds(request.getId(), new PageParams(0, 10_000), ActionListener.wrap( expandedIdsAndHits -> { request.setExpandedIds(new HashSet<>(expandedIdsAndHits.v2())); request.setNodes(DataFrameNodes.dataFrameTaskNodes(expandedIdsAndHits.v2(), clusterService.state())); super.doExecute(task, request, finalListener); }, listener::onFailure )); } }	s/expandedidsandhits/hitsandexpandedids/ to make variable name indicate the order of tuple elements?
protected void randomFieldOrScript(ValuesSourceAggregationBuilder<?, ?> factory, String field) { int choice = randomInt(2); switch (choice) { case 0: factory.field(field); break; case 1: factory.field(field); factory.script(mockScript("_value + 1")); break; case 2: factory.script(mockScript("doc[" + field + "] + 1")); break; default: throw new UnsupportedOperationException("Unknow random operation [" + choice + "]"); } }	i would make this an assertionerror, it should be impossible.
private void assertHighlightOneDoc(String fieldName, String []markedUpInputs, Query query, Locale locale, BreakIterator breakIterator, int noMatchSize, String[] expectedPassages, int maxAnalyzedOffset, boolean limitToMaxAnalyzedOffset) throws Exception { Directory dir = null; DirectoryReader reader = null; try { dir = newDirectory(); // Annotated fields wrap the usual analyzer with one that injects extra tokens Analyzer wrapperAnalyzer = new AnnotationAnalyzerWrapper(new StandardAnalyzer()); ; IndexWriterConfig iwc = newIndexWriterConfig(wrapperAnalyzer); iwc.setMergePolicy(newTieredMergePolicy(random())); RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc); FieldType ft = new FieldType(TextField.TYPE_STORED); if (randomBoolean()) { ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); } else { ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); } ft.freeze(); Document doc = new Document(); for (String input : markedUpInputs) { Field field = new Field(fieldName, "", ft); field.setStringValue(input); doc.add(field); } iw.addDocument(doc); reader = iw.getReader(); IndexSearcher searcher = newSearcher(reader); iw.close(); AnnotatedText[] annotations = new AnnotatedText[markedUpInputs.length]; for (int i = 0; i < markedUpInputs.length; i++) { annotations[i] = AnnotatedText.parse(markedUpInputs[i]); } AnnotatedHighlighterAnalyzer hiliteAnalyzer = new AnnotatedHighlighterAnalyzer(wrapperAnalyzer); hiliteAnalyzer.setAnnotations(annotations); AnnotatedPassageFormatter passageFormatter = new AnnotatedPassageFormatter(new DefaultEncoder()); passageFormatter.setAnnotations(annotations); ArrayList<Object> plainTextForHighlighter = new ArrayList<>(annotations.length); for (int i = 0; i < annotations.length; i++) { plainTextForHighlighter.add(annotations[i].textMinusMarkup); } TopDocs topDocs = searcher.search(new MatchAllDocsQuery(), 1, Sort.INDEXORDER); assertThat(topDocs.totalHits.value, equalTo(1L)); String rawValue = Strings.collectionToDelimitedString(plainTextForHighlighter, String.valueOf(MULTIVAL_SEP_CHAR)); CustomUnifiedHighlighter highlighter = new CustomUnifiedHighlighter( searcher, hiliteAnalyzer, UnifiedHighlighter.OffsetSource.ANALYSIS, passageFormatter, locale, breakIterator, "index", "text", query, noMatchSize, expectedPassages.length, name -> "text".equals(name), maxAnalyzedOffset, limitToMaxAnalyzedOffset ); highlighter.setFieldMatcher((name) -> "text".equals(name)); final Snippet[] snippets = highlighter.highlightField(getOnlyLeafReader(reader), topDocs.scoreDocs[0].doc, () -> rawValue); assertEquals(expectedPassages.length, snippets.length); for (int i = 0; i < snippets.length; i++) { assertEquals(expectedPassages[i], snippets[i].getText()); } } finally { if (reader != null) { reader.close(); } if (dir != null) { dir.close(); } } }	you can use try-with-resources here i think?
public final void writeTo(StreamOutput out) throws IOException { out.writeOptionalStringArray(preTags); out.writeOptionalStringArray(postTags); out.writeOptionalVInt(fragmentSize); out.writeOptionalVInt(numOfFragments); out.writeOptionalString(highlighterType); out.writeOptionalString(fragmenter); boolean hasQuery = highlightQuery != null; out.writeBoolean(hasQuery); if (hasQuery) { out.writeNamedWriteable(highlightQuery); } out.writeOptionalWriteable(order); out.writeOptionalBoolean(highlightFilter); out.writeOptionalBoolean(forceSource); out.writeOptionalWriteable(boundaryScannerType); out.writeOptionalVInt(boundaryMaxScan); boolean hasBounaryChars = boundaryChars != null; out.writeBoolean(hasBounaryChars); if (hasBounaryChars) { out.writeString(String.valueOf(boundaryChars)); } boolean hasBoundaryScannerLocale = boundaryScannerLocale != null; out.writeBoolean(hasBoundaryScannerLocale); if (hasBoundaryScannerLocale) { out.writeString(boundaryScannerLocale.toLanguageTag()); } out.writeOptionalVInt(noMatchSize); out.writeOptionalVInt(phraseLimit); boolean hasOptions = options != null; out.writeBoolean(hasOptions); if (hasOptions) { out.writeMap(options); } out.writeOptionalBoolean(requireFieldMatch); out.writeOptionalBoolean(limitToMaxAnalyzedOffset); doWriteTo(out); }	you'll need to do a version check here so that mixed-cluster highlighting works
public String text() { return this.text; }	i can't comment on the constructor, but can we have one without the text parameter? it's always null now . i'm okay with leaving the current constructor there as well...
public static void buildFromContent(BytesReference content, AnalyzeRequest analyzeRequest) throws ElasticsearchIllegalArgumentException { try (XContentParser parser = XContentHelper.createParser(content)) { if (parser.nextToken() != XContentParser.Token.START_OBJECT) { throw new ElasticsearchIllegalArgumentException("Malforrmed content, must start with an object"); } else { XContentParser.Token token; String currentFieldName = null; int count = 0; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if ("prefer_local".equals(currentFieldName) && token == XContentParser.Token.VALUE_BOOLEAN) { analyzeRequest.preferLocal(parser.booleanValue()); } else if ("text".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) { analyzeRequest.text(parser.text()); } else if ("analyzer".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) { analyzeRequest.analyzer(parser.text()); } else if ("field".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) { analyzeRequest.field(parser.text()); } else if ("tokenizer".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) { analyzeRequest.tokenizer(parser.text()); }else if (("token_filters".equals(currentFieldName) || "filters".equals(currentFieldName)) && token == XContentParser.Token.START_ARRAY) { List<String> filters = Lists.newArrayList(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token.isValue() == false) { throw new ElasticsearchIllegalArgumentException(currentFieldName + " array element should only contain token filter's name"); } filters.add(parser.text()); } analyzeRequest.tokenFilters(filters.toArray(new String[0])); } else if ("char_filters".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY) { List<String> charFilters = Lists.newArrayList(); while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token.isValue() == false) { throw new ElasticsearchIllegalArgumentException(currentFieldName + " array element should only contain char filter's name"); } charFilters.add(parser.text()); } analyzeRequest.tokenFilters(charFilters.toArray(new String[0])); } else { throw new ElasticsearchIllegalArgumentException("Unknown param [" + currentFieldName + "] in request body"); } } } } catch (IOException e) { throw new ElasticsearchIllegalArgumentException("Failed to parse request body", e); } }	i think you should added or "parameter is of the wrong type" since it may be a valid name ..
public static void buildFromContent(BytesReference content, ClearScrollRequest clearScrollRequest) throws ElasticsearchIllegalArgumentException { try (XContentParser parser = XContentHelper.createParser(content)) { if (parser.nextToken() != XContentParser.Token.START_OBJECT) { throw new ElasticsearchIllegalArgumentException("Malforrmed content, must start with an object"); } else { XContentParser.Token token; String currentFieldName = null; while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if ("scroll_id".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY) { while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) { if (token.isValue() == false) { throw new ElasticsearchIllegalArgumentException("scroll_id array element should only contain scroll_id"); } clearScrollRequest.addScrollId(parser.text()); } } else { throw new ElasticsearchIllegalArgumentException("Unknown param [" + currentFieldName + "] in request body"); } } } } catch (IOException e) { throw new ElasticsearchIllegalArgumentException("Failed to parse request body", e); } }	same here regarding field types
public void sendExecuteQuery(Transport.Connection connection, final ShardSearchTransportRequest request, SearchTask task, final SearchActionListener<SearchPhaseResult> listener) { // we optimize this and expect a QueryFetchSearchResult if we only have a single shard in the search request // this used to be the QUERY_AND_FETCH which doesn't exist anymore. final boolean fetchDocuments = request.numberOfShards() == 1; Supplier<SearchPhaseResult> supplier = fetchDocuments ? QueryFetchSearchResult::new : QuerySearchResult::new; transportService.sendChildRequest(connection, QUERY_ACTION_NAME, request, task, new ActionListenerResponseHandler<>(listener, supplier)); }	does the query_fetch_action_name constant still need to exist?
@Override public void authenticationSuccess(String requestId, Authentication authentication, RestRequest request) { if (events.contains(AUTHENTICATION_SUCCESS) && eventFilterPolicyRegistry.ignorePredicate() .test(new AuditEventMetaInfo( Optional.of(authentication.getUser()), Optional.of(effectiveRealmName(authentication)), Optional.empty(), Optional.empty())) == false) { final StringMapMessage logEntry = new LogEntryBuilder() .with(EVENT_TYPE_FIELD_NAME, REST_ORIGIN_FIELD_VALUE) .with(EVENT_ACTION_FIELD_NAME, "authentication_success") .withRestUriAndMethod(request) .withRequestId(requestId) .withAuthentication(authentication) .withRestOrigin(request) .withRequestBody(request) .withOpaqueId(threadContext) .withXForwardedFor(threadContext) .build(); logger.info(AUDIT_MARKER, logEntry); } }	authentication_success is different because it returned the realm inside the realm field instead of user.realm field. it almost makes sense, but the problem is that the user.name could reside in the lookup realm, which is not audited. also the authn realm name could be _es_api_key which is even less helpful. i feel this change is borderline. happy to discuss it.
@Override public void tamperedRequest(String requestId, Authentication authentication, String action, TransportRequest transportRequest) { if (events.contains(TAMPERED_REQUEST)) { final Optional<String[]> indices = indices(transportRequest); if (eventFilterPolicyRegistry.ignorePredicate().test(new AuditEventMetaInfo( Optional.of(authentication.getUser()), Optional.of(effectiveRealmName(authentication)), Optional.empty(), indices)) == false) { final StringMapMessage logEntry = new LogEntryBuilder() .with(EVENT_TYPE_FIELD_NAME, TRANSPORT_ORIGIN_FIELD_VALUE) .with(EVENT_ACTION_FIELD_NAME, "tampered_request") .with(ACTION_FIELD_NAME, action) .with(REQUEST_NAME_FIELD_NAME, transportRequest.getClass().getSimpleName()) .withRequestId(requestId) .withRestOrTransportOrigin(transportRequest, threadContext) .withAuthentication(authentication) .with(INDICES_FIELD_NAME, indices.orElse(null)) .withOpaqueId(threadContext) .withXForwardedFor(threadContext) .build(); logger.info(AUDIT_MARKER, logEntry); } } }	this tampered_request can now be filtered by ignore policies.
static Request refresh(RefreshRequest refreshRequest) { String endpoint = endpoint(refreshRequest.indices(), "_refresh"); return new Request(HttpPost.METHOD_NAME, endpoint, Collections.emptyMap(), null); }	we need to also take the indices options from the refresh request and set them as parameters. see params#withindicesoptions
public void testRefresh() { String[] indices = randomIndicesNames(1, 5); RefreshRequest refreshRequest = new RefreshRequest(indices); Request request = Request.refresh(refreshRequest); StringJoiner endpoint = new StringJoiner("/", "/", "").add(String.join(",", indices)).add("_refresh"); assertThat(endpoint.toString(), equalTo(request.getEndpoint())); assertThat(request.getParameters().size(), equalTo(0)); assertThat(request.getEntity(), nullValue()); assertThat(request.getMethod(), equalTo(HttpPost.METHOD_NAME)); }	add tests for indices options, see setrandomindicesoptions
public void testRefreshIndex() throws Exception { RestHighLevelClient client = highLevelClient(); { createIndex("index1", Settings.EMPTY); createIndex("index2", Settings.EMPTY); } { // tag::refresh-request RefreshRequest request = new RefreshRequest("index1"); // <1> RefreshRequest requestMultiple = new RefreshRequest("index1", "index2"); // <2> RefreshRequest requestAll = new RefreshRequest(); // <3> // end::refresh-request // tag::refresh-execute RefreshResponse refreshResponse = client.indices().refresh(request); // end::refresh-execute // tag::refresh-response int totalShards = refreshResponse.getTotalShards(); // <1> int successfulShards = refreshResponse.getSuccessfulShards(); // <2> int failedShards = refreshResponse.getFailedShards(); // <3> DefaultShardOperationFailedException[] failures = refreshResponse.getShardFailures(); // <4> // end::refresh-response // tag::refresh-execute-listener ActionListener<RefreshResponse> listener = new ActionListener<RefreshResponse>() { @Override public void onResponse(RefreshResponse refreshResponse) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::refresh-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::refresh-execute-async client.indices().refreshAsync(request, listener); // <1> // end::refresh-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } { // tag::refresh-notfound try { RefreshRequest request = new RefreshRequest("does_not_exist"); client.indices().refresh(request); } catch (ElasticsearchException exception) { if (exception.status() == RestStatus.NOT_FOUND) { // <1> } } // end::refresh-notfound } }	do we need to create the second index?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(_SHARDS_FIELD.getPreferredName()); builder.field(TOTAL_FIELD.getPreferredName(), totalShards); builder.field(SUCCESSFUL_FIELD.getPreferredName(), successfulShards); builder.field(FAILED_FIELD.getPreferredName(), failedShards); if (shardFailures != EMPTY) { builder.startArray(FAILURES_FIELD.getPreferredName()); for (DefaultShardOperationFailedException failure : shardFailures) { builder.startObject(); failure.toXContent(builder, params); builder.endObject(); } builder.endArray(); } builder.endObject(); return builder; }	why not calling restactions#buildbroadcastshardsheader instead ? aren't we losing support for the group_shard_failures flag? it is not relevant for the high-level rest client as there is no way to set it but i think it's important given that the parsing code is in es core. which reminds me, we should probably test this as well in refreshresponsetests. this param can be passed in as part of the toxcontent.params when calling toxcontent
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { RefreshRequest refreshRequest = new RefreshRequest(Strings.splitStringByCommaToArray(request.param("index"))); refreshRequest.indicesOptions(IndicesOptions.fromRequest(request, refreshRequest.indicesOptions())); return channel -> client.admin().indices().refresh(refreshRequest, new RestBuilderListener<RefreshResponse>(channel) { @Override public RestResponse buildResponse(RefreshResponse response, XContentBuilder builder) throws Exception { builder.startObject(); response.toXContent(builder, request); builder.endObject(); return new BytesRestResponse(response.getStatus(), builder); } }); }	remove the unused static import from this class?
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { RefreshRequest refreshRequest = new RefreshRequest(Strings.splitStringByCommaToArray(request.param("index"))); refreshRequest.indicesOptions(IndicesOptions.fromRequest(request, refreshRequest.indicesOptions())); return channel -> client.admin().indices().refresh(refreshRequest, new RestBuilderListener<RefreshResponse>(channel) { @Override public RestResponse buildResponse(RefreshResponse response, XContentBuilder builder) throws Exception { builder.startObject(); response.toXContent(builder, request); builder.endObject(); return new BytesRestResponse(response.getStatus(), builder); } }); }	you can shorten this by using resttoxcontentlistener instead of restbuilderlistener. if a response implements toxcontent, we don't need to do anything special.
*/ private void snapshotFile( BlobStoreIndexShardSnapshot.FileInfo fileInfo, IndexId indexId, ShardId shardId, SnapshotId snapshotId, IndexShardSnapshotStatus snapshotStatus, Store store ) throws IOException { final BlobContainer shardContainer = shardContainer(indexId, shardId); final String file = fileInfo.physicalName(); try (IndexInput indexInput = store.openVerifyingInput(file, IOContext.READONCE, fileInfo.metadata())) { for (int i = 0; i < fileInfo.numberOfParts(); i++) { final long partBytes = fileInfo.partBytes(i); // Make reads abortable by mutating the snapshotStatus object final InputStream inputStream = new FilterInputStream( maybeRateLimitSnapshots(new InputStreamIndexInput(indexInput, partBytes)) ) { @Override public int read() throws IOException { checkAborted(); return super.read(); } @Override public int read(byte[] b, int off, int len) throws IOException { checkAborted(); return super.read(b, off, len); } private void checkAborted() { if (snapshotStatus.isAborted()) { logger.debug("[{}] [{}] Aborted on the file [{}], exiting", shardId, snapshotId, fileInfo.physicalName()); throw new AbortedSnapshotException(); } } }; final String partName = fileInfo.partName(i); logger.trace("[{}] Writing [{}] to [{}]", metadata.name(), partName, shardContainer.path()); final long startMS = threadPool.relativeTimeInMillis(); shardContainer.writeBlob(partName, inputStream, partBytes, false); logger.trace( "[{}] Writing [{}] of size [{}b] to [{}] took [{}ms]", metadata.name(), partName, partBytes, shardContainer.path(), threadPool.relativeTimeInMillis() - startMS ); } Store.verify(indexInput); snapshotStatus.addProcessedFile(fileInfo.length()); } catch (Exception t) { failStoreIfCorrupted(store, t); snapshotStatus.addProcessedFile(0); throw t; } }	a true nit, but i prefer to capture the threadpool.relativetimeinmillis() explicitly right after the write to not have to worry about any time spent in any of the (obviously fast) method calls here.
@Override public Void visitIf(final IfContext ctx) { final StatementMetadata ifsmd = adapter.getStatementMetadata(ctx); final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression()); final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx); expremd.to = definition.booleanType; visit(exprctx); markCast(expremd); if (expremd.postConst != null) { throw new IllegalArgumentException(error(ctx) + "If statement is not necessary."); } final BlockContext blockctx0 = ctx.block(0); final StatementMetadata blocksmd0 = adapter.createStatementMetadata(blockctx0); blocksmd0.lastSource = ifsmd.lastSource; blocksmd0.inLoop = ifsmd.inLoop; blocksmd0.lastLoop = ifsmd.lastLoop; incrementScope(); visit(blockctx0); decrementScope(); ifsmd.anyContinue = blocksmd0.anyContinue; ifsmd.anyBreak = blocksmd0.anyBreak; ifsmd.count = blocksmd0.count; if (ctx.ELSE() != null) { final BlockContext blockctx1 = ctx.block(1); final StatementMetadata blocksmd1 = adapter.createStatementMetadata(blockctx1); blocksmd1.lastSource = ifsmd.lastSource; incrementScope(); visit(blockctx1); decrementScope(); ifsmd.methodEscape = blocksmd0.methodEscape && blocksmd1.methodEscape; ifsmd.loopEscape = blocksmd0.loopEscape && blocksmd1.loopEscape; ifsmd.allLast = blocksmd0.allLast && blocksmd1.allLast; ifsmd.anyContinue |= blocksmd1.anyContinue; ifsmd.anyBreak |= blocksmd1.anyBreak; if (blocksmd1.count > ifsmd.count) { ifsmd.count = blocksmd1.count; } } return null; }	maybe a method on statementmetdata to copy the appropriate metadata. it saves a few lines of code and forces you to name the operation.
@Override public Void visitIf(final IfContext ctx) { final StatementMetadata ifsmd = adapter.getStatementMetadata(ctx); final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression()); final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx); expremd.to = definition.booleanType; visit(exprctx); markCast(expremd); if (expremd.postConst != null) { throw new IllegalArgumentException(error(ctx) + "If statement is not necessary."); } final BlockContext blockctx0 = ctx.block(0); final StatementMetadata blocksmd0 = adapter.createStatementMetadata(blockctx0); blocksmd0.lastSource = ifsmd.lastSource; blocksmd0.inLoop = ifsmd.inLoop; blocksmd0.lastLoop = ifsmd.lastLoop; incrementScope(); visit(blockctx0); decrementScope(); ifsmd.anyContinue = blocksmd0.anyContinue; ifsmd.anyBreak = blocksmd0.anyBreak; ifsmd.count = blocksmd0.count; if (ctx.ELSE() != null) { final BlockContext blockctx1 = ctx.block(1); final StatementMetadata blocksmd1 = adapter.createStatementMetadata(blockctx1); blocksmd1.lastSource = ifsmd.lastSource; incrementScope(); visit(blockctx1); decrementScope(); ifsmd.methodEscape = blocksmd0.methodEscape && blocksmd1.methodEscape; ifsmd.loopEscape = blocksmd0.loopEscape && blocksmd1.loopEscape; ifsmd.allLast = blocksmd0.allLast && blocksmd1.allLast; ifsmd.anyContinue |= blocksmd1.anyContinue; ifsmd.anyBreak |= blocksmd1.anyBreak; if (blocksmd1.count > ifsmd.count) { ifsmd.count = blocksmd1.count; } } return null; }	maybe name them thenblockctx and elseblockctx instead of 0 and 1?
@Override public Void visitIf(final IfContext ctx) { final StatementMetadata ifsmd = adapter.getStatementMetadata(ctx); final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression()); final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx); expremd.to = definition.booleanType; visit(exprctx); markCast(expremd); if (expremd.postConst != null) { throw new IllegalArgumentException(error(ctx) + "If statement is not necessary."); } final BlockContext blockctx0 = ctx.block(0); final StatementMetadata blocksmd0 = adapter.createStatementMetadata(blockctx0); blocksmd0.lastSource = ifsmd.lastSource; blocksmd0.inLoop = ifsmd.inLoop; blocksmd0.lastLoop = ifsmd.lastLoop; incrementScope(); visit(blockctx0); decrementScope(); ifsmd.anyContinue = blocksmd0.anyContinue; ifsmd.anyBreak = blocksmd0.anyBreak; ifsmd.count = blocksmd0.count; if (ctx.ELSE() != null) { final BlockContext blockctx1 = ctx.block(1); final StatementMetadata blocksmd1 = adapter.createStatementMetadata(blockctx1); blocksmd1.lastSource = ifsmd.lastSource; incrementScope(); visit(blockctx1); decrementScope(); ifsmd.methodEscape = blocksmd0.methodEscape && blocksmd1.methodEscape; ifsmd.loopEscape = blocksmd0.loopEscape && blocksmd1.loopEscape; ifsmd.allLast = blocksmd0.allLast && blocksmd1.allLast; ifsmd.anyContinue |= blocksmd1.anyContinue; ifsmd.anyBreak |= blocksmd1.anyBreak; if (blocksmd1.count > ifsmd.count) { ifsmd.count = blocksmd1.count; } } return null; }	maybe name this too?
@Override public Void visitIf(final IfContext ctx) { final StatementMetadata ifsmd = adapter.getStatementMetadata(ctx); final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression()); final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx); expremd.to = definition.booleanType; visit(exprctx); markCast(expremd); if (expremd.postConst != null) { throw new IllegalArgumentException(error(ctx) + "If statement is not necessary."); } final BlockContext blockctx0 = ctx.block(0); final StatementMetadata blocksmd0 = adapter.createStatementMetadata(blockctx0); blocksmd0.lastSource = ifsmd.lastSource; blocksmd0.inLoop = ifsmd.inLoop; blocksmd0.lastLoop = ifsmd.lastLoop; incrementScope(); visit(blockctx0); decrementScope(); ifsmd.anyContinue = blocksmd0.anyContinue; ifsmd.anyBreak = blocksmd0.anyBreak; ifsmd.count = blocksmd0.count; if (ctx.ELSE() != null) { final BlockContext blockctx1 = ctx.block(1); final StatementMetadata blocksmd1 = adapter.createStatementMetadata(blockctx1); blocksmd1.lastSource = ifsmd.lastSource; incrementScope(); visit(blockctx1); decrementScope(); ifsmd.methodEscape = blocksmd0.methodEscape && blocksmd1.methodEscape; ifsmd.loopEscape = blocksmd0.loopEscape && blocksmd1.loopEscape; ifsmd.allLast = blocksmd0.allLast && blocksmd1.allLast; ifsmd.anyContinue |= blocksmd1.anyContinue; ifsmd.anyBreak |= blocksmd1.anyBreak; if (blocksmd1.count > ifsmd.count) { ifsmd.count = blocksmd1.count; } } return null; }	ifsmd.count = math.max(ifsmd.count, blocksmd1.count); would be a bit more clear to me.
@Override public Void visitWhile(final WhileContext ctx) { final StatementMetadata whilesmd = adapter.getStatementMetadata(ctx); incrementScope(); final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression()); final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx); expremd.to = definition.booleanType; visit(exprctx); markCast(expremd); boolean continuous = false; if (expremd.postConst != null) { continuous = (boolean)expremd.postConst; if (!continuous) { throw new IllegalArgumentException(error(ctx) + "The loop will never be executed."); } if (ctx.empty() != null) { throw new IllegalArgumentException(error(ctx) + "The loop is continuous."); } } final BlockContext blockctx = ctx.block(); if (blockctx != null) { final StatementMetadata blocksmd = adapter.createStatementMetadata(blockctx); blocksmd.topLoop = true; blocksmd.inLoop = true; visit(blockctx); if (blocksmd.loopEscape && !blocksmd.anyContinue) { throw new IllegalArgumentException(error(ctx) + "All paths escape so the loop is not necessary."); } if (continuous && !blocksmd.anyBreak) { whilesmd.methodEscape = true; whilesmd.allLast = true; } } whilesmd.count = 1; decrementScope(); return null; }	i'm not sure i'd understand this error message if i got it.
@Override public Void visitReturn(final ReturnContext ctx) { final StatementMetadata returnsmd = adapter.getStatementMetadata(ctx); final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression()); final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx); expremd.to = definition.objectType; visit(exprctx); markCast(expremd); returnsmd.methodEscape = true; returnsmd.loopEscape = true; returnsmd.allLast = true; returnsmd.count = 1; return null; }	blocksmd.copycontext(trysmd);? i know you use "context" to mean something and this might not be the right use of that word though.
@Override public Void visitReturn(final ReturnContext ctx) { final StatementMetadata returnsmd = adapter.getStatementMetadata(ctx); final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression()); final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx); expremd.to = definition.objectType; visit(exprctx); markCast(expremd); returnsmd.methodEscape = true; returnsmd.loopEscape = true; returnsmd.allLast = true; returnsmd.count = 1; return null; }	can incrementscope live inside of the visit implementation?
@Override public Void visitTry(final TryContext ctx) { final StatementMetadata trysmd = adapter.getStatementMetadata(ctx); final BlockContext blockctx = ctx.block(); final StatementMetadata blocksmd = adapter.createStatementMetadata(blockctx); blocksmd.lastSource = trysmd.lastSource; blocksmd.inLoop = trysmd.inLoop; blocksmd.lastLoop = trysmd.lastLoop; incrementScope(); visit(blockctx); decrementScope(); trysmd.methodEscape = blocksmd.methodEscape; trysmd.loopEscape = blocksmd.loopEscape; trysmd.allLast = blocksmd.allLast; trysmd.anyContinue = blocksmd.anyContinue; trysmd.anyBreak = blocksmd.anyBreak; int trapcount = 0; for (final TrapContext trapctx : ctx.trap()) { final StatementMetadata trapsmd = adapter.createStatementMetadata(trapctx); trapsmd.lastSource = trysmd.lastSource; trapsmd.inLoop = trysmd.inLoop; trapsmd.lastLoop = trysmd.lastLoop; incrementScope(); visit(trapctx); decrementScope(); trysmd.methodEscape &= trapsmd.methodEscape; trysmd.loopEscape &= trapsmd.loopEscape; trysmd.allLast &= trapsmd.allLast; trysmd.anyContinue |= trapsmd.anyContinue; trysmd.anyBreak |= trapsmd.anyBreak; trapcount = trapcount < trapsmd.count ? trapsmd.count : trapcount; } trysmd.count = blocksmd.count + trapcount; return null; }	this is functionally very similar to what you do with the else side of a loop. can you share it?
*/ private boolean writeExactInstruction(final Sort osort, final Sort psort) { if (psort == Sort.DOUBLE) { if (osort == Sort.FLOAT) { execute.invokeStatic(definition.utilityType.type, TOFLOATWOOVERFLOW_DOUBLE); } else if (osort == Sort.FLOAT_OBJ) { execute.invokeStatic(definition.utilityType.type, TOFLOATWOOVERFLOW_DOUBLE); execute.checkCast(definition.floatobjType.type); } else if (osort == Sort.LONG) { execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_DOUBLE); } else if (osort == Sort.LONG_OBJ) { execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_DOUBLE); execute.checkCast(definition.longobjType.type); } else if (osort == Sort.INT) { execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_DOUBLE); } else if (osort == Sort.INT_OBJ) { execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_DOUBLE); execute.checkCast(definition.intobjType.type); } else if (osort == Sort.CHAR) { execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_DOUBLE); } else if (osort == Sort.CHAR_OBJ) { execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_DOUBLE); execute.checkCast(definition.charobjType.type); } else if (osort == Sort.SHORT) { execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_DOUBLE); } else if (osort == Sort.SHORT_OBJ) { execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_DOUBLE); execute.checkCast(definition.shortobjType.type); } else if (osort == Sort.BYTE) { execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_DOUBLE); } else if (osort == Sort.BYTE_OBJ) { execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_DOUBLE); execute.checkCast(definition.byteobjType.type); } else { return false; } } else if (psort == Sort.FLOAT) { if (osort == Sort.LONG) { execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_FLOAT); } else if (osort == Sort.LONG_OBJ) { execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_FLOAT); execute.checkCast(definition.longobjType.type); } else if (osort == Sort.INT) { execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_FLOAT); } else if (osort == Sort.INT_OBJ) { execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_FLOAT); execute.checkCast(definition.intobjType.type); } else if (osort == Sort.CHAR) { execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_FLOAT); } else if (osort == Sort.CHAR_OBJ) { execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_FLOAT); execute.checkCast(definition.charobjType.type); } else if (osort == Sort.SHORT) { execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_FLOAT); } else if (osort == Sort.SHORT_OBJ) { execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_FLOAT); execute.checkCast(definition.shortobjType.type); } else if (osort == Sort.BYTE) { execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_FLOAT); } else if (osort == Sort.BYTE_OBJ) { execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_FLOAT); execute.checkCast(definition.byteobjType.type); } else { return false; } } else if (psort == Sort.LONG) { if (osort == Sort.INT) { execute.invokeStatic(definition.mathType.type, TOINTEXACT_LONG); } else if (osort == Sort.INT_OBJ) { execute.invokeStatic(definition.mathType.type, TOINTEXACT_LONG); execute.checkCast(definition.intobjType.type); } else if (osort == Sort.CHAR) { execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_LONG); } else if (osort == Sort.CHAR_OBJ) { execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_LONG); execute.checkCast(definition.charobjType.type); } else if (osort == Sort.SHORT) { execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_LONG); } else if (osort == Sort.SHORT_OBJ) { execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_LONG); execute.checkCast(definition.shortobjType.type); } else if (osort == Sort.BYTE) { execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_LONG); } else if (osort == Sort.BYTE_OBJ) { execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_LONG); execute.checkCast(definition.byteobjType.type); } else { return false; } } else if (psort == Sort.INT) { if (osort == Sort.CHAR) { execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_INT); } else if (osort == Sort.CHAR_OBJ) { execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_INT); execute.checkCast(definition.charobjType.type); } else if (osort == Sort.SHORT) { execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_INT); } else if (osort == Sort.SHORT_OBJ) { execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_INT); execute.checkCast(definition.shortobjType.type); } else if (osort == Sort.BYTE) { execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_INT); } else if (osort == Sort.BYTE_OBJ) { execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_INT); execute.checkCast(definition.byteobjType.type); } else { return false; } } else { return false; } return true; }	i'd be more comfortable with a map<tuple<sort, sort>, method> to register these rather than the if tree. not for this pr though.
public void testInfiniteLoops() { try { exec("boolean x = true; while (x) {}"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("while (true) {int y = 5}"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("while (true) { boolean x = true; while (x) {} }"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("while (true) { boolean x = false; while (x) {} }"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("boolean x = true; for (;x;) {}"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("for (;;) {int x = 5}"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("def x = true; do {int y = 5;} while (x)"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } }	can you add a test where the code tries to catch planaerror?
public void testInfiniteLoops() { try { exec("boolean x = true; while (x) {}"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("while (true) {int y = 5}"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("while (true) { boolean x = true; while (x) {} }"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("while (true) { boolean x = false; while (x) {} }"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("boolean x = true; for (;x;) {}"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("for (;;) {int x = 5}"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } try { exec("def x = true; do {int y = 5;} while (x)"); fail("should have hit PlanAError"); } catch (PlanAError expected) { assertTrue(expected.getMessage().contains( "The maximum number of statements that can be executed in a loop has been reached.")); } }	maybe we should test around the boundaries of the loop limit (rather than just infinite ones, finite-but-large ones) to ensure it works exactly as we expect?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject("nodes"); for (ObjectCursor<DiscoveryNode> node : nodes.values()) { node.value.toXContent(builder, params); } builder.endObject(); return builder; }	this is tricky - discoverynodes carries more info then just the nodes - it also tells you which one is local and which one is master. if we do have a toxcontent implementation this should be represented - but you may not want to expose it everywhere. since we only use it in one place now, maybe just roll it back and remove this?
* @return the raw string representation of the setting value */ String innerGetRaw(final Settings settings) { return settings.get(getKey(), defaultValue.apply(settings), isListSetting()); }	instead of having an islistsetting, since this is already package private anyways, can this just check instanceof listsetting?
public String get(String setting, String defaultValue) { String retVal = get(setting); return retVal == null ? defaultValue : retVal; }	can this be package private?
public interface Factory extends ScriptFactory { LeafFactory newFactory(String fieldName, Map<String, Object> params, SearchLookup searchLookup, DateFormatter formatter); } public interface LeafFactory { DateFieldScript newInstance(LeafReaderContext ctx); } public static final Factory PARSE_FROM_SOURCE = (field, params, lookup, formatter) -> (LeafFactory) ctx -> new DateFieldScript( field, params, lookup, formatter, ctx ) { @Override public void execute() { for (Object v : extractFromSource(field)) { if (v instanceof String) { try { emit(formatter.parseMillis((String) v)); } catch (Exception e) { // ignore } } } } }; private final DateFormatter formatter; public DateFieldScript( String fieldName, Map<String, Object> params, SearchLookup searchLookup, DateFormatter formatter, LeafReaderContext ctx ) { super(fieldName, params, searchLookup, ctx); this.formatter = formatter; } public static class Emit { private final DateFieldScript script; public Emit(DateFieldScript script) { this.script = script; } public void emit(long v) { script.emit(v); } } /** * Temporary parse method that takes into account the date format. We'll * remove this when we have "native" source parsing fields. */ public static class Parse { private final DateFieldScript script; public Parse(DateFieldScript script) { this.script = script; } public long parse(Object str) { return script.formatter.parseMillis(str.toString()); } } public static class EmitValues { private final DateFieldScript script; public EmitValues(DateFieldScript script) { this.script = script; } public void emitFromPath(String path) { for (Object v : script.extractFromSource(path)) { if (v instanceof String) { try { script.emit(script.formatter.parseMillis((String) v)); } catch (Exception e) { // ignore } } } } }	let's remove the parse function that's exposed for dates?
public long parse(Object str) { return script.formatter.parseMillis(str.toString()); }	this is no longer needed right?
private void innerCreateNoLock(Create create, IndexWriter writer, long currentVersion, VersionValue versionValue) throws IOException { // same logic as index long updatedVersion; long expectedVersion = create.version(); if (create.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) { if (create.origin() == Operation.Origin.RECOVERY) { return; } else { throw new VersionConflictEngineException(shardId, create.type(), create.id(), currentVersion, expectedVersion); } } updatedVersion = create.versionType().updateVersion(currentVersion, expectedVersion); // if the doc exists boolean doUpdate = false; if ((versionValue != null && versionValue.delete() == false) || (versionValue == null && currentVersion != Versions.NOT_FOUND)) { if (create.origin() == Operation.Origin.RECOVERY) { return; } else if (create.origin() == Operation.Origin.REPLICA) { // #7142: the primary already determined it's OK to index this document, and we confirmed above that the version doesn't // conflict, so we must also update here on the replica to remain consistent: doUpdate = true; } else if (create.origin() == Operation.Origin.PRIMARY && create.autoGeneratedId() && create.canHaveDuplicates() && currentVersion == 1 && create.version() == Versions.MATCH_ANY) { doUpdate = true; updatedVersion = 1; } else { // On primary, we throw DAEE if the _uid is already in the index with an older version: assert create.origin() == Operation.Origin.PRIMARY; throw new DocumentAlreadyExistsException(shardId, create.type(), create.id()); } } create.updateVersion(updatedVersion); if (doUpdate) { if (create.docs().size() > 1) { writer.updateDocuments(create.uid(), create.docs(), create.analyzer()); } else { writer.updateDocument(create.uid(), create.docs().get(0), create.analyzer()); } } else { if (create.docs().size() > 1) { writer.addDocuments(create.docs(), create.analyzer()); } else { writer.addDocument(create.docs().get(0), create.analyzer()); } } Translog.Location translogLocation = translog.add(new Translog.Create(create)); versionMap.putUnderLock(create.uid().bytes(), new VersionValue(updatedVersion, translogLocation)); indexingService.postCreateUnderLock(create); }	can you add a comment on why we have this check?
private void innerCreateNoLock(Create create, IndexWriter writer, long currentVersion, VersionValue versionValue) throws IOException { // same logic as index long updatedVersion; long expectedVersion = create.version(); if (create.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) { if (create.origin() == Operation.Origin.RECOVERY) { return; } else { throw new VersionConflictEngineException(shardId, create.type(), create.id(), currentVersion, expectedVersion); } } updatedVersion = create.versionType().updateVersion(currentVersion, expectedVersion); // if the doc exists boolean doUpdate = false; if ((versionValue != null && versionValue.delete() == false) || (versionValue == null && currentVersion != Versions.NOT_FOUND)) { if (create.origin() == Operation.Origin.RECOVERY) { return; } else if (create.origin() == Operation.Origin.REPLICA) { // #7142: the primary already determined it's OK to index this document, and we confirmed above that the version doesn't // conflict, so we must also update here on the replica to remain consistent: doUpdate = true; } else if (create.origin() == Operation.Origin.PRIMARY && create.autoGeneratedId() && create.canHaveDuplicates() && currentVersion == 1 && create.version() == Versions.MATCH_ANY) { doUpdate = true; updatedVersion = 1; } else { // On primary, we throw DAEE if the _uid is already in the index with an older version: assert create.origin() == Operation.Origin.PRIMARY; throw new DocumentAlreadyExistsException(shardId, create.type(), create.id()); } } create.updateVersion(updatedVersion); if (doUpdate) { if (create.docs().size() > 1) { writer.updateDocuments(create.uid(), create.docs(), create.analyzer()); } else { writer.updateDocument(create.uid(), create.docs().get(0), create.analyzer()); } } else { if (create.docs().size() > 1) { writer.addDocuments(create.docs(), create.analyzer()); } else { writer.addDocument(create.docs().get(0), create.analyzer()); } } Translog.Location translogLocation = translog.add(new Translog.Create(create)); versionMap.putUnderLock(create.uid().bytes(), new VersionValue(updatedVersion, translogLocation)); indexingService.postCreateUnderLock(create); }	extra newline.... i think this optimization deserves a rather longish comment. can you add it?
public void close() { closed = true; } /** * Wait for in-flight operations to finish and executes {@code onBlocked} under the guarantee that no new operations are started. Queues * operations that are occurring in the meanwhile and runs them once {@code onBlocked} has executed. * * @param timeout the maximum time to wait for the in-flight operations block * @param timeUnit the time unit of the {@code timeout} argument * @param onActiveOperations the action to run before trying to acquire the block if there are active operations * @param onBlocked the action to run once the block has been acquired * @param <E> the type of checked exception thrown by {@code onBlocked}	it seems the only production use case for this method is in relocation so admit it's a little noisy to add this kind of general callback here, but it still seems like the smallest possible change to get a hook to run the refresh conditionally here (after preventing new operations from piling on more waits concurrently).
public PersistentTasksCustomMetaData.Assignment getAssignment(DataFrameTransform params, ClusterState clusterState) { List<String> unavailableIndices = verifyIndicesPrimaryShardsAreActive(clusterState); if (unavailableIndices.size() != 0) { String reason = "Not starting data frame transform [" + params.getId() + "], " + "because not all primary shards are active for the following indices [" + String.join(",", unavailableIndices) + "]"; logger.debug(reason); return new PersistentTasksCustomMetaData.Assignment(null, reason); } if (clusterState.nodes().getMinNodeVersion().onOrAfter(params.getVersion())) { return super.getAssignment(params, clusterState); } else { DiscoveryNode discoveryNode = selectLeastLoadedNode(clusterState, (node) -> node.isDataNode() && node.getVersion().onOrAfter(params.getVersion()) ); if (discoveryNode == null) { return NO_NODE_FOUND; } else { return new PersistentTasksCustomMetaData.Assignment(discoveryNode.getId(), ""); } } }	the code in the else block here is basically the same as super.getassignment but with the extra requirement of a node version. however, if someone altered the super class code without realising this override existed then the logic of the else block would deviate from the super class, and that could lead to weird assignments during rolling upgrades. so i think it would be better to scrap the getminnodeversion check and _always_ use the code that's in the else block.
public static ShardPath selectNewPathForShard(NodeEnvironment env, ShardId shardId, IndexSettings indexSettings, long avgShardSizeInBytes, Map<Path,Integer> dataPathToShardCount) throws IOException { final Path dataPath; final Path statePath; if (indexSettings.hasCustomDataPath()) { dataPath = env.resolveCustomLocation(indexSettings, shardId); statePath = env.nodePaths()[0].resolve(shardId); } else { BigInteger totFreeSpace = BigInteger.ZERO; for (NodeEnvironment.NodePath nodePath : env.nodePaths()) { totFreeSpace = totFreeSpace.add(BigInteger.valueOf(nodePath.fileStore.getUsableSpace())); } // TODO: this is a hack!! We should instead keep track of incoming (relocated) shards since we know // how large they will be once they're done copying, instead of a silly guess for such cases: // Very rough heuristic of how much disk space we expect the shard will use over its lifetime, the max of current average // shard size across the cluster and 5% of the total available free space on this node: BigInteger estShardSizeInBytes = BigInteger.valueOf(avgShardSizeInBytes).max(totFreeSpace.divide(BigInteger.valueOf(20))); // TODO - do we need something more extensible? Yet, this does the job for now... final NodeEnvironment.NodePath[] paths = env.nodePaths(); // If no better path is chosen, use the one with the most space by default NodeEnvironment.NodePath bestPath = getPathWithMostFreeSpace(env); if (paths.length != 1) { int shardCount = indexSettings.getNumberOfShards(); // Maximum number of shards that a path should have for a particular index assuming // all the shards were assigned to this node. For example, with a node with 4 data // paths and an index with 9 primary shards, the maximum number of shards per path // would be 3. int maxShardsPerPath = Math.floorDiv(shardCount, paths.length) + ((shardCount % paths.length) == 0 ? 0 : 1); Map<NodeEnvironment.NodePath, Long> pathToShardCount = env.shardCountPerPath(shardId.getIndex()); // Compute how much space there is on each path final Map<NodeEnvironment.NodePath, BigInteger> pathsToSpace = new HashMap<>(paths.length); for (NodeEnvironment.NodePath nodePath : paths) { FileStore fileStore = nodePath.fileStore; BigInteger usableBytes = BigInteger.valueOf(fileStore.getUsableSpace()); pathsToSpace.put(nodePath, usableBytes); } final List<NodeEnvironment.NodePath> possiblePaths = Arrays.stream(paths) // Filter out paths that have enough space .filter((path) -> pathsToSpace.get(path).subtract(estShardSizeInBytes).compareTo(BigInteger.ZERO) > 0) // Sort by the number of shards for this index .sorted((p1, p2) -> { int cmp = Long.compare(pathToShardCount.getOrDefault(p1, 0L), pathToShardCount.getOrDefault(p2, 0L)); if (cmp == 0) { // if the number of shards is equal, tie-break with the usable bytes cmp = pathsToSpace.get(p2).compareTo(pathsToSpace.get(p1)); } return cmp; }) .collect(Collectors.toList()); if (possiblePaths.size() > 0) { bestPath = possiblePaths.get(0); } } statePath = bestPath.resolve(shardId); dataPath = statePath; } return new ShardPath(indexSettings.hasCustomDataPath(), dataPath, statePath, shardId); }	getfirst() would be enough?
public void testUnmappedField() throws IOException { AvgAggregationBuilder aggregationBuilder = new AvgAggregationBuilder("_name").field("number"); this.<AvgAggregationBuilder, InternalAvg>testCase(aggregationBuilder, new DocValuesFieldExistsQuery("number"), iw -> { iw.addDocument(singleton(new NumericDocValuesField("number", 7))); iw.addDocument(singleton(new NumericDocValuesField("number", 1))); }, avg -> { assertEquals(Double.NaN, avg.getValue(), 0); assertFalse(AggregationInspectionHelper.hasValue(avg)); }, (MappedFieldType) null); }	do we need the < bits?
protected void testCase(CardinalityAggregationBuilder aggregationBuilder, Query query, CheckedConsumer<RandomIndexWriter, IOException> buildIndex, Consumer<InternalCardinality> verify, MappedFieldType fieldType) throws IOException { super.testCase(aggregationBuilder, query, buildIndex, verify, fieldType); }	maybe rename these so they don't "hide" the superclass method and require the casts?
protected void testCase(CardinalityAggregationBuilder aggregationBuilder, Query query, CheckedConsumer<RandomIndexWriter, IOException> buildIndex, Consumer<InternalCardinality> verify, MappedFieldType fieldType) throws IOException { super.testCase(aggregationBuilder, query, buildIndex, verify, fieldType); }	creating a method with specific (and not generic) arguments is a shortcut that saves us from writing this.<cardinalityaggregationbuilder, internalcardinality>testcase(.....) at every test case. this method simply passes the arguments to the generic aggregatortestcase.testcase() method. i don't have a strong opinion about which option is best and i am happy go with any of the two methods.
public static Collection<String> findMatching(String[] patterns, Set<String> items) { if (items.isEmpty()) { return Collections.emptyList(); } if (Strings.isAllOrWildcard(patterns)) { return items; } List<String> matchingItems = new ArrayList<>(); for (String pattern : patterns) { if (items.contains(pattern)) { matchingItems.add(pattern); } else if (Regex.isSimpleMatchPattern(pattern)) { for (String item : items) { if (Regex.simpleMatch(pattern, item)) { matchingItems.add(item); } } } } return matchingItems; }	can it be that an item will be added twice to matchingitems because it matches 2 patterns? e.g. item "abc" and patterns "a*" and "ab*"? please add a unit test for this case.
@Override public int indexShard(String id, @Nullable String routing, XContentType sourceType, BytesReference source) { checkRoutingRequired(id, routing); return shardId(id, routing); }	id is never used.
public DateHistogramInterval getInterval() { RoundingInfo roundingInfo = this.bucketInfo.roundingInfos[this.bucketInfo.roundingIdx]; String unitAbbreviation = roundingInfo.unitAbbreviation; if (buckets.size() <= 1) { int innerInterval = roundingInfo.innerIntervals[0]; return new DateHistogramInterval(Integer.toString(innerInterval) + unitAbbreviation); } long intervalInMillis = buckets.get(1).key - buckets.get(0).key; for (int interval : roundingInfo.innerIntervals) { if (roundingInfo.getRoughEstimateDurationMillis() * interval == intervalInMillis) { return new DateHistogramInterval(Integer.toString(interval) + unitAbbreviation); } } return new DateHistogramInterval(Integer.toString(roundingInfo.innerIntervals[0]) + unitAbbreviation); }	i'm not sure we should calculate the interval here as we could end up with bugs where this is different from the actual interval used. instead could we have a field for the interval store in this class which is populated int he autodatehistogramaggregator.buildaggregation() and also in doreduce() when we build the new instance following a reduce? i think we will have all the information we need in both those places?
public void testAutoCreateNonPrimaryIndex() throws Exception { CreateIndexRequest request = new CreateIndexRequest(INDEX_NAME + "-2"); client().execute(AutoCreateAction.INSTANCE, request).get(); GetIndexResponse response = client().admin().indices().prepareGetIndex().addIndices(INDEX_NAME + "-2").get(); assertThat(response.indices().length, is(1)); assertThat(response.aliases().size(), is(1)); assertThat(response.aliases().get(INDEX_NAME + "-2").size(), is(1)); assertThat(response.aliases().get(INDEX_NAME + "-2").get(0), equalTo(AliasMetadata.builder(INDEX_NAME).isHidden(true).build())); }	would it be possible to test the opposite scenario? such that, the first api request makes the non primary index index_name-2 and then we add the primary name - 1.
public void testNonPrimarySystemIndexIsAutoCreatedViaConcreteName() throws Exception { final String nonPrimarySystemIndex = INDEX_NAME + "-2"; internalCluster().startNodes(1); // Trigger the creation of the system index indexDoc(nonPrimarySystemIndex, "1", "foo", "bar"); ensureGreen(nonPrimarySystemIndex); assertFalse(indexExists(PRIMARY_INDEX_NAME)); assertTrue(indexExists(INDEX_NAME + "-2")); // Check that a non-primary system index is not automatically assigned an alias final GetAliasesResponse getAliasesResponse = client().admin() .indices() .getAliases(new GetAliasesRequest().indicesOptions(IndicesOptions.strictExpandHidden())) .actionGet(); assertThat(getAliasesResponse.getAliases().size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get(nonPrimarySystemIndex).size(), equalTo(1)); assertThat( getAliasesResponse.getAliases().get(nonPrimarySystemIndex).get(0), equalTo(AliasMetadata.builder(INDEX_NAME).isHidden(true).build()) ); }	i'm a little bit confused by the comment here, i guess there's an alias, but it's not an alias we can use for writing?
* @param client For putting the template * @param templateConfig The config * @param listener Async listener */ public static void installIndexTemplateIfRequired( ClusterState clusterState, Client client, IndexTemplateConfig templateConfig, ActionListener<Boolean> listener ) { String templateName = templateConfig.getTemplateName(); // The check for existence of the template is against the cluster state, so very cheap if (haveIndexTemplate(clusterState, templateName)) { listener.onResponse(true); return; } PutIndexTemplateRequest request = new PutIndexTemplateRequest(templateName) .source(templateConfig.loadBytes(), XContentType.JSON); request.masterNodeTimeout(TimeValue.timeValueMinutes(1)); ActionListener<AcknowledgedResponse> innerListener = ActionListener.wrap( response -> { if (response.isAcknowledged() == false) { logger.error("error adding legacy template [{}], request was not acknowledged", templateName); } listener.onResponse(response.isAcknowledged()); }, listener::onFailure); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request, innerListener, client.admin().indices()::putTemplate); }	should we make this a warn? if it's an error i would expect it causes listener.onfailure to fire.
* @param client For putting the template * @param templateConfig The config * @param listener Async listener */ public static void installIndexTemplateIfRequired( ClusterState clusterState, Client client, IndexTemplateConfig templateConfig, ActionListener<Boolean> listener ) { String templateName = templateConfig.getTemplateName(); // The check for existence of the template is against the cluster state, so very cheap if (haveIndexTemplate(clusterState, templateName)) { listener.onResponse(true); return; } PutIndexTemplateRequest request = new PutIndexTemplateRequest(templateName) .source(templateConfig.loadBytes(), XContentType.JSON); request.masterNodeTimeout(TimeValue.timeValueMinutes(1)); ActionListener<AcknowledgedResponse> innerListener = ActionListener.wrap( response -> { if (response.isAcknowledged() == false) { logger.error("error adding legacy template [{}], request was not acknowledged", templateName); } listener.onResponse(response.isAcknowledged()); }, listener::onFailure); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, request, innerListener, client.admin().indices()::putTemplate); }	nit: should this be hasindextemplate?
@Override public Query buildFilteredQuery(Query query) { List<Query> filters = new ArrayList<>(); if (mapperService().hasNested() && new NestedHelper(mapperService()).mightMatchNestedDocs(query) && (aliasFilter == null || new NestedHelper(mapperService()).mightMatchNestedDocs(aliasFilter))) { filters.add(Queries.newNonNestedFilter()); } if (aliasFilter != null) { filters.add(aliasFilter); } if (sliceBuilder != null) { Query slicedQuery = sliceBuilder.toFilter(clusterService, request, queryShardContext); if ( slicedQuery instanceof MatchNoDocsQuery){ return slicedQuery; }else { filters.add(slicedQuery); } } if (filters.isEmpty()) { return query; } else { BooleanQuery.Builder builder = new BooleanQuery.Builder(); builder.add(query, Occur.MUST); for (Query filter : filters) { builder.add(filter, Occur.FILTER); } return builder.build(); } }	extra space after (
@Override public Query buildFilteredQuery(Query query) { List<Query> filters = new ArrayList<>(); if (mapperService().hasNested() && new NestedHelper(mapperService()).mightMatchNestedDocs(query) && (aliasFilter == null || new NestedHelper(mapperService()).mightMatchNestedDocs(aliasFilter))) { filters.add(Queries.newNonNestedFilter()); } if (aliasFilter != null) { filters.add(aliasFilter); } if (sliceBuilder != null) { Query slicedQuery = sliceBuilder.toFilter(clusterService, request, queryShardContext); if ( slicedQuery instanceof MatchNoDocsQuery){ return slicedQuery; }else { filters.add(slicedQuery); } } if (filters.isEmpty()) { return query; } else { BooleanQuery.Builder builder = new BooleanQuery.Builder(); builder.add(query, Occur.MUST); for (Query filter : filters) { builder.add(filter, Occur.FILTER); } return builder.build(); } }	nit: space before else
protected void doExecute(final IndexRequest request, final ActionListener<IndexResponse> listener) { // if we don't have a master, we don't have metadata, that's fine, let it find a master using create index API ClusterState state = clusterService.state(); if (autoCreateIndex.shouldAutoCreate(request.index(), state)) { if (!settings.getAsBoolean("index.mapper.dynamic", true)) { throw new MapperParsingException("trying to auto create mapping, but dynamic mapping is disabled"); } CreateIndexRequest createIndexRequest = new CreateIndexRequest(request); createIndexRequest.index(request.index()); createIndexRequest.mapping(request.type()); createIndexRequest.cause("auto(index api)"); createIndexRequest.masterNodeTimeout(request.timeout()); createIndexAction.execute(createIndexRequest, new ActionListener<CreateIndexResponse>() { @Override public void onResponse(CreateIndexResponse result) { innerExecute(request, listener); } @Override public void onFailure(Throwable e) { if (ExceptionsHelper.unwrapCause(e) instanceof IndexAlreadyExistsException) { // we have the index, do it try { innerExecute(request, listener); } catch (Throwable e1) { listener.onFailure(e1); } } else { listener.onFailure(e); } } }); } else { innerExecute(request, listener); } }	can we make "index.mapper.dynamic" a constant in mapperservice?
public PrefixTreeStrategy resolvePrefixTreeStrategy(String strategyName) { if (SpatialStrategy.RECURSIVE.getStrategyName().equals(strategyName)) { return recursiveStrategy; } if (SpatialStrategy.TERM.getStrategyName().equals(strategyName)) { return termStrategy; } throw new IllegalArgumentException("Unknown prefix tree strategy [" + strategyName + "]"); } } private final Version indexCreatedVersion; private final Builder builder; public LegacyGeoShapeFieldMapper(String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, LegacyGeoShapeIndexer indexer, LegacyGeoShapeParser parser, Builder builder) { super(simpleName, mappedFieldType, Collections.singletonMap(mappedFieldType.name(), Lucene.KEYWORD_ANALYZER), builder.ignoreMalformed.get(), builder.coerce.get(), builder.ignoreZValue.get(), builder.orientation.get(), multiFields, copyTo, indexer, parser); if (builder.indexCreatedVersion.onOrAfter(Version.V_8_0_0)) { throw new IllegalArgumentException("mapper [" + name() + "] of type [geo_shape] with deprecated parameters is no longer allowed"); } this.indexCreatedVersion = builder.indexCreatedVersion; this.builder = builder; } @Override public GeoShapeFieldType fieldType() { return (GeoShapeFieldType) super.fieldType(); } String strategy() { return fieldType().strategy().getStrategyName(); } @Override protected void addStoredFields(ParseContext context, Shape geometry) { // noop: we do not store geo_shapes; and will not store legacy geo_shape types } @Override protected void addDocValuesFields(String name, Shape geometry, List<IndexableField> fields, ParseContext context) { // doc values are not supported } @Override protected void addMultiFields(ParseContext context, Shape geometry) { // noop (completion suggester currently not compatible with geo_shape) } @Override protected String contentType() { return CONTENT_TYPE; } @Override public FieldMapper.Builder getMergeBuilder() { return new Builder(simpleName(), indexCreatedVersion, builder.ignoreMalformed.getDefaultValue().value(), builder.coerce.getDefaultValue().value()).init(this); } @Override protected void checkIncomingMergeType(FieldMapper mergeWith) { if (mergeWith instanceof GeoShapeFieldMapper) { throw new IllegalArgumentException("mapper [" + name() + "] of type [geo_shape] cannot change strategy from [" + strategy() + "] to [BKD]"); }	it would be good to be a bit clearer about what is causing the error here. maybe instead of having the version check here we should move it to containsdeprecatedparameter so that the error message can explicitly contain the parameters that were used and are no longer allowed?
* @return map of index to index metadata blob id to delete */ public Map<IndexId, String> indexMetaDataToRemoveAfterRemovingSnapshot(SnapshotId snapshotId) { Collection<IndexId> indicesForSnapshot = indicesToUpdateAfterRemovingSnapshot(snapshotId); final Set<String> allIdentifiers = indexMetaDataGenerations.lookup.entrySet().stream() .filter(e -> e.getKey().equals(snapshotId) == false).flatMap(e -> e.getValue().values().stream()) .map(indexMetaDataGenerations::getIndexMetaBlobId).collect(Collectors.toSet()); final Map<IndexId, String> toRemove = new HashMap<>(); for (IndexId indexId : indicesForSnapshot) { final String identifier = indexMetaDataGenerations.indexMetaBlobId(snapshotId, indexId); if (allIdentifiers.contains(identifier) == false) { final String prev = toRemove.put(indexId, identifier); assert prev == null : "Saw double entry [" + prev + "][" + identifier + "]"; } } return toRemove; } /** * Add a snapshot and its indices to the repository; returns a new instance. If the snapshot * already exists in the repository data, this method throws an IllegalArgumentException. * * @param snapshotId Id of the new snapshot * @param snapshotState State of the new snapshot * @param shardGenerations Updated shard generations in the new snapshot. For each index contained in the snapshot an array of new * generations indexed by the shard id they correspond to must be supplied. * @param indexMetaBlobs Map of index metadata blob uuids * @param newIdentifiers Map of new index metadata blob uuids keyed by the identifiers of the * {@link org.elasticsearch.cluster.metadata.IndexMetaData}	perhaps call this allremainingidentifiers
protected void execute(Terminal terminal, OptionSet options) throws Exception { terminal.println("Arguments: " + options.nonOptionArguments().toString()); } } DummyMultiCommand multiCommand; @Before public void setupCommand() { multiCommand = new DummyMultiCommand(); } @Override protected Command newCommand() { return multiCommand; } public void testNoCommandsConfigured() throws Exception { IllegalStateException e = expectThrows(IllegalStateException.class, () -> { execute(); }); assertEquals("No subcommands configured", e.getMessage()); } public void testUnknownCommand() throws Exception { multiCommand.subcommands.put("something", new DummySubCommand()); UserException e = expectThrows(UserException.class, () -> { execute("somethingelse"); }); assertEquals(ExitCodes.USAGE, e.exitCode); assertEquals("Unknown command [somethingelse]", e.getMessage()); } public void testMissingCommand() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); UserException e = expectThrows(UserException.class, () -> { execute(); }); assertEquals(ExitCodes.USAGE, e.exitCode); assertEquals("Missing command", e.getMessage()); } public void testHelp() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); multiCommand.subcommands.put("command2", new DummySubCommand()); execute("-h"); String output = terminal.getOutput(); assertTrue(output, output.contains("command1")); assertTrue(output, output.contains("command2")); } public void testSubcommandHelp() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); multiCommand.subcommands.put("command2", new DummySubCommand()); execute("command2", "-h"); String output = terminal.getOutput(); assertFalse(output, output.contains("command1")); assertTrue(output, output.contains("A dummy subcommand")); } public void testSubcommandArguments() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); execute("command1", "foo", "bar"); String output = terminal.getOutput(); assertFalse(output, output.contains("command1")); assertTrue(output, output.contains("Arguments: [foo, bar]")); } public void testClose() throws Exception { Command spySubCommand1 = spy(new DummySubCommand()); Command spySubCommand2 = spy(new DummySubCommand()); multiCommand.subcommands.put("command1", spySubCommand1); multiCommand.subcommands.put("command2", spySubCommand2); multiCommand.close(); verify(spySubCommand1, times(1)).close(); verify(spySubCommand2, times(1)).close(); } public void testCloseWhenSubCommandCloseThrowsException() throws Exception { Command subCommand1 = mock(DummySubCommand.class); Mockito.doThrow(new IOException()).when(subCommand1).close(); Command spySubCommand2 = spy(new DummySubCommand()); multiCommand.subcommands.put("command1", subCommand1); multiCommand.subcommands.put("command2", spySubCommand2); IOException ioe = null; try { multiCommand.close(); } catch (IOException e) { ioe = e; }	i like mockito when it is necessary but sometimes it isn't and i feel that way here. i'd suggest having an atomicboolean or atomicinteger that gets modified in the close method of a anonymous command class. then you can assert on that value.
protected void execute(Terminal terminal, OptionSet options) throws Exception { terminal.println("Arguments: " + options.nonOptionArguments().toString()); } } DummyMultiCommand multiCommand; @Before public void setupCommand() { multiCommand = new DummyMultiCommand(); } @Override protected Command newCommand() { return multiCommand; } public void testNoCommandsConfigured() throws Exception { IllegalStateException e = expectThrows(IllegalStateException.class, () -> { execute(); }); assertEquals("No subcommands configured", e.getMessage()); } public void testUnknownCommand() throws Exception { multiCommand.subcommands.put("something", new DummySubCommand()); UserException e = expectThrows(UserException.class, () -> { execute("somethingelse"); }); assertEquals(ExitCodes.USAGE, e.exitCode); assertEquals("Unknown command [somethingelse]", e.getMessage()); } public void testMissingCommand() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); UserException e = expectThrows(UserException.class, () -> { execute(); }); assertEquals(ExitCodes.USAGE, e.exitCode); assertEquals("Missing command", e.getMessage()); } public void testHelp() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); multiCommand.subcommands.put("command2", new DummySubCommand()); execute("-h"); String output = terminal.getOutput(); assertTrue(output, output.contains("command1")); assertTrue(output, output.contains("command2")); } public void testSubcommandHelp() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); multiCommand.subcommands.put("command2", new DummySubCommand()); execute("command2", "-h"); String output = terminal.getOutput(); assertFalse(output, output.contains("command1")); assertTrue(output, output.contains("A dummy subcommand")); } public void testSubcommandArguments() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); execute("command1", "foo", "bar"); String output = terminal.getOutput(); assertFalse(output, output.contains("command1")); assertTrue(output, output.contains("Arguments: [foo, bar]")); } public void testClose() throws Exception { Command spySubCommand1 = spy(new DummySubCommand()); Command spySubCommand2 = spy(new DummySubCommand()); multiCommand.subcommands.put("command1", spySubCommand1); multiCommand.subcommands.put("command2", spySubCommand2); multiCommand.close(); verify(spySubCommand1, times(1)).close(); verify(spySubCommand2, times(1)).close(); } public void testCloseWhenSubCommandCloseThrowsException() throws Exception { Command subCommand1 = mock(DummySubCommand.class); Mockito.doThrow(new IOException()).when(subCommand1).close(); Command spySubCommand2 = spy(new DummySubCommand()); multiCommand.subcommands.put("command1", subCommand1); multiCommand.subcommands.put("command2", spySubCommand2); IOException ioe = null; try { multiCommand.close(); } catch (IOException e) { ioe = e; }	same comment about not really needing mockito
protected void execute(Terminal terminal, OptionSet options) throws Exception { terminal.println("Arguments: " + options.nonOptionArguments().toString()); } } DummyMultiCommand multiCommand; @Before public void setupCommand() { multiCommand = new DummyMultiCommand(); } @Override protected Command newCommand() { return multiCommand; } public void testNoCommandsConfigured() throws Exception { IllegalStateException e = expectThrows(IllegalStateException.class, () -> { execute(); }); assertEquals("No subcommands configured", e.getMessage()); } public void testUnknownCommand() throws Exception { multiCommand.subcommands.put("something", new DummySubCommand()); UserException e = expectThrows(UserException.class, () -> { execute("somethingelse"); }); assertEquals(ExitCodes.USAGE, e.exitCode); assertEquals("Unknown command [somethingelse]", e.getMessage()); } public void testMissingCommand() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); UserException e = expectThrows(UserException.class, () -> { execute(); }); assertEquals(ExitCodes.USAGE, e.exitCode); assertEquals("Missing command", e.getMessage()); } public void testHelp() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); multiCommand.subcommands.put("command2", new DummySubCommand()); execute("-h"); String output = terminal.getOutput(); assertTrue(output, output.contains("command1")); assertTrue(output, output.contains("command2")); } public void testSubcommandHelp() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); multiCommand.subcommands.put("command2", new DummySubCommand()); execute("command2", "-h"); String output = terminal.getOutput(); assertFalse(output, output.contains("command1")); assertTrue(output, output.contains("A dummy subcommand")); } public void testSubcommandArguments() throws Exception { multiCommand.subcommands.put("command1", new DummySubCommand()); execute("command1", "foo", "bar"); String output = terminal.getOutput(); assertFalse(output, output.contains("command1")); assertTrue(output, output.contains("Arguments: [foo, bar]")); } public void testClose() throws Exception { Command spySubCommand1 = spy(new DummySubCommand()); Command spySubCommand2 = spy(new DummySubCommand()); multiCommand.subcommands.put("command1", spySubCommand1); multiCommand.subcommands.put("command2", spySubCommand2); multiCommand.close(); verify(spySubCommand1, times(1)).close(); verify(spySubCommand2, times(1)).close(); } public void testCloseWhenSubCommandCloseThrowsException() throws Exception { Command subCommand1 = mock(DummySubCommand.class); Mockito.doThrow(new IOException()).when(subCommand1).close(); Command spySubCommand2 = spy(new DummySubCommand()); multiCommand.subcommands.put("command1", subCommand1); multiCommand.subcommands.put("command2", spySubCommand2); IOException ioe = null; try { multiCommand.close(); } catch (IOException e) { ioe = e; }	use expectthrows here instead of the try catch
private void executeMasterOperation(Task task, Request request, ClusterState state, ActionListener<Response> listener) throws Exception { if (task instanceof CancellableTask && ((CancellableTask) task).isCancelled()) { throw new CancellationException("Task was cancelled"); } masterOperation(task, request, state, listener); }	i don't think we exercise this in the tests any more? at least i removed it and the tests still passed several hundred iterations.
private void cancelTask(SearchTask task, Exception exc) { CancelTasksRequest req = new CancelTasksRequest() .setTaskId(new TaskId(client.getLocalNodeId(), task.getId())) .setReason(exc.getMessage()); // force the origin to execute the cancellation as a system user new OriginSettingClient(client, TASKS_ORIGIN).admin().cluster().cancelTasks(req, ActionListener.wrap(() -> {})); }	maybe add some prefix to the message? i am always afraid that the exc message may be null or not so understandable, with a prefix we would know for sure where the cancel comes from.
* @param listener Returns true or failure */ public void groupExists(String groupId, ActionListener<Boolean> listener) { BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder(); boolQueryBuilder.filter(new TermQueryBuilder(Job.JOB_TYPE.getPreferredName(), Job.ANOMALY_DETECTOR_JOB_TYPE)); boolQueryBuilder.filter(new TermsQueryBuilder(Job.GROUPS.getPreferredName(), groupId)); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder() .query(boolQueryBuilder); sourceBuilder.fetchSource(false); SearchRequest searchRequest = client.prepareSearch(AnomalyDetectorsIndex.configIndexName()) .setIndicesOptions(IndicesOptions.lenientExpandOpen()) .setSource(sourceBuilder).request(); executeAsyncWithOrigin(client.threadPool().getThreadContext(), ML_ORIGIN, searchRequest, ActionListener.<SearchResponse>wrap( response -> { if (response.getHits().totalHits > 0) { listener.onResponse(Boolean.TRUE); } else { listener.onFailure(ExceptionsHelper.missingJobException(groupId)); } }, listener::onFailure) , client::search); }	i think you should make .size(0) on the search as we don't really care about any of the hits, just the total number of them.
* @param blobs list of blobs in the container */ protected void finalize(List<SnapshotFiles> snapshots, int fileListGeneration, ImmutableMap<String, BlobMetaData> blobs) { BlobStoreIndexShardSnapshots newSnapshots = new BlobStoreIndexShardSnapshots(snapshots); String newSnapshotIndexName = SNAPSHOT_INDEX_PREFIX + fileListGeneration; // If we deleted all snapshots - we don't need to create the index file if (snapshots.size() > 0) { try (OutputStream output = blobContainer.createOutput(SNAPSHOT_TEMP_PREFIX + fileListGeneration)) { XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON, output); newSnapshots.toXContent(builder, ToXContent.EMPTY_PARAMS); builder.flush(); } catch (IOException e) { throw new IndexShardSnapshotFailedException(shardId, "Failed to write file list", e); } try { blobContainer.move(SNAPSHOT_TEMP_PREFIX + fileListGeneration, newSnapshotIndexName); } catch (IOException e) { throw new IndexShardSnapshotFailedException(shardId, "Failed to rename file list", e); } } // now go over all the blobs, and if they don't exists in a snapshot, delete them for (String blobName : blobs.keySet()) { // delete old file lists if (blobName.startsWith(SNAPSHOT_TEMP_PREFIX) || blobName.startsWith(SNAPSHOT_INDEX_PREFIX)) { if (!newSnapshotIndexName.equals(blobName)) { try { blobContainer.deleteBlob(blobName); } catch (IOException e) { logger.debug("[{}] [{}] error deleting blob [{}] during cleanup", e, snapshotId, shardId, blobName); } } } else if (blobName.startsWith("__")) { if (newSnapshots.findNameFile(FileInfo.canonicalName(blobName)) == null) { try { blobContainer.deleteBlob(blobName); } catch (IOException e) { logger.debug("[{}] [{}] error deleting blob [{}] during cleanup", e, snapshotId, shardId, blobName); } } } } }	the snapshot file can be compressed, do you think this snapshot index file should be compressed too?
* @return BlobStoreIndexShardSnapshots */ protected Tuple<BlobStoreIndexShardSnapshots, Integer> buildBlobStoreIndexShardSnapshots(ImmutableMap<String, BlobMetaData> blobs) { int latest = -1; for (String name : blobs.keySet()) { if (name.startsWith(SNAPSHOT_INDEX_PREFIX)) { try { int gen = Integer.parseInt(name.substring(SNAPSHOT_INDEX_PREFIX.length())); if (gen > latest) { latest = gen; } } catch (NumberFormatException ex) { logger.warn("failed to parse index file name [{}]", name); } } } if (latest >= 0) { try (InputStream stream = blobContainer.openInput(SNAPSHOT_INDEX_PREFIX + latest)) { try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(stream)) { parser.nextToken(); return new Tuple<>(BlobStoreIndexShardSnapshots.fromXContent(parser), latest); } } catch (IOException e) { logger.warn("failed to read index file [{}]", e, SNAPSHOT_INDEX_PREFIX + latest); } } List<SnapshotFiles> snapshots = Lists.newArrayList(); for (String name : blobs.keySet()) { if (name.startsWith(SNAPSHOT_PREFIX)) { try (InputStream stream = blobContainer.openInput(name)) { BlobStoreIndexShardSnapshot snapshot = readSnapshot(stream); snapshots.add(new SnapshotFiles(snapshot.snapshot(), snapshot.indexFiles())); } catch (IOException e) { logger.warn("failed to read commit point [{}]", e, name); } } } return new Tuple<>(new BlobStoreIndexShardSnapshots(snapshots), -1); }	maybe use the method readsnapshot(stream) again?
private void consumeUser(User user, Map<Realm, Tuple<String, Exception>> messages) { if (user == null) { messages.forEach((realm, tuple) -> { final String message = tuple.v1(); final String cause = tuple.v2() == null ? "" : " (Caused by " + tuple.v2() + ")"; logger.warn("Authentication to realm {} failed - {}{}", realm.name(), message, cause); }); List<Realm> unlicensedRealms = realms.getUnlicensedRealms(); if (unlicensedRealms.isEmpty() == false) { logger.warn("Authentication failed." + " The following realms were skipped because they are not permitted on the current license: [{}]", Strings.collectionToCommaDelimitedString(unlicensedRealms)); } listener.onFailure(request.authenticationFailed(authenticationToken)); } else { threadContext.putTransient(AuthenticationResult.THREAD_CONTEXT_KEY, authenticationResult); if (runAsEnabled) { final String runAsUsername = threadContext.getHeader(AuthenticationServiceField.RUN_AS_USER_HEADER); if (runAsUsername != null && runAsUsername.isEmpty() == false) { lookupRunAsUser(user, runAsUsername, this::finishAuthentication); } else if (runAsUsername == null) { finishAuthentication(user); } else { assert runAsUsername.isEmpty() : "the run as username may not be empty"; logger.debug("user [{}] attempted to runAs with an empty username", user.principal()); listener.onFailure(request.runAsDenied( new Authentication(new User(runAsUsername, null, user), authenticatedBy, lookedupBy), authenticationToken)); } } else { finishAuthentication(user); } } }	maybe we can split this to two warnings, one saying that authentication failed because user could not be authenticated to any of the available realms (which we can log either way) and another saying that these x realms were skipped even though configured as they are not permitted in the current license ?
@Override public void readFrom(StreamInput in) throws IOException { name = in.readString(); filter = in.readOptionalString(); indexRouting = in.readOptionalString(); searchRouting = in.readOptionalString(); if (in.getVersion().onOrAfter(Version.V_7_0_0_alpha1)) { writeIndex = in.readOptionalBoolean(); } }	set to null on an else clause? these are mutable, so just to be strict.
static IndicesAliasesClusterStateUpdateRequest prepareRolloverAliasesUpdateRequest(String oldIndex, String newIndex, RolloverRequest request) { List<AliasAction> actions = unmodifiableList(Arrays.asList( new AliasAction.Add(newIndex, request.getAlias(), null, null, null, null), new AliasAction.Remove(oldIndex, request.getAlias()))); final IndicesAliasesClusterStateUpdateRequest updateRequest = new IndicesAliasesClusterStateUpdateRequest(actions) .ackTimeout(request.ackTimeout()) .masterNodeTimeout(request.masterNodeTimeout()); return updateRequest; }	we can do this in the follow up but we shouldn't forget to carry over the index write flag of the alias.
@Override protected void assertEqualInstances(AliasMetaData expectedInstance, AliasMetaData newInstance) { assertNotSame(newInstance, expectedInstance); if (expectedInstance.writeIndex() == null) { expectedInstance = AliasMetaData.builder(expectedInstance.alias()) .filter(expectedInstance.filter()) .indexRouting(expectedInstance.indexRouting()) .searchRouting(expectedInstance.searchRouting()) .writeIndex(Boolean.FALSE) .build(); } assertEquals(expectedInstance, newInstance); assertEquals(expectedInstance.hashCode(), newInstance.hashCode()); }	why is that force false?
private static AliasMetaData createTestItem() { Builder builder = AliasMetaData.builder(randomAlphaOfLengthBetween(3, 10)); if (randomBoolean()) { builder.routing(randomAlphaOfLengthBetween(3, 10)); } if (randomBoolean()) { builder.searchRouting(randomAlphaOfLengthBetween(3, 10)); } if (randomBoolean()) { builder.indexRouting(randomAlphaOfLengthBetween(3, 10)); } if (randomBoolean()) { builder.filter("{\\\\"term\\\\":{\\\\"year\\\\":2016}}"); } builder.writeIndex(randomBoolean()); return builder.build(); }	sometime make it null?
public void testAddWriteOnlyWithNoExistingAliases() { ClusterState before = createIndex(ClusterState.builder(ClusterName.DEFAULT).build(), "test"); ClusterState after = service.innerExecute(before, Arrays.asList( new AliasAction.Add("test", "alias", null, null, null, false))); assertFalse(after.metaData().index("test").getAliases().get("alias").writeIndex()); assertThat(((AliasOrIndex.Alias) after.metaData().getAliasAndIndexLookup().get("alias")).getWriteIndex(), equalTo(after.metaData().index("test"))); after = service.innerExecute(before, Arrays.asList( new AliasAction.Add("test", "alias", null, null, null, null))); assertNull(after.metaData().index("test").getAliases().get("alias").writeIndex()); assertThat(((AliasOrIndex.Alias) after.metaData().getAliasAndIndexLookup().get("alias")).getWriteIndex(), equalTo(after.metaData().index("test"))); after = service.innerExecute(before, Arrays.asList( new AliasAction.Add("test", "alias", null, null, null, true))); assertTrue(after.metaData().index("test").getAliases().get("alias").writeIndex()); assertThat(((AliasOrIndex.Alias) after.metaData().getAliasAndIndexLookup().get("alias")).getWriteIndex(), equalTo(after.metaData().index("test"))); }	i think we need more tests here - adding a write index to a group of non write indexes. removing an index from a default group of two indices (and seeing there is a write index)
@Override protected void masterOperation(final GetRepositoriesRequest request, ClusterState state, final ActionListener<GetRepositoriesResponse> listener) { MetaData metaData = state.metaData(); RepositoriesMetaData repositories = metaData.custom(RepositoriesMetaData.TYPE); if (request.repositories().length == 0 || (request.repositories().length == 1 && "_all".equals(request.repositories()[0]))) { if (repositories != null) { listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData(repositories.repositories()))); } else { listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData(Collections.<RepositoryMetaData>emptyList()))); } } else { if (repositories != null) { Set<String> repositoriesToGet = new LinkedHashSet<>(); // to keep insertion order for (String repositoryOrPattern : request.repositories()) { if (Regex.isSimpleMatchPattern(repositoryOrPattern) == false) { repositoriesToGet.add(repositoryOrPattern); } else { for (RepositoryMetaData repository : repositories.repositories()) { if (Regex.simpleMatch(repositoryOrPattern, repository.name())) { repositoriesToGet.add(repository.name()); } } } } List<RepositoryMetaData> repositoryListBuilder = new ArrayList<>(); for (String repository : repositoriesToGet) { RepositoryMetaData repositoryMetaData = repositories.repository(repository); if (repositoryMetaData == null) { listener.onFailure(new RepositoryMissingException(repository)); return; } repositoryListBuilder.add(repositoryMetaData); } listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData((repositoryListBuilder)))); } else { listener.onFailure(new RepositoryMissingException(request.repositories()[0])); } } }	nit: there is an extra set of parens
@Override protected void masterOperation(final GetRepositoriesRequest request, ClusterState state, final ActionListener<GetRepositoriesResponse> listener) { MetaData metaData = state.metaData(); RepositoriesMetaData repositories = metaData.custom(RepositoriesMetaData.TYPE); if (request.repositories().length == 0 || (request.repositories().length == 1 && "_all".equals(request.repositories()[0]))) { if (repositories != null) { listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData(repositories.repositories()))); } else { listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData(Collections.<RepositoryMetaData>emptyList()))); } } else { if (repositories != null) { Set<String> repositoriesToGet = new LinkedHashSet<>(); // to keep insertion order for (String repositoryOrPattern : request.repositories()) { if (Regex.isSimpleMatchPattern(repositoryOrPattern) == false) { repositoriesToGet.add(repositoryOrPattern); } else { for (RepositoryMetaData repository : repositories.repositories()) { if (Regex.simpleMatch(repositoryOrPattern, repository.name())) { repositoriesToGet.add(repository.name()); } } } } List<RepositoryMetaData> repositoryListBuilder = new ArrayList<>(); for (String repository : repositoriesToGet) { RepositoryMetaData repositoryMetaData = repositories.repository(repository); if (repositoryMetaData == null) { listener.onFailure(new RepositoryMissingException(repository)); return; } repositoryListBuilder.add(repositoryMetaData); } listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData((repositoryListBuilder)))); } else { listener.onFailure(new RepositoryMissingException(request.repositories()[0])); } } }	why is a new repositoriesmetadata created instead of using the existing?
@Override protected void masterOperation(final GetRepositoriesRequest request, ClusterState state, final ActionListener<GetRepositoriesResponse> listener) { MetaData metaData = state.metaData(); RepositoriesMetaData repositories = metaData.custom(RepositoriesMetaData.TYPE); if (request.repositories().length == 0 || (request.repositories().length == 1 && "_all".equals(request.repositories()[0]))) { if (repositories != null) { listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData(repositories.repositories()))); } else { listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData(Collections.<RepositoryMetaData>emptyList()))); } } else { if (repositories != null) { Set<String> repositoriesToGet = new LinkedHashSet<>(); // to keep insertion order for (String repositoryOrPattern : request.repositories()) { if (Regex.isSimpleMatchPattern(repositoryOrPattern) == false) { repositoriesToGet.add(repositoryOrPattern); } else { for (RepositoryMetaData repository : repositories.repositories()) { if (Regex.simpleMatch(repositoryOrPattern, repository.name())) { repositoriesToGet.add(repository.name()); } } } } List<RepositoryMetaData> repositoryListBuilder = new ArrayList<>(); for (String repository : repositoriesToGet) { RepositoryMetaData repositoryMetaData = repositories.repository(repository); if (repositoryMetaData == null) { listener.onFailure(new RepositoryMissingException(repository)); return; } repositoryListBuilder.add(repositoryMetaData); } listener.onResponse(new GetRepositoriesResponse(new RepositoriesMetaData((repositoryListBuilder)))); } else { listener.onFailure(new RepositoryMissingException(request.repositories()[0])); } } }	is the explicit generic needed? it should be inferred by the compiler?
private Table buildTable(RestRequest req, GetRepositoriesResponse getRepositoriesResponse) { Table table = getTableWithHeader(req); for (RepositoryMetaData repositoryMetaData : getRepositoriesResponse.repositories().repositories()) { table.startRow(); table.addCell(repositoryMetaData.name()); table.addCell(repositoryMetaData.type()); table.endRow(); } return table; }	repositories().repositories() looks very confusing. maybe the getrepositoriesresponse.repositories() should continue to return the list, internally calling repositories.repositories()?
public static SearchRequest prepareRequest(Client client, SearchSourceBuilder source, TimeValue timeout, boolean includeFrozen, String... indices) { source.timeout(timeout); SearchRequest searchRequest = new SearchRequest(MISSING_ORDER_IN_COMPOSITE_AGGS_VERSION); searchRequest.indices(indices); searchRequest.source(source); searchRequest.allowPartialSearchResults(false); searchRequest.indicesOptions( includeFrozen ? IndexResolver.FIELD_CAPS_FROZEN_INDICES_OPTIONS : IndexResolver.FIELD_CAPS_INDICES_OPTIONS); return searchRequest; }	not sure wether this is necessary, in theory there is only a few queries that need missing_order, most queries would still succeed on a node without this capability.
@Override public final FieldMapper merge(Mapper mergeWith) { FieldMapper merged = clone(); List<String> conflicts = new ArrayList<>(); if (mergeWith instanceof FieldMapper == false) { throw new IllegalArgumentException("mapper [" + fieldType.name() + "] cannot be changed from type [" + contentType() + "] to [" + mergeWith.getClass().getSimpleName() + "]"); } merged.mergeSharedOptions((FieldMapper)mergeWith, conflicts); if (conflicts.isEmpty() == false) { throw new IllegalArgumentException("Mapper for [" + name() + "] conflicts with existing mapping:\\\\n" + conflicts.toString()); } return merged; }	small comment, calling mergeoptions directly from here could be nice: fieldmapper fieldmapper = (fieldmapper) mergewith; merged.mergesharedoptions(fieldmapper, conflicts); merged.mergeoptions(fieldmapper, conflicts);
private void mergeSharedOptions(FieldMapper mergeWith, List<String> conflicts) { if (Objects.equals(this.contentType(), mergeWith.contentType()) == false) { throw new IllegalArgumentException("mapper [" + fieldType().name() + "] cannot be changed from type [" + contentType() + "] to [" + mergeWith.contentType() + "]"); } MappedFieldType other = mergeWith.fieldType; boolean indexed = fieldType.indexOptions() != IndexOptions.NONE; boolean mergeWithIndexed = other.indexOptions() != IndexOptions.NONE; // TODO: should be validating if index options go "up" (but "down" is ok) if (indexed != mergeWithIndexed) { conflicts.add("mapper [" + name() + "] has different [index] values"); } if (fieldType.stored() != other.stored()) { conflicts.add("mapper [" + name() + "] has different [store] values"); } if (fieldType.hasDocValues() != other.hasDocValues()) { conflicts.add("mapper [" + name() + "] has different [doc_values] values"); } if (fieldType.omitNorms() && !other.omitNorms()) { conflicts.add("mapper [" + name() + "] has different [norms] values, cannot change from disable to enabled"); } if (fieldType.storeTermVectors() != other.storeTermVectors()) { conflicts.add("mapper [" + name() + "] has different [store_term_vector] values"); } if (fieldType.storeTermVectorOffsets() != other.storeTermVectorOffsets()) { conflicts.add("mapper [" + name() + "] has different [store_term_vector_offsets] values"); } if (fieldType.storeTermVectorPositions() != other.storeTermVectorPositions()) { conflicts.add("mapper [" + name() + "] has different [store_term_vector_positions] values"); } if (fieldType.storeTermVectorPayloads() != other.storeTermVectorPayloads()) { conflicts.add("mapper [" + name() + "] has different [store_term_vector_payloads] values"); } // null and "default"-named index analyzers both mean the default is used if (fieldType.indexAnalyzer() == null || "default".equals(fieldType.indexAnalyzer().name())) { if (other.indexAnalyzer() != null && "default".equals(other.indexAnalyzer().name()) == false) { conflicts.add("mapper [" + name() + "] has different [analyzer]"); } } else if (other.indexAnalyzer() == null || "default".equals(other.indexAnalyzer().name())) { conflicts.add("mapper [" + name() + "] has different [analyzer]"); } else if (fieldType.indexAnalyzer().name().equals(other.indexAnalyzer().name()) == false) { conflicts.add("mapper [" + name() + "] has different [analyzer]"); } if (Objects.equals(fieldType.similarity(), other.similarity()) == false) { conflicts.add("mapper [" + name() + "] has different [similarity]"); } mergeOptions(mergeWith, conflicts); if (conflicts.isEmpty()) { multiFields = multiFields.merge(mergeWith.multiFields); // apply changeable values this.fieldType = mergeWith.fieldType; this.copyTo = mergeWith.copyTo; } }	great that this is abstract, i like how it encourages implementors to think through what can be updated vs. not.
public void testDeleteIndex() { String[] indices = IndicesClientIT.randomIndicesNames(0, 5); DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(indices); Map<String, String> expectedParams = new HashMap<>(); setRandomTimeout(deleteIndexRequest::timeout, AcknowledgedRequest.DEFAULT_ACK_TIMEOUT, expectedParams); setRandomMasterTimeout(deleteIndexRequest, expectedParams); setRandomIndicesOptions(deleteIndexRequest::indicesOptions, deleteIndexRequest::indicesOptions, expectedParams); Request request = Request.deleteIndex(deleteIndexRequest); assertEquals("/" + String.join(",", indices), request.getEndpoint()); assertEquals(expectedParams, request.getParameters()); assertEquals("DELETE", request.getMethod()); assertNull(request.getEntity()); }	i am not too happy with sharing this method between an integration test and this unit test. furthermore, i have second thoughts on the randomization of indices in the open close index test. maybe we should rather have randomization only here, as in a unit test it fits better, while in an integration test it can cause noise as there are many more moving parts.
public static boolean isDedicatedClient(Mode mode) { return mode == JDBC || mode == ODBC || mode == CLI; }	return isdriver(mode) || mode == cli
@Override public void readFrom(StreamInput in) throws IOException { term = in.readString(); startOffset = in.readInt(); endOffset = in.readInt(); position = in.readVInt(); Integer len = in.readOptionalVInt(); if (len != null) { positionLength = len; } else { positionLength = 1; } type = in.readOptionalString(); if (in.getVersion().onOrAfter(Version.V_2_2_0)) { attributes = (Map<String, Object>) in.readGenericValue(); } }	do we need any versioning here? are we ever expected to be able to readfrom a node running an older es version that doesn't send the poslen?
* @param clusterAlias the remote cluster alias * @param leaderIndex the name of the leader index * @param onFailure the failure consumer * @param consumer the consumer for supplying the leader index metadata and historyUUIDs of all leader shards */ public void checkRemoteClusterLicenseAndFetchLeaderIndexMetadataAndHistoryUUIDs( final Client client, final String clusterAlias, final String leaderIndex, final Consumer<Exception> onFailure, final BiConsumer<String[], Tuple<ClusterState, IndexMetaData>> consumer) { final ClusterStateRequest request = new ClusterStateRequest(); request.clear(); request.metaData(true); request.indices(leaderIndex); checkRemoteClusterLicenseAndFetchClusterState( client, clusterAlias, client.getRemoteClusterClient(clusterAlias), request, onFailure, remoteClusterStateResponse -> { ClusterState remoteClusterState = remoteClusterStateResponse.getState(); IndexMetaData leaderIndexMetaData = remoteClusterState.getMetaData().index(leaderIndex); if (leaderIndexMetaData == null) { onFailure.accept(new IndexNotFoundException(leaderIndex)); return; } final Client remoteClient = client.getRemoteClusterClient(clusterAlias); hasPrivilegesToFollowIndices(remoteClient, new String[] {leaderIndex}, e -> { if (e == null) { fetchLeaderHistoryUUIDs(remoteClient, leaderIndexMetaData, onFailure, historyUUIDs -> consumer.accept(historyUUIDs, new Tuple<>(remoteClusterState, leaderIndexMetaData))); } else { onFailure.accept(e); } }); }, licenseCheck -> indexMetadataNonCompliantRemoteLicense(leaderIndex, licenseCheck), e -> indexMetadataUnknownRemoteLicense(leaderIndex, clusterAlias, e)); } /** * Fetches the leader cluster state from the remote cluster by the specified cluster state request. Before fetching the cluster state, * the remote cluster is checked for license compliance with CCR. If the remote cluster is not licensed for CCR, * the {@code onFailure}	don't you need to set .nodes(true) on the cluster state request in order to get the minnodeversion? why is this not failing any tests?
private void createFollowerIndex( final Tuple<ClusterState, IndexMetaData> metaDataTuple, final String [] historyUUID, final PutFollowAction.Request request, final ActionListener<PutFollowAction.Response> listener, final Version localClusterMinNodeVersion) { IndexMetaData leaderIndexMetaData = metaDataTuple.v2(); if (leaderIndexMetaData == null) { listener.onFailure(new IllegalArgumentException("leader index [" + request.getLeaderIndex() + "] does not exist")); return; } if (IndexSettings.INDEX_SOFT_DELETES_SETTING.get(leaderIndexMetaData.getSettings()) == false) { listener.onFailure( new IllegalArgumentException("leader index [" + request.getLeaderIndex() + "] does not have soft deletes enabled. " + "soft deletes must be enabled when the index is created by setting " + IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey() + " to true")); return; } boolean pre67CompatibilityMode = localClusterMinNodeVersion.before(Version.V_6_7_0) || metaDataTuple.v1().getNodes().getMinNodeVersion().before(Version.V_6_7_0); if (pre67CompatibilityMode) { logger.warn("Pre-6.7 nodes present in local/remote cluster. Cannot bootstrap from remote. Creating empty follower index [{}] " + "and initiating following [{}, {}].", request.getFollowRequest().getFollowerIndex(), request.getRemoteCluster(), request.getLeaderIndex()); pre67PutFollow.doPre67PutFollow(request, leaderIndexMetaData, historyUUID, listener); } else { final Settings.Builder settingsBuilder = Settings.builder() .put(IndexMetaData.SETTING_INDEX_PROVIDED_NAME, request.getFollowRequest().getFollowerIndex()) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true); final String leaderClusterRepoName = CcrRepository.NAME_PREFIX + request.getRemoteCluster(); final RestoreSnapshotRequest restoreRequest = new RestoreSnapshotRequest(leaderClusterRepoName, CcrRepository.LATEST) .indices(request.getLeaderIndex()).indicesOptions(request.indicesOptions()).renamePattern("^(.*)$") .renameReplacement(request.getFollowRequest().getFollowerIndex()).masterNodeTimeout(request.masterNodeTimeout()) .indexSettings(settingsBuilder); final Client clientWithHeaders = CcrLicenseChecker.wrapClient(this.client, threadPool.getThreadContext().getHeaders()); threadPool.executor(ThreadPool.Names.SNAPSHOT).execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { listener.onFailure(e); } @Override protected void doRun() throws Exception { restoreService.restoreSnapshot(restoreRequest, new ActionListener<RestoreService.RestoreCompletionResponse>() { @Override public void onResponse(RestoreService.RestoreCompletionResponse response) { afterRestoreStarted(clientWithHeaders, request, listener, response); } @Override public void onFailure(Exception e) { listener.onFailure(e); } }); } }); } }	why log a warning? isn't this just a normal thing? log at debug level here.
private PluginsService newPluginService(final Settings settings) { final Settings.Builder settingsBuilder = settingsBuilder() .put(NettyTransport.PING_SCHEDULE.getKey(), "5s") // enable by default the transport schedule ping interval .put(InternalSettingsPreparer.prepareSettings(settings)) .put(NetworkService.NETWORK_SERVER.getKey(), false) //nocommit not too sure if these settings are needed here... .put(Node.NODE_MASTER_SETTING.getKey(), false) .put(Node.NODE_DATA_SETTING.getKey(), false) .put(Node.NODE_INGEST_SETTING.getKey(), false) .put(CLIENT_TYPE_SETTING_S.getKey(), CLIENT_TYPE); return new PluginsService(settingsBuilder.build(), null, null, pluginClasses); }	why do we need this, we know that we are a transport client, can't we make this explicit somehow?
* @param hostAddress the nodes host address * @param address the nodes transport address * @param attributes node attributes * @param version the version of the node. */ public DiscoveryNode(String nodeName, String nodeId, String hostName, String hostAddress, TransportAddress address, ImmutableOpenMap<String, String> attributes, Version version) { this(nodeName, nodeId, hostName, hostAddress, address, copyAttributes(attributes), version); } private DiscoveryNode(String nodeName, String nodeId, String hostName, String hostAddress, TransportAddress address, ImmutableOpenMap.Builder<String, String> attributesBuilder, Version version) { if (nodeName != null) { this.nodeName = nodeName.intern(); } else { this.nodeName = ""; } this.nodeId = nodeId.intern(); this.hostName = hostName.intern(); this.hostAddress = hostAddress.intern(); this.address = address; if (version == null) { this.version = Version.CURRENT; } else { this.version = version; } this.attributes = attributesBuilder.build(); this.roles = resolveRoles(this.attributes); } private static ImmutableOpenMap.Builder<String, String> copyAttributes(ImmutableOpenMap<String, String> attributes) { //we could really use copyOf and get rid of this method but we call String#intern while copying... ImmutableOpenMap.Builder<String, String> builder = ImmutableOpenMap.builder(); for (ObjectObjectCursor<String, String> entry : attributes) { builder.put(entry.key.intern(), entry.value.intern()); } return builder; }	can we make a collections.immutableset here?
public static DiscoveryNode readNode(StreamInput in) throws IOException { return PROTOTYPE.readFrom(in); }	can we get rid of prototype and remove the readnode method alltogether. i thikn we should just have a discoverynode(streaminput in) method and do all the reading there.
Map<String, String> buildAttributes() { Map<String, String> attributes = new HashMap<>(Node.NODE_ATTRIBUTES.get(this.settings).getAsMap()); attributes.remove("name"); // name is extracted in other places if (attributes.containsKey("client")) { throw new IllegalArgumentException("node.client setting is no longer supported, use " + Node.NODE_MASTER_SETTING.getKey() + ", " + Node.NODE_DATA_SETTING.getKey() + " and " + Node.NODE_INGEST_SETTING.getKey() + " explicitly instead"); } //nocommit why don't we remove master as well if it's true? and ingest? if (attributes.containsKey(DiscoveryNode.Role.DATA.getRoleName())) { if (attributes.get(DiscoveryNode.Role.DATA.getRoleName()).equals("true")) { attributes.remove(DiscoveryNode.Role.DATA.getRoleName()); } } for (CustomAttributesProvider provider : customAttributesProviders) { try { Map<String, String> customAttributes = provider.buildAttributes(); if (customAttributes != null) { for (Map.Entry<String, String> entry : customAttributes.entrySet()) { if (!attributes.containsKey(entry.getKey())) { attributes.put(entry.getKey(), entry.getValue()); } } } } catch (Exception e) { logger.warn("failed to build custom attributes from provider [{}]", e, provider); } } return attributes; }	dude this entire method makes no sense to me. can we just make sure we make these attributes dense rather than sparse and just fill all the roles for every node?
private DiscoveryNode findMaster() { logger.trace("starting to ping"); ZenPing.PingResponse[] fullPingResponses = pingService.pingAndWait(pingTimeout); if (fullPingResponses == null) { logger.trace("No full ping responses"); return null; } if (logger.isTraceEnabled()) { StringBuilder sb = new StringBuilder(); if (fullPingResponses.length == 0) { sb.append(" {none}"); } else { for (ZenPing.PingResponse pingResponse : fullPingResponses) { sb.append("\\\\n\\\\t--> ").append(pingResponse); } } logger.trace("full ping responses:{}", sb); } // filter responses List<ZenPing.PingResponse> pingResponses = new ArrayList<>(); for (ZenPing.PingResponse pingResponse : fullPingResponses) { DiscoveryNode node = pingResponse.node(); //nocommit we should rename this and its setting, also we ignore node.ingest, but maybe it's ok here if (masterElectionFilterClientNodes && node.masterNode() == false && node.dataNode() == false) { // filter out nodes that don't hold data and are not master eligible } else if (masterElectionFilterDataNodes && node.masterNode() == false && node.dataNode()) { // filter out dedicated data nodes } else { pingResponses.add(pingResponse); } } if (logger.isDebugEnabled()) { StringBuilder sb = new StringBuilder(); if (pingResponses.isEmpty()) { sb.append(" {none}"); } else { for (ZenPing.PingResponse pingResponse : pingResponses) { sb.append("\\\\n\\\\t--> ").append(pingResponse); } } logger.debug("filtered ping responses: (filter_client[{}], filter_data[{}]){}", masterElectionFilterClientNodes, masterElectionFilterDataNodes, sb); } final DiscoveryNode localNode = clusterService.localNode(); List<DiscoveryNode> pingMasters = new ArrayList<>(); for (ZenPing.PingResponse pingResponse : pingResponses) { if (pingResponse.master() != null) { // We can't include the local node in pingMasters list, otherwise we may up electing ourselves without // any check / verifications from other nodes in ZenDiscover#innerJoinCluster() if (!localNode.equals(pingResponse.master())) { pingMasters.add(pingResponse.master()); } } } // nodes discovered during pinging Set<DiscoveryNode> activeNodes = new HashSet<>(); // nodes discovered who has previously been part of the cluster and do not ping for the very first time Set<DiscoveryNode> joinedOnceActiveNodes = new HashSet<>(); if (localNode.masterNode()) { activeNodes.add(localNode); long joinsCounter = clusterJoinsCounter.get(); if (joinsCounter > 0) { logger.trace("adding local node to the list of active nodes that have previously joined the cluster (joins counter is [{}])", joinsCounter); joinedOnceActiveNodes.add(localNode); } } for (ZenPing.PingResponse pingResponse : pingResponses) { activeNodes.add(pingResponse.node()); if (pingResponse.hasJoinedOnce()) { joinedOnceActiveNodes.add(pingResponse.node()); } } if (pingMasters.isEmpty()) { if (electMaster.hasEnoughMasterNodes(activeNodes)) { // we give preference to nodes who have previously already joined the cluster. Those will // have a cluster state in memory, including an up to date routing table (which is not persistent to disk // by the gateway) DiscoveryNode master = electMaster.electMaster(joinedOnceActiveNodes); if (master != null) { return master; } return electMaster.electMaster(activeNodes); } else { // if we don't have enough master nodes, we bail, because there are not enough master to elect from logger.trace("not enough master nodes [{}]", activeNodes); return null; } } else { assert !pingMasters.contains(localNode) : "local node should never be elected as master when other nodes indicate an active master"; // lets tie break between discovered nodes return electMaster.electMaster(pingMasters); } }	not sure what you mean by that nocommit?
public static void checkSegmentInfoIntegrity(final Directory directory) throws IOException { new SegmentInfos.FindSegmentsFile(directory) { @Override protected Object doBody(String segmentFileName) throws IOException { try (IndexInput input = directory.openInput(segmentFileName, IOContext.READ)) { final int format = input.readInt(); if (format == CodecUtil.CODEC_MAGIC) { CodecUtil.checksumEntireFile(input); } // legacy.... } return null; } }.run(); }	can we drop that if (format == codecutil.codec_magic) { here as well i mean we can't even pretend we checked it no?
public void testKerberosRealmWithInvalidKeytabPathConfigurations() throws IOException { final String keytabPathCase = randomFrom("keytabPathAsDirectory", "keytabFileDoesNotExist", "keytabPathWithNoReadPermissions"); final String expectedErrorMessage; final String keytabPath; final Set<PosixFilePermission> filePerms; switch (keytabPathCase) { case "keytabPathAsDirectory": final String dirName = randomAlphaOfLength(5); Files.createDirectory(dir.resolve(dirName)); keytabPath = dir.resolve(dirName).toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] is a directory"; break; case "keytabFileDoesNotExist": keytabPath = dir.resolve(randomAlphaOfLength(5) + ".keytab").toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] does not exist"; break; case "keytabPathWithNoReadPermissions": final String fileName = randomAlphaOfLength(5); final Path keytabFilePath = Files.createTempFile(dir, fileName, ".keytab"); Files.write(keytabFilePath, randomAlphaOfLength(5).getBytes(StandardCharsets.UTF_8)); final Set<String> supportedAttributes = keytabFilePath.getFileSystem().supportedFileAttributeViews(); if (supportedAttributes.contains("posix")) { final PosixFileAttributeView fileAttributeView = Files.getFileAttributeView(keytabFilePath, PosixFileAttributeView.class); fileAttributeView.setPermissions(PosixFilePermissions.fromString("---------")); } else if (supportedAttributes.contains("acl")) { final UserPrincipal principal = Files.getOwner(keytabFilePath); final AclFileAttributeView view = Files.getFileAttributeView(keytabFilePath, AclFileAttributeView.class); final AclEntry entry = AclEntry.newBuilder() .setType(AclEntryType.DENY) .setPrincipal(principal) .setPermissions(AclEntryPermission.READ_DATA, AclEntryPermission.READ_ATTRIBUTES).build(); final List<AclEntry> acl = view.getAcl(); acl.add(0, entry); view.setAcl(acl); } else { throw new UnsupportedOperationException("Unsupported file attributes for this test"); } keytabPath = keytabFilePath.toString(); expectedErrorMessage = "configured service key tab file [" + keytabPath + "] must have read permission"; break; default: throw new IllegalArgumentException("Unknown test case :" + keytabPathCase); } settings = KerberosTestCase.buildKerberosRealmSettings(keytabPath, 100, "10m", true, randomBoolean()); config = new RealmConfig("test-kerb-realm", settings, globalSettings, TestEnvironment.newEnvironment(globalSettings), new ThreadContext(globalSettings)); mockNativeRoleMappingStore = roleMappingStore(Arrays.asList("user")); mockKerberosTicketValidator = mock(KerberosTicketValidator.class); final IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () -> new KerberosRealm(config, mockNativeRoleMappingStore, mockKerberosTicketValidator, threadPool, null)); assertThat(iae.getMessage(), is(equalTo(expectedErrorMessage))); }	i think this message should be a bit more clear. can you include: - the path - the supportedattributes - some explanation about what attributes we're looking for it can just be "don't know how to make file {} non-readable on a filesystem with attributes {}"
@Override protected ClusterStatsNodeResponse nodeOperation(ClusterStatsNodeRequest nodeRequest) { NodeInfo nodeInfo = nodeService.info(false, true, false, true, false, true, false, true, false, true); NodeStats nodeStats = nodeService.stats(CommonStatsFlags.NONE, false, true, true, false, true, false, false, false, false, false, false); List<ShardStats> shardsStats = new ArrayList<>(); for (IndexService indexService : indicesService) { for (IndexShard indexShard : indexService) { if (indexShard.routingEntry() != null && indexShard.routingEntry().active()) { // only report on fully started shards shardsStats.add(new ShardStats(indexShard.routingEntry(), indexShard.shardPath(), new CommonStats(indicesService.getIndicesQueryCache(), indexShard, SHARD_STATS_FLAGS), indexShard.commitStats())); } } } ClusterHealthStatus clusterStatus = null; if (clusterService.state().nodes().isLocalNodeElectedMaster()) { clusterStatus = new ClusterStateHealth(clusterService.state()).getStatus(); } return new ClusterStatsNodeResponse(nodeInfo.getNode(), clusterStatus, nodeInfo, nodeStats, shardsStats.toArray(new ShardStats[shardsStats.size()])); }	given all the falses here, do we want to default to false too? indexingbuffer seems too low level for clusterstats
private NodeInfo createNodeInfo() { Build build = Build.CURRENT; DiscoveryNode node = new DiscoveryNode("test_node", DummyTransportAddress.INSTANCE, emptyMap(), emptySet(), VersionUtils.randomVersion(random())); Map<String, String> serviceAttributes = new HashMap<>(); serviceAttributes.put("test", "attribute"); Settings settings = Settings.builder().put("test", "setting").build(); OsInfo osInfo = DummyOsInfo.INSTANCE; ProcessInfo process = new ProcessInfo(randomInt(), randomBoolean()); JvmInfo jvm = JvmInfo.jvmInfo(); List<ThreadPool.Info> threadPoolInfos = new ArrayList<>(); threadPoolInfos.add(new ThreadPool.Info("test_threadpool", ThreadPool.ThreadPoolType.FIXED, 5)); ThreadPoolInfo threadPoolInfo = new ThreadPoolInfo(threadPoolInfos); Map<String, BoundTransportAddress> profileAddresses = new HashMap<>(); BoundTransportAddress dummyBoundTransportAddress = new BoundTransportAddress(new TransportAddress[]{DummyTransportAddress.INSTANCE}, DummyTransportAddress.INSTANCE); profileAddresses.put("test_address", dummyBoundTransportAddress); TransportInfo transport = new TransportInfo(dummyBoundTransportAddress, profileAddresses); HttpInfo htttpInfo = new HttpInfo(dummyBoundTransportAddress, randomLong()); PluginsAndModules plugins = new PluginsAndModules(); plugins.addModule(DummyPluginInfo.INSTANCE); plugins.addPlugin(DummyPluginInfo.INSTANCE); IngestInfo ingestInfo = new IngestInfo(Collections.emptyList()); return new NodeInfo(VersionUtils.randomVersion(random()), build, node, serviceAttributes, settings, osInfo, process, jvm, threadPoolInfo, transport, htttpInfo, plugins, ingestInfo, null); }	don't we want to randomly pass some byte size for indexing buffer?
public final void collectBucket(LeafBucketCollector subCollector, int doc, long bucketOrd) throws IOException { grow(bucketOrd + 1); collectExistingBucket(subCollector, doc, bucketOrd); } /** * Same as {@link #collectBucket(LeafBucketCollector, int, long)}	hm, i think this isn't quite right. doc is the doc id, so that could be all over the place, and also all going into the same bucket. i think there are two options here: 1. down below, we do if (doccounts.increment(bucketord, 1) == 1) { <breaker stuff> } which i think will work because the increment method returns the count after incrementing. so if we have a doc count of 1, it's the first doc and a new bucket so we can account it 2. alternatively, we could just account for it up in collectbucket without a conditional, since theoretically that should only be called on new buckets. it's not guaranteed by the api but in practice that's how aggs use it. there are two other issues we need to address though: 1. the old breaker logic only checked every 1024 buckets, since checking the real-memory breaker has a certain amount of overhead. so we should re-implement that somehow 2. trickier situation which i didn't think about when suggesting bucketsaggregator... if we add the 1024 threshold back, it's only a local count so aggs with 1023 buckets will never trigger the breaker even if the overall query has millions of buckets. perhaps we continue to use the multibucketconsumer service thing, but move the breaker accounting to a different method? that way it could maintain the global count and bucketsaggregator just calls a method on it or something? not sure, we can discuss more offline
public void testClusterHealthYellowClusterLevel() throws IOException { createIndex("index", Settings.EMPTY); createIndex("index2", Settings.EMPTY); ClusterHealthRequest request = new ClusterHealthRequest(); request.timeout("5s"); ClusterHealthResponse response = execute(request, highLevelClient().cluster()::health, highLevelClient().cluster()::healthAsync); logger.info("Shard stats\\\\n{}", EntityUtils.toString( client().performRequest(new Request("GET", "/_cat/shards")).getEntity())); assertThat(response.getIndices().size(), equalTo(0)); }	i think if we can surround this with if (randomboolean()) { ... } this way we can test both situations when our cluster have only these two indices and when something else is present.
public void testClusterHealthNotFoundIndex() throws IOException { ClusterHealthRequest request = new ClusterHealthRequest("notexisted-index"); request.timeout("5s"); ClusterHealthResponse response = execute(request, highLevelClient().cluster()::health, highLevelClient().cluster()::healthAsync); assertThat(response, notNullValue()); assertThat(response.isTimedOut(), equalTo(true)); assertThat(response.status(), equalTo(RestStatus.REQUEST_TIMEOUT)); assertThat(response.getStatus(), equalTo(ClusterHealthStatus.RED)); }	i am not sure that i fully understand the removal of assertnoindices. in the comment you mention that you removed it from testclusterhealthgreen, but it looks like it was removed from testclusterhealthnotfoundindex where it checks that we didn't get anything back when we asked for a non-existing index. i think it might be better to return this check back and, maybe, randomly create a bogus index to make sure that existing indices in the test don't interfere with it (basically do something similar to what you did in testclusterhealthyellowindiceslevel). and just in case we can just remove this line with activeshardspercent from here?
@Override public void deleteBlobs(List<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } try (AmazonS3Reference clientReference = blobStore.clientReference()) { // S3 API only allows 1k blobs per delete so we split up the given blobs into requests of max. 1k deletes final List<DeleteObjectsRequest> deleteRequests = new ArrayList<>(); final List<String> partition = new ArrayList<>(); for (String blob : blobNames) { partition.add(buildKey(blob)); if (partition.size() == 1000 ) { deleteRequests.add(bulkDelete(blobStore.bucket(), partition)); partition.clear(); } } if (partition.isEmpty() == false) { deleteRequests.add(bulkDelete(blobStore.bucket(), partition)); } SocketAccess.doPrivilegedVoid(() -> { for (DeleteObjectsRequest deleteRequest : deleteRequests) { clientReference.client().deleteObjects( deleteRequest); } }); } catch (final AmazonClientException e) { throw new IOException("Exception when deleting blobs [" + blobNames + "]", e); } }	refactor this magic number as a constant?
@Override public void deleteBlobs(List<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } try (AmazonS3Reference clientReference = blobStore.clientReference()) { // S3 API only allows 1k blobs per delete so we split up the given blobs into requests of max. 1k deletes final List<DeleteObjectsRequest> deleteRequests = new ArrayList<>(); final List<String> partition = new ArrayList<>(); for (String blob : blobNames) { partition.add(buildKey(blob)); if (partition.size() == 1000 ) { deleteRequests.add(bulkDelete(blobStore.bucket(), partition)); partition.clear(); } } if (partition.isEmpty() == false) { deleteRequests.add(bulkDelete(blobStore.bucket(), partition)); } SocketAccess.doPrivilegedVoid(() -> { for (DeleteObjectsRequest deleteRequest : deleteRequests) { clientReference.client().deleteObjects( deleteRequest); } }); } catch (final AmazonClientException e) { throw new IOException("Exception when deleting blobs [" + blobNames + "]", e); } }	nit: should probably be on one line
@Override public void deleteBlobs(List<String> blobNames) throws IOException { if (blobNames.isEmpty()) { return; } try (AmazonS3Reference clientReference = blobStore.clientReference()) { // S3 API only allows 1k blobs per delete so we split up the given blobs into requests of max. 1k deletes final List<DeleteObjectsRequest> deleteRequests = new ArrayList<>(); final List<String> partition = new ArrayList<>(); for (String blob : blobNames) { partition.add(buildKey(blob)); if (partition.size() == 1000 ) { deleteRequests.add(bulkDelete(blobStore.bucket(), partition)); partition.clear(); } } if (partition.isEmpty() == false) { deleteRequests.add(bulkDelete(blobStore.bucket(), partition)); } SocketAccess.doPrivilegedVoid(() -> { for (DeleteObjectsRequest deleteRequest : deleteRequests) { clientReference.client().deleteObjects( deleteRequest); } }); } catch (final AmazonClientException e) { throw new IOException("Exception when deleting blobs [" + blobNames + "]", e); } }	if there is an ioexception we do not proceed even if we have more deleterequests to be sent. previously when performing deletes - if one delete failed, we still were proceeding with the next delete requests.
* * @param blobName * The name of the blob to delete. * @throws NoSuchFileException if the blob does not exist * @throws IOException if the blob exists but could not be deleted. */ void deleteBlob(String blobName) throws IOException; /** * Deletes the blobs with giving names. Unlike {@link #deleteBlob(String)}	with the given names
* * @param blobName * The name of the blob to delete. * @throws NoSuchFileException if the blob does not exist * @throws IOException if the blob exists but could not be deleted. */ void deleteBlob(String blobName) throws IOException; /** * Deletes the blobs with giving names. Unlike {@link #deleteBlob(String)}	this @throws contradicts "unlike {@link #deleteblob(string)} this method will not throw an exception..."
@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId) { if (isReadOnly()) { throw new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository"); } final RepositoryData repositoryData = getRepositoryData(); SnapshotInfo snapshot = null; try { snapshot = getSnapshotInfo(snapshotId); } catch (SnapshotMissingException ex) { throw ex; } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) { logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex); } try { // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots final RepositoryData updatedRepositoryData = repositoryData.removeSnapshot(snapshotId); writeIndexGen(updatedRepositoryData, repositoryStateId); // delete the snapshot file deleteSnapshotBlobIgnoringErrors(snapshot, snapshotId.getUUID()); // delete the global metadata file deleteGlobalMetaDataBlobIgnoringErrors(snapshot, snapshotId.getUUID()); // Now delete all indices if (snapshot != null) { final List<String> indices = snapshot.indices(); for (String index : indices) { final IndexId indexId = repositoryData.resolveIndexId(index); IndexMetaData indexMetaData = null; try { indexMetaData = getSnapshotIndexMetaData(snapshotId, indexId); } catch (ElasticsearchParseException | IOException ex) { logger.warn(() -> new ParameterizedMessage("[{}] [{}] failed to read metadata for index", snapshotId, index), ex); } deleteIndexMetaDataBlobIgnoringErrors(snapshot, indexId); if (indexMetaData != null) { for (int shardId = 0; shardId < indexMetaData.getNumberOfShards(); shardId++) { try { delete(snapshotId, indexId, new ShardId(indexMetaData.getIndex(), shardId)); } catch (SnapshotException ex) { final int finalShardId = shardId; logger.warn(() -> new ParameterizedMessage("[{}] failed to delete shard data for shard [{}][{}]", snapshotId, index, finalShardId), ex); } } } } } // cleanup indices that are no longer part of the repository final Collection<IndexId> indicesToCleanUp = Sets.newHashSet(repositoryData.getIndices().values()); indicesToCleanUp.removeAll(updatedRepositoryData.getIndices().values()); final BlobContainer indicesBlobContainer = blobStore().blobContainer(basePath().add("indices")); try { indicesBlobContainer.deleteBlobs(indicesToCleanUp.stream().map(IndexId::getId).collect(Collectors.toList())); } catch (IOException ioe) { // a different IOException occurred while trying to delete - will just log the issue for now logger.warn(() -> new ParameterizedMessage( "[{}] indices [{}] are no longer part of any snapshots in the repository, " + "but failed to clean up their index folders.", metadata.name(), indicesToCleanUp), ioe); } } catch (IOException | ResourceNotFoundException ex) { throw new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex); } }	we no longer know what particular indices are not removed. we just log all indices, including those that are successful. the same thing applies to deleteblobs usage below. probably we can add this kind of information to ioexception thrown by deleteblobs?
@Override public String toString() { return "peer recovery"; } } private static EnumSet<RecoverySource.Type> INITIAL_RECOVERY_TYPES = EnumSet.of(Type.EMPTY_STORE, Type.LOCAL_SHARDS); /** * returns true for recovery types that indicate that a primary is being allocated for the very first time. * This recoveries can be controlled by {@link IndexMetaData#INDEX_ROUTING_INITIAL_RECOVERY_GROUP_SETTING}	@ywelsch can you look at this one please
@Override protected boolean isPrivateSetting(String key) { switch (key) { case IndexMetaData.SETTING_CREATION_DATE: case IndexMetaData.SETTING_INDEX_UUID: case IndexMetaData.SETTING_VERSION_CREATED: case IndexMetaData.SETTING_VERSION_UPGRADED: case IndexMetaData.SETTING_INDEX_PROVIDED_NAME: case MergePolicyConfig.INDEX_MERGE_ENABLED: case IndexMetaData.INDEX_SHRINK_SOURCE_UUID_KEY: case IndexMetaData.INDEX_SHRINK_SOURCE_NAME_KEY: return true; default: if (key.startsWith(IndexMetaData.INDEX_ROUTING_INITIAL_RECOVERY_GROUP_SETTING.getKey())) { return true; } else { return false; } } }	maybe use return indexmetadata.index_routing_initial_recovery_group_setting.getrawkey().match(key)
public void testDeletePolicy() throws Exception { RestHighLevelClient client = highLevelClient(); { // Add a policy, so that it can be deleted: PutPolicyRequest putPolicyRequest = new PutPolicyRequest( "users-policy", "exact_match", List.of("users"), "email", List.of("address", "zip", "city", "state")); client.enrich().putPolicy(putPolicyRequest, RequestOptions.DEFAULT); } // tag::enrich-delete-policy-request DeletePolicyRequest deletePolicyRequest = new DeletePolicyRequest("users-policy"); // end::enrich-delete-policy-request // tag::enrich-delete-policy-execute AcknowledgedResponse deletePolicyResponse = client.enrich() .deletePolicy(deletePolicyRequest, RequestOptions.DEFAULT); // end::enrich-delete-policy-execute // tag::enrich-delete-policy-response boolean isAcknowledged = deletePolicyResponse.isAcknowledged(); // <1> // end::enrich-delete-policy-response // tag::enrich-delete-policy-execute-listener ActionListener<AcknowledgedResponse> listener = new ActionListener<>() { @Override public void onResponse(AcknowledgedResponse response) { // <1> boolean isAcknowledged = deletePolicyResponse.isAcknowledged(); } @Override public void onFailure(Exception e) { // <2> } }; // end::enrich-delete-policy-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::enrich-delete-policy-execute-async client.enrich().deletePolicyAsync(deletePolicyRequest, RequestOptions.DEFAULT, listener); // <1> // end::enrich-delete-policy-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); }	this should be response not deletepolicyresponse methinks
public void testGetRoles() throws Exception { final RestHighLevelClient client = highLevelClient(); addRole("my_role"); addRole("my_role2"); addRole("my_role3"); { //tag::get-roles-request GetRolesRequest request = new GetRolesRequest("my_role"); //end::get-roles-request //tag::get-roles-execute GetRolesResponse response = client.security().getRoles(request, RequestOptions.DEFAULT); //end::get-roles-execute //tag::get-roles-response List<Role> roles = response.getRoles(); //end::get-roles-response assertNotNull(response); assertThat(roles.size(), equalTo(1)); assertThat(roles.get(0).getClusterPrivileges().contains("all"), equalTo(true)); } { //tag::get-roles-list-request GetRolesRequest request = new GetRolesRequest("my_role", "my_role2"); GetRolesResponse response = client.security().getRoles(request, RequestOptions.DEFAULT); //end::get-roles-list-request List<Role> roles = response.getRoles(); assertNotNull(response); assertThat(roles.size(), equalTo(2)); assertThat(roles.get(0).getClusterPrivileges().contains("all"), equalTo(true)); assertThat(roles.get(1).getClusterPrivileges().contains("all"), equalTo(true)); } { //tag::get-roles-all-request GetRolesRequest request = new GetRolesRequest(); GetRolesResponse response = client.security().getRoles(request, RequestOptions.DEFAULT); //end::get-roles-all-request List<Role> roles = response.getRoles(); assertNotNull(response); // 21 system roles plus the three we created assertThat(roles.size(), equalTo(24)); } { //tag::get-roles-execute-listener GetRolesRequest request = new GetRolesRequest("my_role"); ActionListener<GetRolesResponse> listener = new ActionListener<GetRolesResponse>() { @Override public void onResponse(GetRolesResponse getRolesResponse) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; //end::get-roles-execute-listener //Replace the emtpty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); //tag::get-roles-execute-async client.security().getRolesAsync(request, RequestOptions.DEFAULT, listener); // <1> //end::get-roles-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); } }	replace listener with future and verify the results just like we did for others.
@Override public TemporalAccessor parse(String input) { if (Strings.isNullOrEmpty(input)) { throw new IllegalArgumentException("cannot parse empty date"); } try { return doParse(input); } catch (DateTimeParseException e) { throw new IllegalArgumentException("failed to parse date field [" + input + "] with format [" + format + "]", e); } }	this could be statically initialized in the ctor?
@Override protected long pick(SortedNumericDocValues values, long missingValue, DocIdSetIterator docItr, int startDoc, int endDoc, Integer maxChildren) throws IOException { boolean hasValue = false; long maxValue = Long.MIN_VALUE; if (maxChildren == null) { for (int doc = startDoc; doc < endDoc; doc = docItr.nextDoc()) { if (values.advanceExact(doc)) { final int count = values.docValueCount(); for (int i = 0; i < count - 1; ++i) { values.nextValue(); } maxValue = Math.max(maxValue, values.nextValue()); hasValue = true; } } } else { List<Long> populatedValues = new ArrayList<>(); for (int doc = startDoc; doc < endDoc; doc = docItr.nextDoc()) { if (values.advanceExact(doc)) { final int count = values.docValueCount(); for (int i = 0; i < count - 1; ++i) { values.nextValue(); } populatedValues.add(values.nextValue()); } } int startIndex = populatedValues.size() - maxChildren < 0 ? 0 : populatedValues.size() - maxChildren; for (int i = startIndex; i < populatedValues.size(); i++) { maxValue = Math.max(maxValue, populatedValues.get(i)); hasValue = true; } } return hasValue ? maxValue : missingValue; }	why not an int and make the option defaults to integer.max_value ?
@Override protected long pick(SortedNumericDocValues values, long missingValue, DocIdSetIterator docItr, int startDoc, int endDoc, Integer maxChildren) throws IOException { int totalCount = 0; long totalValue = 0; if (maxChildren == null) { for (int doc = startDoc; doc < endDoc; doc = docItr.nextDoc()) { if (values.advanceExact(doc)) { final int count = values.docValueCount(); for (int index = 0; index < count; ++index) { totalValue += values.nextValue(); } totalCount += count; } } } else { List<List<Long>> populatedValues = new ArrayList<>(); for (int doc = startDoc; doc < endDoc; doc = docItr.nextDoc()) { if (values.advanceExact(doc)) { final int count = values.docValueCount(); List<Long> valuesList = new ArrayList<>(); for (int index = 0; index < count; ++index) { valuesList.add(values.nextValue()); } populatedValues.add(valuesList); } } int startIndex = populatedValues.size() - maxChildren < 0 ? 0 : populatedValues.size() - maxChildren; for (int i = startIndex; i < populatedValues.size(); i++) { List<Long> value = populatedValues.get(i); totalValue += value.stream().mapToLong(Long::longValue).sum(); totalCount += value.size(); } } return totalCount > 0 ? totalValue : missingValue; }	i think you can do: int count = 0; for (int doc = startdoc; doc < enddoc; doc = docitr.nextdoc()) { if (count++ > maxchildren) { break; } .... we match the children in the same order than what was indexed so you can just count the number of matching children for the current parent document and stops when you have more than maxchildren.
void authenticateWithApiKeyIfPresent(ThreadContext ctx, ActionListener<AuthenticationResult> listener) { if (enabled) { final ApiKeyCredentials credentials; try { credentials = getCredentialsFromHeader(ctx); } catch (ElasticsearchSecurityException ese) { listener.onResponse(AuthenticationResult.terminate(ese.getMessage(), ese)); return; } if (credentials != null) { final GetRequest getRequest = client.prepareGet(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE, credentials.getId()) .setFetchSource(true).request(); executeAsyncWithOrigin(ctx, SECURITY_ORIGIN, getRequest, ActionListener.<GetResponse>wrap(response -> { if (response.isExists()) { try (ApiKeyCredentials ignore = credentials) { validateApiKeyCredentials(response.getSource(), credentials, clock, listener); } } else { credentials.close(); listener.onResponse(AuthenticationResult.unsuccessful("unable to authenticate", null)); } }, e -> { credentials.close(); listener.onResponse(AuthenticationResult.unsuccessful("apikey auth encountered a failure", e)); }), client::get); } else { listener.onResponse(AuthenticationResult.notHandled()); } } else { listener.onResponse(AuthenticationResult.notHandled()); } }	why is a malformed apikey (missing colon) a terminate but a non-existent id is unsuccessful? i feel like we should either say - "if there is an apikey header, we will not try any other authc method", or - "if apikey fails we fall back to the standard authc chain". but this looks like a mix of those.
void authenticateWithApiKeyIfPresent(ThreadContext ctx, ActionListener<AuthenticationResult> listener) { if (enabled) { final ApiKeyCredentials credentials; try { credentials = getCredentialsFromHeader(ctx); } catch (ElasticsearchSecurityException ese) { listener.onResponse(AuthenticationResult.terminate(ese.getMessage(), ese)); return; } if (credentials != null) { final GetRequest getRequest = client.prepareGet(SecurityIndexManager.SECURITY_INDEX_NAME, TYPE, credentials.getId()) .setFetchSource(true).request(); executeAsyncWithOrigin(ctx, SECURITY_ORIGIN, getRequest, ActionListener.<GetResponse>wrap(response -> { if (response.isExists()) { try (ApiKeyCredentials ignore = credentials) { validateApiKeyCredentials(response.getSource(), credentials, clock, listener); } } else { credentials.close(); listener.onResponse(AuthenticationResult.unsuccessful("unable to authenticate", null)); } }, e -> { credentials.close(); listener.onResponse(AuthenticationResult.unsuccessful("apikey auth encountered a failure", e)); }), client::get); } else { listener.onResponse(AuthenticationResult.notHandled()); } } else { listener.onResponse(AuthenticationResult.notHandled()); } }	clear apikeyhashchars after use?
@Override protected void masterOperation(Task task, DeleteEnrichPolicyAction.Request request, ClusterState state, ActionListener<AcknowledgedResponse> listener) throws Exception { List<PipelineConfiguration> pipelines = IngestService.getPipelines(state); EnrichPolicy policy = EnrichStore.getPolicy(request.getName(), state); List<String> pipelinesWithProcessors = new ArrayList<>(); for (PipelineConfiguration pipelineConfiguration : pipelines) { List<AbstractEnrichProcessor> enrichProcessors = ingestService.getProcessorsInPipeline(pipelineConfiguration.getId(), AbstractEnrichProcessor.class); for (AbstractEnrichProcessor processor: enrichProcessors) { if (processor.getPolicyName().equalsIgnoreCase(request.getName())) { pipelinesWithProcessors.add(pipelineConfiguration.getId()); } } } if (pipelinesWithProcessors.isEmpty() == false) { listener.onFailure( new ElasticsearchStatusException("Could not delete policy [{}] because a pipeline is referencing it {}", RestStatus.CONFLICT, request.getName(), pipelinesWithProcessors)); } EnrichStore.deletePolicy(request.getName(), clusterService, e -> { if (e == null) { listener.onResponse(new AcknowledgedResponse(true)); } else { listener.onFailure(e); } }); }	i think you need to collect and join (comma seperated) these pipelines, and likely adjust the corresponding test.
*/ public HttpEntity getEntity() { return response.getEntity(); }	could you line all of these up vertically?
public static LifecycleExecutionState fromIndexMetadata(IndexMetadata indexMetadata) { Map<String, String> customData = indexMetadata.getCustomData(ILM_CUSTOM_METADATA_KEY); if (customData != null && customData.isEmpty() == false) { return fromCustomMetadata(customData); } else { return LifecycleExecutionState.builder().build(); } }	nit: add a constant for this one?
public void testRolloverDataStreamInFollowClusterForbidden() throws Exception { if ("follow".equals(targetCluster) == false) { return; } final int numDocs = 64; final String dataStreamName = "logs-tomcat-prod"; int initialNumberOfSuccessfulFollowedIndices = getNumberOfSuccessfulFollowedIndices(); // Create auto follow pattern createAutoFollowPattern(client(), "test_pattern", "logs-*", "leader_cluster"); // Create data stream and ensure that is is auto followed { try (RestClient leaderClient = buildLeaderClient()) { for (int i = 0; i < numDocs; i++) { Request indexRequest = new Request("POST", "/" + dataStreamName + "/_doc"); indexRequest.addParameter("refresh", "true"); indexRequest.setJsonEntity("{\\\\"@timestamp\\\\": \\\\"" + DATE_FORMAT.format(new Date()) + "\\\\",\\\\"message\\\\":\\\\"abc\\\\"}"); assertOK(leaderClient.performRequest(indexRequest)); } verifyDataStream(leaderClient, dataStreamName, backingIndexName(dataStreamName, 1)); verifyDocuments(leaderClient, dataStreamName, numDocs); } assertBusy(() -> { assertThat(getNumberOfSuccessfulFollowedIndices(), equalTo(initialNumberOfSuccessfulFollowedIndices + 1)); verifyDataStream(client(), dataStreamName, backingIndexName(dataStreamName, 1)); ensureYellow(dataStreamName); verifyDocuments(client(), dataStreamName, numDocs); }); } // Rollover in leader cluster and ensure second backing index is replicated: { try (RestClient leaderClient = buildLeaderClient()) { Request rolloverRequest = new Request("POST", "/" + dataStreamName + "/_rollover"); assertOK(leaderClient.performRequest(rolloverRequest)); verifyDataStream(leaderClient, dataStreamName, backingIndexName(dataStreamName, 1), backingIndexName(dataStreamName, 2)); Request indexRequest = new Request("POST", "/" + dataStreamName + "/_doc"); indexRequest.addParameter("refresh", "true"); indexRequest.setJsonEntity("{\\\\"@timestamp\\\\": \\\\"" + DATE_FORMAT.format(new Date()) + "\\\\",\\\\"message\\\\":\\\\"abc\\\\"}"); assertOK(leaderClient.performRequest(indexRequest)); verifyDocuments(leaderClient, dataStreamName, numDocs + 1); } assertBusy(() -> { assertThat(getNumberOfSuccessfulFollowedIndices(), equalTo(initialNumberOfSuccessfulFollowedIndices + 2)); verifyDataStream(client(), dataStreamName, backingIndexName(dataStreamName, 1), backingIndexName(dataStreamName, 2)); ensureYellow(dataStreamName); verifyDocuments(client(), dataStreamName, numDocs + 1); }); } // Try rollover in follow cluster { Request rolloverRequest1 = new Request("POST", "/" + dataStreamName + "/_rollover"); Exception e = expectThrows(ResponseException.class, () -> client().performRequest(rolloverRequest1)); assertThat(e.getMessage(), containsString("data stream [" + dataStreamName + "] cannot be rolled over, " + "because it is a replicated data stream")); verifyDataStream(client(), dataStreamName, backingIndexName(dataStreamName, 1), backingIndexName(dataStreamName, 2)); // Unfollow .ds-logs-tomcat-prod-000001 pauseFollow(backingIndexName(dataStreamName, 1)); closeIndex(backingIndexName(dataStreamName, 1)); unfollow(backingIndexName(dataStreamName, 1)); // Try again Request rolloverRequest2 = new Request("POST", "/" + dataStreamName + "/_rollover"); e = expectThrows(ResponseException.class, () -> client().performRequest(rolloverRequest2)); assertThat(e.getMessage(), containsString("data stream [" + dataStreamName + "] cannot be rolled over, " + "because it is a replicated data stream")); verifyDataStream(client(), dataStreamName, backingIndexName(dataStreamName, 1), backingIndexName(dataStreamName, 2)); // Promote local data stream Request promoteRequest = new Request("POST", "/_data_stream/_promote/" + dataStreamName); assertOK(client().performRequest(promoteRequest)); // Try again and now the rollover should be successful because local data stream is now : Request rolloverRequest3 = new Request("POST", "/" + dataStreamName + "/_rollover"); assertOK(client().performRequest(rolloverRequest3)); verifyDataStream(client(), dataStreamName, backingIndexName(dataStreamName, 1), backingIndexName(dataStreamName, 2), backingIndexName(dataStreamName, 3)); // TODO: verify that following a backing index for logs-tomcat-prod data stream in remote cluster fails, // because local data stream isn't a replicated data stream anymore. // Unfollow .ds-logs-tomcat-prod-000002, // which is now possible because this index can now be closed as it is no longer the write index. pauseFollow(backingIndexName(dataStreamName, 2)); closeIndex(backingIndexName(dataStreamName, 2)); unfollow(backingIndexName(dataStreamName, 2)); } // Cleanup: { deleteAutoFollowPattern("test_pattern"); deleteDataStream(dataStreamName); } }	@martijnvg sorry, i think i didn't explain well my concern about supporting bi-directional replication. what i meant was jasonz's [blog](https://www.elastic.co/blog/bi-directional-replication-with-elasticsearch-cross-cluster-replication-ccr). in his setup, we can send the same indexing request (uses the same write alias) to either cluster. in this test, we use different data streams for indexing requests. that means users can't simply reroute all indexing to a single cluster when another is not available.
@Override protected RestChannelConsumer innerPrepareRequest(RestRequest request, NodeClient client) throws IOException { String tokenName = request.param("name"); if (Strings.isNullOrEmpty(tokenName)) { tokenName = new String(Hasher.SHA256.hash(new SecureString(UUIDs.base64UUID().toCharArray()))); } final CreateServiceAccountTokenRequest createServiceAccountTokenRequest = new CreateServiceAccountTokenRequest( request.param("namespace"), request.param("service"), tokenName); final String refreshPolicy = request.param("refresh"); if (refreshPolicy != null) { createServiceAccountTokenRequest.setRefreshPolicy(WriteRequest.RefreshPolicy.parse(refreshPolicy)); } return channel -> client.execute(CreateServiceAccountTokenAction.INSTANCE, createServiceAccountTokenRequest, new RestToXContentListener<>(channel)); }	this feels like a weird fix. we were using a random uuid in url safe base64. but because it's possible for that to generate a string with a leading _ (i think that's the problem), we're generating the hash of that string and hex-encoding it. the hex encoding is the bit that actually solves the problem, but we're applying a bunch of other stuff as well. personally, i'd prefer that we drop leading _ or regenerate rather than apply hashing and hex encoding just to work around a tiny incompatibility with the bast64 encoding.
* @return The index metadata of the concrete index * @throws IllegalArgumentException if the parameter points to an alias with more than one index * @throws IndexNotFoundException if the parameter points neither to an index nor to an alias */ public IndexMetaData findConcreteIndexMetaData(String aliasOrIndexName) { assert aliasOrIndexName != null; AliasOrIndex aliasOrIndex = getAliasAndIndexLookup().get(aliasOrIndexName); if (aliasOrIndex == null) { throw new IndexNotFoundException(aliasOrIndexName); } // regular index if (indices.containsKey(aliasOrIndexName)) { return indices.get(aliasOrIndexName); } // alias if (aliasOrIndex.getIndices().size() == 1) { String concreteIndex = aliasOrIndex.getIndices().get(0).getIndex().getName(); return indices.get(concreteIndex); } else { List<String> indices = aliasOrIndex.getIndices().stream() .map(IndexMetaData::getIndex).map(Index::getName).sorted().collect(Collectors.toList()); throw new IllegalArgumentException("alias [" + aliasOrIndexName + "] points to indices " + indices + ", cannot get concrete index"); } }	nit: should we prefer this as if it exists? i suspect it's the more common path?
@SuppressWarnings("unchecked") private long[] extractIds(List<Map<String, Object>> events) { final int len = events.size(); final long[] ids = new long[len]; String idField = tiebreaker() == null ? idField() : tiebreaker(); for (int i = 0; i < len; i++) { Map<String, Object> event = events.get(i); Map<String, Object> source = (Map<String, Object>) event.get("_source"); Object field = source.get(idField); ids[i] = ((Number) field).longValue(); } return ids; }	i think this could be clearer if this logic is pushed to idfield(). the default implementation can return tiebreaker() and eqlsampletestcase.idfield() stays as is.
public void setup() throws Exception { boolean shouldLoadData = true; RestClient provisioningClient = provisioningClient(); String[] splitNames = index.split(","); int i = 0; while (shouldLoadData && i < splitNames.length) { String indexName = splitNames[i++]; if (provisioningClient.performRequest(new Request("HEAD", "/" + unqualifiedIndexName(indexName))) .getStatusLine() .getStatusCode() == 200) { shouldLoadData = false; } } if (shouldLoadData) { DataLoader.loadDatasetIntoEs(highLevelClient(provisioningClient), this::createParser); } }	i think stream.allmatch would help a lot here to communicate the intent. as in: boolean shouldloaddata = arrays.stream(index.split(",")) .allmatch( indexname -> provisioningclient.performrequest(new request("head", "/" + unqualifiedindexname(indexname))) .getstatusline() .getstatuscode() == 200 );
public ClusterBlockException indicesAllowReleaseResources(String[] indices) { boolean indexIsBlocked = false; for (String index : indices) { if (indexBlocked(ClusterBlockLevel.METADATA_WRITE, index)) { indexIsBlocked = true; } } if (globalBlocked(ClusterBlockLevel.METADATA_WRITE) == false && indexIsBlocked == false) { return null; } Function<String, Stream<ClusterBlock>> blocksForIndexAtLevel = index -> blocksForIndex(ClusterBlockLevel.METADATA_WRITE, index) .stream(); Stream<ClusterBlock> blocks = concat( global(ClusterBlockLevel.METADATA_WRITE).stream(), Stream.of(indices).flatMap(blocksForIndexAtLevel)).filter(clusterBlock -> clusterBlock.isAllowReleaseResources() == false); Set<ClusterBlock> clusterBlocks = unmodifiableSet(blocks.collect(toSet())); if (clusterBlocks.isEmpty()) { return null; } return new ClusterBlockException(clusterBlocks); }	it seems indexblocked already checks for the global blocks, so another check here is redundant. the only exception is if someone passes in an empty array but that is not the intended use of the method.
public ClusterBlockException indicesAllowReleaseResources(String[] indices) { boolean indexIsBlocked = false; for (String index : indices) { if (indexBlocked(ClusterBlockLevel.METADATA_WRITE, index)) { indexIsBlocked = true; } } if (globalBlocked(ClusterBlockLevel.METADATA_WRITE) == false && indexIsBlocked == false) { return null; } Function<String, Stream<ClusterBlock>> blocksForIndexAtLevel = index -> blocksForIndex(ClusterBlockLevel.METADATA_WRITE, index) .stream(); Stream<ClusterBlock> blocks = concat( global(ClusterBlockLevel.METADATA_WRITE).stream(), Stream.of(indices).flatMap(blocksForIndexAtLevel)).filter(clusterBlock -> clusterBlock.isAllowReleaseResources() == false); Set<ClusterBlock> clusterBlocks = unmodifiableSet(blocks.collect(toSet())); if (clusterBlocks.isEmpty()) { return null; } return new ClusterBlockException(clusterBlocks); }	why do we need the first part of the method to shortcut this second half in the case we have no blocks? can't we make do with the second part only?
public void testClusterUpdateSettingsWithBlocks() { String key1 = "cluster.routing.allocation.enable"; Settings transientSettings = Settings.builder().put(key1, EnableAllocationDecider.Allocation.NONE.name()).build(); String key2 = "cluster.routing.allocation.node_concurrent_recoveries"; Settings persistentSettings = Settings.builder().put(key2, "5").build(); ClusterUpdateSettingsRequestBuilder request = client().admin().cluster().prepareUpdateSettings() .setTransientSettings(transientSettings) .setPersistentSettings(persistentSettings); // Cluster settings updates are blocked when the cluster is read only try { setClusterReadOnly(true); assertBlocked(request, MetaData.CLUSTER_READ_ONLY_BLOCK); // But it's possible to update the settings to update the "cluster.blocks.read_only" setting Settings settings = Settings.builder().putNull(MetaData.SETTING_READ_ONLY_SETTING.getKey()).build(); assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings).get()); } finally { setClusterReadOnly(false); } // Cluster settings updates are blocked when the cluster is read only try { // But it's possible to update the settings to update the "cluster.blocks.read_only" setting Settings settings = Settings.builder().put(MetaData.SETTING_READ_ONLY_ALLOW_DELETE_SETTING.getKey(), true).build(); assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings).get()); assertBlocked(request, MetaData.CLUSTER_READ_ONLY_ALLOW_DELETE_BLOCK); // But it's possible to update the settings to update the "cluster.blocks.read_only" setting settings = Settings.builder().putNull(MetaData.SETTING_READ_ONLY_ALLOW_DELETE_SETTING.getKey()).build(); assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings).get()); } finally { Settings s = Settings.builder().putNull(MetaData.SETTING_READ_ONLY_ALLOW_DELETE_SETTING.getKey()).build(); assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(s).get()); } // It should work now ClusterUpdateSettingsResponse response = request.execute().actionGet(); assertAcked(response); assertThat(response.getTransientSettings().get(key1), notNullValue()); assertThat(response.getTransientSettings().get(key2), nullValue()); assertThat(response.getPersistentSettings().get(key1), nullValue()); assertThat(response.getPersistentSettings().get(key2), notNullValue()); }	this is funky - we can remove the previous non finaly logic
public static void registerAggregators(ValuesSourceRegistry.Builder builder) { DateHistogramValuesSourceBuilder.register(builder); HistogramValuesSourceBuilder.register(builder); GeoTileGridValuesSourceBuilder.register(builder); TermsValuesSourceBuilder.register(builder); builder.registerUsage(NAME); }	this preserves the existing behavior (i.e. it tracks composite usage like it was tracked before setting up the registry). it might be worth more detailed tracking, since we can capture what key types and values sources are being used. but this pr is big enough already, and we don't have a clear plan for composite usage tracking, so i'm leaving it be for now. can open a ticket if folks think it's worth tinkering with, or we can just leave it alone.
public static void register(ValuesSourceRegistry.Builder builder) { builder.registerComposite( TYPE, List.of(CoreValuesSourceType.DATE, CoreValuesSourceType.NUMERIC), ((valuesSourceConfig, compositeBucketStrategy, name, hasScript, format, missingBucket, order) -> { ValuesSource.Numeric numeric = (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(); // TODO once composite is plugged in to the values source registry or at least understands Date values source types use it // here Rounding rounding = compositeBucketStrategy.getRounding(); Rounding.Prepared preparedRounding = rounding.prepareForUnknown(); RoundingValuesSource vs = new RoundingValuesSource(numeric, preparedRounding); // is specified in the builder. final DocValueFormat docValueFormat = format == null ? DocValueFormat.RAW : valuesSourceConfig.format(); final MappedFieldType fieldType = valuesSourceConfig.fieldType(); return new CompositeValuesSourceConfig( name, fieldType, vs, docValueFormat, order, missingBucket, hasScript, ( BigArrays bigArrays, IndexReader reader, int size, LongConsumer addRequestCircuitBreakerBytes, CompositeValuesSourceConfig compositeValuesSourceConfig) -> { final RoundingValuesSource roundingValuesSource = (RoundingValuesSource) compositeValuesSourceConfig.valuesSource(); return new LongValuesSource( bigArrays, compositeValuesSourceConfig.fieldType(), roundingValuesSource::longValues, roundingValuesSource::round, compositeValuesSourceConfig.format(), compositeValuesSourceConfig.missingBucket(), size, compositeValuesSourceConfig.reverseMul() ); } ); }) ); }	we should be able to do this now, but i didn't.
public static List<?> fetchSourceValue(FieldMapper mapper, Object sourceValue, String format) throws IOException { String field = mapper.name(); MapperService mapperService = mock(MapperService.class); when(mapperService.sourcePath(field)).thenReturn(Set.of(field)); ValueFetcher fetcher = mapper.valueFetcher(mapperService, null, format); SourceLookup lookup = new SourceLookup(); lookup.setSource(Collections.singletonMap(field, sourceValue)); return fetcher.fetchValues(lookup); } /** * Use a {@linkplain FieldMapper}	this is pretty complex, and although mappertestcase is a unit test, it feels closer to an integration test. are you primarily trying to add coverage docvalue_fields on dates? it could be natural to just add some cases to searchfieldsit. it feels like a bigger change (perhaps for a follow-up?) to figure out how to test docvalue_fields effectively. as we discussed, there's not great test coverage for it, especially not in terms of unit tests.
@Override public void readBytes(byte[] b, int offset, int len) throws IOException { int read = read(b, offset, len); if (read < len) { throw new IndexOutOfBoundsException(); } }	what was wrong with eofexception?
public void testDST_Europe_Rome() { // time zone "Europe/Rome", rounding to days. Rome had two midnights on the day the clocks went back in 1978, and // timeZone.convertLocalToUTC() gives the later of the two because Rome is east of UTC, whereas we want the earlier. DateTimeUnit timeUnit = DateTimeUnit.DAY_OF_MONTH; DateTimeZone tz = DateTimeZone.forID("Europe/Rome"); Rounding rounding = new TimeUnitRounding(timeUnit, tz); long timeAfterSecondMidnight = time("1978-10-01T06:00:00+01:00"); long floor = rounding.round(timeAfterSecondMidnight); assertThat(floor, isDate(time("1978-10-01T00:00:00+02:00"), tz)); long prevFloor = rounding.round(floor - 1); assertThat(prevFloor, lessThan(floor)); }	+1, i had to look up this change to be convinced, but i think this is the intuitively expected behaviour.
public void testDST_Europe_Rome() { // time zone "Europe/Rome", rounding to days. Rome had two midnights on the day the clocks went back in 1978, and // timeZone.convertLocalToUTC() gives the later of the two because Rome is east of UTC, whereas we want the earlier. DateTimeUnit timeUnit = DateTimeUnit.DAY_OF_MONTH; DateTimeZone tz = DateTimeZone.forID("Europe/Rome"); Rounding rounding = new TimeUnitRounding(timeUnit, tz); long timeAfterSecondMidnight = time("1978-10-01T06:00:00+01:00"); long floor = rounding.round(timeAfterSecondMidnight); assertThat(floor, isDate(time("1978-10-01T00:00:00+02:00"), tz)); long prevFloor = rounding.round(floor - 1); assertThat(prevFloor, lessThan(floor)); }	can you add a time before the first midnight and a time between first and second just to make it easier to understand whats going on when reading the test later? i did this locally to just make sure i understand this case better, i think it helps when revisiting these tests in the future.
private Set<RoleDescriptor> filterRolesNotUsingLicensedFeatures(Set<RoleDescriptor> roleDescriptors) { final Map<Boolean, Set<RoleDescriptor>> roles = roleDescriptors.stream() .collect(Collectors.partitioningBy(RoleDescriptor::isUsingDocumentOrFieldLevelSecurity, Collectors.toSet())); final Set<RoleDescriptor> rolesUsingFeatures = roles.get(true); logger.warn( "User roles [{}] are disabled since they require document or field level security to determine user access. " + "These security features [{}, {}] are not available under the current license. " + "Users will be denied access to documents or fields granted by above roles. " + "To re-enable the roles, upgrade license to [{}] or above, or renew if it's expired.", DOCUMENT_LEVEL_SECURITY_FEATURE.getName(), FIELD_LEVEL_SECURITY_FEATURE.getName(), rolesUsingFeatures.stream().map(RoleDescriptor::getName).collect(Collectors.joining(",")), DOCUMENT_LEVEL_SECURITY_FEATURE.getMinimumOperationMode() ); return roles.get(false); }	sanity check: i'm assuming there generally won't be many roles so it's fine to include them all. is that valid or should i add a e.g. limit(5) to keep this from getting too lengthy?
private void handleLoadSuccess(String modelId, TrainedModelConfig trainedModelConfig) { Queue<ActionListener<Model>> listeners; LocalModel loadedModel = new LocalModel(trainedModelConfig.getModelId(), trainedModelConfig.getDefinition()); synchronized (loadingListeners) { listeners = loadingListeners.remove(modelId); // If there is no loadingListener that means the loading was canceled and the listener was already notified as such // Consequently, we should not store the retrieved model if (listeners == null) { return; } localModelCache.put(modelId, loadedModel); shouldNotAudit.remove(modelId); } for (ActionListener<Model> listener = listeners.poll(); listener != null; listener = listeners.poll()) { listener.onResponse(loadedModel); } }	i think the implementation has a small race: if >1 threads call this with the same modelid/config the model gets loaded twice, gets overwritten in the cache but also will be twice in memory as the loadmodel instances are returned (line 180). if line 173 kicks in you also load the model superfluously. i suggest to remove line 168 and use computeifabsent on line 176 instead of put. that ensures that the model is loaded only if necessary and exactly once.
private void handleLoadSuccess(String modelId, TrainedModelConfig trainedModelConfig) { Queue<ActionListener<Model>> listeners; LocalModel loadedModel = new LocalModel(trainedModelConfig.getModelId(), trainedModelConfig.getDefinition()); synchronized (loadingListeners) { listeners = loadingListeners.remove(modelId); // If there is no loadingListener that means the loading was canceled and the listener was already notified as such // Consequently, we should not store the retrieved model if (listeners == null) { return; } localModelCache.put(modelId, loadedModel); shouldNotAudit.remove(modelId); } for (ActionListener<Model> listener = listeners.poll(); listener != null; listener = listeners.poll()) { listener.onResponse(loadedModel); } }	what if the cluster state update does not change anything w.r.t. models, which is probably mostly the case? i miss a shortcut optimization.
private void handleLoadSuccess(String modelId, TrainedModelConfig trainedModelConfig) { Queue<ActionListener<Model>> listeners; LocalModel loadedModel = new LocalModel(trainedModelConfig.getModelId(), trainedModelConfig.getDefinition()); synchronized (loadingListeners) { listeners = loadingListeners.remove(modelId); // If there is no loadingListener that means the loading was canceled and the listener was already notified as such // Consequently, we should not store the retrieved model if (listeners == null) { return; } localModelCache.put(modelId, loadedModel); shouldNotAudit.remove(modelId); } for (ActionListener<Model> listener = listeners.poll(); listener != null; listener = listeners.poll()) { listener.onResponse(loadedModel); } }	nit: would be good for reading to mark the end of a synchronized block as comment e.g. } // synchronized(...)
public SubsetResult isSubsetOf(Group other) { final boolean isSubsetExcludingDls = areIndicesASubset(other) && arePrivilegesASubset(other) && areFieldPermissionsASubset(other); if (isSubsetExcludingDls) { return isDlsASubset(other); } else { return SubsetResult.isNotASubset(); } }	i do not think this is correct. if the subset does not have a query but the other one does then that is an issue since other is the group we want to see if this group is a subset of.
private Response performRequest(final NodeTuple<Iterator<Node>> nodeTuple, final InternalRequest request, Exception previousException) throws IOException { RequestContext context = request.createContextForNextAttempt(nodeTuple.nodes.next(), nodeTuple.authCache); HttpResponse httpResponse; try { httpResponse = client.execute(context.requestProducer, context.asyncResponseConsumer, context.context, null).get(); } catch(Exception e) { RequestLogger.logFailedRequest(logger, request.httpRequest, context.node, e); onFailure(context.node); Exception cause = extractAndWrapCause(e); addSuppressedException(previousException, cause); if (nodeTuple.nodes.hasNext()) { return performRequest(nodeTuple, request, cause); } if (cause instanceof IOException) { throw (IOException) cause; } if (cause instanceof RuntimeException) { throw (RuntimeException) cause; } throw new IllegalStateException("cause must be either RuntimeException or IOException", cause); } ResponseOrResponseException responseOrResponseException = convertResponse(request, context.node, httpResponse); if (responseOrResponseException.responseException == null) { return responseOrResponseException.response; } addSuppressedException(previousException, responseOrResponseException.responseException); if (nodeTuple.nodes.hasNext()) { return performRequest(nodeTuple, request, responseOrResponseException.responseException); } throw responseOrResponseException.responseException; }	huh. i think you might be able to drop these instanceof if you moved some stuff into extractandwrapcause. you already know the type. i'm not sure you *need* to do that though. it'd be slightly cleaner, i think, but it isn't worth holding up the pr for it.
private Response performRequest(final NodeTuple<Iterator<Node>> nodeTuple, final InternalRequest request, Exception previousException) throws IOException { RequestContext context = request.createContextForNextAttempt(nodeTuple.nodes.next(), nodeTuple.authCache); HttpResponse httpResponse; try { httpResponse = client.execute(context.requestProducer, context.asyncResponseConsumer, context.context, null).get(); } catch(Exception e) { RequestLogger.logFailedRequest(logger, request.httpRequest, context.node, e); onFailure(context.node); Exception cause = extractAndWrapCause(e); addSuppressedException(previousException, cause); if (nodeTuple.nodes.hasNext()) { return performRequest(nodeTuple, request, cause); } if (cause instanceof IOException) { throw (IOException) cause; } if (cause instanceof RuntimeException) { throw (RuntimeException) cause; } throw new IllegalStateException("cause must be either RuntimeException or IOException", cause); } ResponseOrResponseException responseOrResponseException = convertResponse(request, context.node, httpResponse); if (responseOrResponseException.responseException == null) { return responseOrResponseException.response; } addSuppressedException(previousException, responseOrResponseException.responseException); if (nodeTuple.nodes.hasNext()) { return performRequest(nodeTuple, request, responseOrResponseException.responseException); } throw responseOrResponseException.responseException; }	maybe "unexpected exception type so wrapping into an expected one to prevent even more chaos"?
public void testTaskCancellation() { AtomicLong capturedTaskId = new AtomicLong(); AtomicReference<ActionListener<CancelTasksResponse>> capturedListener = new AtomicReference<>(); Client client = mock(Client.class); when(client.settings()).thenReturn(Settings.EMPTY); PersistentTasksService persistentTasksService = new PersistentTasksService(null, null, client) { @Override void sendCancelRequest(final long taskId, final String reason, final ActionListener<CancelTasksResponse> listener) { capturedTaskId.set(taskId); capturedListener.set(listener); } @Override public void sendCompletionRequest(final String taskId, final long taskAllocationId, final Exception taskFailure, final ActionListener<PersistentTask<?>> listener) { fail("Shouldn't be called during Cluster State cancellation"); } }; @SuppressWarnings("unchecked") PersistentTasksExecutor<TestParams> action = mock(PersistentTasksExecutor.class); when(action.getExecutor()).thenReturn(ThreadPool.Names.SAME); when(action.getTaskName()).thenReturn("test"); when(action.createTask(anyLong(), anyString(), anyString(), any(), any(), any())) .thenReturn(new TestPersistentTasksPlugin.TestTask(1, "persistent", "test", "", new TaskId("cluster", 1), Collections.emptyMap())); PersistentTasksExecutorRegistry registry = new PersistentTasksExecutorRegistry(Collections.singletonList(action)); int nonLocalNodesCount = randomInt(10); MockExecutor executor = new MockExecutor(); TaskManager taskManager = new TaskManager(Settings.EMPTY, threadPool, Collections.emptySet()); PersistentTasksNodeService coordinator = new PersistentTasksNodeService(persistentTasksService, registry, taskManager, executor); ClusterState state = createInitialClusterState(nonLocalNodesCount, Settings.EMPTY); ClusterState newClusterState = state; // Allocate first task state = newClusterState; newClusterState = addTask(state, "test", null, "this_node"); coordinator.clusterChanged(new ClusterChangedEvent("test", newClusterState, state)); // Check the the task is know to the task manager assertThat(taskManager.getTasks().size(), equalTo(1)); AllocatedPersistentTask runningTask = (AllocatedPersistentTask)taskManager.getTasks().values().iterator().next(); String persistentId = runningTask.getPersistentTaskId(); long localId = runningTask.getId(); // Make sure it returns correct status Task.Status status = runningTask.getStatus(); assertThat(status.toString(), equalTo("{\\\\"state\\\\":\\\\"STARTED\\\\"}")); state = newClusterState; // Relocate the task to some other node or remove it completely if (randomBoolean()) { newClusterState = reallocateTask(state, persistentId, "some_other_node"); } else { newClusterState = removeTask(state, persistentId); } coordinator.clusterChanged(new ClusterChangedEvent("test", newClusterState, state)); // Make sure it returns correct status assertThat(taskManager.getTasks().size(), equalTo(1)); assertThat(taskManager.getTasks().values().iterator().next().getStatus().toString(), equalTo("{\\\\"state\\\\":\\\\"PENDING_CANCEL\\\\"}")); // That should trigger cancellation request assertThat(capturedTaskId.get(), equalTo(localId)); // Notify successful cancellation capturedListener.get().onResponse(new CancelTasksResponse()); // finish or fail task if (randomBoolean()) { executor.get(0).task.markAsCompleted(); } else { executor.get(0).task.markAsFailed(new IOException("test")); } // Check the the task is now removed from task manager assertThat(taskManager.getTasks().values(), empty()); }	null didn't work here because persistenttasksservice uses the originsettingsclient which extends filterclient which fetches things from the client that you pass it.
* @param request * The request object for the action * @param listener * The listener to call when the action is complete */ public static <Request extends ActionRequest, Response extends ActionResponse> void executeWithHeadersAsync(Map<String, String> headers, String origin, Client client, Action<Response> action, Request request, ActionListener<Response> listener) { Map<String, String> filteredHeaders = headers.entrySet().stream().filter(e -> SECURITY_HEADER_FILTERS.contains(e.getKey())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); final ThreadContext threadContext = client.threadPool().getThreadContext(); // No headers (e.g. security not installed/in use) so execute as origin if (filteredHeaders.isEmpty()) { ClientHelper.executeAsyncWithOrigin(client, origin, action, request, listener); } else { // Otherwise stash the context and copy in the saved headers before executing final Supplier<ThreadContext.StoredContext> supplier = threadContext.newRestorableContext(false); try (ThreadContext.StoredContext ignore = stashWithHeaders(threadContext, filteredHeaders)) { client.execute(action, request, new ContextPreservingActionListener<>(supplier, listener)); } } }	moved into the server.
private static Map<String, RoleDescriptor> initializeReservedRoles() { return MapBuilder.<String, RoleDescriptor>newMapBuilder() .put("superuser", new RoleDescriptor("superuser", new String[] { "all" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("all").build()}, new String[] { "*" }, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("transport_client", new RoleDescriptor("transport_client", new String[] { "transport_client" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_user", new RoleDescriptor("kibana_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("manage", "read", "index", "delete") .build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("all").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("monitoring_user", new RoleDescriptor("monitoring_user", new String[] { "cluster:monitor/main" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_agent", new RoleDescriptor("remote_monitoring_agent", new String[] { "manage_index_templates", "manage_ingest_pipelines", "monitor", "cluster:monitor/xpack/watcher/watch/get", "cluster:admin/xpack/watcher/watch/put", "cluster:admin/xpack/watcher/watch/delete", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices("metricbeat-*").privileges("index", "create_index").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_collector", new RoleDescriptor( "remote_monitoring_collector", new String[] { "monitor" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("monitor").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("read").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null )) .put("ingest_admin", new RoleDescriptor("ingest_admin", new String[] { "manage_index_templates", "manage_pipeline" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) // reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role .put("reporting_user", new RoleDescriptor("reporting_user", null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_dashboard_only_user", new RoleDescriptor( "kibana_dashboard_only_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*").privileges("read", "view_index_metadata").build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put(KibanaUser.ROLE_NAME, new RoleDescriptor(KibanaUser.ROLE_NAME, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*", ".reporting-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build() }, null, new ConditionalClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("logstash_system", new RoleDescriptor("logstash_system", new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("beats_admin", new RoleDescriptor("beats_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.BEATS_ROLE, new RoleDescriptor(UsernamesField.BEATS_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.APM_ROLE, new RoleDescriptor(UsernamesField.APM_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_user", new RoleDescriptor("machine_learning_user", new String[] { "monitor_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-notifications").privileges("view_index_metadata", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_admin", new RoleDescriptor("machine_learning_admin", new String[] { "manage_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-*").privileges("view_index_metadata", "read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_admin", new RoleDescriptor("watcher_admin", new String[] { "manage_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX, TriggeredWatchStoreField.INDEX_NAME, HistoryStoreField.INDEX_PREFIX + "*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_user", new RoleDescriptor("watcher_user", new String[] { "monitor_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX) .privileges("read") .build(), RoleDescriptor.IndicesPrivileges.builder().indices(HistoryStoreField.INDEX_PREFIX + "*") .privileges("read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("logstash_admin", new RoleDescriptor("logstash_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".logstash*") .privileges("create", "delete", "index", "manage", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_user", new RoleDescriptor("rollup_user", new String[] { "monitor_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_admin", new RoleDescriptor("rollup_admin", new String[] { "manage_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .immutableMap(); }	shouldn't be needed any more.
public void testKibanaSystemRole() { final TransportRequest request = mock(TransportRequest.class); RoleDescriptor roleDescriptor = new ReservedRolesStore().roleDescriptor("kibana_system"); assertNotNull(roleDescriptor); assertThat(roleDescriptor.getMetadata(), hasEntry("_reserved", true)); Role kibanaRole = Role.builder(roleDescriptor, null).build(); assertThat(kibanaRole.cluster().check(ClusterHealthAction.NAME, request), is(true)); assertThat(kibanaRole.cluster().check(ClusterStateAction.NAME, request), is(true)); assertThat(kibanaRole.cluster().check(ClusterStatsAction.NAME, request), is(true)); assertThat(kibanaRole.cluster().check(PutIndexTemplateAction.NAME, request), is(true)); assertThat(kibanaRole.cluster().check(GetIndexTemplatesAction.NAME, request), is(true)); assertThat(kibanaRole.cluster().check(ClusterRerouteAction.NAME, request), is(false)); assertThat(kibanaRole.cluster().check(ClusterUpdateSettingsAction.NAME, request), is(false)); assertThat(kibanaRole.cluster().check(MonitoringBulkAction.NAME, request), is(true)); // SAML assertThat(kibanaRole.cluster().check(SamlPrepareAuthenticationAction.NAME, request), is(true)); assertThat(kibanaRole.cluster().check(SamlAuthenticateAction.NAME, request), is(true)); assertThat(kibanaRole.cluster().check(InvalidateTokenAction.NAME, request), is(true)); assertThat(kibanaRole.cluster().check(CreateTokenAction.NAME, request), is(false)); // Application Privileges DeletePrivilegesRequest deleteKibanaPrivileges = new DeletePrivilegesRequest("kibana-.kibana", new String[]{ "all", "read" }); DeletePrivilegesRequest deleteLogstashPrivileges = new DeletePrivilegesRequest("logstash", new String[]{ "all", "read" }); assertThat(kibanaRole.cluster().check(DeletePrivilegesAction.NAME, deleteKibanaPrivileges), is(true)); assertThat(kibanaRole.cluster().check(DeletePrivilegesAction.NAME, deleteLogstashPrivileges), is(false)); GetPrivilegesRequest getKibanaPrivileges = new GetPrivilegesRequest(); getKibanaPrivileges.application("kibana-.kibana-sales"); GetPrivilegesRequest getApmPrivileges = new GetPrivilegesRequest(); getApmPrivileges.application("apm"); assertThat(kibanaRole.cluster().check(GetPrivilegesAction.NAME, getKibanaPrivileges), is(true)); assertThat(kibanaRole.cluster().check(GetPrivilegesAction.NAME, getApmPrivileges), is(false)); PutPrivilegesRequest putKibanaPrivileges = new PutPrivilegesRequest(); putKibanaPrivileges.setPrivileges(Collections.singletonList(new ApplicationPrivilegeDescriptor( "kibana-.kibana-" + randomAlphaOfLengthBetween(2,6), "all", Collections.emptySet(), Collections.emptyMap()))); PutPrivilegesRequest putSwiftypePrivileges = new PutPrivilegesRequest(); putSwiftypePrivileges.setPrivileges(Collections.singletonList(new ApplicationPrivilegeDescriptor( "swiftype-kibana" , "all", Collections.emptySet(), Collections.emptyMap()))); assertThat(kibanaRole.cluster().check(PutPrivilegesAction.NAME, putKibanaPrivileges), is(true)); assertThat(kibanaRole.cluster().check(PutPrivilegesAction.NAME, putSwiftypePrivileges), is(false)); // Everything else assertThat(kibanaRole.runAs().check(randomAlphaOfLengthBetween(1, 12)), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(IndexAction.NAME).test("foo"), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(IndexAction.NAME).test(".reporting"), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher("indices:foo").test(randomAlphaOfLengthBetween(8, 24)), is(false)); Arrays.asList(".kibana", ".kibana-devnull", ".reporting-" + randomAlphaOfLength(randomIntBetween(0, 13))).forEach((index) -> { logger.info("index name [{}]", index); assertThat(kibanaRole.indices().allowedIndicesMatcher("indices:foo").test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher("indices:bar").test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(DeleteIndexAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(CreateIndexAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(IndexAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(DeleteAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(UpdateSettingsAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(SearchAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(MultiSearchAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(GetAction.NAME).test(index), is(true)); // inherits from 'all' assertThat(kibanaRole.indices().allowedIndicesMatcher(READ_CROSS_CLUSTER_NAME).test(index), is(true)); }); // read-only index access Arrays.asList(".monitoring-" + randomAlphaOfLength(randomIntBetween(0, 13))).forEach((index) -> { logger.info("index name [{}]", index); assertThat(kibanaRole.indices().allowedIndicesMatcher("indices:foo").test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher("indices:bar").test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(DeleteIndexAction.NAME).test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(CreateIndexAction.NAME).test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(IndexAction.NAME).test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(DeleteAction.NAME).test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(UpdateSettingsAction.NAME).test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(SearchAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(MultiSearchAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(GetAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(READ_CROSS_CLUSTER_NAME).test(index), is(true)); }); // Beats management index final String index = ".management-beats"; assertThat(kibanaRole.indices().allowedIndicesMatcher("indices:foo").test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher("indices:bar").test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(DeleteIndexAction.NAME).test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(CreateIndexAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(IndexAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(DeleteAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(UpdateSettingsAction.NAME).test(index), is(false)); assertThat(kibanaRole.indices().allowedIndicesMatcher(SearchAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(MultiSearchAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(GetAction.NAME).test(index), is(true)); assertThat(kibanaRole.indices().allowedIndicesMatcher(READ_CROSS_CLUSTER_NAME).test(index), is(false)); }	these *shouldn't* be required any more.
public int purgeCacheEntriesForDatabase(Path databaseFile) { String databasePath = databaseFile.toString(); int counter = 0; for (CacheKey key : cache.keys()) { if (key.databasePath.equals(databasePath)) { cache.invalidate(key); counter++; } } return counter; }	is this synchronized in any way? is it possible that we will add new key during invalidation of old ones?
public static LatestConfig randomLatestConfig() { return new LatestConfig( new ArrayList<>( new HashSet<>(randomList(1, 10, () -> randomAlphaOfLengthBetween(1, 10)) ) ), randomAlphaOfLengthBetween(1, 10) ); }	nit, estestcase has a method randomunique: randomunique(() -> randomalphaoflengthbetween(1, 10), randomintbetween(1, 10))
public interface WritableValue { void writeTo(OutputStream os) throws IOException; } /** * Write the serialization of a {@link WritableValue} via {@link WritableValue#writeTo(OutputStream)}	we can't use writable here as it's defined in the main module.
private void refreshTrainedModelTasks(PersistentTasksCustomMetadata persistentTasks) { memoryRequirementByTrainedModelTask.clear(); List<PersistentTasksCustomMetadata.PersistentTask<?>> trainedModelTasks = persistentTasks.tasks().stream() .filter(task -> MlTasks.TRAINED_MODEL_DEPLOYMENT_TASK_NAME.equals(task.getTaskName())).collect(Collectors.toList()); for (PersistentTasksCustomMetadata.PersistentTask<?> task : trainedModelTasks) { StartTrainedModelDeploymentAction.TaskParams taskParams = (StartTrainedModelDeploymentAction.TaskParams) task.getParams(); memoryRequirementByTrainedModelTask.put(taskParams.getModelId(), taskParams.estimateMemoryUsageBytes()); } }	is there a reason that it's cleared and than recalculated? seems potentially buggy to me, even if currently the access pattern might be safe. the type is a _concurrent_ hashmap, so indicates concurrent access. in an unlucky moment it returns a wrong result. why not calculating it fresh and reassigning it? (^ this is tricky, it invalidates final and might be unsafe, too. you could loop through the tasks and just update the values and at the same time remember the seen tasks in a set. afterwards you loop again and delete entries for unseen tasks.)
@Override public void validate() { if (featureNames.isEmpty()) { throw ExceptionsHelper.badRequestException("[feature_names] must not be empty for tree model"); } checkTargetType(); detectMissingNodes(); detectCycle(); }	would it make sense to use feature_names.getpreferredname() here rather than literal?
*/ static <Response> void completeWith(ActionListener<Response> listener, CheckedSupplier<Response, ? extends Exception> supplier) { Response response; try { response = supplier.get(); } catch (Exception e) { try { listener.onFailure(e); } catch (RuntimeException ex) { assert false : ex; throw ex; } return; } try { listener.onResponse(response); } catch (RuntimeException ex) { assert false : ex; throw ex; } }	i'm starting to wonder ... maybe (as the eventual goal of our efforts here, don't think it's possible right now), we should just make actionlistener an abstract class and enforce that nothing is thrown in onresponse and onfailure by simply making these lines call some protected inneronresponse or so. it seems just enforcing the not throwing in oncomplete and map is simply a small part of the overall issue of enforcing the callbacks to handle their own exceptions and we could just dry things up that way?
@Override public boolean shouldFlushToFreeTranslog() { ensureOpen(); final long flushThreshold = config().getIndexSettings().getFlushThresholdSize().getBytes(); final long uncommittedSizeOfCurrentCommit = translog.uncommittedSizeInBytes(); // If flushThreshold is too small, we may continuously flush even there is no uncommitted operations. if (uncommittedSizeOfCurrentCommit < flushThreshold || translog.uncommittedOperations() == 0) { return false; } /* * We should only flush ony if the shouldFlush condition can become false after flushing. * This condition will change if the `uncommittedSize` of the new commit is smaller than * the `uncommittedSize` of the current commit. This method is to maintain translog only, * thus the IndexWriter#hasUncommittedChanges condition is not considered. */ final long uncommittedSizeOfNewCommit = translog.sizeOfGensAboveSeqNoInBytes(localCheckpointTracker.getCheckpoint() + 1); return uncommittedSizeOfNewCommit < uncommittedSizeOfCurrentCommit; }	maybe put the check translog.uncommittedoperations() == 0 at the beginning of the shouldflushtofreetranslog method.
public void testShouldFlushAfterPeerRecovery() throws Exception { try (ReplicationGroup shards = createGroup(0)) { shards.startAll(); long translogSizeOnPrimary = 0; int numDocs = shards.indexDocs(between(10, 100)); translogSizeOnPrimary += shards.getPrimary().getTranslog().uncommittedSizeInBytes(); shards.flush(); final IndexShard replica = shards.addReplica(); IndexMetaData.Builder builder = IndexMetaData.builder(replica.indexSettings().getIndexMetaData()); long flushThreshold = RandomNumbers.randomLongBetween(random(), 100, translogSizeOnPrimary); builder.settings(Settings.builder().put(replica.indexSettings().getSettings()) .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), flushThreshold + "b") ); replica.indexSettings().updateIndexMetaData(builder.build()); replica.onSettingsChanged(); shards.recoverReplica(replica); assertBusy(() -> assertThat(getEngine(replica).shouldFlushToFreeTranslog(), equalTo(false))); assertThat(replica.getTranslog().totalOperations(), equalTo(numDocs)); shards.assertAllEqual(numDocs); } }	can you add javadoc to this method to explain what the goal of this test is?
public void testShouldFlushAfterPeerRecovery() throws Exception { try (ReplicationGroup shards = createGroup(0)) { shards.startAll(); long translogSizeOnPrimary = 0; int numDocs = shards.indexDocs(between(10, 100)); translogSizeOnPrimary += shards.getPrimary().getTranslog().uncommittedSizeInBytes(); shards.flush(); final IndexShard replica = shards.addReplica(); IndexMetaData.Builder builder = IndexMetaData.builder(replica.indexSettings().getIndexMetaData()); long flushThreshold = RandomNumbers.randomLongBetween(random(), 100, translogSizeOnPrimary); builder.settings(Settings.builder().put(replica.indexSettings().getSettings()) .put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), flushThreshold + "b") ); replica.indexSettings().updateIndexMetaData(builder.build()); replica.onSettingsChanged(); shards.recoverReplica(replica); assertBusy(() -> assertThat(getEngine(replica).shouldFlushToFreeTranslog(), equalTo(false))); assertThat(replica.getTranslog().totalOperations(), equalTo(numDocs)); shards.assertAllEqual(numDocs); } }	just define translogsizeonprimary here (no need to initialize)
public void testDeleteByQueryOnReadOnlyIndex() throws Exception { createIndex("test"); final int docs = randomIntBetween(1, 50); List<IndexRequestBuilder> builders = new ArrayList<>(); for (int i = 0; i < docs; i++) { builders.add(client().prepareIndex("test").setId(Integer.toString(i)).setSource("field", 1)); } indexRandom(true, true, true, builders); String block = randomFrom(SETTING_READ_ONLY, SETTING_READ_ONLY_ALLOW_DELETE); try { enableIndexBlock("test", block); if (block.equals(SETTING_READ_ONLY_ALLOW_DELETE)) { setDiskAllocationDeciderEnabled(false); } assertThat(deleteByQuery().source("test").filter(QueryBuilders.matchAllQuery()).refresh(true).get(), matcher().deleted(0).failures(docs)); } finally { disableIndexBlock("test", block); if (block.equals(SETTING_READ_ONLY_ALLOW_DELETE)) { setDiskAllocationDeciderEnabled(true); } } assertHitCount(client().prepareSearch("test").setSize(0).get(), docs); }	i think we should randomize this such that we sometimes test with the decider enabled and sometimes with it disabled. then verify that the the delete by query succeeds when the decider is enabled. we should change the cluster.info.update.interval in the enabled case to be smaller than the default, maybe 500ms.
public void testDeleteByQueryOnReadOnlyIndex() throws Exception { createIndex("test"); final int docs = randomIntBetween(1, 50); List<IndexRequestBuilder> builders = new ArrayList<>(); for (int i = 0; i < docs; i++) { builders.add(client().prepareIndex("test").setId(Integer.toString(i)).setSource("field", 1)); } indexRandom(true, true, true, builders); String block = randomFrom(SETTING_READ_ONLY, SETTING_READ_ONLY_ALLOW_DELETE); try { enableIndexBlock("test", block); if (block.equals(SETTING_READ_ONLY_ALLOW_DELETE)) { setDiskAllocationDeciderEnabled(false); } assertThat(deleteByQuery().source("test").filter(QueryBuilders.matchAllQuery()).refresh(true).get(), matcher().deleted(0).failures(docs)); } finally { disableIndexBlock("test", block); if (block.equals(SETTING_READ_ONLY_ALLOW_DELETE)) { setDiskAllocationDeciderEnabled(true); } } assertHitCount(client().prepareSearch("test").setSize(0).get(), docs); }	we should in the disk decider disabled case change the retry policy for the delete by query request to ensure the test runs faster. i suggest to do: maxretries=2, retrybackoffinitialtime=50ms in that case. the other cases should just run with defaults.
public static void enableIndexBlock(String index, String block) { Settings settings = Settings.builder().put(block, true).build(); client().admin().indices().prepareUpdateSettings(index).setSettings(settings).get(); }	i think we should keep this in the specific test class instead for now.
* @throws Exception when command fails immediately. */ public Shell.Result runElasticsearchStartCommand() throws Exception { switch (distribution.packaging) { case TAR: case ZIP: return Archives.runElasticsearchStartCommand(installation, sh); case DEB: case RPM: return Packages.runElasticsearchStartCommand(sh); case DOCKER: // nothing, "installing" docker image is running it } return Shell.NO_OP; }	shouldn't this be an illegal state? can we return this directly from the docker case above and remove this bare return, so that if we added another packaging type we would not implicitly fall through to this?
* @throws Exception when command fails immediately. */ public Shell.Result runElasticsearchStartCommand() throws Exception { switch (distribution.packaging) { case TAR: case ZIP: return Archives.runElasticsearchStartCommand(installation, sh); case DEB: case RPM: return Packages.runElasticsearchStartCommand(sh); case DOCKER: // nothing, "installing" docker image is running it } return Shell.NO_OP; }	we should probably keep the style consistent, either using packaging conditional, or a switch statement as in the previous method. the former has the advantage that adding any new packaging type will force us to add new cases, although we obviously aren't likely to add any, at least not soon or very often.
public ShardRouting activeReplicaWithHighestVersion(ShardId shardId) { return assignedShards(shardId).stream() .filter(shr -> !shr.primary() && shr.active()) .filter(shr -> node(shr.currentNodeId()) != null) .max(Comparator.comparing(shr -> node(shr.currentNodeId()).node(), Comparator.nullsFirst(Comparator.comparing(DiscoveryNode::getVersion)))) .orElse(null); }	can you readd the comment why we need to consider "null" here?
public void testRandomClusterPromotesNewestReplica() { // we have an IndicesClusterStateService per node in the cluster final Map<DiscoveryNode, IndicesClusterStateService> clusterStateServiceMap = new HashMap<>(); ClusterState state = randomInitialClusterState(clusterStateServiceMap, MockIndicesService::new); // randomly add nodes of mixed versions logger.info("--> adding random nodes"); for (int i = 0; i < randomIntBetween(4, 8); i++) { DiscoveryNodes newNodes = DiscoveryNodes.builder(state.nodes()) .add(createRandomVersionNode()).build(); state = ClusterState.builder(state).nodes(newNodes).build(); state = cluster.reroute(state, new ClusterRerouteRequest()); // always reroute after node leave updateNodes(state, clusterStateServiceMap, MockIndicesService::new); } // Log the shard versions (for debugging if necessary) for (ObjectCursor<DiscoveryNode> cursor : state.nodes().getDataNodes().values()) { Version nodeVer = cursor.value.getVersion(); logger.info("--> node [{}] has version [{}]", cursor.value.getId(), nodeVer); } // randomly create some indices logger.info("--> creating some indices"); for (int i = 0; i < randomIntBetween(2, 5); i++) { String name = "index_" + randomAlphaOfLength(15).toLowerCase(Locale.ROOT); Settings.Builder settingsBuilder = Settings.builder() .put(SETTING_NUMBER_OF_SHARDS, randomIntBetween(1, 3)) .put(SETTING_NUMBER_OF_REPLICAS, randomIntBetween(1, 3)) .put("index.routing.allocation.total_shards_per_node", 1); CreateIndexRequest request = new CreateIndexRequest(name, settingsBuilder.build()).waitForActiveShards(ActiveShardCount.NONE); state = cluster.createIndex(state, request); assertTrue(state.metaData().hasIndex(name)); } state = cluster.reroute(state, new ClusterRerouteRequest()); ClusterState previousState = state; // apply cluster state to nodes (incl. master) for (DiscoveryNode node : state.nodes()) { IndicesClusterStateService indicesClusterStateService = clusterStateServiceMap.get(node); ClusterState localState = adaptClusterStateToLocalNode(state, node); ClusterState previousLocalState = adaptClusterStateToLocalNode(previousState, node); final ClusterChangedEvent event = new ClusterChangedEvent("simulating change", localState, previousLocalState); indicesClusterStateService.applyClusterState(event); // check that cluster state has been properly applied to node assertClusterStateMatchesNodeState(localState, indicesClusterStateService); } logger.info("--> starting shards"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> starting replicas"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> state before failing shards: {}", state); for (int i = 0; i < randomIntBetween(5, 10); i++) { for (ShardRouting shardRouting : state.getRoutingNodes().shardsWithState(STARTED)) { if (shardRouting.primary() && randomBoolean()) { ShardRouting replicaToBePromoted = state.getRoutingNodes() .activeReplicaWithHighestVersion(shardRouting.shardId()); if (replicaToBePromoted != null) { Version replicaNodeVersion = state.nodes().getDataNodes() .get(replicaToBePromoted.currentNodeId()).getVersion(); List<FailedShard> shardsToFail = new ArrayList<>(); logger.info("--> found replica that should be promoted: {}", replicaToBePromoted); logger.info("--> failing shard {}", shardRouting); shardsToFail.add(new FailedShard(shardRouting, "failed primary", new Exception())); state = cluster.applyFailedShards(state, shardsToFail); ShardRouting newPrimary = state.routingTable().index(shardRouting.index()) .shard(shardRouting.id()).primaryShard(); assertThat(newPrimary.allocationId().getId(), equalTo(replicaToBePromoted.allocationId().getId())); } } state = cluster.reroute(state, new ClusterRerouteRequest()); } } } /** * This test ensures that when a node joins a brand new cluster (different cluster UUID), * different from the cluster it was previously a part of, the in-memory index data structures * are all removed but the on disk contents of those indices remain so that they can later be * imported as dangling indices. Normally, the first cluster state update that the node * receives from the new cluster would contain a cluster block that would cause all in-memory * structures to be removed (see {@link IndicesClusterStateService#applyClusterState(ClusterChangedEvent)}	this test does not require indicesclusterstateservice, only the clusterstatechanges class. all the code in this block can go away, it does not add anything to the test. the test can be put into failedshardsroutingtests.
public void testRandomClusterPromotesNewestReplica() { // we have an IndicesClusterStateService per node in the cluster final Map<DiscoveryNode, IndicesClusterStateService> clusterStateServiceMap = new HashMap<>(); ClusterState state = randomInitialClusterState(clusterStateServiceMap, MockIndicesService::new); // randomly add nodes of mixed versions logger.info("--> adding random nodes"); for (int i = 0; i < randomIntBetween(4, 8); i++) { DiscoveryNodes newNodes = DiscoveryNodes.builder(state.nodes()) .add(createRandomVersionNode()).build(); state = ClusterState.builder(state).nodes(newNodes).build(); state = cluster.reroute(state, new ClusterRerouteRequest()); // always reroute after node leave updateNodes(state, clusterStateServiceMap, MockIndicesService::new); } // Log the shard versions (for debugging if necessary) for (ObjectCursor<DiscoveryNode> cursor : state.nodes().getDataNodes().values()) { Version nodeVer = cursor.value.getVersion(); logger.info("--> node [{}] has version [{}]", cursor.value.getId(), nodeVer); } // randomly create some indices logger.info("--> creating some indices"); for (int i = 0; i < randomIntBetween(2, 5); i++) { String name = "index_" + randomAlphaOfLength(15).toLowerCase(Locale.ROOT); Settings.Builder settingsBuilder = Settings.builder() .put(SETTING_NUMBER_OF_SHARDS, randomIntBetween(1, 3)) .put(SETTING_NUMBER_OF_REPLICAS, randomIntBetween(1, 3)) .put("index.routing.allocation.total_shards_per_node", 1); CreateIndexRequest request = new CreateIndexRequest(name, settingsBuilder.build()).waitForActiveShards(ActiveShardCount.NONE); state = cluster.createIndex(state, request); assertTrue(state.metaData().hasIndex(name)); } state = cluster.reroute(state, new ClusterRerouteRequest()); ClusterState previousState = state; // apply cluster state to nodes (incl. master) for (DiscoveryNode node : state.nodes()) { IndicesClusterStateService indicesClusterStateService = clusterStateServiceMap.get(node); ClusterState localState = adaptClusterStateToLocalNode(state, node); ClusterState previousLocalState = adaptClusterStateToLocalNode(previousState, node); final ClusterChangedEvent event = new ClusterChangedEvent("simulating change", localState, previousLocalState); indicesClusterStateService.applyClusterState(event); // check that cluster state has been properly applied to node assertClusterStateMatchesNodeState(localState, indicesClusterStateService); } logger.info("--> starting shards"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> starting replicas"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> state before failing shards: {}", state); for (int i = 0; i < randomIntBetween(5, 10); i++) { for (ShardRouting shardRouting : state.getRoutingNodes().shardsWithState(STARTED)) { if (shardRouting.primary() && randomBoolean()) { ShardRouting replicaToBePromoted = state.getRoutingNodes() .activeReplicaWithHighestVersion(shardRouting.shardId()); if (replicaToBePromoted != null) { Version replicaNodeVersion = state.nodes().getDataNodes() .get(replicaToBePromoted.currentNodeId()).getVersion(); List<FailedShard> shardsToFail = new ArrayList<>(); logger.info("--> found replica that should be promoted: {}", replicaToBePromoted); logger.info("--> failing shard {}", shardRouting); shardsToFail.add(new FailedShard(shardRouting, "failed primary", new Exception())); state = cluster.applyFailedShards(state, shardsToFail); ShardRouting newPrimary = state.routingTable().index(shardRouting.index()) .shard(shardRouting.id()).primaryShard(); assertThat(newPrimary.allocationId().getId(), equalTo(replicaToBePromoted.allocationId().getId())); } } state = cluster.reroute(state, new ClusterRerouteRequest()); } } } /** * This test ensures that when a node joins a brand new cluster (different cluster UUID), * different from the cluster it was previously a part of, the in-memory index data structures * are all removed but the on disk contents of those indices remain so that they can later be * imported as dangling indices. Normally, the first cluster state update that the node * receives from the new cluster would contain a cluster block that would cause all in-memory * structures to be removed (see {@link IndicesClusterStateService#applyClusterState(ClusterChangedEvent)}	reroute happens as part of applystartedshards in the line above
public void testRandomClusterPromotesNewestReplica() { // we have an IndicesClusterStateService per node in the cluster final Map<DiscoveryNode, IndicesClusterStateService> clusterStateServiceMap = new HashMap<>(); ClusterState state = randomInitialClusterState(clusterStateServiceMap, MockIndicesService::new); // randomly add nodes of mixed versions logger.info("--> adding random nodes"); for (int i = 0; i < randomIntBetween(4, 8); i++) { DiscoveryNodes newNodes = DiscoveryNodes.builder(state.nodes()) .add(createRandomVersionNode()).build(); state = ClusterState.builder(state).nodes(newNodes).build(); state = cluster.reroute(state, new ClusterRerouteRequest()); // always reroute after node leave updateNodes(state, clusterStateServiceMap, MockIndicesService::new); } // Log the shard versions (for debugging if necessary) for (ObjectCursor<DiscoveryNode> cursor : state.nodes().getDataNodes().values()) { Version nodeVer = cursor.value.getVersion(); logger.info("--> node [{}] has version [{}]", cursor.value.getId(), nodeVer); } // randomly create some indices logger.info("--> creating some indices"); for (int i = 0; i < randomIntBetween(2, 5); i++) { String name = "index_" + randomAlphaOfLength(15).toLowerCase(Locale.ROOT); Settings.Builder settingsBuilder = Settings.builder() .put(SETTING_NUMBER_OF_SHARDS, randomIntBetween(1, 3)) .put(SETTING_NUMBER_OF_REPLICAS, randomIntBetween(1, 3)) .put("index.routing.allocation.total_shards_per_node", 1); CreateIndexRequest request = new CreateIndexRequest(name, settingsBuilder.build()).waitForActiveShards(ActiveShardCount.NONE); state = cluster.createIndex(state, request); assertTrue(state.metaData().hasIndex(name)); } state = cluster.reroute(state, new ClusterRerouteRequest()); ClusterState previousState = state; // apply cluster state to nodes (incl. master) for (DiscoveryNode node : state.nodes()) { IndicesClusterStateService indicesClusterStateService = clusterStateServiceMap.get(node); ClusterState localState = adaptClusterStateToLocalNode(state, node); ClusterState previousLocalState = adaptClusterStateToLocalNode(previousState, node); final ClusterChangedEvent event = new ClusterChangedEvent("simulating change", localState, previousLocalState); indicesClusterStateService.applyClusterState(event); // check that cluster state has been properly applied to node assertClusterStateMatchesNodeState(localState, indicesClusterStateService); } logger.info("--> starting shards"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> starting replicas"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> state before failing shards: {}", state); for (int i = 0; i < randomIntBetween(5, 10); i++) { for (ShardRouting shardRouting : state.getRoutingNodes().shardsWithState(STARTED)) { if (shardRouting.primary() && randomBoolean()) { ShardRouting replicaToBePromoted = state.getRoutingNodes() .activeReplicaWithHighestVersion(shardRouting.shardId()); if (replicaToBePromoted != null) { Version replicaNodeVersion = state.nodes().getDataNodes() .get(replicaToBePromoted.currentNodeId()).getVersion(); List<FailedShard> shardsToFail = new ArrayList<>(); logger.info("--> found replica that should be promoted: {}", replicaToBePromoted); logger.info("--> failing shard {}", shardRouting); shardsToFail.add(new FailedShard(shardRouting, "failed primary", new Exception())); state = cluster.applyFailedShards(state, shardsToFail); ShardRouting newPrimary = state.routingTable().index(shardRouting.index()) .shard(shardRouting.id()).primaryShard(); assertThat(newPrimary.allocationId().getId(), equalTo(replicaToBePromoted.allocationId().getId())); } } state = cluster.reroute(state, new ClusterRerouteRequest()); } } } /** * This test ensures that when a node joins a brand new cluster (different cluster UUID), * different from the cluster it was previously a part of, the in-memory index data structures * are all removed but the on disk contents of those indices remain so that they can later be * imported as dangling indices. Normally, the first cluster state update that the node * receives from the new cluster would contain a cluster block that would cause all in-memory * structures to be removed (see {@link IndicesClusterStateService#applyClusterState(ClusterChangedEvent)}	there is no guarantee that all replicas are started (as we have throttling). it's good to test the situation where not all replicas are started though, so maybe we can call applystartedshards a random number of times.
public void testRandomClusterPromotesNewestReplica() { // we have an IndicesClusterStateService per node in the cluster final Map<DiscoveryNode, IndicesClusterStateService> clusterStateServiceMap = new HashMap<>(); ClusterState state = randomInitialClusterState(clusterStateServiceMap, MockIndicesService::new); // randomly add nodes of mixed versions logger.info("--> adding random nodes"); for (int i = 0; i < randomIntBetween(4, 8); i++) { DiscoveryNodes newNodes = DiscoveryNodes.builder(state.nodes()) .add(createRandomVersionNode()).build(); state = ClusterState.builder(state).nodes(newNodes).build(); state = cluster.reroute(state, new ClusterRerouteRequest()); // always reroute after node leave updateNodes(state, clusterStateServiceMap, MockIndicesService::new); } // Log the shard versions (for debugging if necessary) for (ObjectCursor<DiscoveryNode> cursor : state.nodes().getDataNodes().values()) { Version nodeVer = cursor.value.getVersion(); logger.info("--> node [{}] has version [{}]", cursor.value.getId(), nodeVer); } // randomly create some indices logger.info("--> creating some indices"); for (int i = 0; i < randomIntBetween(2, 5); i++) { String name = "index_" + randomAlphaOfLength(15).toLowerCase(Locale.ROOT); Settings.Builder settingsBuilder = Settings.builder() .put(SETTING_NUMBER_OF_SHARDS, randomIntBetween(1, 3)) .put(SETTING_NUMBER_OF_REPLICAS, randomIntBetween(1, 3)) .put("index.routing.allocation.total_shards_per_node", 1); CreateIndexRequest request = new CreateIndexRequest(name, settingsBuilder.build()).waitForActiveShards(ActiveShardCount.NONE); state = cluster.createIndex(state, request); assertTrue(state.metaData().hasIndex(name)); } state = cluster.reroute(state, new ClusterRerouteRequest()); ClusterState previousState = state; // apply cluster state to nodes (incl. master) for (DiscoveryNode node : state.nodes()) { IndicesClusterStateService indicesClusterStateService = clusterStateServiceMap.get(node); ClusterState localState = adaptClusterStateToLocalNode(state, node); ClusterState previousLocalState = adaptClusterStateToLocalNode(previousState, node); final ClusterChangedEvent event = new ClusterChangedEvent("simulating change", localState, previousLocalState); indicesClusterStateService.applyClusterState(event); // check that cluster state has been properly applied to node assertClusterStateMatchesNodeState(localState, indicesClusterStateService); } logger.info("--> starting shards"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> starting replicas"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> state before failing shards: {}", state); for (int i = 0; i < randomIntBetween(5, 10); i++) { for (ShardRouting shardRouting : state.getRoutingNodes().shardsWithState(STARTED)) { if (shardRouting.primary() && randomBoolean()) { ShardRouting replicaToBePromoted = state.getRoutingNodes() .activeReplicaWithHighestVersion(shardRouting.shardId()); if (replicaToBePromoted != null) { Version replicaNodeVersion = state.nodes().getDataNodes() .get(replicaToBePromoted.currentNodeId()).getVersion(); List<FailedShard> shardsToFail = new ArrayList<>(); logger.info("--> found replica that should be promoted: {}", replicaToBePromoted); logger.info("--> failing shard {}", shardRouting); shardsToFail.add(new FailedShard(shardRouting, "failed primary", new Exception())); state = cluster.applyFailedShards(state, shardsToFail); ShardRouting newPrimary = state.routingTable().index(shardRouting.index()) .shard(shardRouting.id()).primaryShard(); assertThat(newPrimary.allocationId().getId(), equalTo(replicaToBePromoted.allocationId().getId())); } } state = cluster.reroute(state, new ClusterRerouteRequest()); } } } /** * This test ensures that when a node joins a brand new cluster (different cluster UUID), * different from the cluster it was previously a part of, the in-memory index data structures * are all removed but the on disk contents of those indices remain so that they can later be * imported as dangling indices. Normally, the first cluster state update that the node * receives from the new cluster would contain a cluster block that would cause all in-memory * structures to be removed (see {@link IndicesClusterStateService#applyClusterState(ClusterChangedEvent)}	you're testing the method activereplicawithhighestversion here using the method itself? i see no checks here that the primary is indeed on the node with the highest version. i think for the purpose of the test it is sufficient to check that 1) if there was at least one active replica while the primary was failed, that a new active primary got assigned 2) that the new active primary is on a node with higher or equal version than the replicas.
public void testRandomClusterPromotesNewestReplica() { // we have an IndicesClusterStateService per node in the cluster final Map<DiscoveryNode, IndicesClusterStateService> clusterStateServiceMap = new HashMap<>(); ClusterState state = randomInitialClusterState(clusterStateServiceMap, MockIndicesService::new); // randomly add nodes of mixed versions logger.info("--> adding random nodes"); for (int i = 0; i < randomIntBetween(4, 8); i++) { DiscoveryNodes newNodes = DiscoveryNodes.builder(state.nodes()) .add(createRandomVersionNode()).build(); state = ClusterState.builder(state).nodes(newNodes).build(); state = cluster.reroute(state, new ClusterRerouteRequest()); // always reroute after node leave updateNodes(state, clusterStateServiceMap, MockIndicesService::new); } // Log the shard versions (for debugging if necessary) for (ObjectCursor<DiscoveryNode> cursor : state.nodes().getDataNodes().values()) { Version nodeVer = cursor.value.getVersion(); logger.info("--> node [{}] has version [{}]", cursor.value.getId(), nodeVer); } // randomly create some indices logger.info("--> creating some indices"); for (int i = 0; i < randomIntBetween(2, 5); i++) { String name = "index_" + randomAlphaOfLength(15).toLowerCase(Locale.ROOT); Settings.Builder settingsBuilder = Settings.builder() .put(SETTING_NUMBER_OF_SHARDS, randomIntBetween(1, 3)) .put(SETTING_NUMBER_OF_REPLICAS, randomIntBetween(1, 3)) .put("index.routing.allocation.total_shards_per_node", 1); CreateIndexRequest request = new CreateIndexRequest(name, settingsBuilder.build()).waitForActiveShards(ActiveShardCount.NONE); state = cluster.createIndex(state, request); assertTrue(state.metaData().hasIndex(name)); } state = cluster.reroute(state, new ClusterRerouteRequest()); ClusterState previousState = state; // apply cluster state to nodes (incl. master) for (DiscoveryNode node : state.nodes()) { IndicesClusterStateService indicesClusterStateService = clusterStateServiceMap.get(node); ClusterState localState = adaptClusterStateToLocalNode(state, node); ClusterState previousLocalState = adaptClusterStateToLocalNode(previousState, node); final ClusterChangedEvent event = new ClusterChangedEvent("simulating change", localState, previousLocalState); indicesClusterStateService.applyClusterState(event); // check that cluster state has been properly applied to node assertClusterStateMatchesNodeState(localState, indicesClusterStateService); } logger.info("--> starting shards"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> starting replicas"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> state before failing shards: {}", state); for (int i = 0; i < randomIntBetween(5, 10); i++) { for (ShardRouting shardRouting : state.getRoutingNodes().shardsWithState(STARTED)) { if (shardRouting.primary() && randomBoolean()) { ShardRouting replicaToBePromoted = state.getRoutingNodes() .activeReplicaWithHighestVersion(shardRouting.shardId()); if (replicaToBePromoted != null) { Version replicaNodeVersion = state.nodes().getDataNodes() .get(replicaToBePromoted.currentNodeId()).getVersion(); List<FailedShard> shardsToFail = new ArrayList<>(); logger.info("--> found replica that should be promoted: {}", replicaToBePromoted); logger.info("--> failing shard {}", shardRouting); shardsToFail.add(new FailedShard(shardRouting, "failed primary", new Exception())); state = cluster.applyFailedShards(state, shardsToFail); ShardRouting newPrimary = state.routingTable().index(shardRouting.index()) .shard(shardRouting.id()).primaryShard(); assertThat(newPrimary.allocationId().getId(), equalTo(replicaToBePromoted.allocationId().getId())); } } state = cluster.reroute(state, new ClusterRerouteRequest()); } } } /** * This test ensures that when a node joins a brand new cluster (different cluster UUID), * different from the cluster it was previously a part of, the in-memory index data structures * are all removed but the on disk contents of those indices remain so that they can later be * imported as dangling indices. Normally, the first cluster state update that the node * receives from the new cluster would contain a cluster block that would cause all in-memory * structures to be removed (see {@link IndicesClusterStateService#applyClusterState(ClusterChangedEvent)}	you're testing only one failure at a time. instead, the test could select a subset of the primary shards at random (and also a few replica shards) and fail them in one go.
public void testRandomClusterPromotesNewestReplica() { // we have an IndicesClusterStateService per node in the cluster final Map<DiscoveryNode, IndicesClusterStateService> clusterStateServiceMap = new HashMap<>(); ClusterState state = randomInitialClusterState(clusterStateServiceMap, MockIndicesService::new); // randomly add nodes of mixed versions logger.info("--> adding random nodes"); for (int i = 0; i < randomIntBetween(4, 8); i++) { DiscoveryNodes newNodes = DiscoveryNodes.builder(state.nodes()) .add(createRandomVersionNode()).build(); state = ClusterState.builder(state).nodes(newNodes).build(); state = cluster.reroute(state, new ClusterRerouteRequest()); // always reroute after node leave updateNodes(state, clusterStateServiceMap, MockIndicesService::new); } // Log the shard versions (for debugging if necessary) for (ObjectCursor<DiscoveryNode> cursor : state.nodes().getDataNodes().values()) { Version nodeVer = cursor.value.getVersion(); logger.info("--> node [{}] has version [{}]", cursor.value.getId(), nodeVer); } // randomly create some indices logger.info("--> creating some indices"); for (int i = 0; i < randomIntBetween(2, 5); i++) { String name = "index_" + randomAlphaOfLength(15).toLowerCase(Locale.ROOT); Settings.Builder settingsBuilder = Settings.builder() .put(SETTING_NUMBER_OF_SHARDS, randomIntBetween(1, 3)) .put(SETTING_NUMBER_OF_REPLICAS, randomIntBetween(1, 3)) .put("index.routing.allocation.total_shards_per_node", 1); CreateIndexRequest request = new CreateIndexRequest(name, settingsBuilder.build()).waitForActiveShards(ActiveShardCount.NONE); state = cluster.createIndex(state, request); assertTrue(state.metaData().hasIndex(name)); } state = cluster.reroute(state, new ClusterRerouteRequest()); ClusterState previousState = state; // apply cluster state to nodes (incl. master) for (DiscoveryNode node : state.nodes()) { IndicesClusterStateService indicesClusterStateService = clusterStateServiceMap.get(node); ClusterState localState = adaptClusterStateToLocalNode(state, node); ClusterState previousLocalState = adaptClusterStateToLocalNode(previousState, node); final ClusterChangedEvent event = new ClusterChangedEvent("simulating change", localState, previousLocalState); indicesClusterStateService.applyClusterState(event); // check that cluster state has been properly applied to node assertClusterStateMatchesNodeState(localState, indicesClusterStateService); } logger.info("--> starting shards"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> starting replicas"); state = cluster.applyStartedShards(state, state.getRoutingNodes().shardsWithState(INITIALIZING));; state = cluster.reroute(state, new ClusterRerouteRequest()); logger.info("--> state before failing shards: {}", state); for (int i = 0; i < randomIntBetween(5, 10); i++) { for (ShardRouting shardRouting : state.getRoutingNodes().shardsWithState(STARTED)) { if (shardRouting.primary() && randomBoolean()) { ShardRouting replicaToBePromoted = state.getRoutingNodes() .activeReplicaWithHighestVersion(shardRouting.shardId()); if (replicaToBePromoted != null) { Version replicaNodeVersion = state.nodes().getDataNodes() .get(replicaToBePromoted.currentNodeId()).getVersion(); List<FailedShard> shardsToFail = new ArrayList<>(); logger.info("--> found replica that should be promoted: {}", replicaToBePromoted); logger.info("--> failing shard {}", shardRouting); shardsToFail.add(new FailedShard(shardRouting, "failed primary", new Exception())); state = cluster.applyFailedShards(state, shardsToFail); ShardRouting newPrimary = state.routingTable().index(shardRouting.index()) .shard(shardRouting.id()).primaryShard(); assertThat(newPrimary.allocationId().getId(), equalTo(replicaToBePromoted.allocationId().getId())); } } state = cluster.reroute(state, new ClusterRerouteRequest()); } } } /** * This test ensures that when a node joins a brand new cluster (different cluster UUID), * different from the cluster it was previously a part of, the in-memory index data structures * are all removed but the on disk contents of those indices remain so that they can later be * imported as dangling indices. Normally, the first cluster state update that the node * receives from the new cluster would contain a cluster block that would cause all in-memory * structures to be removed (see {@link IndicesClusterStateService#applyClusterState(ClusterChangedEvent)}	an alternative to explicit shard failing is to remove nodes where the shards are allocated (i.e. when a node disconnects from the cluster). this would also test the scenario where discoverynode is null in the routingnode.
private List<Stats> buildStatsWithDefaults(List<DataFrameAnalyticsConfig> configs, List<Stats> stats) { Map<String, DataFrameAnalyticsConfig> configById = new HashMap<>(); for (DataFrameAnalyticsConfig config : configs) { configById.put(config.getId(), config); } List<Stats> statsWithDefaults = new ArrayList<>(stats.size()); for (Stats statsItem : stats) { DataFrameAnalyticsConfig config = configById.get(statsItem.getId()); statsWithDefaults.add(new Stats( statsItem.getId(), statsItem.getState(), statsItem.getFailureReason(), statsItem.getProgress(), statsItem.getDataCounts() == null ? new DataCounts(config.getId()) : statsItem.getDataCounts(), statsItem.getMemoryUsage() == null ? new MemoryUsage(config.getId(), config.getCreateTime(), 0) : statsItem.getMemoryUsage(), statsItem.getAnalysisStats(), statsItem.getNode(), statsItem.getAssignmentExplanation() )); } return statsWithDefaults; }	why can't this and the memory usage be initialized in the stats ctor? this.memoryusage = memoryusage == null ? new memoryusage(config.getid(), instant.now(), 0) : memoryusage; this.datacounts = datacounts == null ? new datacounts(config.getid()) : datacounts; why do we need to set memory_usage create time to the config's create time? is that adding value for the added complexity? or maybe i am misunderstanding the stats#getid() value...
public void printLogo(CliTerminal terminal) { terminal.clear(); int lineLength = 0; try (InputStream in = Cli.class.getResourceAsStream("/logo.txt")) { if (in == null) { throw new FatalCliException("Could not find logo!"); } try (BufferedReader reader = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8))) { String line; while ((line = reader.readLine()) != null) { lineLength = Math.max(lineLength, line.length()); terminal.println(line); } } } catch (IOException e) { throw new FatalCliException("Could not load logo!", e); } // print the version centered on the last line char[] whitespaces = new char[lineLength / 2 - Version.CURRENT.version.length() / 2]; Arrays.fill(whitespaces, ' '); terminal.println(new StringBuilder().append(whitespaces).append(Version.CURRENT.version).toString()); terminal.println(); }	to minimize rounding errors use (linelength-version.current.version.length())/2. it's also shorter.
*/ private static ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards( @Nullable SnapshotsInProgress snapshotsInProgress, @Nullable SnapshotDeletionsInProgress deletionsInProgress, Metadata metadata, RoutingTable routingTable, List<IndexId> indices, boolean useShardGenerations, RepositoryData repositoryData, String repoName) { ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> builder = ImmutableOpenMap.builder(); final ShardGenerations shardGenerations = repositoryData.shardGenerations(); final Set<ShardId> inProgressShards = busyShardsForRepo(repoName, snapshotsInProgress); final boolean readyToExecute = deletionsInProgress == null || deletionsInProgress.getEntries().stream() .noneMatch(entry -> entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.STARTED); for (IndexId index : indices) { final String indexName = index.getName(); final boolean isNewIndex = repositoryData.getIndices().containsKey(indexName) == false; IndexMetadata indexMetadata = metadata.index(indexName); if (indexMetadata == null) { // The index was deleted before we managed to start the snapshot - mark it as missing. builder.put(new ShardId(indexName, IndexMetadata.INDEX_UUID_NA_VALUE, 0), ShardSnapshotStatus.MISSING); } else { final IndexRoutingTable indexRoutingTable = routingTable.index(indexName); assert indexRoutingTable != null; for (int i = 0; i < indexMetadata.getNumberOfShards(); i++) { ShardId shardId = new ShardId(indexMetadata.getIndex(), i); final String shardRepoGeneration; if (useShardGenerations) { if (isNewIndex) { assert shardGenerations.getShardGen(index, shardId.getId()) == null : "Found shard generation for new index [" + index + "]"; shardRepoGeneration = ShardGenerations.NEW_SHARD_GEN; } else { shardRepoGeneration = shardGenerations.getShardGen(index, shardId.getId()); } } else { shardRepoGeneration = null; } final ShardSnapshotStatus shardSnapshotStatus; ShardRouting primary = indexRoutingTable.shard(i).primaryShard(); if (readyToExecute == false || inProgressShards.contains(shardId)) { shardSnapshotStatus = ShardSnapshotStatus.UNASSIGNED_QUEUED; } else if (primary == null || !primary.assignedToNode()) { shardSnapshotStatus = new ShardSnapshotStatus(null, ShardState.MISSING, "primary shard is not allocated", shardRepoGeneration); } else if (primary.relocating() || primary.initializing()) { shardSnapshotStatus = new ShardSnapshotStatus( primary.currentNodeId(), ShardState.WAITING, shardRepoGeneration); } else if (!primary.started()) { shardSnapshotStatus = new ShardSnapshotStatus(primary.currentNodeId(), ShardState.MISSING, "primary shard hasn't been started yet", shardRepoGeneration); } else { shardSnapshotStatus = new ShardSnapshotStatus(primary.currentNodeId(), shardRepoGeneration); } builder.put(shardId, shardSnapshotStatus); } } } return builder.build(); }	before concurrent snapshots this spot would cover all possible scenarios because we'd only be dealing with shard ids for indices that still exist in the repo ever beyond this point. if an index was deleted after assignment then it would just fail in the snapshotshardsservice and things would work out that way. but with concurrent snapshots where we could have indices deleted from under a queued up shard snapshot we have to explicitly deal with this situation.
@Override public void clusterChanged(ClusterChangedEvent event) { if (event.state().blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK)) { // Wait until the gateway has recovered from disk. return; } // The atomic flag prevents multiple simultaneous attempts to create the // index if there is a flurry of cluster state updates in quick succession if (event.localNodeMaster() && isIndexCreationInProgress.compareAndSet(false, true)) { AnnotationIndex.createAnnotationsIndexIfNecessary(client, event.state(), ActionListener.wrap( r -> { isIndexCreationInProgress.set(false); if (r) { logger.info("Created ML annotations index and aliases"); } }, e -> { isIndexCreationInProgress.set(false); logger.error("Error creating ML annotations index or aliases", e); })); } }	this still leaves open the following bug (unsure how to fix it really). - node running the job is updated - job gets reallocated to a new node - job writes annotation for a model snapshot - the master node is still old, consequently never crated the annotations index - annotation persistence auto-created the index and now the ui fails to read from the index. it seems to me that we need to verify if the index exists already and create it if necessary when we write annotations.
static DataStream updateLocalDataStream(Index backingIndexToFollow, DataStream localDataStream, DataStream remoteDataStream) { if (localDataStream == null) { // The data stream and the backing indices have been created and validated in the remote cluster, // just copying the data stream is in this case safe. return new DataStream(remoteDataStream.getName(), remoteDataStream.getTimeStampField(), List.of(backingIndexToFollow), remoteDataStream.getGeneration(), remoteDataStream.getMetadata()); } else { List<Index> backingIndices = new ArrayList<>(localDataStream.getIndices()); backingIndices.add(backingIndexToFollow); // When following an older backing index it should be positioned before the newer backing indices. // Currently the assumption is that the newest index (highest generation) is the write index. // (just appending an older backing index to the list of backing indices would break that assumption) // (string sorting works because of the naming backing index naming scheme) backingIndices.sort(Comparator.comparing(Index::getName)); return new DataStream(localDataStream.getName(), localDataStream.getTimeStampField(), backingIndices, remoteDataStream.getGeneration(), remoteDataStream.getMetadata()); } }	i think we can't use the generation from the remotedatastream as it will break the [assertion](https://github.com/elastic/elasticsearch/blob/709264ac4e932fb54c0cad02f06f6b69c29b3ce9/server/src/main/java/org/elasticsearch/cluster/metadata/datastream.java#l60) in datastream's constructor. for example, here we are following .ds-1 and the ds generation on the remote is 2 already.
static DataStream updateLocalDataStream(Index backingIndexToFollow, DataStream localDataStream, DataStream remoteDataStream) { if (localDataStream == null) { // The data stream and the backing indices have been created and validated in the remote cluster, // just copying the data stream is in this case safe. return new DataStream(remoteDataStream.getName(), remoteDataStream.getTimeStampField(), List.of(backingIndexToFollow), remoteDataStream.getGeneration(), remoteDataStream.getMetadata()); } else { List<Index> backingIndices = new ArrayList<>(localDataStream.getIndices()); backingIndices.add(backingIndexToFollow); // When following an older backing index it should be positioned before the newer backing indices. // Currently the assumption is that the newest index (highest generation) is the write index. // (just appending an older backing index to the list of backing indices would break that assumption) // (string sorting works because of the naming backing index naming scheme) backingIndices.sort(Comparator.comparing(Index::getName)); return new DataStream(localDataStream.getName(), localDataStream.getTimeStampField(), backingIndices, remoteDataStream.getGeneration(), remoteDataStream.getMetadata()); } }	what if the generation of the local datastream is higher than the remote?
public IngestDocument execute(IngestDocument ingestDocument) throws Exception { String path; Map<String, Object> map; if (this.path != null) { path = this.path + "." + field; map = ingestDocument.getFieldValue(this.path, Map.class); } else { path = field; map = ingestDocument.getSourceAndMetadata(); } if (this.field.equals("*")) { for (String key : new ArrayList<>(map.keySet())) { if (key.indexOf('.') > 0) { path = this.path != null ? this.path + "." + key : key; expandDot(ingestDocument, path, key, map); } } } else { expandDot(ingestDocument, path, field, map); } return ingestDocument; }	i don't think the keyset needs to be wrapped in a list: suggestion for (string key : map.keyset()) {
public Index getWriteIndex(DocWriteRequest<?> request, Metadata metadata) { if (type != Type.TSDB) { return getWriteIndex(); } if (request instanceof IndexRequest == false || request.opType() != DocWriteRequest.OpType.CREATE) { return getWriteIndex(); } // TODO: this really needs be parsed in a streaming manner: String timestampAsString = (String) ((IndexRequest) request).sourceAsMap().get("@timestamp"); long timestamp = DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parseMillis(timestampAsString); for (int i = indices.size() - 1; i >= 0; i--) { Index index = indices.get(i); IndexMetadata im = metadata.index(index); // TODO: make start and end time fields in IndexMetadata class. // (this to avoid the Settings overhead that happens here now) String startAsString = im.getSettings().get(IndexSettings.TIME_SERIES_START_TIME.getKey()); long start = Instant.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(startAsString)).toEpochMilli(); String endAsString = im.getSettings().get(IndexSettings.TIME_SERIES_END_TIME.getKey()); long end = Instant.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(endAsString)).toEpochMilli(); if (timestamp > start && timestamp <= end) { return index; } } throw new IllegalArgumentException("no index available for a document with an @timestamp of [" + timestampAsString + "]"); }	maybe a bad idea - could we check that the getwriteindex().mode() == tsdb? i was sort of hoping that indexrouting could handle this kind of redirection somehow because it "feels right". we already have a specialization of it for tsdb. and, it's, like, testable without building the whole world.
public Index getWriteIndex(DocWriteRequest<?> request, Metadata metadata) { if (type != Type.TSDB) { return getWriteIndex(); } if (request instanceof IndexRequest == false || request.opType() != DocWriteRequest.OpType.CREATE) { return getWriteIndex(); } // TODO: this really needs be parsed in a streaming manner: String timestampAsString = (String) ((IndexRequest) request).sourceAsMap().get("@timestamp"); long timestamp = DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parseMillis(timestampAsString); for (int i = indices.size() - 1; i >= 0; i--) { Index index = indices.get(i); IndexMetadata im = metadata.index(index); // TODO: make start and end time fields in IndexMetadata class. // (this to avoid the Settings overhead that happens here now) String startAsString = im.getSettings().get(IndexSettings.TIME_SERIES_START_TIME.getKey()); long start = Instant.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(startAsString)).toEpochMilli(); String endAsString = im.getSettings().get(IndexSettings.TIME_SERIES_END_TIME.getKey()); long end = Instant.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(endAsString)).toEpochMilli(); if (timestamp > start && timestamp <= end) { return index; } } throw new IllegalArgumentException("no index available for a document with an @timestamp of [" + timestampAsString + "]"); }	xcontentparser now takes a filter. so you can build it with the filter and then do try (xcontentparser parser = ts_extract_config.parser(request.type(), request.source())) { expect(start_object, parser.next()); expect(field_name, parser.next()); expect(value, parser.next()); return datefieldmaper.default_date_time_formatter.parsemillis(parser.text()); }
public Index getWriteIndex(DocWriteRequest<?> request, Metadata metadata) { if (type != Type.TSDB) { return getWriteIndex(); } if (request instanceof IndexRequest == false || request.opType() != DocWriteRequest.OpType.CREATE) { return getWriteIndex(); } // TODO: this really needs be parsed in a streaming manner: String timestampAsString = (String) ((IndexRequest) request).sourceAsMap().get("@timestamp"); long timestamp = DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parseMillis(timestampAsString); for (int i = indices.size() - 1; i >= 0; i--) { Index index = indices.get(i); IndexMetadata im = metadata.index(index); // TODO: make start and end time fields in IndexMetadata class. // (this to avoid the Settings overhead that happens here now) String startAsString = im.getSettings().get(IndexSettings.TIME_SERIES_START_TIME.getKey()); long start = Instant.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(startAsString)).toEpochMilli(); String endAsString = im.getSettings().get(IndexSettings.TIME_SERIES_END_TIME.getKey()); long end = Instant.from(DateFieldMapper.DEFAULT_DATE_TIME_FORMATTER.parse(endAsString)).toEpochMilli(); if (timestamp > start && timestamp <= end) { return index; } } throw new IllegalArgumentException("no index available for a document with an @timestamp of [" + timestampAsString + "]"); }	we should probably throw an error in the mapping if the field doesn't have the default formatter. hmmmmmm.......... what about nanos?
public void testStartingAndStoppingNodes() throws IOException { logger.info("--> cluster has [{}] nodes", internalCluster().size()); if (internalCluster().size() < 5) { final int nodesToStart = randomIntBetween(Math.max(2, internalCluster().size() + 1), 5); logger.info("--> growing to [{}] nodes", nodesToStart); internalCluster().startNodes(nodesToStart); } ensureGreen(); while (internalCluster().size() > 1) { final int nodesToRemain = randomIntBetween(1, internalCluster().size() - 1); logger.info("--> reducing to [{}] nodes", nodesToRemain); internalCluster().ensureAtMostNumDataNodes(nodesToRemain); assertThat(internalCluster().size(), lessThanOrEqualTo(nodesToRemain)); } ensureGreen(); }	i'm not sure what you're testing here with this assertion. why not just repeatedly call stoprandomnode(), and maybe check that the cluster is alive and healthy before shutting down the last node.
public void testDecompressRandomBytes() throws IOException { for (int i = 0; i < 15; ++i) { int uncompressedBytesLength = randomFrom(16, 32, 64, 128, 256, 512, 1024) * 1024; BytesStreamOutput bytesStreamOutput = new BytesStreamOutput(uncompressedBytesLength); for (int j = 0; j < uncompressedBytesLength / 4; ++j) { bytesStreamOutput.writeInt(randomFrom(0, 1, randomInt())); } byte[] uncompressed = new byte[uncompressedBytesLength]; bytesStreamOutput.bytes().streamInput().read(uncompressed); byte[] compressed = new byte[uncompressed.length + uncompressed.length / 255 + 16]; LZ4Compressor compressor = LZ4Factory.safeInstance().fastCompressor(); compressor.compress(uncompressed, compressed); LZ4FastDecompressor decompressor = ESLZ4Decompressor.INSTANCE; byte[] output = new byte[uncompressed.length]; decompressor.decompress(compressed, output); assertArrayEquals(uncompressed, output); } }	given that the test in eslz4compressortests now verifies that the compression gives same results, perhaps move this there too?
private static Mapper getMapper(final ParseContext context, ObjectMapper objectMapper, String fieldName, String[] subfields) { String fieldPath = context.path().pathAsText(fieldName); // Check if mapper is a metadata mapper first Mapper mapper = context.docMapper().mapping().getMetadataMapper(fieldPath); if (mapper != null) { return mapper; } for (int i = 0; i < subfields.length - 1; ++i) { mapper = objectMapper.getMapper(subfields[i]); if (mapper instanceof ObjectMapper == false) { return null; } objectMapper = (ObjectMapper)mapper; if (objectMapper.nested().isNested()) { throw new MapperParsingException("Cannot add a value for field [" + fieldName + "] since one of the intermediate objects is mapped as a nested object: [" + mapper.name() + "]"); } } String leafName = subfields[subfields.length - 1]; return objectMapper.getMapper(leafName); }	i can't seem to find a name that clearly explains why we have this new method as opposed to the existing getmapper one. shall we add a comment to explain what is the difference between the two and why?
private static Mapper getLeafMapper(final ParseContext context, ObjectMapper objectMapper, String fieldName, String[] subfields) { Mapper mapper = getMapper(context, objectMapper, fieldName, subfields); if (mapper != null) { return mapper; } // concrete fields take precedence over runtime fields when parsing documents // if a leaf field is not mapped, and is defined as a runtime field, then we // don't create a dynamic mapping for it and don't index it. String fieldPath = context.path().pathAsText(fieldName); RuntimeFieldType runtimeFieldType = context.docMapper().mapping().root.getRuntimeFieldType(fieldPath); if (runtimeFieldType != null) { return new NoOpFieldMapper(subfields[subfields.length - 1], runtimeFieldType); } return null; }	thanks for clarifying this comment, i meant to go back and update it :)
public void testRuntimeFieldDoesNotShadowObjectChildren() throws IOException { DocumentMapper mapper = createDocumentMapper(topMapping(b -> { b.field("dynamic", "true"); b.startObject("runtime"); { b.startObject("location").field("type", "test").endObject(); b.startObject("country").field("type", "test").endObject(); } b.endObject(); b.startObject("properties"); { b.startObject("timestamp").field("type", "date").endObject(); b.startObject("concrete").field("type", "keyword").endObject(); } b.endObject(); })); { ParsedDocument doc = mapper.parse(source(b -> { b.field("timestamp", "1998-04-30T14:30:17-05:00"); b.startObject("location"); { b.field("lat", 13.5); b.field("lon", 34.89); } b.endObject(); b.field("country", "de"); b.field("concrete", "foo"); })); assertNotNull(doc.rootDoc().getField("timestamp")); assertNotNull(doc.rootDoc().getField("_source")); assertNotNull(doc.rootDoc().getField("location.lat")); assertNotNull(doc.rootDoc().getField("concrete")); assertNull(doc.rootDoc().getField("country")); } { ParsedDocument doc = mapper.parse(source(b -> { b.field("timestamp", "1998-04-30T14:30:17-05:00"); b.startArray("location"); { b.startObject().field("lat", 13.5).field("lon", 34.89).endObject(); b.startObject().field("lat", 14.5).field("lon", 89.33).endObject(); } b.endArray(); b.field("country", "de"); b.field("concrete", "foo"); })); assertNotNull(doc.rootDoc().getField("timestamp")); assertNotNull(doc.rootDoc().getField("_source")); assertThat(doc.rootDoc().getFields("location.lat").length, equalTo(4)); assertNotNull(doc.rootDoc().getField("concrete")); assertNull(doc.rootDoc().getField("country")); } { ParsedDocument doc = mapper.parse(source(b -> { b.field("timestamp", "1998-04-30T14:30:17-05:00"); b.startObject("location"); { b.array("lat", 13.5, 14.5); b.array("lon", 34.89, 89.33); } b.endObject(); b.field("country", "de"); b.field("concrete", "foo"); })); assertNotNull(doc.rootDoc().getField("timestamp")); assertNotNull(doc.rootDoc().getField("_source")); assertThat(doc.rootDoc().getFields("location.lat").length, equalTo(4)); assertNotNull(doc.rootDoc().getField("concrete")); assertNull(doc.rootDoc().getField("country")); } }	are you not checking for location.lon on purpose?
protected void doRun() throws Exception { while (context.hasMoreOperationsToExecute()) { if (executeBulkItemRequest(context, updateHelper, nowInMillisSupplier, mappingUpdater, waitForMappingUpdate, ActionListener.wrap(v -> executor.execute(this), this::onRejection)) == false) { // We are waiting for a mapping update on another thread, that will invoke this action again once its done // so we just break out here. return; } assert context.isInitial(); // either completed and moved to next or reset } // We're done, there's no more operations to execute so we resolve the wrapped listener finishRequest(); primary.getBulkOperationListener().afterBulk(request.totalSizeInBytes(), System.nanoTime() - startBulkTime); }	i believe this should be moved to before the finishrequest(), finishing the request entails running a number of things that happen as part of the listener passed in to this method, and while it could be good to capture the time spent doing those things, right now we do *not* capture them for the replica operation (since they are done after shardoperationonreplica is called. if we want to stick with tracking only the indexing time (rather than refresh, checkpoint update, etc), we should track the time prior to the finishrequest() call
public void testBulkStats() throws Exception { final String index = "test"; assertAcked(prepareCreate(index).setSettings(settingsBuilder().put("index.number_of_shards", 2) .put("index.number_of_replicas", 1))); ensureGreen(); final BulkRequest request1 = new BulkRequest(); for (int i = 0; i < 500; ++i) { request1.add(new IndexRequest(index).source(Collections.singletonMap("key", "value" + i))) .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); } BulkResponse bulkResponse = client().bulk(request1).get(); assertThat(bulkResponse.hasFailures(), equalTo(false)); assertThat(bulkResponse.getItems().length, equalTo(500)); for (BulkItemResponse bulkItemResponse : bulkResponse) { assertThat(bulkItemResponse.getIndex(), equalTo(index)); } IndicesStatsResponse stats = client().admin().indices().prepareStats(index).setBulk(true).get(); assertThat(stats.getTotal().bulk.getTotalOperations(), equalTo(4L)); assertThat(stats.getTotal().bulk.getTotalTimeInMillis(), greaterThan(0L)); assertThat(stats.getTotal().bulk.getTotalSizeInBytes(), greaterThan(0L)); assertThat(stats.getPrimaries().bulk.getTotalOperations(), equalTo(2L)); assertThat(stats.getPrimaries().bulk.getTotalTimeInMillis(), greaterThan(0L)); assertThat(stats.getPrimaries().bulk.getTotalSizeInBytes(), greaterThan(0L)); } /** * Test that we can safely concurrently index and get stats. This test was inspired by a serialization issue that arose due to a race * getting doc stats during heavy indexing. The race could lead to deleted docs being negative which would then be serialized as a * variable-length long. Since serialization of negative longs using a variable-length format was unsupported * ({@link org.elasticsearch.common.io.stream.StreamOutput#writeVLong(long)}	i don't think we need to index 500 documents here? we could probably get away with a smaller amount like 20, so that the test doesn't take as long? was this added to try and ensure that the total time was greater than 0?
public static ByteSizeValue calculateMaxModelMemoryLimitToFit(ClusterSettings clusterSettings, DiscoveryNodes nodes) { long maxMlMemory = -1; int numMlNodes = 0; for (DiscoveryNode node : nodes) { OptionalLong limit = allowedBytesForMl(node, clusterSettings); if (limit.isEmpty()) { continue; } maxMlMemory = Math.max(maxMlMemory, limit.getAsLong()); ++numMlNodes; } // It is possible that there is scope for more ML nodes to be added // to the cluster, in which case take those into account too long maxMlNodeSize = clusterSettings.get(MAX_ML_NODE_SIZE).getBytes(); int maxLazyNodes = clusterSettings.get(MAX_LAZY_ML_NODES); if (maxMlNodeSize > 0 && numMlNodes < maxLazyNodes) { maxMlMemory = Math.max( maxMlMemory, allowedBytesForMl( maxMlNodeSize, clusterSettings.get(MAX_MACHINE_MEMORY_PERCENT), clusterSettings.get(USE_AUTO_MACHINE_MEMORY_PERCENT) ) ); } if (maxMlMemory <= 0) { // This implies there are currently no ML nodes in the cluster, and // no automatic mechanism for adding one, so we have no idea what // the effective limit would be if one were added return null; } maxMlMemory -= Math.max(Job.PROCESS_MEMORY_OVERHEAD.getBytes(), DataFrameAnalyticsConfig.PROCESS_MEMORY_OVERHEAD.getBytes()); maxMlMemory -= MachineLearning.NATIVE_EXECUTABLE_CODE_OVERHEAD.getBytes(); return ByteSizeValue.ofMb(ByteSizeUnit.BYTES.toMB(Math.max(0L, maxMlMemory))); }	i initially thought we should filter these on ones where job allocation is enabled, then i saw we are taking advantage of the side-effect of not setting memory limits on the node unless it has the ml role. this seems possibly unsafe. it would be nice to filter by role here to be explicit and pass in the list of nodes. what do you think?
@Override public boolean equals(Object obj) { // TODO: Do this if/when we can assume scripts are pure functions // and they have a reliable equals impl /*if (this == obj) return true; if (sameClassAs(obj) == false) return false; ScriptQuery other = (ScriptQuery) obj; return Objects.equals(script, other.script);*/ return false; }	i guess you have to return this == obj to ensure queryutils dont' go wild?
@Override public void trimUnreferencedTranslogFiles() { try (ReleasableLock lock = readLock.acquire()) { ensureOpen(); final Map<String, String> commitUserData = getLastCommittedSegmentInfos().getUserData(); final String translogUuid = commitUserData.get(Translog.TRANSLOG_UUID_KEY); if (translogUuid == null) { throw new IllegalStateException("commit doesn't contain translog unique id"); } if (commitUserData.containsKey(Translog.TRANSLOG_GENERATION_KEY) == false) { throw new IllegalStateException("commit doesn't contain translog generation id"); } final long lastCommitGeneration = Long.parseLong(commitUserData.get(Translog.TRANSLOG_GENERATION_KEY)); final TranslogConfig translogConfig = engineConfig.getTranslogConfig(); final long minTranslogGeneration = Translog.readMinTranslogGeneration(translogConfig.getTranslogPath(), translogUuid); if (minTranslogGeneration < lastCommitGeneration) { // a translog deletion policy that retains nothing but the last translog generation from safe commit final TranslogDeletionPolicy translogDeletionPolicy = new TranslogDeletionPolicy(-1, -1); translogDeletionPolicy.setTranslogGenerationOfLastCommit(lastCommitGeneration); translogDeletionPolicy.setMinTranslogGenerationForRecovery(lastCommitGeneration); try (Translog translog = new Translog(translogConfig, translogUuid, translogDeletionPolicy, engineConfig.getGlobalCheckpointSupplier(), engineConfig.getPrimaryTermSupplier())) { translog.trimUnreferencedReaders(); } } } catch (final Exception e) { try { failEngine("translog trimming failed", e); } catch (Exception inner) { e.addSuppressed(inner); } throw new EngineException(shardId, "failed to trim translog", e); } }	should we trim translog only if we have a single index commit? we do not trim [unsafe commits](https://github.com/elastic/elasticsearch/pull/41041) when opening readonlyengine and we [might not](https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/index/engine/readonlyengine.java#l142) verify the condition max_seq_no = global_checkpoint if the index was created before 7.2.0. it's a bit unsafe to advance the min_translog_generation_for_recovery if we have more than one commit. wdyt?
public void testAsyncTranslogTrimTaskOnClosedIndex() throws Exception { final String indexName = "test"; IndexService indexService = createIndex(indexName, Settings.builder() .put(TRANSLOG_RETENTION_CHECK_INTERVAL_SETTING.getKey(), "100ms") .build()); final Translog translog = IndexShardTestCase.getTranslog(indexService.getShard(0)); final int numDocs = scaledRandomIntBetween(10, 100); for (int i = 0; i < numDocs; i++) { client().prepareIndex().setIndex(indexName).setId(String.valueOf(i)).setSource("{\\\\"foo\\\\": \\\\"bar\\\\"}", XContentType.JSON).get(); if (randomBoolean()) { client().admin().indices().prepareFlush(indexName).get(); } } assertAcked(client().admin().indices().prepareClose("test")); indexService = getInstanceFromNode(IndicesService.class).indexServiceSafe(indexService.index()); assertTrue(indexService.getTrimTranslogTask().mustReschedule()); final long lastCommitedTranslogGeneration; try (Engine.IndexCommitRef indexCommitRef = getEngine(indexService.getShard(0)).acquireLastIndexCommit(false)) { Map<String, String> lastCommittedUserData = indexCommitRef.getIndexCommit().getUserData(); lastCommitedTranslogGeneration = Long.parseLong(lastCommittedUserData.get(Translog.TRANSLOG_GENERATION_KEY)); } assertBusy(() -> { long minTranslogGen = Translog.readMinTranslogGeneration(translog.getConfig().getTranslogPath(), translog.getTranslogUUID()); assertThat(minTranslogGen, equalTo(lastCommitedTranslogGeneration)); }); }	can we reopen the index and verify that there's no operation in translogstats?
Client filterWithHeader(Map<String, String> headers); /** * Returns a client to a remote cluster with the given cluster alias. * This method is optinoal and might throw {@link UnsupportedOperationException}	oh one thing s/optinoal/optional :p
public synchronized PrimaryContext startRelocationHandoff(String targetAllocationId) { assert invariant(); assert primaryMode; assert handoffInProgress == false; assert pendingInSync.isEmpty() : "relocation handoff started while there are still shard copies pending in-sync: " + pendingInSync; if (checkpoints.containsKey(targetAllocationId) == false) { // can happen if the relocation target was removed from cluster but the recovery process isn't aware of that. throw new IllegalStateException("relocation target [" + targetAllocationId + "] is no longer part of the group"); } handoffInProgress = true; // copy clusterStateVersion and checkpoints and return // all the entries from checkpoints that are inSync: the reason we don't need to care about initializing non-insync entries // is that they will have to undergo a recovery attempt on the relocation target, and will hence be supplied by the cluster state // update on the relocation target once relocation completes). We could alternatively also copy the map as-is (its safe), and it // would be cleaned up on the target by cluster state updates. Map<String, CheckpointState> localCheckpointsCopy = new HashMap<>(); for (Map.Entry<String, CheckpointState> entry : checkpoints.entrySet()) { localCheckpointsCopy.put(entry.getKey(), entry.getValue().copy()); } assert invariant(); return new PrimaryContext(appliedClusterStateVersion, localCheckpointsCopy, routingTable); }	part of the *replication* group
* @param consumer a {@link Runnable} that is executed after operations are blocked * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation * @throws IllegalStateException if the relocation target is no longer part of the group * @throws InterruptedException if blocking operations is interrupted */ public void relocated(final String targetAllocationId, final Consumer<ReplicationTracker.PrimaryContext> consumer) throws IllegalIndexShardStateException, IllegalStateException, InterruptedException { assert shardRouting.primary() : "only primaries can be marked as relocated: " + shardRouting; final Releasable forceRefreshes = refreshListeners.forceRefreshes(); try { indexShardOperationPermits.blockOperations(30, TimeUnit.MINUTES, () -> { forceRefreshes.close(); // no shard operation permits are being held here, move state from started to relocated assert indexShardOperationPermits.getActiveOperationsCount() == OPERATIONS_BLOCKED : "in-flight operations in progress while moving shard state to relocated"; /* * We should not invoke the runnable under the mutex as the expected implementation is to handoff the primary context via a * network operation. Doing this under the mutex can implicitly block the cluster state update thread on network operations. */ verifyRelocatingState(); final ReplicationTracker.PrimaryContext primaryContext = replicationTracker.startRelocationHandoff(targetAllocationId); try { consumer.accept(primaryContext); synchronized (mutex) { verifyRelocatingState(); replicationTracker.completeRelocationHandoff(); // make changes to primaryMode and relocated flag only under mutex } } catch (final Exception e) { try { replicationTracker.abortRelocationHandoff(); } catch (final Exception inner) { e.addSuppressed(inner); } throw e; } }); } catch (TimeoutException e) { logger.warn("timed out waiting for relocation hand-off to complete"); // This is really bad as ongoing replication operations are preventing this shard from completing relocation hand-off. // Fail primary relocation source and target shards. failShard("timed out waiting for relocation hand-off to complete", null); throw new IndexShardClosedException(shardId(), "timed out waiting for relocation hand-off to complete"); } finally { forceRefreshes.close(); } }	of the replication group
private void updateWriterOnOpen() throws IOException { Objects.requireNonNull(historyUUID); final Map<String, String> commitUserData = commitDataAsMap(indexWriter); boolean needsCommit = false; if (historyUUID.equals(commitUserData.get(HISTORY_UUID_KEY)) == false) { needsCommit = true; } else { assert config().getForceNewHistoryUUID() == false : "config forced a new history uuid but it didn't change"; assert openMode != EngineConfig.OpenMode.CREATE_INDEX_AND_TRANSLOG : "new index but it already has an existing history uuid"; } if (translog.getTranslogUUID().equals(commitUserData.get(Translog.TRANSLOG_UUID_KEY)) == false) { needsCommit = true; } else { assert openMode == EngineConfig.OpenMode.OPEN_INDEX_AND_TRANSLOG : "translog uuid didn't change but open mode is " + openMode; } if (needsCommit) { commitIndexWriter(indexWriter, translog, openMode == EngineConfig.OpenMode.OPEN_INDEX_CREATE_TRANSLOG ? commitUserData.get(SYNC_COMMIT_ID) : null); } }	commitindexwriter still has a check if (historyuuid != null). is that one obsolete now?
private EngineConfig newEngineConfig(EngineConfig.OpenMode openMode) { Sort indexSort = indexSortSupplier.get(); final boolean forceNewHistoryUUID; switch (shardRouting.recoverySource().getType()) { case EMPTY_STORE: case EXISTING_STORE: case PEER: forceNewHistoryUUID = false; break; case SNAPSHOT: case LOCAL_SHARDS: forceNewHistoryUUID = true; break; default: throw new AssertionError("unknown recovery type: [" + shardRouting.recoverySource().getType() + "]"); } return new EngineConfig(openMode, shardId, shardRouting.allocationId().getId(), threadPool, indexSettings, warmer, store, indexSettings.getMergePolicy(), mapperService.indexAnalyzer(), similarityService.similarity(mapperService), codecService, shardEventListener, indexCache.query(), cachingPolicy, forceNewHistoryUUID, translogConfig, IndexingMemoryController.SHARD_INACTIVE_TIME_SETTING.get(indexSettings.getSettings()), Arrays.asList(refreshListeners, new RefreshMetricUpdater(refreshMetric)), indexSort, this::runTranslogRecovery); }	empty_store should force a new history uuid ([see also my comment here](https://github.com/elastic/elasticsearch/pull/26577#discussion_r138639785)). empty_store is used for two situations: - creating a shard of a fresh index: here, we trivially want to force a fresh uuid. - force allocating an empty shard: here, we are creating a fresh history, so we want to force a fresh uuid too. note that this leaves the case of allocating a stale primary, which should also force a fresh history. here, we have no way at the moment though to detect that based on the recovery source object. for this, if the recovery source is existing_store, we can compare the on-disk allocation id (shardstatemetadata.format.load(...)) with the expected allocation id in the shard routing object, and if they mismatch, force a new history id. loading the shardstatemetadata can happen in the indexshard constructor. i'm ok if you do this in a follow-up, but i still think empty_store should be correctly set here to force a new history uuid.
@Override public int hashCode() { return System.identityHashCode(this); } /** * Merges the default trust configuration with the provided {@link TrustConfig} * @param trustConfig the trust configuration to merge with * @return a {@link TrustConfig}	can you add the parameter to the javadocs and include that this password is for the default jdk store?
*/ private KeyStore getSystemTrustStore() throws KeyStoreException, CertificateException, NoSuchAlgorithmException, IOException { if (System.getProperty("javax.net.ssl.trustStoreType", "").equalsIgnoreCase("PKCS11")) { KeyStore keyStore = KeyStore.getInstance("PKCS11"); if (trustStorePassword.length() == 0) { trustStorePassword = new SecureString(System.getProperty("javax.net.ssl.trustStorePassword", "").toCharArray()); } keyStore.load(null, trustStorePassword.getChars()); return keyStore; } return null; }	just to be paranoid add a null check?
static Request deleteRepository(DeleteRepositoryRequest deleteRepositoryRequest) { String endpoint = new EndpointBuilder().addPathPartAsIs("_snapshot").addPathPartAsIs(deleteRepositoryRequest.name()).build(); Request request = new Request(HttpDelete.METHOD_NAME, endpoint); Params parameters = new Params(request); parameters.withMasterTimeout(deleteRepositoryRequest.masterNodeTimeout()); parameters.withTimeout(deleteRepositoryRequest.timeout()); return request; }	the second part should use addpathpart and go through escaping, given that it's user provided.
public void testDeleteRepository() { Map<String, String> expectedParams = new HashMap<>(); String repoName = "test"; StringBuilder endpoint = new StringBuilder("/_snapshot/" + repoName); DeleteRepositoryRequest deleteRepositoryRequest = new DeleteRepositoryRequest(); deleteRepositoryRequest.name(repoName); setRandomMasterTimeout(deleteRepositoryRequest, expectedParams); setRandomTimeout(deleteRepositoryRequest::timeout, AcknowledgedRequest.DEFAULT_ACK_TIMEOUT, expectedParams); Request request = RequestConverters.deleteRepository(deleteRepositoryRequest); assertThat(endpoint.toString(), equalTo(request.getEndpoint())); assertThat(HttpDelete.METHOD_NAME, equalTo(request.getMethod())); assertThat(expectedParams, equalTo(request.getParameters())); }	shall we assert that the body is null?
static DeprecationIssue checkShardLimit(ClusterState state) { int shardsPerNode = MetaData.SETTING_CLUSTER_MAX_SHARDS_PER_NODE.get(state.metaData().settings()); int nodeCount = state.getNodes().getDataNodes().size(); int maxShardsInCluster = shardsPerNode * nodeCount; int currentOpenShards = state.getMetaData().getTotalOpenIndexShards(); if (nodeCount != 0 && currentOpenShards >= maxShardsInCluster) { return new DeprecationIssue(DeprecationIssue.Level.WARNING, "Number of open shards exceeds cluster soft limit", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_70_cluster_changes.html", "There are [" + currentOpenShards + "] open shards in this cluster, but the cluster is limited to [" + shardsPerNode + "] per data node, for [" + maxShardsInCluster + "] maximum."); } return null; }	this is a fix for bug that was revealed by the test for the new cluster-level deprecation check, but i sincerely doubt it would ever be hit in practice and i didn't think it was worth its own pr.
static DeprecationIssue indexNameCheck(IndexMetaData indexMetaData) { String clusterName = indexMetaData.getIndex().getName(); if (clusterName.contains(":")) { return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, "Index name cannot contain ':'", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-7.0.html" + "#_literal_literal_is_no_longer_allowed_in_index_name", "This index is named [" + clusterName + "], which contains the illegal character ':'."); } return null; }	is there guidance on what is critical vs. warning ?
* @param results the search phase results to obtain the sort docs from * @param bufferedTopDocs the pre-consumed buffered top docs * @param topDocsStats the top docs stats to fill * @param from the offset into the search results top docs * @param size the number of hits to return from the merged top docs */ public SortedTopDocs sortDocs(boolean ignoreFrom, Collection<? extends SearchPhaseResult> results, final Collection<TopDocs> bufferedTopDocs, final TopDocsStats topDocsStats, int from, int size) { if (results.isEmpty()) { return SortedTopDocs.EMPTY; } final Collection<TopDocs> topDocs = bufferedTopDocs == null ? new ArrayList<>() : bufferedTopDocs; final Map<String, List<Suggestion<CompletionSuggestion.Entry>>> groupedCompletionSuggestions = new HashMap<>(); for (SearchPhaseResult sortedResult : results) { // TODO we can move this loop into the reduce call to only loop over this once /* We loop over all results once, group together the completion suggestions if there are any and collect relevant * top docs results. Each top docs gets it's shard index set on all top docs to simplify top docs merging down the road * this allowed to remove a single shared optimization code here since now we don't materialized a dense array of * top docs anymore but instead only pass relevant results / top docs to the merge method*/ QuerySearchResult queryResult = sortedResult.queryResult(); if (queryResult.hasConsumedTopDocs() == false) { // already consumed? final TopDocs td = queryResult.consumeTopDocs(); assert td != null; topDocsStats.add(td); if (td.scoreDocs.length > 0) { // make sure we set the shard index before we add it - the consumer didn't do that yet setShardIndex(td, queryResult.getShardIndex()); topDocs.add(td); } } if (queryResult.hasSuggestHits()) { Suggest shardSuggest = queryResult.suggest(); for (CompletionSuggestion suggestion : shardSuggest.filter(CompletionSuggestion.class)) { suggestion.setShardIndex(sortedResult.getShardIndex()); List<Suggestion<CompletionSuggestion.Entry>> suggestions = groupedCompletionSuggestions.computeIfAbsent(suggestion.getName(), s -> new ArrayList<>()); suggestions.add(suggestion); } } } final boolean hasNoHits = groupedCompletionSuggestions.isEmpty() && topDocs.isEmpty(); if (hasNoHits == false) { final TopDocs mergedTopDocs = mergeTopDocs(topDocs, size, ignoreFrom ? 0 : from); final ScoreDoc[] mergedScoreDocs = mergedTopDocs == null ? EMPTY_DOCS : mergedTopDocs.scoreDocs; ScoreDoc[] scoreDocs = mergedScoreDocs; if (groupedCompletionSuggestions.isEmpty() == false) { int numSuggestDocs = 0; List<Suggestion<? extends Entry<? extends Entry.Option>>> completionSuggestions = new ArrayList<>(groupedCompletionSuggestions.size()); for (List<Suggestion<CompletionSuggestion.Entry>> groupedSuggestions : groupedCompletionSuggestions.values()) { final CompletionSuggestion completionSuggestion = CompletionSuggestion.reduceTo(groupedSuggestions); assert completionSuggestion != null; numSuggestDocs += completionSuggestion.getOptions().size(); completionSuggestions.add(completionSuggestion); } scoreDocs = new ScoreDoc[mergedScoreDocs.length + numSuggestDocs]; System.arraycopy(mergedScoreDocs, 0, scoreDocs, 0, mergedScoreDocs.length); int offset = mergedScoreDocs.length; Suggest suggestions = new Suggest(completionSuggestions); for (CompletionSuggestion completionSuggestion : suggestions.filter(CompletionSuggestion.class)) { for (CompletionSuggestion.Entry.Option option : completionSuggestion.getOptions()) { scoreDocs[offset++] = option.getDoc(); } } } final boolean isSorted; final SortField[] sortFields; if (mergedTopDocs != null && mergedTopDocs instanceof TopFieldDocs) { TopFieldDocs fieldDocs = (TopFieldDocs) mergedTopDocs; isSorted = (fieldDocs instanceof CollapseTopFieldDocs && fieldDocs.fields.length == 1 && fieldDocs.fields[0].getType() == SortField.Type.SCORE) == false; sortFields = fieldDocs.fields; } else { isSorted = false; sortFields = null; } return new SortedTopDocs(scoreDocs, isSorted, sortFields); } else { // no relevant docs return SortedTopDocs.EMPTY; } }	can we avoid the double negation that makes things a bit harder to read by calling the var hashits?
* @param results the search phase results to obtain the sort docs from * @param bufferedTopDocs the pre-consumed buffered top docs * @param topDocsStats the top docs stats to fill * @param from the offset into the search results top docs * @param size the number of hits to return from the merged top docs */ public SortedTopDocs sortDocs(boolean ignoreFrom, Collection<? extends SearchPhaseResult> results, final Collection<TopDocs> bufferedTopDocs, final TopDocsStats topDocsStats, int from, int size) { if (results.isEmpty()) { return SortedTopDocs.EMPTY; } final Collection<TopDocs> topDocs = bufferedTopDocs == null ? new ArrayList<>() : bufferedTopDocs; final Map<String, List<Suggestion<CompletionSuggestion.Entry>>> groupedCompletionSuggestions = new HashMap<>(); for (SearchPhaseResult sortedResult : results) { // TODO we can move this loop into the reduce call to only loop over this once /* We loop over all results once, group together the completion suggestions if there are any and collect relevant * top docs results. Each top docs gets it's shard index set on all top docs to simplify top docs merging down the road * this allowed to remove a single shared optimization code here since now we don't materialized a dense array of * top docs anymore but instead only pass relevant results / top docs to the merge method*/ QuerySearchResult queryResult = sortedResult.queryResult(); if (queryResult.hasConsumedTopDocs() == false) { // already consumed? final TopDocs td = queryResult.consumeTopDocs(); assert td != null; topDocsStats.add(td); if (td.scoreDocs.length > 0) { // make sure we set the shard index before we add it - the consumer didn't do that yet setShardIndex(td, queryResult.getShardIndex()); topDocs.add(td); } } if (queryResult.hasSuggestHits()) { Suggest shardSuggest = queryResult.suggest(); for (CompletionSuggestion suggestion : shardSuggest.filter(CompletionSuggestion.class)) { suggestion.setShardIndex(sortedResult.getShardIndex()); List<Suggestion<CompletionSuggestion.Entry>> suggestions = groupedCompletionSuggestions.computeIfAbsent(suggestion.getName(), s -> new ArrayList<>()); suggestions.add(suggestion); } } } final boolean hasNoHits = groupedCompletionSuggestions.isEmpty() && topDocs.isEmpty(); if (hasNoHits == false) { final TopDocs mergedTopDocs = mergeTopDocs(topDocs, size, ignoreFrom ? 0 : from); final ScoreDoc[] mergedScoreDocs = mergedTopDocs == null ? EMPTY_DOCS : mergedTopDocs.scoreDocs; ScoreDoc[] scoreDocs = mergedScoreDocs; if (groupedCompletionSuggestions.isEmpty() == false) { int numSuggestDocs = 0; List<Suggestion<? extends Entry<? extends Entry.Option>>> completionSuggestions = new ArrayList<>(groupedCompletionSuggestions.size()); for (List<Suggestion<CompletionSuggestion.Entry>> groupedSuggestions : groupedCompletionSuggestions.values()) { final CompletionSuggestion completionSuggestion = CompletionSuggestion.reduceTo(groupedSuggestions); assert completionSuggestion != null; numSuggestDocs += completionSuggestion.getOptions().size(); completionSuggestions.add(completionSuggestion); } scoreDocs = new ScoreDoc[mergedScoreDocs.length + numSuggestDocs]; System.arraycopy(mergedScoreDocs, 0, scoreDocs, 0, mergedScoreDocs.length); int offset = mergedScoreDocs.length; Suggest suggestions = new Suggest(completionSuggestions); for (CompletionSuggestion completionSuggestion : suggestions.filter(CompletionSuggestion.class)) { for (CompletionSuggestion.Entry.Option option : completionSuggestion.getOptions()) { scoreDocs[offset++] = option.getDoc(); } } } final boolean isSorted; final SortField[] sortFields; if (mergedTopDocs != null && mergedTopDocs instanceof TopFieldDocs) { TopFieldDocs fieldDocs = (TopFieldDocs) mergedTopDocs; isSorted = (fieldDocs instanceof CollapseTopFieldDocs && fieldDocs.fields.length == 1 && fieldDocs.fields[0].getType() == SortField.Type.SCORE) == false; sortFields = fieldDocs.fields; } else { isSorted = false; sortFields = null; } return new SortedTopDocs(scoreDocs, isSorted, sortFields); } else { // no relevant docs return SortedTopDocs.EMPTY; } }	out of curiosity, why was it an issue to return empty_docs?
* @param bufferedTopDocs a list of pre-collected / buffered top docs. if this list is non-null all top docs have been consumed * from all non-null query results. * @param numReducePhases the number of non-final reduce phases applied to the query results. * @see QuerySearchResult#consumeAggs() * @see QuerySearchResult#consumeProfileResult() */ private ReducedQueryPhase reducedQueryPhase(Collection<? extends SearchPhaseResult> queryResults, List<InternalAggregations> bufferedAggs, List<TopDocs> bufferedTopDocs, TopDocsStats topDocsStats, int numReducePhases, boolean isScrollRequest) { assert numReducePhases >= 0 : "num reduce phases must be >= 0 but was: " + numReducePhases; numReducePhases++; // increment for this phase boolean timedOut = false; Boolean terminatedEarly = null; if (queryResults.isEmpty()) { // early terminate we have nothing to reduce return new ReducedQueryPhase(topDocsStats.totalHits, topDocsStats.fetchHits, topDocsStats.maxScore, timedOut, terminatedEarly, null, null, null, EMPTY_DOCS, null, null, numReducePhases, false, 0, 0, true); } final QuerySearchResult firstResult = queryResults.stream().findFirst().get().queryResult(); final boolean hasSuggest = firstResult.suggest() != null; final boolean hasProfileResults = firstResult.hasProfileResults(); final boolean consumeAggs; final List<InternalAggregations> aggregationsList; if (bufferedAggs != null) { consumeAggs = false; // we already have results from intermediate reduces and just need to perform the final reduce assert firstResult.hasAggs() : "firstResult has no aggs but we got non null buffered aggs?"; aggregationsList = bufferedAggs; } else if (firstResult.hasAggs()) { // the number of shards was less than the buffer size so we reduce agg results directly aggregationsList = new ArrayList<>(queryResults.size()); consumeAggs = true; } else { // no aggregations aggregationsList = Collections.emptyList(); consumeAggs = false; } // count the total (we use the query result provider here, since we might not get any hits (we scrolled past them)) final Map<String, List<Suggestion>> groupedSuggestions = hasSuggest ? new HashMap<>() : Collections.emptyMap(); final Map<String, ProfileShardResult> profileResults = hasProfileResults ? new HashMap<>(queryResults.size()) : Collections.emptyMap(); int from = 0; int size = 0; for (SearchPhaseResult entry : queryResults) { QuerySearchResult result = entry.queryResult(); from = result.from(); size = result.size(); if (result.searchTimedOut()) { timedOut = true; } if (result.terminatedEarly() != null) { if (terminatedEarly == null) { terminatedEarly = result.terminatedEarly(); } else if (result.terminatedEarly()) { terminatedEarly = true; } } if (hasSuggest) { assert result.suggest() != null; for (Suggestion<? extends Suggestion.Entry<? extends Suggestion.Entry.Option>> suggestion : result.suggest()) { List<Suggestion> suggestionList = groupedSuggestions.computeIfAbsent(suggestion.getName(), s -> new ArrayList<>()); suggestionList.add(suggestion); } } if (consumeAggs) { aggregationsList.add((InternalAggregations) result.consumeAggs()); } if (hasProfileResults) { String key = result.getSearchShardTarget().toString(); profileResults.put(key, result.consumeProfileResult()); } } final Suggest suggest = groupedSuggestions.isEmpty() ? null : new Suggest(Suggest.reduce(groupedSuggestions)); ReduceContext reduceContext = new ReduceContext(bigArrays, scriptService, true); final InternalAggregations aggregations = aggregationsList.isEmpty() ? null : reduceAggs(aggregationsList, firstResult.pipelineAggregators(), reduceContext); final SearchProfileShardResults shardResults = profileResults.isEmpty() ? null : new SearchProfileShardResults(profileResults); final SortedTopDocs scoreDocs = this.sortDocs(isScrollRequest, queryResults, bufferedTopDocs, topDocsStats, from, size); return new ReducedQueryPhase(topDocsStats.totalHits, topDocsStats.fetchHits, topDocsStats.maxScore, timedOut, terminatedEarly, suggest, aggregations, shardResults, scoreDocs.scoreDocs, scoreDocs.sortFields, firstResult != null ? firstResult.sortValueFormats() : null, numReducePhases, scoreDocs.sorted, size, from, firstResult == null); } /** * Performs an intermediate reduce phase on the aggregations. For instance with this reduce phase never prune information * that relevant for the final reduce step. For final reduce see {@link #reduceAggs(List, List, ReduceContext)}	can you fix the indentation here? all parameters do not seem to start on the same column
int getNumReducePhases() { return numReducePhases; } } /** * Returns a new SearchPhaseResults instance. This might return an instance that reduces search responses incrementally. */ InitialSearchPhase.SearchPhaseResults<SearchPhaseResult> newSearchPhaseResults(SearchRequest request, int numShards) { SearchSourceBuilder source = request.source(); boolean isScrollRequest = request.scroll() != null; final boolean hasAggs = source != null && source.aggregations() != null; final boolean hasTopDocs = source == null || source.size() != 0; if (isScrollRequest == false && (hasAggs || hasTopDocs)) { // no incremental reduce if scroll is used - we only hit a single shard or sometimes more... if (request.getBatchedReduceSize() < numShards) { // only use this if there are aggs and if there are more shards than we should reduce at once return new QueryPhaseResultConsumer(this, numShards, request.getBatchedReduceSize(), hasTopDocs, hasAggs); } } return new InitialSearchPhase.SearchPhaseResults(numShards) { @Override public ReducedQueryPhase reduce() { return reducedQueryPhase(results.asList(), isScrollRequest); } }; } static final class TopDocsStats { long totalHits; long fetchHits; float maxScore = Float.NEGATIVE_INFINITY; void add(TopDocs topDocs) { totalHits += topDocs.totalHits; fetchHits += topDocs.scoreDocs.length; if (!Float.isNaN(topDocs.getMaxScore())) { maxScore = Math.max(maxScore, topDocs.getMaxScore()); } } } static class SortedTopDocs { static final SortedTopDocs EMPTY = new SortedTopDocs(EMPTY_DOCS, false, null); final ScoreDoc[] scoreDocs; final boolean sorted; final SortField[] sortFields; SortedTopDocs(ScoreDoc[] scoreDocs, boolean sorted, SortField[] sortFields) { this.scoreDocs = scoreDocs; this.sorted = sorted; this.sortFields = sortFields; }	is it what is called issortedbyfield elsewhere?
*/ public final OpenPointInTimeResponse openPointInTime(OpenPointInTimeRequest openRequest, RequestOptions options) throws IOException { return performRequestAndParseEntity(openRequest, RequestConverters::openPointInTime, options, OpenPointInTimeResponse::fromXContent, emptySet()); } /** * Asynchronously open a point in time before using it in search requests * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/point-in-time-api.html"> Point in time API </a> * @param openRequest the open request * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	i was thinking that open and delete pit apis should return almost instantly, thus is there a need for openpointintimeasync and closepointintimeasync ? or, am i wrong, and there are cases when these apis may take some time?
private static void setRandomSearchParams(SearchRequest searchRequest, Map<String, String> expectedParams) { expectedParams.put(RestSearchAction.TYPED_KEYS_PARAM, "true"); if (randomBoolean()) { searchRequest.routing(randomAlphaOfLengthBetween(3, 10)); expectedParams.put("routing", searchRequest.routing()); } if (randomBoolean()) { searchRequest.preference(randomAlphaOfLengthBetween(3, 10)); expectedParams.put("preference", searchRequest.preference()); } if (randomBoolean()) { searchRequest.searchType(randomFrom(SearchType.CURRENTLY_SUPPORTED)); } expectedParams.put("search_type", searchRequest.searchType().name().toLowerCase(Locale.ROOT)); if (randomBoolean()) { searchRequest.requestCache(randomBoolean()); expectedParams.put("request_cache", Boolean.toString(searchRequest.requestCache())); } if (randomBoolean()) { searchRequest.allowPartialSearchResults(randomBoolean()); expectedParams.put("allow_partial_search_results", Boolean.toString(searchRequest.allowPartialSearchResults())); } if (randomBoolean()) { searchRequest.setBatchedReduceSize(randomIntBetween(2, Integer.MAX_VALUE)); } expectedParams.put("batched_reduce_size", Integer.toString(searchRequest.getBatchedReduceSize())); if (randomBoolean()) { searchRequest.scroll(randomTimeValue()); expectedParams.put("scroll", searchRequest.scroll().keepAlive().getStringRep()); } if (randomBoolean()) { boolean ccsMinimizeRoundtrips = randomBoolean(); searchRequest.setCcsMinimizeRoundtrips(ccsMinimizeRoundtrips); if (ccsMinimizeRoundtrips == false) { expectedParams.put("ccs_minimize_roundtrips", "false"); } } if (searchRequest.isCcsMinimizeRoundtrips() == false) { expectedParams.put("ccs_minimize_roundtrips", "false"); } if (randomBoolean()) { searchRequest.setMaxConcurrentShardRequests(randomIntBetween(1, Integer.MAX_VALUE)); } expectedParams.put("max_concurrent_shard_requests", Integer.toString(searchRequest.getMaxConcurrentShardRequests())); if (randomBoolean()) { searchRequest.setPreFilterShardSize(randomIntBetween(2, Integer.MAX_VALUE)); } if (searchRequest.getPreFilterShardSize() != null) { expectedParams.put("pre_filter_shard_size", Integer.toString(searchRequest.getPreFilterShardSize())); } }	isn't this already covered in lines 1981-83?
@Override public boolean equals(Object obj) { return super.equals(obj) && Objects.equals(term, ((Bucket) obj).term); }	this probably needs the traditional class comparison to make sure the cast doesn't blow up.
public void testLargeMapping() throws Exception { Request doc1 = new Request(HttpPut.METHOD_NAME, "/index/_doc/1"); String now = DateUtils.nowWithMillisResolution().format(DateTimeFormatter.ISO_DATE_TIME); StringBuilder sb = new StringBuilder(); sb.append("{"); for (int i = 0; i < 250; i++) { sb.append("\\\\"datetime" + i + "\\\\":\\\\"" + now + "\\\\""); sb.append(","); } sb.append("\\\\"event_type\\\\": \\\\"process\\\\","); sb.append("\\\\"serial_event_id\\\\": 1"); sb.append("}"); doc1.setJsonEntity(sb.toString()); client().performRequest(doc1); client().performRequest(new Request(HttpPost.METHOD_NAME, "/_refresh")); EqlClient eql = highLevelClient().eql(); EqlSearchRequest request = new EqlSearchRequest("index", "process where true"); EqlSearchResponse response = execute(request, eql::search, eql::searchAsync); assertNotNull(response); assertNotNull(response.hits()); assertThat(response.hits().events().size(), equalTo(1)); }	i would add a comment here explaining the reason for 250 value (that value larger than the 100 doc_values limit in es). maybe take the default limit in es and use here a value made of that limit + 100 or something like that?
public long nextScheduledTimeAfter(long startTime, long time) { assert time >= startTime; return Arrays.stream(crons) .map(cron -> cron.getNextValidTimeAfter(time)) // filter out expired dates before sorting .filter(nextValidTime -> nextValidTime > -1) .sorted() // no date in the future found, return -1 to the caller .findFirst().orElse(-1L); }	nit: would you put the orelse on its own line?
private void internalAddCompletionListener(ActionListener<AsyncSearchResponse> listener, TimeValue waitForCompletion) { boolean executeImmediately = false; synchronized (this) { if (hasCompleted || waitForCompletion.getMillis() == 0) { executeImmediately = true; } else { // ensure that we consumes the listener only once AtomicBoolean hasRun = new AtomicBoolean(false); long id = completionId++; final Cancellable cancellable; try { cancellable = threadPool.schedule( () -> { if (hasRun.compareAndSet(false, true)) { // timeout occurred before completion removeCompletionListener(id); getResponseWithHeaders(listener); } }, waitForCompletion, "generic"); } catch(Exception exc) { listener.onFailure(exc); return; } completionListeners.register( id, ActionListener.wrap( resp -> { if (hasRun.compareAndSet(false, true)) { // completion occurred before timeout cancellable.cancel(); listener.onResponse(resp); } }, listener::onFailure )); } } if (executeImmediately) { getResponseWithHeaders(listener); } }	we still need to cancel the cancellable on failure
private void executeCompletionListeners() { synchronized (this) { if (hasCompleted) { return; } hasCompleted = true; } // we don't need to restore the response headers, they should be included in the current // context since we are called by the search action listener. getResponse(completionListeners); //TODO is clearing the map necessary? we will only execute the listeners once anyways //completionListeners.clear(); } /** * Returns the current {@link AsyncSearchResponse}	should we copy the completion listeners in the synchronized block to avoid concurrent delete (unregister) ?
public CompiledScript compileInternal(Script script) { if (script == null) { throw new IllegalArgumentException("The parameter script (Script) must not be null."); } String lang = script.getLang(); if (lang == null) { lang = defaultLang; } if (logger.isTraceEnabled()) { logger.trace("Compiling lang: [{}] type: [{}] script: {}", lang, script.getType(), script.getScript()); } ScriptEngineService scriptEngineService = getScriptEngineServiceForLang(lang); CacheKey cacheKey = newCacheKey(scriptEngineService, script.getScript()); if (script.getType() == ScriptType.FILE) { CompiledScript compiled = staticCache.get(cacheKey); //On disk scripts will be loaded into the staticCache by the listener if (compiled == null) { throw new IllegalArgumentException("Unable to find on disk script " + script.getScript()); } return compiled; } // Name will remain null for inline scripts. String name = null; String code = script.getScript(); if (script.getType() == ScriptType.INDEXED) { final IndexedScript indexedScript = new IndexedScript(lang, script.getScript()); name = indexedScript.id; code = getScriptFromIndex(indexedScript.lang, indexedScript.id); cacheKey = newCacheKey(scriptEngineService, code); } CompiledScript compiled = cache.getIfPresent(cacheKey); if (compiled == null) { //Either an un-cached inline script or an indexed script try { compiled = new CompiledScript(script.getType(), name, lang, scriptEngineService.compile(code)); } catch (Exception e) { String message = "Compilation error with " + script.getType() + " script "; if (name != null) { message += "[" + name + "] "; } message += "using [" + lang + "]: " + ExceptionsHelper.detailedMessage(e); throw new ScriptException(message, e); } //Since the cache key is the script content itself we don't need to //invalidate/check the cache if an indexed script changes. cache.put(cacheKey, compiled); } // An indexed or inline script may be mistaken for the other since the cache key relies on // only the code of the script. Two indexed scripts may also have the same code, but different // names. If this is case, the name and type parameters must be modified accordingly. // Then cache the most recent version used. if (compiled.type() != script.getType() || name != null && !name.equals(compiled.name())) { compiled = new CompiledScript(script.getType(), name, compiled.lang(), compiled.compiled()); cache.put(cacheKey, compiled); } return compiled; }	i find this new code block confusing, can we maybe add the script type to the cache key instead or something along those lines?
public static BucketSortPipelineAggregationBuilder bucketSort(String name, List<FieldSortBuilder> sorts) { return new BucketSortPipelineAggregationBuilder(name, sorts); }	why is field sort builder specified, instead of a more flexible sort builder ?
public void testCancellationBeforeFieldCaps() throws InterruptedException { Client client = mock(Client.class); EqlSearchTask task = mock(EqlSearchTask.class); when(task.isCancelled()).thenReturn(true); ClusterService mockClusterService = mockClusterService(); IndexResolver indexResolver = new IndexResolver(client, randomAlphaOfLength(10), DefaultDataTypeRegistry.INSTANCE); PlanExecutor planExecutor = new PlanExecutor(client, indexResolver, new NamedWriteableRegistry(Collections.emptyList())); CountDownLatch countDownLatch = new CountDownLatch(1); TransportEqlSearchAction.operation(planExecutor, task, new EqlSearchRequest().query("foo where blah"), "", mock(TransportService.class), mockClusterService, new ActionListener<>() { @Override public void onResponse(EqlSearchResponse eqlSearchResponse) { fail("Shouldn't be here"); countDownLatch.countDown(); } @Override public void onFailure(Exception e) { assertThat(e, instanceOf(TaskCancelledException.class)); countDownLatch.countDown(); } }); countDownLatch.await(); verify(task, times(1)).isCancelled(); verify(task, times(1)).getId(); verify(client, times(1)).settings(); verify(client, times(1)).threadPool(); verifyNoMoreInteractions(client, task); }	minor: since mock(transportservice.class) is reused, you could also assign it to a var.
*/ private int updateShardStatuses(EnumSet<ShardStatusChangeType> changes, List<ShardId> activeToInactiveIndexingShards) { int activeShards = 0; for (ShardId shardId : availableShards()) { final ShardIndexingStatus currentStatus = getTranslogStatus(shardId); if (currentStatus == null) { // shard was closed.. continue; } final long nanoTime = currentTimeInNanos(); ShardIndexingStatus status = shardsIndicesStatus.get(shardId); if (status == null) { status = new ShardIndexingStatus(); shardsIndicesStatus.put(shardId, status); changes.add(ShardStatusChangeType.ADDED); } // consider shard inactive if it has same translogFileGeneration and no operations for a long time if (status.translogId == currentStatus.translogId && status.translogNumberOfOperations == currentStatus.translogNumberOfOperations) { if (status.nanoTime == -1) { // first time we noticed the shard become idle status.nanoTime = nanoTime; } // mark it as inactive only if enough time has passed if (status.activeIndexing && (nanoTime - status.nanoTime) > inactiveTime.nanos()) { // inactive for this amount of time, mark it activeToInactiveIndexingShards.add(shardId); status.activeIndexing = false; changes.add(ShardStatusChangeType.BECAME_INACTIVE); logger.debug("marking shard {} as inactive (inactive_time[{}]) indexing wise, setting size to [{}]", shardId, inactiveTime, EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER); } } else { // since we sync flush once a shard becomes inactive, the translog id can change, however that // doesn't mean the an indexing operation has happened. Not that if we're really unlucky and a flush happens // immediately after an indexing operation we may not become active immediately. The following // indexing operation will mark the shard as active, so it's OK. If that one doesn't come, we might as well stay // inactive if (!status.activeIndexing && currentStatus.translogNumberOfOperations > 0) { status.activeIndexing = true; changes.add(ShardStatusChangeType.BECAME_ACTIVE); logger.debug("marking shard {} as active indexing wise", shardId); } status.nanoTime = -1; } status.translogId = currentStatus.translogId; status.translogNumberOfOperations = currentStatus.translogNumberOfOperations; if (status.activeIndexing) { activeShards++; } } return activeShards; }	thank you for cutting over to a better clock :)
*/ private int updateShardStatuses(EnumSet<ShardStatusChangeType> changes, List<ShardId> activeToInactiveIndexingShards) { int activeShards = 0; for (ShardId shardId : availableShards()) { final ShardIndexingStatus currentStatus = getTranslogStatus(shardId); if (currentStatus == null) { // shard was closed.. continue; } final long nanoTime = currentTimeInNanos(); ShardIndexingStatus status = shardsIndicesStatus.get(shardId); if (status == null) { status = new ShardIndexingStatus(); shardsIndicesStatus.put(shardId, status); changes.add(ShardStatusChangeType.ADDED); } // consider shard inactive if it has same translogFileGeneration and no operations for a long time if (status.translogId == currentStatus.translogId && status.translogNumberOfOperations == currentStatus.translogNumberOfOperations) { if (status.nanoTime == -1) { // first time we noticed the shard become idle status.nanoTime = nanoTime; } // mark it as inactive only if enough time has passed if (status.activeIndexing && (nanoTime - status.nanoTime) > inactiveTime.nanos()) { // inactive for this amount of time, mark it activeToInactiveIndexingShards.add(shardId); status.activeIndexing = false; changes.add(ShardStatusChangeType.BECAME_INACTIVE); logger.debug("marking shard {} as inactive (inactive_time[{}]) indexing wise, setting size to [{}]", shardId, inactiveTime, EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER); } } else { // since we sync flush once a shard becomes inactive, the translog id can change, however that // doesn't mean the an indexing operation has happened. Not that if we're really unlucky and a flush happens // immediately after an indexing operation we may not become active immediately. The following // indexing operation will mark the shard as active, so it's OK. If that one doesn't come, we might as well stay // inactive if (!status.activeIndexing && currentStatus.translogNumberOfOperations > 0) { status.activeIndexing = true; changes.add(ShardStatusChangeType.BECAME_ACTIVE); logger.debug("marking shard {} as active indexing wise", shardId); } status.nanoTime = -1; } status.translogId = currentStatus.translogId; status.translogNumberOfOperations = currentStatus.translogNumberOfOperations; if (status.activeIndexing) { activeShards++; } } return activeShards; }	can we rename .nanotime to maybe .idlenanotime?
private void calcAndSetShardBuffers(int activeShards, String reason) { if (activeShards == 0) { logger.debug("no active shards (reason={})", reason); return; } ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShards); if (shardIndexingBufferSize.bytes() < minShardIndexBufferSize.bytes()) { shardIndexingBufferSize = minShardIndexBufferSize; } if (shardIndexingBufferSize.bytes() > maxShardIndexBufferSize.bytes()) { shardIndexingBufferSize = maxShardIndexBufferSize; } ByteSizeValue shardTranslogBufferSize = new ByteSizeValue(translogBuffer.bytes() / activeShards); if (shardTranslogBufferSize.bytes() < minShardTranslogBufferSize.bytes()) { shardTranslogBufferSize = minShardTranslogBufferSize; } if (shardTranslogBufferSize.bytes() > maxShardTranslogBufferSize.bytes()) { shardTranslogBufferSize = maxShardTranslogBufferSize; } logger.debug("recalculating shard indexing buffer (reason={}), total is [{}] with [{}] active shards, each shard set to indexing=[{}], translog=[{}]", reason, indexingBuffer, activeShards, shardIndexingBufferSize, shardTranslogBufferSize); for (ShardId shardId : availableShards()) { ShardIndexingStatus status = shardsIndicesStatus.get(shardId); if (status == null || status.activeIndexing) { updateShardBuffers(shardId, shardIndexingBufferSize, shardTranslogBufferSize); } } } } protected long currentTimeInNanos() { return System.nanoTime(); } // update inactive indexing buffer size protected void markShardAsInActive(ShardId shardId) { String ignoreReason = null; try { IndexShard shard = getShard(shardId); if (shard != null) { shard.markAsInactive(); } else { ignoreReason = "shard not found"; } } catch (EngineClosedException e) { // ignore ignoreReason = "EngineClosedException"; } catch (FlushNotAllowedEngineException e) { // ignore ignoreReason = "FlushNotAllowedEngineException"; } if (ignoreReason != null) { logger.trace("ignore [{}] while marking shard {} as inactive", ignoreReason, shardId); } } private static enum ShardStatusChangeType { ADDED, DELETED, BECAME_ACTIVE, BECAME_INACTIVE }	also shrink wrap just around this line?
public void testOverride() throws Exception { Map<String, Object> source = new HashMap<>(); Map<String, Object> inner = new HashMap<>(); inner.put("bar", "baz1"); inner.put("qux", "quux"); source.put("foo", inner); source.put("foo.bar", "baz2"); IngestDocument document = new IngestDocument(source, Map.of()); DotExpanderProcessor processor = new DotExpanderProcessor("_tag", null, null, "foo.bar", true); processor.execute(document); assertThat(document.getFieldValue("foo", Map.class).size(), equalTo(2)); assertThat(document.getFieldValue("foo.bar", String.class), equalTo("baz2")); assertThat(document.getFieldValue("foo.qux", String.class), equalTo("quux")); }	can you include a test here that shows that wildcard dot-expanding applies only to top-level fields?
void validate(String id, String scriptLang, BytesReference scriptBytes) { validateScriptSize(id, scriptBytes.length()); try (XContentParser parser = XContentFactory.xContent(scriptBytes).createParser(scriptBytes)) { parser.nextToken(); Template template = TemplateQueryBuilder.parse(scriptLang, parser, parseFieldMatcher, "params", "script", "template"); if (Strings.hasLength(template.getScript())) { //Just try and compile it try { ScriptEngineService scriptEngineService = getScriptEngineServiceForLang(scriptLang); //we don't know yet what the script will be used for, but if all of the operations for this lang with //indexed scripts are disabled, it makes no sense to even compile it. if (isAnyScriptContextEnabled(scriptLang, scriptEngineService, ScriptType.STORED)) { Object compiled = scriptEngineService.compile(id, template.getScript(), Collections.emptyMap()); if (compiled == null) { throw new IllegalArgumentException("Unable to parse [" + template.getScript() + "] lang [" + scriptLang + "] (ScriptService.compile returned null)"); } } else { logger.warn( "skipping compile of script [{}], lang [{}] as all scripted operations are disabled for indexed scripts", template.getScript(), scriptLang); } } catch (Exception e) { // TODO: remove this when all script engines have good exceptions! if (e instanceof ScriptException) { throw e; // its already good! } throw new IllegalArgumentException("Unable to parse [" + template.getScript() + "] lang [" + scriptLang + "]", e); } } else { throw new IllegalArgumentException("Unable to find script in : " + scriptBytes.toUtf8()); } } catch (IOException e) { throw new IllegalArgumentException("failed to parse template script", e); } }	nit pick why not: java } catch (scriptexception e) { throw e;// its already good! } catch (exception e) { ...
@Override void write(MethodWriter writer) { if (cat) { writer.writeDebugInfo(offset); } if (cat) { writer.writeNewStrings(); } ALink last = links.get(links.size() - 1); for (ALink link : links) { link.write(writer); if (link == last && link.store) { if (cat) { writer.writeDup(link.size, 1); link.load(writer); writer.writeAppendStrings(link.after); expression.write(writer); if (!(expression instanceof EBinary) || ((EBinary)expression).operation != Operation.ADD || expression.actual.sort != Sort.STRING) { writer.writeAppendStrings(expression.actual); } writer.writeToStrings(); writer.writeCast(back); if (link.load) { writer.writeDup(link.after.sort.size, link.size); } link.store(writer); } else if (operation != null) { writer.writeDup(link.size, 0); link.load(writer); if (link.load && post) { writer.writeDup(link.after.sort.size, link.size); } writer.writeCast(there); expression.write(writer); writer.writeBinaryInstruction(location, promote, operation); writer.writeCast(back); if (link.load && !post) { writer.writeDup(link.after.sort.size, link.size); } link.store(writer); } else { expression.write(writer); if (link.load) { writer.writeDup(link.after.sort.size, link.size); } link.store(writer); } } else { link.load(writer); } } writer.writeBranch(tru, fals); }	is there a reason these two aren't combined?
@Override public final String toString(String f) { StringBuilder buffer = new StringBuilder(); if (field.equals(f) == false) { buffer.append(field); buffer.append(":"); } buffer.append("\\\\""); Iterator<Term[]> i = termArrays.iterator(); while (i.hasNext()) { Term[] terms = i.next(); if (terms.length > 1) { buffer.append("("); for (int j = 0; j < terms.length; j++) { buffer.append(terms[j].text()); if (j < terms.length - 1) { if (i.hasNext()) { buffer.append(" "); } else { buffer.append("* "); } } } if (i.hasNext()) { buffer.append(") "); } else { buffer.append("*)"); } } else { buffer.append(terms[0].text()); if (i.hasNext()) { buffer.append(" "); } else { buffer.append("*"); } } } buffer.append("\\\\""); if (slop != 0) { buffer.append("~"); buffer.append(slop); } return buffer.toString(); }	are we safe to remove the null check here?
public void testFastPhrasePrefixes() throws IOException { QueryShardContext queryShardContext = indexService.newQueryShardContext( randomInt(20), null, () -> { throw new UnsupportedOperationException(); }, null); String mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties") .startObject("field") .field("type", "text") .field("analyzer", "my_stop_analyzer") .startObject("index_prefixes") .field("min_chars", 2) .field("max_chars", 10) .endObject() .endObject() .startObject("synfield") .field("type", "text") .field("analyzer", "standard") // will be replaced with MockSynonymAnalyzer .field("index_phrases", true) .startObject("index_prefixes") .field("min_chars", 2) .field("max_chars", 10) .endObject() .endObject() .endObject() .endObject().endObject()); queryShardContext.getMapperService().merge("type", new CompressedXContent(mapping), MergeReason.MAPPING_UPDATE); Query q = new MatchPhrasePrefixQueryBuilder("field", "two words").toQuery(queryShardContext); Query expected = new SpanNearQuery.Builder("field", true) .addClause(new SpanTermQuery(new Term("field", "two"))) .addClause(new FieldMaskingSpanQuery( new SpanTermQuery(new Term("field._index_prefix", "words")), "field") ) .build(); assertThat(q, equalTo(expected)); Query q2 = new MatchPhrasePrefixQueryBuilder("field", "three words here").toQuery(queryShardContext); expected = new SpanNearQuery.Builder("field", true) .addClause(new SpanTermQuery(new Term("field", "three"))) .addClause(new SpanTermQuery(new Term("field", "words"))) .addClause(new FieldMaskingSpanQuery( new SpanTermQuery(new Term("field._index_prefix", "here")), "field") ) .build(); assertThat(q2, equalTo(expected)); Query q3 = new MatchPhrasePrefixQueryBuilder("field", "two words").slop(1).toQuery(queryShardContext); expected = new SpanNearQuery.Builder("field", true) .setSlop(1) .addClause(new SpanTermQuery(new Term("field", "two"))) .addClause(new FieldMaskingSpanQuery( new SpanTermQuery(new Term("field._index_prefix", "words")), "field") ) .build(); assertThat(q3, equalTo(expected)); Query q4 = new MatchPhrasePrefixQueryBuilder("field", "singleton").toQuery(queryShardContext); assertThat(q4, is(new SynonymQuery(new Term("field._index_prefix", "singleton")))); Query q5 = new MatchPhrasePrefixQueryBuilder("field", "sparkle a stopword").toQuery(queryShardContext); expected = new SpanNearQuery.Builder("field", true) .addClause(new SpanTermQuery(new Term("field", "sparkle"))) .addGap(1) .addClause(new FieldMaskingSpanQuery( new SpanTermQuery(new Term("field._index_prefix", "stopword")), "field") ) .build(); assertThat(q5, equalTo(expected)); MatchQuery matchQuery = new MatchQuery(queryShardContext); matchQuery.setAnalyzer(new MockSynonymAnalyzer()); Query q6 = matchQuery.parse(MatchQuery.Type.PHRASE_PREFIX, "synfield", "motor dogs"); expected = new SpanNearQuery.Builder("synfield", true) .addClause(new SpanTermQuery(new Term("synfield", "motor"))) .addClause( new SpanOrQuery( new FieldMaskingSpanQuery( new SpanTermQuery(new Term("synfield._index_prefix", "dogs")), "synfield" ), new FieldMaskingSpanQuery( new SpanTermQuery(new Term("synfield._index_prefix", "dog")), "synfield" ) ) ) .build(); assertThat(q6, equalTo(expected)); Query q7 = matchQuery.parse(MatchQuery.Type.PHRASE_PREFIX, "field", "motor d"); MultiPhrasePrefixQuery mpq = new MultiPhrasePrefixQuery("field"); mpq.add(new Term("field", "motor")); mpq.add(new Term("field", "d")); assertThat(q7, equalTo(mpq)); }	does this change the semantics of slop of a match phrase prefix? slop on a phrase query includes swapped positions, while slop on a span near only includes gaps.
public void testFastPhrasePrefixes() throws IOException { QueryShardContext queryShardContext = indexService.newQueryShardContext( randomInt(20), null, () -> { throw new UnsupportedOperationException(); }, null); String mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties") .startObject("field") .field("type", "text") .field("analyzer", "my_stop_analyzer") .startObject("index_prefixes") .field("min_chars", 2) .field("max_chars", 10) .endObject() .endObject() .startObject("synfield") .field("type", "text") .field("analyzer", "standard") // will be replaced with MockSynonymAnalyzer .field("index_phrases", true) .startObject("index_prefixes") .field("min_chars", 2) .field("max_chars", 10) .endObject() .endObject() .endObject() .endObject().endObject()); queryShardContext.getMapperService().merge("type", new CompressedXContent(mapping), MergeReason.MAPPING_UPDATE); Query q = new MatchPhrasePrefixQueryBuilder("field", "two words").toQuery(queryShardContext); Query expected = new SpanNearQuery.Builder("field", true) .addClause(new SpanTermQuery(new Term("field", "two"))) .addClause(new FieldMaskingSpanQuery( new SpanTermQuery(new Term("field._index_prefix", "words")), "field") ) .build(); assertThat(q, equalTo(expected)); Query q2 = new MatchPhrasePrefixQueryBuilder("field", "three words here").toQuery(queryShardContext); expected = new SpanNearQuery.Builder("field", true) .addClause(new SpanTermQuery(new Term("field", "three"))) .addClause(new SpanTermQuery(new Term("field", "words"))) .addClause(new FieldMaskingSpanQuery( new SpanTermQuery(new Term("field._index_prefix", "here")), "field") ) .build(); assertThat(q2, equalTo(expected)); Query q3 = new MatchPhrasePrefixQueryBuilder("field", "two words").slop(1).toQuery(queryShardContext); expected = new SpanNearQuery.Builder("field", true) .setSlop(1) .addClause(new SpanTermQuery(new Term("field", "two"))) .addClause(new FieldMaskingSpanQuery( new SpanTermQuery(new Term("field._index_prefix", "words")), "field") ) .build(); assertThat(q3, equalTo(expected)); Query q4 = new MatchPhrasePrefixQueryBuilder("field", "singleton").toQuery(queryShardContext); assertThat(q4, is(new SynonymQuery(new Term("field._index_prefix", "singleton")))); Query q5 = new MatchPhrasePrefixQueryBuilder("field", "sparkle a stopword").toQuery(queryShardContext); expected = new SpanNearQuery.Builder("field", true) .addClause(new SpanTermQuery(new Term("field", "sparkle"))) .addGap(1) .addClause(new FieldMaskingSpanQuery( new SpanTermQuery(new Term("field._index_prefix", "stopword")), "field") ) .build(); assertThat(q5, equalTo(expected)); MatchQuery matchQuery = new MatchQuery(queryShardContext); matchQuery.setAnalyzer(new MockSynonymAnalyzer()); Query q6 = matchQuery.parse(MatchQuery.Type.PHRASE_PREFIX, "synfield", "motor dogs"); expected = new SpanNearQuery.Builder("synfield", true) .addClause(new SpanTermQuery(new Term("synfield", "motor"))) .addClause( new SpanOrQuery( new FieldMaskingSpanQuery( new SpanTermQuery(new Term("synfield._index_prefix", "dogs")), "synfield" ), new FieldMaskingSpanQuery( new SpanTermQuery(new Term("synfield._index_prefix", "dog")), "synfield" ) ) ) .build(); assertThat(q6, equalTo(expected)); Query q7 = matchQuery.parse(MatchQuery.Type.PHRASE_PREFIX, "field", "motor d"); MultiPhrasePrefixQuery mpq = new MultiPhrasePrefixQuery("field"); mpq.add(new Term("field", "motor")); mpq.add(new Term("field", "d")); assertThat(q7, equalTo(mpq)); }	might be interesting in future to use the wildcard expansion from #36703 here as well?
public void testMatchPhrasePrefixWithBoost() throws Exception { QueryShardContext context = createShardContext(); { // field boost is ignored on a single term query MatchPhrasePrefixQueryBuilder builder = new MatchPhrasePrefixQueryBuilder("string_boost", "foo"); Query query = builder.toQuery(context); assertThat(query, instanceOf(MultiPhrasePrefixQuery.class)); } { // field boost is ignored on phrase query MatchPhrasePrefixQueryBuilder builder = new MatchPhrasePrefixQueryBuilder("string_boost", "foo bar"); Query query = builder.toQuery(context); assertThat(query, instanceOf(MultiPhrasePrefixQuery.class)); } }	why has this changed?
private void tryLockingPolicy(String policyName) { Semaphore runLock = policyLocks.computeIfAbsent(policyName, (name) -> new Semaphore(1)); if (runLock.tryAcquire() == false) { throw new EsRejectedExecutionException("Policy execution failed. Policy execution for [" + policyName + "] is already in progress."); } if (policyExecutionPermits.tryAcquire() == false) { // Release policy lock, and throw a different exception policyLocks.remove(policyName); throw new EsRejectedExecutionException("Policy execution failed. Exceeded maximum concurrent policy executions [" + maximumConcurrentPolicyExecutions + "]"); } }	should we [block for a bit](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/semaphore.html#tryacquire-long-java.util.concurrent.timeunit-) here waiting on permit ? maybe 5 minutes ?
private void tryLockingPolicy(String policyName) { Semaphore runLock = policyLocks.computeIfAbsent(policyName, (name) -> new Semaphore(1)); if (runLock.tryAcquire() == false) { throw new EsRejectedExecutionException("Policy execution failed. Policy execution for [" + policyName + "] is already in progress."); } if (policyExecutionPermits.tryAcquire() == false) { // Release policy lock, and throw a different exception policyLocks.remove(policyName); throw new EsRejectedExecutionException("Policy execution failed. Exceeded maximum concurrent policy executions [" + maximumConcurrentPolicyExecutions + "]"); } }	can you name the policy that failed to execute in this error ?
* @throws ActionRequestValidationException when there are validation failures. */ public Builder validate() { // We require a definition to be available here even though it will be stored in a different doc ActionRequestValidationException validationException = null; if (definition == null) { validationException = addValidationError("[" + DEFINITION.getPreferredName() + "] must not be null.", validationException); } if (modelId == null) { validationException = addValidationError("[" + MODEL_ID.getPreferredName() + "] must not be null.", validationException); } if (modelId != null && MlStrings.isValidId(modelId) == false) { validationException = addValidationError(Messages.getMessage(Messages.INVALID_ID, TrainedModelConfig.MODEL_ID.getPreferredName(), modelId), validationException); } if (modelId != null && MlStrings.hasValidLengthForId(modelId) == false) { validationException = addValidationError(Messages.getMessage(Messages.ID_TOO_LONG, TrainedModelConfig.MODEL_ID.getPreferredName(), modelId, MlStrings.ID_LENGTH_LIMIT), validationException); } List<String> badTags = tags.stream() .filter(tag -> (MlStrings.isValidId(tag) && MlStrings.hasValidLengthForId(tag)) == false) .collect(Collectors.toList()); if (badTags.isEmpty() == false) { validationException = addValidationError(Messages.getMessage(Messages.INFERENCE_INVALID_TAGS, badTags, MlStrings.ID_LENGTH_LIMIT), validationException); } for(String tag : tags) { if (tag.equals(modelId)) { validationException = addValidationError("none of the tags must equal the model_id", validationException); break; } } validationException = checkIllegalSetting(version, VERSION.getPreferredName(), validationException); validationException = checkIllegalSetting(createdBy, CREATED_BY.getPreferredName(), validationException); validationException = checkIllegalSetting(createTime, CREATE_TIME.getPreferredName(), validationException); validationException = checkIllegalSetting(estimatedHeapMemory, ESTIMATED_HEAP_MEMORY_USAGE_BYTES.getPreferredName(), validationException); validationException = checkIllegalSetting(estimatedOperations, ESTIMATED_OPERATIONS.getPreferredName(), validationException); validationException = checkIllegalSetting(licenseLevel, LICENSE_LEVEL.getPreferredName(), validationException); if (validationException != null) { throw validationException; } return this; }	validate() can only be called once on put as the builder will set these fields so if called on a built config it will error. maybe add a iscreatetime flag or similar. putjobaction has the same issue and does the validation of create time settings [in the action](https://github.com/elastic/elasticsearch/blob/b4bc6ac08fcf38e40880ca3d8b04fbb43d395b84/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/action/putjobaction.java#l62)
static List<String> determineJobIdsWithoutLiveStats(List<String> requestedJobIds, List<GetJobsStatsAction.Response.JobStats> stats) { Set<String> excludeJobIds = stats.stream().map(GetJobsStatsAction.Response.JobStats::getJobId).collect(Collectors.toSet()); return requestedJobIds.stream().filter(jobId -> !excludeJobIds.contains(jobId)).collect(Collectors.toList()); }	this is the tricky bit as it is no longer easy to find if the job is being deleted without making an async call for each job. before this change if a job was being deleted then the calls to gatherforecaststats and gatherdatacountsandmodelsizestats would not be made, it is still safe to make those calls if those documents are deleted the response accepts null for forecast stats and model size stats and gather data counts returns a default constructed object if the document is not found. some jobs will take a long time to delete and the job config document is the last thing to be removed so it is possible that forecast stats are in the process of deletion meaning that in a long running delete subsequent calls. it's a reasonable argument to say the api is doing what is says is it doing in this case. compare with get jobs with returns all jobs including deleting jobs. one helpful change is to add an excludedeleting parameter to jobconfigprovider.expandjobsids when called in doexecute which would make the behaviour closer to the current albeit with race.
@SuppressForbidden(reason = "accesses fully qualified URLs to configure security") static Policy readPolicy(URL policyFile, Set<URL> codebases) { try { List<String> propertiesSet = new ArrayList<>(); try { // set codebase properties for (URL url : codebases) { String shortName = PathUtils.get(url.toURI()).getFileName().toString(); if (shortName.endsWith(".jar") == false) { continue; // tests :( } String property = "codebase." + shortName; if (shortName.startsWith("elasticsearch-rest-client")) { final String esVersion = Version.CURRENT + (Build.CURRENT.isSnapshot() ? "-SNAPSHOT" : ""); final int index = property.indexOf("-" + esVersion + ".jar"); assert index >= 0; String restClientAlias = property.substring(0, index); propertiesSet.add(restClientAlias); System.setProperty(restClientAlias, url.toString()); } propertiesSet.add(property); String previous = System.setProperty(property, url.toString()); if (previous != null) { throw new IllegalStateException("codebase property already set: " + shortName + "->" + previous); } } return Policy.getInstance("JavaPolicy", new URIParameter(policyFile.toURI())); } finally { // clear codebase properties for (String property : propertiesSet) { System.out.println("Unsetting " + property); System.clearProperty(property); } } } catch (NoSuchAlgorithmException | URISyntaxException e) { throw new IllegalArgumentException("unable to parse policy file `" + policyFile + "`", e); } }	can you put in a message for this?
@SuppressForbidden(reason = "accesses fully qualified URLs to configure security") static Policy readPolicy(URL policyFile, Set<URL> codebases) { try { List<String> propertiesSet = new ArrayList<>(); try { // set codebase properties for (URL url : codebases) { String shortName = PathUtils.get(url.toURI()).getFileName().toString(); if (shortName.endsWith(".jar") == false) { continue; // tests :( } String property = "codebase." + shortName; if (shortName.startsWith("elasticsearch-rest-client")) { final String esVersion = Version.CURRENT + (Build.CURRENT.isSnapshot() ? "-SNAPSHOT" : ""); final int index = property.indexOf("-" + esVersion + ".jar"); assert index >= 0; String restClientAlias = property.substring(0, index); propertiesSet.add(restClientAlias); System.setProperty(restClientAlias, url.toString()); } propertiesSet.add(property); String previous = System.setProperty(property, url.toString()); if (previous != null) { throw new IllegalStateException("codebase property already set: " + shortName + "->" + previous); } } return Policy.getInstance("JavaPolicy", new URIParameter(policyFile.toURI())); } finally { // clear codebase properties for (String property : propertiesSet) { System.out.println("Unsetting " + property); System.clearProperty(property); } } } catch (NoSuchAlgorithmException | URISyntaxException e) { throw new IllegalArgumentException("unable to parse policy file `" + policyFile + "`", e); } }	this is a leftover?
public synchronized IndexShard createShard(int sShardId, boolean primary) throws ElasticsearchException { /* * TODO: we execute this in parallel but it's a synced method. Yet, we might * be able to serialize the execution via the cluster state in the future. for now we just * keep it synced. */ if (closed.get()) { throw new ElasticsearchIllegalStateException("Can't create shard [" + index.name() + "][" + sShardId + "], closed"); } final ShardId shardId = new ShardId(index, sShardId); ShardLock lock = null; boolean success = false; Injector shardInjector = null; try { ShardPath path = ShardPath.loadShardPath(logger, nodeEnv, shardId, indexSettings); if (path == null) { path = ShardPath.selectNewPathForShard(nodeEnv, shardId, indexSettings); logger.debug("{} creating using a new path [{}]", shardId, path); } else { logger.debug("{} creating using an existing path [{}]", shardId, path); } lock = nodeEnv.shardLock(shardId, TimeUnit.SECONDS.toMillis(5)); if (shards.containsKey(shardId.id())) { throw new IndexShardAlreadyExistsException(shardId + " already exists"); } indicesLifecycle.beforeIndexShardCreated(shardId, indexSettings); logger.debug("creating shard_id {}", shardId); // if we are on a shared FS we only own the shard (ie. we can safely delete it) if we are the primary. final boolean canDeleteShardContent = IndexMetaData.isOnSharedFilesystem(indexSettings) == false || (primary && IndexMetaData.isOnSharedFilesystem(indexSettings)); ModulesBuilder modules = new ModulesBuilder(); modules.add(new ShardsPluginsModule(indexSettings, pluginsService)); modules.add(new IndexShardModule(shardId, primary, indexSettings)); modules.add(new ShardIndexingModule()); modules.add(new ShardSearchModule()); modules.add(new ShardGetModule()); modules.add(new StoreModule(indexSettings, injector.getInstance(IndexStore.class).shardDirectory(), lock, new StoreCloseListener(shardId, canDeleteShardContent), path)); modules.add(new DeletionPolicyModule(indexSettings)); modules.add(new MergePolicyModule(indexSettings)); modules.add(new MergeSchedulerModule(indexSettings)); modules.add(new ShardFilterCacheModule()); modules.add(new ShardQueryCacheModule()); modules.add(new ShardBitsetFilterCacheModule()); modules.add(new ShardFieldDataModule()); modules.add(new TranslogModule(indexSettings)); modules.add(new IndexShardGatewayModule()); modules.add(new PercolatorShardModule()); modules.add(new ShardTermVectorsModule()); modules.add(new IndexShardSnapshotModule()); modules.add(new SuggestShardModule()); try { shardInjector = modules.createChildInjector(injector); } catch (CreationException e) { throw new IndexShardCreationException(shardId, Injectors.getFirstErrorFailure(e)); } catch (Throwable e) { throw new IndexShardCreationException(shardId, e); } IndexShard indexShard = shardInjector.getInstance(IndexShard.class); indicesLifecycle.indexShardStateChanged(indexShard, null, "shard created"); indicesLifecycle.afterIndexShardCreated(indexShard); shards = newMapBuilder(shards).put(shardId.id(), new Tuple<>(indexShard, shardInjector)).immutableMap(); success = true; return indexShard; } catch (IOException ex) { throw new IndexShardCreationException(shardId, ex); } finally { if (success == false) { IOUtils.closeWhileHandlingException(lock); if (shardInjector != null) { IndexShard indexShard = shardInjector.getInstance(IndexShard.class); closeShardInjector("initialization failed", shardId, shardInjector, indexShard); } } } }	this might end up throwing an elasticsearchillegalstateexception if there is an existing left over shard state but with the wrong uuid, causing the shard creation to fail if there is a local copy of an old shard (we used to safely ignore it). it's a minor issue and we agreed to open a follow up issue.
public static void parseMultiField(AbstractFieldMapper.Builder builder, String name, Map<String, Object> node, Mapper.TypeParser.ParserContext parserContext, String propName, Object propNode) { if (propName.equals("path")) { builder.multiFieldPathType(parsePathType(name, propNode.toString())); } else if (propName.equals("fields") && propNode instanceof Map) { @SuppressWarnings("unchecked") Map<String, Object> multiFieldsPropNodes = (Map<String, Object>) propNode; for (Map.Entry<String, Object> multiFieldEntry : multiFieldsPropNodes.entrySet()) { String multiFieldName = multiFieldEntry.getKey(); if (!(multiFieldEntry.getValue() instanceof Map)) { throw new MapperParsingException("Illegal field [" + multiFieldName + "], only fields can be specified inside fields"); } @SuppressWarnings("unchecked") Map<String, Object> multiFieldNodes = (Map<String, Object>) multiFieldEntry.getValue(); String type; Object typeNode = multiFieldNodes.get("type"); if (typeNode != null) { type = typeNode.toString(); } else { throw new MapperParsingException("No type specified for property [" + multiFieldName + "]"); } Mapper.TypeParser typeParser = parserContext.typeParser(type); if (typeParser == null) { throw new MapperParsingException("No handler for type [" + type + "] declared on field [" + multiFieldName + "]"); } builder.addMultiField(typeParser.parse(multiFieldName, multiFieldNodes, parserContext)); } } }	i think we should not just ignore when something else than a map is provided? maybe we could do something like: java } else if (propname.equals("fields") { final map<string, object> multifieldspropnodes; if (propnode instance of list && ((list<?>) propnode.isempty()) { multifieldspropnodes = collections.emptymap(); } else if (propnode instanceof map) { multifieldspropnodes = (map<string, object>) propnode; } else { throw new mapperparsingexception("expected map for property [fields] on field [" + multifieldname + "] or [" + type + "] but got a " + propnode.getclass()); } }
protected static void parseProperties(ObjectMapper.Builder objBuilder, Map<String, Object> propsNode, ParserContext parserContext) { for (Map.Entry<String, Object> entry : propsNode.entrySet()) { String propName = entry.getKey(); if(entry.getValue() instanceof Map) { Map<String, Object> propNode = (Map<String, Object>) entry.getValue(); String type; Object typeNode = propNode.get("type"); if (typeNode != null) { type = typeNode.toString(); } else { // lets see if we can derive this... if (propNode.get("properties") != null) { type = ObjectMapper.CONTENT_TYPE; } else if (propNode.size() == 1 && propNode.get("enabled") != null) { // if there is a single property with the enabled flag on it, make it an object // (usually, setting enabled to false to not index any type, including core values, which // non enabled object type supports). type = ObjectMapper.CONTENT_TYPE; } else { throw new MapperParsingException("No type specified for property [" + propName + "]"); } } Mapper.TypeParser typeParser = parserContext.typeParser(type); if (typeParser == null) { throw new MapperParsingException("No handler for type [" + type + "] declared on field [" + propName + "]"); } objBuilder.add(typeParser.parse(propName, propNode, parserContext)); } } }	i have a similar concern here for when entry.getvalue is not a map
private synchronized DocumentMapper internalMerge(DocumentMapper mapper, MergeReason reason) { boolean hasNested = this.hasNested; Map<String, ObjectMapper> fullPathObjectMappers = this.fullPathObjectMappers; FieldTypeLookup fieldTypes = this.fieldTypes; DocumentMapper newMapper = null; if (mapper != null) { // check naming validateTypeName(mapper.type()); // compute the merged DocumentMapper DocumentMapper oldMapper = this.mapper; if (oldMapper != null) { newMapper = oldMapper.merge(mapper.mapping()); } else { newMapper = mapper; } // check basic sanity of the new mapping List<ObjectMapper> objectMappers = new ArrayList<>(); List<FieldMapper> fieldMappers = new ArrayList<>(); List<FieldAliasMapper> fieldAliasMappers = new ArrayList<>(); MetadataFieldMapper[] metadataMappers = newMapper.mapping().metadataMappers; Collections.addAll(fieldMappers, metadataMappers); MapperUtils.collect(newMapper.mapping().root(), objectMappers, fieldMappers, fieldAliasMappers); MapperMergeValidator.validateNewMappers(objectMappers, fieldMappers, fieldAliasMappers, fieldTypes); checkPartitionedIndexConstraints(newMapper); // update lookup data-structures fieldTypes = fieldTypes.copyAndAddAll(newMapper.type(), fieldMappers, fieldAliasMappers); for (ObjectMapper objectMapper : objectMappers) { if (fullPathObjectMappers == this.fullPathObjectMappers) { // first time through the loops fullPathObjectMappers = new HashMap<>(this.fullPathObjectMappers); } fullPathObjectMappers.put(objectMapper.fullPath(), objectMapper); if (objectMapper.nested().isNested()) { hasNested = true; } } MapperMergeValidator.validateFieldReferences(fieldMappers, fieldAliasMappers, fullPathObjectMappers, fieldTypes); ContextMapping.validateContextPaths(indexSettings.getIndexVersionCreated(), fieldMappers, fieldTypes::get); if (reason == MergeReason.MAPPING_UPDATE || reason == MergeReason.MAPPING_UPDATE_PREFLIGHT) { // this check will only be performed on the master node when there is // a call to the update mapping API. For all other cases like // the master node restoring mappings from disk or data nodes // deserializing cluster state that was sent by the master node, // this check will be skipped. // Also, don't take metadata mappers into account for the field limit check checkTotalFieldsLimit(objectMappers.size() + fieldMappers.size() - metadataMappers.length + fieldAliasMappers.size() ); checkFieldNameSoftLimit(objectMappers, fieldMappers, fieldAliasMappers); } } if (reason == MergeReason.MAPPING_UPDATE || reason == MergeReason.MAPPING_UPDATE_PREFLIGHT) { // this check will only be performed on the master node when there is // a call to the update mapping API. For all other cases like // the master node restoring mappings from disk or data nodes // deserializing cluster state that was sent by the master node, // this check will be skipped. checkNestedFieldsLimit(fullPathObjectMappers); checkDepthLimit(fullPathObjectMappers.keySet()); } checkIndexSortCompatibility(indexSettings.getIndexSortConfig(), hasNested); if (newMapper != null) { DocumentMapper updatedDocumentMapper = newMapper.updateFieldType(fieldTypes.fullNameToFieldType); if (updatedDocumentMapper != newMapper) { // update both mappers and result newMapper = updatedDocumentMapper; } } if (reason == MergeReason.MAPPING_UPDATE_PREFLIGHT) { return newMapper; } // only need to immutably rewrap these if the previous reference was changed. // if not then they are already implicitly immutable. if (fullPathObjectMappers != this.fullPathObjectMappers) { fullPathObjectMappers = Collections.unmodifiableMap(fullPathObjectMappers); } // commit the change if (newMapper != null) { this.mapper = newMapper; } this.fieldTypes = fieldTypes; this.hasNested = hasNested; this.fullPathObjectMappers = fullPathObjectMappers; assert assertMappersShareSameFieldType(); assert newMapper == null || assertSerialization(newMapper); return newMapper; }	aside: i think the way this is called by the other internalmapping method now, we should always get a non-null mapper argument. not entirely sure though, but maybe you can see if you agree and then simplify?
public void testApplyDataFromTemplate() throws Exception { addMatchingTemplate(builder -> builder .putAlias(AliasMetaData.builder("alias1")) .putMapping("mapping1", createMapping()) .settings(Settings.builder().put("key1", "value1")) ); final ClusterState result = executeTask(); assertThat(result.metaData().index("test").getAliases(), hasKey("alias1")); assertThat(result.metaData().index("test").getSettings().get("key1"), equalTo("value1")); assertThat(getMappingsFromResponse(), Matchers.hasKey("type")); }	the changes in this test probably have to do with the question i had above about the (temporary?) introduction of a type argument in the merge method? just trying to confirm, maybe you can explain briefly.
public Optional<Tuple<Version, String>> minRequiredClusterVersion() { return runtimeMappings.isEmpty() ? Optional.empty() : Optional.of(Tuple.tuple(RUNTIME_MAPPINGS_INTRODUCED, "runtime mappings")); } /** * Get the fully parsed query from the semi-parsed stored {@code Map<String, Object>}	should we replace "runtime_mappings" with searchsourcebuilder.runtime_mappings_field ?
@Override protected void doExecute(Task task, BulkRequest bulkRequest, ActionListener<BulkResponse> listener) { long indexingBytes = DocWriteRequest.writeSizeInBytes(bulkRequest.requests.stream()); final Releasable releasable = writeMemoryLimits.markCoordinatingOperationStarted(indexingBytes); final ActionListener<BulkResponse> releasingListener = ActionListener.runAfter(listener, releasable::close); doInternalExecute(task, bulkRequest, releasingListener); }	dointernalexecute throws exceptions, which would lead to "leaked" memory in the tracker.
protected void handleReplicaRequest(final ConcreteReplicaRequest<ReplicaRequest> replicaRequest, final TransportChannel channel, final Task task) { Releasable releasable = checkReplicaLimits(replicaRequest.getRequest()); ActionListener<ReplicaResponse> listener = ActionListener.runAfter(new ChannelActionListener<>(channel, transportReplicaAction, replicaRequest), releasable::close); try { new AsyncReplicaAction(replicaRequest, listener, (ReplicationTask) task).run(); } catch (RuntimeException e) { listener.onFailure(e); } }	nit: not that it matters a lot, but i think runbefore is more logical in that we release the memory before responding.
public static KeyStoreWrapper load(Path configDir) throws IOException { Path keystoreFile = keystorePath(configDir); if (Files.exists(keystoreFile) == false) { return null; } SimpleFSDirectory directory = new SimpleFSDirectory(configDir); try (IndexInput indexInput = directory.openInput(KEYSTORE_FILENAME, IOContext.READONCE)) { ChecksumIndexInput input = new BufferedChecksumIndexInput(indexInput); int formatVersion = CodecUtil.checkHeader(input, KEYSTORE_FILENAME, MIN_FORMAT_VERSION, FORMAT_VERSION); byte hasPasswordByte = input.readByte(); boolean hasPassword = hasPasswordByte == 1; if (hasPassword == false && hasPasswordByte != 0) { throw new IllegalStateException("hasPassword boolean is corrupt: " + String.format(Locale.ROOT, "%02x", hasPasswordByte)); } if (formatVersion <= 2) { String type = input.readString(); if (type.equals("PKCS12") == false) { throw new IllegalStateException("Corrupted legacy keystore string encryption algorithm"); } final String stringKeyAlgo = input.readString(); if (stringKeyAlgo.equals("PBE") == false) { throw new IllegalStateException("Corrupted legacy keystore string encryption algorithm"); } if (formatVersion == 2) { final String fileKeyAlgo = input.readString(); if (fileKeyAlgo.equals("PBE") == false) { throw new IllegalStateException("Corrupted legacy keystore file encryption algorithm"); } } } final byte[] dataBytes; if (formatVersion == 2) { // For v2 we had a map of strings containing the types for each setting. In v3 this map is now // part of the encrypted bytes. Unfortunately we cannot seek backwards with checksum input, so // we cannot just read the map and find out how long it is. So instead we read the map and // store it back using java's builtin DataOutput in a byte array, along with the actual keystore bytes Map<String, String> settingTypes = input.readMapOfStrings(); ByteArrayOutputStream bytes = new ByteArrayOutputStream(); try (DataOutputStream output = new DataOutputStream(bytes)) { output.writeInt(settingTypes.size()); for (Map.Entry<String, String> entry : settingTypes.entrySet()) { output.writeUTF(entry.getKey()); output.writeUTF(entry.getValue()); } int keystoreLen = input.readInt(); byte[] keystoreBytes = new byte[keystoreLen]; input.readBytes(keystoreBytes, 0, keystoreLen); output.write(keystoreBytes); } dataBytes = bytes.toByteArray(); } else { int dataBytesLen = input.readInt(); dataBytes = new byte[dataBytesLen]; input.readBytes(dataBytes, 0, dataBytesLen); } CodecUtil.checkFooter(input); return new KeyStoreWrapper(formatVersion, hasPassword, dataBytes); } }	this won't work if the jvm is in fips approved mode only as pbe is not available and secretkeyfactory.getinstance("pbe"); will throw an exception. i'm thinking of the case where someone wants to upgrade to 6.3.0 and at the same time enable fips approved only mode. we can highlight this and document manual steps that will ensure the keystore gets upgraded before fips approved mode is enabled.
private void checkFieldsWithCardinalityLimit() { for (Map.Entry<String, Long> entry : config.getAnalysis().getFieldCardinalityLimits().entrySet()) { String fieldName = entry.getKey(); long limit = entry.getValue(); long cardinality = fieldCardinalities.get(fieldName); if (cardinality > limit) { throw ExceptionsHelper.badRequestException( "Field [{}] must have at most [{}] distinct values but there were at least [{}]", fieldName, limit, cardinality); } } }	could this line cause npe if fieldname is not in the index?
private void checkResultsFieldIsNotPresent() { // If the task is restarting we do not mind the index containing the results field, we will overwrite all docs if (isTaskRestarting) { return; } String resultsField = config.getDest().getResultsField(); Map<String, FieldCapabilities> indexToFieldCaps = fieldCapabilitiesResponse.getField(resultsField); if (indexToFieldCaps != null && indexToFieldCaps.isEmpty() == false) { throw ExceptionsHelper.badRequestException( "A field that matches the {}.{} [{}] already exists; please set a different {}", DataFrameAnalyticsConfig.DEST.getPreferredName(), DataFrameAnalyticsDest.RESULTS_FIELD.getPreferredName(), resultsField, DataFrameAnalyticsDest.RESULTS_FIELD.getPreferredName()); } }	is it ok if the config.getdest() is null (this can be the case for memory estimation endpoint as we do not require providing dest).
public IoStats getIoStats() { return ioStats; }	super minor nit suggestion public path[] getpaths() {
public void merge(long bucket, AbstractLinearCounting other) { if (precision() != other.precision()) { throw new IllegalArgumentException(); } hll.ensureCapacity(bucket + 1); hll.bucket = bucket; lc.bucket = bucket; final AbstractLinearCounting.HashesIterator values = other.values(); while (values.next()) { final int encoded = values.value(); if (algorithm.get(bucket) == LINEAR_COUNTING) { final int newSize = lc.addEncoded(encoded); if (newSize > lc.threshold) { upgradeToHll(bucket); } } else { hll.collectEncoded(encoded); } } }	missing exception message here?
@Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }, CARDINALITY() { @Override public ValuesSource getEmpty() { // TODO: Is this the correct exception type here? throw new IllegalArgumentException("Can't deal with unmapped CardinalityValuesSource type " + this.value()); } @Override public ValuesSource getScript(AggregationScript.LeafFactory script, ValueType scriptValueType) { throw new AggregationExecutionException("value source of type [" + this.value() + "] is not supported by scripts"); } @Override public ValuesSource getField(FieldContext fieldContext, AggregationScript.LeafFactory script) { final IndexFieldData<?> indexFieldData = fieldContext.indexFieldData(); if (!(indexFieldData instanceof IndexHllFieldData)) { throw new IllegalArgumentException("Expected cardinality type on field [" + fieldContext.field() + "], but got [" + fieldContext.fieldType().typeName() + "]"); } return new HllValuesSource.HllSketch.Fielddata((IndexHllFieldData) indexFieldData); } @Override public ValuesSource replaceMissing(ValuesSource valuesSource, Object rawMissing, DocValueFormat docValueFormat, LongSupplier now) { throw new IllegalArgumentException("Can't apply missing values on a " + valuesSource.getClass()); } }; public static ValuesSourceType fromString(String name) { return valueOf(name.trim().toUpperCase(Locale.ROOT)); } public String value() { return name().toLowerCase(Locale.ROOT); }	style nit: let's expand this out to == false
public void testIpFieldExecutionContext() throws IOException { ScriptService scriptService = getInstanceFromNode(ScriptService.class); IndexService indexService = createIndex("index", Settings.EMPTY, "doc", "test_ip", "type=ip"); Request.ContextSetup contextSetup = new Request.ContextSetup("index", new BytesArray("{\\\\"test_ip\\\\":\\\\"192.168.1.254\\\\"}"), new MatchAllQueryBuilder()); contextSetup.setXContentType(XContentType.JSON); Request request = new Request(new Script(ScriptType.INLINE, "painless", "emit(doc['test_ip'].value);", emptyMap()), "ip_field", contextSetup); Response response = innerShardOperation(request, scriptService, indexService); assertEquals(response.getResult(), Collections.singletonList("192.168.1.254")); contextSetup = new Request.ContextSetup("index", new BytesArray("{}"), new MatchAllQueryBuilder()); contextSetup.setXContentType(XContentType.JSON); request = new Request(new Script(ScriptType.INLINE, "painless", "emit(\\\\"192.168.0.1\\\\"); emit(\\\\"127.0.0.1\\\\"); emit(\\\\"255.255.255.255\\\\"); emit(\\\\"0.0.0.0\\\\");", emptyMap()), "ip_field", contextSetup); response = innerShardOperation(request, scriptService, indexService); assertEquals(response.getResult(), Arrays.asList("0.0.0.0", "127.0.0.1", "192.168.0.1", "255.255.255.255")); }	add an ipv6 in here please.
private void sendMessage(TcpChannel channel, OutboundMessage networkMessage, ActionListener<Void> listener) throws IOException { final BytesStreamOutput bytesStreamOutput = new ReleasableBytesStreamOutput(bigArrays); SendContext sendContext = new SendContext(channel, networkMessage, ActionListener.runBefore(listener, bytesStreamOutput::close)); final BytesReference message; try { message = networkMessage.serialize(bytesStreamOutput); } catch (Exception e) { sendContext.onFailure(e); throw e; } internalSend(channel, message, sendContext); }	can we do this before we even create the sendcontext? we haven't set the starttime by this point so the slow logging doesn't work, and logging send message failed is a bit weird on a serialization exception since it's probably a version incompatibility that caused the exception.
public void testSkipRefreshIfShardIsRefreshingAlready() throws Exception { SetOnce<CountDownLatch> refreshLatch = new SetOnce<>(); ReferenceManager.RefreshListener refreshListener = new ReferenceManager.RefreshListener() { @Override public void beforeRefresh() { if (refreshLatch.get() != null) { try { refreshLatch.get().await(); } catch (InterruptedException e) { throw new AssertionError(e); } } } @Override public void afterRefresh(boolean didRefresh) { } }; IndexShard shard = newStartedShard(randomBoolean(), Settings.EMPTY, config -> new InternalEngine(configWithRefreshListener(config, refreshListener))); refreshLatch.set(new CountDownLatch(1)); // block refresh final RefreshStats refreshStats = shard.refreshStats(); final IndexingMemoryController controller = new IndexingMemoryController( Settings.builder().put("indices.memory.interval", "200h") // disable it .put("indices.memory.index_buffer_size", "1024b").build(), threadPool, Collections.singleton(shard)) { @Override protected long getIndexBufferRAMBytesUsed(IndexShard shard) { return randomLongBetween(1025, 10 * 1024 * 1024); } @Override protected long getShardWritingBytes(IndexShard shard) { return 0L; } }; int iterations = randomIntBetween(10, 100); for (int i = 0; i < iterations; i++) { controller.forceCheck(); } assertBusy(() -> { for (ThreadPoolStats.Stats stats : threadPool.stats()) { if (stats.getName().equals(ThreadPool.Names.REFRESH)) { assertThat(stats.getQueue(), equalTo(0)); assertThat(stats.getActive(), equalTo(1)); } } }); refreshLatch.get().countDown(); // allow refresh assertBusy(() -> { for (ThreadPoolStats.Stats stats : threadPool.stats()) { if (stats.getName().equals(ThreadPool.Names.REFRESH)) { assertThat(stats.getActive(), equalTo(0)); assertThat(stats.getQueue(), equalTo(0)); } } }); assertThat(shard.refreshStats().getTotal(), equalTo(refreshStats.getTotal() + 1)); closeShards(shard); }	nit: i prefer to find the stats object to assert on first and then assert to ensure we get a npe if stats for some reason do not contain "refresh" rather than pass the test.
public ImmutableOpenMap<String, AliasMetaData> getAliases() { return this.aliases; } /** * Return an object that maps each type to the associated mappings. * The return value is never {@code null} but may be empty if the index * has no mappings. * @deprecated Use {@link #mapping()}	note that we should remove this one too in the future but decided to keep it for a follow-up pr as this pr would otherwise be much larger.
public void testXPackInfo() throws IOException { XPackInfoRequest request = new XPackInfoRequest(); request.setCategories(EnumSet.allOf(XPackInfoRequest.Category.class)); request.setVerbose(true); XPackInfoResponse info = highLevelClient().xPackInfo(request, RequestOptions.DEFAULT); MainResponse mainResponse = highLevelClient().info(RequestOptions.DEFAULT); assertEquals(mainResponse.getBuild().shortHash(), info.getBuildInfo().getHash()); assertEquals("basic", info.getLicenseInfo().getType()); assertEquals("basic", info.getLicenseInfo().getMode()); assertEquals(LicenseStatus.ACTIVE, info.getLicenseInfo().getStatus()); FeatureSet graph = info.getFeatureSetsInfo().getFeatureSets().get("graph"); assertEquals("Graph Data Exploration for the Elastic Stack", graph.description()); assertFalse(graph.available()); assertTrue(graph.enabled()); assertNull(graph.nativeCodeInfo()); FeatureSet monitoring = info.getFeatureSetsInfo().getFeatureSets().get("monitoring"); assertEquals("Monitoring for the Elastic Stack", monitoring.description()); assertTrue(monitoring.available()); assertTrue(monitoring.enabled()); assertNull(monitoring.nativeCodeInfo()); FeatureSet ml = info.getFeatureSetsInfo().getFeatureSets().get("ml"); assertEquals("Machine Learning for the Elastic Stack", ml.description()); assertFalse(ml.available()); assertTrue(ml.enabled()); assertEquals(mainResponse.getVersion().toString(), ml.nativeCodeInfo().get("version").toString().replace("-SNAPSHOT", "")); }	are we considering all of the description etc a contract that we need to validate does not change? the reason i ask is cuz maybe it does not make sense to test this much detail as to what the output of the strings are.. i get that we can easily check available/enabled, but id hate to see a test fail here cuz we changed the description (unless we view it as a contract)
ClusterState addComponentTemplate(final ClusterState currentState, final boolean create, final String name, final ComponentTemplate template) throws Exception { if (create && currentState.metadata().componentTemplates().containsKey(name)) { throw new IllegalArgumentException("component template [" + name + "] already exists"); } CompressedXContent mappings = template.template().mappings(); String stringMappings = mappings == null ? null : mappings.string(); // We may need to normalize index settings, so do that also Settings finalSettings = template.template().settings(); if (finalSettings != null) { finalSettings = Settings.builder() .put(finalSettings).normalizePrefix(IndexMetadata.INDEX_SETTING_PREFIX) .build(); } validateTemplate(finalSettings, stringMappings, indicesService, xContentRegistry); // if we're updating a component template, let's check if it's part of any V2 template that will yield the CT update invalid if (create == false && finalSettings != null) { // if the CT is specifying the `index.hidden` setting it cannot be part of any global template if (IndexMetadata.INDEX_HIDDEN_SETTING.exists(finalSettings)) { Map<String, IndexTemplateV2> existingTemplates = currentState.metadata().templatesV2(); List<String> globalTemplatesThatUseThisComponent = new ArrayList<>(); for (Map.Entry<String, IndexTemplateV2> entry : existingTemplates.entrySet()) { IndexTemplateV2 templateV2 = entry.getValue(); if (templateV2.composedOf().contains(name) && templateV2.indexPatterns().stream().anyMatch(Regex::isMatchAllPattern)) { // global templates don't support configuring the `index.hidden` setting so we don't need to resolve the settings as // no other component template can remove this setting from the resolved settings, so just invalidate this update globalTemplatesThatUseThisComponent.add(entry.getKey()); } } if (globalTemplatesThatUseThisComponent.isEmpty() == false) { throw new IllegalArgumentException("cannot update component template [" + name + "] because the following global templates would resolve to specifying the [" + IndexMetadata.SETTING_INDEX_HIDDEN + "] setting: [" + String.join(",", globalTemplatesThatUseThisComponent) + "]"); } } } // Mappings in component templates don't include _doc, so update the mappings to include this single type if (stringMappings != null) { Map<String, Object> parsedMappings = MapperService.parseMapping(xContentRegistry, stringMappings); if (parsedMappings.size() > 0) { stringMappings = Strings.toString(XContentFactory.jsonBuilder() .startObject() .field(MapperService.SINGLE_MAPPING_NAME, parsedMappings) .endObject()); } } final Template finalTemplate = new Template(finalSettings, stringMappings == null ? null : new CompressedXContent(stringMappings), template.template().aliases()); final ComponentTemplate finalComponentTemplate = new ComponentTemplate(finalTemplate, template.version(), template.metadata()); logger.info("adding component template [{}]", name); return ClusterState.builder(currentState) .metadata(Metadata.builder(currentState.metadata()).put(name, finalComponentTemplate)) .build(); }	this can be replaced with stream version, which for me is more readable: java list<string> globaltemplatesthatusethiscomponent = currentstate.metadata().templatesv2().entryset().stream() .filter(e -> e.getvalue().composedof().contains(name)) .filter(e -> e.getvalue().indexpatterns().stream().anymatch(regex::ismatchallpattern)) .map(map.entry::getkey) .collect(collectors.tolist());
public synchronized IndexShard createShard(int sShardId, boolean primary) { /* * TODO: we execute this in parallel but it's a synced method. Yet, we might * be able to serialize the execution via the cluster state in the future. for now we just * keep it synced. */ if (closed.get()) { throw new IllegalStateException("Can't create shard [" + index.name() + "][" + sShardId + "], closed"); } final ShardId shardId = new ShardId(index, sShardId); ShardLock lock = null; boolean success = false; Injector shardInjector = null; try { ShardPath path = ShardPath.loadShardPath(logger, nodeEnv, shardId, indexSettings); if (path == null) { long totFreeSpace = 0; for (NodeEnvironment.NodePath nodePath : nodeEnv.nodePaths()) { totFreeSpace += nodePath.fileStore.getUsableSpace(); } // Very rough heurisic of how much disk space we expect the shard will use over its lifetime, the max of current average // shard size across the cluster and 5% of the total available free space on this node: long estShardSizeInBytes = Math.max(avgShardSizeInBytes, (long) (totFreeSpace/20.0)); // Just collate predicted disk usage on each path.data: Map<Path,Long> estReserveBytes = new HashMap<>(); for(Tuple<IndexShard,Injector> shardInjectorTuple : shards.values()) { IndexShard shard = shardInjectorTuple.v1(); // Remove indices/<index>/<shardID> subdirs from the statePath to get back to the path.data: Path nodeDataPath = shard.shardPath().getShardStatePath().getParent().getParent().getParent(); Long curBytes = estReserveBytes.get(nodeDataPath); if (curBytes == null) { curBytes = 0L; } estReserveBytes.put(nodeDataPath, curBytes + estShardSizeInBytes); } path = ShardPath.selectNewPathForShard(nodeEnv, shardId, indexSettings, estReserveBytes); logger.debug("{} creating using a new path [{}]", shardId, path); } else { logger.debug("{} creating using an existing path [{}]", shardId, path); } lock = nodeEnv.shardLock(shardId, TimeUnit.SECONDS.toMillis(5)); if (shards.containsKey(shardId.id())) { throw new IndexShardAlreadyExistsException(shardId + " already exists"); } indicesLifecycle.beforeIndexShardCreated(shardId, indexSettings); logger.debug("creating shard_id {}", shardId); // if we are on a shared FS we only own the shard (ie. we can safely delete it) if we are the primary. final boolean canDeleteShardContent = IndexMetaData.isOnSharedFilesystem(indexSettings) == false || (primary && IndexMetaData.isOnSharedFilesystem(indexSettings)); ModulesBuilder modules = new ModulesBuilder(); modules.add(new ShardsPluginsModule(indexSettings, pluginsService)); modules.add(new IndexShardModule(shardId, primary, indexSettings)); modules.add(new ShardIndexingModule()); modules.add(new ShardSearchModule()); modules.add(new ShardGetModule()); modules.add(new StoreModule(injector.getInstance(IndexStore.class).shardDirectory(), lock, new StoreCloseListener(shardId, canDeleteShardContent), path)); modules.add(new DeletionPolicyModule(indexSettings)); modules.add(new MergePolicyModule(indexSettings)); modules.add(new MergeSchedulerModule(indexSettings)); modules.add(new ShardFilterCacheModule()); modules.add(new ShardQueryCacheModule()); modules.add(new ShardBitsetFilterCacheModule()); modules.add(new ShardFieldDataModule()); modules.add(new IndexShardGatewayModule()); modules.add(new PercolatorShardModule()); modules.add(new ShardTermVectorsModule()); modules.add(new IndexShardSnapshotModule()); modules.add(new SuggestShardModule()); try { shardInjector = modules.createChildInjector(injector); } catch (CreationException e) { throw new IndexShardCreationException(shardId, Injectors.getFirstErrorFailure(e)); } catch (Throwable e) { throw new IndexShardCreationException(shardId, e); } IndexShard indexShard = shardInjector.getInstance(IndexShard.class); indicesLifecycle.indexShardStateChanged(indexShard, null, "shard created"); indicesLifecycle.afterIndexShardCreated(indexShard); shards = newMapBuilder(shards).put(shardId.id(), new Tuple<>(indexShard, shardInjector)).immutableMap(); success = true; return indexShard; } catch (IOException ex) { throw new IndexShardCreationException(shardId, ex); } finally { if (success == false) { IOUtils.closeWhileHandlingException(lock); if (shardInjector != null) { IndexShard indexShard = shardInjector.getInstance(IndexShard.class); closeShardInjector("initialization failed", shardId, shardInjector, indexShard); } } } }	any change we can move this heuristic into a utils class or into shardpath as a static method?
public synchronized IndexShard createShard(int sShardId, boolean primary) { /* * TODO: we execute this in parallel but it's a synced method. Yet, we might * be able to serialize the execution via the cluster state in the future. for now we just * keep it synced. */ if (closed.get()) { throw new IllegalStateException("Can't create shard [" + index.name() + "][" + sShardId + "], closed"); } final ShardId shardId = new ShardId(index, sShardId); ShardLock lock = null; boolean success = false; Injector shardInjector = null; try { ShardPath path = ShardPath.loadShardPath(logger, nodeEnv, shardId, indexSettings); if (path == null) { long totFreeSpace = 0; for (NodeEnvironment.NodePath nodePath : nodeEnv.nodePaths()) { totFreeSpace += nodePath.fileStore.getUsableSpace(); } // Very rough heurisic of how much disk space we expect the shard will use over its lifetime, the max of current average // shard size across the cluster and 5% of the total available free space on this node: long estShardSizeInBytes = Math.max(avgShardSizeInBytes, (long) (totFreeSpace/20.0)); // Just collate predicted disk usage on each path.data: Map<Path,Long> estReserveBytes = new HashMap<>(); for(Tuple<IndexShard,Injector> shardInjectorTuple : shards.values()) { IndexShard shard = shardInjectorTuple.v1(); // Remove indices/<index>/<shardID> subdirs from the statePath to get back to the path.data: Path nodeDataPath = shard.shardPath().getShardStatePath().getParent().getParent().getParent(); Long curBytes = estReserveBytes.get(nodeDataPath); if (curBytes == null) { curBytes = 0L; } estReserveBytes.put(nodeDataPath, curBytes + estShardSizeInBytes); } path = ShardPath.selectNewPathForShard(nodeEnv, shardId, indexSettings, estReserveBytes); logger.debug("{} creating using a new path [{}]", shardId, path); } else { logger.debug("{} creating using an existing path [{}]", shardId, path); } lock = nodeEnv.shardLock(shardId, TimeUnit.SECONDS.toMillis(5)); if (shards.containsKey(shardId.id())) { throw new IndexShardAlreadyExistsException(shardId + " already exists"); } indicesLifecycle.beforeIndexShardCreated(shardId, indexSettings); logger.debug("creating shard_id {}", shardId); // if we are on a shared FS we only own the shard (ie. we can safely delete it) if we are the primary. final boolean canDeleteShardContent = IndexMetaData.isOnSharedFilesystem(indexSettings) == false || (primary && IndexMetaData.isOnSharedFilesystem(indexSettings)); ModulesBuilder modules = new ModulesBuilder(); modules.add(new ShardsPluginsModule(indexSettings, pluginsService)); modules.add(new IndexShardModule(shardId, primary, indexSettings)); modules.add(new ShardIndexingModule()); modules.add(new ShardSearchModule()); modules.add(new ShardGetModule()); modules.add(new StoreModule(injector.getInstance(IndexStore.class).shardDirectory(), lock, new StoreCloseListener(shardId, canDeleteShardContent), path)); modules.add(new DeletionPolicyModule(indexSettings)); modules.add(new MergePolicyModule(indexSettings)); modules.add(new MergeSchedulerModule(indexSettings)); modules.add(new ShardFilterCacheModule()); modules.add(new ShardQueryCacheModule()); modules.add(new ShardBitsetFilterCacheModule()); modules.add(new ShardFieldDataModule()); modules.add(new IndexShardGatewayModule()); modules.add(new PercolatorShardModule()); modules.add(new ShardTermVectorsModule()); modules.add(new IndexShardSnapshotModule()); modules.add(new SuggestShardModule()); try { shardInjector = modules.createChildInjector(injector); } catch (CreationException e) { throw new IndexShardCreationException(shardId, Injectors.getFirstErrorFailure(e)); } catch (Throwable e) { throw new IndexShardCreationException(shardId, e); } IndexShard indexShard = shardInjector.getInstance(IndexShard.class); indicesLifecycle.indexShardStateChanged(indexShard, null, "shard created"); indicesLifecycle.afterIndexShardCreated(indexShard); shards = newMapBuilder(shards).put(shardId.id(), new Tuple<>(indexShard, shardInjector)).immutableMap(); success = true; return indexShard; } catch (IOException ex) { throw new IndexShardCreationException(shardId, ex); } finally { if (success == false) { IOUtils.closeWhileHandlingException(lock); if (shardInjector != null) { IndexShard indexShard = shardInjector.getInstance(IndexShard.class); closeShardInjector("initialization failed", shardId, shardInjector, indexShard); } } } }	this seems error prone if the structure could change in the future? i dont know a better way though..
public void testResolveDataStreams() { String dataStreamName = "foo_logs"; long epochMillis = randomLongBetween(1580536800000L, 1583042400000L); String dateString = DataStream.DATE_FORMATTER.formatMillis(epochMillis); IndexMetadata firstBackingIndexMetadata = createBackingIndex(dataStreamName, 1, epochMillis).build(); IndexMetadata secondBackingIndexMetadata = createBackingIndex(dataStreamName, 2, epochMillis).build(); Metadata.Builder mdBuilder = Metadata.builder() .put(indexBuilder("foo_foo").state(State.OPEN)) .put(indexBuilder("bar_bar").state(State.OPEN)) .put(indexBuilder("foo_index").state(State.OPEN).putAlias(AliasMetadata.builder("foo_alias"))) .put(indexBuilder("bar_index").state(State.OPEN).putAlias(AliasMetadata.builder("foo_alias"))) .put(firstBackingIndexMetadata, true) .put(secondBackingIndexMetadata, true) .put(new DataStream(dataStreamName, createTimestampField("@timestamp"), List.of(firstBackingIndexMetadata.getIndex(), secondBackingIndexMetadata.getIndex()))); ClusterState state = ClusterState.builder(new ClusterName("_name")).metadata(mdBuilder).build(); IndexNameExpressionResolver.WildcardExpressionResolver resolver = new IndexNameExpressionResolver.WildcardExpressionResolver(); { IndicesOptions indicesAndAliasesOptions = IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), true, false, true, false, false, false); IndexNameExpressionResolver.Context indicesAndAliasesContext = new IndexNameExpressionResolver.Context(state, indicesAndAliasesOptions, false); // data streams are not included but expression matches the data stream List<String> indices = resolver.resolve(indicesAndAliasesContext, Collections.singletonList("foo_*")); assertThat(indices, containsInAnyOrder("foo_index", "foo_foo", "bar_index")); // data streams are not included and expression doesn't match the data steram indices = resolver.resolve(indicesAndAliasesContext, Collections.singletonList("bar_*")); assertThat(indices, containsInAnyOrder("bar_bar", "bar_index")); } { IndicesOptions indicesAndAliasesOptions = IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), true, false, true, false, false, false); IndexNameExpressionResolver.Context indicesAliasesAndDataStreamsContext = new IndexNameExpressionResolver.Context(state, indicesAndAliasesOptions, false, false, true, false); // data stream's corresponding backing indices are resolved List<String> indices = resolver.resolve(indicesAliasesAndDataStreamsContext, Collections.singletonList("foo_*")); assertThat(indices, containsInAnyOrder("foo_index", "bar_index", "foo_foo", ".ds-foo_logs-" + dateString + "-000001", ".ds-foo_logs-" + dateString + "-000002")); // include all wildcard adds the data stream's backing indices indices = resolver.resolve(indicesAliasesAndDataStreamsContext, Collections.singletonList("*")); assertThat(indices, containsInAnyOrder("foo_index", "bar_index", "foo_foo", "bar_bar", ".ds-foo_logs-" + dateString + "-000001", ".ds-foo_logs-" + dateString + "-000002")); } { IndicesOptions indicesAliasesAndExpandHiddenOptions = IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), true, false, true, true, false, false, false); IndexNameExpressionResolver.Context indicesAliasesDataStreamsAndHiddenIndices = new IndexNameExpressionResolver.Context(state, indicesAliasesAndExpandHiddenOptions, false, false, true, false); // data stream's corresponding backing indices are resolved List<String> indices = resolver.resolve(indicesAliasesDataStreamsAndHiddenIndices, Collections.singletonList("foo_*")); assertThat(indices, containsInAnyOrder("foo_index", "bar_index", "foo_foo", ".ds-foo_logs-" + dateString + "-000001", ".ds-foo_logs-" + dateString + "-000002")); // include all wildcard adds the data stream's backing indices indices = resolver.resolve(indicesAliasesDataStreamsAndHiddenIndices, Collections.singletonList("*")); assertThat(indices, containsInAnyOrder("foo_index", "bar_index", "foo_foo", "bar_bar", ".ds-foo_logs-" + dateString + "-000001", ".ds-foo_logs-" + dateString + "-000002")); } }	maybe use the getdefaultbackingindexname(...) method that accepts a timestamp?
* @param dataLocations the data-locations to try. * @return the latest state or <code>null</code> if no state was found. */ public T loadLatestState(Logger logger, NamedXContentRegistry namedXContentRegistry, Path... dataLocations) throws IOException { RETRY_LOOP: while (true) { List<PathAndStateId> files = new ArrayList<>(); long maxStateId = -1; if (dataLocations != null) { // select all eligible files first for (Path dataLocation : dataLocations) { final Path stateDir = dataLocation.resolve(STATE_DIR_NAME); // now, iterate over the current versions, and find latest one // we don't check if the stateDir is present since it could be deleted // after the check. Also if there is a _state file and it's not a dir something is really wrong // we don't pass a glob since we need the group part for parsing try (DirectoryStream<Path> paths = Files.newDirectoryStream(stateDir)) { for (Path stateFile : paths) { final Matcher matcher = stateFilePattern.matcher(stateFile.getFileName().toString()); if (matcher.matches()) { final long stateId = Long.parseLong(matcher.group(1)); maxStateId = Math.max(maxStateId, stateId); PathAndStateId pav = new PathAndStateId(stateFile, stateId); logger.trace("found state file: {}", pav); files.add(pav); } } } catch (NoSuchFileException | FileNotFoundException ex) { // no _state directory -- move on } } } // NOTE: we might have multiple version of the latest state if there are multiple data dirs.. for this case // we iterate only over the ones with the max version. long finalMaxStateId = maxStateId; Collection<PathAndStateId> pathAndStateIds = files .stream() .filter(pathAndStateId -> pathAndStateId.id == finalMaxStateId) .collect(Collectors.toCollection(ArrayList::new)); final List<Throwable> exceptions = new ArrayList<>(); for (PathAndStateId pathAndStateId : pathAndStateIds) { try { T state = read(namedXContentRegistry, pathAndStateId.file); logger.trace("state id [{}] read from [{}]", pathAndStateId.id, pathAndStateId.file.getFileName()); return state; } catch (NoSuchFileException | FileNotFoundException e) { // A state file was deleted by a concurrent process so the files list is not correct any longer // and we have to start over and fetch the list of state files again logger.debug( () -> new ParameterizedMessage( "{}: failed to read [{}] because it was deleted by a concurrent action, retrying...", pathAndStateId.file.toAbsolutePath(), prefix ), e ); continue RETRY_LOOP; } catch (Exception e) { exceptions.add(new IOException("failed to read " + pathAndStateId.toString(), e)); logger.debug(() -> new ParameterizedMessage( "{}: failed to read [{}], ignoring...", pathAndStateId.file.toAbsolutePath(), prefix), e); } } // if we reach this something went wrong ExceptionsHelper.maybeThrowRuntimeAndSuppress(exceptions); if (files.size() > 0) { // We have some state files but none of them gave us a usable state throw new IllegalStateException("Could not find a state file to recover from among " + files); } return null; } }	this can only happen if a concurrent delete occurred i think. just starting over with new files should work out eventually.
* @param dataLocations the data-locations to try. * @return the latest state or <code>null</code> if no state was found. */ public T loadLatestState(Logger logger, NamedXContentRegistry namedXContentRegistry, Path... dataLocations) throws IOException { RETRY_LOOP: while (true) { List<PathAndStateId> files = new ArrayList<>(); long maxStateId = -1; if (dataLocations != null) { // select all eligible files first for (Path dataLocation : dataLocations) { final Path stateDir = dataLocation.resolve(STATE_DIR_NAME); // now, iterate over the current versions, and find latest one // we don't check if the stateDir is present since it could be deleted // after the check. Also if there is a _state file and it's not a dir something is really wrong // we don't pass a glob since we need the group part for parsing try (DirectoryStream<Path> paths = Files.newDirectoryStream(stateDir)) { for (Path stateFile : paths) { final Matcher matcher = stateFilePattern.matcher(stateFile.getFileName().toString()); if (matcher.matches()) { final long stateId = Long.parseLong(matcher.group(1)); maxStateId = Math.max(maxStateId, stateId); PathAndStateId pav = new PathAndStateId(stateFile, stateId); logger.trace("found state file: {}", pav); files.add(pav); } } } catch (NoSuchFileException | FileNotFoundException ex) { // no _state directory -- move on } } } // NOTE: we might have multiple version of the latest state if there are multiple data dirs.. for this case // we iterate only over the ones with the max version. long finalMaxStateId = maxStateId; Collection<PathAndStateId> pathAndStateIds = files .stream() .filter(pathAndStateId -> pathAndStateId.id == finalMaxStateId) .collect(Collectors.toCollection(ArrayList::new)); final List<Throwable> exceptions = new ArrayList<>(); for (PathAndStateId pathAndStateId : pathAndStateIds) { try { T state = read(namedXContentRegistry, pathAndStateId.file); logger.trace("state id [{}] read from [{}]", pathAndStateId.id, pathAndStateId.file.getFileName()); return state; } catch (NoSuchFileException | FileNotFoundException e) { // A state file was deleted by a concurrent process so the files list is not correct any longer // and we have to start over and fetch the list of state files again logger.debug( () -> new ParameterizedMessage( "{}: failed to read [{}] because it was deleted by a concurrent action, retrying...", pathAndStateId.file.toAbsolutePath(), prefix ), e ); continue RETRY_LOOP; } catch (Exception e) { exceptions.add(new IOException("failed to read " + pathAndStateId.toString(), e)); logger.debug(() -> new ParameterizedMessage( "{}: failed to read [{}], ignoring...", pathAndStateId.file.toAbsolutePath(), prefix), e); } } // if we reach this something went wrong ExceptionsHelper.maybeThrowRuntimeAndSuppress(exceptions); if (files.size() > 0) { // We have some state files but none of them gave us a usable state throw new IllegalStateException("Could not find a state file to recover from among " + files); } return null; } }	i know continuing a named loop is kinda dirty here, but with the way we collect exceptions and such it seemed like the cleanest/shortest way to add a retry.
static PercolateQuery.QueryStore createStore(MappedFieldType queryBuilderFieldType, QueryShardContext context, boolean mapUnmappedFieldsAsString) { Version indexVersion = context.indexVersionCreated(); NamedWriteableRegistry registry = context.getWriteableRegistry(); return ctx -> { LeafReader leafReader = ctx.reader(); BinaryDocValues binaryDocValues = leafReader.getBinaryDocValues(queryBuilderFieldType.name()); if (binaryDocValues == null) { return docId -> null; } if (indexVersion.onOrAfter(Version.V_6_0_0_beta2)) { return docId -> { if (binaryDocValues.advanceExact(docId)) { BytesRef qbSource = binaryDocValues.binaryValue(); try (InputStream in = new ByteArrayInputStream(qbSource.bytes, qbSource.offset, qbSource.length)) { try (StreamInput input = new NamedWriteableAwareStreamInput(new InputStreamStreamInput(in, qbSource.length), registry)) { input.setVersion(indexVersion); // Query builder's content is stored via BinaryFieldMapper, which has a custom encoding // to encode multiple binary values into a single binary doc values field. // This is the reason we need to first need to read the number of values and // then the length of the field value in bytes. int numValues = input.readVInt(); assert numValues == 1; int valueLength = input.readVInt(); assert valueLength > 0; QueryBuilder queryBuilder = input.readNamedWriteable(QueryBuilder.class); assert in.read() == -1; return PercolatorFieldMapper.toQuery(context, mapUnmappedFieldsAsString, queryBuilder); } } } else { return null; } }; } else { return docId -> { if (binaryDocValues.advanceExact(docId)) { BytesRef qbSource = binaryDocValues.binaryValue(); if (qbSource.length > 0) { XContent xContent = PercolatorFieldMapper.QUERY_BUILDER_CONTENT_TYPE.xContent(); try (XContentParser sourceParser = xContent.createParser(context.getXContentRegistry(), qbSource.bytes, qbSource.offset, qbSource.length)) { return parseQuery(context, mapUnmappedFieldsAsString, sourceParser); } } else { return null; } } else { return null; } }; } }; }	can you change the line wrapping on this somehow? like stick new inputstreamstreaminput on a new line and indent it? i think as is it'd break how i visually scan try-with-resources.
public void testLicenseUpdateFailureHandlerUpdate() throws Exception { Settings settings = Settings.builder(). put("xpack.security.authc.api_key.enabled", "true"). build(); Collection<Object> components = createComponentsWithSecurityNotExplicitlyEnabled(settings); AuthenticationService service = findComponent(AuthenticationService.class, components); assertNotNull(service); RestRequest request = new FakeRestRequest(); final AtomicBoolean completed = new AtomicBoolean(false); service.authenticate(request, ActionListener.wrap(result -> {assertTrue(completed.compareAndSet(false, true)); }, this::logAndFail)); assertTrue(completed.compareAndSet(true, false)); threadContext.stashContext(); licenseState.update( randomFrom(License.OperationMode.GOLD, License.OperationMode.ENTERPRISE, License.OperationMode.PLATINUM), true, null); service.authenticate(request, ActionListener.wrap(result -> {assertTrue(completed.compareAndSet(false, true)); }, this::VerifyMissingAuthenticationException)); if(completed.get()){ fail("authentication succeeded but it shouldn't"); } }	our typical style is: suggestion service.authenticate(request, actionlistener.wrap(result -> { asserttrue(completed.compareandset(false, true)); }, this::logandfail));
public void testGetUsers() throws Exception { RestHighLevelClient client = highLevelClient(); addUser(client, "testUser", "testPassword"); { GetUsersRequest getUsersRequest = new GetUsersRequest(); GetUsersResponse getUsersResponse = client.security().getUsers(getUsersRequest, RequestOptions.DEFAULT); assertNotNull(getUsersResponse.getUsers()); } }	this is not complete yet, i would simply use testgetroles as a blueprint. the same with the documentation, use ./docs/java-rest/high-level/security/get-roles.asciidoc as an example. and don't forget to add it to ./x-pack/docs/en/rest-api/security.asciidoc.
public void deactivate(DiscoveryNode leader) { synchronized (mutex) { logger.trace("deactivating PeerFinder and setting leader to {}", leader); active = false; handleWakeUp(); this.leader = Optional.of(leader); assert assertInactiveWithNoKnownPeers(); } }	assert active == false?
T get() throws IOException; } private static <T> List<T> parseArray(XContentParser parser, IOSupplier<T> supplier) throws IOException { List<T> list = new ArrayList<>(); if (parser.currentToken().isValue() || parser.currentToken() == XContentParser.Token.VALUE_NULL || parser.currentToken() == XContentParser.Token.START_OBJECT) { list.add(supplier.get()); // single value } else { while (parser.nextToken() != XContentParser.Token.END_ARRAY) { if (parser.currentToken().isValue() || parser.currentToken() == XContentParser.Token.VALUE_NULL || parser.currentToken() == XContentParser.Token.START_OBJECT) { list.add(supplier.get()); } else { throw new IllegalStateException("expected value but got [" + parser.currentToken() + "]"); } } }	can you indent this one extra time? when the body of the block below it and a part of the if statement line up i have trouble separating them properly when reading quickly.
private static void configureLoggerLevels(Settings settings) { if (ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING.exists(settings)) { final Level level = ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING.get(settings); Loggers.setLevel(ESLoggerFactory.getRootLogger(), level); } final Map<String, String> levels = settings.filter(ESLoggerFactory.LOG_LEVEL_SETTING::match).getAsMap(); for (String key : levels.keySet()) { final Level level = ESLoggerFactory.LOG_LEVEL_SETTING.getConcreteSetting(key).get(settings); Loggers.setLevel(ESLoggerFactory.getLogger(key.substring("logger.".length())), level); } } /** * Set system properties that can be used in configuration files to specify paths and file patterns for log files. We expose three * properties here: * <ul> * <li> * {@code es.logs.base_path} the base path containing the log files * </li> * <li> * {@code es.logs.cluster_name} the cluster name, used as the prefix of log filenames in the default configuration * </li> * <li> * {@code es.logs.node_name} the node name, can be used as part of log filenames (only exposed if {@link Node#NODE_NAME_SETTING}	when this is backported to the 5.x branch (for inclusion in 5.3.0), i will keep this property around for backwards compatibility purposes and include a note in the migration docs that this property is being removed in 6.0.0. i investigated the possibility of adding deprecation logging for use of this property, but it is not possible without some horrible reflection-based hacks.
private static void configureLoggerLevels(Settings settings) { if (ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING.exists(settings)) { final Level level = ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING.get(settings); Loggers.setLevel(ESLoggerFactory.getRootLogger(), level); } final Map<String, String> levels = settings.filter(ESLoggerFactory.LOG_LEVEL_SETTING::match).getAsMap(); for (String key : levels.keySet()) { final Level level = ESLoggerFactory.LOG_LEVEL_SETTING.getConcreteSetting(key).get(settings); Loggers.setLevel(ESLoggerFactory.getLogger(key.substring("logger.".length())), level); } } /** * Set system properties that can be used in configuration files to specify paths and file patterns for log files. We expose three * properties here: * <ul> * <li> * {@code es.logs.base_path} the base path containing the log files * </li> * <li> * {@code es.logs.cluster_name} the cluster name, used as the prefix of log filenames in the default configuration * </li> * <li> * {@code es.logs.node_name} the node name, can be used as part of log filenames (only exposed if {@link Node#NODE_NAME_SETTING}	what's the behavior if the property is not set but defined in the logging.yml file? it will be replaced by an empty string?
protected final void onChannelClosed(Channel channel) { Optional<Map.Entry<Long, HandshakeResponseHandler>> first = pendingHandshakes.entrySet().stream() .filter((entry) -> entry.getValue().channel == channel).findFirst(); if(first.isPresent()) { final Long requestId = first.get().getKey(); HandshakeResponseHandler handler = first.get().getValue(); pendingHandshakes.remove(requestId); handler.handleException(new TransportException("connection reset")); } }	shortcut: versionhandshakeresponsetransportresponsehandler handler = pendinghandshakes.remove(requestid);
public void testGettingInvalidShort() throws Exception { createIndex("test"); updateMappingForNumericValuesTests("test"); updateMapping("test", builder -> { builder.startObject("test_keyword").field("type", "keyword").endObject(); builder.startObject("test_date").field("type", "date").endObject(); }); int intNotShort = randomIntBetween(Short.MAX_VALUE + 1, Integer.MAX_VALUE); long longNotShort = randomLongBetween(Short.MAX_VALUE + 1, Long.MAX_VALUE); double doubleNotShort = randomDoubleBetween(Short.MAX_VALUE + 1, Double.MAX_VALUE, true); float floatNotShort = randomFloatBetween(Short.MAX_VALUE + 1, Float.MAX_VALUE); String randomString = randomUnicodeOfCodepointLengthBetween(128, 256); long randomDate = randomNonNegativeLong(); String doubleErrorMessage = (doubleNotShort > Long.MAX_VALUE || doubleNotShort < Long.MIN_VALUE) ? Double.toString(doubleNotShort) : Long.toString(Math.round(doubleNotShort)); index("test", "1", builder -> { builder.field("test_integer", intNotShort); builder.field("test_long", longNotShort); builder.field("test_double", doubleNotShort); builder.field("test_float", floatNotShort); builder.field("test_keyword", randomString); builder.field("test_date", randomDate); }); doWithQuery(SELECT_WILDCARD, (results) -> { results.next(); SQLException sqle = expectThrows(SQLException.class, () -> results.getShort("test_integer")); assertEquals(format(Locale.ROOT, "Numeric %s out of range", intNotShort), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getObject("test_integer", Short.class)); assertEquals(format(Locale.ROOT, "Numeric %s out of range", intNotShort), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getShort("test_long")); assertEquals(format(Locale.ROOT, "Numeric %s out of range", Long.toString(longNotShort)), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getObject("test_long", Short.class)); assertEquals(format(Locale.ROOT, "Numeric %s out of range", Long.toString(longNotShort)), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getShort("test_double")); assertEquals(format(Locale.ROOT, "Numeric %s out of range", doubleErrorMessage), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getObject("test_double", Short.class)); assertEquals(format(Locale.ROOT, "Numeric %s out of range", doubleErrorMessage), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getShort("test_float")); assertEquals(format(Locale.ROOT, "Numeric %s out of range", Double.toString(floatNotShort)), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getObject("test_float", Short.class)); assertEquals(format(Locale.ROOT, "Numeric %s out of range", Double.toString(floatNotShort)), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getShort("test_keyword")); assertEquals(format(Locale.ROOT, "Unable to convert value [%.128s] of type [KEYWORD] to [Short]", randomString), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getObject("test_keyword", Short.class)); assertEquals(format(Locale.ROOT, "Unable to convert value [%.128s] of type [KEYWORD] to [Short]", randomString), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getShort("test_date")); assertEquals(format(Locale.ROOT, "Unable to convert value [%.128s] of type [DATETIME] to [Short]", of(randomDate, timeZoneId)), sqle.getMessage()); sqle = expectThrows(SQLException.class, () -> results.getObject("test_date", Short.class)); assertEquals(format(Locale.ROOT, "Unable to convert value [%.128s] of type [DATETIME] to [Short]", of(randomDate, timeZoneId)), sqle.getMessage()); }); }	i find it cumbersome to call every time this static method with the timezoneid that's initialized outside the tests methods. maybe create a of method inside the resultsettestcase class that will, in turn, call the jdbctestutils' of method with the timezoneid param?
@Override protected CompositeKeyExtractor mutateInstance(CompositeKeyExtractor instance) { return new CompositeKeyExtractor(instance.key() + "mutated", instance.property(), instance.zoneId(), instance.isDateTimeBased()); }	is this mutation mechanism accurate here? not talking about this pr specific change, but the method in general. shouldn't this have 4 different mutations? (the mutated key, the property, the zone id, and the freshly added boolean) something similar to https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/sql/sql-action/src/test/java/org/elasticsearch/xpack/sql/action/sqlqueryrequesttests.java#l102.
Map<String, DatafeedConfig> expandClusterStateDatafeeds(String datafeedExpression, boolean allowNoDatafeeds, ClusterState clusterState) { Map<String, DatafeedConfig> configById = new HashMap<>(); try { MlMetadata mlMetadata = MlMetadata.getMlMetadata(clusterState); Set<String> expandedDatafeedIds = mlMetadata.expandDatafeedIds(datafeedExpression, allowNoDatafeeds); for (String expandedDatafeedId : expandedDatafeedIds) { configById.put(expandedDatafeedId, mlMetadata.getDatafeed(expandedDatafeedId)); } } catch (Exception e){ // ignore } return configById; }	based on your comment above it sounds like exceptions are unexpected here? if so, we could assert and display the exception and stack trace. then we'd get ci failures that might flag situations we hadn't thought of. (but obviously not if i've misunderstood and exceptions here are expected.)
public static <T> Boolean multiValueDocValues(Map<String, ScriptDocValues<T>> doc, String fieldName, Predicate<T> script) { if (doc.containsKey(fieldName)) { ScriptDocValues<T> docValues = doc.get(fieldName); if (docValues.isEmpty() == false) { for (int i = 0; i < docValues.size(); i++) { T value = docValues.get(i); if (script.test(value)) { return true; } } return false; } } return false; }	should be covered by the for loop and the docvalues.size() check, no? or maybe a check if the docvalues is null could be necessary (if that's a possibility).
public static Query toQuery(Expression e, TranslatorHandler handler) { Query translation = null; int i = 0; while (translation == null && i < QUERY_TRANSLATORS.size()) { translation = QUERY_TRANSLATORS.get(i).translate(e, handler); i++; } if (translation != null) { if (translation instanceof ScriptQuery) { // check the operators and the expressions involved in these operations so that all can be used // in a doc-values multi-valued context boolean multiValuedCompatible = false == e.anyMatch(exp -> { if (exp instanceof Literal == false && exp instanceof FieldAttribute == false && exp instanceof MultiValuedOperationCompatible == false) { return true; } else { return false; }}); if (multiValuedCompatible) { ScriptQuery query = (ScriptQuery) translation; return new NoNullSafetyScriptQuery(query.source(), Scripts.multiValueDocValuesReplacement(query.script())); } } return translation; } throw new QlIllegalArgumentException("Don't know how to translate {} {}", e.nodeName(), e); }	maybe consider extracting to a method.
public static Query toQuery(Expression e, TranslatorHandler handler) { Query translation = null; int i = 0; while (translation == null && i < QUERY_TRANSLATORS.size()) { translation = QUERY_TRANSLATORS.get(i).translate(e, handler); i++; } if (translation != null) { if (translation instanceof ScriptQuery) { // check the operators and the expressions involved in these operations so that all can be used // in a doc-values multi-valued context boolean multiValuedCompatible = false == e.anyMatch(exp -> { if (exp instanceof Literal == false && exp instanceof FieldAttribute == false && exp instanceof MultiValuedOperationCompatible == false) { return true; } else { return false; }}); if (multiValuedCompatible) { ScriptQuery query = (ScriptQuery) translation; return new NoNullSafetyScriptQuery(query.source(), Scripts.multiValueDocValuesReplacement(query.script())); } } return translation; } throw new QlIllegalArgumentException("Don't know how to translate {} {}", e.nodeName(), e); }	couldn't the lambda be just: suggestion return exp instanceof literal || exp instanceof fieldattribute || exp instanceof multivaluedoperationcompatible; });
public static String classPackageAsPrefix(Class<?> function) { String prefix = function.getPackageName().substring(PKG_LENGTH); int index = prefix.indexOf('.'); Check.isTrue(index > 0, "invalid package {}", prefix); return "{" + prefix.substring(0, index) + "}"; } /** * This method replaces any .docValue(doc,params.%s) call with a variable. * Each variable is then used in a {@code java.util.function.Predicate} to iterate over the doc_values in a Painless script * * For example, a query of the form fieldA - fieldB > 0 that gets translated into the following Painless script * {@code InternalQlScriptUtils.nullSafeFilter(InternalQlScriptUtils.gt(InternalQlScriptUtils.sub( * InternalQlScriptUtils.docValue(doc,params.v0),InternalQlScriptUtils.docValue(doc,params.v1)),params.v2))} * will become, after this method rewrite * {@code InternalEqlScriptUtils.multiValueDocValues(doc,params.v0,X1 -> InternalEqlScriptUtils.multiValueDocValues(doc,params.v1, * X2 -> InternalQlScriptUtils.nullSafeFilter(InternalQlScriptUtils.gt(InternalQlScriptUtils.sub(X1,X2),params.v2))))}	proposal: suggestion * this method replaces any .docvalue(doc,params.%s) call with a "xn" variable.
public static String classPackageAsPrefix(Class<?> function) { String prefix = function.getPackageName().substring(PKG_LENGTH); int index = prefix.indexOf('.'); Check.isTrue(index > 0, "invalid package {}", prefix); return "{" + prefix.substring(0, index) + "}"; } /** * This method replaces any .docValue(doc,params.%s) call with a variable. * Each variable is then used in a {@code java.util.function.Predicate} to iterate over the doc_values in a Painless script * * For example, a query of the form fieldA - fieldB > 0 that gets translated into the following Painless script * {@code InternalQlScriptUtils.nullSafeFilter(InternalQlScriptUtils.gt(InternalQlScriptUtils.sub( * InternalQlScriptUtils.docValue(doc,params.v0),InternalQlScriptUtils.docValue(doc,params.v1)),params.v2))} * will become, after this method rewrite * {@code InternalEqlScriptUtils.multiValueDocValues(doc,params.v0,X1 -> InternalEqlScriptUtils.multiValueDocValues(doc,params.v1, * X2 -> InternalQlScriptUtils.nullSafeFilter(InternalQlScriptUtils.gt(InternalQlScriptUtils.sub(X1,X2),params.v2))))}	nit: would the index of the x args not start from 0 also in this case? not sure if we'll still want to merge #73681, but if we do, afterall, i guess this example could be refreshed.
@Override public void clear(String reason) { logger.debug("full cache clear, reason [{}]", reason); seenReaders.clear(); indicesFilterCache.cache().invalidateAll(); // There is an explicit call to cache.cleanUp() here because cache // invalidation in Guava does not immediately remove values from the // cache. In the case of a cache with a rare write or read rate, // it's possible for values to persist longer than desired. In the // case of the circuit breaker, when clearing the entire cache all // entries should immediately be evicted so that their sizes are // removed from the breaker estimates. // // Note this is intended by the Guava developers, see: // https://code.google.com/p/guava-libraries/wiki/CachesExplained#Eviction // (the "When Does Cleanup Happen" section) indicesFilterCache.cache().cleanUp(); }	i wonder if we can get a race here stuff that is in seen readers but not in the cache or so?
@Override public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { Iterable<ShardRouting> assignedShards = allocation.routingNodes().assignedShards(shardRouting.shardId()); Decision decision = decideSameNode(shardRouting, node, allocation, assignedShards); if (decision.type() == Decision.Type.NO || sameHost == false) { // if its already a NO decision looking at the node, or we aren't configured to look at the host, return the decision return decision; } if (INDEX_AUTO_EXPAND_REPLICAS_SETTING.get(allocation.metadata().getIndexSafe(shardRouting.index()).getSettings()) .expandToAllNodes()) { return YES_AUTO_EXPAND_ALL; } Hosts hosts = allocation.getHosts(); Host host = hosts.getHost(node.nodeId()); // return yes if can not find the host info of candidate node(no host name & address) if (node.node() != null && host != null) { for (ShardRouting assignedShard : assignedShards) { if (hosts.getHost(assignedShard.currentNodeId()) == host) { return allocation.debugDecision() ? debugNoAlreadyAllocatedToHost(node, allocation) : Decision.NO; } } } return YES_NONE_HOLD_COPY; }	i think we are guaranteed to have a host now since there always is a host address? can we assert that instead?
@Override public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) { Iterable<ShardRouting> assignedShards = allocation.routingNodes().assignedShards(shardRouting.shardId()); Decision decision = decideSameNode(shardRouting, node, allocation, assignedShards); if (decision.type() == Decision.Type.NO || sameHost == false) { // if its already a NO decision looking at the node, or we aren't configured to look at the host, return the decision return decision; } if (INDEX_AUTO_EXPAND_REPLICAS_SETTING.get(allocation.metadata().getIndexSafe(shardRouting.index()).getSettings()) .expandToAllNodes()) { return YES_AUTO_EXPAND_ALL; } Hosts hosts = allocation.getHosts(); Host host = hosts.getHost(node.nodeId()); // return yes if can not find the host info of candidate node(no host name & address) if (node.node() != null && host != null) { for (ShardRouting assignedShard : assignedShards) { if (hosts.getHost(assignedShard.currentNodeId()) == host) { return allocation.debugDecision() ? debugNoAlreadyAllocatedToHost(node, allocation) : Decision.NO; } } } return YES_NONE_HOLD_COPY; }	i wonder if we even need the hosts structure now that we always have a host address and only check for that? can we do something like: allocation.nodes().getdatanodes().get(assignedshard.currentnodeid()).gethostaddress().equals(node.node().gethostaddress()) with your follow-up pr for host name, i would expect it to become so that we only check either the address or the name, never both, which should make the same possible for the host name case?
private static ConstructingObjectParser<TimeSyncConfig, Void> createParser(boolean lenient) { ConstructingObjectParser<TimeSyncConfig, Void> parser = new ConstructingObjectParser<>(NAME, lenient, args -> { String field = (String) args[0]; TimeValue delay = (TimeValue) args[1]; return new TimeSyncConfig(field, delay); }); parser.declareString(constructorArg(), DataFrameField.FIELD); parser.declareField(optionalConstructorArg(), (p, c) -> TimeValue.parseTimeValue(p.textOrNull(), DEFAULT_DELAY, DataFrameField.DELAY.getPreferredName()), DataFrameField.DELAY, ObjectParser.ValueType.STRING_OR_NULL); return parser; }	i know you didn't introduce it in this pr, but since you're changing this line of code please can you change string_or_null to just string and p.textornull() two lines higher to p.text(). timevalue.parsetimevalue() will throw npe if passed null, so we're not helping anyone by having the parser accept an explicit null. (not present at all is fine but doesn't go through the null code path.)
public void testMultiValueQueryText() throws IOException { index("{\\\\"text\\\\":[" + toJson("one") + "," + toJson("two, three") + "," + toJson("\\\\"four\\\\"") + "], \\\\"number\\\\" : [1, [2, 3], 4] }"); String expected = " t | n \\\\n" + "-------------------------------+---------------\\\\n" + "[\\\\"one\\\\",\\\\"two, three\\\\",\\\\"\\\\\\\\\\\\"four\\\\\\\\\\\\"\\\\"]|[1,2,3,4] \\\\n"; Tuple<String, String> response = runSqlAsText("SELECT ARRAY(text) t, ARRAY(number) n FROM test", "text/plain"); assertEquals(expected, response.v1()); }	why do you use tojson only for the values but not also for the field names?
private static SqlQueryResponse withData(List<List<Object>> rows) { List<ColumnInfo> headers = new ArrayList<>(); if (rows.isEmpty() == false) { // headers for (Object o : rows.get(0)) { if (o instanceof Collection) { Collection<?> col = (Collection<?>) o; o = col.isEmpty() ? null : col.toArray()[0]; } String typeName = fromJava(o).typeName(); headers.add(new ColumnInfo("index", typeName + "_column", typeName)); } } return new SqlQueryResponse(null, Mode.JDBC, DATE_NANOS_SUPPORT_VERSION, false, headers, rows); }	i don't remember our take here, shouldn't the type name reveal that it's an array? something like byte_array or _array (postgres format) ?
public final DocumentParserContext createNestedContext(String fullPath) { if (isWithinCopyTo()) { // nested context will already have been set up for copy_to fields return this; } final LuceneDocument doc = new LuceneDocument(fullPath, doc(), doc().getDimensions()); addDoc(doc); return switchDoc(doc); }	does it mean that nested documents can contribute dimensions into the root document? i don't think we should allow it.
@Override public IndexFieldData<?> build(IndexFieldDataCache cache, CircuitBreakerService breakerService) { IndexOrdinalsFieldData delegate = new SortedSetOrdinalsIndexFieldData( cache, fieldName, valuesSourceType, breakerService, AbstractLeafOrdinalsFieldData.DEFAULT_SCRIPT_FUNCTION); return new KeyedFlattenedFieldData(key, delegate); } } } /** * A field type that represents all 'root' values. This field type is used in * searches on the flattened field itself, e.g. 'my_flattened: some_value'. */ public static final class RootFlattenedFieldType extends StringFieldType { private final boolean splitQueriesOnWhitespace; public RootFlattenedFieldType(String name, boolean indexed, boolean hasDocValues, Map<String, String> meta, boolean splitQueriesOnWhitespace) { super(name, indexed, false, hasDocValues, splitQueriesOnWhitespace ? TextSearchInfo.WHITESPACE_MATCH_ONLY : TextSearchInfo.SIMPLE_MATCH_ONLY, meta); this.splitQueriesOnWhitespace = splitQueriesOnWhitespace; } @Override public String typeName() { return CONTENT_TYPE; } @Override public Object valueForDisplay(Object value) { if (value == null) { return null; } BytesRef binaryValue = (BytesRef) value; return binaryValue.utf8ToString(); } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName, Supplier<SearchLookup> searchLookup) { failIfNoDocValues(); return new SortedSetOrdinalsIndexFieldData.Builder(name(), CoreValuesSourceType.KEYWORD); } @Override public ValueFetcher valueFetcher(SearchExecutionContext context, String format) { return SourceValueFetcher.identity(name(), context, format); } } private final FlattenedFieldParser fieldParser; private final Builder builder; private FlattenedFieldMapper(String simpleName, MappedFieldType mappedFieldType, Builder builder) { super(simpleName, mappedFieldType, Lucene.KEYWORD_ANALYZER, CopyTo.empty()); this.builder = builder; this.fieldParser = new FlattenedFieldParser(mappedFieldType.name(), keyedFieldName(), mappedFieldType, builder.depthLimit.get(), builder.ignoreAbove.get(), builder.nullValue.get()); } @Override protected String contentType() { return CONTENT_TYPE; } int depthLimit() { return builder.depthLimit.get(); } int ignoreAbove() { return builder.ignoreAbove.get(); } @Override public RootFlattenedFieldType fieldType() { return (RootFlattenedFieldType) super.fieldType(); } @Override public KeyedFlattenedFieldType keyedFieldType(String key) { return new KeyedFlattenedFieldType(keyedFieldName(), name(), key, fieldType()); } public String keyedFieldName() { return mappedFieldType.name() + KEYED_FIELD_SUFFIX; } @Override protected void parseCreateField(ParseContext context) throws IOException { if (context.parser().currentToken() == XContentParser.Token.VALUE_NULL) { return; } if (mappedFieldType.isSearchable() == false && mappedFieldType.hasDocValues() == false) { context.parser().skipChildren(); return; } XContentParser xContentParser = context.parser(); context.doc().addAll(fieldParser.parse(xContentParser)); if (mappedFieldType.hasDocValues() == false) { createFieldNamesField(context); }	i think rootname will always be the same as name here?
@SuppressWarnings("unchecked") public void testFlattenedField() throws IOException { XContentBuilder mapping = XContentFactory.jsonBuilder().startObject() .startObject("_doc") .startObject("properties") .startObject("flat") .field("type", "flattened") .endObject() .endObject() .endObject() .endObject(); MapperService mapperService = createMapperService(mapping); XContentBuilder source = XContentFactory.jsonBuilder().startObject() .startObject("flat") .field("f1", "value1") .field("f2", 1) .endObject() .endObject(); // requesting via wildcard should retrieve the root field as a structured map Map<String, DocumentField> fields = fetchFields(mapperService, source, fieldAndFormatList("*", null, false)); assertEquals(1, fields.size()); assertThat(fields.keySet(), containsInAnyOrder("flat")); Map<String, Object> flattendedValue = (Map<String, Object>) fields.get("flat").getValue(); assertThat(flattendedValue.keySet(), containsInAnyOrder("f1", "f2")); assertEquals("value1", flattendedValue.get("f1")); assertEquals(1, flattendedValue.get("f2")); // direct retrieval of subfield is possible List<FieldAndFormat> fieldAndFormatList = new ArrayList<>(); fieldAndFormatList.add(new FieldAndFormat("flat.f1", null)); fields = fetchFields(mapperService, source, fieldAndFormatList); assertEquals(1, fields.size()); assertThat(fields.keySet(), containsInAnyOrder("flat.f1")); assertThat(fields.get("flat.f1").getValue(), equalTo("value1")); // direct retrieval of root field and subfield is possible fieldAndFormatList.add(new FieldAndFormat("*", null)); fields = fetchFields(mapperService, source, fieldAndFormatList); assertEquals(2, fields.size()); assertThat(fields.keySet(), containsInAnyOrder("flat", "flat.f1")); flattendedValue = (Map<String, Object>) fields.get("flat").getValue(); assertThat(flattendedValue.keySet(), containsInAnyOrder("f1", "f2")); assertEquals("value1", flattendedValue.get("f1")); assertEquals(1, flattendedValue.get("f2")); assertThat(fields.get("flat.f1").getValue(), equalTo("value1")); // retrieval of subfield with widlcard is not possible fields = fetchFields(mapperService, source, fieldAndFormatList("flat.f*", null, false)); assertEquals(0, fields.size()); // retrieval of non-existing subfield returns empty result fields = fetchFields(mapperService, source, fieldAndFormatList("flat.baz", null, false)); assertEquals(0, fields.size()); }	you can use the mapping or fieldmapping static methods to avoid a lot of this ceremony around json builders
private NodesToAllocate buildNodesToAllocate(RoutingAllocation allocation, List<NodeGatewayStartedShards> nodeShardStates, ShardRouting shardRouting, boolean forceAllocate) { List<DecidedNode> yesNodeShards = new ArrayList<>(); List<DecidedNode> throttledNodeShards = new ArrayList<>(); List<DecidedNode> noNodeShards = new ArrayList<>(); for (NodeGatewayStartedShards nodeShardState : nodeShardStates) { RoutingNode node = allocation.routingNodes().node(nodeShardState.getNode().getId()); if (node == null) { continue; } Decision decision = forceAllocate ? allocation.deciders().canForceAllocatePrimary(shardRouting, node, allocation) : allocation.deciders().canAllocate(shardRouting, node, allocation); DecidedNode decidedNode = new DecidedNode(nodeShardState, decision); if (decision.type() == Type.THROTTLE) { throttledNodeShards.add(decidedNode); } else if (decision.type() == Type.NO) { noNodeShards.add(decidedNode); } else { yesNodeShards.add(decidedNode); } } return new NodesToAllocate(Collections.unmodifiableList(yesNodeShards), Collections.unmodifiableList(throttledNodeShards), Collections.unmodifiableList(noNodeShards)); }	i believe here you will also want to check out allocation.debugdecision() and setting the flag on the allocation to ensure that the deciders are not short-circuited in the case where the explain flag is set
private static boolean isResponsibleFor(final ShardRouting shard) { return shard.primary() // must be primary && shard.unassigned() // must be unassigned // only handle either an existing store or a snapshot recovery && (shard.recoverySource().getType() == RecoverySource.Type.EXISTING_STORE || shard.recoverySource().getType() == RecoverySource.Type.SNAPSHOT); }	i like that this is explicit in the file, i think it makes reading the code easier
public void processExistingRecoveries(RoutingAllocation allocation) { MetaData metaData = allocation.metaData(); RoutingNodes routingNodes = allocation.routingNodes(); List<Runnable> shardCancellationActions = new ArrayList<>(); for (RoutingNode routingNode : routingNodes) { for (ShardRouting shard : routingNode) { if (shard.primary() == true) { continue; } if (shard.initializing() == false) { continue; } if (shard.relocatingNodeId() != null) { continue; } // if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one... if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) { continue; } AsyncShardFetch.FetchResult<TransportNodesListShardStoreMetaData.NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation); if (shardStores.hasData() == false) { logger.trace("{}: fetching new stores for initializing shard", shard); continue; // still fetching } ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId()); assert primaryShard != null : "the replica shard can be allocated on at least one node, so there must be an active primary"; TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryShard, allocation, shardStores); if (primaryStore == null) { // if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed) // just let the recovery find it out, no need to do anything about it for the initializing shard logger.trace("{}: no primary shard store found or allocated, letting actual allocation figure it out", shard); continue; } MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, primaryStore, shardStores, false); if (matchingNodes.getNodeWithHighestMatch() != null) { DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId()); DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch(); // current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider final String currentSyncId; if (shardStores.getData().containsKey(currentNode)) { currentSyncId = shardStores.getData().get(currentNode).storeFilesMetaData().syncId(); } else { currentSyncId = null; } if (currentNode.equals(nodeWithHighestMatch) == false && Objects.equals(currentSyncId, primaryStore.syncId()) == false && matchingNodes.isNodeMatchBySyncID(nodeWithHighestMatch) == true) { // we found a better match that has a full sync id match, the existing allocation is not fully synced // so we found a better one, cancel this one logger.debug("cancelling allocation of replica on [{}], sync id match found on node [{}]", currentNode, nodeWithHighestMatch); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + currentNode + "] cancelled, sync id match found on node ["+ nodeWithHighestMatch + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT); // don't cancel shard in the loop as it will cause a ConcurrentModificationException shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } } } } for (Runnable action : shardCancellationActions) { action.run(); } } /** * Is the allocator responsible for allocating the given {@link ShardRouting}	this explanation doesn't include nodes returning throttle
public void processExistingRecoveries(RoutingAllocation allocation) { MetaData metaData = allocation.metaData(); RoutingNodes routingNodes = allocation.routingNodes(); List<Runnable> shardCancellationActions = new ArrayList<>(); for (RoutingNode routingNode : routingNodes) { for (ShardRouting shard : routingNode) { if (shard.primary() == true) { continue; } if (shard.initializing() == false) { continue; } if (shard.relocatingNodeId() != null) { continue; } // if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one... if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) { continue; } AsyncShardFetch.FetchResult<TransportNodesListShardStoreMetaData.NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation); if (shardStores.hasData() == false) { logger.trace("{}: fetching new stores for initializing shard", shard); continue; // still fetching } ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId()); assert primaryShard != null : "the replica shard can be allocated on at least one node, so there must be an active primary"; TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryShard, allocation, shardStores); if (primaryStore == null) { // if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed) // just let the recovery find it out, no need to do anything about it for the initializing shard logger.trace("{}: no primary shard store found or allocated, letting actual allocation figure it out", shard); continue; } MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, primaryStore, shardStores, false); if (matchingNodes.getNodeWithHighestMatch() != null) { DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId()); DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch(); // current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider final String currentSyncId; if (shardStores.getData().containsKey(currentNode)) { currentSyncId = shardStores.getData().get(currentNode).storeFilesMetaData().syncId(); } else { currentSyncId = null; } if (currentNode.equals(nodeWithHighestMatch) == false && Objects.equals(currentSyncId, primaryStore.syncId()) == false && matchingNodes.isNodeMatchBySyncID(nodeWithHighestMatch) == true) { // we found a better match that has a full sync id match, the existing allocation is not fully synced // so we found a better one, cancel this one logger.debug("cancelling allocation of replica on [{}], sync id match found on node [{}]", currentNode, nodeWithHighestMatch); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + currentNode + "] cancelled, sync id match found on node ["+ nodeWithHighestMatch + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT); // don't cancel shard in the loop as it will cause a ConcurrentModificationException shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } } } } for (Runnable action : shardCancellationActions) { action.run(); } } /** * Is the allocator responsible for allocating the given {@link ShardRouting}	how did tab indentation get into our codebase! :)
public void processExistingRecoveries(RoutingAllocation allocation) { MetaData metaData = allocation.metaData(); RoutingNodes routingNodes = allocation.routingNodes(); List<Runnable> shardCancellationActions = new ArrayList<>(); for (RoutingNode routingNode : routingNodes) { for (ShardRouting shard : routingNode) { if (shard.primary() == true) { continue; } if (shard.initializing() == false) { continue; } if (shard.relocatingNodeId() != null) { continue; } // if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one... if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) { continue; } AsyncShardFetch.FetchResult<TransportNodesListShardStoreMetaData.NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation); if (shardStores.hasData() == false) { logger.trace("{}: fetching new stores for initializing shard", shard); continue; // still fetching } ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId()); assert primaryShard != null : "the replica shard can be allocated on at least one node, so there must be an active primary"; TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryShard, allocation, shardStores); if (primaryStore == null) { // if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed) // just let the recovery find it out, no need to do anything about it for the initializing shard logger.trace("{}: no primary shard store found or allocated, letting actual allocation figure it out", shard); continue; } MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, primaryStore, shardStores, false); if (matchingNodes.getNodeWithHighestMatch() != null) { DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId()); DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch(); // current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider final String currentSyncId; if (shardStores.getData().containsKey(currentNode)) { currentSyncId = shardStores.getData().get(currentNode).storeFilesMetaData().syncId(); } else { currentSyncId = null; } if (currentNode.equals(nodeWithHighestMatch) == false && Objects.equals(currentSyncId, primaryStore.syncId()) == false && matchingNodes.isNodeMatchBySyncID(nodeWithHighestMatch) == true) { // we found a better match that has a full sync id match, the existing allocation is not fully synced // so we found a better one, cancel this one logger.debug("cancelling allocation of replica on [{}], sync id match found on node [{}]", currentNode, nodeWithHighestMatch); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + currentNode + "] cancelled, sync id match found on node ["+ nodeWithHighestMatch + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT); // don't cancel shard in the loop as it will cause a ConcurrentModificationException shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } } } } for (Runnable action : shardCancellationActions) { action.run(); } } /** * Is the allocator responsible for allocating the given {@link ShardRouting}	unallocated copy of the shard is a little confusing here, maybe "has an existing copy of the shard", since we want people that aren't familiar with es terminology to hopefully get benefit from the explanations
public void processExistingRecoveries(RoutingAllocation allocation) { MetaData metaData = allocation.metaData(); RoutingNodes routingNodes = allocation.routingNodes(); List<Runnable> shardCancellationActions = new ArrayList<>(); for (RoutingNode routingNode : routingNodes) { for (ShardRouting shard : routingNode) { if (shard.primary() == true) { continue; } if (shard.initializing() == false) { continue; } if (shard.relocatingNodeId() != null) { continue; } // if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one... if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) { continue; } AsyncShardFetch.FetchResult<TransportNodesListShardStoreMetaData.NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation); if (shardStores.hasData() == false) { logger.trace("{}: fetching new stores for initializing shard", shard); continue; // still fetching } ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId()); assert primaryShard != null : "the replica shard can be allocated on at least one node, so there must be an active primary"; TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryShard, allocation, shardStores); if (primaryStore == null) { // if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed) // just let the recovery find it out, no need to do anything about it for the initializing shard logger.trace("{}: no primary shard store found or allocated, letting actual allocation figure it out", shard); continue; } MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, primaryStore, shardStores, false); if (matchingNodes.getNodeWithHighestMatch() != null) { DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId()); DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch(); // current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider final String currentSyncId; if (shardStores.getData().containsKey(currentNode)) { currentSyncId = shardStores.getData().get(currentNode).storeFilesMetaData().syncId(); } else { currentSyncId = null; } if (currentNode.equals(nodeWithHighestMatch) == false && Objects.equals(currentSyncId, primaryStore.syncId()) == false && matchingNodes.isNodeMatchBySyncID(nodeWithHighestMatch) == true) { // we found a better match that has a full sync id match, the existing allocation is not fully synced // so we found a better one, cancel this one logger.debug("cancelling allocation of replica on [{}], sync id match found on node [{}]", currentNode, nodeWithHighestMatch); UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, "existing allocation of replica to [" + currentNode + "] cancelled, sync id match found on node ["+ nodeWithHighestMatch + "]", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT); // don't cancel shard in the loop as it will cause a ConcurrentModificationException shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes())); } } } } for (Runnable action : shardCancellationActions) { action.run(); } } /** * Is the allocator responsible for allocating the given {@link ShardRouting}	i think the wording is a little complex here, maybe: "not allocating this shard, no nodes contain data for the replica and allocation is delayed"
static Request fieldCaps(FieldCapabilitiesRequest fieldCapabilitiesRequest) throws IOException { String methodName = fieldCapabilitiesRequest.indexFilter() != null ? HttpPut.METHOD_NAME : HttpGet.METHOD_NAME; Request request = new Request(methodName, endpoint(fieldCapabilitiesRequest.indices(), "_field_caps")); Params params = new Params(); params.withFields(fieldCapabilitiesRequest.fields()); params.withIndicesOptions(fieldCapabilitiesRequest.indicesOptions()); request.addParameters(params.asMap()); if (fieldCapabilitiesRequest.indexFilter() != null) { request.setEntity(createEntity(fieldCapabilitiesRequest, REQUEST_BODY_CONTENT_TYPE)); } return request; }	put? shouldn't it be a post?
protected NetworkPartition addRandomIsolation(String isolatedNode) { Set<String> side1 = new HashSet<>(); Set<String> side2 = new HashSet<>(Arrays.asList(cluster().getNodeNames())); side1.add(isolatedNode); side2.remove(isolatedNode); NetworkPartition partition; if (randomBoolean()) { partition = new NetworkUnresponsivePartition(side1, side2, getRandom()); } else { partition = new NetworkDisconnectPartition(side1, side2, getRandom()); } cluster().setDisruptionScheme(partition); return partition; }	maybe this method should be in the testcluster?
private void assertQueryWithAllFieldsWildcard(Query query) { assertEquals(DisjunctionMaxQuery.class, query.getClass()); DisjunctionMaxQuery disjunctionMaxQuery = (DisjunctionMaxQuery) query; int noMatchNoDocsQueries = 0; for (Query q : disjunctionMaxQuery.getDisjuncts()) { if (q.getClass() == MatchNoDocsQuery.class) { noMatchNoDocsQueries++; } } assertEquals(11, noMatchNoDocsQueries); assertThat(disjunctionMaxQuery.getDisjuncts(), hasItems(new TermQuery(new Term(STRING_FIELD_NAME, "hello")), new TermQuery(new Term(STRING_FIELD_NAME_2, "hello")))); }	small suggestion: i think we have 11 clauses because that its the number of fields defined in abstractbuildertestcase#mapped_leaf_field_names. to make the test more robust to future changes, maybe use that array size here?
public SingleNodeShutdownMetadata build() { if (startedAtMillis == -1) { throw new IllegalArgumentException("start timestamp must be set"); } TimeValue delayOrDefault = shardReallocationDelay; if (Type.RESTART.equals(type) && delayOrDefault == null) { delayOrDefault = DEFAULT_RESTART_SHARD_ALLOCATION_DELAY; } return new SingleNodeShutdownMetadata( nodeId, type, reason, startedAtMillis, delayOrDefault ); }	this feels like the wrong place to implement this default, i think rather than here, it should go into the getter inside of singlenodeshutdownmetadata, otherwise calling toxcontent on the metadata makes it appear that it has been set (when in reality the default value will be used)
*/ public long getRemainingDelay( final long nanoTimeNow, final Settings indexSettings, final Map<String, SingleNodeShutdownMetadata> nodesShutdownMap ) { Map<String, SingleNodeShutdownMetadata> nodeShutdowns = nodesShutdownMap != null ? nodesShutdownMap : Collections.emptyMap(); long delayTimeoutNanos = Optional.ofNullable(lastAllocatedNodeId) .map(nodeShutdowns::get) .filter(shutdownMetadata -> SingleNodeShutdownMetadata.Type.RESTART.equals(shutdownMetadata.getType())) .map(SingleNodeShutdownMetadata::getShardReallocationDelay) .map(TimeValue::nanos) .orElse(INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.get(indexSettings).nanos()); assert nanoTimeNow >= unassignedTimeNanos; return Math.max(0L, delayTimeoutNanos - (nanoTimeNow - unassignedTimeNanos)); }	i don't think we need this null check here, since metadata#nodeshutdowns() returns an empty map if there are no shutdowns. maybe we can replace it with an assert instead?
@SuppressWarnings("HiddenField") protected SearchResponse executeSearchScrollRequest(String scrollId) { SearchResponse searchResponse = ClientHelper.executeWithHeaders( context.headers, ClientHelper.ML_ORIGIN, client, () -> new SearchScrollRequestBuilder(client, SearchScrollAction.INSTANCE).setScroll(SCROLL_TIMEOUT).setScrollId(scrollId).get() ); checkForSkippedClusters(searchResponse); return searchResponse; }	we don't need to check this for subsequent scroll requests, but it's okay to leave this check as is here.
private Tuple<AccessToken, JWT> buildTokens(JWTClaimsSet idToken, Key key, String alg, String keyId, String subject, boolean withAccessToken, boolean forged) throws Exception { AccessToken accessToken = null; if (withAccessToken) { accessToken = new BearerAccessToken(Base64.getUrlEncoder().encodeToString(randomByteArrayOfLength(32))); AccessTokenHash expectedHash = AccessTokenHash.compute(accessToken, JWSAlgorithm.parse(alg)); Map<String, Object> idTokenMap = idToken.toJSONObject(); idTokenMap.put("at_hash", expectedHash.getValue()); // This is necessary as if nonce claim is of type Nonce, the library won't take it into consideration when serializing the JWT idTokenMap.put("nonce", idTokenMap.get("nonce").toString()); idToken = JWTClaimsSet.parse(idTokenMap); } SignedJWT jwt = new SignedJWT( new JWSHeader.Builder(JWSAlgorithm.parse(alg)).keyID(keyId).build(), idToken); if (key instanceof RSAPrivateKey) { jwt.sign(new RSASSASigner((PrivateKey) key)); } else if (key instanceof ECPrivateKey) { jwt.sign(new ECDSASigner((ECPrivateKey) key)); } else if (key instanceof SecretKey) { jwt.sign(new MACSigner((SecretKey) key)); } if (forged) { // Change the sub claim to "attacker" String[] serializedParts = jwt.serialize().split("\\\\\\\\."); String legitimatePayload = new String(Base64.getUrlDecoder().decode(serializedParts[1]), StandardCharsets.UTF_8); String forgedPayload = legitimatePayload.replace(subject, "attacker"); String encodedForgedPayload = Base64.getUrlEncoder().withoutPadding().encodeToString(forgedPayload.getBytes(StandardCharsets.UTF_8)); String fordedTokenString = serializedParts[0] + "." + encodedForgedPayload + "." + serializedParts[2]; jwt = SignedJWT.parse(fordedTokenString); } return new Tuple<>(accessToken, jwt); }	could you please explain a bit more about this? is it not needed before the version upgrade? since it is necessary now, do we need somehow reflect this in production code?
public void testGetSettings() throws Exception { RestHighLevelClient client = highLevelClient(); { Settings settings = Settings.builder().put("number_of_shards", 3).build(); CreateIndexResponse createIndexResponse = client.indices().create(new CreateIndexRequest("index", settings)); assertTrue(createIndexResponse.isAcknowledged()); } // tag::get-settings-request GetSettingsRequest request = new GetSettingsRequest().indices("index"); // tag::get-settings-request // tag::get-settings-request-names request.names("index.number_of_shards"); // <1> // end::get-settings-request-names // tag::get-settings-request-indicesOptions request.indicesOptions(IndicesOptions.lenientExpandOpen()); // <1> // end::get-settings-request-indicesOptions // tag::get-settings-execute GetSettingsResponse getSettingsResponse = client.indices().getSettings(request); // end::get-settings-execute // tag::get-settings-response String numberOfShardsString = getSettingsResponse.getSetting("index", "index.number_of_shards"); // <1> Settings indexSettings = getSettingsResponse.getIndexToSettings().get("index"); // <2> Integer numberOfShards = indexSettings.getAsInt("index.number_of_shards", null); // <3> // end::get-settings-response assertEquals("3", numberOfShardsString); assertEquals(Integer.valueOf(3), numberOfShards); assertNull("refresh_interval returned but was never set!", getSettingsResponse.getSetting("index", "index.refresh_interval")); // tag::get-settings-execute-listener ActionListener<GetSettingsResponse> listener = new ActionListener<GetSettingsResponse>() { @Override public void onResponse(GetSettingsResponse GetSettingsResponse) { // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::get-settings-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::get-settings-execute-async client.indices().getSettingsAsync(request, listener); // <1> // end::get-settings-execute-async assertTrue(latch.await(30L, TimeUnit.SECONDS)); }	this should be an end tag ;)
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; GetSettingsRequest that = (GetSettingsRequest) o; return humanReadable == that.humanReadable && includeDefaults == that.includeDefaults && Arrays.equals(indices, that.indices) && Objects.equals(indicesOptions, that.indicesOptions) && Arrays.equals(names, that.names); }	remove this empty line?
private static String toCamelCase(String s) { Matcher m = UNDERSCORE_THEN_ANYTHING.matcher(s); StringBuilder sb = new StringBuilder(); while (m.find()) { m.appendReplacement(sb, m.group(1).toUpperCase()); } m.appendTail(sb); sb.setCharAt(0, Character.toUpperCase(sb.charAt(0))); return sb.toString(); }	this one has the same problem with java 8.
public Object getValue(String key) throws IOException { if (key.charAt(0) == '$' && key.charAt(1) != '{') { return unstash(key.substring(1)); } Matcher matcher = EXTENDED_KEY.matcher(key); /* * String*Buffer* because that is what the Matcher API takes. In modern versions of java the uncontended synchronization is very, * very cheap so that should not be a problem. */ StringBuilder result = new StringBuilder(key.length()); if (false == matcher.find()) { throw new IllegalArgumentException("Doesn't contain any stash keys [" + key + "]"); } do { matcher.appendReplacement(result, Matcher.quoteReplacement(unstash(matcher.group(1)).toString())); }	this comment looks obsolete
public Object getValue(String key) throws IOException { if (key.charAt(0) == '$' && key.charAt(1) != '{') { return unstash(key.substring(1)); } Matcher matcher = EXTENDED_KEY.matcher(key); /* * String*Buffer* because that is what the Matcher API takes. In modern versions of java the uncontended synchronization is very, * very cheap so that should not be a problem. */ StringBuilder result = new StringBuilder(key.length()); if (false == matcher.find()) { throw new IllegalArgumentException("Doesn't contain any stash keys [" + key + "]"); } do { matcher.appendReplacement(result, Matcher.quoteReplacement(unstash(matcher.group(1)).toString())); }	this one also has the same problem with java 8.
private Object getValue(List<Object> path, String key) throws IOException { Matcher matcher = PATH.matcher(key); if (false == matcher.find()) { return getValue(key); } StringBuilder pathBuilder = new StringBuilder(); Iterator<Object> element = path.iterator(); if (element.hasNext()) { pathBuilder.append(element.next().toString().replace(".", "\\\\\\\\.")); while (element.hasNext()) { pathBuilder.append('.'); pathBuilder.append(element.next().toString().replace(".", "\\\\\\\\.")); } } String builtPath = Matcher.quoteReplacement(pathBuilder.toString()); StringBuilder newKey = new StringBuilder(key.length()); do { matcher.appendReplacement(newKey, builtPath); } while (matcher.find()); matcher.appendTail(newKey); return getValue(newKey.toString()); }	this one also uses a java 9 method.
private String validateAdd(DiscoveryNode node) { for (ObjectCursor<DiscoveryNode> cursor : nodes.values()) { final DiscoveryNode existingNode = cursor.value; if (node.getAddress().equals(existingNode.getAddress()) && node.getId().equals(existingNode.getId()) == false) { return "can't add node " + node + ", found existing node " + existingNode + " with same address"; } if (node.getId().equals(existingNode.getId()) && node.equals(existingNode) == false) { return "can't add node " + node + ", found existing node " + existingNode + " with the same id is a different node instance"; } } return null; }	s/same id is/same id but it is/
public RoutingAllocation.Result applyFailedShards(ClusterState clusterState, List<FailedRerouteAllocation.FailedShard> failedShards) { RoutingNodes routingNodes = getMutableRoutingNodes(clusterState); // shuffle the unassigned nodes, just so we won't have things like poison failed shards routingNodes.unassigned().shuffle(); long currentNanoTime = currentNanoTime(); FailedRerouteAllocation allocation = new FailedRerouteAllocation(allocationDeciders, routingNodes, clusterState, failedShards, clusterInfoService.getClusterInfo(), currentNanoTime); boolean changed = false; // as failing primaries also fail associated replicas, we fail replicas first here so that their nodes are added to ignore list List<FailedRerouteAllocation.FailedShard> orderedFailedShards = new ArrayList<>(failedShards); orderedFailedShards.sort(Comparator.comparing(failedShard -> failedShard.shard.primary())); for (FailedRerouteAllocation.FailedShard failedShard : orderedFailedShards) { UnassignedInfo unassignedInfo = failedShard.shard.unassignedInfo(); final int failedAllocations = unassignedInfo != null ? unassignedInfo.getNumFailedAllocations() : 0; changed |= applyFailedShard(allocation, failedShard.shard, true, new UnassignedInfo(UnassignedInfo.Reason.ALLOCATION_FAILED, failedShard.message, failedShard.failure, failedAllocations + 1, currentNanoTime, System.currentTimeMillis(), false, AllocationStatus.NO_ATTEMPT)); } if (!changed) { return new RoutingAllocation.Result(false, clusterState.routingTable(), clusterState.metaData()); } gatewayAllocator.applyFailedShards(allocation); // elect primaries *before* allocating unassigned, so backups of primaries that failed // will be moved to primary state and not wait for primaries to be allocated and recovered (*from gateway*) electPrimariesAndUnassignedDanglingReplicas(allocation); reroute(allocation); String failedShardsAsString = firstListElementsToCommaDelimitedString(failedShards, s -> s.shard.shardId().toString()); return buildResultAndLogHealthChange(allocation, "shards failed [" + failedShardsAsString + "] ..."); }	how about adding an overloaded version of the method where reroute is true (like we have for startedshards / failedshards)? alternatively, we could make it even more explicit when reroute is not called, i.e. have deassociatedeadnodes always do the reroute and add a method deassociatedeadnodeswithoutreroute for the rare cases where we don't reroute.
private boolean reroute(RoutingAllocation allocation) { boolean changed = false; assert deassociateDeadNodes(allocation) == false : "dead nodes should be explicitly cleaned up. See deassociateDeadNodes"; assert electPrimariesAndUnassignedDanglingReplicas(allocation) == false: "unassigned primaries with assigned replicas on reroute: " + allocation.routingNodes().prettyPrint(); // now allocate all the unassigned to available nodes if (allocation.routingNodes().unassigned().size() > 0) { changed |= removeDelayMarkers(allocation); changed |= gatewayAllocator.allocateUnassigned(allocation); } changed |= shardsAllocator.allocate(allocation); assert RoutingNodes.assertShardStats(allocation.routingNodes()); return changed; }	i was wondering why this assertion didn't trip when we cancel a primary shard using cancelallocationcommand (we even have a test for that -> allocationcommandtests.testcancelcommand, the last lines of the test). after investigation, i noticed that the electprimariesandunassigneddanglingreplicas method does not correctly set the changed flag to true when candidate != null (it currently only does if primaryswappedcandidate.relocatingnodeid() != null or indexmetadata.isindexusingshadowreplicas(index.getsettings()) but it should do it always when candidate != null).
private ClusterState removeNodes(ClusterState clusterState, AllocationService service, int numNodes) { logger.info("Removing [{}] nodes", numNodes); DiscoveryNodes.Builder nodes = DiscoveryNodes.builder(clusterState.nodes()); ArrayList<DiscoveryNode> discoveryNodes = CollectionUtils.iterableAsArrayList(clusterState.nodes()); Collections.shuffle(discoveryNodes, random()); boolean removed = false; for (DiscoveryNode node : discoveryNodes) { nodes.remove(node.getId()); removed = true; numNodes--; if (numNodes <= 0) { break; } } clusterState = ClusterState.builder(clusterState).nodes(nodes.build()).build(); if (removed) { clusterState = ClusterState.builder(clusterState) .routingResult(service.deassociateDeadNodes(clusterState, true, "reroute")).build(); } RoutingNodes routingNodes = clusterState.getRoutingNodes(); logger.info("start all the primary shards, replicas will start initializing"); RoutingTable routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); routingNodes = clusterState.getRoutingNodes(); logger.info("start the replica shards"); routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); routingNodes = clusterState.getRoutingNodes(); logger.info("rebalancing"); routingTable = service.reroute(clusterState, "reroute").routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); routingNodes = clusterState.getRoutingNodes(); logger.info("complete rebalancing"); RoutingTable prev = routingTable; while (true) { logger.debug("ClusterState: {}", clusterState.getRoutingNodes().prettyPrint()); routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable(); clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build(); routingNodes = clusterState.getRoutingNodes(); if (routingTable == prev) break; prev = routingTable; } return clusterState; }	instead of introducing boolean flag, maybe just discoverynodes.isempty() == false
public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Longs(sortedNumericDocValues), name); } } public static class ToSeqNoScriptField extends ToScriptField { public static final ToSeqNoScriptField INSTANCE = new ToSeqNoScriptField(); private ToSeqNoScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Longs(sortedNumericDocValues), name); } } public static class ToBooleanScriptField extends ToScriptField { public static final ToBooleanScriptField INSTANCE = new ToBooleanScriptField(); private ToBooleanScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Booleans(sortedNumericDocValues), name); } } public static class ToByteScriptField extends ToScriptField { public static final ToByteScriptField INSTANCE = new ToByteScriptField(); private ToByteScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Longs(sortedNumericDocValues), name); } } public static class ToShortScriptField extends ToScriptField { public static final ToShortScriptField INSTANCE = new ToShortScriptField(); private ToShortScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Longs(sortedNumericDocValues), name); } } public static class ToIntScriptField extends ToScriptField { public static final ToIntScriptField INSTANCE = new ToIntScriptField(); private ToIntScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Longs(sortedNumericDocValues), name); } } public static class ToLongScriptField extends ToScriptField { public static final ToLongScriptField INSTANCE = new ToLongScriptField(); private ToLongScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Longs(sortedNumericDocValues), name); } } public static class ToHalfFloatScriptField extends ToScriptField { public static final ToHalfFloatScriptField INSTANCE = new ToHalfFloatScriptField(); private ToHalfFloatScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDoubleValues sortedNumericDoubleValues, String name) { return new DelegateDocValuesField(new Doubles(sortedNumericDoubleValues), name); } } public static class ToFloatScriptField extends ToScriptField { public static final ToFloatScriptField INSTANCE = new ToFloatScriptField(); private ToFloatScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDoubleValues sortedNumericDoubleValues, String name) { return new DelegateDocValuesField(new Doubles(sortedNumericDoubleValues), name); } } public static class ToScaledFloatScriptField extends ToScriptField { public static final ToScaledFloatScriptField INSTANCE = new ToScaledFloatScriptField(); private ToScaledFloatScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDoubleValues sortedNumericDoubleValues, String name) { return new DelegateDocValuesField(new Doubles(sortedNumericDoubleValues), name); } } public static class ToDoubleScriptField extends ToScriptField { public static final ToDoubleScriptField INSTANCE = new ToDoubleScriptField(); private ToDoubleScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDoubleValues sortedNumericDoubleValues, String name) { return new DelegateDocValuesField(new Doubles(sortedNumericDoubleValues), name); } } public static class ToDateMillisScriptField extends ToScriptField { public static final ToDateMillisScriptField INSTANCE = new ToDateMillisScriptField(); private ToDateMillisScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Dates(sortedNumericDocValues, false), name); } } public static class ToDateNanosScriptField extends ToScriptField { public static final ToDateNanosScriptField INSTANCE = new ToDateNanosScriptField(); private ToDateNanosScriptField() { } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { return new DelegateDocValuesField(new Dates(sortedNumericDocValues, true), name); } } public DocValuesField<?> getScriptField(SortedNumericDocValues sortedNumericDocValues, String name) { throw new UnsupportedOperationException(); }	i wonder if we could use generics to collapse this down to a single method? then this could be a functional interface which would make a lot of this clearer; something like public interface toscriptfield<t> { public docvaluesfield<?> getscriptfield(t docvalues, string name); final toscriptfield<sortednumericdocvalues> date_nanos = (v, n) -> return new delegatedocvaluesfield(new dates(v, true), n); }
*/ public void changePasswordAsync(ChangePasswordRequest request, RequestOptions options, ActionListener<EmptyResponse> listener) { restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::changePassword, options, EmptyResponse::fromXContent, listener, emptySet()); } /** * Delete a role mapping. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role-mapping.html"> * the docs</a> for more. * @param request the request with the role mapping information to be deleted. * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	s/role mapping information/role mapping name/
* @throws IOException in case there is a problem sending the request or parsing back the response */ public DeleteRoleMappingResponse deleteRoleMapping(DeleteRoleMappingRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequestAndParseEntity(request, SecurityRequestConverters::deleteRoleMapping, options, DeleteRoleMappingResponse::fromXContent, emptySet()); } /** * Asynchronously delete a role mapping. * See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role-mapping.html"> * the docs</a> for more. * @param request the request with the role mapping information to be deleted. * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	s/role mapping information/role mapping name/
public static Logger getLogger(String prefix, Logger logger) { /* * In a followup we'll throw an exception if prefix is null or empty * redirecting folks to LogManager.getLogger. * * This and more is tracker in https://github.com/elastic/elasticsearch/issues/32174 */ if (prefix == null || prefix.length() == 0) { return logger; } return new PrefixLogger((ExtendedLogger)logger, logger.getName(), prefix); }	typo: tracker -> tracked
@Override protected void parseCreateField(ParseContext context) throws IOException { BytesRef id = Uid.encodeId(context.sourceToParse().id()); context.doc().add(new Field(NAME, id, Defaults.FIELD_TYPE)); }	i wonder if ever allowed disabling indexing the id! oh well, :+1:
public void registerLicense(final PutLicenseRequest request, final ActionListener<PutLicenseResponse> listener) { final License newLicense = request.license(); final long now = clock.millis(); if (!LicenseVerifier.verifyLicense(newLicense) || newLicense.issueDate() > now || newLicense.startDate() > now) { listener.onResponse(new PutLicenseResponse(true, LicensesStatus.INVALID)); return; } final License.LicenseType licenseType; try { licenseType = License.LicenseType.resolve(newLicense); } catch (Exception e) { listener.onFailure(e); return; } if (licenseType == License.LicenseType.BASIC) { listener.onFailure(new IllegalArgumentException("Registering basic licenses is not allowed.")); } else if (isAllowedLicenseType(licenseType) == false) { listener.onFailure(new IllegalArgumentException( "Registering [" + licenseType.getTypeName() + "] licenses is not allowed on this cluster")); } else if (newLicense.expiryDate() < now) { listener.onResponse(new PutLicenseResponse(true, LicensesStatus.EXPIRED)); } else { if (!request.acknowledged()) { // TODO: ack messages should be generated on the master, since another node's cluster state may be behind... final License currentLicense = getLicense(); if (currentLicense != null) { Map<String, String[]> acknowledgeMessages = getAckMessages(newLicense, currentLicense); if (acknowledgeMessages.isEmpty() == false) { // needs acknowledgement listener.onResponse(new PutLicenseResponse(false, LicensesStatus.VALID, ACKNOWLEDGEMENT_HEADER, acknowledgeMessages)); return; } } } // This check would be incorrect if "basic" licenses were allowed here // because the defaults there mean that security can be "off", even if the setting is "on" // BUT basic licenses are explicitly excluded earlier in this method, so we don't need to worry if (XPackSettings.SECURITY_ENABLED.get(settings)) { // TODO we should really validate that all nodes have xpack installed and are consistently configured but this // should happen on a different level and not in this code if (XPackLicenseState.isTransportTlsRequired(newLicense, settings) && XPackSettings.TRANSPORT_SSL_ENABLED.get(settings) == false && isProductionMode(settings, clusterService.localNode())) { // security is on but TLS is not configured we gonna fail the entire request and throw an exception throw new IllegalStateException("Cannot install a [" + newLicense.operationMode() + "] license unless TLS is configured or security is disabled"); } else if (XPackSettings.FIPS_MODE_ENABLED.get(settings) && XPackLicenseState.isFipsAllowedForOperationMode(newLicense.operationMode())) { throw new IllegalStateException("Cannot install a [" + newLicense.operationMode() + "] license unless FIPS mode is disabled"); } } clusterService.submitStateUpdateTask("register license [" + newLicense.uid() + "]", new AckedClusterStateUpdateTask<PutLicenseResponse>(request, listener) { @Override protected PutLicenseResponse newResponse(boolean acknowledged) { return new PutLicenseResponse(acknowledged, LicensesStatus.VALID); } @Override public ClusterState execute(ClusterState currentState) throws Exception { XPackPlugin.checkReadyForXPackCustomMetadata(currentState); final Version oldestNodeVersion = currentState.nodes().getSmallestNonClientNodeVersion(); if (licenseIsCompatible(newLicense, oldestNodeVersion) == false) { throw new IllegalStateException("The provided license is not compatible with node version [" + oldestNodeVersion + "]"); } MetaData currentMetadata = currentState.metaData(); LicensesMetaData licensesMetaData = currentMetadata.custom(LicensesMetaData.TYPE); Version trialVersion = null; if (licensesMetaData != null) { trialVersion = licensesMetaData.getMostRecentTrialVersion(); } MetaData.Builder mdBuilder = MetaData.builder(currentMetadata); mdBuilder.putCustom(LicensesMetaData.TYPE, new LicensesMetaData(newLicense, trialVersion)); return ClusterState.builder(currentState).metaData(mdBuilder).build(); } }); } }	~~address feedback https://github.com/elastic/elasticsearch/pull/51864#discussion_r376830270~~ see https://github.com/elastic/elasticsearch/pull/52118#discussion_r376853676
@Override public InputStream readBlob(String blobName) throws IOException { if (!blobExists(blobName)) { throw new NoSuchFileException("Blob [" + blobName + "] does not exist"); } // FSDataInputStream does buffering internally return store.execute((Operation<InputStream>) fileContext -> { FSDataInputStream is = fileContext.open(new Path(path, blobName), bufferSize); return new InputStream() { @Override public int read() throws IOException { try { SpecialPermission.check(); // FSDataInputStream can open connection on read() return AccessController.doPrivileged((PrivilegedExceptionAction<Integer>) is::read); } catch (PrivilegedActionException e) { throw (IOException) e.getCause(); } } }; }); }	can we maybe try to trigger this differently? i mean can we for instance try to call #available() or can we maybe read the first byte on open and wrap in a bufferedinputstream and then do this: java inputstream stream = is.ismarksupported() ? is : new bufferedinputstream(is); // do the following in doprivileged? stream.mark(1); stream.skip(1); stream.reset(); return stream;
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeInt(threads); out.writeBoolean(ignoreIdleThreads); out.writeEnum(type); out.writeTimeValue(interval); out.writeInt(snapshots); }	you'll need a similar version conditional here to match the read conditional.
private void movePlugin(Path tmpRoot, Path destination) throws IOException { Files.move(tmpRoot, destination, StandardCopyOption.ATOMIC_MOVE); Files.walkFileTree(destination, new SimpleFileVisitor<Path>() { @Override public FileVisitResult visitFile(final Path file, final BasicFileAttributes attrs) throws IOException { final String parentDirName = file.getParent().getFileName().toString(); if ("bin".equals(parentDirName)) { setFileAttributes(file, BIN_FILES_PERMS); } else if (Constants.MAC_OS_X && "MacOS".equals(parentDirName)) { // MacOS is an alternative to bin on macOS setFileAttributes(file, BIN_FILES_PERMS); } else { setFileAttributes(file, PLUGIN_FILES_PERMS); } return FileVisitResult.CONTINUE; } @Override public FileVisitResult postVisitDirectory(final Path dir, final IOException exc) throws IOException { setFileAttributes(dir, PLUGIN_DIR_PERMS); return FileVisitResult.CONTINUE; } }); } /** Copies the files from {@code tmpBinDir} into {@code destBinDir}	same comment here, i wonder if this would be better suited as an || with the if.
public Map<String, IndexStats> getIndices() { if (indicesStats != null) { return indicesStats; } final Map<String, IndexStatsBuilder> indicesStatsBuilder = new HashMap<>(); for (ShardStats shard : shards) { Index index = shard.getShardRouting().index(); String indexName = index.getName(); IndexStatsBuilder indicesStatsBuildr = indicesStatsBuilder.computeIfAbsent(indexName, k -> new IndexStatsBuilder(k, index.getUUID())); indicesStatsBuildr.add(shard); } indicesStats = indicesStatsBuilder.entrySet().stream() .collect(Collectors.toMap(Map.Entry::getKey, entry -> entry.getValue().build())); return indicesStats; }	since this is only used once now, does it make sense to call index.getname() inline and remove this local variable?
public Map<String, IndexStats> getIndices() { if (indicesStats != null) { return indicesStats; } final Map<String, IndexStatsBuilder> indicesStatsBuilder = new HashMap<>(); for (ShardStats shard : shards) { Index index = shard.getShardRouting().index(); String indexName = index.getName(); IndexStatsBuilder indicesStatsBuildr = indicesStatsBuilder.computeIfAbsent(indexName, k -> new IndexStatsBuilder(k, index.getUUID())); indicesStatsBuildr.add(shard); } indicesStats = indicesStatsBuilder.entrySet().stream() .collect(Collectors.toMap(Map.Entry::getKey, entry -> entry.getValue().build())); return indicesStats; }	i find this naming misleading. indicesstatsbuildr looks awfully similar to the hashmap indicesstatsbuilder, but without an e. i think the hashmap could use a different name that reflects its data-structure. what do you think?
public void testEarlyTermination() throws Exception { SearchResponse searchResponse = client().prepareSearch("idx") .setTrackTotalHits(false) .setQuery(matchAllQuery()) .addAggregation(max("max").field("values")) .addAggregation(count("count").field("values")) .get(); Max max = searchResponse.getAggregations().get("max"); assertThat(max, notNullValue()); assertThat(max.getName(), equalTo("max")); assertThat(max.getValue(), equalTo(12.0)); ValueCount count = searchResponse.getAggregations().get("count"); assertThat(count.getName(), equalTo("count")); assertThat(count.getValue(), equalTo(20L)); }	an unrelated question: is the count agg required here to check for correctness? it looks like count doesn't use the early termination optimization, so wasn't sure if that is being used as a proxy to make sure that other aggs are unaffected, or something? asking because @csoulios is converting the max it to unit tests, and we were discussing the other early termination test last week. i wasn't quite sure the purpose of count here, or if it was included because why not :)
protected static Transition buildTransition(ZoneOffsetTransition transition, LocalTimeOffset previous) { long utcStart = transition.toEpochSecond() * 1000; long offsetBeforeMillis = transition.getOffsetBefore().getTotalSeconds() * 1000; long offsetAfterMillis = transition.getOffsetAfter().getTotalSeconds() * 1000; assert (false == previous instanceof Transition) || ((Transition) previous).startUtcMillis < utcStart : "transition list out of order at [" + previous + "] and [" + transition + "]"; assert previous.millis != offsetAfterMillis : "transition list is has a duplicate at [" + previous + "] and [" + transition + "]"; if (transition.isGap()) { long firstMissingLocalTime = utcStart + offsetBeforeMillis; long firstLocalTimeAfterGap = utcStart + offsetAfterMillis; return new Gap(offsetAfterMillis, previous, utcStart, firstMissingLocalTime, firstLocalTimeAfterGap); } long firstOverlappingLocalTime = utcStart + offsetAfterMillis; long firstNonOverlappingLocalTime = utcStart + offsetBeforeMillis; return new Overlap( offsetAfterMillis, previous, utcStart, firstOverlappingLocalTime, firstNonOverlappingLocalTime, movesBackToPreviousDay(transition) ); }	should we check gethour() too for safety? eg. if such a thing existed as moving from 1am to 11pm the day before it looks like we'd still consider that the transition doesn't move back to the previous day with this code?
public void testQueryRewriteDates() { Client client = client(); assertAcked(client.admin().indices().prepareCreate("index").addMapping("type", "d", "type=date") .setSettings(Settings.builder().put(IndicesRequestCache.INDEX_CACHE_REQUEST_ENABLED_SETTING.getKey(), true) .put(INDEX_REFRESH_INTERVAL_SETTING.getKey(), "-1") // disable automatic refreshes .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)).get()); BulkRequestBuilder bulkBuilder = client().prepareBulk(); bulkBuilder.add(client.prepareIndex("index", "type", "1").setSource("d", "2014-01-01T00:00:00")); bulkBuilder.add(client.prepareIndex("index", "type", "2").setSource("d", "2014-02-01T00:00:00")); bulkBuilder.add(client.prepareIndex("index", "type", "3").setSource("d", "2014-03-01T00:00:00")); bulkBuilder.add(client.prepareIndex("index", "type", "4").setSource("d", "2014-04-01T00:00:00")); bulkBuilder.add(client.prepareIndex("index", "type", "5").setSource("d", "2014-05-01T00:00:00")); bulkBuilder.add(client.prepareIndex("index", "type", "6").setSource("d", "2014-06-01T00:00:00")); bulkBuilder.add(client.prepareIndex("index", "type", "7").setSource("d", "2014-07-01T00:00:00")); bulkBuilder.add(client.prepareIndex("index", "type", "8").setSource("d", "2014-08-01T00:00:00")); bulkBuilder.add(client.prepareIndex("index", "type", "9").setSource("d", "2014-09-01T00:00:00")); BulkResponse actionGet = bulkBuilder.execute().actionGet(); assertThat(actionGet.hasFailures() ? actionGet.buildFailureMessage() : "", actionGet.hasFailures(), equalTo(false)); ensureSearchable("index"); refresh("index"); assertCacheState(client, "index", 0, 0); final SearchResponse r1 = client.prepareSearch("index").setSearchType(SearchType.QUERY_THEN_FETCH).setSize(0) .setQuery(QueryBuilders.rangeQuery("d").gte("2013-01-01T00:00:00").lte("now")) .get(); ElasticsearchAssertions.assertAllSuccessful(r1); assertThat(r1.getHits().getTotalHits(), equalTo(9L)); assertCacheState(client, "index", 0, 1); final SearchResponse r2 = client.prepareSearch("index").setSearchType(SearchType.QUERY_THEN_FETCH).setSize(0) .setQuery(QueryBuilders.rangeQuery("d").gte("2013-01-01T00:00:00").lte("now")) .get(); ElasticsearchAssertions.assertAllSuccessful(r2); assertThat(r2.getHits().getTotalHits(), equalTo(9L)); assertCacheState(client, "index", 1, 1); final SearchResponse r3 = client.prepareSearch("index").setSearchType(SearchType.QUERY_THEN_FETCH).setSize(0) .setQuery(QueryBuilders.rangeQuery("d").gte("2013-01-01T00:00:00").lte("now")) .get(); ElasticsearchAssertions.assertAllSuccessful(r3); assertThat(r3.getHits().getTotalHits(), equalTo(9L)); assertCacheState(client, "index", 2, 1); }	nit: actionget isn't a good name ;)
static Request analyze(AnalyzeRequest request) throws IOException { RequestConverters.EndpointBuilder builder = new RequestConverters.EndpointBuilder(); String index = request.index(); if (index != null) { builder.addPathPart(index); } builder.addPathPartAsIs("_analyze"); Request req = new Request(HttpGet.METHOD_NAME, builder.build()); req.setEntity(RequestConverters.createEntity(request, RequestConverters.REQUEST_BODY_CONTENT_TYPE)); return req; }	just double checking that these should be added with all the other oss api, i think it makes sense but let's be sure. does it make sense to add unit testing for these to indicesrequestconverterstests?
static Request freezeIndex(FreezeIndexRequest freezeIndexRequest) { String endpoint = new RequestConverters.EndpointBuilder() .addPathPart(freezeIndexRequest.getIndices()) .addPathPartAsIs("_freeze") .build(); return new Request(HttpPost.METHOD_NAME, endpoint); }	not sure if it should be unfreeze rather than unfreeze?
public void testExecuteAllocateNotCompleteOnlyOneCopyAllocated() throws Exception { Index index = new Index(randomAlphaOfLengthBetween(1, 20), randomAlphaOfLengthBetween(1, 20)); Map<String, String> includes = AllocateActionTests.randomMap(1, 5); Map<String, String> excludes = randomValueOtherThanMany(map -> map.keySet().stream().anyMatch(includes::containsKey), () -> AllocateActionTests.randomMap(1, 5)); Map<String, String> requires = randomValueOtherThanMany(map -> map.keySet().stream().anyMatch(includes::containsKey) || map.keySet().stream().anyMatch(excludes::containsKey), () -> AllocateActionTests.randomMap(1, 5)); Settings.Builder existingSettings = Settings.builder().put(IndexMetadata.SETTING_VERSION_CREATED, Version.CURRENT.id) .put(IndexMetadata.SETTING_INDEX_UUID, index.getUUID()); Settings.Builder node1Settings = Settings.builder(); Settings.Builder node2Settings = Settings.builder(); includes.forEach((k, v) -> { existingSettings.put(IndexMetadata.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + k, v); node1Settings.put(Node.NODE_ATTRIBUTES.getKey() + k, v); }); excludes.forEach((k, v) -> { existingSettings.put(IndexMetadata.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + k, v); }); requires.forEach((k, v) -> { existingSettings.put(IndexMetadata.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getKey() + k, v); node1Settings.put(Node.NODE_ATTRIBUTES.getKey() + k, v); }); boolean primaryOnNode1 = randomBoolean(); IndexRoutingTable.Builder indexRoutingTable = IndexRoutingTable.builder(index) .addShard(TestShardRouting.newShardRouting(new ShardId(index, 0), "node1", primaryOnNode1, ShardRoutingState.STARTED)) .addShard(TestShardRouting.newShardRouting(new ShardId(index, 0), "node2", primaryOnNode1 == false, ShardRoutingState.STARTED)); AllocationRoutedStep step = new AllocationRoutedStep(randomStepKey(), randomStepKey()); logger.info("running test with routing configurations:\\\\n\\\\t includes: [{}]\\\\n\\\\t excludes: [{}]\\\\n\\\\t requires: [{}]", mapToString(includes), mapToString(excludes), mapToString(requires)); assertAllocateStatus(index, 2, 0, step, existingSettings, node1Settings, node2Settings, indexRoutingTable, new ClusterStateWaitStep.Result(false, allShardsActiveAllocationInfo(0, 1))); }	we shouldn't need this function, map.tostring() already returns things in a format like {key=value, food=eggplant}
* @param sslService Realm config for SSL. * @return Initialized HTTPS client. */ public static CloseableHttpAsyncClient createHttpClient(final RealmConfig realmConfig, final SSLService sslService) { try { SpecialPermission.check(); return java.security.AccessController.doPrivileged((PrivilegedExceptionAction<CloseableHttpAsyncClient>) () -> { final ConnectingIOReactor ioReactor = new DefaultConnectingIOReactor(); final String sslKey = RealmSettings.realmSslPrefix(realmConfig.identifier()); final SslConfiguration sslConfiguration = sslService.getSSLConfiguration(sslKey); final SSLContext clientContext = sslService.sslContext(sslConfiguration); final HostnameVerifier verifier = SSLService.getHostnameVerifier(sslConfiguration); final Registry<SchemeIOSessionStrategy> registry = RegistryBuilder.<SchemeIOSessionStrategy>create() .register("https", new SSLIOSessionStrategy(clientContext, verifier)) .build(); final PoolingNHttpClientConnectionManager connectionManager = new PoolingNHttpClientConnectionManager(ioReactor, registry); connectionManager.setDefaultMaxPerRoute(realmConfig.getSetting(JwtRealmSettings.HTTP_MAX_ENDPOINT_CONNECTIONS)); connectionManager.setMaxTotal(realmConfig.getSetting(JwtRealmSettings.HTTP_MAX_CONNECTIONS)); final RequestConfig requestConfig = RequestConfig.custom() .setConnectTimeout(Math.toIntExact(realmConfig.getSetting(JwtRealmSettings.HTTP_CONNECT_TIMEOUT).getMillis())) .setConnectionRequestTimeout( Math.toIntExact(realmConfig.getSetting(JwtRealmSettings.HTTP_CONNECTION_READ_TIMEOUT).getSeconds()) ) .setSocketTimeout(Math.toIntExact(realmConfig.getSetting(JwtRealmSettings.HTTP_SOCKET_TIMEOUT).getMillis())) .build(); final HttpAsyncClientBuilder httpAsyncClientBuilder = HttpAsyncClients.custom() .setConnectionManager(connectionManager) .setDefaultRequestConfig(requestConfig); final CloseableHttpAsyncClient httpAsyncClient = httpAsyncClientBuilder.build(); httpAsyncClient.start(); return httpAsyncClient; }); } catch (PrivilegedActionException e) { throw new IllegalStateException("Unable to create a HttpAsyncClient instance", e); } }	just for my own knowledge: we have decided to not support plain http in fetching jwks?
protected final <Req extends Validatable, Resp> Optional<Resp> performRequestAndParseOptionalEntity(Req request, CheckedFunction<Req, Request, IOException> requestConverter, RequestOptions options, CheckedFunction<XContentParser, Resp, IOException> entityParser ) throws IOException { Optional<ValidationException> validationException = request.validate(); if (validationException != null && validationException.isPresent()) { throw validationException.get(); } Request req = requestConverter.apply(request); req.setOptions(options); Response response; try { response = client.performRequest(req); } catch (ResponseException e) { if (404 == e.getResponse().getStatusLine().getStatusCode()) { return Optional.empty(); } throw parseResponseException(e); } try { return Optional.of(parseEntity(response.getEntity(), entityParser)); } catch (Exception e) { throw new IOException("Unable to parse response body for " + response, e); } } /** * @deprecated If creating a new HLRC ReST API call, consider creating new actions instead of reusing server actions. The Validation * layer has been added to the ReST client, and requests should extend {@link Validatable} instead of {@link ActionRequest}	can you use reststatus.not_found.getstatus() here instead of just 404. the async method will need to be updated too.
static Request getTask(GetTaskRequest getTaskRequest) { Request request = new Request(HttpGet.METHOD_NAME, "_tasks/" + getTaskRequest.getNodeId() + ":" + getTaskRequest.getTaskId()); RequestConverters.Params params = new RequestConverters.Params(request); params.withTimeout(getTaskRequest.getTimeout()).withWaitForCompletion(getTaskRequest.getWaitForCompletion()); return request; }	can you use the endpointbuilder here?
static Request getTask(GetTaskRequest getTaskRequest) { Request request = new Request(HttpGet.METHOD_NAME, "_tasks/" + getTaskRequest.getNodeId() + ":" + getTaskRequest.getTaskId()); RequestConverters.Params params = new RequestConverters.Params(request); params.withTimeout(getTaskRequest.getTimeout()).withWaitForCompletion(getTaskRequest.getWaitForCompletion()); return request; }	can u put these on 2 lines? u can keep builder if u want, its just easier to see it split on 2 lines
public void testGetValidTask() throws IOException { // Run a Reindex to create a task final String sourceIndex = "source1"; final String destinationIndex = "dest"; Settings settings = Settings.builder().put("number_of_shards", 1).put("number_of_replicas", 0).build(); createIndex(sourceIndex, settings); createIndex(destinationIndex, settings); BulkRequest bulkRequest = new BulkRequest() .add(new IndexRequest(sourceIndex, "type", "1").source(Collections.singletonMap("foo", "bar"), XContentType.JSON)) .add(new IndexRequest(sourceIndex, "type", "2").source(Collections.singletonMap("foo2", "bar2"), XContentType.JSON)) .setRefreshPolicy(RefreshPolicy.IMMEDIATE); assertEquals(RestStatus.OK, highLevelClient().bulk(bulkRequest, RequestOptions.DEFAULT).status()); // (need to use low level client because currently high level client // doesn't support async return of task id - needs // https://github.com/elastic/elasticsearch/pull/35202 ) RestClient lowClient = highLevelClient().getLowLevelClient(); Request request = new Request("POST", "_reindex"); request.addParameter("wait_for_completion", "false"); request.setJsonEntity("{" + " \\\\"source\\\\": {\\\\n" + " \\\\"index\\\\": \\\\"source1\\\\"\\\\n" + " },\\\\n" + " \\\\"dest\\\\": {\\\\n" + " \\\\"index\\\\": \\\\"dest\\\\"\\\\n" + " }" + "}"); Response response = lowClient.performRequest(request); String responseBody = EntityUtils.toString(response.getEntity()); Map<String, Object> map = XContentHelper.convertToMap(JsonXContent.jsonXContent, responseBody, false); Object taskId = map.get("task"); assertNotNull(taskId); TaskId childTaskId = new TaskId(taskId.toString()); GetTaskRequest gtr = new GetTaskRequest(childTaskId.getNodeId(), childTaskId.getId()); Optional<GetTaskResponse> getTaskResponse = execute(gtr, highLevelClient().tasks()::get, highLevelClient().tasks()::getAsync); assertTrue(getTaskResponse.isPresent()); TaskInfo info = getTaskResponse.get().getTaskInfo(); assertTrue(info.isCancellable()); assertEquals("reindex from [source1] to [dest]", info.getDescription()); assertEquals("indices:data/write/reindex", info.getAction()); }	feel free to use entityasmap - available from super dlass
public void testGetValidTask() throws IOException { // Run a Reindex to create a task final String sourceIndex = "source1"; final String destinationIndex = "dest"; Settings settings = Settings.builder().put("number_of_shards", 1).put("number_of_replicas", 0).build(); createIndex(sourceIndex, settings); createIndex(destinationIndex, settings); BulkRequest bulkRequest = new BulkRequest() .add(new IndexRequest(sourceIndex, "type", "1").source(Collections.singletonMap("foo", "bar"), XContentType.JSON)) .add(new IndexRequest(sourceIndex, "type", "2").source(Collections.singletonMap("foo2", "bar2"), XContentType.JSON)) .setRefreshPolicy(RefreshPolicy.IMMEDIATE); assertEquals(RestStatus.OK, highLevelClient().bulk(bulkRequest, RequestOptions.DEFAULT).status()); // (need to use low level client because currently high level client // doesn't support async return of task id - needs // https://github.com/elastic/elasticsearch/pull/35202 ) RestClient lowClient = highLevelClient().getLowLevelClient(); Request request = new Request("POST", "_reindex"); request.addParameter("wait_for_completion", "false"); request.setJsonEntity("{" + " \\\\"source\\\\": {\\\\n" + " \\\\"index\\\\": \\\\"source1\\\\"\\\\n" + " },\\\\n" + " \\\\"dest\\\\": {\\\\n" + " \\\\"index\\\\": \\\\"dest\\\\"\\\\n" + " }" + "}"); Response response = lowClient.performRequest(request); String responseBody = EntityUtils.toString(response.getEntity()); Map<String, Object> map = XContentHelper.convertToMap(JsonXContent.jsonXContent, responseBody, false); Object taskId = map.get("task"); assertNotNull(taskId); TaskId childTaskId = new TaskId(taskId.toString()); GetTaskRequest gtr = new GetTaskRequest(childTaskId.getNodeId(), childTaskId.getId()); Optional<GetTaskResponse> getTaskResponse = execute(gtr, highLevelClient().tasks()::get, highLevelClient().tasks()::getAsync); assertTrue(getTaskResponse.isPresent()); TaskInfo info = getTaskResponse.get().getTaskInfo(); assertTrue(info.isCancellable()); assertEquals("reindex from [source1] to [dest]", info.getDescription()); assertEquals("indices:data/write/reindex", info.getAction()); }	this or assertions below might become flakey as there is no guarantee that the task will finish so quickly would you consider waiting here for the status to finish? booleansupplier hasupgradecompleted = checkcompletionstatus(task); awaitbusy(hasupgradecompleted); } } private booleansupplier checkcompletionstatus(taskid taskid) { return () -> { try { response response = client().performrequest(new request("get", "/_tasks/" + taskid.tostring())); return (boolean) entityasmap(response).get("completed"); } catch (ioexception e) { fail(e.getmessage()); return false; } }; } have a look at my draft https://github.com/elastic/elasticsearch/pull/35202/files
protected void processNodePaths(Terminal terminal, Path[] dataPaths) throws IOException { terminal.println(Terminal.Verbosity.VERBOSE, "Loading node metadata"); final NodeMetaData nodeMetaData = NodeMetaData.FORMAT.loadLatestState(logger, namedXContentRegistry, dataPaths); if (nodeMetaData == null) { throw new ElasticsearchException(NO_NODE_METADATA_FOUND_MSG); } String nodeId = nodeMetaData.nodeId(); terminal.println(Terminal.Verbosity.VERBOSE, "Current nodeId is " + nodeId); final Tuple<Manifest, MetaData> manifestMetaDataTuple = loadMetaData(terminal, dataPaths); final Manifest manifest = manifestMetaDataTuple.v1(); final MetaData metaData = manifestMetaDataTuple.v2(); final CoordinationMetaData coordinationMetaData = metaData.coordinationMetaData(); if (coordinationMetaData == null || coordinationMetaData.getLastCommittedConfiguration() == null || coordinationMetaData.getLastCommittedConfiguration().isEmpty()) { throw new ElasticsearchException(EMPTY_LAST_COMMITTED_VOTING_CONFIG_MSG); } terminal.println(String.format(Locale.ROOT, CLUSTER_STATE_TERM_VERSION_MSG_FORMAT, coordinationMetaData.term(), metaData.version())); confirm(terminal, CONFIRMATION_MSG); CoordinationMetaData newCoordinationMetaData = CoordinationMetaData.builder(coordinationMetaData) .clearVotingConfigExclusions() .lastAcceptedConfiguration(new CoordinationMetaData.VotingConfiguration(Collections.singleton(nodeId))) .lastCommittedConfiguration(new CoordinationMetaData.VotingConfiguration(Collections.singleton(nodeId))) .build(); Settings persistentSettings = Settings.builder() .put(metaData.persistentSettings()) .put(UNSAFE_BOOTSTRAP.getKey(), true) .build(); MetaData newMetaData = MetaData.builder(metaData) .clusterUUID(MetaData.UNKNOWN_CLUSTER_UUID) .generateClusterUuidIfNeeded() .clusterUUIDCommitted(true) .persistentSettings(persistentSettings) .coordinationMetaData(newCoordinationMetaData) .build(); long newCurrentTerm = manifest.getCurrentTerm() + 1; writeNewMetaData(terminal, manifest, newCurrentTerm, manifest.getClusterStateVersion(), metaData, newMetaData, dataPaths); }	is incrementing the term necessary given that we changed the cluster uuid? the node will anyway bump its term when electing itself. let's only do the minimum changes required to the state.
public void testHistoryIsWrittenWithFailure() throws Exception { String index = "index"; createNewSingletonPolicy("hot", new RolloverAction(null, null, 1L)); Request createIndexTemplate = new Request("PUT", "_template/rolling_indexes"); createIndexTemplate.setJsonEntity("{" + "\\\\"index_patterns\\\\": [\\\\""+ index + "-*\\\\"], \\\\n" + " \\\\"settings\\\\": {\\\\n" + " \\\\"number_of_shards\\\\": 1,\\\\n" + " \\\\"number_of_replicas\\\\": 0,\\\\n" + " \\\\"index.lifecycle.name\\\\": \\\\"" + policy+ "\\\\"\\\\n" + " }\\\\n" + "}"); client().performRequest(createIndexTemplate); createIndexWithSettings(index + "-1", Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0), false); // Index a document index(client(), index +"-1", "1", "foo", "bar"); Request refreshIndex = new Request("POST", "/" + index + "-1/_refresh"); client().performRequest(refreshIndex); assertBusy(() -> { assertHistoryIsPresent(policy, index + "-1", false, "ERROR"); }); }	some minor formatting issue here
@Override public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) { if (oldState.equals(newState) == false) { IndexMetaData indexMetaData = newState.metaData().index(index); if (indexMetaData != null) { LifecycleExecutionState exState = LifecycleExecutionState.fromIndexMetadata(indexMetaData); if (ErrorStep.NAME.equals(exState.getStep()) && this.failure != null) { lifecycleRunner.registerFailedOperation(indexMetaData, failure); } else { lifecycleRunner.registerSuccessfulOperation(indexMetaData); } if (nextStepKey != null && nextStepKey != TerminalPolicyStep.KEY) { logger.trace("[{}] step sequence starting with {} has completed, running next step {} if it is an async action", index.getName(), startStep.getKey(), nextStepKey); // After the cluster state has been processed and we have moved // to a new step, we need to conditionally execute the step iff // it is an `AsyncAction` so that it is executed exactly once. lifecycleRunner.maybeRunAsyncAction(newState, indexMetaData, policy, nextStepKey); } } } }	shall we wrap this trace call with if (logger.istraceenabled()) { ...?
* @return shard snapshot metadata */ public static BlobStoreIndexShardSnapshot fromXContent(XContentParser parser) throws IOException { String snapshot = null; long indexVersion = -1; long startTime = 0; long time = 0; int incrementalFileCount = 0; long incrementalSize = 0; List<FileInfo> indexFiles = null; if (parser.currentToken() == null) { // fresh parser? move to the first token parser.nextToken(); } XContentParser.Token token = parser.currentToken(); XContentParserUtils.ensureExpectedToken(XContentParser.Token.START_OBJECT, token, parser); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { XContentParserUtils.ensureExpectedToken(XContentParser.Token.FIELD_NAME, token, parser); final String currentFieldName = parser.currentName(); token = parser.nextToken(); if (token.isValue()) { if (PARSE_NAME.match(currentFieldName, parser.getDeprecationHandler())) { snapshot = parser.text(); } else if (PARSE_INDEX_VERSION.match(currentFieldName, parser.getDeprecationHandler())) { // The index-version is needed for backward compatibility with v 1.0 indexVersion = parser.longValue(); } else if (PARSE_START_TIME.match(currentFieldName, parser.getDeprecationHandler())) { startTime = parser.longValue(); } else if (PARSE_TIME.match(currentFieldName, parser.getDeprecationHandler())) { time = parser.longValue(); } else if (PARSE_INCREMENTAL_FILE_COUNT.match(currentFieldName, parser.getDeprecationHandler())) { incrementalFileCount = parser.intValue(); } else if (PARSE_INCREMENTAL_SIZE.match(currentFieldName, parser.getDeprecationHandler())) { incrementalSize = parser.longValue(); } else { XContentParserUtils.throwUnknownField(currentFieldName, parser.getTokenLocation()); } } else if (token == XContentParser.Token.START_ARRAY) { if (PARSE_FILES.match(currentFieldName, parser.getDeprecationHandler())) { indexFiles = XContentParserUtils.parseList(parser, FileInfo::fromXContent); } else { XContentParserUtils.throwUnknownField(currentFieldName, parser.getTokenLocation()); } } else { XContentParserUtils.throwUnknownToken(token, parser.getTokenLocation()); } } return new BlobStoreIndexShardSnapshot( snapshot, indexVersion, indexFiles == null ? List.of() : indexFiles, startTime, time, incrementalFileCount, incrementalSize ); }	if indexfiles is non-null and non-empty then it's just an arraylist i think, so we're losing the unmodifiability.
private static Map<String, RoleDescriptor> initializeReservedRoles() { return MapBuilder.<String, RoleDescriptor>newMapBuilder() .put("superuser", SUPERUSER_ROLE_DESCRIPTOR) .put("transport_client", new RoleDescriptor("transport_client", new String[] { "transport_client" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_user", new RoleDescriptor("kibana_user", null, null, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("all").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("monitoring_user", new RoleDescriptor("monitoring_user", new String[] { "cluster:monitor/main", "cluster:monitor/xpack/info" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_agent", new RoleDescriptor("remote_monitoring_agent", new String[] { "manage_index_templates", "manage_ingest_pipelines", "monitor", "cluster:monitor/xpack/watcher/watch/get", "cluster:admin/xpack/watcher/watch/put", "cluster:admin/xpack/watcher/watch/delete", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices("metricbeat-*").privileges("index", "create_index").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_collector", new RoleDescriptor( "remote_monitoring_collector", new String[] { "monitor" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices("*").privileges("monitor").allowRestrictedIndices(true).build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*").privileges("read").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null )) .put("ingest_admin", new RoleDescriptor("ingest_admin", new String[] { "manage_index_templates", "manage_pipeline" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) // reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role .put("reporting_user", new RoleDescriptor("reporting_user", null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_dashboard_only_user", new RoleDescriptor( "kibana_dashboard_only_user", null, null, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put(KibanaUser.ROLE_NAME, new RoleDescriptor(KibanaUser.ROLE_NAME, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", "manage_token" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*", ".reporting-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".code-*").privileges("all").build(), }, null, new ConditionalClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("logstash_system", new RoleDescriptor("logstash_system", new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("beats_admin", new RoleDescriptor("beats_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.BEATS_ROLE, new RoleDescriptor(UsernamesField.BEATS_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.APM_ROLE, new RoleDescriptor(UsernamesField.APM_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("apm_user", new RoleDescriptor("apm_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("apm-*").privileges("read", "view_index_metadata").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_user", new RoleDescriptor("machine_learning_user", new String[] { "monitor_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-notifications*") .privileges("view_index_metadata", "read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*") .privileges("view_index_metadata", "read", "write").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_admin", new RoleDescriptor("machine_learning_admin", new String[] { "manage_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".ml-anomalies*", ".ml-notifications*", ".ml-state*", ".ml-meta*") .privileges("view_index_metadata", "read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*") .privileges("view_index_metadata", "read", "write").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_admin", new RoleDescriptor("watcher_admin", new String[] { "manage_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX, TriggeredWatchStoreField.INDEX_NAME, HistoryStoreField.INDEX_PREFIX + "*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_user", new RoleDescriptor("watcher_user", new String[] { "monitor_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX) .privileges("read") .build(), RoleDescriptor.IndicesPrivileges.builder().indices(HistoryStoreField.INDEX_PREFIX + "*") .privileges("read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("logstash_admin", new RoleDescriptor("logstash_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".logstash*") .privileges("create", "delete", "index", "manage", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_user", new RoleDescriptor("rollup_user", new String[] { "monitor_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_admin", new RoleDescriptor("rollup_admin", new String[] { "manage_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("code_admin", new RoleDescriptor("code_admin", new String[] {}, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".code-*").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("code_user", new RoleDescriptor("code_user", new String[] {}, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".code-*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("snapshot_user", new RoleDescriptor("snapshot_user", new String[] { "create_snapshot", GetRepositoriesAction.NAME }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices("*") .privileges("view_index_metadata") .allowRestrictedIndices(true) .build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .immutableMap(); }	users with platinum also need access to ml anomaly indicies .ml-anomalies*
public Translog.Snapshot newLuceneChangesSnapshot(String source, MapperService mapperService, long minSeqNo, long maxSeqNo, boolean requiredFullRange) throws IOException { // TODO: Should we defer the refresh until we really need it? ensureOpen(); if (lastRefreshedCheckpoint() < maxSeqNo) { refresh(source, SearcherScope.INTERNAL); } refresh(source, SearcherScope.INTERNAL); return new LuceneChangesSnapshot(() -> acquireSearcher(source, SearcherScope.INTERNAL), mapperService, minSeqNo, maxSeqNo, requiredFullRange); }	this seems like a mistake?
@Override public long softUpdateDocuments(Term term, Iterable<? extends Iterable<? extends IndexableField>> docs, Field... softDeletes) throws IOException { assert softDeleteEnabled : "Call #softUpdateDocuments but soft-deletes is disabled"; return super.softUpdateDocuments(term, docs, softDeletes); } } /** * Returned the maximum local checkpoint value has been refreshed internally. */ final long lastRefreshedCheckpoint() { return lastRefreshedCheckpointListener.refreshedCheckpoint.get(); } private final class LastRefreshedCheckpointListener implements ReferenceManager.RefreshListener { final AtomicLong refreshedCheckpoint; private long pendingCheckpoint; LastRefreshedCheckpointListener(long initialLocalCheckpoint) { this.refreshedCheckpoint = new AtomicLong(initialLocalCheckpoint); } @Override public void beforeRefresh() { pendingCheckpoint = localCheckpointTracker.getCheckpoint(); // All change until this point should be visible after refresh } @Override public void afterRefresh(boolean didRefresh) { if (didRefresh) { refreshedCheckpoint.getAndUpdate(prev -> Math.max(prev, pendingCheckpoint)); } }	nit: the *last* local checkpoint
@Override public long softUpdateDocuments(Term term, Iterable<? extends Iterable<? extends IndexableField>> docs, Field... softDeletes) throws IOException { assert softDeleteEnabled : "Call #softUpdateDocuments but soft-deletes is disabled"; return super.softUpdateDocuments(term, docs, softDeletes); } } /** * Returned the maximum local checkpoint value has been refreshed internally. */ final long lastRefreshedCheckpoint() { return lastRefreshedCheckpointListener.refreshedCheckpoint.get(); } private final class LastRefreshedCheckpointListener implements ReferenceManager.RefreshListener { final AtomicLong refreshedCheckpoint; private long pendingCheckpoint; LastRefreshedCheckpointListener(long initialLocalCheckpoint) { this.refreshedCheckpoint = new AtomicLong(initialLocalCheckpoint); } @Override public void beforeRefresh() { pendingCheckpoint = localCheckpointTracker.getCheckpoint(); // All change until this point should be visible after refresh } @Override public void afterRefresh(boolean didRefresh) { if (didRefresh) { refreshedCheckpoint.getAndUpdate(prev -> Math.max(prev, pendingCheckpoint)); } }	i think this is unsafe? you make capture things that didn't make it into the reader
@Override public final NodeChannels openConnection(DiscoveryNode node, ConnectionProfile connectionProfile) throws IOException { if (node == null) { throw new ConnectTransportException(null, "can't open connection to a null node"); } boolean success = false; NodeChannels nodeChannels = null; connectionProfile = resolveConnectionProfile(connectionProfile, defaultConnectionProfile); closeLock.readLock().lock(); // ensure we don't open connections while we are closing try { ensureOpen(); try { final AtomicBoolean runOnce = new AtomicBoolean(false); final AtomicReference<NodeChannels> connectionRef = new AtomicReference<>(); Consumer<Channel> onClose = c -> { assert isOpen(c) == false : "channel is still open when onClose is called"; try { onChannelClosed(c); } finally { // we only need to disconnect from the nodes once since all other channels // will also try to run this we protect it from running multiple times. if (runOnce.compareAndSet(false, true)) { NodeChannels connection = connectionRef.get(); if (connection != null) { disconnectFromNodeCloseAndNotify(node, connection); } } } }; nodeChannels = connectToChannels(node, connectionProfile, this::onChannelOpen, onClose); if (!Arrays.stream(nodeChannels.channels).allMatch(this::isOpen)) { throw new ConnectTransportException(node, "a channel closed while connecting"); } final Channel channel = nodeChannels.getChannels().get(0); // one channel is guaranteed by the connection profile final TimeValue connectTimeout = connectionProfile.getConnectTimeout() == null ? defaultConnectionProfile.getConnectTimeout() : connectionProfile.getConnectTimeout(); final TimeValue handshakeTimeout = connectionProfile.getHandshakeTimeout() == null ? connectTimeout : connectionProfile.getHandshakeTimeout(); final Version version = executeHandshake(node, channel, handshakeTimeout); nodeChannels = new NodeChannels(nodeChannels, version); // clone the channels - we now have the correct version transportService.onConnectionOpened(nodeChannels); connectionRef.set(nodeChannels); success = true; return nodeChannels; } catch (ConnectTransportException e) { throw e; } catch (Exception e) { // ConnectTransportExceptions are handled specifically on the caller end - we wrap the actual exception to ensure // only relevant exceptions are logged on the caller end.. this is the same as in connectToNode throw new ConnectTransportException(node, "general node connection failure", e); } finally { if (success == false) { IOUtils.closeWhileHandlingException(nodeChannels); } } } finally { closeLock.readLock().unlock(); } }	i this this to be moved to after connectionref.set(nodechannels); is called. otherwise we still have a race condition when things go wrong later on?
@Override protected MockTransportService build( Settings settings, Version version, ClusterSettings clusterSettings, boolean doHandshake, Consumer<NioChannel> onChannelOpen) { settings = Settings.builder().put(settings) .put(TcpTransport.PORT.getKey(), "0") .build(); MockTransportService transportService = nioFromThreadPool(settings, threadPool, version, clusterSettings, doHandshake, onChannelOpen); transportService.start(); return transportService; }	out of curiosity - this reads weird - we wait on close but not actively close? is that ok?
@Override protected void updateRemoteCluster(String clusterAlias, Settings settings) { CountDownLatch latch = new CountDownLatch(1); updateRemoteCluster(clusterAlias, settings, ActionListener.runAfter(new ActionListener<Void>() { @Override public void onResponse(Void o) { logger.debug("connected to new remote cluster {}", clusterAlias); } @Override public void onFailure(Exception e) { logger.debug("initial connection to new remote cluster {} failed", clusterAlias); } }, latch::countDown)); try { // Wait 10 seconds for a connections. We must use a latch instead of a future because we // are on the cluster state thread and our custom future implementation will throw an // assertion. if (latch.await(10, TimeUnit.SECONDS) == false) { logger.warn("failed to connect to new remote cluster {} within {}", clusterAlias, TimeValue.timeValueSeconds(10)); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); } }	i think "initial" is a bit confusing here, since it could be an update of a remote cluster that was previously connected and it could now reconnect? suggestion logger.debug("connection to new remote cluster {} failed", clusteralias);
public IndexInput openInputRaw(String name, IOContext context) throws IOException { StoreFileMetaData metaData = filesMetadata.get(name); if (metaData == null) { throw new FileNotFoundException(name); } return metaData.directory().openInput(name, context); }	this needs formatting, spaces after == and { should go on the same line as the if statement
@Override public IndexInput openInput(String name, IOContext context) throws IOException { ensureOpen(); StoreFileMetaData metaData = filesMetadata.get(name); if (metaData == null) { throw new FileNotFoundException(name); } IndexInput in = metaData.directory().openInput(name, context); boolean success = false; try { // Only for backward comp. since we now use Lucene codec compression if (name.endsWith(".fdt") || name.endsWith(".tvf")) { Compressor compressor = CompressorFactory.compressor(in); if (compressor != null) { in = compressor.indexInput(in); } } success = true; } finally { if (!success) { IOUtils.closeWhileHandlingException(in); } } if(context.context==Context.READ) { in=new TimeLimitedIndexInput(name, in); } return in; }	i wonder if we can make this opetional ie. i would wanna be able to opt our here?
@Override public IndexInput openInput(String name, IOContext context) throws IOException { ensureOpen(); StoreFileMetaData metaData = filesMetadata.get(name); if (metaData == null) { throw new FileNotFoundException(name); } IndexInput in = metaData.directory().openInput(name, context); boolean success = false; try { // Only for backward comp. since we now use Lucene codec compression if (name.endsWith(".fdt") || name.endsWith(".tvf")) { Compressor compressor = CompressorFactory.compressor(in); if (compressor != null) { in = compressor.indexInput(in); } } success = true; } finally { if (!success) { IOUtils.closeWhileHandlingException(in); } } if(context.context==Context.READ) { in=new TimeLimitedIndexInput(name, in); } return in; }	can this be a static inner class (also needs formatting)
SearchContext createContext(ShardSearchRequest request, @Nullable Engine.Searcher searcher) throws ElasticSearchException { IndexService indexService = indicesService.indexServiceSafe(request.index()); IndexShard indexShard = indexService.shardSafe(request.shardId()); SearchShardTarget shardTarget = new SearchShardTarget(clusterService.localNode().id(), request.index(), request.shardId()); Engine.Searcher engineSearcher = searcher == null ? indexShard.acquireSearcher("search") : searcher; SearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher, indexService, indexShard, scriptService, cacheRecycler); SearchContext.setCurrent(context); try { context.scroll(request.scroll()); parseSource(context, request.source()); parseSource(context, request.extraSource()); // if the from and size are still not set, default them if (context.from() == -1) { context.from(0); } if (context.size() == -1) { context.size(10); } //Only now that the context is fully parsed (i.e. complete with timeout info) check to see if we // need to register this thread for timeout monitoring if(context.timeoutInMillis()>0) { ActivityTimeMonitor.start(context.timeoutInMillis()); } // pre process dfsPhase.preProcess(context); queryPhase.preProcess(context); fetchPhase.preProcess(context); // compute the context keep alive long keepAlive = defaultKeepAlive; if (request.scroll() != null && request.scroll().keepAlive() != null) { keepAlive = request.scroll().keepAlive().millis(); } context.keepAlive(keepAlive); } catch (Throwable e) { context.release(); throw ExceptionsHelper.convertToRuntime(e); } return context; }	i think instead of doing start/stop manually we should maybe hack the threadpool and just allow to call start and stop is called once the thread returns to the threadpool? this way we don't need to take care of this? just and idea...
public void testChangeTimestampFieldInComposableTemplatePriorToRollOver() throws Exception { createIndexTemplate("id1", "logs-foo*", "@timestamp"); // Index doc that triggers creation of a data stream IndexRequest indexRequest = new IndexRequest("logs-foobar").source("{}", XContentType.JSON).opType("create"); IndexResponse indexResponse = client().index(indexRequest).actionGet(); assertThat(indexResponse.getIndex(), equalTo(DataStream.getDefaultBackingIndexName("logs-foobar", 1))); assertBackingIndex(DataStream.getDefaultBackingIndexName("logs-foobar", 1), "properties.@timestamp"); // Rollover data stream RolloverResponse rolloverResponse = client().admin().indices().rolloverIndex(new RolloverRequest("logs-foobar", null)).get(); assertThat(rolloverResponse.getNewIndex(), equalTo(DataStream.getDefaultBackingIndexName("logs-foobar", 2))); assertTrue(rolloverResponse.isRolledOver()); assertBackingIndex(DataStream.getDefaultBackingIndexName("logs-foobar", 2), "properties.@timestamp"); // Index another doc into a data stream indexRequest = new IndexRequest("logs-foobar").source("{}", XContentType.JSON).opType("create"); indexResponse = client().index(indexRequest).actionGet(); assertThat(indexResponse.getIndex(), equalTo(DataStream.getDefaultBackingIndexName("logs-foobar", 2))); // Change the template to have a different timestamp field createIndexTemplate("id1", "logs-foo*", "@timestamp2"); // Rollover again, eventhough there is no mapping in the template, the timestamp field mapping in data stream // should be applied in the new backing index rolloverResponse = client().admin().indices().rolloverIndex(new RolloverRequest("logs-foobar", null)).get(); assertThat(rolloverResponse.getNewIndex(), equalTo(DataStream.getDefaultBackingIndexName("logs-foobar", 3))); assertTrue(rolloverResponse.isRolledOver()); assertBackingIndex(DataStream.getDefaultBackingIndexName("logs-foobar", 3), "properties.@timestamp"); // Index another doc into a data stream indexRequest = new IndexRequest("logs-foobar").source("{}", XContentType.JSON).opType("create"); indexResponse = client().index(indexRequest).actionGet(); assertThat(indexResponse.getIndex(), equalTo(DataStream.getDefaultBackingIndexName("logs-foobar", 3))); DeleteDataStreamAction.Request deleteDataStreamRequest = new DeleteDataStreamAction.Request("logs-foobar"); client().admin().indices().deleteDataStream(deleteDataStreamRequest).actionGet(); }	i think we should rename createindextemplate to putindextemplate to signal that it also updates existing.
private ClusterState applyCreateIndexRequestWithV2Template(final ClusterState currentState, final CreateIndexClusterStateUpdateRequest request, final boolean silent, final String templateName, final BiConsumer<Metadata.Builder, IndexMetadata> metadataTransformer) throws Exception { logger.debug("applying create index request using composable template [{}]", templateName); final Map<String, Object> mappings = resolveV2Mappings(request.mappings(), currentState, templateName, xContentRegistry); if (request.dataStreamName() != null) { DataStream dataStream = currentState.metadata().dataStreams().get(request.dataStreamName()); if (dataStream != null) { String mappingPath = convertFieldPathToMappingPath(dataStream.getTimeStampField().getFieldName()); String parentObjectFieldPath = mappingPath.substring(0, mappingPath.lastIndexOf('.')); Map<String, Object> parentObjectMapper = ObjectPath.eval("_doc." + parentObjectFieldPath, mappings); parentObjectMapper.put(dataStream.getTimeStampField().getFieldName(), dataStream.getTimeStampField().getFieldMapping()); } } final Settings aggregatedIndexSettings = aggregateIndexSettings(currentState, request, MetadataIndexTemplateService.resolveSettings(currentState.metadata(), templateName), mappings, null, settings, indexScopedSettings, shardLimitValidator); int routingNumShards = getIndexNumberOfRoutingShards(aggregatedIndexSettings, null); IndexMetadata tmpImd = buildAndValidateTemporaryIndexMetadata(currentState, aggregatedIndexSettings, request, routingNumShards); return applyCreateIndexWithTemporaryService(currentState, request, silent, null, tmpImd, mappings, indexService -> resolveAndValidateAliases(request.index(), request.aliases(), MetadataIndexTemplateService.resolveAliases(currentState.metadata(), templateName), currentState.metadata(), aliasValidator, // the context is only used for validation so it's fine to pass fake values for the // shard id and the current timestamp xContentRegistry, indexService.newQueryShardContext(0, null, () -> 0L, null)), Collections.singletonList(templateName), metadataTransformer); }	maybe assert that the parentobjectmapper does not contain a definition for the timestamp field already? should potentially be a production code check in case a component template can have the timestamp field too? not sure if we guard against that.
public TransportService createCapturingTransportService(Settings settings, ThreadPool threadPool, TransportInterceptor interceptor, Function<BoundTransportAddress, DiscoveryNode> localNodeFactory, @Nullable ClusterSettings clusterSettings, Set<String> taskHeaders) { StubbableConnectionManager connectionManager = new StubbableConnectionManager(new ConnectionManager(settings, this, threadPool), settings, this, threadPool); connectionManager.setDefaultNodeConnectedBehavior((cm, discoveryNode) -> true); connectionManager.setDefaultConnectBehavior((cm, discoveryNode) -> new Connection() { @Override public DiscoveryNode getNode() { return discoveryNode; } @Override public void sendRequest(long requestId, String action, TransportRequest request, TransportRequestOptions options) throws TransportException { requests.put(requestId, Tuple.tuple(discoveryNode, action)); capturedRequests.add(new CapturedRequest(discoveryNode, requestId, action, request)); } @Override public void addCloseListener(ActionListener<Void> listener) { } @Override public boolean isClosed() { return false; } @Override public void close() { } }); return new TransportService(settings, this, threadPool, interceptor, localNodeFactory, clusterSettings, taskHeaders, connectionManager); } /** returns all requests captured so far. Doesn't clear the captured request list. See {@link #clear()}	bit late to the party here, but could this have been the following? connectionmanager.setdefaultconnectbehavior((cm, discoverynode) -> openconnection(discoverynode, null)); that'd let me continue to override the default connection behaviour by subclassing capturingtransport.
private void logFailures(MultiGetResponse multiGetResponse) { List<String> ids = Arrays.stream(multiGetResponse.getResponses()) .filter(MultiGetItemResponse::isFailed) .filter(itemResponse -> itemResponse.getFailure() != null) .map(itemResponse -> itemResponse.getFailure().getId()) .collect(Collectors.toList()); if (ids.isEmpty() == false) { logger.info("Could not retrieve logstash pipelines with ids: {}", ids); } }	i have a concern about the user experience of wildcard pattern across logstash and elasticsearch. currently logstash accept alphabet, number, _, - as pipeline id and only accept * as wildcard to match any character in pipeline id. for example pattern *mashhur* matches mashhur123, 123mashhur456 the proposed api translate * -> .* to be a regex pattern, which means it accepts other special characters like +, ., ?. (1) although this pattern looks very similar to regex, it is not exactly the same because of * -> .* translation. this is confusing to the user. another confusion is the inconsistent behavior of logastash and elasticsearch. users want to get pipelines from a wildcard pattern. (2) they face different syntax to get the same content. for example elasticsearch api accept mash+ur* to match mashhur while logstash throw exception because of special character +. i would suggest to unify the user experience by either accept regex in api and logstash config (a breaking change) or escape the special character in api so it only consider * in matching.
@Before public void setupTest() { clusterName = randomAlphaOfLength(10); localNode = new DiscoveryNode("local", buildNewFakeTransportAddress(), Version.CURRENT); localNode = new DiscoveryNode( "node1", "local", buildNewFakeTransportAddress(), emptyMap(), EnumSet.allOf(DiscoveryNode.Role.class), Version.CURRENT ); otherNode = new DiscoveryNode( "node2", "other", buildNewFakeTransportAddress(), emptyMap(), EnumSet.allOf(DiscoveryNode.Role.class), Version.CURRENT ); final MockTransport transport = new MockTransport() { @Override protected void onSendRequest(long requestId, String action, TransportRequest request, DiscoveryNode node) { if (action.equals(HANDSHAKE_ACTION_NAME) && node.getAddress().equals(otherNode.getAddress())) { handleResponse(requestId, new HandshakeResponse(otherNode, new ClusterName(clusterName), Version.CURRENT)); } } }; transportService = transport.createTransportService( Settings.builder().put(CLUSTER_NAME_SETTING.getKey(), clusterName).build(), threadPool, TransportService.NOOP_TRANSPORT_INTERCEPTOR, boundTransportAddress -> localNode, null, emptySet()); final ClusterSettings clusterSettings = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS); coordinator = new Coordinator("local", Settings.EMPTY, clusterSettings, transportService, writableRegistry(), ESAllocationTestCase.createAllocationService(Settings.EMPTY), new MasterService("local", Settings.EMPTY, threadPool), () -> new InMemoryPersistedState(0, ClusterState.builder(new ClusterName(clusterName)).build()), r -> emptyList(), new NoOpClusterApplier(), new Random(random().nextLong())); }	i think this line is unnecessary - it's immediately overwritten by the next.
@Before public void setupTest() { clusterName = randomAlphaOfLength(10); localNode = new DiscoveryNode("local", buildNewFakeTransportAddress(), Version.CURRENT); localNode = new DiscoveryNode( "node1", "local", buildNewFakeTransportAddress(), emptyMap(), EnumSet.allOf(DiscoveryNode.Role.class), Version.CURRENT ); otherNode = new DiscoveryNode( "node2", "other", buildNewFakeTransportAddress(), emptyMap(), EnumSet.allOf(DiscoveryNode.Role.class), Version.CURRENT ); final MockTransport transport = new MockTransport() { @Override protected void onSendRequest(long requestId, String action, TransportRequest request, DiscoveryNode node) { if (action.equals(HANDSHAKE_ACTION_NAME) && node.getAddress().equals(otherNode.getAddress())) { handleResponse(requestId, new HandshakeResponse(otherNode, new ClusterName(clusterName), Version.CURRENT)); } } }; transportService = transport.createTransportService( Settings.builder().put(CLUSTER_NAME_SETTING.getKey(), clusterName).build(), threadPool, TransportService.NOOP_TRANSPORT_INTERCEPTOR, boundTransportAddress -> localNode, null, emptySet()); final ClusterSettings clusterSettings = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS); coordinator = new Coordinator("local", Settings.EMPTY, clusterSettings, transportService, writableRegistry(), ESAllocationTestCase.createAllocationService(Settings.EMPTY), new MasterService("local", Settings.EMPTY, threadPool), () -> new InMemoryPersistedState(0, ClusterState.builder(new ClusterName(clusterName)).build()), r -> emptyList(), new NoOpClusterApplier(), new Random(random().nextLong())); }	a minor style nit: i think these closing parentheses should go on the preceding line. there's only room for so many lines on a screen.
public void testNodeVersionIsUpdated() throws IOException, NodeValidationException { TransportClient client = (TransportClient) internalCluster().client(); try (Node node = new MockNode(Settings.builder() .put(internalCluster().getDefaultSettings()) .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()) .put("node.name", "testNodeVersionIsUpdated") .put("transport.type", getTestTransportType()) .put(Node.NODE_DATA_SETTING.getKey(), false) .put("cluster.name", "foobar") .put(TestZenDiscovery.USE_ZEN2.getKey(), true) .putList( ClusterBootstrapService.INITIAL_MASTER_NODES_SETTING.getKey(), Collections.singletonList("testNodeVersionIsUpdated") ) .build(), Arrays.asList(getTestTransportPlugin(), TestZenDiscovery.TestPlugin.class, MockHttpTransport.TestPlugin.class)).start()) { TransportAddress transportAddress = node.injector().getInstance(TransportService.class).boundAddress().publishAddress(); client.addTransportAddress(transportAddress); // since we force transport clients there has to be one node started that we connect to. assertThat(client.connectedNodes().size(), greaterThanOrEqualTo(1)); // connected nodes have updated version for (DiscoveryNode discoveryNode : client.connectedNodes()) { assertThat(discoveryNode.getVersion(), equalTo(Version.CURRENT)); } for (DiscoveryNode discoveryNode : client.listedNodes()) { assertThat(discoveryNode.getId(), startsWith("#transport#-")); assertThat(discoveryNode.getVersion(), equalTo(Version.CURRENT.minimumCompatibilityVersion())); } assertThat(client.filteredNodes().size(), equalTo(1)); for (DiscoveryNode discoveryNode : client.filteredNodes()) { assertThat(discoveryNode.getVersion(), equalTo(Version.CURRENT.minimumCompatibilityVersion())); } } }	there's a varargs version of this to avoid the newlines: .putlist(clusterbootstrapservice.initial_master_nodes_setting.getkey(), "testnodeversionisupdated")
static Optional<DeprecationIssue> checkModelSnapshot(ModelSnapshot modelSnapshot) { if (modelSnapshot.getMinVersion().before(MIN_CHECKED_SUPPORTED_SNAPSHOT_VERSION)) { StringBuilder details = new StringBuilder( String.format( Locale.ROOT, // Important: the Kibana upgrade assistant expects this to match the pattern /[Mm]odel snapshot/ // and if it doesn't then the expected "Fix" button won't appear for this deprecation. "Model snapshot [%s] for job [%s] has an obsolete minimum version [%s].", modelSnapshot.getSnapshotId(), modelSnapshot.getJobId(), modelSnapshot.getMinVersion() ) ); if (modelSnapshot.getLatestRecordTimeStamp() != null) { details.append( String.format( Locale.ROOT, " The model snapshot's latest record timestamp is [%s].", XContentElasticsearchExtension.DEFAULT_FORMATTER.format(modelSnapshot.getLatestRecordTimeStamp().toInstant()) ) ); } return Optional.of( new DeprecationIssue( DeprecationIssue.Level.CRITICAL, String.format( Locale.ROOT, "Delete model snapshot [%s] or update it to %s or greater.", modelSnapshot.getSnapshotId(), MIN_REPORTED_SUPPORTED_SNAPSHOT_VERSION ), "https://www.elastic.co/guide/en/elasticsearch/reference/master/ml-upgrade-job-model-snapshot.html", details.toString(), false, Map.of("job_id", modelSnapshot.getJobId(), "snapshot_id", modelSnapshot.getSnapshotId()) ) ); } return Optional.empty(); }	@masseyke and @debadair i've added one word compared to what you used in #79387: "model snapshot" instead of just "snapshot" at the beginning. please let me know if you strongly object to this.
public void testRateLimitingIsEmployed() throws Exception { restartClustersWithSettings(Settings.builder().put(CcrSettings.FOLLOWER_RECOVERY_MAX_BYTES_READ_PER_SECOND.getKey(), new ByteSizeValue(500)).build()); String leaderClusterRepoName = CcrRepository.NAME_PREFIX + "leader_cluster"; String leaderIndex = "index1"; String followerIndex = "index2"; final int numberOfPrimaryShards = randomIntBetween(1, 3); final String leaderIndexSettings = getIndexSettings(numberOfPrimaryShards, between(0, 1), singletonMap(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), "true")); assertAcked(leaderClient().admin().indices().prepareCreate(leaderIndex).setSource(leaderIndexSettings, XContentType.JSON)); ensureLeaderGreen(leaderIndex); final RestoreService restoreService = getFollowerCluster().getCurrentMasterNodeInstance(RestoreService.class); final ClusterService clusterService = getFollowerCluster().getCurrentMasterNodeInstance(ClusterService.class); List<CcrRepository> repositories = new ArrayList<>(); try { for (RepositoriesService repositoriesService : getFollowerCluster().getDataOrMasterNodeInstances(RepositoriesService.class)) { Repository repository = repositoriesService.repository(leaderClusterRepoName); repositories.add((CcrRepository) repository); } } catch (RepositoryMissingException e) { fail("need repository"); } final int firstBatchNumDocs = 10; logger.info("Indexing [{}] docs as first batch", firstBatchNumDocs); for (int i = 0; i < firstBatchNumDocs; i++) { final String source = String.format(Locale.ROOT, "{\\\\"f\\\\":%d}", i); leaderClient().prepareIndex("index1", "doc", Integer.toString(i)).setSource(source, XContentType.JSON).get(); } leaderClient().admin().indices().prepareFlush(leaderIndex).setForce(true).setWaitIfOngoing(true).get(); try { Settings.Builder settingsBuilder = Settings.builder() .put(IndexMetaData.SETTING_INDEX_PROVIDED_NAME, followerIndex) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true); RestoreService.RestoreRequest restoreRequest = new RestoreService.RestoreRequest(leaderClusterRepoName, CcrRepository.LATEST, new String[]{leaderIndex}, indicesOptions, "^(.*)$", followerIndex, Settings.EMPTY, new TimeValue(1, TimeUnit.HOURS), false, false, true, settingsBuilder.build(), new String[0], "restore_snapshot[" + leaderClusterRepoName + ":" + leaderIndex + "]"); PlainActionFuture<RestoreInfo> future = PlainActionFuture.newFuture(); restoreService.restoreSnapshot(restoreRequest, waitForRestore(clusterService, future)); assertBusy(() -> assertTrue(repositories.stream().anyMatch(cr -> cr.getRestoreThrottleTimeInNanos() > 0))); } finally { restartClustersWithSettings(Settings.EMPTY); } }	by making the setting truly dynamic, we won't need the restarts here, which will greatly speed up the tests. see sharedclustersnapshotrestoreit#testthrottling for how it's tested for snapshot /restore.
public void createTestDoc() throws IOException { XContentBuilder mappingsBuilder = randomXContentBuilder(); mappingsBuilder.startObject(); mappingsBuilder.startObject("mappings"); mappingsBuilder.startObject("type"); mappingsBuilder.startObject("properties"); mappingsBuilder.startObject("title"); mappingsBuilder.field("type", "text"); mappingsBuilder.field("store", "true"); mappingsBuilder.endObject(); mappingsBuilder.startObject("content"); mappingsBuilder.field("type", "text"); mappingsBuilder.field("store", "true"); mappingsBuilder.endObject(); mappingsBuilder.endObject(); mappingsBuilder.endObject(); mappingsBuilder.endObject(); mappingsBuilder.endObject(); Map<String, String> params = new HashMap<>(); client().performRequest("PUT", "test", params, new StringEntity(mappingsBuilder.string())); params.put("refresh", "wait_for"); XContentBuilder document = randomXContentBuilder(); document.startObject(); document.startArray("content"); document.value("buzz cola"); document.value("some buzz"); document.endArray(); document.field("title", "some title"); document.endObject(); client().performRequest("PUT", "test/type/1", params, new StringEntity(document.string())); }	if we'll need this in other tests, we should probably try to shorten this setup part of test by re-using what we have in our java api, that allows to provide object... source , but we also want to be able to randomize the xcontent type, which is why we need to adapt it a bit
final SearchContext createAndPutContext(ShardSearchRequest request) throws IOException { if (numActiveContexts >= MAX_OPEN_CONTEXT.get(settings)) { throw new ElasticsearchException( "Trying to create too many search contexts. Must be less than or equal to: [" + MAX_OPEN_CONTEXT.get(settings) + "]. This limit can be set by changing the [" + MAX_OPEN_CONTEXT.getKey() + "] setting."); } SearchContext context = createContext(request); boolean success = false; try { putContext(context); if (request.scroll() != null) { context.indexShard().getSearchOperationListener().onNewScrollContext(context); } context.indexShard().getSearchOperationListener().onNewContext(context); success = true; numActiveContexts++; return context; } finally { if (!success) { freeContext(context.id()); } } }	can this function be called by multiple threads at the same time? and if so, should numactivecontexts be volatile?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(index); out.writeBoolean(managedByILM); if (managedByILM) { out.writeString(policyName); out.writeOptionalLong(lifecycleDate); out.writeOptionalString(phase); out.writeOptionalString(action); out.writeOptionalString(step); out.writeOptionalString(failedStep); out.writeOptionalLong(phaseTime); out.writeOptionalLong(actionTime); out.writeOptionalLong(stepTime); out.writeOptionalBytesReference(stepInfo); out.writeOptionalWriteable(phaseExecutionInfo); out.writeOptionalBoolean(isAutoRetryableError); out.writeOptionalVInt(failedStepRetryCount); out.writeOptionalString(repositoryName); out.writeOptionalString(snapshotName); out.writeOptionalString(shrinkIndexName); out.writeOptionalLong(indexCreationDate); } }	this needs to only happen (even though it's an optional write, it will write false if there's no value) if (out.getversion().onorafter(version.v_8_1_0))
@SuppressWarnings("unchecked") public void testExplainIndicesDatesAndAges() throws Exception { createNewSingletonPolicy(client(), policy, "delete", DeleteAction.WITH_SNAPSHOT_DELETE, TimeValue.timeValueDays(100)); String withOriginationDate = this.index + "-with-origination-date"; String withoutOriginationDate = this.index + "-without-origination-date"; long originationDate = randomLongBetween(0, System.currentTimeMillis()); createIndexWithSettings( client(), withOriginationDate, alias + withOriginationDate, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(LifecycleSettings.LIFECYCLE_NAME, policy) .put(LifecycleSettings.LIFECYCLE_ORIGINATION_DATE, originationDate) ); createIndexWithSettings( client(), withoutOriginationDate, alias + withoutOriginationDate, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(LifecycleSettings.LIFECYCLE_NAME, policy) ); assertBusy(() -> { Map<String, Map<String, Object>> explain = explain(client(), this.index + "*", false, false); Map<String, Object> explainIndexWithOriginationDate = explain.get(withOriginationDate); assertThat(explainIndexWithOriginationDate.get("age").equals(explainIndexWithOriginationDate.get("index_age")), is(false)); Map<String, Object> explainIndexWithoutOriginationDate = explain.get(withoutOriginationDate); assertThat(explainIndexWithoutOriginationDate.get("age").equals(explainIndexWithoutOriginationDate.get("index_age")), is(true)); }); }	nit: you don't have to specify an alias, it doesn't look like you'd need one here?
@SuppressWarnings("unchecked") public void testExplainIndicesDatesAndAges() throws Exception { createNewSingletonPolicy(client(), policy, "delete", DeleteAction.WITH_SNAPSHOT_DELETE, TimeValue.timeValueDays(100)); String withOriginationDate = this.index + "-with-origination-date"; String withoutOriginationDate = this.index + "-without-origination-date"; long originationDate = randomLongBetween(0, System.currentTimeMillis()); createIndexWithSettings( client(), withOriginationDate, alias + withOriginationDate, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(LifecycleSettings.LIFECYCLE_NAME, policy) .put(LifecycleSettings.LIFECYCLE_ORIGINATION_DATE, originationDate) ); createIndexWithSettings( client(), withoutOriginationDate, alias + withoutOriginationDate, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(LifecycleSettings.LIFECYCLE_NAME, policy) ); assertBusy(() -> { Map<String, Map<String, Object>> explain = explain(client(), this.index + "*", false, false); Map<String, Object> explainIndexWithOriginationDate = explain.get(withOriginationDate); assertThat(explainIndexWithOriginationDate.get("age").equals(explainIndexWithOriginationDate.get("index_age")), is(false)); Map<String, Object> explainIndexWithoutOriginationDate = explain.get(withoutOriginationDate); assertThat(explainIndexWithoutOriginationDate.get("age").equals(explainIndexWithoutOriginationDate.get("index_age")), is(true)); }); }	nit: you don't have to specify an alias, it doesn't look like you'd need one here?
@SuppressWarnings("unchecked") public void testExplainIndicesDatesAndAges() throws Exception { createNewSingletonPolicy(client(), policy, "delete", DeleteAction.WITH_SNAPSHOT_DELETE, TimeValue.timeValueDays(100)); String withOriginationDate = this.index + "-with-origination-date"; String withoutOriginationDate = this.index + "-without-origination-date"; long originationDate = randomLongBetween(0, System.currentTimeMillis()); createIndexWithSettings( client(), withOriginationDate, alias + withOriginationDate, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(LifecycleSettings.LIFECYCLE_NAME, policy) .put(LifecycleSettings.LIFECYCLE_ORIGINATION_DATE, originationDate) ); createIndexWithSettings( client(), withoutOriginationDate, alias + withoutOriginationDate, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(LifecycleSettings.LIFECYCLE_NAME, policy) ); assertBusy(() -> { Map<String, Map<String, Object>> explain = explain(client(), this.index + "*", false, false); Map<String, Object> explainIndexWithOriginationDate = explain.get(withOriginationDate); assertThat(explainIndexWithOriginationDate.get("age").equals(explainIndexWithOriginationDate.get("index_age")), is(false)); Map<String, Object> explainIndexWithoutOriginationDate = explain.get(withoutOriginationDate); assertThat(explainIndexWithoutOriginationDate.get("age").equals(explainIndexWithoutOriginationDate.get("index_age")), is(true)); }); }	should we assert there are some values in the fields? if index_age is null for example, this assertion could pass (for the wrong reasons)
@SuppressWarnings("unchecked") public void testExplainIndicesDatesAndAges() throws Exception { createNewSingletonPolicy(client(), policy, "delete", DeleteAction.WITH_SNAPSHOT_DELETE, TimeValue.timeValueDays(100)); String withOriginationDate = this.index + "-with-origination-date"; String withoutOriginationDate = this.index + "-without-origination-date"; long originationDate = randomLongBetween(0, System.currentTimeMillis()); createIndexWithSettings( client(), withOriginationDate, alias + withOriginationDate, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(LifecycleSettings.LIFECYCLE_NAME, policy) .put(LifecycleSettings.LIFECYCLE_ORIGINATION_DATE, originationDate) ); createIndexWithSettings( client(), withoutOriginationDate, alias + withoutOriginationDate, Settings.builder() .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1) .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0) .put(LifecycleSettings.LIFECYCLE_NAME, policy) ); assertBusy(() -> { Map<String, Map<String, Object>> explain = explain(client(), this.index + "*", false, false); Map<String, Object> explainIndexWithOriginationDate = explain.get(withOriginationDate); assertThat(explainIndexWithOriginationDate.get("age").equals(explainIndexWithOriginationDate.get("index_age")), is(false)); Map<String, Object> explainIndexWithoutOriginationDate = explain.get(withoutOriginationDate); assertThat(explainIndexWithoutOriginationDate.get("age").equals(explainIndexWithoutOriginationDate.get("index_age")), is(true)); }); }	i don't think this is stable given we use system.currenttimemillis() which would be a "wall clock" read. i suggest we unit test indexlifecycleexplainresponse and maybe we pass in a timestamp provider to getage and getindexage so we can control the values we pass in? so we could say getage( () -> system.currenttimemillis() ). what do you think?
public void testCollectNodes() throws Exception { List<DiscoveryNode> knownNodes = new CopyOnWriteArrayList<>(); try (MockTransportService seedTransport = startTransport("seed_node", knownNodes, Version.CURRENT)) { DiscoveryNode seedNode = seedTransport.getLocalDiscoNode(); knownNodes.add(seedTransport.getLocalDiscoNode()); try (MockTransportService service = MockTransportService.createNewService(Settings.EMPTY, Version.CURRENT, threadPool, null)) { service.start(); service.acceptIncomingRequests(); String clusterAlias = "test-cluster"; Settings settings = buildRandomSettings(clusterAlias, addresses(seedNode)); try (RemoteClusterConnection connection = new RemoteClusterConnection(settings, clusterAlias, service)) { CountDownLatch responseLatch = new CountDownLatch(1); AtomicReference<Function<String, DiscoveryNode>> reference = new AtomicReference<>(); AtomicReference<Exception> failReference = new AtomicReference<>(); ActionListener<Function<String, DiscoveryNode>> shardsListener = ActionListener.wrap( x -> { reference.set(x); responseLatch.countDown(); }, x -> { failReference.set(x); responseLatch.countDown(); }); connection.collectNodes(shardsListener); responseLatch.await(); assertNull(failReference.get()); assertNotNull(reference.get()); Function<String, DiscoveryNode> function = reference.get(); assertEquals(seedNode, function.apply(seedNode.getId())); assertNull(function.apply(seedNode.getId() + "foo")); assertTrue(connection.assertNoRunningConnections()); } } } }	nit: could we do a normal .get to not lose the full stack trace if this ever fails and maybe use 10s for the timeout just because we upped so many of these 5s ones to 10s+ eventually?
@Override protected StringScriptFieldRegexpQuery mutate(StringScriptFieldRegexpQuery orig) { Script script = orig.script(); String fieldName = orig.fieldName(); String pattern = orig.pattern(); int flags = orig.flags(); switch (randomInt(3)) { case 0: script = randomValueOtherThan(script, this::randomScript); break; case 1: fieldName += "modified"; break; case 2: pattern += "modified"; break; case 3: flags = randomValueOtherThan(flags, () -> randomInt(RegExp.ALL)); break; default: fail(); } return new StringScriptFieldRegexpQuery(script, leafFactory, fieldName, pattern, flags, Operations.DEFAULT_MAX_DETERMINIZED_STATES); }	i think the change is ok but i wonder why that failed the test. we're supposed to rewrite the old all transparently. @markharwood can you check that the leniency is applied correctly ?
public void addOnStartedListener(Runnable runnable) { onStartedListeners.add(runnable); }	our plugin api on the node level is pull based not push. can we add a method to pull these from clusterplugin.java that way we are consistent and we fully control who calls this method.
public static ByteSizeValue parseBytesSizeValue(String sValue, ByteSizeValue defaultValue, String settingName) throws ElasticsearchParseException { settingName = Objects.requireNonNull(settingName); if (sValue == null) { return defaultValue; } switch (sValue) { case "0": case "0b": case "0B": return ZERO; case "1b": case "1B": return ONE; case "-1": case "-1b": case "-1B": return MINUS_ONE; } String lowerSValue = sValue.toLowerCase(Locale.ROOT).trim(); if (lowerSValue.endsWith("k")) { return parse(sValue, lowerSValue, "k", ByteSizeUnit.KB, settingName); } else if (lowerSValue.endsWith("kb")) { return parse(sValue, lowerSValue, "kb", ByteSizeUnit.KB, settingName); } else if (lowerSValue.endsWith("m")) { return parse(sValue, lowerSValue, "m", ByteSizeUnit.MB, settingName); } else if (lowerSValue.endsWith("mb")) { return parse(sValue, lowerSValue, "mb", ByteSizeUnit.MB, settingName); } else if (lowerSValue.endsWith("g")) { return parse(sValue, lowerSValue, "g", ByteSizeUnit.GB, settingName); } else if (lowerSValue.endsWith("gb")) { return parse(sValue, lowerSValue, "gb", ByteSizeUnit.GB, settingName); } else if (lowerSValue.endsWith("t")) { return parse(sValue, lowerSValue, "t", ByteSizeUnit.TB, settingName); } else if (lowerSValue.endsWith("tb")) { return parse(sValue, lowerSValue, "tb", ByteSizeUnit.TB, settingName); } else if (lowerSValue.endsWith("p")) { return parse(sValue, lowerSValue, "p", ByteSizeUnit.PB, settingName); } else if (lowerSValue.endsWith("pb")) { return parse(sValue, lowerSValue, "pb", ByteSizeUnit.PB, settingName); } else if (lowerSValue.endsWith("b")) { return parseBytes(lowerSValue, settingName, sValue); } else { // Missing units: throw new ElasticsearchParseException( "failed to parse setting [{}] with value [{}] as a size in bytes: unit is missing or unrecognized", settingName, sValue); } }	this is correct but looks like an accident, suggest a comment here too. suggestion // "1" is deliberately omitted, the units are required for all values except "0" and "-1" return one;
void releaseOnClose() { synchronized (this) { final CacheFile currentCacheFile = cacheFile.getAndSet(null); if (currentCacheFile != null) { currentCacheFile.release(this); try { // It's possible that a different CachedBlobContainerIndexInput is using the same // underlying CacheFile, in that case we register a listener that would track the // CacheFile eviction once its ref count reaches 0. If this was the last reference // we just track the file cache eviction. RecoveryStateCacheFileEvictionListener listener = new RecoveryStateCacheFileEvictionListener( cacheKey, currentCacheFile, directory ); if (currentCacheFile.acquire(listener) == false) { directory.trackCacheFileEviction(cacheKey.getFileName(), currentCacheFile); } } catch (AlreadyClosedException | IOException e) { directory.trackCacheFileEviction(cacheKey.getFileName(), currentCacheFile); } } } }	i wish we could avoid doing something like this, because adding eviction listeners has an impact on file handles and we don't want to keep a filechannel opened for the cache file if there's no more indexinput opened on it. i think that this recoverystatecachefileevictionlistener is just needed in case of indexinput being all close and cachefile not evicted yet, so that when it will be evicted we can still update the recovery file details for it? otherwise, indexinput that use the same cachefile should register themselves as eviction listeners and should call directory.trackcachefileeviction() when the cachefile is evicted. i'm wondering if we could keep the cachefile in the cachedfiledetail and check in the recovered() method if it's being evicted, and then remove it from the cachefiles set there?
protected void populateIndex(String indexName, int maxIndexRequests) throws InterruptedException { final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>(); for (int i = between(10, maxIndexRequests); i >= 0; i--) { indexRequestBuilders.add(client().prepareIndex(indexName).setSource("foo", randomBoolean() ? "bar" : "baz")); } indexRandom(true, true, indexRequestBuilders); refresh(indexName); assertThat( client().admin().indices().prepareForceMerge(indexName).setOnlyExpungeDeletes(true).setFlush(true).get().getFailedShards(), equalTo(0) ); }	i think we can rename this to asserttotalhits() ?
* from the aggregation standard set of parameters */ public void register(String aggregationName, List<ValuesSourceType> valuesSourceTypes, AggregatorSupplier aggregatorSupplier) { register(aggregationName, (candidate) -> { for (ValuesSourceType valuesSourceType : valuesSourceTypes) { if (valuesSourceType.equals(candidate)) { return true; } } return false; }, aggregatorSupplier); } /** * Register an aggregator that applies to any values source type. This is a convenience method for aggregations that do not care at all * about the types of their inputs. Aggregations using this version of registration should not make any other registrations, as the * aggregator registered using this function will be applied in all cases. * * @param aggregationName The name of the family of aggregations, typically found via {@link ValuesSourceAggregationBuilder#getType()}	out of curiosity, what would happen if an agg registered "any" as well as a specific type?
public void testTopHitsAggregationWithTwoArgs() { { PhysicalPlan p = optimizeAndPlan("SELECT FIRST(keyword, int) FROM test"); assertEquals(EsQueryExec.class, p.getClass()); EsQueryExec eqe = (EsQueryExec) p; assertEquals(1, eqe.output().size()); assertEquals("FIRST(keyword, int)", eqe.output().get(0).qualifiedName()); assertTrue(eqe.output().get(0).dataType() == DataType.KEYWORD); assertThat(eqe.queryContainer().aggs().asAggBuilder().toString().replaceAll("\\\\\\\\s+", ""), endsWith("\\\\"top_hits\\\\":{\\\\"from\\\\":0,\\\\"size\\\\":1,\\\\"version\\\\":false,\\\\"seq_no_primary_term\\\\":false," + "\\\\"explain\\\\":false,\\\\"docvalue_fields\\\\":[{\\\\"field\\\\":\\\\"keyword\\\\"}]," + "\\\\"sort\\\\":[{\\\\"int\\\\":{\\\\"order\\\\":\\\\"asc\\\\",\\\\"missing\\\\":\\\\"_last\\\\",\\\\"unmapped_type\\\\":\\\\"integer\\\\"}}," + "{\\\\"keyword\\\\":{\\\\"order\\\\":\\\\"asc\\\\",\\\\"missing\\\\":\\\\"_last\\\\",\\\\"unmapped_type\\\\":\\\\"keyword\\\\"}}]}}}}}")); } { PhysicalPlan p = optimizeAndPlan("SELECT LAST(keyword, int) FROM test"); assertEquals(EsQueryExec.class, p.getClass()); EsQueryExec eqe = (EsQueryExec) p; assertEquals(1, eqe.output().size()); assertEquals("LAST(keyword, int)", eqe.output().get(0).qualifiedName()); assertTrue(eqe.output().get(0).dataType() == DataType.KEYWORD); assertThat(eqe.queryContainer().aggs().asAggBuilder().toString().replaceAll("\\\\\\\\s+", ""), endsWith("\\\\"top_hits\\\\":{\\\\"from\\\\":0,\\\\"size\\\\":1,\\\\"version\\\\":false,\\\\"seq_no_primary_term\\\\":false," + "\\\\"explain\\\\":false,\\\\"docvalue_fields\\\\":[{\\\\"field\\\\":\\\\"keyword\\\\"}]," + "\\\\"sort\\\\":[{\\\\"int\\\\":{\\\\"order\\\\":\\\\"desc\\\\",\\\\"missing\\\\":\\\\"_last\\\\",\\\\"unmapped_type\\\\":\\\\"integer\\\\"}}," + "{\\\\"keyword\\\\":{\\\\"order\\\\":\\\\"desc\\\\",\\\\"missing\\\\":\\\\"_last\\\\",\\\\"unmapped_type\\\\":\\\\"keyword\\\\"}	i cannot wrap my head around missing option for top_hits in the context of first/last/min/max... it's about how null/missing values are treated when sorting: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_missing_values. is it ok to keep missing with the default (_last), or i'm missing something, or there are no tests that have missing: _first scenario included...?
public Settings getByPrefix(String prefix) { if (prefix.isEmpty()) { return this; } char[] toPrefixCharArr = prefix.toCharArray(); toPrefixCharArr[toPrefixCharArr.length - 1]++; String toPrefix = new String(toPrefixCharArr); final Map<String, Object> subMap = settings.subMap(prefix, toPrefix); return Settings.of(subMap.isEmpty() ? Map.of() : new FilteredMap(subMap, null, prefix), secureSettings == null ? null : new PrefixedSecureSettings(secureSettings, prefix, s -> s.startsWith(prefix))); }	it took me a little bit thinking about how this works. a short comment would be appreciated, explaining this adjusts the last character of the prefix for the ending key that is non-inclusive.
public boolean isPackage() { return packaging == Packaging.RPM || packaging == Packaging.DEB; }	why is this needed? it doesn't seem any clearer than packaging == packaging.docker
* @param generation generation of the data stream * @return backing index name */ public static String getDefaultBackingIndexName(String dataStreamName, long generation) { return getDefaultBackingIndexName(dataStreamName, generation, System.currentTimeMillis(), NEW_FEATURES_VERSION); }	nit: maybe use version.current instead of new_features_version? the outcome is the same, but in other places we do the same.
public void testRollover() { DataStream ds = DataStreamTestHelper.randomInstance().promoteDataStream(); Index newWriteIndex = new Index(getDefaultBackingIndexName(ds.getName(), ds.getGeneration() + 1), UUIDs.randomBase64UUID(random())); DataStream rolledDs = ds.rollover(newWriteIndex, DataStream.NEW_FEATURES_VERSION); assertThat(rolledDs.getName(), equalTo(ds.getName())); assertThat(rolledDs.getTimeStampField(), equalTo(ds.getTimeStampField())); assertThat(rolledDs.getGeneration(), equalTo(ds.getGeneration() + 1)); assertThat(rolledDs.getIndices().size(), equalTo(ds.getIndices().size() + 1)); assertTrue(rolledDs.getIndices().containsAll(ds.getIndices())); assertTrue(rolledDs.getIndices().contains(newWriteIndex)); }	maybe also add a test here with a version before 7.11?
private XContentBuilder value(ToXContent value, ToXContent.Params params) throws IOException { if (value == null) { return nullValue(); } value.toXContent(this, params); return this; }	can we add these methods in a separate pr please?
public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, QueryShardContext context) { throw new QueryShardException(context, "Can only use prefix queries on keyword and text fields - not on [" + name + "] which is of type [" + typeName() + "]"); }	i think the same reasoning can apply for wildcard, prefix and regex queries so the default impl should throw a queryshardexception ? only stringfieldtype fields should be able to build a wildcard query.
@Override protected Query doToQuery(QueryShardContext context) throws IOException { MappedFieldType fieldType = context.fieldMapper(fieldName); Query query; if (fieldType == null) { Term term = new Term(fieldName, BytesRefs.toBytesRef(value)); query = new WildcardQuery(term); } else { query = fieldType.wildcardQuery(value, context); } if (query instanceof MultiTermQuery) { MultiTermQuery.RewriteMethod rewriteMethod = QueryParsers.parseRewriteMethod( rewrite, null, LoggingDeprecationHandler.INSTANCE); QueryParsers.setRewriteMethod((MultiTermQuery) query, rewriteMethod); } return query; }	nit: if the field does not exist we could return a matchnodocsquery ?
static ClusterState addIndexTemplateV2(final ClusterState currentState, final boolean create, final String name, final IndexTemplateV2 template) { if (create && currentState.metaData().templatesV2().containsKey(name)) { throw new IllegalArgumentException("index template [" + name + "] already exists"); } Map<String, List<String>> overlaps = findConflictingV1Templates(currentState, name, template.indexPatterns()); if (overlaps.size() > 0) { String warning = String.format(Locale.ROOT, "index template [%s] has index patterns %s matching patterns from " + "existing older templates [%s] with patterns (%s); this template [%s] will take precedence during new index creation", name, template.indexPatterns(), Strings.collectionToCommaDelimitedString(overlaps.keySet()), overlaps.entrySet().stream() .map(e -> e.getKey() + " => " + e.getValue()) .collect(Collectors.joining(",")), name); logger.warn(warning); deprecationLogger.deprecated(warning); } // TODO: validation of index template // validateAndAddTemplate(request, templateBuilder, indicesService, xContentRegistry); logger.info("adding index template [{}]", name); return ClusterState.builder(currentState) .metaData(MetaData.builder(currentState.metaData()).put(name, template)) .build(); }	i think we should overlapping v2 templates with the same priority in the same way (but then throw an error) (in another change)
static Map<String, List<String>> findConflictingV1Templates(final ClusterState state, final String candidateName, final List<String> indexPatterns) { Automaton v2automaton = Regex.simpleMatchToAutomaton(indexPatterns.toArray(Strings.EMPTY_ARRAY)); Map<String, List<String>> overlappingTemplates = new HashMap<>(); for (ObjectObjectCursor<String, IndexTemplateMetaData> cursor : state.metaData().templates()) { String name = cursor.key; IndexTemplateMetaData template = cursor.value; Automaton v1automaton = Regex.simpleMatchToAutomaton(template.patterns().toArray(Strings.EMPTY_ARRAY)); if (Operations.isEmpty(Operations.intersection(v2automaton, v1automaton)) == false) { logger.debug("index template {} and old template {} would overlap: {} <=> {}", candidateName, name, indexPatterns, template.patterns()); overlappingTemplates.put(name, template.patterns()); } } return overlappingTemplates; }	this method makes validating overlapping templates really easy for index templates v2 :)
public static void setThreadContext(ThreadContext threadContext) { Objects.requireNonNull(threadContext, "Cannot register a null ThreadContext"); // add returning false means it _did_ have it already if (THREAD_CONTEXT.add(threadContext) == false) { throw new IllegalStateException("Double-setting ThreadContext not allowed!"); } } /** * Remove the {@link ThreadContext} used to add deprecation headers to network responses. * <p> * This is expected to <em>only</em> be invoked by the {@code Node}'s {@code close} method (therefore once outside of tests). * Node: This method should be called before closing a <code>ThreadContext</code>. * @see ThreadContext#close() * @param threadContext The thread context owned by the {@code ThreadPool} (and implicitly a {@code Node}	this exception should not be thrown, closing a threadpool now also removes threadcontext from a deprecationlogger. it is possible to close a threadpool multiple times as per nodestests.testawaitclosetimeoutsonnoninterruptibletask this could happen when someone is calling an api multiple times.
public void test70RestartServer() throws IOException { installation = install(distribution()); assertInstalled(distribution()); Shell sh = newShell(); startElasticsearch(sh); restartElasticsearch(sh); runElasticsearchTests(); stopElasticsearch(sh); }	would it make sense to have this as a field set up in a @before ?
public void test70RestartServer() throws IOException { installation = install(distribution()); assertInstalled(distribution()); Shell sh = newShell(); startElasticsearch(sh); restartElasticsearch(sh); runElasticsearchTests(); stopElasticsearch(sh); }	could this be done in @after or @before if you wish to avoid the repetitive calls and make sure we always start clean ? if we have tests that break with this now, i think that's something we need to fix as it makes the test hard to read if different tests depend on each-other even when run in sequence.
@Override public boolean verify(SecureString text, char[] hash) { return CharArrays.constantTimeEquals(text.getChars(), hash); }	i don't think we need this introduced here as part of the enum. there is no use for it and it might confuse someone to actually use it for password hashing while they should not. i suggest a private method for the hashing ( we don't need the verification part ) and calling that from the hash() and verify() of the newly introduced hasher. we should probably remove the sha256 from here and i shouldn't have added it in the first place. ( can do the latter in an unrelated pr ) unless we feel there is need/value in exposing unsalted sha256 / sha512 for caching hash algorithms but i don't think we should.
* * @param name The name of the algorithm and cost combination identifier * @return the hasher associated with the identifier */ public static Hasher resolve(String name) { switch (name.toLowerCase(Locale.ROOT)) { case "bcrypt": return BCRYPT; case "bcrypt4": return BCRYPT4; case "bcrypt5": return BCRYPT5; case "bcrypt6": return BCRYPT6; case "bcrypt7": return BCRYPT7; case "bcrypt8": return BCRYPT8; case "bcrypt9": return BCRYPT9; case "bcrypt10": return BCRYPT; case "bcrypt11": return BCRYPT11; case "bcrypt12": return BCRYPT12; case "bcrypt13": return BCRYPT13; case "bcrypt14": return BCRYPT14; case "pbkdf2": return PBKDF2; case "pbkdf2_1000": return PBKDF2_1000; case "pbkdf2_10000": return PBKDF2; case "pbkdf2_50000": return PBKDF2_50000; case "pbkdf2_100000": return PBKDF2_100000; case "pbkdf2_500000": return PBKDF2_500000; case "pbkdf2_1000000": return PBKDF2_1000000; case "pbkdf2_stretch": return PBKDF2_STRETCH; case "pbkdf2_stretch_1000": return PBKDF2_STRETCH_1000; case "pbkdf2_stretch_10000": return PBKDF2_STRETCH; case "pbkdf2_stretch_50000": return PBKDF2_STRETCH_50000; case "pbkdf2_stretch_100000": return PBKDF2_STRETCH_100000; case "pbkdf2_stretch_500000": return PBKDF2_STRETCH_500000; case "pbkdf2_stretch_1000000": return PBKDF2_STRETCH_1000000; case "sha1": return SHA1; case "md5": return MD5; case "ssha256": return SSHA256; case "noop": case "clear_text": return NOOP; default: throw new IllegalArgumentException("unknown hash function [" + name + "]"); } } /** * Returns a {@link Hasher} instance that can be used to verify the {@code hash} by inspecting the * hash prefix and determining the algorithm used for its generation. If no specific algorithm * prefix, can be determined {@code Hasher.NOOP}	nit: i understand this follows an existing pattern for pbkdf2. but strictly speaking i think it is incorrect. we have an explicit entry of pbkdf2_stretch_10000 and it should be returned here. this currently works because default cost is 10k. but the pbkdf2_default_cost parameter has no inherent connection to the string of "pbkdf2_stretch_10000". that is, if someone updates pbkdf2_default_cost, there is no guarantee or enforcement that this part of code would be updated accordingly as well.
public void testAliasWithSubfieldsAndDifferentRootFields_AndObjects() throws Exception { createIndexWithMapping("test1", builder -> { builder.startObject("id").field("type", "keyword").endObject(); builder.startObject("name") .field("type", "text") .startObject("fields") .startObject("raw") .field("type", "keyword") .endObject() .endObject() .endObject(); builder.startObject("address") .startObject("properties") .startObject("city") .field("type", "text") .startObject("fields") .startObject("raw") .field("type", "keyword") .endObject() .endObject() .endObject() .startObject("county") .field("type", "keyword") .startObject("fields") .startObject("raw") .field("type", "keyword") .endObject() .endObject() .endObject() .endObject() .endObject(); }); createIndexWithMapping("test2", builder -> { builder.startObject("id").field("type", "keyword").endObject(); builder.startObject("name") .field("type", "keyword") // <-------- first difference in mapping .startObject("fields") .startObject("raw") .field("type", "keyword") .endObject() .endObject() .endObject(); builder.startObject("address") .startObject("properties") .startObject("city") .field("type", "text") .startObject("fields") .startObject("raw") .field("type", "keyword") .endObject() .endObject() .endObject() .startObject("county") .field("type", "text") // <-------- second difference in mapping .startObject("fields") .startObject("raw") .field("type", "keyword") .endObject() .endObject() .endObject() .endObject() .endObject(); }); createAliases(builder -> { builder.startObject().startObject("add").field("index", "test1").field("alias", "test_alias").endObject().endObject(); builder.startObject().startObject("add").field("index", "test2").field("alias", "test_alias").endObject().endObject(); }); assertResultsForQuery("SYS COLUMNS", new String[][] { {"test1" ,"address.city" ,"TEXT"}, {"test1" ,"address.city.raw" ,"KEYWORD"}, {"test1" ,"address.county" ,"KEYWORD"}, {"test1" ,"address.county.raw","KEYWORD"}, {"test1" ,"id" ,"KEYWORD"}, {"test1" ,"name" ,"TEXT"}, {"test1" ,"name.raw" ,"KEYWORD"}, {"test2" ,"address.city" ,"TEXT"}, {"test2" ,"address.city.raw" ,"KEYWORD"}, {"test2" ,"address.county" ,"TEXT"}, {"test2" ,"address.county.raw","KEYWORD"}, {"test2" ,"id" ,"KEYWORD"}, {"test2" ,"name" ,"KEYWORD"}, {"test2" ,"name.raw" ,"KEYWORD"}, {"test_alias" ,"address.city" ,"TEXT"}, {"test_alias" ,"address.city.raw" ,"KEYWORD"}, {"test_alias" ,"id" ,"KEYWORD"} // address.county gets removed since it has conflicting mappings }); }	that is incorrect - the field should show up as invalid. as should its subfields. otherwise test_alias looks like an invalid alias, potentially pointing to other indices.
*/ public static boolean update(Map<String, Object> source, Map<String, Object> changes, boolean checkUpdatesAreUnequal) { boolean modified = false; for (Map.Entry<String, Object> changesEntry : changes.entrySet()) { if (!source.containsKey(changesEntry.getKey())) { // safe to copy, change does not exist in source source.put(changesEntry.getKey(), changesEntry.getValue()); modified = true; continue; } Object old = source.get(changesEntry.getKey()); if (old instanceof Map && changesEntry.getValue() instanceof Map) { // recursive merge maps modified = update((Map<String, Object>) source.get(changesEntry.getKey()), (Map<String, Object>) changesEntry.getValue(), checkUpdatesAreUnequal); continue; } // update the field source.put(changesEntry.getKey(), changesEntry.getValue()); if (modified) { continue; } if (!checkUpdatesAreUnequal || old == null) { modified = true; continue; } modified = !old.equals(changesEntry.getValue()); } return modified; } /** * Merges the defaults provided as the second parameter into the content of the first. Only does recursive merge * for inner maps. */ @SuppressWarnings({"unchecked"}	should it be |= instead of =? otherwise the last one would always win?
protected StartTransformResponse startTransformWithRetryOnConflict(String id, RequestOptions options) throws Exception { ElasticsearchStatusException lastConflict = null; for (int retries = 10; retries > 0; --retries) { try (RestHighLevelClient restClient = new TestRestHighLevelClient()) { return restClient.transform().startTransform(new StartTransformRequest(id), options); } catch (ElasticsearchStatusException e) { if (RestStatus.CONFLICT.equals(e.status()) == false) { throw e; } lastConflict = e; --retries; Thread.sleep(5); } } throw lastConflict; }	this line is not needed anymore.
void recordUpdateTimeInClusterState(ActionListener<Boolean> listener) { clusterService.submitStateUpdateTask("ml-memory-last-update-time", new AckedClusterStateUpdateTask<Boolean>(ACKED_REQUEST, listener) { @Override protected Boolean newResponse(boolean acknowledged) { return acknowledged; } @Override public ClusterState execute(ClusterState currentState) { MlMetadata currentMlMetadata = MlMetadata.getMlMetadata(currentState); MlMetadata.Builder builder = new MlMetadata.Builder(currentMlMetadata); MlMetadata newMlMetadata = builder.build(); builder.setLastMemoryRefreshTime(lastUpdateTime); if (newMlMetadata.equals(currentMlMetadata)) { // Return same reference if nothing has changed return currentState; } else { ClusterState.Builder newState = ClusterState.builder(currentState); newState.metaData(MetaData.builder(currentState.getMetaData()).putCustom(MlMetadata.TYPE, newMlMetadata).build()); return newState.build(); } } }); }	this line should come before mlmetadata newmlmetadata = builder.build(); or are you wanting to compare the old and new metadatas before modifying the last refresh time? either way line 246 should put the updated metadata
@Override public String segString(Directory dir) { return "IndexUpgraderMergeSpec[" + super.segString(dir) + "]"; } } static MergeSpecification upgradedMergeSpecification(MergeSpecification spec) { if (spec == null) { return null; } MergeSpecification upgradedSpec = new IndexUpgraderMergeSpecification(); for (OneMerge merge : spec.merges) { upgradedSpec.add(merge); } return upgradedSpec; } @Override public MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos segmentInfos, IndexWriter writer) throws IOException { return upgradedMergeSpecification(delegate.findMerges(mergeTrigger, segmentInfos, writer)); } @Override public MergeSpecification findForcedMerges(SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentCommitInfo,Boolean> segmentsToMerge, IndexWriter writer) throws IOException { if (force) { List<SegmentCommitInfo> segments = Lists.newArrayList(); for (SegmentCommitInfo info : segmentInfos) { if (segmentsToMerge.containsKey(info)) { segments.add(info); } } if (!segments.isEmpty()) { MergeSpecification spec = new IndexUpgraderMergeSpecification(); spec.add(new OneMerge(segments)); return spec; } } return upgradedMergeSpecification(delegate.findForcedMerges(segmentInfos, maxSegmentCount, segmentsToMerge, writer)); } @Override public MergeSpecification findForcedDeletesMerges(SegmentInfos segmentInfos, IndexWriter writer) throws IOException { return upgradedMergeSpecification(delegate.findForcedDeletesMerges(segmentInfos, writer)); } @Override public void close() { delegate.close(); } @Override public boolean useCompoundFile(SegmentInfos segments, SegmentCommitInfo newSegment, IndexWriter writer) throws IOException { return delegate.useCompoundFile(segments, newSegment, writer); } /** * When <code>force</code> is true, running a force merge will cause a merge even if there * is a single segment in the directory. This will apply to all calls to * {@link IndexWriter#forceMerge} that are handled by this {@link MergePolicy}. */ public void setForce(boolean force) { this.force = force; }	i think you missed to remove the clone calls in tieredmergepolicyprovider.customtieredmergepolicyprovider logbytesizemergepolicyprovider.customlogbytesizemergepolicy
@Override boolean isLazy() { return false; } }, EAGER_PER_FIELD { @Override CreateIndexRequestBuilder createIndex(String indexName, String type, String fieldName) throws Exception { return client().admin().indices().prepareCreate(indexName).setSettings(ImmutableSettings.builder().put(SINGLE_SHARD_NO_REPLICA).put(SearchService.NORMS_LOADING_KEY, Loading.LAZY_VALUE)).addMapping(type, JsonXContent.contentBuilder() .startObject() .startObject(type) .startObject("properties") .startObject(fieldName) .field("type", "string") .startObject("norms") .field("loading", Loading.EAGER_VALUE) .endObject() .endObject() .endObject() .endObject() .endObject() ); } @Override boolean isLazy() { return false; } }; private static Settings SINGLE_SHARD_NO_REPLICA = ImmutableSettings.builder().put("number_of_shards", 1).put("number_of_replicas", 0).build(); abstract CreateIndexRequestBuilder createIndex(String indexName, String type, String fieldName) throws Exception; boolean isLazy() { return true; } } // NOTE: we have to ensure we defeat compression strategies of the default codec... public void testEagerLoading() throws Exception { for (LoadingMethod method : LoadingMethod.values()) { logger.debug("METHOD " + method); String indexName = method.name().toLowerCase(Locale.ROOT); assertAcked(method.createIndex(indexName, "t", "foo")); // index a doc with 1 token, and one with 3 tokens so we dont get CONST compressed (otherwise norms take zero memory usage) client().prepareIndex(indexName, "t", "1").setSource("foo", "bar").execute().actionGet(); client().prepareIndex(indexName, "t", "2").setSource("foo", "bar baz foo").setRefresh(true).execute().actionGet(); ensureGreen(indexName); long memoryUsage0 = getSegmentsMemoryUsage(indexName); // queries load norms if they were not loaded before client().prepareSearch(indexName).setQuery(QueryBuilders.matchQuery("foo", "bar")).execute().actionGet(); long memoryUsage1 = getSegmentsMemoryUsage(indexName); if (method.isLazy()) { assertThat(memoryUsage1, greaterThan(memoryUsage0)); } else { assertThat(memoryUsage1, equalTo(memoryUsage0)); }	that was a nice test bug to have :)
@Override public void readFrom(StreamInput in) throws IOException { int version = in.readVInt(); // version id = in.readString(); type = in.readString(); source = in.readBytesReference(); try { if (version >= 1) { if (in.readBoolean()) { routing = in.readString(); } } if (version >= 2) { if (in.readBoolean()) { parent = in.readString(); } } if (version >= 3) { this.version = in.readLong(); } if (version >= 4) { this.timestamp = in.readLong(); } if (version >= 5) { this.ttl = in.readLong(); } if (version >= 6) { this.versionType = VersionType.fromValue(in.readByte()); } } catch (Throwable t) { throw new ElasticsearchException("failed to read [" + type + "][" + id + "]", t); } assert versionType.validateVersionForWrites(version); }	cathcing throwable is always a bad idea, because it also catches interruptedexception and errors and this causes the interrupt status to be ignored later. it also hides stuff like oom!!! you should be more selective in catching those exceptions, the only one that can happen here should be illegalstateex and ioexeption? so please, use a multi-catch!
public void testThatStatusGetsUpdated() { WatcherClient watcherClient = watcherClient(); watcherClient.preparePutWatch("_name") .setSource(watchBuilder() .trigger(schedule(interval(5, SECONDS))) .input(simpleInput()) .condition(NeverCondition.INSTANCE) .addAction("_logger", loggingAction("logged text"))) .get(); timeWarp().trigger("_name"); GetWatchResponse getWatchResponse = watcherClient.prepareGetWatch().setId("_name").get(); assertThat(getWatchResponse.isFound(), is(true)); assertThat(getWatchResponse.getSource(), notNullValue()); assertThat(getWatchResponse.getStatus().lastChecked(), is(notNullValue())); GetResponse getResponse = client().prepareGet(".watches", "doc", "_name").get(); getResponse.getSource(); XContentSource source = new XContentSource(getResponse.getSourceAsBytesRef(), XContentType.JSON); String lastChecked = source.getValue("status.last_checked"); assertThat(lastChecked, WatcherTestUtils.isSameDate(getWatchResponse.getStatus().lastChecked())); assertThat(getWatchResponse.getStatus().lastChecked(), isMillisResolution()); // not started yet, so both nulls String lastMetCondition = source.getValue("status.last_met_condition"); assertThat(lastMetCondition, is(nullValue())); assertThat(getWatchResponse.getStatus().lastMetCondition(), is(nullValue())); }	alternatively just actual.getnano() % 1000 == 0 - not sure if it is more readable to be honest, so feel free to ignore me :-)
@Override protected void removeDataBefore(Job job, long cutoffEpochMs, ActionListener<Boolean> listener) { listener.onResponse(Boolean.TRUE); } } private OriginSettingClient originSettingClient; private Client client; @Before public void setUpTests() { client = mock(Client.class); originSettingClient = MockOriginSettingClient.mockOriginSettingClient(client, ClientHelper.ML_ORIGIN); } static SearchResponse createSearchResponse(List<? extends ToXContent> toXContents) throws IOException { return createSearchResponse(toXContents, toXContents.size()); } @SuppressWarnings("unchecked") static void givenJobs(Client client, List<Job> jobs) throws IOException { SearchResponse response = AbstractExpiredJobDataRemoverTests.createSearchResponse(jobs); doAnswer(invocationOnMock -> { ActionListener<SearchResponse> listener = (ActionListener<SearchResponse>) invocationOnMock.getArguments()[2]; listener.onResponse(response); return null; }).when(client).execute(eq(SearchAction.INSTANCE), any(), any()); } private static SearchResponse createSearchResponse(List<? extends ToXContent> toXContents, int totalHits) throws IOException { SearchHit[] hitsArray = new SearchHit[toXContents.size()]; for (int i = 0; i < toXContents.size(); i++) { hitsArray[i] = new SearchHit(randomInt()); XContentBuilder jsonBuilder = JsonXContent.contentBuilder(); toXContents.get(i).toXContent(jsonBuilder, ToXContent.EMPTY_PARAMS); hitsArray[i].sourceRef(BytesReference.bytes(jsonBuilder)); } SearchHits hits = new SearchHits(hitsArray, new TotalHits(totalHits, TotalHits.Relation.EQUAL_TO), 1.0f); SearchResponse searchResponse = mock(SearchResponse.class); when(searchResponse.getHits()).thenReturn(hits); return searchResponse; } public void testRemoveGivenNoJobs() throws IOException { SearchResponse response = createSearchResponse(Collections.emptyList()); mockSearchResponse(response); TestListener listener = new TestListener(); ConcreteExpiredJobDataRemover remover = new ConcreteExpiredJobDataRemover(originSettingClient); remover.remove(listener, () -> false); listener.waitToCompletion(); assertThat(listener.success, is(true)); assertEquals(0, remover.getRetentionDaysCallCount); } @SuppressWarnings("unchecked") public void testRemoveGivenMultipleBatches() throws IOException { // This is testing AbstractExpiredJobDataRemover.WrappedBatchedJobsIterator int totalHits = 7; List<SearchResponse> responses = new ArrayList<>(); responses.add(createSearchResponse(Arrays.asList( JobTests.buildJobBuilder("job1").build(), JobTests.buildJobBuilder("job2").build(), JobTests.buildJobBuilder("job3").build() ), totalHits)); responses.add(createSearchResponse(Arrays.asList( JobTests.buildJobBuilder("job4").build(), JobTests.buildJobBuilder("job5").build(), JobTests.buildJobBuilder("job6").build() ), totalHits)); responses.add(createSearchResponse(Collections.singletonList( JobTests.buildJobBuilder("job7").build() ), totalHits)); AtomicInteger searchCount = new AtomicInteger(0); doAnswer(invocationOnMock -> { ActionListener<SearchResponse> listener = (ActionListener<SearchResponse>) invocationOnMock.getArguments()[2]; listener.onResponse(responses.get(searchCount.getAndIncrement())); return null; }).when(client).execute(eq(SearchAction.INSTANCE), any(), any()); TestListener listener = new TestListener(); ConcreteExpiredJobDataRemover remover = new ConcreteExpiredJobDataRemover(originSettingClient); remover.remove(listener, () -> false); listener.waitToCompletion(); assertThat(listener.success, is(true)); assertEquals(3, searchCount.get()); assertEquals(7, remover.getRetentionDaysCallCount); } public void testRemoveGivenTimeOut() throws IOException { int totalHits = 3; SearchResponse response = createSearchResponse(Arrays.asList( JobTests.buildJobBuilder("job1").build(), JobTests.buildJobBuilder("job2").build(), JobTests.buildJobBuilder("job3").build() ), totalHits); final int timeoutAfter = randomIntBetween(0, totalHits - 1); AtomicInteger attemptsLeft = new AtomicInteger(timeoutAfter); mockSearchResponse(response); TestListener listener = new TestListener(); ConcreteExpiredJobDataRemover remover = new ConcreteExpiredJobDataRemover(originSettingClient); remover.remove(listener, () -> (attemptsLeft.getAndDecrement() <= 0)); listener.waitToCompletion(); assertThat(listener.success, is(false)); assertEquals(timeoutAfter, remover.getRetentionDaysCallCount); } @SuppressWarnings("unchecked") private void mockSearchResponse(SearchResponse searchResponse) { doAnswer(invocationOnMock -> { ActionListener<SearchResponse> listener = (ActionListener<SearchResponse>) invocationOnMock.getArguments()[2]; listener.onResponse(searchResponse); return null; }).when(client).execute(eq(SearchAction.INSTANCE), any(), any()); } static class TestListener implements ActionListener<Boolean> { boolean success; private final CountDownLatch latch = new CountDownLatch(1); @Override public void onResponse(Boolean aBoolean) { success = aBoolean; latch.countDown(); } @Override public void onFailure(Exception e) { latch.countDown(); } void waitToCompletion() { try { latch.await(3, TimeUnit.SECONDS); } catch (InterruptedException e) { fail("listener timed out before completing"); } }	could this snippet be replaced with an appropriate mocksearchresponse call?
public void testRemove_GivenJobsWithoutRetentionPolicy() throws IOException { givenClientRequestsSucceed(Arrays.asList( JobTests.buildJobBuilder("foo").build(), JobTests.buildJobBuilder("bar").build() )); createExpiredModelSnapshotsRemover().remove(listener, () -> false); listener.waitToCompletion(); assertThat(listener.success, is(true)); verify(client).execute(eq(SearchAction.INSTANCE), any(), any()); }	why is this removed? are there any new client interactions expected after your change?
public void setUpTests() { capturedDeleteByQueryRequests = new ArrayList<>(); client = org.mockito.Mockito.mock(Client.class); originSettingClient = MockOriginSettingClient.mockOriginSettingClient(client, ClientHelper.ML_ORIGIN); listener = mock(ActionListener.class); }	could org.mockito.mockito.mock be imported to avoid qualified name?
void updateRefreshedCheckpoint(long checkpoint) { refreshedCheckpoint.updateAndGet(curr -> Math.max(curr, checkpoint)); assert refreshedCheckpoint.get() >= checkpoint : refreshedCheckpoint.get() + " < " + checkpoint; } } @Override public final long getMaxSeenAutoIdTimestamp() { return maxSeenAutoIdTimestamp.get(); } @Override public final void updateMaxUnsafeAutoIdTimestamp(long newTimestamp) { updateAutoIdTimestamp(newTimestamp, true); } private void updateAutoIdTimestamp(long newTimestamp, boolean unsafe) { assert newTimestamp >= -1 : "invalid timestamp [" + newTimestamp + "]"; maxSeenAutoIdTimestamp.updateAndGet(curr -> Math.max(curr, newTimestamp)); if (unsafe) { maxUnsafeAutoIdTimestamp.updateAndGet(curr -> Math.max(curr, newTimestamp)); } assert maxUnsafeAutoIdTimestamp.get() <= maxSeenAutoIdTimestamp.get(); } private boolean assertMaxSeqNoOfUpdatesIsAdvanced(Term id, long seqNo, boolean allowDeleted, boolean relaxIfGapInSeqNo) { final long maxSeqNoOfUpdates = getMaxSeqNoOfUpdatesOrDeletes(); // If the primary is on an old version which does not replicate msu, we need to relax this assertion for that. if (maxSeqNoOfUpdates == SequenceNumbers.UNASSIGNED_SEQ_NO) { assert config().getIndexSettings().getIndexVersionCreated().before(Version.V_6_5_0); return true; } // We treat a delete on the tombstones on replicas as a regular document, then use updateDocument (not addDocument). if (allowDeleted) { final VersionValue versionValue = versionMap.getVersionForAssert(id.bytes()); if (versionValue != null && versionValue.isDelete()) { return true; } } // Operations can be processed on a replica in a different order than on the primary. If the order on the primary is index-1, // delete-2, index-3, and the order on a replica is index-1, index-3, delete-2, then the msu of index-3 on the replica is 2 // even though it is an update (overwrites index-1). We should relax this assertion if there is a pending gap in the seq_no. if (relaxIfGapInSeqNo && getLocalCheckpoint() < maxSeqNoOfUpdates) { return true; } assert seqNo <= maxSeqNoOfUpdates : "id=" + id + " seq_no=" + seqNo + " msu=" + maxSeqNoOfUpdates; return true; } @Override public void reinitializeMaxSeqNoOfUpdatesOrDeletes() { final long maxSeqNo = SequenceNumbers.max(localCheckpointTracker.getMaxSeqNo(), translog.getMaxSeqNo()); advanceMaxSeqNoOfUpdatesOrDeletes(maxSeqNo); } private static void trimUnsafeCommits(EngineConfig engineConfig) throws IOException { final Store store = engineConfig.getStore(); final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); final Path translogPath = engineConfig.getTranslogConfig().getTranslogPath(); final long globalCheckpoint = Translog.readGlobalCheckpoint(translogPath, translogUUID); final long minRetainedTranslogGen = Translog.readMinTranslogGeneration(translogPath, translogUUID); store.trimUnsafeCommits(globalCheckpoint, minRetainedTranslogGen, engineConfig.getIndexSettings().getIndexVersionCreated()); } protected void verifyEngineBeforeIndexClosing() { final long globalCheckpoint = translog.getLastSyncedGlobalCheckpoint(); final long maxSeqNo = localCheckpointTracker.getMaxSeqNo(); if (globalCheckpoint != maxSeqNo) { throw new IllegalStateException("Global checkpoint [" + globalCheckpoint + "] mismatches maximum sequence number [" + maxSeqNo + "] on index shard " + shardId); } } @Override public String prepareEngineBeforeIndexClosing(String syncId) throws IOException { try (ReleasableLock ignored = writeLock.acquire()) { ensureOpen(); syncTranslog(); // make sure that we persist the global checkpoint to translog checkpoint verifyEngineBeforeIndexClosing(); // we can reuse the existing syncId if there was no indexing activity since the last synced-flush. if (indexWriter.hasUncommittedChanges() == false && lastCommittedSegmentInfos.userData.containsKey(Engine.SYNC_COMMIT_ID)) { syncId = lastCommittedSegmentInfos.userData.get(Engine.SYNC_COMMIT_ID); } final CommitId commitId = flush(true, true); if (syncId != null) { syncFlush(syncId, commitId); } return syncId; }	could we not just bail out here instead? if the sync marker is already present, we are good? especially if we are recovering, it seems like we would disturb more than help by following lines.
void updateRefreshedCheckpoint(long checkpoint) { refreshedCheckpoint.updateAndGet(curr -> Math.max(curr, checkpoint)); assert refreshedCheckpoint.get() >= checkpoint : refreshedCheckpoint.get() + " < " + checkpoint; } } @Override public final long getMaxSeenAutoIdTimestamp() { return maxSeenAutoIdTimestamp.get(); } @Override public final void updateMaxUnsafeAutoIdTimestamp(long newTimestamp) { updateAutoIdTimestamp(newTimestamp, true); } private void updateAutoIdTimestamp(long newTimestamp, boolean unsafe) { assert newTimestamp >= -1 : "invalid timestamp [" + newTimestamp + "]"; maxSeenAutoIdTimestamp.updateAndGet(curr -> Math.max(curr, newTimestamp)); if (unsafe) { maxUnsafeAutoIdTimestamp.updateAndGet(curr -> Math.max(curr, newTimestamp)); } assert maxUnsafeAutoIdTimestamp.get() <= maxSeenAutoIdTimestamp.get(); } private boolean assertMaxSeqNoOfUpdatesIsAdvanced(Term id, long seqNo, boolean allowDeleted, boolean relaxIfGapInSeqNo) { final long maxSeqNoOfUpdates = getMaxSeqNoOfUpdatesOrDeletes(); // If the primary is on an old version which does not replicate msu, we need to relax this assertion for that. if (maxSeqNoOfUpdates == SequenceNumbers.UNASSIGNED_SEQ_NO) { assert config().getIndexSettings().getIndexVersionCreated().before(Version.V_6_5_0); return true; } // We treat a delete on the tombstones on replicas as a regular document, then use updateDocument (not addDocument). if (allowDeleted) { final VersionValue versionValue = versionMap.getVersionForAssert(id.bytes()); if (versionValue != null && versionValue.isDelete()) { return true; } } // Operations can be processed on a replica in a different order than on the primary. If the order on the primary is index-1, // delete-2, index-3, and the order on a replica is index-1, index-3, delete-2, then the msu of index-3 on the replica is 2 // even though it is an update (overwrites index-1). We should relax this assertion if there is a pending gap in the seq_no. if (relaxIfGapInSeqNo && getLocalCheckpoint() < maxSeqNoOfUpdates) { return true; } assert seqNo <= maxSeqNoOfUpdates : "id=" + id + " seq_no=" + seqNo + " msu=" + maxSeqNoOfUpdates; return true; } @Override public void reinitializeMaxSeqNoOfUpdatesOrDeletes() { final long maxSeqNo = SequenceNumbers.max(localCheckpointTracker.getMaxSeqNo(), translog.getMaxSeqNo()); advanceMaxSeqNoOfUpdatesOrDeletes(maxSeqNo); } private static void trimUnsafeCommits(EngineConfig engineConfig) throws IOException { final Store store = engineConfig.getStore(); final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY); final Path translogPath = engineConfig.getTranslogConfig().getTranslogPath(); final long globalCheckpoint = Translog.readGlobalCheckpoint(translogPath, translogUUID); final long minRetainedTranslogGen = Translog.readMinTranslogGeneration(translogPath, translogUUID); store.trimUnsafeCommits(globalCheckpoint, minRetainedTranslogGen, engineConfig.getIndexSettings().getIndexVersionCreated()); } protected void verifyEngineBeforeIndexClosing() { final long globalCheckpoint = translog.getLastSyncedGlobalCheckpoint(); final long maxSeqNo = localCheckpointTracker.getMaxSeqNo(); if (globalCheckpoint != maxSeqNo) { throw new IllegalStateException("Global checkpoint [" + globalCheckpoint + "] mismatches maximum sequence number [" + maxSeqNo + "] on index shard " + shardId); } } @Override public String prepareEngineBeforeIndexClosing(String syncId) throws IOException { try (ReleasableLock ignored = writeLock.acquire()) { ensureOpen(); syncTranslog(); // make sure that we persist the global checkpoint to translog checkpoint verifyEngineBeforeIndexClosing(); // we can reuse the existing syncId if there was no indexing activity since the last synced-flush. if (indexWriter.hasUncommittedChanges() == false && lastCommittedSegmentInfos.userData.containsKey(Engine.SYNC_COMMIT_ID)) { syncId = lastCommittedSegmentInfos.userData.get(Engine.SYNC_COMMIT_ID); } final CommitId commitId = flush(true, true); if (syncId != null) { syncFlush(syncId, commitId); } return syncId; }	maybe add a comment here on why this is safe? my head is still spinning on whether this is safe or not, but i currently think the cluster block and primary all permit is enough. also, i think we can assert that we hold all permits.
public void testOperationFailsWithNoBlock() { setState(clusterService, new ClusterState.Builder(new ClusterName("test")).build()); IllegalStateException exception = expectThrows(IllegalStateException.class, this::executeOnPrimaryOrReplica); assertThat(exception.getMessage(), equalTo("Index shard " + indexShard.shardId() + " must be blocked by " + clusterBlock + " before closing")); }	i think it could make sense to verify that prepareshardbeforeindexclosing is not called.
@Override public Object valueForDisplay(Object value) { if (value == null) { return null; } BytesRef binaryValue = (BytesRef) value; return binaryValue.utf8ToString(); } } protected ParentIdFieldMapper(String name, boolean eagerGlobalOrdinals) { super(name, new ParentIdFieldType(name, eagerGlobalOrdinals), Lucene.KEYWORD_ANALYZER, MultiFields.empty(), CopyTo.empty()); } @Override protected void parseCreateField(ParseContext context) { throw new UnsupportedOperationException("Cannot directly call parse() on a ParentIdFieldMapper"); } public void indexValue(ParseContext context, String refId) { BytesRef binaryValue = new BytesRef(refId); Field field = new Field(fieldType().name(), binaryValue, Defaults.FIELD_TYPE); context.doc().add(field); context.doc().add(new SortedDocValuesField(fieldType().name(), binaryValue)); } @Override protected String contentType() { return CONTENT_TYPE; }	small comment, maybe createidfield would be more precise, since we aren't actually indexing values here. a similar thought applies to binaryfieldmapper#indexvalue.
public void testLockTryingToDelete() throws Exception { createIndex("test"); ensureGreen(); NodeEnvironment env = getInstanceFromNode(NodeEnvironment.class); ClusterService cs = getInstanceFromNode(ClusterService.class); final Index index = cs.state().metadata().index("test").getIndex(); Path[] shardPaths = env.availableShardPaths(new ShardId(index, 0)); logger.info("--> paths: [{}]", (Object)shardPaths); // Should not be able to acquire the lock because it's already open try { NodeEnvironment.acquireFSLockForPaths(IndexSettingsModule.newIndexSettings("test", Settings.EMPTY), shardPaths); fail("should not have been able to acquire the lock"); } catch (LockObtainFailedException e) { assertTrue("msg: " + e.getMessage(), e.getMessage().contains("unable to acquire write.lock")); } // Test without the regular shard lock to assume we can acquire it // (worst case, meaning that the shard lock could be acquired and // we're green to delete the shard's directory) final ShardLock sLock = new DummyShardLock(new ShardId(index, 0)); final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings("test", Settings.EMPTY); final AtomicBoolean listener = new AtomicBoolean(); final LockObtainFailedException exception = expectThrows(LockObtainFailedException.class, () -> env.deleteShardDirectoryUnderLock(sLock, indexSettings, indexPaths -> listener.set(true))); assertThat(exception.getMessage(), exception.getMessage(), containsString("unable to acquire write.lock")); assertFalse("Listener should not have been called", listener.get()); }	suggest assert false : indexpaths rather than listener.set(true), it might be useful to see the stack trace that led to calling this listener unexpectedly.
private static TaskProvider<?> configureDistroTest( Project project, ElasticsearchDistribution distribution, Provider<DockerSupportService> dockerSupport ) { return project.getTasks().register(destructiveDistroTestTaskName(distribution), Test.class, t -> { // Disable Docker distribution tests unless a Docker installation is available t.onlyIf(t2 -> distribution.getType() != Type.DOCKER || dockerSupport.get().getDockerAvailability().isAvailable); t.getOutputs().doNotCacheIf("Build cache is disabled for packaging tests", Specs.satisfyAll()); t.setMaxParallelForks(1); t.setWorkingDir(project.getProjectDir()); t.systemProperty(DISTRIBUTION_SYSPROP, distribution.toString()); if (System.getProperty(IN_VM_SYSPROP) == null) { t.dependsOn(distribution); } }); }	why onlyif here instead of using the requiresdocker property?
private void refreshVersioningTable(long time) { // we need to refresh in order to clear older version values refresh(new Refresh("version_table").force(true)); // TODO: not good that we reach into LiveVersionMap here; can we move this inside VersionMap instead? problem is the dirtyLock... // we only need to prune deletes; the adds/updates are cleared whenever reader is refreshed: for (Map.Entry<BytesRef, VersionValue> entry : versionMap.deletes.entrySet()) { BytesRef uid = entry.getKey(); synchronized (dirtyLock(uid)) { // can we do it without this lock on each value? maybe batch to a set and get the lock once per set? VersionValue versionValue = versionMap.deletes.get(uid); if (versionValue == null) { // another thread has re-added this uid since we started refreshing: continue; } if (time - versionValue.time() <= 0) { continue; // its a newer value, from after/during we refreshed, don't clear it } assert versionValue.delete(); if (enableGcDeletes && (time - versionValue.time()) > gcDeletesInMillis) { versionMap.deletes.remove(uid); } } } }	can we hide this behind a method?
private boolean assertDocDoesNotExist(final Index index, final boolean allowDeleted) throws IOException { final VersionValue versionValue = versionMap.getUnderLock(index.uid().bytes()); // this uses direct access to the version map - // no refresh needed here if (versionValue != null) { if (versionValue.isDelete() == false || allowDeleted == false) { throw new AssertionError("doc [" + index.type() + "][" + index.id() + "] exists in version map (version " + versionValue + ")"); } } else { try (Searcher searcher = acquireSearcher("assert doc doesn't exist", SearcherScope.INTERNAL)) { final long docsWithId = searcher.searcher().count(new TermQuery(index.uid())); if (docsWithId > 0) { throw new AssertionError("doc [" + index.type() + "][" + index.id() + "] exists [" + docsWithId + "] times in index"); } } } return true; }	is it true that it's not needed or is it more that we don't want to change refresh semantics in an assert method.
public void testVersionMapAfterAutoIDDocument() throws IOException { ParsedDocument doc = testParsedDocument("1", null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index operation = appendOnlyReplica(doc, false, 1, randomIntBetween(0, 5)); engine.index(operation); assertFalse(engine.isSafeAccessRequired()); doc = testParsedDocument("1", null, testDocumentWithTextField("updated"), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index update = indexForDoc(doc); engine.index(update); assertTrue(engine.isSafeAccessRequired()); assertEquals(1, engine.getVersionMapSize()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(0, searcher.reader().numDocs()); } try (Engine.Searcher searcher = engine.acquireSearcher("test", Engine.SearcherScope.INTERNAL)) { assertEquals(1, searcher.reader().numDocs()); TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), 1); org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[0].doc); assertEquals("test", luceneDoc.get("value")); } // now lets make this document visible engine.refresh("test"); assertTrue("safe access should be required we carried it over", engine.isSafeAccessRequired()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(1, searcher.reader().numDocs()); TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), 1); org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[0].doc); assertEquals("updated", luceneDoc.get("value")); } doc = testParsedDocument("2", null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); operation = appendOnlyPrimary(doc, false, 1); engine.index(operation); assertTrue("safe access should be required", engine.isSafeAccessRequired()); assertEquals(1, engine.getVersionMapSize()); // now we add this to the map engine.refresh("test"); if (randomBoolean()) { // force empty refresh to ensure we carry it over engine.refresh("test"); } try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(2, searcher.reader().numDocs()); } assertFalse("safe access should NOT be required last indexing round was only append only", engine.isSafeAccessRequired()); engine.delete(new Engine.Delete(operation.type(), operation.id(), operation.uid())); assertTrue("safe access should be required", engine.isSafeAccessRequired()); engine.refresh("test"); assertTrue("safe access should be required", engine.isSafeAccessRequired()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(1, searcher.reader().numDocs()); } }	should we randomize between primary and replica? they slightly different code paths in the engine.
public void testVersionMapAfterAutoIDDocument() throws IOException { ParsedDocument doc = testParsedDocument("1", null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index operation = appendOnlyReplica(doc, false, 1, randomIntBetween(0, 5)); engine.index(operation); assertFalse(engine.isSafeAccessRequired()); doc = testParsedDocument("1", null, testDocumentWithTextField("updated"), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index update = indexForDoc(doc); engine.index(update); assertTrue(engine.isSafeAccessRequired()); assertEquals(1, engine.getVersionMapSize()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(0, searcher.reader().numDocs()); } try (Engine.Searcher searcher = engine.acquireSearcher("test", Engine.SearcherScope.INTERNAL)) { assertEquals(1, searcher.reader().numDocs()); TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), 1); org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[0].doc); assertEquals("test", luceneDoc.get("value")); } // now lets make this document visible engine.refresh("test"); assertTrue("safe access should be required we carried it over", engine.isSafeAccessRequired()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(1, searcher.reader().numDocs()); TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), 1); org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[0].doc); assertEquals("updated", luceneDoc.get("value")); } doc = testParsedDocument("2", null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); operation = appendOnlyPrimary(doc, false, 1); engine.index(operation); assertTrue("safe access should be required", engine.isSafeAccessRequired()); assertEquals(1, engine.getVersionMapSize()); // now we add this to the map engine.refresh("test"); if (randomBoolean()) { // force empty refresh to ensure we carry it over engine.refresh("test"); } try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(2, searcher.reader().numDocs()); } assertFalse("safe access should NOT be required last indexing round was only append only", engine.isSafeAccessRequired()); engine.delete(new Engine.Delete(operation.type(), operation.id(), operation.uid())); assertTrue("safe access should be required", engine.isSafeAccessRequired()); engine.refresh("test"); assertTrue("safe access should be required", engine.isSafeAccessRequired()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(1, searcher.reader().numDocs()); } }	shall we add here another randomized empty refresh?
public void testVersionMapAfterAutoIDDocument() throws IOException { ParsedDocument doc = testParsedDocument("1", null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index operation = appendOnlyReplica(doc, false, 1, randomIntBetween(0, 5)); engine.index(operation); assertFalse(engine.isSafeAccessRequired()); doc = testParsedDocument("1", null, testDocumentWithTextField("updated"), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index update = indexForDoc(doc); engine.index(update); assertTrue(engine.isSafeAccessRequired()); assertEquals(1, engine.getVersionMapSize()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(0, searcher.reader().numDocs()); } try (Engine.Searcher searcher = engine.acquireSearcher("test", Engine.SearcherScope.INTERNAL)) { assertEquals(1, searcher.reader().numDocs()); TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), 1); org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[0].doc); assertEquals("test", luceneDoc.get("value")); } // now lets make this document visible engine.refresh("test"); assertTrue("safe access should be required we carried it over", engine.isSafeAccessRequired()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(1, searcher.reader().numDocs()); TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), 1); org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[0].doc); assertEquals("updated", luceneDoc.get("value")); } doc = testParsedDocument("2", null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); operation = appendOnlyPrimary(doc, false, 1); engine.index(operation); assertTrue("safe access should be required", engine.isSafeAccessRequired()); assertEquals(1, engine.getVersionMapSize()); // now we add this to the map engine.refresh("test"); if (randomBoolean()) { // force empty refresh to ensure we carry it over engine.refresh("test"); } try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(2, searcher.reader().numDocs()); } assertFalse("safe access should NOT be required last indexing round was only append only", engine.isSafeAccessRequired()); engine.delete(new Engine.Delete(operation.type(), operation.id(), operation.uid())); assertTrue("safe access should be required", engine.isSafeAccessRequired()); engine.refresh("test"); assertTrue("safe access should be required", engine.isSafeAccessRequired()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(1, searcher.reader().numDocs()); } }	same comment about randomizations
public void testVersionMapAfterAutoIDDocument() throws IOException { ParsedDocument doc = testParsedDocument("1", null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index operation = appendOnlyReplica(doc, false, 1, randomIntBetween(0, 5)); engine.index(operation); assertFalse(engine.isSafeAccessRequired()); doc = testParsedDocument("1", null, testDocumentWithTextField("updated"), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index update = indexForDoc(doc); engine.index(update); assertTrue(engine.isSafeAccessRequired()); assertEquals(1, engine.getVersionMapSize()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(0, searcher.reader().numDocs()); } try (Engine.Searcher searcher = engine.acquireSearcher("test", Engine.SearcherScope.INTERNAL)) { assertEquals(1, searcher.reader().numDocs()); TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), 1); org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[0].doc); assertEquals("test", luceneDoc.get("value")); } // now lets make this document visible engine.refresh("test"); assertTrue("safe access should be required we carried it over", engine.isSafeAccessRequired()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(1, searcher.reader().numDocs()); TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), 1); org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[0].doc); assertEquals("updated", luceneDoc.get("value")); } doc = testParsedDocument("2", null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); operation = appendOnlyPrimary(doc, false, 1); engine.index(operation); assertTrue("safe access should be required", engine.isSafeAccessRequired()); assertEquals(1, engine.getVersionMapSize()); // now we add this to the map engine.refresh("test"); if (randomBoolean()) { // force empty refresh to ensure we carry it over engine.refresh("test"); } try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(2, searcher.reader().numDocs()); } assertFalse("safe access should NOT be required last indexing round was only append only", engine.isSafeAccessRequired()); engine.delete(new Engine.Delete(operation.type(), operation.id(), operation.uid())); assertTrue("safe access should be required", engine.isSafeAccessRequired()); engine.refresh("test"); assertTrue("safe access should be required", engine.isSafeAccessRequired()); try (Engine.Searcher searcher = engine.acquireSearcher("test")) { assertEquals(1, searcher.reader().numDocs()); } }	i don't think the comment is valid here - the flag is reset by the previous refresh. i don't think this extra refresh much, maybe remove it?
public void testConcurrentAppendUpdateAndRefresh() throws InterruptedException, IOException { int numDocsPerThread = scaledRandomIntBetween(100, 1000); CountDownLatch latch = new CountDownLatch(2); AtomicInteger threadsRunning = new AtomicInteger(); AtomicInteger numDeletes = new AtomicInteger(); Thread thread = new Thread(() -> { try { threadsRunning.incrementAndGet(); latch.countDown(); latch.await(); for (int j = 0; j < numDocsPerThread; j++) { String docID = Integer.toString(j); ParsedDocument doc = testParsedDocument(docID, null, testDocumentWithTextField(), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index operation = appendOnlyPrimary(doc, false, 1); engine.index(operation); if (rarely()) { engine.delete(new Engine.Delete(operation.type(), operation.id(), operation.uid())); numDeletes.incrementAndGet(); } else { doc = testParsedDocument(docID, null, testDocumentWithTextField("updated"), new BytesArray("{}".getBytes(Charset.defaultCharset())), null); Engine.Index update = indexForDoc(doc); engine.index(update); } } } catch (Exception e) { throw new AssertionError(e); } finally { threadsRunning.decrementAndGet(); } }); thread.start(); latch.countDown(); latch.await(); while (threadsRunning.get() != 0) { engine.refresh("test", Engine.SearcherScope.INTERNAL); } thread.join(); engine.refresh("test", Engine.SearcherScope.INTERNAL); try (Engine.Searcher searcher = engine.acquireSearcher("test", Engine.SearcherScope.INTERNAL)) { TopDocs search = searcher.searcher().search(new MatchAllDocsQuery(), searcher.reader().numDocs()); for (int i = 0; i < search.scoreDocs.length; i++) { org.apache.lucene.document.Document luceneDoc = searcher.searcher().doc(search.scoreDocs[i].doc); assertEquals("updated", luceneDoc.get("value")); } int totalNumDocs = numDocsPerThread - numDeletes.get(); assertEquals(totalNumDocs, searcher.reader().numDocs()); } }	it seems this can be an atomic boolean? we ended up with just one thread..
public void testCarryOnSafeAccess() throws IOException { LiveVersionMap map = new LiveVersionMap(); assertFalse(map.isUnsafe()); assertFalse(map.isSafeAccessRequired()); map.enforceSafeAccess(); assertTrue(map.isSafeAccessRequired()); assertFalse(map.isUnsafe()); int numIters = randomIntBetween(1, 5); for (int i = 0; i < numIters; i++) { // if we don't do anything ie. no adds etc we will stay with the safe access required map.beforeRefresh(); map.afterRefresh(randomBoolean()); assertTrue("failed in iter: " + i, map.isSafeAccessRequired()); } map.maybePutUnderLock(new BytesRef(""), new VersionValue(randomLong(), randomLong(), randomLong())); assertFalse(map.isUnsafe()); assertEquals(1, map.getAllCurrent().size()); map.beforeRefresh(); map.afterRefresh(randomBoolean()); assertFalse(map.isUnsafe()); assertFalse(map.isSafeAccessRequired()); map.maybePutUnderLock(new BytesRef(""), new VersionValue(randomLong(), randomLong(), randomLong())); assertTrue(map.isUnsafe()); assertFalse(map.isSafeAccessRequired()); assertEquals(0, map.getAllCurrent().size()); }	add assert for issafeaccessmode?
public void testCarryOnSafeAccess() throws IOException { LiveVersionMap map = new LiveVersionMap(); assertFalse(map.isUnsafe()); assertFalse(map.isSafeAccessRequired()); map.enforceSafeAccess(); assertTrue(map.isSafeAccessRequired()); assertFalse(map.isUnsafe()); int numIters = randomIntBetween(1, 5); for (int i = 0; i < numIters; i++) { // if we don't do anything ie. no adds etc we will stay with the safe access required map.beforeRefresh(); map.afterRefresh(randomBoolean()); assertTrue("failed in iter: " + i, map.isSafeAccessRequired()); } map.maybePutUnderLock(new BytesRef(""), new VersionValue(randomLong(), randomLong(), randomLong())); assertFalse(map.isUnsafe()); assertEquals(1, map.getAllCurrent().size()); map.beforeRefresh(); map.afterRefresh(randomBoolean()); assertFalse(map.isUnsafe()); assertFalse(map.isSafeAccessRequired()); map.maybePutUnderLock(new BytesRef(""), new VersionValue(randomLong(), randomLong(), randomLong())); assertTrue(map.isUnsafe()); assertFalse(map.isSafeAccessRequired()); assertEquals(0, map.getAllCurrent().size()); }	add assert for issafeaccessmode?
@Override protected void masterOperation(final GetSnapshotsRequest request, final ClusterState state, final ActionListener<GetSnapshotsResponse> listener) { final String[] repositories = request.repositories(); transportService.sendRequest(transportService.getLocalNode(), GetRepositoriesAction.NAME, new GetRepositoriesRequest(repositories), new ActionListenerResponseHandler<>( ActionListener.wrap( response -> // We need to switch back to GENERIC thread pool, because we can work with BlobStoreRepository // only on GENERIC or SNAPSHOT thread pool threadPool.executor(ThreadPool.Names.GENERIC).execute( () -> getMultipleReposSnapshotInfo(response.repositories(), request.snapshots(), request.ignoreUnavailable(), request.verbose(), listener)), listener::onFailure), GetRepositoriesResponse::new)); }	nit: we could use actionlistener#completewith here :)
private DocumentMapper getMapperForUpdate(MapperService mapperService, String type) { DocumentMapper mapper = mapperService.documentMapper(type); if (mapper == null && type.equals(MapperService.SINGLE_MAPPING_NAME) && mapperService.getIndexSettings().getIndexVersionCreated().onOrAfter(Version.V_6_0_0)) { Iterator<DocumentMapper> docMappersIt = mapperService.docMappers(false).iterator(); if (docMappersIt.hasNext()) { mapper = docMappersIt.next(); } if (docMappersIt.hasNext()) { throw new AssertionError("Index has multiple types: " + mapperService.types()); } } return mapper; }	instead of checking the index version here, would it make sense to check if the index has multiple mappings? that would be consistent with how typeless 'get mappings' works on indices with multiple types.
@Override public void process(Map<String, Object> fields) { Object field = fields.get(fieldName); if ((field instanceof String) == false) { return; } String text = (String) field; text = FeatureUtils.cleanAndLowerText(text); text = FeatureUtils.truncateToNumValidBytes(text, MAX_STRING_SIZE_IN_BYTES); String finalText = text; if (text.isEmpty() || text.isBlank()) { fields.put( destField, Arrays.asList( new ByteSizeAndEmbedding( // Don't count white spaces as bytes for the prediction finalText.trim().getBytes(StandardCharsets.UTF_8).length, concatEmbeddings( FEATURE_EXTRACTORS.stream() .map((featureExtractor) -> featureExtractor.extractFeatures(finalText)) .collect(Collectors.toList()) ) ) ) ); return; } List<ByteSizeAndEmbedding> embeddings = new ArrayList<>(); int[] codePoints = finalText.codePoints().toArray(); for (int i = 0; i < codePoints.length - 1;) { while (i < codePoints.length - 1 && Character.isLetter(codePoints[i]) == false) { i++; } if (i >= codePoints.length) { break; } ScriptCode currentCode = ScriptCode.unicodeScriptToULScript(Character.UnicodeScript.of(codePoints[i])); int j = i + 1; for (; j < codePoints.length; j++) { while (j < codePoints.length && Character.isLetter(codePoints[j]) == false) { j++; } if (j >= codePoints.length) { break; } ScriptCode j1 = ScriptCode.unicodeScriptToULScript(Character.UnicodeScript.of(codePoints[j])); if (j1 != currentCode && j1 != ScriptCode.Inherited) { if (j < codePoints.length - 1) { ScriptCode j2 = ScriptCode.unicodeScriptToULScript(Character.UnicodeScript.of(codePoints[j + 1])); if (j2 != ScriptCode.Common && j2 != currentCode) { break; } } } } // Knowing the start and the end of the section is important for feature building, so make sure its wrapped in spaces String str = new String(codePoints, i, j - i); StringBuilder builder = new StringBuilder(); if (str.startsWith(" ") == false) { builder.append(" "); } builder.append(str); if (str.endsWith(" ") == false) { builder.append(" "); } embeddings.add( new ByteSizeAndEmbedding( // Don't count white spaces as bytes for the prediction str.trim().getBytes(StandardCharsets.UTF_8).length, concatEmbeddings( FEATURE_EXTRACTORS.stream() .map((featureExtractor) -> featureExtractor.extractFeatures(builder.toString())) .collect(Collectors.toList()) ) ) ); i = j; } fields.put(destField, embeddings); }	might be clearer if it's explicitly final suggestion final string finaltext = text;
@Override public void process(Map<String, Object> fields) { Object field = fields.get(fieldName); if ((field instanceof String) == false) { return; } String text = (String) field; text = FeatureUtils.cleanAndLowerText(text); text = FeatureUtils.truncateToNumValidBytes(text, MAX_STRING_SIZE_IN_BYTES); String finalText = text; if (text.isEmpty() || text.isBlank()) { fields.put( destField, Arrays.asList( new ByteSizeAndEmbedding( // Don't count white spaces as bytes for the prediction finalText.trim().getBytes(StandardCharsets.UTF_8).length, concatEmbeddings( FEATURE_EXTRACTORS.stream() .map((featureExtractor) -> featureExtractor.extractFeatures(finalText)) .collect(Collectors.toList()) ) ) ) ); return; } List<ByteSizeAndEmbedding> embeddings = new ArrayList<>(); int[] codePoints = finalText.codePoints().toArray(); for (int i = 0; i < codePoints.length - 1;) { while (i < codePoints.length - 1 && Character.isLetter(codePoints[i]) == false) { i++; } if (i >= codePoints.length) { break; } ScriptCode currentCode = ScriptCode.unicodeScriptToULScript(Character.UnicodeScript.of(codePoints[i])); int j = i + 1; for (; j < codePoints.length; j++) { while (j < codePoints.length && Character.isLetter(codePoints[j]) == false) { j++; } if (j >= codePoints.length) { break; } ScriptCode j1 = ScriptCode.unicodeScriptToULScript(Character.UnicodeScript.of(codePoints[j])); if (j1 != currentCode && j1 != ScriptCode.Inherited) { if (j < codePoints.length - 1) { ScriptCode j2 = ScriptCode.unicodeScriptToULScript(Character.UnicodeScript.of(codePoints[j + 1])); if (j2 != ScriptCode.Common && j2 != currentCode) { break; } } } } // Knowing the start and the end of the section is important for feature building, so make sure its wrapped in spaces String str = new String(codePoints, i, j - i); StringBuilder builder = new StringBuilder(); if (str.startsWith(" ") == false) { builder.append(" "); } builder.append(str); if (str.endsWith(" ") == false) { builder.append(" "); } embeddings.add( new ByteSizeAndEmbedding( // Don't count white spaces as bytes for the prediction str.trim().getBytes(StandardCharsets.UTF_8).length, concatEmbeddings( FEATURE_EXTRACTORS.stream() .map((featureExtractor) -> featureExtractor.extractFeatures(builder.toString())) .collect(Collectors.toList()) ) ) ); i = j; } fields.put(destField, embeddings); }	it seems potentially confusing to mix text and finaltext in the main algorithm. since finaltext needs to be used in lambdas i'd just use it everywhere to avoid making the reader double-check if there's a difference. suggestion if (finaltext.isempty() || finaltext.isblank()) {
@Override public InferenceResults infer(Map<String, Object> fields, InferenceConfig config, Map<String, String> featureDecoderMap) { if (config.requestingImportance()) { throw ExceptionsHelper.badRequestException("[{}] model does not supports feature importance", NAME.getPreferredName()); } if (config instanceof ClassificationConfig == false) { throw ExceptionsHelper.badRequestException("[{}] model only supports classification", NAME.getPreferredName()); } Object vector = fields.get(embeddedVectorFeatureName); if (vector instanceof List<?> == false) { throw ExceptionsHelper.badRequestException( "[{}] model could not find non-null collection of embeddings separated by unicode script type [{}]. " + "Please verify that the input is a string.", NAME.getPreferredName(), embeddedVectorFeatureName ); } List<?> embeddedVector = (List<?>) vector; double[] scores = new double[LANGUAGE_NAMES.size()]; int totalByteSize = 0; for (Object vec : embeddedVector) { if (vec instanceof CustomWordEmbedding.ByteSizeAndEmbedding == false) { continue; } CustomWordEmbedding.ByteSizeAndEmbedding byteSizeAndEmbedding = (CustomWordEmbedding.ByteSizeAndEmbedding) vec; int square = (int) Math.pow(byteSizeAndEmbedding.getUtf8ByteSize(), 2); totalByteSize += square; double[] h0 = hiddenLayer.productPlusBias(false, byteSizeAndEmbedding.getEmbedding()); double[] score = softmaxLayer.productPlusBias(true, h0); sumDoubleArrays(scores, score, Math.max(square, 1)); } if (totalByteSize != 0) { divMut(scores, totalByteSize); } double[] probabilities = softMax(scores); ClassificationConfig classificationConfig = (ClassificationConfig) config; Tuple<InferenceHelpers.TopClassificationValue, List<TopClassEntry>> topClasses = InferenceHelpers.topClasses( probabilities, LANGUAGE_NAMES, null, classificationConfig.getNumTopClasses(), PredictionFieldType.STRING ); final InferenceHelpers.TopClassificationValue classificationValue = topClasses.v1(); assert classificationValue.getValue() >= 0 && classificationValue.getValue() < LANGUAGE_NAMES.size() : "Invalid language predicted. Predicted language index " + topClasses.v1(); return new ClassificationInferenceResults( classificationValue.getValue(), LANGUAGE_NAMES.get(classificationValue.getValue()), topClasses.v2(), Collections.emptyList(), classificationConfig, classificationValue.getProbability(), classificationValue.getScore() ); }	i strongly suspect multiplying two integers is much faster than using some generic x^y algorithm that works on arbitrary floating point numbers. suggestion int square = bytesizeandembedding.getutf8bytesize() * bytesizeandembedding.getutf8bytesize();
public void testGetLifecyclePolicy() throws IOException { RestHighLevelClient client = highLevelClient(); // Set up some policies so we have something to get { Map<String, Phase> phases = new HashMap<>(); Map<String, LifecycleAction> hotActions = new HashMap<>(); hotActions.put(RolloverAction.NAME, new RolloverAction( new ByteSizeValue(50, ByteSizeUnit.GB), null, null)); phases.put("hot", new Phase("hot", TimeValue.ZERO, hotActions)); Map<String, LifecycleAction> deleteActions = Collections.singletonMap(DeleteAction.NAME, new DeleteAction()); phases.put("delete", new Phase("delete", new TimeValue(90, TimeUnit.DAYS), deleteActions)); LifecyclePolicy policy = new LifecyclePolicy("my_policy", phases); PutLifecyclePolicyRequest putRequest = new PutLifecyclePolicyRequest(policy); LifecyclePolicy policy2 = new LifecyclePolicy("other_policy", phases); PutLifecyclePolicyRequest putRequest2 = new PutLifecyclePolicyRequest(policy2); AcknowledgedResponse putResponse = client.indexLifecycle(). putLifecyclePolicy(putRequest, RequestOptions.DEFAULT); assertTrue(putResponse.isAcknowledged()); AcknowledgedResponse putResponse2 = client.indexLifecycle(). putLifecyclePolicy(putRequest2, RequestOptions.DEFAULT); assertTrue(putResponse2.isAcknowledged()); } // tag::ilm-get-lifecycle-policy-request GetLifecyclePolicyRequest allRequest = new GetLifecyclePolicyRequest(); // <1> GetLifecyclePolicyRequest request = new GetLifecyclePolicyRequest("my_policy", "other_policy"); // <2> // end::ilm-get-lifecycle-policy-request // tag::ilm-get-lifecycle-policy-execute GetLifecyclePolicyResponse response = client.indexLifecycle() .getLifecyclePolicy(request, RequestOptions.DEFAULT); // end::ilm-get-lifecycle-policy-execute // tag::ilm-get-lifecycle-policy-response ImmutableOpenMap<String, LifecyclePolicyMetadata> policies = response.getPolicies(); LifecyclePolicyMetadata myPolicyMetadata = policies.get("my_policy"); // <1> String myPolicyName = myPolicyMetadata.getName(); long version = myPolicyMetadata.getVersion(); String lastModified = myPolicyMetadata.getModifiedDateString(); long lastModifiedDate = myPolicyMetadata.getModifiedDate(); LifecyclePolicy myPolicy = myPolicyMetadata.getPolicy(); // <2> // end::ilm-get-lifecycle-policy-response // tag::ilm-get-lifecycle-policy-execute-listener ActionListener<GetLifecyclePolicyResponse> listener = new ActionListener<GetLifecyclePolicyResponse>() { @Override public void onResponse(GetLifecyclePolicyResponse response) { ImmutableOpenMap<String, LifecyclePolicyMetadata> policies = response.getPolicies(); // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::ilm-get-lifecycle-policy-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::ilm-get-lifecycle-policy-execute-async client.indexLifecycle().getLifecyclePolicyAsync(request, RequestOptions.DEFAULT, listener); // <1> // end::ilm-get-lifecycle-policy-execute-async }	could we make the two policies slightly different (even if its only the min age on the phase so we can differentiate them when we assert the policy that is returned later?
public void testGetLifecyclePolicy() throws IOException { RestHighLevelClient client = highLevelClient(); // Set up some policies so we have something to get { Map<String, Phase> phases = new HashMap<>(); Map<String, LifecycleAction> hotActions = new HashMap<>(); hotActions.put(RolloverAction.NAME, new RolloverAction( new ByteSizeValue(50, ByteSizeUnit.GB), null, null)); phases.put("hot", new Phase("hot", TimeValue.ZERO, hotActions)); Map<String, LifecycleAction> deleteActions = Collections.singletonMap(DeleteAction.NAME, new DeleteAction()); phases.put("delete", new Phase("delete", new TimeValue(90, TimeUnit.DAYS), deleteActions)); LifecyclePolicy policy = new LifecyclePolicy("my_policy", phases); PutLifecyclePolicyRequest putRequest = new PutLifecyclePolicyRequest(policy); LifecyclePolicy policy2 = new LifecyclePolicy("other_policy", phases); PutLifecyclePolicyRequest putRequest2 = new PutLifecyclePolicyRequest(policy2); AcknowledgedResponse putResponse = client.indexLifecycle(). putLifecyclePolicy(putRequest, RequestOptions.DEFAULT); assertTrue(putResponse.isAcknowledged()); AcknowledgedResponse putResponse2 = client.indexLifecycle(). putLifecyclePolicy(putRequest2, RequestOptions.DEFAULT); assertTrue(putResponse2.isAcknowledged()); } // tag::ilm-get-lifecycle-policy-request GetLifecyclePolicyRequest allRequest = new GetLifecyclePolicyRequest(); // <1> GetLifecyclePolicyRequest request = new GetLifecyclePolicyRequest("my_policy", "other_policy"); // <2> // end::ilm-get-lifecycle-policy-request // tag::ilm-get-lifecycle-policy-execute GetLifecyclePolicyResponse response = client.indexLifecycle() .getLifecyclePolicy(request, RequestOptions.DEFAULT); // end::ilm-get-lifecycle-policy-execute // tag::ilm-get-lifecycle-policy-response ImmutableOpenMap<String, LifecyclePolicyMetadata> policies = response.getPolicies(); LifecyclePolicyMetadata myPolicyMetadata = policies.get("my_policy"); // <1> String myPolicyName = myPolicyMetadata.getName(); long version = myPolicyMetadata.getVersion(); String lastModified = myPolicyMetadata.getModifiedDateString(); long lastModifiedDate = myPolicyMetadata.getModifiedDate(); LifecyclePolicy myPolicy = myPolicyMetadata.getPolicy(); // <2> // end::ilm-get-lifecycle-policy-response // tag::ilm-get-lifecycle-policy-execute-listener ActionListener<GetLifecyclePolicyResponse> listener = new ActionListener<GetLifecyclePolicyResponse>() { @Override public void onResponse(GetLifecyclePolicyResponse response) { ImmutableOpenMap<String, LifecyclePolicyMetadata> policies = response.getPolicies(); // <1> } @Override public void onFailure(Exception e) { // <2> } }; // end::ilm-get-lifecycle-policy-execute-listener // Replace the empty listener by a blocking listener in test final CountDownLatch latch = new CountDownLatch(1); listener = new LatchedActionListener<>(listener, latch); // tag::ilm-get-lifecycle-policy-execute-async client.indexLifecycle().getLifecyclePolicyAsync(request, RequestOptions.DEFAULT, listener); // <1> // end::ilm-get-lifecycle-policy-execute-async }	can we add some asserts to ensure we have retrieved the correct thing from the api. like checking that the policy we got back matches the policy we sent?
public void testTranslogSyncAfterGlobalCheckpointSync() throws Exception { final IndicesService indicesService = mock(IndicesService.class); final Index index = new Index("index", "uuid"); final IndexService indexService = mock(IndexService.class); when(indicesService.indexServiceSafe(index)).thenReturn(indexService); final int id = randomIntBetween(0, 4); final IndexShard indexShard = mock(IndexShard.class); when(indexService.getShard(id)).thenReturn(indexShard); final ShardId shardId = new ShardId(index, id); when(indexShard.shardId()).thenReturn(shardId); final Translog.Durability durability = randomFrom(Translog.Durability.ASYNC, Translog.Durability.REQUEST); when(indexShard.getTranslogDurability()).thenReturn(durability); final Translog translog = mock(Translog.class); when(indexShard.getTranslog()).thenReturn(translog); final long globalCheckpoint = randomIntBetween(Math.toIntExact(SequenceNumbers.NO_OPS_PERFORMED), Integer.MAX_VALUE); final long lastSyncedGlobalCheckpoint; if (randomBoolean() && globalCheckpoint != SequenceNumbers.NO_OPS_PERFORMED) { lastSyncedGlobalCheckpoint = randomIntBetween(Math.toIntExact(SequenceNumbers.NO_OPS_PERFORMED), Math.toIntExact(globalCheckpoint) - 1); assert lastSyncedGlobalCheckpoint < globalCheckpoint; } else { lastSyncedGlobalCheckpoint = globalCheckpoint; } when(indexShard.getGlobalCheckpoint()).thenReturn(globalCheckpoint); when(translog.getLastSyncedGlobalCheckpoint()).thenReturn(lastSyncedGlobalCheckpoint); final GlobalCheckpointSyncAction action = new GlobalCheckpointSyncAction( Settings.EMPTY, transportService, clusterService, indicesService, threadPool, shardStateAction, new ActionFilters(Collections.emptySet()), new IndexNameExpressionResolver(Settings.EMPTY)); final GlobalCheckpointSyncAction.Request primaryRequest = new GlobalCheckpointSyncAction.Request(indexShard.shardId()); if (randomBoolean()) { action.shardOperationOnPrimary(primaryRequest, indexShard); } else { action.shardOperationOnReplica(new GlobalCheckpointSyncAction.Request(indexShard.shardId()), indexShard); } if (durability == Translog.Durability.ASYNC || lastSyncedGlobalCheckpoint == globalCheckpoint) { verify(translog, never()).sync(); } else { verify(translog).sync(); } }	maybe make this lastsyncedglobalcheckpoint = globalcheckpoint + randomintbetween(0, 10);?
public void testCompositeParsing(){ //in all these examples the second pattern will be used assertSameDate("2014-06-06T12:01:02.123", "yyyy-MM-dd'T'HH:mm:ss||yyyy-MM-dd'T'HH:mm:ss.SSS"); assertSameDate("2014-06-06T12:01:02.123", "strictDateTimeNoMillis||yyyy-MM-dd'T'HH:mm:ss.SSS"); assertSameDate("2014-06-06T12:01:02.123", "yyyy-MM-dd'T'HH:mm:ss+HH:MM||yyyy-MM-dd'T'HH:mm:ss.SSS"); }	i think we need some more tests: * 3 patterns * random ordering of many patterns
public ActionRequestValidationException validate() { ActionRequestValidationException validationException = super.validate(); if (Strings.hasLength(type) == false) { validationException = addValidationError("type is missing", validationException); } if (Strings.hasLength(id) == false) { validationException = addValidationError("id is missing", validationException); } if (versionType.validateVersionForWrites(version) == false) { validationException = addValidationError("illegal version value [" + version + "] for version type [" + versionType.name() + "]", validationException); } if (versionType == VersionType.FORCE) { validationException = addValidationError("version type [force] may no longer be used", validationException); } return validationException; }	nit: maybe use strings.isempty.
public ClusterState execute(ClusterState currentState, List<TaskContext<ClusterStateUpdateTask>> taskContexts) throws Exception { ClusterState clusterState = currentState; for (final var taskContext : taskContexts) { try { final var task = taskContext.getTask(); clusterState = task.execute(clusterState); taskContext.success(new LegacyClusterTaskResultActionListener(task, currentState)); } catch (Exception e) { taskContext.onFailure(e); } } return clusterState; }	i wish java has val
@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, ActionListener<Void> listener) { if (isReadOnly()) { listener.onFailure(new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository")); } else { SnapshotInfo snapshot = null; try { snapshot = getSnapshotInfo(snapshotId); } catch (SnapshotMissingException ex) { listener.onFailure(ex); return; } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) { logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex); } // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots final RepositoryData updatedRepositoryData; final BlobContainer indicesBlobContainer = blobStore().blobContainer(basePath().add("indices")); final Map<String, BlobContainer> foundIndices; try { final RepositoryData repositoryData = getRepositoryData(); updatedRepositoryData = repositoryData.removeSnapshot(snapshotId); // Cache the indices that were found before writing out the new index-N blob so that a stuck master will never // delete an index that was created by another master node after writing this index-N blob. foundIndices = indicesBlobContainer.children(); writeIndexGen(updatedRepositoryData, repositoryStateId); } catch (Exception ex) { listener.onFailure(new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex)); return; } final SnapshotInfo finalSnapshotInfo = snapshot; try { blobContainer().deleteBlobsIgnoringIfNotExists( Arrays.asList(snapshotFormat.blobName(snapshotId.getUUID()), globalMetaDataFormat.blobName(snapshotId.getUUID()))); } catch (IOException e) { logger.warn(() -> new ParameterizedMessage("[{}] Unable to delete global metadata files", snapshotId), e); } final var survivingIndices = updatedRepositoryData.getIndices(); deleteIndices( Optional.ofNullable(finalSnapshotInfo) .map(info -> info.indices().stream().filter(survivingIndices::containsKey) .map(updatedRepositoryData::resolveIndexId).collect(Collectors.toList())) .orElse(Collections.emptyList()), snapshotId, ActionListener.map(listener, v -> { cleanupStaleIndices(foundIndices, survivingIndices); return null; }) ); } }	move this to the try block where it's used
@Override public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, ActionListener<Void> listener) { if (isReadOnly()) { listener.onFailure(new RepositoryException(metadata.name(), "cannot delete snapshot from a readonly repository")); } else { SnapshotInfo snapshot = null; try { snapshot = getSnapshotInfo(snapshotId); } catch (SnapshotMissingException ex) { listener.onFailure(ex); return; } catch (IllegalStateException | SnapshotException | ElasticsearchParseException ex) { logger.warn(() -> new ParameterizedMessage("cannot read snapshot file [{}]", snapshotId), ex); } // Delete snapshot from the index file, since it is the maintainer of truth of active snapshots final RepositoryData updatedRepositoryData; final BlobContainer indicesBlobContainer = blobStore().blobContainer(basePath().add("indices")); final Map<String, BlobContainer> foundIndices; try { final RepositoryData repositoryData = getRepositoryData(); updatedRepositoryData = repositoryData.removeSnapshot(snapshotId); // Cache the indices that were found before writing out the new index-N blob so that a stuck master will never // delete an index that was created by another master node after writing this index-N blob. foundIndices = indicesBlobContainer.children(); writeIndexGen(updatedRepositoryData, repositoryStateId); } catch (Exception ex) { listener.onFailure(new RepositoryException(metadata.name(), "failed to delete snapshot [" + snapshotId + "]", ex)); return; } final SnapshotInfo finalSnapshotInfo = snapshot; try { blobContainer().deleteBlobsIgnoringIfNotExists( Arrays.asList(snapshotFormat.blobName(snapshotId.getUUID()), globalMetaDataFormat.blobName(snapshotId.getUUID()))); } catch (IOException e) { logger.warn(() -> new ParameterizedMessage("[{}] Unable to delete global metadata files", snapshotId), e); } final var survivingIndices = updatedRepositoryData.getIndices(); deleteIndices( Optional.ofNullable(finalSnapshotInfo) .map(info -> info.indices().stream().filter(survivingIndices::containsKey) .map(updatedRepositoryData::resolveIndexId).collect(Collectors.toList())) .orElse(Collections.emptyList()), snapshotId, ActionListener.map(listener, v -> { cleanupStaleIndices(foundIndices, survivingIndices); return null; }) ); } }	add an assert false here?
@Override public SnapshotInfo finalizeSnapshot(final SnapshotId snapshotId, final List<IndexId> indices, final long startTime, final String failure, final int totalShards, final List<SnapshotShardFailure> shardFailures, final long repositoryStateId, final boolean includeGlobalState, final Map<String, Object> userMetadata) { SnapshotInfo blobStoreSnapshot = new SnapshotInfo(snapshotId, indices.stream().map(IndexId::getName).collect(Collectors.toList()), startTime, failure, System.currentTimeMillis(), totalShards, shardFailures, includeGlobalState, userMetadata); try { final BlobContainer indicesBlobContainer = blobStore().blobContainer(basePath().add("indices")); final Map<String, BlobContainer> foundIndices = indicesBlobContainer.children(); final RepositoryData updatedRepositoryData = getRepositoryData().addSnapshot(snapshotId, blobStoreSnapshot.state(), indices); final var survivingIndices = updatedRepositoryData.getIndices(); snapshotFormat.write(blobStoreSnapshot, blobContainer(), snapshotId.getUUID()); writeIndexGen(updatedRepositoryData, repositoryStateId); cleanupStaleIndices(foundIndices, survivingIndices); } catch (FileAlreadyExistsException ex) { // if another master was elected and took over finalizing the snapshot, it is possible // that both nodes try to finalize the snapshot and write to the same blobs, so we just // log a warning here and carry on throw new RepositoryException(metadata.name(), "Blob already exists while " + "finalizing snapshot, assume the snapshot has already been saved", ex); } catch (IOException ex) { throw new RepositoryException(metadata.name(), "failed to update snapshot in repository", ex); } return blobStoreSnapshot; }	as discussed, let's not do the clean-up as part of the snapshotting, but only during deletion. instead let's think about how we can add a way for the user to force a clean even if there are no snapshots to be deleted.
@After public void assertRepoConsistency() { if (skipRepoConsistencyCheckReason == null) { client().admin().cluster().prepareGetRepositories().get().repositories() .stream() .map(RepositoryMetaData::name) .forEach(name -> BlobStoreTestUtil.assertRepoConsistency(internalCluster(), name)); } }	perhaps log the reason in the else branch here
static DeprecationIssue checkFollowedSystemIndices(ClusterState state) { Set<String> systemIndexFollowers = new HashSet<>(); for (ObjectObjectCursor<String, IndexMetadata> indexEntry : state.metadata().getIndices()) { final IndexMetadata indexMetadata = indexEntry.value; final Map<String, String> ccrMetadata = indexMetadata.getCustomData(CCR.CCR_CUSTOM_METADATA_KEY); if (ccrMetadata == null) { continue; } if (indexMetadata.isSystem()) { systemIndexFollowers.add(indexEntry.key); } } if (systemIndexFollowers.isEmpty()) { return null; } return new DeprecationIssue(DeprecationIssue.Level.WARNING, "Some follower indices follow remote system indices", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-8.0.html", "Follower indices " + systemIndexFollowers + " follow remote system indices and this will not work in the next major version." ); }	same comment on the scope as above, we should only log deprecation issues when the index was originally auto-followed i think?
static DeprecationIssue checkFollowedSystemIndices(ClusterState state) { Set<String> systemIndexFollowers = new HashSet<>(); for (ObjectObjectCursor<String, IndexMetadata> indexEntry : state.metadata().getIndices()) { final IndexMetadata indexMetadata = indexEntry.value; final Map<String, String> ccrMetadata = indexMetadata.getCustomData(CCR.CCR_CUSTOM_METADATA_KEY); if (ccrMetadata == null) { continue; } if (indexMetadata.isSystem()) { systemIndexFollowers.add(indexEntry.key); } } if (systemIndexFollowers.isEmpty()) { return null; } return new DeprecationIssue(DeprecationIssue.Level.WARNING, "Some follower indices follow remote system indices", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-8.0.html", "Follower indices " + systemIndexFollowers + " follow remote system indices and this will not work in the next major version." ); }	where should i add the deprecation note?
public void testPruneOnlyDeletesAtMostLocalCheckpoint() throws Exception { final IndexSettings indexSettings = engine.config().getIndexSettings(); final IndexMetaData indexMetaData = IndexMetaData.builder(indexSettings.getIndexMetaData()) .settings(Settings.builder().put(indexSettings.getSettings()) .put(IndexSettings.INDEX_GC_DELETES_SETTING.getKey(), "-1")).build(); indexSettings.updateIndexMetaData(indexMetaData); engine.onSettingsChanged(); engine.engineConfig.setEnableGcDeletes(true); int addedDocs = scaledRandomIntBetween(0, 10); for (int i = 0; i < addedDocs; i++) { index(engine, i); } final AtomicLong clock = new AtomicLong(); final Set<Long> trimmedDeletes = new HashSet<>(); final int trimmedBatch = between(10, 20); for (int i = 0; i < trimmedBatch; i++) { final long seqno = engine.getLocalCheckpointTracker().generateSeqNo(); engine.delete(replicaDeleteForDoc(UUIDs.randomBase64UUID(), 1, seqno, clock.incrementAndGet())); trimmedDeletes.add(seqno); } final long gapSeqNo = engine.getLocalCheckpointTracker().generateSeqNo(); // Gap here. final Set<Long> rememberedDeletes = new HashSet<>(); final int rememberedBatch = between(10, 20); for (int i = 0; i < rememberedBatch; i++) { final long seqno = engine.getLocalCheckpointTracker().generateSeqNo(); engine.delete(replicaDeleteForDoc(UUIDs.randomBase64UUID(), 1, seqno, clock.incrementAndGet())); rememberedDeletes.add(seqno); } assertThat(engine.getDeletedTombstones().values().stream().map(deleteVersion -> deleteVersion.seqNo).collect(Collectors.toSet()), equalTo(Sets.union(trimmedDeletes, rememberedDeletes))); engine.refresh("test"); // Only prune deletes below the local checkpoint. engine.maybePruneDeletes(); assertThat(engine.getDeletedTombstones().values().stream().map(deleteVersion -> deleteVersion.seqNo).collect(Collectors.toSet()), equalTo(rememberedDeletes)); // Fill the gap - should be able to prune all deletes. engine.index(replicaIndexForDoc(testParsedDocument("d", null, testDocumentWithTextField(), SOURCE, null), 1, gapSeqNo, false)); engine.maybePruneDeletes(); assertThat(engine.getDeletedTombstones().entrySet(), empty()); }	how does this relate to clock? also refresh already maybe prunes deletes
*/ @Override public Object run() { try { return executable.execute(variables, scorer, doc, aggregationValue); // Note that it is safe to catch any of the following errors since Painless is stateless. } catch (PainlessError | BootstrapMethodError | OutOfMemoryError | StackOverflowError | Exception e) { throw convertToScriptException(e); } }	thanks for killing this illegalaccesserror: i added this on accident when debugging long ago!
public XContentBuilder startObject(ParseField field) throws IOException { return startObject(field.getPreferredName()); }	i was thinking of adding these helpers, i'm glad you did :)
public static ClusterSearchShardsGroup fromXContent(XContentParser parser) throws IOException { ensureExpectedToken(XContentParser.Token.START_ARRAY, parser.currentToken(), parser::getTokenLocation); parser.nextToken(); List<ShardRouting> routings = new ArrayList<>(); while (parser.currentToken() != XContentParser.Token.END_ARRAY) { final ShardRouting routing = ShardRouting.fromXContent(parser); routings.add(routing); parser.nextToken(); } ensureExpectedToken(XContentParser.Token.END_ARRAY, parser.currentToken(), parser::getTokenLocation); if (!routings.isEmpty()) { final ShardRouting[] shards = routings.toArray(new ShardRouting[0]); ShardId shardId = shards[0].shardId(); return new ClusterSearchShardsGroup(shardId, shards); } return new ClusterSearchShardsGroup(); }	we tend to use routings.isempty() == false since it's harder to typo
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(getId()); builder.field( NAME.getPreferredName(), getName()); builder.field(EPHEMERAL_ID.getPreferredName(), getEphemeralId()); builder.field(TRANSPORT_ADDRESS.getPreferredName(), getAddress().toString()); builder.field(ATTRIBUTES.getPreferredName(), attributes); builder.endObject(); return builder; }	nit: extra space here
private SearchRequestBuilder createLatestDatafeedTimingStatsSearch(String indexName, String jobId) { return client.prepareSearch(indexName) .setSize(1) .setIndicesOptions(IndicesOptions.lenientExpandOpen()) .setQuery(QueryBuilders.idsQuery().addIds(DatafeedTimingStats.documentId(jobId))) .addSort(SortBuilders.fieldSort(DatafeedTimingStats.TOTAL_SEARCH_TIME_MS.getPreferredName()) .unmappedType("double").order(SortOrder.DESC)); }	@droberts195 there is also a sort on bucket_count in this file. is that one going to cause problems as well? i am currently trying to find it in the mappings.
private MemoryUsageEstimationResult runJob(String jobId, DataFrameAnalyticsConfig config, DataFrameDataExtractorFactory dataExtractorFactory) { DataFrameDataExtractor dataExtractor = dataExtractorFactory.newExtractor(false); DataFrameDataExtractor.DataSummary dataSummary = dataExtractor.collectDataSummary(); if (dataSummary.rows == 0) { throw ExceptionsHelper.badRequestException( "[{}] Unable to estimate memory usage as no documents in the source indices [{}] contained all the fields selected for " + "analysis. If you are relying on automatic field selection then there are currently mapped fields that do not exist " + "in any indexed documents, and you will have to switch to explicit field selection and include only fields that " + "exist in indexed documents.", jobId, Strings.arrayToCommaDelimitedString(config.getSource().getIndex())); } Set<String> categoricalFields = dataExtractor.getCategoricalFields(config.getAnalysis()); AnalyticsProcessConfig processConfig = new AnalyticsProcessConfig( jobId, dataSummary.rows, dataSummary.cols, // For memory estimation the model memory limit here should be set high enough not to trigger an error when C++ code // compares the limit to the result of estimation. new ByteSizeValue(1, ByteSizeUnit.PB), 1, "", categoricalFields, config.getAnalysis()); AnalyticsProcess<MemoryUsageEstimationResult> process = processFactory.createAnalyticsProcess( config, processConfig, null, executorServiceForProcess, // The handler passed here will never be called as AbstractNativeProcess.detectCrash method returns early when // (processInStream == null) which is the case for MemoryUsageEstimationProcess. reason -> {}); try { return readResult(jobId, process); } catch (Exception e) { String errorMsg = new ParameterizedMessage( "[{}] Error while processing process output [{}], process errors: [{}]", jobId, e.getMessage(), process.readError()).getFormattedMessage(); throw ExceptionsHelper.serverError(errorMsg, e); } finally { process.consumeAndCloseOutputStream(); try { LOGGER.debug("[{}] Closing process", jobId); process.close(); LOGGER.debug("[{}] Closed process", jobId); } catch (Exception e) { String errorMsg = new ParameterizedMessage( "[{}] Error while closing process [{}], process errors: [{}]", jobId, e.getMessage(), process.readError()).getFormattedMessage(); throw ExceptionsHelper.serverError(errorMsg, e); } } } /** * Extracts {@link MemoryUsageEstimationResult}	should these be kept in sync with log-levels in analyticsprocessmanager.java (which are currently info)?
private static Map<String, RoleDescriptor> initializeReservedRoles() { return MapBuilder.<String, RoleDescriptor>newMapBuilder() .put("superuser", new RoleDescriptor("superuser", new String[] { "all" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("all").build()}, new String[] { "*" }, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("transport_client", new RoleDescriptor("transport_client", new String[] { "transport_client" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_user", new RoleDescriptor("kibana_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("manage", "read", "index", "delete") .build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("all").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("monitoring_user", new RoleDescriptor("monitoring_user", new String[] { "cluster:monitor/main" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_agent", new RoleDescriptor("remote_monitoring_agent", new String[] { "manage_index_templates", "manage_ingest_pipelines", "monitor", "cluster:monitor/xpack/watcher/watch/get", "cluster:admin/xpack/watcher/watch/put", "cluster:admin/xpack/watcher/watch/delete", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices("metricbeat-*").privileges("index", "create_index").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("remote_monitoring_collector", new RoleDescriptor( "remote_monitoring_collector", new String[] { "monitor" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("monitor").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*").privileges("read").build() }, null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null )) .put("ingest_admin", new RoleDescriptor("ingest_admin", new String[] { "manage_index_templates", "manage_pipeline" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) // reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role .put("reporting_user", new RoleDescriptor("reporting_user", null, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("kibana_dashboard_only_user", new RoleDescriptor( "kibana_dashboard_only_user", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*").privileges("read", "view_index_metadata").build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana").resources("*").privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put(KibanaUser.ROLE_NAME, new RoleDescriptor(KibanaUser.ROLE_NAME, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".kibana*", ".reporting-*").privileges("all").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build() }, null, new ConditionalClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null)) .put("logstash_system", new RoleDescriptor("logstash_system", new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("beats_admin", new RoleDescriptor("beats_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("all").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.BEATS_ROLE, new RoleDescriptor(UsernamesField.BEATS_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put(UsernamesField.APM_ROLE, new RoleDescriptor(UsernamesField.APM_ROLE, new String[] { "monitor", MonitoringBulkAction.NAME}, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_user", new RoleDescriptor("machine_learning_user", new String[] { "monitor_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-notifications").privileges("view_index_metadata", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("machine_learning_admin", new RoleDescriptor("machine_learning_admin", new String[] { "manage_ml" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".ml-*").privileges("view_index_metadata", "read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_admin", new RoleDescriptor("watcher_admin", new String[] { "manage_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX, TriggeredWatchStoreField.INDEX_NAME, HistoryStoreField.INDEX_PREFIX + "*").privileges("read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("watcher_user", new RoleDescriptor("watcher_user", new String[] { "monitor_watcher" }, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(Watch.INDEX) .privileges("read") .build(), RoleDescriptor.IndicesPrivileges.builder().indices(HistoryStoreField.INDEX_PREFIX + "*") .privileges("read") .build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("logstash_admin", new RoleDescriptor("logstash_admin", null, new RoleDescriptor.IndicesPrivileges[] { RoleDescriptor.IndicesPrivileges.builder().indices(".logstash*") .privileges("create", "delete", "index", "manage", "read").build() }, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_user", new RoleDescriptor("rollup_user", new String[] { "monitor_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .put("rollup_admin", new RoleDescriptor("rollup_admin", new String[] { "manage_rollup" }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA)) .immutableMap(); }	@jaymode any thoughts here? this is grant write access to the metricbeat-* indices for an existing role. on the one hand i worry about expanding the access for roles that are in use by customers, on the other hand creating an ever increasing set of new roles is a usability nightmare for customers. are you happy to run with this as is, and handle it in the release notes?
@Override protected Field parseCreateField(ParseContext context) throws IOException { XContentParser parser = context.parser(); if (parser.currentName() != null && parser.currentName().equals(Defaults.NAME) && parser.currentToken().isValue()) { // we are in the parse Phase String id = parser.text(); if (context.id() != null && !context.id().equals(id)) { throw new MapperParsingException("Provided id [" + context.id() + "] does not match the content one [" + id + "]"); } context.id(id); if (!fieldType.indexed() && !fieldType.stored()) { return null; } return new Field(names.indexName(), context.id(), fieldType); } else if (parser.currentName() != null && parser.currentName().equals(Defaults.NAME) && parser.currentToken().equals(XContentParser.Token.START_OBJECT)) { throw new MapperParsingException("Content id cannot be an object."); } else { // we are in the pre/post parse phase if (!fieldType.indexed() && !fieldType.stored()) { return null; } return new Field(names.indexName(), context.id(), fieldType); } }	you could remove the null check by changing the second check to defaults.name.equals(parser.currentname())
public void apply(Project project) { TaskContainer tasks = project.getTasks(); TestFixtureExtension extension = project.getExtensions().create("testFixtures", TestFixtureExtension.class, project); Provider<DockerComposeThrottle> dockerComposeThrottle = project.getGradle() .getSharedServices() .registerIfAbsent(DOCKER_COMPOSE_THROTTLE, DockerComposeThrottle.class, spec -> spec.getMaxParallelUsages().set(1)); ExtraPropertiesExtension ext = project.getExtensions().getByType(ExtraPropertiesExtension.class); File testfixturesDir = project.file("testfixtures_shared"); ext.set("testFixturesDir", testfixturesDir); if (project.file(DOCKER_COMPOSE_YML).exists()) { Task buildFixture = project.getTasks().create("buildFixture"); Task pullFixture = project.getTasks().create("pullFixture"); Task preProcessFixture = project.getTasks().create("preProcessFixture"); preProcessFixture.doFirst((task) -> { try { Files.createDirectories(testfixturesDir.toPath()); } catch (IOException e) { throw new UncheckedIOException(e); } }); preProcessFixture.getOutputs().dir(testfixturesDir); buildFixture.dependsOn(preProcessFixture); pullFixture.dependsOn(preProcessFixture); Task postProcessFixture = project.getTasks().create("postProcessFixture"); postProcessFixture.dependsOn(buildFixture); preProcessFixture.onlyIf(spec -> buildFixture.getEnabled()); postProcessFixture.onlyIf(spec -> buildFixture.getEnabled()); if (dockerComposeSupported() == false) { preProcessFixture.setEnabled(false); postProcessFixture.setEnabled(false); buildFixture.setEnabled(false); pullFixture.setEnabled(false); } else { project.getPluginManager().apply(BasePlugin.class); project.getPluginManager().apply(DockerComposePlugin.class); ComposeExtension composeExtension = project.getExtensions().getByType(ComposeExtension.class); composeExtension.setUseComposeFiles(Collections.singletonList(DOCKER_COMPOSE_YML)); composeExtension.setRemoveContainers(true); composeExtension.setExecutable( project.file("/usr/local/bin/docker-compose").exists() ? "/usr/local/bin/docker-compose" : "/usr/bin/docker-compose" ); buildFixture.dependsOn(tasks.named("composeUp")); pullFixture.dependsOn(tasks.named("composePull")); tasks.named("composeUp").configure(t -> { // Avoid running docker-compose tasks in parallel in CI due to some issues on certain Linux distributions if (BuildParams.isCi()) { t.usesService(dockerComposeThrottle); } t.mustRunAfter(preProcessFixture); }); tasks.named("composePull").configure(t -> t.mustRunAfter(preProcessFixture)); tasks.named("composeDown").configure(t -> t.doLast(t2 -> project.delete(testfixturesDir))); configureServiceInfoForTask( postProcessFixture, project, false, (name, port) -> postProcessFixture.getExtensions().getByType(ExtraPropertiesExtension.class).set(name, port) ); } } else { project.afterEvaluate(spec -> { if (extension.fixtures.isEmpty()) { // if only one fixture is used, that's this one, but without a compose file that's not a valid configuration throw new IllegalStateException( "No " + DOCKER_COMPOSE_YML + " found for " + project.getPath() + " nor does it use other fixtures." ); } }); } extension.fixtures.matching(fixtureProject -> fixtureProject.equals(project) == false) .all(fixtureProject -> project.evaluationDependsOn(fixtureProject.getPath())); conditionTaskByType(tasks, extension, Test.class); conditionTaskByType(tasks, extension, getTaskClass("org.elasticsearch.gradle.test.RestIntegTestTask")); conditionTaskByType(tasks, extension, TestingConventionsTasks.class); conditionTaskByType(tasks, extension, ComposeUp.class); if (dockerComposeSupported() == false) { project.getLogger() .debug( "Tests for {} require docker-compose at /usr/local/bin/docker-compose or /usr/bin/docker-compose " + "but none could be found so these will be skipped", project.getPath() ); return; } tasks.withType(Test.class, task -> extension.fixtures.all(fixtureProject -> { fixtureProject.getTasks().matching(it -> it.getName().equals("buildFixture")).all(task::dependsOn); fixtureProject.getTasks().matching(it -> it.getName().equals("composeDown")).all(task::finalizedBy); configureServiceInfoForTask( task, fixtureProject, true, (name, host) -> task.getExtensions().getByType(SystemPropertyCommandLineArgumentProvider.class).systemProperty(name, host) ); task.dependsOn(fixtureProject.getTasks().getByName("postProcessFixture")); })); }	can we use info instead? debug level logging is verbose to the point of being nearly useless and info is not logged by default either unless you ask with --info.
public void indicesAliases(final IndicesAliasesClusterStateUpdateRequest request, final ActionListener<ClusterStateUpdateResponse> listener) { clusterService.submitStateUpdateTask("index-aliases", new AckedClusterStateUpdateTask<ClusterStateUpdateResponse>(Priority.URGENT, request, listener) { @Override protected ClusterStateUpdateResponse newResponse(boolean acknowledged) { return new ClusterStateUpdateResponse(acknowledged); } @Override public ClusterState execute(ClusterState currentState) { return updateAliasesInClusterState(currentState, request.actions()); } }); }	i don't think these comments add anything over the fact that compilation would break if we remove it without adapting corresponding tests, and our ides can show us all the call-sites/overrides.
public void indicesAliases(final IndicesAliasesClusterStateUpdateRequest request, final ActionListener<ClusterStateUpdateResponse> listener) { clusterService.submitStateUpdateTask("index-aliases", new AckedClusterStateUpdateTask<ClusterStateUpdateResponse>(Priority.URGENT, request, listener) { @Override protected ClusterStateUpdateResponse newResponse(boolean acknowledged) { return new ClusterStateUpdateResponse(acknowledged); } @Override public ClusterState execute(ClusterState currentState) { return updateAliasesInClusterState(currentState, request.actions()); } }); }	does it just need to be visible, or overridable? if not the latter, can you make it final?
*/ public synchronized IndexerState stop() { AtomicBoolean wasStartedAndSetStopped = new AtomicBoolean(false); IndexerState currentState = state.updateAndGet(previousState -> { if (previousState == IndexerState.INDEXING) { return IndexerState.STOPPING; } else if (previousState == IndexerState.STARTED) { wasStartedAndSetStopped.set(true); return IndexerState.STOPPED; } else { return previousState; } }); if (wasStartedAndSetStopped.get()) { onStop(); } return currentState; } /** * Sets the internal state to {@link IndexerState#ABORTING}. It returns false if * an async job is running in the background and in such case {@link #onAbort} * will be called as soon as the background job detects that the indexer is * aborted. If there is no job running when this function is called, it returns * true and {@link #onAbort()}	onstop is now persisting state so it must be called after the state has been updated.
private static XContentBuilder addDataFrameTransformStateAndStatsMappings(XContentBuilder builder) throws IOException { return builder .startObject(DataFrameTransformStateAndStats.STATE_FIELD.getPreferredName()) .startObject(PROPERTIES) .startObject(DataFrameTransformState.TASK_STATE.getPreferredName()) .field(TYPE, KEYWORD) .endObject() .startObject(DataFrameTransformState.INDEXER_STATE.getPreferredName()) .field(TYPE, KEYWORD) .endObject() .startObject(DataFrameTransformState.CURRENT_POSITION.getPreferredName()) .field(ENABLED, false) .endObject() .startObject(DataFrameTransformState.CHECKPOINT.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameTransformState.REASON.getPreferredName()) .field(TYPE, KEYWORD) .endObject() .startObject(DataFrameTransformState.PROGRESS.getPreferredName()) .startObject(PROPERTIES) .startObject(DataFrameTransformProgress.TOTAL_DOCS.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameTransformProgress.DOCS_REMAINING.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameTransformProgress.PERCENT_COMPLETE) .field(TYPE, FLOAT) .endObject() .endObject() .endObject() .endObject() .endObject() .startObject(DataFrameField.STATS_FIELD.getPreferredName()) .startObject(PROPERTIES) .startObject(DataFrameIndexerTransformStats.NUM_PAGES.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.NUM_INPUT_DOCUMENTS.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.NUM_OUTPUT_DOCUMENTS.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.NUM_INVOCATIONS.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.INDEX_TIME_IN_MS.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.SEARCH_TIME_IN_MS.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.INDEX_TOTAL.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.SEARCH_TOTAL.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.SEARCH_FAILURES.getPreferredName()) .field(TYPE, LONG) .endObject() .startObject(DataFrameIndexerTransformStats.INDEX_FAILURES.getPreferredName()) .field(TYPE, LONG) .endObject() .endObject() .endObject() .startObject(DataFrameTransformStateAndStats.CHECKPOINTING_INFO_FIELD.getPreferredName()) .field(ENABLED, false) .endObject(); }	i've added explicit mappings for the state and stats objects but not the checkpoints. maintaining the mappings is a burden and the tests may not fail after they have change, i think this is sufficient
public void testBlocks() throws ExecutionException, InterruptedException { Request request = new Request(); PlainActionFuture<Response> listener = new PlainActionFuture<>(); ReplicationTask task = maybeTask(); Action action = new Action(Settings.EMPTY, "testActionWithBlocks", transportService, clusterService, shardStateAction, threadPool) { @Override protected ClusterBlockLevel globalBlockLevel() { return ClusterBlockLevel.WRITE; } }; ClusterBlocks.Builder block = ClusterBlocks.builder() .addGlobalBlock(new ClusterBlock(1, "non retryable", false, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL)); setState(clusterService, ClusterState.builder(clusterService.state()).blocks(block)); Action.ReroutePhase reroutePhase = action.new ReroutePhase(task, request, listener); reroutePhase.run(); assertListenerThrows("primary phase should fail operation", listener, ClusterBlockException.class); assertPhase(task, "failed"); block = ClusterBlocks.builder() .addGlobalBlock(new ClusterBlock(1, "retryable", true, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL)); setState(clusterService, ClusterState.builder(clusterService.state()).blocks(block)); listener = new PlainActionFuture<>(); reroutePhase = action.new ReroutePhase(task, new Request().timeout("5ms"), listener); reroutePhase.run(); assertListenerThrows("failed to timeout on retryable block", listener, ClusterBlockException.class); assertPhase(task, "failed"); assertFalse(request.isRetrySet.get()); listener = new PlainActionFuture<>(); reroutePhase = action.new ReroutePhase(task, request = new Request(), listener); reroutePhase.run(); assertFalse("primary phase should wait on retryable block", listener.isDone()); assertPhase(task, "waiting_for_retry"); assertTrue(request.isRetrySet.get()); block = ClusterBlocks.builder() .addGlobalBlock(new ClusterBlock(1, "non retryable", false, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL)); setState(clusterService, ClusterState.builder(clusterService.state()).blocks(block)); assertListenerThrows("primary phase should fail operation when moving from a retryable block to a non-retryable one", listener, ClusterBlockException.class); assertIndexShardUninitialized(); }	should we add a variant that checks the null causes blocks to be ignored?
public static Tuple<List<Object>, List<Object>> randomStoredFieldValues(Random random, XContentType xContentType) { int numValues = randomIntBetween(random, 1, 5); List<Object> originalValues = randomStoredFieldValues(random, numValues); List<Object> expectedParsedValues = new ArrayList<>(numValues); for (Object originalValue : originalValues) { expectedParsedValues.add(getExpectedParsedValue(xContentType, originalValue)); } return Tuple.tuple(originalValues, expectedParsedValues); }	maybe we can also use lucenetests#randomsortvalue() here and get rif of the private helper alltogether? i see its slightly different in the way it e.g. draws strings, but then again we also use it in https://github.com/elastic/elasticsearch/pull/36597/files#diff-c5ebccd66a4f939ea845842b8cfea549r54 for the single-value case as well
@Override protected void masterOperation(final PutIndexTemplateRequest request, final ClusterState state, final ActionListener<PutIndexTemplateResponse> listener) { String cause = request.cause(); if (cause.length() == 0) { cause = "api"; } final Settings.Builder templateSettingsBuilder = Settings.builder(); templateSettingsBuilder.put(request.settings()).normalizePrefix(IndexMetaData.INDEX_SETTING_PREFIX); indexScopedSettings.validate(templateSettingsBuilder.build(), true); // templates must be consistent with reg. to dependencies indexTemplateService.putTemplate(new MetaDataIndexTemplateService.PutRequest(cause, request.name()) .patterns(request.patterns()) .order(request.order()) .settings(templateSettingsBuilder.build()) .mappings(request.mappings()) .aliases(request.aliases()) .customs(request.customs()) .create(request.create()) .masterTimeout(request.masterNodeTimeout()) .version(request.version()), new MetaDataIndexTemplateService.PutListener() { @Override public void onResponse(MetaDataIndexTemplateService.PutResponse response) { listener.onResponse(new PutIndexTemplateResponse(response.acknowledged())); } @Override public void onFailure(Exception e) { logger.debug((Supplier<?>) () -> new ParameterizedMessage("failed to put template [{}]", request.name()), e); listener.onFailure(e); } }); }	can you expand reg.? it took me a few seconds to figure out what it meant (regards) and i do not want to have to do it every time i read this comment?
public void testTransferMaxSeenAutoIdTimestampOnResync() throws Exception { try (ReplicationGroup shards = createGroup(2)) { shards.startAll(); IndexShard primary = shards.getPrimary(); IndexShard replica1 = shards.getReplicas().get(0); IndexShard replica2 = shards.getReplicas().get(1); long maxTimestampOnReplica1 = -1; long maxTimestampOnReplica2 = -1; List<IndexRequest> replicationRequests = new ArrayList<>(); for (int numDocs = between(1, 10), i = 0; i < numDocs; i++) { final IndexRequest indexRequest = new IndexRequest(index.getName(), "type").source("{}", XContentType.JSON); indexRequest.process(Version.CURRENT, null, index.getName()); final IndexRequest copyRequest; if (randomBoolean()) { copyRequest = copyIndexRequest(indexRequest); indexRequest.onRetry(); } else { copyRequest = copyIndexRequest(indexRequest); copyRequest.onRetry(); } replicationRequests.add(copyRequest); final BulkShardRequest bulkShardRequest = indexOnPrimary(indexRequest, primary); if (randomBoolean()) { indexOnReplica(bulkShardRequest, shards, replica1); maxTimestampOnReplica1 = Math.max(maxTimestampOnReplica1, indexRequest.getAutoGeneratedTimestamp()); } else { indexOnReplica(bulkShardRequest, shards, replica2); maxTimestampOnReplica2 = Math.max(maxTimestampOnReplica2, indexRequest.getAutoGeneratedTimestamp()); } } assertThat(replica1.getMaxSeenAutoIdTimestamp(), equalTo(maxTimestampOnReplica1)); assertThat(replica2.getMaxSeenAutoIdTimestamp(), equalTo(maxTimestampOnReplica2)); shards.promoteReplicaToPrimary(replica1).get(); assertThat(replica2.getMaxSeenAutoIdTimestamp(), equalTo(maxTimestampOnReplica1)); for (IndexRequest request : replicationRequests) { shards.index(request); // deliver via normal replication } } }	i think we miss an assertion to check that all shards are identical?
public boolean isValue() { return true; } }, VALUE_NULL { @Override public boolean isValue() { return false; } }; public abstract boolean isValue(); } enum NumberType { INT, LONG, FLOAT, DOUBLE } XContentType contentType(); Token nextToken() throws IOException; void skipChildren() throws IOException; Token currentToken(); String currentName() throws IOException; Map<String, Object> map() throws IOException; Map<String, Object> mapOrdered() throws IOException; Map<String, String> mapStrings() throws IOException; Map<String, String> mapStringsOrdered() throws IOException; List<Object> list() throws IOException; List<Object> listOrderedMap() throws IOException; String text() throws IOException; String textOrNull() throws IOException; /** * Returns a BytesRef holding UTF-8 bytes or null if a null value is {@link Token#VALUE_NULL}. * This method should be used to read text only binary content should be read through {@link #binaryValue()} */ BytesRef utf8BytesOrNull() throws IOException; /** * Returns a BytesRef holding UTF-8 bytes. * This method should be used to read text only binary content should be read through {@link #binaryValue()} */ BytesRef utf8Bytes() throws IOException; Object objectText() throws IOException; Object objectBytes() throws IOException; /** * Method that can be used to determine whether calling of textCharacters() would be the most efficient way to * access textual content for the event parser currently points to. * * Default implementation simply returns false since only actual * implementation class has knowledge of its internal buffering * state. * * This method shouldn't be used to check if the token contains text or not. */ boolean hasTextCharacters(); char[] textCharacters() throws IOException; int textLength() throws IOException; int textOffset() throws IOException; Number numberValue() throws IOException; NumberType numberType() throws IOException; short shortValue(boolean coerce) throws IOException; int intValue(boolean coerce) throws IOException; long longValue(boolean coerce) throws IOException; float floatValue(boolean coerce) throws IOException; double doubleValue(boolean coerce) throws IOException; short shortValue() throws IOException; int intValue() throws IOException; long longValue() throws IOException; float floatValue() throws IOException; double doubleValue() throws IOException; /** * returns true if the current value is boolean in nature. * values that are considered booleans: * - boolean value (true/false) * - numeric integers (=0 is considered as false, !=0 is true) * - one of the following strings: "true","false","on","off","yes","no","1","0" */ boolean isBooleanValue() throws IOException; boolean booleanValue() throws IOException; /** * Reads a plain binary value that was written via one of the following methods: * * <ul> * <li>{@link XContentBuilder#field(String, org.apache.lucene.util.BytesRef)}</li> * <li>{@link XContentBuilder#field(String, org.elasticsearch.common.bytes.BytesReference)}</li> * <li>{@link XContentBuilder#field(String, byte[], int, int)}}</li> * <li>{@link XContentBuilder#field(String, byte[])}}</li> * </ul> * * as well as via their <code>String</code> variants of the separated value methods. * Note: Do not use this method to read values written with: * <ul> * <li>{@link XContentBuilder#utf8Field(String, org.apache.lucene.util.BytesRef)}</li> * <li>{@link XContentBuilder#utf8Field(String, org.apache.lucene.util.BytesRef)}</li> * </ul> * * these methods write UTF-8 encoded strings and must be read through: * <ul> * <li>{@link XContentParser#utf8Bytes()}	can you add a test for these new methods?
static DeprecationIssue checkFractionalByteValueSettings(final Settings settings, final PluginsAndModules pluginsAndModules, final ClusterState clusterState, final XPackLicenseState licenseState) { Map<String, String> fractionalByteSettings = new HashMap<>(); for (String key : settings.keySet()) { try { settings.getAsBytesSize(key, ByteSizeValue.ZERO); String stringValue = settings.get(key); if (stringValue.contains(".")) { fractionalByteSettings.put(key, stringValue); } } catch (Exception ignoreThis) { // We expect anything that is not a byte setting to throw an exception, but we don't care about those } } if (fractionalByteSettings.isEmpty()) { return null; } String url = "https://www.elastic.co/guide/en/elasticsearch/reference/master/logging.html#deprecation-logging"; String message = "support for fractional byte size values is deprecated and will be removed in a future release"; String details = "change the following settings to non-fractional values: [" + fractionalByteSettings.entrySet().stream().map(fractionalByteSetting -> fractionalByteSetting.getKey() + "->" + fractionalByteSetting.getValue()).collect(Collectors.joining(", ")) + "]"; return new DeprecationIssue(DeprecationIssue.Level.WARNING, message, url, details, false, null); }	hmm, i'm not a huge fan of exception based control flow. i know that settings are registered with clustersettings and indexscopedsettings so that settings can be validated against the master set, but i don't know if we can filter on the setting's data type due to type erasure, and i'm not sure if those are comprehensive for every setting either.
public static void specify(int spaces, String from, StringBuilder to) throws Exception { try (BufferedReader reader = new BufferedReader(new FastStringReader(from))) { String line; while ((line = reader.readLine()) != null) { for (int i = 0; i < spaces; i++) { to.append(' '); } to.append(line).append('\\\\n'); } } }	the name is correct as-is, it adds spaces.
public static String substring(String s, int beginIndex, int endIndex) { if (s == null) { return null; } int realEndIndex = s.length() > 0 ? s.length() - 1 : 0; if (endIndex > realEndIndex) { return s.substring(beginIndex); } else { return s.substring(beginIndex, endIndex); } }	this is fine as-is.
public static String cleanTruncate(String s, int length) { if (s == null) { return null; } /* * Its pretty silly for you to truncate to 0 length but just in case * someone does this shouldn't break. */ if (length == 0) { return ""; } if (length >= s.length()) { return s; } if (Character.isHighSurrogate(s.charAt(length - 1))) { length--; } return s.substring(0, length); }	this is fine as-is.
public static String randomNodeName(InputStream namesFile) { try { List<String> names = new ArrayList<>(); try (BufferedReader reader = new BufferedReader(new InputStreamReader(namesFile, Charsets.UTF_8))) { String name = reader.readLine(); while (name != null) { names.add(name); name = reader.readLine(); } } int index = ((ThreadLocalRandom.current().nextInt(names.size())) % names.size()); return names.get(index); } catch (IOException e) { throw new RuntimeException("Could not read node names list", e); } }	i know you did not add it, but taking the remainder seems unnecessary?
@Override public void close() { ords.close(); } } public static class FromManySmall extends LongKeyedBucketOrds { private final LongHash ords; private final int owningBucketOrdShift; private final long owningBucketOrdMask; public FromManySmall(BigArrays bigArrays, int owningBucketOrdShift) { ords = new LongHash(2, bigArrays); this.owningBucketOrdShift = owningBucketOrdShift; this.owningBucketOrdMask = -1L << owningBucketOrdShift; } private long encode(long owningBucketOrd, long value) { // This is in the critical path for collecting lots of aggs. Be careful of performance. return (owningBucketOrd << owningBucketOrdShift) | value; } @Override public long add(long owningBucketOrd, long value) { // This is in the critical path for collecting lots of aggs. Be careful of performance. long enc = encode(owningBucketOrd, value); if (owningBucketOrd != (enc >>> owningBucketOrdShift)) { throw new IllegalArgumentException("[" + owningBucketOrd + "] must fit in [" + owningBucketOrdShift + "] bits"); } if ((enc & ~owningBucketOrdMask) != value) { throw new IllegalArgumentException("[" + value + "] must fit in [" + (64 - owningBucketOrdShift) + "] bits"); } return ords.add(enc); } @Override public long find(long owningBucketOrd, long value) { if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return -1; } if ((value & owningBucketOrdMask) != 0) { return -1; } return ords.find(encode(owningBucketOrd, value)); } @Override public long get(long ordinal) { return ords.get(ordinal) & ~owningBucketOrdMask; } @Override public long bucketsInOrd(long owningBucketOrd) { // TODO it'd be faster to count the number of buckets in a list of these ords rather than one at a time if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return 0; } long count = 0; long enc = owningBucketOrd << owningBucketOrdShift; for (long i = 0; i < ords.size(); i++) { if ((ords.get(i) & owningBucketOrdMask) == enc) { count++; } } return count; } @Override public long size() { return ords.size(); } @Override public long maxOwningBucketOrd() { // TODO this is fairly expensive to compute. Can we avoid needing it? long max = -1; for (long i = 0; i < ords.size(); i++) { max = Math.max(max, (ords.get(i) & owningBucketOrdMask) >>> owningBucketOrdShift); } return max; } @Override public String decribe() { return "many bucket ords packed using [" + (64 - owningBucketOrdShift) + "/" + owningBucketOrdShift + "] bits"; } @Override public BucketOrdsEnum ordsEnum(long owningBucketOrd) { // TODO it'd be faster to iterate many ords at once rather than one at a time if (Long.numberOfLeadingZeros(owningBucketOrd) < owningBucketOrdShift) { return BucketOrdsEnum.EMPTY; } final long encodedOwningBucketOrd = owningBucketOrd << owningBucketOrdShift; return new BucketOrdsEnum() { private long ord = -1; private long value; @Override public boolean next() { while (true) { ord++; if (ord >= ords.size()) { return false; } long encoded = ords.get(ord); if ((encoded & owningBucketOrdMask) == encodedOwningBucketOrd) { value = encoded & ~owningBucketOrdMask; return true; } } } @Override public long value() { return value; } @Override public long ord() { return ord; } }; } @Override public void close() { ords.close(); }	this isn't 100% true - it's only the hot path for a terms agg under an agg that knows its cardinality like range or filters. or a date_histogram when it's been rewritten into range or filters.
private AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchData(ShardRouting shard, RoutingAllocation allocation) { final ShardId shardId = shard.shardId(); final Settings indexSettings = allocation.metadata().index(shard.index()).getSettings(); final SnapshotId snapshotId = new SnapshotId( SNAPSHOT_SNAPSHOT_NAME_SETTING.get(indexSettings), SNAPSHOT_SNAPSHOT_ID_SETTING.get(indexSettings) ); final DiscoveryNodes nodes = allocation.nodes(); final AsyncCacheStatusFetch asyncFetch = asyncFetchStore.computeIfAbsent(shardId, sid -> new AsyncCacheStatusFetch()); final DiscoveryNode[] dataNodes = asyncFetch.addFetches(nodes.getDataNodes().values().toArray(DiscoveryNode.class)); if (dataNodes.length > 0) { client.execute( TransportSearchableSnapshotCacheStoresAction.TYPE, new TransportSearchableSnapshotCacheStoresAction.Request(snapshotId, shardId, dataNodes), ActionListener.runAfter(new ActionListener<>() { @Override public void onResponse(NodesCacheFilesMetadata nodesCacheFilesMetadata) { final Map<DiscoveryNode, NodeCacheFilesMetadata> res = new HashMap<>(nodesCacheFilesMetadata.getNodesMap().size()); for (Map.Entry<String, NodeCacheFilesMetadata> entry : nodesCacheFilesMetadata.getNodesMap().entrySet()) { res.put(nodes.get(entry.getKey()), entry.getValue()); } asyncFetch.addData(res); } @Override public void onFailure(Exception e) { logger.warn("Failure when trying to fetch existing cache sizes", e); final Map<DiscoveryNode, NodeCacheFilesMetadata> res = new HashMap<>(dataNodes.length); for (DiscoveryNode dataNode : dataNodes) { res.put(dataNode, new NodeCacheFilesMetadata(dataNode, 0L)); } asyncFetch.addData(res); } }, () -> client.admin().cluster().prepareReroute().execute(REROUTE_LISTENER)) ); } return new AsyncShardFetch.FetchResult<>(shardId, asyncFetch.data(), Collections.emptySet()); }	i am curious why we are not using the existing asyncshardfetch for this? no need to change anything, i did not spot any issues, so purely a question to figure out if we need a follow-up later.
private AllocateUnassignedDecision decideAllocation(RoutingAllocation allocation, ShardRouting shardRouting) { assert shardRouting.unassigned(); assert ExistingShardsAllocator.EXISTING_SHARDS_ALLOCATOR_SETTING.get( allocation.metadata().getIndexSafe(shardRouting.index()).getSettings() ).equals(ALLOCATOR_NAME); if (shardRouting.recoverySource().getType() == RecoverySource.Type.SNAPSHOT && allocation.snapshotShardSizeInfo().getShardSize(shardRouting) == null) { return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null); } final AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchedCacheData = fetchData(shardRouting, allocation); if (fetchedCacheData.hasData() == false) { return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null); } final boolean explain = allocation.debugDecision(); final MatchingNodes matchingNodes = findMatchingNodes(shardRouting, allocation, fetchedCacheData, explain); assert explain == false || matchingNodes.nodeDecisions != null : "in explain mode, we must have individual node decisions"; // pre-check if it can be allocated to any node that currently exists, so we won't list the cache sizes for it for nothing // TODO: in the following logic, we do not account for existing cache size when handling disk space checks, should and can we // reliably do this in a world of concurrent cache evictions or are we ok with the cache size just being a best effort hint // here? Tuple<Decision, Map<String, NodeAllocationResult>> result = canBeAllocatedToAtLeastOneNode(shardRouting, allocation); Decision allocateDecision = result.v1(); if (allocateDecision.type() != Decision.Type.YES && (explain == false || asyncFetchStore.get(shardRouting.shardId()) == null)) { // only return early if we are not in explain mode, or we are in explain mode but we have not // yet attempted to fetch any shard data logger.trace("{}: ignoring allocation, can't be allocated on any node", shardRouting); return AllocateUnassignedDecision.no( UnassignedInfo.AllocationStatus.fromDecision(allocateDecision.type()), result.v2() != null ? new ArrayList<>(result.v2().values()) : null ); } List<NodeAllocationResult> nodeDecisions = augmentExplanationsWithStoreInfo(result.v2(), matchingNodes.nodeDecisions); if (allocateDecision.type() != Decision.Type.YES) { return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.fromDecision(allocateDecision.type()), nodeDecisions); } else if (matchingNodes.getNodeWithHighestMatch() != null) { RoutingNode nodeWithHighestMatch = allocation.routingNodes().node(matchingNodes.getNodeWithHighestMatch().getId()); // we only check on THROTTLE since we checked before on NO Decision decision = allocation.deciders().canAllocate(shardRouting, nodeWithHighestMatch, allocation); if (decision.type() == Decision.Type.THROTTLE) { // TODO: does this make sense? Unlike with the store we could evict the cache concurrently and wait for nothing? logger.debug( "[{}][{}]: throttling allocation [{}] to [{}] in order to reuse its unallocated persistent cache", shardRouting.index(), shardRouting.id(), shardRouting, nodeWithHighestMatch.node() ); return AllocateUnassignedDecision.throttle(nodeDecisions); } else { logger.debug( "[{}][{}]: allocating [{}] to [{}] in order to reuse its persistent cache", shardRouting.index(), shardRouting.id(), shardRouting, nodeWithHighestMatch.node() ); return AllocateUnassignedDecision.yes(nodeWithHighestMatch.node(), null, nodeDecisions, true); } } // TODO: do we need handling of delayed allocation for leaving replicas here? return AllocateUnassignedDecision.NOT_TAKEN; }	i think a part of the purpose here is to not fetch data unnecessarily and that this should go before fetchdata above? looks like that is the case in replicashardallocator too.
public long getCacheSize(ShardId shardId, SnapshotId snapshotId) { long aggregateSize = 0L; final CacheIndexWriter writer = writers.get(0); try (IndexReader indexReader = DirectoryReader.open(writer.indexWriter)) { final IndexSearcher searcher = new IndexSearcher(indexReader); searcher.setQueryCache(null); final Weight weight = searcher.createWeight( new BooleanQuery.Builder().add(new TermQuery(new Term(SNAPSHOT_ID_FIELD, snapshotId.getUUID())), BooleanClause.Occur.MUST) .add(new TermQuery(new Term(SHARD_INDEX_ID_FIELD, shardId.getIndex().getUUID())), BooleanClause.Occur.MUST) .add(new TermQuery(new Term(SHARD_ID_FIELD, String.valueOf(shardId.getId()))), BooleanClause.Occur.MUST) .build(), ScoreMode.COMPLETE_NO_SCORES, 0.0f ); for (LeafReaderContext leafReaderContext : searcher.getIndexReader().leaves()) { final Scorer scorer = weight.scorer(leafReaderContext); if (scorer != null) { final Bits liveDocs = leafReaderContext.reader().getLiveDocs(); final IntPredicate isLiveDoc = liveDocs == null ? i -> true : liveDocs::get; final DocIdSetIterator docIdSetIterator = scorer.iterator(); while (docIdSetIterator.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) { if (isLiveDoc.test(docIdSetIterator.docID())) { final Document document = leafReaderContext.reader().document(docIdSetIterator.docID()); var ranges = buildCacheFileRanges(document); for (Tuple<Long, Long> range : ranges) { aggregateSize += range.v2() - range.v1(); } } } } } } catch (IOException e) { throw new UncheckedIOException(e); } return aggregateSize; } /** * This method repopulates the {@link CacheService} by looking at the files on the disk and for each file found, retrieves the latest * synchronized information and puts the cache file into the searchable snapshots cache. * * This method iterates over all node data paths and all shard directories in order to found the "snapshot_cache" directories that * contain the cache files. When such a directory is found, the method iterates over the cache files and looks up their name/UUID in * the existing Lucene documents that were loaded when instanciating the persistent cache index). If no information is found (ie no * matching docs in the map of Lucene documents) then the file is deleted from disk. If a doc is found the stored fields are extracted * from the Lucene document and are used to rebuild the necessary {@link CacheKey}, {@link SnapshotId}, {@link IndexId}, {@link ShardId} * and cache file ranges objects. The Lucene document is then indexed again in the new persistent cache index (the current * {@link CacheIndexWriter}) and the cache file is added back to the searchable snapshots cache again. Note that adding cache * file to the cache service might trigger evictions so previously reindexed Lucene cache files might be delete again (see * CacheService#onCacheFileRemoval(CacheFile) method which calls {@link #removeCacheFile(CacheFile)}. * * @param cacheService the {@link CacheService} to use when repopulating {@link CacheFile}	there is a persistent cache lucene index per node data path and a data node can have more than one data paths. a shard is assigned to one of the data path and its cache files will be indexed into the persistent cache lucene index that exist on that node path. what you can do here is iterating over cacheindexwriter writers and stop if the search query returns at least a document.
public void getApiKeysForCurrentUser(final GetMyApiKeyRequest getMyApiKeyRequest, ActionListener<GetApiKeyResponse> listener) { ensureEnabled(); final Authentication authentication = Authentication.getAuthentication(threadPool.getThreadContext()); final String userName; final String realmName; final String apiKeyId; final String apiKeyName; if (authentication.getAuthenticatedBy().getType().equals("_es_api_key")) { // in case of authenticated by API key, fetch key by API key id from authentication metadata realmName = null; userName = null; apiKeyId = (String) authentication.getMetadata().get(API_KEY_ID_KEY); apiKeyName = null; } else { realmName = authentication.getLookedUpBy() == null ? authentication.getAuthenticatedBy().getName() : authentication.getLookedUpBy().getName(); userName = authentication.getUser().principal(); apiKeyId = getMyApiKeyRequest.getApiKeyId(); apiKeyName = getMyApiKeyRequest.getApiKeyName(); } findApiKeysForUserRealmApiKeyIdAndNameCombination(userName, realmName, apiKeyName, apiKeyId, false, false, ActionListener.wrap(apiKeyInfos -> { if (apiKeyInfos.isEmpty()) { logger.warn("No active api keys found for {}", getMyApiKeyRequest); listener.onResponse(GetApiKeyResponse.emptyResponse()); } else { listener.onResponse(new GetApiKeyResponse(apiKeyInfos)); } }, listener::onFailure)); } /** * Get API keys.<br> * @param getApiKeyRequest {@link GetApiKeyRequest} * @param listener listener for {@link GetApiKeyResponse}	this looks like code that should live in the privileges layer, not in a service.
boolean checkSameUserPermissions(String action, TransportRequest request, Authentication authentication) { final boolean actionAllowed = SAME_USER_PRIVILEGE.test(action); if (actionAllowed) { if (request instanceof UserRequest) { UserRequest userRequest = (UserRequest) request; String[] usernames = userRequest.usernames(); if (usernames == null || usernames.length != 1 || usernames[0] == null) { assert false : "this role should only be used for actions to apply to a single user"; return false; } final String username = usernames[0]; final boolean sameUsername = authentication.getUser().principal().equals(username); if (sameUsername && ChangePasswordAction.NAME.equals(action)) { return checkChangePasswordAction(authentication); } assert AuthenticateAction.NAME.equals(action) || HasPrivilegesAction.NAME.equals(action) || GetUserPrivilegesAction.NAME.equals(action) || sameUsername == false : "Action '" + action + "' should not be possible when sameUsername=" + sameUsername; return sameUsername; } else if (request instanceof GetMyApiKeyRequest) { if (authentication.getAuthenticatedBy().getType().equals("_es_api_key")) { return true; } } else { assert false : "only a user request or get my API key request should be allowed"; return false; } } return false; }	i would like us to move away from adding more special cases to rbacengine. we should aim to put as much as we can in the privileges model, not the engine.
public Automaton allowedActionsMatcher(String index) { List<Automaton> automatonList = new ArrayList<>(); for (Group group : groups) { if (group.indexNameMatcher.test(index)) { automatonList.add(group.privilege.getAutomaton()); } } return automatonList.isEmpty() ? Automatons.EMPTY : Automatons.unionAndMinimize(automatonList); } /** * Determines if this {@link IndicesPermission} is a subset of other indices * permission. This iteratively determines if all of the * {@link IndicesPermission.Group} for this IndicesPermission is a subset of * some group in the given indices permission. * * @param other indices permission * @return in case it is not a subset then returns * {@link SubsetResult.Result#NO} and if it is clearly a subset will return * {@link SubsetResult.Result#YES}. It will return * {@link SubsetResult.Result#MAYBE}	i think we should always call subsetresult.merge and remove this if statement
public SubsetResult isSubsetOf(Group other) { SubsetResult result = SubsetResult.isNotASubset(); if (Operations.subsetOf(Automatons.patterns(this.indices()), Automatons.patterns(other.indices()))) { if (Operations.subsetOf(this.privilege().getAutomaton(), other.privilege().getAutomaton())) { final Automaton thisFieldsPermissionAutomaton = FieldPermissions .initializePermittedFieldsAutomaton(this.getFieldPermissions().getFieldPermissionsDefinition()); final Automaton otherFieldsPermissionAutomaton = FieldPermissions .initializePermittedFieldsAutomaton(other.getFieldPermissions().getFieldPermissionsDefinition()); boolean isSubset = Operations.subsetOf(thisFieldsPermissionAutomaton, otherFieldsPermissionAutomaton); if (isSubset == true) { if (this.getQuery() == null || other.getQuery() == null) { result = SubsetResult.isASubset(); } else { if (Sets.difference(this.getQuery(), other.getQuery()).isEmpty()) { result = SubsetResult.isASubset(); } else { result = SubsetResult.mayBeASubset(Sets.newHashSet(this.indices())); } } } } } return result; }	can you extract these calls into boolean variables to enhance readability
static Map<String, long[]> extractIndexCheckPoints(ShardStats[] shards, Set<String> userIndices) { Map<String, TreeMap<Integer, Long>> checkpointsByIndex = new TreeMap<>(); for (ShardStats shard : shards) { String indexName = shard.getShardRouting().getIndexName(); if (userIndices.contains(indexName)) { SeqNoStats seqNoStats = shard.getSeqNoStats(); // SeqNoStats could be `null`. This indicates that an `AlreadyClosed` exception was thrown somewhere down the stack // Indicates that the index COULD be closed, or at least that the shard is not fully recovered yet. if (seqNoStats == null) { logger.warn("failure gathering checkpoint information for index [{}] as seq_no_stats were null. Shard Stats [{}]", indexName, Strings.toString(shard)); throw new CheckpointException( "Unable to gather checkpoint information for index [" + indexName + "]. seq_no_stats are missing."); } if (checkpointsByIndex.containsKey(indexName)) { // we have already seen this index, just check/add shards TreeMap<Integer, Long> checkpoints = checkpointsByIndex.get(indexName); if (checkpoints.containsKey(shard.getShardRouting().getId())) { // there is already a checkpoint entry for this index/shard combination, check if they match if (checkpoints.get(shard.getShardRouting().getId()) != shard.getSeqNoStats().getGlobalCheckpoint()) { throw new CheckpointException("Global checkpoints mismatch for index [" + indexName + "] between shards of id [" + shard.getShardRouting().getId() + "]"); } } else { // 1st time we see this shard for this index, add the entry for the shard checkpoints.put(shard.getShardRouting().getId(), shard.getSeqNoStats().getGlobalCheckpoint()); } } else { // 1st time we see this index, create an entry for the index and add the shard checkpoint checkpointsByIndex.put(indexName, new TreeMap<>()); checkpointsByIndex.get(indexName).put(shard.getShardRouting().getId(), shard.getSeqNoStats().getGlobalCheckpoint()); } } } // create the final structure Map<String, long[]> checkpointsByIndexReduced = new TreeMap<>(); checkpointsByIndex.forEach((indexName, checkpoints) -> { checkpointsByIndexReduced.put(indexName, checkpoints.values().stream().mapToLong(l -> l).toArray()); }); return checkpointsByIndexReduced; }	@hendrikmuhs let me know what you think. it is possible for this to throw an npe, so to protect against that, i am still throwing a checkpoint exception with the index that failed, and that we were unable to gather seq_no_stats. the user making the call should not see any privileged information. i opted to log the shardstats as a warning. if that is too much information, we will have to change this to figure out how to log actionable information given the seq_no_stats being null.
public void testReadBlobInSmallChunks() throws Exception { final int maxRetries = randomIntBetween(1, 5); final int chunkSize = randomIntBetween(4096, 4096 * 2); final byte[] bytes = randomBlobContent(); httpServer.createContext("/account/container/read_blob_small_chunks", exchange -> { try { Streams.readFully(exchange.getRequestBody()); if ("HEAD".equals(exchange.getRequestMethod())) { exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(bytes.length)); exchange.getResponseHeaders().add("Content-Length", String.valueOf(bytes.length)); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1); } else if ("GET".equals(exchange.getRequestMethod())) { final int rangeStart = getRangeStart(exchange); assertThat(rangeStart, lessThan(bytes.length)); int rangeSize = Math.min(chunkSize, bytes.length - rangeStart); exchange.getResponseHeaders().add("Content-Type", "application/octet-stream"); exchange.getResponseHeaders().add("Content-Range", "bytes " + rangeStart + "-" + (rangeStart + rangeSize) + "/" + bytes.length); exchange.getResponseHeaders().add("x-ms-blob-content-length", String.valueOf(rangeSize)); exchange.getResponseHeaders().add("Content-Length", String.valueOf(rangeSize)); exchange.getResponseHeaders().add("x-ms-blob-type", "blockblob"); exchange.getResponseHeaders().add("ETag", UUIDs.base64UUID()); exchange.sendResponseHeaders(RestStatus.PARTIAL_CONTENT.getStatus(), rangeSize); exchange.getResponseBody().write(bytes, rangeStart, rangeSize); } } finally { exchange.close(); } }); final BlobContainer blobContainer = createBlobContainer(maxRetries); try (InputStream inputStream = blobContainer.readBlob("read_blob_small_chunks")) { assertArrayEquals(bytes, BytesReference.toBytes(Streams.readFully(inputStream))); } }	this test fails right now, since the sdk triggers a head request to get the blob size. the problem comes when a range that goes beyond the blob end the api would return an error 416 (requested range not satisfiable). my guess is that the expectation is that client would know the blob length beforehand and won't request past that boundary, is that correct? if that's the expectation that we have, i'll write a custom inputstream that just bypass the head request as we were doing before.
public void testSetIterator() { assertEquals(3, exec("Set x = new HashSet(); x.add(2); x.add(3); x.add(-2); Iterator y = x.iterator(); " + "int total = 0; while (y.hasNext()) total += y.next(); return total;")); assertEquals("abc", exec("Set x = new HashSet(); x.add(\\\\"a\\\\"); x.add(\\\\"b\\\\"); x.add(\\\\"c\\\\"); " + "Iterator y = x.iterator(); String total = \\\\"\\\\"; while (y.hasNext()) total += y.next(); return total;")); }	were these supposed to be removed?
private Aggregations search() { LOGGER.debug("[{}] Executing aggregated search", context.jobId); SearchResponse searchResponse = executeSearchRequest(buildSearchRequest(buildBaseSearchSource())); LOGGER.debug("[{}] Search response was obtained", context.jobId); timingStatsReporter.reportSearchDuration(searchResponse.getTook()); return validateAggs(searchResponse.getAggregations()); }	what about adding .setallowpartialsearchresults(false) on the output of buildsearchrequest(...) instead of doing it per sub-class? seems cleaner to me.
@Override public List<ActionHandler<? extends ActionRequest, ? extends ActionResponse>> getActions() { List<ActionHandler<?, ?>> actions = new ArrayList<>( Arrays.asList( new ActionHandler<>(RollupSearchAction.INSTANCE, TransportRollupSearchAction.class), new ActionHandler<>(PutRollupJobAction.INSTANCE, TransportPutRollupJobAction.class), new ActionHandler<>(StartRollupJobAction.INSTANCE, TransportStartRollupAction.class), new ActionHandler<>(StopRollupJobAction.INSTANCE, TransportStopRollupAction.class), new ActionHandler<>(DeleteRollupJobAction.INSTANCE, TransportDeleteRollupJobAction.class), new ActionHandler<>(GetRollupJobsAction.INSTANCE, TransportGetRollupJobAction.class), new ActionHandler<>(GetRollupCapsAction.INSTANCE, TransportGetRollupCapsAction.class), new ActionHandler<>(GetRollupIndexCapsAction.INSTANCE, TransportGetRollupIndexCapsAction.class), new ActionHandler<>(XPackUsageFeatureAction.ROLLUP, RollupUsageTransportAction.class), new ActionHandler<>(XPackInfoFeatureAction.ROLLUP, RollupInfoTransportAction.class), // Rollup / Downsampling new ActionHandler<>(RollupIndexerAction.INSTANCE, TransportRollupIndexerAction.class), new ActionHandler<>(RollupAction.INSTANCE, TransportRollupAction.class) ) ); return actions; }	should this also be behind the tsdb feature flag?
*/ public synchronized void updateGlobalCheckpointOnReplica(final long globalCheckpoint) { assert invariant(); assert primaryMode == false; /* * The global checkpoint here is a local knowledge which is updated under the mandate of the primary. It can happen that the primary * information is lagging compared to a replica (e.g., if a replica is promoted to primary but has stale info relative to other * replica shards). In these cases, the local knowledge of the global checkpoint could be higher than sync from the lagging primary. */ if (this.globalCheckpoint <= globalCheckpoint) { this.globalCheckpoint = globalCheckpoint; logger.trace("global checkpoint updated from primary to [{}]", globalCheckpoint); } assert invariant(); } /** * Initializes the global checkpoint tracker in primary mode (see {@link #primaryMode}	how would you feel about naming this method (and it's counterpart) activateprimarymode ? i was confused a couple of times as initialize and primary terms already used in the indexshard context (a primary relocation target is a primary shard and is already initializing long before the method is called) .
*/ public synchronized void updateGlobalCheckpointOnReplica(final long globalCheckpoint) { assert invariant(); assert primaryMode == false; /* * The global checkpoint here is a local knowledge which is updated under the mandate of the primary. It can happen that the primary * information is lagging compared to a replica (e.g., if a replica is promoted to primary but has stale info relative to other * replica shards). In these cases, the local knowledge of the global checkpoint could be higher than sync from the lagging primary. */ if (this.globalCheckpoint <= globalCheckpoint) { this.globalCheckpoint = globalCheckpoint; logger.trace("global checkpoint updated from primary to [{}]", globalCheckpoint); } assert invariant(); } /** * Initializes the global checkpoint tracker in primary mode (see {@link #primaryMode}	can we add a message with localcheckpoints.get(allocationid) and allocationid ?
* @param inSyncAllocationIds the allocation IDs of the currently in-sync shard copies * @param initializingAllocationIds the allocation IDs of the currently initializing shard copies * @param pre60AllocationIds the allocation IDs of shards that are allocated to pre-6.0 nodes */ public synchronized void updateFromMaster(final long applyingClusterStateVersion, final Set<String> inSyncAllocationIds, final Set<String> initializingAllocationIds, final Set<String> pre60AllocationIds) { assert invariant(); if (applyingClusterStateVersion > appliedClusterStateVersion) { // check that the master does not fabricate new in-sync entries out of thin air once we are in primary mode assert !primaryMode || inSyncAllocationIds.stream().allMatch( inSyncId -> localCheckpoints.containsKey(inSyncId) && localCheckpoints.get(inSyncId).inSync); // remove entries which don't exist on master boolean removedEntries = localCheckpoints.keySet().removeIf( aid -> !inSyncAllocationIds.contains(aid) && !initializingAllocationIds.contains(aid)); if (primaryMode) { // add new initializingIds that are missing locally. These are fresh shard copies - and not in-sync for (String initializingId : initializingAllocationIds) { if (localCheckpoints.containsKey(initializingId) == false) { final boolean inSync = inSyncAllocationIds.contains(initializingId); assert inSync == false; final long localCheckpoint = pre60AllocationIds.contains(initializingId) ? SequenceNumbersService.PRE_60_NODE_LOCAL_CHECKPOINT : SequenceNumbersService.UNASSIGNED_SEQ_NO; localCheckpoints.put(initializingId, new LocalCheckPointState(localCheckpoint, inSync)); } } } else { for (String initializingId : initializingAllocationIds) { final long localCheckpoint = pre60AllocationIds.contains(initializingId) ? SequenceNumbersService.PRE_60_NODE_LOCAL_CHECKPOINT : SequenceNumbersService.UNASSIGNED_SEQ_NO; localCheckpoints.put(initializingId, new LocalCheckPointState(localCheckpoint, false)); } for (String inSyncId : inSyncAllocationIds) { final long localCheckpoint = pre60AllocationIds.contains(inSyncId) ? SequenceNumbersService.PRE_60_NODE_LOCAL_CHECKPOINT : SequenceNumbersService.UNASSIGNED_SEQ_NO; localCheckpoints.put(inSyncId, new LocalCheckPointState(localCheckpoint, true)); } } appliedClusterStateVersion = applyingClusterStateVersion; if (primaryMode && removedEntries) { updateGlobalCheckpointOnPrimary(); } } assert invariant(); }	can we add a message that tells us which aid the master fabricated?
* @param inSyncAllocationIds the allocation IDs of the currently in-sync shard copies * @param initializingAllocationIds the allocation IDs of the currently initializing shard copies * @param pre60AllocationIds the allocation IDs of shards that are allocated to pre-6.0 nodes */ public synchronized void updateFromMaster(final long applyingClusterStateVersion, final Set<String> inSyncAllocationIds, final Set<String> initializingAllocationIds, final Set<String> pre60AllocationIds) { assert invariant(); if (applyingClusterStateVersion > appliedClusterStateVersion) { // check that the master does not fabricate new in-sync entries out of thin air once we are in primary mode assert !primaryMode || inSyncAllocationIds.stream().allMatch( inSyncId -> localCheckpoints.containsKey(inSyncId) && localCheckpoints.get(inSyncId).inSync); // remove entries which don't exist on master boolean removedEntries = localCheckpoints.keySet().removeIf( aid -> !inSyncAllocationIds.contains(aid) && !initializingAllocationIds.contains(aid)); if (primaryMode) { // add new initializingIds that are missing locally. These are fresh shard copies - and not in-sync for (String initializingId : initializingAllocationIds) { if (localCheckpoints.containsKey(initializingId) == false) { final boolean inSync = inSyncAllocationIds.contains(initializingId); assert inSync == false; final long localCheckpoint = pre60AllocationIds.contains(initializingId) ? SequenceNumbersService.PRE_60_NODE_LOCAL_CHECKPOINT : SequenceNumbersService.UNASSIGNED_SEQ_NO; localCheckpoints.put(initializingId, new LocalCheckPointState(localCheckpoint, inSync)); } } } else { for (String initializingId : initializingAllocationIds) { final long localCheckpoint = pre60AllocationIds.contains(initializingId) ? SequenceNumbersService.PRE_60_NODE_LOCAL_CHECKPOINT : SequenceNumbersService.UNASSIGNED_SEQ_NO; localCheckpoints.put(initializingId, new LocalCheckPointState(localCheckpoint, false)); } for (String inSyncId : inSyncAllocationIds) { final long localCheckpoint = pre60AllocationIds.contains(inSyncId) ? SequenceNumbersService.PRE_60_NODE_LOCAL_CHECKPOINT : SequenceNumbersService.UNASSIGNED_SEQ_NO; localCheckpoints.put(inSyncId, new LocalCheckPointState(localCheckpoint, true)); } } appliedClusterStateVersion = applyingClusterStateVersion; if (primaryMode && removedEntries) { updateGlobalCheckpointOnPrimary(); } } assert invariant(); }	add a message with aid?
public void testCheckpointsAdvance() throws Exception { try (ReplicationGroup shards = createGroup(randomInt(3))) { shards.startPrimary(); int numDocs = 0; int startedShards; do { numDocs += shards.indexDocs(randomInt(20)); startedShards = shards.startReplicas(randomIntBetween(1, 2)); } while (startedShards > 0); for (IndexShard shard : shards) { final SeqNoStats shardStats = shard.seqNoStats(); final ShardRouting shardRouting = shard.routingEntry(); assertThat(shardRouting + " local checkpoint mismatch", shardStats.getLocalCheckpoint(), equalTo(numDocs - 1L)); /* * After the last indexing operation completes, the primary will advance its global checkpoint. Without another indexing * operation, or a background sync, the primary will not have broadcast this global checkpoint to its replicas. However, a * shard could have recovered from the primary in which case its global checkpoint will be in-sync with the primary. * Therefore, we can only assert that the global checkpoint is number of docs minus one (matching the primary, in case of a * recovery), or number of docs minus two (received indexing operations but has not received a global checkpoint sync after * the last operation completed). */ final Matcher<Long> globalCheckpointMatcher; if (shardRouting.primary()) { globalCheckpointMatcher = numDocs == 0 ? equalTo(SequenceNumbersService.NO_OPS_PERFORMED) : equalTo(numDocs - 1L); } else { globalCheckpointMatcher = numDocs == 0 ? equalTo(SequenceNumbersService.NO_OPS_PERFORMED) : anyOf(equalTo(numDocs - 1L), equalTo(numDocs - 2L)); } assertThat(shardRouting + " global checkpoint mismatch", shardStats.getGlobalCheckpoint(), globalCheckpointMatcher); assertThat(shardRouting + " max seq no mismatch", shardStats.getMaxSeqNo(), equalTo(numDocs - 1L)); } // simulate a background global checkpoint sync at which point we expect the global checkpoint to advance on the replicas shards.syncGlobalCheckpoint(); final long noOpsPerformed = SequenceNumbersService.NO_OPS_PERFORMED; for (IndexShard shard : shards) { final SeqNoStats shardStats = shard.seqNoStats(); final ShardRouting shardRouting = shard.routingEntry(); assertThat(shardRouting + " local checkpoint mismatch", shardStats.getLocalCheckpoint(), equalTo(numDocs - 1L)); assertThat( shardRouting + " global checkpoint mismatch", shardStats.getGlobalCheckpoint(), numDocs == 0 ? equalTo(noOpsPerformed) : equalTo(numDocs - 1L)); assertThat(shardRouting + " max seq no mismatch", shardStats.getMaxSeqNo(), equalTo(numDocs - 1L)); } } }	++. it is a good that this is fixed and we start with no ops performed.
public DatafeedConfig apply(DatafeedConfig datafeedConfig, Map<String, String> headers, Version minNodeVersion) { if (id.equals(datafeedConfig.getId()) == false) { throw new IllegalArgumentException("Cannot apply update to datafeedConfig with different id"); } DatafeedConfig.Builder builder = new DatafeedConfig.Builder(datafeedConfig); if (jobId != null) { if (datafeedConfig.getJobId() != null && datafeedConfig.getJobId().equals(jobId) == false) { throw ExceptionsHelper.badRequestException(ERROR_MESSAGE_ON_JOB_ID_UPDATE); } builder.setJobId(jobId); } if (queryDelay != null) { builder.setQueryDelay(queryDelay); } if (frequency != null) { builder.setFrequency(frequency); } if (indices != null) { builder.setIndices(indices); } if (queryProvider != null) { builder.setQueryProvider(queryProvider); } if (aggProvider != null) { DatafeedConfig.validateAggregations(aggProvider.getParsedAggs()); builder.setAggProvider(aggProvider); } if (scriptFields != null) { builder.setScriptFields(scriptFields); } if (scrollSize != null) { builder.setScrollSize(scrollSize); } if (chunkingConfig != null) { builder.setChunkingConfig(chunkingConfig); } if (delayedDataCheckConfig != null) { builder.setDelayedDataCheckConfig(delayedDataCheckConfig); } if (maxEmptySearches != null) { builder.setMaxEmptySearches(maxEmptySearches); } if (indicesOptions != null) { builder.setIndicesOptions(indicesOptions); } if (runtimeMappings != null) { builder.setRuntimeMappings(runtimeMappings); } if (headers.isEmpty() == false) { builder.setHeaders(ClientHelper.getPersistableSafeSecurityHeadersForVersion(headers, minNodeVersion)); } return builder.build(); }	ideally, authentication headers are already filtered and converted to minnodeversion when persisted as part of a task configuration. when they are retrieved and used, there is no need to filter and convert them again. but it is safe to do it again with very little cost. it may also help headers that are persisted before this change.
private void messageReceived(TcpChannel channel, InboundMessage message) throws IOException { final InetSocketAddress remoteAddress = channel.getRemoteAddress(); final Header header = message.getHeader(); assert header.needsToReadVariableHeader() == false; ThreadContext threadContext = threadPool.getThreadContext(); try (ThreadContext.StoredContext existing = threadContext.stashContext()) { // Place the context with the headers from the message threadContext.setHeaders(header.getHeaders()); threadContext.putTransient("_remote_address", remoteAddress); if (header.isRequest()) { handleRequest(channel, header, message); } else { // Responses do not support short circuiting currently assert message.isShortCircuit() == false; final TransportResponseHandler<?> handler; long requestId = header.getRequestId(); if (header.isHandshake()) { handler = handshaker.removeHandlerForHandshake(requestId); } else { TransportResponseHandler<? extends TransportResponse> theHandler = responseHandlers.onResponseReceived(requestId, messageListener); if (theHandler == null && header.isError()) { handler = handshaker.removeHandlerForHandshake(requestId); } else { handler = theHandler; } } // ignore if its null, the service logs it if (handler != null) { final StreamInput streamInput; if (message.getContentLength() > 0) { streamInput = namedWriteableStream(message.openOrGetStreamInput()); assertRemoteVersion(streamInput, header.getVersion()); if (header.isError()) { handlerResponseError(streamInput, handler); } else { handleResponse(remoteAddress, streamInput, handler); } // Check the entire message has been read final int nextByte = streamInput.read(); // calling read() is useful to make sure the message is fully read, even if there is an EOS marker if (nextByte != -1) { throw new IllegalStateException("Message not fully read (response) for requestId [" + requestId + "], handler [" + handler + "], error [" + header.isError() + "]; resetting"); } } else { assert header.isError() == false; handleResponse(remoteAddress, EMPTY_STREAM_INPUT, handler); } } } } }	this may introduce a bwc problem in the case that a response is only empty in some versions, because empty_stream_input#getversion() returns version.current which may not match header.getversion().
public void testEmptyDataTierPreference() { List<String> indexNames = new ArrayList<>(); indexNames.add(".some_dotted_index"); for (int i = 0; i < 15; i++) { indexNames.add("test-index-name-" + i); } List<IndexMetadata> indices = new ArrayList<>(); for (String indexName : indexNames) { indices.add( IndexMetadata.builder(indexName) .settings(settings(Version.CURRENT).put(DataTier.TIER_PREFERENCE_SETTING.getKey(), " ")) .numberOfShards(randomIntBetween(1, 100)) .numberOfReplicas(randomIntBetween(1, 100)) .build() ); } { List<DeprecationIssue> issues = DeprecationChecks.filterChecks(CLUSTER_SETTINGS_CHECKS, c -> c.apply(ClusterState.EMPTY_STATE)); assertThat(issues, empty()); } { Metadata.Builder metadata = Metadata.builder(); for (IndexMetadata indexMetadata : indices) { metadata.put(indexMetadata, false); } ClusterState clusterState = ClusterState.builder(clusterStateWithoutAllDataRoles()).metadata(metadata).build(); List<DeprecationIssue> issues = DeprecationChecks.filterChecks(CLUSTER_SETTINGS_CHECKS, c -> c.apply(clusterState)); assertThat( issues, contains( new DeprecationIssue( DeprecationIssue.Level.WARNING, "No [index.routing.allocation.include._tier_preference] is set for indices [test-index-name-0, " + "test-index-name-1, test-index-name-10, test-index-name-11, test-index-name-12, test-index-name-13, " + "test-index-name-14, test-index-name-2, test-index-name-3, test-index-name-4, test-index-name-5, " + "test-index-name-6, test-index-name-7, test-index-name-8, ... (16 in total, 2 omitted)].", "https://ela.st/es-deprecation-7-empty-tier-preference", "Specify a data tier preference for these indices.", false, null ) ) ); } }	could someone from the docs team review this message? i would expect a list like this to be structured more like: > no index.routing.allocation.include._tier_preference index setting is set for indices test-index-name-1, test-index-name-10, test-index-name-11, and 13 more. the main changes being: * removal of formatting that can't be communicated in speech which in this case is square brackets. i understand this might mean inconsistency with messages elsewhere so this might not be worth changing for this release. * listing fewer indices since long lists are difficult to read if they're presented in sentence form. * simplifying the "and more" messaging to indicate how many are unlisted, rather than providing a total count vs. omitted count. because folks might want to know which specific indices are being omitted, i also suggest providing a reference to an api that can be used to retrieve the full list. for example: > to see the full list of indices, use the get indices api with the flimborkle parameter set to "stun".
protected Settings nodeSettings(int nodeOrdinal) { //Have very low pool and queue sizes to overwhelm internal pools easily return Settings.builder() .put(super.nodeSettings(nodeOrdinal)) // don't mess with this one! It's quite sensitive to a low queue size // (see also ThreadedActionListener which is happily spawning threads even when we already got rejected) //.put("thread_pool.listener.queue_size", 1) .put("thread_pool.get.queue_size", 1) // default is 50 .put("thread_pool.write.queue_size", 30) .build(); }	nitpick, but can we update this to 200 too (or remove the comment?).
private Settings getHttpTransportSSLSettings(Settings settings) { Settings httpSSLSettings = settings.getByPrefix(XPackSettings.HTTP_SSL_PREFIX); if (httpSSLSettings.isEmpty()) { return httpSSLSettings; } Settings.Builder builder = Settings.builder().put(httpSSLSettings); if (builder.get("client_authentication") == null && builder.get("verification_mode") == null) { // by default, Security HTTPS does not validate the client builder.put("client_authentication", SSLClientAuth.NONE); builder.put("verification_mode", VerificationMode.NONE); } else if (builder.get("client_authentication") == null) { // otherwise it defaults to 'required' which is incompatible with `verification_mode` 'none` if (VerificationMode.parse(builder.get("verification_mode")).equals(VerificationMode.NONE)) { builder.put("client_authentication", SSLClientAuth.NONE); } } else if (builder.get("verification_mode") == null) { // otherwise it defaults to `full` which is incompatible with `client_authentication` `none` if (SSLClientAuth.parse(builder.get("client_authentication")).equals(SSLClientAuth.NONE)) { builder.put("verification_mode", VerificationMode.NONE); } } return builder.build(); }	is this correct? if i have this config: xpack.security.http.ssl: enabled: true keystore.path: keys.p12 verification_mode: certificate that would previously result in a somewhat non-sensical ssl config of: - client-auth: none - verification-mode: certificate - truststore: jdk but (assuming i'm reading this correctly) it would now give: - client-auth: required - verification-mode: certificate - truststore: jdk
public double apply(double n) { return Math.sqrt(n); } }, RECIPROCAL { @Override public double apply(double n) { return 1.0 / n; } }; public abstract double apply(double n); @Override public String toString() { return super.toString().toLowerCase(Locale.ROOT); } public static Modifier fromString(String modifier) { try { return valueOf(modifier.toUpperCase(Locale.ROOT)); } catch (Throwable t) { throw new IllegalArgumentException("Illegal modifier: " + modifier); }	why not wrap the caught exception instead of swallowing the actual cause of the failure?
@Override protected String configUsers() { final Hasher passwdHasher = inFipsJvm() ? Hasher.resolve(randomFrom("pbkdf2", "pbkdf2_1000")) : Hasher.resolve(randomFrom("pbkdf2", "pbkdf2_1000", "bcrypt", "bcrypt9")) ; final String usersPasswdHashed = new String(passwdHasher.hash(SecuritySettingsSourceField.TEST_PASSWORD_SECURE_STRING)); return super.configUsers() + "user_a:" + usersPasswdHashed + "\\\\n" + "user_b:" + usersPasswdHashed + "\\\\n" + "user_c:" + usersPasswdHashed + "\\\\n" + "user_d:" + usersPasswdHashed + "\\\\n" + "user_e:" + usersPasswdHashed + "\\\\n"; }	the password was passwd which is different from securitysettingssourcefield.test_password_secure_string. this and a few similar changes in other places are causing ci failure for this pr. i think the fix should be updating abstractprivilegetestcase#setuser to use the new password as well.
public static void disableInFips(){ assumeFalse("We are using hashing algorithms that won't be available in FIPS-140 mode", inFipsJvm()); }	i don't think we should disable this whole test in fips. it seems like an unnecessary reduction in test coverage.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(key); out.writeDouble(from); out.writeOptionalDouble(originalFrom); out.writeDouble(to); out.writeOptionalDouble(originalTo); out.writeVLong(docCount); aggregations.writeTo(out); }	you need to add version check on the sending side as well. otherwise, you will be sending something that another (older) side is not expecting to receive.
private void notifyListeners(final long globalCheckpoint, final IndexShardClosedException e) { assert Thread.holdsLock(this); assert (globalCheckpoint == UNASSIGNED_SEQ_NO && e != null) || (globalCheckpoint >= NO_OPS_PERFORMED && e == null); final Map<GlobalCheckpointListener, Tuple<Long, ScheduledFuture<?>>> listenersToNotify; if (globalCheckpoint != UNASSIGNED_SEQ_NO) { listenersToNotify = listeners .entrySet() .stream() .filter(entry -> entry.getValue().v1() <= globalCheckpoint) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); listenersToNotify.keySet().forEach(listeners::remove); } else { listenersToNotify = new HashMap<>(listeners); listeners.clear(); } if (listenersToNotify.isEmpty() == false) { executor.execute(() -> listenersToNotify .forEach((listener, t) -> { /* * We do not want to interrupt any timeouts that fired, these will detect that the listener has been * notified and not trigger the timeout. */ FutureUtils.cancel(t.v2()); notifyListener(listener, globalCheckpoint, e); })); } }	should we break this assertion into two branches of the following if-else statement?
public void testStringToVersion() { Converter conversion = converterFor(KEYWORD, VERSION); assertNull(conversion.convert(null)); assertEquals("2.1.4", conversion.convert("2.1.4")); assertEquals("2.1.4-SNAPSHOT", conversion.convert("2.1.4-SNAPSHOT")); }	also could use some of the not-so-nice versions to make clear that any string is being converted.
public InputStream maybeRateLimitRestores(InputStream stream) { return maybeRateLimit(maybeRateLimit(stream, restoreRateLimiter, restoreRateLimitingTimeInNanos), recoverySettings.rateLimiter(), restoreRateLimitingTimeInNanos); }	recoverysettings.ratelimiter() doesn't stay constant if you completely enable/disable recovery rate limiting. in peer recovery we get it anew for each file chunk sent/received, whereas here a change may only take effect at the start of a blob.
public void scheduleUnlessShuttingDown(TimeValue delay, String executor, Runnable command) { if (!Names.SAME.equals(executor)) { command = new ThreadedRunnableAllowShutdown(command, executor(executor)); } try { scheduler.schedule(command, delay.millis(), TimeUnit.MILLISECONDS); } catch (EsRejectedExecutionException e) { if (e.isExecutorShutdown()) { logger.debug(new ParameterizedMessage("could not schedule execution of [{}] after [{}] on [{}] as executor is shut down", command, delay, executor), e); } else { throw e; } } }	what's the reason for this change? it's unrelated to this pr, no?
public static Suggestion<? extends Entry<? extends Option>> fromXContent(XContentParser parser) throws IOException { ensureExpectedToken(XContentParser.Token.FIELD_NAME, parser.currentToken(), parser::getTokenLocation); String typeAndName = parser.currentName(); // we need to extract the type prefix from the name and throw error if it is not present int delimiterPos = typeAndName.indexOf(InternalAggregation.TYPED_KEYS_DELIMITER); String type = null; String name = null; if (delimiterPos > 0) { type = typeAndName.substring(0, delimiterPos); name = typeAndName.substring(delimiterPos + 1); } else { throw new ParsingException(parser.getTokenLocation(), "Cannot parse suggestion response without type information. Set [" + RestSearchAction.TYPED_KEYS_PARAM + "] parameter on the request to ensure the type information is added to the response output"); } Suggestion suggestion = null; Function<XContentParser, Entry> entryParser = null; /// the "size" parameter and the SortBy for TermSuggestion cannot be parsed from the response, use default values switch (type) { case Suggestion.NAME: suggestion = new Suggestion(name, -1); entryParser = Suggestion.Entry::fromXContent; break; case PhraseSuggestion.NAME: suggestion = new PhraseSuggestion(name, -1); entryParser = PhraseSuggestion.Entry::fromXContent; break; case TermSuggestion.NAME: suggestion = new TermSuggestion(name, -1, SortBy.SCORE); entryParser = TermSuggestion.Entry::fromXContent; break; case CompletionSuggestion.NAME: suggestion = new CompletionSuggestion(name, -1); entryParser = CompletionSuggestion.Entry::fromXContent; break; } ensureExpectedToken(XContentParser.Token.START_ARRAY, parser.nextToken(), parser::getTokenLocation); while ((parser.nextToken()) != XContentParser.Token.END_ARRAY) { suggestion.addTerm(entryParser.apply(parser)); } return suggestion; }	is the indentation off here?
public static Suggestion<? extends Entry<? extends Option>> fromXContent(XContentParser parser) throws IOException { ensureExpectedToken(XContentParser.Token.FIELD_NAME, parser.currentToken(), parser::getTokenLocation); String typeAndName = parser.currentName(); // we need to extract the type prefix from the name and throw error if it is not present int delimiterPos = typeAndName.indexOf(InternalAggregation.TYPED_KEYS_DELIMITER); String type = null; String name = null; if (delimiterPos > 0) { type = typeAndName.substring(0, delimiterPos); name = typeAndName.substring(delimiterPos + 1); } else { throw new ParsingException(parser.getTokenLocation(), "Cannot parse suggestion response without type information. Set [" + RestSearchAction.TYPED_KEYS_PARAM + "] parameter on the request to ensure the type information is added to the response output"); } Suggestion suggestion = null; Function<XContentParser, Entry> entryParser = null; /// the "size" parameter and the SortBy for TermSuggestion cannot be parsed from the response, use default values switch (type) { case Suggestion.NAME: suggestion = new Suggestion(name, -1); entryParser = Suggestion.Entry::fromXContent; break; case PhraseSuggestion.NAME: suggestion = new PhraseSuggestion(name, -1); entryParser = PhraseSuggestion.Entry::fromXContent; break; case TermSuggestion.NAME: suggestion = new TermSuggestion(name, -1, SortBy.SCORE); entryParser = TermSuggestion.Entry::fromXContent; break; case CompletionSuggestion.NAME: suggestion = new CompletionSuggestion(name, -1); entryParser = CompletionSuggestion.Entry::fromXContent; break; } ensureExpectedToken(XContentParser.Token.START_ARRAY, parser.nextToken(), parser::getTokenLocation); while ((parser.nextToken()) != XContentParser.Token.END_ARRAY) { suggestion.addTerm(entryParser.apply(parser)); } return suggestion; }	shall we add a todo that this switch won't be needed once we introduce namedxcontentregistry?
public void initClients() throws IOException { if (restHighLevelClient == null) { final RestClient restClient = mock(RestClient.class); restHighLevelClient = new CustomRestClient(restClient); doAnswer(inv -> mockPerformRequest((Request) inv.getArguments()[0])) .when(restClient) .performRequest(any(Request.class)); doAnswer(inv -> mockPerformRequestAsync( ((Request) inv.getArguments()[0]), (ResponseListener) inv.getArguments()[1])) .when(restClient) .performRequestAsync(any(Request.class), any(ResponseListener.class)); } }	can you fix the javadocs link for mockperformrequestasync ?
private void sendRequestToChannel(final DiscoveryNode node, final Channel targetChannel, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options, Version channelVersion, byte status) throws IOException, TransportException { if (compress) { options = TransportRequestOptions.builder(options).withCompress(true).build(); } status = TransportStatus.setRequest(status); ReleasableBytesStreamOutput bStream = new ReleasableBytesStreamOutput(bigArrays); // we wrap this in a release once since if the onRequestSent callback throws an exception // we might release things twice and this should be prevented final Releasable toRelease = Releasables.releaseOnce(() -> Releasables.close(bStream.bytes())); StreamOutput stream = bStream; try { // only compress if asked, and, the request is not bytes, since then only // the header part is compressed, and the "body" can't be extracted as compressed if (options.compress() && canCompress(request)) { status = TransportStatus.setCompress(status); stream = CompressorFactory.COMPRESSOR.streamOutput(stream); } // we pick the smallest of the 2, to support both backward and forward compatibility // note, this is the only place we need to do this, since from here on, we use the serialized version // as the version to use also when the node receiving this request will send the response with Version version = Version.min(getCurrentVersion(), channelVersion); stream.setVersion(version); threadPool.getThreadContext().writeTo(stream); stream.writeString(action); BytesReference message = buildMessage(requestId, status, node.getVersion(), request, stream, bStream); final TransportRequestOptions finalOptions = options; // this might be called in a different thread SendListener onRequestSent = new SendListener(toRelease, () -> transportServiceAdapter.onRequestSent(node, requestId, action, request, finalOptions)); internalSendMessage(targetChannel, message, onRequestSent); } finally { IOUtils.close(stream); } }	did we check if we have this somewhere else as well and mabye we can reuse the class?
@Override public void writeTo(StreamOutput out) throws IOException { out.writeVInt(numBuckets); out.writeVInt(bitsPerEntry); out.writeVInt(entriesPerBucket); out.writeVInt(count); out.writeVInt(evictedFingerprint); if (out.getVersion().before(Version.V_8_0_0)) { // This is probably slow but it should only happen if we have a mixed clusters (e.g during upgrade). PackedInts.Mutable mutable = PackedInts.getMutable(numBuckets * entriesPerBucket, bitsPerEntry, PackedInts.COMPACT); for (int i = 0; i < count; i++) { mutable.set(i, data.get(i)); } mutable.save(new DataOutput() { @Override public void writeByte(byte b) throws IOException { out.writeByte(b); } @Override public void writeBytes(byte[] b, int offset, int length) throws IOException { out.writeBytes(b, offset, length); } }); } else { data.save(out); } }	similarly, save() is no longer in the lucene 9 api
@Override public boolean equals(Object other) { if (this == other) { return true; } if (other == null || getClass() != other.getClass()) { return false; } final CuckooFilter that = (CuckooFilter) other; return Objects.equals(this.numBuckets, that.numBuckets) && Objects.equals(this.bitsPerEntry, that.bitsPerEntry) && Objects.equals(this.entriesPerBucket, that.entriesPerBucket) && Objects.equals(this.count, that.count) && Objects.equals(this.evictedFingerprint, that.evictedFingerprint); }	s/.vthe/. the/ maybe this should just be javadoc? no big deal either way.
public static void parseRequest(TermVectorRequest termVectorRequest, XContentParser parser) throws IOException { XContentParser.Token token; String currentFieldName = null; List<String> fields = new ArrayList<>(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } else if (currentFieldName != null) { if (currentFieldName.equals("fields")) { if (token == XContentParser.Token.START_ARRAY) { while (parser.nextToken() != XContentParser.Token.END_ARRAY) { fields.add(parser.text()); } } else { throw new ElasticsearchParseException( "The parameter fields must be given as an array! Use syntax : \\\\"fields\\\\" : [\\\\"field1\\\\", \\\\"field2\\\\",...]"); } } else if (currentFieldName.equals("offsets")) { termVectorRequest.offsets(parser.booleanValue()); } else if (currentFieldName.equals("positions")) { termVectorRequest.positions(parser.booleanValue()); } else if (currentFieldName.equals("payloads")) { termVectorRequest.payloads(parser.booleanValue()); } else if (currentFieldName.equals("term_statistics") || currentFieldName.equals("termStatistics")) { termVectorRequest.termStatistics(parser.booleanValue()); } else if (currentFieldName.equals("field_statistics") || currentFieldName.equals("fieldStatistics")) { termVectorRequest.fieldStatistics(parser.booleanValue()); } else if ("_index".equals(currentFieldName)) { // the following is important for multi request parsing. termVectorRequest.index = parser.text(); } else if ("_type".equals(currentFieldName)) { termVectorRequest.type = parser.text(); } else if ("_id".equals(currentFieldName)) { termVectorRequest.id = parser.text(); } else if ("_routing".equals(currentFieldName) || "routing".equals(currentFieldName)) { termVectorRequest.routing = parser.text(); } else if ("_version".equals(currentFieldName) || "version".equals(currentFieldName)) { termVectorRequest.version = parser.longValue(); } else if ("_version_type".equals(currentFieldName) || "_versionType".equals(currentFieldName) || "version_type".equals(currentFieldName) || "versionType".equals(currentFieldName)) { termVectorRequest.versionType = VersionType.fromString(parser.text()); } else { throw new ElasticsearchParseException("The parameter " + currentFieldName + " is not valid for term vector request!"); } } } if (fields.size() > 0) { String[] fieldsAsArray = new String[fields.size()]; termVectorRequest.selectedFields(fields.toArray(fieldsAsArray)); } }	any reason we're setting ourselves up for two different field names rather than just picking one (prefer _version)?
@Override public BatchResult<PutMappingClusterStateUpdateRequest> execute(ClusterState currentState, List<PutMappingClusterStateUpdateRequest> tasks) throws Exception { Map<Index, MapperService> indexMapperServices = new HashMap<>(); BatchResult.Builder<PutMappingClusterStateUpdateRequest> builder = BatchResult.builder(); try { for (PutMappingClusterStateUpdateRequest request : tasks) { try { for (Index index : request.indices()) { final IndexMetaData indexMetaData = currentState.metaData().getIndexSafe(index); if (indexMapperServices.containsKey(indexMetaData.getIndex()) == false) { MapperService mapperService = indicesService.createIndexMapperService(indexMetaData); indexMapperServices.put(index, mapperService); // add mappings for all types, we need them for cross-type validation for (ObjectCursor<MappingMetaData> mapping : indexMetaData.getMappings().values()) { mapperService.merge(mapping.value.type(), mapping.value.source(), MapperService.MergeReason.MAPPING_RECOVERY, request.updateAllTypes()); } } } currentState = applyRequest(currentState, request, indexMapperServices); builder.success(request); } catch (Exception e) { builder.failure(request, e); } } return builder.build(currentState); } finally { IOUtils.close(indexMapperServices.values()); } }	i must have missed it but don't we have to close this now? i don't see where it's closed... also on exception we should close all opened ones?
public MapperService newIndexMapperService(MapperRegistry mapperRegistry) throws IOException { return new MapperService(indexSettings, analysisRegistry.build(indexSettings), new SimilarityService(indexSettings, similarities), mapperRegistry, () -> { throw new UnsupportedOperationException("no index query shard context available"); }); } /** * Forces a certain query cache to use instead of the default one. If this is set * and query caching is not disabled with {@code index.queries.cache.enabled}	i wonder if we should throw and assertionerror here instead... it should not happen and should not be caught?
private synchronized IndexService createIndexService(final String reason, IndexMetaData indexMetaData, IndicesQueryCache indicesQueryCache, IndicesFieldDataCache indicesFieldDataCache, List<IndexEventListener> builtInListeners, IndexingOperationListener... indexingOperationListeners) throws IOException { final Index index = indexMetaData.getIndex(); final Predicate<String> indexNameMatcher = (indexExpression) -> indexNameExpressionResolver.matchesIndex(index.getName(), indexExpression, clusterService.state()); final IndexSettings idxSettings = new IndexSettings(indexMetaData, this.settings, indexNameMatcher, indexScopeSetting); logger.debug("creating Index [{}], shards [{}]/[{}{}] - reason [{}]", indexMetaData.getIndex(), idxSettings.getNumberOfShards(), idxSettings.getNumberOfReplicas(), idxSettings.isShadowReplicaIndex() ? "s" : "", reason); final IndexModule indexModule = new IndexModule(idxSettings, indexStoreConfig, analysisRegistry); for (IndexingOperationListener operationListener : indexingOperationListeners) { indexModule.addIndexOperationListener(operationListener); } pluginsService.onIndexModule(indexModule); for (IndexEventListener listener : builtInListeners) { indexModule.addIndexEventListener(listener); } return indexModule.newIndexService(nodeEnv, this, circuitBreakerService, bigArrays, threadPool, scriptService, indicesQueriesRegistry, clusterService, client, indicesQueryCache, mapperRegistry, indicesFieldDataCache); } /** * creates a new mapper service for the given index, in order to do administrative work like mapping updates. * This *should not* be used for document parsing. Doing so will result in an exception. * * Note: the returned {@link MapperService}	can we add a test that actually uses a custom field mapper and ensure that it's registered here?
public static ColumnInfo readColumnInfo(StreamInput in) throws IOException { String table = in.readOptionalString(); String name = in.readString(); String baseName = in.readOptionalString(); String esType = in.readString(); Integer displaySize = in.readOptionalVInt(); return new ColumnInfo(table, name, esType, displaySize); }	in sql all fields belong to the same schema - we don't really differentiate between different sources and at this point, i think it's fine to use the base name as the table name.
public V get(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException, ExecutionException { assert Transports.isTransportThread(Thread.currentThread()) == false : Thread.currentThread().getName(); return sync.get(unit.toNanos(timeout)); } /* * Improve the documentation of when InterruptedException is thrown. Our * behavior matches the JDK's, but the JDK's documentation is misleading. */ /** * {@inheritDoc} * <p/> * <p>The default {@link BaseFuture} implementation throws {@code * InterruptedException} if the current thread is interrupted before or during * the call, even if the value is already available. * * @throws InterruptedException if the current thread was interrupted before * or during the call (optional but recommended). * @throws CancellationException {@inheritDoc}	may add something like "thread ["+ ..+"] executed a blocking operation but is a network thread" . i think it will make it easier to debug.
private synchronized void batchListStartedShards( ShardId shardId, String customDataPath, DiscoveryNode[] nodes, ActionListener<BaseNodesResponse<NodeGatewayStartedShards>> listener ) { pendingFetchShardCount.incrementAndGet(); // group shards by node for (DiscoveryNode node : nodes) { Map<ShardId, ShardRequestInfo<NodeGatewayStartedShards>> nodeLevelRequests = queuedRequests.computeIfAbsent( node, n -> new HashMap<>() ); nodeLevelRequests.put(shardId, new ShardRequestInfo<>(shardId, customDataPath, listener)); } if (logger.isTraceEnabled()) { for (DiscoveryNode node : nodes) { logger.trace( "Queued number of [{}] async fetch shard requests for node [{}]", queuedRequests.get(node).size(), node.getId() ); } } }	@howardhuanghua shardrequestinfo need not to create for each node, the listener in each shardrequestinfo is the same listener. this can reduce shardrequestinfo object number from o(shard) * o(node ) to o (shard)
protected void doExecute(Task task, HasPrivilegesRequest request, ActionListener<HasPrivilegesResponse> listener) { final String username = request.username(); final Authentication authentication = Authentication.getAuthentication(threadPool.getThreadContext()); final User user = authentication.getUser(); if (user.principal().equals(username) == false) { listener.onFailure(new IllegalArgumentException("users may only check the privileges of their own account")); return; } if (request.indexPrivileges() != null) { try { DLSRoleQueryValidator.validateQueryField(request.indexPrivileges(), xContentRegistry); } catch (ElasticsearchException | IllegalArgumentException e) { listener.onFailure(e); return; } } resolveApplicationPrivileges(request, ActionListener.wrap(applicationPrivilegeDescriptors -> authorizationService.checkPrivileges(authentication, request, applicationPrivilegeDescriptors, listener), listener::onFailure)); }	i don't think this is right. has-privileges shouldn't be doing anything with dls...
@Nullable private static RoleDescriptor checkDescriptor(RoleDescriptor descriptor, Path path, Logger logger, Settings settings, NamedXContentRegistry xContentRegistry) { String roleName = descriptor.getName(); // first check if FLS/DLS is enabled on the role... if (descriptor.isUsingDocumentOrFieldLevelSecurity()) { if (XPackSettings.DLS_FLS_ENABLED.get(settings) == false) { logger.error("invalid role definition [{}] in roles file [{}]. document and field level security is not " + "enabled. set [{}] to [true] in the configuration file. skipping role...", roleName, path .toAbsolutePath(), XPackSettings.DLS_FLS_ENABLED.getKey()); return null; } else { try { DLSRoleQueryValidator.validateQueryField(descriptor.getIndicesPrivileges(), xContentRegistry); } catch (ElasticsearchException | IllegalArgumentException e) { logger.error((Supplier<?>) () -> new ParameterizedMessage( "invalid role definition [{}] in roles file [{}]. failed to validate query field. skipping role...", roleName, path.toAbsolutePath()), e); } } } return descriptor; }	why is this not a failure? shouldn't it return null?
protected Query asQuery(org.elasticsearch.xpack.ql.expression.predicate.logical.BinaryLogic e, TranslatorHandler handler) { if (e instanceof And) { return and(e.source(), toQuery(e.left(), handler), toQuery(e.right(), handler)); } if (e instanceof Or) { return or(e.source(), toQuery(e.left(), handler), toQuery(e.right(), handler)); } return null; } } public static Object valueOf(Expression e) { if (e.foldable()) { return e.fold(); } throw new QlIllegalArgumentException("Cannot determine value for {}", e); } public static class Scalars extends ExpressionTranslator<ScalarFunction> { @Override protected Query asQuery(ScalarFunction f, TranslatorHandler handler) { return doTranslate(f, handler); } public static Query doTranslate(ScalarFunction f, TranslatorHandler handler) { if (f instanceof CIDRMatch) { CIDRMatch cm = (CIDRMatch) f; if (cm.input() instanceof FieldAttribute && Expressions.foldable(cm.addresses())) { String targetFieldName = handler.nameOf(((FieldAttribute) cm.input()).exactAttribute()); Set<Object> set = new LinkedHashSet<>(CollectionUtils.mapSize(cm.addresses().size())); for (Expression e : cm.addresses()) { set.add(valueOf(e)); } return new TermsQuery(f.source(), targetFieldName, set); } } return handler.wrapFunctionQuery(f, f, new ScriptQuery(f.source(), f.asScript())); } } public static class CaseInsensitiveScalarFunctions extends ExpressionTranslator<CaseInsensitiveScalarFunction> { @Override protected Query asQuery(CaseInsensitiveScalarFunction f, TranslatorHandler handler) { return doTranslate(f, handler); } public static Query doTranslate(CaseInsensitiveScalarFunction f, TranslatorHandler handler) { Query q = ExpressionTranslators.Scalars.doKnownTranslate(f, handler); if (q != null) { return q; } if (f instanceof BinaryComparisonCaseInsensitiveFunction) { BinaryComparisonCaseInsensitiveFunction bccif = (BinaryComparisonCaseInsensitiveFunction) f; String targetFieldName = null; String wildcardQuery = null; Expression field = bccif.left(); Expression constant = bccif.right(); if (field instanceof FieldAttribute && constant.foldable()) { targetFieldName = handler.nameOf(((FieldAttribute) field).exactAttribute()); String string = (String) constant.fold(); if (f instanceof StringContains) { wildcardQuery = "*" + string + "*"; } else if (f instanceof EndsWith) { wildcardQuery = "*" + string; } } q = wildcardQuery != null ? new WildcardQuery(f.source(), targetFieldName, wildcardQuery, f.isCaseInsensitive()) : null; } return q; }	not sure if feasible by now (if already mostly used like that in the codebase), but extracting the wildcard character into a constant might make it cleaner.
public DeprecationLogger deprecate( final DeprecationCategory category, final String key, final String msg, final Object... params) { return logDeprecation(CRITICAL, category, key, msg, params); } /** * Logs a message at the {@link Level#INFO}	hmm maybe deprecatenoncritical is better?
@Override public String getChannelType() { return "direct"; } } /** * This interface allows plugins to intercept requests on both the sender and the receiver side. */ public interface TransportInterceptor { /** * This is called for each handler that is registered via * {@link #registerRequestHandler(String, Supplier, String, boolean, boolean, TransportRequestHandler)} or * {@link #registerRequestHandler(String, Supplier, String, TransportRequestHandler)}. The returned handler is * used instead of the passed in handler. By default the provided handler is returned. */ default <T extends TransportRequest> TransportRequestHandler<T> interceptHandler(String action, TransportRequestHandler<T> actualHandler) { return actualHandler; } /** * This is called up-front providing the actual low level {@link AsyncSender} that performs the low level send request. * The returned sender is used to send all requests that come in via * {@link #sendRequest(DiscoveryNode, String, TransportRequest, TransportResponseHandler)} or * {@link #sendRequest(DiscoveryNode, String, TransportRequest, TransportRequestOptions, TransportResponseHandler)}. * This allows plugins to perform actions on each send request including modifying the request context etc. */ default AsyncSender asyncSender(AsyncSender sender) { return sender; } } /** * A simple interface to decorate * {@link #sendRequest(DiscoveryNode, String, TransportRequest, TransportRequestOptions, TransportResponseHandler)}	nit: can this be named in parallel, ie interceptsender?
public void setFlavor(Flavor flavor) { this.flavor.set(flavor); }	i think one thing for not originally naming this "has" is that we configure one of these when we _request_ a distribution and so the naming there makes a bit less sense when the context is that we are _asking_ for a distribution w/ or w/o a bundled jdk.
private List<ElasticsearchDistribution> configureDistributions(Project project, Version upgradeVersion) { NamedDomainObjectContainer<ElasticsearchDistribution> distributions = DistributionDownloadPlugin.getContainer(project); List<ElasticsearchDistribution> currentDistros = new ArrayList<>(); List<ElasticsearchDistribution> upgradeDistros = new ArrayList<>(); for (Architecture architecture : Architecture.values()) { for (Type type : List.of(Type.DEB, Type.RPM, Type.DOCKER)) { for (Flavor flavor : Flavor.values()) { for (boolean bundledJdk : Arrays.asList(true, false)) { // All our Docker images include a bundled JDK so it doesn't make sense to test without one. // Also we'll never publish an ARM (aarch64) build without a bundled JDK. boolean skip = bundledJdk == false && (type == Type.DOCKER || architecture == Architecture.AARCH64); if (skip == false) { addDistro( distributions, architecture, type, null, flavor, bundledJdk, VersionProperties.getElasticsearch(), currentDistros ); } } } // We don't configure distributions for prior versions for Docker. This is because doing // so prompts Gradle to try and resolve the Docker dependencies, which doesn't work as // they can't be downloaded via Ivy (configured in DistributionDownloadPlugin). Since we // need these for the BATS upgrade tests, and those tests only cover .rpm and .deb, it's // OK to omit creating such distributions in the first place. We may need to revisit // this in the future, so allow upgrade testing using Docker containers. if (type != Type.DOCKER) { // upgrade version is always bundled jdk // NOTE: this is mimicking the old VagrantTestPlugin upgrade behavior. It will eventually be replaced // witha dedicated upgrade test from every bwc version like other bwc tests addDistro(distributions, architecture, type, null, Flavor.DEFAULT, true, upgradeVersion.toString(), upgradeDistros); if (upgradeVersion.onOrAfter("6.3.0")) { addDistro(distributions, architecture, type, null, Flavor.OSS, true, upgradeVersion.toString(), upgradeDistros); } } } } for (Architecture architecture : Architecture.values()) { for (Platform platform : Arrays.asList(Platform.LINUX, Platform.WINDOWS)) { for (Flavor flavor : Flavor.values()) { for (boolean bundledJdk : Arrays.asList(true, false)) { if (bundledJdk == false && architecture != Architecture.X64) { // We will never publish distributions for non-x86 (amd64) platforms // without a bundled JDK continue; } addDistro( distributions, architecture, Type.ARCHIVE, platform, flavor, bundledJdk, VersionProperties.getElasticsearch(), currentDistros ); } } } } // temporary until distro tests have one test per distro Configuration packagingConfig = project.getConfigurations().create(DISTRIBUTIONS_CONFIGURATION); List<Configuration> distroConfigs = currentDistros.stream() .filter(d -> d.getType() != Type.DOCKER) .map(ElasticsearchDistribution::getConfiguration) .collect(Collectors.toList()); packagingConfig.setExtendsFrom(distroConfigs); Configuration packagingUpgradeConfig = project.getConfigurations().create(UPGRADE_CONFIGURATION); List<Configuration> distroUpgradeConfigs = upgradeDistros.stream() .map(ElasticsearchDistribution::getConfiguration) .collect(Collectors.toList()); packagingUpgradeConfig.setExtendsFrom(distroUpgradeConfigs); return currentDistros; }	i'd like clarification on this. why wouldn't we? one main difference between the archives is the version of the bundled jdk but there are other differences (config changes, eventually ml binaries, etc). @jasontedor what's the rationale for offering a no-jdk distro for x86 but not arm?
* @return Base URL + path */ private URL buildPath(BlobPath path) throws MalformedURLException { List<String> paths = path.parts(); if (paths.isEmpty()) { return path(); } URL blobPath = new URL(this.path, paths.get(0) + "/"); if (paths.size() > 1) { for (int i = 1; i < paths.size(); i++) { blobPath = new URL(blobPath, paths.get(i) + "/"); } } return blobPath; }	this if isn't necessary, we check the condition in the loop anyway.
private Path buildPath(BlobPath path) { List<String> paths = path.parts(); if (paths.isEmpty()) { return path(); } Path blobPath = this.path.resolve(paths.get(0)); if (paths.size() > 1) { for (int i = 1; i < paths.size(); i++) { blobPath = blobPath.resolve(paths.get(i)); } } return blobPath; }	this if isn't necessary, we check the condition in the for loop anyway.
private static boolean authorizeMappingUpdateBwcSpecialCase(Group group, String action) { return (action.equals(PutMappingAction.NAME) || action.equals(AutoPutMappingAction.NAME)) && (group.privilege().name().containsAll(IndexPrivilege.CREATE_DOC.name()) || group.privilege().name().containsAll(IndexPrivilege.CREATE.name()) || group.privilege().name().containsAll(IndexPrivilege.INDEX.name()) || group.privilege().name().containsAll(IndexPrivilege.WRITE.name())); }	nit: i'd simply the name to be just authorizemappingupdatebwc.
@Override protected IterationResult<DataFrameIndexerPosition> doProcess(SearchResponse searchResponse) { final Aggregations aggregations = searchResponse.getAggregations(); // Treat this as a "we reached the end". // This should only happen when all underlying indices have gone away. Consequently, there is no more data to read. if (aggregations == null) { logger.info("[" + getJobId() + "] unexpected null aggregations in search response. " + "Concrete indices may have disappeared or have been closed"); auditor.info(getJobId(), "Data frame transform unexpectedly stopped seeing data. " + "Please verify that these indices exist and are open [" + Strings.arrayToCommaDelimitedString(getConfig().getSource().getIndex()) + "]."); return new IterationResult<>(Collections.emptyList(), null, true); } final CompositeAggregation agg = aggregations.get(COMPOSITE_AGGREGATION_NAME); switch (runState) { case FULL_RUN: return processBuckets(agg); case PARTIAL_RUN_APPLY_CHANGES: return processPartialBucketUpdates(agg); case PARTIAL_RUN_IDENTIFY_CHANGES: return processChangedBuckets(agg); default: // Any other state is a bug, should not happen logger.warn("Encountered unexpected run state [" + runState + "]"); throw new IllegalStateException("DataFrame indexer job encountered an illegal state [" + runState + "]"); } }	i waffled a bit on how to word this. from my understanding, aggregations should only ever be null here iff the underlying indices are all closed/deleted
public static void renderThrowable(XContentBuilder builder, Params params, Throwable t) throws IOException { builder.startObject("error"); final ElasticsearchException[] rootCauses = ElasticsearchException.guessRootCauses(t); builder.field("root_cause"); builder.startArray(); for (ElasticsearchException rootCause : rootCauses){ builder.startObject(); rootCause.toXContent(builder, new ToXContent.DelegatingMapParams(Collections.singletonMap(ElasticsearchException.REST_EXCEPTION_SKIP_CAUSE, "true"), params)); builder.endObject(); } builder.endArray(); ElasticsearchException.toXContent(builder, params, t); builder.endObject(); }	this seems a bit meaningless. maybe a javadoc on the interface about how its built rather than on apply?
private void addDebugDecision(Decision.Multi ret, Decision decision, RoutingAllocation allocation) { if (decision != Decision.ALWAYS && (allocation.getDebugMode() == RoutingAllocation.DebugMode.ON || decision.type() != Decision.Type.YES)) { ret.add(decision); } }	perhaps add a comment explaining this condition. i had to rewrite it in my head to debugmode != on => type != yes
private synchronized void mergeTopDocs(QuerySearchResult result) { if (result.isNull() || hasPrimaryFieldSort == false || result.topDocs().topDocs instanceof TopFieldDocs == false) { return; } // merge the current best bottom field doc with the new query result TopFieldDocs topDocs = (TopFieldDocs) result.topDocs().topDocs; final ScoreDoc[] bottomDocs; if (topDocs.scoreDocs.length == topDocsSize) { FieldDoc bottom = (FieldDoc) topDocs.scoreDocs[topDocsSize - 1]; bottomDocs = new FieldDoc[] { new FieldDoc(bottom.doc, bottom.score, bottom.fields, result.getShardIndex()) }; bottomDocs[0].shardIndex = result.getShardIndex(); } else { bottomDocs = new ScoreDoc[0]; } TopFieldDocs toMerge = new TopFieldDocs(topDocs.totalHits, bottomDocs, topDocs.fields); if (bottomTopDocs == null) { bottomTopDocs = toMerge; } else { final Sort sort = new Sort(bottomTopDocs.fields); bottomTopDocs = TopFieldDocs.merge(sort, 0, 1, new TopFieldDocs[]{bottomTopDocs, toMerge}, false); } }	previously using fieldcomparator.comparevalues looks to be faster than using topfielddocs.merge (as topfielddocs.merge doing a lot of stuff that we don't really need), especially considering that mergetopdocs is a synchronized function, and we want to be quite fast.
@Override protected void masterOperation(final CreateIndexRequest request, final ClusterState state, final ActionListener<CreateIndexResponse> listener) { String cause = request.cause(); if (cause.isEmpty()) { cause = "api"; } final String indexName = indexNameExpressionResolver.resolveDateMathExpression(request.index()); final SystemIndexDescriptor descriptor = systemIndices.findMatchingDescriptor(indexName); final boolean isSystemIndex = descriptor != null && descriptor.isAutomaticallyManaged(); final CreateIndexClusterStateUpdateRequest updateRequest; // Requests that a cluster generates itself are permitted to create a system index with // different mappings, settings etc. This is so that rolling upgrade scenarios still work. // We check this via the request's origin. Eventually, `SystemIndexManager` will reconfigure // the index to the latest settings. if (isSystemIndex && Strings.isNullOrEmpty(request.origin())) { final String message = descriptor.checkMinimumNodeVersion("create index", state.nodes().getMinNodeVersion()); if (message != null) { logger.warn(message); listener.onFailure(new IllegalStateException(message)); return; } updateRequest = buildSystemIndexUpdateRequest(request, cause, descriptor); } else { updateRequest = buildUpdateRequest(request, cause, indexName); } createIndexService.createIndex(updateRequest, listener.map(response -> new CreateIndexResponse(response.isAcknowledged(), response.isShardsAcknowledged(), indexName))); }	this is pretty minor but request.origin() should never be null based on my reading of the code. is this correct? if it should never be null, then i believe we should just call request.origin().isempty() here and the other two places.
String getDescription() { final List<String> clusterStateNodes = StreamSupport.stream(clusterState.nodes().getMasterNodes().values().spliterator(), false) .map(n -> n.value.toString()).collect(Collectors.toList()); final String discoveryWillContinueDescription = String.format(Locale.ROOT, "discovery will continue using %s from hosts providers and %s from last-known cluster state; " + "node term %d, last-accepted version %d in term %d", resolvedAddresses, clusterStateNodes, currentTerm, clusterState.version(), clusterState.term()); final String discoveryStateIgnoringQuorum = String.format(Locale.ROOT, "have discovered %s; %s", foundPeers, discoveryWillContinueDescription); if (clusterState.nodes().getLocalNode().isMasterNode() == false) { return String.format(Locale.ROOT, "master not discovered yet: %s", discoveryStateIgnoringQuorum); } if (clusterState.getLastAcceptedConfiguration().isEmpty()) { final String bootstrappingDescription; if (INITIAL_MASTER_NODES_SETTING.get(Settings.EMPTY).equals(INITIAL_MASTER_NODES_SETTING.get(settings))) { bootstrappingDescription = "[" + INITIAL_MASTER_NODES_SETTING.getKey() + "] is empty on this node"; } else { bootstrappingDescription = String.format(Locale.ROOT, "this node must discover master-eligible nodes %s to bootstrap a cluster", INITIAL_MASTER_NODES_SETTING.get(settings)); } return String.format(Locale.ROOT, "master not discovered yet, this node has not previously joined a bootstrapped cluster, and %s: %s", bootstrappingDescription, discoveryStateIgnoringQuorum); } assert clusterState.getLastCommittedConfiguration().isEmpty() == false; if (clusterState.getLastCommittedConfiguration().equals(VotingConfiguration.MUST_JOIN_ELECTED_MASTER)) { return String.format(Locale.ROOT, "master not discovered yet and this node was detached from its previous cluster, have discovered %s; %s", foundPeers, discoveryWillContinueDescription); } final String quorumDescription; if (clusterState.getLastAcceptedConfiguration().equals(clusterState.getLastCommittedConfiguration())) { quorumDescription = describeQuorum(clusterState.getLastAcceptedConfiguration()); } else { quorumDescription = describeQuorum(clusterState.getLastAcceptedConfiguration()) + " and " + describeQuorum(clusterState.getLastCommittedConfiguration()); } final VoteCollection voteCollection = new VoteCollection(); foundPeers.forEach(voteCollection::addVote); final String isQuorumOrNot = electionStrategy.isElectionQuorum(clusterState.nodes().getLocalNode(), currentTerm, clusterState.term(), clusterState.version(), clusterState.getLastCommittedConfiguration(), clusterState.getLastAcceptedConfiguration(), voteCollection) ? "is a quorum" : "is not a quorum"; return String.format(Locale.ROOT, "master not discovered or elected yet, an election requires %s, have discovered %s which %s; %s", quorumDescription, foundPeers, isQuorumOrNot, discoveryWillContinueDescription); }	done, see call to clusterbootstrapservice::isbootstrapplaceholder in describequorum.
private void handleFollowerCheck(FollowerCheckRequest request, TransportChannel transportChannel) throws IOException { FastResponseState responder = this.fastResponseState; if (responder.mode == Mode.FOLLOWER && responder.term == request.term) { logger.trace("responding to {} on fast path", request); transportChannel.sendResponse(Empty.INSTANCE); return; } if (request.term < responder.term) { throw new CoordinationStateRejectedException("rejecting " + request + " since local state is " + this); } transportService.getThreadPool().generic().execute(new AbstractRunnable() { @Override protected void doRun() throws IOException { logger.trace("responding to {} on slow path", request); try { handleRequestAndUpdateState.accept(request); } catch (Exception e) { transportChannel.sendResponse(e); return; } transportChannel.sendResponse(Empty.INSTANCE); } @Override public void onFailure(Exception e) { logger.debug(new ParameterizedMessage("exception while responding to {}", request), e); } @Override public String toString() { return "slow path response to " + request; } }); }	this happens by sending a publish response with no associated join.
void sendPublishRequest() { if (isFailed()) { return; } assert state == PublicationTargetState.NOT_STARTED : state + " -> " + PublicationTargetState.SENT_PUBLISH_REQUEST; state = PublicationTargetState.SENT_PUBLISH_REQUEST; Publication.this.sendPublishRequest(discoveryNode, publishRequest, new PublishResponseHandler()); assert publicationCompletedIffAllTargetsInactiveOrCancelled(); }	don't think so, it notifies the listener on a failure.
private static RoleDescriptor buildViewerRoleDescriptor() { return new RoleDescriptor( "viewer", new String[] {}, new RoleDescriptor.IndicesPrivileges[] { // Stack RoleDescriptor.IndicesPrivileges.builder() .indices("/~(([.]|ilm-history-).*)/") .privileges("read", "view_index_metadata").build(), // Security RoleDescriptor.IndicesPrivileges.builder().indices(".siem-signals-*").privileges("read", "view_index_metadata").build() }, new RoleDescriptor.ApplicationResourcePrivileges[] { RoleDescriptor.ApplicationResourcePrivileges.builder() .application("kibana-.kibana") .resources("*") .privileges("read").build() }, null, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null); }	there are other indices that might be of interest such as: .monitoring-* and .ml-*, and maybe others (search for ". in reservedrolesstore). only pointing it out, i don't have the full picture, but those two fit the pr description "full access to data and configuration" .
public static Map<RealmConfig.RealmIdentifier, Settings> getRealmSettings(Settings globalSettings) { Settings settingsByType = globalSettings.getByPrefix(RealmSettings.PREFIX); return settingsByType.names().stream() .flatMap(type -> { final Settings settingsByName = settingsByType.getAsSettings(type); return settingsByName.names().stream().map(name -> { final RealmConfig.RealmIdentifier id = new RealmConfig.RealmIdentifier(type, name); final Settings realmSettings = settingsByName.getAsSettings(name); verifyRealmSettings(id, realmSettings); return new Tuple<>(id, realmSettings); }); }) .collect(Collectors.toMap(Tuple::v1, Tuple::v2)); }	nit: we could add hasnonsecuresettings method on settings instead of copying all the non-secure settings
@Override LeafBucketCollector getLeafCollector(Comparable<BytesRef> value, LeafReaderContext context, LeafBucketCollector next) throws IOException { final boolean leafReaderContextChanged = context.ord != leafReaderOrd; assert leafReaderContextChanged == false || invariant(); // for performance reasons only check invariant upon change if (value.getClass() != BytesRef.class) { throw new IllegalArgumentException("Expected BytesRef, got " + value.getClass()); } BytesRef term = (BytesRef) value; if (leafReaderContextChanged) { // use a separate instance for ordinal and term lookups, that is cached per segment // to speed up sorted collections that call getLeafCollector once per term final SortedSetDocValues newLookup = dvsLookup.computeIfAbsent(context.ord, k -> { try { return docValuesFunc.apply(context); } catch (IOException e) { throw new UncheckedIOException(e); } }); remapOrdinals(lookup, newLookup); lookup = newLookup; } currentValueOrd = lookup.lookupTerm(term); currentValueUnmapped = null; leafReaderOrd = context.ord; assert currentValueOrd >= 0; assert leafReaderContextChanged == false || invariant(); // for performance reasons only check invariant upon change return next; }	this comment isn't right for this method. here, after your refactoring, we don't have a separate instance as there is no iteration anymore.
public void testShrinkThenSplitWithFailedNode() throws Exception { internalCluster().ensureAtLeastNumDataNodes(3); final int shardCount = between(2, 5); prepareCreate("original").setSettings(Settings.builder().put(indexSettings()) .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, shardCount)).get(); for (int i = 0; i < 30; i++) { client().prepareIndex("original", "_doc") .setSource("{\\\\"foo\\\\" : \\\\"bar\\\\", \\\\"i\\\\" : " + i + "}", XContentType.JSON).get(); } client().admin().indices().prepareFlush("original").get(); ensureGreen(); final String shrinkNode = client().admin().cluster().prepareNodesInfo("data:true").clear().get().getNodes().get(0).getNode().getName(); client().admin().indices().prepareUpdateSettings("original") .setSettings(Settings.builder() .put(IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getConcreteSettingForNamespace("_name").getKey(), shrinkNode) .put(IndexMetaData.SETTING_BLOCKS_WRITE, true)).get(); ensureGreen(); assertAcked(client().admin().indices().prepareResizeIndex("original", "shrunk").setSettings(Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1) .putNull(IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getConcreteSettingForNamespace("_name").getKey()) .build()).setResizeType(ResizeType.SHRINK).get()); ensureGreen(); internalCluster().stopRandomNode(InternalTestCluster.nameFilter(shrinkNode)); // demonstrate that the index.routing.allocation.initial_recovery setting from the shrink doesn't carry over into the split index, // because this would cause the shrink to fail as the initial_recovery node is no longer present. logger.info("--> executing split"); assertAcked(client().admin().indices().prepareResizeIndex("shrunk", "splitagain").setSettings(Settings.builder() .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0) .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, shardCount) .putNull(IndexMetaData.INDEX_ROUTING_REQUIRE_GROUP_SETTING.getConcreteSettingForNamespace("_name").getKey()) .build()).setResizeType(ResizeType.SPLIT)); ensureGreen("splitagain"); }	nit: .setsource("foo", "bar", "i" , i) seems nicer? :)
* @param shardIt the shard iterator */ protected void onShardResult(Result result, SearchShardIterator shardIt) { assert result.getShardIndex() != -1 : "shard index is not set"; assert result.getSearchShardTarget() != null : "search shard target must not be null"; hasShardResponse.set(true); successfulOps.incrementAndGet(); results.consumeResult(result); if (logger.isTraceEnabled()) { logger.trace("got first-phase result from {}", result != null ? result.getSearchShardTarget() : null); } // clean a previous error on this shard group (note, this code will be serialized on the same shardIndex value level // so its ok concurrency wise to miss potentially the shard failures being created because of another failure // in the #addShardFailure, because by definition, it will happen on *another* shardIndex AtomicArray<ShardSearchFailure> shardFailures = this.shardFailures.get(); if (shardFailures != null) { shardFailures.set(result.getShardIndex(), null); } // we need to increment successful ops first before we compare the exit condition otherwise if we // are fast we could concurrently update totalOps but then preempt one of the threads which can // cause the successor to read a wrong value from successfulOps if second phase is very fast ie. count etc. // increment all the "future" shards to update the total ops since we some may work and some may not... // and when that happens, we break on total ops, so we must maintain them successfulShardExecution(shardIt); }	nit: can we use successfulops instead? also, i think we can have up to maxconcurrentrequestspernode responses instead of one. but i don't see a way to avoid this as any shard request can fail.
public String preference() { return preference; } /** * Returns true if the caller can handle null response {@link QuerySearchResult#nullInstance()}	nit: i think canreturnnullresponseifmatchnodocs is a better name.
private DeleteResult innerDelete(Delete delete) throws IOException { assert assertSequenceNumber(delete.origin(), delete.seqNo()); final Translog.Location location; final long updatedVersion; final boolean found; DeleteResult deleteResult = null; long seqNo = delete.seqNo(); try (Releasable ignored = acquireLock(delete.uid())) { lastWriteNanos = delete.startTime(); final long currentVersion; final boolean deleted; final VersionValue versionValue = versionMap.getUnderLock(delete.uid()); assert incrementVersionLookup(); if (versionValue == null) { currentVersion = loadCurrentVersionFromIndex(delete.uid()); deleted = currentVersion == Versions.NOT_FOUND; } else { currentVersion = checkDeletedAndGCed(versionValue); deleted = versionValue.delete(); } final long expectedVersion = delete.version(); final Optional<DeleteResult> result = checkVersionConflict( delete, currentVersion, expectedVersion, deleted, () -> new DeleteResult(expectedVersion, delete.seqNo(), true), e -> new DeleteResult(e, expectedVersion, delete.seqNo())); if (result.isPresent()) { deleteResult = result.get(); } else { if (delete.origin() == Operation.Origin.PRIMARY) { seqNo = seqNoService().generateSeqNo(); } updatedVersion = delete.versionType().updateVersion(currentVersion, expectedVersion); found = deleteIfFound(delete.uid(), currentVersion, deleted, versionValue); deleteResult = new DeleteResult(updatedVersion, seqNo, found); location = delete.origin() != Operation.Origin.LOCAL_TRANSLOG_RECOVERY ? translog.add(new Translog.Delete(delete, deleteResult)) : null; versionMap.putUnderLock(delete.uid().bytes(), new DeleteVersionValue(updatedVersion, engineConfig.getThreadPool().estimatedTimeInMillis())); deleteResult.setTranslogLocation(location); } deleteResult.setTook(System.nanoTime() - delete.startTime()); deleteResult.freeze(); return deleteResult; } finally { if (seqNo != SequenceNumbersService.UNASSIGNED_SEQ_NO) { seqNoService().markSeqNoAsCompleted(deleteResult.getSeqNo()); } } }	this should use the seqno variable
void getAndValidateToken(ThreadContext ctx, ActionListener<UserToken> listener) { if (isEnabled()) { final String token = getFromHeader(ctx); if (token == null) { listener.onResponse(null); } else { decodeToken(token, ActionListener.wrap(userToken -> { if (userToken != null) { checkIfTokenIsValid(userToken, listener); } else { listener.onResponse(null); } }, listener::onFailure)); } } else { listener.onResponse(null); } } /** * Decodes the provided token, and validates it (for format, expiry and invalidation). * If valid, the token's {@link Authentication} (see {@link UserToken#getAuthentication()} is provided to the listener. * If the token is invalid (expired etc), then {@link ActionListener#onFailure(Exception)} will be called. * If tokens are not enabled, or the token does not exist, {@link ActionListener#onResponse} will be called with a * {@code null}	i believe this new method should return the new [fancy 400 exceptions](https://github.com/elastic/elasticsearch/pull/52811) when the token service is not enabled (since it's not used in the authentication chain).
void getAndValidateToken(ThreadContext ctx, ActionListener<UserToken> listener) { if (isEnabled()) { final String token = getFromHeader(ctx); if (token == null) { listener.onResponse(null); } else { decodeToken(token, ActionListener.wrap(userToken -> { if (userToken != null) { checkIfTokenIsValid(userToken, listener); } else { listener.onResponse(null); } }, listener::onFailure)); } } else { listener.onResponse(null); } } /** * Decodes the provided token, and validates it (for format, expiry and invalidation). * If valid, the token's {@link Authentication} (see {@link UserToken#getAuthentication()} is provided to the listener. * If the token is invalid (expired etc), then {@link ActionListener#onFailure(Exception)} will be called. * If tokens are not enabled, or the token does not exist, {@link ActionListener#onResponse} will be called with a * {@code null}	also because this new method is not used in the authn chain, when a token fails decoding it's a client error. i believe: listener.onfailure(new illegalargumentexception()); would do, it would return 400 (currently this falls back to 500 in apikeygenerator#generateapikey).
static DeprecationIssue checkClusterRoutingAllocationIncludeRelocationsSetting(final Settings settings, final PluginsAndModules pluginsAndModules, final ClusterState clusterState) { DeprecationIssue nodeDeprecationIssue = getClusterRoutingAllocationIncludeRelocationsSettingDeprecationIssue(settings); if (nodeDeprecationIssue != null) { return nodeDeprecationIssue; } // The setting is dynamic so it can be defined stored in the ClusterState settings return getClusterRoutingAllocationIncludeRelocationsSettingDeprecationIssue(clusterState.metadata().settings()); }	i think it's critical at least in the case where it's in the node settings. if it's set dynamically then that's technically ok to proceed, we'll just archive it, but that's also a case we can fix at runtime so maybe not so bad to call it critical too? will the upgrade assistant automatically help users remove this setting if set dynamically or do we need to take action on that front too?
protected void assertResponse(EqlSearchResponse response) { Hits hits = response.hits(); if (hits.events() != null) { assertSearchHits(hits.events()); } else if (hits.sequences() != null) { assertSequences(hits.sequences()); } else { fail("No events or sequences found"); } }	this is an odd formatting for if else if .... seems like the same line is more common in the current code
private void maybeStartApiKeyRemover() { if (securityIndex.isAvailable()) { if (client.threadPool().relativeTimeInMillis() - lastExpirationRunMs > deleteInterval.getMillis()) { expiredApiKeysRemover.submit(client.threadPool()); lastExpirationRunMs = client.threadPool().relativeTimeInMillis(); } } } /** * Get API key information for given realm, user, API key name and id combination * @param realmName realm name * @param username user name * @param apiKeyName API key name * @param apiKeyId API key id * @param listener listener for {@link GetApiKeysResult}	apikeyservice has two apis for retrieving and invalidating api keys. they do not enforce any authorization but with validation that atleast one of the parameters must be specified. getapikeys(string realmname, string username, string apikeyname, string apikeyid); invalidateapikeys(string realmname, string username, string apikeyname, string apikeyid);
boolean checkSameUserPermissions(String action, TransportRequest request, Authentication authentication) { final boolean actionAllowed = SAME_USER_PRIVILEGE.test(action); if (actionAllowed) { if (request instanceof UserRequest) { UserRequest userRequest = (UserRequest) request; String[] usernames = userRequest.usernames(); if (usernames == null || usernames.length != 1 || usernames[0] == null) { assert false : "this role should only be used for actions to apply to a single user"; return false; } final String username = usernames[0]; final boolean sameUsername = authentication.getUser().principal().equals(username); if (sameUsername && ChangePasswordAction.NAME.equals(action)) { return checkChangePasswordAction(authentication); } assert AuthenticateAction.NAME.equals(action) || HasPrivilegesAction.NAME.equals(action) || GetUserPrivilegesAction.NAME.equals(action) || sameUsername == false : "Action '" + action + "' should not be possible when sameUsername=" + sameUsername; return sameUsername; } else if (request instanceof GetApiKeyRequest) { GetApiKeyRequest getApiKeyRequest = (GetApiKeyRequest) request; if (authentication.getAuthenticatedBy().getType().equals("_es_api_key")) { // if authenticated by API key id then the request must also contain same API key id String authenticatedApiKeyId = (String) authentication.getMetadata().get("_security_api_key_id"); if (Strings.hasText(getApiKeyRequest.getApiKeyId())) { return getApiKeyRequest.getApiKeyId().equals(authenticatedApiKeyId); } } } else { assert false : "right now only a user request or get api key request for API key should be allowed"; return false; } } return false; }	this is required for case where the api key is used for authentication and it should be able to get the details and it does not have any api key privileges in the assigned role.
public StatusILMResponse StatusILM(TimedRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequestAndParseEntity(request, RequestConverters::statusILM, options, StatusILMResponse::fromXContent, emptySet()); } /** * Asynchronously get the status of index lifecycle management * See <a href="https://fix-me-when-we-have-docs.com"> * the docs</a> for more. * * @param request the request with user defined timeouts. * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	~~method names should start with a lowercase letter, so this should be statusilm.~~ ~~or maybe getilmstatus? statusilm doesn't make much sense seeing it for the first time - there's no verb there.~~ lee beat me to it and left better feedback, see his comment instead.
public StatusILMResponse StatusILM(TimedRequest request, RequestOptions options) throws IOException { return restHighLevelClient.performRequestAndParseEntity(request, RequestConverters::statusILM, options, StatusILMResponse::fromXContent, emptySet()); } /** * Asynchronously get the status of index lifecycle management * See <a href="https://fix-me-when-we-have-docs.com"> * the docs</a> for more. * * @param request the request with user defined timeouts. * @param options the request options (e.g. headers), use {@link RequestOptions#DEFAULT}	as above re: method names.
public void nextToken() throws IOException { this.fillBytesRef(result); if (posIncAttr.getPositionIncrement() > 0 && result.get().bytesEquals(candidate.term)) { BytesRef term = result.toBytesRef(); // We should not use frequency(term) here because it will analyze the term again // If preFilter and postFilter are the same analyzer it would fail. TermStats termStats = internalTermStats(term); candidates.add(new Candidate(result.toBytesRef(), termStats, candidate.stringDistance, score(candidate.termStats, candidate.stringDistance, sumTotalTermFreq), false)); } else { candidates.add(new Candidate(result.toBytesRef(), candidate.termStats, nonErrorLikelihood, score(candidate.termStats, candidate.stringDistance, sumTotalTermFreq), false)); } }	i'd love to have a test that calls this with big numbers and validates that it returns integer.max_value.
Integer maxInspections() { return maxInspections; } /** * Sets a maximum threshold in number of documents a suggest text token * can exist in order to be corrected. Can be a relative percentage * number (e.g 0.4) or an absolute number to represent document * frequencies. If an value higher than 1 is specified then fractional * can not be specified. Defaults to {@code 0.01}	this might to a bit overzealous copy and replace.
public void setIndex() throws IOException { index = getTestName().toLowerCase(Locale.ROOT); // TODO prevents 6.x warnings; remove in 8.0 if (isRunningAgainstOldCluster()) { XContentBuilder template = jsonBuilder(); template.startObject(); { template.field("index_patterns", "*"); template.field("order", "0"); template.startObject("settings"); template.field("number_of_shards", 5); template.endObject(); } template.endObject(); Request createTemplate = new Request("PUT", "/_template/template"); createTemplate.setJsonEntity(Strings.toString(template)); client().performRequest(createTemplate); } }	two things here: 1. could you add something like assertthat("we don't need this branch if we aren't compatible with 6.0", version.current.minimumindexcompatibilityversion(), lessthanorequalto(version.6.0.0)); 2. could you check if getoldclusterversion() is less than 7 before doing this? it won't matter right now, but once we release 7.0 it'll show up as an "old" version and we don't want this template.
public void waitForMlTemplates() throws Exception { XPackRestTestHelper.waitForMlTemplates(client()); // TODO prevents 6.x warnings; remove in 8.0 if (isRunningAgainstOldCluster()) { XContentBuilder template = jsonBuilder(); template.startObject(); { template.field("index_patterns", "*"); template.field("order", "0"); template.startObject("settings"); template.field("number_of_shards", 5); template.endObject(); } template.endObject(); Request createTemplate = new Request("PUT", "/_template/template"); createTemplate.setJsonEntity(Strings.toString(template)); client().performRequest(createTemplate); } }	same note as above about the checking if the version is in range and asserting the index compatible version.
private ElasticsearchException assignmentFailureReason(ReindexTaskStateDoc oldDoc) { if (oldDoc.isResilient()) { if (oldDoc.getAllocationId() == null || allocationId > oldDoc.getAllocationId()) { return null; } else { return new ElasticsearchException("A newer task has already been allocated"); } } else { if (oldDoc.getAllocationId() == null) { return null; } else { return new ElasticsearchException("A prior task has already been allocated and reindexing is configured to be " + "non-resilient"); } } }	i think we should add a reference to the cluster level setting, mentioning its name here. this could nudge some users into trying it out. if you defer adding the cluster level setting to a follow-up, let us add a todo here so that we remember it.
public void testFailoverAssignmentFailsIfNonResilient() throws Exception { String taskId = randomAlphaOfLength(10); ReindexIndexClient reindexClient = getReindexClient(); createDoc(reindexClient, taskId, false); ReindexTaskStateUpdater updater = new ReindexTaskStateUpdater(reindexClient, client().threadPool(), taskId, 0, ActionListener.wrap(() -> {}), () -> {}); CountDownLatch successLatch = new CountDownLatch(1); updater.assign(new ActionListener<>() { @Override public void onResponse(ReindexTaskStateDoc stateDoc) { successLatch.countDown(); } @Override public void onFailure(Exception exception) { successLatch.countDown(); fail(); } }); successLatch.await(); ReindexTaskStateUpdater oldAllocationUpdater = new ReindexTaskStateUpdater(reindexClient, client().threadPool(), taskId, 1, ActionListener.wrap(() -> {}), () -> {}); CountDownLatch failureLatch = new CountDownLatch(1); AtomicReference<Exception> exceptionRef = new AtomicReference<>(); oldAllocationUpdater.assign(new ActionListener<>() { @Override public void onResponse(ReindexTaskStateDoc stateDoc) { failureLatch.countDown(); fail(); } @Override public void onFailure(Exception exception) { exceptionRef.set(exception); failureLatch.countDown(); } }); failureLatch.await(); assertThat(exceptionRef.get().getMessage(), equalTo("A prior task has already been allocated and reindexing is configured to be non-resilient")); }	nit: not sure i understand the name oldallocationupdater (seems to be a copy-paste from another test)? i would call this one failedoverupdater or something similar?
@Override public synchronized void updateMetaData(final IndexMetaData currentIndexMetaData, final IndexMetaData newIndexMetaData) { final Translog.Durability oldTranslogDurability = indexSettings.getTranslogDurability(); final TimeValue oldSyncInterval = indexSettings.getTranslogSyncInterval(); final boolean updateIndexMetaData = indexSettings.updateIndexMetaData(newIndexMetaData); if (Assertions.ENABLED && currentIndexMetaData != null && currentIndexMetaData.getCreationVersion().onOrAfter(Version.V_6_5_0)) { final long currentSettingsVersion = currentIndexMetaData.getSettingsVersion(); final long newSettingsVersion = newIndexMetaData.getSettingsVersion(); if (currentSettingsVersion == newSettingsVersion) { assert updateIndexMetaData == false; } else { assert updateIndexMetaData; assert currentSettingsVersion < newSettingsVersion : "expected current settings version [" + currentSettingsVersion + "] " + "to be less than new settings version [" + newSettingsVersion + "]"; } } if (updateIndexMetaData) { for (final IndexShard shard : this.shards.values()) { try { shard.onSettingsChanged(); } catch (Exception e) { logger.warn( () -> new ParameterizedMessage( "[{}] failed to notify shard about setting change", shard.shardId().id()), e); } } if (refreshTask.getInterval().equals(indexSettings.getRefreshInterval()) == false) { // once we change the refresh interval we schedule yet another refresh // to ensure we are in a clean and predictable state. // it doesn't matter if we move from or to <code>-1</code> in both cases we want // docs to become visible immediately. This also flushes all pending indexing / search requests // that are waiting for a refresh. threadPool.executor(ThreadPool.Names.REFRESH).execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { logger.warn("forced refresh failed after interval change", e); } @Override protected void doRun() throws Exception { maybeRefreshEngine(true); } @Override public boolean isForceExecution() { return true; } }); rescheduleRefreshTasks(); } final Translog.Durability durability = indexSettings.getTranslogDurability(); final TimeValue syncInterval = indexSettings.getTranslogSyncInterval(); if (syncInterval.equals(oldSyncInterval) == false) { rescheduleFsyncTask(() -> new AsyncTranslogFSync(IndexService.this)); } else if (durability != oldTranslogDurability) { rescheduleFsyncTask(durability); } } }	what if both durability and sync interval are changed in the same update? this update here ignores the change to durability. in particular, if durability is sync, this will now suddenly schedule a sync task when the sync interval is changed even though there's no such sync task supposed to run due to durability sync. it might be better to restructure the sync task scheduling code to not look at oldtranslogdurability but just at whether we have a current task and whether we're supposed to have one, and if we have both, check that the intervals match (you can get the current tasks interval directly from the task (see refreshtask.getinterval() as an example a few lines above it), and then just do a plain reschedulefsynctask without parameters where it gets all it needs from the indexsettings (which have been updated).
protected void doMerge(final ObjectMapper mergeWith, MergeReason reason) { if (nested().isNested()) { if (!mergeWith.nested().isNested()) { throw new IllegalArgumentException("object mapping [" + name() + "] can't be changed from nested to non-nested"); } } else { if (mergeWith.nested().isNested()) { throw new IllegalArgumentException("object mapping [" + name() + "] can't be changed from non-nested to nested"); } } if (mergeWith.dynamic != null) { this.dynamic = mergeWith.dynamic; } checkObjectMapperParameters(mergeWith); for (Mapper mergeWithMapper : mergeWith) { Mapper mergeIntoMapper = mappers.get(mergeWithMapper.simpleName()); Mapper merged; if (mergeIntoMapper == null) { // no mapping, simply add it merged = mergeWithMapper; } else { if (mergeIntoMapper instanceof ObjectMapper) { ObjectMapper objectMapper = (ObjectMapper) mergeIntoMapper; merged = objectMapper.merge(mergeWithMapper, reason); } else { assert mergeIntoMapper instanceof FieldMapper || mergeIntoMapper instanceof FieldAliasMapper; // If we're merging template mappings when creating an index, then a field definition always // replaces an existing one. if (reason == MergeReason.INDEX_TEMPLATE) { merged = mergeWithMapper; } else { merged = mergeIntoMapper.merge(mergeWithMapper); } } } putMapper(merged); } }	this is the most interesting part, we decide to overwrite leaf fields instead of merging them.
@Override protected DateRangeAggregatorFactory innerBuild(SearchContext context, ValuesSourceConfig<Numeric> config, AggregatorFactory<?> parent, Builder subFactoriesBuilder) throws IOException { // We need to call processRanges here so they are parsed and we know whether `now` has been used before we make // the decision of whether to cache the request Range[] ranges = processRanges(context, config); return new DateRangeAggregatorFactory(name, config, ranges, keyed, rangeFactory, context, parent, subFactoriesBuilder, metaData); }	could we put the check here so that it is run for both the transport and rest apis? when put here we should make it an illegalargumentexception and we should update the other range aggregation builders too.
private void executeHealth(final ClusterHealthRequest request, final ActionListener<ClusterHealthResponse> listener) { int waitFor = 0; if (request.waitForStatus() != null) { waitFor++; } if (request.waitForNoRelocatingShards()) { waitFor++; } if (request.waitForActiveShards().equals(ActiveShardCount.NONE) == false) { waitFor++; } if (request.waitForNodes().isEmpty() == false) { waitFor++; } if (request.indices() != null && request.indices().length > 0) { // check that they actually exists in the meta data waitFor++; } if (request.waitForNoInitializingShards()) { waitFor++; } final ClusterState state = clusterService.state(); final ClusterStateObserver observer = new ClusterStateObserver(state, clusterService, null, logger, threadPool.getThreadContext()); if (request.timeout().millis() == 0) { listener.onResponse(getResponse(request, state, waitFor, request.timeout().millis() == 0)); return; } final int concreteWaitFor = waitFor; final Predicate<ClusterState> validationPredicate = newState -> validateRequest(request, newState, concreteWaitFor); final ClusterStateObserver.Listener stateListener = new ClusterStateObserver.Listener() { @Override public void onNewClusterState(ClusterState clusterState) { listener.onResponse(getResponse(request, clusterState, concreteWaitFor, false)); } @Override public void onClusterServiceClose() { listener.onFailure(new IllegalStateException("ClusterService was close during health call")); } @Override public void onTimeout(TimeValue timeout) { final ClusterHealthResponse response = getResponse(request, observer.setAndGetObservedState(), concreteWaitFor, true); listener.onResponse(response); } }; if (validationPredicate.test(state)) { stateListener.onNewClusterState(state); } else { observer.waitForNextChange(stateListener, validationPredicate, request.timeout()); } }	let's move this directly under waitfornorelocatingshards
@Override public void applyClusterState(ClusterChangedEvent event) { final Metadata metadata = event.state().metadata(); // clear out mappers for indices that no longer exist or whose timestamp range is no longer known fieldTypesByIndex.keySet().removeIf(index -> hasUsefulTimestampField(metadata.index(index)) == false); // capture mappers for indices that do exist for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) { final IndexMetadata indexMetadata = cursor.value; final Index index = indexMetadata.getIndex(); if (hasUsefulTimestampField(indexMetadata) && fieldTypesByIndex.containsKey(index) == false) { logger.trace("computing timestamp mapping for {}", index); final PlainActionFuture<DateFieldMapper.DateFieldType> future = new PlainActionFuture<>() { @Override protected boolean blockingAllowed() { // We only get the future result once it's completed // so technically we don't block the calling thread. // See #getTimestampFieldType() return true; } }; fieldTypesByIndex.put(index, future); final IndexService indexService = indicesService.indexService(index); if (indexService == null) { logger.trace("computing timestamp mapping for {} async", index); executor.execute(new AbstractRunnable() { @Override public void onFailure(Exception e) { logger.debug(new ParameterizedMessage("failed to compute mapping for {}", index), e); future.onResponse(null); // no need to propagate a failure to create the mapper service to searches } @Override protected void doRun() throws Exception { try (MapperService mapperService = indicesService.createIndexMapperService(indexMetadata)) { mapperService.merge(indexMetadata, MapperService.MergeReason.MAPPING_RECOVERY); future.onResponse(fromMapperService(mapperService)); } } }); } else { logger.trace("computing timestamp mapping for {} using existing index service", index); try { future.onResponse(fromMapperService(indexService.mapperService())); } catch (Exception e) { assert false : e; future.onResponse(null); } } } } }	we can remove this override now that we're setting the timeout.
@Override protected boolean extendedCheck(String action, TransportRequest request, Authentication authentication) { if(authentication == null) { //No valid credentials found return false; } if (request == null) { throw new IllegalArgumentException( "manage own cluster permission check only supported in context of an API key request"); } if (request instanceof CreateApiKeyRequest) { return true; } else if (request instanceof GetApiKeyRequest) { final GetApiKeyRequest getApiKeyRequest = (GetApiKeyRequest) request; return checkIfUserIsOwnerOfApiKeys(authentication, getApiKeyRequest.getApiKeyId(), getApiKeyRequest.getUserName(), getApiKeyRequest.getRealmName(), getApiKeyRequest.ownedByAuthenticatedUser()); } else if (request instanceof InvalidateApiKeyRequest) { final InvalidateApiKeyRequest invalidateApiKeyRequest = (InvalidateApiKeyRequest) request; final String[] apiKeyIds = invalidateApiKeyRequest.getIds(); if (apiKeyIds == null) { return checkIfUserIsOwnerOfApiKeys(authentication, null, invalidateApiKeyRequest.getUserName(), invalidateApiKeyRequest.getRealmName(), invalidateApiKeyRequest.ownedByAuthenticatedUser()); } else { return Arrays.stream(apiKeyIds).allMatch(id -> checkIfUserIsOwnerOfApiKeys(authentication, id, invalidateApiKeyRequest.getUserName(), invalidateApiKeyRequest.getRealmName(), invalidateApiKeyRequest.ownedByAuthenticatedUser())); } } throw new IllegalArgumentException( "manage own api key privilege only supports API key requests (not " + request.getClass().getName() + ")"); }	we should decide between returning false or throwing exceptions when the parameters are null or non-conformant, not both. although i would go with exceptions because the permission in its natural authorization context shouldn't encounter such parameters (ie it's an unexpected condition), the permissions are checked in a stream and if one throws the others are not checked further, which is not what we want in the auditing context. so my recommendation is: if (request == null || authentication == null ) { // audit filtering may invoke this check method in a very loose way, and we shouldn't throw return false; }
* @return an LRU-ordered {@link Iterable} over the values in the cache */ public Iterable<V> values() { return () -> new Iterator<V>() { private CacheIterator iterator = new CacheIterator(head); @Override public boolean hasNext() { return iterator.hasNext(); } @Override public V next() { return iterator.next().value; } @Override public void remove() { iterator.remove(); } }; } /** * An LRU sequencing of the entries in the cache. This sequence is not protected from mutations * to the cache (except for {@link Iterator#remove()}. The result of iteration under any other mutation is * undefined. * * @return an LRU-ordered {@link Iterable}	this looks unused now?
* @throws IOException if the cache file failed to be fsync * @throws java.nio.file.NoSuchFileException if the cache file does not exist */ public SortedSet<Tuple<Long, Long>> fsync() throws IOException { if (refCounter.tryIncRef()) { try { if (needsFsync.compareAndSet(true, false)) { boolean success = false; try { // Capture the completed ranges before fsyncing; ranges that are completed after this point won't be considered as // persisted on disk by the caller of this method, even if they are fully written to disk at the time the file // fsync is effectively executed final SortedSet<Tuple<Long, Long>> completedRanges = tracker.getCompletedRanges(); assert completedRanges != null; assert completedRanges.isEmpty() == false; IOUtils.fsync(file, false, false); success = true; return completedRanges; } finally { if (success == false) { markAsNeedsFSync(); } } } } finally { refCounter.decRef(); } } else { assert evicted.get(); } return Collections.emptySortedSet(); }	if the compareandset above does not succeed, should we then perhaps fail when running tests (using an assert)?
public static void main(String[] args) { System.setProperty("es.logger.prefix", ""); bootstrap = new Bootstrap(); final String pidFile = System.getProperty("es.pidfile", System.getProperty("es-pidfile")); if (pidFile != null) { try { File fPidFile = new File(pidFile); if (fPidFile.getParentFile() != null) { FileSystemUtils.mkdirs(fPidFile.getParentFile()); } FileOutputStream outputStream = new FileOutputStream(fPidFile); outputStream.write(Long.toString(JvmInfo.jvmInfo().pid()).getBytes(Charsets.UTF_8)); outputStream.close(); fPidFile.deleteOnExit(); } catch (Exception e) { String errorMessage = buildErrorMessage("pid", e); System.err.println(errorMessage); System.err.flush(); System.exit(3); } } boolean foreground = System.getProperty("es.foreground", System.getProperty("es-foreground")) != null; // handle the wrapper system property, if its a service, don't run as a service if (System.getProperty("wrapper.service", "XXX").equalsIgnoreCase("true")) { foreground = false; } Tuple<Settings, Environment> tuple = null; try { tuple = initialSettings(); setupLogging(tuple); } catch (Exception e) { String errorMessage = buildErrorMessage("Setup", e); System.err.println(errorMessage); System.err.flush(); System.exit(3); } if (System.getProperty("es.max-open-files", "false").equals("true")) { ESLogger logger = Loggers.getLogger(Bootstrap.class); logger.info("max_open_files [{}]", JmxProcessProbe.getMaxFileDescriptorCount()); } // warn if running using the client VM if (JvmInfo.jvmInfo().vmName().toLowerCase(Locale.ROOT).contains("client")) { ESLogger logger = Loggers.getLogger(Bootstrap.class); logger.warn("jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line"); } String stage = "Initialization"; try { if (!foreground) { Loggers.disableConsoleLogging(); System.out.close(); } bootstrap.setup(true, tuple); stage = "Startup"; bootstrap.start(); if (!foreground) { System.err.close(); } keepAliveLatch = new CountDownLatch(1); // keep this thread alive (non daemon thread) until we shutdown Runtime.getRuntime().addShutdownHook(new Thread() { @Override public void run() { keepAliveLatch.countDown(); } }); keepAliveThread = new Thread(new Runnable() { @Override public void run() { try { keepAliveLatch.await(); } catch (InterruptedException e) { // bail out } } }, "elasticsearch[keepAlive/" + Version.CURRENT + "]"); keepAliveThread.setDaemon(false); keepAliveThread.start(); } catch (Throwable e) { ESLogger logger = Loggers.getLogger(Bootstrap.class); if (bootstrap.node != null) { logger = Loggers.getLogger(Bootstrap.class, bootstrap.node.settings().get("name")); } String errorMessage = buildErrorMessage(stage, e); if (foreground) { System.err.println(errorMessage); System.err.flush(); } else { logger.error(errorMessage); } Loggers.disableConsoleLogging(); logger.error("Exception", e); System.exit(3); } }	since you had to check the log at the debug level to see the error message, it implies that it must have been printed to the console, or else you should have seen it as part of the logger.error(errormessage) printed in the log from line 243. i think the more appropriate change--if any--would be to remove the else block from line 242 and then keep the rest of this change. that way the exception is _always_ logged with the stacktrace (makes sense to have once you start jumping through logs), and foreground usage remains unchanged. this would also mean that line 238 can go into the system.err.println on line 240 rather than always being processed.
@Override public final void parse(ParseContext context) throws IOException { if (hasScript) { throw new MapperParsingException("failed to parse field [" + fieldType().name() + "] of type + " + contentType() + "]", new IllegalArgumentException("Cannot index data directly into a field with a [script] parameter")); } parser.parse(context.parser(), v -> index(context, v), e -> { if (ignoreMalformed()) { context.addIgnoredField(fieldType().name()); } else { throw new MapperParsingException( "failed to parse field [" + fieldType().name() + "] of type [" + contentType() + "]", e ); } }); }	grr i forgot about the mappers that override parse when i added this to the base class. is it worth extracting the check to a common method so that it can be called from here?
public void testGetResultMappings_DependentVariableMappingIsPresent() { Map<String, Object> expectedTopClassesMapping = new HashMap<>() {{ put("type", "nested"); put("properties", new HashMap<>() {{ put("class_name", singletonMap("type", "dummy")); put("class_probability", singletonMap("type", "double")); put("class_score", singletonMap("type", "double")); }}); }}; FieldCapabilitiesResponse fieldCapabilitiesResponse = new FieldCapabilitiesResponse( new String[0], Collections.singletonMap("foo", Collections.singletonMap("dummy", createFieldCapabilities("foo", "dummy")))); Map<String, Object> resultMappings = new Classification("foo").getResultMappings("results", fieldCapabilitiesResponse); assertThat(resultMappings, hasEntry("results.foo_prediction", singletonMap("type", "dummy"))); assertThat(resultMappings, hasEntry("results.prediction_probability", singletonMap("type", "double"))); assertThat(resultMappings, hasEntry("results.prediction_score", singletonMap("type", "double"))); assertThat(resultMappings, hasEntry("results.is_training", singletonMap("type", "boolean"))); assertThat(resultMappings, hasEntry("results.top_classes", expectedTopClassesMapping)); assertThat(resultMappings, hasEntry("results.feature_importance", Classification.FEATURE_IMPORTANCE_MAPPING)); }	[optional] an alternative which i sometimes find more readable is: assertthat(resultmappings, allof(hasentry(...), ..., hasentry(...)));
public void testGetResultMappings() { Map<String, Object> resultMappings = new Regression("foo").getResultMappings("results", null); assertThat(resultMappings, hasEntry("results.foo_prediction", Collections.singletonMap("type", "double"))); assertThat(resultMappings, hasEntry("results.feature_importance", Regression.FEATURE_IMPORTANCE_MAPPING)); }	could you also add is_training here?
private boolean electPrimariesAndUnassignedDanglingReplicas(RoutingAllocation allocation) { boolean changed = false; final RoutingNodes routingNodes = allocation.routingNodes(); if (routingNodes.unassigned().getNumPrimaries() == 0) { // move out if we don't have unassigned primaries return changed; } // now, go over and elect a new primary if possible, not, from this code block on, if one is elected, // routingNodes.hasUnassignedPrimaries() will potentially be false for (ShardRouting shardEntry : routingNodes.unassigned()) { if (shardEntry.primary()) { // remove dangling replicas that are initializing for primary shards changed |= failReplicasForUnassignedPrimary(allocation, shardEntry); ShardRouting candidate = allocation.routingNodes().activeReplica(shardEntry); if (candidate != null) { IndexMetaData index = allocation.metaData().index(candidate.index()); routingNodes.swapPrimaryFlag(shardEntry, candidate); if (candidate.relocatingNodeId() != null) { changed = true; // its also relocating, make sure to move the other routing to primary RoutingNode node = routingNodes.node(candidate.relocatingNodeId()); if (node != null) { for (ShardRouting shardRouting : node) { if (shardRouting.shardId().equals(candidate.shardId()) && !shardRouting.primary()) { routingNodes.swapPrimaryFlag(shardRouting); break; } } } } if (IndexMetaData.isIndexUsingShadowReplicas(index.getSettings())) { routingNodes.reinitShadowPrimary(candidate); changed = true; } } } } return changed; }	thank you far factoring this into a function!
void sendFiles(Store store, StoreFileMetaData[] files, Function<StoreFileMetaData, OutputStream> outputStreamFactory) throws Throwable { store.incRef(); try { ArrayUtil.timSort(files, (a,b) -> Long.compare(a.length(), b.length())); // send smallest first for (int i = 0; i < files.length; i++) { final StoreFileMetaData md = files[i]; try (final IndexInput indexInput = store.directory().openInput(md.name(), IOContext.READONCE)) { // it's fine that we are only having the indexInput int he try/with block. The copy methods handles // exceptions during close correctly and doesn't hide the original exception. Streams.copy(new InputStreamIndexInput(indexInput, md.length()), outputStreamFactory.apply(md)); } catch (Throwable t) { final IOException corruptIndexException; if ((corruptIndexException = ExceptionsHelper.unwrapCorruption(t)) != null) { if (store.checkIntegrityNoException(md) == false) { // we are corrupted on the primary -- fail! logger.warn("{} Corrupted file detected {} checksum mismatch", shardId, md); failEngine(corruptIndexException); throw corruptIndexException; } else { // corruption has happened on the way to replica RemoteTransportException exception = new RemoteTransportException("File corruption occurred on recovery but checksums are ok", null); exception.addSuppressed(t); logger.warn("{} Remote file corruption on node {}, recovering {}. local checksum OK", corruptIndexException, shardId, request.targetNode(), md); throw exception; } } else { throw t; } } } } finally { store.decRef(); } }	out of curiosity (entirely out of the scope of this pr), do you think it would be worthwhile to implement the index priority logic here so higher priority indices get relocated before lower priority indices?
void sendFiles(Store store, StoreFileMetaData[] files, Function<StoreFileMetaData, OutputStream> outputStreamFactory) throws Throwable { store.incRef(); try { ArrayUtil.timSort(files, (a,b) -> Long.compare(a.length(), b.length())); // send smallest first for (int i = 0; i < files.length; i++) { final StoreFileMetaData md = files[i]; try (final IndexInput indexInput = store.directory().openInput(md.name(), IOContext.READONCE)) { // it's fine that we are only having the indexInput int he try/with block. The copy methods handles // exceptions during close correctly and doesn't hide the original exception. Streams.copy(new InputStreamIndexInput(indexInput, md.length()), outputStreamFactory.apply(md)); } catch (Throwable t) { final IOException corruptIndexException; if ((corruptIndexException = ExceptionsHelper.unwrapCorruption(t)) != null) { if (store.checkIntegrityNoException(md) == false) { // we are corrupted on the primary -- fail! logger.warn("{} Corrupted file detected {} checksum mismatch", shardId, md); failEngine(corruptIndexException); throw corruptIndexException; } else { // corruption has happened on the way to replica RemoteTransportException exception = new RemoteTransportException("File corruption occurred on recovery but checksums are ok", null); exception.addSuppressed(t); logger.warn("{} Remote file corruption on node {}, recovering {}. local checksum OK", corruptIndexException, shardId, request.targetNode(), md); throw exception; } } else { throw t; } } } } finally { store.decRef(); } }	minor nit: "int he" -> "in the"
public void testSingleNodesDoNotDiscoverEachOther() throws IOException, InterruptedException { final TransportService service = internalCluster().getInstance(TransportService.class); final int port = service.boundAddress().publishAddress().getPort(); final int upperPortRangeBound = port + 5 - 1; assumeTrue("port must be in range", upperPortRangeBound <= 65535); final NodeConfigurationSource configurationSource = new NodeConfigurationSource() { @Override public Settings nodeSettings(int nodeOrdinal) { return Settings .builder() .put("discovery.type", "single-node") .put("transport.type", getTestTransportType()) /* * We align the port ranges of the two as then with zen discovery these two * nodes would find each other. */ .put("transport.port", port + "-" + upperPortRangeBound) .build(); } @Override public Path nodeConfigPath(int nodeOrdinal) { return null; } }; try (InternalTestCluster other = new InternalTestCluster( randomLong(), createTempDir(), false, false, 1, 1, internalCluster().getClusterName(), configurationSource, 0, "other", Arrays.asList(getTestTransportPlugin(), MockHttpTransport.TestPlugin.class), Function.identity())) { other.beforeTest(random()); final ClusterState first = internalCluster().getInstance(ClusterService.class).state(); final ClusterState second = other.getInstance(ClusterService.class).state(); assertThat(first.nodes().getSize(), equalTo(1)); assertThat(second.nodes().getSize(), equalTo(1)); assertThat( first.nodes().getMasterNodeId(), not(equalTo(second.nodes().getMasterNodeId()))); assertThat( first.metaData().clusterUUID(), not(equalTo(second.metaData().clusterUUID()))); } }	this is a little strange, we just skip the test if we get unlucky with the randomly assigned port for the node?
public void writeTo(StreamOutput out) throws IOException { out.writeString(name); out.writeVLong(length); out.writeVLong(recovered); out.writeBoolean(reused); }	can we remove these methods? looks like there are not really necessary now
public synchronized void resetStopTime() { assert stopTime != 0: "expected to be stopped"; stopTime = 0; }	can we remove this method?
public void writeTo(StreamOutput out) throws IOException { out.writeString(name); out.writeVLong(length); out.writeVLong(recovered); out.writeBoolean(reused); }	with prewarming it is possible that a single file is recovered concurrently with multiple chunks. i think that file should allow to concurrently increment the recovered counter
private static void checkFilterOnAggs(LogicalPlan p, Set<Failure> localFailures, AttributeMap<Expression> attributeRefs) { if (p instanceof Filter) { Filter filter = (Filter) p; if (filter.anyMatch(Aggregate.class::isInstance) == false) { filter.condition().forEachDown(Expression.class, e -> { if (Functions.isAggregate(attributeRefs.resolve(e, e))) { if (filter.child() instanceof Project) { filter.condition().forEachDown(FieldAttribute.class, f -> localFailures.add(fail(e, "[{}] field must appear in the GROUP BY clause or in an aggregate function", Expressions.name(f))) ); } else { localFailures.add(fail(e, "Cannot use WHERE filtering on aggregate function [{}], use HAVING instead", Expressions.name(e))); } } }); } else { Set<Expression> unsupported = new LinkedHashSet<>(); filter.condition().forEachDown(Expression.class, e -> { Expression f = attributeRefs.resolve(e, e); if (f instanceof TopHits) { unsupported.add(f); } }); if (unsupported.isEmpty() == false) { String plural = unsupported.size() > 1 ? "s" : StringUtils.EMPTY; localFailures.add( fail(filter.condition(), "HAVING filter is unsupported for function" + plural + " {}", Expressions.names(unsupported))); } } } }	i have mixed feelings about the error message. for select * from (select first(salary) as f from test_emp) where f > 0 there is no having in the query, but the error message says there is one. this can be confusing to the user, who doesn't know that we transform the where into a having because this is how an aggregation is being filtered. how about filtering is unsupported for function [first(salary)] for that query?
static DeprecationIssue checkFrozenCacheLeniency(final Settings settings, final PluginsAndModules pluginsAndModules, final ClusterState clusterState, final XPackLicenseState licenseState) { final String cacheSizeSettingKey = "xpack.searchable.snapshot.shared_cache.size"; Setting<ByteSizeValue> cacheSizeSetting = Setting.byteSizeSetting(cacheSizeSettingKey, ByteSizeValue.ZERO); if (cacheSizeSetting.exists(settings)) { ByteSizeValue cacheSize = cacheSizeSetting.get(settings); if (cacheSize.getBytes() > 0) { final List<DiscoveryNodeRole> roles = NodeRoleSettings.NODE_ROLES_SETTING.get(settings); if (DataTier.isFrozenNode(new HashSet<>(roles)) == false) { String message = String.format(Locale.ROOT, "setting [%s] cannot be greater than zero on non-frozen nodes", cacheSizeSettingKey); String url = "https://www.elastic.co/guide/en/elasticsearch/reference/master/migrating-8.0.html#breaking_80_settings_changes"; String details = String.format(Locale.ROOT, "setting [%s] cannot be greater than zero on non-frozen nodes, and is " + "currently set to [%s]", cacheSizeSettingKey, settings.get(cacheSizeSettingKey)); return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, message, url, details, false, null); } } } return null; }	just double checking here because i know that -1 usually means "disabled": is this expected to be 0 or negative, or should it be always negative?
@Override public void lookupUser(String username, ActionListener<User> listener) { listener.onResponse(null); }	this method can be removed, it does not do anything else than creating x509authenticationtoken
static String getPrincipalFromSubjectDN(Pattern principalPattern, X509AuthenticationToken token, Logger logger) { String dn = token.credentials()[0].getSubjectX500Principal().toString(); Matcher matcher = principalPattern.matcher(dn); if (false == matcher.find()) { logger.debug((Supplier<?>) () -> new ParameterizedMessage("Could not extract principal from DN [{}] using pattern [{}]", dn, principalPattern.toString())); return null; } String principal = matcher.group(1); if (Strings.isNullOrEmpty(principal)) { logger.debug((Supplier<?>) () -> new ParameterizedMessage("The extracted principal from DN [{}] using pattern [{}] is empty", dn, principalPattern.toString())); return null; } return principal; }	i think our logger messages start with lowercase, i see it in other parts of the code so my assumption.
protected void masterOperation(Task task, PutPipelineRequest request, ClusterState state, ActionListener<AcknowledgedResponse> listener) throws Exception { Map<String, Object> pipelineConfig = null; IngestMetadata currentIngestMetadata = state.metadata().custom(IngestMetadata.TYPE); if (currentIngestMetadata != null && currentIngestMetadata.getPipelines().containsKey(request.getId())) { pipelineConfig = XContentHelper.convertToMap(request.getSource(), false, request.getXContentType()).v2(); var currentPipeline = currentIngestMetadata.getPipelines().get(request.getId()); if (currentPipeline.getConfigAsMap().equals(pipelineConfig)) { // existing pipeline matches request pipeline -- no need to update listener.onResponse(AcknowledgedResponse.TRUE); return; } } if (state.getNodes().getMinNodeVersion().before(Version.V_7_15_0)) { pipelineConfig = pipelineConfig == null ? XContentHelper.convertToMap(request.getSource(), false, request.getXContentType()).v2() : pipelineConfig; if (pipelineConfig.containsKey(Pipeline.META_KEY)) { throw new IllegalStateException("pipelines with _meta field require minimum node version of " + Version.V_7_15_0); } } NodesInfoRequest nodesInfoRequest = new NodesInfoRequest(); nodesInfoRequest.clear().addMetric(NodesInfoRequest.Metric.INGEST.metricName()); client.admin().cluster().nodesInfo( nodesInfoRequest, ActionListener.wrap( nodeInfos -> { Map<DiscoveryNode, IngestInfo> ingestInfos = new HashMap<>(); for (NodeInfo nodeInfo : nodeInfos.getNodes()) { ingestInfos.put(nodeInfo.getNode(), nodeInfo.getInfo(IngestInfo.class)); } ingestService.putPipeline(ingestInfos, request, listener); }, listener::onFailure) ); }	i was wondering whether maps.deepequals(...) be used here? but i don't think that is the case since we never have arrays as value in this map. so this should be good.
private CreateIndexClusterStateUpdateRequest buildUpdateRequest(String indexName) { boolean isSystemIndex = false; String mappings = null; Settings settings = null; String aliasName = null; String concreteIndexName = indexName; final SystemIndexDescriptor descriptor = systemIndices.findMatchingDescriptor(indexName); if (descriptor != null && descriptor.isAutomaticallyManaged()) { isSystemIndex = true; mappings = descriptor.getMappings(); settings = descriptor.getSettings(); aliasName = descriptor.getAliasName(); concreteIndexName = descriptor.getPrimaryIndex(); } CreateIndexClusterStateUpdateRequest updateRequest = new CreateIndexClusterStateUpdateRequest(request.cause(), concreteIndexName, request.index()) .ackTimeout(request.timeout()).masterNodeTimeout(request.masterNodeTimeout()); if (isSystemIndex) { updateRequest.waitForActiveShards(ActiveShardCount.ALL); } if (mappings != null) { updateRequest.mappings(mappings); } if (settings != null) { updateRequest.settings(settings); } if (aliasName != null) { updateRequest.aliases(Set.of(new Alias(aliasName))); } if (isSystemIndex) { logger.info("Auto-creating system index {}", concreteIndexName); } else { logger.debug("Auto-creating index {}", concreteIndexName); } return updateRequest; }	can we simplify this method a bit? we don't really share logic between non-system index creation and system index creation as far as i can see so i think we can test for the non system case and simply return the update request there?
@Override protected void masterOperation(Task task, final CreateIndexRequest request, final ClusterState state, final ActionListener<CreateIndexResponse> listener) { String cause = request.cause(); if (cause.length() == 0) { cause = "api"; } final String indexName = indexNameExpressionResolver.resolveDateMathExpression(request.index()); String mappings = request.mappings(); Settings settings = request.settings(); Set<Alias> aliases = request.aliases(); String concreteIndexName = indexName; boolean isSystemIndex = false; SystemIndexDescriptor descriptor = systemIndices.findMatchingDescriptor(indexName); if (descriptor != null && descriptor.isAutomaticallyManaged()) { isSystemIndex = true; // System indices define their own settings and mappings, which cannot be overridden. mappings = descriptor.getMappings(); settings = descriptor.getSettings(); concreteIndexName = descriptor.getPrimaryIndex(); if (descriptor.getAliasName() == null) { aliases = Set.of(); } else { aliases = Set.of(new Alias(descriptor.getAliasName())); } } final CreateIndexClusterStateUpdateRequest updateRequest = new CreateIndexClusterStateUpdateRequest(cause, concreteIndexName, request.index()) .ackTimeout(request.timeout()).masterNodeTimeout(request.masterNodeTimeout()) .aliases(aliases) .waitForActiveShards(request.waitForActiveShards()); if (isSystemIndex) { updateRequest.waitForActiveShards(ActiveShardCount.ALL); } if (mappings != null) { updateRequest.mappings(mappings); } if (settings != null) { updateRequest.settings(settings); } createIndexService.createIndex(updateRequest, ActionListener.map(listener, response -> new CreateIndexResponse(response.isAcknowledged(), response.isShardsAcknowledged(), indexName))); }	similar to the comment in autocreateaction, can we break up the system and non-system case since we don't really need to share anything between the two cases?
public void addField(ParseContext.Document document, String name, String input, int weight, Map<String, Set<CharSequence>> contexts) { document.add(new TypedContextField(name, input, weight, contexts, document)); }	do we still need to have deprecation_logger variable in this module?
public AnalysisService build(IndexSettings indexSettings) throws IOException { final Map<String, Settings> charFiltersSettings = indexSettings.getSettings().getGroups(INDEX_ANALYSIS_CHAR_FILTER); final Map<String, Settings> tokenFiltersSettings = indexSettings.getSettings().getGroups(INDEX_ANALYSIS_FILTER); final Map<String, Settings> tokenizersSettings = indexSettings.getSettings().getGroups(INDEX_ANALYSIS_TOKENIZER); final Map<String, Settings> analyzersSettings = indexSettings.getSettings().getGroups("index.analysis.analyzer"); final Map<String, CharFilterFactory> charFilterFactories = buildMapping(false, "charfilter", indexSettings, charFiltersSettings, charFilters, prebuiltAnalysis.charFilterFactories); final Map<String, TokenizerFactory> tokenizerFactories = buildMapping(false, "tokenizer", indexSettings, tokenizersSettings, tokenizers, prebuiltAnalysis.tokenizerFactories); Map<String, AnalysisModule.AnalysisProvider<TokenFilterFactory>> tokenFilters = new HashMap<>(this.tokenFilters); /* * synonym is different than everything else since it needs access to the tokenizer factories for this index. * instead of building the infrastructure for plugins we rather make it a real exception to not pollute the general interface and * hide internal data-structures as much as possible. */ tokenFilters.put("synonym", requriesAnalysisSettings((is, env, name, settings) -> new SynonymTokenFilterFactory(is, env, this, name, settings))); final Map<String, TokenFilterFactory> tokenFilterFactories = buildMapping(false, "tokenfilter", indexSettings, tokenFiltersSettings, Collections.unmodifiableMap(tokenFilters), prebuiltAnalysis.tokenFilterFactories); final Map<String, AnalyzerProvider<?>> analyzierFactories = buildMapping(true, "analyzer", indexSettings, analyzersSettings, analyzers, prebuiltAnalysis.analyzerProviderFactories); return new AnalysisService(indexSettings, analyzierFactories, tokenizerFactories, charFilterFactories, tokenFilterFactories); }	i think it'd be nice to have javadoc for these so it is clear to the casual observer that they are for building the provider in the context of some index's settings.
@Override protected Pivot replaceChild(LogicalPlan newChild) { java.util.function.Function<Expression, Expression> withQualifierNull = (Expression e)-> { if (e instanceof Attribute && newChild instanceof EsRelation) { Attribute fa = (Attribute) e; return fa.withQualifier(null); } return e; }; Expression newColumn = column.transformUp(withQualifierNull); List<NamedExpression> newAggregates = aggregates.stream().map((NamedExpression aggregate)-> (NamedExpression) aggregate.transformUp(withQualifierNull) ).collect(Collectors.toUnmodifiableList()); return new Pivot(source(), newChild, newColumn, values, newAggregates); }	could you please add some comment on why this is needed?
@Override protected Pivot replaceChild(LogicalPlan newChild) { java.util.function.Function<Expression, Expression> withQualifierNull = (Expression e)-> { if (e instanceof Attribute && newChild instanceof EsRelation) { Attribute fa = (Attribute) e; return fa.withQualifier(null); } return e; }; Expression newColumn = column.transformUp(withQualifierNull); List<NamedExpression> newAggregates = aggregates.stream().map((NamedExpression aggregate)-> (NamedExpression) aggregate.transformUp(withQualifierNull) ).collect(Collectors.toUnmodifiableList()); return new Pivot(source(), newChild, newColumn, values, newAggregates); }	i would extract this to a private static attribute.
public void testForgetFollower() throws IOException { final String forgetLeader = "forget-leader"; final String forgetFollower = "forget-follower"; if ("leader".equals(targetCluster)) { logger.info("running against leader cluster"); final Settings indexSettings = Settings.builder() .put("index.number_of_replicas", 0) .put("index.number_of_shards", 1) .put("index.soft_deletes.enabled", true) .build(); createIndex(forgetLeader, indexSettings); } else { logger.info("running against follower cluster"); followIndex(client(), "leader_cluster", forgetLeader, forgetFollower); final Response response = client().performRequest(new Request("GET", "/" + forgetFollower + "/_stats")); final String followerIndexUUID = ObjectPath.createFromResponse(response).evaluate("indices." + forgetFollower + ".uuid"); assertOK(client().performRequest(new Request("POST", "/" + forgetFollower + "/_ccr/pause_follow"))); try (RestClient leaderClient = buildLeaderClient(restClientSettings())) { final Request request = new Request("POST", "/" + forgetLeader + "/_ccr/forget_follower"); final String requestBody = "{" + "\\\\"follower_cluster\\\\":\\\\"follow-cluster\\\\"," + "\\\\"follower_index\\\\":\\\\"" + forgetFollower + "\\\\"," + "\\\\"follower_index_uuid\\\\":\\\\"" + followerIndexUUID + "\\\\"," + "\\\\"leader_remote_cluster\\\\":\\\\"leader_cluster\\\\"" + "}"; request.setJsonEntity(requestBody); assertOK(leaderClient.performRequest(request)); final Request retentionLeasesRequest = new Request("GET", "/" + forgetLeader + "/_stats"); retentionLeasesRequest.addParameter("level", "shards"); final Response retentionLeasesResponse = leaderClient.performRequest(retentionLeasesRequest); final ArrayList<Object> shardsStats = ObjectPath.createFromResponse(retentionLeasesResponse).evaluate("indices." + forgetLeader + ".shards.0"); assertThat(shardsStats, hasSize(1)); @SuppressWarnings("unchecked") final Map<String, Object> shardStatsAsMap = (Map<String, Object>)shardsStats.get(0); @SuppressWarnings("unchecked") final Map<String, Object> retentionLeasesStats = (Map<String, Object>) shardStatsAsMap.get("retention_leases"); @SuppressWarnings("unchecked") final ArrayList<Object> leases = (ArrayList<Object>)retentionLeasesStats.get("leases"); assertThat(leases, empty()); } } }	also writing the following code as: java final map<?, ?> shardstatsasmap = (map<?, ?>) shardsstats.get(0); final map<?, ?> retentionleasesstats = (map<?, ?>) shardstatsasmap.get("retention_leases"); final list<?> leases = (list<?>) retentionleasesstats.get("leases"); is easier to read.
private static long[] extractIds(List<SearchHit> events) { final int len = events.size(); final long ids[] = new long[len]; for (int i = 0; i < events.size(); i++) { ids[i] = ((Number) events.get(i).getSourceAsMap().get("serial_event_id")).longValue(); } return ids; }	len defined above, but not actually used here.
@Override protected int doHashCode() { String timeZoneId = timeZone == null ? null : timeZone.getId(); return Objects.hash(fieldName, from, to, timeZoneId, includeLower, includeUpper, format); }	is this needed ? zoneid implements hashcode and equals.
@Override protected boolean doEquals(RangeQueryBuilder other) { String timeZoneId = timeZone == null ? null : timeZone.getId(); return Objects.equals(fieldName, other.fieldName) && Objects.equals(from, other.from) && Objects.equals(to, other.to) && Objects.equals(timeZoneId, other.timeZone()) && Objects.equals(includeLower, other.includeLower) && Objects.equals(includeUpper, other.includeUpper) && Objects.equals(format, other.format); }	same here, why can't you use the timezone directly ?
public void validateIndexSettings(String indexName, Settings settings) throws ElasticsearchException { String customPath = settings.get(IndexMetaData.SETTING_DATA_PATH, null); if (customPath != null && nodeEnv.isCustomPathsEnabled() == false) { throw new IndexCreationException(new Index(indexName), new ElasticsearchIllegalArgumentException("custom data_paths for indices is disabled")); } Integer number_of_primaries = settings.getAsInt(IndexMetaData.SETTING_NUMBER_OF_SHARDS, null); Integer number_of_replicas = settings.getAsInt(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, null); List<String> validationErrors = Lists.newArrayList(); if (number_of_primaries != null && number_of_primaries <= 0) { validationErrors.add("index must have 1 or more primary shards"); } if (number_of_replicas != null && number_of_replicas < 0) { validationErrors.add("index must have 0 or more replica shards"); } if (validationErrors.isEmpty() == false) { throw new ElasticsearchIllegalArgumentException(getMessage(validationErrors)); } }	would it make sense to include this error into the validationerrors list as well?
public static String getContainerName() { /* Have a different name per test so that there is no possible race condition. As the long can be negative, * there mustn't be an hyphen between the 2 concat numbers * (can't have 2 consecutives hypens on Azure containers) */ String testName = "snapshot-itest-" .concat(RandomizedTest.getContext().getRunnerSeedAsString().toLowerCase(Locale.ROOT)) .concat(new Long(RandomizedTest.getContext().getRandom().nextLong()).toString()); return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName; }	this can simply be randomlong(). however, i don't think this will work because the cleanup in wipeazurerepositories() would then not know about the generated name. if these need to be unique, then this method needs to stash the generated name so that the repository can be cleaned up after the test.
@Override public String executor() { return ThreadPool.Names.SAME; } } static final class HandshakeRequest extends TransportRequest { private final Version version; HandshakeRequest(Version version) { this.version = version; } HandshakeRequest(StreamInput streamInput) throws IOException { super(streamInput); BytesReference remainingMessage; try { remainingMessage = streamInput.readBytesReference(); } catch (EOFException e) { remainingMessage = null; } if (remainingMessage == null) { version = null; } else { try (StreamInput messageStreamInput = remainingMessage.streamInput()) { this.version = Version.readVersion(messageStreamInput); } } } @Override public void readFrom(StreamInput in) { throw new UnsupportedOperationException("usage of Streamable is to be replaced by Writeable"); } @Override public void writeTo(StreamOutput streamOutput) throws IOException { super.writeTo(streamOutput); assert version != null; try (BytesStreamOutput messageStreamOutput = new BytesStreamOutput(4)) { Version.writeVersion(version, messageStreamOutput); BytesReference reference = messageStreamOutput.bytes(); streamOutput.writeBytesReference(reference); } } } static final class HandshakeResponse extends TransportResponse { private final Version responseVersion; HandshakeResponse(Version responseVersion) { this.responseVersion = responseVersion; } private HandshakeResponse(StreamInput in) throws IOException { super.readFrom(in); responseVersion = Version.readVersion(in); } @Override public void readFrom(StreamInput in) { throw new UnsupportedOperationException("usage of Streamable is to be replaced by Writeable"); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); assert responseVersion != null; Version.writeVersion(responseVersion, out); } Version getResponseVersion() { return responseVersion; } } @FunctionalInterface interface HandshakeRequestSender { void sendRequest(DiscoveryNode node, TcpChannel channel, long requestId, Version version) throws IOException; }	if the plan is to replace internal:transport/handshake with this message, does it also need the clustername? today we rely on the fact that the transport handshake validates that the cluster names match during discovery - i think that we don't otherwise verify that we're talking to nodes from the right cluster.
private void createFollowerIndex( final IndexMetaData leaderIndexMetaData, final String[] historyUUIDs, final PutFollowAction.Request request, final ActionListener<PutFollowAction.Response> listener) { if (leaderIndexMetaData == null) { listener.onFailure(new IllegalArgumentException("leader index [" + request.getLeaderIndex() + "] does not exist")); return; } if (leaderIndexMetaData.getSettings().getAsBoolean(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), false) == false) { listener.onFailure( new IllegalArgumentException("leader index [" + request.getLeaderIndex() + "] does not have soft deletes enabled")); return; } String remoteCluster = request.getRemoteCluster(); ActionListener<RestoreSnapshotResponse> restoreCompleteHandler = new ActionListener<RestoreSnapshotResponse>() { @Override public void onResponse(RestoreSnapshotResponse restoreSnapshotResponse) { RestoreInfo restoreInfo = restoreSnapshotResponse.getRestoreInfo(); if (restoreInfo.failedShards() == 0) { initiateFollowing(request, listener); } else { listener.onFailure(new ElasticsearchException("failed to restore [" + restoreInfo.failedShards() + "] shards")); } } @Override public void onFailure(Exception e) { listener.onFailure(e); } }; client.admin().cluster().preparePutRepository(remoteCluster).setType(RemoteClusterRepository.TYPE) .execute(new ActionListener<AcknowledgedResponse>() { @Override public void onResponse(AcknowledgedResponse acknowledgedResponse) { if (acknowledgedResponse.isAcknowledged()) { Settings.Builder settingsBuilder = Settings.builder() .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true) // TODO: Figure out what to do with private setting SETTING_INDEX_PROVIDED_NAME .put(IndexMetaData.SETTING_INDEX_PROVIDED_NAME, request.getFollowRequest().getFollowerIndex()) // Overwriting UUID here, because otherwise we can't follow indices in the same cluster .put(IndexMetaData.SETTING_INDEX_UUID, UUIDs.randomBase64UUID()) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true); RestoreService.RestoreRequest restoreRequest = new RestoreService.RestoreRequest(remoteCluster, leaderIndexMetaData.getIndex().getName(), new String[]{request.getLeaderIndex()}, request.indicesOptions(), "^(.*)$", request.getFollowRequest().getFollowerIndex(), Settings.EMPTY, request.masterNodeTimeout(), false, false, true, settingsBuilder.build(), new String[0], "restore_snapshot[" + remoteCluster + "]"); initiateRestore(restoreRequest, restoreCompleteHandler); } else { listener.onFailure(new ElasticsearchException("remote cluster repository put not acknowledged")); } } @Override public void onFailure(Exception e) { listener.onFailure(e); } }); }	is this necessary? afaics restoreservice is already generating a fresh uuid (the only exception being if there is an existing closed index).
private void createFollowerIndex( final IndexMetaData leaderIndexMetaData, final String[] historyUUIDs, final PutFollowAction.Request request, final ActionListener<PutFollowAction.Response> listener) { if (leaderIndexMetaData == null) { listener.onFailure(new IllegalArgumentException("leader index [" + request.getLeaderIndex() + "] does not exist")); return; } if (leaderIndexMetaData.getSettings().getAsBoolean(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), false) == false) { listener.onFailure( new IllegalArgumentException("leader index [" + request.getLeaderIndex() + "] does not have soft deletes enabled")); return; } String remoteCluster = request.getRemoteCluster(); ActionListener<RestoreSnapshotResponse> restoreCompleteHandler = new ActionListener<RestoreSnapshotResponse>() { @Override public void onResponse(RestoreSnapshotResponse restoreSnapshotResponse) { RestoreInfo restoreInfo = restoreSnapshotResponse.getRestoreInfo(); if (restoreInfo.failedShards() == 0) { initiateFollowing(request, listener); } else { listener.onFailure(new ElasticsearchException("failed to restore [" + restoreInfo.failedShards() + "] shards")); } } @Override public void onFailure(Exception e) { listener.onFailure(e); } }; client.admin().cluster().preparePutRepository(remoteCluster).setType(RemoteClusterRepository.TYPE) .execute(new ActionListener<AcknowledgedResponse>() { @Override public void onResponse(AcknowledgedResponse acknowledgedResponse) { if (acknowledgedResponse.isAcknowledged()) { Settings.Builder settingsBuilder = Settings.builder() .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true) // TODO: Figure out what to do with private setting SETTING_INDEX_PROVIDED_NAME .put(IndexMetaData.SETTING_INDEX_PROVIDED_NAME, request.getFollowRequest().getFollowerIndex()) // Overwriting UUID here, because otherwise we can't follow indices in the same cluster .put(IndexMetaData.SETTING_INDEX_UUID, UUIDs.randomBase64UUID()) .put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true); RestoreService.RestoreRequest restoreRequest = new RestoreService.RestoreRequest(remoteCluster, leaderIndexMetaData.getIndex().getName(), new String[]{request.getLeaderIndex()}, request.indicesOptions(), "^(.*)$", request.getFollowRequest().getFollowerIndex(), Settings.EMPTY, request.masterNodeTimeout(), false, false, true, settingsBuilder.build(), new String[0], "restore_snapshot[" + remoteCluster + "]"); initiateRestore(restoreRequest, restoreCompleteHandler); } else { listener.onFailure(new ElasticsearchException("remote cluster repository put not acknowledged")); } } @Override public void onFailure(Exception e) { listener.onFailure(e); } }); }	this block of code is copy-pasted from transportrestoresnapshotaction. do we need this here? can we possibly share code?
public void testRangeQuery() { KeyedJsonFieldType ft = createDefaultFieldType(); ft.setName("field"); TermRangeQuery expected = new TermRangeQuery("field", new BytesRef("key\\\\0lower"), new BytesRef("key\\\\0upper"), false, false); assertEquals(expected, ft.rangeQuery("lower", "upper", false, false, null)); IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> ft.rangeQuery("lower", null, false, false, null)); assertEquals("[range] queries on keyed [json] fields must include both an upper and a lower bound.", e.getMessage()); e = expectThrows(IllegalArgumentException.class, () -> ft.rangeQuery(null, "upper", false, false, null)); assertEquals("[range] queries on keyed [json] fields must include both an upper and a lower bound.", e.getMessage()); }	should we also check that we do the right thing if the bounds are inclusive?
private synchronized final void closeWithTragicEvent(Throwable throwable) throws IOException { if (tragedy == null) { tragedy = throwable; } else { tragedy.addSuppressed(throwable); } close(); }	paranoia: can we assert throwable != null here?
protected Exception checkFeatureAvailable(RestRequest request) { Exception failedFeature = super.checkFeatureAvailable(request); if (failedFeature != null) { return failedFeature; } else if (XPackSettings.ENROLLMENT_ENABLED.get(settings) == false) { return new ElasticsearchSecurityException("Enrollment mode [" + XPackSettings.ENROLLMENT_ENABLED.getKey() + "] is not " + "configured", RestStatus.FORBIDDEN); } else { return null; } }	nit again: i find "not enabled" to be less confusing than "not configured" but i'll leave this up to you. ( it can "be configured" ==> explicitly set to false and we'd need to throw here.
protected ScriptTemplate scriptWithAggregate(AggregateFunction aggregate) { String template = basicTemplate(aggregate); return new ScriptTemplate(processScript(template), paramsBuilder().agg(aggregate).build(), dataType()); }	shouldn't we remove this method as currently there is no such case that a groupingfunction (actually histogram) results into a script?
@Override public String toString() { return "local shards recovery"; } } /** * recovery from a snapshot */ public static class SnapshotRecoverySource extends RecoverySource { private final String restoreUUID; private final Snapshot snapshot; private final String index; @Nullable private final IndexId indexId; private final Version version; public SnapshotRecoverySource(String restoreUUID, Snapshot snapshot, Version version, IndexId indexId) { this.restoreUUID = restoreUUID; this.snapshot = Objects.requireNonNull(snapshot); this.version = Objects.requireNonNull(version); this.indexId = Objects.requireNonNull(indexId); this.index = indexId.getName(); } SnapshotRecoverySource(StreamInput in) throws IOException { restoreUUID = in.readString(); snapshot = new Snapshot(in); version = Version.readVersion(in); index = in.readString(); if (in.getVersion().onOrAfter(Version.V_8_0_0)) { final String indexUUID = in.readOptionalString(); if (indexUUID == null) { indexId = null; } else { indexId = new IndexId(index, indexUUID); } } else { indexId = null; } } /** * Gets the {@link IndexId} of the recovery source. * * @return IndexId or {@code null} if running against old version master that did not add an IndexId to the recovery source */ @Nullable public IndexId indexId() { return indexId; } public String restoreUUID() { return restoreUUID; } public Snapshot snapshot() { return snapshot; } public String index() { return index; } public Version version() { return version; } @Override protected void writeAdditionalFields(StreamOutput out) throws IOException { out.writeString(restoreUUID); snapshot.writeTo(out); Version.writeVersion(version, out); out.writeString(index); if (out.getVersion().onOrAfter(Version.V_8_0_0)) { out.writeOptionalString(indexId == null ? null : indexId.getId()); } } @Override public Type getType() { return Type.SNAPSHOT; } @Override public void addAdditionalFields(XContentBuilder builder, ToXContent.Params params) throws IOException { builder.field("repository", snapshot.getRepository()) .field("snapshot", snapshot.getSnapshotId().getName()) .field("version", version.toString()) .field("index", index) .field("restoreUUID", restoreUUID); } @Override public String toString() { return "snapshot recovery [" + restoreUUID + "] from " + snapshot; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } SnapshotRecoverySource that = (SnapshotRecoverySource) o; return restoreUUID.equals(that.restoreUUID) && snapshot.equals(that.snapshot) && index.equals(that.index) && version.equals(that.version) && Objects.equals(indexId, that.indexId); } @Override public int hashCode() { return Objects.hash(restoreUUID, snapshot, index, version, indexId); } } /** * peer recovery from a primary shard */ public static class PeerRecoverySource extends RecoverySource { public static final PeerRecoverySource INSTANCE = new PeerRecoverySource(); private PeerRecoverySource() { } @Override public Type getType() { return Type.PEER; } @Override public String toString() { return "peer recovery"; } @Override public boolean expectEmptyRetentionLeases() { return false; }	the approach here does not allow us to later switch to just reading directly indexid from stream after backport to 7.x. can you change things so that you conditionally do in.readstring(); or new indexid(in)
@Override protected void writeAdditionalFields(StreamOutput out) throws IOException { out.writeString(restoreUUID); snapshot.writeTo(out); Version.writeVersion(version, out); out.writeString(index); if (out.getVersion().onOrAfter(Version.V_8_0_0)) { out.writeOptionalString(indexId == null ? null : indexId.getId()); } }	can we avoid sending this field if indexid != null (i.e. in the non-bwc case)
private static String keyUsageDescription(X509Certificate certificate) { boolean[] keyUsage = certificate.getKeyUsage(); if (keyUsage == null || keyUsage.length == 0) { return "no keyUsage"; } final String[] keyUsageGlossary = {"digitalSignature", "nonRepudiation", "keyEncipherment", "dataEncipherment", "keyAgreement", "keyCertSign", "cRLSign", "encipherOnly", "decipherOnly"}; List<String> keyUsageDescription = new ArrayList<>(); IntStream.range(0, keyUsage.length).forEach(i -> { if (keyUsage[i]) { keyUsageDescription.add(keyUsageGlossary[i]); } }); return keyUsageDescription.stream() .reduce((a, b) -> a + ", " + b) .map(str -> "keyUsage [" + str + "]") .orElse("no keyUsage"); }	for safety we should check that i < keyusageglossary.length
private static String keyUsageDescription(X509Certificate certificate) { boolean[] keyUsage = certificate.getKeyUsage(); if (keyUsage == null || keyUsage.length == 0) { return "no keyUsage"; } final String[] keyUsageGlossary = {"digitalSignature", "nonRepudiation", "keyEncipherment", "dataEncipherment", "keyAgreement", "keyCertSign", "cRLSign", "encipherOnly", "decipherOnly"}; List<String> keyUsageDescription = new ArrayList<>(); IntStream.range(0, keyUsage.length).forEach(i -> { if (keyUsage[i]) { keyUsageDescription.add(keyUsageGlossary[i]); } }); return keyUsageDescription.stream() .reduce((a, b) -> a + ", " + b) .map(str -> "keyUsage [" + str + "]") .orElse("no keyUsage"); }	the style use have used here is fine, but i would have written it like this: suggestion string keyusagedescription = intstream.range(0, keyusage.length) .filter(i -> keyusage[i]) .map(i -> i < keyusageglossary.length ? keyusageglossary[i] : "#" + i) .collect(collectors.joining(", ")); and then tested whether that was empty or not.
private static String keyUsageDescription(X509Certificate certificate) { boolean[] keyUsage = certificate.getKeyUsage(); if (keyUsage == null || keyUsage.length == 0) { return "no keyUsage"; } final String[] keyUsageGlossary = {"digitalSignature", "nonRepudiation", "keyEncipherment", "dataEncipherment", "keyAgreement", "keyCertSign", "cRLSign", "encipherOnly", "decipherOnly"}; List<String> keyUsageDescription = new ArrayList<>(); IntStream.range(0, keyUsage.length).forEach(i -> { if (keyUsage[i]) { keyUsageDescription.add(keyUsageGlossary[i]); } }); return keyUsageDescription.stream() .reduce((a, b) -> a + ", " + b) .map(str -> "keyUsage [" + str + "]") .orElse("no keyUsage"); }	you have this orelse case both here, and in generateextendedkeyusagedescription and i don't think that's necessary.
private static String keyUsageDescription(X509Certificate certificate) { boolean[] keyUsage = certificate.getKeyUsage(); if (keyUsage == null || keyUsage.length == 0) { return "no keyUsage"; } final String[] keyUsageGlossary = {"digitalSignature", "nonRepudiation", "keyEncipherment", "dataEncipherment", "keyAgreement", "keyCertSign", "cRLSign", "encipherOnly", "decipherOnly"}; List<String> keyUsageDescription = new ArrayList<>(); IntStream.range(0, keyUsage.length).forEach(i -> { if (keyUsage[i]) { keyUsageDescription.add(keyUsageGlossary[i]); } }); return keyUsageDescription.stream() .reduce((a, b) -> a + ", " + b) .map(str -> "keyUsage [" + str + "]") .orElse("no keyUsage"); }	should i use an enum here too like i've done with extendedkeyusage, for uniformity? i could define the index as the enum field.
@Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException { final String[] indices = Strings.splitStringByCommaToArray(request.param("index")); final String[] fields = Strings.splitStringByCommaToArray(request.param("fields")); GetFieldMappingsRequest getMappingsRequest = new GetFieldMappingsRequest(); getMappingsRequest.indices(indices).fields(fields).includeDefaults(request.paramAsBoolean("include_defaults", false)); getMappingsRequest.indicesOptions(IndicesOptions.fromRequest(request, getMappingsRequest.indicesOptions())); if (request.hasParam("local")) { deprecationLogger.deprecatedAndMaybeLog("get_field_mapping_local", "Use [local] in get field mapping requests is deprecated." + "The parameter will be removed in the next major version"); } getMappingsRequest.local(request.paramAsBoolean("local", getMappingsRequest.local())); return channel -> client.admin().indices().getFieldMappings(getMappingsRequest, new RestBuilderListener<>(channel) { @Override public RestResponse buildResponse(GetFieldMappingsResponse response, XContentBuilder builder) throws Exception { Map<String, Map<String, FieldMappingMetadata>> mappingsByIndex = response.mappings(); RestStatus status = OK; if (mappingsByIndex.isEmpty() && fields.length > 0) { status = NOT_FOUND; } response.toXContent(builder, request); return new BytesRestResponse(status, builder); } }); }	maybe add a space between the two sentences?
public void onInit(List<? extends IndexCommit> commits) throws IOException { switch (openMode) { case CREATE_INDEX_AND_TRANSLOG: assert commits.isEmpty() : "index is created, but we have commits"; break; case OPEN_INDEX_CREATE_TRANSLOG: assert commits.isEmpty() == false : "index is opened, but we have no commits"; // When an engine starts with OPEN_INDEX_CREATE_TRANSLOG, a new fresh index commit will be created immediately. // We therefore can simply skip processing here as `onCommit` will be called right after with a new commit. break; case OPEN_INDEX_AND_TRANSLOG: assert commits.isEmpty() == false : "index is opened, but we have no commits"; if (startingIndexCommit == null) { onCommit(commits); } else { assert commits.contains(startingIndexCommit) : "Existing commits must contain the starting commit; " + "startingCommit [" + startingIndexCommit + "], commits [" + commits + "]"; commits.stream().filter(commit -> startingIndexCommit.equals(commit) == false).forEach(IndexCommit::delete); updateTranslogDeletionPolicy(startingIndexCommit, startingIndexCommit); } break; default: throw new IllegalArgumentException("unknown openMode [" + openMode + "]"); } }	why do we need special handling here and need the start commit point? can you explain?
private void updateTranslogDeletionPolicy(final IndexCommit minRequiredCommit, final IndexCommit lastCommit) throws IOException { assert minRequiredCommit.isDeleted() == false : "The minimum required commit must not be deleted"; final long minRequiredGen = Long.parseLong(minRequiredCommit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY)); assert lastCommit.isDeleted() == false : "The last commit must not be deleted"; final long lastGen = Long.parseLong(lastCommit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY)); assert minRequiredGen <= lastGen : "minRequiredGen must not be greater than lastGen"; translogDeletionPolicy.setTranslogGenerationOfLastCommit(lastGen); translogDeletionPolicy.setMinTranslogGenerationForRecovery(minRequiredGen); } /** * Selects a starting commit point from a list of existing commits based on the persisted global checkpoint from translog * and the retained translog generations. All the required translog files of a starting commit point must exist, * and its max seqno should be at most the global checkpoint from the translog checkpoint. * * @param commits a list of existing commit points * @param globalCheckpoint the persisted global checkpoint from the translog, see {@link Translog#readGlobalCheckpoint(Path)} * @param minRetainedTranslogGen the minimum translog generation is retained, see {@link Translog#readMinReferencedTranslogGen(Path)}	why do we do this? isn't the logic in indexofkeptcommits enough to deal with this?
private EngineConfig newEngineConfig(EngineConfig.OpenMode openMode) throws IOException { Sort indexSort = indexSortSupplier.get(); final boolean forceNewHistoryUUID; final RecoverySource.Type recoveryType = shardRouting.recoverySource().getType(); switch (recoveryType) { case EXISTING_STORE: case PEER: forceNewHistoryUUID = false; break; case EMPTY_STORE: case SNAPSHOT: case LOCAL_SHARDS: forceNewHistoryUUID = true; break; default: throw new AssertionError("unknown recovery type: [" + recoveryType + "]"); } final IndexCommit startingCommit; if (recoveryType == RecoverySource.Type.EXISTING_STORE) { startingCommit = CombinedDeletionPolicy.startingCommitPoint(DirectoryReader.listCommits(store.directory()), Translog.readGlobalCheckpoint(translogConfig.getTranslogPath()), Translog.readMinReferencedTranslogGen(translogConfig.getTranslogPath())); } else { startingCommit = null; } return new EngineConfig(openMode, shardId, shardRouting.allocationId().getId(), threadPool, indexSettings, warmer, store, indexSettings.getMergePolicy(), mapperService.indexAnalyzer(), similarityService.similarity(mapperService), codecService, shardEventListener, indexCache.query(), cachingPolicy, forceNewHistoryUUID, translogConfig, IndexingMemoryController.SHARD_INACTIVE_TIME_SETTING.get(indexSettings.getSettings()), Collections.singletonList(refreshListeners), Collections.singletonList(new RefreshMetricUpdater(refreshMetric)), indexSort, this::runTranslogRecovery, circuitBreakerService, startingCommit); }	why does this needs to be out of engine? can't we do it in the engine construct as an invariant when opening an index (and translog)
@Override protected QueryBuilder doRewrite(QueryRewriteContext queryRewriteContext) throws IOException { SearchExecutionContext sec = queryRewriteContext.convertToSearchExecutionContext(); if (sec == null) { return this; } // If we're using the default keyword analyzer then we can rewrite this to a TermQueryBuilder // and possibly shortcut if (analyzer != null) { if (sec.getIndexAnalyzers().get(analyzer) == Lucene.KEYWORD_ANALYZER) { TermQueryBuilder termQueryBuilder = new TermQueryBuilder(fieldName, value); return termQueryBuilder.rewrite(queryRewriteContext); } } else { MappedFieldType mft = sec.getFieldType(fieldName); if (mft != null && mft.getTextSearchInfo().getSearchAnalyzer() == Lucene.KEYWORD_ANALYZER) { TermQueryBuilder termQueryBuilder = new TermQueryBuilder(fieldName, value); return termQueryBuilder.rewrite(queryRewriteContext); } } return this; }	nit: maybe also add a test for this code path, i think only keyword fields are covered atm
@Override protected QueryBuilder doRewrite(QueryRewriteContext queryRewriteContext) throws IOException { if (fuzziness != null || lenient) { // Term queries can be neither fuzzy nor lenient, so don't rewrite under these conditions return this; } SearchExecutionContext sec = queryRewriteContext.convertToSearchExecutionContext(); if (sec == null) { return this; } // If we're using the default keyword analyzer then we can rewrite this to a TermQueryBuilder // and possibly shortcut if (analyzer != null) { if (sec.getIndexAnalyzers().get(analyzer) == Lucene.KEYWORD_ANALYZER) { TermQueryBuilder termQueryBuilder = new TermQueryBuilder(fieldName, value); return termQueryBuilder.rewrite(queryRewriteContext); } } else { MappedFieldType mft = sec.getFieldType(fieldName); if (mft != null && mft.getTextSearchInfo().getSearchAnalyzer() == Lucene.KEYWORD_ANALYZER) { TermQueryBuilder termQueryBuilder = new TermQueryBuilder(fieldName, value); return termQueryBuilder.rewrite(queryRewriteContext); } } return this; }	nit: maybe also add a test for this code path, i think only keyword fields are covered atm
public Expression expr(String source) { return parser.createExpression(source); }	i guess should be private and moved in the beginning or end of class.
* @param jobId The job id * @return The id of document the job is persisted in */ public static String documentId(String jobId) { return "job-" + jobId; }	since we're back at having the jobs indexed, we should double check the length limit on the id is correct
public static void createStateIndexAndAliasIfNecessary(Client client, ClusterState state, IndexNameExpressionResolver resolver, final ActionListener<Boolean> finalListener) { String[] stateIndices = resolver.concreteIndexNames(state, IndicesOptions.lenientExpandOpen(), jobStateIndexPattern()); Optional<IndexMetaData> indexPointedByCurrentWriteAlias = state.getMetaData().hasAlias(".ml-state-write") ? state.getMetaData().getAliasAndIndexLookup().get(".ml-state-write").getIndices().stream().findFirst() : Optional.empty(); String legacyJobStateIndex = AnomalyDetectorsIndexFields.STATE_INDEX_PREFIX; if (stateIndices.length == 0) { if (indexPointedByCurrentWriteAlias.isEmpty()) { createInitialStateIndex(client, true, finalListener); return; } logger.error( "There are no indices matching '.ml-state*' pattern but '.ml-state-write' alias points at [{}]. " + "This should never happen.", indexPointedByCurrentWriteAlias.get()); } else if (stateIndices.length == 1 && stateIndices[0].equals(legacyJobStateIndex)) { if (indexPointedByCurrentWriteAlias.isEmpty()) { createInitialStateIndex(client, true, finalListener); return; } if (indexPointedByCurrentWriteAlias.get().getIndex().getName().equals(legacyJobStateIndex)) { createInitialStateIndex( client, false, ActionListener.wrap( unused -> updateStateWriteAlias(client, legacyJobStateIndex, initialJobStateIndex(), finalListener), finalListener::onFailure) ); return; } logger.error( "There is exactly one index (i.e. '.ml-state') matching '.ml-state*' pattern but '.ml-state-write' alias points at [{}]. " + "This should never happen.", indexPointedByCurrentWriteAlias.get()); } else { if (indexPointedByCurrentWriteAlias.isEmpty()) { assert stateIndices.length > 0; String latestStateIndex = Arrays.stream(stateIndices).max(STATE_INDEX_NAME_COMPARATOR).get(); updateStateWriteAlias(client, null, latestStateIndex, finalListener); return; } } // If the .ml-state-write alias is set, there is nothing more to do. finalListener.onResponse(false); }	what happens if .ml-state-write is a concrete index? i may have missed a logic branch somewhere.
@Override protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) throws IOException { String datafeedId = restRequest.param(DatafeedConfig.ID.getPreferredName()); IndicesOptions indicesOptions = null; if (restRequest.hasParam("expand_wildcards") || restRequest.hasParam("ignore_unavailable") || restRequest.hasParam("allow_no_indices") || restRequest.hasParam("ignore_throttled")) { indicesOptions = IndicesOptions.fromRequest(restRequest, SearchRequest.DEFAULT_INDICES_OPTIONS); } XContentParser parser = restRequest.contentParser(); PutDatafeedAction.Request putDatafeedRequest = PutDatafeedAction.Request.parseRequest(datafeedId, indicesOptions, parser); putDatafeedRequest.timeout(restRequest.paramAsTime("timeout", putDatafeedRequest.timeout())); putDatafeedRequest.masterNodeTimeout(restRequest.paramAsTime("master_timeout", putDatafeedRequest.masterNodeTimeout())); return channel -> client.execute(PutDatafeedAction.INSTANCE, putDatafeedRequest, new RestToXContentListener<>(channel)); }	could we avoid checking if restrequest has those params? i think it'd work to do indicesoptions indicesoptions = indicesoptions.fromrequest(restrequest, searchrequest.default_indices_options)
public static List<CompressedXContent> collectMappings(final ClusterState state, final String templateName, final String indexName, final NamedXContentRegistry xContentRegistry) throws Exception { final ComposableIndexTemplate template = state.metadata().templatesV2().get(templateName); assert template != null : "attempted to resolve mappings for a template [" + templateName + "] that did not exist in the cluster state"; if (template == null) { return List.of(); } final Map<String, ComponentTemplate> componentTemplates = state.metadata().componentTemplates(); List<CompressedXContent> mappings = template.composedOf().stream() .map(componentTemplates::get) .filter(Objects::nonNull) .map(ComponentTemplate::template) .map(Template::mappings) .filter(Objects::nonNull) .collect(Collectors.toCollection(LinkedList::new)); // Add the actual index template's mappings, since it takes the highest precedence Optional.ofNullable(template.template()) .map(Template::mappings) .ifPresent(mappings::add); if (template.getDataStreamTemplate() != null) { // add a default mapping for the `@timestamp` field, at the lowest precedence, to make bootstrapping data streams more // straightforward mappings.add(0, new CompressedXContent(wrapMappingsIfNecessary(DEFAULT_TIMESTAMP_MAPPING, xContentRegistry))); } // Only include _timestamp mapping snippet if creating backing index. if (indexName.startsWith(DataStream.BACKING_INDEX_PREFIX)) { // Only if template has data stream definition this should be added and // adding this template last, since _timestamp field should have highest precedence: Optional.ofNullable(template.getDataStreamTemplate()) .map(ComposableIndexTemplate.DataStreamTemplate::getDataSteamMappingSnippet) .map(mapping -> { try (XContentBuilder builder = XContentBuilder.builder(XContentType.JSON.xContent())) { builder.value(mapping); return new CompressedXContent(BytesReference.bytes(builder)); } catch (IOException e) { throw new UncheckedIOException(e); } }) .ifPresent(mappings::add); } return Collections.unmodifiableList(mappings); }	i think we need to move this side the body of if (indexname.startswith(datastream.backing_index_prefix)) {, because this should only be applied when a backing index is created (and otherwise a regular create index call and if a template matches with a 'data_stream' definition would get this mapping applied as well).
public void testDefinedTimestampMappingIsAddedForDataStreamTemplates() throws Exception { final MetadataIndexTemplateService service = getMetadataIndexTemplateService(); ClusterState state = ClusterState.EMPTY_STATE; ComponentTemplate ct1 = new ComponentTemplate(new Template(null, new CompressedXContent("{\\\\n" + " \\\\"properties\\\\": {\\\\n" + " \\\\"field1\\\\": {\\\\n" + " \\\\"type\\\\": \\\\"keyword\\\\"\\\\n" + " }\\\\n" + " }\\\\n" + " }"), null), null, null); state = service.addComponentTemplate(state, true, "ct1", ct1); ComposableIndexTemplate it = new ComposableIndexTemplate(List.of("i*"), new Template(null, new CompressedXContent("{\\\\n" + " \\\\"properties\\\\": {\\\\n" + " \\\\"field2\\\\": {\\\\n" + " \\\\"type\\\\": \\\\"integer\\\\"\\\\n" + " }\\\\n" + " }\\\\n" + " }"), null), List.of("ct1"), 0L, 1L, null, new ComposableIndexTemplate.DataStreamTemplate(DEFAULT_TIMESTAMP_FIELD)); state = service.addIndexTemplateV2(state, true, "my-template", it); List<CompressedXContent> mappings = MetadataIndexTemplateService.collectMappings(state, "my-template", "my-index", xContentRegistry()); assertNotNull(mappings); assertThat(mappings.size(), equalTo(3)); List<Map<String, Object>> parsedMappings = mappings.stream() .map(m -> { try { return MapperService.parseMapping(new NamedXContentRegistry(List.of()), m.string()); } catch (Exception e) { logger.error(e); fail("failed to parse mappings: " + m.string()); return null; } }) .collect(Collectors.toList()); assertThat(parsedMappings.get(0), equalTo(Map.of("_doc", Map.of("properties", Map.of(DEFAULT_TIMESTAMP_FIELD, Map.of("type", "date")))))); assertThat(parsedMappings.get(1), equalTo(Map.of("_doc", Map.of("properties", Map.of("field1", Map.of("type", "keyword")))))); assertThat(parsedMappings.get(2), equalTo(Map.of("_doc", Map.of("properties", Map.of("field2", Map.of("type", "integer")))))); }	these are good tests. maybe also add another test that tests that a user adds a mapping for @timestamp in a composable index template?
static DeprecationIssue checkImplicitlyDisabledNativeRealms(final Settings settings, final PluginsAndModules pluginsAndModules) { final Map<RealmConfig.RealmIdentifier, Settings> realmSettings = RealmSettings.getRealmSettings(settings); if (realmSettings.isEmpty()) { return null; } // If all configured realms are disabled, this equals to no realm is configured. The implicitly behaviour in this case // is to add file and native realms. So we are good here. if (false == realmSettings.entrySet().stream().anyMatch( e -> e.getValue().getAsBoolean(RealmSettings.ENABLED_SETTING_KEY, true))) { return null; } final List<String> implicitlyDisabledNativeRealmTypes = new ArrayList<>(org.elasticsearch.common.collect.List.of(FileRealmSettings.TYPE, NativeRealmSettings.TYPE)); realmSettings.keySet().forEach(ri -> implicitlyDisabledNativeRealmTypes.remove(ri.getType())); if (implicitlyDisabledNativeRealmTypes.isEmpty()) { return null; } final String details = String.format( Locale.ROOT, "Found implicitly disabled native %s: [%s]. %s disabled because there are other explicitly configured realms." + "In next major release, native realms will always be enabled unless explicitly disabled.", implicitlyDisabledNativeRealmTypes.size() == 1 ? "realm" : "realms", Strings.collectionToDelimitedString(implicitlyDisabledNativeRealmTypes, ","), implicitlyDisabledNativeRealmTypes.size() == 1 ? "It is" : "They are"); return new DeprecationIssue( DeprecationIssue.Level.CRITICAL, "File and/or native realms cannot be implicitly disabled in next major release.", "https://www.elastic.co/guide/en/elasticsearch/reference/7.13/deprecated-7.13.html#implicitly-disabled-native-realms", details ); }	instead of critical i would say warning is better, because "failures will occur unless this is resolved before upgrading" is not accurate in this case.
static DeprecationIssue checkImplicitlyDisabledNativeRealms(final Settings settings, final PluginsAndModules pluginsAndModules) { final Map<RealmConfig.RealmIdentifier, Settings> realmSettings = RealmSettings.getRealmSettings(settings); if (realmSettings.isEmpty()) { return null; } // If all configured realms are disabled, this equals to no realm is configured. The implicitly behaviour in this case // is to add file and native realms. So we are good here. if (false == realmSettings.entrySet().stream().anyMatch( e -> e.getValue().getAsBoolean(RealmSettings.ENABLED_SETTING_KEY, true))) { return null; } final List<String> implicitlyDisabledNativeRealmTypes = new ArrayList<>(org.elasticsearch.common.collect.List.of(FileRealmSettings.TYPE, NativeRealmSettings.TYPE)); realmSettings.keySet().forEach(ri -> implicitlyDisabledNativeRealmTypes.remove(ri.getType())); if (implicitlyDisabledNativeRealmTypes.isEmpty()) { return null; } final String details = String.format( Locale.ROOT, "Found implicitly disabled native %s: [%s]. %s disabled because there are other explicitly configured realms." + "In next major release, native realms will always be enabled unless explicitly disabled.", implicitlyDisabledNativeRealmTypes.size() == 1 ? "realm" : "realms", Strings.collectionToDelimitedString(implicitlyDisabledNativeRealmTypes, ","), implicitlyDisabledNativeRealmTypes.size() == 1 ? "It is" : "They are"); return new DeprecationIssue( DeprecationIssue.Level.CRITICAL, "File and/or native realms cannot be implicitly disabled in next major release.", "https://www.elastic.co/guide/en/elasticsearch/reference/7.13/deprecated-7.13.html#implicitly-disabled-native-realms", details ); }	nit: cannot be implicitly disabled -> are implicitly enabled, or are enabled by default.
public TimeValue getModelPruneWindow() { return modelPruneWindow; }	remove this, as immutable classes shouldn't have setters.
public void validateModelSnapshotRetentionSettingsAndSetDefaults() { validateModelSnapshotRetentionSettings(); if (dailyModelSnapshotRetentionAfterDays == null && modelSnapshotRetentionDays != null && modelSnapshotRetentionDays > DEFAULT_DAILY_MODEL_SNAPSHOT_RETENTION_AFTER_DAYS) { dailyModelSnapshotRetentionAfterDays = DEFAULT_DAILY_MODEL_SNAPSHOT_RETENTION_AFTER_DAYS; } analysisConfig.setDefaultModelPruneWindowIfNoneProvided(); } /** * Validates that {@link #modelSnapshotRetentionDays} and {@link #dailyModelSnapshotRetentionAfterDays}	since analysisconfig is immutable, this should be changed as follows: 1. check if the current analysisconfig has a model prune window 2. if it doesn't, construct an analysisconfig.builder from analysisconfig 3. call setmodelprunewindow on that analysisconfig.builder 4. call setanalysisconfig on the job.builder that this method relates to, passing the analysisconfig.builder - this will replace the old analysisconfig on the job.builder with the adjusted one
@Nullable public <VS extends ValuesSource> VS valuesSource(ValuesSourceConfig<VS> config, SearchContext context) throws IOException { assert config.valid() : "value source config is invalid - must have either a field context or a script or marked as unmapped"; final VS vs; if (config.unmapped) { if (config.missing == null) { // otherwise we will have values because of the missing value vs = null; } else if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) { vs = (VS) ValuesSource.Numeric.EMPTY; } else if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) { vs = (VS) ValuesSource.GeoPoint.EMPTY; } else if (ValuesSource.class.isAssignableFrom(config.valueSourceType) || ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType) || ValuesSource.Bytes.WithOrdinals.class.isAssignableFrom(config.valueSourceType)) { vs = (VS) ValuesSource.Bytes.EMPTY; } else { throw new SearchParseException(searchContext, "Can't deal with unmapped ValuesSource type " + config.valueSourceType, null); } } else { vs = valuesSourceWithMissingValues(config); } if (config.missing == null) { return vs; } if (vs instanceof ValuesSource.Bytes) { final BytesRef missing = new BytesRef(config.missing.toString()); if (vs instanceof ValuesSource.Bytes.WithOrdinals) { return (VS) MissingValues.replaceMissing((ValuesSource.Bytes.WithOrdinals) vs, missing); } else { return (VS) MissingValues.replaceMissing((ValuesSource.Bytes) vs, missing); } } else if (vs instanceof ValuesSource.Numeric) { Number missing = null; if (config.missing instanceof Number) { missing = (Number) config.missing; } else { if (config.fieldContext != null && config.fieldContext.mapper() instanceof DateFieldMapper) { final DateFieldMapper mapper = (DateFieldMapper) config.fieldContext.mapper(); try { missing = mapper.dateTimeFormatter().parser().parseDateTime(config.missing.toString()).getMillis(); } catch (IllegalArgumentException e) { throw new SearchParseException(context, "Expected a date value in [missing] but got [" + config.missing + "]", null, e); } } else { try { missing = Double.parseDouble(config.missing.toString()); } catch (NumberFormatException e) { throw new SearchParseException(context, "Expected a numeric value in [missing] but got [" + config.missing + "]", null, e); } } } return (VS) MissingValues.replaceMissing((ValuesSource.Numeric) vs, missing); } else if (vs instanceof ValuesSource.GeoPoint) { // TODO: also support the structured formats of geo points final GeoPoint missing = GeoUtils.parseGeoPoint(config.missing.toString(), new GeoPoint()); return (VS) MissingValues.replaceMissing((ValuesSource.GeoPoint) vs, missing); } else { // Should not happen throw new SearchParseException(searchContext, "Can't apply missing values on a " + vs.getClass(), null); } }	i expected this method to give the valuessource with the missing values replaced but it actually gives the original valuessource. maybe this could be made clearer in the name or at least by a javadoc?
public static RoleDescriptor kibanaSystemRoleDescriptor(String name) { return new RoleDescriptor(name, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", "manage_token", "manage_oidc", InvalidateApiKeyAction.NAME, "grant_api_key", GetBuiltinPrivilegesAction.NAME, "delegate_pki", GetLifecycleAction.NAME, PutLifecycleAction.NAME, // To facilitate ML UI functionality being controlled using Kibana security privileges "manage_ml", // The symbolic constant for this one is in SecurityActionMapper, so not accessible from X-Pack core "cluster:admin/analyze", // To facilitate using the file uploader functionality "monitor_text_structure", // To cancel tasks and delete async searches "cancel_task" }, new RoleDescriptor.IndicesPrivileges[] { // System indices defined in KibanaPlugin RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*", ".reporting-*").privileges("all").allowRestrictedIndices(true).build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".management-beats").privileges("create_index", "read", "write").build(), // To facilitate ML UI functionality being controlled using Kibana security privileges RoleDescriptor.IndicesPrivileges.builder() .indices(".ml-anomalies*", ".ml-stats-*") .privileges("read").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".ml-annotations*", ".ml-notifications*") .privileges("read", "write").build(), // APM agent configuration - system index defined in KibanaPlugin RoleDescriptor.IndicesPrivileges.builder() .indices(".apm-agent-configuration").privileges("all").allowRestrictedIndices(true).build(), // APM custom link index creation - system index defined in KibanaPlugin RoleDescriptor.IndicesPrivileges.builder() .indices(".apm-custom-link").privileges("all").allowRestrictedIndices(true).build(), // APM telemetry queries APM indices in kibana task runner RoleDescriptor.IndicesPrivileges.builder() .indices("apm-*") .privileges("read", "read_cross_cluster") .build(), // Data telemetry reads mappings, metadata and stats of indices RoleDescriptor.IndicesPrivileges.builder() .indices("*") .privileges("view_index_metadata", "monitor") .build(), // Endpoint diagnostic information. Kibana reads from these indices to send telemetry RoleDescriptor.IndicesPrivileges.builder() .indices(".logs-endpoint.diagnostic.collection-*") .privileges("read").build(), // Fleet Server indices. Kibana create this indice before Fleet Server use them. // Fleet Server indices. Kibana read and write to this indice to manage Elastic Agents RoleDescriptor.IndicesPrivileges.builder() .indices(".fleet*") .allowRestrictedIndices(true) .privileges("all").build(), // Legacy "Alerts as data" used in Security Solution. // Kibana user creates these indices; reads / writes to them. RoleDescriptor.IndicesPrivileges.builder() .indices(ReservedRolesStore.ALERTS_LEGACY_INDEX) .privileges("all").build(), // "Alerts as data" internal backing indices used in Security Solution, Observability, etc. // Kibana system user creates these indices; reads / writes to them via the aliases (see below). RoleDescriptor.IndicesPrivileges.builder() .indices(ReservedRolesStore.ALERTS_BACKING_INDEX) .privileges("all").build(), // "Alerts as data" public index aliases used in Security Solution, Observability, etc. // Kibana system user uses them to read / write alerts. RoleDescriptor.IndicesPrivileges.builder() .indices(ReservedRolesStore.ALERTS_INDEX_ALIAS) .privileges("all").build(), // Endpoint / Fleet policy responses. Kibana requires read access to send telemetry RoleDescriptor.IndicesPrivileges.builder() .indices("metrics-endpoint.policy-*") .privileges("read").build(), // Endpoint metrics. Kibana requires read access to send telemetry RoleDescriptor.IndicesPrivileges.builder() .indices("metrics-endpoint.metrics-*") .privileges("read").build() }, null, new ConfigurableClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null); }	nit: unnecrssary formatting change
public static RoleDescriptor kibanaSystemRoleDescriptor(String name) { return new RoleDescriptor( name, new String[] { "monitor", "manage_index_templates", MonitoringBulkAction.NAME, "manage_saml", "manage_token", "manage_oidc", // For Fleet package upgrade "manage_pipeline", "manage_ilm", // For the endpoint package that ships a transform "manage_transform", InvalidateApiKeyAction.NAME, "grant_api_key", "manage_own_api_key", GetBuiltinPrivilegesAction.NAME, "delegate_pki", // To facilitate ML UI functionality being controlled using Kibana security privileges "manage_ml", // The symbolic constant for this one is in SecurityActionMapper, so not accessible from X-Pack core "cluster:admin/analyze", // To facilitate using the file uploader functionality "monitor_text_structure", // To cancel tasks and delete async searches "cancel_task" }, new RoleDescriptor.IndicesPrivileges[] { // System indices defined in KibanaPlugin RoleDescriptor.IndicesPrivileges.builder() .indices(".kibana*", ".reporting-*") .privileges("all") .allowRestrictedIndices(true) .build(), RoleDescriptor.IndicesPrivileges.builder().indices(".monitoring-*").privileges("read", "read_cross_cluster").build(), RoleDescriptor.IndicesPrivileges.builder().indices(".management-beats").privileges("create_index", "read", "write").build(), // To facilitate ML UI functionality being controlled using Kibana security privileges RoleDescriptor.IndicesPrivileges.builder().indices(".ml-anomalies*", ".ml-stats-*").privileges("read").build(), RoleDescriptor.IndicesPrivileges.builder() .indices(".ml-annotations*", ".ml-notifications*") .privileges("read", "write") .build(), // APM agent configuration - system index defined in KibanaPlugin RoleDescriptor.IndicesPrivileges.builder() .indices(".apm-agent-configuration") .privileges("all") .allowRestrictedIndices(true) .build(), // APM custom link index creation - system index defined in KibanaPlugin RoleDescriptor.IndicesPrivileges.builder() .indices(".apm-custom-link") .privileges("all") .allowRestrictedIndices(true) .build(), // APM telemetry queries APM indices in kibana task runner RoleDescriptor.IndicesPrivileges.builder().indices("apm-*").privileges("read", "read_cross_cluster").build(), // Data telemetry reads mappings, metadata and stats of indices RoleDescriptor.IndicesPrivileges.builder().indices("*").privileges("view_index_metadata", "monitor").build(), // Endpoint diagnostic information. Kibana reads from these indices to send telemetry RoleDescriptor.IndicesPrivileges.builder().indices(".logs-endpoint.diagnostic.collection-*").privileges("read").build(), // Fleet Server indices. Kibana create this indice before Fleet Server use them. // Fleet Server indices. Kibana read and write to this indice to manage Elastic Agents RoleDescriptor.IndicesPrivileges.builder().indices(".fleet*").allowRestrictedIndices(true).privileges("all").build(), // Legacy "Alerts as data" used in Security Solution. // Kibana user creates these indices; reads / writes to them. RoleDescriptor.IndicesPrivileges.builder().indices(ReservedRolesStore.ALERTS_LEGACY_INDEX).privileges("all").build(), // "Alerts as data" internal backing indices used in Security Solution, Observability, etc. // Kibana system user creates these indices; reads / writes to them via the aliases (see below). RoleDescriptor.IndicesPrivileges.builder().indices(ReservedRolesStore.ALERTS_BACKING_INDEX).privileges("all").build(), // "Alerts as data" public index aliases used in Security Solution, Observability, etc. // Kibana system user uses them to read / write alerts. RoleDescriptor.IndicesPrivileges.builder().indices(ReservedRolesStore.ALERTS_INDEX_ALIAS).privileges("all").build(), // "Alerts as data" public index alias used in Security Solution // Kibana system user uses them to read / write alerts. RoleDescriptor.IndicesPrivileges.builder().indices(ReservedRolesStore.PREVIEW_ALERTS_INDEX_ALIAS).privileges("all").build(), // "Alerts as data" internal backing indices used in Security Solution // Kibana system user creates these indices; reads / writes to them via the aliases (see below). RoleDescriptor.IndicesPrivileges.builder() .indices(ReservedRolesStore.PREVIEW_ALERTS_BACKING_INDEX_ALIAS) .privileges("all") .build(), // Endpoint / Fleet policy responses. Kibana requires read access to send telemetry RoleDescriptor.IndicesPrivileges.builder().indices("metrics-endpoint.policy-*").privileges("read").build(), // Endpoint metrics. Kibana requires read access to send telemetry RoleDescriptor.IndicesPrivileges.builder().indices("metrics-endpoint.metrics-*").privileges("read").build(), // Fleet package install and upgrade RoleDescriptor.IndicesPrivileges.builder() .indices( "logs-*", "synthetics-*", "traces-*", "/metrics-.*&~(metrics-endpoint\\\\\\\\.metadata_current_default)/", ".logs-endpoint.action.responses-*", ".logs-endpoint.diagnostic.collection-*", ".logs-endpoint.actions-*" ) .privileges(UpdateSettingsAction.NAME, PutMappingAction.NAME, RolloverAction.NAME) .build(), // Endpoint specific action responses. Kibana reads from these to display responses to the user. RoleDescriptor.IndicesPrivileges.builder().indices(".logs-endpoint.action.responses-*").privileges("read").build(), // Endpoint specific actions. Kibana reads and writes to this index to track new actions and display them. RoleDescriptor.IndicesPrivileges.builder() .indices(".logs-endpoint.actions-*") .privileges("create_index", "read", "write") .build(), // For ILM policy for APM & Endpoint packages that have delete action RoleDescriptor.IndicesPrivileges.builder() .indices(".logs-endpoint.diagnostic.collection-*", "traces-apm.sampled-*") .privileges(DeleteIndexAction.NAME) .build(), // For src/dest indices of the Endpoint package that ships a transform RoleDescriptor.IndicesPrivileges.builder() .indices("metrics-endpoint.metadata*") .privileges("read", "view_index_metadata") .build(), RoleDescriptor.IndicesPrivileges.builder() .indices( "metrics-endpoint.metadata_current_default", ".metrics-endpoint.metadata_current_default", ".metrics-endpoint.metadata_united_default" ) .privileges("create_index", "delete_index", "read", "index") .build(), }, null, new ConfigurableClusterPrivilege[] { new ManageApplicationPrivileges(Collections.singleton("kibana-*")) }, null, MetadataUtils.DEFAULT_RESERVED_METADATA, null ); }	do we actually need create_index or could we use auto_configure instead and rely on an index template in the endpoint package?
@Override public InferencePipelineAggregationBuilder rewrite(QueryRewriteContext context) { if (model != null) { return this; } SetOnce<LocalModel> loadedModel = new SetOnce<>(); BiConsumer<Client, ActionListener<?>> modelLoadAction = (client, listener) -> modelLoadingService.get() .getModelForSearch(modelId, listener.delegateFailure((delegate, model) -> { loadedModel.set(model); boolean isLicensed = MachineLearningField.ML_API_FEATURE.check(licenseState) || model.getLicenseLevel() == License.OperationMode.BASIC; if (isLicensed) { delegate.onResponse(null); } else { delegate.onFailure(LicenseUtils.newComplianceException(XPackField.MACHINE_LEARNING)); } })); context.registerAsyncAction((client, listener) -> { if (XPackSettings.SECURITY_ENABLED.get(settings)) { // check the user has ml privileges SecurityContext securityContext = new SecurityContext(settings, client.threadPool().getThreadContext()); useSecondaryAuthIfAvailable(securityContext, () -> { final String username = securityContext.getUser().principal(); final HasPrivilegesRequest privRequest = new HasPrivilegesRequest(); privRequest.username(username); privRequest.clusterPrivileges(GetTrainedModelsAction.NAME); privRequest.indexPrivileges(new RoleDescriptor.IndicesPrivileges[] {}); privRequest.applicationPrivileges(new RoleDescriptor.ApplicationResourcePrivileges[] {}); ActionListener<HasPrivilegesResponse> privResponseListener = ActionListener.wrap(r -> { if (r.isCompleteMatch()) { modelLoadAction.accept(client, listener); } else { listener.onFailure( Exceptions.authorizationError( "user [" + username + "] does not have the privilege to get trained models so cannot use ml inference" ) ); } }, listener::onFailure); client.execute(HasPrivilegesAction.INSTANCE, privRequest, privResponseListener); }); } else { modelLoadAction.accept(client, listener); } }); return new InferencePipelineAggregationBuilder( name, bucketPathMap, loadedModel::get, modelId, inferenceConfig, licenseState, settings ); }	should the order wrt the ml check above be reversed? that way if it is basic we don't trigger tracking for ml use?
@Override public UnaryOperator<RestHandler> getRestHandlerWrapper(ThreadContext threadContext) { final boolean ssl = HTTP_SSL_ENABLED.get(settings); final SSLConfiguration httpSSLConfig = getSslService().getHttpTransportSSLConfiguration(); boolean extractClientCertificate = ssl && getSslService().isSSLClientAuthEnabled(httpSSLConfig); return handler -> new SecurityRestFilter(getLicenseState(), threadContext, authcService.get(), secondayAuthc.get(), handler, extractClientCertificate); }	i think we need this: suggestion final boolean ssl = enabled && http_ssl_enabled.get(settings); if security is disabled, then http ssl will not be enabled either (even if the setting is true).
public void testAwsCredsDefaultSettings() { Settings repositorySettings = generateRepositorySettings(null, null, "eu-central", null, null); MockSecureSettings secureSettings = new MockSecureSettings(); secureSettings.setString("aws.config.default.access_key", "aws_key"); secureSettings.setString("aws.config.default.secret_key", "aws_secret"); Settings settings = Settings.builder().setSecureSettings(secureSettings).build(); launchAWSCredentialsWithElasticsearchSettingsTest(repositorySettings, settings, "aws_key", "aws_secret"); }	don't forget to update to s3.client. here. but i'm sure you'll think about it when you will run the tests :) i don't comment on other lines in this test but same applies.
public TaskProvider<? extends Task> createTask(Project project) { Configuration jarHellConfig = project.getConfigurations().create("jarHell"); if (BuildParams.isInternal() && project.getPath().equals(":libs:elasticsearch-core") == false) { // ideally we would configure this as a default dependency. But Default dependencies do not work correctly // with gradle project dependencies as they're resolved to late in the build and don't setup according task // dependencies properly project.getDependencies().add("jarHell", project.project(":libs:elasticsearch-core")); } TaskProvider<JarHellTask> jarHell = project.getTasks().register("jarHell", JarHellTask.class); jarHell.configure(t -> { SourceSet testSourceSet = Util.getJavaTestSourceSet(project).get(); t.setClasspath(testSourceSet.getRuntimeClasspath().plus(jarHellConfig)); t.dependsOn(jarHellConfig); }); return jarHell; }	i think that's fine anyway as we don't expect this to be replaced.
@Override public void beforeStart() { int debugPort = 5005; int httpPort = 9200; int transportPort = 9300; Map<String, String> additionalSettings = System.getProperties().entrySet().stream() .filter(entry -> entry.getKey().toString().startsWith(CUSTOM_SETTINGS_PREFIX)) .collect(Collectors.toMap( entry -> entry.getKey().toString().substring(CUSTOM_SETTINGS_PREFIX.length()), entry -> entry.getValue().toString() )); Path baseDataPath = null; String dataPathSetting = System.getProperty(DATA_DIR_SETTING); if (dataPathSetting != null) { baseDataPath = Paths.get(dataPathSetting).toAbsolutePath(); } for (ElasticsearchCluster cluster : getClusters()) { cluster.getFirstNode().setHttpPort(String.valueOf(httpPort)); httpPort++; cluster.getFirstNode().setTransportPort(String.valueOf(transportPort)); transportPort++; Path clusterDataPath = null; if (baseDataPath != null) { clusterDataPath = baseDataPath.resolve(cluster.getName()); } for (ElasticsearchNode node : cluster.getNodes()) { additionalSettings.forEach(node::setting); if (clusterDataPath != null) { node.setDataPath(clusterDataPath.resolve(node.getName())); } if (debug) { logger.lifecycle( "Running elasticsearch in debug mode, {} suspending until connected on debugPort {}", node, debugPort ); node.jvmArgs("-agentlib:jdwp=transport=dt_socket,server=n,suspend=y,address=" + debugPort); debugPort += 1; } } } }	why all this complexity? why not just set the data path to exactly what folks provide, otherwise go with the default? creating subdirectories under the provided path is a bit unintuitive and won't give the expected result if folks have an existing data dir that they want to test against.
@Override protected String location() { BlobPath location = BlobPath.cleanPath(); location = location.add(container); for (String path : basePath()) { location = location.add(path); } return location.buildAsString(); }	why not just cache this in a field instead of container when we always build the same string from a constant value of the base path and container?
private void followLeaderIndex(String clusterAlias, Index indexToFollow, AutoFollowPattern pattern, Map<String,String> headers, Consumer<Exception> onResult) { final String leaderIndexName = indexToFollow.getName(); final String followIndexName = getFollowerIndexName(pattern, leaderIndexName); ResumeFollowAction.Request request = new ResumeFollowAction.Request(); if ("_local_".equals(clusterAlias) == false) { request.setLeaderClusterAlias(clusterAlias); } request.setLeaderIndex(indexToFollow.getName()); request.setFollowerIndex(followIndexName); request.setMaxBatchOperationCount(pattern.getMaxBatchOperationCount()); request.setMaxConcurrentReadBatches(pattern.getMaxConcurrentReadBatches()); request.setMaxBatchSize(pattern.getMaxBatchSize()); request.setMaxConcurrentWriteBatches(pattern.getMaxConcurrentWriteBatches()); request.setMaxWriteBufferSize(pattern.getMaxWriteBufferSize()); request.setMaxRetryDelay(pattern.getMaxRetryDelay()); request.setPollTimeout(pattern.getPollTimeout()); // Execute if the create and follow api call succeeds: Runnable successHandler = () -> { LOGGER.info("Auto followed leader index [{}] as follow index [{}]", leaderIndexName, followIndexName); // This function updates the auto follow metadata in the cluster to record that the leader index has been followed: // (so that we do not try to follow it in subsequent auto follow runs) Function<ClusterState, ClusterState> function = recordLeaderIndexAsFollowFunction(clusterAlias, indexToFollow); // The coordinator always runs on the elected master node, so we can update cluster state here: updateAutoFollowMetadata(function, onResult); }; createAndFollow(headers, request, successHandler, onResult); }	this is an example where i think this should match the body field leader_cluster.
@SuppressWarnings("unchecked") private void assertRollUpJob(final String rollupJob) throws Exception { final Matcher<?> expectedStates = anyOf(equalTo("indexing"), equalTo("started")); waitForRollUpJob(rollupJob, expectedStates); // check that the rollup job is started using the RollUp API final Request getRollupJobRequest; if (isRunningAgainstOldCluster()) { getRollupJobRequest = new Request("GET", "/_xpack/rollup/job/" + rollupJob); } else { getRollupJobRequest = new Request("GET", "/_rollup/job/" + rollupJob); } Map<String, Object> getRollupJobResponse = entityAsMap(client().performRequest(getRollupJobRequest)); Map<String, Object> job = getJob(getRollupJobResponse, rollupJob); if (job != null) { assertThat(ObjectPath.eval("status.job_state", job), expectedStates); } // check that the rollup job is started using the Tasks API final Request taskRequest = new Request("GET", "_tasks"); taskRequest.addParameter("detailed", "true"); taskRequest.addParameter("actions", "xpack/rollup/*"); Map<String, Object> taskResponse = entityAsMap(client().performRequest(taskRequest)); Map<String, Object> taskResponseNodes = (Map<String, Object>) taskResponse.get("nodes"); Map<String, Object> taskResponseNode = (Map<String, Object>) taskResponseNodes.values().iterator().next(); Map<String, Object> taskResponseTasks = (Map<String, Object>) taskResponseNode.get("tasks"); Map<String, Object> taskResponseStatus = (Map<String, Object>) taskResponseTasks.values().iterator().next(); assertThat(ObjectPath.eval("status.job_state", taskResponseStatus), expectedStates); // check that the rollup job is started using the Cluster State API final Request clusterStateRequest = new Request("GET", "_cluster/state/metadata"); Map<String, Object> clusterStateResponse = entityAsMap(client().performRequest(clusterStateRequest)); List<Map<String, Object>> rollupJobTasks = ObjectPath.eval("metadata.persistent_tasks.tasks", clusterStateResponse); boolean hasRollupTask = false; for (Map<String, Object> task : rollupJobTasks) { if (ObjectPath.eval("id", task).equals(rollupJob)) { hasRollupTask = true; // Persistent task state field has been renamed in 6.4.0 from "status" to "state" final String stateFieldName = (isRunningAgainstOldCluster() && getOldClusterVersion().before(Version.V_6_4_0)) ? "status" : "state"; final String jobStateField = "task.xpack/rollup/job." + stateFieldName + ".job_state"; assertThat("Expected field [" + jobStateField + "] to be started or indexing in " + task.get("id"), ObjectPath.eval(jobStateField, task), expectedStates); break; } } if (hasRollupTask == false) { fail("Expected persistent task for [" + rollupJob + "] but none found."); } }	these are required because we can do a full cluster restart against versions that don't have the new endpoint.
public void testExtractValueMixedDottedObjectNotation() throws IOException { XContentBuilder builder = XContentFactory.jsonBuilder().startObject() .startObject("foo").field("cat", "meow").endObject() .field("foo.cat", "miau") .endObject(); try (XContentParser parser = createParser(JsonXContent.jsonXContent, Strings.toString(builder))) { Map<String, Object> map = parser.map(); assertThat((List<?>) XContentMapValues.extractValue("foo.cat", map), containsInAnyOrder("meow", "miau")); } }	what happens if we have "foo.cat" : ["miaw", "purr"] here? i guess we get a list that contains a string and a further list, which is probably fine given that this is an edge case already.
* @param principalName user principal name * @return result string after stripping realm name */ private String stripRealmName(final String principalName) { if (this.stripRealmName) { int foundAtIndex = principalName.indexOf('@'); if (foundAtIndex > 0) { return principalName.substring(0, foundAtIndex); } } return principalName; }	this should probably be named something like mayberemoverealmname
public void testAuthenticateDifferentFailureScenarios() throws LoginException, GSSException { final String username = randomPrincipalName(); final String outToken = randomAlphaOfLength(10); final KerberosRealm kerberosRealm = createKerberosRealm(username); final boolean validTicket = rarely(); final boolean throwExceptionForInvalidTicket = validTicket ? false : randomBoolean(); final boolean throwLoginException = randomBoolean(); final byte[] decodedTicket = randomByteArrayOfLength(5); final Path keytabPath = config.env().configFile().resolve(KerberosRealmSettings.HTTP_SERVICE_KEYTAB_PATH.get(config.settings())); final boolean krbDebug = KerberosRealmSettings.SETTING_KRB_DEBUG_ENABLE.get(config.settings()); if (validTicket) { mockKerberosTicketValidator(decodedTicket, keytabPath, krbDebug, new Tuple<>(username, outToken), null); } else { if (throwExceptionForInvalidTicket) { if (throwLoginException) { mockKerberosTicketValidator(decodedTicket, keytabPath, krbDebug, null, new LoginException("Login Exception")); } else { mockKerberosTicketValidator(decodedTicket, keytabPath, krbDebug, null, new GSSException(GSSException.FAILURE)); } } else { mockKerberosTicketValidator(decodedTicket, keytabPath, krbDebug, new Tuple<>(null, outToken), null); } } final boolean nullKerberosAuthnToken = rarely(); final KerberosAuthenticationToken kerberosAuthenticationToken = nullKerberosAuthnToken ? null : new KerberosAuthenticationToken(decodedTicket); final PlainActionFuture<AuthenticationResult> future = new PlainActionFuture<>(); kerberosRealm.authenticate(kerberosAuthenticationToken, future); AuthenticationResult result = future.actionGet(); assertThat(result, is(notNullValue())); if (nullKerberosAuthnToken) { assertThat(result.getStatus(), is(equalTo(AuthenticationResult.Status.CONTINUE))); } else { if (validTicket) { final User expectedUser = new User(stripRealmName(username), roles.toArray(new String[roles.size()]), null, null, null, true); assertSuccessAuthenticationResult(expectedUser, outToken, result); } else { assertThat(result.getStatus(), is(equalTo(AuthenticationResult.Status.TERMINATE))); if (throwExceptionForInvalidTicket == false) { assertThat(result.getException(), is(instanceOf(ElasticsearchSecurityException.class))); final List<String> wwwAuthnHeader = ((ElasticsearchSecurityException) result.getException()) .getHeader(KerberosAuthenticationToken.WWW_AUTHENTICATE); assertThat(wwwAuthnHeader, is(notNullValue())); assertThat(wwwAuthnHeader.get(0), is(equalTo(KerberosAuthenticationToken.NEGOTIATE_AUTH_HEADER_PREFIX + outToken))); assertThat(result.getMessage(), is(equalTo("failed to authenticate user, gss context negotiation not complete"))); } else { if (throwLoginException) { assertThat(result.getMessage(), is(equalTo("failed to authenticate user, service login failure"))); } else { assertThat(result.getMessage(), is(equalTo("failed to authenticate user, gss context negotiation failure"))); } assertThat(result.getException(), is(instanceOf(ElasticsearchSecurityException.class))); final List<String> wwwAuthnHeader = ((ElasticsearchSecurityException) result.getException()) .getHeader(KerberosAuthenticationToken.WWW_AUTHENTICATE); assertThat(wwwAuthnHeader, is(notNullValue())); assertThat(wwwAuthnHeader.get(0), is(equalTo(KerberosAuthenticationToken.NEGOTIATE_SCHEME_NAME))); } } verify(mockKerberosTicketValidator).validateTicket(aryEq(decodedTicket), eq(keytabPath), eq(krbDebug), any(ActionListener.class)); } }	nit: its much cleaner if you just move the whole new user(... statement to a new line
private HttpUriRequest performRandomRequest(String method) throws Exception { String uriAsString = "/" + randomStatusCode(getRandom()); URIBuilder uriBuilder = new URIBuilder(uriAsString); final Map<String, String> params = new HashMap<>(); boolean hasParams = randomBoolean(); if (hasParams) { int numParams = randomIntBetween(1, 3); for (int i = 0; i < numParams; i++) { String paramKey = "param-" + i; String paramValue = randomAsciiOfLengthBetween(3, 10); params.put(paramKey, paramValue); uriBuilder.addParameter(paramKey, paramValue); } } if (randomBoolean()) { //randomly add some ignore parameter, which doesn't get sent as part of the request String ignore = Integer.toString(randomFrom(RestClientTestUtil.getAllErrorStatusCodes())); if (randomBoolean()) { ignore += "," + Integer.toString(randomFrom(RestClientTestUtil.getAllErrorStatusCodes())); } params.put("ignore", ignore); } URI uri = uriBuilder.build(); HttpUriRequest request; switch(method) { case "DELETE": request = new HttpDeleteWithEntity(uri); break; case "GET": request = new HttpGetWithEntity(uri); break; case "HEAD": request = new HttpHead(uri); break; case "OPTIONS": request = new HttpOptions(uri); break; case "PATCH": request = new HttpPatch(uri); break; case "POST": request = new HttpPost(uri); break; case "PUT": request = new HttpPut(uri); break; case "TRACE": request = new HttpTrace(uri); break; default: throw new UnsupportedOperationException("method not supported: " + method); } HttpEntity entity = null; boolean hasBody = request instanceof HttpEntityEnclosingRequest && getRandom().nextBoolean(); if (hasBody) { entity = new StringEntity(randomAsciiOfLengthBetween(10, 100), ContentType.APPLICATION_JSON); ((HttpEntityEnclosingRequest) request).setEntity(entity); } Header[] headers = new Header[0]; final Set<String> uniqueNames = new HashSet<>(); if (randomBoolean()) { headers = RestClientTestUtil.randomHeaders(getRandom(), "Header"); for (Header header : headers) { request.addHeader(header); uniqueNames.add(header.getName()); } } for (Header defaultHeader : defaultHeaders) { // request level headers override default headers if (uniqueNames.contains(defaultHeader.getName()) == false) { request.addHeader(defaultHeader); } } try { if (hasParams == false && hasBody == false && randomBoolean()) { restClient.performRequest(method, uriAsString, headers); } else if (hasBody == false && randomBoolean()) { restClient.performRequest(method, uriAsString, params, headers); } else { restClient.performRequest(method, uriAsString, params, entity, headers); } } catch(ResponseException e) { //all good } return request; }	thanks for fixing all these.
public E resolve(Object key, E defaultValue) { AttributeMap<Object> map = (AttributeMap<Object>) this; Object candidate = defaultValue; Object value = null; while ((value = map.getOrDefault(key, NOT_FOUND)) != NOT_FOUND && value != key) { key = candidate = value; } return (E) candidate; }	as this method builds on other types methods in this class, it shouldn't have unchecked warnings nor used not_found. also not sure why the candidate is returned instead of value, should be the other way around. further more this implementation looks buggy: if the map contains (e,e) calling resolve(e, default) would incorrectly return default and not e. further more there's no handling of cycles which leads to an infinite loop: (a, b), (b, a). below a suggestion to fix both issues: suggestion public e resolveordefault(object key, e defaultvalue) { e value = defaultvalue; e candidate; int allowedlookups = 10; while (key != value && ((candidate = get(key)) != null || containskey(key))) { if (--allowedlookups == 0) { throw new qlillegalargumentexception("potential cycle detected"); } key = candidate; value = candidate; } return value; }
static DateFormatter forPattern(String input) { if (Strings.hasLength(input) == false) { throw new IllegalArgumentException("No date pattern provided"); } if (input.startsWith("8") == false) { return Joda.forPattern(input); } // force java 8 date format List<DateFormatter> formatters = new ArrayList<>(); for (String pattern : Strings.delimitedListToStringArray(input, "||")) { if (Strings.hasLength(input) == false) { throw new IllegalArgumentException("Cannot have empty element in multi date format pattern: " + input); } formatters.add(DateFormatters.forPattern(pattern.substring(1))); } if (formatters.size() == 1) { return formatters.get(0); } return new DateFormatters.MergedDateFormatter(input, formatters); }	this needs a check for each of the pattern if it starts with 8 in order to return the proper joda time pattern for sth like 8date_optional_time||date_optional_time
public void testRandomGeometryIntersection() throws IOException { int testPointCount = randomIntBetween(100, 200); Point[] testPoints = new Point[testPointCount]; double extentSize = randomDoubleBetween(1, 10, true); boolean[] intersects = new boolean[testPointCount]; for (int i = 0; i < testPoints.length; i++) { testPoints[i] = randomPoint(false); } Geometry geometry = randomGeometryTreeGeometry(); GeoShapeIndexer indexer = new GeoShapeIndexer(true, "test"); Geometry preparedGeometry = indexer.prepareForIndexing(geometry); // TODO: support multi-polygons if (ShapeType.POLYGON == geometry.type() && ShapeType.MULTIPOLYGON == preparedGeometry.type()) { return; } for (int i = 0; i < testPointCount; i++) { int cur = i; intersects[cur] = fold(preparedGeometry, false, (g, s) -> s || intersects(g, testPoints[cur], extentSize)); } for (int i = 0; i < testPointCount; i++) { assertEquals(intersects[i], intersects(preparedGeometry, testPoints[i], extentSize)); } }	i think assumefalse would work better here showing us if the test actually ran or not.
public void testGetAlias() throws IOException { { createIndex("index1", Settings.EMPTY); client().performRequest(HttpPut.METHOD_NAME, "/index1/_alias/alias1"); createIndex("index2", Settings.EMPTY); client().performRequest(HttpPut.METHOD_NAME, "/index2/_alias/alias2"); createIndex("index3", Settings.EMPTY); } { GetAliasesRequest getAliasesRequest = new GetAliasesRequest().aliases("alias1"); GetAliasesResponse getAliasesResponse = execute(getAliasesRequest, highLevelClient().indices()::getAlias, highLevelClient().indices()::getAliasAsync); assertThat(getAliasesResponse.getAliases().size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index1").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index1").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).alias(), equalTo("alias1")); assertThat(getAliasesResponse.getAliases().get("index1").get(0).getFilter(), nullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).getIndexRouting(), nullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).getSearchRouting(), nullValue()); } { GetAliasesRequest getAliasesRequest = new GetAliasesRequest().aliases("alias*"); GetAliasesResponse getAliasesResponse = execute(getAliasesRequest, highLevelClient().indices()::getAlias, highLevelClient().indices()::getAliasAsync); assertThat(getAliasesResponse.getAliases().size(), equalTo(2)); assertThat(getAliasesResponse.getAliases().get("index1").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index1").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).alias(), equalTo("alias1")); assertThat(getAliasesResponse.getAliases().get("index2").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index2").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index2").get(0).alias(), equalTo("alias2")); } { GetAliasesRequest getAliasesRequest = new GetAliasesRequest().aliases("_all"); GetAliasesResponse getAliasesResponse = execute(getAliasesRequest, highLevelClient().indices()::getAlias, highLevelClient().indices()::getAliasAsync); assertThat(getAliasesResponse.getAliases().size(), equalTo(2)); assertThat(getAliasesResponse.getAliases().get("index1").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index1").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).alias(), equalTo("alias1")); assertThat(getAliasesResponse.getAliases().get("index2").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index2").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index2").get(0).alias(), equalTo("alias2")); } { GetAliasesRequest getAliasesRequest = new GetAliasesRequest().aliases("*"); GetAliasesResponse getAliasesResponse = execute(getAliasesRequest, highLevelClient().indices()::getAlias, highLevelClient().indices()::getAliasAsync); assertThat(getAliasesResponse.getAliases().size(), equalTo(2)); assertThat(getAliasesResponse.getAliases().get("index1").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index1").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).alias(), equalTo("alias1")); assertThat(getAliasesResponse.getAliases().get("index2").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index2").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index2").get(0).alias(), equalTo("alias2")); } { GetAliasesRequest getAliasesRequest = new GetAliasesRequest().indices("_all"); GetAliasesResponse getAliasesResponse = execute(getAliasesRequest, highLevelClient().indices()::getAlias, highLevelClient().indices()::getAliasAsync); assertThat(getAliasesResponse.getAliases().size(), equalTo(3)); assertThat(getAliasesResponse.getAliases().get("index1").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index1").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).alias(), equalTo("alias1")); assertThat(getAliasesResponse.getAliases().get("index2").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index2").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index2").get(0).alias(), equalTo("alias2")); assertThat(getAliasesResponse.getAliases().get("index3").size(), equalTo(0)); } { GetAliasesRequest getAliasesRequest = new GetAliasesRequest().indices("ind*"); GetAliasesResponse getAliasesResponse = execute(getAliasesRequest, highLevelClient().indices()::getAlias, highLevelClient().indices()::getAliasAsync); assertThat(getAliasesResponse.getAliases().size(), equalTo(3)); assertThat(getAliasesResponse.getAliases().get("index1").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index1").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).alias(), equalTo("alias1")); assertThat(getAliasesResponse.getAliases().get("index2").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index2").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index2").get(0).alias(), equalTo("alias2")); assertThat(getAliasesResponse.getAliases().get("index3").size(), equalTo(0)); } { GetAliasesRequest getAliasesRequest = new GetAliasesRequest(); GetAliasesResponse getAliasesResponse = execute(getAliasesRequest, highLevelClient().indices()::getAlias, highLevelClient().indices()::getAliasAsync); assertThat(getAliasesResponse.getAliases().size(), equalTo(3)); assertThat(getAliasesResponse.getAliases().get("index1").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index1").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index1").get(0).alias(), equalTo("alias1")); assertThat(getAliasesResponse.getAliases().get("index2").size(), equalTo(1)); assertThat(getAliasesResponse.getAliases().get("index2").get(0), notNullValue()); assertThat(getAliasesResponse.getAliases().get("index2").get(0).alias(), equalTo("alias2")); assertThat(getAliasesResponse.getAliases().get("index3").size(), equalTo(0)); } }	i noticed that when a wildcard expressions is provided for the indices, and no indices match, we return 200 and an empty json object. can we test that too? curl 'localhost:9200/nothing*/_alias?pretty' { }
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeVInt(aliases.size()); for (ObjectObjectCursor<String, List<AliasMetaData>> entry : aliases) { out.writeString(entry.key); out.writeVInt(entry.value.size()); for (AliasMetaData aliasMetaData : entry.value) { aliasMetaData.writeTo(out); } } if (out.getVersion().onOrAfter(Version.V_7_0_0_alpha1)) { // if (out.getVersion().onOrAfter(Version.V_6_3_0)) { if (status != RestStatus.OK) { out.writeBoolean(true); out.writeInt(status.getStatus()); out.writeString(errorMsg); } else { out.writeBoolean(false); } }	why the boolean flag? we could otherwise always serialize the status and serialize the error as an optional string (as i asked above to accept null as well)?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeVInt(aliases.size()); for (ObjectObjectCursor<String, List<AliasMetaData>> entry : aliases) { out.writeString(entry.key); out.writeVInt(entry.value.size()); for (AliasMetaData aliasMetaData : entry.value) { aliasMetaData.writeTo(out); } } if (out.getVersion().onOrAfter(Version.V_7_0_0_alpha1)) { // if (out.getVersion().onOrAfter(Version.V_6_3_0)) { if (status != RestStatus.OK) { out.writeBoolean(true); out.writeInt(status.getStatus()); out.writeString(errorMsg); } else { out.writeBoolean(false); } }	make it static? is this conversion needed because we lose ordering of the aliases though they are stored in a list?
public static GetAliasesResponse fromXContent(XContentParser parser) throws IOException { if (parser.currentToken() == null) { parser.nextToken(); } ensureExpectedToken(Token.START_OBJECT, parser.currentToken(), parser::getTokenLocation); ImmutableOpenMap.Builder<String, List<AliasMetaData>> aliasesBuilder = ImmutableOpenMap.builder(); String currentFieldName; Token token; String exceptionMsg = null; RestStatus status = RestStatus.OK; ElasticsearchException exception = null; while ((token = parser.nextToken()) != Token.END_OBJECT) { if (parser.currentToken() == Token.FIELD_NAME) { currentFieldName = parser.currentName(); if ("status".equals(currentFieldName)) { if ((token = parser.nextToken()) != Token.FIELD_NAME) { ensureExpectedToken(XContentParser.Token.VALUE_NUMBER, token, parser::getTokenLocation); status = RestStatus.fromCode(parser.intValue()); } } else if ("error".equals(currentFieldName)) { if ((token = parser.nextToken()) != Token.FIELD_NAME) { if (token == Token.VALUE_STRING) { exceptionMsg = parser.text(); } else if (token == Token.START_OBJECT) { token = parser.nextToken(); exception = ElasticsearchException.innerFromXContent(parser, true); } } } else { String indexName = parser.currentName(); if (parser.nextToken() == Token.START_OBJECT) { List<AliasMetaData> parseInside = parseAliases(parser); aliasesBuilder.put(indexName, parseInside); } } } } if (exception != null) { throw new ElasticsearchStatusException(exception.getMessage(), status, exception.getCause()); } if (RestStatus.OK != status && aliasesBuilder.isEmpty()) { throw new ElasticsearchStatusException(exceptionMsg, status); } GetAliasesResponse getAliasesResponse = new GetAliasesResponse(aliasesBuilder.build(), status, exceptionMsg); return getAliasesResponse; }	this resembles what elasticsearchexception#failurefromxcontent does. but it will build an exception in the first case with a string message only. that's why i was hoping that we could use an exception in every case, but i don't think it would work at transport, as such exception would be serialized then completely different when calling toxcontent on it. there's generatefailurexcontent that is similar to what we would need, yet not quite what we need. i think what you have is good, unless you want to reuse the parsing code that we already have and then take the string out of the parsed exception which is what you need. not sure really.
public static GetAliasesResponse fromXContent(XContentParser parser) throws IOException { if (parser.currentToken() == null) { parser.nextToken(); } ensureExpectedToken(Token.START_OBJECT, parser.currentToken(), parser::getTokenLocation); ImmutableOpenMap.Builder<String, List<AliasMetaData>> aliasesBuilder = ImmutableOpenMap.builder(); String currentFieldName; Token token; String exceptionMsg = null; RestStatus status = RestStatus.OK; ElasticsearchException exception = null; while ((token = parser.nextToken()) != Token.END_OBJECT) { if (parser.currentToken() == Token.FIELD_NAME) { currentFieldName = parser.currentName(); if ("status".equals(currentFieldName)) { if ((token = parser.nextToken()) != Token.FIELD_NAME) { ensureExpectedToken(XContentParser.Token.VALUE_NUMBER, token, parser::getTokenLocation); status = RestStatus.fromCode(parser.intValue()); } } else if ("error".equals(currentFieldName)) { if ((token = parser.nextToken()) != Token.FIELD_NAME) { if (token == Token.VALUE_STRING) { exceptionMsg = parser.text(); } else if (token == Token.START_OBJECT) { token = parser.nextToken(); exception = ElasticsearchException.innerFromXContent(parser, true); } } } else { String indexName = parser.currentName(); if (parser.nextToken() == Token.START_OBJECT) { List<AliasMetaData> parseInside = parseAliases(parser); aliasesBuilder.put(indexName, parseInside); } } } } if (exception != null) { throw new ElasticsearchStatusException(exception.getMessage(), status, exception.getCause()); } if (RestStatus.OK != status && aliasesBuilder.isEmpty()) { throw new ElasticsearchStatusException(exceptionMsg, status); } GetAliasesResponse getAliasesResponse = new GetAliasesResponse(aliasesBuilder.build(), status, exceptionMsg); return getAliasesResponse; }	shouldn't this one already be covered by the previous if?
@Override protected void masterOperation(GetAliasesRequest request, ClusterState state, ActionListener<GetAliasesResponse> listener) { String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(state, request); ImmutableOpenMap<String, List<AliasMetaData>> result = state.metaData().findAliases(request.aliases(), concreteIndices); SetOnce<String> message = new SetOnce<>(); SetOnce<RestStatus> status = new SetOnce<>(); if (false == Strings.isAllOrWildcard(request.aliases())) { String[] aliasesNames = Strings.EMPTY_ARRAY; if (false == Strings.isAllOrWildcard(request.aliases())) { aliasesNames = request.aliases(); } final Set<String> aliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : result) { for (final AliasMetaData aliasMetaData : cursor.value) { aliasNames.add(aliasMetaData.alias()); } } // first remove requested aliases that are exact matches final SortedSet<String> difference = Sets.sortedDifference(Arrays.stream(aliasesNames).collect(Collectors.toSet()), aliasNames); // now remove requested aliases that contain wildcards that are simple matches final List<String> matches = new ArrayList<>(); outer: for (final String pattern : difference) { if (pattern.contains("*")) { for (final String aliasName : aliasNames) { if (Regex.simpleMatch(pattern, aliasName)) { matches.add(pattern); continue outer; } } } } difference.removeAll(matches); if (false == difference.isEmpty()) { status.set(RestStatus.NOT_FOUND); if (difference.size() == 1) { message.set(String.format(Locale.ROOT, "alias [%s] missing", toNamesString(difference.iterator().next()))); } else { message.set(String.format(Locale.ROOT, "aliases [%s] missing", toNamesString(difference.toArray(new String[0])))); } } } if (status.get() == null) { status.set(RestStatus.OK); } if (message.get() == null) { message.set(""); } listener.onResponse(new GetAliasesResponse(result, status.get(), message.get())); }	i think that i would find this more readable if we declared message and status down here and assigned them in an if/else. in that case i would go back to not using setonce.
@Override protected void masterOperation(GetAliasesRequest request, ClusterState state, ActionListener<GetAliasesResponse> listener) { String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(state, request); ImmutableOpenMap<String, List<AliasMetaData>> result = state.metaData().findAliases(request.aliases(), concreteIndices); SetOnce<String> message = new SetOnce<>(); SetOnce<RestStatus> status = new SetOnce<>(); if (false == Strings.isAllOrWildcard(request.aliases())) { String[] aliasesNames = Strings.EMPTY_ARRAY; if (false == Strings.isAllOrWildcard(request.aliases())) { aliasesNames = request.aliases(); } final Set<String> aliasNames = new HashSet<>(); for (final ObjectObjectCursor<String, List<AliasMetaData>> cursor : result) { for (final AliasMetaData aliasMetaData : cursor.value) { aliasNames.add(aliasMetaData.alias()); } } // first remove requested aliases that are exact matches final SortedSet<String> difference = Sets.sortedDifference(Arrays.stream(aliasesNames).collect(Collectors.toSet()), aliasNames); // now remove requested aliases that contain wildcards that are simple matches final List<String> matches = new ArrayList<>(); outer: for (final String pattern : difference) { if (pattern.contains("*")) { for (final String aliasName : aliasNames) { if (Regex.simpleMatch(pattern, aliasName)) { matches.add(pattern); continue outer; } } } } difference.removeAll(matches); if (false == difference.isEmpty()) { status.set(RestStatus.NOT_FOUND); if (difference.size() == 1) { message.set(String.format(Locale.ROOT, "alias [%s] missing", toNamesString(difference.iterator().next()))); } else { message.set(String.format(Locale.ROOT, "aliases [%s] missing", toNamesString(difference.toArray(new String[0])))); } } } if (status.get() == null) { status.set(RestStatus.OK); } if (message.get() == null) { message.set(""); } listener.onResponse(new GetAliasesResponse(result, status.get(), message.get())); }	why do we set it to an empty string? i think in this case null would be ok?
public void testSearchAndGetAPIsAreThrottled() throws InterruptedException, IOException, ExecutionException { XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("_doc") .startObject("properties").startObject("field").field("type", "text").field("term_vector", "with_positions_offsets_payloads") .endObject().endObject() .endObject().endObject(); createIndex("index", Settings.builder().put("index.number_of_shards", 2).build(), "_doc", mapping); for (int i = 0; i < 10; i++) { client().prepareIndex("index", "_doc", "" + i).setSource("field", "foo bar baz").get(); } assertAcked(client().execute(FreezeIndexAction.INSTANCE, new TransportFreezeIndexAction.FreezeRequest("index")).actionGet()); int numRequests = randomIntBetween(20, 50); CountDownLatch latch = new CountDownLatch(numRequests); ActionListener listener = ActionListener.wrap(latch::countDown); int numRefreshes = 0; for (int i = 0; i < numRequests; i++) { numRefreshes++; switch (randomIntBetween(0, 3)) { case 0: client().prepareGet("index", "_doc", "" + randomIntBetween(0, 9)).execute(listener); break; case 1: client().prepareSearch("index").setIndicesOptions(IndicesOptions.STRICT_EXPAND_OPEN_FORBID_CLOSED) .setSearchType(SearchType.QUERY_THEN_FETCH) .execute(listener); // in total 4 refreshes 1x query & 1x fetch per shard (we have 2) numRefreshes += 3; break; case 2: client().prepareTermVectors("index", "" + randomIntBetween(0, 9)).execute(listener); break; case 3: client().prepareExplain("index", "_doc", "" + randomIntBetween(0, 9)).setQuery(new MatchAllQueryBuilder()) .execute(listener); break; default: assert false; } } latch.await(); IndicesStatsResponse index = client().admin().indices().prepareStats("index").clear().setRefresh(true).get(); assertEquals(numRefreshes, index.getTotal().refresh.getTotal()); }	can you fix indentation?
private boolean prepareResponse(final ClusterHealthRequest request, final ClusterHealthResponse response, ClusterState clusterState, final int waitFor) { int waitForCounter = 0; if (request.waitForStatus() != null && response.getStatus().value() <= request.waitForStatus().value()) { waitForCounter++; } if (request.waitForQuorumActive() == true && response.isQuorumActive() == true) { waitForCounter++; } if (request.waitForRelocatingShards() != -1 && response.getRelocatingShards() <= request.waitForRelocatingShards()) { waitForCounter++; } if (request.waitForActiveShards() != -1 && response.getActiveShards() >= request.waitForActiveShards()) { waitForCounter++; } if (request.indices().length > 0) { try { clusterState.metaData().concreteIndices(IndicesOptions.strictExpand(), request.indices()); waitForCounter++; } catch (IndexMissingException e) { response.status = ClusterHealthStatus.RED; // no indices, make sure its RED // missing indices, wait a bit more... } } if (!request.waitForNodes().isEmpty()) { if (request.waitForNodes().startsWith(">=")) { int expected = Integer.parseInt(request.waitForNodes().substring(2)); if (response.getNumberOfNodes() >= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("ge(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() >= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("<=")) { int expected = Integer.parseInt(request.waitForNodes().substring(2)); if (response.getNumberOfNodes() <= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("le(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() <= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith(">")) { int expected = Integer.parseInt(request.waitForNodes().substring(1)); if (response.getNumberOfNodes() > expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("gt(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() > expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("<")) { int expected = Integer.parseInt(request.waitForNodes().substring(1)); if (response.getNumberOfNodes() < expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("lt(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() < expected) { waitForCounter++; } } else { int expected = Integer.parseInt(request.waitForNodes()); if (response.getNumberOfNodes() == expected) { waitForCounter++; } } } return waitForCounter == waitFor; }	nit pick - we can skip the == true i think :)
private boolean prepareResponse(final ClusterHealthRequest request, final ClusterHealthResponse response, ClusterState clusterState, final int waitFor) { int waitForCounter = 0; if (request.waitForStatus() != null && response.getStatus().value() <= request.waitForStatus().value()) { waitForCounter++; } if (request.waitForQuorumActive() == true && response.isQuorumActive() == true) { waitForCounter++; } if (request.waitForRelocatingShards() != -1 && response.getRelocatingShards() <= request.waitForRelocatingShards()) { waitForCounter++; } if (request.waitForActiveShards() != -1 && response.getActiveShards() >= request.waitForActiveShards()) { waitForCounter++; } if (request.indices().length > 0) { try { clusterState.metaData().concreteIndices(IndicesOptions.strictExpand(), request.indices()); waitForCounter++; } catch (IndexMissingException e) { response.status = ClusterHealthStatus.RED; // no indices, make sure its RED // missing indices, wait a bit more... } } if (!request.waitForNodes().isEmpty()) { if (request.waitForNodes().startsWith(">=")) { int expected = Integer.parseInt(request.waitForNodes().substring(2)); if (response.getNumberOfNodes() >= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("ge(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() >= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("<=")) { int expected = Integer.parseInt(request.waitForNodes().substring(2)); if (response.getNumberOfNodes() <= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("le(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() <= expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith(">")) { int expected = Integer.parseInt(request.waitForNodes().substring(1)); if (response.getNumberOfNodes() > expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("gt(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() > expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("<")) { int expected = Integer.parseInt(request.waitForNodes().substring(1)); if (response.getNumberOfNodes() < expected) { waitForCounter++; } } else if (request.waitForNodes().startsWith("lt(")) { int expected = Integer.parseInt(request.waitForNodes().substring(3, request.waitForNodes().length() - 1)); if (response.getNumberOfNodes() < expected) { waitForCounter++; } } else { int expected = Integer.parseInt(request.waitForNodes()); if (response.getNumberOfNodes() == expected) { waitForCounter++; } } } return waitForCounter == waitFor; }	unrelated but can we add an assertion at the bottom of this method to ensure waitforcounter <= waitfor
public boolean isMet(int total, int active) { return QUORUM.isMet(total, active); } }, ONE() { @Override public boolean isMet(int total, int active) { return active >= 1; } }, QUORUM() { @Override public boolean isMet(int total, int active) { if (total > 2) { return active >= ((total / 2) + 1); } else { return active >= 1; } } }, ALL_MINUS_1() { @Override public boolean isMet(int total, int active) { if (total > 1) { return active >= (total - 1); } else { return active == 1; } } }, ALL() { @Override public boolean isMet(int total, int active) { return active >= total; } }; /** * Is the consistency level met from the total elements and the current "active" elements. */ public abstract boolean isMet(int total, int active); public static ConsistencyLevel readFrom(StreamInput in) throws IOException { return ConsistencyLevel.values()[(int) in.readByte()]; } public void writeTo(StreamOutput out) throws IOException { out.writeByte((byte) ordinal()); } public static ConsistencyLevel fromString(String value) { if (value.equals("default")) { return DEFAULT; } else if (value.equals("one")) { return ONE; } else if (value.equals("quorum")) { return QUORUM; } else if (value.equals("all-1") || value.equals("full-1")) { return ALL_MINUS_1; } else if (value.equals("all") || value.equals("full")) { return ALL; }	can we please only have one value for this. there is no need to have more than one imo
public void testFromXContent() throws IOException { ToXContent.Params params = new ToXContent.MapParams(Collections.singletonMap(RestSearchAction.TYPED_KEYS_PARAM, "true")); Suggest suggest = createTestItem(); XContentType xContentType = randomFrom(XContentType.values()); boolean humanReadable = randomBoolean(); BytesReference originalBytes = toShuffledXContent(suggest, xContentType, params, humanReadable); Suggest parsed; try (XContentParser parser = createParser(xContentType.xContent(), originalBytes)) { ensureExpectedToken(XContentParser.Token.START_OBJECT, parser.nextToken(), parser::getTokenLocation); ensureFieldName(parser, parser.nextToken(), Suggest.NAME); ensureExpectedToken(XContentParser.Token.START_OBJECT, parser.nextToken(), parser::getTokenLocation); parsed = Suggest.fromXContent(parser); assertEquals(XContentParser.Token.END_OBJECT, parser.currentToken()); assertEquals(XContentParser.Token.END_OBJECT, parser.nextToken()); assertNull(parser.nextToken()); } assertEquals(suggest.size(), parsed.size()); for (Suggestion suggestion : suggest) { Suggestion<? extends Entry<? extends Option>> parsedSuggestion = parsed.getSuggestion(suggestion.getName()); assertNotNull(parsedSuggestion); assertEquals(suggestion.getClass(), parsedSuggestion.getClass()); } assertToXContentEquivalent(originalBytes, toXContent(parsed, xContentType, params, humanReadable), xContentType); }	note: this test doesn't need any field randomization i think. the suggest element is a object containing names suggestions, which we already test elsewhere. i think it is not possible that we add anything other inside this object unless we change the whole structure of the xcontent (which is another story).
private void doTestFromXContent(boolean addRandomFields) throws IOException { ToXContent.Params params = new ToXContent.MapParams(Collections.singletonMap(RestSearchAction.TYPED_KEYS_PARAM, "true")); for (Class<Suggestion<? extends Entry<? extends Option>>> type : SUGGESTION_TYPES) { Suggestion suggestion = createTestItem(type); XContentType xContentType = randomFrom(XContentType.values()); boolean humanReadable = randomBoolean(); BytesReference originalBytes = toShuffledXContent(suggestion, xContentType, params, humanReadable); BytesReference mutated; if (addRandomFields) { // - "contexts" is an object consisting of key/array pairs, we shouldn't add anything random there // - there can be inner search hits fields inside this option where we cannot add random stuff // - the root object should be excluded since it contains the named suggestion arrays Predicate<String> excludeFilter = path -> (path.isEmpty() || path.endsWith(CompletionSuggestion.Entry.Option.CONTEXTS.getPreferredName()) || path.endsWith("highlight") || path.endsWith("fields") || path.contains("_source") || path.contains("inner_hits")); mutated = insertRandomFields(xContentType, originalBytes, excludeFilter, random()); } else { mutated = originalBytes; } Suggestion parsed; try (XContentParser parser = createParser(xContentType.xContent(), mutated)) { ensureExpectedToken(XContentParser.Token.START_OBJECT, parser.nextToken(), parser::getTokenLocation); ensureExpectedToken(XContentParser.Token.FIELD_NAME, parser.nextToken(), parser::getTokenLocation); parsed = Suggestion.fromXContent(parser); assertEquals(XContentParser.Token.END_OBJECT, parser.nextToken()); assertNull(parser.nextToken()); } assertEquals(suggestion.getName(), parsed.getName()); assertEquals(suggestion.getEntries().size(), parsed.getEntries().size()); // We don't parse size via xContent, instead we set it to -1 on the client side assertEquals(-1, parsed.getSize()); assertToXContentEquivalent(originalBytes, toXContent(parsed, xContentType, params, humanReadable), xContentType); } }	thanks for the comments
@Override public boolean equals(Object other) { if (this == other) { return true; } if (other == null || getClass() != other.getClass()) { return false; } final CuckooFilter that = (CuckooFilter) other; return Objects.equals(this.numBuckets, that.numBuckets) && Objects.equals(this.bitsPerEntry, that.bitsPerEntry) && Objects.equals(this.entriesPerBucket, that.entriesPerBucket) && Objects.equals(this.count, that.count) && Objects.equals(this.evictedFingerprint, that.evictedFingerprint); }	should we add this to the murmurhash3 class?
@TestLogging(reason = "testing logging at levels DEBUG and above", value="org.elasticsearch.discovery:DEBUG") public void testLogsWarningsIfActiveForLongEnough() throws IllegalAccessException { final DiscoveryNode otherNode = newDiscoveryNode("node-from-hosts-list"); providedAddresses.add(otherNode.getAddress()); transportAddressConnector.unreachableAddresses.add(otherNode.getAddress()); peerFinder.activate(lastAcceptedNodes); final long endTime = deterministicTaskQueue.getCurrentTimeMillis() + VERBOSITY_INCREASE_TIMEOUT_SETTING.get(Settings.EMPTY).millis(); MockLogAppender appender = new MockLogAppender(); try { appender.start(); Loggers.addAppender(LogManager.getLogger("org.elasticsearch.discovery.PeerFinder"), appender); appender.addExpectation(new MockLogAppender.SeenEventExpectation( "connection failed", "org.elasticsearch.discovery.PeerFinder", Level.DEBUG, "*connection failed*")); deterministicTaskQueue.advanceTime(); runAllRunnableTasks(); appender.assertAllExpectationsMatched(); appender.addExpectation(new MockLogAppender.SeenEventExpectation( "connection failed", "org.elasticsearch.discovery.PeerFinder", Level.WARN, "*connection failed: cannot connect to*")); while (deterministicTaskQueue.getCurrentTimeMillis() <= endTime) { deterministicTaskQueue.advanceTime(); runAllRunnableTasks(); } appender.assertAllExpectationsMatched(); } finally { Loggers.removeAppender(LogManager.getLogger("org.elasticsearch.discovery.PeerFinder"), appender); appender.stop(); } }	nit: now that we validate the message, it would be nice to show that it contains both the transport address and the exception message cannot connect to.
@TestLogging(reason = "testing logging at levels DEBUG and above", value="org.elasticsearch.discovery:DEBUG") public void testLogsWarningsIfActiveForLongEnough() throws IllegalAccessException { final DiscoveryNode otherNode = newDiscoveryNode("node-from-hosts-list"); providedAddresses.add(otherNode.getAddress()); transportAddressConnector.unreachableAddresses.add(otherNode.getAddress()); peerFinder.activate(lastAcceptedNodes); final long endTime = deterministicTaskQueue.getCurrentTimeMillis() + VERBOSITY_INCREASE_TIMEOUT_SETTING.get(Settings.EMPTY).millis(); MockLogAppender appender = new MockLogAppender(); try { appender.start(); Loggers.addAppender(LogManager.getLogger("org.elasticsearch.discovery.PeerFinder"), appender); appender.addExpectation(new MockLogAppender.SeenEventExpectation( "connection failed", "org.elasticsearch.discovery.PeerFinder", Level.DEBUG, "*connection failed*")); deterministicTaskQueue.advanceTime(); runAllRunnableTasks(); appender.assertAllExpectationsMatched(); appender.addExpectation(new MockLogAppender.SeenEventExpectation( "connection failed", "org.elasticsearch.discovery.PeerFinder", Level.WARN, "*connection failed: cannot connect to*")); while (deterministicTaskQueue.getCurrentTimeMillis() <= endTime) { deterministicTaskQueue.advanceTime(); runAllRunnableTasks(); } appender.assertAllExpectationsMatched(); } finally { Loggers.removeAppender(LogManager.getLogger("org.elasticsearch.discovery.PeerFinder"), appender); appender.stop(); } }	nit: now that we validate the message, it would be nice to show that it contains the transport address.
protected void assertIndexDirectoryDeleted(final String nodeName, final String indexName) throws Exception { assertBusy(new Runnable() { @Override public void run() { logger.info("checking if meta state exists..."); try { assertFalse("Expecting index directory of " + indexName + " to be deleted from node " + nodeName, indexDirectoryExists(nodeName, indexName)); } catch (Exception e) { logger.info("failed to check for data director of index {} on node {}", indexName, nodeName); fail("could not check if data directory still exists"); } } } ); }	can you simplify this with a java8 lambda!
protected void assertIndexDirectoryDeleted(final String nodeName, final String indexName) throws Exception { assertBusy(new Runnable() { @Override public void run() { logger.info("checking if meta state exists..."); try { assertFalse("Expecting index directory of " + indexName + " to be deleted from node " + nodeName, indexDirectoryExists(nodeName, indexName)); } catch (Exception e) { logger.info("failed to check for data director of index {} on node {}", indexName, nodeName); fail("could not check if data directory still exists"); } } } ); }	why do we have the catch blocks here? assertbusy does this if it fails as well
private String unsupported(String query, String supported) { String thisField = "[" + name() + "] which is of type [runtime] with runtime_type [" + runtimeType() + "]"; return "Can only use " + query + " queries on " + supported + " fields - not on " + thisField; }	this is something i bumped into while working on this. just had the old name left over.
private void assertQueryOnlyOnTextAndKeyword(String queryName, ThrowingRunnable buildQuery) { Exception e = expectThrows(IllegalArgumentException.class, buildQuery); assertThat( e.getMessage(), equalTo( "Can only use " + queryName + " queries on keyword and text fields - not on [test] which is of type [runtime] with runtime_type [" + runtimeType() + "]" ) ); }	this is the other half of the "old name" bug i fixed way above.
default Map<String, Mapper.TypeParser> getMappers() { return Collections.emptyMap(); } /** * Returns additional metadata mapper implementations added by this plugin. * * The key of the returned {@link Map} is the unique name for the metadata mapper, which * is used in the mapping json to configure the metadata mapper, and the value is a * {@link MetadataFieldMapper.TypeParser} to parse the mapper settings into a * {@link MetadataFieldMapper}	maybe add "and should return true if the to show the field and false to hide it".
@Override protected void doSaveState(IndexerState indexerState, Map<String, Object> position, Runnable next) { if (indexerState.equals(IndexerState.ABORTING)) { // If we're aborting, just invoke `next` (which is likely an onFailure handler) next.run(); return; } if(indexerState.equals(IndexerState.STARTED) && getStats().getNumDocuments() > 0) { // if the indexer resets the state to started, it means it is done with a run through the data. // But, if there were no documents, we should allow it to attempt to gather more again, as there is no risk of overwriting // Some reasons for no documents are (but is not limited to): // * Could have failed early on search or index // * Have an empty index // * Have a query that returns no documents generation.compareAndSet(0L, 1L); } final DataFrameTransformState state = new DataFrameTransformState( taskState.get(), indexerState, getPosition(), generation.get(), stateReason.get()); logger.info("Updating persistent state of transform [" + transform.getId() + "] to [" + state.toString() + "]"); persistStateToClusterState(state, ActionListener.wrap(t -> next.run(), e -> next.run())); }	could you add a comment what this is doing? this might solve a problem that has a different root cause: i guess this is about short firing repeated failure messages, but as written above the problem is the scheduling logic. it's good to have this for now, but we should revisit once we fix the scheduler.
@Override public String toString() { return Strings.toString(this); }	in every aggregationbuilder? i would also be fine removing these todos given that we are planning to address them next.
public void apply(Project project) { if (project != project.getRootProject()) { throw new IllegalStateException(this.getClass().getName() + " can only be applied to the root project."); } GlobalInfoExtension extension = project.getExtensions().create(GLOBAL_INFO_EXTENSION_NAME, GlobalInfoExtension.class); JavaVersion minimumCompilerVersion = JavaVersion.toVersion(getResourceContents("/minimumCompilerVersion")); JavaVersion minimumRuntimeVersion = JavaVersion.toVersion(getResourceContents("/minimumRuntimeVersion")); File compilerJavaHome = findCompilerJavaHome(); File runtimeJavaHome = findRuntimeJavaHome(compilerJavaHome); final List<JavaHome> javaVersions = new ArrayList<>(); for (int version = 8; version <= Integer.parseInt(minimumCompilerVersion.getMajorVersion()); version++) { if (System.getenv(getJavaHomeEnvVarName(Integer.toString(version))) != null) { javaVersions.add(JavaHome.of(version, new File(findJavaHome(Integer.toString(version))))); } } GenerateGlobalBuildInfoTask generateTask = project.getTasks().create("generateGlobalBuildInfo", GenerateGlobalBuildInfoTask.class, task -> { task.setJavaVersions(javaVersions); task.setMinimumCompilerVersion(minimumCompilerVersion); task.setMinimumRuntimeVersion(minimumRuntimeVersion); task.setCompilerJavaHome(compilerJavaHome); task.setRuntimeJavaHome(runtimeJavaHome); task.getOutputFile().set(new File(project.getBuildDir(), "global-build-info")); task.getCompilerVersionFile().set(new File(project.getBuildDir(), "java-compiler-version")); task.getRuntimeVersionFile().set(new File(project.getBuildDir(), "java-runtime-version")); task.getFipsJvmFile().set(new File(project.getBuildDir(), "in-fips-jvm")); }); PrintGlobalBuildInfoTask printTask = project.getTasks().create("printGlobalBuildInfo", PrintGlobalBuildInfoTask.class, task -> { task.getBuildInfoFile().set(generateTask.getOutputFile()); task.getCompilerVersionFile().set(generateTask.getCompilerVersionFile()); task.getRuntimeVersionFile().set(generateTask.getRuntimeVersionFile()); task.getFipsJvmFile().set(generateTask.getFipsJvmFile()); task.setGlobalInfoListeners(extension.listeners); }); project.getExtensions().getByType(ExtraPropertiesExtension.class).set("defaultParallel", findDefaultParallel(project)); project.allprojects(p -> { // Make sure than any task execution generates and prints build info p.getTasks().all(task -> { if (task != generateTask && task != printTask) { task.dependsOn(printTask); } }); ExtraPropertiesExtension ext = p.getExtensions().getByType(ExtraPropertiesExtension.class); ext.set("compilerJavaHome", compilerJavaHome); ext.set("runtimeJavaHome", runtimeJavaHome); ext.set("isRuntimeJavaHomeSet", compilerJavaHome.equals(runtimeJavaHome) == false); ext.set("javaVersions", javaVersions); ext.set("minimumCompilerVersion", minimumCompilerVersion); ext.set("minimumRuntimeVersion", minimumRuntimeVersion); ext.set("gradleJavaVersion", Jvm.current().getJavaVersion()); }); }	these shouldn't really be configurable either.
@Override public BinaryFieldMapper build(MapperBuilderContext context) { return new BinaryFieldMapper( name, new BinaryFieldType(context.buildFullName(name), stored.getValue(), hasDocValues.getValue(), meta.getValue()), multiFieldsBuilder.build(this, context), copyTo.build(), this ); } } public static final TypeParser PARSER = new TypeParser((n, c) -> new Builder(n)); public static final class BinaryFieldType extends MappedFieldType { private BinaryFieldType(String name, boolean isStored, boolean hasDocValues, Map<String, String> meta) { super(name, false, isStored, hasDocValues, TextSearchInfo.NONE, meta); } public BinaryFieldType(String name) { this(name, false, true, Collections.emptyMap()); } @Override public String typeName() { return CONTENT_TYPE; } @Override public ValueFetcher valueFetcher(SearchExecutionContext context, String format) { return SourceValueFetcher.identity(name(), context, format); } @Override public DocValueFormat docValueFormat(String format, ZoneId timeZone) { return DocValueFormat.BINARY; } @Override public BytesReference valueForDisplay(Object value) { if (value == null) { return null; } BytesReference bytes; if (value instanceof BytesRef) { bytes = new BytesArray((BytesRef) value); } else if (value instanceof BytesReference) { bytes = (BytesReference) value; } else if (value instanceof byte[]) { bytes = new BytesArray((byte[]) value); } else { bytes = new BytesArray(Base64.getDecoder().decode(value.toString())); } return bytes; } @Override public IndexFieldData.Builder fielddataBuilder(String fullyQualifiedIndexName, Supplier<SearchLookup> searchLookup) { failIfNoDocValues(); return new BytesBinaryIndexFieldData.Builder(name(), CoreValuesSourceType.KEYWORD); } @Override public Query termQuery(Object value, SearchExecutionContext context) { throw new IllegalArgumentException("Binary fields do not support searching"); } } private final boolean stored; private final boolean hasDocValues; protected BinaryFieldMapper( String simpleName, MappedFieldType mappedFieldType, MultiFields multiFields, CopyTo copyTo, Builder builder ) { super(simpleName, mappedFieldType, multiFields, copyTo); this.stored = builder.stored.getValue(); this.hasDocValues = builder.hasDocValues.getValue(); } @Override protected void parseCreateField(DocumentParserContext context) throws IOException { if (stored == false && hasDocValues == false) { return; } if (context.parser().currentToken() == XContentParser.Token.VALUE_NULL) { return; } indexValue(context, context.parser().binaryValue()); } public void indexValue(DocumentParserContext context, byte[] value) { if (value == null) { return; } if (stored) { context.doc().add(new StoredField(fieldType().name(), value)); } if (hasDocValues) { CustomBinaryDocValuesField field = (CustomBinaryDocValuesField) context.doc().getByKey(fieldType().name()); if (field == null) { field = new CustomBinaryDocValuesField(fieldType().name(), value); context.doc().addWithKey(fieldType().name(), field); } else { field.add(value); } } else { // Only add an entry to the field names field if the field is stored // but has no doc values so exists query will work on a field with // no doc values context.addToFieldNames(fieldType().name()); } } @Override public FieldMapper.Builder getMergeBuilder() { return new BinaryFieldMapper.Builder(simpleName()).init(this); } @Override protected String contentType() { return CONTENT_TYPE; } public static class CustomBinaryDocValuesField extends CustomDocValuesField { private final ObjectArrayList<byte[]> bytesList; private int totalSize = 0; public CustomBinaryDocValuesField(String name, byte[] bytes) { super(name); bytesList = new ObjectArrayList<>(); add(bytes); } public void add(byte[] bytes) { bytesList.add(bytes); totalSize += bytes.length; } @Override public BytesRef binaryValue() { try { CollectionUtils.sortAndDedup(bytesList); int size = bytesList.size(); BytesStreamOutput out = new BytesStreamOutput(totalSize + (size + 1) * 5); out.writeVInt(size); // write total number of values for (int i = 0; i < size; i++) { final byte[] value = bytesList.get(i); int valueLength = value.length; out.writeVInt(valueLength); out.writeBytes(value, 0, valueLength); } return out.bytes().toBytesRef(); } catch (IOException e) { throw new ElasticsearchException("Failed to get binary value", e); } } @Override public String toString() { return "<" + name() + ":" + binaryValue() + ">"; }	is there a specific reason to implement tostring() here? i quickly checked on keywordfieldmapper and numberfieldmapper and didn't find an implementation there either, just curious if this is something we generally would want to do or of this here is on special purpose.
public final void testCopyTo() throws IOException { DocumentMapper mapper = createDocumentMapper(mapping(b -> { b.startObject("field"); minimalMapping(b); b.field("copy_to", "copy_field"); b.endObject(); b.startObject("copy_field"); minimalMapping(b); b.endObject(); })); if (supportsCopyTo()) { ParsedDocument doc = mapper.parse(source(this::writeCopyField)); IndexableField[] source = doc.rootDoc().getFields("field"); IndexableField[] copy = doc.rootDoc().getFields("copy_field"); String sourceToString = Arrays.toString(source); String expected = sourceToString.replaceAll("<field:", "<copy_field:"); assertEquals(expected, Arrays.toString(copy)); assertParseMinimalWarnings(); } else { assertWarnings( true, new DeprecationWarning( Level.WARN, "Field [field] is configured with a [copy_to] setting which is unsupported and will be ignored" ) ); } }	is this the reason why tostring was added to some mappers in this pr previously? it's short but i wonder if this could be tested without relying on some tostring implementation in that case.
this.onClose = null; synchronized (openChannels) { openChannels.add(this); } } public void accept(Executor executor) throws IOException { while (isOpen.get()) { Socket incomingSocket = serverSocket.accept(); MockChannel incomingChannel = null; try { configureSocket(incomingSocket); incomingSocket.setSoLinger(true, 0); synchronized (this) { if (isOpen.get()) { incomingChannel = new MockChannel(incomingSocket, new InetSocketAddress(incomingSocket.getLocalAddress(), incomingSocket.getPort()), profile, workerChannels::remove); //establish a happens-before edge between closing and accepting a new connection workerChannels.add(incomingChannel); // this spawns a new thread immediately, so OK under lock incomingChannel.loopRead(executor); // the channel is properly registered and will be cleared by the close code. incomingSocket = null; incomingChannel = null; } } } finally { // ensure we don't leak sockets and channels in the failure case. Note that we null both // if there are no exceptions so this becomes a no op. IOUtils.closeWhileHandlingException(incomingSocket, incomingChannel); } }	i am hesitating to set a 0 timeout for so_linger the reason is that the abnormal termination of a socket that causes it to move into time_wait can be caused due to an exception ie. during request parsing on the server then the server closes the connection and in this case we should really stay in time_wait. the other case where the server closes the connection instead of the client is when we shutdown and then we can set our socket into so_linger=0 since it's the right thing todo in this situation. i actually think we should do this in a broader scope. there is for instance a method tcptransport#closechannels(list<channels> channels, boolean blocking) i wonder if we can piggyback on this boolean and set so_linger to true,0 if we close our transport. it might even be a better idea to add another boolean boolean closingtransport in this case we can do the right thing also in other transport impls?
public long getShardSize(ShardRouting shardRouting, long fallback) { final Long shardSize = getShardSize(shardRouting); assert shardSize != null : "no shard size provided for " + shardRouting; return shardSize == ShardRouting.UNAVAILABLE_EXPECTED_SHARD_SIZE ? fallback : shardSize; }	maybe rename this to getfetchedshardsize? i am not too fond of having two methods with same name and a fallback argument that behaves differently (thinking of the != null assertion meaning this is only legal to call when size has been fetched).
public ActionRequestValidationException validate() { ActionRequestValidationException validationException = null; if (Strings.isNullOrEmpty(spEntityId)) { validationException = addValidationError("entity_id is missing", validationException); } if (Strings.isNullOrEmpty(assertionConsumerService)) { validationException = addValidationError("acs is missing", validationException); } if (samlAuthenticationState != null) { final ValidationException authnStateException = samlAuthenticationState.validate(); if (validationException != null) { ActionRequestValidationException actionRequestValidationException = new ActionRequestValidationException(); actionRequestValidationException.addValidationErrors(authnStateException.validationErrors()); validationException = addValidationError("entity_id is missing", actionRequestValidationException); } } return validationException; }	why do we make acs mandatory, even for idp initiated sso for registered sps?
protected void doExecute(Task task, SamlInitiateSingleSignOnRequest request, ActionListener<SamlInitiateSingleSignOnResponse> listener) { final SamlAuthenticationState authenticationState = request.getSamlAuthenticationState(); identityProvider.getRegisteredServiceProvider( request.getSpEntityId(), request.getAssertionConsumerService(), false, ActionListener.wrap( sp -> { if (null == sp) { final String message = "Service Provider with Entity ID [" + request.getSpEntityId() + "] and ACS [" + request.getAssertionConsumerService() + "] is not registered with this Identity Provider"; logger.debug(message); possiblyReplyWithSamlFailure(authenticationState, StatusCode.RESPONDER, new IllegalArgumentException(message), listener); return; } final SecondaryAuthentication secondaryAuthentication = SecondaryAuthentication.readFromContext(securityContext); if (secondaryAuthentication == null) { possiblyReplyWithSamlFailure(authenticationState, StatusCode.REQUESTER, new ElasticsearchSecurityException("Request is missing secondary authentication", RestStatus.FORBIDDEN), listener); return; } buildUserFromAuthentication(secondaryAuthentication, sp, ActionListener.wrap( user -> { if (user == null) { possiblyReplyWithSamlFailure(authenticationState, StatusCode.REQUESTER, new ElasticsearchSecurityException("User [{}] is not permitted to access service [{}]", RestStatus.FORBIDDEN, secondaryAuthentication.getUser(), sp), listener); return; } final SuccessfulAuthenticationResponseMessageBuilder builder = new SuccessfulAuthenticationResponseMessageBuilder(samlFactory, Clock.systemUTC(), identityProvider); try { final Response response = builder.build(user, authenticationState); listener.onResponse(new SamlInitiateSingleSignOnResponse( user.getServiceProvider().getAssertionConsumerService().toString(), samlFactory.getXmlContent(response), user.getServiceProvider().getEntityId())); } catch (ElasticsearchException e) { listener.onFailure(e); } }, e -> possiblyReplyWithSamlFailure(authenticationState, StatusCode.RESPONDER, e, listener) )); }, e -> possiblyReplyWithSamlFailure(authenticationState, StatusCode.RESPONDER, e, listener) )); }	nit: maybe registered is not the right word anymore? "is not known to this identity provider" ?
private void getSpFromIssuer(Issuer issuer, String acs, ActionListener<SamlServiceProvider> listener) { if (issuer == null || issuer.getValue() == null) { throw new ElasticsearchSecurityException("SAML authentication request has no issuer", RestStatus.BAD_REQUEST); } final String issuerString = issuer.getValue(); idp.getRegisteredServiceProvider(issuerString, acs, false, ActionListener.wrap( serviceProvider -> { if (null == serviceProvider) { throw new ElasticsearchSecurityException( "Service Provider with Entity ID [{}] is not registered with this Identity Provider", RestStatus.BAD_REQUEST, issuerString); } listener.onResponse(serviceProvider); }, listener::onFailure )); }	i can't comment on the exception message, but as above > nit: maybe registered is not the right word anymore? "is not known to this identity provider" ?
private void resolveWildcardService(String entityId, String acs, ActionListener<SamlServiceProvider> listener) { if (acs == null) { listener.onResponse(null); } else { try { listener.onResponse(wildcardServiceResolver.resolve(entityId, acs)); } catch (Exception e) { listener.onFailure(e); } } }	log something here since we only trace log in the wildcardserviceresolver resolve and it won't be easily noticeble if the acs was null or we didn't have any matches ?
public void replaceMatch(String subKey, Object value) { transformations.add(new ReplaceMatch(subKey, MAPPER.convertValue(value, JsonNode.class))); }	can you add a gradle test for these 2 to yamlrestcompattestpluginfunctest / "transform task executes and works as configured" ? when updating these just add the task configuration to the build file, the thing you want to change to the test.yml. the test should fail and output the result. in the past i just copy/paste that output and ensure that the diff looks correct (e.g. the only thing that changed is what is expected).
public Request build() { if (datafeedBuilder != null) { datafeedBuilder.setId("preview_id"); if (datafeedBuilder.getJobId() == null && jobBuilder == null) { throw new IllegalArgumentException("[datafeed_config.job_id] must be set or a [job_config] must be provided"); } if (datafeedBuilder.getJobId() == null) { datafeedBuilder.setJobId("preview_job_id"); } } if (jobBuilder != null) { jobBuilder.setId("preview_job_id"); } if (datafeedId != null && (datafeedBuilder != null || jobBuilder != null)) { throw new IllegalArgumentException( "[datafeed_id] cannot be supplied when either [job_config] or [datafeed_config] is present" ); } return datafeedId != null ? new Request(datafeedId) : new Request(datafeedBuilder == null ? null : datafeedBuilder.build(), jobBuilder); } } } public static class Response extends ActionResponse implements ToXContentObject { private final BytesReference preview; public Response(StreamInput in) throws IOException { super(in); preview = in.readBytesReference(); } public Response(BytesReference preview) { this.preview = preview; } @Override public void writeTo(StreamOutput out) throws IOException { out.writeBytesReference(preview); } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { try (InputStream stream = preview.streamInput()) { builder.rawValue(stream, XContentType.JSON); } return builder; } @Override public int hashCode() { return Objects.hash(preview); } @Override public boolean equals(Object obj) { if (obj == null) { return false; } if (getClass() != obj.getClass()) { return false; } Response other = (Response) obj; return Objects.equals(preview, other.preview); }	is it valid to have a jobbuilder but no datafeedbuilder a null datafeedid? i.e. just the job config
protected void doExecute(Task task, PreviewDatafeedAction.Request request, ActionListener<PreviewDatafeedAction.Response> listener) { ActionListener<DatafeedConfig> datafeedConfigActionListener = ActionListener.wrap( datafeedConfig -> { if (request.getJobConfig() != null) { previewDatafeed(datafeedConfig, request.getJobConfig().build(new Date()), listener); return; } jobConfigProvider.getJob(datafeedConfig.getJobId(), ActionListener.wrap( jobBuilder -> previewDatafeed(datafeedConfig, jobBuilder.build(), listener), listener::onFailure)); }, listener::onFailure ); if (request.getDatafeedConfig() != null) { datafeedConfigActionListener.onResponse(request.getDatafeedConfig()); } else { datafeedConfigProvider.getDatafeedConfig( request.getDatafeedId(), ActionListener.wrap(builder -> datafeedConfigActionListener.onResponse(builder.build()), listener::onFailure)); } }	well if i'd known it was that simple...
@Override protected RestChannelConsumer prepareRequest(RestRequest restRequest, NodeClient client) throws IOException { String datafeedId = restRequest.param(DatafeedConfig.ID.getPreferredName(), null); PreviewDatafeedAction.Request request = restRequest.hasContent() ? PreviewDatafeedAction.Request.fromXContent(restRequest.contentParser()) : new PreviewDatafeedAction.Request(datafeedId); return channel -> client.execute(PreviewDatafeedAction.INSTANCE, request, new RestToXContentListener<>(channel)); }	the defaultvalue of null is unnecessary but i prefer this usage it makes the intent more explicit
private void markShardReassigned(String indexName, int shardId, Map<String, Set<Integer>> reassignments) { final boolean added = reassignments.computeIfAbsent(indexName, k -> new HashSet<>()).add(shardId); assert added; }	maybe add some message in case it occurs in tests?
public ClusterState execute(ClusterState currentState) { final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); final List<SnapshotsInProgress.Entry> updatedEntries = new ArrayList<>(snapshotsInProgress.entries()); boolean changed = false; final String localNodeId = currentState.nodes().getLocalNodeId(); final String repoName = cloneEntry.repository(); final ShardGenerations shardGenerations = repoData.shardGenerations(); for (int i = 0; i < updatedEntries.size(); i++) { final SnapshotsInProgress.Entry entry = updatedEntries.get(i); if (cloneEntry.repository().equals(entry.repository()) == false) { // different repo => just continue without modification continue; } if (cloneEntry.snapshot().getSnapshotId().equals(entry.snapshot().getSnapshotId())) { final ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clonesBuilder = ImmutableOpenMap.builder(); final boolean readyToExecute = currentState.custom( SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY).getEntries().stream() .noneMatch(e -> e.repository().equals(repoName) && e.state() == SnapshotDeletionsInProgress.State.STARTED); final InFlightShardSnapshotStates inFlightShardStates; if (readyToExecute) { inFlightShardStates = InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries()); } else { // no need to compute these, we'll mark all shards as queued anyway because we wait for the delete inFlightShardStates = null; } boolean queuedShards = false; for (Tuple<IndexId, Integer> count : counts) { for (int shardId = 0; shardId < count.v2(); shardId++) { final RepositoryShardId repoShardId = new RepositoryShardId(count.v1(), shardId); final String indexName = repoShardId.indexName(); if (readyToExecute == false || inFlightShardStates.isActive(indexName, shardId)) { clonesBuilder.put(repoShardId, ShardSnapshotStatus.UNASSIGNED_QUEUED); queuedShards = true; } else { clonesBuilder.put(repoShardId, new ShardSnapshotStatus(localNodeId, inFlightShardStates.generationForShard(repoShardId.index(), shardId, shardGenerations))); } } } updatedEntry = cloneEntry.withClones(clonesBuilder.build()); if (queuedShards) { // We queued up some shards based on the in-flight operations found in all snapshots for the current // repository, so in order to make sure we don't set a shard to QUEUED before (as in before it in the // `updatedEntries` list) one that is actively executing we just put it to the back of the list as if we had // just created the entry // TODO: If we could eventually drop the snapshot clone init phase we don't need this any longer updatedEntries.remove(i); updatedEntries.add(updatedEntry); } else { updatedEntries.set(i, updatedEntry); } changed = true; break; } } return updateWithSnapshots(currentState, changed ? SnapshotsInProgress.of(updatedEntries) : null, null); }	readytoexecute kind of lack of meaning, maybe startedsnapshotdeletion (+ anymatch)?
public DiscoveryNode(String nodeName, String nodeId, String hostName, String hostAddress, TransportAddress address, Map<String, String> attributes, Set<Role> roles, Version version) { if (nodeName != null) { this.nodeName = nodeName.intern(); } else { this.nodeName = ""; } this.nodeId = nodeId.intern(); this.hostName = hostName.intern(); this.hostAddress = hostAddress.intern(); this.address = address; if (version == null) { this.version = Version.CURRENT; } else { this.version = version; } this.attributes = Collections.unmodifiableMap(attributes); //verify that no node roles are being provided as attributes boolean assertEnabled = false; assert assertEnabled = true; if (assertEnabled) { for (Role role : Role.values()) { assert attributes.containsKey(role.getRoleName()) == false : "role name [" + role.getRoleName() + "] found in attributes"; } } Set<Role> rolesSet = Collections.unmodifiableSet(roles); this.roles = EnumSet.noneOf(Role.class); this.roles.addAll(rolesSet); } /** * The address that the node can be communicated with. */ public TransportAddress address() { return address; }	i think this collections.unmodifiableset(roles); is unnecessary and we can just do this.roles.addall(roles) ?
public void testPortLimit() { final NetworkService networkService = new NetworkService(Collections.emptyList()); final Transport transport = new MockNioTransport( Settings.EMPTY, Version.CURRENT, threadPool, networkService, PageCacheRecycler.NON_RECYCLING_INSTANCE, new NamedWriteableRegistry(Collections.emptyList()), new NoneCircuitBreakerService()) { @Override public BoundTransportAddress boundAddress() { return new BoundTransportAddress( new TransportAddress[]{new TransportAddress(InetAddress.getLoopbackAddress(), 9500)}, new TransportAddress(InetAddress.getLoopbackAddress(), 9500) ); } }; closeables.push(transport); final TransportService transportService = new TransportService(Settings.EMPTY, transport, threadPool, TransportService.NOOP_TRANSPORT_INTERCEPTOR, x -> null, null, Collections.emptySet()); closeables.push(transportService); final int limitPortCounts = randomIntBetween(1, 10); final List<TransportAddress> transportAddresses = SeedHostsResolver.resolveHostsLists( executorService, logger, Collections.singletonList("127.0.0.1"), limitPortCounts, transportService, TimeValue.timeValueSeconds(30)); assertThat(transportAddresses, hasSize(limitPortCounts)); final Set<Integer> ports = new HashSet<>(); for (final TransportAddress address : transportAddresses) { assertTrue(address.address().getAddress().isLoopbackAddress()); ports.add(address.getPort()); } assertThat(ports, equalTo(IntStream.range(9300, 9300 + limitPortCounts).mapToObj(m -> m).collect(Collectors.toSet()))); }	intellij prefers: suggestion assertthat(ports, equalto(intstream.range(9300, 9300 + limitportcounts).boxed().collect(collectors.toset())));
public void testRemovingLocalAddresses() { final NetworkService networkService = new NetworkService(Collections.emptyList()); final InetAddress loopbackAddress = InetAddress.getLoopbackAddress(); final Transport transport = new MockNioTransport( Settings.EMPTY, Version.CURRENT, threadPool, networkService, PageCacheRecycler.NON_RECYCLING_INSTANCE, new NamedWriteableRegistry(Collections.emptyList()), new NoneCircuitBreakerService()) { @Override public BoundTransportAddress boundAddress() { return new BoundTransportAddress( new TransportAddress[]{ new TransportAddress(loopbackAddress, 9300), new TransportAddress(loopbackAddress, 9301) }, new TransportAddress(loopbackAddress, 9302) ); } }; closeables.push(transport); final TransportService transportService = new TransportService(Settings.EMPTY, transport, threadPool, TransportService.NOOP_TRANSPORT_INTERCEPTOR, x -> null, null, Collections.emptySet()); closeables.push(transportService); final List<TransportAddress> transportAddresses = SeedHostsResolver.resolveHostsLists( executorService, logger, Collections.singletonList(NetworkAddress.format(loopbackAddress)), 10, transportService, TimeValue.timeValueSeconds(30)); assertThat(transportAddresses, hasSize(7)); final Set<Integer> ports = new HashSet<>(); for (final TransportAddress address : transportAddresses) { assertTrue(address.address().getAddress().isLoopbackAddress()); ports.add(address.getPort()); } assertThat(ports, equalTo(IntStream.range(9303, 9310).mapToObj(m -> m).collect(Collectors.toSet()))); }	intellij prefers: suggestion assertthat(ports, equalto(intstream.range(9303, 9310).boxed().collect(collectors.toset())));
public void testReadIdpMetadataFromHttps() throws Exception { final Path path = getDataPath("idp1.xml"); final String body = new String(Files.readAllBytes(path), StandardCharsets.UTF_8); final MockSecureSettings mockSecureSettings = new MockSecureSettings(); mockSecureSettings.setString("xpack.security.transport.ssl.secure_key_passphrase", "testnode"); final Settings settings = Settings.builder() .put("xpack.security.transport.ssl.key", getDataPath("/org/elasticsearch/xpack/security/transport/ssl/certs/simple/testnode.pem")) .put("xpack.security.transport.ssl.certificate", getDataPath("/org/elasticsearch/xpack/security/transport/ssl/certs/simple/testnode.crt")) .put("xpack.security.transport.ssl.certificate_authorities", getDataPath("/org/elasticsearch/xpack/security/transport/ssl/certs/simple/testnode.crt")) .put("path.home", createTempDir()) .setSecureSettings(mockSecureSettings) .build(); TestsSSLService sslService = new TestsSSLService(settings, TestEnvironment.newEnvironment(settings)); try (MockWebServer proxyServer = new MockWebServer(sslService.sslContext(settings.getByPrefix("xpack.security.transport.ssl.")), false)) { proxyServer.start(); proxyServer.enqueue(new MockResponse().setResponseCode(200).setBody(body).addHeader("Content-Type", "application/xml")); proxyServer.enqueue(new MockResponse().setResponseCode(200).setBody(body).addHeader("Content-Type", "application/xml")); assertEquals(0, proxyServer.requests().size()); Tuple<RealmConfig, SSLService> config = buildConfig("https://localhost:" + proxyServer.getPort()); logger.info("Settings\\\\n{}", config.v1().settings().toDelimitedString('\\\\n')); final ResourceWatcherService watcherService = mock(ResourceWatcherService.class); Tuple<AbstractReloadingMetadataResolver, Supplier<EntityDescriptor>> tuple = SamlRealm.initializeResolver(logger, config.v1(), config.v2(), watcherService); try { final int firstRequestCount = proxyServer.requests().size(); assertThat(firstRequestCount, greaterThanOrEqualTo(1)); assertIdp1MetadataParsedCorrectly(tuple.v2().get()); assertBusy(() -> assertThat(proxyServer.requests().size(), greaterThan(firstRequestCount))); } finally { tuple.v1().destroy(); } } }	doesn't really affect anything but we could use xpack.security.http key since the produced sslcontext is used in a mockwebserver
@Override protected Settings nodeSettings(int nodeOrdinal) { Settings baseSettings = super.nodeSettings(nodeOrdinal); Settings.Builder builder = Settings.builder().put(baseSettings); baseSettings.getByPrefix("xpack.security.transport.ssl.") .keySet() .forEach(k -> { String httpKey = "xpack.security.http.ssl." + k; String value = baseSettings.get("xpack.security.transport.ssl." + k); if (value != null) { builder.put(httpKey, baseSettings.get("xpack.security.transport.ssl." + k)); } }); MockSecureSettings secureSettings = (MockSecureSettings) builder.getSecureSettings(); for (String key : new HashSet<>(secureSettings.getSettingNames())) { SecureString value = secureSettings.getString(key); if (value == null) { try { if (key.startsWith("xpack.security.transport.ssl.")) { byte[] file = Streams.readAll(secureSettings.getFile(key)); secureSettings.setFile(key.replace("xpack.security.transport.ssl.", "xpack.security.http.ssl."), file); } } catch (IOException e) { throw new UncheckedIOException(e); } } else if (key.startsWith("xpack.security.transport.ssl.")) { secureSettings.setString(key.replace("xpack.security.transport.ssl.", "xpack.security.http.ssl."), value.toString()); } } return builder // invert the require auth settings .put("xpack.security.transport.ssl.client_authentication", SSLClientAuth.NONE) .put("xpack.security.http.ssl.enabled", true) .put("xpack.security.http.ssl.client_authentication", SSLClientAuth.REQUIRED) .build(); }	maybe move [tobytesarray](https://github.com/elastic/elasticsearch/blob/9e529d9d584ae6f6978ac09056b73d9ccdde4a3c/x-pack/plugin/security/src/test/java/org/elasticsearch/xpack/security/authc/esnative/tool/commandlinehttpclienttests.java#l102) to estestcase and use this instead so that we don't introduce bc dependencies again?
public void testHttps() throws Exception { Path trustedCertPath = getDataPath("/org/elasticsearch/xpack/security/keystore/truststore-testnode-only.crt"); Path certPath = getDataPath("/org/elasticsearch/xpack/security/keystore/testnode.crt"); Path keyPath = getDataPath("/org/elasticsearch/xpack/security/keystore/testnode.pem"); MockSecureSettings secureSettings = new MockSecureSettings(); Settings settings = Settings.builder() .put("xpack.http.ssl.certificate_authorities", trustedCertPath) .setSecureSettings(secureSettings) .build(); try (HttpClient client = new HttpClient(settings, new SSLService(settings, environment), null)) { secureSettings = new MockSecureSettings(); // We can't use the client created above for the server since it is only a truststore secureSettings.setString("xpack.security.transport.ssl.secure_key_passphrase", "testnode"); Settings settings2 = Settings.builder() .put("xpack.security.transport.ssl.key", keyPath) .put("xpack.security.transport.ssl.certificate", certPath) .setSecureSettings(secureSettings) .build(); TestsSSLService sslService = new TestsSSLService(settings2, environment); testSslMockWebserver(client, sslService.sslContext("xpack.security.transport.ssl"), false); } }	same as above, just for being easy to the eye use the http layer settings for the mockwebserver
public void testHttps() throws Exception { Path trustedCertPath = getDataPath("/org/elasticsearch/xpack/security/keystore/truststore-testnode-only.crt"); Path certPath = getDataPath("/org/elasticsearch/xpack/security/keystore/testnode.crt"); Path keyPath = getDataPath("/org/elasticsearch/xpack/security/keystore/testnode.pem"); MockSecureSettings secureSettings = new MockSecureSettings(); Settings settings = Settings.builder() .put("xpack.http.ssl.certificate_authorities", trustedCertPath) .setSecureSettings(secureSettings) .build(); try (HttpClient client = new HttpClient(settings, new SSLService(settings, environment), null)) { secureSettings = new MockSecureSettings(); // We can't use the client created above for the server since it is only a truststore secureSettings.setString("xpack.security.transport.ssl.secure_key_passphrase", "testnode"); Settings settings2 = Settings.builder() .put("xpack.security.transport.ssl.key", keyPath) .put("xpack.security.transport.ssl.certificate", certPath) .setSecureSettings(secureSettings) .build(); TestsSSLService sslService = new TestsSSLService(settings2, environment); testSslMockWebserver(client, sslService.sslContext("xpack.security.transport.ssl"), false); } }	same as above, just for being easy to the eye use the http layer settings for the mockwebserver
@Override public void authenticate(AuthenticationToken authToken, ActionListener<AuthenticationResult> listener) { assert delegatedRealms != null : "Realm has not been initialized correctly"; X509AuthenticationToken token = (X509AuthenticationToken)authToken; try { final BytesKey fingerprint = computeFingerprint(token.credentials()[0]); User user = cache.get(fingerprint); if (user != null) { if (delegatedRealms.hasDelegation()) { delegatedRealms.resolve(token.principal(), listener); } else { listener.onResponse(AuthenticationResult.success(user)); } } else if (isCertificateChainTrusted(trustManager, token, logger) == false) { listener.onResponse(AuthenticationResult.unsuccessful("Certificate for " + token.dn() + " is not trusted", null)); } else { final ActionListener<AuthenticationResult> cachingListener = ActionListener.wrap(result -> { if (result.isAuthenticated()) { try (ReleasableLock ignored = readLock.acquire()) { cache.put(fingerprint, result.getUser()); } } listener.onResponse(result); }, listener::onFailure); if (delegatedRealms.hasDelegation()) { delegatedRealms.resolve(token.principal(), cachingListener); } else { this.buildUser(token, cachingListener); } } } catch (CertificateEncodingException e) { listener.onResponse(AuthenticationResult.unsuccessful("Certificate for " + token.dn() + " has encoding issues", e)); } }	if the user is in the cache, why are we resolving anything?
@Override public void authenticate(AuthenticationToken authToken, ActionListener<AuthenticationResult> listener) { assert delegatedRealms != null : "Realm has not been initialized correctly"; X509AuthenticationToken token = (X509AuthenticationToken)authToken; try { final BytesKey fingerprint = computeFingerprint(token.credentials()[0]); User user = cache.get(fingerprint); if (user != null) { if (delegatedRealms.hasDelegation()) { delegatedRealms.resolve(token.principal(), listener); } else { listener.onResponse(AuthenticationResult.success(user)); } } else if (isCertificateChainTrusted(trustManager, token, logger) == false) { listener.onResponse(AuthenticationResult.unsuccessful("Certificate for " + token.dn() + " is not trusted", null)); } else { final ActionListener<AuthenticationResult> cachingListener = ActionListener.wrap(result -> { if (result.isAuthenticated()) { try (ReleasableLock ignored = readLock.acquire()) { cache.put(fingerprint, result.getUser()); } } listener.onResponse(result); }, listener::onFailure); if (delegatedRealms.hasDelegation()) { delegatedRealms.resolve(token.principal(), cachingListener); } else { this.buildUser(token, cachingListener); } } } catch (CertificateEncodingException e) { listener.onResponse(AuthenticationResult.unsuccessful("Certificate for " + token.dn() + " has encoding issues", e)); } }	hmm i don't like that we do not get the metadata from the pki realm when we use a delegating realm and we do not even attempt to map roles. there may be cases where a pki cert doesn't map to an ad/ldap user but role mapping is desired, so we now need two realms.
@Override public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) { Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern)); // Parse the wildcard, marking ? and * chars with a single invalid char // to create a simple finite automaton we can parse List<Automaton> automata = new ArrayList<>(); for (int i = 0; i < wildcardPattern.length();) { final int c = wildcardPattern.codePointAt(i); int length = Character.charCount(c); switch(c) { case WildcardQuery.WILDCARD_STRING: automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR)); break; case WildcardQuery.WILDCARD_CHAR: automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR)); break; case WildcardQuery.WILDCARD_ESCAPE: // add the next codepoint instead, if it exists if (i + length < wildcardPattern.length()) { final int nextChar = wildcardPattern.codePointAt(i + length); length += Character.charCount(nextChar); automata.add(Automata.makeChar(nextChar)); } else { automata.add(Automata.makeChar(c)); } break; default: automata.add(Automata.makeChar(c)); } i += length; } Automaton approxAutomata = Operations.concatenate(automata); return createApproxAndVerifyQuery(approxAutomata, dvAutomaton, wildcardPattern); }	i am not sure we should differentiate wildcard and regex, i think we should just ignore this setting for wildcard field.
@Override public Query wildcardQuery(String wildcardPattern, RewriteMethod method, QueryShardContext context) { Automaton dvAutomaton = WildcardQuery.toAutomaton(new Term(name(), wildcardPattern)); // Parse the wildcard, marking ? and * chars with a single invalid char // to create a simple finite automaton we can parse List<Automaton> automata = new ArrayList<>(); for (int i = 0; i < wildcardPattern.length();) { final int c = wildcardPattern.codePointAt(i); int length = Character.charCount(c); switch(c) { case WildcardQuery.WILDCARD_STRING: automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR)); break; case WildcardQuery.WILDCARD_CHAR: automata.add(Automata.makeChar(INVALID_AUTOMATON_CHAR)); break; case WildcardQuery.WILDCARD_ESCAPE: // add the next codepoint instead, if it exists if (i + length < wildcardPattern.length()) { final int nextChar = wildcardPattern.codePointAt(i + length); length += Character.charCount(nextChar); automata.add(Automata.makeChar(nextChar)); } else { automata.add(Automata.makeChar(c)); } break; default: automata.add(Automata.makeChar(c)); } i += length; } Automaton approxAutomata = Operations.concatenate(automata); return createApproxAndVerifyQuery(approxAutomata, dvAutomaton, wildcardPattern); }	this is not needed, the field is always indexed ?
private static long convertFromCalendarToUTC(long value, Calendar cal) { if (cal == null) { return value; } Calendar c = (Calendar) cal.clone(); c.setTimeInMillis(value); ZonedDateTime convertedDateTime = ZonedDateTime .ofInstant(c.toInstant(), c.getTimeZone().toZoneId()) .withZoneSameLocal(ZoneOffset.UTC); return convertedDateTime.toInstant().toEpochMilli(); }	just a more concise alternative suggestion: return randommillis - 2 + i (tho not perfectly equivalent).
private void doAuthenticateAndCache(UsernamePasswordToken token, ActionListener<AuthenticationResult> listener) { ActionListener<AuthenticationResult> wrapped = ActionListener.wrap((result) -> { Objects.requireNonNull(result, "AuthenticationResult cannot be null"); if (result.getStatus() == AuthenticationResult.Status.SUCCESS) { UserWithHash userWithHash = new UserWithHash(result.getUser(), token.credentials(), hasher); // it doesn't matter if we already computed it elsewhere cache.put(token.principal(), userWithHash); } listener.onResponse(result); }, listener::onFailure); doAuthenticate(token, wrapped); }	i feel like we should keep this stat as cache (or some other key), rather than removing it entirely. (a) it allows for a comparison between old & new (b) it is indicative of _usage_ in the sense of how many users have recently logged in.
@Override public void writeTo(StreamOutput out) throws IOException { out.writeString(targetAllocationID); out.writeVLong(primaryTerm); // TODO: Change after backport if (out.getVersion().onOrAfter(Version.V_8_0_0)) { out.writeBoolean(rerouteWasLocal); } request.writeTo(out); }	the confusing bit about this name is that this flag does not say whether the reroute action came from a local node, but whether the primary action was sent by a reroute phase on the local node. my suggestion was to mention primarydelegation here (which is the terminology we use when sending a primary action to a different node (otherwise, it's always local to the node where the reroute was executing). by default primarydelegation will be false (i.e. exactly the inverse of this flag here)
public abstract Aggregator subAggregator(String name); /** * Resolve the next step of the sort path as though this aggregation * supported sorting. This is usually the "first step" when resolving * a sort path because most aggs that support sorting their buckets * aren't valid in the middle of a sort path. * <p> * For example, the {@code terms} aggs supports sorting its buckets, but * that sort path itself can't contain a different {@code terms}	i'm not super comfortable with this name, but i think it'll fall away in a followup change. when i dig into that last instanceof in resolvetopmostaggregator.
public List<String> getWarnings() { List<String> warnings = new ArrayList<>(); for (Header header : response.getHeaders("Warning")) { String warning = header.getValue(); final Matcher matcher = WARNING_HEADER_PATTERN.matcher(warning); if (matcher.matches()) { warnings.add(matcher.group(1)); } else { warnings.add(warning); } } return warnings; }	i flipped this around because it lines up a bit better with the rest of our style.
public void testDeprecationWarnings() throws IOException { String chars = randomAsciiAlphanumOfLength(5); deprecationWarningTest(singletonList("poorly formatted " + chars), singletonList("poorly formatted " + chars)); deprecationWarningTest(singletonList(formatWarning(chars)), singletonList(chars)); deprecationWarningTest( Arrays.asList(formatWarning(chars), "another one", "and another"), Arrays.asList(chars, "another one", "and another")); }	maybe this can be static too, and i would rename to assertdeprecationwarnings or expectdeprecationwarnings
private void deprecationWarningTest(List<String> warningHeaderTexts, List<String> warningBodyTexts) throws IOException { String method = randomFrom(getHttpMethods()); Request request = new Request(method, "/200"); RequestOptions.Builder options = request.getOptions().toBuilder(); for (String warningHeaderText : warningHeaderTexts) { options.addHeader("Warning", warningHeaderText); } request.setOptions(options); Response response; if (strictDeprecationMode) { try { restClient.performRequest(request); fail("expected ResponseException"); return; } catch (ResponseException e) { assertThat(e.getMessage(), containsString("\\\\nWarnings: " + warningBodyTexts)); response = e.getResponse(); } } else { response = restClient.performRequest(request); } assertTrue(response.hasWarnings()); assertEquals(warningBodyTexts, response.getWarnings()); }	maybe clarify that we expect an exception due to a warning as strict deprecation mode is enabled?
private ActionListener<Response> createResponseListener(final PrimaryShardReference primaryShardReference) { return new ActionListener<Response>() { @Override public void onResponse(Response response) { if (syncGlobalCheckpointAfterOperation) { try { primaryShardReference.indexShard.maybeSyncGlobalCheckpoint("post-operation"); } catch (final Exception e) { logger.info("post-operation global checkpoint sync failed", e); // intentionally swallow, a missed global checkpoint sync should not fail this operation } } primaryShardReference.close(); // release shard operation lock before responding to caller setPhase(replicationTask, "finished"); try { channel.sendResponse(response); } catch (IOException e) { onFailure(e); } } @Override public void onFailure(Exception e) { primaryShardReference.close(); // release shard operation lock before responding to caller setPhase(replicationTask, "finished"); try { channel.sendResponse(e); } catch (IOException e1) { logger.warn("failed to send response", e); } } }; }	should we have special catch case for "shard closed"?
private void syncGlobalCheckpoints() { for (final IndexShard shard : this.shards.values()) { if (shard.routingEntry().active() && shard.routingEntry().primary()) { switch (shard.state()) { case CLOSED: case CREATED: case RECOVERING: case RELOCATED: continue; case POST_RECOVERY: assert false : "shard " + shard.shardId() + " is in post-recovery but marked as active"; continue; case STARTED: try { shard.acquirePrimaryOperationPermit( ActionListener.wrap( releasable -> { try (Releasable ignored = releasable) { shard.maybeSyncGlobalCheckpoint("background"); } }, e -> logger.info("failed to execute background global checkpoint sync", e)), ThreadPool.Names.SAME); } catch (final AlreadyClosedException | IndexShardClosedException e) { // the shard was closed concurrently, continue } continue; default: throw new IllegalStateException("unknown state [" + shard.state() + "]"); } } } }	just wondering if this could as well be an alreadyclosedexception | indexshardclosedexception and we should not log then.
public void writeTo(StreamOutput out) throws IOException { out.writeString(jobId); String[] groupsArray = groups == null ? null : groups.toArray(new String[0]); out.writeOptionalStringArray(groupsArray); out.writeOptionalString(description); out.writeBoolean(detectorUpdates != null); if (detectorUpdates != null) { out.writeList(detectorUpdates); } out.writeOptionalWriteable(modelPlotConfig); out.writeOptionalWriteable(analysisLimits); out.writeOptionalLong(renormalizationWindowDays); out.writeOptionalTimeValue(backgroundPersistInterval); out.writeOptionalLong(modelSnapshotRetentionDays); out.writeOptionalLong(dailyModelSnapshotRetentionAfterDays); out.writeOptionalLong(resultsRetentionDays); if (out.getVersion().onOrAfter(Version.CURRENT)) { // TODO: 7.15 out.writeOptionalLong(annotationsRetentionDays); } out.writeBoolean(categorizationFilters != null); if (categorizationFilters != null) { out.writeStringCollection(categorizationFilters); } out.writeOptionalWriteable(perPartitionCategorizationConfig); out.writeMap(customSettings); out.writeOptionalString(modelSnapshotId); if (jobVersion != null) { out.writeBoolean(true); Version.writeVersion(jobVersion, out); } else { out.writeBoolean(false); } out.writeOptionalBoolean(clearJobFinishTime); if (modelSnapshotMinVersion != null) { out.writeBoolean(true); Version.writeVersion(modelSnapshotMinVersion, out); } else { out.writeBoolean(false); } out.writeOptionalBoolean(allowLazyOpen); out.writeOptionalWriteable(blocked); }	i think all these wire writes should be writeoptionalvlong as annotationsretentiondays is always >= 0
boolean isNoop(Job job) { return (groups == null || Objects.equals(groups, job.getGroups())) && (description == null || Objects.equals(description, job.getDescription())) && (modelPlotConfig == null || Objects.equals(modelPlotConfig, job.getModelPlotConfig())) && (analysisLimits == null || Objects.equals(analysisLimits, job.getAnalysisLimits())) && updatesDetectors(job) == false && (renormalizationWindowDays == null || Objects.equals(renormalizationWindowDays, job.getRenormalizationWindowDays())) && (backgroundPersistInterval == null || Objects.equals(backgroundPersistInterval, job.getBackgroundPersistInterval())) && (modelSnapshotRetentionDays == null || Objects.equals(modelSnapshotRetentionDays, job.getModelSnapshotRetentionDays())) && (dailyModelSnapshotRetentionAfterDays == null || Objects.equals(dailyModelSnapshotRetentionAfterDays, job.getDailyModelSnapshotRetentionAfterDays())) && (resultsRetentionDays == null || Objects.equals(resultsRetentionDays, job.getResultsRetentionDays())) && (annotationsRetentionDays == null || Objects.equals(annotationsRetentionDays, job.getAnnotationsRetentionDays())) && (categorizationFilters == null || Objects.equals(categorizationFilters, job.getAnalysisConfig().getCategorizationFilters())) && (perPartitionCategorizationConfig == null || Objects.equals(perPartitionCategorizationConfig, job.getAnalysisConfig().getPerPartitionCategorizationConfig())) && (customSettings == null || Objects.equals(customSettings, job.getCustomSettings())) && (modelSnapshotId == null || Objects.equals(modelSnapshotId, job.getModelSnapshotId())) && (modelSnapshotMinVersion == null || Objects.equals(modelSnapshotMinVersion, job.getModelSnapshotMinVersion())) && (jobVersion == null || Objects.equals(jobVersion, job.getJobVersion())) && (clearJobFinishTime == null || clearJobFinishTime == false || job.getFinishedTime() == null) && (allowLazyOpen == null || Objects.equals(allowLazyOpen, job.allowLazyOpen())) && (blocked == null || Objects.equals(blocked, job.getBlocked())); }	i suppose resultsretentiondays has the same issue, so it may not be fixable in this pr. but this prevents annotationsretentiondays from ever being unset once it is set.
static DeprecationIssue oldIndicesCheck(IndexMetaData indexMetaData) { Version createdWith = indexMetaData.getCreationVersion(); boolean hasDefaultMapping = indexMetaData.getMappings().containsKey(DEFAULT_MAPPING); int mappingCount = indexMetaData.getMappings().size(); if (createdWith.before(Version.V_6_0_0)) { if (".tasks".equals(indexMetaData.getIndex().getName())) { return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, ".tasks index must be re-created", "https://www.elastic.co/guide/en/elasticsearch/reference/master/removal-of-types.html" + "#_indices_created_before_7_0", "The .tasks index was created before version 6.0 and cannot be opened in 7.0. " + "You must delete this index and allow it to be re-created by Elasticsearch. If you wish to preserve task history, "+ "reindex this index to a new index before deleting it."); } if ((mappingCount == 2 && !hasDefaultMapping) || mappingCount > 2) { return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, "Index has more than one mapping type", "https://www.elastic.co/guide/en/elasticsearch/reference/master/removal-of-types.html" + "#_migrating_multi_type_indices_to_single_type", "This index has more than one mapping type, which is not supported in 7.0. " + "This index must be reindexed into one or more single-type indices. Mapping types in use: " + indexMetaData.getMappings().keys()); } else { return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, "Index created before 6.0", "https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-7.0.html" + "#_indices_created_before_7_0", "This index was created using version: " + createdWith); } } return null; }	i think this link should be : https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-7.0.html#_indices_created_before_7_0
@Override public Query toQuery(QueryParseContext parseContext) { validate(); Query query = null; MapperService.SmartNameFieldMappers smartNameFieldMappers = parseContext.smartFieldMappers(this.fieldName); if (smartNameFieldMappers != null && smartNameFieldMappers.hasMapper()) { query = smartNameFieldMappers.mapper().termQuery(this.value, parseContext); } if (query == null) { query = new TermQuery(new Term(this.fieldName, BytesRefs.toBytesRef(this.value))); } query.setBoost(this.boost); if (this.queryName != null) { parseContext.addNamedQuery(queryName, query); } return query; }	i think we have to check if these are empty too and barf in that case?
public void testSnapshotFilesThatFailToDownloadAreSentFromSource() throws Exception { try (Store store = newStore(createTempDir("source"), false)) { IndexShard shard = mock(IndexShard.class); when(shard.store()).thenReturn(store); when(shard.state()).thenReturn(IndexShardState.STARTED); final ShardRecoveryPlan shardRecoveryPlan = createShardRecoveryPlan(store, randomIntBetween(10, 20), randomIntBetween(10, 20)); final ShardRecoveryPlan.SnapshotFilesToRecover snapshotFilesToRecover = shardRecoveryPlan.getSnapshotFilesToRecover(); final List<String> fileNamesToBeRecoveredFromSnapshot = snapshotFilesToRecover.getSnapshotFiles() .stream() .map(fileInfo -> fileInfo.metadata().name()) .collect(Collectors.toList()); final List<String> sourceFilesToRecover = shardRecoveryPlan.getSourceFilesToRecover().stream().map(StoreFileMetadata::name).collect(Collectors.toList()); Set<String> filesFailedToDownload = Collections.synchronizedSet(new HashSet<>()); Set<String> filesRecoveredFromSource = Collections.synchronizedSet(new HashSet<>()); Set<String> filesRecoveredFromSnapshot = Collections.synchronizedSet(new HashSet<>()); TestRecoveryTargetHandler recoveryTarget = new Phase1RecoveryTargetHandler() { @Override public void restoreFileFromSnapshot(String repository, IndexId indexId, BlobStoreIndexShardSnapshot.FileInfo snapshotFile, ActionListener<Void> listener) { assertThat(repository, is(equalTo(snapshotFilesToRecover.getRepository()))); assertThat(indexId, is(equalTo(snapshotFilesToRecover.getIndexId()))); assertThat(containsSnapshotFile(snapshotFilesToRecover, snapshotFile), is(equalTo(true))); String fileName = snapshotFile.metadata().name(); if (randomBoolean()) { filesFailedToDownload.add(fileName); if (randomBoolean()) { listener.onFailure(randomFrom(new IOException("Failure"), new CorruptIndexException("idx", ""))); } else { throw new RuntimeException("Unexpected exception"); } } else { filesRecoveredFromSnapshot.add(fileName); listener.onResponse(null); } } @Override public void writeFileChunk(StoreFileMetadata fileMetadata, long position, ReleasableBytesReference content, boolean lastChunk, int totalTranslogOps, ActionListener<Void> listener) { filesRecoveredFromSource.add(fileMetadata.name()); listener.onResponse(null); } }; RecoverySourceHandler handler = new RecoverySourceHandler( shard, recoveryTarget, threadPool, getStartRecoveryRequest(), between(1, 16), between(1, 4), between(1, 4), between(1, 4), true, recoveryPlannerService) { @Override void createRetentionLease(long startingSeqNo, ActionListener<RetentionLease> listener) { listener.onResponse(new RetentionLease("id", startingSeqNo, 0, "test")); } }; PlainActionFuture<RecoverySourceHandler.SendFileResult> future = PlainActionFuture.newFuture(); handler.recoverFilesFromSourceAndSnapshot(shardRecoveryPlan, store, mock(StopWatch.class), future ); future.actionGet(); filesRecoveredFromSource.removeAll(sourceFilesToRecover); filesRecoveredFromSource.removeAll(filesFailedToDownload); assertThat(filesRecoveredFromSource, is(empty())); assertThat(fileNamesToBeRecoveredFromSnapshot.containsAll(filesRecoveredFromSnapshot), is(equalTo(true))); } }	i think this does not verify that we actually recover failed files from source, since this will pass as long as not files beyond the original and failed files are recovered from source. i think filesrecoveredfromsource.equals(sourcefilestorecover + filesfailedtodownload) is what we want to verify?
@Override public String executor() { return ThreadPool.Names.SAME; } } ); } } static class NoLongerMasterException extends ElasticsearchIllegalStateException { @Override public Throwable fillInStackTrace() { return null; } } static class NotMasterException extends ElasticsearchIllegalStateException { @Override public Throwable fillInStackTrace() { return null; } } static class NodeDoesNotExistOnMasterException extends ElasticsearchIllegalStateException { @Override public Throwable fillInStackTrace() { return null; } } private class MasterPingRequestHandler extends BaseTransportRequestHandler<MasterPingRequest> { public static final String ACTION = "discovery/zen/fd/masterPing"; @Override public MasterPingRequest newInstance() { return new MasterPingRequest(); } @Override public void messageReceived(MasterPingRequest request, TransportChannel channel) throws Exception { DiscoveryNodes nodes = nodesProvider.nodes(); // check if we are really the same master as the one we seemed to be think we are // this can happen if the master got "kill -9" and then another node started using the same port if (!request.masterNodeId.equals(nodes.localNodeId())) { throw new NotMasterException(); } if (request.clusterName != null && !request.clusterName.equals(clusterName.value())) { logger.trace("got a master ping from with a different cluster name: [{}]", request.clusterName); throw new NotMasterException(); } // if we are no longer master, fail... if (!nodes.localNodeMaster()) { throw new NoLongerMasterException(); } if (!nodes.nodeExists(request.nodeId)) { throw new NodeDoesNotExistOnMasterException(); } // send a response, and note if we are connected to the master or not channel.sendResponse(new MasterPingResponseResponse(nodes.nodeExists(request.nodeId))); } @Override public String executor() { return ThreadPool.Names.SAME; } } private static class MasterPingRequest extends TransportRequest { private String nodeId; private String masterNodeId; private String clusterName; private MasterPingRequest() { } private MasterPingRequest(String nodeId, String masterNodeId, String clusterName) { this.nodeId = nodeId; this.masterNodeId = masterNodeId; this.clusterName = clusterName; } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); nodeId = in.readString(); masterNodeId = in.readString(); if (in.getVersion().onOrAfter(Version.V_1_4_0)) { clusterName = in.readString(); } } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeString(nodeId); out.writeString(masterNodeId); if (out.getVersion().onOrAfter(Version.V_1_4_0)) { out.writeString(clusterName); } } } private static class MasterPingResponseResponse extends TransportResponse { private boolean connectedToMaster; private MasterPingResponseResponse() { } private MasterPingResponseResponse(boolean connectedToMaster) { this.connectedToMaster = connectedToMaster; }	can we add the option to provide a message here to the exception, and explain that we got it from a different cluster name?
public void testDispatchFailsWithPlainText() { String content = randomAlphaOfLength((int) Math.round(BREAKER_LIMIT.getBytes() / inFlightRequestsBreaker.getOverhead())); RestRequest fakeRestRequest = new FakeRestRequest.Builder(NamedXContentRegistry.EMPTY).withContent(new BytesArray(content), null) .withPath("/foo") .withHeaders(Collections.singletonMap("Content-Type", Collections.singletonList("text/plain"))) .build(); if (randomBoolean()) { fakeRestRequest = new RestRequest(fakeRestRequest); } AssertingChannel channel = new AssertingChannel(fakeRestRequest, true, RestStatus.NOT_ACCEPTABLE); restController.registerHandler( new Route(GET, "/foo"), (request, channel1, client) -> channel1.sendResponse( new BytesRestResponse(RestStatus.OK, BytesRestResponse.TEXT_CONTENT_TYPE, BytesArray.EMPTY) ) ); assertFalse(channel.getSendResponseCalled()); restController.dispatchRequest(fakeRestRequest, channel, client.threadPool().getThreadContext()); assertTrue(channel.getSendResponseCalled()); }	i wonder if it is better to have separate tests that only focus on this copying constructor and content type being null?
public void testReindexWithoutAuthenticationWhenRequired() throws Exception { RemoteInfo remote = new RemoteInfo("http", address.getAddress(), address.getPort(), new BytesArray("{\\\\"match_all\\\\":{}}"), null, null, emptyMap()); ReindexRequestBuilder request = ReindexAction.INSTANCE.newRequestBuilder(client()).source("source").destination("dest") .setRemoteInfo(remote); ElasticsearchStatusException e = expectThrows(ElasticsearchStatusException.class, () -> request.get()); assertEquals(RestStatus.UNAUTHORIZED, e.status()); assertThat(e.getMessage(), containsString("\\\\"reason\\\\":\\\\"Authentication required\\\\"")); assertThat(e.getMessage(), containsString("\\\\"WWW-Authenticate\\\\":\\\\"Basic realm=auth-realm\\\\"")); }	checkstyle is unhappy with this.
public void testReindexFromRemoteWithAuthentication() throws Exception { RemoteInfo remote = new RemoteInfo("http", address.getAddress(), address.getPort(), new BytesArray("{\\\\"match_all\\\\":{}}"), "Aladdin", "open sesame", emptyMap()); ReindexRequestBuilder request = ReindexAction.INSTANCE.newRequestBuilder(client()).source("source").destination("dest") .setRemoteInfo(remote); assertThat(request.get(), matcher().created(1)); }	checkstyle is unhappy with this.
public void testReindexFromRemote() throws Exception { NodeInfo nodeInfo = client().admin().cluster().prepareNodesInfo().get().getNodes().get(0); TransportAddress address = nodeInfo.getHttp().getAddress().publishAddress(); RemoteInfo remote = new RemoteInfo("http", address.getAddress(), address.getPort(), new BytesArray("{\\\\"match_all\\\\":{}}"), null, null, emptyMap()); ReindexRequestBuilder request = ReindexAction.INSTANCE.newRequestBuilder(client()).source("source").destination("dest") .setRemoteInfo(remote); testCase(ReindexAction.NAME, request, matcher().created(DOC_COUNT)); }	checkstyle is unhappy with this.
@Override public synchronized void loadSettings(Settings settings) { calledCreateAccount = true; super.loadSettings(settings); } } public void testReload() throws Exception { Settings settings = Settings.builder() .put("xpack.watcher.enabled", false) .put("path.home", createTempDir()) .build(); TestNotificationService service = new TestNotificationService(settings, "test"); Watcher watcher = new Watcher(settings) { @Override protected List<NotificationService> getReloadableServices() { return Collections.singletonList(service); } }	watcher is not enabled, but reloading works? that could be a noop in this case, or?
private void analyzeRead() { ALink last = links.get(links.size() - 1); // If the load node is a def node, we adapt its after type to use _this_ expected output type: if (last instanceof IDefLink && this.expected != null) { last.after = this.expected; } constant = last.string; statement = last.statement; actual = last.after; } /** * Handles writing byte code for variable/method chains for all given possibilities * including String concatenation, compound assignment, regular assignment, and simple * reads. Includes proper duplication for chained assignments and assignments that are * also read from. * * Example given 'x[0] += 5;' where x is an array of shorts and x[0] is 1. * Note this example has two links -- x (LVariable) and [0] (LBrace). * The following steps occur: * 1. call link{x}.write(...) -- no op [...] * 2. call link{x}.load(...) -- loads the address of the x array onto the stack [..., address(x)] * 3. call writer.dup(...) -- dup's the address of the x array onto the stack for later use with store [..., address(x), address(x)] * 4. call link{[0]}.write(...) -- load the array index value of the constant int 0 onto the stack [..., address(x), address(x), int(0)] * 5. call link{[0]}.load(...) -- load the short value from x[0] onto the stack [..., address(x), short(1)] * 6. call writer.writeCast(there) -- casts the short on the stack to an int so it can be added with the rhs [..., address(x), int(1)] * 7. call expression.write(...) -- puts the expression's value of the constant int 5 onto the stack [..., address(x), int(1), int(5)] * 8. call writer.writeBinaryInstruction(operation) -- writes the int addition instruction [..., address(x), int(6)] * 9. call writer.writeCast(back) -- convert the value on the stack back into a short [..., address(x), short(6)] * 10. call link{[0]}	i think we want to restore the debug information here? since it can hit classcastexception in other cases, its safe to just do it always?
synchronized void handleFailure(Exception e) { logger.warn(new ParameterizedMessage("[{}] transform encountered an exception: ", getJobId()), e); Throwable unwrappedException = ExceptionRootCauseFinder.getRootCauseException(e); if (unwrappedException instanceof CircuitBreakingException) { handleCircuitBreakingException((CircuitBreakingException) unwrappedException); return; } if (unwrappedException instanceof ScriptException) { handleScriptException((ScriptException) unwrappedException); return; } if (unwrappedException instanceof BulkIndexingException && ((BulkIndexingException) unwrappedException).isIrrecoverable()) { handleIrrecoverableBulkIndexingException((BulkIndexingException) unwrappedException); return; } // irrecoverable error without special handling if (unwrappedException instanceof ElasticsearchException) { ElasticsearchException elasticsearchException = (ElasticsearchException) unwrappedException; if (ExceptionRootCauseFinder.IRRECOVERABLE_REST_STATUSES.contains(elasticsearchException.status())) { failIndexer("task encountered irrecoverable failure: " + elasticsearchException.getDetailedMessage()); return; } } if (unwrappedException instanceof IllegalArgumentException) { failIndexer("task encountered irrecoverable failure: " + e.getMessage()); return; } if (context.getAndIncrementFailureCount() > context.getNumFailureRetries()) { failIndexer( "task encountered more than " + context.getNumFailureRetries() + " failures; latest failure: " + ExceptionRootCauseFinder.getDetailedMessage(unwrappedException) ); } else { // Since our schedule fires again very quickly after failures it is possible to run into the same failure numerous // times in a row, very quickly. We do not want to spam the audit log with repeated failures, so only record the first one if (e.getMessage().equals(lastAuditedExceptionMessage) == false) { String message = ExceptionRootCauseFinder.getDetailedMessage(unwrappedException); auditor.warning( getJobId(), "Transform encountered an exception: " + message + " Will attempt again at next scheduled trigger." ); lastAuditedExceptionMessage = message; } } }	you could return; in line 511 and get rid of else clauses, just like you did above.
* @return The first exception considered irrecoverable if there are any, null if no irrecoverable exception found */ public static Throwable getFirstIrrecoverableExceptionFromBulkResponses(Collection<BulkItemResponse> failures) { for (BulkItemResponse failure : failures) { Throwable unwrappedThrowable = org.elasticsearch.ExceptionsHelper.unwrapCause(failure.getFailure().getCause()); if (unwrappedThrowable instanceof IllegalArgumentException) { return unwrappedThrowable; } if (unwrappedThrowable instanceof ElasticsearchException) { ElasticsearchException elasticsearchException = (ElasticsearchException) unwrappedThrowable; if (ExceptionRootCauseFinder.IRRECOVERABLE_REST_STATUSES.contains(elasticsearchException.status())) { return elasticsearchException; } } } return null; }	exceptionrootcausefinder. prefix should not be needed here.
static DeprecationIssue oldIndicesCheck(IndexMetaData indexMetaData) { Version createdWith = indexMetaData.getCreationVersion(); if (createdWith.before(Version.V_7_0_0)) { return new DeprecationIssue(DeprecationIssue.Level.CRITICAL, "Index created before 7.0", "https://www.elastic.co/guide/en/elasticsearch/reference/master/" + "breaking-changes-8.0.html", "This index was created using version: " + createdWith); } return null; }	can you help me understand why indices created prior to 7.0 is a critical (or even an issue), and why are we linking to the 8.0 breaking changes. the code looks fine, i think i am just missing something here.
private BlobStoreIndexShardSnapshot loadShardSnapshot(BlobContainer shardContainer, SnapshotId snapshotId) { try { return indexShardSnapshotFormat.read(shardContainer, snapshotId.getUUID()); } catch (NoSuchFileException ex) { throw new SnapshotMissingException(metadata.name(), snapshotId, ex); } catch (IOException ex) { throw new SnapshotException(metadata.name(), snapshotId, "failed to read shard snapshot file for [" + shardContainer.path() + ']', ex); } } /** * Loads all available snapshots in the repository using the given {@code generation} or falling back to trying to determine it from * the given list of blobs in the shard container. * * @param blobs list of blobs in repository * @param generation shard generation or {@code null} in case there was no shard generation tracked in the {@link RepositoryData} for * this shard because its snapshot was created in a version older than * {@link SnapshotsService#SHARD_GEN_IN_REPO_DATA_VERSION}	i would prefer to move this logic to the caller (there is just one where this is applicable) and assert in this method that the generation is not one of these special generation types
@Override public void finalizeSnapshot(final SnapshotId snapshotId, final ShardGenerations shardGenerations, final long startTime, final String failure, final int totalShards, final List<SnapshotShardFailure> shardFailures, final long repositoryStateId, final boolean includeGlobalState, final MetaData clusterMetaData, final Map<String, Object> userMetadata, final Version version, final ActionListener<SnapshotInfo> listener) { final Collection<IndexId> indices = shardGenerations.indices(); // Once we are done writing the updated index-N blob we remove the now unreferenced index-${uuid} blobs in each shard // directory if all nodes are at least at version SnapshotsService#SHARD_GEN_IN_REPO_DATA_VERSION // If there are older version nodes in the cluster, we don't need to run this cleanup as it will have already happened // when writing the index-${N} to each shard directory. final ActionListener<SnapshotInfo> allMetaListener = new GroupedActionListener<>( ActionListener.wrap(snapshotInfos -> { assert snapshotInfos.size() == 1 : "Should have only received a single SnapshotInfo but received " + snapshotInfos; final SnapshotInfo snapshotInfo = snapshotInfos.iterator().next(); final RepositoryData existingRepositoryData = getRepositoryData(); final RepositoryData updatedRepositoryData = existingRepositoryData.addSnapshot(snapshotId, snapshotInfo.state(), shardGenerations); writeIndexGen(updatedRepositoryData, repositoryStateId, version); if (version.onOrAfter(SnapshotsService.SHARD_GEN_IN_REPO_DATA_VERSION)) { cleanupOldShardGens(existingRepositoryData, updatedRepositoryData); } listener.onResponse(snapshotInfo); }, e -> listener.onFailure(new SnapshotException(metadata.name(), snapshotId, "failed to update snapshot in repository", e))), 2 + indices.size()); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT); // We ignore all FileAlreadyExistsException when writing metadata since otherwise a master failover while in this method will // mean that no snap-${uuid}.dat blob is ever written for this snapshot. This is safe because any updated version of the // index or global metadata will be compatible with the segments written in this snapshot as well. // Failing on an already existing index-${repoGeneration} below ensures that the index.latest blob is not updated in a way // that decrements the generation it points at // Write Global MetaData executor.execute(ActionRunnable.run(allMetaListener, () -> globalMetaDataFormat.write(clusterMetaData, blobContainer(), snapshotId.getUUID(), false))); // write the index metadata for each index in the snapshot for (IndexId index : indices) { executor.execute(ActionRunnable.run(allMetaListener, () -> indexMetaDataFormat.write(clusterMetaData.index(index.getName()), indexContainer(index), snapshotId.getUUID(), false))); } executor.execute(ActionRunnable.supply(allMetaListener, () -> { final SnapshotInfo snapshotInfo = new SnapshotInfo(snapshotId, indices.stream().map(IndexId::getName).collect(Collectors.toList()), startTime, failure, threadPool.absoluteTimeInMillis(), totalShards, shardFailures, includeGlobalState, userMetadata); snapshotFormat.write(snapshotInfo, blobContainer(), snapshotId.getUUID(), false); return snapshotInfo; })); }	this should use same boolean, not spread a "version" field around.
private static void assertShardIndexGenerations(BlobContainer repoRoot, ShardGenerations shardGenerations) throws IOException { final BlobContainer indicesContainer = repoRoot.children().get("indices"); for (IndexId index : shardGenerations.indices()) { final List<String> gens = shardGenerations.getGens(index); if (gens.isEmpty() == false) { final BlobContainer indexContainer = indicesContainer.children().get(index.getId()); final Map<String, BlobContainer> shardContainers = indexContainer.children(); for (int i = 0; i < gens.size(); i++) { final String generation = gens.get(i); assertThat(generation, not(ShardGenerations.DELETED_SHARD_GEN)); if (generation != null && generation.equals(ShardGenerations.NEW_SHARD_GEN) == false) { final String shardId = Integer.toString(i); assertThat(shardContainers, hasKey(shardId)); assertThat(shardContainers.get(shardId).listBlobsByPrefix(BlobStoreRepository.INDEX_FILE_PREFIX), hasKey(BlobStoreRepository.INDEX_FILE_PREFIX + generation)); } } } } }	should we also check which files should not be there?
private static void assertShardIndexGenerations(BlobContainer repoRoot, ShardGenerations shardGenerations) throws IOException { final BlobContainer indicesContainer = repoRoot.children().get("indices"); for (IndexId index : shardGenerations.indices()) { final List<String> gens = shardGenerations.getGens(index); if (gens.isEmpty() == false) { final BlobContainer indexContainer = indicesContainer.children().get(index.getId()); final Map<String, BlobContainer> shardContainers = indexContainer.children(); for (int i = 0; i < gens.size(); i++) { final String generation = gens.get(i); assertThat(generation, not(ShardGenerations.DELETED_SHARD_GEN)); if (generation != null && generation.equals(ShardGenerations.NEW_SHARD_GEN) == false) { final String shardId = Integer.toString(i); assertThat(shardContainers, hasKey(shardId)); assertThat(shardContainers.get(shardId).listBlobsByPrefix(BlobStoreRepository.INDEX_FILE_PREFIX), hasKey(BlobStoreRepository.INDEX_FILE_PREFIX + generation)); } } } } }	assertconsistency is for what's on disk? the repositorydata should not contain new_shard_gen entries?
private Expression combine(And and) { List<Range> ranges = new ArrayList<>(); List<BinaryComparison> bcs = new ArrayList<>(); List<Expression> exps = new ArrayList<>(); boolean changed = false; List<Expression> andExps = Predicates.splitAnd(and); // Ranges need to show up before BinaryComparisons in list, to allow the latter be optimized away into a Range, if possible andExps.sort(new Comparator<Expression>() { @Override public int compare(Expression o1, Expression o2) { if (o1 instanceof Range && o2 instanceof Range) { return 0; // keep ranges' order } else if (o1 instanceof Range || o2 instanceof Range) { return o2 instanceof Range ? 1 : -1; } else { return 0; // keep non-ranges' order } } }); for (Expression ex : andExps) { if (ex instanceof Range) { Range r = (Range) ex; if (findExistingRange(r, ranges, true)) { changed = true; } else { ranges.add(r); } } else if (ex instanceof BinaryComparison && !(ex instanceof Equals)) { BinaryComparison bc = (BinaryComparison) ex; if (bc.right().foldable() && (findConjunctiveComparisonInRange(bc, ranges) || findExistingComparison(bc, bcs, true))) { changed = true; } else { bcs.add(bc); } } else { exps.add(ex); } } // finally try combining any left BinaryComparisons into possible Ranges // this could be a different rule but it's clearer here wrt the order of comparisons for (int i = 0; i < bcs.size() - 1; i++) { BinaryComparison main = bcs.get(i); for (int j = i + 1; j < bcs.size(); j++) { BinaryComparison other = bcs.get(j); if (main.left().semanticEquals(other.left())) { // >/>= AND </<= if ((main instanceof GreaterThan || main instanceof GreaterThanOrEqual) && (other instanceof LessThan || other instanceof LessThanOrEqual)) { bcs.remove(j); bcs.remove(i); ranges.add(new Range(and.source(), main.left(), main.right(), main instanceof GreaterThanOrEqual, other.right(), other instanceof LessThanOrEqual)); changed = true; } // </<= AND >/>= else if ((other instanceof GreaterThan || other instanceof GreaterThanOrEqual) && (main instanceof LessThan || main instanceof LessThanOrEqual)) { bcs.remove(j); bcs.remove(i); ranges.add(new Range(and.source(), main.left(), other.right(), other instanceof GreaterThanOrEqual, main.right(), main instanceof LessThanOrEqual)); changed = true; } } } } return changed ? Predicates.combineAnd(CollectionUtils.combine(exps, bcs, ranges)) : and; }	maybe extract this custom comparator into its own variable inside combinebinarycomparisons and re-use that, without creating it each time the combine method is called?
private boolean findConjunctiveComparisonInRange(BinaryComparison main, List<Range> ranges) { Object value = main.right().fold(); // NB: the loop modifies the list (hence why the int is used) for (int i = 0; i < ranges.size(); i++) { Range other = ranges.get(i); if (main.left().semanticEquals(other.value())) { if (main instanceof GreaterThan || main instanceof GreaterThanOrEqual) { if (other.lower().foldable()) { Integer comp = BinaryComparison.compare(value, other.lower().fold()); if (comp != null) { // 2 < a AND (2 <= a < 3) -> 2 < a < 3 boolean lowerEq = comp == 0 && other.includeLower() && main instanceof GreaterThan; // 2 < a AND (1 < a < 3) -> 2 < a < 3 boolean lower = comp > 0 || lowerEq; if (lower) { ranges.remove(i); ranges.add(i, new Range(other.source(), other.value(), main.right(), lowerEq ? false : main instanceof GreaterThanOrEqual, other.upper(), other.includeUpper())); } // found a match return true; } } } else if (main instanceof LessThan || main instanceof LessThanOrEqual) { if (other.upper().foldable()) { Integer comp = BinaryComparison.compare(value, other.upper().fold()); if (comp != null) { // a < 2 AND (1 < a <= 2) -> 1 < a < 2 boolean upperEq = comp == 0 && other.includeUpper() && main instanceof LessThan; // a < 2 AND (1 < a < 3) -> 1 < a < 2 boolean upper = comp < 0 || upperEq; if (upper) { ranges.remove(i); ranges.add(i, new Range(other.source(), other.value(), other.lower(), other.includeLower(), main.right(), upperEq ? false : main instanceof LessThanOrEqual)); } // found a match return true; } } } return false; } } return false; }	likewise - a whole optimization being skipped...
public ClusterState execute(ClusterState currentState) { ensureRepositoryExists(repositoryName, currentState); ensureSnapshotNameAvailableInRepo(repositoryData, snapshotName, repository); final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY); ensureSnapshotNameNotRunning(snapshots, repositoryName, snapshotName); validate(repositoryName, snapshotName, currentState); final SnapshotDeletionsInProgress deletionsInProgress = currentState.custom( SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY ); ensureNoCleanupInProgress(currentState, repositoryName, snapshotName, "create snapshot"); ensureBelowConcurrencyLimit(repositoryName, snapshotName, snapshots, deletionsInProgress); // Store newSnapshot here to be processed in clusterStateProcessed Map<Boolean, List<String>> requestedIndices = Arrays.stream( indexNameExpressionResolver.concreteIndexNames(currentState, request) ).collect(Collectors.partitioningBy(systemIndices::isSystemIndex)); List<String> requestedSystemIndices = requestedIndices.get(true); if (requestedSystemIndices.isEmpty() == false) { Set<String> explicitlyRequestedSystemIndices = new HashSet<>(requestedSystemIndices); explicitlyRequestedSystemIndices.retainAll(Arrays.asList(request.indices())); if (explicitlyRequestedSystemIndices.isEmpty() == false) { throw new IllegalArgumentException( new ParameterizedMessage( "system indices [{}] were explicitly requested, but snapshots should use the " + "[include_global_state] or [feature_states] parameters to control inclusion of system indices", explicitlyRequestedSystemIndices ).getFormattedMessage() ); } } List<String> indices = requestedIndices.get(false); final Set<SnapshotFeatureInfo> featureStates = new HashSet<>(); final Set<String> systemDataStreamNames = new HashSet<>(); // if we have any feature states in the snapshot, we add their required indices to the snapshot indices if they haven't // been requested by the request directly final Set<String> indexNames = new HashSet<>(indices); for (String featureName : featureStatesSet) { SystemIndices.Feature feature = systemIndices.getFeatures().get(featureName); Set<String> featureSystemIndices = feature.getIndexDescriptors() .stream() .flatMap(descriptor -> descriptor.getMatchingIndices(currentState.metadata()).stream()) .collect(Collectors.toSet()); Set<String> featureAssociatedIndices = feature.getAssociatedIndexDescriptors() .stream() .flatMap(descriptor -> descriptor.getMatchingIndices(currentState.metadata()).stream()) .collect(Collectors.toSet()); Set<String> featureSystemDataStreams = new HashSet<>(); Set<String> featureDataStreamBackingIndices = new HashSet<>(); for (SystemDataStreamDescriptor sdd : feature.getDataStreamDescriptors()) { List<String> backingIndexNames = sdd.getBackingIndexNames(currentState.metadata()); if (backingIndexNames.size() > 0) { featureDataStreamBackingIndices.addAll(backingIndexNames); featureSystemDataStreams.add(sdd.getDataStreamName()); } } if (featureSystemIndices.size() > 0 || featureAssociatedIndices.size() > 0 || featureDataStreamBackingIndices.size() > 0) { featureStates.add(new SnapshotFeatureInfo(featureName, List.copyOf(featureSystemIndices))); indexNames.addAll(featureSystemIndices); indexNames.addAll(featureAssociatedIndices); indexNames.addAll(featureDataStreamBackingIndices); systemDataStreamNames.addAll(featureSystemDataStreams); } indices = List.copyOf(indexNames); } final List<String> dataStreams = indexNameExpressionResolver.dataStreamNames( currentState, request.indicesOptions(), request.indices() ); dataStreams.addAll(systemDataStreamNames); logger.trace("[{}][{}] creating snapshot for indices [{}]", repositoryName, snapshotName, indices); final Map<String, IndexId> allIndices = new HashMap<>(); for (SnapshotsInProgress.Entry runningSnapshot : snapshots.forRepo(repositoryName)) { allIndices.putAll(runningSnapshot.indices()); } final Map<String, IndexId> indexIds = repositoryData.resolveNewIndices(indices, allIndices); final Version version = minCompatibleVersion(currentState.nodes().getMinNodeVersion(), repositoryData, null); ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards = shards( snapshots, deletionsInProgress, currentState, indexIds.values(), useShardGenerations(version), repositoryData, repositoryName ); if (request.partial() == false) { Set<String> missing = new HashSet<>(); for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards) { if (entry.value.state() == ShardState.MISSING) { missing.add(entry.key.getIndex().getName()); } } if (missing.isEmpty() == false) { throw new SnapshotException( new Snapshot(repositoryName, snapshotId), "Indices don't have primary shards " + missing ); } } newEntry = SnapshotsInProgress.startedEntry( new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), request.partial(), indexIds, dataStreams, threadPool.absoluteTimeInMillis(), repositoryData.getGenId(), shards, userMeta, version, List.copyOf(featureStates) ); return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, snapshots.withAddedEntry(newEntry)).build(); }	not a blocker, but we may want to mention the indices parameter here. right now, it can be a little unclear where the error is coming from. i'd suggest something like: > the [indices] parameter includes system indices [{}]. to include system indices in a snapshot, use the [include_global_state] or [feature_states] parameters instead. it'd be great if we could somehow surface this error for slm policies too (either at policy creation/update or execution), but that's likely outside of scope here.
@Override public AttachmentProcessor create(Map<String, Processor.Factory> registry, String processorTag, String description, Map<String, Object> config) throws Exception { String field = readStringProperty(TYPE, processorTag, config, "field"); String fileName = readStringProperty(TYPE, processorTag, config, "file_name", "file_name"); String targetField = readStringProperty(TYPE, processorTag, config, "target_field", "attachment"); List<String> propertyNames = readOptionalList(TYPE, processorTag, config, "properties"); int indexedChars = readIntProperty(TYPE, processorTag, config, "indexed_chars", NUMBER_OF_CHARS_INDEXED); boolean ignoreMissing = readBooleanProperty(TYPE, processorTag, config, "ignore_missing", false); String indexedCharsField = readOptionalStringProperty(TYPE, processorTag, config, "indexed_chars_field"); final Set<Property> properties; if (propertyNames != null) { properties = EnumSet.noneOf(Property.class); for (String fieldName : propertyNames) { try { properties.add(Property.parse(fieldName)); } catch (Exception e) { throw newConfigurationException(TYPE, processorTag, "properties", "illegal field option [" + fieldName + "]. valid values are " + Arrays.toString(Property.values())); } } } else { properties = DEFAULT_PROPERTIES; } return new AttachmentProcessor(processorTag, description, field, targetField, properties, indexedChars, ignoreMissing, indexedCharsField, fileName); } } enum Property { CONTENT, TITLE, AUTHOR, KEYWORDS, DATE, CONTENT_TYPE, CONTENT_LENGTH, LANGUAGE; public static Property parse(String value) { return valueOf(value.toUpperCase(Locale.ROOT)); } public String toLowerCase() { return this.toString().toLowerCase(Locale.ROOT); }	i think this should call readoptionalstringproperty so that any existing fields named file_name that are _not_ intended for tika will not be inadvertently used.
@Override public IngestDocument execute(IngestDocument ingestDocument) { Map<String, Object> additionalFields = new HashMap<>(); byte[] input = ingestDocument.getFieldValueAsBytes(field, ignoreMissing); String fileNameInput = ingestDocument.getFieldValue(fileName, String.class, true); if (input == null && ignoreMissing) { return ingestDocument; } else if (input == null) { throw new IllegalArgumentException("field [" + field + "] is null, cannot parse."); } Integer indexedChars = this.indexedChars; if (indexedCharsField != null) { // If the user provided the number of characters to be extracted as part of the document, we use it indexedChars = ingestDocument.getFieldValue(indexedCharsField, Integer.class, true); if (indexedChars == null) { // If the field does not exist we fall back to the global limit indexedChars = this.indexedChars; } } Metadata metadata = new Metadata(); if (fileNameInput != null) { metadata.set(Metadata.RESOURCE_NAME_KEY, fileNameInput); } String parsedContent = ""; try { parsedContent = TikaImpl.parse(input, metadata, indexedChars); } catch (ZeroByteFileException e) { // tika 1.17 throws an exception when the InputStream has 0 bytes. // previously, it did not mind. This is here to preserve that behavior. } catch (Exception e) { throw new ElasticsearchParseException("Error parsing document in field [{}]", e, field); } if (properties.contains(Property.CONTENT) && Strings.hasLength(parsedContent)) { // somehow tika seems to append a newline at the end automatically, lets remove that again additionalFields.put(Property.CONTENT.toLowerCase(), parsedContent.trim()); } if (properties.contains(Property.LANGUAGE) && Strings.hasLength(parsedContent)) { // TODO: stop using LanguageIdentifier... LanguageIdentifier identifier = new LanguageIdentifier(parsedContent); String language = identifier.getLanguage(); additionalFields.put(Property.LANGUAGE.toLowerCase(), language); } if (properties.contains(Property.DATE)) { String createdDate = metadata.get(TikaCoreProperties.CREATED); if (createdDate != null) { additionalFields.put(Property.DATE.toLowerCase(), createdDate); } } if (properties.contains(Property.TITLE)) { String title = metadata.get(TikaCoreProperties.TITLE); if (Strings.hasLength(title)) { additionalFields.put(Property.TITLE.toLowerCase(), title); } } if (properties.contains(Property.AUTHOR)) { String author = metadata.get("Author"); if (Strings.hasLength(author)) { additionalFields.put(Property.AUTHOR.toLowerCase(), author); } } if (properties.contains(Property.KEYWORDS)) { String keywords = metadata.get("Keywords"); if (Strings.hasLength(keywords)) { additionalFields.put(Property.KEYWORDS.toLowerCase(), keywords); } } if (properties.contains(Property.CONTENT_TYPE)) { String contentType = metadata.get(Metadata.CONTENT_TYPE); if (Strings.hasLength(contentType)) { additionalFields.put(Property.CONTENT_TYPE.toLowerCase(), contentType); } } if (properties.contains(Property.CONTENT_LENGTH)) { String contentLength = metadata.get(Metadata.CONTENT_LENGTH); long length; if (Strings.hasLength(contentLength)) { length = Long.parseLong(contentLength); } else { length = parsedContent.length(); } additionalFields.put(Property.CONTENT_LENGTH.toLowerCase(), length); } ingestDocument.setFieldValue(targetField, additionalFields); return ingestDocument; }	when readoptionalstringproperty is used as i suggested in another comment, this should retrieve the field value only when filename != null.
@Override public String toString() { return delegate + " offset by " + offset; } } public static Rounding read(StreamInput in) throws IOException { byte id = in.readByte(); switch (id) { case TimeUnitRounding.ID: return new TimeUnitRounding(in); case TimeIntervalRounding.ID: return new TimeIntervalRounding(in); case OffsetRounding.ID: return new OffsetRounding(in); default: throw new ElasticsearchException("unknown rounding id [" + id + "]"); } } /** * Implementation of {@link Prepared} using pre-calculated "round down" points. */ private static class ArrayRounding implements Prepared { private final long[] values; private final int max; private final Prepared delegate; private ArrayRounding(long[] values, int max, Prepared delegate) { this.values = values; this.max = max; this.delegate = delegate; } @Override public long round(long utcMillis) { assert values[0] <= utcMillis : utcMillis + " must be after " + values[0]; int idx = Arrays.binarySearch(values, 0, max, utcMillis); assert idx != -1 : "The insertion point is before the array! This should have tripped the assertion above."; assert -1 - idx <= values.length : "This insertion point is after the end of the array."; if (idx < 0) { idx = -2 - idx; } return values[idx]; } @Override public long nextRoundingValue(long utcMillis) { return delegate.nextRoundingValue(utcMillis); } @Override public double roundingSize(Long utcMillis, DateTimeUnit timeUnit) { return delegate.roundingSize(utcMillis, timeUnit); } @Override public long[] fixedRoundingPoints() { return Arrays.copyOf(values, max); }	any chance you could group this so unit.ismillisbased is called once? i think it might be a bit easier to read if you juggle the "arms" of the if statement.
@Override public void prepareForTranslogOperations(boolean fileBasedRecovery, int totalTranslogOps, long recoverUpToSeqNo, ActionListener<Optional<Store.MetadataSnapshot>> listener) { ActionListener.completeWith(listener, () -> { if (Assertions.ENABLED && recoverUpToSeqNo != SequenceNumbers.UNASSIGNED_SEQ_NO) { final long gcp = readGlobalCheckpointFromTranslog(); assert recoverUpToSeqNo == gcp : recoverUpToSeqNo + " != " + gcp; } indexShard().openEngineAndRecoverFromTranslog(recoverUpToSeqNo); if (indexShard.getLocalCheckpoint() < recoverUpToSeqNo) { // This can happen if the previous recovery was aborted after copying segments, then the target does not have all translog. indexShard.closeEngine(); return Optional.of(indexShard.snapshotStoreMetadata()); } state().getTranslog().totalOperations(state().getTranslog().totalOperations() + totalTranslogOps); return Optional.empty(); }); }	in case this fails as part of pre-phase1, we should fall back to phase1
@Override public void prepareForTranslogOperations(boolean fileBasedRecovery, int totalTranslogOps, long recoverUpToSeqNo, ActionListener<Optional<Store.MetadataSnapshot>> listener) { ActionListener.completeWith(listener, () -> { if (Assertions.ENABLED && recoverUpToSeqNo != SequenceNumbers.UNASSIGNED_SEQ_NO) { final long gcp = readGlobalCheckpointFromTranslog(); assert recoverUpToSeqNo == gcp : recoverUpToSeqNo + " != " + gcp; } indexShard().openEngineAndRecoverFromTranslog(recoverUpToSeqNo); if (indexShard.getLocalCheckpoint() < recoverUpToSeqNo) { // This can happen if the previous recovery was aborted after copying segments, then the target does not have all translog. indexShard.closeEngine(); return Optional.of(indexShard.snapshotStoreMetadata()); } state().getTranslog().totalOperations(state().getTranslog().totalOperations() + totalTranslogOps); return Optional.empty(); }); }	could we bake this part into indexshard.openengineonreplica?
@Override public void prepareForTranslogOperations(boolean fileBasedRecovery, int totalTranslogOps, long recoverUpToSeqNo, ActionListener<Optional<Store.MetadataSnapshot>> listener) { ActionListener.completeWith(listener, () -> { if (Assertions.ENABLED && recoverUpToSeqNo != SequenceNumbers.UNASSIGNED_SEQ_NO) { final long gcp = readGlobalCheckpointFromTranslog(); assert recoverUpToSeqNo == gcp : recoverUpToSeqNo + " != " + gcp; } indexShard().openEngineAndRecoverFromTranslog(recoverUpToSeqNo); if (indexShard.getLocalCheckpoint() < recoverUpToSeqNo) { // This can happen if the previous recovery was aborted after copying segments, then the target does not have all translog. indexShard.closeEngine(); return Optional.of(indexShard.snapshotStoreMetadata()); } state().getTranslog().totalOperations(state().getTranslog().totalOperations() + totalTranslogOps); return Optional.empty(); }); }	why do we need to send the updated store metadata here?
@Override public void prepareForTranslogOperations(boolean fileBasedRecovery, int totalTranslogOps, long recoverUpToSeqNo, ActionListener<Optional<Store.MetadataSnapshot>> listener) { ActionListener.completeWith(listener, () -> { if (Assertions.ENABLED && recoverUpToSeqNo != SequenceNumbers.UNASSIGNED_SEQ_NO) { final long gcp = readGlobalCheckpointFromTranslog(); assert recoverUpToSeqNo == gcp : recoverUpToSeqNo + " != " + gcp; } indexShard().openEngineAndRecoverFromTranslog(recoverUpToSeqNo); if (indexShard.getLocalCheckpoint() < recoverUpToSeqNo) { // This can happen if the previous recovery was aborted after copying segments, then the target does not have all translog. indexShard.closeEngine(); return Optional.of(indexShard.snapshotStoreMetadata()); } state().getTranslog().totalOperations(state().getTranslog().totalOperations() + totalTranslogOps); return Optional.empty(); }); }	should we commit here? it'd save us from having to replay these operations again if something goes wrong from here on.
* @return transformed version of transformMe. This may actually be the same object as sourceAsMap */ Map<String, Object> transformSourceAsMap(Map<String, Object> sourceAsMap); } /** * Script based source transformation. */ private static class ScriptTransform implements SourceTransform { private final ScriptService scriptService; /** * Contents of the script to transform the source document before indexing. */ private final String script; /** * Language of the script to transform the source document before indexing. */ private final String language; /** * Parameters passed to the transform script. */ private final Map<String, Object> parameters; public ScriptTransform(ScriptService scriptService, String script, String language, Map<String, Object> parameters) { this.scriptService = scriptService; this.script = script; this.language = language; this.parameters = parameters; } @SuppressWarnings("unchecked") public Map<String, Object> transformSourceAsMap(Map<String, Object> sourceAsMap) { try { ExecutableScript executable = scriptService.executable(language, script, parameters); executable.setNextVar("source", sourceAsMap); executable.run(); return (Map<String, Object>) executable.unwrap(sourceAsMap); } catch (Exception e) { throw new ElasticsearchIllegalArgumentException("failed to execute script", e); } } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("script", script); builder.field("lang", language); if (parameters != null) { builder.field("params", parameters); } builder.endObject(); return builder; }	i think we should try to make it consistent with the [update api](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-update.html) by exposing a ctx object that has a _source property instead of exposing source directly (see updatehelper)? updatehelper also has some logic to make metadata like the timestamp and the ttl updatable (you can set them on the context) but i don't think we need to support it here (we can add it in the future if there are needs for it but i think we can skip it for now).
public IndexMetaData upgradeIndexMetaData(IndexMetaData indexMetaData) { // Throws an exception if there are too-old segments: if (isUpgraded(indexMetaData)) { assert indexMetaData == archiveBrokenIndexSettings(indexMetaData) : "all settings must have been upgraded before"; return indexMetaData; } checkSupportedVersion(indexMetaData); IndexMetaData newMetaData = indexMetaData; // we have to run this first otherwise in we try to create IndexSettings // with broken settings and fail in checkMappingsCompatibility try { newMetaData = archiveBrokenIndexSettings(newMetaData); } catch (Exception ex) { logger.error("{} failed to process index settings: {}", newMetaData.getIndex(), ex.getMessage()); throw ex; } // only run the check with the upgraded settings!! checkMappingsCompatibility(newMetaData); return markAsUpgraded(newMetaData); }	shouldn't we add the information to the exception?
public void apply(Project project) { project.getRootProject().getPluginManager().apply(DockerSupportPlugin.class); project.getPluginManager().apply(DistributionDownloadPlugin.class); project.getPluginManager().apply("elasticsearch.build"); Provider<DockerSupportService> dockerSupport = GradleUtils.getBuildService( project.getGradle().getSharedServices(), DockerSupportPlugin.DOCKER_SUPPORT_SERVICE_NAME ); // TODO: it would be useful to also have the SYSTEM_JAVA_HOME setup in the root project, so that running from GCP only needs // a java for gradle to run, and the tests are self sufficient and consistent with the java they use Version upgradeVersion = getUpgradeVersion(project); Provider<Directory> distributionsDir = project.getLayout().getBuildDirectory().dir("packaging/distributions"); Provider<Directory> upgradeDir = project.getLayout().getBuildDirectory().dir("packaging/upgrade"); Provider<Directory> pluginsDir = project.getLayout().getBuildDirectory().dir("packaging/plugins"); List<ElasticsearchDistribution> distributions = configureDistributions(project, upgradeVersion); TaskProvider<Copy> copyDistributionsTask = configureCopyDistributionsTask(project, distributionsDir); TaskProvider<Copy> copyUpgradeTask = configureCopyUpgradeTask(project, upgradeVersion, upgradeDir); TaskProvider<Copy> copyPluginsTask = configureCopyPluginsTask(project, pluginsDir); // Create lifecycle tasks so we can run all tests of a given type Map<ElasticsearchDistribution.Type, TaskProvider<?>> lifecyleTasks = new HashMap<>(); lifecyleTasks.put(Type.DOCKER, project.getTasks().register("destructiveDistroTest#docker")); lifecyleTasks.put(Type.ARCHIVE, project.getTasks().register("destructiveDistroTest#archives")); lifecyleTasks.put(Type.DEB, project.getTasks().register("destructiveDistroTest#packages")); lifecyleTasks.put(Type.RPM, lifecyleTasks.get(Type.DEB)); TaskProvider<Task> destructiveDistroTest = project.getTasks().register("destructiveDistroTest"); for (ElasticsearchDistribution distribution : distributions) { TaskProvider<?> destructiveTask = configureDistroTest(project, distribution, dockerSupport); destructiveDistroTest.configure(t -> t.dependsOn(destructiveTask)); Optional.ofNullable(lifecyleTasks.get(distribution.getType())).ifPresent(p -> p.configure(t -> t.dependsOn(destructiveTask))); } Map<String, TaskProvider<?>> batsTests = new HashMap<>(); configureBatsTest(project, "plugins", distributionsDir, copyDistributionsTask, copyPluginsTask).configure( t -> t.setPluginsDir(pluginsDir) ); configureBatsTest(project, "upgrade", distributionsDir, copyDistributionsTask, copyUpgradeTask).configure( t -> t.setUpgradeDir(upgradeDir) ); project.subprojects(vmProject -> { vmProject.getPluginManager().apply(VagrantBasePlugin.class); vmProject.getPluginManager().apply(JdkDownloadPlugin.class); List<Object> vmDependencies = new ArrayList<>(configureVM(vmProject)); vmDependencies.add(project.getConfigurations().getByName("testRuntimeClasspath")); TaskProvider<Task> distroTest = vmProject.getTasks().register("distroTest"); for (ElasticsearchDistribution distribution : distributions) { String destructiveTaskName = destructiveDistroTestTaskName(distribution); Platform platform = distribution.getPlatform(); // this condition ensures windows boxes get windows distributions, and linux boxes get linux distributions if (isWindows(vmProject) == (platform == Platform.WINDOWS)) { TaskProvider<GradleDistroTestTask> vmTask = configureVMWrapperTask( vmProject, distribution.getName() + " distribution", destructiveTaskName, vmDependencies ); vmTask.configure(t -> t.dependsOn(distribution)); distroTest.configure(t -> { // Only VM sub-projects that are specifically opted-in to testing Docker should // have the Docker task added as a dependency. Although we control whether Docker // is installed in the VM via `Vagrantfile` and we could auto-detect its presence // in the VM, the test tasks e.g. `destructiveDistroTest.default-docker` are defined // on the host during Gradle's configuration phase and not in the VM, so // auto-detection doesn't work. // // The shouldTestDocker property could be null, hence we use Boolean.TRUE.equals() boolean shouldExecute = distribution.getType() != Type.DOCKER || Boolean.TRUE.equals(vmProject.findProperty("shouldTestDocker")); if (shouldExecute) { t.dependsOn(vmTask); } }); } } batsTests.forEach((desc, task) -> { configureVMWrapperTask(vmProject, desc, task.getName(), vmDependencies).configure(t -> { t.setProgressHandler(new BatsProgressLogger(project.getLogger())); t.onlyIf(spec -> isWindows(vmProject) == false); // bats doesn't run on windows t.dependsOn(copyDistributionsTask); }); }); }); }	we've used . as the separator in the rest of the tasks in this plugin. can we reuse that here? it is easier when typing out the task name to not need to add quotes as # requires.
public void apply(Project project) { project.getRootProject().getPluginManager().apply(DockerSupportPlugin.class); project.getPluginManager().apply(DistributionDownloadPlugin.class); project.getPluginManager().apply("elasticsearch.build"); Provider<DockerSupportService> dockerSupport = GradleUtils.getBuildService( project.getGradle().getSharedServices(), DockerSupportPlugin.DOCKER_SUPPORT_SERVICE_NAME ); // TODO: it would be useful to also have the SYSTEM_JAVA_HOME setup in the root project, so that running from GCP only needs // a java for gradle to run, and the tests are self sufficient and consistent with the java they use Version upgradeVersion = getUpgradeVersion(project); Provider<Directory> distributionsDir = project.getLayout().getBuildDirectory().dir("packaging/distributions"); Provider<Directory> upgradeDir = project.getLayout().getBuildDirectory().dir("packaging/upgrade"); Provider<Directory> pluginsDir = project.getLayout().getBuildDirectory().dir("packaging/plugins"); List<ElasticsearchDistribution> distributions = configureDistributions(project, upgradeVersion); TaskProvider<Copy> copyDistributionsTask = configureCopyDistributionsTask(project, distributionsDir); TaskProvider<Copy> copyUpgradeTask = configureCopyUpgradeTask(project, upgradeVersion, upgradeDir); TaskProvider<Copy> copyPluginsTask = configureCopyPluginsTask(project, pluginsDir); // Create lifecycle tasks so we can run all tests of a given type Map<ElasticsearchDistribution.Type, TaskProvider<?>> lifecyleTasks = new HashMap<>(); lifecyleTasks.put(Type.DOCKER, project.getTasks().register("destructiveDistroTest#docker")); lifecyleTasks.put(Type.ARCHIVE, project.getTasks().register("destructiveDistroTest#archives")); lifecyleTasks.put(Type.DEB, project.getTasks().register("destructiveDistroTest#packages")); lifecyleTasks.put(Type.RPM, lifecyleTasks.get(Type.DEB)); TaskProvider<Task> destructiveDistroTest = project.getTasks().register("destructiveDistroTest"); for (ElasticsearchDistribution distribution : distributions) { TaskProvider<?> destructiveTask = configureDistroTest(project, distribution, dockerSupport); destructiveDistroTest.configure(t -> t.dependsOn(destructiveTask)); Optional.ofNullable(lifecyleTasks.get(distribution.getType())).ifPresent(p -> p.configure(t -> t.dependsOn(destructiveTask))); } Map<String, TaskProvider<?>> batsTests = new HashMap<>(); configureBatsTest(project, "plugins", distributionsDir, copyDistributionsTask, copyPluginsTask).configure( t -> t.setPluginsDir(pluginsDir) ); configureBatsTest(project, "upgrade", distributionsDir, copyDistributionsTask, copyUpgradeTask).configure( t -> t.setUpgradeDir(upgradeDir) ); project.subprojects(vmProject -> { vmProject.getPluginManager().apply(VagrantBasePlugin.class); vmProject.getPluginManager().apply(JdkDownloadPlugin.class); List<Object> vmDependencies = new ArrayList<>(configureVM(vmProject)); vmDependencies.add(project.getConfigurations().getByName("testRuntimeClasspath")); TaskProvider<Task> distroTest = vmProject.getTasks().register("distroTest"); for (ElasticsearchDistribution distribution : distributions) { String destructiveTaskName = destructiveDistroTestTaskName(distribution); Platform platform = distribution.getPlatform(); // this condition ensures windows boxes get windows distributions, and linux boxes get linux distributions if (isWindows(vmProject) == (platform == Platform.WINDOWS)) { TaskProvider<GradleDistroTestTask> vmTask = configureVMWrapperTask( vmProject, distribution.getName() + " distribution", destructiveTaskName, vmDependencies ); vmTask.configure(t -> t.dependsOn(distribution)); distroTest.configure(t -> { // Only VM sub-projects that are specifically opted-in to testing Docker should // have the Docker task added as a dependency. Although we control whether Docker // is installed in the VM via `Vagrantfile` and we could auto-detect its presence // in the VM, the test tasks e.g. `destructiveDistroTest.default-docker` are defined // on the host during Gradle's configuration phase and not in the VM, so // auto-detection doesn't work. // // The shouldTestDocker property could be null, hence we use Boolean.TRUE.equals() boolean shouldExecute = distribution.getType() != Type.DOCKER || Boolean.TRUE.equals(vmProject.findProperty("shouldTestDocker")); if (shouldExecute) { t.dependsOn(vmTask); } }); } } batsTests.forEach((desc, task) -> { configureVMWrapperTask(vmProject, desc, task.getName(), vmDependencies).configure(t -> { t.setProgressHandler(new BatsProgressLogger(project.getLogger())); t.onlyIf(spec -> isWindows(vmProject) == false); // bats doesn't run on windows t.dependsOn(copyDistributionsTask); }); }); }); }	when would this not be present? shouldn't that be an error?
private void explain(Task task, PutDataFrameAnalyticsAction.Request request, ActionListener<ExplainDataFrameAnalyticsAction.Response> listener) { final ExtractedFieldsDetectorFactory extractedFieldsDetectorFactory = new ExtractedFieldsDetectorFactory( new ParentTaskAssigningClient(client, task.getParentTaskId()) ); if (licenseState.isSecurityEnabled()) { useSecondaryAuthIfAvailable(this.securityContext, () -> { // Use secondary auth headers for the request always DataFrameAnalyticsConfig config = new DataFrameAnalyticsConfig.Builder(request.getConfig()) .setHeaders(filterSecurityHeaders(threadPool.getThreadContext().getHeaders())) .build(); extractedFieldsDetectorFactory.createFromSource( config, ActionListener.wrap( extractedFieldsDetector -> explain(task, config, extractedFieldsDetector, listener), listener::onFailure ) ); }); } else { extractedFieldsDetectorFactory.createFromSource( request.getConfig(), ActionListener.wrap( extractedFieldsDetector -> explain(task, request.getConfig(), extractedFieldsDetector, listener), listener::onFailure ) ); } }	this. is reduntant here afaict
private void explain(Task task, PutDataFrameAnalyticsAction.Request request, ActionListener<ExplainDataFrameAnalyticsAction.Response> listener) { final ExtractedFieldsDetectorFactory extractedFieldsDetectorFactory = new ExtractedFieldsDetectorFactory( new ParentTaskAssigningClient(client, task.getParentTaskId()) ); if (licenseState.isSecurityEnabled()) { useSecondaryAuthIfAvailable(this.securityContext, () -> { // Use secondary auth headers for the request always DataFrameAnalyticsConfig config = new DataFrameAnalyticsConfig.Builder(request.getConfig()) .setHeaders(filterSecurityHeaders(threadPool.getThreadContext().getHeaders())) .build(); extractedFieldsDetectorFactory.createFromSource( config, ActionListener.wrap( extractedFieldsDetector -> explain(task, config, extractedFieldsDetector, listener), listener::onFailure ) ); }); } else { extractedFieldsDetectorFactory.createFromSource( request.getConfig(), ActionListener.wrap( extractedFieldsDetector -> explain(task, request.getConfig(), extractedFieldsDetector, listener), listener::onFailure ) ); } }	what does "always" mean in this context? we're in the conditional branch, that's why i'm asking.
@Override public String toString() { return Strings.toString(this, true, true); }	do you think it would make sense to make dataframetransformconfig constructor private now that we have "builder()" as a recommended entry point to this class? the same question for other classes...
* @return true if a job has been triggered, false otherwise */ public synchronized boolean maybeTriggerAsyncJob(long now) { final IndexerState currentState = state.get(); switch (currentState) { case INDEXING: case STOPPING: case ABORTING: logger.warn("Schedule was triggered for job [" + getJobId() + "], but prior indexer is still running " + "(with state [" + currentState + "]"); return false; case STOPPED: logger.debug("Schedule was triggered for job [" + getJobId() + "] but job is stopped. Ignoring trigger."); return false; case STARTED: logger.debug("Schedule was triggered for job [" + getJobId() + "], state: [" + currentState + "]"); stats.incrementNumInvocations(1); if (state.compareAndSet(IndexerState.STARTED, IndexerState.INDEXING)) { // fire off the search. Note this is async, the method will return from here executor.execute(() -> { try { onStart(now, () -> { stats.markStartSearch(); doNextSearch(buildSearchRequest(), ActionListener.wrap(this::onSearchResponse, this::finishWithSearchFailure)); }); } catch (Exception e) { logger.error("Indexer failed on start", e); doSaveState(finishAndSetState(), position.get(), () -> onFailure(e)); } }); logger.debug("Beginning to index [" + getJobId() + "], state: [" + currentState + "]"); return true; } else { logger.debug("Could not move from STARTED to INDEXING state because current state is [" + state.get() + "]"); return false; } default: logger.warn("Encountered unexpected state [" + currentState + "] while indexing"); throw new IllegalStateException("Job encountered an illegal state [" + currentState + "]"); } }	i don't think we should catch the exception here, if an error occurs in onstart we should provide an actionlistener to let the caller deals with the error (e.g. call listener.onfailure(e)): onstart(now, actionlistener.wrap((o) -> { stats.markstartsearch(); donextsearch(buildsearchrequest(), actionlistener.wrap(this::onsearchresponse, this::finishwithsearchfailure)); }, (e) -> dosavestate(finishandsetstate(), position.get(), () -> onfailure(e));
*/ protected abstract SearchRequest buildSearchRequest(); /** * Called at startup after job has been triggered using {@link #maybeTriggerAsyncJob(long)} and the * internal state is {@link IndexerState#STARTED}. * * Implementors MUST ensure that next.run() gets called. * * @param now The current time in milliseconds passed through from {@link #maybeTriggerAsyncJob(long)}	since we're expecting async calls to be fired in onstart we should provide an actionlistener<void> rather than a runnable ? this way the implementation can fail the job nicely if an error occurs ?
private void onSearchResponse(SearchResponse searchResponse) { stats.markEndSearch(); try { if (checkState(getState()) == false) { return; } if (searchResponse.getShardFailures().length != 0) { throw new RuntimeException("Shard failures encountered while running indexer for job [" + getJobId() + "]: " + Arrays.toString(searchResponse.getShardFailures())); } stats.incrementNumPages(1); IterationResult<JobPosition> iterationResult = doProcess(searchResponse); if (iterationResult.isDone()) { logger.debug("Finished indexing for job [" + getJobId() + "], saving state and shutting down."); // execute finishing tasks try { onFinish(() -> { // Change state first, then try to persist. This prevents in-progress // STOPPING/ABORTING from // being persisted as STARTED but then stop the job doSaveState(finishAndSetState(), position.get(), () -> {}); }); } catch (Exception e) { logger.error("Indexer failed on finish", e); doSaveState(finishAndSetState(), position.get(), () -> onFailure(e)); } return; } final List<IndexRequest> docs = iterationResult.getToIndex(); final BulkRequest bulkRequest = new BulkRequest(); docs.forEach(bulkRequest::add); // TODO this might be a valid case, e.g. if implementation filters assert bulkRequest.requests().size() > 0; stats.markStartIndexing(); doNextBulk(bulkRequest, ActionListener.wrap(bulkResponse -> { // TODO we should check items in the response and move after accordingly to // resume the failing buckets ? if (bulkResponse.hasFailures()) { logger.warn("Error while attempting to bulk index documents: " + bulkResponse.buildFailureMessage()); } stats.incrementNumOutputDocuments(bulkResponse.getItems().length); // check if indexer has been asked to stop, state {@link IndexerState#STOPPING} if (checkState(getState()) == false) { return; } JobPosition newPosition = iterationResult.getPosition(); position.set(newPosition); onBulkResponse(bulkResponse, newPosition); }, this::finishWithIndexingFailure)); } catch (Exception e) { finishWithSearchFailure(e); } }	i think we should rather document the fact that the runnable provided with onfinish (and maybe onfailure) should always run (even on errors). if we don't expect async calls in onfinish we can also keep the current code and just switch the execution order of onfinish and dosavestate.
@AwaitsFix(bugUrl = "https://github.com/elastic/ml-cpp/issues/1121") public void testNumMatchesAndCategoryPreference() throws Exception { String index = "hadoop_logs"; client().admin().indices().prepareCreate(index) .setMapping("time", "type=date,format=epoch_millis", "msg", "type=text") .get(); nowMillis = System.currentTimeMillis(); BulkRequestBuilder bulkRequestBuilder = client().prepareBulk(); IndexRequest indexRequest = new IndexRequest(index); indexRequest.source("time", nowMillis - TimeValue.timeValueHours(8).millis(), "msg", "2015-10-18 18:01:51,963 INFO [main] org.mortbay.log: jetty-6.1.26"); bulkRequestBuilder.add(indexRequest); indexRequest = new IndexRequest(index); indexRequest.source("time", nowMillis - TimeValue.timeValueHours(7).millis(), "msg", "2015-10-18 18:01:52,728 INFO [main] org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:62267"); bulkRequestBuilder.add(indexRequest); indexRequest = new IndexRequest(index); indexRequest.source("time", nowMillis - TimeValue.timeValueHours(6).millis(), "msg", "2015-10-18 18:01:53,400 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules"); bulkRequestBuilder.add(indexRequest); indexRequest = new IndexRequest(index); indexRequest.source("time", nowMillis - TimeValue.timeValueHours(5).millis(), "msg", "2015-10-18 18:01:53,447 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true"); bulkRequestBuilder.add(indexRequest); indexRequest = new IndexRequest(index); indexRequest.source("time", nowMillis - TimeValue.timeValueHours(4).millis(), "msg", "2015-10-18 18:01:52,728 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Web app /mapreduce started at 62267"); bulkRequestBuilder.add(indexRequest); indexRequest = new IndexRequest(index); indexRequest.source("time", nowMillis - TimeValue.timeValueHours(2).millis(), "msg", "2015-10-18 18:01:53,557 INFO [main] org.apache.hadoop.yarn.client.RMProxy: " + "Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030"); bulkRequestBuilder.add(indexRequest); indexRequest = new IndexRequest(index); indexRequest.source("time", nowMillis - TimeValue.timeValueHours(1).millis(), "msg", "2015-10-18 18:01:53,713 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: " + "maxContainerCapability: <memory:8192, vCores:32>"); bulkRequestBuilder.add(indexRequest); indexRequest = new IndexRequest(index); indexRequest.source("time", nowMillis, "msg", "2015-10-18 18:01:53,713 INFO [main] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: " + "yarn.client.max-cached-nodemanagers-proxies : 0"); bulkRequestBuilder.add(indexRequest); BulkResponse bulkResponse = bulkRequestBuilder .setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE) .get(); assertThat(bulkResponse.hasFailures(), is(false)); Job.Builder job = newJobBuilder("categorization-with-preferred-categories", Collections.emptyList()); registerJob(job); putJob(job); openJob(job.getId()); String datafeedId = job.getId() + "-feed"; DatafeedConfig.Builder datafeedConfig = new DatafeedConfig.Builder(datafeedId, job.getId()); datafeedConfig.setIndices(Collections.singletonList(index)); DatafeedConfig datafeed = datafeedConfig.build(); registerDatafeed(datafeed); putDatafeed(datafeed); startDatafeed(datafeedId, 0, nowMillis + 1); waitUntilJobIsClosed(job.getId()); List<CategoryDefinition> categories = getCategories(job.getId()); assertThat(categories, hasSize(7)); CategoryDefinition category1 = categories.get(0); assertThat(category1.getNumMatches(), equalTo(2L)); long[] expectedPreferenceTo = new long[]{2L, 3L, 4L, 5L, 6L, 7L}; assertThat(category1.getPreferredToCategories(), equalTo(expectedPreferenceTo)); client().admin().indices().prepareDelete(index).get(); }	i removed the assertion on the length because in the case where the results are not as expected it obscures the useful information of what the actual results were. equal length should be implied by the equalto assertion on the line below.
public void testUsage() throws InterruptedException, ExecutionException, IOException { Client client = mock(Client.class); when(licenseState.isDataFrameAllowed()).thenReturn(true); DataFrameFeatureSet featureSet = new DataFrameFeatureSet(Settings.EMPTY, client, licenseState); List<DataFrameTransformStateAndStats> transformsStateAndStats = new ArrayList<>(); for (int i = 0; i < randomIntBetween(0, 10); ++i) { transformsStateAndStats.add(DataFrameTransformStateAndStatsTests.randomDataFrameTransformStateAndStats()); } List<DataFrameTransformConfig> transformConfigWithoutTasks = new ArrayList<>(); for (int i = 0; i < randomIntBetween(0, 10); ++i) { transformConfigWithoutTasks.add(DataFrameTransformConfigTests.randomDataFrameTransformConfig()); } List<DataFrameTransformConfig> transformConfigWithTasks = new ArrayList<>(transformsStateAndStats.size()); transformsStateAndStats.forEach(stats -> transformConfigWithTasks.add(DataFrameTransformConfigTests.randomDataFrameTransformConfig(stats.getId()))); List<DataFrameTransformConfig> allConfigs = new ArrayList<>(transformConfigWithoutTasks.size() + transformConfigWithTasks.size()); allConfigs.addAll(transformConfigWithoutTasks); allConfigs.addAll(transformConfigWithTasks); GetDataFrameTransformsStatsAction.Response mockResponse = new GetDataFrameTransformsStatsAction.Response(transformsStateAndStats); GetDataFrameTransformsAction.Response mockTransformsResponse = new GetDataFrameTransformsAction.Response(allConfigs); doAnswer(invocationOnMock -> { @SuppressWarnings("unchecked") ActionListener<Response> listener = (ActionListener<Response>) invocationOnMock.getArguments()[2]; listener.onResponse(mockResponse); return Void.TYPE; }).when(client).execute(same(GetDataFrameTransformsStatsAction.INSTANCE), any(), any()); doAnswer(invocationOnMock -> { @SuppressWarnings("unchecked") ActionListener<GetDataFrameTransformsAction.Response> listener = (ActionListener<GetDataFrameTransformsAction.Response>) invocationOnMock.getArguments()[2]; listener.onResponse(mockTransformsResponse); return Void.TYPE; }).when(client).execute(same(GetDataFrameTransformsAction.INSTANCE), any(), any()); PlainActionFuture<Usage> future = new PlainActionFuture<>(); featureSet.usage(future); XPackFeatureSet.Usage usage = future.get(); assertTrue(usage.enabled()); try (XContentBuilder builder = XContentFactory.jsonBuilder()) { usage.toXContent(builder, ToXContent.EMPTY_PARAMS); XContentParser parser = createParser(builder); Map<String, Object> usageAsMap = parser.map(); assertTrue((boolean) XContentMapValues.extractValue("available", usageAsMap)); if (transformsStateAndStats.isEmpty() && transformConfigWithoutTasks.isEmpty()) { // no transforms, no stats assertEquals(null, XContentMapValues.extractValue("transforms", usageAsMap)); assertEquals(null, XContentMapValues.extractValue("stats", usageAsMap)); } else { assertEquals(transformsStateAndStats.size() + transformConfigWithoutTasks.size(), XContentMapValues.extractValue("transforms._all", usageAsMap)); Map<String, Integer> stateCounts = new HashMap<>(); transformsStateAndStats.stream().map(x -> x.getTransformState().getIndexerState().value()) .forEach(x -> stateCounts.merge(x, 1, Integer::sum)); transformConfigWithoutTasks.forEach(ignored -> stateCounts.merge(IndexerState.STOPPED.value(), 1, Integer::sum)); stateCounts.forEach((k, v) -> assertEquals(v, XContentMapValues.extractValue("transforms." + k, usageAsMap))); Optional<DataFrameIndexerTransformStats> combinedStats = transformsStateAndStats.stream().map(x -> x.getTransformStats()) .reduce((l, r) -> l.merge(r)); if (combinedStats.isPresent()) { assertEquals(toIntExact(combinedStats.get().getIndexFailures()), XContentMapValues.extractValue("stats.index_failures", usageAsMap)); assertEquals(toIntExact(combinedStats.get().getIndexTotal()), XContentMapValues.extractValue("stats.index_total", usageAsMap)); assertEquals(toIntExact(combinedStats.get().getSearchTime()), XContentMapValues.extractValue("stats.search_time_in_ms", usageAsMap)); assertEquals(toIntExact(combinedStats.get().getNumDocuments()), XContentMapValues.extractValue("stats.documents_processed", usageAsMap)); } } } }	i think the problem of this test is on line 131, the if is broken, it broke because transformconfigwithouttasks is empty but transformsstateandstats isn't. i admit this isn't a master piece in test writing (i think i originally wrote it), it would be clearer and cleaner if we split this test into the empty and non-empty case. we then do not need to workaround here, which i am not sure about: it disables all asserts and tests nothing if ispresent() returns false, which means we have no coverage for this case.
private static boolean checkWaitRequirements(GetDiscoveredNodesRequest request, Set<DiscoveryNode> nodes) { Set<String> requiredNodes = new HashSet<>(request.getRequiredNodes()); for (final DiscoveryNode node : nodes) { boolean matchedAddress = requiredNodes.remove(node.getAddress().toString()); boolean matchedName = requiredNodes.remove(node.getName()); if (matchedAddress && matchedName) { throw new IllegalArgumentException( "Node [" + node + "] matched both a name as well as an address entry" + " in the nodes list specified in setting [cluster.initial_master_nodes]." ); } if (requiredNodes.isEmpty()) { break; } } return requiredNodes.isEmpty() && nodes.size() >= request.getWaitForNodes(); }	ok, could we also match on the ip address part of the transport address (transportaddress#getaddress()) (with tests, obvs)? this opens up a slight complexity in this check-for-duplicates, because two nodes may share an ip address, so we need to consider this (i.e. have a test for it): {node1}{10.20.30.40:9301} {node2}{10.20.30.40:9302} we should reject requirednodes being "node1", "10.20.30.40" because 10.20.30.40 matches both, but the obvious change here wouldn't always pick it up.
public void testErrStillRunsFailureHandlerWhenRetrieving() throws Exception { ThreadPool threadPool = new TestThreadPool("slm-test"); final String policyId = "policy"; final String repoId = "repo"; try (ClusterService clusterService = ClusterServiceUtils.createClusterService(threadPool); Client noOpClient = new NoOpClient("slm-test") { @Override @SuppressWarnings("unchecked") protected <Request extends ActionRequest, Response extends ActionResponse> void doExecute(ActionType<Response> action, Request request, ActionListener<Response> listener) { if (request instanceof GetSnapshotsRequest) { logger.info("--> called"); listener.onResponse((Response) new GetSnapshotsResponse( Collections.singleton(GetSnapshotsResponse.Response.snapshots(repoId, Collections.emptyList())))); } else { super.doExecute(action, request, listener); } } }) { SnapshotLifecyclePolicy policy = new SnapshotLifecyclePolicy(policyId, "snap", "1 * * * * ?", repoId, null, new SnapshotRetentionConfiguration(TimeValue.timeValueDays(30), null, null)); ClusterState state = createState(policy); ClusterServiceUtils.setState(clusterService, state); SnapshotRetentionTask task = new SnapshotRetentionTask(noOpClient, clusterService, System::nanoTime, new SnapshotLifecycleTaskTests.VerifyingHistoryStore(noOpClient, ZoneOffset.UTC, (historyItem) -> fail("should never write history")), threadPool); AtomicReference<Exception> errHandlerCalled = new AtomicReference<>(null); task.getAllRetainableSnapshots(Collections.singleton(repoId), new ActionListener<>() { @Override public void onResponse(Map<String, List<SnapshotInfo>> stringListMap) { logger.info("--> forcing failure"); throw new ElasticsearchException("forced failure"); } @Override public void onFailure(Exception e) { fail("we have another err handler that should have been called"); } }, errHandlerCalled::set); assertNotNull(errHandlerCalled.get()); assertThat(errHandlerCalled.get().getMessage(), equalTo("forced failure")); } finally { threadPool.shutdownNow(); threadPool.awaitTermination(10, TimeUnit.SECONDS); } }	i found this confusing, but i see it's just how getallretainablesnapshots does the listener callback, so maybe it's a discussion we should have on a separate ocasion
public void cleanUpTransforms() throws IOException { for (String transformId : transformsToClean) { highLevelClient().dataFrame().stopDataFrameTransform( new StopDataFrameTransformRequest(transformId, Boolean.TRUE, TimeValue.timeValueSeconds(20)), RequestOptions.DEFAULT); } for (String transformId : transformsToClean) { highLevelClient().dataFrame().deleteDataFrameTransform( new DeleteDataFrameTransformRequest(transformId), RequestOptions.DEFAULT); } transformsToClean = new ArrayList<>(); }	boolean.true should probably just be true
public void testWriteFileChunksConcurrently() throws Exception { IndexShard sourceShard = newStartedShard(true); int numDocs = between(20, 100); for (int i = 0; i < numDocs; i++) { indexDoc(sourceShard, "_doc", Integer.toString(i)); } sourceShard.flush(new FlushRequest()); Store.MetadataSnapshot sourceSnapshot = sourceShard.store().getMetadata(null); List<StoreFileMetaData> mdFiles = new ArrayList<>(); for (StoreFileMetaData md : sourceSnapshot) { mdFiles.add(md); } final IndexShard targetShard = newShard(false); final DiscoveryNode pNode = getFakeDiscoNode(sourceShard.routingEntry().currentNodeId()); final DiscoveryNode rNode = getFakeDiscoNode(targetShard.routingEntry().currentNodeId()); targetShard.markAsRecovering("test-peer-recovery", new RecoveryState(targetShard.routingEntry(), rNode, pNode)); final RecoveryTarget recoveryTarget = new RecoveryTarget(targetShard, null, null, null); recoveryTarget.receiveFileInfo( mdFiles.stream().map(StoreFileMetaData::name).collect(Collectors.toList()), mdFiles.stream().map(StoreFileMetaData::length).collect(Collectors.toList()), Collections.emptyList(), Collections.emptyList(), 0 ); List<RecoveryFileChunkRequest> requests = new ArrayList<>(); for (StoreFileMetaData md : mdFiles) { int pos = 0; IndexInput in = sourceShard.store().directory().openInput(md.name(), IOContext.READONCE); while (pos < md.length()) { int length = between(1, Math.toIntExact(md.length() - pos)); byte[] buffer = new byte[length]; in.readBytes(buffer, 0, length); requests.add(new RecoveryFileChunkRequest(0, sourceShard.shardId(), md, pos, new BytesArray(buffer), pos + length == md.length(), 1, 1)); pos += length; } in.close(); } Randomness.shuffle(requests); BlockingQueue<RecoveryFileChunkRequest> queue = new ArrayBlockingQueue<>(requests.size()); queue.addAll(requests); Thread[] senders = new Thread[between(1, 4)]; CountDownLatch latch = new CountDownLatch(senders.length + 1); for (int i = 0; i < senders.length; i++) { senders[i] = new Thread(() -> { latch.countDown(); try { latch.await(); RecoveryFileChunkRequest r; while ((r = queue.poll()) != null) { recoveryTarget.writeFileChunk(r.metadata(), r.position(), r.content(), r.lastChunk(), r.totalTranslogOps()).get(); } } catch (Exception e) { throw new AssertionError(e); } }); senders[i].start(); } latch.countDown(); for (Thread sender : senders) { sender.join(); } recoveryTarget.renameAllTempFiles(); recoveryTarget.decRef(); Store.MetadataSnapshot targetSnapshot = targetShard.snapshotStoreMetadata(); Store.RecoveryDiff diff = sourceSnapshot.recoveryDiff(targetSnapshot); assertThat(diff.different, empty()); closeShards(sourceShard, targetShard); }	nit: just try-with-resources the indexinput instead of the explicit close?
public void testWriteFileChunksConcurrently() throws Exception { IndexShard sourceShard = newStartedShard(true); int numDocs = between(20, 100); for (int i = 0; i < numDocs; i++) { indexDoc(sourceShard, "_doc", Integer.toString(i)); } sourceShard.flush(new FlushRequest()); Store.MetadataSnapshot sourceSnapshot = sourceShard.store().getMetadata(null); List<StoreFileMetaData> mdFiles = new ArrayList<>(); for (StoreFileMetaData md : sourceSnapshot) { mdFiles.add(md); } final IndexShard targetShard = newShard(false); final DiscoveryNode pNode = getFakeDiscoNode(sourceShard.routingEntry().currentNodeId()); final DiscoveryNode rNode = getFakeDiscoNode(targetShard.routingEntry().currentNodeId()); targetShard.markAsRecovering("test-peer-recovery", new RecoveryState(targetShard.routingEntry(), rNode, pNode)); final RecoveryTarget recoveryTarget = new RecoveryTarget(targetShard, null, null, null); recoveryTarget.receiveFileInfo( mdFiles.stream().map(StoreFileMetaData::name).collect(Collectors.toList()), mdFiles.stream().map(StoreFileMetaData::length).collect(Collectors.toList()), Collections.emptyList(), Collections.emptyList(), 0 ); List<RecoveryFileChunkRequest> requests = new ArrayList<>(); for (StoreFileMetaData md : mdFiles) { int pos = 0; IndexInput in = sourceShard.store().directory().openInput(md.name(), IOContext.READONCE); while (pos < md.length()) { int length = between(1, Math.toIntExact(md.length() - pos)); byte[] buffer = new byte[length]; in.readBytes(buffer, 0, length); requests.add(new RecoveryFileChunkRequest(0, sourceShard.shardId(), md, pos, new BytesArray(buffer), pos + length == md.length(), 1, 1)); pos += length; } in.close(); } Randomness.shuffle(requests); BlockingQueue<RecoveryFileChunkRequest> queue = new ArrayBlockingQueue<>(requests.size()); queue.addAll(requests); Thread[] senders = new Thread[between(1, 4)]; CountDownLatch latch = new CountDownLatch(senders.length + 1); for (int i = 0; i < senders.length; i++) { senders[i] = new Thread(() -> { latch.countDown(); try { latch.await(); RecoveryFileChunkRequest r; while ((r = queue.poll()) != null) { recoveryTarget.writeFileChunk(r.metadata(), r.position(), r.content(), r.lastChunk(), r.totalTranslogOps()).get(); } } catch (Exception e) { throw new AssertionError(e); } }); senders[i].start(); } latch.countDown(); for (Thread sender : senders) { sender.join(); } recoveryTarget.renameAllTempFiles(); recoveryTarget.decRef(); Store.MetadataSnapshot targetSnapshot = targetShard.snapshotStoreMetadata(); Store.RecoveryDiff diff = sourceSnapshot.recoveryDiff(targetSnapshot); assertThat(diff.different, empty()); closeShards(sourceShard, targetShard); }	nit: we can save a line here using java.util.concurrent.cyclicbarrier instead of the latch i guess :)
public void testSendFileChunksConcurrently() throws Exception { final IndexShard shard = mock(IndexShard.class); when(shard.state()).thenReturn(IndexShardState.STARTED); final TestRecoveryTargetHandler recoveryTarget = new TestRecoveryTargetHandler(); final int maxConcurrentChunks = between(1, 8); final int chunkSize = between(1, 128); final RecoverySourceHandler handler = new RecoverySourceHandler(shard, recoveryTarget, getStartRecoveryRequest(), chunkSize, maxConcurrentChunks); final List<CompletableFuture<Void>> unackedChunks = recoveryTarget.unacknowledgedChunks; int totalChunks = between(1, 100); AtomicInteger sentChunks = new AtomicInteger(); Thread sender = new Thread(() -> { try { byte[] buffer = randomByteArrayOfLength(totalChunks * chunkSize); StoreFileMetaData md = new StoreFileMetaData("test", buffer.length, "checksum", org.apache.lucene.util.Version.LATEST); try (RecoverySourceHandler.RecoveryOutputStream out = handler.new RecoveryOutputStream(md, () -> 1)) { while(sentChunks.get() < totalChunks) { out.write(buffer, sentChunks.get() * chunkSize, chunkSize); sentChunks.incrementAndGet(); } } } catch (IOException ex) { throw new AssertionError(ex); } }); sender.start(); assertBusy(() -> { assertThat(sentChunks.get(), equalTo(Math.min(totalChunks, maxConcurrentChunks))); assertThat(unackedChunks, hasSize(sentChunks.get())); }); while (sentChunks.get() < totalChunks || unackedChunks.isEmpty() == false) { List<CompletableFuture<Void>> chunksToAck = randomSubsetOf(between(1, unackedChunks.size()), unackedChunks); unackedChunks.removeAll(chunksToAck); int chunksToSend = Math.min(totalChunks - sentChunks.get(), chunksToAck.size()); int expectedSentChunks = sentChunks.get() + chunksToSend; int expectedUnackedChunks = unackedChunks.size() + chunksToSend; chunksToAck.forEach(f -> f.complete(null)); assertBusy(() -> { assertThat(sentChunks.get(), equalTo(expectedSentChunks)); assertThat(unackedChunks, hasSize(expectedUnackedChunks)); }); } sender.join(); }	nit: missing space after while -> should be while (sentchunks.get() < totalchunks) {
@Override public Decision canRebalance(RoutingAllocation allocation) { if (allocation.ignoreDisable()) { return allocation.decision(Decision.YES, NAME, "allocation is explicitly ignoring any disabling of rebalancing"); } for (IndexMetaData indexMetaData : allocation.metaData()) { if (INDEX_ROUTING_REBALANCE_ENABLE_SETTING.exists(indexMetaData.getSettings())) { return allocation.decision(Decision.YES, NAME, "rebalancing is overridden on one or more indices"); } } if (this.enableRebalance == Rebalance.NONE) { return allocation.decision(Decision.NO, NAME, "no rebalancing is allowed due to %s", setting(this.enableRebalance, false)); } return allocation.decision(Decision.YES, NAME, "rebalancing is not globally disabled"); }	would it make sense to also be able to short circuit if that setting is set to rebalance.none?
public void testBwcWith73() throws IOException { for (int i = 0; i < NUMBER_OF_TEST_RUNS; i++) { TransformStats stats = new TransformStats( "bwc-id", STARTED, randomBoolean() ? null : randomAlphaOfLength(100), randomBoolean() ? null : NodeAttributeTests.randomNodeAttributes(), new TransformIndexerStats(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0.0, 0.0, 0.0), new TransformCheckpointingInfo( new TransformCheckpointStats(0, null, null, 10, 100), new TransformCheckpointStats(0, null, null, 100, 1000), // changesLastDetectedAt aren't serialized back 100, null ) ); try (BytesStreamOutput output = new BytesStreamOutput()) { output.setVersion(Version.V_7_3_0); stats.writeTo(output); try (StreamInput in = output.bytes().streamInput()) { in.setVersion(Version.V_7_3_0); TransformStats statsFromOld = new TransformStats(in); assertThat(statsFromOld, equalTo(stats)); } } } }	do not get confused, testbwcwith73: this test bwc with 7.3 where we had no exponential averages, so its ok to construct the stats with 0.0
private GeoPoint resetFromWKT(String value, boolean ignoreZValue) { try { PointBuilder point = (PointBuilder)GeoWKTParser.parseExpectedType(value, GeoShapeType.POINT, ignoreZValue, false); return reset(point.latitude(), point.longitude()); } catch (IOException e) { throw new ElasticsearchParseException("Invalid WKT format", e); } }	could you replace it with [wellknowntext](https://github.com/elastic/elasticsearch/blob/8029b479b8b1e1d3cc05bb87d1fdd9ec6325ee1b/libs/geo/src/main/java/org/elasticsearch/geo/utils/wellknowntext.java#l47) parser?
public GeoPoint resetFromString(String value, final boolean ignoreZValue) { if (value.contains(",")) { return resetFromCoordinates(value, ignoreZValue); } else if (value.contains("POINT")) { return resetFromWKT(value, ignoreZValue); } return resetFromGeoHash(value); }	i think this comparison should be case insensitive.
@Override public void parse(ParseContext context) throws IOException { context.path().add(simpleName()); try { GeoPoint sparse = context.parseExternalValue(GeoPoint.class); if (sparse != null) { parse(context, sparse); } else { sparse = new GeoPoint(); XContentParser.Token token = context.parser().currentToken(); if (token == XContentParser.Token.START_ARRAY) { token = context.parser().nextToken(); if (token == XContentParser.Token.VALUE_NUMBER) { double lon = context.parser().doubleValue(); context.parser().nextToken(); double lat = context.parser().doubleValue(); token = context.parser().nextToken(); if (token == XContentParser.Token.VALUE_NUMBER) { GeoPoint.assertZValue(ignoreZValue.value(), context.parser().doubleValue()); } else if (token != XContentParser.Token.END_ARRAY) { throw new ElasticsearchParseException("[{}] field type does not accept > 3 dimensions", CONTENT_TYPE); } parse(context, sparse.reset(lat, lon)); } else { while (token != XContentParser.Token.END_ARRAY) { if (token == XContentParser.Token.VALUE_STRING) { parseGeoPointStringIgnoringMalformed(context, sparse); } else { parseGeoPointIgnoringMalformed(context, sparse); } token = context.parser().nextToken(); } } } else if (token == XContentParser.Token.VALUE_STRING) { parseGeoPointStringIgnoringMalformed(context, sparse); } else if (token == XContentParser.Token.VALUE_NULL) { if (fieldType.nullValue() != null) { parse(context, (GeoPoint) fieldType.nullValue()); } } else { parseGeoPointIgnoringMalformed(context, sparse); } } } catch (Exception ex) { throw new MapperParsingException("failed to parse field [{}] of type [{}]", ex, fieldType().name(), fieldType().typeName()); } context.path().remove(); }	when i said that we don't need to support multipoint at this time, i meant that we don't need to support new formats. but we cannot just remove formats that are already supported like this one.
public void testGeoPointReset() throws IOException { double lat = 1 + randomDouble() * 89; double lon = 1 + randomDouble() * 179; GeoPoint point = new GeoPoint(0, 0); GeoPoint point2 = new GeoPoint(0, 0); assertPointsEqual(point, point2); assertPointsEqual(point.reset(lat, lon), point2.reset(lat, lon)); assertPointsEqual(point.reset(0, 0), point2.reset(0, 0)); assertPointsEqual(point.resetLat(lat), point2.reset(lat, 0)); assertPointsEqual(point.resetLat(0), point2.reset(0, 0)); assertPointsEqual(point.resetLon(lon), point2.reset(0, lon)); assertPointsEqual(point.resetLon(0), point2.reset(0, 0)); assertCloseTo(point.resetFromGeoHash(stringEncode(lon, lat)), lat, lon); assertPointsEqual(point.reset(0, 0), point2.reset(0, 0)); assertPointsEqual(point.resetFromString(Double.toString(lat) + ", " + Double.toHexString(lon)), point2.reset(lat, lon)); assertPointsEqual(point.reset(0, 0), point2.reset(0, 0)); assertPointsEqual(point.resetFromString("POINT(" + lon + " " + lat + ")"), point2.reset(lat, lon)); }	could you also add a negative test that makes sure that we handling something like "not a point" or "multipoint (10 40, 40 30, 20 20, 30 10)" in a reasonable way?
public void testPrefixLogger() throws IOException, IllegalAccessException { setupLogging("prefix"); final String prefix = randomAsciiOfLength(16); final Logger logger = Loggers.getLogger("prefix", prefix); logger.info("test"); logger.info("{}", "test"); final Exception e = new Exception("exception"); logger.info(new ParameterizedMessage("{}", "test"), e); final String path = System.getProperty("es.logs") + ".log"; final List<String> events = Files.readAllLines(PathUtils.get(path)); final StringWriter sw = new StringWriter(); final PrintWriter pw = new PrintWriter(sw); e.printStackTrace(pw); final int stackTraceLength = sw.toString().split(System.getProperty("line.separator")).length; final int expectedLogLines = 3; assertThat(events.size(), equalTo(expectedLogLines + stackTraceLength)); for (int i = 0; i < expectedLogLines; i++) { assertThat(events.get(i), startsWith("[" + prefix + "]")); } }	should we add an empty prefix (null) option?
public Metadata withLifecycleState(final Index index, final LifecycleExecutionState lifecycleState) { Objects.requireNonNull(index, "index must not be null"); Objects.requireNonNull(lifecycleState, "lifecycleState must not be null"); // build a new index metadata with the version incremented and the new lifecycle state IndexMetadata indexMetadata = getIndexSafe(index); IndexMetadata.Builder indexMetadataBuilder = IndexMetadata.builder(indexMetadata); indexMetadataBuilder.version(indexMetadataBuilder.version() + 1); indexMetadataBuilder.putCustom(ILM_CUSTOM_METADATA_KEY, lifecycleState.asMap()); // drop it into the indices final ImmutableOpenMap.Builder<String, IndexMetadata> builder = ImmutableOpenMap.builder(indices); builder.put(index.getName(), indexMetadataBuilder.build()); return new Metadata( clusterUUID, clusterUUIDCommitted, version, coordinationMetadata, transientSettings, persistentSettings, settings, hashesOfConsistentSettings, totalNumberOfShards, totalOpenIndexShards, builder.build(), aliasedIndices, templates, customs, allIndices, visibleIndices, allOpenIndices, visibleOpenIndices, allClosedIndices, visibleClosedIndices, indicesLookup, mappingsByHash, oldestIndexVersion ); }	i think it might be worth adding a comment about why this doesn't use metadata.builder(this).etc(...) but directly constructs the metadata, so that it doesn't get accidentally undone in the future. what do you think?
public void setupTestPlugin() throws InterruptedException { TestPlugin testPlugin = getTestPlugin(); testPlugin.preMigrationHook.set((state) -> Collections.emptyMap()); testPlugin.postMigrationHook.set((state, metadata) -> {}); }	you could make this method generic and take the class instance for the plugin class to retrieve, so that it can be reused in the multi test.
public void testSearchWithSuggest() throws IOException { SearchRequest searchRequest = new SearchRequest("index"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.suggest(new SuggestBuilder().addSuggestion("sugg1", new PhraseSuggestionBuilder("type")) .setGlobalText("type")); searchSourceBuilder.size(0); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = execute(searchRequest, highLevelClient()::search, highLevelClient()::searchAsync); assertSearchHeader(searchResponse); assertNull(searchResponse.getAggregations()); assertEquals(Collections.emptyMap(), searchResponse.getProfileResults()); assertEquals(0, searchResponse.getHits().totalHits); assertEquals(Float.NaN, searchResponse.getHits().getMaxScore(), 0f); assertEquals(0, searchResponse.getHits().getHits().length); assertEquals(1, searchResponse.getSuggest().size()); Suggest.Suggestion<? extends Suggest.Suggestion.Entry<? extends Suggest.Suggestion.Entry.Option>> sugg = searchResponse .getSuggest().iterator().next(); assertEquals("sugg1", sugg.getName()); for (Suggest.Suggestion.Entry<? extends Suggest.Suggestion.Entry.Option> options : sugg) { assertEquals("type", options.getText().string()); assertEquals(0, options.getOffset()); assertEquals(4, options.getLength()); assertEquals(2 ,options.getOptions().size()); for (Suggest.Suggestion.Entry.Option option : options) { assertThat(option.getScore(), greaterThan(0f)); assertThat(option.getText().string(), either(equalTo("type1")).or(equalTo("type2"))); } } }	i am not too happy that all the failures i got came from integration tests and docs tests. i added some assertions to queryphasetests but maybe that is not enough. suggestions are welcome on where to add tests for this change.
public void testSerializingWithRuntimeFieldsBeforeSupportedThrows() { SearchSourceBuilder original = new SearchSourceBuilder().runtimeMappings(randomRuntimeMappings()); Version v = Version.V_8_0_0.minimumCompatibilityVersion(); Exception e = expectThrows(IllegalArgumentException.class, () -> copyBuilder(original, v)); assertThat(e.getMessage(), equalTo("Versions before 8.0.0 don't support [runtime_mappings] and search was sent to [" + v + "]")); }	can you extract this behaviour to the description of the pr so it is highlighted? i think it makes sense but it differs from how we've done these things in the past.
@Override protected MultiSearchResponse shardOperation(Request request, ShardId shardId) throws IOException { final IndexService indexService = indicesService.indexService(shardId.getIndex()); final IndexShard indexShard = indicesService.getShardOrNull(shardId); try (Engine.Searcher searcher = indexShard.acquireSearcher("enrich_msearch")) { final FieldsVisitor visitor = new FieldsVisitor(true); /* * Enrich doesn't support defining runtime fields in the search * request so we use an empty map to signal that they aren't * any. */ Map<String, Object> runtimeFields = emptyMap(); final QueryShardContext context = indexService.newQueryShardContext( shardId.id(), searcher, () -> { throw new UnsupportedOperationException(); }, null, runtimeFields ); final MultiSearchResponse.Item[] items = new MultiSearchResponse.Item[request.multiSearchRequest.requests().size()]; for (int i = 0; i < request.multiSearchRequest.requests().size(); i++) { final SearchSourceBuilder searchSourceBuilder = request.multiSearchRequest.requests().get(i).source(); final QueryBuilder queryBuilder = searchSourceBuilder.query(); final int from = searchSourceBuilder.from(); final int size = searchSourceBuilder.size(); final FetchSourceContext fetchSourceContext = searchSourceBuilder.fetchSource(); final Query luceneQuery = queryBuilder.rewrite(context).toQuery(context); final int n = from + size; final TopDocs topDocs = searcher.search(luceneQuery, n, new Sort(SortField.FIELD_DOC)); final SearchHit[] hits = new SearchHit[topDocs.scoreDocs.length]; for (int j = 0; j < topDocs.scoreDocs.length; j++) { final ScoreDoc scoreDoc = topDocs.scoreDocs[j]; visitor.reset(); searcher.doc(scoreDoc.doc, visitor); visitor.postProcess(field -> { if (context.isFieldMapped(field) == false) { throw new IllegalStateException("Field [" + field + "] exists in the index but not in mappings"); } return context.getFieldType(field); }); final SearchHit hit = new SearchHit(scoreDoc.doc, visitor.id(), Map.of(), Map.of()); hit.sourceRef(filterSource(fetchSourceContext, visitor.source())); hits[j] = hit; } items[i] = new MultiSearchResponse.Item(createSearchResponse(topDocs, hits), null); } return new MultiSearchResponse(items, 1L); } } } private static BytesReference filterSource(FetchSourceContext fetchSourceContext, BytesReference source) throws IOException { if (fetchSourceContext.includes().length == 0 && fetchSourceContext.excludes().length == 0) { return source; } Set<String> includes = Set.of(fetchSourceContext.includes()); Set<String> excludes = Set.of(fetchSourceContext.excludes()); XContentBuilder builder = new XContentBuilder( XContentType.SMILE.xContent(), new BytesStreamOutput(source.length()), includes, excludes ); XContentParser sourceParser = XContentHelper.createParser( NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, source, XContentType.SMILE ); builder.copyCurrentStructure(sourceParser); return BytesReference.bytes(builder); }	you have expanded the comment but not the why :) what made you decide that enrich does not support runtime fields defined in the search request?
@Override public Map<String, AnalysisProvider<TokenFilterFactory>> getTokenFilters() { Map<String, AnalysisProvider<TokenFilterFactory>> filters = new TreeMap<>(); filters.put("apostrophe", ApostropheFilterFactory::new); filters.put("arabic_normalization", ArabicNormalizationFilterFactory::new); filters.put("arabic_stem", ArabicStemTokenFilterFactory::new); filters.put("asciifolding", ASCIIFoldingTokenFilterFactory::new); filters.put("bengali_normalization", BengaliNormalizationFilterFactory::new); filters.put("brazilian_stem", BrazilianStemTokenFilterFactory::new); filters.put("cjk_bigram", CJKBigramFilterFactory::new); filters.put("cjk_width", CJKWidthFilterFactory::new); filters.put("classic", ClassicFilterFactory::new); filters.put("czech_stem", CzechStemTokenFilterFactory::new); filters.put("common_grams", requiresAnalysisSettings(CommonGramsTokenFilterFactory::new)); filters.put("condition", requiresAnalysisSettings((i, e, n, s) -> new ScriptedConditionTokenFilterFactory(i, n, s, scriptService.get()))); filters.put("decimal_digit", DecimalDigitFilterFactory::new); filters.put("delimited_payload_filter", LegacyDelimitedPayloadTokenFilterFactory::new); filters.put("delimited_payload", DelimitedPayloadTokenFilterFactory::new); filters.put("dictionary_decompounder", requiresAnalysisSettings(DictionaryCompoundWordTokenFilterFactory::new)); filters.put("dutch_stem", DutchStemTokenFilterFactory::new); filters.put("edge_ngram", EdgeNGramTokenFilterFactory::new); filters.put("edgeNGram", (IndexSettings indexSettings, Environment environment, String name, Settings settings) -> { if (indexSettings.getIndexVersionCreated().onOrAfter(org.elasticsearch.Version.V_7_6_0)) { throw new IllegalArgumentException("The [edgeNGram] token filter name was deprecated in 6.4 and " + "cannot be used in new indices. Please change the filter name to [edge_ngram] instead."); } else { deprecationLogger.deprecatedAndMaybeLog("edgeNGram_deprecation", "The [edgeNGram] token filter name is deprecated and will be removed in a future version. " + "Please change the filter name to [edge_ngram] instead."); } return new EdgeNGramTokenFilterFactory(indexSettings, environment, name, settings); }); filters.put("elision", requiresAnalysisSettings(ElisionTokenFilterFactory::new)); filters.put("fingerprint", FingerprintTokenFilterFactory::new); filters.put("flatten_graph", FlattenGraphTokenFilterFactory::new); filters.put("french_stem", FrenchStemTokenFilterFactory::new); filters.put("german_normalization", GermanNormalizationFilterFactory::new); filters.put("german_stem", GermanStemTokenFilterFactory::new); filters.put("hindi_normalization", HindiNormalizationFilterFactory::new); filters.put("hyphenation_decompounder", requiresAnalysisSettings(HyphenationCompoundWordTokenFilterFactory::new)); filters.put("indic_normalization", IndicNormalizationFilterFactory::new); filters.put("keep", requiresAnalysisSettings(KeepWordFilterFactory::new)); filters.put("keep_types", requiresAnalysisSettings(KeepTypesFilterFactory::new)); filters.put("keyword_marker", requiresAnalysisSettings(KeywordMarkerTokenFilterFactory::new)); filters.put("kstem", KStemTokenFilterFactory::new); filters.put("length", LengthTokenFilterFactory::new); filters.put("limit", LimitTokenCountFilterFactory::new); filters.put("lowercase", LowerCaseTokenFilterFactory::new); filters.put("min_hash", MinHashTokenFilterFactory::new); filters.put("multiplexer", MultiplexerTokenFilterFactory::new); filters.put("ngram", NGramTokenFilterFactory::new); filters.put("nGram", (IndexSettings indexSettings, Environment environment, String name, Settings settings) -> { if (indexSettings.getIndexVersionCreated().onOrAfter(org.elasticsearch.Version.V_7_6_0)) { throw new IllegalArgumentException("The [nGram] token filter name was deprecated in 6.4 and cannot be used in new indices. " + "Please change the filter name to [ngram] instead."); } else { deprecationLogger.deprecatedAndMaybeLog("nGram_deprecation", "The [nGram] token filter name is deprecated and will be removed in a future version. " + "Please change the filter name to [ngram] instead."); } return new NGramTokenFilterFactory(indexSettings, environment, name, settings); }); filters.put("pattern_capture", requiresAnalysisSettings(PatternCaptureGroupTokenFilterFactory::new)); filters.put("pattern_replace", requiresAnalysisSettings(PatternReplaceTokenFilterFactory::new)); filters.put("persian_normalization", PersianNormalizationFilterFactory::new); filters.put("porter_stem", PorterStemTokenFilterFactory::new); filters.put("predicate_token_filter", requiresAnalysisSettings((i, e, n, s) -> new PredicateTokenFilterScriptFactory(i, n, s, scriptService.get()))); filters.put("remove_duplicates", RemoveDuplicatesTokenFilterFactory::new); filters.put("reverse", ReverseTokenFilterFactory::new); filters.put("russian_stem", RussianStemTokenFilterFactory::new); filters.put("scandinavian_folding", ScandinavianFoldingFilterFactory::new); filters.put("scandinavian_normalization", ScandinavianNormalizationFilterFactory::new); filters.put("serbian_normalization", SerbianNormalizationFilterFactory::new); filters.put("snowball", SnowballTokenFilterFactory::new); filters.put("sorani_normalization", SoraniNormalizationFilterFactory::new); filters.put("stemmer_override", requiresAnalysisSettings(StemmerOverrideTokenFilterFactory::new)); filters.put("stemmer", StemmerTokenFilterFactory::new); filters.put("synonym", requiresAnalysisSettings(SynonymTokenFilterFactory::new)); filters.put("synonym_graph", requiresAnalysisSettings(SynonymGraphTokenFilterFactory::new)); filters.put("trim", TrimTokenFilterFactory::new); filters.put("truncate", requiresAnalysisSettings(TruncateTokenFilterFactory::new)); filters.put("unique", UniqueTokenFilterFactory::new); filters.put("uppercase", UpperCaseTokenFilterFactory::new); filters.put("word_delimiter_graph", WordDelimiterGraphTokenFilterFactory::new); filters.put("word_delimiter", WordDelimiterTokenFilterFactory::new); return filters; }	unfortunately we can only error for these cases starting with 7.6 now since there might already be existing indices <7.6 that we don't want to break on minor version upgrade. those should at least be logging a deprecation now.
private boolean assertPendingRangeExists(Range range) { assert Thread.holdsLock(mutex); final SortedSet<Range> existingRanges = ranges.tailSet(range); assert existingRanges.isEmpty() == false; final Range existingRange = existingRanges.first(); assert existingRange.start == range.start && existingRange.end == range.end && existingRange.isPending(); return true; }	can we assert existingrange == range? i think it should always be the very same object here.
private boolean assertPendingRangeExists(Range range) { assert Thread.holdsLock(mutex); final SortedSet<Range> existingRanges = ranges.tailSet(range); assert existingRanges.isEmpty() == false; final Range existingRange = existingRanges.first(); assert existingRange.start == range.start && existingRange.end == range.end && existingRange.isPending(); return true; }	naming suggestion (here and elsewhere): suggestion private void ongapsuccess(final range gaprange) {
@Override protected GetFieldMappingsResponse shardOperation(final GetFieldMappingsIndexRequest request, ShardId shardId) { assert shardId != null; IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex()); Predicate<String> metadataFieldPredicate = (f) -> indexService.mapperService().isMetadataField(f); Predicate<String> baseFieldPredicate = metadataFieldPredicate.or(indicesService.getFieldFilter().apply(shardId.getIndexName())); DocumentMapper documentParser = indexService.mapperService().documentMapper(); if (documentParser == null) { // no mappings return new GetFieldMappingsResponse(singletonMap(shardId.getIndexName(), Collections.emptyMap())); } Predicate<String> fieldPredicate = fieldPredicate(baseFieldPredicate, request.fields()); ToXContent.Params params = request.includeDefaults() ? new ToXContent.MapParams(Collections.singletonMap("include_defaults", "true")) : ToXContent.EMPTY_PARAMS; Map<String, FieldMappingMetadata> mappings = new HashMap<>(); documentParser.mapping().forEachMapper(m -> { FieldMappingMetadata metadata = buildFieldMappingMetadata(fieldPredicate, m.name(), m, params); if (metadata != null) { mappings.put(metadata.fullName(), metadata); } }); for (RuntimeField rf : documentParser.mapping().runtimeFields()) { FieldMappingMetadata metadata = buildFieldMappingMetadata(fieldPredicate, rf.name(), rf, params); if (metadata != null) { mappings.put(metadata.fullName(), metadata); } } return new GetFieldMappingsResponse(singletonMap(shardId.getIndexName(), mappings)); }	i think i would use mappinglookup. we have been replacing usages of documentmapper in favour of mappinglookup, they are effectively the same, and one advantage with mapping lookup is that you no longer need the null check.
@Override protected GetFieldMappingsResponse shardOperation(final GetFieldMappingsIndexRequest request, ShardId shardId) { assert shardId != null; IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex()); Predicate<String> metadataFieldPredicate = (f) -> indexService.mapperService().isMetadataField(f); Predicate<String> baseFieldPredicate = metadataFieldPredicate.or(indicesService.getFieldFilter().apply(shardId.getIndexName())); DocumentMapper documentParser = indexService.mapperService().documentMapper(); if (documentParser == null) { // no mappings return new GetFieldMappingsResponse(singletonMap(shardId.getIndexName(), Collections.emptyMap())); } Predicate<String> fieldPredicate = fieldPredicate(baseFieldPredicate, request.fields()); ToXContent.Params params = request.includeDefaults() ? new ToXContent.MapParams(Collections.singletonMap("include_defaults", "true")) : ToXContent.EMPTY_PARAMS; Map<String, FieldMappingMetadata> mappings = new HashMap<>(); documentParser.mapping().forEachMapper(m -> { FieldMappingMetadata metadata = buildFieldMappingMetadata(fieldPredicate, m.name(), m, params); if (metadata != null) { mappings.put(metadata.fullName(), metadata); } }); for (RuntimeField rf : documentParser.mapping().runtimeFields()) { FieldMappingMetadata metadata = buildFieldMappingMetadata(fieldPredicate, rf.name(), rf, params); if (metadata != null) { mappings.put(metadata.fullName(), metadata); } } return new GetFieldMappingsResponse(singletonMap(shardId.getIndexName(), mappings)); }	i wonder why we used to do this so differently and make it so much more complicated.
public static ShapeBuilder parseExpectedType(XContentParser parser, final GeoShapeType shapeType, final BaseGeoShapeFieldMapper shapeMapper) throws IOException, ElasticsearchParseException { String text = parser.text(); boolean ignoreZValue = ((shapeMapper == null) ? BaseGeoShapeFieldMapper.Defaults.IGNORE_Z_VALUE : shapeMapper.ignoreZValue()).value(); boolean coerce = ((shapeMapper == null) ? BaseGeoShapeFieldMapper.Defaults.COERCE : shapeMapper.coerce()).value(); return parseExpectedType(text, shapeType, ignoreZValue, coerce); }	let's revert this since we are not going to be using this parser.
* @param settings the settings * @param validateDependencies true if dependent settings should be validated * @param validateInternalOrPrivateIndex true if internal index settings should be validated * @throws IllegalArgumentException if the setting is invalid */ void validate( final String key, final Settings settings, final boolean validateDependencies, final boolean validateInternalOrPrivateIndex) { Setting setting = getRaw(key); if (setting == null) { LevenshteinDistance ld = new LevenshteinDistance(); List<Tuple<Float, String>> scoredKeys = new ArrayList<>(); for (String k : this.keySettings.keySet()) { float distance = ld.getDistance(key, k); if (distance > 0.7f) { scoredKeys.add(new Tuple<>(distance, k)); } } CollectionUtil.timSort(scoredKeys, (a,b) -> b.v1().compareTo(a.v1())); String msgPrefix = "unknown setting"; SecureSettings secureSettings = settings.getSecureSettings(); if (secureSettings != null && settings.getSecureSettings().getSettingNames().contains(key)) { msgPrefix = "unknown secure setting"; } String msg = msgPrefix + " [" + key + "]"; List<String> keys = scoredKeys.stream().map((a) -> a.v2()).collect(Collectors.toList()); if (keys.isEmpty() == false) { msg += " did you mean " + (keys.size() == 1 ? "[" + keys.get(0) + "]": "any of " + keys.toString()) + "?"; } else { msg += " please check that any required plugins are installed, or check the breaking changes documentation for removed " + "settings"; } throw new IllegalArgumentException(msg); } else { Set<Setting<?>> settingsDependencies = setting.getSettingsDependencies(key); if (setting.hasComplexMatcher()) { setting = setting.getConcreteSetting(key); } if (validateDependencies && settingsDependencies.isEmpty() == false) { for (final Setting<?> settingDependency : settingsDependencies) { if (settingDependency.exists(settings) == false) { final String message = String.format( Locale.ROOT, "missing required setting [%s] for setting [%s]", settingDependency.getKey(), setting.getKey()); throw new IllegalArgumentException(message); } } } // the only time that validateInternalOrPrivateIndex should be true is if this call is coming via the update settings API if (validateInternalOrPrivateIndex) { if (setting.isInternalIndex()) { throw new IllegalArgumentException( "can not update internal setting [" + setting.getKey() + "]; this setting is managed via a dedicated API"); } else if (setting.isPrivateIndex()) { throw new IllegalArgumentException( "can not update private setting [" + setting.getKey() + "]; this setting is managed by Elasticsearch"); } } } setting.get(settings); }	this change is made so that we can validate dependencies exist using the actual settings object, and can therefore use fallback settings when doing the validation, which would not be possible if using the top-level dependent key only.
* @param settings the settings * @param validateDependencies true if dependent settings should be validated * @param validateInternalOrPrivateIndex true if internal index settings should be validated * @throws IllegalArgumentException if the setting is invalid */ void validate( final String key, final Settings settings, final boolean validateDependencies, final boolean validateInternalOrPrivateIndex) { Setting setting = getRaw(key); if (setting == null) { LevenshteinDistance ld = new LevenshteinDistance(); List<Tuple<Float, String>> scoredKeys = new ArrayList<>(); for (String k : this.keySettings.keySet()) { float distance = ld.getDistance(key, k); if (distance > 0.7f) { scoredKeys.add(new Tuple<>(distance, k)); } } CollectionUtil.timSort(scoredKeys, (a,b) -> b.v1().compareTo(a.v1())); String msgPrefix = "unknown setting"; SecureSettings secureSettings = settings.getSecureSettings(); if (secureSettings != null && settings.getSecureSettings().getSettingNames().contains(key)) { msgPrefix = "unknown secure setting"; } String msg = msgPrefix + " [" + key + "]"; List<String> keys = scoredKeys.stream().map((a) -> a.v2()).collect(Collectors.toList()); if (keys.isEmpty() == false) { msg += " did you mean " + (keys.size() == 1 ? "[" + keys.get(0) + "]": "any of " + keys.toString()) + "?"; } else { msg += " please check that any required plugins are installed, or check the breaking changes documentation for removed " + "settings"; } throw new IllegalArgumentException(msg); } else { Set<Setting<?>> settingsDependencies = setting.getSettingsDependencies(key); if (setting.hasComplexMatcher()) { setting = setting.getConcreteSetting(key); } if (validateDependencies && settingsDependencies.isEmpty() == false) { for (final Setting<?> settingDependency : settingsDependencies) { if (settingDependency.exists(settings) == false) { final String message = String.format( Locale.ROOT, "missing required setting [%s] for setting [%s]", settingDependency.getKey(), setting.getKey()); throw new IllegalArgumentException(message); } } } // the only time that validateInternalOrPrivateIndex should be true is if this call is coming via the update settings API if (validateInternalOrPrivateIndex) { if (setting.isInternalIndex()) { throw new IllegalArgumentException( "can not update internal setting [" + setting.getKey() + "]; this setting is managed via a dedicated API"); } else if (setting.isPrivateIndex()) { throw new IllegalArgumentException( "can not update private setting [" + setting.getKey() + "]; this setting is managed by Elasticsearch"); } } } setting.get(settings); }	here we change from matching the key to using exists, so that we can use fallback settings if needed.
public static <T> Setting<List<T>> listSetting( final String key, final @Nullable Setting<List<T>> fallbackSetting, final Function<String, T> singleValueParser, final Function<Settings, List<String>> defaultStringValue, final Property... properties) { if (defaultStringValue.apply(Settings.EMPTY) == null) { throw new IllegalArgumentException("default value function must not return null"); } Function<String, List<T>> parser = (s) -> parseableStringToList(s).stream().map(singleValueParser).collect(Collectors.toList()); return new ListSetting<>(key, fallbackSetting, defaultStringValue, parser, properties); }	fallback settings were not being pushed down here, so listsettings did not know until this change that they could have fallback settings.
public boolean exists(final Settings settings) { Setting<?> current = this; do { if (settings.keySet().contains(current.getKey())) { return true; } current = current.fallbackSetting; } while (current != null); return false; }	this method was not checking fallback settings, so we could get false here if the fallback setting was set but not the actual setting.
public static ElasticsearchStatusException errorFromXContent(XContentParser parser) throws IOException { XContentParser.Token token = parser.nextToken(); ensureExpectedToken(XContentParser.Token.START_OBJECT, token, parser::getTokenLocation); ElasticsearchException exception = null; RestStatus status = null; String currentFieldName = parser.currentName(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } if (STATUS.equals(currentFieldName)) { if (token.isValue()) { status = RestStatus.fromCode(parser.intValue()); } } else { exception = ElasticsearchException.failureFromXContent(parser); } } if (exception == null) { throw new IllegalStateException("Failed to parse elasticsearch status exception: no exception was found"); } ElasticsearchStatusException result = new ElasticsearchStatusException(exception.getMessage(), status, exception.getCause()); for (String header : exception.getHeaderKeys()) { result.addHeader(header, exception.getHeader(header)); } for (String metadata : exception.getMetadataKeys()) { result.addMetadata(metadata, exception.getMetadata(metadata)); } return result; }	i think you can just initialize to null
public static ElasticsearchStatusException errorFromXContent(XContentParser parser) throws IOException { XContentParser.Token token = parser.nextToken(); ensureExpectedToken(XContentParser.Token.START_OBJECT, token, parser::getTokenLocation); ElasticsearchException exception = null; RestStatus status = null; String currentFieldName = parser.currentName(); while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) { if (token == XContentParser.Token.FIELD_NAME) { currentFieldName = parser.currentName(); } if (STATUS.equals(currentFieldName)) { if (token.isValue()) { status = RestStatus.fromCode(parser.intValue()); } } else { exception = ElasticsearchException.failureFromXContent(parser); } } if (exception == null) { throw new IllegalStateException("Failed to parse elasticsearch status exception: no exception was found"); } ElasticsearchStatusException result = new ElasticsearchStatusException(exception.getMessage(), status, exception.getCause()); for (String header : exception.getHeaderKeys()) { result.addHeader(header, exception.getHeader(header)); } for (String metadata : exception.getMetadataKeys()) { result.addMetadata(metadata, exception.getMetadata(metadata)); } return result; }	this can't ever be anything else than a value right? shall we throw exception?
*/ void add(ShardRouting shard) { if (shards.containsKey(shard.shardId())) { throw new IllegalStateException("Trying to add a shard " + shard.shardId() + " to a node [" + nodeId + "] where it already exists. current [" + shards.get(shard.shardId()) + "]. new [" + shard + "]"); } shards.put(shard.shardId(), shard); if (shard.initializing()) { initializingShards.add(shard); } else if(shard.relocating()) { relocatingShards.add(shard); } assert invariant(); }	whitespace nit: suggestion } else if (shard.relocating()) {
* @return number of shards */ public int numberOfShardsWithState(ShardRoutingState... states) { if (states.length == 1) { if(states[0] == ShardRoutingState.INITIALIZING) { return initializingShards.size(); } else if (states[0] == ShardRoutingState.RELOCATING) { return relocatingShards.size(); } } int count = 0; for (ShardRouting shardEntry : this) { for (ShardRoutingState state : states) { if (shardEntry.state() == state) { count++; } } } return count; }	whitespace nit: suggestion if (states[0] == shardroutingstate.initializing) {
* @return List of shards */ public List<ShardRouting> shardsWithState(ShardRoutingState... states) { if (states.length == 1) { if(states[0] == ShardRoutingState.INITIALIZING) { return new ArrayList<>(initializingShards); } else if (states[0] == ShardRoutingState.RELOCATING) { return new ArrayList<>(relocatingShards); } } List<ShardRouting> shards = new ArrayList<>(); for (ShardRouting shardEntry : this) { for (ShardRoutingState state : states) { if (shardEntry.state() == state) { shards.add(shardEntry); } } } return shards; }	whitespace nit: suggestion if (states[0] == shardroutingstate.initializing) {
* @param states set of states which should be listed * @return a list of shards */ public List<ShardRouting> shardsWithState(String index, ShardRoutingState... states) { List<ShardRouting> shards = new ArrayList<>(); if (states.length == 1) { if(states[0] == ShardRoutingState.INITIALIZING) { for (ShardRouting shardEntry : initializingShards) { if (shardEntry.getIndexName().equals(index) == false) { continue; } shards.add(shardEntry); } return shards; } else if (states[0] == ShardRoutingState.RELOCATING) { for (ShardRouting shardEntry : relocatingShards) { if (shardEntry.getIndexName().equals(index) == false) { continue; } shards.add(shardEntry); } return shards; } } for (ShardRouting shardEntry : this) { if (!shardEntry.getIndexName().equals(index)) { continue; } for (ShardRoutingState state : states) { if (shardEntry.state() == state) { shards.add(shardEntry); } } } return shards; }	whitespace nit: suggestion if (states[0] == shardroutingstate.initializing) {
public boolean isEmpty() { return shards.isEmpty(); }	please would you also assert this on entry to the methods that change the state (add(), remove() and update())
boolean checkSameUserPermissions(String action, TransportRequest request, Authentication authentication) { final boolean actionAllowed = SAME_USER_PRIVILEGE.test(action); if (actionAllowed) { if (request instanceof UserRequest) { UserRequest userRequest = (UserRequest) request; String[] usernames = userRequest.usernames(); if (usernames == null || usernames.length != 1 || usernames[0] == null) { assert false : "this role should only be used for actions to apply to a single user"; return false; } final String username = usernames[0]; final boolean sameUsername = authentication.getUser().principal().equals(username); if (sameUsername && ChangePasswordAction.NAME.equals(action)) { return checkChangePasswordAction(authentication); } assert AuthenticateAction.NAME.equals(action) || HasPrivilegesAction.NAME.equals(action) || GetUserPrivilegesAction.NAME.equals(action) || sameUsername == false : "Action '" + action + "' should not be possible when sameUsername=" + sameUsername; return sameUsername; } else if (request instanceof GetApiKeyRequest) { GetApiKeyRequest getApiKeyRequest = (GetApiKeyRequest) request; if (authentication.getAuthenticatedBy().getType().equals(ApiKeyService.API_KEY_REALM_TYPE)) { assert authentication.getLookedUpBy() == null : "runAs not supported for api key authentication"; // if authenticated by API key then the request must also contain same API key id String authenticatedApiKeyId = (String) authentication.getMetadata().get(ApiKeyService.API_KEY_ID_KEY); if (Strings.hasText(getApiKeyRequest.getApiKeyId())) { return getApiKeyRequest.getApiKeyId().equals(authenticatedApiKeyId); } } } else { assert false : "right now only a user request or get api key request should be allowed"; return false; } } return false; }	} else { return false; } rather than fall through to the last else branch, especially since that has an assert.
private void initialize(String jobId) { this.jobId = jobId; this.sourceIndex = jobId + "_source_index"; this.destIndex = sourceIndex + "_results"; this.analysisUsesExistingDestIndex = false; createIndex(sourceIndex); if (analysisUsesExistingDestIndex) { createIndex(destIndex); } }	should this be reverted?
public Builder scalingFactor(double scalingFactor) { this.scalingFactor = scalingFactor; scalingFactorSet = true; return this; }	this is so much better!
@Override protected void mergeOptions(FieldMapper other, List<String> conflicts) { // TODO we should ban updating analyzers as well if (this.enablePositionIncrements != ((TokenCountFieldMapper)other).enablePositionIncrements) { conflicts.add("mapper [" + name() + "] has a different [enable_position_increments] setting"); } if (Objects.equals(this.nullValue, ((TokenCountFieldMapper)other).nullValue) == false) { conflicts.add("mapper [" + name() + "] has a different [null_value] setting"); } this.analyzer = ((TokenCountFieldMapper)other).analyzer; }	for my info, are these newly introduced checks or were they just moved?
@Override protected Settings nodeSettings(int nodeOrdinal, Settings otherSettings) { Settings.Builder settings = Settings.builder().put(super.nodeSettings(nodeOrdinal, otherSettings)); if (ENDPOINT != null) { String endpoint = getTestName().endsWith("Cli") ? ENDPOINT + "cli/overview.json" : ENDPOINT; settings.put(GeoIpDownloader.ENDPOINT_SETTING.getKey(), endpoint); } return settings.build(); }	maybe set/overwrite the custom endpoint for cli files in the testgeoipdatabasesdownloadcli() test instead of here with a conditional? i think it is clearer to have this logic where it is used.
public void testReadNonexistentBlobThrowsNoSuchFileException() { final BlobContainer blobContainer = createBlobContainer(between(1, 5), null, null, null); final Exception exception = expectThrows(NoSuchFileException.class, () -> blobContainer.readBlob("read_nonexistent_blob")); assertThat(exception.getMessage().toLowerCase(Locale.ROOT), containsString("blob object [read_nonexistent_blob] not found")); }	nit: can fit on the same line
public void testReadBlobWithPrematureConnectionClose() { final BlobContainer blobContainer = createBlobContainer(between(1, 5), null, null, null); // HTTP server closes connection immediately httpServer.createContext("/bucket/read_blob_no_response", HttpExchange::close); Exception exception = expectThrows(SdkClientException.class, () -> blobContainer.readBlob("read_blob_no_response")); assertThat(exception.getMessage().toLowerCase(Locale.ROOT), containsString("the target server failed to respond")); assertThat(exception.getCause(), instanceOf(NoHttpResponseException.class)); // HTTP server sends a partial response final byte[] bytes = randomBlobContent(); httpServer.createContext("/bucket/read_blob_incomplete", exchange -> { sendIncompleteContent(exchange, bytes); exchange.close(); }); exception = expectThrows(ConnectionClosedException.class, () -> { try (InputStream stream = blobContainer.readBlob("read_blob_incomplete")) { Streams.readFully(stream); } }); assertThat(exception.getMessage().toLowerCase(Locale.ROOT), containsString("premature end of content-length delimited message body")); }	nit: can we move this at the end of the class file?
public void testReadBlobWithPrematureConnectionClose() { final BlobContainer blobContainer = createBlobContainer(between(1, 5), null, null, null); // HTTP server closes connection immediately httpServer.createContext("/bucket/read_blob_no_response", HttpExchange::close); Exception exception = expectThrows(SdkClientException.class, () -> blobContainer.readBlob("read_blob_no_response")); assertThat(exception.getMessage().toLowerCase(Locale.ROOT), containsString("the target server failed to respond")); assertThat(exception.getCause(), instanceOf(NoHttpResponseException.class)); // HTTP server sends a partial response final byte[] bytes = randomBlobContent(); httpServer.createContext("/bucket/read_blob_incomplete", exchange -> { sendIncompleteContent(exchange, bytes); exchange.close(); }); exception = expectThrows(ConnectionClosedException.class, () -> { try (InputStream stream = blobContainer.readBlob("read_blob_incomplete")) { Streams.readFully(stream); } }); assertThat(exception.getMessage().toLowerCase(Locale.ROOT), containsString("premature end of content-length delimited message body")); }	nit: can be static. i also think we still prefer static methods to be located at the end of the class file?
@Override public void finalizeSnapshot(final ShardGenerations shardGenerations, final long repositoryStateId, final Metadata clusterMetadata, SnapshotInfo snapshotInfo, Version repositoryMetaVersion, Function<ClusterState, ClusterState> stateTransformer, final ActionListener<RepositoryData> listener) { assert repositoryStateId > RepositoryData.UNKNOWN_REPO_GEN : "Must finalize based on a valid repository generation but received [" + repositoryStateId + "]"; final Collection<IndexId> indices = shardGenerations.indices(); final SnapshotId snapshotId = snapshotInfo.snapshotId(); // Once we are done writing the updated index-N blob we remove the now unreferenced index-${uuid} blobs in each shard // directory if all nodes are at least at version SnapshotsService#SHARD_GEN_IN_REPO_DATA_VERSION // If there are older version nodes in the cluster, we don't need to run this cleanup as it will have already happened // when writing the index-${N} to each shard directory. final boolean writeShardGens = SnapshotsService.useShardGenerations(repositoryMetaVersion); final Consumer<Exception> onUpdateFailure = e -> listener.onFailure(new SnapshotException(metadata.name(), snapshotId, "failed to update snapshot in repository", e)); final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT); final boolean writeIndexGens = SnapshotsService.useIndexGenerations(repositoryMetaVersion); final StepListener<RepositoryData> repoDataListener = new StepListener<>(); getRepositoryData(repoDataListener); repoDataListener.whenComplete(existingRepositoryData -> { final int existingSnapshotCount = existingRepositoryData.getSnapshotIds().size(); if (existingSnapshotCount >= maxSnapshotCount) { listener.onFailure(new RepositoryException(metadata.name(), "Can not add another snapshot to this repository as it " + "already contains [" + existingSnapshotCount + "] snapshots and is configured to hold up to [" + maxSnapshotCount + "] snapshots only. Please increase repository setting [" + MAX_SNAPSHOTS_SETTING.getKey() + "] to be able to add additional snapshots to this repository.")); return; } final Map<IndexId, String> indexMetas; final Map<String, String> indexMetaIdentifiers; if (writeIndexGens) { indexMetaIdentifiers = ConcurrentCollections.newConcurrentMap(); indexMetas = ConcurrentCollections.newConcurrentMap(); } else { indexMetas = null; indexMetaIdentifiers = null; } final ActionListener<Void> allMetaListener = new GroupedActionListener<>( ActionListener.wrap(v -> { final RepositoryData updatedRepositoryData = existingRepositoryData.addSnapshot( snapshotId, snapshotInfo.state(), Version.CURRENT, shardGenerations, indexMetas, indexMetaIdentifiers); writeIndexGen(updatedRepositoryData, repositoryStateId, repositoryMetaVersion, stateTransformer, ActionListener.wrap( newRepoData -> { if (writeShardGens) { cleanupOldShardGens(existingRepositoryData, updatedRepositoryData); } listener.onResponse(newRepoData); }, onUpdateFailure)); }, onUpdateFailure), 2 + indices.size()); // We ignore all FileAlreadyExistsException when writing metadata since otherwise a master failover while in this method will // mean that no snap-${uuid}.dat blob is ever written for this snapshot. This is safe because any updated version of the // index or global metadata will be compatible with the segments written in this snapshot as well. // Failing on an already existing index-${repoGeneration} below ensures that the index.latest blob is not updated in a way // that decrements the generation it points at // Write Global MetaData executor.execute(ActionRunnable.run(allMetaListener, () -> GLOBAL_METADATA_FORMAT.write(clusterMetadata, blobContainer(), snapshotId.getUUID(), compress, bigArrays))); // write the index metadata for each index in the snapshot for (IndexId index : indices) { executor.execute(ActionRunnable.run(allMetaListener, () -> { final IndexMetadata indexMetaData = clusterMetadata.index(index.getName()); if (writeIndexGens) { final String identifiers = IndexMetaDataGenerations.buildUniqueIdentifier(indexMetaData); String metaUUID = existingRepositoryData.indexMetaDataGenerations().getIndexMetaBlobId(identifiers); if (metaUUID == null) { // We don't yet have this version of the metadata so we write it metaUUID = UUIDs.base64UUID(); INDEX_METADATA_FORMAT.write(indexMetaData, indexContainer(index), metaUUID, compress, bigArrays); indexMetaIdentifiers.put(identifiers, metaUUID); } indexMetas.put(index, identifiers); } else { INDEX_METADATA_FORMAT.write( clusterMetadata.index(index.getName()), indexContainer(index), snapshotId.getUUID(), compress, bigArrays); } } )); } executor.execute(ActionRunnable.run(allMetaListener, () -> SNAPSHOT_FORMAT.write(snapshotInfo, blobContainer(), snapshotId.getUUID(), compress, bigArrays))); }, onUpdateFailure); }	i think we should name the setting in the message (as done here) but think we shouldn't recommend increasing it if you hit the limit and should instead suggest deleting some snapshots and/or adjusting retention settings to bring the snapshot count below the limit.
public void authorize(final Authentication authentication, final String action, final TransportRequest originalRequest, final ActionListener<Void> listener) { final String auditId; try { auditId = requireAuditId(authentication, action, originalRequest); } catch (ElasticsearchSecurityException e) { listener.onFailure(e); return; } if (checkOperatorPrivileges(authentication, action, originalRequest, listener) == false) { return; } // sometimes a request might be wrapped within another, which is the case for proxied // requests and concrete shard requests final TransportRequest unwrappedRequest = maybeUnwrapRequest(authentication, originalRequest, action, auditId); final RequestInfo requestInfo = new RequestInfo(authentication, unwrappedRequest, action); // Detect cases where a child action can be authorized automatically because the parent was authorized final AuthorizationEngine engine = getAuthorizationEngine(authentication); if (shouldAuthorizeAsChildAction(engine, requestInfo)) { authorizeChildAction(engine, requestInfo, auditId, listener); return; } /* authorization fills in certain transient headers, which must be observed in the listener (action handler execution) * as well, but which must not bleed across different action context (eg parent-child action contexts). * <p> * Therefore we begin by clearing the existing ones up, as they might already be set during the authorization of a * previous parent action that ran under the same thread context (also on the same node). * When the returned {@code StoredContext} is closed, ALL the original headers are restored. */ try (ThreadContext.StoredContext ignore = threadContext.newStoredContext(false, ACTION_SCOPE_AUTHORIZATION_KEYS)) { // this does not clear {@code AuthorizationServiceField.ORIGINATING_ACTION_KEY} // prior to doing any authorization lets set the originating action in the thread context // the originating action is the current action if no originating action has yet been set in the current thread context // if there is already an original action, that stays put (eg. the current action is a child action) putTransientIfNonExisting(ORIGINATING_ACTION_KEY, action); if (SystemUser.is(authentication.getUser())) { // this never goes async so no need to wrap the listener authorizeSystemUser(authentication, action, auditId, unwrappedRequest, listener); } else { final ActionListener<AuthorizationInfo> authzInfoListener = wrapPreservingContext(ActionListener.wrap( authorizationInfo -> { threadContext.putTransient(AUTHORIZATION_INFO_KEY, authorizationInfo); maybeAuthorizeRunAs(requestInfo, auditId, authorizationInfo, listener); }, listener::onFailure), threadContext); engine.resolveAuthorizationInfo(requestInfo, authzInfoListener); } } } /** * @return {@code true}	can this be part of shouldauthorizeaschildaction and replaced by a simple assert?
public void authorize(final Authentication authentication, final String action, final TransportRequest originalRequest, final ActionListener<Void> listener) { final String auditId; try { auditId = requireAuditId(authentication, action, originalRequest); } catch (ElasticsearchSecurityException e) { listener.onFailure(e); return; } if (checkOperatorPrivileges(authentication, action, originalRequest, listener) == false) { return; } // sometimes a request might be wrapped within another, which is the case for proxied // requests and concrete shard requests final TransportRequest unwrappedRequest = maybeUnwrapRequest(authentication, originalRequest, action, auditId); final RequestInfo requestInfo = new RequestInfo(authentication, unwrappedRequest, action); // Detect cases where a child action can be authorized automatically because the parent was authorized final AuthorizationEngine engine = getAuthorizationEngine(authentication); if (shouldAuthorizeAsChildAction(engine, requestInfo)) { authorizeChildAction(engine, requestInfo, auditId, listener); return; } /* authorization fills in certain transient headers, which must be observed in the listener (action handler execution) * as well, but which must not bleed across different action context (eg parent-child action contexts). * <p> * Therefore we begin by clearing the existing ones up, as they might already be set during the authorization of a * previous parent action that ran under the same thread context (also on the same node). * When the returned {@code StoredContext} is closed, ALL the original headers are restored. */ try (ThreadContext.StoredContext ignore = threadContext.newStoredContext(false, ACTION_SCOPE_AUTHORIZATION_KEYS)) { // this does not clear {@code AuthorizationServiceField.ORIGINATING_ACTION_KEY} // prior to doing any authorization lets set the originating action in the thread context // the originating action is the current action if no originating action has yet been set in the current thread context // if there is already an original action, that stays put (eg. the current action is a child action) putTransientIfNonExisting(ORIGINATING_ACTION_KEY, action); if (SystemUser.is(authentication.getUser())) { // this never goes async so no need to wrap the listener authorizeSystemUser(authentication, action, auditId, unwrappedRequest, listener); } else { final ActionListener<AuthorizationInfo> authzInfoListener = wrapPreservingContext(ActionListener.wrap( authorizationInfo -> { threadContext.putTransient(AUTHORIZATION_INFO_KEY, authorizationInfo); maybeAuthorizeRunAs(requestInfo, auditId, authorizationInfo, listener); }, listener::onFailure), threadContext); engine.resolveAuthorizationInfo(requestInfo, authzInfoListener); } } } /** * @return {@code true}	similar to above, i think this can just be an assert.
public void authorize(final Authentication authentication, final String action, final TransportRequest originalRequest, final ActionListener<Void> listener) { final String auditId; try { auditId = requireAuditId(authentication, action, originalRequest); } catch (ElasticsearchSecurityException e) { listener.onFailure(e); return; } if (checkOperatorPrivileges(authentication, action, originalRequest, listener) == false) { return; } // sometimes a request might be wrapped within another, which is the case for proxied // requests and concrete shard requests final TransportRequest unwrappedRequest = maybeUnwrapRequest(authentication, originalRequest, action, auditId); final RequestInfo requestInfo = new RequestInfo(authentication, unwrappedRequest, action); // Detect cases where a child action can be authorized automatically because the parent was authorized final AuthorizationEngine engine = getAuthorizationEngine(authentication); if (shouldAuthorizeAsChildAction(engine, requestInfo)) { authorizeChildAction(engine, requestInfo, auditId, listener); return; } /* authorization fills in certain transient headers, which must be observed in the listener (action handler execution) * as well, but which must not bleed across different action context (eg parent-child action contexts). * <p> * Therefore we begin by clearing the existing ones up, as they might already be set during the authorization of a * previous parent action that ran under the same thread context (also on the same node). * When the returned {@code StoredContext} is closed, ALL the original headers are restored. */ try (ThreadContext.StoredContext ignore = threadContext.newStoredContext(false, ACTION_SCOPE_AUTHORIZATION_KEYS)) { // this does not clear {@code AuthorizationServiceField.ORIGINATING_ACTION_KEY} // prior to doing any authorization lets set the originating action in the thread context // the originating action is the current action if no originating action has yet been set in the current thread context // if there is already an original action, that stays put (eg. the current action is a child action) putTransientIfNonExisting(ORIGINATING_ACTION_KEY, action); if (SystemUser.is(authentication.getUser())) { // this never goes async so no need to wrap the listener authorizeSystemUser(authentication, action, auditId, unwrappedRequest, listener); } else { final ActionListener<AuthorizationInfo> authzInfoListener = wrapPreservingContext(ActionListener.wrap( authorizationInfo -> { threadContext.putTransient(AUTHORIZATION_INFO_KEY, authorizationInfo); maybeAuthorizeRunAs(requestInfo, auditId, authorizationInfo, listener); }, listener::onFailure), threadContext); engine.resolveAuthorizationInfo(requestInfo, authzInfoListener); } } } /** * @return {@code true}	nit suggestion authorizationinfo).onresponse(authorizationresult.granted()); the above change still creates a new object underlying and we are doing this in a few other places. i wouldn't mind having a shared "granted" object that is reused. might be better because of less churn?
@Override protected SearchRequest buildSearchRequest() { SearchRequest searchRequest = new SearchRequest(getConfig().getSource().getIndex()); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.aggregation(pivot.buildAggregation(getPosition(), pageSize)); sourceBuilder.size(0); QueryBuilder pivotQueryBuilder = getConfig().getSource().getQueryConfig().getQuery(); sourceBuilder.query(pivotQueryBuilder); searchRequest.source(sourceBuilder); return searchRequest; } /** * Handle the circuit breaking case: A search consumed to much memory and got aborted. * * Going out of memory we smoothly reduce the page size which reduces memory consumption. * * Implementation details: We take the values from the circuit breaker as a hint, but * note that it breaks early, that's why we also reduce using * * @param e Exception thrown, only {@link CircuitBreakingException}	this code looks to functionally the same as pivot.buildsearchrequest(getconfig().getsource(), getposition, pagesize)
@Override public Explanation explain(LeafReaderContext context, int doc) throws IOException { shardKeyMap.add(context.reader()); return in.explain(context, doc); }	i *think* we could not call this method since weight#count would never create a cache entry in the query cache under the hood (since count is supposed to run in constant-time and creating a cache entry cannot possibly run in constant-time). mentioning it in case this shardcorekeymap#add call is responsible for the slowdown.
@Test public void testDfs() throws ElasticsearchException, ExecutionException, InterruptedException, IOException { logger.info("Setting up the index ..."); ImmutableSettings.Builder settings = settingsBuilder() .put(indexSettings()) .put("index.analysis.analyzer", "standard"); assertAcked(prepareCreate("test") .setSettings(settings) .addMapping("type1", "text", "type=string")); ensureGreen(); // get the number of shards assigned GetSettingsResponse getSettingsResponse = client().admin().indices().prepareGetSettings("test").get(); int numberShards = Integer.parseInt(getSettingsResponse.getSetting("test", "index.number_of_shards")); logger.info("Index 'cat' in each shard out of {} shards ...", numberShards); List<IndexRequestBuilder> builders = new ArrayList<>(); for (int i = 0; i < numberShards; i++) { builders.add(client().prepareIndex("test", "type1", i + "") .setRouting(i + "") .setSource("text", "cat")); } indexRandom(true, builders); logger.info("Without dfs 'cat' should appear only once."); TermVectorResponse response = client().prepareTermVector("test", "type1", randomIntBetween(0, numberShards - 1) + "") .setSelectedFields("text") .setFieldStatistics(true) .setTermStatistics(true) .get(); checkStats(response.getFields(), jsonBuilder() .startObject() .startObject("text") .startObject("field_statistics") .field("sum_doc_freq", 1) .field("doc_count", 1) .field("sum_ttf", 1) .endObject() .startObject("terms") .startObject("cat") .field("doc_freq", 1) .field("ttf", 1) .endObject() .endObject() .endObject() .endObject()); logger.info("With dfs 'cat' should appear number {} times.", numberShards); response = client().prepareTermVector("test", "type1", randomIntBetween(0, numberShards - 1) + "") .setSelectedFields("text") .setFieldStatistics(true) .setTermStatistics(true) .setDfs(true) .get(); checkStats(response.getFields(), jsonBuilder() .startObject() .startObject("text") .startObject("field_statistics") .field("sum_doc_freq", numberShards) .field("doc_count", numberShards) .field("sum_ttf", numberShards) .endObject() .startObject("terms") .startObject("cat") .field("doc_freq", numberShards) .field("ttf", numberShards) .endObject() .endObject() .endObject() .endObject()); }	fyi there is a helper method called getnumshards(string index) that can do that too
@Test public void testDfs() throws ElasticsearchException, ExecutionException, InterruptedException, IOException { logger.info("Setting up the index ..."); ImmutableSettings.Builder settings = settingsBuilder() .put(indexSettings()) .put("index.analysis.analyzer", "standard"); assertAcked(prepareCreate("test") .setSettings(settings) .addMapping("type1", "text", "type=string")); ensureGreen(); // get the number of shards assigned GetSettingsResponse getSettingsResponse = client().admin().indices().prepareGetSettings("test").get(); int numberShards = Integer.parseInt(getSettingsResponse.getSetting("test", "index.number_of_shards")); logger.info("Index 'cat' in each shard out of {} shards ...", numberShards); List<IndexRequestBuilder> builders = new ArrayList<>(); for (int i = 0; i < numberShards; i++) { builders.add(client().prepareIndex("test", "type1", i + "") .setRouting(i + "") .setSource("text", "cat")); } indexRandom(true, builders); logger.info("Without dfs 'cat' should appear only once."); TermVectorResponse response = client().prepareTermVector("test", "type1", randomIntBetween(0, numberShards - 1) + "") .setSelectedFields("text") .setFieldStatistics(true) .setTermStatistics(true) .get(); checkStats(response.getFields(), jsonBuilder() .startObject() .startObject("text") .startObject("field_statistics") .field("sum_doc_freq", 1) .field("doc_count", 1) .field("sum_ttf", 1) .endObject() .startObject("terms") .startObject("cat") .field("doc_freq", 1) .field("ttf", 1) .endObject() .endObject() .endObject() .endObject()); logger.info("With dfs 'cat' should appear number {} times.", numberShards); response = client().prepareTermVector("test", "type1", randomIntBetween(0, numberShards - 1) + "") .setSelectedFields("text") .setFieldStatistics(true) .setTermStatistics(true) .setDfs(true) .get(); checkStats(response.getFields(), jsonBuilder() .startObject() .startObject("text") .startObject("field_statistics") .field("sum_doc_freq", numberShards) .field("doc_count", numberShards) .field("sum_ttf", numberShards) .endObject() .startObject("terms") .startObject("cat") .field("doc_freq", numberShards) .field("ttf", numberShards) .endObject() .endObject() .endObject() .endObject()); }	why do you set a routing key that is the same as the id?
public String getXOpaqueId(Set<ThreadContext> threadContexts) { for (ThreadContext threadContext : threadContexts) { try { if (threadContext.isClosed() == false) { String header = threadContext.getHeader(Task.X_OPAQUE_ID); if (header != null) { return header; } } } catch (IllegalStateException e) { // ignore exception as this is due to a race condition between isClosed and getHeader. Only in test env. } } return ""; }	can we rethrow this if we aren't in a test environment? could we fix the race condition? blanket hiding of exceptions like this is dangerous because other exception sources can sneak in over time and we would be masking them unknowingly.
public void setChildInnerHits(Map<String, InnerHitSubContext> childInnerHits) { this.childInnerHits = new InnerHitsContext(childInnerHits); }	this felt a bit hacky. we take a similar strategy in fetchsourcecontext#getfilter.
protected <A extends InternalAggregation, C extends Aggregator> A searchAndReduce(IndexSearcher searcher, Query query, AggregationBuilder builder, MappedFieldType... fieldTypes) throws IOException { final IndexReaderContext ctx = searcher.getTopReaderContext(); final ShardSearcher[] subSearchers; if (ctx instanceof LeafReaderContext) { subSearchers = new ShardSearcher[1]; subSearchers[0] = new ShardSearcher((LeafReaderContext) ctx, ctx); } else { final CompositeReaderContext compCTX = (CompositeReaderContext) ctx; final int size = compCTX.leaves().size(); subSearchers = new ShardSearcher[size]; for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) { final LeafReaderContext leave = compCTX.leaves().get(searcherIDX); subSearchers[searcherIDX] = new ShardSearcher(leave, compCTX); } } List<InternalAggregation> aggs = new ArrayList<> (); Query rewritten = searcher.rewrite(query); Weight weight = searcher.createWeight(rewritten, true); C root = createAggregator(builder, searcher, fieldTypes); try { for (ShardSearcher subSearcher : subSearchers) { C a = createAggregator(builder, subSearcher, fieldTypes); a.preCollection(); subSearcher.search(weight, a); a.postCollection(); aggs.add(a.buildAggregation(0L)); } if (aggs.isEmpty()) { return null; } else { if (randomBoolean() && aggs.size() > 1) { // sometimes do an incremental reduce int toReduceSize = aggs.size(); Collections.shuffle(aggs, random()); int r = randomIntBetween(1, toReduceSize); List<InternalAggregation> toReduce = aggs.subList(0, r); A reduced = (A) aggs.get(0).doReduce(toReduce, new InternalAggregation.ReduceContext(root.context().bigArrays(), null, false)); aggs = aggs.subList(r, toReduceSize); aggs.add(reduced); } // now do the final reduce @SuppressWarnings("unchecked") A internalAgg = (A) aggs.get(0).doReduce(aggs, new InternalAggregation.ReduceContext(root.context().bigArrays(), null, true)); return internalAgg; } } finally { Releasables.close(releasables); releasables.clear(); } }	maybe do new arraylist<>(aggs.sublist(r, toreducesize)); since you call add again below?
@Override public List<Route> routes() { return List.of( new Route(POST, "/{index}/_pit"), new Route(POST, "/_pit")); }	maybe we should disable creating a _pit for all indices ?
@Override public DeleteStoredScriptRequestBuilder prepareDeleteStoredScript(String id){ return prepareDeleteStoredScript().setId(id); }	same story as for the action names. we should consider whether to include this under indices or under a new "client" interface? will add to our weekly sync, this should not block merging this.
private void moveToStep(Index index, String policy, StepKey currentStepKey, StepKey nextStepKey) { logger.debug("moveToStep[" + policy + "] [" + index.getName() + "]" + currentStepKey + " -> " + nextStepKey); clusterService.submitStateUpdateTask("ILM-move-to-step", new MoveToNextStepUpdateTask(index, policy, currentStepKey, nextStepKey, nowSupplier)); }	while you're here, would you make these lowercase please: ilm -> ilm
public static void validateTimestampFieldMapping(String timestampFieldName, MapperService mapperService) { MappedFieldType timestampFieldMapper = mapperService.fieldType(timestampFieldName); if (timestampFieldMapper == null) { throw new IllegalArgumentException("expected timestamp field [" + timestampFieldName + "], but found no timestamp field"); } String type = timestampFieldMapper.typeName(); if (ALLOWED_TIMESTAMPFIELD_TYPES.contains(type) == false) { throw new IllegalArgumentException("expected timestamp field [" + timestampFieldName + "] to be of types [" + ALLOWED_TIMESTAMPFIELD_TYPES + "], but instead found type [" + type + "]"); } }	super minor: suggestion throw new illegalargumentexception("expected timestamp field [" + timestampfieldname + "] to be of types " + allowed_timestampfield_types + ", but instead found type [" + type + "]"); to avoid the [[]] double brackets
static Request putStoredScript(PutStoredScriptRequest putStoredScriptRequest) throws IOException { String endpoint = new EndpointBuilder().addPathPartAsIs("_scripts").addPathPart(putStoredScriptRequest.id()).build(); Request request = new Request(HttpPost.METHOD_NAME, endpoint); Params params = new Params(request); params.withTimeout(putStoredScriptRequest.timeout()); params.withMasterTimeout(putStoredScriptRequest.masterNodeTimeout()); request.setEntity(createEntity(putStoredScriptRequest, REQUEST_BODY_CONTENT_TYPE)); return request; }	don't we also support a context parameter?
public void testPutStoredScript() throws Exception { PutStoredScriptRequest putStoredScriptRequest = new PutStoredScriptRequest(); String id = randomAlphaOfLengthBetween(5, 10); putStoredScriptRequest.id(id); XContentType xContentType = randomFrom(XContentType.values()); try (XContentBuilder builder = XContentBuilder.builder(xContentType.xContent())) { builder.startObject(); builder.startObject("script") .field("lang", "painless") .field("source", "Math.log(_score * 2) + params.multiplier") .endObject(); builder.endObject(); putStoredScriptRequest.content(BytesReference.bytes(builder), xContentType); } Map<String, String> expectedParams = new HashMap<>(); setRandomMasterTimeout(putStoredScriptRequest, expectedParams); setRandomTimeout(putStoredScriptRequest::timeout, AcknowledgedRequest.DEFAULT_ACK_TIMEOUT, expectedParams); Request request = RequestConverters.putStoredScript(putStoredScriptRequest); assertThat(request.getEndpoint(), equalTo("/_scripts/" + id)); assertThat(request.getParameters(), equalTo(expectedParams)); assertThat(request.getEntity(), notNullValue()); }	here we should test the context param too?
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { if (source == null) { source = StoredScriptSource.parse(content, xContentType); } builder.field("script"); source.toXContent(builder, params); return builder; }	when is this if exercised? i think that you can make the blank constructor package protected and assume that once toxcontent is called the source has already a value. that has the nice effect that we don't modify the state of the request when toxcontent is called which is not ideal.
static List<Index> getLeaderIndicesToFollow(String remoteCluster, AutoFollowPattern autoFollowPattern, ClusterState leaderClusterState, ClusterState followerClusterState, List<String> followedIndexUUIDs) { List<Index> leaderIndicesToFollow = new ArrayList<>(); for (IndexMetaData leaderIndexMetaData : leaderClusterState.getMetaData()) { if (autoFollowPattern.match(leaderIndexMetaData.getIndex().getName())) { IndexRoutingTable indexRoutingTable = leaderClusterState.routingTable().index(leaderIndexMetaData.getIndex()); if (indexRoutingTable != null && indexRoutingTable.allPrimaryShardsActive() && followedIndexUUIDs.contains(leaderIndexMetaData.getIndex().getUUID()) == false) { // TODO: iterate over the indices in the followerClusterState and check whether a IndexMetaData // has a leader index uuid custom metadata entry that matches with uuid of leaderIndexMetaData variable // If so then handle it differently: not follow it, but just add an entry to // AutoFollowMetadata#followedLeaderIndexUUIDs if (leaderIndexMetaData.getSettings().getAsBoolean(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), false)) { leaderIndicesToFollow.add(leaderIndexMetaData.getIndex()); } } } } return leaderIndicesToFollow; }	looks good. can you leave a comment explaining this though?
private static boolean shouldSortShards(MinAndMax<?>[] minAndMaxes) { Class<?> clazz = null; for (MinAndMax<?> minAndMax : minAndMaxes) { if (clazz == null) { clazz = minAndMax == null ? null : minAndMax.getMin().getClass(); } else if (minAndMax != null && clazz != minAndMax.getMax().getClass()) { // we don't support sort values that mix different types (e.g.: long/double, numeric/keyword). // TODO: we could fail the request because there is a high probability // that the merging of topdocs will fail later for the same reason ? return false; } } return clazz != null; }	it's slightly confusing that we use getmin for the initial getclass and then getmax. use getmin all the time maybe?
@Override public void afterRefresh(boolean didRefresh) throws IOException { // We can now drop old because these operations are now visible via the newly opened searcher. Even if didRefresh is false, which // means Lucene did not actually open a new reader because it detected no changes, it's possible old has some entries in it, which // is fine: it means they were actually already included in the previously opened reader, so we can still safely drop them in that // case. This is because we assign new maps (in beforeRefresh) slightly before Lucene actually flushes any segments for the // reopen, and so any concurrent indexing requests can still sneak in a few additions to that current map that are in fact reflected // in the previous reader. We don't touch tombstones here: they expire on their own index.gc_deletes timeframe: maps = new Maps(maps.current, Collections.emptyMap()); }	i'm probably lacking the larger context here but is it ok to change the implementation class from concurrenthashmap to hashmap here?
public void setUp() throws Exception { super.setUp(); format = randomNumericDocValueFormat(); AutoDateHistogramAggregationBuilder aggregationBuilder = new AutoDateHistogramAggregationBuilder("_name"); // TODO[PCS]: timezone set automagically here? roundingInfos = aggregationBuilder.buildRoundings(); }	i'm actually not sure how the time zone would be picked up here which makes me think that its not being used currently and we are testing with only utc at the moment so we probably need to fix that but we could do that in a follow up pr and just pass in utc to create the roundings for this pr. i also think we need to create the roundings in createtestinstance() since because that method should be randomising the time zone the roundings may be different for each instance.
public void testRolloverOnEmptyIndex() throws Exception { Alias testAlias = new Alias("test_alias"); boolean explicitWriteIndex = randomBoolean(); if (explicitWriteIndex) { testAlias.writeIndex(true); } assertAcked(prepareCreate("test_index-1").addAlias(testAlias).get()); final RolloverResponse response = client().admin().indices().prepareRolloverIndex("test_alias").get(); assertThat(response.getOldIndex(), equalTo("test_index-1")); assertThat(response.getNewIndex(), equalTo("test_index-000002")); assertThat(response.isDryRun(), equalTo(false)); assertThat(response.isRolledOver(), equalTo(true)); assertThat(response.getConditionStatus().size(), equalTo(0)); final ClusterState state = client().admin().cluster().prepareState().get().getState(); final IndexMetaData oldIndex = state.metaData().index("test_index-1"); if (explicitWriteIndex) { assertTrue(oldIndex.getAliases().containsKey("test_alias")); assertFalse(oldIndex.getAliases().get("test_alias").writeIndex()); } else { assertFalse(oldIndex.getAliases().containsKey("test_alias")); } final IndexMetaData newIndex = state.metaData().index("test_index-000002"); assertTrue(newIndex.getAliases().containsKey("test_alias")); }	can you change these to be randomboolean() rather than always being set to true (if set at all) so we make sure that explicit false settings have the correct behavior?
UnassignedBytesUnassignedShards storagePreventsAllocation0() { RoutingAllocation allocation = new RoutingAllocation(allocationDeciders, state, info, shardSizeInfo, System.nanoTime()); List<ShardRouting> unassignedShards = StreamSupport.stream(state.getRoutingNodes().unassigned().spliterator(), false) .filter(shard -> canAllocate(shard, allocation) == false) .filter(shard -> cannotAllocateDueToStorage(shard, allocation)) .toList(); return new UnassignedBytesUnassignedShards(unassignedShards.stream().mapToLong(this::sizeOf).sum(), unassignedShards); }	i think we should sort the list of shardid to ensure they come out in order and that differences are easier to spot for humans.
UnassignedBytesUnassignedShards storagePreventsAllocation0() { RoutingAllocation allocation = new RoutingAllocation(allocationDeciders, state, info, shardSizeInfo, System.nanoTime()); List<ShardRouting> unassignedShards = StreamSupport.stream(state.getRoutingNodes().unassigned().spliterator(), false) .filter(shard -> canAllocate(shard, allocation) == false) .filter(shard -> cannotAllocateDueToStorage(shard, allocation)) .toList(); return new UnassignedBytesUnassignedShards(unassignedShards.stream().mapToLong(this::sizeOf).sum(), unassignedShards); }	same comment as for storagepreventsallocation0.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { throw new UnsupportedOperationException(); } } } public static class ReactiveReason implements AutoscalingDeciderResult.Reason { private final String reason; private final long unassigned; private final long assigned; private final Set<ShardId> unassignedShardIds; private final Set<ShardId> assignedShardIds; public ReactiveReason(String reason, long unassigned, long assigned) { this(reason, unassigned, assigned, Set.<ShardId>of(), Set.<ShardId>of()); } public ReactiveReason( String reason, long unassigned, long assigned, Collection<ShardRouting> unassignedShardIds, Collection<ShardRouting> assignedShardIds ) { this( reason, unassigned, assigned, unassignedShardIds.stream().map(ShardRouting::shardId).collect(Collectors.toSet()), assignedShardIds.stream().map(ShardRouting::shardId).collect(Collectors.toSet()) ); } ReactiveReason(String reason, long unassigned, long assigned, Set<ShardId> unassignedShardIds, Set<ShardId> assignedShardIds) { this.reason = reason; this.unassigned = unassigned; this.assigned = assigned; this.unassignedShardIds = unassignedShardIds; this.assignedShardIds = assignedShardIds; } public ReactiveReason(StreamInput in) throws IOException { this.reason = in.readString(); this.unassigned = in.readLong(); this.assigned = in.readLong(); if (in.getVersion().onOrAfter(Version.V_8_3_0)) { unassignedShardIds = in.readSet(ShardId::new); assignedShardIds = in.readSet(ShardId::new); } else { unassignedShardIds = Set.of(); assignedShardIds = Set.of(); } } @Override public String summary() { return reason; } public long unassigned() { return unassigned; } public long assigned() { return assigned; } public Set<ShardId> unassignedShardIds() { return unassignedShardIds; } public Set<ShardId> assignedShardIds() { return assignedShardIds; } @Override public String getWriteableName() { return ReactiveStorageDeciderService.NAME; } @Override public void writeTo(StreamOutput out) throws IOException { out.writeString(reason); out.writeLong(unassigned); out.writeLong(assigned); if (out.getVersion().onOrAfter(Version.V_8_3_0)) { out.writeCollection(unassignedShardIds); out.writeCollection(assignedShardIds); } } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("reason", reason); builder.field("unassigned", unassigned); builder.field("assigned", assigned); builder.field("unassigned_shard_ids", unassignedShardIds); builder.field("assigned_shard_ids", assignedShardIds); builder.endObject(); return builder; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; ReactiveReason that = (ReactiveReason) o; return unassigned == that.unassigned && assigned == that.assigned && reason.equals(that.reason) && unassignedShardIds.equals(that.unassignedShardIds) && assignedShardIds.equals(that.assignedShardIds); }	i'd like to remove the original constructor instead and fix usages. the usage in proactivestoragedeciderservice looks like a copy-paste bug that we can fix.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field("reason", reason); builder.field("unassigned", unassigned); builder.field("assigned", assigned); builder.field("unassigned_shard_ids", unassignedShardIds); builder.field("assigned_shard_ids", assignedShardIds); builder.endObject(); return builder; }	can we use collectiontodelimitedstringwithlimit to avoid too heavy responses here?
public void connectToNode(DiscoveryNode node, ConnectionProfile connectionProfile, ConnectionValidator connectionValidator, ActionListener<Void> listener) throws ConnectTransportException { ConnectionProfile resolvedProfile = ConnectionProfile.resolveConnectionProfile(connectionProfile, defaultProfile); if (node == null) { listener.onFailure(new ConnectTransportException(null, "can't connect to a null node")); return; } if (connectingRefCounter.tryIncRef() == false) { listener.onFailure(new IllegalStateException("connection manager is closed")); return; } if (pendingConnections.containsKey(node)) { connectingRefCounter.decRef(); listener.onResponse(null); return; } final ListenableFuture<Void> currentListener = new ListenableFuture<>(); final ListenableFuture<Void> existingListener = pendingConnections.putIfAbsent(node, currentListener); if (existingListener != null) { try { // wait on previous entry to complete connection attempt existingListener.addListener(listener, EsExecutors.newDirectExecutorService()); } finally { connectingRefCounter.decRef(); } return; } currentListener.addListener(listener, EsExecutors.newDirectExecutorService()); final RunOnce releaseOnce = new RunOnce(connectingRefCounter::decRef); internalOpenConnection(node, resolvedProfile, ActionListener.wrap(conn -> { connectionValidator.validate(conn, resolvedProfile, ActionListener.wrap( ignored -> { assert Transports.assertNotTransportThread("connection validator success"); try { if (connectedNodes.putIfAbsent(node, conn) != null) { logger.debug("existing connection to node [{}], closing new redundant connection", node); IOUtils.closeWhileHandlingException(conn); } else { logger.debug("connected to node [{}]", node); try { connectionListener.onNodeConnected(node); } finally { final Transport.Connection finalConnection = conn; conn.addCloseListener(ActionListener.wrap(() -> { logger.trace("unregistering {} after connection close and marking as disconnected", node); connectedNodes.remove(node, finalConnection); connectionListener.onNodeDisconnected(node); })); } } } finally { ListenableFuture<Void> future = pendingConnections.remove(node); assert future == currentListener : "Listener in pending map is different than the expected listener"; releaseOnce.run(); future.onResponse(null); } }, e -> { assert Transports.assertNotTransportThread("connection validator failure"); IOUtils.closeWhileHandlingException(conn); failConnectionListeners(node, releaseOnce, e, currentListener); })); }, e -> { assert Transports.assertNotTransportThread("internalOpenConnection failure"); failConnectionListeners(node, releaseOnce, e, currentListener); })); }	i'm confused. if we have a pending connection, a second concurrent attempt now returns "ok", making it look like a connection has already been established?
private void performOnReplica(final ShardRouting shard, final ReplicaRequest replicaRequest, final long globalCheckpoint) { if (logger.isTraceEnabled()) { logger.trace("[{}] sending op [{}] to replica {} for request [{}]", shard.shardId(), opType, shard, replicaRequest); } totalShards.incrementAndGet(); pendingActions.incrementAndGet(); replicasProxy.performOn(shard, replicaRequest, globalCheckpoint, new ActionListener<ReplicaResponse>() { @Override public void onResponse(ReplicaResponse response) { successfulShards.incrementAndGet(); try { primary.updateLocalCheckpointForShard(response.allocationId(), response.localCheckpoint()); } catch (final Exception e) { final String message = String.format(Locale.ROOT, "primary failed updating local checkpoint for replica %s", shard); primary.failShard(message, e); finishAsFailed(new RetryOnPrimaryException(primary.routingEntry().shardId(), message, e)); } decPendingAndFinishIfNeeded(); } @Override public void onFailure(Exception replicaException) { logger.trace( (org.apache.logging.log4j.util.Supplier<?>) () -> new ParameterizedMessage( "[{}] failure while performing [{}] on replica {}, request [{}]", shard.shardId(), opType, shard, replicaRequest), replicaException); if (TransportActions.isShardNotAvailableException(replicaException)) { decPendingAndFinishIfNeeded(); } else { RestStatus restStatus = ExceptionsHelper.status(replicaException); shardReplicaFailures.add(new ReplicationResponse.ShardInfo.Failure( shard.shardId(), shard.currentNodeId(), replicaException, restStatus, false)); String message = String.format(Locale.ROOT, "failed to perform %s on replica %s", opType, shard); replicasProxy.failShardIfNeeded(shard, replicaRequest.primaryTerm(), message, replicaException, ReplicationOperation.this::decPendingAndFinishIfNeeded, ReplicationOperation.this::onPrimaryDemoted, throwable -> decPendingAndFinishIfNeeded()); } } }); }	i think we need to be careful hare. it's ok to have closed the shard. for example if the index was deleted (or more complicated, if this a target primary and the source node left the cluster before the master activated this primary. in these cases, we shouldn't fail the operation (nor the shard, for that matter) but rather let things finish in their natural flow. if the op was properly replicated to all active shards, we're all good. if not, we're in trouble but that would be picked up by the logic handling failures from replicas.
public void onResponse(ReplicaResponse response) { successfulShards.incrementAndGet(); try { primary.updateLocalCheckpointForShard(response.allocationId(), response.localCheckpoint()); } catch (final Exception e) { final String message = String.format(Locale.ROOT, "primary failed updating local checkpoint for replica %s", shard); primary.failShard(message, e); finishAsFailed(new RetryOnPrimaryException(primary.routingEntry().shardId(), message, e)); } decPendingAndFinishIfNeeded(); }	we need to fix the java docs on failshard.
static Tuple<List<Version>, List<Version>> resolveReleasedVersions(Version current, Class<?> versionClass) { Field[] fields = versionClass.getFields(); List<Version> versions = new ArrayList<>(fields.length); for (final Field field : fields) { final int mod = field.getModifiers(); if (false == Modifier.isStatic(mod) && Modifier.isFinal(mod) && Modifier.isPublic(mod)) { continue; } if (field.getType() != Version.class) { continue; } assert field.getName().matches("(V(_\\\\\\\\d+)+(_(alpha|beta|rc)\\\\\\\\d+)?|CURRENT)") : field.getName(); if (false == VERSION_NAME.matcher(field.getName()).matches()) { continue; } try { versions.add(((Version) field.get(null))); } catch (final IllegalAccessException e) { throw new RuntimeException(e); } } Collections.sort(versions); assert versions.get(versions.size() - 1).equals(current) : "The highest version must be the current one " + "but was [" + versions.get(versions.size() - 1) + "] and current was [" + current + "]"; if (current.revision != 0) { /* If we are in a patch release there should be no unreleased version constants. * If there are, gradle will yell about it. */ return new Tuple<>(unmodifiableList(versions), emptyList()); } /* If we are on a patch release then we know that at least the version before the * current one is unreleased. If it is released then gradle would be complaining. */ /* Technically we don't support backwards compatiblity for alphas, betas, * and rcs. But the testing infrastructure requires that we act as though we * do. This is a difference between the gradle and Java logic but should be * fairly safe as it is errs on us being more compatible rather than less.... * Anyway, the upshot is that we never declare alphas as unreleased, no * matter where they are in the list */ int unreleasedIndex = versions.size() - 2; while (true) { if (versions.get(unreleasedIndex).isRelease()) { break; } unreleasedIndex--; if (unreleasedIndex < 0) { throw new IllegalArgumentException("Couldn't find first non-alpha release"); } } Version unreleased = versions.remove(unreleasedIndex); if (unreleased.revision == 0) { /* If the last unreleased version is itself a patch release then gradle enforces * that there is yet another unreleased version before that. */ unreleasedIndex--; Version earlierUnreleased = versions.remove(unreleasedIndex); return new Tuple<>(unmodifiableList(versions), unmodifiableList(Arrays.asList(earlierUnreleased, unreleased))); } return new Tuple<>(unmodifiableList(versions), singletonList(unreleased)); }	patch release -> stable branch?
static Tuple<List<Version>, List<Version>> resolveReleasedVersions(Version current, Class<?> versionClass) { Field[] fields = versionClass.getFields(); List<Version> versions = new ArrayList<>(fields.length); for (final Field field : fields) { final int mod = field.getModifiers(); if (false == Modifier.isStatic(mod) && Modifier.isFinal(mod) && Modifier.isPublic(mod)) { continue; } if (field.getType() != Version.class) { continue; } assert field.getName().matches("(V(_\\\\\\\\d+)+(_(alpha|beta|rc)\\\\\\\\d+)?|CURRENT)") : field.getName(); if (false == VERSION_NAME.matcher(field.getName()).matches()) { continue; } try { versions.add(((Version) field.get(null))); } catch (final IllegalAccessException e) { throw new RuntimeException(e); } } Collections.sort(versions); assert versions.get(versions.size() - 1).equals(current) : "The highest version must be the current one " + "but was [" + versions.get(versions.size() - 1) + "] and current was [" + current + "]"; if (current.revision != 0) { /* If we are in a patch release there should be no unreleased version constants. * If there are, gradle will yell about it. */ return new Tuple<>(unmodifiableList(versions), emptyList()); } /* If we are on a patch release then we know that at least the version before the * current one is unreleased. If it is released then gradle would be complaining. */ /* Technically we don't support backwards compatiblity for alphas, betas, * and rcs. But the testing infrastructure requires that we act as though we * do. This is a difference between the gradle and Java logic but should be * fairly safe as it is errs on us being more compatible rather than less.... * Anyway, the upshot is that we never declare alphas as unreleased, no * matter where they are in the list */ int unreleasedIndex = versions.size() - 2; while (true) { if (versions.get(unreleasedIndex).isRelease()) { break; } unreleasedIndex--; if (unreleasedIndex < 0) { throw new IllegalArgumentException("Couldn't find first non-alpha release"); } } Version unreleased = versions.remove(unreleasedIndex); if (unreleased.revision == 0) { /* If the last unreleased version is itself a patch release then gradle enforces * that there is yet another unreleased version before that. */ unreleasedIndex--; Version earlierUnreleased = versions.remove(unreleasedIndex); return new Tuple<>(unmodifiableList(versions), unmodifiableList(Arrays.asList(earlierUnreleased, unreleased))); } return new Tuple<>(unmodifiableList(versions), singletonList(unreleased)); }	except for current on master...
static void verifyQuery(QueryBuilder queryBuilder) { if (queryBuilder.getName().equals("has_child")) { throw new IllegalArgumentException("the [has_child] query is unsupported inside a percolator query"); } else if (queryBuilder.getName().equals("has_parent")) { throw new IllegalArgumentException("the [has_parent] query is unsupported inside a percolator query"); } else if (queryBuilder instanceof BoolQueryBuilder) { BoolQueryBuilder boolQueryBuilder = (BoolQueryBuilder) queryBuilder; List<QueryBuilder> clauses = new ArrayList<>(); clauses.addAll(boolQueryBuilder.filter()); clauses.addAll(boolQueryBuilder.must()); clauses.addAll(boolQueryBuilder.mustNot()); clauses.addAll(boolQueryBuilder.should()); for (QueryBuilder clause : clauses) { verifyQuery(clause); } } else if (queryBuilder instanceof ConstantScoreQueryBuilder) { verifyQuery(((ConstantScoreQueryBuilder) queryBuilder).innerQuery()); } else if (queryBuilder instanceof FunctionScoreQueryBuilder) { verifyQuery(((FunctionScoreQueryBuilder) queryBuilder).query()); } else if (queryBuilder instanceof BoostingQueryBuilder) { verifyQuery(((BoostingQueryBuilder) queryBuilder).negativeQuery()); verifyQuery(((BoostingQueryBuilder) queryBuilder).positiveQuery()); } else if (queryBuilder instanceof DisMaxQueryBuilder) { DisMaxQueryBuilder disMaxQueryBuilder = (DisMaxQueryBuilder) queryBuilder; for (QueryBuilder innerQueryBuilder : disMaxQueryBuilder.innerQueries()) { verifyQuery(innerQueryBuilder); } } }	i don't like this very much to be honest, i do wonder if we can introduce a special rewrite context that we can check for in the rangequery and then rewrite to matchalldocs this would make this change more contained?
public void testRangeQueriesWithNow() throws Exception { assertAcked(client().admin().indices().prepareCreate("test") .addMapping("_doc", "field1", "type=keyword", "field2", "type=date", "query", "type=percolator") ); client().prepareIndex("test", "_doc", "1") .setSource(jsonBuilder().startObject().field("query", rangeQuery("field2").from("now-3s").to("now+3s")).endObject()) .get(); client().prepareIndex("test", "_doc", "2") .setSource(jsonBuilder().startObject().field("query", boolQuery() .filter(termQuery("field1", "value")) .filter(rangeQuery("field2").from("now-3s").to("now+3s")) ).endObject()) .get(); Script script = new Script(ScriptType.INLINE, MockScriptPlugin.NAME, "1==1", Collections.emptyMap()); client().prepareIndex("test", "_doc", "3") .setSource(jsonBuilder().startObject().field("query", boolQuery() .filter(scriptQuery(script)) .filter(rangeQuery("field2").from("now-3s").to("now+3s")) ).endObject()) .get(); client().admin().indices().prepareRefresh().get(); BytesReference source = BytesReference.bytes(jsonBuilder().startObject() .field("field1", "value") .field("field2", System.currentTimeMillis()) .endObject()); SearchResponse response = client().prepareSearch() .setQuery(new PercolateQueryBuilder("query", source, XContentType.JSON)) .get(); assertHitCount(response, 3); Thread.sleep(5000); source = BytesReference.bytes(jsonBuilder().startObject() .field("field1", "value") .field("field2", System.currentTimeMillis()) .endObject()); response = client().prepareSearch() .setQuery(new PercolateQueryBuilder("query", source, XContentType.JSON)) .get(); assertHitCount(response, 3); }	is there another way to do this instead of using a 5 second sleep? maybe you can play with system.currenttimemillis()?
public void testRoundingAroundDST() { Rounding.DateTimeUnit unit = Rounding.DateTimeUnit.DAY_OF_MONTH; ZoneId tz = ZoneId.of("Canada/Newfoundland"); long minLookup = 688618001000L; // 1991-10-28T02:46:41.527Z long maxLookup = 688618001001L; // +1sec // there is a Transition[Overlap at 1991-10-27T00:01-02:30 to -03:30]  long[] bounds = new long[]{minLookup, maxLookup}; assertUnitRoundingSameAsJavaUtilTimeImplementation(unit, tz, bounds[0], bounds[1]); }	how this work: 1991-10-28t02:46:41.527z -> 1991-10-17 23:11:48 -03:30 canada/newfoundland then we round to day_of_month 1991-10-27 00:00:00 -**02**:30 canada/newfoundland previously it would not work because we only included transition for timestamps 1unit before the minutcmillis // long minlookup = minutcmillis - 2*unit.extralocaloffsetlookup(); so for 1991-10-28t02:46:41.527z it would be 1991-10-**27t02**:46:41.527z which means we would miss the relevant transition 1991-10-**27t00**:01-02:30 to -03:30
public void setMaxModelMemory(long numBytes) { if (numBytes <= 0) { throw new IllegalArgumentException("[" + MAX_MODEL_MEMORY.getPreferredName() + "] must be positive."); } this.maxModelMemory = numBytes; }	should we have minimum? a limit of 1 byte doesn't make sense.
static Long getAdjustedMemoryLimit(Job job, Long requestedLimit, AbstractAuditor<? extends AbstractAuditMessage> auditor) { if (requestedLimit == null || requestedLimit == DEFAULT_MAX_MODEL_MEMORY.getBytes()) { return null; } long jobLimitMegaBytes = job.getAnalysisLimits() == null || job.getAnalysisLimits().getModelMemoryLimit() == null ? AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB : job.getAnalysisLimits().getModelMemoryLimit(); long allowedMax = (long)(new ByteSizeValue(jobLimitMegaBytes, ByteSizeUnit.MB).getBytes() * 0.40); long adjustedMax = Math.min(requestedLimit, allowedMax - 1); if (adjustedMax != requestedLimit) { String msg = "requested forecast limit [" + requestedLimit + "] exceeded [" + allowedMax + "] (40% of the anomaly job memory limit). Reducing to allowed maximum before running forecast."; logger.warn("[{}] {}", job.getId(), msg); auditor.warning(job.getId(), msg); } return adjustedMax; }	i think we should just make the default null. doing it the way it's currently done means there's a weird situation where an explicitly configured forecast memory of 20mb works without capping for a job with a 10mb model_memory_limit, but an explicitly configured forecast memory of 19mb is capped and generates an audit message.
static Long getAdjustedMemoryLimit(Job job, Long requestedLimit, AbstractAuditor<? extends AbstractAuditMessage> auditor) { if (requestedLimit == null || requestedLimit == DEFAULT_MAX_MODEL_MEMORY.getBytes()) { return null; } long jobLimitMegaBytes = job.getAnalysisLimits() == null || job.getAnalysisLimits().getModelMemoryLimit() == null ? AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB : job.getAnalysisLimits().getModelMemoryLimit(); long allowedMax = (long)(new ByteSizeValue(jobLimitMegaBytes, ByteSizeUnit.MB).getBytes() * 0.40); long adjustedMax = Math.min(requestedLimit, allowedMax - 1); if (adjustedMax != requestedLimit) { String msg = "requested forecast limit [" + requestedLimit + "] exceeded [" + allowedMax + "] (40% of the anomaly job memory limit). Reducing to allowed maximum before running forecast."; logger.warn("[{}] {}", job.getId(), msg); auditor.warning(job.getId(), msg); } return adjustedMax; }	this should be pre_6_1_default_model_memory_limit_mb. after 6.1 we don't let the model memory limit be null, so the post-6.1 limit is always explicitly set. (we had to do this to achieve bwc when we changed the default in 6.1.)
static Long getAdjustedMemoryLimit(Job job, Long requestedLimit, AbstractAuditor<? extends AbstractAuditMessage> auditor) { if (requestedLimit == null || requestedLimit == DEFAULT_MAX_MODEL_MEMORY.getBytes()) { return null; } long jobLimitMegaBytes = job.getAnalysisLimits() == null || job.getAnalysisLimits().getModelMemoryLimit() == null ? AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB : job.getAnalysisLimits().getModelMemoryLimit(); long allowedMax = (long)(new ByteSizeValue(jobLimitMegaBytes, ByteSizeUnit.MB).getBytes() * 0.40); long adjustedMax = Math.min(requestedLimit, allowedMax - 1); if (adjustedMax != requestedLimit) { String msg = "requested forecast limit [" + requestedLimit + "] exceeded [" + allowedMax + "] (40% of the anomaly job memory limit). Reducing to allowed maximum before running forecast."; logger.warn("[{}] {}", job.getId(), msg); auditor.warning(job.getId(), msg); } return adjustedMax; }	why subtract 1 from allowedmax?
static void validate(Job job, ForecastJobAction.Request request) { if (job.getJobVersion() == null || job.getJobVersion().before(Version.fromString("6.1.0"))) { throw ExceptionsHelper.badRequestException( "Cannot run forecast because jobs created prior to version 6.1 are not supported"); } if (request.getDuration() != null) { TimeValue duration = request.getDuration(); TimeValue bucketSpan = job.getAnalysisConfig().getBucketSpan(); if (duration.compareTo(bucketSpan) < 0) { throw ExceptionsHelper.badRequestException( "[" + DURATION.getPreferredName() + "] must be greater or equal to the bucket span: [" + duration.getStringRep() + "/" + bucketSpan.getStringRep() + "]"); } } if (request.getMaxModelMemory() != null) { if (request.getMaxModelMemory() >= FORECAST_LOCAL_STORAGE_LIMIT.getBytes()) { throw ExceptionsHelper.badRequestException( "[{}] must be less than 500MB", ForecastJobAction.Request.MAX_MODEL_MEMORY.getPreferredName()); } } }	nit: you used the constant in the if, so it makes sense to use it here, too
@Override public byte[] encodePoint(Number value, boolean coerce) { float parsedValue = parse(value, coerce); byte[] bytes = new byte[Integer.BYTES]; HalfFloatPoint.encodeDimension(parsedValue, bytes, 0); return bytes; }	i thought we used 2 bytes for halffloat, not 4?
* @param config The config for the values source metric. */ static Function<byte[], Number> getPointReaderOrNull(SearchContext context, Aggregator parent, ValuesSourceConfig<ValuesSource.Numeric> config) { if (context.query() != null && context.query().getClass() != MatchAllDocsQuery.class) { return null; } if (parent != null) { return null; } if (config.fieldContext() != null && config.script() == null && config.missing() == null) { MappedFieldType fieldType = config.fieldContext().fieldType(); if (fieldType == null || fieldType.indexOptions() == IndexOptions.NONE) { return null; } Function<byte[], Number> converter = null; if (fieldType instanceof NumberFieldMapper.NumberFieldType) { converter = ((NumberFieldMapper.NumberFieldType) fieldType)::parsePoint; } else if (fieldType.getClass() == DateFieldMapper.DateFieldType.class) { converter = (in) -> LongPoint.decodeDimension(in, 0); } return converter; } return null; } /** * Returns the minimum value indexed in the <code>fieldName</code> field or <code>null</code> * if the value cannot be inferred from the indexed {@link PointValues}	this change looks unrelated?
@Override protected void masterOperation(GetDatafeedsStatsAction.Request request, ClusterState state, ActionListener<GetDatafeedsStatsAction.Response> listener) throws Exception { logger.debug("Get stats for datafeed '{}'", request.getDatafeedId()); datafeedConfigProvider.expandDatafeedConfigs( request.getDatafeedId(), request.allowNoDatafeeds(), ActionListener.wrap( datafeedBuilders -> { Map<String, String> jobIdByDatafeedId = datafeedBuilders.stream() .map(DatafeedConfig.Builder::build) .collect(Collectors.toUnmodifiableMap(DatafeedConfig::getId, DatafeedConfig::getJobId)); jobResultsProvider.datafeedTimingStats( jobIdByDatafeedId.values(), timingStatsByJobId -> { PersistentTasksCustomMetaData tasksInProgress = state.getMetaData().custom(PersistentTasksCustomMetaData.TYPE); List<GetDatafeedsStatsAction.Response.DatafeedStats> results = datafeedBuilders.stream() .map(datafeedBuilder -> getDatafeedStats( datafeedBuilder.getId(), state, tasksInProgress, jobIdByDatafeedId, timingStatsByJobId)) .collect(Collectors.toList()); QueryPage<GetDatafeedsStatsAction.Response.DatafeedStats> statsPage = new QueryPage<>(results, results.size(), DatafeedConfig.RESULTS_FIELD); listener.onResponse(new GetDatafeedsStatsAction.Response(statsPage)); }, listener::onFailure); }, listener::onFailure) ); }	it seems to me that this whole piece of logic is required because you are not also storing the datafeed_id with the datafeed stats document. it seems much simpler to me to save the datafeed_id so you don't have to make multiple es client calls to read docs.
@Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); builder.field(NAMES.getPreferredName(), indices); builder.field(PRIVILEGES.getPreferredName(), privileges); if (grantedFields != null || deniedFields != null) { builder.startObject(FIELD_PERMISSIONS.getPreferredName()); if (grantedFields != null) { builder.field(GRANT_FIELDS.getPreferredName(), grantedFields); } if (deniedFields != null) { builder.field(EXCEPT_FIELDS.getPreferredName(), deniedFields); } builder.endObject(); } if (isUsingDocumentLevelSecurity()) { builder.field("query", query); } return builder.endObject(); }	although the previous condition was correctly from the server logic stand point, it would hinder testing in indicesprivilegestests.
@Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Role that = (Role) o; return name.equals(that.name) && clusterPrivileges.equals(that.clusterPrivileges) && Objects.equals(globalApplicationPrivileges, that.globalApplicationPrivileges) && indicesPrivileges.equals(that.indicesPrivileges) && applicationResourcePrivileges.equals(that.applicationResourcePrivileges) && runAsPrivilege.equals(that.runAsPrivilege) && metadata.equals(that.metadata); }	transient metadata is a read only field when retrieving the role and cannot be set on a put role. it should not matter in an equals because a user should be able to create a role that is equal to another one, except for the transient part over which it has no control (cannot set it himself)
public Builder metadata(Map<String, Object> metadata) { this.metadata = Objects.requireNonNull(metadata, "Metadata cannot be null. Pass an empty map instead."); return this; }	transient metadata cannot be set in the builder, by the client, it is created only by the server.
public E resolve(Object key, E defaultValue) { E value = defaultValue; E candidate = null; int allowedLookups = 1000; while ((candidate = get(key)) != null || containsKey(key)) { // instead of circling around, return if (candidate == key) { return candidate; } if (--allowedLookups == 0) { throw new QlIllegalArgumentException("Potential cycle detected"); } key = value = candidate; } return value; }	please break the assignment in two pieces.
@Override public Number getKeyAsNumber() { // this method is needed for scripted numeric aggs try { return Long.parseLong(termBytes.utf8ToString()); } catch (NumberFormatException numberFormatException) { } return Double.parseDouble(termBytes.utf8ToString()); }	i think it's best to name the exception as ignored here; it makes it clearer that the swallowing is intentional (and some ides complain about unused exceptions unless they are named ignored).
@Override public Number getKeyAsNumber() { // this method is needed for scripted numeric aggs try { return Long.parseLong(termBytes.utf8ToString()); } catch (NumberFormatException numberFormatException) { } return Double.parseDouble(termBytes.utf8ToString()); }	i think a comment why this is being swallowed is in order here.
@Override protected Scheduler.Cancellable scheduleBackgroundRetentionLeaseRenewal(final LongSupplier followerGlobalCheckpoint) { final String retentionLeaseId = CcrRetentionLeases.retentionLeaseId( clusterService.getClusterName().value(), params.getFollowShardId().getIndex(), params.getRemoteCluster(), params.getLeaderShardId().getIndex()); /* * We are going to attempt to renew the retention lease. If this fails it is either because the retention lease does not * exist, or something else happened. If the retention lease does not exist, we will attempt to add the retention lease * again. If that fails, it had better not be because the retention lease already exists. Either way, we will attempt to * renew again on the next scheduled execution. */ final ActionListener<RetentionLeaseActions.Response> listener = ActionListener.wrap( r -> {}, e -> { /* * We have to guard against the possibility that the shard follow node task has been stopped and the retention * lease deliberately removed via the act of unfollowing. Note that the order of operations is important in * TransportUnfollowAction. There, we first stop the shard follow node task, and then remove the retention * leases on the leader. This means that if we end up here with the retention lease not existing because of an * unfollow action, then we know that the unfollow action has already stopped the shard follow node task and * there is no race condition with the unfollow action. */ if (isCancelled() || isCompleted()) { return; } final Throwable cause = ExceptionsHelper.unwrapCause(e); logRetentionLeaseFailure(retentionLeaseId, cause); // noinspection StatementWithEmptyBody if (cause instanceof RetentionLeaseNotFoundException) { // note that we do not need to mark as system context here as that is restored from the original renew logger.trace( "{} background adding retention lease [{}] while following", params.getFollowShardId(), retentionLeaseId); try { final ActionListener<RetentionLeaseActions.Response> wrappedListener = ActionListener.wrap( r -> {}, inner -> { /* * If this fails that the retention lease already exists, something highly unusual is * going on. Log it, and renew again after another renew interval has passed. */ final Throwable innerCause = ExceptionsHelper.unwrapCause(inner); logRetentionLeaseFailure(retentionLeaseId, innerCause); }); CcrRetentionLeases.asyncAddRetentionLease( params.getLeaderShardId(), retentionLeaseId, followerGlobalCheckpoint.getAsLong(), remoteClient(params), wrappedListener); } catch (NoSuchRemoteClusterException ignored) { // we will attempt to renew again after another renew interval has passed } } else { // if something else happened, we will attempt to renew again after another renew interval has passed } }); return threadPool.scheduleWithFixedDelay( () -> { final ThreadContext threadContext = threadPool.getThreadContext(); try (ThreadContext.StoredContext ignore = threadContext.stashContext()) { // we have to execute under the system context so that if security is enabled the management is authorized threadContext.markAsSystemContext(); logger.trace( "{} background renewing retention lease [{}] while following", params.getFollowShardId(), retentionLeaseId); CcrRetentionLeases.asyncRenewRetentionLease( params.getLeaderShardId(), retentionLeaseId, followerGlobalCheckpoint.getAsLong(), remoteClient(params), listener); } }, retentionLeaseRenewInterval, Ccr.CCR_THREAD_POOL_NAME); }	should we log the exception here?
* @param blobName blob name */ public T readBlob(BlobContainer blobContainer, String blobName) throws IOException { final BytesReference bytes = Streams.readFully(blobContainer.readBlob(blobName)); final String resourceDesc = "ChecksumBlobStoreFormat.readBlob(blob=\\\\"" + blobName + "\\\\")"; try { final IndexInput indexInput = bytes.length() > 0 ? new ByteBuffersIndexInput( new ByteBuffersDataInput(Arrays.asList(BytesReference.toByteBuffers(bytes))), resourceDesc) : new ByteArrayIndexInput(resourceDesc, BytesRef.EMPTY_BYTES); CodecUtil.checksumEntireFile(indexInput); CodecUtil.checkHeader(indexInput, codec, VERSION, VERSION); long filePointer = indexInput.getFilePointer(); long contentSize = indexInput.length() - CodecUtil.footerLength() - filePointer; try (XContentParser parser = XContentHelper.createParser(namedXContentRegistry, LoggingDeprecationHandler.INSTANCE, bytes.slice((int) filePointer, (int) contentSize), XContentType.SMILE)) { return reader.apply(parser); } } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) { // we trick this into a dedicated exception with the original stacktrace throw new CorruptStateException(ex); } } /** * Writes blob in atomic manner with resolving the blob name using {@link #blobName}	bytebuffersdatainput throws if passed an empty list of buffers so we need this special case handling to throw consistent exceptions when the checksum fails because a blob was truncated to empty for some reason.
@Override public Collection<Object> createComponents( Client client, ClusterService clusterService, ThreadPool threadPool, ResourceWatcherService resourceWatcherService, ScriptService scriptService, NamedXContentRegistry xContentRegistry, Environment environment, NodeEnvironment nodeEnvironment, NamedWriteableRegistry namedWriteableRegistry, IndexNameExpressionResolver indexNameExpressionResolver, Supplier<RepositoriesService> repositoriesServiceSupplier ) { healthIndicatorServices.add(new FixedStatusHealthIndicatorService1(clusterService)); healthIndicatorServices.add(new FixedStatusHealthIndicatorService2(clusterService)); healthIndicatorServices.add(new FixedStatusHealthIndicatorService3(clusterService)); return new ArrayList<>(healthIndicatorServices); }	could we use real values here? i gather we need to stub the components and such. could we use data/ shards_availability/ilm even if they return a status we configure? this 1/2/3 naming and fictional 1/2/3 components are quite hard to read for me (and in case of a test failure i wouldn't know what's correct and what isn't)
protected void doExecute(Task task, Request request, ActionListener<Response> listener) { listener.onResponse( new Response( clusterService.getClusterName(), healthService.getHealth(request.componentName, request.indicatorName, request.computeDetails), request.componentName == null && request.indicatorName == null ) ); }	shall we add a comment to document the why for this &&? (ie. we're drilling down so we don't know don't display the top level status as it would be misleading to show a top level status even though we just evaluated some indicators - something along those lines?)
public List<HealthComponentResult> getHealth(String componentName, String indicatorName, boolean computeDetails) { return List.copyOf( healthIndicatorServices.stream() .filter(service -> componentName == null || service.component().equals(componentName)) .filter(service -> indicatorName == null || service.name().equals(indicatorName)) .map(service -> service.calculate(computeDetails)) .collect( groupingBy( HealthIndicatorResult::component, TreeMap::new, collectingAndThen( toList(), indicators -> HealthService.createComponentFromIndicators( indicators, componentName == null || indicatorName == null ) ) ) ) .values() ); }	do we 404 anywhere if the component/indicator combo is invalid? ie. data/blabla ?
public List<HealthComponentResult> getHealth(String componentName, String indicatorName, boolean computeDetails) { return List.copyOf( healthIndicatorServices.stream() .filter(service -> componentName == null || service.component().equals(componentName)) .filter(service -> indicatorName == null || service.name().equals(indicatorName)) .map(service -> service.calculate(computeDetails)) .collect( groupingBy( HealthIndicatorResult::component, TreeMap::new, collectingAndThen( toList(), indicators -> HealthService.createComponentFromIndicators( indicators, componentName == null || indicatorName == null ) ) ) ) .values() ); }	is this || operation needed since we have computedetails passed in?
static HealthComponentResult createComponentFromIndicators(List<HealthIndicatorResult> indicators, boolean showComponentSummary) { assert indicators.size() > 0 : "Component should not be non empty"; assert indicators.stream().map(HealthIndicatorResult::component).distinct().count() == 1L : "Should not mix indicators from different components"; assert findDuplicatesByName(indicators).isEmpty() : String.format( Locale.ROOT, "Found multiple indicators with the same name within the %s component: %s", indicators.get(0).component(), findDuplicatesByName(indicators) ); return new HealthComponentResult( indicators.get(0).component(), showComponentSummary ? HealthStatus.merge(indicators.stream().map(HealthIndicatorResult::status)) : null, indicators, showComponentSummary ); }	can we avoid checking this flag again inside healthcomponentresult as we're already doing this here? ie. mark healthcomponentresult#statusas @nullable and only display it if it's non-null? this would avoid leaking showcomponentsummary into healthcomponentresult
private Map<String, DocumentField> readFields(StreamInput in) throws IOException { Map<String, DocumentField> fields = null; int size = in.readVInt(); if (size == 0) { fields = new HashMap<>(); } else { fields = new HashMap<>(size); for (int i = 0; i < size; i++) { DocumentField field = DocumentField.readDocumentField(in); fields.put(field.getName(), field); } } return fields; }	should be drop public modifier here?
private GetResult innerGetLoadFromStoredFields(String type, String id, String[] gFields, FetchSourceContext fetchSourceContext, Engine.GetResult get, MapperService mapperService) { Map<String, DocumentField> nonMetaDataFields = null; Map<String, DocumentField> metaDataFields = null; BytesReference source = null; DocIdAndVersion docIdAndVersion = get.docIdAndVersion(); FieldsVisitor fieldVisitor = buildFieldsVisitors(gFields, fetchSourceContext); if (fieldVisitor != null) { try { docIdAndVersion.reader.document(docIdAndVersion.docId, fieldVisitor); } catch (IOException e) { throw new ElasticsearchException("Failed to get type [" + type + "] and id [" + id + "]", e); } source = fieldVisitor.source(); if (!fieldVisitor.fields().isEmpty()) { fieldVisitor.postProcess(mapperService); nonMetaDataFields = new HashMap<>(); metaDataFields = new HashMap<>(); for (Map.Entry<String, List<Object>> entry : fieldVisitor.fields().entrySet()) { boolean isMetadataField = MapperService.isMetadataField(entry.getKey()); if (MapperService.isMetadataField(entry.getKey())) { metaDataFields.put(entry.getKey(), new DocumentField(entry.getKey(), entry.getValue())); } else { nonMetaDataFields.put(entry.getKey(), new DocumentField(entry.getKey(), entry.getValue())); } } } } DocumentMapper docMapper = mapperService.documentMapper(); if (gFields != null && gFields.length > 0) { for (String field : gFields) { Mapper fieldMapper = docMapper.mappers().getMapper(field); if (fieldMapper == null) { if (docMapper.objectMappers().get(field) != null) { // Only fail if we know it is a object field, missing paths / fields shouldn't fail. throw new IllegalArgumentException("field [" + field + "] isn't a leaf field"); } } } } if (!fetchSourceContext.fetchSource()) { source = null; } else if (fetchSourceContext.includes().length > 0 || fetchSourceContext.excludes().length > 0) { Map<String, Object> sourceAsMap; XContentType sourceContentType = null; // TODO: The source might parsed and available in the sourceLookup but that one uses unordered maps so different. Do we care? Tuple<XContentType, Map<String, Object>> typeMapTuple = XContentHelper.convertToMap(source, true); sourceContentType = typeMapTuple.v1(); sourceAsMap = typeMapTuple.v2(); sourceAsMap = XContentMapValues.filter(sourceAsMap, fetchSourceContext.includes(), fetchSourceContext.excludes()); try { source = BytesReference.bytes(XContentFactory.contentBuilder(sourceContentType).map(sourceAsMap)); } catch (IOException e) { throw new ElasticsearchException("Failed to get type [" + type + "] and id [" + id + "] with includes/excludes set", e); } } return new GetResult(shardId.getIndexName(), type, id, get.docIdAndVersion().seqNo, get.docIdAndVersion().primaryTerm, get.version(), get.exists(), source, nonMetaDataFields, metaDataFields); }	looks like ismetadatafield variable is never used
public void testToXContent() throws IOException { { GetResult getResult = new GetResult("index", "type", "id", 0, 1, 1, true, new BytesArray("{ \\\\"field1\\\\" : " + "\\\\"value1\\\\", \\\\"field2\\\\":\\\\"value2\\\\"}"), singletonMap("field1", new DocumentField("field1", singletonList("value1"))),singletonMap("field1", new DocumentField("metafield", singletonList("metavalue")))); String output = Strings.toString(getResult); assertEquals("{\\\\"_index\\\\":\\\\"index\\\\",\\\\"_type\\\\":\\\\"type\\\\",\\\\"_id\\\\":\\\\"id\\\\",\\\\"_version\\\\":1,\\\\"_seq_no\\\\":0,\\\\"_primary_term\\\\":1," + "\\\\"metafield\\\\":\\\\"metavalue\\\\",\\\\"found\\\\":true,\\\\"_source\\\\":{ \\\\"field1\\\\" : \\\\"value1\\\\", \\\\"field2\\\\":\\\\"value2\\\\"}," + "\\\\"fields\\\\":{\\\\"field1\\\\":[\\\\"value1\\\\"]}}", output); } { GetResult getResult = new GetResult("index", "type", "id", UNASSIGNED_SEQ_NO, 0, 1, false, null, null, null); String output = Strings.toString(getResult); assertEquals("{\\\\"_index\\\\":\\\\"index\\\\",\\\\"_type\\\\":\\\\"type\\\\",\\\\"_id\\\\":\\\\"id\\\\",\\\\"found\\\\":false}", output); } }	nit. space before singletonmap
@Override protected LogicalPlan rule(LogicalPlan p) { return p; } } abstract static class AnalyzeRule<SubPlan extends LogicalPlan> extends Rule<SubPlan, LogicalPlan> { // transformUp (post-order) - that is first children and then the node // but with a twist; only if the tree is not resolved or analyzed @Override public final LogicalPlan apply(LogicalPlan plan) { return plan.transformUp(t -> t.analyzed() || skipResolved() && t.resolved() ? t : rule(t), typeToken()); } @Override protected abstract LogicalPlan rule(SubPlan plan); protected boolean skipResolved() { return true; } }	what value does this class add?
@Override public ValueFetcher valueFetcher(MapperService mapperService, String format) { if (format != null) { throw new IllegalArgumentException("Field [" + name() + "] of type [" + typeName() + "] doesn't support formats."); } return lookup -> fieldType().value == null ? List.of() : List.of(fieldType().value); }	fieldtype.value == null ? lookup -> list.of() : lookup -> list.of(fieldtype.value)? it isn't really important, but it'd make me feel good.
private ClusterState applyCreateIndexRequestWithV2Template(final ClusterState currentState, final CreateIndexClusterStateUpdateRequest request, final boolean silent, final String templateName, final BiConsumer<Metadata.Builder, IndexMetadata> metadataTransformer) throws Exception { logger.debug("applying create index request using composable template [{}]", templateName); final Map<String, Object> mappings = resolveV2Mappings(request.mappings(), currentState, templateName, xContentRegistry); if (request.dataStreamName() != null) { String timestampField; DataStream dataStream = currentState.metadata().dataStreams().get(request.dataStreamName()); if (dataStream != null) { timestampField = dataStream.getTimeStampField().getName(); dataStream.getTimeStampField().insertTimestampFieldMapping(mappings); } else { ComposableIndexTemplate template = currentState.metadata().templatesV2().get(templateName); timestampField = template.getDataStreamTemplate().getTimestampField(); } Map<String, Object> changes = Map.of("_doc", Map.of(TimestampFieldMapper.NAME, Map.of("field_name", timestampField))); XContentHelper.update(mappings, changes, false); } final Settings aggregatedIndexSettings = aggregateIndexSettings(currentState, request, MetadataIndexTemplateService.resolveSettings(currentState.metadata(), templateName), null, settings, indexScopedSettings, shardLimitValidator); int routingNumShards = getIndexNumberOfRoutingShards(aggregatedIndexSettings, null); IndexMetadata tmpImd = buildAndValidateTemporaryIndexMetadata(currentState, aggregatedIndexSettings, request, routingNumShards); return applyCreateIndexWithTemporaryService(currentState, request, silent, null, tmpImd, mappings, indexService -> resolveAndValidateAliases(request.index(), request.aliases(), MetadataIndexTemplateService.resolveAliases(currentState.metadata(), templateName), currentState.metadata(), aliasValidator, // the context is only used for validation so it's fine to pass fake values for the // shard id and the current timestamp xContentRegistry, indexService.newQueryShardContext(0, null, () -> 0L, null)), Collections.singletonList(templateName), metadataTransformer); }	is there a case where the data stream does not exist, but we are still creating a new index within it?
@Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); if (in.getVersion().before(Version.V_1_4_0_Beta1)) { //term vector used to read & write the index twice, here and in the parent class in.readString(); } type = in.readString(); id = in.readString(); if (in.getVersion().onOrAfter(Version.V_1_4_0_Beta1)) { if (in.readBoolean()) { doc = in.readBytesReference(); } } routing = in.readOptionalString(); preference = in.readOptionalString(); long flags = in.readVLong(); flagsEnum.clear(); for (Flag flag : Flag.values()) { if ((flags & (1 << flag.ordinal())) != 0) { flagsEnum.add(flag); } } int numSelectedFields = in.readVInt(); if (numSelectedFields > 0) { selectedFields = new HashSet<>(); for (int i = 0; i < numSelectedFields; i++) { selectedFields.add(in.readString()); } } if (in.getVersion().onOrAfter(Version.V_1_5_0)) { if (in.readBoolean()) { perFieldAnalyzer = readPerFieldAnalyzer(in.readMap()); } this.realtime = in.readBoolean(); } if (in.getVersion().onOrAfter(Version.V_2_0_0)) { startTime = in.readVLong(); cache = in.readOptionalBoolean(); } }	no need for these version checks, a full cluster restart will be required for 2.0 anyway
@Override public int hashCode() { return Objects.hash(nodeId, name); } } private List<NodeView> nodes; VerifyRepositoryResponse() { } public VerifyRepositoryResponse(DiscoveryNode[] nodes) { this.nodes = Arrays.stream(nodes).map(dn -> new NodeView(dn.getId(), dn.getName())).collect(Collectors.toList()); } @Override public void readFrom(StreamInput in) throws IOException { super.readFrom(in); this.nodes = in.readList(NodeView::new); } @Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeList(nodes()); } public List<NodeView> nodes() { return nodes; } @Override public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException { builder.startObject(); { builder.startObject(NODES); { for (NodeView node : nodes) { node.toXContent(builder, params); } } builder.endObject(); } builder.endObject(); return builder; }	i think you should keep the method named the same in 6.x and master. also we *said* we'd prefer to go with getters and setters rather than the bare names for these things. as much as i personally like the names better, we said getters and setters.
public void merge(IndexMetaData indexMetaData, MergeReason reason) { internalMerge(indexMetaData, reason, false); }	it would be good to remove mapperservice#gettypeforupdate, since it is no longer used.
@Override public Literal visitParamLiteral(ParamLiteralContext ctx) { Object value = param(ctx.PARAM()); Source source = source(ctx); DataType dataType = null; try { dataType = DataTypes.fromJava(value); } catch (QlIllegalArgumentException ignored) {}; if (dataType == null) { throw new ParsingException(source, "Invalid parameter [{}]", value); } return new Literal(source, value, dataType); }	the try/catch is not needed. and we never catch exceptions and ignore them.
public RecoveryTarget target() { return status; } } private class RecoveryMonitor extends AbstractRunnable { private final long recoveryId; private final TimeValue checkInterval; private volatile long lastSeenAccessTime; private RecoveryMonitor(long recoveryId, long lastSeenAccessTime, TimeValue checkInterval) { this.recoveryId = recoveryId; this.checkInterval = checkInterval; this.lastSeenAccessTime = lastSeenAccessTime; } @Override public void onFailure(Exception e) { logger.error(() -> new ParameterizedMessage("unexpected error while monitoring recovery [{}]", recoveryId), e); } @Override protected void doRun() throws Exception { RecoveryTarget status = onGoingRecoveries.get(recoveryId); if (status == null) { logger.trace("[monitor] no status found for [{}], shutting down", recoveryId); return; } if (status.isRecoveryMonitorEnabled()) { long accessTime = status.lastAccessTime(); if (accessTime == lastSeenAccessTime) { String message = "no activity after [" + checkInterval + "]"; failRecovery(recoveryId, new RecoveryFailedException(status.state(), message, new ElasticsearchTimeoutException(message)), true // to be safe, we don't know what go stuck ); return; } lastSeenAccessTime = accessTime; } else { lastSeenAccessTime = System.nanoTime(); } logger.trace("[monitor] rescheduling check for [{}]. last access time is [{}]", recoveryId, lastSeenAccessTime); threadPool.schedule(this, checkInterval, ThreadPool.Names.GENERIC); }	i think it might be simpler to just fake the progress inside recoverytarget by returning system.nanotime()?
public void testPrimitiveMethodReferences() { assertEquals(1L, exec("long test(Function s) {return s.apply(Integer.valueOf(1));} return test(int::intValue);")); assertEquals(1L, exec("long test(Supplier s) {return s.get();} int i = 1; return test(i::intValue);")); }	it would be good to test for other primitive types.
public void testRefresh() throws IOException { { final int numberOfShards = randomIntBetween(1, 5); final int numberOfReplicas = randomIntBetween(1, 3); String index = "index"; Settings settings = Settings.builder() .put("number_of_shards", numberOfShards) .put("number_of_replicas", numberOfReplicas) .build(); createIndex(index, settings); RefreshRequest refreshRequest = new RefreshRequest(index); RefreshResponse refreshResponse = execute(refreshRequest, highLevelClient().indices()::refresh, highLevelClient().indices()::refreshAsync); assertThat(refreshResponse.getTotalShards(), equalTo(numberOfShards * (numberOfReplicas + 1))); assertThat(refreshResponse.getSuccessfulShards(), greaterThan(0)); assertThat(refreshResponse.getSuccessfulShards() % numberOfShards, equalTo(0)); assertThat(refreshResponse.getFailedShards(), equalTo(0)); assertThat(refreshResponse.getShardFailures(), equalTo(BroadcastResponse.EMPTY)); } { String nonExistentIndex = "non_existent_index"; assertFalse(indexExists(nonExistentIndex)); RefreshRequest refreshRequest = new RefreshRequest(nonExistentIndex); ElasticsearchException exception = expectThrows(ElasticsearchException.class, () -> execute(refreshRequest, highLevelClient().indices()::refresh, highLevelClient().indices()::refreshAsync)); assertEquals(RestStatus.NOT_FOUND, exception.status()); } }	i think i changed my mind on this. given that it's an integration test, let's remove the randomization which complicate things? let's just set 1 shard and 0 replica?
@Override public void writeTo(StreamOutput out) throws IOException { super.writeTo(out); out.writeVInt(totalShards); out.writeVInt(successfulShards); out.writeVInt(failedShards); out.writeVInt(shardFailures.length); for (DefaultShardOperationFailedException exp : shardFailures) { exp.writeTo(out); } }	i know that i told you to add this todo, but i looked deeper and i don't think we will address this anytime soon. let's remove it, i think what we do now is fine.
public void testSnapshotBasedRecovery() throws Exception { final String indexName = "snapshot_based_recovery"; final String repositoryName = "snapshot_based_recovery_repo"; final int numDocs = 200; switch (CLUSTER_TYPE) { case OLD: Settings.Builder settings = Settings.builder() .put(IndexMetadata.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1) .put(IndexMetadata.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0) .put(INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), "100ms") .put(SETTING_ALLOCATION_MAX_RETRY.getKey(), "0"); // fail faster createIndex(indexName, settings.build()); ensureGreen(indexName); indexDocs(indexName, numDocs); flush(indexName, true); registerRepository( repositoryName, "fs", true, Settings.builder() .put("location", "./snapshot_based_recovery") .put(BlobStoreRepository.USE_FOR_PEER_RECOVERY_SETTING.getKey(), true) .build() ); createSnapshot(repositoryName, "snap", true); updateIndexSettings(indexName, Settings.builder().put(IndexMetadata.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 1)); ensureGreen(indexName); break; case MIXED: case UPGRADED: if (FIRST_MIXED_ROUND) { String primaryNodeId = getPrimaryNodeIdOfShard(indexName, 0); Version primaryNodeVersion = getNodeVersion(primaryNodeId); // Sometimes the primary shard ends on the upgraded node (i.e. after a rebalance) // This causes issues when removing and adding replicas, since then we cannot allocate to any of the old nodes. // That is an issue only for the first mixed round. // In that case we exclude the upgraded node from the shard allocation and cancel the shard to force moving // the primary to a node in the old version, this allows adding replicas in the first mixed round. if (primaryNodeVersion.after(UPGRADE_FROM_VERSION)) { updateIndexSettings( indexName, Settings.builder() .put("index.routing.allocation.exclude._id", primaryNodeId) .build() ); cancelShard(indexName, 0, primaryNodeId); String currentPrimaryNodeId = getPrimaryNodeIdOfShard(indexName, 0); assertThat(getNodeVersion(currentPrimaryNodeId), is(equalTo(UPGRADE_FROM_VERSION))); updateIndexSettings( indexName, Settings.builder() .putNull("index.routing.allocation.exclude._id") .build() ); } } // Drop replicas updateIndexSettings(indexName, Settings.builder().put(IndexMetadata.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0)); ensureGreen(indexName); updateIndexSettings(indexName, Settings.builder().put(IndexMetadata.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 1)); ensureGreen(indexName); assertMatchAllReturnsAllDocuments(indexName, numDocs); assertMatchQueryReturnsAllDocuments(indexName, numDocs); break; default: throw new IllegalStateException("unknown type " + CLUSTER_TYPE); } }	i think this risks balancing back the shard to the upgraded node. you can disable balancing and reenable it after reestablishing replica. or we could just keep the exclusion until the next round. i think i prefer the latter.
private User maybeConsolidateRolesForUser(User user) { if (User.isInternal(user)) { return user; } else if (isAnonymousUserEnabled && anonymousUser.equals(user) == false) { if (anonymousUser.roles().length == 0) { throw new IllegalStateException("anonymous is only enabled when the anonymous user has roles"); } User userWithMergedRoles = user.withRoles(mergeRoles(user.roles(), anonymousUser.roles())); if (user.isRunAs()) { final User authUserWithMergedRoles = user.authenticatedUser().withRoles( mergeRoles(user.authenticatedUser().roles(), anonymousUser.roles())); userWithMergedRoles = new User(userWithMergedRoles, authUserWithMergedRoles); } return userWithMergedRoles; } else { return dedupUserRoles(user); } }	do we have to do the length check ? can't this always be user.withroles(set.of(user.roles()).toarray(new string[0]));
public static <K, T extends Diffable<T>> MapDiff<K, T, ImmutableOpenMap<K, T>> readImmutableOpenMapDiff(StreamInput in, KeySerializer<K> keySerializer, Reader<T> reader, Reader<Diff<T>> diffReader) throws IOException { return new ImmutableOpenMapDiff<>(in, keySerializer, new DiffableValueReader<>(reader, diffReader)); }	i don't think this change is needed in the file? because the new method isn't used?
private void checkClusterHealthWithRetries(Environment env, Terminal terminal, String username, SecureString password, int retries, boolean force) throws Exception { CommandLineHttpClient client = clientFunction.apply(env); try { client.checkClusterHealthWithRetriesWaitingForCluster(username, password, 0, force); } catch (Exception e) { if (e instanceof ElasticsearchStatusException && ((ElasticsearchStatusException) e).status().getStatus() == HttpURLConnection.HTTP_UNAUTHORIZED || ((ElasticsearchStatusException) e).status().getStatus() == HttpURLConnection.HTTP_FORBIDDEN) { // We try to write the roles file first and then the users one, but theoretically we could have loaded the users // before we have actually loaded the roles so we also retry on 403 ( temp user is found but has no roles ) if ( retries > 0 ) { terminal.println( Terminal.Verbosity.VERBOSE, "Unexpected http status while attempting to determine cluster health. Will retry at most " + retries + " more times." ); Thread.sleep(1000); retries -= 1; checkClusterHealthWithRetries(env, terminal, username, password, retries, force); } else { throw new UserException( ExitCodes.DATA_ERROR, "Failed to determine the health of the cluster. Unexpected http status."); } } else { terminal.errorPrintln("Failed to determine the health of the cluster. Cluster health is currently RED."); terminal.errorPrintln("This means that some cluster data is unavailable and your cluster is not fully functional."); terminal.errorPrintln("The cluster logs (https://www.elastic.co/guide/en/elasticsearch/reference/" + Version.CURRENT.major + "." + Version.CURRENT.minor + "/logging.html)" + " might contain information/indications for the underlying cause"); terminal.errorPrintln( "It is recommended that you resolve the issues with your cluster before continuing"); terminal.errorPrintln("It is very likely that the command will fail when run against an unhealthy cluster."); terminal.errorPrintln(""); terminal.errorPrintln("If you still want to attempt to execute this command against an unhealthy cluster," + " you can pass the `-f` parameter."); throw new UserException(ExitCodes.UNAVAILABLE, "Failed to determine the health of the cluster. Cluster health is currently RED."); } } }	this doesn't work now, because checkclusterhealthwithretrieswaitingforcluster wraps exception in illegalstateexceptions. i'm starting to regret my suggestion to have a common approach for the cluster health polling. my suggestion now is to do whatever works that follows the following requirements: for baserunassuperusercommand: * fail on connection exception (node is down) * retry on http_unauthorized and http_forbidden for bootstrappasswordandenrollmenttokenforinitialnode: * retry on connection exception * fail on any exception i think it's better that these methods be at the commandlinehttpclient level, to aid with discoverability, and that they reuse code, but i'm not going to gripe about it.
public void deleteResponse(AsyncExecutionId asyncExecutionId, ActionListener<DeleteResponse> listener) { try { DeleteRequest request = new DeleteRequest(index).id(asyncExecutionId.getDocId()); client.delete(request, listener); } catch(Exception e) { listener.onFailure(e); } } /** * Returns the {@link AsyncTask} if the provided <code>asyncTaskId</code> * is registered in the task manager, <code>null</code> otherwise. * * This method throws a {@link ResourceNotFoundException}	can you add an option on gettask instead to enable/disable the authentication check ?
public List<Object> fetchValues(SourceLookup lookup, Set<String> ignoredFields) { List<Object> values = new ArrayList<>(); for (String path : sourcePaths) { if (ignoredFields != null && ignoredFields.contains(path)) { continue; } Object sourceValue = lookup.extractValue(path, nullValue); if (sourceValue == null) { return List.of(); } values.addAll((List<?>) parseSourceValue(sourceValue)); } return values; } /** * Given a value that has been extracted from a document's source, parse it into a standard * format. This parsing logic should closely mirror the value parsing in * {@link FieldMapper#parseCreateField} or {@link FieldMapper#parse}	small comment, could we change the calling code to make sure this parameter is never null? i feel like that'd be clearer/ easier to handle for implementors.
static DeprecationIssue checkClusterRoutingRequireSetting(final Settings settings, final PluginsAndModules pluginsAndModules, final ClusterState clusterState, final XPackLicenseState licenseState) { return checkRemovedSetting(settings, CLUSTER_ROUTING_REQUIRE_SETTING, "https://www.elastic.co/guide/en/elasticsearch/reference/master/migrating-8.0.html#breaking_80_allocation_changes", DeprecationIssue.Level.CRITICAL ); }	this seems to point to an unrelated breaking change in the docs, at least in the docs that are actively hosted on the site. will this breaking change be documented at this doc url or should it be added/updated?
@After public void stopWebservice() throws Exception { if (webServer != null) { webServer.close(); } }	fix for slightly different npe: java.lang.nullpointerexception at org.elasticsearch.xpack.watcher.actions.webhook.webhookhttpsintegrationtests.stopwebservice(webhookhttpsintegrationtests.java:75) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498)
private static boolean canSetPolicy(StepKey currentStepKey, LifecyclePolicy currentPolicy, LifecyclePolicy newPolicy) { if (currentPolicy != null) { if (currentPolicy.isActionSafe(currentStepKey)) { return true; } else { // Index is in an unsafe action so fail it if the action has changed between oldPolicy and newPolicy return isActionChanged(currentStepKey, currentPolicy, newPolicy) == false; } } else { // Index not previously managed by ILM so safe to change policy return true; } }	since getactionfrompolicy can, theoretically, return null, would it be safer to switch these around to newaction.equals(currentaction) == false
public void testSetPolicyForIndexIndexInUnsafeActionUnchanged() { long now = randomNonNegativeLong(); String indexName = randomAlphaOfLength(10); String oldPolicyName = "old_policy"; String newPolicyName = "new_policy"; StepKey currentStep = new StepKey(randomAlphaOfLength(10), MockAction.NAME, randomAlphaOfLength(10)); LifecyclePolicy oldPolicy = createPolicy(oldPolicyName, null, currentStep); LifecyclePolicy newPolicy = createPolicy(newPolicyName, null, currentStep); Settings.Builder indexSettingsBuilder = Settings.builder().put(LifecycleSettings.LIFECYCLE_NAME, oldPolicyName) .put(LifecycleSettings.LIFECYCLE_PHASE, currentStep.getPhase()) .put(LifecycleSettings.LIFECYCLE_ACTION, currentStep.getAction()) .put(LifecycleSettings.LIFECYCLE_STEP, currentStep.getName()).put(LifecycleSettings.LIFECYCLE_SKIP, true); List<LifecyclePolicyMetadata> policyMetadatas = new ArrayList<>(); policyMetadatas.add(new LifecyclePolicyMetadata(oldPolicy, Collections.emptyMap())); ClusterState clusterState = buildClusterState(indexName, indexSettingsBuilder, policyMetadatas); Index index = clusterState.metaData().index(indexName).getIndex(); Index[] indices = new Index[] { index }; List<String> failedIndexes = new ArrayList<>(); ClusterState newClusterState = IndexLifecycleRunner.setPolicyForIndexes(newPolicyName, indices, clusterState, newPolicy, failedIndexes); assertTrue(failedIndexes.isEmpty()); assertClusterStateOnPolicy(clusterState, index, newPolicyName, currentStep, currentStep, newClusterState, now); }	should we also add a change here to remove the action from the policy?
void doTestRawValue(XContent source) throws Exception { ByteArrayOutputStream os = new ByteArrayOutputStream(); try (XContentGenerator generator = source.createGenerator(os)) { generator.writeStartObject(); generator.writeFieldName("foo"); generator.writeNull(); generator.writeEndObject(); } final byte[] rawData = os.toByteArray(); os = new ByteArrayOutputStream(); try (XContentGenerator generator = xcontentType().xContent().createGenerator(os)) { generator.writeRawValue(new BytesArray(rawData).streamInput(), source.type()); } try (XContentParser parser = xcontentType().xContent() .createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, os.toByteArray())) { assertEquals(Token.START_OBJECT, parser.nextToken()); assertEquals(Token.FIELD_NAME, parser.nextToken()); assertEquals("foo", parser.currentName()); assertEquals(Token.VALUE_NULL, parser.nextToken()); assertEquals(Token.END_OBJECT, parser.nextToken()); assertNull(parser.nextToken()); } final AtomicBoolean closed = new AtomicBoolean(false); os = new ByteArrayOutputStream() { @Override public void close() { closed.set(true); } }; try (XContentGenerator generator = xcontentType().xContent().createGenerator(os)) { generator.writeStartObject(); generator.writeFieldName("test"); generator.writeRawValue(new BytesArray(rawData).streamInput(), source.type()); assertFalse(closed.get()); generator.writeEndObject(); } try (XContentParser parser = xcontentType().xContent() .createParser(NamedXContentRegistry.EMPTY, DeprecationHandler.THROW_UNSUPPORTED_OPERATION, os.toByteArray())) { assertEquals(Token.START_OBJECT, parser.nextToken()); assertEquals(Token.FIELD_NAME, parser.nextToken()); assertEquals("test", parser.currentName()); assertEquals(Token.START_OBJECT, parser.nextToken()); assertEquals(Token.FIELD_NAME, parser.nextToken()); assertEquals("foo", parser.currentName()); assertEquals(Token.VALUE_NULL, parser.nextToken()); assertEquals(Token.END_OBJECT, parser.nextToken()); assertEquals(Token.END_OBJECT, parser.nextToken()); assertNull(parser.nextToken()); } }	what do you think about adding a message here? e.g. suggestion assertfalse("generator should not have close the output stream", closed.get());